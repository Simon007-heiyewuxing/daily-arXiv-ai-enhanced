<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 107]
- [cs.DB](#cs.DB) [Total: 9]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy](https://arxiv.org/abs/2602.04994)
*Zhuosen Bao,Xia Du,Zheng Lin,Jizhe Zhou,Zihan Fang,Jiening Wu,Yuxin Zhang,Zhe Chen,Chi-man Pun,Wei Ni,Jun Luo*

Main category: cs.CV

TL;DR: SIDeR通过将人脸分解为身份向量与视觉语义，并在扩散模型潜空间进行语义引导重构与动量扰动优化，实现视觉匿名化且保持机器识别一致，支持带密码的恢复；在公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别在在线金融与身份验证等场景的广泛应用，需要在存储与传输阶段彻底解耦身份信息与视觉表征以保护隐私，同时允许经授权的隐私恢复。

Method: 将人脸图像分解为机器可识别的身份特征向量与视觉语义外观组件；在扩散模型潜空间中通过语义引导重构生成匿名对抗样本；引入动量驱动的无约束扰动优化和语义-视觉平衡因子以合成多样且自然的样本；并实现基于密码的可逆恢复。

Result: 在CelebA-HQ与FFHQ数据集上，SIDeR在黑盒场景下实现99%攻击成功率，并在PSNR恢复质量上较基线方法提升41.28%。

Conclusion: SIDeR有效在视觉上匿名化人脸同时保持机器识别一致性，并支持带密码的恢复；在黑盒攻击下取得高成功率并提升恢复质量。

Abstract: With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.

</details>


### [2] [UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking](https://arxiv.org/abs/2602.05037)
*Bishoy Galoaa,Xiangyu Bai,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: UniTrack是一个通用的可微图论损失，能无改动地整合进现有MOT系统，通过统一优化检测与关联目标显著减少身份切换并提高IDF1与MOTA。


<details>
  <summary>Details</summary>
Motivation: 当前基于图的方法多集中在改造跟踪网络或后处理，缺乏一个通用的、可直接优化跟踪指标（如ID保真度与MOTA）的训练目标；UniTrack旨在填补这一空白，提供可插拔的统一损失以提升不同架构的跟踪表现。

Method: 设计一个端到端可微的图表示与损失，将检测精度、身份保持与时空一致性融合为单一目标。该损失通过构建节点（检测/轨迹）和边（相似性、运动一致性）并在训练中进行图梯度回传，令各类跟踪模型学习整体的运动连续性和身份关系。

Result: 在多种跟踪模型（Trackformer, MOTR, FairMOT, ByteTrack, GTR, MOTE）与多个基准上取得一致提升；在复杂基准中最多减少53%身份切换，IDF1提升12%，GTR在SportsMOT上MOTA最多提升9.7%。

Conclusion: UniTrack作为一个可插拔的图论损失函数，通过统一可微分学习直接优化跟踪目标，从而在不改动现有MOT架构的情况下显著提升多目标跟踪性能。

Abstract: We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\% reduction in identity switches and 12\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\% MOTA on SportsMOT.

</details>


### [3] [VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models](https://arxiv.org/abs/2602.05049)
*Yiye Chen,Yanan Jian,Xiaoyi Dong,Shuxin Cao,Jing Wu,Patricio Vela,Benjamin E. Lundell,Dongdong Chen*

Main category: cs.CV

TL;DR: 识别并修正VLA模型中视觉依赖不足的问题：通过在替代任务上进行偏好优化增强视觉-动作对齐，再用潜在空间蒸馏把改进迁移到指令跟随，提升了离散与连续动作任务的表现。


<details>
  <summary>Details</summary>
Motivation: 观察到将大规模视觉-语言模型扩展到动作空间时会产生视觉-动作错位，即动作预测对当前视觉状态依赖性不足，导致不可靠的动作输出，因此需要显式加强视觉条件化。

Method: 首先在一个轨迹跟踪的替代任务上通过偏好优化（preference optimization）使动作预测与视觉输入对齐，然后在监督微调阶段通过潜在空间蒸馏将这种对齐转移到指令跟随任务上。

Result: 在OpenVLA（离散）和OpenVLA-OFT（连续）数据集上，方法提高了视觉条件化程度并带来一致的任务性能提升，无需修改网络结构或新增数据。

Conclusion: 论文提出通过增强视觉条件化提高视觉-语言-动作（VLA）模型性能，证明成功的轨迹具有更强的视觉依赖性，并提出一种不改动模型结构或额外收集数据的训练框架，在离散与连续动作空间上均有提升。

Abstract: Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .

</details>


### [4] [Food Portion Estimation: From Pixels to Calories](https://arxiv.org/abs/2602.05078)
*Gautham Vinod,Fengqing Zhu*

Main category: cs.CV

TL;DR: 本文回顾了三类主要策略——辅助深度/多视角、模型先验、深度学习——用于从图像估计食物份量，比较了它们的优缺点并提出未来研究方向：数据多样性、鲁棒性和移动端实用化。


<details>
  <summary>Details</summary>
Motivation: 准确、便捷地监测个体饮食对于慢性病和肥胖的预防与管理至关重要，而仅依赖二维图像会丧失深度信息，导致分量估计误差。因此需要探索能从图像恢复三维信息或替代策略以提升估算精度。

Method: 本文通过文献回顾和方法分类，比较和分析了不同策略在食物份量估计中的实现方式和性能。重点讨论了深度信息的获取（深度摄像头、结构光、学习型深度估计）、多视角重建、模板与几何先验、以及深度学习模型在单目估计与多模态输入融合上的架构设计。

Result: 综述显示：1）使用真实深度或多视角方法通常精度最高，但硬件和使用便捷性较差；2）模板和模型先验在受控场景表现良好，但泛化性有限；3）深度学习（尤其多模态融合）在没有额外硬件时可显著提升单目估计性能，但受训练数据质量限制。综述建议构建大规模、多样化标注数据集、研究跨域泛化技术和开发轻量级实时模型。

Conclusion: 该综述指出，基于图像的膳食评估核心难点在于从二维图像估计食物三维尺寸。现有工作可分为三类策略：使用辅助输入（深度图、多视角）、基于模型的方法（模板匹配、形状先验）和基于深度学习的方法（单目或与辅助输入结合）。每种方法在精度、实用性和部署复杂度之间存在权衡，未来研究应聚焦于鲁棒性、数据集多样性和轻量化模型以便移动端实时应用。

Abstract: Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.

</details>


### [5] [Visual concept ranking uncovers medical shortcuts used by large multimodal models](https://arxiv.org/abs/2602.05096)
*Joseph D. Janizek,Sonnet Xu,Junayd Lateef,Roxana Daneshjou*

Main category: cs.CV

TL;DR: 本文提出VCR方法，用于识别并排序LMMs在医疗图像任务中依赖的视觉概念，发现模型在示例提示下存在群体性能差异，并通过人工干预验证了若干关键视觉特征对模型预测的因果作用。


<details>
  <summary>Details</summary>
Motivation: 在医疗等安全关键领域，需要审计方法来发现模型在实际部署可能出现的严重缺陷，尤其是那些导致不同人口群体间不公平或错误诊断的视觉偏差。现有解释方法难以直接针对大规模多模态模型和提示驱动的设置，而VCR旨在填补这一空白，帮助研究者自动发现模型依赖的视觉特征并验证其影响。

Method: 提出Visual Concept Ranking（VCR）：一种自动识别并排序与模型决策高度相关的图像视觉概念的方法。作者在皮肤病理学图像的良恶性分类任务上主要验证该方法，同时辅以胸片和自然图像实验。VCR通过分析模型在包含示例提示下对不同视觉概念响应的变化，生成可供验证的假设，随后通过人工修改图像中对应概念（干预）来测试其对模型输出的影响，以确认概念的因果作用。

Result: 实验证明：1）在带示例的提示设置下，LMMs在不同人口子群间（例如皮肤色素不同群体）表现出显著性能差异；2）VCR能有效识别与模型预测显著相关的视觉概念；3）通过对这些概念的人工干预，验证了它们对模型决策的因果影响，并解释了部分性能差异。

Conclusion: 本文提出了一种用于大规模多模态模型（LMMs）中识别重要视觉概念的方法——Visual Concept Ranking（VCR），并将其用于审计医疗相关任务中的模型行为。研究发现，在使用示例演示（in-context examples）提示时，LMMs在不同人口子群之间表现存在意外差异；通过VCR生成的关于视觉特征依赖性的假设，研究者能够通过人工干预进行验证，揭示模型对某些视觉概念的过度依赖或忽视，从而解释性能差异。

Abstract: Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.

</details>


### [6] [CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology](https://arxiv.org/abs/2602.05126)
*Weiyi Qin,Yingci Liu-Swetz,Shiwei Tan,Hao Wang*

Main category: cs.CV

TL;DR: CLEAR-HPV 是一个基于注意力的 MIL 框架，通过在注意力加权的潜在空间中进行无监督概念发现，生成可解释的概念映射并将切片压缩为紧凑的概念比例向量，从而在保持预测性能的同时显著提升形态学可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力 MIL 在整片预测上表现良好但缺乏形态学可解释性，因此需要一种在不需要概念标签的情况下发现可解释病理学概念的方法。

Method: 在基于注意力的 MIL 潜在空间上重构表示，利用注意力加权潜在特征进行无监督概念发现、生成概念空间映射，并将切片表示为概念比例向量。

Result: 自动发现角化（keratinizing）、基底样（basaloid）和间质（stromal）等形态学概念；将高维特征（如1536维）压缩为10个可解释概念并保持预测性能；在 TCGA-HNSCC、TCGA-CESC 和 CPTAC-HNSCC 数据集上稳定泛化。

Conclusion: CLEAR-HPV 提供了可解释且紧凑的概念级表示，保留 MIL 嵌入的预测信息，同时降低维度并发现表型概念。

Abstract: Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.

</details>


### [7] [ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation](https://arxiv.org/abs/2602.05132)
*Jia Li,Wenjie Zhao,Shijian Deng,Bolin Lai,Yuheng Wu,RUijia Chen,Jon E. Froehlich,Yuhang Zhao,Yapeng Tian*

Main category: cs.CV

TL;DR: ARGaze用带固定长度历史的自回归transformer解码器做在线第一人称凝视估计，利用凝视时间连续性实现有界资源下的SOTA流式推理。


<details>
  <summary>Details</summary>
Motivation: 在线第一人称凝视估计缺乏明显的头部/眼球信号，需要从稀疏间接线索推断注意力。作者观察到在目标导向活动中凝视具有强时间连续性，因此近期凝视提供强先验用于预测当前凝视。

Method: 设计一个transformer解码器，在每个时间步输入当前视觉特征和固定长度的Gaze Context Window（最近凝视目标估计），并强制因果性以支持有界资源的流式推理；借鉴视觉-语言模型中的vision-conditioned autoregressive decoding思想。

Result: 在多个egocentric基准上以在线评估方式实现SOTA性能；消融实验验证自回归建模和有限凝视历史对鲁棒预测至关重要；计划开源代码和预训练模型。

Conclusion: 本文提出ARGaze，将在线第一人称凝视估计重写为自回归序列预测，利用过去凝视窗口作为强先验以提高实时推理性能，实验显示在多数据集上达成SOTA。

Abstract: Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.

</details>


### [8] [AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves](https://arxiv.org/abs/2602.05159)
*Wenhui Cui,Ziyi Kou,Chuan Qin,Ergys Ristani,Li Guan*

Main category: cs.CV

TL;DR: 本文首次系统评估视觉手部追踪模型在传感手套上的表现，发现裸手模型受外观差距影响严重，提出AirGlove方法以有限数据泛化到新手套，实验证明其显著提高了手势姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 动机是：尽管传感手套提供丰富的信号，传感器方法受限于信号与校准质量；而视觉预训练模型在裸手上表现很好，但对外观差异大的手套缺乏研究与适应性，因此需要研究视觉方法如何泛化到多样的手套设计。

Method: 文章首先系统评估了基于视觉的手部追踪模型在传感手套场景下的零样本与微调表现，发现性能下降严重；随后提出AirGlove方法，通过利用已有手套的表征并采用某种泛化学习策略（如域适应或基于少量样本的适应机制）将模型泛化到新手套，实验在多种传感手套上进行验证。

Result: 实验结果表明：现有裸手模型在手套上的性能大幅下降；AirGlove在多种传感手套上均显著提升了姿态估计准确性，并在零样本或少样本设置下优于基线方法。

Conclusion: 本文结论是：现有面向裸手的视觉手部追踪模型在带有手套的手部（传感手套）上性能显著下降，而提出的AirGlove方法能通过利用已有手套的数据并针对少量新手套样本进行泛化训练，从而显著提升在新手套设计上的手部姿态估计性能。

Abstract: Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.

</details>


### [9] [SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition](https://arxiv.org/abs/2602.05162)
*Anay Majee,Rishabh Iyer*

Main category: cs.CV

TL;DR: SHaSaM用子模组合的困难样本挖掘与基于子模条件互信息的损失函数抑制敏感属性学习，兼顾公平性与性能，且在CelebA/UTKFace上取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法受属性组间数据不平衡影响并过度强调敏感属性，导致不公平和性能下降，需一种在表示学习中同时抑制敏感属性依赖与挖掘困难样本的统一机制。

Method: 提出两阶段组合方法：SHaSaM-MINE使用子模子集选择策略挖掘困难正负样本以缓解数据不平衡；SHaSaM-LEARN基于子模条件互信息（Submodular Conditional Mutual Information）设计一族组合损失，最大化目标类间判别边界并最小化敏感属性影响。

Result: 在CelebA和UTKFace上，SHaSaM在公平性（Equalized Odds）上最多提升2.7个百分点，同时准确率提升3.5%，并在更少训练轮次内收敛，优于现有方法。

Conclusion: SHaSaM通过将公平表示学习建模为子模困难样本挖掘问题，有效减轻了敏感属性引起的数据不平衡与模型偏见，从而在保证甚至提升准确率的同时显著改善了公平性指标。

Abstract: Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.

</details>


### [10] [LOBSTgER-enhance: an underwater image enhancement pipeline](https://arxiv.org/abs/2602.05163)
*Andreas Mentzelopoulos,Keith Ellenbogen*

Main category: cs.CV

TL;DR: 利用合成退化+扩散逆向生成，从头训练小型模型，在少量高质量水下摄影图像上实现高感知质量的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 水下摄影常受对比度降低、模糊和颜色失真影响，现有后处理流程复杂且耗时，需一种自动化且泛化能力强的恢复方法以提升意识摄影作品的视觉质量。

Method: 构建合成退化管线（包括对比度降低、空间模糊、波长相关颜色失真等），使用扩散模型进行逆向生成。训练集为约2500张高质量水下摄影图像，模型参数约11M，直接从头训练，输出分辨率512x768。

Result: 在小规模高质量数据集上训练，模型在感知一致性和泛化能力上表现良好，能生成高质量512x768恢复图像，参数量约11M，数据约2.5k张。

Conclusion: 该论文提出了一种基于扩散模型的图像到图像流水线，通过合成水下退化过程并学习逆向恢复，能有效恢复水下照片的色彩与细节。

Abstract: Underwater photography presents significant inherent challenges including reduced contrast, spatial blur, and wavelength-dependent color distortions. These effects can obscure the vibrancy of marine life and awareness photographers in particular are often challenged with heavy post-processing pipelines to correct for these distortions.
  We develop an image-to-image pipeline that learns to reverse underwater degradations by introducing a synthetic corruption pipeline and learning to reverse its effects with diffusion-based generation. Training and evaluation are performed on a small high-quality dataset of awareness photography images by Keith Ellenbogen. The proposed methodology achieves high perceptual consistency and strong generalization in synthesizing 512x768 images using a model of ~11M parameters after training from scratch on ~2.5k images.

</details>


### [11] [ShapePuri: Shape Guided and Appearance Generalized Adversarial Purification](https://arxiv.org/abs/2602.05175)
*Zhe Li,Bernhard Kainz*

Main category: cs.CV

TL;DR: ShapePuri通过SDF形状编码和全局外观去偏，使表示对齐稳定结构，报告在AutoAttack上达到84.06%（clean）/81.64%（robust），被介绍为高效无额外推理成本的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御（如对抗训练与扩散净化）存在高计算代价或信息损失，作者希望利用形状/结构这种对抗攻击不易破坏的稳定信息，构建高效且准确的防御方法。

Method: 方法包含两部分：Shape Encoding Module (SEM) 使用签名距离函数（SDF）提供稠密几何引导；Global Appearance Debiasing (GAD) 通过随机变换减轻外观偏置。总体为表征对齐与去偏置的防御框架，旨在替代昂贵的扩散净化。

Result: 在AutoAttack评估下，报告洁净准确率84.06%，鲁棒准确率81.64%，据称首次在该基准上超过80%。并声称推理期间无需额外模块或计算成本。

Conclusion: Shape Guided Purification (ShapePuri) 提出通过结构不变性提升模型对抗鲁棒性的思路，声称在AutoAttack上实现了突破性结果。

Abstract: Deep neural networks demonstrate impressive performance in visual recognition, but they remain vulnerable to adversarial attacks that is imperceptible to the human. Although existing defense strategies such as adversarial training and purification have achieved progress, diffusion-based purification often involves high computational costs and information loss. To address these challenges, we introduce Shape Guided Purification (ShapePuri), a novel defense framework enhances robustness by aligning model representations with stable structural invariants. ShapePuri integrates two components: a Shape Encoding Module (SEM) that provides dense geometric guidance through Signed Distance Functions (SDF), and a Global Appearance Debiasing (GAD) module that mitigates appearance bias via stochastic transformations. In our experiments, ShapePuri achieves $84.06\%$ clean accuracy and $81.64\%$ robust accuracy under the AutoAttack protocol, representing the first defense framework to surpass the $80\%$ threshold on this benchmark. Our approach provides a scalable and efficient adversarial defense that preserves prediction stability during inference without requiring auxiliary modules or additional computational cost.

</details>


### [12] [PoseGaussian: Pose-Driven Novel View Synthesis for Robust 3D Human Reconstruction](https://arxiv.org/abs/2602.05190)
*Ju Shen,Chen Chen,Tam V. Nguyen,Vijayan K. Asari*

Main category: cs.CV

TL;DR: PoseGaussian在Gaussian Splatting框架中同时将姿态嵌入几何和时间模块，提升动态人体新视角渲染的质量与一致性，同时保持实时性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅将姿态作为条件或用于配准/扭曲，难以在动态人体场景（关节复杂运动、自遮挡严重）中同时保证几何准确性和时间一致性。为此，将姿态信息更全面地嵌入到几何与时间模块中，以提高鲁棒性和泛化能力。

Method: 构建了一个端到端可微分的Gaussian Splatting流水线：将姿态作为结构先验与颜色编码器融合以改进深度估计；同时使用独立的姿态编码器提取时间线索以增强帧间一致性；将这两部分融入训练和渲染过程中以改进动态人体场景的表示。

Result: 在ZJU-MoCap、THuman2.0及内部数据集上取得了领先的感知和结构性评价（PSNR 30.86, SSIM 0.979, LPIPS 0.028），并实现了100 FPS的实时渲染速度。

Conclusion: PoseGaussian通过在几何（深度估计）和时间（帧间一致性）两个阶段嵌入人体姿态信息，有效提升了动态人体新视角渲染的保真度和鲁棒性，兼具实时性（100 FPS）和较高的结构准确度与感知质量指标。

Abstract: We propose PoseGaussian, a pose-guided Gaussian Splatting framework for high-fidelity human novel view synthesis. Human body pose serves a dual purpose in our design: as a structural prior, it is fused with a color encoder to refine depth estimation; as a temporal cue, it is processed by a dedicated pose encoder to enhance temporal consistency across frames. These components are integrated into a fully differentiable, end-to-end trainable pipeline. Unlike prior works that use pose only as a condition or for warping, PoseGaussian embeds pose signals into both geometric and temporal stages to improve robustness and generalization. It is specifically designed to address challenges inherent in dynamic human scenes, such as articulated motion and severe self-occlusion. Notably, our framework achieves real-time rendering at 100 FPS, maintaining the efficiency of standard Gaussian Splatting pipelines. We validate our approach on ZJU-MoCap, THuman2.0, and in-house datasets, demonstrating state-of-the-art performance in perceptual quality and structural accuracy (PSNR 30.86, SSIM 0.979, LPIPS 0.028).

</details>


### [13] [GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling](https://arxiv.org/abs/2602.05202)
*Shivanshu Shekhar,Uttaran Bhattacharya,Raghavendra Addanki,Mehrab Tanjim,Somdeb Sarkhel,Tong Zhang*

Main category: cs.CV

TL;DR: 作者将视频生成模型重构为能量基自监督评价器（Gen-Transformer Self-Supervised Video Judge），通过潜在空间的时间扰动生成难负样本并用对比学习训练，在捕捉时序质量方面显著优于VLM基方法，且标注效率高。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的奖励建模难以捕捉细腻的时间动态；而视频生成模型天然建模时间结构，故可被改造为更合适的奖励模型。

Method: 将现有生成式Transformer视频模型重新表述为能量基模型（EBM），通过对比目标训练，使能量与视频质量呈反向关系。正样本为真实视频或高质量生成，负样本通过潜在空间扰动生成：时间切片、特征交换、帧打乱。模型学习时序感知的判别能力。

Result: 在GenAI-Bench和MonteBench上取得SOTA，使用仅30K人类标注，所需标注数比现有VLM方法少6×到65×。

Conclusion: 本文提出将视频生成模型改造成奖励模型，用于捕捉时间动态，从而更好地与人类偏好对齐。通过将生成模型视为能量模型并用对比学习训练，模型能区分高质量与劣化视频。使用受控潜变量扰动构造困难负样本，避免利用表面差异。方法在两个基准上用更少人类标注达到SOTA。

Abstract: Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.

</details>


### [14] [Dual-Representation Image Compression at Ultra-Low Bitrates via Explicit Semantics and Implicit Textures](https://arxiv.org/abs/2602.05213)
*Chuqin Zhou,Xiaoyue Ling,Yunuo Chen,Jincheng Dai,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出一种无需训练的新框架，将显式语义条件与隐式细节编码结合，通过条件扩散与反向通道编码并配合可插拔编码器，在超低码率下显著提升率-感知性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是现有神经编解码器在超低码率下性能急剧下降，虽然生成式压缩利用预训练模型的语义先验能缓解，但存在语义保真性与感知真实度的权衡：显式方法保结构但缺细节，隐式方法有细节却可能语义漂移。作者旨在整合两者优势。

Method: 方法上，作者使用条件扩散模型（diffusion model）以显式高层语义（explicit high-level semantics）作为条件，同时采用反向通道编码（reverse-channel coding）来隐式传递细粒度细节。此外引入一个可插拔编码器（plug-in encoder），通过调节隐式信息量来灵活控制失真—感知权衡（distortion-perception tradeoff）。该框架为训练免费，不需要针对压缩任务重新训练扩散模型。

Result: 在大量实验中，作者声称该方法在率-感知性能上达到最新水平；具体量化指标为在Kodak、DIV2K和CLIC2020数据集上，相较于DiffC在DISTS BD-Rate上分别提升29.92%、19.33%和20.89%。

Conclusion: 本文提出了一个训练免费（training-free）的统一框架，通过联合显式高层语义表示与隐式细节信息来改善超低码率下的图像压缩效果，从而在语义保真度与感知真实度之间取得更好平衡。

Abstract: While recent neural codecs achieve strong performance at low bitrates when optimized for perceptual quality, their effectiveness deteriorates significantly under ultra-low bitrate conditions. To mitigate this, generative compression methods leveraging semantic priors from pretrained models have emerged as a promising paradigm. However, existing approaches are fundamentally constrained by a tradeoff between semantic faithfulness and perceptual realism. Methods based on explicit representations preserve content structure but often lack fine-grained textures, whereas implicit methods can synthesize visually plausible details at the cost of semantic drift. In this work, we propose a unified framework that bridges this gap by coherently integrating explicit and implicit representations in a training-free manner. Specifically, We condition a diffusion model on explicit high-level semantics while employing reverse-channel coding to implicitly convey fine-grained details. Moreover, we introduce a plug-in encoder that enables flexible control of the distortion-perception tradeoff by modulating the implicit information. Extensive experiments demonstrate that the proposed framework achieves state-of-the-art rate-perception performance, outperforming existing methods and surpassing DiffC by 29.92%, 19.33%, and 20.89% in DISTS BD-Rate on the Kodak, DIV2K, and CLIC2020 datasets, respectively.

</details>


### [15] [E.M.Ground: A Temporal Grounding Vid-LLM with Holistic Event Perception and Matching](https://arxiv.org/abs/2602.05215)
*Jiahao Nie,Wenbin An,Gongjie Zhang,Yicheng Xu,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 提出E.M.Ground：用<event> token聚合事件帧、平滑相似度曲线并做多粒度特征聚合，显著提升了时间视频定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过独立的起止帧token与帧特征逐帧比较，依赖精确时间戳，忽略事件的语义连续性，导致边界模糊与不准确。

Method: 在Vid-LLM框架下，E.M.Ground加入特殊的<event> token以聚合事件所有帧信息，同时对token与帧之间相似度时间序列应用Savitzky-Golay平滑以抑制噪声，并采用多粒度帧特征聚合来增强时间感知与补偿压缩损失。

Result: 在基准数据集上的大量实验表明，E.M.Ground在多个评价指标上显著优于现有最先进的Vid-LLMs，验证了其在TVG任务中的有效性与鲁棒性。

Conclusion: E.M.Ground通过引入<event>聚合token、Savitzky-Golay平滑以及多粒度帧特征聚合，有效提升了TVG任务中事件的整体感知与匹配准确性，解决了基于单帧时间戳匹配的语义连贯性缺失问题。

Abstract: Despite recent advances in Video Large Language Models (Vid-LLMs), Temporal Video Grounding (TVG), which aims to precisely localize time segments corresponding to query events, remains a significant challenge. Existing methods often match start and end frames by comparing frame features with two separate tokens, relying heavily on exact timestamps. However, this approach fails to capture the event's semantic continuity and integrity, leading to ambiguities. To address this, we propose E.M.Ground, a novel Vid-LLM for TVG that focuses on holistic and coherent event perception. E.M.Ground introduces three key innovations: (i) a special <event> token that aggregates information from all frames of a query event, preserving semantic continuity for accurate event matching; (ii) Savitzky-Golay smoothing to reduce noise in token-to-frame similarities across timestamps, improving prediction accuracy; (iii) multi-grained frame feature aggregation to enhance matching reliability and temporal understanding, compensating for compression-induced information loss. Extensive experiments on benchmark datasets show that E.M.Ground consistently outperforms state-of-the-art Vid-LLMs by significant margins.

</details>


### [16] [Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation](https://arxiv.org/abs/2602.05217)
*Jiahao Nie,Guanqiao Fu,Wenbin An,Yap-Peng Tan,Alex C. Kot,Shijian Lu*

Main category: cs.CV

TL;DR: 提出通过累积强增强生成多视图并结合双链多视角一致性学习的渐进适配方法，显著改善跨域少样本分割的适配性能，提升约7%。


<details>
  <summary>Details</summary>
Motivation: 当前跨域少样本分割中，源域训练模型在目标域的few-shot能力初始较弱，且目标样本数量与多样性有限，导致适配效果受限。为克服域差异和样本稀缺，需通过更丰富的数据视图和更有效的策略逐步提升目标域的few-shot能力。

Method: 方法包含两部分：1) 混合渐进增强（Hybrid Progressive Augmentation）：通过累积的强增强生成越来越复杂和多样的视图，制造更具挑战性的学习场景；2) 双链多视角预测（Dual-chain Multi-view Prediction）：设计串行和并行的学习路径，对这些复杂视图进行充分监督和一致性约束，从而联合利用多视角信息进行适配。

Result: 在大量实验中，MPA在目标域适配上优于现有最先进方法，性能提升显著，文中报告的平均提升约为+7.0%。

Conclusion: 该文提出的多视角渐进适配（MPA）通过从数据和策略两方面逐步增强目标域的few-shot能力，能够在跨域少样本分割中显著提升性能。

Abstract: Cross-Domain Few-Shot Segmentation aims to segment categories in data-scarce domains conditioned on a few exemplars. Typical methods first establish few-shot capability in a large-scale source domain and then adapt it to target domains. However, due to the limited quantity and diversity of target samples, existing methods still exhibit constrained performance. Moreover, the source-trained model's initially weak few-shot capability in target domains, coupled with substantial domain gaps, severely hinders the effective utilization of target samples and further impedes adaptation. To this end, we propose Multi-view Progressive Adaptation, which progressively adapts few-shot capability to target domains from both data and strategy perspectives. (i) From the data perspective, we introduce Hybrid Progressive Augmentation, which progressively generates more diverse and complex views through cumulative strong augmentations, thereby creating increasingly challenging learning scenarios. (ii) From the strategy perspective, we design Dual-chain Multi-view Prediction, which fully leverages these progressively complex views through sequential and parallel learning paths under extensive supervision. By jointly enforcing prediction consistency across diverse and complex views, MPA achieves both robust and accurate adaptation to target domains. Extensive experiments demonstrate that MPA effectively adapts few-shot capability to target domains, outperforming state-of-the-art methods by a large margin (+7.0%).

</details>


### [17] [Boosting SAM for Cross-Domain Few-Shot Segmentation via Conditional Point Sparsification](https://arxiv.org/abs/2602.05218)
*Jiahao Nie,Yun Xing,Wenbin An,Qingsong Zhao,Jiawei Shao,Yap-Peng Tan,Alex C. Kot,Shijian Lu,Xuelong Li*

Main category: cs.CV

TL;DR: 针对SAM在跨域少样本分割中因域移位导致的性能下降，提出基于参考掩码的条件点稀疏化策略（CPS），通过自适应减少匹配点密度作为SAM提示，从而提升无训练方法的分割效果。


<details>
  <summary>Details</summary>
Motivation: 观察到现有基于SAM的无训练少样本分割方法在跨域场景（如医学和遥感图像）中表现欠佳，推测是大域移位破坏了SAM学到的点-图像交互，且点的密度在该情形下影响显著，因此希望通过调节点密度来恢复或优化交互效果。

Method: 利用参考样本的真值掩码作为先验信息，对从参考到目标的密集匹配点进行条件稀疏化；将保留的稀疏点作为SAM的提示输入以预测目标掩码。该方法为零训练、与SAM兼容的后处理模块。

Result: 在多种CD-FSS数据集上进行大量实验，结果显示CPS优于现有的无训练SAM基线方法，在跨域分割任务中显著提高了掩码预测的准确性。

Conclusion: 本文提出的Conditional Point Sparsification（CPS）在跨域少样本分割（CD-FSS）任务中，通过对参考图像的密集匹配点进行自适应稀疏化以改善SAM在大域移位下的点-图像交互，从而提升无训练SAM方法的分割性能。

Abstract: Motivated by the success of the Segment Anything Model (SAM) in promptable segmentation, recent studies leverage SAM to develop training-free solutions for few-shot segmentation, which aims to predict object masks in the target image based on a few reference exemplars. These SAM-based methods typically rely on point matching between reference and target images and use the matched dense points as prompts for mask prediction. However, we observe that dense points perform poorly in Cross-Domain Few-Shot Segmentation (CD-FSS), where target images are from medical or satellite domains. We attribute this issue to large domain shifts that disrupt the point-image interactions learned by SAM, and find that point density plays a crucial role under such conditions. To address this challenge, we propose Conditional Point Sparsification (CPS), a training-free approach that adaptively guides SAM interactions for cross-domain images based on reference exemplars. Leveraging ground-truth masks, the reference images provide reliable guidance for adaptively sparsifying dense matched points, enabling more accurate segmentation results. Extensive experiments demonstrate that CPS outperforms existing training-free SAM-based methods across diverse CD-FSS datasets.

</details>


### [18] [PatchFlow: Leveraging a Flow-Based Model with Patch Features](https://arxiv.org/abs/2602.05238)
*Boxiang Zhang,Baijian Yang,Xiaoming Wang,Corey Vian*

Main category: cs.CV

TL;DR: 结合邻域感知patch特征、适配器与正态化流实现无监督铸造件缺陷检测，在多个公开与专有数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 压铸件表面缺陷对工业质量控制影响显著，传统人工检测耗时且主观。现有基于计算机视觉的异常检测方法在工业域存在预训练特征与目标图像分布不匹配的问题，导致精度不足。因此提出一种结合局部邻域patch特征、正态化流与适配器的方案以提升检测性能。

Method: 利用预训练卷积/视觉模型提取patch级特征，设计邻域感知的patch表示（考虑局部相似性/位置关系），然后采用正态化流（normalizing flow）建模正常样本的特征分布以进行无监督异常检测。同时引入轻量适配器模块，对预训练特征在目标工业域上进行微调或域适配，以减少领域差异并提升特征判别性。

Result: 在MVTec AD数据集上，图像级AUROC达99.28%，较最先进方法错误率降低约20%；在VisA数据集图像级AUROC为96.48%，相比最先进模型错误率降低约28.2%；在专有压铸件数据集上实现95.77%的异常检测精度，且训练无需异常样本。

Conclusion: 本文提出结合局部邻近感知patch特征与正态化流模型，并通过引入适配器模块弥合通用预训练特征提取器与工业图像之间差距，从而提高铸造件表面缺陷异常检测的效率与准确性。该方法在不使用异常样本训练的前提下，能有效提升预测性能并适用于多数据集。

Abstract: Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\%. Compared to the state-of-the-art models, this represents a 28.2\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry

</details>


### [19] [Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images](https://arxiv.org/abs/2602.05250)
*Jieyun Tan,Shuo Liu,Guibin Zhang,Ziqi Li,Jian Geng,Lei Zhang,Lei Cao*

Main category: cs.CV

TL;DR: 提出一种主动标签清洗方法，通过主动学习和标签选择模块用少量专家重标注清洗众包噪声，显著提升检测性能并大幅节约专家成本。


<details>
  <summary>Details</summary>
Motivation: 高质量专家标注稀缺且昂贵，众包虽成本低但引入噪声；因此需要一种在有限专家资源下高效去噪的方法。

Method: 基于主动学习选取最有价值的带噪样本进行专家重标注，训练高精度清洗模型；引入标签选择模块，利用众包标签与模型预测的差异进行样本选择与实例级噪声分级。

Result: 在私有数据集上，方法在AP50上达到67.18%，相比直接用噪声标签训练提升了18.83%，接近全专家标注性能（达到95.79%），同时将标注成本降低了73.30%。

Conclusion: 本文提出的主动标签清洗方法能有效提高带噪众包标注数据的质量，显著提升EDD检测性能，同时大幅节省专家标注成本。

Abstract: Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.

</details>


### [20] [RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation](https://arxiv.org/abs/2602.05257)
*Diya He,Qingchen Liu,Cong Zhang,Jiahu Qin*

Main category: cs.CV

TL;DR: RFM-Pose用流匹配替代扩散并结合PPO把采样与评分联合为强化学习任务，既加速了类别级6D位姿生成又保留了解决旋转对称性的能力，在REAL275上表现优秀且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有基于score的扩散模型虽然在旋转对称性上有优势，但采样代价高，限制了效率，故需要更高效的位姿生成与评估方法。

Method: 采用流匹配（flow-matching）生成模型沿最优传输路径从简单先验生成位姿候选；将采样过程视为马尔可夫决策过程并用近端策略优化（PPO）微调采样策略；把流场作为可学习策略并映射估计器到价值网络，从而联合优化生成和假设评分。

Result: 在REAL275基准上，RFM-Pose在性能与计算成本间取得良好折衷，显著降低了计算开销同时维持或提升位姿估计与跟踪的竞争性结果。

Conclusion: RFM-Pose通过将流匹配生成模型与强化学习结合，在类别级6D物体位姿估计中实现了高效生成与评分统一优化，既解决了旋转对称性问题，又显著降低了采样成本。

Abstract: Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.

</details>


### [21] [ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network](https://arxiv.org/abs/2602.05262)
*Junzhou Li,Manqi Zhao,Yilin Gao,Zhiheng Yu,Yin Li,Dongsheng Jiang,Li Xiao*

Main category: cs.CV

TL;DR: ReGLA: efficient hybrid of convolutions and ReLU gated linear attention plus multi-teacher distillation, yielding high accuracy and low latency on high-resolution tasks.


<details>
  <summary>Details</summary>
Motivation: Lightweight Transformer-based models struggle with high latency on high-resolution images; need to maintain accuracy while reducing latency for practical high-resolution visual tasks.

Method: Designs hybrid networks combining ELRF convolution module for large receptive field, RGMA ReLU-gated modulated linear attention for global modeling with linear complexity, and multi-teacher distillation; evaluated on ImageNet-1K, COCO, ADE20K with latency measured at 512px.

Result: ReGLA-M obtains 80.85% Top-1 on ImageNet-1K (224px) and 4.98 ms latency at 512px; surpasses similarly scaled iFormer by +3.1% AP on COCO and +3.6% mIoU on ADE20K.

Conclusion: ReGLA successfully balances accuracy and latency for high-resolution images by combining efficient convolutions and ReLU gated linear attention, achieving state-of-the-art results among lightweight models.

Abstract: Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \textbf{80.85\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \textbf{3.1\%} AP on COCO object detection and \textbf{3.6\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.

</details>


### [22] [Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2602.05271)
*Shengqin Jiang,Xiaoran Feng,Yuankai Qi,Haokui Zhang,Renlong Hang,Qingshan Liu,Lina Yao,Quan Z. Sheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 在FSCIL中，冻结特征提取器并微调原型，通过类特定与任务感知的双重偏移校准决策区域，能以极少参数显著提升增量学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统FSCIL方法依赖冻结的预训练特征提取器生成静态原型，受骨干网络表征偏差限制；而prompt微调在少样本下难以通过小参数量显著提升全局判别能力，因此提出只微调原型以更高效地优化决策边界。

Method: 构建一个高效的原型微调框架，将静态类中心演化为可学习组件；引入双重校准机制：类特定偏移（class-specific offsets）与任务感知偏移（task-aware offsets），协同调整原型位置以改善判别性能。

Result: 在多个基准上取得优越性能，同时仅需极少量可训练参数，证明了原型微调在FSCIL中的有效性与高效性。

Conclusion: 本文提出在FSCIL场景中冻结特征提取器、仅微调原型的范式转变，认为关键在于在高质量、静态的特征空间中优化决策区域，通过可学习的原型偏移提升增量类的判别能力，从而缓解表征偏差并节省可训练参数。

Abstract: Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.

</details>


### [23] [Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs](https://arxiv.org/abs/2602.05275)
*Qi Li,Yanzhe Zhao,Yongxin Zhou,Yameng Wang,Yandong Yang,Yuanjia Zhou,Jue Wang,Zuojian Wang,Jinxiang Liu*

Main category: cs.CV

TL;DR: 提出一种结合视觉token压缩与三阶段渐进训练的高效MLLM嵌入模型，通过继续预训练、对比预训练+困难负样本、以及基于MLLM判官的任务感知微调，实现更高检索性能与更低推理成本。


<details>
  <summary>Details</summary>
Motivation: 动机是解决MLLM在通用多模态检索中因处理大量视觉tokens导致的高计算开销问题，同时尽量提升或保持检索性能。

Method: 方法包括两方面：1) 高效的MLLM架构，进行视觉token压缩以降低推理延迟和内存占用；2) 多阶段渐进训练：先继续大规模预训练恢复多模态能力，再进行大规模对比预训练与困难负样本挖掘提升判别力，最后用MLLM作为判官进行任务感知的微调与数据筛选。

Result: 实验表明，Magic-MM-Embedding在多项检索任务上显著优于现有方法，并在推理效率上更具优势（延迟和内存占用更低）。

Conclusion: 该论文提出了Magic-MM-Embedding，通过视觉token压缩和渐进式多阶段训练，在保证推理高效的同时实现了在通用多模态检索上的SOTA性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.

</details>


### [24] [Fast-SAM3D: 3Dfy Anything in Images but Faster](https://arxiv.org/abs/2602.05293)
*Weilun Feng,Mingqiang Wu,Zhiliang Chen,Chuanguang Yang,Haotong Qin,Yuqi Li,Xiaokun Liu,Guoxin Fan,Zhulin An,Libo Huang,Yulun Zhang,Michele Magno,Yongjun Xu*

Main category: cs.CV

TL;DR: 针对SAM3D推理瓶颈，提出三种异质性感知的训练免费优化策略，实现显著加速（最高2.67×）且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: SAM3D虽能做大规模开放世界三维重建，但推理延迟高，通用加速方法在此场景脆弱，原因在于未考虑管线的多层次异质性：形状与布局的运动学差异、纹理细化的稀疏性、几何的谱差异。

Method: 提出无训练代价的三种机制：1) 模态感知步缓存（Modality-Aware Step Caching）分离形状结构演化与对布局敏感的更新；2) 联合时空Token裁剪（Joint Spatiotemporal Token Carving）在高熵区域集中细化计算；3) 谱感知Token聚合（Spectral-Aware Token Aggregation）自适应地调整解码分辨率。

Result: 在多组实验中，Fast-SAM3D实现了最高2.67×的端到端加速，同时几乎不损失生成保真度，构建了更优的性能/质量帕累托前沿。

Conclusion: Fast-SAM3D通过面向多层次异质性的推理优化，显著降低SAM3D的推理延迟，同时保持生成质量。

Abstract: SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \textbf{2.67$\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.

</details>


### [25] [FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion](https://arxiv.org/abs/2602.05305)
*Zhuokun Chen,Jianfei Cai,Bohan Zhuang*

Main category: cs.CV

TL;DR: 发现区块扩散中块外注意力跨步冗余，提出FlashBlock缓存并复用块外注意力输出，显著降低注意力与KV访问开销，带来显著推理加速且几乎不影响质量。


<details>
  <summary>Details</summary>
Motivation: 长上下文生成（如分钟级视频、长文本）中，区块扩散仍因对不断增长的KV缓存重复计算注意力而开销高；需减少重复计算同时保持生成质量。

Method: 通过实证分析发现：在区块扩散中，跨扩散步的块外注意力输出变化小，而块内注意力变化大。基于此，设计块外注意力缓存机制（FlashBlock），将稳定的块外注意力结果复用，并可与稀疏注意力配合采用残差重用策略。

Result: 在扩散语言模型与视频生成上，FlashBlock在保持生成质量几乎不变的情况下，实现最高1.44×的token吞吐量提升和最高1.6×的注意力时间减少，并在与稀疏注意力结合时在激进稀疏下显著提升模型精度。

Conclusion: 本文提出FlashBlock，在不改动扩散过程的前提下，缓存并重用块外（已完成块）注意力输出，从而减少KV访问与注意力计算，提高长上下文扩散模型的推理效率。

Abstract: Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\times$ higher token throughput and up to 1.6$\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.

</details>


### [26] [Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning](https://arxiv.org/abs/2602.05321)
*Dongki Jung,Jaehoon Choi,Adil Qureshi,Somi Jeong,Dinesh Manocha,Suyong Yeon*

Main category: cs.CV

TL;DR: Wid3R 是一个支持宽视角相机的前馈多视角 3D 重建网络，采用射线+球谐函数+相机模型 token 来处理畸变，首创性地支持直接从 360 图像进行重建，在多个数据集上显著优于以往方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法多针对针孔相机或已去畸变图像，限制了在现实宽视角相机（如鱼眼、360 全景）场景中的适用性，因此需要一种能直接处理宽视角图像的通用重建方法。

Method: 使用基于射线的表示结合球谐函数（spherical harmonics）和在网络中引入相机模型 token 来编码相机畸变信息，形成一个对畸变敏感的多视角 3D 重建管线。

Result: Wid3R 是首个支持从 360 图像直接进行前馈式多视角 3D 重建的通用模型，展现出强大的零样本鲁棒性，在 Stanford2D3D 数据集上相比先前方法最高提升约 +77.33（指标未具体说明）。

Conclusion: Wid3R 提出了一种面向宽视角相机模型的前馈神经网络，用于视觉几何重建，显著扩展了对鱼眼和全景图像的支持，实现了无需去畸变的端到端重建。

Abstract: We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.

</details>


### [27] [MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors](https://arxiv.org/abs/2602.05330)
*Jingdong Zhang,Xiaohang Zhan,Lingzhi Zhang,Yizhou Wang,Zhengming Yu,Jionghao Wang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: MTPano 用透视模型生成伪标签并重投影、任务分组与双分支几何感知网络、ERP token mixers 和梯度截断，以及辅助任务，构建了一个无标签多任务全景基础模型，显著提升了全景密集预测性能。


<details>
  <summary>Details</summary>
Motivation: 面向沉浸式应用的全景场景理解受限于高分辨率多任务标注稀缺，直接迁移透视基础模型到全景域会因几何畸变和坐标系统差异失败，且不同密集预测任务在球面空间的内在关系未被充分利用。

Method: 核心方法包括：1) 将全景图分割为透视补丁并用现成的透视基础模型生成高质量伪标签，再将伪标签重投影回全景作为监督；2) 将任务分为旋转不变（如深度、分割）和旋转可变（如法线），设计 Panoramic Dual BridgeNet 两条特征流，通过几何感知调制层注入绝对位置与光线方向先验；3) 为处理 ERP 畸变引入 ERP token mixers，随后用双分支 BridgeNet 并配合梯度截断以允许有益的跨任务信息共享同时阻断冲突梯度；4) 增加图像梯度、点图等辅助任务以促进多任务学习。

Result: 在多项基准测试上，MTPano 达到或超越了现有方法，获得了 SOTA 性能，并在某些任务上与专门为全景设计的基础模型竞争力相当。

Conclusion: MTPano 提出了一种无监督（label-free）的多任务全景基础模型，通过借助透视基模型生成伪标签并在全景域重投影，结合任务分组与双分支 BridgeNet 以及几何感知的调制层与 ERP token mixers，有效缓解数据稀缺、几何畸变与任务间冲突问题，从而在多个基准上实现了 SOTA 性能并接近专用全景模型的表现。

Abstract: Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.

</details>


### [28] [Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation](https://arxiv.org/abs/2602.05339)
*Yongwoo Kim,Sungmin Cha,Hyunsoo Kim,Jaewon Lee,Donghyun Kim*

Main category: cs.CV

TL;DR: PAIR uses generated unsafe-safe pairs plus paired semantic realignment and Fisher-weighted DoRA initialization to erase unwanted concepts while keeping image structure and semantics, improving over previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing erasure removes unsafe concepts but lacks guidance toward safe alternatives, harming structural and semantic consistency; using unsafe-safe pairs can enable consistency-preserving erasure.

Method: Generate safe counterparts for unsafe inputs to form paired data; introduce Paired Semantic Realignment objective mapping unsafe concepts to safe anchors; use Fisher-weighted Initialization for parameter-efficient low-rank adaptation (DoRA) to suppress unsafe concepts and encourage safe alternatives.

Result: Outperforms SOTA baselines in effective concept erasure while maintaining structural integrity, semantic coherence, and generation quality.

Conclusion: PAIR reframes concept erasure as semantic realignment from unsafe to safe concepts, achieving selective removal while preserving structure and semantics.

Abstract: With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.

</details>


### [29] [Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection](https://arxiv.org/abs/2602.05349)
*Ningkang Peng,JiuTao Zhou,Yuhao Zhang,Xiaoqian Peng,Qianfeng Yu,Linjing Qian,Tingyu Lu,Yi Chen,Yanhui Gu*

Main category: cs.CV

TL;DR: APEX通过自适应原型复杂度（APM）和后验感知评分（PAOS）修复原型方法的两大缺陷，从而显著提升OOD检测效果并在基准上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的方法存在两个普遍问题：Static Homogeneity Assumption（对所有类采用固定表示资源）限制模型容量，和Learning-Inference Disconnect（推理时丢弃丰富的原型质量信息）导致性能损失。需通过自适应分配原型与将原型质量纳入推理来改善特征流形与OOD检测效果。

Method: 提出Two-Stage Repair流程，包括(1) Adaptive Prototype Manifold (APM)：基于最小描述长度（MDL）准则为每个类自动确定最优原型数量K_c^*，消除原型碰撞；(2) Posterior-Aware OOD Scoring (PAOS)：在推理阶段量化原型质量（内聚性与分离性），将训练得到的原型质量知识用于评分。

Result: 在CIFAR-100等基准上进行了全面实验，结果显示APEX在OOD检测任务上优于现有方法，取得新的最先进性能。

Conclusion: 本文通过识别并修复原型表示学习在OOD检测中的两大根本性缺陷，提出了APEX框架，有效提升了OOD检测性能，并在基准数据集上取得了最先进结果。

Abstract: Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.

</details>


### [30] [Multimodal Latent Reasoning via Hierarchical Visual Cues Injection](https://arxiv.org/abs/2602.05359)
*Yiming Zhang,Qiangyu Yan,Borui Jiang,Kai Han*

Main category: cs.CV

TL;DR: HIVE通过在隐空间内的迭代Transformer和层次化视觉线索注入，实现无需文本化推理步骤的多模态“慢思考”，提高理解复杂场景的准确性并降低幻觉。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM多为“快思考”，依赖端到端生成或语言中心的CoT，效率低、冗长且易幻觉；希望在潜在空间中进行更稳健、低幻觉的多模态推理。

Method: 提出HIVE框架：递归扩展Transformer块以形成内部迭代推理环，并将全局场景到细粒度区域的层次化视觉信息以注入方式融合到模型的潜在表示中，从而在对齐潜在空间内进行多步、基于视觉的推理。

Result: 实验表明：在测试时通过整合视觉知识进行扩展有效，且层次化视觉信息显著提升复杂场景理解能力。

Conclusion: 该论文提出在潜在空间中进行多模态推理，通过层次化视觉线索注入实现更稳健的“慢思考”，避免依赖文本化链式思维。

Abstract: The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a "fast thinking" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\emph{HIVE}), a novel framework that instills deliberate, "slow thinking" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.

</details>


### [31] [Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection](https://arxiv.org/abs/2602.05360)
*Ningkang Peng,Xiaoqian Peng,Yuhao Zhang,Qianfeng Yu,Feng Xing,Peirong Ma,Xichen Yang,Yi Chen,Tingyu Lu,Yanhui Gu*

Main category: cs.CV

TL;DR: 提出D-KNN，通过正交分解和双空间校准打破语义主导，恢复残差子空间的OOD信号，显著改善对结构性简单和噪声样本的检测性能。


<details>
  <summary>Details</summary>
Motivation: 观察到SOTA特征型后处理方法在识别语义上细微的OOD样本表现良好，但对结构上明显但语义简单或高频噪声检测失败，提出这是因语义特征在特征空间占主导，掩盖了残差子空间的分布偏移信号。

Method: 基于正交分解的几何解耦（orthogonal decomposition），将语义主成分与结构残差分离；引入双空间校准（dual-space calibration）对主成分与残差进行不同处理；训练无关、可插拔的D-KNN在特征空间分别应用最近邻检测并融合得分。

Result: 在CIFAR与ImageNet基准上取得新的SOTA：解决Simplicity Paradox将FPR95从31.3%降至2.3%；应对高斯噪声等传感器故障时，AUROC从79.7%提升到94.9%。

Conclusion: 该论文揭示并解决了特征级后处理OOD检测中的“Simplicity Paradox”，证明语义主导（Semantic Hegemony）导致对结构性简单或高频噪声的检测无感并提出D-KNN方法有效恢复残差子空间信号，显著提升检测性能。

Abstract: While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.

</details>


### [32] [Imagine a City: CityGenAgent for Procedural 3D City Generation](https://arxiv.org/abs/2602.05362)
*Zishan Liu,Zecong Tang,RuoCheng Wu,Xinzhe Zheng,Jingyu Hu,Ka-Hei Hui,Haoran Xie,Bo Dai,Zhengzhe Liu*

Main category: cs.CV

TL;DR: 提出CityGenAgent：基于可解释程序的两阶段学习（SFT+RL），实现高质量、可控且可编辑的自然语言驱动3D城市生成。


<details>
  <summary>Details</summary>
Motivation: 现有自动化3D城市生成在资产高保真、可控性和可操作性上存在不足，需一个能从自然语言生成并支持编辑的高质量可扩展方案。

Method: 将城市生成拆分为Block Program和Building Program两层，先通过监督微调训练生成满足schema约束的有效程序，再用强化学习优化空间对齐和视觉一致性奖励。

Result: 在语义对齐、视觉质量和可控性上优于现有方法，并支持自然语言编辑与操作，构建了可扩展的3D城市生成基础。

Conclusion: CityGenAgent通过分层可解释的程序化生成框架，实现了高质量3D城市的自然语言驱动生成，兼顾结构正确性和语义对齐。

Abstract: The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.

</details>


### [33] [SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback](https://arxiv.org/abs/2602.05380)
*Xiaoxuan He,Siming Fu,Wanli Li,Zhiyuan Li,Dacheng Yin,Kang Rong,Fengyun Rao,Bo Zhang*

Main category: cs.CV

TL;DR: SAIL通过迭代自我标注与ranked preference mixup，使扩散模型在仅需极少人工偏好样本下实现自我教师式对齐，性能优于现有方法并节省大量标注成本。


<details>
  <summary>Details</summary>
Motivation: 当外部奖励模型不可用或昂贵，且大规模偏好数据获取成本高时，能否仅用最少人工反馈并利用扩散模型的潜在能力实现有效对齐？

Method: 从少量人工注释的偏好对出发，SAIL在闭环中反复生成样本、自我标注偏好并用这些自增数据微调扩散模型；引入ranked preference mixup以平衡探索与保持初始人工先验，防止灾难性遗忘。

Result: 在多个基准上，SAIL在仅用现有方法约6%偏好数据的情况下，持续超过最先进方法，表明扩散模型具备显著的自我提升能力。

Conclusion: SAIL展示了通过最小人工偏好数据与模型自我生成标注进行迭代自我提升，可以有效对齐扩散模型，人为注释和外部奖励模型可大幅减少甚至替代。

Abstract: Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \textbf{SAIL} (\textbf{S}elf-\textbf{A}mplified \textbf{I}terative \textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.

</details>


### [34] [VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs](https://arxiv.org/abs/2602.05382)
*Tina Khezresmaeilzadeh,Jike Zhong,Konstantinos Psounis*

Main category: cs.CV

TL;DR: VRIQ基准显示当前VLM在视觉推理上主要受限于感知能力，抽象题目表现接近随机，需改进感知模块与更细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 评估VLM是否能可靠进行非语言（视觉）推理，并找出失败的根源以指导改进。

Method: 构建VRIQ基准，包含抽象拼图和自然图像两类任务；对比普通模型与工具增强模型；设计诊断探针区分感知与推理错误，并对感知子类（形状、计数、位置、3D/深度等）进行细粒度分析。

Result: 抽象任务平均准确率约28%，自然图像任务约45%；工具增强仅带来小幅提升；诊断显示56%失败源于感知，43%感知+推理，1%纯推理；部分感知类别导致更多错误。

Conclusion: 当前VLM在视觉推理上仍不可靠，主要瓶颈在视觉感知而非纯推理；需要在感知模块改进及更细粒度评测来提升表现。

Abstract: Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.

</details>


### [35] [Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting](https://arxiv.org/abs/2602.05384)
*Hao Feng,Wei Shi,Ke Zhang,Xiang Fei,Lei Liao,Dingkang Yang,Yongkun Du,Xuecheng Wu,Jingqun Tang,Yang Liu,Hong Chen,Can Huang*

Main category: cs.CV

TL;DR: Dolphin-v2 通过先做类型+布局检测再对拍摄文档做全页解析、对数字文档做并行元素解析的两阶段混合策略，同时补充细粒度类别、语义属性与代码缩进识别，显著提升了拍摄文档鲁棒性与整体解析性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档解析领域模型碎片化、选择复杂且多依赖轴对齐边界框，难以处理拍摄或畸变文档；需一个兼顾拍摄文档鲁棒性和数字文档解析精度与效率的统一方法。

Method: 第一阶段联合进行文档类型分类（数码生成 vs 拍摄）与布局分析；对数码文档进一步进行细粒度元素检测并预测阅读顺序。第二阶段采用混合解析：对拍摄文档做整体页面级解析以处理几何失真，对数码文档基于检测到的布局锚点并行逐元素解析以提升效率。还加入代码块识别与缩进保留。

Result: 在 DocPTBench、OmniDocBench 与自建 RealDoc-160 上进行了全面评测。Dolphin-v2 在 OmniDocBench 上总体提升 +14.78 分；对拍摄文档错误率降低 91%；并在并行处理下保持高效推理。同时实现了 21 类细粒度元素与语义属性抽取及代码块缩进保持等功能。

Conclusion: Dolphin-v2 成功通过两阶段混合解析策略解决了拍摄文档的几何畸变问题并提升了数字文档的细粒度元素检测与语义属性抽取，显著优于原始 Dolphin。

Abstract: Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.

</details>


### [36] [Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning](https://arxiv.org/abs/2602.05387)
*Zolnamar Dorjsembe,Hung-Yi Chen,Furen Xiao,Hsing-Kuo Pao*

Main category: cs.CV

TL;DR: 提出一种并行双Swin Transformer增强的3D Med2Transformer，用于从MRI生成合成CT，实现更高的解剖保真度和可接受的剂量误差，支持MRI-only放疗规划。


<details>
  <summary>Details</summary>
Motivation: MRI虽具优越软组织对比且无电离辐射，但缺乏电子密度信息，限制其直接用于放射治疗剂量计算；合并CT增加配准不确定性与流程复杂度，因此希望通过从MRI合成CT实现仅用MRI的规划流程。

Method: 提出一种3D网络架构：先用卷积编码器提取局部特征，随后并行接入两条Swin Transformer分支以分别建模不同尺度的长程依赖，采用多尺度移窗注意力和层次特征聚合以增强结构保真度，最后解码生成合成CT。

Result: 在公开和临床数据集上，所提方法在图像相似性（如PSNR/SSIM等）和几何准确性方面优于基线方法；剂量学评估显示平均靶区剂量误差为1.69%，达到临床可接受范围；并提供了代码仓库供复现。

Conclusion: 本文提出的Parallel Swin Transformer-Enhanced Med2Transformer在合成CT生成任务中有效融合了卷积编码器与双分支Swin Transformer，能够同时捕捉局部解剖细节和长程上下文关系，从而在图像相似性和几何精度上超越基线方法，并在剂量学评估中达到临床可接受的误差水平。

Abstract: MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.

</details>


### [37] [Dataset Distillation via Relative Distribution Matching and Cognitive Heritage](https://arxiv.org/abs/2602.05391)
*Qianxin Xia,Jiawei Du,Yuhan Zhang,Jielei Wang,Guoming Lu*

Main category: cs.CV

TL;DR: 通过一次性加载原始统计量并用单次增强将合成图像与类间统计流对齐，提出的统计流匹配实现了更高效且稳定的数据集蒸馏；同时分类器继承进一步提升推理性能并节省资源。


<details>
  <summary>Details</summary>
Motivation: 现有基于线性梯度匹配的方法需要在每次蒸馏步骤中加载成千上万的真实图像并多次对合成图像做可微增强，造成显著的计算和内存开销。作者希望设计一种既稳定又高效的方法，减少GPU内存和运行时间，同时保持或提升性能。

Method: 提出统计流匹配框架：1) 计算并一次性加载数据集的类中心等原始统计量；2) 将合成图像的分布通过单次增强与目标统计流对齐，优化目标为从目标类中心到非目标类中心的恒定流的匹配；3) 在训练时只对合成图像做一次可微增强，避免对大量真实样本的批次级操作；4) 提出分类器继承（classifier inheritance），在推理时直接复用原始数据训练的分类器，仅在合成特征上增加轻量线性投影器。

Result: 在实验中，该方法在性能上可与或优于最先进方法，同时显著降低资源开销：GPU显存降低约10倍，运行时间缩短约4倍。分类器继承策略在不增加显著存储的情况下带来明显的性能提升。

Conclusion: 该论文提出了基于统计流匹配（statistical flow matching）的数据集蒸馏方法，通过对齐原始数据中从目标类中心到非目标类中心的恒定统计流来优化合成图像，从而在不需要大量真实图像和昂贵增强的情况下实现稳定高效的蒸馏。作者还提出了分类器继承策略，重用在原始数据上训练的分类器并仅训练轻量线性投影器，进一步提升推理性能并节省存储。

Abstract: Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.

</details>


### [38] [Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing](https://arxiv.org/abs/2602.05397)
*Yuechen Yang,Junlin Guo,Ruining Deng,Junchao Zhu,Zhengyi Lu,Chongyu Qu,Yanfan Zhu,Xingyi Guo,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 提出MAD——在VAE隐空间中正则化特征轨迹并引导条件扩散合成，以实现对相关pathomics特征的可控、生物学合理编辑，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统基于深度学习的黑箱方法难以提供可解释、可复现的生物标志物；而pathomics提供了丰富的定量特征，但这些特征间存在固有相关性，直接在条件扩散模型中单独编辑某一特征会产生脱离生物学流形的伪影，限制了实际应用，因此需要一种能在保留生物学合理性的同时可控编辑pathomics特征的方法。

Method: 先用变分自编码器(VAE)学习细胞图像的解缠隐空间，对pathomics特征在该空间中进行正则化优化，使特征变化沿着真实数据流形；再将优化后的特征作为条件输入到条件扩散模型，合成编辑后的高保真细胞图像。在流程中通过对相关特征自动联动调整来避免违背生物学约束的异常样本。

Result: 实验显示MAD能在编辑pathomics特征时保持特征轨迹在数据流形上，生成的细胞图像具有更高的结构一致性和视觉保真度，并且在条件特征编辑任务上显著优于不考虑流形约束的基线方法。

Conclusion: 该论文提出的Manifold-Aware Diffusion (MAD)框架有效解决了传统条件扩散模型在编辑高度相关的pathomics特征时产生非生物学合理样本的问题，通过在VAE学得的解缠(latent)空间中对特征轨迹进行正则，使得目标特征的调控会自动联动相关特征，从而保持在真实细胞分布的流形上。实验结果表明MAD在编辑pathomics特征时能更好地保持结构一致性并生成高保真图像，优于基线方法。

Abstract: Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., "second-order moment") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.

</details>


### [39] [TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions](https://arxiv.org/abs/2602.05414)
*Ngoc Doan-Minh Huynh,Duong Nguyen-Ngoc Tran,Long Hoang Pham,Tai Huu-Phuong Tran,Hyung-Joon Jeon,Huy-Hung Nguyen,Duong Khac Vu,Hyung-Min Jeon,Son Hong Phan,Quoc Pham-Nam Ho,Chi Dai Tran,Trinh Le Ba Khanh,Jae Wook Jeon*

Main category: cs.CV

TL;DR: TSBOW是面向恶劣/极端天气与遮挡场景的交通监控大规模数据集，含大量手工与半标注帧及基准评测，可推进CCTV目标检测与智能交通研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多聚焦于轻度天气（轻雾、雨、雪），无法覆盖日益频发的极端天气对CCTV视频质量及交通监测的影响，亟需真实、规模化、带遮挡与多种天气场景的数据集支持智能交通研究。

Method: 收集超过32小时城市CCTV视频，手工标注48,000+帧并利用半监督方法生成3.2M半标注帧，包含八类交通参与者的边界框；构建基准检测任务并评估在遮挡与恶劣天气下的检测性能。

Result: 发布了TSBOW数据集（32小时+, 48k手工标注帧, 3.2M半标注帧, 八类目标），并提供基准检测评测，证明恶劣天气与遮挡显著降低检测性能，强调数据集对推动研究的价值。

Conclusion: TSBOW填补了极端天气下交通监控数据集的空白，通过大规模真实CCTV视频与详尽标注，提升了遮挡和恶劣天气条件下的目标检测研究基础。

Abstract: Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.

</details>


### [40] [VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection](https://arxiv.org/abs/2602.05415)
*Ningkang Peng,Qianfeng Yu,Yuhao Zhang,Yafei Liu,Xiaoqian Peng,Peirong Ma,Yi Chen,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: 无外部数据的OOD检测：基于vMF在特征超球面上合成边界虚拟异常并用双粒度对比损失分离ID与异常，在长尾设置下超越依赖外部数据的sota。


<details>
  <summary>Details</summary>
Motivation: 长尾分布中尾类样本稀少导致特征边界模糊，现有sota依赖外部真实数据进行Outlier Exposure但存在获取成本和隐私问题，因而提出数据无关的合成异常策略。

Method: 设计了Geometry-guided virtual Outlier Synthesis (GOS)：在特征超球体上基于von Mises-Fisher分布找到低似然的环形区域并方向性采样合成虚拟异常；提出Dual-Granularity Semantic Loss (DGS)：结合全局与局部（类内/类间）对比学习目标，最大化ID特征与合成异常的区分度。

Result: 在CIFAR-LT等基准上广泛实验，方法在不使用任何外部真实图像的情况下，仍优于使用外部数据的sota方法，显示了更好的FPR/TNR/AUROC等OOD检测指标。

Conclusion: 该论文提出了无外部数据的OOD检测方法，通过在特征空间上用vMF分布建模并生成虚拟边界异常样本，结合双粒度语义损失进行对比学习，从而在长尾分布下提升OOD检测性能。

Abstract: Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.

</details>


### [41] [Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring](https://arxiv.org/abs/2602.05420)
*Rui Sun,Yiwen Yang,Kaiyu Guo,Chen Jiang,Dongli Xu,Zhaonan Liu,Tan Pan,Limei Han,Xue Jiang,Wu Wei,Yuan Cheng*

Main category: cs.CV

TL;DR: 发现真实细胞邻接图多含奇数环，提出GBC-FS 2025和Disco（显式标记+隐式消歧）以分治方式处理复杂邻接冲突，提升密集细胞实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于轮廓或距离映射的方法在复杂密集区域效果欠佳；图染色新范式潜力大但在真实场景下未验证，需解决非二分图和复杂拓扑导致的表示冗余与优化困难。

Method: 构建大规模GBC-FS 2025数据集；统计分析多数据集细胞邻接图的染色性，发现大量奇数环；提出Disco框架，包括Explicit Marking（递归分解图、标记冲突集为可学习的分类任务）和Implicit Disambiguation（在冲突区域通过特征去相似性约束学习可分离表示）。

Result: 系统性分析显示真实细胞图多为非二分、以三元环为主的奇数环占优；Disco通过显式拓扑标注和隐式特征辨识有效缓解邻域冲突，提高了密集重叠细胞实例分割的鲁棒性（文中给出定量和定性验证）。

Conclusion: 本文提出并验证了适用于复杂、密集细胞拓扑的实例分割新范式，通过释放大型数据集并提出Disco框架，有效解决了图染色方法在真实组织中因奇数环等非二分属性带来的不足。

Abstract: Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the "divide and conquer" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, "Explicit Marking" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a "conflict set." Second, "Implicit Disambiguation" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.

</details>


### [42] [NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks](https://arxiv.org/abs/2602.05423)
*Pengcheng Chen,Yue Hu,Wenhao Li,Nicole M Gunderson,Andrew Feng,Zhenglong Sun,Peter Beerel,Eric J Seibel*

Main category: cs.CV

TL;DR: NeVStereo将NeRF与置信度引导深度、多视图BA和迭代几何-辐射场优化结合，能从RGB多视图同时产出高质量位姿、深度、渲染和网格，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有前馈重建方法缺乏显式新视点合成输出且几何不够精细；NeRF类方法虽具高质量渲染和细节但对固定相机位姿敏感且对位姿误差脆弱，因此需要一个能同时提供准确位姿、可靠深度、高质量渲染与精确表面的单一框架。

Method: NeVStereo将NeRF用于生成对立体友好的渲染，引入置信度引导的多视图深度估计，并通过NeRF耦合的BA（bundle adjustment）进行位姿精修，最后采用迭代优化同时更新深度与辐射场以增强几何一致性，从而减少表面堆叠与伪影并弱化位姿-深度耦合。

Result: 在室内、室外、台面和航拍基准上进行零样本评估，NeVStereo在深度误差上最多降低36%，位姿精度提升10.4%，新视图合成保真度提高4.5%，并在网格质量上取得SOTA（F1 91.93%，Chamfer 4.35 mm）。

Conclusion: NeVStereo提出了一个结合NeRF驱动的新视图合成与立体深度估计的统一框架，能够从仅有RGB的多视角输入联合输出相机位姿、多视角深度、新视图合成与表面重建，显著改善了深度、位姿、渲染质量和网格质量。

Abstract: In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.

</details>


### [43] [Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications](https://arxiv.org/abs/2602.05426)
*Wahyu Rahmaniar,Kenji Suzuki*

Main category: cs.CV

TL;DR: Multi-AD结合SE注意力、教师-学生知识蒸馏与判别器，通过多尺度融合实现跨医学与工业图像的强健无监督异常检测，全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实际应用中标注数据稀缺，特别是跨域场景（医学与工业）中异常样本难以获得；需要一种能在无监督条件下泛化并检测细微异常的模型。

Method: 基于教师-学生架构的卷积神经网络，结合SE通道注意力模块增强特征提取，使用知识蒸馏将教师模型的判别性特征传递给学生模型，并加入判别器（对抗训练）以提高异常与正常样本区分能力。推理阶段通过多尺度特征融合检测不同大小的异常。

Result: 在多种医学（脑MRI、肝CT、视网膜OCT）和工业（MVTec AD）数据集上取得优异结果：图像级AUROC分别为医学81.4%和工业99.6%；像素级AUROC分别为医学97.0%和工业98.4%，平均性能优于SOTA。

Conclusion: 提出的Multi-AD在医学与工业图像上的无监督异常检测表现优异，能同时实现图像级与像素级检测，并在多数据集上超过现有方法。

Abstract: Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.

</details>


### [44] [LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects](https://arxiv.org/abs/2602.05434)
*Sanghoon Jeon,Gihyun Jung,Suhyeon Ka,Jae-Sang Hyun*

Main category: cs.CV

TL;DR: 提出LD-SLRO：用潜在扩散模型基于编码的反射潜变量恢复高反射物体的相移条纹，有效抑制镜面伪影并显著提升3D重建精度（RMSE从1.8176降到0.9619 mm）。


<details>
  <summary>Details</summary>
Motivation: 传统条纹投影在高反射、低粗糙度表面上因为镜面反射和间接光照导致条纹严重失真或缺失，影响相位解算和三维重建，需一种鲁棒的条纹恢复方法。

Method: 先用相移条纹图像编码提取反射特性潜变量，作为条件输入到潜在扩散模型；模型通过时变通道仿射层、注意力模块及镜面反射编码器有针对性地去噪和恢复条纹，输出高质量条纹图像并支持灵活的输入输出条纹配置。

Result: 在实验中，LD-SLRO在条纹质量和三维重建精度上均优于现有方法，平均均方根误差从1.8176 mm降至0.9619 mm。

Conclusion: 该论文提出了基于潜在扩散模型的结构光恢复方法LD-SLRO，有效抑制高反射表面上的镜面反射伪影并恢复丢失条纹，从而提升三维重建精度。

Abstract: Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.

</details>


### [45] [Stable Velocity: A Variance Perspective on Flow Matching](https://arxiv.org/abs/2602.05435)
*Donglin Yang,Yongxing Zhang,Xin Yu,Liang Hou,Xin Tao,Pengfei Wan,Xiaojuan Qi,Renjie Liao*

Main category: cs.CV

TL;DR: 识别并利用flow matching中高/低方差区间，提出Stable Velocity（含StableVM、VA-REPA、StableVS），实现训练稳定性与低方差区间采样超过2×加速，广泛适用于图像与视频生成大模型。


<details>
  <summary>Details</summary>
Motivation: 原flow matching依赖单样本条件速度作为训练目标，导致目标高方差，训练不稳定且收敛缓慢；但在靠近数据分布的低方差区，条件速度与边际速度近似一致，可利用该性质改进训练与采样。

Method: 分析conditional velocity的方差随时间的变化，识别先验附近的高方差区和数据分布附近的低方差区。训练方面设计Stable Velocity Matching（无偏方差缩减）与Variance-Aware Representation Alignment（针对低方差区域自适应提高辅助监督强度）。推理方面利用低方差区间动力学的闭式解简化采样流程，提出Stable Velocity Sampling无需微调即可加速采样。

Result: 在ImageNet 256×256以及SD3.5、Flux、Qwen-Image、Wan2.2等大模型上，StableVM+VA-REPA提升训练效率；StableVS在低方差区间提供超过2倍的采样加速且不损失样本质量（经实验证明）。代码开源。

Conclusion: 提出Stable Velocity框架，通过显式刻画flow matching中条件速度的高方差问题，从训练和采样两方面改进：StableVM（无偏方差缩减目标）和VA-REPA（低方差区域自适应增强监督），以及StableVS（低方差区间闭式简化加速）。在ImageNet 256和多种大规模预训练生成模型上验证，提升训练效率并在低方差区间实现>2×采样加速且不损失质量。

Abstract: While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.

</details>


### [46] [Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations](https://arxiv.org/abs/2602.05440)
*Natascha Jeziorski,Petra Gospodnetić,Claudia Redenbach*

Main category: cs.CV

TL;DR: 提出参数化3D缺陷模型并结合蒙特卡洛物理仿真生成像真实NDT数据的可扩展合成数据集，支持包含稀有缺陷与像素级标注，促进自动化缺陷检测研究。


<details>
  <summary>Details</summary>
Motivation: NDT中需要大量高质量标注数据来训练机器学习检测模型，实测数据获取困难且稀有缺陷样本不足，故采用合成数据与数字孪生以可控生成更多样本并包含罕见缺陷。

Method: 基于规则的方法构建可控的缺陷参数化模型（针对铸件常见缺陷但可迁移），将缺陷模型并入目标几何体生成有缺陷的数字孪生体，然后通过物理基蒙特卡洛仿真生成检测数据（示例为可视表面检测），并同步产生精确标注。

Result: 实现了可变且任意规模的合成数据集生成流程，能包含稀有缺陷并提供像素级精确标注，示例说明适用于可视检测且可扩展至其它NDT方法。

Conclusion: 本文提出了用于生成带缺陷三维网格对象的参数化模型，并通过蒙特卡洛物理仿真生成与实际无损检测(NDT)数据相似的合成训练数据，从而支持自动化缺陷检测与像素级标注。

Abstract: In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.

</details>


### [47] [DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching](https://arxiv.org/abs/2602.05449)
*Chang Zou,Changlin Li,Yang Li,Patrol Li,Jianbing Wu,Xiao He,Songtao Liu,Zhao Zhong,Kailin Huang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出可学习的特征缓存与Restricted MeanFlow蒸馏结合，在视频扩散模型上实现高达11.8×的无损加速。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型计算成本高。现有加速方法如训练自由的特征缓存在压缩时导致语义和细节丢失，训练感知的步骤蒸馏在视频上用少步数退化严重，且将两者简单叠加会加剧质量损失。需要一种既能与蒸馏兼容又能在高压缩率下保持质量的加速机制。

Method: 用轻量可学习的神经预测器替代传统的训练自由启发式特征缓存，以更准确拟合高维特征随采样步变化的演化；在蒸馏方面，提出Restricted MeanFlow以提升对高度压缩蒸馏的稳定性和无损性；二者结合允许在稀疏步骤下仍应用特征缓存。

Result: 方法在大规模视频模型上实现最高11.8×加速，同时保持生成质量（通过大量实验验证）。论文还提供了代码补充材料并将公开。

Conclusion: 本文提出了一种首创的、与蒸馏兼容的可学习特征缓存机制，并结合保守的Restricted MeanFlow策略，实现了在视频扩散模型上的极限加速（达到11.8×）同时保持生成质量。

Abstract: While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.

</details>


### [48] [Attention Retention for Continual Learning with Vision Transformers](https://arxiv.org/abs/2602.05454)
*Yue Lu,Xiangyu Zhou,Shizhou Zhang,Yinghui Xing,Guoqiang Liang,Wencong Zhang*

Main category: cs.CV

TL;DR: 针对ViT在连续学习中注意力漂移导致的遗忘，提出基于注意力图的梯度掩码与缩放更新的保留框架，有效减缓灾难性遗忘并取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 观测到在连续学习中Transformer的注意力会显著偏移，导致已学视觉概念被破坏；受人类视觉系统选择性注意力启发，希望通过保持注意力聚焦来保护先前知识。

Method: 基于层级rollout提取先前任务的注意力图，生成实例自适应二值掩码；在新任务训练时用这些掩码对梯度进行屏蔽（将对应注意力区域的梯度置零），并通过按比例缩放参数更新以兼容现代优化器，维持更新相对幅度。

Result: 实验与可视化显示该方法能有效缓解遗忘、保存视觉概念，在多种连续学习情景下达到或优于现有最先进性能，并具有良好泛化性。

Conclusion: 本文认为注意力漂移是视觉Transformer在连续学习中遗忘的主要原因，提出通过约束注意力漂移以减缓灾难性遗忘的方法。

Abstract: Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.

</details>


### [49] [MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation](https://arxiv.org/abs/2602.05467)
*Dekang Qi,Shuang Zeng,Xinyuan Chang,Feng Xiong,Shichao Xie,Xiaolong Wu,Mu Xu*

Main category: cs.CV

TL;DR: 提出Memory-Execute-Review框架，通过记忆、执行与复盘三模块协同，在对象目标导航任务上同时提升成功率与泛化能力，多个数据集上取得显著领先。


<details>
  <summary>Details</summary>
Motivation: 当前SFT方法虽然SR较高但泛化差，TF方法泛化好但SR低，难以同时兼顾两者。为此设计一个能兼顾高SR与强泛化的新框架。

Method: 框架由三部分构成：层级记忆模块用于信息支持、执行模块负责常规决策与动作、复盘模块用于处理异常并修正行为。将该框架应用于对象目标导航任务并在多数据集上进行评估。

Result: 在4个数据集的平均SR上，相较于所有基线方法在TF和ZS设定下分别提升了7%和5%；在HM3D_v0.1和HM3D_OVON上ZS情形分别提升8%和6%；在MP3D和HM3D_OVON上不仅超越所有TF方法，也超过所有SFT方法，在SR和泛化上实现全面领先（分别提升5%和2%）。

Conclusion: 该论文提出了一个名为Memory-Execute-Review的框架，在视觉语言导航（VLN）任务中显著提升了成功率（SR）与泛化能力。

Abstract: Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.

</details>


### [50] [SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing](https://arxiv.org/abs/2602.05480)
*Peihao Wu,Yongxiang Yao,Yi Wan,Wenfei Zhang,Ruipeng Zhao,Jiayuan Li,Yongjun Zhang*

Main category: cs.CV

TL;DR: SOMA-1M：一个1.3M+的像素级对齐SAR-光学多分辨率数据集，采用粗到细配准方法，支撑四类任务基准并显著提升算法性能，特别是在多模态匹配上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基准数据集存在单一分辨率、数据规模不足和对齐精度低等问题，不足以支持多尺度遥感基础模型的训练与泛化，因而需要一个大规模、多分辨率且像素级对齐的MRSI数据集。

Method: 提出了一套严谨的粗到细图像匹配框架，用于解决多模态投影变形和大规模数据配准问题；数据集整合了Sentinel-1、PIESAT-1、Capella Space和Google Earth影像，统一为512×512像素并覆盖0.5 m–10 m多分辨率；构建了包括图像匹配、图像融合、SAR辅助去云和跨模态翻译在内的四级评测基准，评估了30余种主流算法。

Result: SOMA-1M包含超过130万对地理参考的像素级精确对齐图像（512×512），覆盖全球、12类典型地物、分辨率0.5–10 m；在基准评测中，使用SOMA-1M进行监督训练显著提升了30余种算法在四类任务上的表现，其中多模态配准达到了最先进水平。

Conclusion: 本文构建了一个大规模、多分辨率、像素级对齐的SAR-光学配准数据集SOMA-1M，并通过严格的粗到细配准方法实现高精度配准；基于该数据集建立了四类任务的基准并验证了监督训练显著提升性能，尤其在多模态配准上达到了SOTA水平。

Abstract: Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.

</details>


### [51] [Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014](https://arxiv.org/abs/2602.05487)
*Julien Moreau,S. Ambellouis,Yassine Ruichek*

Main category: cs.CV

TL;DR: 2014年博士工作草稿：提供PFSeq鱼眼视频数据集并用2014年常规特征器对鱼眼图像做了系统评估，旨在支持鱼眼相机自标定与城市定位研究；未提出新算法，结果仅代表当时状态。


<details>
  <summary>Details</summary>
Motivation: 在车顶朝天安装的鱼眼相机用于城市场景定位时，缺乏适合鱼眼畸变的特征选择策略；同时存在“投影模型-特征”相互依赖的鸡与蛋问题，需要评估在未知或粗略投影模型情况下哪些特征更适用以推动自标定与视觉测程/立体重建。

Method: 收集并公开了PFSeq（Photorealistic Fisheye Sequence）数据集；对多种现有特征检测与描述器在鱼眼图像上的性能进行了系统对比实验（基于2014年前的工具和实现）；分析了特征匹配在自标定流程中的作用与限制。

Result: 发布了PFSeq数据集（DOI: https://doi.org/10.57745/DYIVVU）并给出详尽基准实验结果，报告了不同特征在匹配稳定性、重复率和对自标定影响方面的表现（基于2014年方法），但未包含对专为全景/鱼眼设计算法的比较，且未经过同行评审。

Conclusion: 该报告呈现了作者2014年博士研究期间针对鱼眼图像特征检测与描述的系统性评估与数据公开工作，结论为在没有精确投影模型的前提下，需选择对投影畸变更鲁棒的特征方法以便用于自标定与视觉里程计；作者未提出新算法，但提供了数据集PFSeq与详尽实验供后续研究参考。

Abstract: What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).

</details>


### [52] [VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency](https://arxiv.org/abs/2602.05508)
*Zhuang Xiong,Chen Zhang,Qingshan Xu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出VGGT-Motion：结合光流引导的运动感知分段、锚点驱动的直接Sim(3)稠密配准与轻量级位姿图优化，实现了高效鲁棒的公里级无标定单目SLAM，显著降低尺度漂移并提升精度与效率。


<details>
  <summary>Details</summary>
Motivation: 现有无标定单目SLAM在长序列下尺度漂移严重；运动无感知的分段破坏上下文导致零运动漂移，而传统几何配准计算开销大，难以在公里级轨迹上保持一致性与效率。

Method: 方法包括：1) 运动感知子图构建，使用光流引导自适应分段、静态冗余剪枝和转弯封装；2) 锚点驱动的直接Sim(3)配准，基于上下文平衡的锚点实现无搜索、像素级稠密配准以进行回环检测；3) 轻量级子图级位姿图优化，以线性复杂度实现可扩展的全局一致性。

Result: 实验表明VGGT-Motion在零样本、长距离的无标定单目SLAM任务上显著提升轨迹精度与效率，达到了最先进的性能。

Conclusion: VGGT-Motion通过运动感知的子图构建、锚点驱动的直接Sim(3)配准和轻量级的子图级位姿图优化，解决了长序列单目SLAM的尺度漂移和效率问题，实现了无标定、长距离下的全局一致性。

Abstract: Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.

</details>


### [53] [Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification](https://arxiv.org/abs/2602.05522)
*Jeongbin You,Donggun Kim,Sejun Park,Seungsang Oh*

Main category: cs.CV

TL;DR: 本文提出轻量的Mapper-GIN，通过Mapper生成区域图并用GIN分类，实现了在噪声与变换腐蚀下的高鲁棒性，证明结构抽象是提高3D点云鲁棒性的有效且可解释途径。


<details>
  <summary>Details</summary>
Motivation: 探究仅通过结构抽象（而非放大骨干或复杂数据增强）能否提升3D点云分类的鲁棒性，并评估拓扑分解的方法是否提供有效且轻量的鲁棒性来源。

Method: 提出Mapper-GIN：用Mapper算法（PCA lens、立方覆盖、基于密度的聚类）将点云划分为重叠区域，构建基于区域重叠的图结构，并用Graph Isomorphism Network对该区域图进行分类。

Result: 在ModelNet40-C腐蚀基准上，Mapper-GIN在噪声与变换类腐蚀下表现出竞争性且稳定的准确率，参数量仅0.5M，显示出通过区域图抽象与GIN消息传递可获得强鲁棒性。

Conclusion: 引入拓扑启发的区域图抽象能在不增加大量模型复杂度或特殊数据增强的前提下，提高点云分类在噪声与变换腐蚀下的鲁棒性。

Abstract: Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.

</details>


### [54] [Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains](https://arxiv.org/abs/2602.05527)
*Ben Isselmann,Dilara Göksu,Andreas Weinmann*

Main category: cs.CV

TL;DR: 在显微镜图像任务中，使用在相关生物图像（HPA）上进行DINO自监督预训练的ViT能比ImageNet或OpenCell预训练更好地迁移到OpenCell的蛋白质定位任务，显示领域相关大规模SSL预训练的价值。


<details>
  <summary>Details</summary>
Motivation: 任务特定的显微镜数据通常样本量太小，难以训练鲁棒的深度表示；自监督学习可通过大规模无标签数据预训练来缓解，但不同显微镜域（染色、通道差异）之间的表示迁移能力尚不明确，因此要研究DINO表示的跨域迁移性。

Method: 作者在三个DINO预训练骨干（ImageNet-1k、Human Protein Atlas（HPA）、OpenCell）上生成图像嵌入，并在OpenCell数据集标签上训练一个监督分类头来评估迁移性能。评估指标为平均macro F1分数，并比较不同预训练来源与直接在OpenCell上训练的DINO模型。

Result: 所有预训练模型均能良好迁移。HPA预训练的显微镜特定模型表现最好：mean macro F1 = 0.8221 ± 0.0062，略优于直接在OpenCell上训练的DINO（0.8057 ± 0.0090）。表明大规模领域相关自监督预训练能在标注有限时带来强下游性能。

Conclusion: 该论文表明在显微镜图像领域，使用自监督学习（DINO）在大规模相关数据上预训练的Vision Transformer能有效迁移到不同染色和通道配置的显微镜数据集上，且领域相关的预训练（HPA）比通用自然图像（ImageNet）更有利。

Abstract: Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.

</details>


### [55] [SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation](https://arxiv.org/abs/2602.05534)
*Youngwoo Shin,Jiwan Hur,Junmo Kim*

Main category: cs.CV

TL;DR: 提出SSG（推理时强调语义残差的频域引导）和DSE（频域先验构造），以恢复和强化VAR模型的粗到细层级，提升生成质量且无需训练。


<details>
  <summary>Details</summary>
Motivation: 实践中VAR模型在推理时会偏离训练时的粗到细层级（hierarchy），主要由于模型容量有限与误差累积，导致后续尺度无法补充有区分性的细节。论文从信息论角度提出：若每尺度能贡献先前尺度无法解释的高频信息，可减轻这一偏差，从而保留粗到细的生成过程。

Method: 从信息论出发，分析多尺度生成中每一级应贡献的互补高频信息；提出SSG——一种在推理时操作语义残差（高频目标信号）的指导方法；为构造更准确的共尺度先验，提出DSE，在频域上处理和锐化先验以更好分离语义残差；SSG可应用于使用离散视觉token的任意VAR模型，在推理阶段不需重新训练。

Result: 提出的SSG配合DSE在多种VAR模型与token化设计上均带来一致的性能提升：更高的图像保真度与多样性，同时保持低推理延迟。实验验证了SSG能在不改变训练流程下改善粗到细生成的层级性和输出质量。公开了代码实现。

Conclusion: 本文提出了在推理阶段保持视觉自回归模型（VAR）分层生成结构的方法，通过强调每一尺度应贡献未被先前尺度解释的高频信息，从信息论角度缓解训练-推理不一致问题。引入的Scaled Spatial Guidance（SSG）为训练免费且在推理时可插拔的引导策略，结合一种频域先验构造方法Discrete Spatial Enhancement（DSE）以更好隔离和强化语义残差。实验显示SSG在多种VAR设置上提升了图像保真度和多样性，同时保持低延迟。

Abstract: Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.

</details>


### [56] [A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments](https://arxiv.org/abs/2602.05538)
*Malaz Tamim,Andrea Matic-Flierl,Karsten Roscher*

Main category: cs.CV

TL;DR: 在JRDB数据集上，融合（DAL）优于单一模态，尤在困难场景；但需解决错位与激光雷达特定损坏的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 在机器人、工业监测与监控等对安全性要求高的场景中，评估不同传感器组合的3D行人检测性能与鲁棒性，以指导工程应用与未来研究。

Method: 选取代表性模型：BEVDepth（仅相机）、PointPillars（仅激光雷达）、DAL（相机-激光雷达融合），在JRDB室内外数据集上对比评估，按遮挡、距离分级并施加传感器腐败与错位干扰，分析性能变化。

Result: 实验表明：1) 融合模型DAL整体性能最好，尤其在高遮挡与远距情况下改善明显；2) 激光雷达模型PointPillars在近距与无遮挡时表现较好；3) 相机模型BEVDepth性能最低，受遮挡、距离与噪声影响最大；4) 融合模型对传感器腐败有一定鲁棒性但对传感器错位敏感。

Conclusion: 融合方法在多数情形下优于单一传感器，特别是在遮挡和远距场景，但仍对传感器错位与某些激光雷达损坏敏感。

Abstract: Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.

</details>


### [57] [FastVMT: Eliminating Redundancy in Video Motion Transfer](https://arxiv.org/abs/2602.05551)
*Yue Ma,Zhikai Wang,Tianhao Ren,Mingzhe Zheng,Hongyu Liu,Jiayi Guo,Mark Fong,Yuxuan Xue,Zixiang Zhao,Konrad Schindler,Qifeng Chen,Linfeng Zhang*

Main category: cs.CV

TL;DR: FastVMT通过局部注意力掩码和梯度重用机制去除运动和梯度冗余，实现了对DiT基视频动作迁移的显著加速（3.43x），且保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于DiT的视频动作迁移方法计算开销大，已有加速方法只关注计算优化而忽略结构性低效，作者识别出两类冗余并提出结构性改进以进一步加速。

Method: 针对运动冗余，采用局部注意力掩码限制注意力交互到邻域像素；针对梯度冗余，设计梯度重用与跳过计算的优化方案，在扩散过程沿时间步复用先前梯度并跳过不必要的梯度计算。

Result: 在实验中，FastVMT在不牺牲生成视频的视觉保真度和时间一致性的前提下，平均实现3.43倍的推理速度提升。

Conclusion: 本文提出FastVMT，通过去除运动冗余和梯度冗余加速视频动作迁移任务，在不损失视觉质量和时间一致性的情况下实现了平均3.43倍的速度提升。

Abstract: Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.

</details>


### [58] [IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools](https://arxiv.org/abs/2602.05555)
*Panagiotis Sapoutzoglou,Orestis Vaggelis,Athina Zacharia,Evangelos Sartinas,Maria Pateraki*

Main category: cs.CV

TL;DR: IndustryShapes：首个面向工业装配场景的 RGB-D 6D 姿态数据集（classic:4.6k 图像/6k 姿态；extended:多模态与序列数据），提供现实挑战性场景并基准评测，揭示现有方法在工业部署中仍需改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多集中于家用或消费产品、合成或受控实验室环境，缺乏真实工业装配场景的数据；为评估在工业部署中实际可行的6D 姿态与感知方法，需提供现实、具有挑战性的 RGB-D 数据。

Method: 数据集由两部分组成：classic 集合（约4.6k 图像、6k 标注姿态）与 extended 集合（扩展模态以支持无模板与基于序列的方法），并包含五类具有挑战性的新物体类型，场景从简单到复杂、包含多实例情况。作者使用代表性的最新实例级与新物体6D 姿态估计方法以及目标检测、分割方法对数据集进行了基准评估。

Result: IndustryShapes 成功提供首个包含 RGB-D 静态上岗（onboarding）序列的数据集，并展示在多个基准方法上的评测结果，表明当前方法在该现实工业场景下仍有明显改进空间。数据集已上线（https://pose-lab.github.io/IndustryShapes）。

Conclusion: IndustryShapes 是一个面向工业机器人场景的 RGB-D 基准数据集，涵盖实例级与新物体（novel object）6D 姿态估计任务，提供现实工业装配环境下的多样化、具有挑战性的物体与场景，弥合实验室研究与实际制造部署之间的差距。

Abstract: We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.

</details>


### [59] [PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds](https://arxiv.org/abs/2602.05557)
*Michael Schwingshackl,Fabio F. Oberweger,Mario Niedermeyer,Huemer Johannes,Markus Murschitz*

Main category: cs.CV

TL;DR: PIRATR是一个基于点云的参数化6-DoF检测系统，模块化可扩展、合成数据训练并能零次微调泛化到真实场景，mAP=0.919。


<details>
  <summary>Details</summary>
Motivation: 弥合几何感知与可执行世界模型之间的差距，使感知系统不仅能定位物体也能预测可操作的参数（参数化对象），便于在机器人操作任务中直接使用。

Method: 在PI3DETR基础上扩展，采用模块化的类特定检测头，联合学习类别、6-DoF位姿和参数化属性（如夹持器开合度），并在合成数据上训练以在真实室外LiDAR数据上泛化。

Result: 在自动叉车平台的三类对象（吊钩式夹持器、装载平台、托盘）上验证，纯合成训练模型在真实LiDAR上达到mAP 0.919，无需额外微调，展现出强泛化能力。

Conclusion: PIRATR提出了一套端到端的、面向机器人的三维目标检测框架，能够直接从受遮挡的点云中估计多类6自由度姿态及类特定参数化属性，从而实现几何定位与任务相关属性估计的结合。

Abstract: We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.

</details>


### [60] [ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors](https://arxiv.org/abs/2602.05572)
*Zhenxiao Liang,Ning Zhang,Youbao Tang,Ruei-Sung Lin,Qixing Huang,Peng Chang,Jing Xiao*

Main category: cs.CV

TL;DR: ShapeGaussian：模板自由、基于视觉先验的两步方法（粗几何+神经细化），在单目视频4D人体重建中比SMPL等模板方法更准确、更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有通用无模板方法在高变形运动下缺乏稳健的视觉先验，难以从单目视频恢复高质量动态几何；而基于模板（如SMPL）的方法对姿态估计错误敏感，导致不真实伪影。需一种兼具高保真与鲁棒性的模板自由方法。

Method: 两步流程：1) 使用预训练模型学习粗糙可变形几何体，提供数据驱动的先验；2) 通过神经形变模型在该基础上细化，捕捉精细动态细节。采用多参考帧和2D视觉先验来处理2D关键点不可见问题并降低姿态估计误差带来的伪影。

Result: 在大量实验中，ShapeGaussian 在重建准确性、视觉质量和对不同人体动作的鲁棒性上均优于模板方法，能在休闲单目视频中生成更真实的4D人体重建。

Conclusion: ShapeGaussian 是一种结合模板自由的视觉先验以实现高保真、鲁棒的单目视频四维人体重建方法。该方法在保持细节和广泛动作鲁棒性方面，优于依赖 SMPL 的模板方法和无强先验的通用方法。

Abstract: We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.

</details>


### [61] [Visual Implicit Geometry Transformer for Autonomous Driving](https://arxiv.org/abs/2602.05573)
*Arsenii Shirokov,Mikhail Kuznetsov,Danila Stepochkin,Egor Evdokimov,Daniil Glazkov,Nikolay Patakin,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: ViGT是一个无校准、自监督的Transformer模型，从环视摄像头估计BEV连续3D占用场，在多个自动驾驶数据集上展现了出色的可扩展性和竞争性能。


<details>
  <summary>Details</summary>
Motivation: 目标是构建可扩展、结构简单且可泛化的自动驾驶基础几何模型，克服现有像素对齐方法的局限，提供统一的度量级BEV连续占用表示以支持多种几何任务，并减少对人工标注的依赖。

Method: 提出Visual Implicit Geometry Transformer（ViGT），采用校准自由的架构通过Transformer从多视图图像推断单一度量坐标系下的连续3D占用场。训练采用自监督策略，利用同步图像-激光雷达对进行无人工注释学习，并在五个大型数据集上混合训练。

Result: 在点图（pointmap）估计任务上取得SOTA级别的平均排名最好；在Occ3D-nuScenes基准上与有监督方法表现相当，验证了其可扩展性与泛化能力。源码已开源。

Conclusion: ViGT提出了一种面向自动驾驶的几何模型，能够从环视相机估计连续的3D占用场。通过无校准设计、自监督训练和BEV连续场建模，ViGT实现了跨传感器配置的可扩展性与泛化能力，在多个数据集上表现优异。

Abstract: We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.

</details>


### [62] [A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features](https://arxiv.org/abs/2602.05574)
*Mengyu Li,Ingibjörg Kristjánsdóttir,Thilo van Eimeren,Kathrin Giehl,Lotta M. Ellingsen,the ASAP Neuroimaging Initiative*

Main category: cs.CV

TL;DR: 本文提出一种将CNN图像特征与体积基于ML特征融合的混合框架，用于APD亚型与PD的差异诊断，达到了AUC 0.95/0.86/0.92的良好结果，提示该方法可提升早期诊断可靠性。


<details>
  <summary>Details</summary>
Motivation: 早期APD与PD临床表现重叠，导致误诊率高；寻找可靠的影像学生物标志物以改进早期差异诊断具有重要临床价值。

Method: 提出一个混合模型：使用卷积神经网络处理T1加权MRI图像和12个深部脑结构的分割掩码，提取空间特征；同时提取对应结构的体积测量作为机器学习输入（例如随机森林或SVM）；将CNN特征与体积特征融合并训练分类器，分别完成PSP vs PD、MSA vs PD、PSP vs MSA的二分类任务。

Result: 融合多模态信息后，模型在三个二分类任务上达到了较好性能：PSP vs PD的AUC=0.95，MSA vs PD的AUC=0.86，PSP vs MSA的AUC=0.92，表明结合空间和结构（体积）信息能提高分型稳健性。

Conclusion: 该研究提出的混合框架将CNN特征与基于体积的机器学习特征融合，能够提高APD亚型（PSP、MSA）与PD之间以及亚型互相区分的分类准确性，显示出在早期差异诊断中的潜力。

Abstract: Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.

</details>


### [63] [LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization](https://arxiv.org/abs/2602.05577)
*Shiyu Wu,Shuyan Li,Jing Li,Jing Liu,Yequan Wang*

Main category: cs.CV

TL;DR: 提出LocateEdit-Bench：231K指令驱动编辑图像数据集，含4种编辑模型和3类编辑类型，及两套多指标评估协议，用于推动伪造定位方法针对最新编辑范式的发展。


<details>
  <summary>Details</summary>
Motivation: 现有伪造定位方法主要针对基于修补(inpainting)的操纵，而对指令驱动的编辑方法效果不足；随着可控语义编辑的发展，需要新的基准来评估定位性能。

Method: 构建一个包含231K编辑图像的大规模数据集，整合了4种先进的编辑模型并覆盖3类常见编辑类型；设计多尺度评价协议（两种多指标评估协议）并对现有定位方法进行评估与分析。

Result: 构建了大规模数据集LocateEdit-Bench并完成了详细数据分析，对比评估了现有方法的性能，展示了当前方法在指令驱动编辑场景下的不足，为后续研究提供了测试基线与评估协议。

Conclusion: 该论文提出了针对指令驱动图像编辑的伪造定位基准数据集LocateEdit-Bench，填补了现有方法对最新编辑范式适应不足的空白。

Abstract: Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.

</details>


### [64] [LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2602.05578)
*Junyang Chen,Xiangbo Lv,Zhiqiang Kou,Xingdong Sheng,Ning Xu,Yiguo Qiao*

Main category: cs.CV

TL;DR: LoGoSeg是一种高效单阶段OVSS方法，通过对象先验、区域对齐与双流融合解决了VLM基方法的空间错配与幻觉问题，在多项基准上展现出良好性能与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM（如CLIP）的OVSS方法受限于图像级预训练导致空间对齐不足，且缺乏强对象先验与区域约束，容易产生对象幻觉或漏检。需要高效且具区域约束的单阶段方案来改善在复杂场景中的分割质量。

Method: 提出单阶段框架LoGoSeg：1) 对象存在先验：基于全局图文相似度动态加权类别，抑制无关类别的激活；2) 区域感知对齐：在区域层面建立精确的视觉-文本对应，提升空间对齐；3) 双流融合：融合局部结构与全局语义信息以获得更准确的分割预测。无需外部掩膜提议、额外骨干或数据。

Result: 在六个基准（A-847, PC-459, A-150, PC-59, PAS-20, PAS-20b）上进行了广泛实验，LoGoSeg表现出具有竞争力的性能和强的开放词汇泛化能力，同时保持效率和轻量性。

Conclusion: LoGoSeg通过引入对象存在先验、区域感知对齐和双流融合，有效缓解了开放词汇语义分割中的错配与幻觉问题，提升了开放类别下的空间精度与泛化能力。

Abstract: Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.

</details>


### [65] [Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation](https://arxiv.org/abs/2602.05582)
*Joe-Mei Feng,Sheng-Wei Yu*

Main category: cs.CV

TL;DR: 提出基于李群SE(3)的几何可观测性指标GOI，通过曲率谱描述单特征对位姿估计的影响，统一并扩展了影响函数、费舍尔信息和条件数分析，能解释退化情况并提供无训练的动态特征诊断工具。


<details>
  <summary>Details</summary>
Motivation: 现有的条件数分析、欧几里得扰动和费舍尔信息界不能解释单个图像特征如何影响位姿估计，也不能解释动态或不一致观测为何会对现代SLAM/结构光系统产生巨大失真，需一个几何一致的单测量影响描述。

Method: 将影响函数理论推广到矩阵李群，构造左平移的内在扰动算子，使用曲率算子和李代数的可观测子空间结构来定义GOI，并通过谱分解分析可观测性与灵敏度的关系；在总体极限下将GOI与费舍尔信息几何对齐；将结果嵌入Gauss-Newton流水线以产生日志无训练诊断信号。

Result: 导出GOI并证明其谱分解与弱可观测性和增幅灵敏度直接对应，解释经典退化情形（如纯旋转、消失视差）和动态特征沿弱曲率方向的放大效应；在总体下GOI等价于SE(3)上的费舍尔信息，且能作为单测量的Cramer-Rao类界；实用上可用于在不改动SLAM架构下识别动态特征和弱可观测配置的轻量诊断信号。

Conclusion: 该论文提出了在李群SE(3)上分析相机位姿估计中单特征灵敏度的统一算子理论框架，扩展了影响函数到矩阵李群，导出针对左平移M估计器的内在扰动算子，并定义了几何可观测性指标（GOI）。

Abstract: We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).
  The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.
  The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.

</details>


### [66] [A Mixed Reality System for Robust Manikin Localization in Childbirth Training](https://arxiv.org/abs/2602.05588)
*Haojie Cheng,Chang Liu,Abhiram Kanneganti,Mahesh Arjandas Choolani,Arundhati Tushar Gosavi,Eng Tat Khoo*

Main category: cs.CV

TL;DR: 本文提出一种将虚拟引导与实体触觉结合的混合现实分娩训练系统，通过外接RGB-D相机标定与粗到细定位实现空间精确的虚拟-实体对齐，独立头显上运行稳定。在83名医学生的对照试验中，MR显著优于VR并更受欢迎。


<details>
  <summary>Details</summary>
Motivation: 临床轮转时间缩短、患者对教学分娩的顾虑以及分娩不可预测性，导致医学生难以获得足够的产科分娩实践机会；因此需要一种既能保留真实触觉又支持独立训练并减轻临床教学负担的替代性教学工具。

Method: 系统在商用头显的透视(passthrough)能力上扩展，通过标定外接RGB-D相机实现物理训练对象的实时视觉融合。采用粗到细的定位流程：先用fiducial标记对准母体人模以定义分娩区域，再在该区域内登记预先扫描的新生儿头部模型，从而实现虚拟指导双手在实体人模附近的空间精确叠加并与触觉交互结合。系统在独立头显上实现稳定的本体定位，无需外部计算资源。进行了大规模用户研究与定量评估。

Result: 系统在标定和注册任务中表现出准确且稳定的人模定位；用户研究（83名四年级医学生，4名高级产科医生评分）显示，MR训练在分娩操作、产后处理及总体表现上显著优于纯VR训练，且受训者更偏好MR。

Conclusion: 该论文提出并验证了一种将混合现实(MR)与实体产科人模触觉结合的分娩训练系统，能够在无需持续现场专家监督的情况下提供空间精确的虚拟引导并保留真实触觉反馈，从而提升教学效率和独立练习能力。

Abstract: Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.

</details>


### [67] [EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality](https://arxiv.org/abs/2602.05590)
*Haojie Cheng,Shaun Jing Heng Ong,Shaoyu Cai,Aiden Tat Yang Koh,Fuxi Ouyang,Eng Tat Khoo*

Main category: cs.CV

TL;DR: EgoPoseVR结合头显运动与RGB-D时空特征并用运动学优化，解决了HMD场景下的时序不稳定与下半身估计问题，表现优于现有方法并通过大规模合成数据和真实用户研究验证。


<details>
  <summary>Details</summary>
Motivation: 现有头戴摄像头方法在VR HMD场景下存在时序不稳定、下半身估计差和实时性能不足等问题，且VR应用要求高精度、时序一致的全身追踪以提升沉浸感和意图表达。

Method: 提出双模态融合流水线：使用时空编码器提取帧级与关节级表征，利用跨注意力机制融合头显运动信号与RGB-D特征，最后通过运动学优化模块引入HMD约束以增强稳定性与物理一致性。同时构建大规模合成数据集用于训练与评估。

Result: 在1.8M帧的合成数据及真实场景用户研究中，EgoPoseVR在准确性、稳定性、具身感和未来使用意愿上均显著优于最先进的基线方法，证明其可在不依赖额外佩戴传感器或房间级跟踪的条件下实现鲁棒全身追踪。

Conclusion: EgoPoseVR通过融合头显运动与RGB-D视觉信息，并结合时空编码与运动学约束，显著提升了VR环境下的全身位姿估计的准确性和稳定性，且在合成与真实数据上均优于现有方法。

Abstract: Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.

</details>


### [68] [CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion](https://arxiv.org/abs/2602.05598)
*Aon Safdar,Mohamed Saadeldin*

Main category: cs.CV

TL;DR: 提出CAViT，用通道自注意力替代静态MLP，实现内容感知的token混合，提升精度并减小计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统ViT使用固定的MLP进行通道混合，缺乏对输入内容的自适应能力；为此提出一种统一的、内容感知的token混合策略，通过注意力机制增强通道间交互。

Method: 在每个Transformer块中先执行空间自注意力（标准的ViT自注意力），随后执行通道自注意力模块以实现基于全局图像上下文的通道重校准。该通道自注意力替代传统MLP，采用注意力权重对通道间交互进行动态建模。整个架构保持块数与深度不变。

Result: 在五个基准数据集（涵盖自然图像与医学影像）上，CAViT相比标准ViT最高提升约+3.6%准确率，同时参数量与FLOPs均降低超过30%。注意力可视化显示更清晰且语义相关的激活模式。

Conclusion: CAViT通过将固定的通道MLP替换为基于注意力的通道自注意力模块，实现了动态的通道混合，从而提升了表示能力并保持模型复杂度不增。实验表明在自然与医学数据集上均优于标准ViT，同时显著降低参数与FLOPs。

Abstract: Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.

</details>


### [69] [Multi-instance robust fitting for non-classical geometric models](https://arxiv.org/abs/2602.05602)
*Zongliang Zhang,Shuxiang Li,Xingwang Huang,Zongyue Wang*

Main category: cs.CV

TL;DR: 提出一种基于模型到数据误差的免阈值估计器与元启发式优化器组合，用于从噪声数据中鲁棒重建多个非经典模型实例，并在多种模型上验证了效果，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 现有稳健拟合方法多针对经典模型且多仅重建单实例，缺乏对螺旋曲线、程序化字符、自由形面等非经典模型的多实例稳健重建方法，且很多方法依赖预定义误差阈值或可微目标，限制了应用范围和鲁棒性。

Method: 将多实例拟合表述为包含估计器和优化器的优化问题；设计基于模型到数据误差的非可微估计器以避免预设误差阈值；采用元启发式算法（全局搜索）优化非可微目标以获得全局最优或近似最优解。

Result: 在多种非经典模型上的实验证明了该方法在含噪与含外点数据下的有效性；代码开源（提供GitHub链接），展示了方法能在无阈值设定下鲁棒地重建多个模型实例。

Conclusion: 本文提出了一种用于从含噪数据中重建多个非经典模型实例的稳健拟合框架，通过基于模型到数据误差的新型估计器结合元启发式优化器，实现了对异常值的免阈值处理与全局搜索能力。

Abstract: Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.

</details>


### [70] [Unified Sensor Simulation for Autonomous Driving](https://arxiv.org/abs/2602.05617)
*Nikolay Patakin,Arsenii Shirokov,Anton Konushin,Dmitry Senushkin*

Main category: cs.CV

TL;DR: XSIM：将3DGUT splatting扩展到通用滚动快门与球面相机场景，提出相位建模与双不透明度3D高斯以修正方位边界时间不连续和颜色-几何不匹配，在多数据集上实现SOTA并开源。


<details>
  <summary>Details</summary>
Motivation: 传统3D Gaussian splatting与3DGUT在面对运动传感器（滚动快门）和球面投影（LiDAR）时存在时间/空间不连续、投影错误以及颜色-几何不匹配等问题，导致仿真信度不足。自动驾驶场景对传感器畸变建模与高保真渲染有严格需求，因此需要一个统一且灵活的框架来同时建模外观与几何并正确处理滚动快门与球面相机的挑战。

Method: 基于3DGUT（3D Gaussian Unscented Transform） splatting，作者：1) 提出通用的滚动快门模型用于建模运动传感器（尤其自动驾驶场景）中的时间维度；2) 针对球面相机/ LiDAR的方位周期问题设计相位（phase）建模机制，保证在方位边界处高斯投影的时间/形状连续性处理正确；3) 将单一不透明度扩展为两个不透明度参数，分别与几何和颜色相关，以解决颜色溢出或几何不一致问题；4) 在多个自动驾驶数据集上进行定量与定性评估，比较近期强基线。实现细节包括基于Unscented Transform的高斯投影、滚动快门时间采样和扩展的不透明度优化。

Result: 提出的XSIM在Waymo Open Dataset、Argoverse 2和PandaSet上均超越了近期强基线，取得了SOTA性能。定性结果显示增强的几何一致性和更逼真的外观渲染；定量指标（论文中）也显著提升。代码已开源于作者仓库。

Conclusion: 该论文提出了XSIM，一种面向自动驾驶的传感器仿真框架，通过扩展3D Gaussian splatting并引入通用滚动快门建模，能够在动态场景中渲染复杂的传感器畸变。针对球面相机（如LiDAR）在3DGUT投影时的方位边界周期性和时间不连续导致粒子投影错误的问题，提出了相位模型以显式处理由Unscented Transform投影的高斯在方位边界处的时间与形状不连续性。论文还扩展了3D高斯表示，引入两个不同的不透明度参数以解决几何与颜色分布不匹配问题。实验在Waymo、Argoverse2和PandaSet上验证，优于多个强基线并达到SOTA，且代码开源。

Abstract: In this work, we introduce \textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.

</details>


### [71] [ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing](https://arxiv.org/abs/2602.05629)
*Jianlei Chi,Yuzhen Wu,Jiaxuan Hou,Xiaodong Zhang,Ming Fan,Suhui Sun,Weijun Dai,Bo Li,Jianguo Sun,Jun Sun*

Main category: cs.CV

TL;DR: ROMAN通过多头注意力与基于LLM的交通法加权，能生成更复杂、多样且高风险的违法场景，显著优于ABLE和LawBreaker，提升ADS测试覆盖与漏洞发现能力。


<details>
  <summary>Details</summary>
Motivation: 现有ADS测试方法难以生成复杂、高风险的违法场景，且难以考虑多车交互与关键情形，导致无法充分发现ADS在交通法违背下的安全隐患。

Method: 提出一个包含多头注意力网络以建模车辆与信号等要素交互，以及一个基于大模型的风险加权模块的交通法加权机制来衡量违规的严重性与发生概率；在CARLA仿真平台对Baidu Apollo进行评估并与ABLE、LawBreaker比较。

Result: 在CARLA平台上测试Baidu Apollo时，ROMAN在平均违规次数上比ABLE高7.91%、比LawBreaker高55.96%，并保持更高场景多样性；且能针对输入法规的每一条生成违规场景，发现更多高风险违规。

Conclusion: ROMAN能够通过结合多头注意力机制与交通法规加权模块，有效生成高风险、违反交通法规的测试场景，从而比现有方法更全面地暴露ADS的弱点。

Abstract: Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.

</details>


### [72] [UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos](https://arxiv.org/abs/2602.05638)
*Jinlin Wu,Felix Holm,Chuxi Chen,An Wang,Yaxin Hu,Xiaofan Ye,Zelin Zang,Miao Xu,Lihua Zhou,Huai Liao,Danny T. M. Chan,Ming Feng,Wai S. Poon,Hongliang Ren,Dong Yi,Nassir Navab,Gaofeng Meng,Jiebo Luo,Hongbin Liu,Zhen Lei*

Main category: cs.CV

TL;DR: UniSurg放弃像素重建，采用运动驱动的潜在预测与多项正则化，在大规模手术视频上预训练，显著提升多项外科视频理解任务表现，成为新的通用基准。


<details>
  <summary>Details</summary>
Motivation: 像素级重建浪费模型容量在烟雾、镜面反光等低层视觉细节上，无法有效学习对外科理解关键的语义与运动结构，因此需要面向运动的潜在表征学习。

Method: 基于V-JEPA框架，提出三项针对手术视频的技术：运动引导的潜在预测（优先语义区域）、时空亲和自蒸馏（保持关系一致性）、特征多样性正则化（防止表征塌缩）。并构建了规模化的UniSurg-15M数据集用于预训练。

Result: 在17个基准上全面领先：在手术工作流识别上相较SOTA提升+14.6% F1（EgoSurgery）、+10.3%（PitVis）；动作三元组识别CholecT50取得39.54% mAP-IVT；并在技能评估、息肉分割、深度估计等任务显著改善。

Conclusion: UniSurg通过在视频原生的潜在运动预测范式上替代像素重建，实现了更聚焦语义结构的手术视频表征，显著提升多项下游任务性能，是外科视频理解的新基准。

Abstract: While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.

</details>


### [73] [Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances](https://arxiv.org/abs/2602.05650)
*Amir Ansari,Jana Subirana,Bruna Silva,Sergio Escalera,David Gallardo-Pujol,Cristina Palmero*

Main category: cs.CV

TL;DR: 用UDIVA数据和跨模态、跨主体Transformer模型，细粒度（nuance）的人格标签比facet和trait更易被视听互动信号预测，MSE最多下降74%。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统人格识别常依赖聚合的广泛特质评分作为标注，且训练数据有限，导致同一特质分数可能对应多样且受情境影响的行为，影响泛化；因此探索更细粒度的人格层次是否能提供更稳定、更可预测的目标。

Method: 作者使用UDIVA v0.5数据集，构建了基于Transformer的模型，包含跨模态（音频-视频）和跨主体（考虑对话双人关系）的注意力机制，对不同层级（trait、facet、nuance）的人格标签分别进行预测并比较性能。

Result: 在多种互动情境下，nuance级别模型的表现持续优于facet和trait级模型，平均平方误差显著降低，表明细粒度的人格标签更有利于从行为数据中恢复人格信息。

Conclusion: 本文结论是：在视听互动数据上，利用更细粒度的人格层次（尤其是nuance级别）可以显著提升人格识别性能，细粒度模型相比facet和trait模型在均方误差上最高可降低74%。

Abstract: Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.

</details>


### [74] [ShapeUP: Scalable Image-Conditioned 3D Editing](https://arxiv.org/abs/2602.05676)
*Inbar Gat,Dana Cohen-Bar,Guy Levy,Elad Richardson,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: ShapeUP通过在预训练3D模型上进行监督的潜变量-to-潜变量翻译（使用图像作为提示并采用3D DiT），实现可扩展且严格几何一致的精细3D编辑，优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法在视觉可控性、几何一致性与可扩展性之间存在权衡：优化方法慢，多视图2D传播会产生视觉漂移，训练-free潜变量操作受限于冻结先验且难以从规模化收益。为解决这些问题，作者提出在原生3D表示上进行有监督的潜变量翻译，以兼具可控性与一致性并能扩展。

Method: 作者在预训练的3D基础模型上进行监督微调，训练数据为三元组（源3D形状、编辑后的2D图像、对应的编辑后3D形状），并使用3D Diffusion Transformer (DiT) 学习直接映射，将图像作为提示来引导潜变量编辑，实现隐式无掩码局部与整体编辑。

Result: 实验证明ShapeUP在保持身份一致性和编辑保真度方面，稳定优于现有有训练的和无训练的基线方法，能在本地与全局编辑中提供更精细的控制并维持结构一致性。

Conclusion: ShapeUP是一个可扩展的、基于图像条件的3D编辑框架，通过在原生3D表示空间内将编辑视为监督的潜变量到潜变量翻译，从而在保持几何一致性的同时实现精细的视觉可控性。

Abstract: Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.

</details>


### [75] [Poster: Camera Tampering Detection for Outdoor IoT Systems](https://arxiv.org/abs/2602.05706)
*Shadi Attarha,Kanaga Shanmugi,Anna Förster*

Main category: cs.CV

TL;DR: 针对静态图像的摄像头篡改检测，深度学习更准，规则方法更轻量并适合资源受限场景；文中并发布了相关数据集。


<details>
  <summary>Details</summary>
Motivation: 室外智能摄像头易受人为破坏和环境影响，且许多实际应用只提供静态图像而非连续视频，需开发单帧图像的篡改检测方法并评估不同方法在准确率、计算成本与训练数据需求上的权衡。

Method: 提出两种方法：1) 规则基方法，通过图像统计特征（如边缘密度、频谱能量等）与阈值判断模糊和旋转篡改；2) 深度学习方法，训练卷积神经网络对单帧图像进行篡改分类。

Result: 实验表明：深度学习模型在检测精度上显著优于规则方法；规则方法在计算开销低、训练数据需求小且无需长时间校准的条件下表现稳健。提供了包括正常、模糊与旋转图像的公开数据集以支持后续研究。

Conclusion: 深度学习方法在准确率上优于规则方法，但规则方法在资源受限或无法长期校准的场景更适用。

Abstract: Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.

</details>


### [76] [Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization](https://arxiv.org/abs/2602.05718)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

TL;DR: 本文在点监督动作定位中首次引入三项自监督时序理解任务，采用多任务学习提升模型的时序一致性理解，实验验证了该方法能显著改善动作时段定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有点监督方法只进行片段级分类，缺乏显式的时序关系建模，而动作的完整定义依赖于帧间时序关系，故引入自监督任务促进模型理解动作的时间一致性，从而更准确地定位完整动作片段。

Method: 在点监督框架下，作者设计三项自监督时序理解任务：动作完成（Action Completion）、动作顺序理解（Action Order Understanding）和动作规律理解（Action Regularity Understanding），并将其以多任务学习方式与原有的片段级分类头联合训练，使模型从单帧标签学习时序一致性信息。

Result: 在四个基准数据集上进行广泛实验，结果显示所提方法相较于若干最新方法具有更好的定位性能，证明通过自监督时序任务能有效提升点监督动作定位效果。

Conclusion: 本文提出利用多任务自监督任务增强点监督时序动作定位模型的时序理解能力，从而改善动作时段定位效果。实验表明在四个基准数据集上优于现有方法。

Abstract: Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.

</details>


### [77] [Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification](https://arxiv.org/abs/2602.05729)
*Lexiang Hu,Youze Xue,Dian Li,Gang Liu,Zhouchen Lin*

Main category: cs.CV

TL;DR: 提出通过多嵌入生成+自适应融合（AGFF-Embed）并结合EGA增强训练，达到同时提升全局与细粒度多模态理解的SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 观察到现有CLIP类与MLLM类嵌入模型多聚焦全局语义，且现实复杂场景常包含全局和细粒度混合信息，单一尺度嵌入难以兼顾，因而需要一种兼容的融合机制来同时捕获两类信息。

Method: 在推理阶段对MLLM进行prompting，使其输出多个关注不同语义层面的嵌入（全局与若干细粒度视角），随后设计自适应融合模块对这些嵌入加权平滑聚合。为提升训练时的判别性，引入Explicit Gradient Amplification(EGA)以放大批内难负样本梯度，从而实现更强的区分能力而不需额外数据编辑。

Result: 在MMEB与MMVP-VLM基准上，AGFF-Embed在通用与细粒度理解任务上都实现了SOTA性能，表明其在混合感知场景中优于现有多模态嵌入方法。

Conclusion: 该论文提出了名为AGFF-Embed的方法，通过让多模态大模型(MLLM)生成多种聚焦不同语义维度的嵌入，然后自适应平滑地融合全局与细粒度信息，从而提升对混合感知场景的理解能力；并结合显式梯度放大(EGA)在批内增强难负样本，而无需对数据集做细粒度编辑。

Abstract: Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.

</details>


### [78] [Depth as Prior Knowledge for Object Detection](https://arxiv.org/abs/2602.05730)
*Moussa Kassem Sbeyti,Nadja Klein*

Main category: cs.CV

TL;DR: 把深度作为训练与推理的先验（权重、分层和阈值）而不是融合特征，能显著改善小/远目标检测，且易于部署。


<details>
  <summary>Details</summary>
Motivation: 小和远距离目标因尺度变化、低分辨率和背景干扰导致检测性能差，且安全关键场景需要可靠检测；现有利用深度的方法通常需要复杂的模型改动，作者希望提供一种轻量且通用的方法利用深度信息。

Method: 提出DepthPrior框架，包括训练阶段的Depth-Based Loss Weighting (DLW)和Depth-Based Loss Stratification (DLS)，以及推理阶段的Depth-Aware Confidence Thresholding (DCT)。框架把深度用于作为先验权重和分层策略而非特征融合，唯一额外开销是预估深度图的成本。

Result: 在四个基准（KITTI、MS COCO、VisDrone、SUN RGB-D）和两个检测器（YOLOv11、EfficientDet）上验证，DepthPrior对小目标带来最多+9% mAP_S和+7% mAR_S提升，且在推理时能达到高达95:1的真/伪检出恢复比。无需额外传感器或架构修改。

Conclusion: 该论文表明深度信息作为监督先验能够缓解小尺度和远距离物体检测的系统性性能下降，通过不改变检测器架构即可获得显著提升。

Abstract: Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.

</details>


### [79] [Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing](https://arxiv.org/abs/2602.05737)
*Luca Ciampi,Ludovico Iannello,Fabrizio Tonelli,Gabriele Lagani,Angelo Di Garbo,Federico Cremisi,Giuseppe Amato*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.

</details>


### [80] [FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)
*Ti Wang,Xiaohang Yu,Mackenzie Weygandt Mathis*

Main category: cs.CV

TL;DR: FMPose3D用Flow Matching将高斯噪声通过学得的ODE速度场快速输运到条件3D姿态分布，生成多样假设并用RPEA重投影聚合，兼顾效率与准确性，在多项人体与动物数据集上达到或超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 单视图3D姿态估计存在深度歧义与遮挡，需生成多种可行的3D假设。扩散模型虽然效果好但推理慢，故寻求更高效的生成方法。

Method: 基于Flow Matching的方法学习从标准高斯到条件3D姿态分布的连续变换（ODE velocity field），以少量积分步生成样本；通过不同噪声种子获得多假设；引入Reprojection-based Posterior Expectation Aggregation（RPEA）对多个假设在2D重投影误差上进行后验期望近似并输出最终预测。

Result: 在Human3.6M与MPI-INF-3DHP上优于现有方法，并在Animal3D与CtrlAni3D上达到SOTA，展示了在人与动物两类3D姿态数据集上的强泛化与效率。

Conclusion: FMPose3D将单视图3D人体/动物姿态估计建模为条件分布运输问题，通过Flow Matching学习定义在ODE上的速度场，从高斯先验高效生成多样的3D姿态样本，并用RPEA模块对样本进行重投影后聚合以求得单一准确预测。

Abstract: Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.

</details>


### [81] [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785)
*Timur Mamedov,Karina Kvanchiani,Anton Konushin,Vadim Konushin*

Main category: cs.CV

TL;DR: ReText通过在多摄像头Re-ID数据与带文本的单摄像头数据上联合学习（Re-ID、图像-文本匹配、文本引导重建），有效利用单摄像头语义信息，显著提升跨域Re-ID泛化性能。


<details>
  <summary>Details</summary>
Motivation: 虽然样式多样的单摄像头数据易于采集并可提升泛化，但其缺乏跨视角变化导致表征有限。通过为单摄像头数据配备文本描述并引入跨模态任务，能够补充语义信息并弥补视角缺失，从而改善在未见域的Re-ID效果。

Method: ReText在训练阶段联合优化三项任务：1）在多摄像头数据上进行标准的Re-ID任务；2）对单摄像头图像与其文本描述进行图像-文本匹配学习，增强跨模态语义一致性；3）基于文本引导的图像重建任务，利用文本信息丰富单摄像头图像的语义细节。网络在多模态监督下共享表征，从而提升在未见域的识别能力。

Result: 在若干跨域Re-ID基准数据集上，ReText显著优于最先进方法。具体表现为排名准确率和mAP等指标均有明显提升，证明将单摄像头+文本与多摄像头数据混合训练的可行性和有效性。

Conclusion: 本文提出ReText，通过在多摄像头Re-ID数据与带文本描述的单摄像头数据混合训练，实现更强的跨域泛化能力。结合图像-文本匹配和文本引导的图像重建任务，使单摄像头数据的语义信息得到充分利用，从而补偿其缺乏多视角的劣势。实验表明ReText在跨域Re-ID基准上显著优于现有方法。

Abstract: Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.

</details>


### [82] [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789)
*Hengyi Wang,Ruiqiang Zhang,Chang Liu,Guanjie Wang,Zehua Ma,Han Fang,Weiming Zhang*

Main category: cs.CV

TL;DR: 提出一个无训练的几何先验+提示方法，将重建的3D几何确定性地变换到与指令语义对齐的allocentric参考帧，以显式地处理视角变换，从而显著提升VLM在allocentric空间推理任务上的表现（≈10%提升）并保持egocentric能力。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言导航/动作等任务对空间推理的需求增加，现有VLM在需要视角变换的allocentric问题上表现不稳健，因这些问题要求在目标中心的坐标系中推理，而非观察相机视角。作者旨在让VLM更可靠地处理此类需要显式视角变换的任务，且不依赖昂贵的标注或微调。

Method: 方法包括：1) 用off-the-shelf几何专家（如深度估计、SLAM或多视几何重建）从单/多视图恢复绝对度量3D几何和语义特征；2) 根据用户指令解析出目标中心/参照物并构建一个查询条件的allocentric参考帧；3) 将重建的几何确定性地变换到该目标框架；4) 构造结构化的、基于几何的提示（prompt）并输入到预训练的VLM骨干，使模型在显式的、与任务语义对齐的坐标系下执行推理，减少需要隐式“心智旋转”的能力需求。该方法无需对VLM进行额外训练或微调。

Result: 在多个基准上对不同的VLM骨干进行评估，Allocentric Perceiver在allocentric空间推理任务上平均提升约10%，同时不损失egocentric任务性能，并超过了针对空间感知微调的模型以及当前最先进的开源和专有模型。

Conclusion: 该论文提出了Allocentric Perceiver，一种无训练（training-free）方法，通过使用现成的几何模块从一张或多张图像中恢复度量级3D状态，并根据指令语义意图实例化以查询为条件的allocentric参考帧，从而将心智旋转的负担从隐式推理转移到显式计算上。实验表明在多个VLM骨干上，allocentric任务性能大幅提升≈10%，同时保持egocentric任务的强性能，并优于进行了空间感知微调的模型与最先进的开源和专有模型。

Abstract: With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.

</details>


### [83] [Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning](https://arxiv.org/abs/2602.05809)
*Enwei Tong,Yuanchao Bai,Yao Zhu,Junjun Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: FSR为一种训练免裁剪的视觉token选择框架，仿真人类“聚焦-扫描-精炼”策略，通过同时考虑视觉显著性与指令相关性并引入互补性选择与相似度聚合，在不增加token预算下改善VLM在强压缩下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有训练免剪枝方法在极端压缩下，难以平衡局部证据（query相关关键区域）与全局上下文，导致性能下降。人类在视觉问答时会先关注关键证据，再在需要时全局扫描并整合细节，受此启发设计FSR以提高压缩下的效率与准确性。

Method: FSR包含三步：1) Focus：结合视觉显著性和指令相关性选取关键证据，避免只关注视觉显著但与查询无关区域；2) Scan：在已聚焦的证据条件下，选择与焦点最不相似的tokens作为补充全局上下文；3) Refine：通过基于相似度的分配与分数加权融合，将附近的有信息tokens聚合到扫描锚点中以细化上下文，同时不增加tokens预算。该方法为训练免裁剪、即插即用。

Result: 在多种VLM骨干与视觉语言基准上进行的大量实验表明，FSR在准确率与效率的权衡上持续优于现有最先进的裁剪方法。作者还公开了源码以利复现与推广。

Conclusion: 该工作提出了一个灵感来源于人类视觉问答流程的训练免裁剪框架FSR，通过聚焦、扫描与精炼三阶段选择视觉tokens，针对视觉重要性与指令相关性同时考虑，解决了以往方法在极度压缩下难以兼顾局部证据与全局上下文的问题。实验表明FSR在多个VLM骨干与基准上，在准确率-效率权衡上优于现有最优方法。

Abstract: Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR

</details>


### [84] [NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects](https://arxiv.org/abs/2602.05822)
*Musawar Ali,Manuel Carranza-García,Nicola Fioraio,Samuele Salti,Luigi Di Stefano*

Main category: cs.CV

TL;DR: NVS-HO是首个面向真实手持物体且仅用RGB输入的新视角合成基准：每个物体包含手持和ChArUco板两类序列，利用板序列提供评估真值；使用SfM与VGGT估计位姿，基于NeRF和Gaussian Splatting训练基线，结果显示现有方法在手持场景下性能显著不足。


<details>
  <summary>Details</summary>
Motivation: 填补真实环境中仅使用RGB对手持物体进行新视角合成的基准空白，并评估当前方法在无约束手持条件下的鲁棒性和性能差距。

Method: 构建包含每个物体两类RGB序列的数据集：手持序列（物体在静止相机前被操控）和板序列（物体固定在ChArUco板上以通过标记检测获得准确相机位姿）。采用SfM和预训练的VGGT作为位姿估计器，并基于NeRF和Gaussian Splatting训练NVS模型进行基线对比，使用板序列图像作为评估的真实目标。

Result: 实验表明，在无约束手持条件下，现有的位姿估计和NVS方法（包括基于NeRF和Gaussian Splatting的方法）存在明显性能差距，表明需要更鲁棒的算法。NVS-HO提供了具有挑战性的真实场景基准以促进该领域发展。

Conclusion: 该论文提出了NVS-HO基准，用于仅用RGB输入在真实环境中对手持物体进行新视角合成，强调了在手持拍摄条件下现有方法的性能不足，并提供了基线评估结果，推动更鲁棒方法的发展。

Abstract: We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.

</details>


### [85] [Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation](https://arxiv.org/abs/2602.05827)
*Hai Zhang,Siqi Liang,Li Chen,Yuxian Li,Yukuan Xu,Yichao Zhong,Fu Zhang,Hongyang Li*

Main category: cs.CV

TL;DR: 本文首提将视频生成模型用于超视距导航，设计SparseVideoNav以生成20s稀疏未来并实现亚秒决策，带来更高成功率与显著加速，且能在夜间场景零样本部署。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航依赖详细逐步指令，但现实导航需求是用高层意图在未知环境中定位远处目标；LLM方法短视且难以扩展监督时序，视频生成模型天生适合长时序对齐，因此可解决BVN问题。

Method: 提出利用视频生成模型的长时序对齐能力，生成20秒视域外稀疏未来表示，结合稀疏轨迹推断策略实现27倍推理加速；在训练上避免直接延长LLM监督时序以稳定训练，并在推理阶段用生成的视频指导导航决策。

Result: SparseVideoNav在大量真实世界零样本实验中，比最先进LLM基线在BVN任务上成功率提升2.5倍，并首次在夜间场景实现该能力；推理速度较未优化方法快27倍，能在可部署延迟内工作。

Conclusion: 本文提出将视频生成模型引入“视域之外导航”（BVN）任务，通过生成稀疏未来视频来提供长时序监督，并在此基础上设计SparseVideoNav实现亚秒轨迹推断，显著提升零样本真实环境下的成功率。

Abstract: Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.

</details>


### [86] [Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning](https://arxiv.org/abs/2602.05829)
*Yudi Shi,Shangzhe Di,Qirui Chen,Qinian Wang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie*

Main category: cs.CV

TL;DR: Weaver：一个可动态调用工具并结合强化学习的端到端多模态视频推理agent，在长视频推理上显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本链式思维方法在视频推理中存在表征不匹配和感知能力受限的问题，需要更好的多模态推理方法。

Method: 提出一个端到端可训练的多模态推理agent系统，策略模型可动态调用多种工具，并结合强化学习在无轨迹数据上探索工具组合策略。

Result: 在多项复杂视频推理基准上取得性能提升，特别是长视频场景。

Conclusion: Weaver有效提升了复杂视频推理性能，尤其在长视频上表现优越。

Abstract: Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.

</details>


### [87] [UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents](https://arxiv.org/abs/2602.05832)
*Han Xiao,Guozhi Wang,Hao Wang,Shilong Liu,Yuxiang Chai,Yue Pan,Yufeng Zhou,Xiaoxin Chen,Yafei Wen,Hongsheng Li*

Main category: cs.CV

TL;DR: UI-Mem通过分层参数化经验记忆与分层组采样等机制，实现有效的跨任务经验迁移与在线自我进化，从而在GUI在线RL任务上取得更高效与更具泛化性的表现。


<details>
  <summary>Details</summary>
Motivation: 在线RL在GUI领域受困于长时序任务的稀疏/错误回报导致的信用分配困难，以及缺乏经验复用导致的跨任务重复错误，亟需一种能结构化积累并迁移经验的机制。

Method: 提出Hierarchical Experience Memory以参数化模板形式存储工作流、子任务技能与失败模式；设计Stratified Group Sampling在每个rollout组内按不同指导强度注入记忆引导；引入Self-Evolving Loop自动抽象新策略与错误并更新记忆。

Result: 在多项在线GUI基准上，UI-Mem显著优于传统RL基线和静态复用策略，并对未见应用展示出强泛化能力。

Conclusion: UI-Mem通过分层经验记忆、分层组采样和自我进化循环，显著改善了GUI在线强化学习在长时序任务中的回溯赋责和跨任务迁移问题，最终提升了学习效率与泛化能力。

Abstract: Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io

</details>


### [88] [Self-Supervised Learning with a Multi-Task Latent Space Objective](https://arxiv.org/abs/2602.05845)
*Pierre-François De Plaen,Abhishek Jha,Luc Van Gool,Tinne Tuytelaars,Marc Proesmans*

Main category: cs.CV

TL;DR: 将多裁剪视为多任务对齐问题，为每种视图类型（全局/局部/遮挡）分配独立预测器，从而稳定并提升Siamese预测器型自监督学习的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决在使用多裁剪增强时，预测器基于共享参数在不同视图之间引起的训练不稳定和性能下降问题；通过识别每种视图的对齐任务差异，提出为不同视图类型使用独立预测器的策略以稳定训练并提升表示质量。

Method: 作者的方法包括：对多裁剪策略中的不同视图类型（全局、大裁剪、小局部裁剪、以及cutout遮挡裁剪）分别分配独立的预测器，形成多任务不对称Siamese结构；训练过程中将每种视图作为单独的对齐任务，并在编码器之后使用对应预测器进行预测与对齐，从而避免共享预测器导致的冲突与不稳定；在多种骨干（ResNet、ViT）和数据增强设置上进行了实验验证。

Result: 结果显示：为不同视图类型使用独立预测器能够稳定多裁剪训练并带来显著性能改进；将cutout视图加入并采用多任务对齐进一步提升了性能；方法在ResNet和ViT上均取得一致的ImageNet表现提升，且对骨干网络通用。

Conclusion: 论文结论是：在基于Siamese网络的自监督学习中，为不同视图类型使用独立的预测器（而不是共享预测器）可以稳定多裁剪（multi-crop）策略下的训练，并显著提升性能；进一步将不同的空间变换视为各自的对齐任务，加入cutout（部分遮挡）视图，构成一个统一的多任务不对称Siamese SSL框架，适用于不同骨干网络并在ImageNet上对ResNet和ViT均有一致提升。

Abstract: Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.

</details>


### [89] [Pathwise Test-Time Correction for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.05871)
*Xunzhi Xiang,Zixuan Duan,Guiyu Zhang,Haiyu Zhang,Zhe Gao,Junta Wu,Shaofeng Zhang,Tengfei Wang,Qi Fan,Chunchao Guo*

Main category: cs.CV

TL;DR: 提出无训练的Test-Time Correction，利用初始帧锚定并在采样过程中校准中间随机状态，有效抑制长序列生成漂移，兼顾实时性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 蒸馏的自回归扩散模型在实时短视频生成方面表现良好，但在长序列生成中出现严重误差累积。现有的测试时优化方法对短序列有效，但在长序列上因奖励函数不稳定和蒸馏参数超敏感而失效，因此需要一种无训练且稳定的校正方法。

Method: 在推理过程中利用初始帧作为参考，通过对中间随机状态进行校准（可能包括对噪声或特征的调整）来纠正漂移，无需额外训练或优化步骤，可与已有蒸馏自回归扩散模型无缝集成并保持实时性。

Result: TTC在各种蒸馏模型上均有效，能在几乎没有额外开销的情况下延长生成长度，并在30秒基准测试上达到与需大量资源的训练方法相当的质量。

Conclusion: 提出了TTC（Test-Time Correction），一种无训练的校正方法，通过使用初始帧作为稳定参考锚点，校准采样轨迹中的中间随机状态，来减缓长序列生成时的误差积累。

Abstract: Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.

</details>


### [90] [Contour Refinement using Discrete Diffusion in Low Data Regime](https://arxiv.org/abs/2602.05880)
*Fei Yu Guan,Ian Keefe,Sophie Wilkinson,Daniel D. B. Perrakis,Steven Waslander*

Main category: cs.CV

TL;DR: 提出一种面向低数据、低资源场景的轻量级离散扩散轮廓精化方法，通过条件CNN+简化扩散迭代去噪，从稀疏轮廓恢复出高质量边界，KVASIR上优于SOTA且推理更快。


<details>
  <summary>Details</summary>
Motivation: 不规则和半透明目标的边界检测在医疗、环境监测和制造业中很重要，但这些应用常常面临标注不足和现场计算资源受限。现有分割研究侧重掩码对齐，边界检测尤其是在低数据 regime 下研究不足，因此需要一种轻量且在低样本下鲁棒的边界检测方法。

Method: 核心为一个带自注意力层的卷积神经网络，条件输入为分割掩码；采用简化的扩散过程，迭代去噪稀疏轮廓表示，最终通过最小后处理生成密集孤立轮廓。模型架构和扩散步骤均针对低样本量进行了定制以提高效率和性能。

Result: 在训练样本少于500张的设置中，方法在KVASIR数据集上超过多个SOTA基线，在HAM10K与自建的Smoke数据集上表现有竞争力；推理速度提升约3.5倍，且能在稀疏监督下生成密集、孤立的轮廓。

Conclusion: 本文提出了一个轻量级的离散扩散轮廓精化管线，在低数据场景下能够稳健地检测不规则和半透明物体的边界。该方法在医疗影像KVASIR数据集上优于多种SOTA基线，在HAM10K与定制的森林火灾烟雾数据集上具有竞争力，同时推理帧率提升约3.5倍。

Abstract: Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.

</details>


### [91] [EoCD: Encoder only Remote Sensing Change Detection](https://arxiv.org/abs/2602.05882)
*Mubashir Noman,Mustansar Fiaz,Hiyam Debary,Abdul Hannan,Shah Nawaz,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: EoCD用早期融合+无参数多尺度融合替代Siamese编码器和复杂解码器，显著降低复杂度并在四个数据集上实现了性能与速度的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法普遍采用Siamese编码器+复杂解码器的架构，导致参数量大、计算开销高；而少数早期融合方法虽然减少了编码器开销，但仍依赖复杂解码器且性能不及晚期融合方法。为此提出更轻量且性能兼顾的方案。

Method: 对时间序列图像在编码器输入阶段进行早期融合，使单一编码器负责特征提取；去掉传统的任务专用解码器，使用一个无参数的多尺度特征融合模块进行特征融合与预测；在多种编码器架构上验证方法的通用性与效率。

Result: 在四个具有挑战性的变化检测数据集上进行广泛实验，结果显示EoCD在性能与预测速度之间取得了最佳平衡；证明模型性能主要取决于编码器，解码器可被简化为无参融合模块，从而降低整体复杂度同时保持竞争性表现。

Conclusion: 本论文提出了一种名为EoCD（encoder only change detection）的简洁高效变化检测方法，主张以早期融合替代传统的Siamese编码器并去除复杂解码器，用参数无关的多尺度特征融合模块替代，从而在保持或提升性能的同时显著降低模型复杂度与计算开销。

Abstract: Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.

</details>


### [92] [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884)
*Gino E. Jansen,Carolina Brás,R. Nils Planken,Mark J. Schuuring,Berto J. Bouma,Ivana Išgum*

Main category: cs.CV

TL;DR: 基于神经隐函数的方法能从模拟的少量二维超声切面分割中重建准确的三维心脏结构，显著优于Simpson双平面法的腔体体积估计。


<details>
  <summary>Details</summary>
Motivation: 2D TTE是临床常用但信息稀疏的成像手段，常用的Simpson双平面法在房室腔体积估计上存在较大误差。通过从有限的二维分割推断完整三维形状，可以提高腔体定量的准确性，进而改善临床评估与决策。

Method: 训练阶段用多层感知机（MLP）从CTA的三维分割中学习形状先验；测试阶段对模拟的TTE视图（四个顶点切面）对应的CTA切片分割，通过联合优化潜在编码和将观测平面映射到三维空间的刚性变换，重建多类别三维体积。采用神经隐式表示实现体素级重建并以Dice和体积误差评价。

Result: 在独立CTA分割测试集上，方法在所有结构上平均Dice为0.86±0.04；相较Simpson双平面法，左心室体积误差显著降低（4.88±4.26 mL vs. 8.14±6.04 mL），左心房体积误差大幅降低（6.40±7.37 mL vs. 37.76±22.96 mL），表明方法能在模拟TTE视图下更准确地量化三维腔体。

Conclusion: 本文提出了一种基于神经隐函数（neural implicit function）的三维心脏重建方法，可从稀疏的CT血管造影（CTA）平面分割重建完整的心脏室腔及左心室心肌，旨在应用于二维经胸超声（TTE）场景。

Abstract: Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\pm$ 4.26 mL vs. 8.14 $\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\pm$ 7.37 mL vs. 37.76 $\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.

</details>


### [93] [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/abs/2602.05909)
*Kangjie Zhang,Wenxuan Huang,Xin Zhou,Boxiang Zhou,Dejia Song,Yuan Xie,Baochang Zhang,Lizhuang Ma,Nemo Chen,Xu Tang,Yao Hu,Shaohui Lin*

Main category: cs.CV

TL;DR: 提出CLIP-Map：用可学习映射矩阵+Kronecker分解压缩CLIP，并用对角继承初始化缓解分布偏移，在高压缩比下比选择性继承方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于选择的权重继承方法（通过掩码优化或重要性度量选取子集）在极端压缩下严重损害特征表示能力，因而需要一种能在高压缩比下更好保留原始信息的压缩方式。

Method: CLIP-Map采用Full-Mapping策略利用可学习矩阵对预训练权重进行变换与组合，并引入Kronecker因子化以降低参数与计算开销。为缓解映射学习引入的分布偏移与优化难题，提出Diagonal Inheritance Initialization（对角继承初始化），以保留初始权重的分布特性，促进快速有效的收敛。

Result: 大量实验表明，CLIP-Map在多种压缩比下均优于选择类压缩框架，尤其在高压缩比场景中取得显著性能提升，验证了映射式压缩在保留信息与维持性能上的优势。

Conclusion: 本文提出了CLIP-Map，一种基于映射的CLIP压缩框架，通过可学习矩阵对预训练权重进行全量映射并结合Kronecker分解，旨在在高压缩比下尽量保留原始权重信息，从而改进特征表达能力。

Abstract: Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.

</details>


### [94] [Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2602.05937)
*Lingrui Li,Yanfeng Zhou,Nan Pu,Xin Chen,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出MGIPT，通过AIP与MGP的多尺度全局-实例提示调优，有效解决CTTA中误差累积与遗忘问题，显著提升医学图像分割的跨域适应性能。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法多更新模型参数导致误差累积与灾难性遗忘，提示调优虽有缓解但缺乏多尺度多样性、实例知识和隐私保护机制。

Method: 设计AIP以动态学习轻量的实例专属提示并通过自适应尺度选择缓解误差累积；设计MGP以多尺度捕获域级知识并在加权集成中与AIP互补，实现全局与局部信息的联合适配。

Result: 在多中心医学图像分割基准上，MGIPT优于现有最先进方法，在长期连续变化目标域中表现更稳健。

Conclusion: MGIPT通过结合自适应实例级提示（AIP）和多尺度全局提示（MGP），在CTTA任务上实现了更稳健的跨域适应，减轻误差累积和遗忘，提升了多中心医学图像分割的性能。

Abstract: Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.

</details>


### [95] [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951)
*Junwan Kim,Jiho Park,Seonghu Jeon,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出在条件流匹配中学习与条件相关的源分布，通过方差正则化和方向对齐解决不稳定性，并分析目标表征影响。实验显示可稳健提升文图生成，FID收敛可加速至3倍。


<details>
  <summary>Details</summary>
Motivation: 同时利用流匹配允许任意源分布的灵活性，探索是否通过设计/学习更合适的条件依赖源分布以提升文图生成效果，而非沿用扩散模型中的高斯先验。

Method: 在流匹配目标下，提出学习条件依赖的源分布；识别并修正直接结合条件信息时的失败模式（分布塌陷、不稳定），通过引入方差正则化和源-目标方向对齐策略稳定训练；分析目标表示空间对结构化源的影响；在多基准上进行广泛实验验证。

Result: 在多个文图基准上取得一致性提升，训练更稳健，FID收敛速度最高达3倍加速，表明原则性地设计源分布在条件流匹配中有实用价值。

Conclusion: 学习条件依赖的源分布在流匹配任务中可行且有益。通过方差正则化和源-目标方向对齐可避免分布崩溃和不稳定性。针对目标表示空间的分析表明在特定表征下结构化源更有效。实验表明在多个文图基准上带来一致改进，训练收敛速度和FID显著提升。

Abstract: Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.

</details>


### [96] [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966)
*Mirlan Karimov,Teodora Spasojevic,Markus Braun,Julian Wiederer,Vasileios Belagiannis,Marc Pollefeys*

Main category: cs.CV

TL;DR: 通过在动态物体局部区域对齐生成与真实视频的语义特征，LSA在微调后显著提升了视频生成的时序一致性，且无需推理时控制信号。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法在推理时依赖控制信号以保持动态物体的时序一致性，限制了其作为大规模、通用数据引擎的适用性。目标是无需推理控制信号也能提升时序一致性，使生成模型更易推广到自动驾驶场景的数据合成。

Method: 使用离线特征提取器（off-the-shelf）在以动态物体为中心的局部时空窗口上提取语义特征，构造语义特征一致性损失（semantic feature consistency loss），并与常规扩散模型损失联合进行微调。微调过程仅需一轮（一个epoch），不增加推理开销。为评估时序一致性，还引入了来自目标检测的mAP和mIoU作为补充指标。

Result: 在nuScenes和KITTI数据集上进行大量实验，结果显示LSA在常用视频生成评测指标上超越基线，并在引入的mAP和mIoU上也表现出时序一致性的提升。方法无需额外推理控制信号或推理开销。

Conclusion: 文章提出的Localized Semantic Alignment (LSA) 方法，通过对生成视频和真实视频在动态物体区域的语义特征进行对齐，从而提升时序一致性。对预训练视频生成模型进行微调，一个epoch即可在主流评测指标上超越基线，并在无需推理时控制信号的情况下改善生成稳定性。

Abstract: Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.

</details>


### [97] [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986)
*Mingxin Liu,Shuran Ma,Shibei Meng,Xiangyu Zhao,Zicheng Zhang,Shaofeng Zhang,Zhihang Zhong,Peixian Chen,Haoyu Cao,Xing Sun,Haodong Duan,Xue Yang*

Main category: cs.CV

TL;DR: 提出 RISE-Video：一个面向推理的 TI2V 基准与四维评价协议，并用 LMM 驱动的自动评估验证了11款模型在推理与世界模拟方面的系统性不足。


<details>
  <summary>Details</summary>
Motivation: 当前 TI2V 研究多集中于视觉保真度，而忽视模型是否能内化并推理隐含世界规则。需要一个专门测评推理能力的基准来推动模型在认知层面的进步。

Method: 构建一个以推理为导向的基准数据集（RISE-Video），包含467条人工注释样本，覆盖八类推理任务；设计四维评估指标（Reasoning Alignment、Temporal Consistency、Physical Rationality、Visual Quality）；并提出基于大型多模态模型的自动评估流水线以实现可扩展的评分。

Result: 在对11个最先进 TI2V 模型的广泛测试中，模型在复杂场景模拟、隐式约束遵守和动态因果关系推理上普遍表现薄弱，自动化评估与人工标注一致性较高，验证了评估流水线的可用性。

Conclusion: RISE-Video 揭示现有 TI2V 模型在隐含世界规则推理上的显著不足，尽管视觉质量较高，但在推理一致性、时序连贯性与物理合理性方面普遍表现欠佳，表明未来研究需更加聚焦于模型的世界建模与因果/物理常识能力。

Abstract: While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.

</details>


### [98] [VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation](https://arxiv.org/abs/2602.05998)
*Jie Deng,Kaichun Yao,Libo Zhang*

Main category: cs.CV

TL;DR: VisRefiner通过差异对齐监督和基于渲染差异的强化自我精炼，让模型学会从视觉差别推断代码修改，显著提升截图到代码的准确性与可迭代改进能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型直接从截图生成代码，但在训练时未观察生成代码的可视化结果；人类开发者通过渲染-对比-修正的迭代过程改进实现，启发提出让模型学习视觉差异与代码变化之间的联系。

Method: 使用差异对齐的监督信号，将渲染预测与参考设计之间的视觉差异与相应代码修改配对训练；进一步引入强化学习阶段进行自我精炼，让模型在观察渲染输出与目标设计、识别视觉差异后迭代地更新代码。

Result: 实验证明VisRefiner显著提升了单步生成质量和布局保真度，并使模型具备强大的自我精炼能力，从而在截图到代码任务上取得明显进步。

Conclusion: VisRefiner通过让模型学习渲染差异与代码编辑的对应关系，有效提升了从界面截图到前端代码的生成质量和布局一致性，并赋予模型自我迭代改进代码的能力。

Abstract: Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.

</details>


### [99] [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013)
*Ruihang Li,Leigang Qu,Jingxu Zhang,Dongnan Gui,Mengde Xu,Xiaosong Zhang,Han Hu,Wenjie Wang,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出GenArena，用两两比较替代点式评分，使VLM评估更稳定、更符合人类判断，显著提高评估准确率与与经典排行榜的相关性。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型快速发展，但传统基于点式的自动评估（使用VLMs）表现出随机性高且与人类偏好不一致，导致评估不可靠，因此需要更稳健且与人类一致的评估范式。

Method: 通过将评估范式从绝对点式打分切换为两两比较，并在多任务上使用VLMs作为裁判进行对比实验，展示两两比较在一致性与与人类对齐方面的优势；采用统计汇总方法将多次两两比较结果聚合为稳定评分，并与LMArena排行榜进行相关性对比。

Result: 采用两两比较范式后，开源模型的相对排名显著提升，评估准确率提高20%以上，与LMArena的Spearman相关性达0.86，远高于点式方法的0.36；并基于此对多个视觉生成模型在多任务上进行了基准测试。

Conclusion: 本文提出GenArena，一种基于两两比较的统一评估框架，用于解决当前视觉生成评估中点式评分的不稳定性与与人类感知对齐不足的问题。

Abstract: The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.

</details>


### [100] [MambaVF: State Space Model for Efficient Video Fusion](https://arxiv.org/abs/2602.06017)
*Zixiang Zhao,Yukun Cui,Lilun Deng,Haowen Bai,Haotong Qin,Tao Feng,Konrad Schindler*

Main category: cs.CV

TL;DR: MambaVF：用SSM替代光流+对齐，线性复杂度实现高效视频融合，性能与效率兼得。


<details>
  <summary>Details</summary>
Motivation: 现有视频融合方法依赖光流与特征扭曲，导致计算复杂度高、内存占用大且扩展性差。作者希望找到一种无需显式运动估计、效率更高且能捕捉长程依赖的方案。

Method: 将视频融合视为序列状态更新，使用线性复杂度的SSM捕捉长程时序依赖；设计轻量级SSM融合模块，采用时空双向扫描替代传统光流引导的对齐，实现跨帧信息高效聚合；通过端到端训练并在多任务上验证。

Result: 在多曝光、多焦、多谱（红外-可见）及医学视频融合基准上达到或优于现有最优方法；在参数量上最多减少92.25%，FLOPs减少88.79%，推理速度提升约2.1倍。

Conclusion: MambaVF提出了基于状态空间模型(SSM)的高效视频融合框架，通过将视频融合重写为序列状态更新过程，避免显式光流估计与特征对齐，从而在保持或提升融合质量的同时显著降低计算与参数开销。

Abstract: Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io

</details>


### [101] [Context Forcing: Consistent Autoregressive Video Generation with Long Context](https://arxiv.org/abs/2602.06028)
*Shuo Chen,Cong Wei,Sun Sun,Ping Nie,Kai Zhou,Ge Zhang,Ming-Hsuan Yang,Wenhu Chen*

Main category: cs.CV

TL;DR: 通过用长上下文教师并用Slow-Fast Memory管理冗余上下文，Context Forcing消除了学生-教师监督不匹配，显著提升了长视频生成的可扩展上下文长度与长期一致性。


<details>
  <summary>Details</summary>
Motivation: 现有实时长视频生成采用短上下文教师监督长上下文学生，导致教师无法建模全局时序依赖，限制了学生的上下文长度与长期一致性。

Method: 在训练框架中用长上下文教师替换短上下文教师，并引入Slow-Fast Memory上下文管理，将线性增长的上下文压缩为包含慢速（低帧率/摘要）与快速（近期高精度）记忆的混合结构以减少冗余，辅助进行极长时长（如2分钟）训练。

Result: 在多项长视频评估指标上，Context Forcing把有效上下文长度扩展到20秒以上（比LongLive和Infinite-RoPE提升2~10倍），在长期一致性和质量上均优于SOTA。

Conclusion: 提出的Context Forcing有效解决了学生-教师不匹配问题，使教师能访问完整生成历史，从而提升长期一致性并扩大上下文长度。

Abstract: Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.

</details>


### [102] [Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation](https://arxiv.org/abs/2602.06032)
*David Shavin,Sagie Benaim*

Main category: cs.CV

TL;DR: 提出Splat and Distill：用前馈3D高斯提升并将特征splatted到新视角，作为对学生模型的蒸馏监督，以高效注入3D几何感知到2D视觉基础模型，显著提升深度、法线、多视图对应和语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 尽管现有VFM在2D任务上表现优异，但缺乏对3D几何的内在感知，限制了它们在需要深度或几何理解的下游任务上的表现。以往尝试通过每场景优化构建3D表示但效率低且导致特征平均化伪影，因此需要一种快速、可扩展且能保留细节的方式将3D几何知识注入2D模型。

Method: 给定教师模型输出的2D特征，方法使用一个前馈模块将这些特征提升为显式的3D高斯体素表示，然后通过将3D特征从原视点“splatted”到新视点来生成新的2D特征图作为监督信号；学生模型在这些几何约束的2D特征上被蒸馏学习。重要设计包括：显式3D高斯表示、前馈而非每场景优化的提升（避免平均化），以及基于新视角投影的特征重采样用于蒸馏。

Result: 在单目深度估计、表面法线估计、多视图对应和语义分割等多项任务上，Splat and Distill显著优于先前方法，既提升了3D感知能力也增强了2D特征的语义富集。相较于基线，其在多个指标上取得显著增益，且由于前馈立体提升，训练/推理效率更高。

Conclusion: 本文提出的Splat and Distill通过将2D视觉基础模型（VFM）产生的特征提到显式3D高斯表示，并将其在新视角下“splatted”为2D特征图以监督学生模型，实现了对2D VFM的快速、前馈式3D感知增强。该方法替代了以往慢速的每场景优化，避免了特征平均伪影，并通过动态师生共同提升一致性来获得更好的几何与语义表征。实验证明在单目深度、法线估计、多视图对应和语义分割等任务上都有显著提升。

Abstract: Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/

</details>


### [103] [V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval](https://arxiv.org/abs/2602.06034)
*Dongyang Chen,Chaoyang Wang,Dezhao SU,Xi Xiao,Zeyu Zhang,Jing Xiong,Qing Li,Yuzhang Shang,Shichao Ka*

Main category: cs.CV

TL;DR: V-Retrver通过使MLLM在推理过程中主动获取并验证视觉证据，采用课程化训练策略，显著提升多模态检索的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM驱动的检索方法过度依赖静态视觉编码与语言推理，缺乏主动验证细粒度视觉证据的能力，导致在视觉歧义场景下出现臆断。

Method: 设计一个交替进行的多模态交织推理流程：模型在生成假设（链式思考）与目标化视觉验证之间交替，借助外部视觉工具选择性获取视觉证据；训练上采用课程学习，结合监督激活、基于拒绝的精炼与带有证据对齐目标的强化学习。

Result: 在多项多模态检索基准上，V-Retrver平均提升检索准确率23.0%，并在感知驱动的推理可靠性与泛化能力上表现出一致提升。

Conclusion: 本文提出V-Retrver，通过将多模态检索重构为以视觉证据为基础的代理式推理，有效减少基于语言的臆断，提高检索精度与推理可靠性。

Abstract: Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.

</details>


### [104] [InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions](https://arxiv.org/abs/2602.06035)
*Sirui Xu,Samuel Schulter,Morteza Ziyadi,Xialin He,Xiaohan Fei,Yu-Xiong Wang,Liangyan Gui*

Main category: cs.CV

TL;DR: InterPrior通过模仿蒸馏+物理扰动增强+强化学习微调，学习出能泛化的大规模全身交互运动先验，提升类人机器人在多场景下的稳健性与控制能力。


<details>
  <summary>Details</summary>
Motivation: 人类通常不以显式的全身动作规划来完成物体交互，而是依赖高层意图和内在的物理/运动先验生成协调的平衡、接触和操控行为。如何扩展这些先验以便类人机器人在多样场景中组合与泛化全身操控技能，是研究的核心动机。

Method: 先用全参考模仿专家蒸馏出一个可条件化的变分策略，该策略从多模态观测和高层意图重构动作；然后通过物理扰动的数据增强和强化学习微调，巩固并拓展潜在技能流形，使策略能够在未见目标和初始条件下泛化。

Result: 实验证明，经蒸馏、数据增强和强化学习微调的InterPrior能超越训练数据，在未见物体交互和不同初始化下表现出更强的泛化性；还展示了其在用户交互控制和真实机器人部署上的潜力。

Conclusion: 该论文提出了InterPrior，一种可扩展的统一生成式控制器框架，通过大规模模仿预训练和强化学习后训练，学习可通用的运动先验，从而在多样化的全身操控场景中实现物理一致的全身协调。

Abstract: Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.

</details>


### [105] [Thinking with Geometry: Active Geometry Integration for Spatial Reasoning](https://arxiv.org/abs/2602.06037)
*Haoyuan Li,Qihang Cao,Tao Tang,Kun Xiang,Zihan Guo,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: GeoThinker通过在关键层实施语义驱动的几何检索与选择性融合（frame-strict cross-attention + Importance Gating），从被动融合转向主动感知，显著提升MLLM的空间理解与下游表现，在VSI-Bench创下SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有将3D编码器几何先验被动并全局地混合到多模态大模型中，导致语义与几何失配和冗余信息，限制了空间推理能力。作者提出主动感知以按需检索几何证据以解决该问题。

Method: 在VLM的若干选定层引入Spatial-Grounded Fusion：利用视觉语义先验有选择地查询3D几何证据，并通过frame-strict cross-attention进行整合；同时引入Importance Gating对每帧注意力进行偏置，以突出任务相关结构。

Result: 在多项空间智能评估上，GeoThinker在VSI-Bench上达到72.6的峰值分数，优于现有方法；并在复杂下游场景（如具身指称、自动驾驶）中表现出更强的泛化和空间感知能力。

Conclusion: GeoThinker通过将几何信息从被动混合转为条件性检索与选择性融合，提升了MLLMs在空间推理任务中的表现，显著提高了空间感知能力并在VSI-Bench上达到新的SOTA。

Abstract: Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.

</details>


### [106] [SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs](https://arxiv.org/abs/2602.06040)
*Jintao Tong,Shilin Yan,Hongwei Xue,Xiaojun Tang,Kunyu Shi,Guannan Zhang,Ruixuan Li,Yixiong Zou*

Main category: cs.CV

TL;DR: 提出可切换三种推理模式的MLLM（SwimBird），用混合自回归和92K监督数据训练，使模型能按需在文本与视觉思维间切换，从而兼顾文本逻辑与视觉理解，取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多采用文本CoT，限制视觉密集任务表现；注入固定数量的连续视觉“思维”可提升视觉能力但会损失文本逻辑能力；核心问题是固定、预定义的推理模式无法根据不同查询选择合适的思维模态。

Method: 采用混合自回归表述同时统一文本下一个token预测与视觉下一个embedding预测；定义三种推理模式（纯文本、纯视觉、交错视觉-文本）；构建了包含三种模式的92K监督微调数据集SwimBird-SFT-92K，并设计了推理模式策划策略以训练模型学会根据输入自适应选择模式。

Result: SwimBird在多项文本推理与视觉理解基准上取得了SOTA结果，在视觉密集任务上显著优于固定模式方法，并在保持文本逻辑能力方面有稳健提升。

Conclusion: SwimBird提出了一个可切换推理模式的多模态大模型，通过在文本思维和视觉思维之间动态选择，解决了固定推理模式在视觉密集任务或文本逻辑任务上的权衡问题。

Abstract: Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as "visual thoughts" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.

</details>


### [107] [Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning](https://arxiv.org/abs/2602.06041)
*Xuejun Zhang,Aditi Tiwari,Zhenhailong Wang,Heng Ji*

Main category: cs.CV

TL;DR: 提出基于相机位姿的多视图框架CAMCUE，通过位姿注入、语言到位姿的对齐与想象视图合成，显著提升多图像空间推理与视角指定任务的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型难以在多视图下建立一致的三维场景理解，尤其无法从多视图观测推理出新的语言指定视角的场景。作者希望通过位姿显式建模解决跨视图融合及视角指定推理问题。

Method: 在每个视图的视觉tokens中注入对应位姿信息，将自然语言描述的目标视点与目标相机位姿对齐，并合成位姿条件的想象目标视图以辅助回答；同时构建并训练CAMCUE-DATA数据集用于评估。

Result: 在作者构建的数据集上，CAMCUE总体准确率提升9.06%；从自然语言视点描述直接预测目标位姿的旋转精度在20°内达90%以上，平移在0.5误差阈值内也表现良好；并将推理时间从256.6s降至1.45s每例。

Conclusion: CAMCUE通过显式利用相机位姿作为几何锚，实现了多视图跨视角融合与新视角推理，从而在多图像空间推理任务（尤其为视角转换）上有显著提升。

Abstract: Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [108] [Pruning Minimal Reasoning Graphs for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2602.04926)
*Ning Wang,Kuanyan Zhu,Daniel Yuehwoon Yee,Yitang Gao,Shiying Huang,Zirun Xu,Sainyam Galhotra*

Main category: cs.DB

TL;DR: AutoPrunedRetriever通过持久化与增量扩展最小推理子图、符号化存储与两层合并裁剪策略，实现高效RAG，显著提高复杂推理准确率并大幅降低token成本，适合长会话与多代理管线。


<details>
  <summary>Details</summary>
Motivation: 传统RAG对每个查询都从头检索长文本并重推理，造成tokens与延迟浪费；需要一种能在多轮会话与演进语料中重用先前推理成果的高效机制。

Method: 将实体与关系以ID索引的紧凑codebook存储，并把问题、事实与答案表示为边序列，实现基于符号结构的检索与提示。采用两层合并策略：快速ANN/KNN别名检测及在内存阈值触发的选择性k-means聚合，同时裁剪低价值结构；提示中只保留重叠代表与真正新证据。实现两种前端：基于REBEL的REBEL解析器与基于LLM的提取器。

Result: 在GraphRAG-Benchmark（Medical与Novel）上，两种变体在复杂推理准确率上优于HippoRAG2约9–11个百分点，并在上下文摘要与生成任务上保持竞争力；在更难的STEM与TV基准上同样位列第一，同时比图密集基线少用最多两个数量级的tokens。

Conclusion: 本论文提出AutoPrunedRetriever，一种基于图的RAG系统，通过持久化并增量扩展此前问题的最小推理子图来避免重复检索与推理，从而显著减少token、延迟和成本。

Abstract: Retrieval-augmented generation (RAG) is now standard for knowledge-intensive LLM tasks, but most systems still treat every query as fresh, repeatedly re-retrieving long passages and re-reasoning from scratch, inflating tokens, latency, and cost. We present AutoPrunedRetriever, a graph-style RAG system that persists the minimal reasoning subgraph built for earlier questions and incrementally extends it for later ones. AutoPrunedRetriever stores entities and relations in a compact, ID-indexed codebook and represents questions, facts, and answers as edge sequences, enabling retrieval and prompting over symbolic structure instead of raw text. To keep the graph compact, we apply a two-layer consolidation policy (fast ANN/KNN alias detection plus selective $k$-means once a memory threshold is reached) and prune low-value structure, while prompts retain only overlap representatives and genuinely new evidence. We instantiate two front ends: AutoPrunedRetriever-REBEL, which uses REBEL as a triplet parser, and AutoPrunedRetriever-llm, which swaps in an LLM extractor. On GraphRAG-Benchmark (Medical and Novel), both variants achieve state-of-the-art complex reasoning accuracy, improving over HippoRAG2 by roughly 9--11 points, and remain competitive on contextual summarize and generation. On our harder STEM and TV benchmarks, AutoPrunedRetriever again ranks first, while using up to two orders of magnitude fewer tokens than graph-heavy baselines, making it a practical substrate for long-running sessions, evolving corpora, and multi-agent pipelines.

</details>


### [109] [DistillER: Knowledge Distillation in Entity Resolution with Large Language Models](https://arxiv.org/abs/2602.05452)
*Alexandros Zeakis,George Papadakis,Dimitrios Skoutas,Manolis Koubarakis*

Main category: cs.DB

TL;DR: DistillER利用LLM作为教师生成噪声标签，采用数据选择与监督微调的知识蒸馏策略，使小模型在ER任务上既高效又具竞争性性能，弥补了现有方法在成本与标注需求上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的实体解析要么依赖超大模型导致高计算/成本，要么依赖监督标签不可行，存在时间效率与效果之间的缺口。作者希望通过KD在不需金标的情况下兼顾二者。

Method: DistillER包含三大维度：数据选择（选取信息性子集）、知识引导（比较单教师与多教师、LLM与SLM的组合）、蒸馏算法（评估监督微调与强化学习）。主要采用由LLM生成的噪声标签对Student进行监督微调，并探索解释生成能力。

Result: 实验表明：用LLM教师生成噪声标签后对Student进行监督微调，比其他KD策略表现更好，同时能生成高质量解释。在与已有监督/无监督ER方法（基于LLM和SLM）对比中，DistillER在效果和效率上均显著提高。

Conclusion: 本文提出DistillER，通过知识蒸馏在无金标的情况下将大模型（Teacher）的能力转移到小模型（Student），在ER任务上实现了效率与效果的兼顾。

Abstract: Recent advances in Entity Resolution (ER) have leveraged Large Language Models (LLMs), achieving strong performance but at the cost of substantial computational resources or high financial overhead. Existing LLM-based ER approaches operate either in unsupervised settings and rely on very large and costly models, or in supervised settings and require ground-truth annotations, leaving a critical gap between time efficiency and effectiveness. To make LLM-powered ER more practical, we investigate Knowledge Distillation (KD) as a means to transfer knowledge from large, effective models (Teachers) to smaller, more efficient models (Students) without requiring gold labels. We introduce DistillER, the first framework that systematically bridges this gap across three dimensions: (i) Data Selection, where we study strategies for identifying informative subsets of data; (ii) Knowledge Elicitation, where we compare single- and multi-teacher settings across LLMs and smaller language models (SLMs); and (iii) Distillation Algorithms, where we evaluate supervised fine-tuning and reinforcement learning approaches. Our experiments reveal that supervised fine-tuning of Students on noisy labels generated by LLM Teachers consistently outperforms alternative KD strategies, while also enabling high-quality explanation generation. Finally, we benchmark DistillER against established supervised and unsupervised ER methods based on LLMs and SLMs, demonstrating significant improvements in both effectiveness and efficiency.

</details>


### [110] [Repairing Property Graphs under PG-Constraints](https://arxiv.org/abs/2602.05503)
*Christopher Spinrath,Angela Bonifati,Rachid Echahed*

Main category: cs.DB

TL;DR: 本文针对含递归的PG-Constraints子集，提出基于拓扑变化的Property Graph修复流程与三种算法（ILP、朴素、LP引导贪心）；实验显示标签删除能显著减少删除量，LP引导贪心在效率上显著优于ILP且质量相当。


<details>
  <summary>Details</summary>
Motivation: 随着图数据库标准（GQL、SQL/PGQ）和PG-Constraints出现，存在如何在这些约束下修复Property Graphs的需求，特别需要支持递归约束同时保持可分析性和可实现的修复策略。

Method: 首先定义了一类包含否定约束并支持递归的PG-Constraints子集，允许基于自动机的结构性错误分析；然后设计修复管道，通过拓扑修改进行修复，允许节点、边和可选的标签删除；提出三种算法实现：ILP精确求解、朴素贪心、以及利用线性规划松弛指导的贪心算法；最后在真实数据集上进行实验比较。

Result: 在多数据集实验中：允许标签删除的策略相比仅节点/边删除能减少约59%的删除项；LP引导贪心算法在保持相同修复质量的情况下，比ILP在运行时上最多快97%。

Conclusion: 本文提出了在PG-Constraints下对Property Graphs进行修复的完整流程，并提出了三种算法策略（ILP、朴素、LP引导贪心）来执行基于拓扑变化的修复，实验表明标签删除能显著减少节点/边删除，LP引导贪心在运行时上比ILP快且质量相当。

Abstract: Recent standardization efforts for graph databases lead to standard query languages like GQL and SQL/PGQ, and constraint languages like Property Graph Constraints (PG-Constraints). In this paper, we embark on the study of repairing property graphs under PG-Constraints. We identify a significant subset of PG-Constraints, encoding denial constraints and including recursion as a key feature, while still permitting automata-based structural analyses of errors. We present a comprehensive repair pipeline for these constraints to repair Property Graphs, involving changes in the graph topology and leading to node, edge and, optionally, label deletions. We investigate three algorithmic strategies for the repair procedure, based on Integer Linear Programming (ILP), a naive, and an LP-guided greedy algorithm. Our experiments on various real-world datasets reveal that repairing with label deletions can achieve a 59% reduction in deletions compared to node/edge deletions. Moreover, the LP-guided greedy algorithm offers a runtime advantage of up to 97% compared to the ILP strategy, while matching the same quality.

</details>


### [111] [Taking the Leap: Efficient and Reliable Fine-Grained NUMA Migration in User-space](https://arxiv.org/abs/2602.05540)
*Felix Schuhknecht,Nick Rassau*

Main category: cs.DB

TL;DR: 提出用户态page_leap()方法，高效异步迁移NUMA页面，解决自动平衡与move_pages()的缺陷，适用于并行数据库的内存本地化。


<details>
  <summary>Details</summary>
Motivation: 在多插槽（多socket）NUMA系统中，内存访问延迟存在显著非一致性。Linux自动NUMA平衡和move_pages()系统调用各有功能或性能上的不足，难以满足数据库并行查询对数据本地化的需求。需要更灵活高效的用户态迁移方案。

Method: 设计并实现page_leap()，利用虚拟内存子系统特性在用户态触发迁移；实现保证最终迁移所有页面、正确处理并发写入、支持内存池、基于工作负载自适应调整迁移粒度，并兼容小页和大页。

Result: page_leap()实现了高性能异步迁移，弥补了现有方案在特性和性能上的不足；支持所有页面最终迁移、并发写入正确性、内存池和自适应粒度，且兼容小页与大页。

Conclusion: 本文提出了一种名为page_leap()的用户态页面迁移方法，能够高效、异步地将内存页面从一个NUMA区域迁移到另一个区域，以改善并行查询的内存局部性。

Abstract: Modern multi-socket architectures offer a single virtual address space, but physically divide main-memory across multiple regions, where each region is attached to a CPU and its cores. While this simplifies the usage, developers must be aware of non-uniform memory access (NUMA), where an access by a thread running on a core-local NUMA region is significantly cheaper than an access from a core-remote region. Obviously, if query answering is parallelized across the cores of multiple regions, then the portion of the database on which the query is operating should be distributed across the same regions to ensure local accesses. As the present data placement might not fit this, migrating pages from one NUMA region to another can be performed to improve the situation. To do so, different options exist: One option is to rely on automatic NUMA balancing integrated in Linux, which is steered by the observed access patterns and frequency. Another option is to actively trigger migration via the system call move_pages(). Unfortunately, both variants have significant downsides in terms of their feature set and performance. As an alternative, we propose a new user-space migration method called page_leap() that can perform page migration asynchronously at a high performance by exploiting features of the virtual memory subsystem. The method is (a) actively triggered by the user, (b) ensures that all pages are eventually migrated, (c) handles concurrent writes correctly, (d) supports pooled memory, (e) adaptively adjusts its migration granularity based on the workload, and (f) supports both small pages and huge pages.

</details>


### [112] [One Size Does NOT Fit All: On the Importance of Physical Representations for Datalog Evaluation](https://arxiv.org/abs/2602.05651)
*Nick Rassau,Felix Schuhknecht*

Main category: cs.DB

TL;DR: 通过实验揭示Datalog物理表示与七类工作负载特征的关系，提出并实现基于决策树的自动选择器，以替换僵化的一刀切表示策略。


<details>
  <summary>Details</summary>
Motivation: 现有Datalog引擎通常采用单一物理表示，但Datalog程序的执行涉及插入、查找、包含检测等多种操作，这些操作对物理结构的需求随工作负载特征（如大小、重复性、元数、规则交错及递归特性）而显著变化，因此需要多样化的物理表示并自动选择。

Method: 对多种可能的物理表示和七个工作负载维度进行深入实验分析，提取影响性能的关键属性；基于实验结果，使用决策树构建自动选择策略。

Result: 识别出哪些工作负载属性对表示选择关键、展示不同表示在真实Datalog程序中的性能差异，并实现一个基于决策树的自动选择机制，能为不同工作负载推荐合适的物理表示。

Conclusion: 本文揭示了Datalog引擎在物理表示选择上的限制，提出基于工作负载特征的自动选择机制，并验证了不同表示对性能的影响。

Abstract: Datalog is an increasingly popular recursive query language that is declarative by design, meaning its programs must be translated by an engine into the actual physical execution plan. When generating this plan, a central decision is how to physically represent all involved relations, an aspect in which existing Datalog engines are surprisingly restrictive and often resort to one-size-fits-all solutions. The reason for this is that the typical execution plan of a Datalog program not only performs a single type of operation against the physical representations, but a mixture of operations, such as insertions, lookups, and containment-checks. Further, the relevance of each operation type highly depends on the workload characteristics, which range from familiar properties such as the size, multiplicity, and arity of the individual relations to very specific Datalog properties, such as the "interweaving" of rules when relations occur multiple times, and in particular the recursiveness of the query which might generate new tuples on the fly during evaluation. This indicates that a variety of physical representations, each with its own strengths and weaknesses, is required to meet the specific needs of different workload situations. To evaluate this, we conduct an in-depth experimental study of the interplay between potentially suitable physical representations and seven dimensions of workload characteristics that vary across actual Datalog programs, revealing which properties actually matter. Based on these insights, we design an automatic selection mechanism that utilizes a set of decision trees to identify suitable physical representations for a given workload.

</details>


### [113] [Fast Private Adaptive Query Answering for Large Data Domains](https://arxiv.org/abs/2602.05674)
*Miguel Fuentes,Brett Mullins,Yingtai Xiao,Daniel Kifer,Cameron Musco,Daniel Sheldon*

Main category: cs.DB

TL;DR: 通过多维数组化的残差查询、延迟更新与自适应隐私预算，将残差重构并入AIM，得到更快且可扩展的AIM+GReM，误差与性能均优。


<details>
  <summary>Details</summary>
Motivation: 现有在发布边缘分布(marginals)时，重构从噪声测量中恢复估计存在计算瓶颈；此前残差查询在批量查询重构中展现高效性，因此希望将其引入自适应机制以加速重构并降低误差。

Method: 引入多维数组的残差查询概念框架、延迟更新策略与每轮隐私预算的自适应优化；将残差基的快速重构替换AIM中的图模型重构，形成AIM+GReM机制。

Result: AIM+GReM比原始AIM在速度上快数量级，同时在误差方面具有竞争力并显著提升可扩展性；新技术简化了残差操作并减少了总体误差。

Conclusion: 本文提出将残差查询(residual queries)整合到自适应差分隐私机制中，从而在保持差分隐私的同时显著提升重构速度与可扩展性，并在误差上与最先进方法竞争。

Abstract: Privately releasing marginals of a tabular dataset is a foundational problem in differential privacy. However, state-of-the-art mechanisms suffer from a computational bottleneck when marginal estimates are reconstructed from noisy measurements. Recently, residual queries were introduced and shown to lead to highly efficient reconstruction in the batch query answering setting. We introduce new techniques to integrate residual queries into state-of-the-art adaptive mechanisms such as AIM. Our contributions include a novel conceptual framework for residual queries using multi-dimensional arrays, lazy updating strategies, and adaptive optimization of the per-round privacy budget allocation. Together these contributions reduce error, improve speed, and simplify residual query operations. We integrate these innovations into a new mechanism (AIM+GReM), which improves AIM by using fast residual-based reconstruction instead of a graphical model approach. Our mechanism is orders of magnitude faster than the original framework and demonstrates competitive error and greatly improved scalability.

</details>


### [114] [Cost-Efficient RAG for Entity Matching with LLMs: A Blocking-based Exploration](https://arxiv.org/abs/2602.05708)
*Chuangtao Ma,Zeyu Zhang,Arijit Khan,Sebastian Schelter,Paul Groth*

Main category: cs.DB

TL;DR: 提出一种基于阻塞的成本高效RAG架构（CE-RAG4EM），通过批量检索與生成在大规模实体匹配中显著降低计算与时间开销，同时保持或提升匹配效果，并分析了性能-开销权衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在知识密集型任务表现优异，但在大规模实体匹配场景中，会因检索与生成成本过高而难以扩展，迫切需要降低计算与时间开销的方案。

Method: 提出CE-RAG4EM架构，采用阻塞（blocking）策略将实体对分组，进行批量检索与批量生成以减少重复检索与生成开销；并给出统一评估框架，分析阻塞感知优化与检索粒度对性能与成本的影响。

Result: 大量实验表明CE-RAG4EM在保持或提升匹配质量的同时，显著减少端到端运行时间；并发现关键配置参数（如阻塞策略、检索与生成粒度）在性能与开销之间存在显著权衡。

Conclusion: CE-RAG4EM在实体匹配任务中通过基于阻塞的批量检索与批量生成显著降低了计算开销，同时在匹配质量上与强基线持平或更优。作者指出配置参数在性能与开销之间存在权衡，为构建高效可扩展的RAG系统提供了实用指导。

Abstract: Retrieval-augmented generation (RAG) enhances LLM reasoning in knowledge-intensive tasks, but existing RAG pipelines incur substantial retrieval and generation overhead when applied to large-scale entity matching. To address this limitation, we introduce CE-RAG4EM, a cost-efficient RAG architecture that reduces computation through blocking-based batch retrieval and generation. We also present a unified framework for analyzing and evaluating RAG systems for entity matching, focusing on blocking-aware optimizations and retrieval granularity. Extensive experiments suggest that CE-RAG4EM can achieve comparable or improved matching quality while substantially reducing end-to-end runtime relative to strong baselines. Our analysis further reveals that key configuration parameters introduce an inherent trade-off between performance and overhead, offering practical guidance for designing efficient and scalable RAG systems for entity matching and data integration.

</details>


### [115] [Even Faster Geosocial Reachability Queries](https://arxiv.org/abs/2602.05928)
*Rick van der Heijden,Nikolay Yakovets,Thekla Hamm*

Main category: cs.DB

TL;DR: 提出2DReach：用每个SCC的2D R树替代3DReach的区间标注+3D R树，构建更快、索引更小、查询更稳定。


<details>
  <summary>Details</summary>
Motivation: 简化并提升地理社交可达性查询的索引构建效率与查询表现，解决3DReach基于区间标注与3D R树的复杂性与开销问题。

Method: 将有向图压缩为SCC构成的DAG，针对每个SCC直接维护一个二维R树（或压缩后共享的R树），查询为单次R树检索；不使用区间标签，构建更快、查询更稳定。

Result: 在四个真实数据集上，2DReach比3DReach拥有更快的索引构建时间，压缩变体索引体积最小；查询性能相当或更好，响应时间更稳定。

Conclusion: 2DReach在避免区间标记的同时，通过为每个SCC存储可达空间顶点的二维R树实现了更简单高效的解决方案，压缩变体通过排除空间汇点与共享R树进一步降低了存储开销。

Abstract: Geosocial reachability queries (\textsc{RangeReach}) determine whether a given vertex in a geosocial network can reach any spatial vertex within a query region. The state-of-the-art 3DReach method answers such queries by encoding graph reachability through interval labelling and indexing spatial vertices in a 3D R-tree. We present 2DReach, a simpler approach that avoids interval labelling entirely. Like 3DReach, 2DReach collapses strongly connected components (SCCs) into a DAG, but instead of computing interval labels, it directly stores a 2D R-tree per component over all reachable spatial vertices. A query then reduces to a single 2D R-tree lookup. We further propose compressed variants that reduce storage by excluding spatial sinks and sharing R-trees between components with identical reachable sets. Experiments on four real-world datasets show that 2DReach achieves faster index construction than 3DReach, with the compressed variant yielding the smallest index size among all methods. 2DReach delivers competitive or superior query performance with more stable response times across varying query parameters.

</details>


### [116] ["Detective Work We Shouldn't Have to Do": Practitioner Challenges in Regulatory-Aligned Data Quality in Machine Learning Systems](https://arxiv.org/abs/2602.05944)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 在EU监管背景下，数据质量合规在实践中受工具、组织与文化限制，亟需合规化工具、明确责任与主动治理。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR和AI Act等法规扩展，数据质量在法律与技术层面要求交织，作者旨在理解从业者如何在实际工作中解读并执行这些与监管对齐的数据质量要求。

Method: 对欧盟境内在监管场景下从事机器学习系统的数据从业者进行半结构化访谈，并对访谈内容进行定性分析。

Result: 发现法律原则与工程流程之间存在持续差距，数据管道碎片化、现有工具能力有限、技术与法律团队责任边界不清晰，以及以审计为驱动的被动质量实践。受访者需要合规感知的工具、更清晰的治理结构及向主动数据治理的文化转变。

Conclusion: 监管要求与工程实践存在显著断层，导致数据质量合规落实困难。

Abstract: Ensuring data quality in machine learning (ML) systems has become increasingly complex as regulatory requirements expand. In the European Union (EU), frameworks such as the General Data Protection Regulation (GDPR) and the Artificial Intelligence Act (AI Act) articulate data quality requirements that closely parallel technical concerns in ML practice, while also extending to legal obligations related to accountability, risk management, and human rights protection. This paper presents a qualitative interview study with EU-based data practitioners working on ML systems in regulated contexts. Through semi-structured interviews, we investigate how practitioners interpret regulatory-aligned data quality, the challenges they encounter, and the supports they identify as necessary. Our findings reveal persistent gaps between legal principles and engineering workflows, fragmentation across data pipelines, limitations of existing tools, unclear responsibility boundaries between technical and legal teams, and a tendency toward reactive, audit-driven quality practices. We also identify practitioners' needs for compliance-aware tooling, clearer governance structures, and cultural shifts toward proactive data governance.

</details>
