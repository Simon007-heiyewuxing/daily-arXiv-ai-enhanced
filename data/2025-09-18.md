<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 通过检索近邻示例并用Dempster-Shafer融合其预测，论文提出基于证据的实例自适应阈值，比固定熵阈值更可解释并在CIFAR上减少自信错误，且只需少量证据。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性判定通常使用单一固定阈值（如预测熵）来决定是否复核，忽视了样本间的差异性和可解释需求。作者旨在通过显式检索支持证据来实现实例自适应的、可审计的不确定性判断。

Method: 对每个测试样本在嵌入空间检索近邻示例，使用Dempster-Shafer理论融合这些示例的预测分布，得到融合后的置信信念作为每个样本的阈值；与基于预测熵的固定阈值对比评估。

Result: 在CIFAR-10/100和BiT、ViT骨干网络上，所提方法在不确定性感知的性能上与或优于熵阈值方法，同时显著减少了自信但错误的决策，且只需少量证据即可达到这些优势，增加证据数量带来变化有限。

Conclusion: 该论文提出了一种基于证据检索的、不依赖单一全局阈值的自适应不确定性判定机制，能够提高决策透明性并在多个基准上减少自信错误。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 该研究比较了混合量子-经典神经网络与纯经典CNN在MNIST、CIFAR100、STL10三数据集上的表现，结果显示混合模型在准确率、训练速度、参数效率和资源使用方面普遍优于经典模型，但在复杂数据集上的对抗鲁棒性仍然脆弱。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算与经典深度学习相结合是否能够在实际视觉任务上带来性能、效率及鲁棒性上的优势。

Method: 将参数化量子电路嵌入到传统深度学习架构中，构建混合模型；与纯CNN作为基线比较。在每个数据集上训练50个epoch，评估验证/测试准确率、训练时间、资源消耗及对抗鲁棒性（ε=0.1）。

Result: 混合模型在验证准确率上分别为MNIST 99.38%、CIFAR100 41.69%、STL10 74.05%，均高于经典模型（98.21%、32.25%、63.76%）；训练速度快5–12×，参数减少6–32%，内存和CPU占用更低；对抗鲁棒性在简单数据集显著更好，在复杂数据集两者均接近1%。

Conclusion: 混合量子-经典架构在准确率、训练效率和参数规模上表现出显著优势，尤其在复杂视觉任务上增益更明显；但对抗鲁棒性在复杂数据集上并未改善，需进一步研究。

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 改进YOLOv11与DeepSort提高感知精度与跟踪稳定性，GRU-Attention增强时序预测，整体框架显著提升高速公路拥堵检测与提前预警性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在遮挡条件下车辆感知准确率低、拥堵预测丢失长序列依赖，需提高检测/跟踪鲁棒性并增强时序建模能力以实现精准提前预警。

Method: 在感知端，基于YOLOv11替换DIoU Loss得到YOLOv11-DIoU，并在DeepSort中融合马氏距离与余弦相似度以增强跟踪；在预测端，构建GRU-Attention模型，输入流量、密度、速度，捕捉拥堵前兆。

Result: YOLOv11-DIoU在测试视频上达95.7% mAP与5.3%遮挡漏检率；改进DeepSort达93.8% MOTA与4次ID切换；在高密度工况验证格林伯格模型速度与密度呈显著负相关（r=-0.97）；GRU-Attention在300轮训练后测试准确率99.7%，10分钟提前预警时间误差≤1分钟，独立视频验证预警准确率95%。

Conclusion: 该研究提出了一个集成化的“检测-预测”框架，通过改进目标检测与多目标跟踪算法以及构建带注意力机制的GRU预测模型，有效提升了高速公路拥堵感知与提前预警能力。

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 本文使用卷积神经网络自动分析车载超声传感器众包检测数据，从而替代人工地面真实测试，显著提高了路边停车服务的数据处理效率，人工时间最多节省99.58%。


<details>
  <summary>Details</summary>
Motivation: 提升基于众包车队数据的路边停车可用信息服务的质量和效率，减少人工地面真实标注（ground truth）测试过程中的人力成本与时间消耗。

Method: 构建并训练卷积神经网络进行图像/模式识别，用于对众包车载超声传感器生成的数据进行分类；将该模型集成到现有的基于云的实时停车服务中，对测试流程进行自动化替代。

Result: 通过自动化分析工具，论文报告了在测试流程中最高可达99.58%的人工时间减少，并给出若干预定义指标来评估性能。

Conclusion: 该论文通过将卷积神经网络（CNN）应用于车载超声传感器检测数据的分析，实现了对路侧停车空位检测结果的自动化标注和验证，从而显著减少了人工参与。

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 论文通过系统实验证明VLM凭借语义嵌入在零样本OOD检测上优于单模态方法，但对提示词高度敏感，提出了对机制与稳健性的新理解并给出设计建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽发现VLM在零样本OOD检测表现优越，但欠缺对其工作原理、相比单模态方法的具体优势及稳健性边界的全面理解，故需系统化实证分析以指导更可靠的设计。

Method: 通过在VLM嵌入空间对ID与OOD提示进行系统化实验，分析关键操作特性；与单模态方法比较性能并进行消融或对比实验；评估在图像噪声与提示词变化下的稳健性。

Result: 量化证明VLM在零样本OOD检测上优于单模态方法，归因于其语义新颖性捕捉能力；揭示了提示词措辞引起的显著性能波动以及对图像噪声的相对鲁棒性。

Conclusion: 该论文系统性地分析了视觉-语言模型（VLM）在零样本OOD检测中的机制、优势和敏感性，结论是VLM通过在联合嵌入空间利用丰富的语义信息实现了优越的OOD检测能力，但对提示词措辞高度敏感，尽管对图像噪声有较强鲁棒性。

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [6] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 论文引入一种基于截面曲率的离散度量空间几何轮廓，用于评估数据表示质量和估计内在维度，实验验证了其在网络和降维评估上的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的几何与拓扑工具对离散或非欧几里得数据表示的局部与大尺度几何特征刻画不足，需一种能在离散度量空间中捕捉三点间度量关系并用于评估表示保真度和内在维度的新工具。

Method: 定义并计算依赖三元组与其他点间距离关系的截面曲率，基于曲率分布构建几何轮廓并提出定量评估指标，用于比较原始数据与降维后表示的几何保真度；通过实验在真实网络和合成数据上验证方法，展示其估计内在维度和评估降维效果的能力。

Result: 实验显示曲率轮廓能区分不同内在维度的数据，能有效评估多种降维方法的几何保真度，并揭示经验网络的宏观几何结构。

Conclusion: 该论文提出了一种基于新型截面曲率概念的离散度量空间几何描述方法，并用此构建曲率轮廓用于评估数据表示和估计内在维度。

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [7] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 该文用Landsat-8、Google Earth Engine、k-means与CNN对斐济Nadi 2013–2024年土地覆盖/利用变化进行检测与可视化，结果显示显著城市扩张，方法具实用价值但需更严谨的评估与数据增强。


<details>
  <summary>Details</summary>
Motivation: 应对斐济快速城市化带来的土地利用/覆盖变化，提供技术支持以进行空间规划与政策制定，监测住房、公路和土木工程开发对环境与土地利用的影响。

Method: 使用Landsat-8影像作为基础数据，通过标注构建监督学习训练集；在Google Earth Engine上应用无监督k-means聚类生成土地覆盖图；使用卷积神经网络（CNN）对选定区域进行土地覆盖分类；并可视化2013至2024年的变化以突出城市扩张。

Result: 生成了基于Landsat-8的土地覆盖图与变化可视化，CNN分类与k-means聚类结合提供了2013–2024年间城市区域扩张的证据，证明该框架可用于长期监测。

Conclusion: 该研究展示了结合遥感与机器学习方法用于斐济Nadi地区2013–2024年土地利用/覆盖变化检测的可行性，并指出城市扩张明显，方法对监测和决策有实用价值，但需在数据、方法细化与评估上加强。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [8] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 三阶段：YOLOv7分割定位 + ConvNeXt+三元组损失嵌入 + 特征辅助IoU跟踪；支持混合精度边缘部署与增量嵌入更新，在真实视频与Jetson上表现良好。


<details>
  <summary>Details</summary>
Motivation: 动机是为电力输电线路场景提供一个在复杂环境中能实时检测并持久跟踪异物的系统，满足遮挡、动态变化与资源受限设备上的部署需求，并保持后续添加新目标时无需重训练模型的可扩展性。

Method: 方法包括：1）使用YOLOv7分割模型进行快速鲁棒的目标定位；2）采用ConvNeXt特征提取器并用三元组损失训练以生成判别性嵌入；3）结合特征的IoU跟踪器以增强遮挡与运动下的多目标跟踪鲁棒性。同时通过混合精度推理和数据库式增量更新支持低成本边缘部署与在线扩展。

Result: 在真实监控与无人机视频数据集上的大量实验表明该框架在多种FOI场景下具有较高准确率与鲁棒性；在NVIDIA Jetson设备上的硬件基准测试验证了其实时性与可扩展性，适合实际边缘应用。

Conclusion: 该工作提出了一个面向电力输电系统的实时异物入侵（FOI）检测与跟踪三阶段框架，能够在嵌入式边缘设备上实现高效、可扩展的部署。

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [9] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent通过把图像分解为对象、合成多样指令并结合VLM与开放词汇检测器、语义特征和人类偏好模型，提供更可靠、可扩展的多轮指令编辑评估；并基于此构建EdiVal-Bench评测多种编辑模型和发现失败模式。


<details>
  <summary>Details</summary>
Motivation: 解决基于指令的图像编辑评估缺乏可靠性和可解释性的问题，当前评估要么依赖成对参考图像导致覆盖有限并继承生成模型偏差，要么仅依赖零样本视觉—语言模型（VLM），其基于提示的评估不精确。

Method: EdiVal-Agent先将图像分解为语义对象并合成上下文感知的编辑指令；评估阶段结合VLM与开放词汇对象检测器评估指令遵循，用语义级特征提取器评估内容一致性，并用人类偏好模型判断视觉质量；构建包含9种指令类型和11个模型的EdiVal-Bench进行实验对比。

Result: 提出EdiVal-Agent：一个自动化、可扩展且细粒度的评估框架，从目标中心角度进行多轮指令编辑评估，结合VLM、开放词汇目标检测器、语义特征提取器和人类偏好模型，构建EdiVal-Bench基准，覆盖9类指令和11个先进编辑模型，并证明结合目标检测器的VLM在指令遵循评估上比单独VLM和CLIP更符合人工判断。

Conclusion: 结合VLM与对象检测器能显著提升指令遵循评估与人工判断的一致性，模块化管道便于未来工具接入，从而逐步提升评估准确性；EdiVal-Bench揭示现有编辑模型的缺陷并能指导未来改进。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [10] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything是一个统一的Transformer前馈模型，通过分解的多视图几何表示和跨数据集统一监督，在单次前向中完成多种3D重建与定位任务，性能与专用模型相当并支持高效联合训练。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉任务被多种专用方法分散（如SfM、MVS、单目深度估计、相机定位等）；期望一种通用的、高效的前馈模型，能统一处理多种输入形式与任务，并在精度与效率上匹配或超越专用模型，从而简化训练/部署并促进共享表示学习。

Method: 提出MapAnything：一个Transformer编码-解码前馈网络，接收多图像与可选相机内参/位姿/深度/部分重建作为输入，输出一组分解的多视图几何要素（深度图、局部射线图、相机姿态、尺度因子）；采用统一监督格式跨任务训练，并通过输入增强实现任务适配，模型训练和推理为单次前向传播。

Result: 大量消融与实验表明MapAnything在多个任务上优于或不逊于各类专用前馈模型，并具备更高效的联合训练行为，验证其作为通用3D重建骨干的可行性。

Conclusion: MapAnything提出了一种统一的基于Transformer的前馈模型，能同时输入多张图像和可选几何信息并直接回归度量级的3D场景几何与相机参数，其分解表示（深度图、局部光线图、相机位姿和尺度因子）将局部重建升级为全局一致的度量框架；通过跨数据集的标准化监督与灵活的数据增强，模型能在一次前馈中处理多种3D任务并与专用模型性能相当或更好，从而推进通用3D重建骨干的发展。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [11] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: 提出SCM-PR：将语义信息与几何信息在跨模态（RGB→LiDAR）匹配中融合，通过VMamba、SAFF、语义化LiDAR描述子、NetVLAD语义注意力及对比学习中的语义一致性损失等组件，显著提升KITTI系列数据集上的跨模态定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于RGB的VPR对光照、天气和季节变化敏感；现有跨模态方法在复杂场景、细粒度或高分辨率匹配及视角变化时表现不足。通过引入高层语义信息并将其与几何信息融合，期望提高跨模态定位的鲁棒性和匹配精度。

Method: 方法包括：1) 使用VMamba作为RGB特征提取主干；2) 设计Semantic-Aware Feature Fusion（SAFF）模块，将场所描述子与语义分割掩码融合；3) 为LiDAR构建同时包含语义与几何信息的描述子；4) 在NetVLAD中引入跨模态语义注意力以增强匹配；5) 在对比学习框架下提出Multi-View Semantic-Geometric Matching与Semantic Consistency Loss以利用多视角语义-几何约束。

Result: 在KITTI与KITTI-360数据集上的实验表明，SCM-PR在检索精度和鲁棒性上优于现有跨模态VPR方法，尤其在复杂场景与视角变化下提升显著。

Conclusion: 本文提出的SCM-PR框架通过在跨模态（RGB图像与LiDAR）匹配中引入语义信息，有效提升了在光照、天气、季节变化以及视角变动下的定位鲁棒性。实验（KITTI与KITTI-360）表明方法在跨模态地图定位任务上优于现有方法，达到或接近当前最先进水平。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [12] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 用场景自适应格子矢量量化替代统一标量量化，可在保持低复杂度和兼容性的前提下，显著提升3DGS数据压缩的R-D性能，并支持单模型多码率。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS生成的数据量巨大，需要高效压缩。现有方法主要采用简单的USQ，存在R-D性能提升空间。研究者希望在保持系统简洁和低开销的前提下，通过更复杂的量化器提升压缩性能。

Method: 在已有锚点神经压缩框架中，用格子矢量量化（LVQ）替换USQ，并通过针对每个场景优化格子基底实现场景自适应（SALVQ）。另外通过缩放格子基向量来动态调整格子密度，从而实现单模型多码率。该方法训练开销和系统改动很小，可无缝集成入现有管线。

Result: 实验证明，将USQ替换为经场景优化的LVQ能显著改善R-D性能；通过缩放基向量可用单模型覆盖多个比特率，减少训练时间和内存需求。SALVQ兼具矢量量化的效率和USQ的低复杂度，且能无缝集成进现有3DGS压缩架构。

Conclusion: 该论文提出将统一标量量化（USQ）替换为场景自适应格子矢量量化（SALVQ），通过为每个场景优化格子基底并按需缩放基向量，提升3D Gaussian Splatting（3DGS）数据的压缩率-失真（R-D）性能，同时保持低复杂度和与现有架构的兼容性。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [13] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: 提出群体社交区域检测任务与MINGLE三阶段管线；构建100K注释街景数据集，结合人工与自动标签，支持群体社交互动的可视化检测与定位研究。


<details>
  <summary>Details</summary>
Motivation: 城市规划与公共空间设计需要理解群体层面的社交互动，但传统目标检测无法捕捉复杂的人际关系、接近度与共同行动等语义信号，因此引入了将抽象人际关系在图像中空间定位的新任务。

Method: MINGLE由三部分组成：1) 使用现成的人体检测与深度估计获取个体及其空间信息；2) 基于视觉语言模型（VLM）进行两两社交归属关系推理以判断个体间是否互动；3) 通过轻量级空间聚合算法将具有社交联系的个体组合并定位为群体区域。数据集的标注结合了人工标注与MINGLE产出的标签，以保证语义丰富性和大规模覆盖。

Result: 提出的任务与方法并行提供了可用于大规模研究的数据集（100K注释街景图像）与一个可操作的检测管线MINGLE。论文宣称该方法能基于视觉线索有效推断并定位社交群体，且数据集结合人工与自动标签提高了语义覆盖度。

Conclusion: 本文提出了MINGLE，一种用于检测公共空间中群体社交互动的三阶段管线，并构建了一个包含100K街景图像的注释数据集，用于支持和评估群体社交区域检测任务。

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [14] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap通过分析跨注意力归因图发现并量化TTI模型中人口学与语义的空间耦合（用IoU/SoftIoU），并提出能量引导的去噪采样减偏方法，能够在概念层面上减少偏差耦合，补充现有分布级减偏方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有偏差发现和干预主要关注生成物的人口分布，而忽视内部表征是否将人口学与语义纠缠在一起；即使输出分布被“校正”，模型内部的概念耦合仍可能导致语义关联性偏见。BiasMap旨在深入表征级别发现并干预这种隐藏的概念耦合偏差。

Method: BiasMap首先提取跨注意力归因图以定位人口学和语义概念的空间分布；然后用IoU/SoftIoU量化两个概念间的空间重叠作为耦合度指标；最后在扩散生成时引入基于SoftIoU的能量项，通过修改潜在噪声并在去噪步骤中最小化期望SoftIoU来实现概念级减偏。

Result: BiasMap提出了一种模型不可知的框架，通过跨注意力归因图（cross-attention attribution maps）挖掘Stable Diffusion模型中潜在的概念级表示偏差，并通过IoU量化人口学（如性别、种族）与语义（如职业）之间的空间耦合度。论文还将该方法扩展为一种基于能量引导的扩散采样减偏方法，通过在去噪过程中直接修改潜在噪声并最小化期望SoftIoU，从而在生成过程中降低概念耦合。主要结论是：现有的公平性干预虽能缩小输出级别的人口分布差距，但往往不能解除概念级耦合；而BiasMap能够在图像生成过程中缓解概念耦合，并与分布级偏差缓解方法互补。

Conclusion: BiasMap证明了在Stable Diffusion中存在概念级表示偏差，并提出了一个可操作的发现与干预流程：使用跨注意力归因图量化概念耦合（IoU），并通过能量引导的采样最小化SoftIoU从而降低耦合；该方法能与输出分布调节方法互补，带来更全面的公平性改善。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [15] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePyxel是一个Python GUI工具，连接显微镜等实时设备，提供贝塞尔样条、二值掩码与非破坏性图层，基于OpenCV/NumPy优化，旨在简化实验室实时图像标注并加速AI开发；开源地址在GitHub。


<details>
  <summary>Details</summary>
Motivation: 现有标注工具多依赖预先上传的数据集，不适合需要实时成像与按需标注的实验室场景，限制了AI在科研仪器联动中的应用。

Method: 实现一个基于Python的GUI，集成OpenCV和NumPy，支持视频设备兼容性、可编辑图层、贝塞尔样条和二值掩码，并针对目标检测进行了性能优化。

Result: 提供了一个开源工具LivePyxel（LivePixel），支持实时数据采集与高性能标注，简化采集-标注-训练流程，加速模型开发。

Conclusion: LivePyxel通过实时接入显微镜等成像设备，显著降低了实验室图像数据采集与标注的门槛，促进了AI模型在实验流程中的部署。

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [16] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 用DEFT在不改动大模型参数的情况下，通过训练小型h-transform网络高效适配扩散模型到虚拟试衣任务；再用自适应一致性损失在15步去噪下保持性能并加速推理。


<details>
  <summary>Details</summary>
Motivation: 现实场景下VTO受限于训练与部署预算，大型端到端微调成本高，需高效参数与推理优化以便实用部署。

Method: 冻结预训练扩散模型参数，训练小型h-transform网络学习条件h-transform；使用自适应一致性损失与去噪评分匹配的组合进行低成本微调，实现在15步去噪下高质量VTO生成。

Result: Paper proposes DEFT (Doob's h-transform efficient fine-tuning) to adapt large pretrained unconditional diffusion models for image-conditioned virtual try-on (VTO) by freezing main model and training small h-transform network; trains only 1.42% of parameters vs 5.52% baseline; introduces adaptive consistency loss combining consistency and denoising score matching in data-adaptive way to distill into faster model; achieves SOTA with as few as 15 denoising steps.

Conclusion: DEFT能以极低的参数与计算开销将无条件预训练扩散模型高效迁移到图像条件VTO任务，并结合自适应一致性损失实现低步数快速推理同时维持或超越现有方法性能。

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [17] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 通过在Cityscapes上插入虚拟行人并用新型生成对抗网络调整光照，减少合成与真实域差距，从而提升行人相关的语义与实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要覆盖特定交通场景，真实数据难以涵盖所有情况，合成数据可补充但会带来域间差异，故希望通过更真实的增强来提升行人检测/分割性能。

Method: 构建在Cityscapes数据集上的虚拟行人插入管线，并设计一种新的生成对抗网络来模拟并匹配数据集的光照条件以减少域间差距，最后在语义分割与实例分割任务上进行评估。

Result: 文中在语义与实例分割任务上展示了通过加入虚拟行人并使用光照对抗生成网络后性能提升（具体数值未在摘要中给出）。

Conclusion: 本文提出通过数据增强在Cityscapes上加入虚拟行人以提升行人识别，并通过一种新的生成网络在光照条件上进行对抗学习以提高增强的真实性，从而在语义与实例分割任务上验证方法效果。

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [18] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出FunKAN—把Kolmogorov-Arnold表示推广到函数空间并用Hermite基做傅里叶分解学习内部函数，从而在医学图像增强与分割任务上既保留可解释性又提升性能；U-FunKAN在三个数据集上显著优于其他KAN骨干网络。


<details>
  <summary>Details</summary>
Motivation: Address loss of spatial structure in KANs by extending Kolmogorov-Arnold theorem to functional spaces and learning inner functions via Fourier decomposition on Hermite basis to preserve spatial structure and interpretability.

Method: Propose FunKAN and U-FunKAN architectures with functional KAN using Hermite-Fourier inner functions.

Result: FunKAN reduces Gibbs ringing and improves PSNR/TV on IXI dataset; U-FunKAN achieves state-of-the-art segmentation on BUSI, GlaS, CVC-ClinicDB with higher IoU and F1 than KAN baselines.

Conclusion: FunKAN为可解释的医学图像处理提供了一条可行路径，保留空间结构且提升性能，适用于增强与分割任务，未来可扩展到更多模态与多尺度结构。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [19] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出MultiHateGNN：通过实例图与互补权重图在图神经网络框架下突出仇恨实例并系统建模模态间结构关系，从而在仇恨视频分类上取得SOTA并具可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态仿恨视频检测方法对所有内容一视同仁，忽略了“只要有少量仇恨内容即为仇恨视频”的判定特性，且缺乏系统化地建模视频中结构化信息与跨模态关系，导致融合效果有限与可解释性不足。

Method: 方法包括：将视频分割为多个实例并提取实例级多模态特征；构建实例图以刻画实例间关系；构建互补权重图以为每个实例分配重要性权重以突出仇恨实例；将权重与实例特征融合以生成视频标签；采用图神经网络在图结构上进行特征传播与融合，实现跨模态结构化建模。

Result: 在公开数据集上进行了大量实验，结果显示该模型在仇恨视频分类任务上达到最新最好（state-of-the-art）性能，并且能提供强烈的可解释性（通过重要性权重指示仇恨实例）。代码已开源。

Conclusion: 该论文提出了一个用于仇恨视频分类的多模态双流图神经网络（MultiHateGNN），通过构建实例图和互补权重图来强调视频中具有仇恨内容的实例，并通过图卷积建模模态内与模态间结构化关系，从而提高分类性能并增强可解释性。

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [20] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter使用扩散模型和合成训练结合风格迁移，生成时间一致的结肠镜单目深度图，零样本在C3VD上领先，可用于点云生成与覆盖评估


<details>
  <summary>Details</summary>
Motivation: 解决结肠镜检查中单目视频深度估计在时间一致性上的不足，以支持三维重建和临床应用

Method: 使用合成结肠镜序列训练扩散式深度生成模型以学习稳健几何先验；设计保留几何结构的风格迁移将真实视频映射到合成域；在C3VD上进行零样本评估并演示点云与覆盖评估应用

Result: 提出ColonCrafter，一种基于扩散模型的深度估计方法，利用合成序列学习几何先验并通过风格迁移将真实临床视频适配到合成域，能生成时间一致的深度图并在C3VD数据集上实现零样本SOTA

Conclusion: ColonCrafter在时间一致性深度估计上显著优于现有方法，尽管完整轨迹三维重建仍具挑战，但已可支持实用的临床相关功能，如点云和表面覆盖率评估

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [21] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 通过基于体素的高斯合并和Patch-Grid初始化，该工作在保持实时性的同时减少GPU内存占用并提升渲染质量，推动3D Gaussian Splatting在MAV等嵌入式平台上的应用。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting研究集中在高性能桌面GPU上，忽视了嵌入式平台（如微型无人机）上的应用，这些平台计算资源和内存受限，需在性能与重建质量间做出权衡。论文旨在降低GPU内存占用并提升渲染质量，使3DGS更适用于嵌入式设备。

Method: 方法包括两个关键部分：1) 在体素空间基于几何相似性合并冗余的3D高斯基元以降低GPU内存占用，2) 使用Patch-Grid(PG)点采样对3D高斯基元进行初始化，以更准确地建模场景，从而提升渲染质量。

Result: 在公开数据集上的定量和定性评估显示，所提出的合并策略在不影响运行时间的前提下显著降低了GPU内存使用；PG初始采样则在渲染质量指标上带来提升，综合效果优于原始方法。

Conclusion: 该论文通过在体素空间对相似的3D高斯基元进行合并，并引入Patch-Grid点采样初始化高斯，提高了系统的渲染质量和GPU内存效率，特别适用于资源受限的嵌入式平台如MAVs。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [22] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 提出一种基于QCD并带自适应误差模态建模的轨迹级OOD检测方法，在真实数据集上显著改善检测延迟、误报率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 训练数据与部署环境存在分布差异，尤其是罕见或低频交通场景会导致轨迹预测模型出现失配，现有OOD研究多集中于视觉任务，轨迹层面的OOD检测尚缺乏针对性的理论与实用方法。作者借助QCD的形式化分析希望在延迟与误报之间获得可控权衡，并通过自适应建模应对时变的误差模式。

Method: 基于快速变化检测（QCD）理论，方法首先对轨迹预测误差进行时间序列分析，识别出多模态误差模式并构建相应的统计模型；引入自适应机制实时更新模型参数以应对数据集特定的动态变化；最终在QCD框架下综合各误差模式的检测统计量，从而在保证理论误警/延迟权衡的同时提升实际性能。

Result: 在多个真实世界数据集上的广泛实验显示：①误差分布具有随时间演化且与数据集相关的多模态特性；②显式建模这些误差模态并采用自适应更新显著降低检测延迟与误警率；③与基于不确定性（UQ）和视觉的现有OOD方法相比，该框架在准确性和计算效率方面均有明显优势。

Conclusion: 本文提出了一种面向轨迹预测的自适应异常检测框架，通过显式建模预测误差的多模态（mode-dependent）时序分布，提高在分布漂移下的异常（OOD）检测性能。实验证明该方法比现有不确定性或基于视觉的方法在检测延迟、误报率和计算效率上有显著提升。

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [23] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出一种基于深度学习的卫星图像对变化检测与语义注释方法，用于自动检测亚马逊砍伐并生成关键词注释，结果显示有效且具有通用性。


<details>
  <summary>Details</summary>
Motivation: 监测亚马逊森林砍伐以评估对碳排放和生物多样性的影响，提供自动化工具提高监测效率和语义理解。

Method: 基于深度学习的变化检测，比较不同时相的卫星图像对，结合视觉语义模型将检测到的变化自动标注关键字；候选注释从亚马逊相关科学文献中提取。

Result: 在一个亚马逊图像对数据集上评估，展示了在检测砍伐和生成相关注释方面的有效性，表明方法具有应用潜力且可推广到其他领域。

Conclusion: 方法有效且通用，可用于遥感变化检测并生成语义注释，但需更多细节验证与消融研究。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [24] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 提出一个基于Google Gemini 2.5 Flash的多模态医疗影像分析框架，结合视觉特征、NLP、坐标校验与高斯建模，实现跨模态肿瘤检测与临床报告生成，表现出高检测性能与零样本学习能力，需临床多中心验证。


<details>
  <summary>Details</summary>
Motivation: Automate tumor detection and report generation across CT/MRI/X-ray/Ultrasound using VLMs to improve diagnostic efficiency and reduce dependence on large labeled datasets

Method: Vision-Language integration with Gemini 2.5 Flash, coordinate verification, Gaussian anomaly modeling, multi-modal visualizations, prompt-engineered text extraction

Result: High anomaly detection performance across modalities, 80-pixel average location deviation, zero-shot capabilities, Gradio interface for integration

Conclusion: 框架在自动化诊断支持和放射学工作流效率上有显著提升潜力，但必须进行严格的临床验证与多中心评估后才能广泛应用。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [25] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP用聚类替代传统外点剔除，增强了定位与拼接的鲁棒性，并可推广到3D场景，已在RoboCup实战中验证。


<details>
  <summary>Details</summary>
Motivation: 传统基于重投影误差的外点剔除（如RANSAC）在高噪声、异常匹配比例高或模型空间复杂时表现欠佳。提出CLAP旨在通过聚类机制提高鲁棒性，减少对随机采样和大量迭代的依赖，使定位与拼接在实际机器人赛事环境中更可靠。

Method: CLAP通过在候选解空间中对样本进行聚类，识别出一致性的解簇；然后基于簇内数据评估并选取最可能的全局解。与RANSAC不同，CLAP不依赖随机抽样验证单一模型，而是利用密度/一致性聚类来抑制错误匹配。扩展方法将2D变换推广到3D几何变换及图像配准场景，保留聚类与验证流程。

Result: 在RoboCup 2024实战中，CLAP帮助团队获得冠军；论文展示了CLAP在2D任务的效果，并通过实例与理论说明其对3D定位和图像拼接的扩展可行性。此外讨论了CLAP与RANSAC/Hough的关联与互补性。

Conclusion: CLAP是一种基于聚类的稳健定位框架，能有效抑制异常值并在RoboCup 2024中验证其效果。扩展到3D定位与图像拼接后，CLAP成了一种通用工具，可替代或补充RANSAC和Hough变换用于噪声与不确定性处理。

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [26] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR uses SAM's pretrained image encoder to produce structure-aware features, refines them with a lightweight 3D head, and applies hierarchical feature consistency loss for better coarse-to-fine registration, outperforming prior methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address lack of anatomical weak labels by leveraging SAM's pretrained encoder to extract structure-aware embeddings for medical images, enabling better feature extraction for registration without manual labels.

Method: Discuss SAMIR paper methods and contributions

Result: Proposed SAMIR framework: SAM encoder + 3D refinement head + Hierarchical Feature Consistency Loss; shows improved registration performance on cardiac and abdomen CT datasets (ACDC +2.68%, abdomen +6.44%).

Conclusion: Utilizing large-scale pretrained vision models (SAM) for medical registration improves feature quality and registration accuracy; SAMIR is an effective label-free alternative to weakly supervised methods.

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [27] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出基于联邦学习的分布式卫星影像毁林识别与定位框架，使用FLOWER与RAY协调客户端（边缘卫星中心）仿真与训练，采用YOLOS-small、Faster R-CNN(ResNet50)和Faster R-CNN(MobileNetV3)在公开数据集上实验。


<details>
  <summary>Details</summary>
Motivation: 集中式训练需要汇集原始数据，存在隐私和安全风险；因此希望通过联邦学习在不共享原始数据的前提下实现跨站点协同训练，以准确识别和定位毁林区域。

Method: 基于FLOWER实现联邦学习流程，以RAY在单机上模拟与管理多个边缘卫星中心客户端，客户端各自使用本地数据训练模型（YOLOS-small、Faster R-CNN+ResNet50、Faster R-CNN+MobileNetV3）；训练过程通过参数聚合完成全局模型更新，评估在公开卫星图像数据集上的检测/分割性能。

Result: 在公开数据集上分别对YOLOS-small与两种Faster R-CNN模型进行了分布式训练与测试，证明在联邦设置下这些模型能够有效完成毁林识别与定位任务，并展示了RAY在客户生成与仿真上的效率优势。

Conclusion: 提出的FL框架在保护客户端数据隐私的同时，能在分布式环境下训练目标检测/分割模型以识别和定位毁林，利用FLOWER与RAY实现高效客户端管理与任务调度，并展示了Transformer与两种Faster R-CNN骨干网络在该任务上的可行性。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS通过将两张图像各自重建为公制GMM并直接对齐，实现了训练免费、鲁棒且具有绝对尺度的两视图相对位姿估计，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统两视图方法仅能恢复相对尺度平移并在大基线、无纹理或反光表面表现差；将单视感知（度量深度）与多视几何桥接能够获得鲁棒且有尺度的相对位姿。

Method: 使用度量单视深度估计器与高斯场景重建器为每张图像生成公制GMM，接着用前馈两视图估计器给出初始位姿，并通过可微的GMM对齐目标函数优化包含几何、颜色、各向异性协方差和语义特征的一致性。

Result: 在RealEstate10K数据集上，GARPS在多数评测指标上优于经典方法和最新学习方法（包括MASt3R），证明该方法在鲁棒性和尺度恢复方面的优势。

Conclusion: GARPS提供了一个训练免费、将两视图相对位姿估计转化为独立重建3D场景直接对齐的框架，利用公制单视深度和高斯混合场景来获得绝对尺度的位姿估计，优于现有方法。

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 用可微分的查找表替代卷积中的乘法，训练得到的lookup networks在能耗与速度上更高效且性能接近或达到常规模型。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中的乘法运算计算复杂、能耗高且推理时间长，不利于资源受限的边缘设备部署；受查找表在嵌入式设备上降低计算开销的启发，提出用查找操作替代乘法以降低能耗和加速推理。

Method: 将乘法运算替换为可微分构造的查找操作，通过可训练的查找表并设计多种训练策略以促进收敛；将该查找操作作为基本模块搭建“lookup networks”，并在图像分类、图像超分辨率和点云分类等任务上进行实证验证。

Result: 替换乘法为查找操作后，提出的lookup networks在能耗与推理速度上均有提升，并在多个任务和数据类型上保持或接近于原始卷积网络的性能，实验结果表明该方法在分类与回归任务上达到了先进水平。

Conclusion: 本文提出了一种以查找表（lookup tables）为基本操作的神经网络构建方法，旨在用低开销的查找操作替代卷积网络中高能耗的乘法运算，从而在能耗和推理速度上提升效率的同时保持模型性能。

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 用SAM生成的语义超像素作为视觉“单词”进行压缩性投影与位置编码，能在保持语义的同时极大减少视觉token，提升RIS任务中MLLM+SAM框架的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于patch的视觉投影器在减少token数量与保持语义清晰之间难以权衡，导致视觉token冗余且计算开销大。作者受文本tokenizer启发，认为通过语义分割得到的超像素可作为更具语义意义的视觉“词”，从而实现自适应的token压缩。

Method: 核心方法包括：1) 使用SAM生成语义超像素并将每个超像素压缩投影为一个视觉token（语义视觉投影器）；2) 为超像素设计位置嵌入（语义超像素位置嵌入）以增强模型对几何与位置信息的感知；3) 引入语义超像素聚合器，在超像素内部保持细粒度信息、在超像素之间保留全局上下文。

Result: 实验表明该方法将视觉token数量减少约93%而不损失性能，在RIS任务上比现有压缩视觉投影器表现更好，同时显著加速MLLM的训练与推理。

Conclusion: 该论文提出通过语义超像素作为视觉“单词”来压缩视觉token，从而在大幅减少token数量的同时保持语义完整，应用于RIS任务能显著加速MLLM与SAM联合框架的训练与推理，并在不损失性能的前提下优于现有压缩方法。

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV针对鱼眼摄像头的几何畸变和时空不稳定问题，提出了DRME、U-SCA和D-TSA三模块协同的BEV分割框架，在Synwoodscapes上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于针孔摄像头的BEV方法直接迁移到鱼眼摄像头会受严重几何畸变、多视角对应不确定和时间动态不稳定影响，导致性能下降，因此需要专门设计以处理这些问题。

Method: 方法包含三大模块：1) DRME主干用于在保留尺度一致性的同时学习对畸变鲁棒的多尺度特征；2) U-SCA利用不确定性估计来引导跨视图对齐，提高特征融合的可靠性；3) D-TSA在时间维度引入距离感知的自注意力以在近场细节和远场上下文间自适应平衡，保证时序一致性。

Result: 在Synwoodscapes环视鱼眼BEV分割任务上，FishBEV在各项评估指标上均优于当前SOTA方法，表现稳定提升，证明了所提模块的有效性。

Conclusion: 该论文提出了专为鱼眼摄像头设计的BEV分割框架FishBEV，通过结合畸变鲁棒的多尺度特征提取、基于不确定性的空间跨视图注意力和距离感知的时间自注意力模块来应对鱼眼图像的特殊挑战。实验表明在Synwoodscapes数据集上优于现有SOTA基线。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 提出三种基于B样条的轻量KAN模型，尤其是SBTAYLOR-KAN在少样本、跨域和不平衡医疗影像任务中表现出高效、可解释且参数极少的优势。


<details>
  <summary>Details</summary>
Motivation: 在临床资源受限环境中，深度卷积神经网络参数量大、对数据量需求高且可解释性差，迫切需要一种轻量、稳健且可解释的模型以应对少量、多样且不平衡的医疗影像数据。

Method: 引入三种基于B样条的KAN变体：SBTAYLOR-KAN（B样条+Taylor级数）、SBRBF-KAN（B样条+径向基函数）和SBWAVELET-KAN（B样条+Morlet小波），直接从未预处理的原始影像学习，通过Spline函数逼近局部与全局非线性，并采用Grad-CAM进行可解释性可视化。

Result: 在脑MRI、胸片、结核X线和皮肤病变等数据集上进行了跨数据集验证与数据量削减实验。SBTAYLOR-KAN最高可达98.93%准确率，在仅用30%训练数据时仍能保持>86%准确率；在皮肤癌不平衡数据上表现为68.22%准确率。模型参数远小于传统CNN（SBTAYLOR-KAN仅2872个可训练参数，ResNet50为24.18M），且在Grad-CAM可视化中能定位到相关病灶区域。

Conclusion: 提出的基于样条的Kolmogorov-Arnold网络（KANs）在资源受限和数据稀缺的医疗影像分类场景中表现出轻量、可解释且泛化良好的特性，尤其是SBTAYLOR-KAN在多项任务上取得了最优或接近最优的结果。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: 通过分析跨注意力层的风格敏感性，选取并仅更新这些层以构建轻量级的StyleProtect，能有效防止扩散模型的风格微调复制艺术与动画风格，兼顾防护效果与不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型（尤指扩散模型）能力提升，恶意微调可廉价复制艺术家独特风格，侵害创作者权益。因此需要一种高效且不可察觉的防护机制，既能抵抗针对性微调又不破坏原作视觉质量。

Method: 先衡量扩散模型中各跨注意力层对风格与内容表示的激活强度及其与外部特征提取模型的相关性；基于敏感性指标选择若干关键跨注意力层；在这些层上应用轻量化参数更新（StyleProtect），而不改变模型其余部分，从而在攻击者微调时阻断风格内化。

Result: 在WikiArt挑选的30位风格鲜明艺术家及Anita动画数据上，StyleProtect在多种评估下表现良好：显著降低微调模型复现目标风格的能力，同时对原图视觉影响较小，实现了网络参数量与计算开销的有效节约。

Conclusion: 该论文提出了一种针对扩散模型微调后样式抄袭的防护方法，通过分析跨注意力层对风格的敏感性，选取并只更新这些敏感的跨注意力层以实现高效的风格保护。实验证明在WikiArt和Anita数据集上的30位艺术家与动画风格保护上能有效抵抗微调定制，同时保持较好的不可察觉性。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth在训练时通过光流驱动的教师-学生和像素不确定性引导的加权损失，改善了自监督单目深度估计在动态/低纹理区域的性能，且无推理开销，达到了KITTI上的SOTA。


<details>
  <summary>Details</summary>
Motivation: 自监督单目深度估计虽然免标注，但在低纹理和动态区域因光度一致性弱或被破坏导致深度误差大，且现有运动感知方法往往需额外标签或在推理时增加开销。为此，提出一种在训练阶段利用运动信息和不确定性改进监督而不增加推理成本的方法。

Method: 提出教师-学生训练框架：教师网络在训练时利用光流和不确定性估计产生更强监督，学生网络在推理时不依赖光流或额外网络；在架构上添加不确定性分支以衡量像素级可信度，并在损失中对光度一致性和几何项进行不确定性加权；结合运动感知模块在训练阶段改善动态区域的伪标签。

Result: 在KITTI和Cityscapes数据集上进行的大量实验显示：UM-Depth在单目自监督深度和位姿估计上达到或超越现有最先进方法；对动态物体边界和无纹理区域的深度误差显著降低，同时保持推理时的实时性能。

Conclusion: UM-Depth通过在自监督单目深度估计中引入不确定性估计和运动感知的精化机制，在动态物体边界和无纹理区域提高了深度精度；其教师-学生训练策略仅在训练阶段使用光流，避免了推理开销，并在KITTI和Cityscapes上达到或超越现有最优结果。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: TQF splits referring queries into three dynamic components (appearance, spatial interaction, motion), uses visual+linguistic guidance, and introduces motion-aware aggregation to reduce query bias and improve RVOS performance.


<details>
  <summary>Details</summary>
Motivation: Static textual object queries in existing RVOS methods are misled by distractors with similar appearance or motion, causing query selection bias; therefore, more structured, dynamic queries and motion-aware aggregation are needed.

Method: TQF factorizes queries into appearance, intra-frame interaction, and inter-frame motion queries, dynamically integrates linguistic and visual cues, and adds Intra-frame Interaction Aggregation and Inter-frame Motion Aggregation modules for enhanced token representations and trajectory-guided alignment.

Result: Extensive experiments on multiple RVOS benchmarks show TQF outperforms prior approaches, demonstrating the effectiveness of structured queries and motion-aware aggregation for robust cross-modal alignment and temporal consistency.

Conclusion: The paper introduces Triple Query Former (TQF) to address query selection bias in RVOS by factorizing referring queries into three specialized, dynamically constructed queries and adding motion-aware aggregation modules, leading to improved temporal coherence and robustness against distractors.

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG uses instance queries and prior reference points to jointly handle GREC and GRES with instance-level consistency, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing works treat GREC and GRES separately and often ignore instance-aware modeling and consistency between instance-level boxes and masks.

Method: Introduce instance queries with prior reference points in a multi-task framework to jointly predict point, box, and mask for each instance; use the reference point for matching and consistency between boxes and masks.

Result: State-of-the-art performance on ten datasets across four tasks, significantly outperforming previous methods.

Conclusion: InstanceVG successfully unifies GREC and GRES with instance-aware mechanisms, enabling consistent multi-granularity predictions and improving performance over prior methods.

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: 提出FMFA框架：A-SDM用于修正未对齐正样本，EFA用于显式稀疏化相似度矩阵并硬编码局部对齐，结合隐式推理实现全模式对齐，在三数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有TIPR方法依赖注意力等隐式局部对齐机制，缺乏验证局部特征是否正确对齐的能力，且在训练中主要关注困难负样本，常忽视错误匹配的正样本，导致全局与局部对齐不足。

Method: 设计A-SDM模块自适应拉近未匹配的正样本对以改进全局对齐；引入EFA模块通过稀疏化相似度矩阵和硬编码局部对齐增强显式细粒度交互；结合隐式关系推理实现“全模式”对齐，无需额外监督。

Result: 在三个公共数据集上，FMFA在所有全局匹配方法中取得了最先进的性能。

Conclusion: FMFA通过结合自适应相似度分布匹配和显式细粒度对齐模块，改进了文本-图像行人检索中的全局匹配与局部验证能力，从而提升了正负样本区分与未对齐正样本的纠正，实验证明在三个数据集上优于现有全局匹配方法。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出颜色映射模块，将目标RGB直接映射到文本嵌入，实现更精确与连续的文本驱动图像颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 文本描述的固有模糊性与离散性导致颜色编辑难以精确与连续控制，线性插值文本嵌入无法精确控制颜色范围且插值系数与图像颜色关系不明确。

Method: 设计并训练一个颜色映射模块，输入目标RGB值，输出对应的文本嵌入向量；在图像编辑流程中，用该嵌入替换或插值原文本嵌入，以控制颜色变化，同时保持语义一致性。

Result: 实验表明该方法在颜色连续性与可控性方面表现良好，用户可指定目标RGB范围以生成在期望范围内连续变化的图像颜色。

Conclusion: 该论文提出通过颜色映射模块在文本嵌入与图像RGB值之间建立显式对应，从而实现对生成图像颜色的精确、连续和可控编辑。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 作者提出利用VLM的视觉反馈进行迭代提示词精炼，打造更安全且保留用户意图的T2I生成流程，并构建了带视觉与文本安全标签的数据集，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的提示词精炼方法仅依赖文本，不考虑生成的图像，可能导致未能检测到图像中的不安全内容或对本已安全的提示过度修改，因此需要引入视觉反馈以提高安全性和保持用户意图。

Method: 提出迭代提示词精炼算法：在每次生成后，使用VLM对生成图像进行视觉评估，并结合对原始提示的文本评估来判断是否存在安全问题；若存在问题，算法通过细化或替换提示词进行修正，并重新生成图像，循环直到满足安全标准或达到迭代上限。同时构建了带有文本和视觉安全信号的新数据集，利用多模态LLM进行标签生成并用于有监督微调。

Result: 实验表明，IPR在不损失用户意图对齐性的前提下，使生成图像更加安全；此外，新数据集和监督微调进一步提升了性能。作者提供了代码并提示论文包含有害示例。

Conclusion: 该论文提出了一种基于视觉反馈的迭代提示词精炼算法（IPR），通过视觉语言模型（VLMs）分析输入提示词与生成图像，从而更有效地提升生成图像的安全性，同时尽量保持用户意图和与现有基于LLM的方法相当的可靠性。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出TA-ISP：通过预测轻量级多尺度调制算子，将RAW数据转为任务导向的RGB表示，既提升感知任务性能，又大幅降低计算与参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大型ISP网络导致计算开销大，要么受限于传统ISP管线的表示能力，无法充分利用RAW数据来提升视觉感知任务的性能。

Method: 提出了一个紧凑的RAW-to-RGB框架，通过预测少量轻量级多尺度调制算子（全局、区域、像素级）来重塑图像统计，替代冗重的稠密卷积ISP网络或传统ISP调参方法。

Result: 在多个白天和夜间的RAW域检测与分割基准上，TA-ISP在提高下游任务精度的同时显著减少参数量和推理时间，适合部署在算力受限的设备上。

Conclusion: TA-ISP有效在保持低计算开销的同时，为预训练视觉模型生成面向任务的RGB表示，从而提升检测和分割精度并适用于资源受限设备。

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出针对夜间低光雨条纹的NDLPNet，核心为位置感知模块与夜间真实数据集NSR，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去雨方法多针对白天场景，夜间光照复杂导致雨条纹分布空间异质性和光依赖条纹可见性，使得白天模型在夜间表现差，因此需要专门针对夜间特性的模型与数据集。

Method: 提出Nighttime Deraining Location-enhanced Perceptual Network（NDLPNet），其中核心为Position Perception Module（PPM）用于捕捉空间位置上下文并重校通道重要性，结合感知损失与训练数据在低光条件下去雨条纹并保留背景信息。

Result: 在构建的900对真实夜间雨景NSR数据集和现有数据集上，NDLPNet在定性与定量指标上均超过若干SOTA方法；提供代码与数据集开源。

Conclusion: NDLPNet通过引入位置感知模块（PPM）与感知网络结构，在低光夜间场景下提高了对雨条纹的定位与去除能力，同时保持背景细节，实验结果显示优于现有SOTA方法。

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [42] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI用交叉注意力融合视频/音频/音位，并配合对比学习，显著提升rtMRI发音器官分割精度与鲁棒性，达SOTA。


<details>
  <summary>Details</summary>
Motivation: 利用音频与音位信息补充rtMRI视觉信息，提升实时磁共振成像中发音器官分割的精度与鲁棒性。

Method: 基于视频编码器提取视觉特征，音频与音位分别编码并通过交叉注意力与视觉特征融合；同时训练对比学习目标以强化模态间对齐；在USC-75子集上进行定量评估与消融实验。

Result: 提出VocSegMRI多模态框架，融合视频、音频与音位输入，通过交叉注意力对齐动态特征，并引入对比学习使得在推理时即使无音频也能维持性能；在USC-75子集上达成Dice 0.95和HD_95 4.20 mm，超越多种基线。

Conclusion: 跨模态融合与对比学习显著改善rtMRI器官分割，即便推理时缺失音频也维持高性能，表明整合音频与音位信息对声道分析有重要价值。

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [43] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 提出以扩散模型先验为核心的生成式图像压缩框架，结合预优化编码器、轻量适配器、注意力融合与分布重归一化，在低码率下显著提升视觉保真度和压缩效率，且能低成本适配多种预训练扩散模型。


<details>
  <summary>Details</summary>
Motivation: 面对自然图像与AI生成图像混合的复杂视觉内容，现有传统编码器和学习方法在主观质量或高压缩比条件下表现不佳，需借助生成式先验提升低码率感知质量和泛化能力。

Method: 使用预优化编码器生成通用压缩域表示，并通过轻量适配器和注意力融合模块将该表示注入预训练扩散模型的内部特征；引入分布重归一化方法以提升重建保真度。框架可在不同预训练模型间高效适配，训练代价低。

Result: 在大量实验中，方法在低码率下主观视觉质量优于现有方法；在压缩性能方面，相比H.266/VVC最高提升约79%；并展示了对AI生成内容的高效适配能力。

Conclusion: 该论文提出了一种基于扩散先验的生成式压缩框架，通过预训练扩散模型提升低码率下的视觉重建质量，兼顾了保真度与生成能力。

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [44] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: AdaThinkDrive通过快慢双模训练与自适应奖励机制，实现按需调用CoT，提升自动驾驶决策质量并降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 观察到在简单场景中盲目使用Chain of Thought带来额外计算开销但并不提升决策质量，因而希望设计按需推理机制平衡性能与效率。

Method: 先在大规模AD场景上用QA和轨迹数据进行预训练，SFT阶段用包含无需CoT的快速回答和带CoT的慢思考的双模数据集训练模型；引入Adaptive Think Reward与Group Relative Policy Optimization，通过比较不同推理模式下轨迹质量来奖励选择性使用CoT。

Result: 在Navsim基准上得到PDMS 90.3，超越最佳视觉基线1.7点；相比始终不思考和始终思考基线分别提升2.0和1.4点；推理时间比始终思考减少14%。

Conclusion: AdaThinkDrive提出了双模推理机制，在端到端自动驾驶中按需选择是否使用CoT，从而在保证决策质量的同时提高效率。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [45] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 提出双分支独立预测+形态学融合的伪造区域定位方法，有效抑噪并提升空间一致性，从而改善定位精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然在分类上进步显著，但在伪造区域定位上仍存在局部细节与全局语义互补未充分利用、以及简单融合策略放大噪声的不足，因此需要设计独立预测并采用更稳健的融合机制。

Method: 方法包括双分支网络分别进行局部和全局预测，独立生成二值化伪造概率图；随后采用形态学操作（例如开闭运算、腐蚀膨胀）对预测结果进行噪声抑制与连通性增强；最后通过基于形态学的融合策略将两分支结果结合，避免简单加权带来的噪声放大。

Result: 大量实验证明各模块（双分支预测、形态学去噪与融合策略）均能提升定位精度与鲁棒性，形态学融合在抑制噪声和增强空间一致性方面效果明显。

Conclusion: 本文提出了一种同时从局部细节和全局语义两个视角独立预测伪造区域，并通过形态学操作融合二者输出的方法，旨在提高伪造区域定位的准确性与空间一致性。

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [46] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 提出可变速空间事件Mamba，直接处理原始事件流，结合轻量因果空间编码器与Mamba状态空间模型，并通过自适应控制器在高效与低延迟间动态平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法需将事件流转换为中间表示（帧/体素/点云），受限于预定义时间窗口引入窗口延迟，点级检测计算代价高难以实时；因此需无需中间表示且高效的处理方法。

Method: 设计了轻量级因果空间邻域编码器捕捉局部几何关系，随后使用基于Mamba的状态空间模型进行线性复杂度的时间建模；推理时引入控制器根据事件率自适应调整处理速度。

Result: 方法实现了在不依赖中间帧/体素/点云的情况下，实现可扩展的时间建模和自适应速率推理，从而在窗口延迟与推理延迟间取得平衡，并具备线性复杂度、实时潜力。

Conclusion: 该论文提出了一种直接处理原始事件流的可变速空间事件Mamba架构，兼顾低窗口延迟与实时计算效率，适用于高速视觉任务。

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [47] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: 提出训练-free的块级缓存策略（BWCache），基于块特征相似性在推理时重用DiT中间特征，最大加速2.24×且视觉质量无显著下降。


<details>
  <summary>Details</summary>
Motivation: DiT的逐步去噪推理开销大，导致实际应用中的延迟。现有加速方法要么改变结构影响质量，要么未能在合适粒度上重用中间特征。论文旨在在不改模型、无训练的前提下降低延迟。

Method: 分析发现DiT块在扩散时间步中呈U型特征变化，中间时步特征高度相似。基于此，BWCache在推理阶段动态缓存DiT块特征，并用相似性指标判断相邻时间步块特征差异是否低于阈值，从而决定是否重用缓存，避免不必要的计算。

Result: 在多种视频扩散模型上，BWCache在保持可比视觉质量的同时，最多实现约2.24×的加速。

Conclusion: 该论文提出了BWCache，一种针对DiT（Diffusion Transformer）视频生成模型的训练-free加速方法，通过块级缓存与基于相似度的重用策略，在保持视觉质量的同时显著减少推理时间。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [48] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出一个基于LIRR的有监督域适应方法用于航天器关键点回归与位姿估计，能用极少量真实标签显著缩小合成到真实的性能差距，在SPEED+上表现优异，轻量且可实用。


<details>
  <summary>Details</summary>
Motivation: 现有混合管道在合成数据上表现好但在真实图像上因域差显著退化，且无监督域适应在有少量目标域标签时效果不佳；因此需要一个能利用有限真实标签提升泛化的有监督域适应方案。

Method: 基于LIRR（Learning Invariant Representation and Risk）范式，方法同时最小化源域和目标域的任务损失并学习域不变特征表示；结合关键点回归与PnP求解位姿，框架轻量、与主干网络无关，并在训练中使用有限真实标注样本进行监督域适应。

Result: 在SPEED+基准上广泛实验表明该方法优于仅源训练、微调及某些上界（oracle）基线；尤其在仅有5%目标标签时即可匹配或超越在更多标签下的oracle性能。

Conclusion: 该文提出首个针对航天器位姿关键点回归的有监督域适应框架，通过联合优化域不变表示与任务风险，在合成与少量真实标注数据上训练，显著提升了在真实/实验室图像上的性能，且在SPEED+数据集上用仅5%标签即可达到或超越较多标签的基线。

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [49] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: Generate a short tldr


<details>
  <summary>Details</summary>
Motivation: Describe the motivation of the paper

Method: Please analyze the method and contributions

Result: Summarize main results

Conclusion: Give concise conclusion and potential future work

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [50] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 提出了一种用于UDA语义分割的辅助任务：Masked Feature Modeling (MFM)。在特征空间进行掩码与重建，通过轻量级重建模块Rebuilder与分割解码器配合训练，推理时无额外开销。MFM将重建目标与像素级分割任务对齐，在多种架构和基准上均显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法中对比学习等自监督任务改善了判别性，但基于掩码建模方法在UDA语义分割中少有应用，主要因架构不兼容与优化目标不一致。MFM旨在解决这些问题，将掩码建模与分割任务对齐。

Method: 在中间特征空间随机掩码并使用轻量级的Rebuilder模块重建被掩的特征，重建后的特征通过分割解码器进行分类，联合训练但Rebuilder仅在训练时存在，推理不增加计算成本。

Result: 在多种主流架构（如DeepLab、DAFormer）和UDA基准上，MFM稳定提高分割性能，证明了其有效性、通用性和零推理开销的优点。

Conclusion: MFM作为一种简单高效的辅助任务，能够与主分割任务紧密耦合，在不改变推理流程的前提下提升UDA语义分割性能，具有通用性和实用价值。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [51] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: 作者提出将MiniROCKET/HDC-MiniROCKET用于光谱分类，展示其在小样本设置下比轻量级神经网络更鲁棒，且在一般设置与之相当。


<details>
  <summary>Details</summary>
Motivation: 尽管空间-光谱方法表现最佳，但纯光谱分类因模型小、训练数据需求少等优点仍有价值；而现有的轻量级模型1D-Justo-LiuNet在数据受限时性能下降，因而需要更鲁棒的替代方法。

Method: 作者将MiniROCKET和HDC-MiniROCKET应用于光谱分类任务，利用无训练参数的特征提取器生成特征，再用简单的分类器（可能是线性或轻量级分类器）进行分类，并在有限数据和常规数据设置下与1D-Justo-LiuNet进行比较评估。

Result: 实验证明：MiniROCKET尽管参数更多，但在有限训练样本场景下优于1D-Justo-LiuNet；在数据充足场景下，两者性能相当。

Conclusion: 论文结论是：在训练数据有限的情况下，基于MiniROCKET和HDC-MiniROCKET的方法优于现有的参数极少的1D-Justo-LiuNet；在一般（数据充足）情况下，MiniROCKET表现与1D-Justo-LiuNet相当。

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [52] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出首个用于半监督病理图像分割的多任务Mixture-of-Experts（Semi-MOE），通过三专家协同、门控伪标签融合和自适应多目标损失，显著降低伪标签噪声并在GlaS与CRAG数据集上取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 现有半监督方法在病理图像中受伪标签噪声影响大，主要由腺体边界模糊与形态学误分导致。通过多任务和专家分工可以更精准建模多样的形态信息，从而生成更可靠的伪标签。

Method: 构建三专家MoE框架：主分割专家、符号距离场回归专家和边界预测专家；引入多门控伪标签模块（Multi-Gating Pseudo-labeling）用于动态聚合专家特征并进行融合-精炼伪标签；提出自适应多目标损失以自动平衡多任务训练。

Result: 在GlaS和CRAG基准数据集的低标签设置下，Semi-MOE优于当前先进方法，实验显示其在分割准确性和鲁棒性上有显著提升。作者已开源代码。

Conclusion: Semi-MOE有效提升了半监督组织病理图像分割在低标注条件下的性能，通过专家网络组合与自适应多目标损失减少伪标签噪声并改善边界与形态特征建模。

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [53] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 提出VHBench-10细粒度幻想基准并设计VisionWeaver上下文感知动态路由以融合多专家视觉特征，显著减少LVLM对象幻想并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在实际应用中受对象幻想问题严重限制，而视觉编码器作为视觉理解核心，其训练范式可能赋予不同归纳偏置，导致幻想表现差异；现有基准过于粗粒，无法揭示细粒度幻想类型和编码器间差异，因此需要更细致的基准与针对性融合方法来减少幻想。

Method: 构建约1万样本的VHBench-10，细分为10类细粒度幻想场景；评估多种视觉编码器在LVLM中的幻想特性；提出Context-Aware Routing Network（VisionWeaver），使用全局视觉特征生成路由信号，按需聚合多个专家的视觉特征以供下游LVLM使用；在多个数据集/任务上进行对比实验并消融分析。

Result: 实验显示不同视觉编码器确实具有独特的幻想特性；VHBench-10成功量化十类细粒度幻想；VisionWeaver较简单特征融合方法显著降低幻想率并提升整体性能（准确率/回答质量等指标），且在消融实验中证明路由机制与专家多样性贡献明显。

Conclusion: 本文提出VHBench-10基准和VisionWeaver路由网络，表明不同视觉编码器因训练范式产生不同归纳偏置，导致各自不同的幻想（hallucination）表现；VisionWeaver通过上下文感知的动态路由融合多专家特征，有效降低幻想并提升性能。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [54] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 论文提出Consistent View Alignment，通过显式对齐不同视图的表示以避免伪阳性，从而改进自监督表示学习，在公开挑战中取得优异成绩并开源代码。


<details>
  <summary>Details</summary>
Motivation: 当前基于视图对比或对齐的表征学习方法普遍假设未经显式结构化的视图对齐就能得到有意义的潜在表示，作者认为该假设不成立并希望通过新方法证明需要显式结构化对齐。

Method: 提出Consistent View Alignment（CVA）方法，通过对齐不同视图的表示以捕捉互补信息，同时设计机制避免产生伪阳性对齐；在实验中将CVA用于自监督学习，结合Primus ViT与ResEnc CNN进行评测。

Result: CVA在下游任务上提升了性能；在MICCAI 2025 SSL3D挑战赛中，使用Primus ViT和ResEnc CNN分别取得第一名和第二名；公开了代码与预训练权重。

Conclusion: 作者质疑“无需额外约束即可从不相关视图学习有意义表示”的常见假设，认为潜在空间的有结构表征不会自发出现，必须显式诱导。

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [55] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出SpecDiff：通过自我推测结合历史信息进行多层级特征缓存与选择，训练无关，实现更高的加速-准确率折中。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存仅依赖历史信息，导致加速与精度受限。通过引入未来信息（自我推测），可以更充分利用信息，提高缓存决策的准确性，从而突破速度-精度折中。

Method: (1) 在同一时间步跨不同迭代中利用信息相似性进行自我推测，得到未来信息；(2) 为每个token计算动态重要性评分，结合历史与自我推测信息进行缓存特征选择；(3) 基于重要性分数对tokens进行多级分类，采用不同计算策略以节省计算；(4) 整体为训练无关的多层特征缓存框架。

Result: SpecDiff提出了一种在特征缓存中引入“自我推测（self-speculation）”以融合历史与未来信息的新范式，通过动态重要性评分和多层级分类策略，实现训练无关的加速推理。实验在Stable Diffusion 3/3.5和FLUX上显示，平均分别获得约2.80×、2.74×和3.17×的加速，同时质量下降可忽略。

Conclusion: SpecDiff通过融合自我推测信息与历史信息，采用重要性评分进行缓存选择并按等级计算特征，实质性改进了扩散模型推理的速度-质量权衡，且无需重新训练模型。

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [56] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS通过VLM+LLM融合图像隐含文本语义，并用双原型引导的扩散模型生成合成数据，显著提升数据集蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏侧重低级视觉特征，忽视图像的高级语义和结构信息；为弥补这一不足，引入隐含文本语义以提升蒸馏效果。

Method: 使用VLM生成外部文本并通过Global Semantic Query模块与图像特征融合形成先验聚类缓存；Local Semantic Awareness从缓存中选择代表性样本构建图像与文本原型，文本原型通过对LLM进行精心设计的提示生成；最后通过扩散模型和Dual Prototype Guidance生成最终合成数据集。

Result: 通过大量实验验证了方法有效性，代码已开源。

Conclusion: EDITS提出通过利用图像中的隐含文本语义信息改进数据集蒸馏，从而在保持性能的同时合成紧凑数据集。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [57] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss integrates Gaussian Splatting and a laminography detector-to-world transform with artifact-filtered initialization to reconstruct high-quality volumes from highly sparse laminographic projections


<details>
  <summary>Details</summary>
Motivation: Improve sparse-view laminography reconstruction by combining Gaussian Splatting with explicit laminographic geometry and artifact-aware initialization to avoid fitting false structures

Method: LamiGauss: Gaussian Splatting with laminographic-aware detector model

Result: Direct optimization from sparse projections yields accurate, efficient reconstructions; outperforms existing methods on synthetic and real data, achieving superior results using only 3% of full views

Conclusion: Artifact-aware initialization plus laminography-aware Gaussian Splatting enables accurate reconstruction from extremely sparse views, concentrating model capacity on real structures and outperforming traditional iterative methods even with far fewer projections

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [58] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: 提出可插拔的抗干扰记忆模块与自省管理策略（DAM4SAM），显著降低对干扰物的漂移并提升遮挡后重检测，在多个基准上优于SAM2.1，集成到不同跟踪器均有明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的视频分割/跟踪模型（例如SAM2）在分割任务中表现优秀，但未针对视觉上类似的干扰物进行特别优化，导致视觉跟踪中易发生漂移并降低遮挡后的重检测效果。作者旨在设计一个模块以增强对干扰物的鲁棒性并提升跟踪质量。

Method: 作者设计了一个“抗干扰”记忆模块和自省（introspection）管理策略，用于过滤并管理记忆库中的干扰帧或对象特征，作为对SAM2的替代记忆单元（drop-in）。此外构建了DiDi（Distractor-Distilled）数据集来评估干扰物存在下的跟踪性能，并将模块集成到不同跟踪器（如EfficientTAM和EdgeTAM）进行实证评估。

Result: DAM4SAM 在13个基准上超越了SAM2.1，在10个基准上创下新的最优结果。将其记忆模块接入实时跟踪器EfficientTAM带来11%的提升，使其在多项基准上达到与非实时SAM2.1-L相当的跟踪质量；接入EdgeTAM带来4%提升，显示良好泛化性。作者同时发布了DiDi数据集以便评估抗干扰能力。

Conclusion: 该论文提出了一个面向 SAM2 的可插拔“抗干扰”记忆模块（DAM）与基于自省的管理方法，有效降低跟踪过程中因视觉相似物体导致的漂移，并提升遮挡后的重检测能力。将其集成到不同跟踪器上均能显著提升性能，且在多项基准上优于或超过 SAM2.1，证明了方法的有效性与泛化能力。

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [59] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: PelFANet uses Fused Attention Blocks to combine raw and segmented pelvic X-rays, improving fracture detection—high accuracy and AUC on visible cases and good generalization to invisible cases


<details>
  <summary>Details</summary>
Motivation: Subtle or invisible pelvic fractures are hard to detect on standard radiographs; anatomy-aware dual-input can improve classification

Method: Dual-stream attention network fusing raw X-rays and segmented bone images

Result: PelFANet achieves 88.68% accuracy and 0.9334 AUC on visible fractures; 82.29% accuracy and 0.8688 AUC on invisible fractures (without training on them)

Conclusion: Anatomy-aware dual-input architectures like PelFANet provide robust fracture detection, beneficial for subtle radiographic presentations

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [60] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: 提出基于事件相机的轻量级第一视角3D手追踪（EvHand-FPV），通过手腕 ROI、端到端 ROI 偏移嵌入与多任务几何辅助头，在大幅降低参数与计算的同时提升2D表现，适合嵌入式 XR 应用。


<details>
  <summary>Details</summary>
Motivation: 解决帧基方法在精度、延迟和能效上的不足，并利用事件相机高时间分辨率和低功耗优势，在资源受限的第一视角场景下实现高效手部追踪。

Method: 构建合成训练数据与真实2D标注测试集，提出基于手腕的 ROI 几何定位、将 ROI 偏移嵌入网络的端到端映射以避免显式重建，以及带辅助几何特征头的多任务学习策略。

Result: 在真实 FPV 测试集上，2D-AUCp 从0.77提升到0.85，参数量由11.2M降至1.2M（降89%），FLOPs由1.648G降至0.185G（降89%）；在合成3D数据上保持竞争力的3D-AUCp 0.84。

Conclusion: EvHand-FPV 提出了一种轻量级事件相机驱动的第一视角三维手部追踪框架，兼顾高精度与低计算开销，适合 XR 设备。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [61] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出WARM：在交叉注意力间加入白化与着色变换以对齐并恢复特征分布，改进原型生成，显著提升少样本3D点云分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有FS-PCS方法使用随机初始的传统算法（如最远点采样）构建原型，初始随机性影响性能且原型生成过程未得到充分研究，作者因此探索基于注意力的高级原型生成方法并解决其分布不匹配问题。

Method: 作者将原型生成视为注意力操作，提出在交叉注意力前对支持特征进行白化以对齐到可学习原型的分布，注意力计算后再进行着色以恢复原始分布，从而缓解分布差异并保持语义关系。

Result: 在多个FS-PCS基准上，WARM方法以显著优势达到或超过最先进性能，实验证明其通过更具代表性的原型提高了分割效果。

Conclusion: 本文提出WARM模块，通过在交叉注意力前后加入白化与着色变换，解决了可学习原型与支持特征分布不一致的问题，从而生成更稳健的原型并提升FS-PCS性能。

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [62] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出SRC：先让模型生成论证再答题，搜索候选并用R-Scorer成对打分，基于置信度采集偏好做偏好微调，从而校准论证与答案，提高LVLM在视觉问答中的一致性与准确性。


<details>
  <summary>Details</summary>
Motivation: 动机是当前LVLM在视觉问答中虽然整体能力强，但论证（rationale）与最终答案常不匹配，导致推理不一致和错误回答，故需对齐论证与答案以提高可靠性。

Method: 方法包括：1) 轻量级的论证微调，让模型在生成答案前输出论证；2) 对每个样本从微调模型中搜索多样化候选响应；3) 使用定制评分模型R-Scorer进行成对评分，评估论证质量与事实一致性；4) 基于置信度加权的偏好策划，将校准问题转化为偏好微调。

Result: 在多个基准上，SRC在感知、推理与泛化能力上显著提升LVLM性能，表明以论证为中心的对齐能够挖掘LVLM潜能。

Conclusion: 该论文提出通过自我论证校准（SRC）框架改善大型视觉语言模型（LVLMs）中论证与答案不一致的问题，从而提升模型的推理与回答质量。

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [63] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: 提出AntiPure对抗净化方法，通过频域与时间步误导，使保护噪声在常见净化流程后仍能保持并在定制化（customization）阶段导致图像扭曲。


<details>
  <summary>Details</summary>
Motivation: 当前保护性扰动易被净化移除，导致图像在被滥用（如深度伪造、版权侵权）时重新暴露风险；需要设计在净化-定制化流程中依然有效的扰动方法，以真正防止图像滥用。

Method: 引入两种引导机制：1) Patch-wise Frequency Guidance，限制模型对净化后高频信息的调整，从而保留关键扰动；2) Erroneous Timestep Guidance，通过在不同去噪时间步注入错误引导，干扰模型的去噪策略。结合额外引导，训练出在代表性净化设置下依然保持的不可察觉扰动。

Result: 在一系列净化和定制化评估中，AntiPure在保持低感知差异（视觉上不可见）同时，实现对定制化输出的最大畸变，优于其他方法，能作为测试净化鲁棒性的压力测试工具。

Conclusion: AntiPure能在不显著改变视觉感知的前提下，在多种净化策略下保持防护扰动，最终在模型定制化输出中造成显著失真，效果优于现有保护扰动方法。

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [64] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 提出一种无需额外训练或反向传播的噪声水平优化方法NLG，通过优化初始噪声与引导对齐以提升扩散模型生成质量与条件一致性，实验验证了其有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的起始随机噪声会影响最终输出，导致图像质量和对提示语的遵从性波动；现有噪声水平优化方法通常需要额外数据、网络或反向传播，限制了实用性，作者旨在提出一种更简单高效且通用的替代方案。

Method: NLG通过对初始随机高斯噪声进行高效优化，使其更可能与给定的扩散级别引导对齐；该方法无需构建额外数据集或训练额外网络，也不依赖反向传播，且能统一适配有条件与无条件扩散模型及多种引导形式，能与现有引导方法无缝结合并保持计算效率。

Result: 在五个标准基准上进行的广泛实验显示，NLG能提高生成图像的质量和对输入条件的遵从性，同时与现有引导方法兼容且保持计算效率，展示出良好的实用性与可扩展性。

Conclusion: 本文提出了Noise Level Guidance (NLG)，一种无需额外训练数据、辅助网络或反向传播的噪声水平优化方法，通过提高初始噪声与通用引导（guidance）对齐的似然，改善扩散模型生成图像的质量与条件一致性。

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [65] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally是一个包含681张图、每图两类目标的细粒度视觉计数基准；实验表明现有模型在区分细微差异并按用户意图计数方面表现不佳，需进一步研究与改进。


<details>
  <summary>Details</summary>
Motivation: 评估模型在意图驱动的细粒度计数能力，填补现有数据集对选择性计数场景的缺失，诊断模型在区分细微外观或语义差异时的弱点。

Method: 构建了包含681张高分辨率图像的数据集，每张图包含两个目标类别，设置了跨类(inter-category)与同类内(intra-category)的计数场景；对比评测了示例基方法、语言提示方法及大型视觉语言模型（VLMs）。

Result: 基准测试显示，无论是示例式方法还是大型VLM，都难以在细粒度和视觉模糊情况下稳定完成用户期望的计数任务，证明PairTally是诊断与改进此类系统的有用资源。

Conclusion: PairTally表明当前模型在细粒度视觉计数任务上仍存在明显不足，尤其在区分类似子类别或视觉模糊情形下难以满足用户意图。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [66] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA把多模态大模型的区域语义蒸馏到轻量目标检测器，通过翻译模块和局部+全局对齐损失，在少样本个性化检测上显著提升性能，平均+10.1分，且推理无需文本输入。


<details>
  <summary>Details</summary>
Motivation: 利用强大的视觉-语言大模型提供的区域级多模态语义，提升资源受限的视觉仅目标检测器在少样本个性化检测场景下的性能，同时保持推理时无需文本输入与教师模型参数不变。

Method: 提出一个翻译模块，将学生（仅视觉）特征映射到与教师（视觉-语言大模型）共享的嵌入空间；设计双目标损失，包括局部（区域/目标级）对齐损失和全局关系一致性损失，联合训练学生和翻译器。

Result: 在四个少样本个性化检测基准上，MOCHA相较基线平均提升+10.1分；在设计紧凑的学生网络上也能达到与更大多模态模型相当的性能，证明了其实用性。

Conclusion: MOCHA通过在目标级别进行多模态语义对齐，实现了大模型到轻量目标检测器的有效知识蒸馏，从而在少样本个性化检测任务上显著提升性能。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [67] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 通过合成数据、骨干升级和中间FEDER特征融合，改进的YOLO-FEDER FusionNet在复杂背景下显著提高无人机检测的召回和精度。


<details>
  <summary>Details</summary>
Motivation: 在背景杂乱、目标尺度小及伪装效应强的场景中，标准通用检测器（如YOLO）性能下降，需要结合伪装检测技术与更丰富数据与特征融合策略以提高可分离性和鲁棒性。

Method: 在训练数据上采用大规模光照真实感合成数据并辅以少量真实样本；系统评估中间多尺度FEDER特征的贡献；比较不同YOLO骨干网络配置；在架构上引入改进的特征融合策略及骨干网络升级（例如YOLOv8l），并从DWD模块提取FEDER特征进行融合。

Result: 在最优配置（YOLOv8l骨干，DWD模块提供中间FEDER特征）下，相较基线在IoU=0.5时FNR降低最多39.1个百分点，mAP提升最多62.8个百分点，表明中间FEDER特征与骨干升级显著改善检测效果。

Conclusion: 本文通过将通用目标检测与伪装目标检测技术融合，改进YOLO-FEDER FusionNet，在复杂视觉环境下显著提升无人机检测性能。

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [68] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: 提出SAIL-VL2，通过大规模数据筛选、渐进训练框架和稀疏MoE架构改进，实现多模态理解与推理的新基准。


<details>
  <summary>Details</summary>
Motivation: 提升多模态模型在感知到复杂推理任务上的表现，并提供一个高效、可扩展的开源基础视觉-语言模型。

Method: 1) 大规模数据策划与评分过滤；2) 渐进式训练：预训练视觉编码器->多模态预训练->思考融合SFT-RL混合训练；3) 架构改进：引入稀疏Mixture-of-Experts。

Result: SAIL-VL2在2B和8B参数规模上，在图像和视频基准测试中表现出色，涵盖从细粒度感知到复杂推理的能力。

Conclusion: 三项核心创新（数据管道、渐进训练、架构改进）使SAIL-VL2在多模态任务和推理基准上取得领先，并成为开源多模态社区的高效基础模型。

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [69] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: 提出通过中间融合多模态嵌入并结合Cox回归的PROFUSEme方法，提升术时前列腺癌生化复发预测性能，内部验证C-index~0.86，外部保持集C-index~0.71。


<details>
  <summary>Details</summary>
Motivation: 约30%的前列腺癌患者在根治性切除后出现生化复发，若能在术时准确预测BCR，可实现及时的个体化临床决策并改善预后，因此需要利用多模态数据提升预测性能。

Method: 采用中间融合架构将临床、影像学与病理学嵌入进行拼接与交互建模，然后基于Cox比例风险回归器进行生存分析与BCR风险评分。与后期融合比较，强调跨模态特征在融合层的联合优化。评估使用内部5折嵌套交叉验证和CHIMERA 2025挑战赛的保留验证集。

Result: 内部5折嵌套交叉验证中，方法平均C-index为0.861（σ=0.112）；在CHIMERA 2025保留验证集上C-index为0.7103，优于后期融合配置。

Conclusion: 本文提出的PROFUSEme方法能在术时利用临床、放射与病理多模态信息的中间融合学习跨模态交互，结合Cox回归实现对根治性前列腺切除术后生化复发(BCR)的预测。

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [70] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate: unified framework for animating or replacing characters using skeleton-driven motion, implicit facial features, and a Relighting LoRA for lighting adaptation; achieves SOTA and will be open-sourced.


<details>
  <summary>Details</summary>
Motivation: To create a single framework that can both animate characters (faithfully transferring motion and expression) and replace characters in videos (maintaining appearance while integrating environmental lighting), addressing controllability, expressiveness, and visual seamlessness.

Method: Built on the Wan model with a modified input paradigm that separates reference conditions and generation regions into a common symbolic representation; uses spatially-aligned skeletons for body motion, implicit facial features from source images for expressions, and an auxiliary Relighting LoRA to adapt lighting and color tone during replacement.

Result: Demonstrated state-of-the-art performance in experiments; model weights and source code will be open-sourced.

Conclusion: Wan-Animate presents a unified, controllable framework for both animating characters from images to follow a reference video and replacing characters in a target video while preserving environmental consistency. It leverages spatially-aligned skeleton signals and implicit facial features, and introduces a Relighting LoRA for seamless lighting integration, achieving state-of-the-art results.

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [71] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: 提出基于视觉-语义增强的MOT框架（VSE-MOT），通过三分支架构、MOT-Adapter和VSFM，将视觉-语言模型的全局语义融入跟踪器，在低质量视频上显著提升跟踪效果（+8%~20%）。


<details>
  <summary>Details</summary>
Motivation: 现有MOT在低质量视频（模糊、噪声、压缩等）下性能严重下降，需引入更鲁棒的全局语义信息来增强特征表达以适应真实场景。

Method: 设计了三分支网络，使用视觉-语言模型提取全局视觉语义并与查询向量融合；引入MOT-Adapter用于将语义信息适配到MOT任务，引入VSFM提升特征融合效果。

Result: 在真实世界低质量视频场景上，VSE-MOT较现有方法提升约8%~20%的跟踪指标，同时在常规场景仍保持稳健性能。

Conclusion: 该论文提出的VSE-MOT通过引入视觉-语义信息显著提升了低质量视频的多目标跟踪性能，验证了在恶劣画质场景下的有效性。

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [72] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: 本文首提将DINOv3用于零样本异常检测，通过视觉-文本多模态对比学习、双端轻量适配器和异常感知校准模块，有效弥合领域差异并提升局部异常识别，从而在多项基准上优于或匹配SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统ZSAD多基于CLIP，但新一代视觉基础模型如DINOv3在表征迁移上更强；然而DINOv3存在与异常检测任务的领域偏差和对全局语义的偏袒，导致细微异常难以检测。论文动机是将DINOv3引入ZSAD并解决上述挑战。

Method: 将异常检测表述为多模态对比学习问题：使用DINOv3提取patch token与CLS token，使用CLIP文本编码器生成正常/异常提示的文本向量，在视觉与文本两端加入轻量适配器以缩小领域偏差，并设计AACM引导CLS token关注异常区域，从而增强判别力。

Result: 在八个工业与医疗基准上进行广泛实验，AD-DINOv3持续达到或超过现有SOTA方法，证明其作为通用零样本异常检测框架的有效性。

Conclusion: 该论文提出了基于DINOv3的视觉-语言多模态框架AD-DINOv3，针对无监督领域适配和局部异常识别提出轻量级适配器与异常感知校准模块（AACM），实现了在多个工业与医疗基准上的零样本异常检测上与或超越SOTA的性能。

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [73] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: EMA伪监督+类感知跨模态一致性，提供稳定分段监督与精确模态对齐，提升弱监督音视视频解析性能并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法重视全局预测的对比或协同学习，忽视了稳定的分段级监督与类感知的跨模态对齐，导致时间定位不稳定和模态不一致。

Method: 方法包含两个关键模块：1) EMA-guided伪监督框架，用教师模型的指数移动平均生成可靠的分段级掩码，利用自适应阈值或top-k选择为学生提供稳定的时间监督；2) 类感知跨模态一致性（CMA）损失，在可靠的段-类对上对齐音频和视觉嵌入，从而保持模态间一致性并保护时间结构。

Result: 在LLP和UnAV-100数据集上，多项指标达到或超过现有SOTA，证明了提出方法在时间结构建模和跨模态一致性方面的有效性。

Conclusion: 该论文提出通过EMA引导的伪监督和类感知跨模态一致性损失，改善弱监督音视视频解析的时间级别标注缺失问题，并实现了SOTA性能。

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [74] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 作者将Soft MoE引入跨传感器掩码自编码器并配合主题-气候描述子采样，得到CSMoE，在大幅降低计算成本的同时维持或提升遥感任务表现，展示了高效遥感基础模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型在训练和推理阶段计算复杂度高或表示能力不足，限制了实际应用。通过引入Soft MoE希望在保持表示力的同时提高计算效率。

Method: 在CSMAE基础上引入Soft Mixture-of-Experts，使部分专家专注于特定模态（传感器）而共享跨传感器的表示学习；并提出基于主题-气候描述子的采样策略构建多样化训练集。对场景分类、语义分割和基于内容的图像检索进行了广泛实验。

Result: 实验表明CSMoE在计算需求上减半（或超过两倍效率提升），在场景分类、语义分割、图像检索等任务中保持或优于现有方法，达成了表示能力、精度和计算效率之间的更好平衡。

Conclusion: 该论文提出将Soft MoE机制整合到跨传感器掩码自编码器（CSMAE）中，形成CSMoE，以在保持或提升表征性能的同时显著降低计算开销，从而实现更高的计算效率和良好表现的遥感基础模型。

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [75] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 通过级联配准解决配准困难，使虚拟染色在无/粗配对数据上也能获得稳健且显著的性能提升，降低了数据采集门槛并推动临床应用。


<details>
  <summary>Details</summary>
Motivation: 真实染色难以获得多次染色的完美配对数据，且化学染色会扭曲组织结构；大多数数据为无配对或粗配对，现有方法受限于像素级监督难以推广至临床。

Method: 设计了级联配准模块用于校正生成图像与对齐地面真值之间的空间不匹配，结合现有虚拟染色生成网络进行训练，以实现鲁棒的像素级监管。

Result: 在五个数据集上平均优于最先进模型：内部数据集提升3.2%，外部数据集提升10.1%；在严重错位的数据集中，峰值信噪比（PSNR）相比基线模型提升23.8%。

Conclusion: 提出了一种具级联配准机制的虚拟染色框架，能有效解决输出与真值之间的空间错位，从而在多套数据集上显著优于现有方法。

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [76] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 美颜/平滑滤镜会显著降低深度伪造与人脸合成检测器的效果，需开发对这些变换鲁棒的检测方法。


<details>
  <summary>Details</summary>
Motivation: 研究美颜滤镜对深度伪造与合成攻击检测器可靠性影响，评估脸部增强如何干扰自动与人工识别。

Method: 在若干基准数据集上，对比多种现行顶尖检测器（深度伪造与合成检测），对原始与经不同平滑/美颜滤镜处理的数据进行实验，评估检测率、误报率与指标变化，并分析失效模式。

Result: 在多种基准数据集和最先进检测器上，应用平滑型美颜滤镜后检测性能普遍下降，暴露出检测器对人像增强的脆弱性。

Conclusion: 现有检测器对面部美化处理敏感，应引入鲁棒性训练、数据增强或专门特征以抵御美化滤镜影响。

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [77] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025 launched a multimodal reasoning benchmark with Lens and AdsQA datasets, three competition tracks, 40+ baselines, and 40+ valid submissions; resources are publicly available.


<details>
  <summary>Details</summary>
Motivation: Summarize and benchmark multimodal reasoning approaches and LLMs via a large public challenge to track state-of-the-art and stimulate progress.

Method: Organized a public challenge with tailored test sets, evaluated many baselines, opened competition tracks, collected submissions, and released datasets, code, and rankings.

Result: Released two datasets (Lens and AdsQA), evaluated 40+ baselines, opened three competition tracks (VG-RS, VQA-SA, VR-Ads), received 76 team registrations and 40+ valid submissions, and published data, code, and rankings on GitHub.

Conclusion: MARS2 broadened multimodal reasoning evaluation with real-world and ad-specific tasks, provided datasets and baselines, and fostered community participation to advance MLLMs.

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [78] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 本文构建HAID，从多抽象尺度系统评估抽象图像在分类、分割与检测上的表现，发现抽象图像能保留部分语义但性能随抽象加强显著下降，提示未来需设计更适配抽象表示的方法。


<details>
  <summary>Details</summary>
Motivation: 探究仅由原始形状构成的抽象图像中可保留多少高层语义，并解释其表征为何不及栅格图像。

Method: 构建分层抽象图像数据集HAID（多抽象级别，从细节到极简）；在分类、分割、目标检测等任务上用常规视觉模型训练与评估；分析表征差距与语义保留情况。

Result: HAID显示：随着抽象级别提高，任务性能逐步下降；某些类别在高抽象下仍能被识别；模型在纹理/边缘损失处性能下降显著；抽象图像对模型泛化与轻量化有潜力，但需改进表征/训练策略。

Conclusion: 抽象图像能传递部分语义信息，但在视觉任务上仍落后于栅格图像，差距源于细节、纹理与空间精度损失以及训练数据/模型偏差。

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [79] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++ uses a depth-guided teacher, geometry-aligned student, and uncertainty-aware EMA to address multi-space domain shifts in BEV 3D detection, achieving large gains across cross-domain benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current BEV perception models suffer large performance drops under domain shift in real-world cross-domain scenarios due to accumulation of domain gaps across multiple geometric spaces (2D, 3D voxel, BEV). There's a need for DA methods tailored to multi-view BEV detection.

Method: Design RDT to fuse target LiDAR with reliable depth predictions using uncertainty estimation to create depth-aware signals; design GCS to map multi-space features into a unified geometric embedding to align distributions; introduce UEMA to weigh teacher updates based on uncertainty to prevent error accumulation; train in teacher-student scheme for cross-domain adaptation.

Result: Comprehensive experiments across four cross-domain scenarios show SOTA results, e.g., +12.9% NDS and +9.5% mAP on Day-Night adaptation.

Conclusion: This paper proposes BEVUDA++, a geometric-aware teacher-student framework for domain adaptation in multi-view BEV 3D object detection, combining a Reliable Depth Teacher and Geometric Consistent Student, plus an Uncertainty-guided EMA, to reduce domain shift and improve cross-domain performance.

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [80] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP通过动态超分块合并（dCTS）与早停机制联合降低ViT令牌数与计算成本，在保证≤2%精度损失下实现2.6–4×计算下降与显著吞吐提升，适合高分辨率语义分割。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在语义分割上表现优异但计算与内存开销高，尤其对高分辨率图像（如1024×1024）不友好，需设计高效的令牌压缩与早停机制以提升吞吐与降低复杂度。

Method: 提出了混合令牌压缩框架STEP：dCTS用于灵活将相邻patch合并为superpatches以减少令牌数，编码器中加入early-exits用于提前移除高置信度supertokens，从而降低后续计算。dCTS是轻量CNN策略网络，可在多尺度高分辨率输入上动态决策合并。

Result: 在多项高分辨率语义分割基准上：仅dCTS将令牌数减少2.5倍，计算量减少2.6倍，使用ViT-Large时吞吐提升3.4倍；完整STEP达成最多4×计算复杂度下降、1.7×推理速度提升，精度最大下降≤2.0%，并能在早期层对多达40%令牌提前停止预测。

Conclusion: STEP通过结合dCTS（基于轻量CNN的动态超分块合并）与早停分支，实现了在高分辨率语义分割任务中显著降低ViT计算与内存开销，同时仅带来可控的精度下降。

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [81] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出用于高帧率视频理解的GRT方法与DIVE基准，通过运动补偿跳过静态像素并融合场景内token，实现高效、可扩展的密集时序视频理解。


<details>
  <summary>Details</summary>
Motivation: 当前VLLM普遍采用低帧率采样，丢弃大量时序信息以避免对每帧标记化的高昂开销，但在需要精确时序对齐的任务（如讲座理解）中效果不足，因此需要在高FPS下保持效率的标记化策略和适配的基准。

Method: GRT包含两部分：1）Motion-Compensated Inter-Gated Tokenization：利用像素级运动估计跳过静态区域，从而实现标记数与计算的亚线性增长；2）Semantic-Scene Intra-Tokenization Merging：在场景内将静态区域的token进行语义级融合，进一步减少冗余。

Result: 在作者提出的新基准DIVE上，GRT比更大VLLM基线表现更好，且随着FPS提高，模型性能持续提升，验证了高时序密度信息的重要性与GRT的可扩展性。

Conclusion: 该论文提出了一种用于高帧率视频理解的高效标记化方法Gated Residual Tokenization（GRT），并构建了新的密集时序推理基准DIVE。通过运动补偿与语义融合两阶段策略，GRT在保持动态语义的同时显著降低了标记化的时间和标记数，实验证明在DIVE上优于更大规模的VLLM基线并且随FPS增加表现更好。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: Cinéaste: 3,119 challenging QA pairs over 200 movies for fine-grained long-form narrative reasoning; GPT-4o-generated Qs and strict filtering; current MLLMs perform poorly (best 63.15%), highlighting struggles with long-range temporal reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on short clips or template questions, leaving gap in fine-grained reasoning over long-form narratives; need to evaluate deep narrative comprehension.

Method: Collected 3,119 MCQ pairs from 1,805 scenes across 200 movies; used GPT-4o to generate context-rich questions combining visual descriptions, captions, titles, summaries; applied two-stage filtering: Context-Independence and Contextual Veracity.

Result: Top open-source model achieves 63.15% accuracy; experiments show MLLMs underperform, long-range temporal reasoning is main bottleneck; demonstrates need for improved long-form movie comprehension models.

Conclusion: Paper introduces Cinéaste, a benchmark for long-form movie understanding revealing MLLMs struggle, esp. long-range temporal reasoning.

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam是首个多学科文本到图像考试基准（1000题、10科、四层分类、标准图和细粒度评分），揭示当前图像生成模型在严谨绘图任务上的不足，促进对整合知识、推理与生成能力的评估。


<details>
  <summary>Details</summary>
Motivation: 现有生成基准缺乏对严谨绘图/考试式图像生成能力的评估，难以衡量模型在整合知识、推理与生成方面的能力；因此需要一个更接近专家级考试的基准来推动研究。

Method: 构建1000道跨10科目的考试式文本到图像题目，设计四层题目分类法，为每题提供标准图像与细粒度评分点，以便对语义正确性和视觉合理性进行精确评估；并用多种现有图像生成模型进行基线测试。

Result: 在GenExam上，领先模型（如GPT-Image-1、Gemini-2.5-Flash-Image）严格评分均低于15%，大多数模型接近0%，显示出当前模型在该基准上的显著不足。

Conclusion: GenExam提出了一个严谨的多学科文本到图像考试基准，强调语义正确性与视觉合理性的精细化评估，实验证明现有模型在该任务上表现很差，表明该基准具有挑战性并推动通用人工智能研究。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [84] [The NIAID Discovery Portal: A Unified Search Engine for Infectious and Immune-Mediated Disease Datasets](https://arxiv.org/abs/2509.13524)
*Ginger Tsueng,Emily Bullen,Candice Czech,Dylan Welzel,Leandro Collares,Jason Lin,Everaldo Rodolpho,Zubair Qazi,Nichollette Acosta,Lisa M. Mayer,Sudha Venkatachari,Zorana Mitrović Vučičević,Poromendro N. Burman,Deepti Jain,Jack DiGiovanna,Maria Giovanni,Asiyah Lin,Wilbert Van Panhuis,Laura D. Hughes,Andrew I. Su,Chunlei Wu*

Main category: cs.DB

TL;DR: NIAID数据生态发现门户通过整合和标准化异构元数据，为IID研究者提供了一个易用的统一搜索与API访问平台，提升数据发现与重用，促进科学研究与公共投资回报。


<details>
  <summary>Details</summary>
Motivation: 许多有价值的数据集难以被发现，影响二次利用与科学推进。需要一个对不同技术水平研究者都友好的集中式检索平台来提升数据的可发现性与可重用性。

Method: 收集来自领域专门与通用数据存储库的异构元数据，通过标准化关键元数据字段、构建用户友好的过滤器、预置查询和数据集集合，并提供文档与API实现程序化访问。

Result: 门户整合了超过400万条与IID研究相关的数据集，支持流行病学、临床与多组学等多种资源的探索与精确检索，提供可视化过滤、预置查询、集合以及开放API，促进假设生成与比较分析。

Conclusion: 该门户通过统一检索和元数据标准化，显著降低了研究者发现和重用传染病与免疫相关数据的门槛，增强了数据可发现性、可访问性与可重用性，最大化了公共研究投入的影响。

Abstract: The NIAID Data Ecosystem Discovery Portal (https://data.niaid.nih.gov)
provides a unified search interface for over 4 million datasets relevant to
infectious and immune-mediated disease (IID) research. Integrating metadata
from domain-specific and generalist repositories, the Portal enables
researchers to identify and access datasets using user-friendly filters or
advanced queries, without requiring technical expertise. The Portal supports
discovery of a wide range of resources, including epidemiological, clinical,
and multi-omic datasets, and is designed to accommodate exploratory browsing
and precise searches. The Portal provides filters, prebuilt queries, and
dataset collections to simplify the discovery process for users. The Portal
additionally provides documentation and an API for programmatic access to
harmonized metadata. By easing access barriers to important biomedical
datasets, the NIAID Data Ecosystem Discovery Portal serves as an entry point
for researchers working to understand, diagnose, or treat IID.
  Valuable datasets are often overlooked because they are difficult to locate.
The NIAID Data Ecosystem Discovery Portal fills this gap by providing a
centralized, searchable interface that empowers users with varying levels of
technical expertise to find and reuse data. By standardizing key metadata
fields and harmonizing heterogeneous formats, the Portal improves data
findability, accessibility, and reusability. This resource supports hypothesis
generation, comparative analysis, and secondary use of public data by the IID
research community, including those funded by NIAID. The Portal supports data
sharing by standardizing metadata and linking to source repositories, and
maximizes the impact of public investment in research data by supporting
scientific advancement via secondary use.

</details>


### [85] [Tractability Frontiers of the Shapley Value for Aggregate Conjunctive Queries](https://arxiv.org/abs/2509.13565)
*Christoph Standke,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 针对不同聚合函数，鉴定出对局部值函数可多项式计算Shapley值的最大层次化CQ类，并证明类外情况可使问题#P-难；不同聚合函数对应不同的层次化推广（all-hierarchical或q-hierarchical）。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于sum与count的结果，回答Livshits等人提出的关于min、max、count-distinct、average及quantile等聚合函数的复杂性开放问题，刻画何种CQ结构允许高效计算Shapley值。

Method: 通过复杂性证明与构造，针对每个聚合函数定义相应的层次化CQ类，证明该类上Shapley值可多项式时间计算；并构造具体局部值函数和归约证明，展示类外问题为#P-难，从而证明最大性。

Result: 本文研究了在聚合连接查询(aggregate conjunctive query)中计算元组Shapley值的复杂性，重点对比不同聚合函数（如sum、count、min、max、count-distinct、average、quantile）下的可处置类与#P-难类。主要结论是：对每种聚合函数，给出了一个对局部值函数(local value function)适用的、多项式可解的层次化CQ类，并证明该类是最大化的——对于类外任一CQ，都存在一个局部值函数使得Shapley值变为#P-难。此外，不同聚合函数对应不同的层次化类推广：max、min与count-distinct对应all-hierarchical类，而average与quantile对应更窄的q-hierarchical类。

Conclusion: 为每个常见聚合函数给出一个最大可解的层次化CQ类（针对局部值函数）；类外存在使Shapley值计算#P-难的局部值函数；max/min/count-distinct对应all-hierarchical，average/quantile对应q-hierarchical。

Abstract: In recent years, the Shapley value has emerged as a general game-theoretic
measure for assessing the contribution of a tuple to the result of a database
query. We study the complexity of calculating the Shapley value of a tuple for
an aggregate conjunctive query, which applies an aggregation function to the
result of a conjunctive query (CQ) based on a value function that assigns a
number to each query answer. Prior work by Livshits, Bertossi, Kimelfeld, and
Sebag (2020) established that this task is #P-hard for every nontrivial
aggregation function when the query is non-hierarchical with respect to its
existential variables, assuming the absence of self-joins. They further showed
that this condition precisely characterizes the class of intractable CQs when
the aggregate function is sum or count. In addition, they posed as open
problems the complexity of other common aggregate functions such as min, max,
count-distinct, average, and quantile (including median). Towards the
resolution of these problems, we identify for each aggregate function a class
of hierarchical CQs where the Shapley value is tractable with every value
function, as long as it is local (i.e., determined by the tuples of one
relation). We further show that each such class is maximal: for every CQ
outside of this class, there is a local (easy-to-compute) value function that
makes the Shapley value #P-hard. Interestingly, our results reveal that each
aggregate function corresponds to a different generalization of the class of
hierarchical CQs from Boolean to non-Boolean queries. In particular, max, min,
and count-distinct match the class of CQs that are all-hierarchical (i.e.,
hierarchical with respect to all variables), and average and quantile match the
narrower class of q-hierarchical CQs introduced by Berkholz, Keppeler, and
Schweikardt (2017) in the context of the fine-grained complexity of query
answering.

</details>


### [86] [XASDB -- Design and Implementation of an Open-Access Spectral Database](https://arxiv.org/abs/2509.13566)
*Denis Spasyuk*

Main category: cs.DB

TL;DR: XASDB is a web platform (Node.js/MongoDB) hosting 1000+ XAS spectra with in-browser processing (XASproc) and viewer (XASVue), promoting FAIR data, cross-facility compatibility, and enabling LCF and ML workflows


<details>
  <summary>Details</summary>
Motivation: Address growing volume/heterogeneity of XAS data, enable FAIR sharing, cross-beamline interoperability, and browser-based processing

Method: Web-based XAS data management and processing platform

Result: XASDB: Node.js/MongoDB backend, >1000 spectra (40 elements, 324 compounds), XASproc JS for in-browser processing, XASVue viewer, standardized outputs and metadata, supports LCF, ML, education

Conclusion: Web-centric infrastructure like XASDB can democratize XAS analysis, improve data reuse and collaboration across materials, environmental, chemical, and biological research

Abstract: The increasing volume and complexity of X-ray absorption spectroscopy (XAS)
data generated at synchrotron facilities worldwide require robust
infrastructure for data management, sharing, and analysis. This paper
introduces the XAS Database (XASDB), a comprehensive web-based platform
developed and hosted by the Canadian Light Source (CLS). The database houses
more than 1000 reference spectra spanning 40 elements and 324 chemical
compounds. The platform employs a Node.js/MongoDB architecture designed to
handle diverse data formats from multiple beamlines and synchrotron facilities.
A key innovation is the XASproc JavaScript library, which enables browser-based
XAS data processing including normalization, background sub- traction, extended
X-ray absorption fine structure (EXAFS) extraction, and preliminary analysis
traditionally limited to desktop applications. The integrated XASVue spectral
viewer provides installation-free data visualization and analysis with broad
accessibility across devices and operating systems. By offering standardized
data output, comprehensive metadata, and integrated analytical ca- pabilities,
XASDB facilitates collaborative research and promotes FAIR (Findable,
Accessible, In- teroperable, and Reusable) data principles. The platform serves
as a valuable resource for linear combination fitting (LCF) analysis, machine
learning applications, and educational purposes. This initiative demonstrates
the potential for web-centric approaches in XAS data analysis, accelerating
advances in materials science, environmental research, chemistry, and biology.

</details>


### [87] [Algorithms for Optimizing Acyclic Queries](https://arxiv.org/abs/2509.14144)
*Zheng Luo,Wim Van den Broeck,Guy Van den Broeck,Yisu Remy Wang*

Main category: cs.DB

TL;DR: 为无环查询（alpha、Berge、gamma）分别提出枚举所有连接树、用MCS构造唯一最浅连接树并将左深线性计划转化为连接树的三种方法，支持成本优化、并行执行与复用二元连接优化基础设施。


<details>
  <summary>Details</summary>
Motivation: 传统查询优化侧重于二元连接算子，但理论最优的连接算法（如Yannakakis）依赖于连接树，与二元连接的算子树不同，需要新的优化技术，因此需要有效构建连接树的方法以支持成本优化、并行执行和复用已有基础设施。

Method: 1) 通过编辑操作以摊销常数延迟枚举alpha-无环查询的所有连接树；2) 利用Tarjan和Yannakakis的最大基数搜索（MCS）算法为Berge-无环查询在任意关系为根时构造唯一的最浅连接树；3) 提出简单算法将任何连通的左深线性计划转化为gamma-无环查询的连接树。

Result: 给出可用于成本基优化器的枚举算法、证明MCS构造唯一最浅连接树以便并行化，以及展示左深计划到连接树的转换可行性，从而在不同无环查询类（alpha、Berge、gamma）上提供实用构建手段。

Conclusion: 本文提出了三种构建无环查询连接树的方法，分别用于枚举、构造最浅连接树以及将左深线性计划转化为连接树，从而为基于理论最优算法的连接树优化提供工具，能支持成本优化并启用并行执行与复用现有二元连接优化基础设施。

Abstract: Most research on query optimization has centered on binary join algorithms
like hash join and sort-merge join. However, recent years have seen growing
interest in theoretically optimal algorithms, notably Yannakakis' algorithm.
These algorithms rely on join trees, which differ from the operator trees for
binary joins and require new optimization techniques. We propose three
approaches to constructing join trees for acyclic queries. First, we give an
algorithm to enumerate all join trees of an alpha-acyclic query by edits with
amortized constant delay, which forms the basis of a cost-based optimizer for
acyclic joins. Second, we show that the Maximum Cardinality Search algorithm by
Tarjan and Yannakakis constructs a unique shallowest join tree, rooted at any
relation, for a Berge-acyclic query; this tree enables parallel execution of
large join queries. Finally, we prove that any connected left-deep linear plan
for a gamma-acyclic query can be converted into a join tree by a simple
algorithm, allowing reuse of optimization infrastructure developed for binary
joins.

</details>
