<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 97]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Hybrid Deep Learning for Hyperspectral Single Image Super-Resolution](https://arxiv.org/abs/2510.00033)
*Usman Muhammad,Jorma Laaksonen*

Main category: cs.CV

TL;DR: 提出SSUF模块与Spatial-Spectral Gradient Loss，结合光谱解混和空间-光谱特征提取，嵌入ResNet式2D卷积网络，能在保持光谱保真度的同时提升空间超分辨，且模型更轻量，三数据集实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型难以同时恢复细致的空间结构和跨波段的一致光谱信息，高光谱SISR任务对光谱保真度要求高，需要新的模块和损失来平衡空间细节与光谱完整性。

Method: 设计了Spectral-Spatial Unmixing Fusion (SSUF)模块，将光谱解混与光谱-空间特征提取结合，并嵌入ResNet式的2D卷积网络；同时提出Spatial-Spectral Gradient Loss，将MSE与空间和光谱方向的梯度损失结合用于训练。

Result: 在三个公开高光谱遥感数据集上的实验显示，所提混合模型在性能上具有竞争力，同时降低了模型复杂度（参数或计算量），证明了方法的有效性。

Conclusion: 该文提出SSUF模块与空间-光谱梯度损失，能在2D卷积架构中提升高光谱单幅超分辨重建的空间细节和光谱保真度，实验在三个数据集上表明在降低模型复杂度的同时取得有竞争力的性能。

Abstract: Hyperspectral single image super-resolution (SISR) is a challenging task due
to the difficulty of restoring fine spatial details while preserving spectral
fidelity across a wide range of wavelengths, which limits the performance of
conventional deep learning models. To address this challenge, we introduce
Spectral-Spatial Unmixing Fusion (SSUF), a novel module that can be seamlessly
integrated into standard 2D convolutional architectures to enhance both spatial
resolution and spectral integrity. The SSUF combines spectral unmixing with
spectral--spatial feature extraction and guides a ResNet-based convolutional
neural network for improved reconstruction. In addition, we propose a custom
Spatial-Spectral Gradient Loss function that integrates mean squared error with
spatial and spectral gradient components, encouraging accurate reconstruction
of both spatial and spectral features. Experiments on three public remote
sensing hyperspectral datasets demonstrate that the proposed hybrid deep
learning model achieves competitive performance while reducing model
complexity.

</details>


### [2] [Review of Hallucination Understanding in Large Language and Vision Models](https://arxiv.org/abs/2510.00034)
*Zhengyi Ho,Siyuan Liang,Dacheng Tao*

Main category: cs.CV

TL;DR: 本文通过任务-模态交错方法，提出统一多层框架解析图文生成模型幻觉根源，认为主要源自数据分布模式与继承偏差，为更稳健的解决策略奠定基础。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言与视觉模型在现实应用中的广泛部署，幻觉问题造成的错误传播带来显著负面影响，现有研究零散且对根源理解不足，亟需统一的理论框架。

Method: 通过任务-模态交错的方法对现有文献进行梳理，构建多层次分类框架并分析幻觉在模型训练、数据、推理等生命周期环节中的成因联系。

Result: 提出了一个统一的多层次幻觉表征框架，识别出幻觉常由数据分布规律和遗传偏差导致，并据此指出未来研究方向以提高生成式AI的鲁棒性。

Conclusion: 本文综述提出了一个统一的多层框架来描述图像与文本生成模型中的幻觉问题，并将其与模型生命周期中的具体机制相联系，认为幻觉多源于数据分布的可预测模式和继承偏差，旨在为更有效的解决方案奠定理论基础。

Abstract: The widespread adoption of large language and vision models in real-world
applications has made urgent the need to address hallucinations -- instances
where models produce incorrect or nonsensical outputs. These errors can
propagate misinformation during deployment, leading to both financial and
operational harm. Although much research has been devoted to mitigating
hallucinations, our understanding of it is still incomplete and fragmented.
Without a coherent understanding of hallucinations, proposed solutions risk
mitigating surface symptoms rather than underlying causes, limiting their
effectiveness and generalizability in deployment. To tackle this gap, we first
present a unified, multi-level framework for characterizing both image and text
hallucinations across diverse applications, aiming to reduce conceptual
fragmentation. We then link these hallucinations to specific mechanisms within
a model's lifecycle, using a task-modality interleaved approach to promote a
more integrated understanding. Our investigations reveal that hallucinations
often stem from predictable patterns in data distributions and inherited
biases. By deepening our understanding, this survey provides a foundation for
developing more robust and effective solutions to hallucinations in real-world
generative AI systems.

</details>


### [3] [On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations](https://arxiv.org/abs/2510.00037)
*Jianing Guo,Zhenhong Wu,Chang Tu,Yiyao Ma,Xiangqi Kong,Zhiqian Liu,Jiaming Ji,Shuning Zhang,Yuanpei Chen,Kai Chen,Xianglong Liu,Qi Dou,Yaodong Yang,Huijie Zhao,Weifeng Lv,Simin Li*

Main category: cs.CV

TL;DR: 提出RobustVLA：结合输出端最坏情况优化与输入端一致性约束并用UCB自动识别有害噪声，显著提升跨模态扰动下VLA鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA对真实世界多模态扰动（动作、指令、环境、观测）缺乏系统性鲁棒性研究与方法，尤其动作模态最脆弱，需要跨模态鲁棒性策略。

Method: 1) 在17种跨四模态扰动下评估现有VLA；2) 对输出实施最坏情况噪声的离线鲁棒优化（类似对抗训练、标签平滑和异常惩罚）；3) 对输入实施语义保持变换下一致性约束；4) 将多扰动视为多臂赌博机并用上置信界算法选择最有害噪声。

Result: 在LIBERO数据集上，RobustVLA比基线在17种扰动上分别提升pi0骨干12.6%、OpenVLA骨干10.4%，比现有视觉鲁棒VLA推理快50.6倍；在混合扰动下提升10.4%；在有限示范的真实FR5机器人上，四模态扰动下提升65.6%。

Conclusion: 论文提出了针对多模态扰动的VLA模型鲁棒性问题，提出RobustVLA在输入和输出两端同时进行鲁棒优化，并通过多臂赌博机自动识别最有害噪声，取得明显性能提升。

Abstract: In Vision-Language-Action (VLA) models, robustness to real-world
perturbations is critical for deployment. Existing methods target simple visual
disturbances, overlooking the broader multi-modal perturbations that arise in
actions, instructions, environments, and observations. Here, we first evaluate
the robustness of mainstream VLAs under 17 perturbations across four
modalities. We find (1) actions as the most fragile modality, (2) Existing
visual-robust VLA do not gain robustness in other modality, and (3) pi0
demonstrates superior robustness with a diffusion-based action head. To build
multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA
inputs and outputs. For output robustness, we perform offline robust
optimization against worst-case action noise that maximizes mismatch in flow
matching objective. This can be seen as adversarial training, label smoothing,
and outlier penalization. For input robustness, we enforce consistent actions
across input variations that preserve task semantics. To account for multiple
perturbations, we formulate robustness as a multi-armed bandit problem and
apply an upper confidence bound algorithm to automatically identify the most
harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers
absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the
OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference
than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations.
Our RobustVLA is particularly effective on real-world FR5 robot with limited
demonstrations, showing absolute gains by 65.6% under perturbations of four
modalities.

</details>


### [4] [Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models](https://arxiv.org/abs/2510.00040)
*Junjie Li,Ziao Wang,Jianghong Ma,Xiaofeng Zhang*

Main category: cs.CV

TL;DR: CADC通过梯度轨迹发现内在能力并将样本归因于这些能力，进而进行能力感知的平衡选择和分阶段编排；在仅使用少量数据的情况下还能超越全量训练表现。


<details>
  <summary>Details</summary>
Motivation: 现有的启发式数据筛选策略将模型视为黑盒，忽略了支配学习过程的内在能力，导致在数据预算受限时模型性能退化。作者希望通过显式分析和利用这些内在能力，实现可控且高效的指令微调数据构建。

Method: CADC包含三个主要步骤：1) 利用梯度基的学习轨迹无监督发现内在能力（intrinsic capabilities）；2) 用影响函数或类似的影响力估计方法将训练样本归因到已发现的能力上；3) 基于归因结果，执行平衡选择和分阶段序列化形成能力感知的课程，以此构建更小但更高效的指令微调集。

Result: 在多模态基准测试上，使用仅5%原始数据的CADC训练结果超过了使用全部数据进行训练的基线，证明了能力本质在模型学习中的核心作用，并表明CADC在指令数据策划上的有效性。

Conclusion: 该论文提出了Capability-Attributed Data Curation (CADC)框架，通过无监督地从梯度学习轨迹中发现模型的内在能力，并用影响力估计将训练样本关联到这些能力，进而进行能力感知的数据筛选与分阶段编排，从而在大幅减少指令微调数据的情况下提升多模态基准表现。

Abstract: Large vision-language models (VLMs) achieve strong benchmark performance, but
controlling their behavior through instruction tuning remains difficult.
Reducing the budget of instruction tuning dataset often causes regressions, as
heuristic strategies treat models as black boxes and overlook the latent
capabilities that govern learning. We introduce Capability-Attributed Data
Curation (CADC), a framework that shifts curation from task-specific heuristics
to intrinsic capability analysis. CADC discovers intrinsic capabilities in an
unsupervised manner from gradient-based learning trajectories, attributes
training data to these capabilities via influence estimation, and curates
capability-aware curricula through balanced selection and staged sequencing.
This transforms black-box instruction tuning into a controllable,
capability-driven process. With as little as 5% of the original data, CADC
surpasses full-data training on multimodal benchmarks. These results validate
intrinsic capabilities as the fundamental building blocks of model learning and
establish CADC as a principle paradigm for instruction data curation.

</details>


### [5] [Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness](https://arxiv.org/abs/2510.00041)
*Yuchen Song,Andong Chen,Wenxin Zhu,Kehai Chen,Xuefeng Bai,Muyun Yang,Tiejun Zhao*

Main category: cs.CV

TL;DR: 提出C^3B：一个基于漫画的多文化、多语言、多任务基准（2000+图、18000+QA），揭示当前MLLM在文化理解与生成上的明显不足，呼吁研究提升文化敏感能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准任务设计难度不足且缺乏跨语言、跨文化设置，大多数使用的实景图像仅反映单一文化，使得评测对模型过于宽松；因此需要更具挑战性且多文化的基准来测评和推动MLLM的文化意识能力。

Method: 作者构建了包含2000+漫画图像与18000+问答对的跨文化、多任务、多语种基准C^3B，任务从图像识别到文化冲突理解再到文化内容生成，设计了不同难度梯度，并用11个开源多模态大模型进行评估以比较其与人类表现的差距。

Result: 实验显示11个开源MLLM在C^3B上均显著落后于人类，且在更高阶的文化冲突理解与生成任务上表现尤其差，表明C^3B能有效区分模型能力并揭示关键弱点。

Conclusion: 该论文通过构建C^3B基准，揭示了现有多模态大模型在跨文化理解与生成任务上的显著不足，表明当前模型在文化敏感性与复杂语境理解方面仍有大量改进空间。

Abstract: Cultural awareness capabilities has emerged as a critical capability for
Multimodal Large Language Models (MLLMs). However, current benchmarks lack
progressed difficulty in their task design and are deficient in cross-lingual
tasks. Moreover, current benchmarks often use real-world images. Each
real-world image typically contains one culture, making these benchmarks
relatively easy for MLLMs. Based on this, we propose C$^3$B ($\textbf{C}$omics
$\textbf{C}$ross-$\textbf{C}$ultural $\textbf{B}$enchmark), a novel
multicultural, multitask and multilingual cultural awareness capabilities
benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs,
constructed on three tasks with progressed difficulties, from basic visual
recognition to higher-level cultural conflict understanding, and finally to
cultural content generation. We conducted evaluations on 11 open-source MLLMs,
revealing a significant performance gap between MLLMs and human performance.
The gap demonstrates that C$^3$B poses substantial challenges for current
MLLMs, encouraging future research to advance the cultural awareness
capabilities of MLLMs.

</details>


### [6] [Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions](https://arxiv.org/abs/2510.00045)
*Franck Vandewiele,Remi Synave,Samuel Delepoulle,Remi Cozot*

Main category: cs.CV

TL;DR: TL;DR：六个开源文本到图像模型在医疗职业生成上存在明显且模型特异的性别偏见，提示词会显著影响结果，需通过偏见感知设计、平衡默认设置和用户指引来缓解。


<details>
  <summary>Details</summary>
Motivation: 动机：文本到图像模型被广泛用于专业和创意场景，但可能嵌入并放大社会偏见。研究旨在量化这些模型在医疗职业性别表示上的偏差，评估不同模型和提示词对结果的影响，进而为设计更公平的生成模型提供建议。

Method: 方法：对六个开源模型（HunyuanImage 2.1、HiDream-I1-dev、Qwen-Image、FLUX.1-dev、Stable-Diffusion 3.5 Large、Stable-Diffusion-XL）使用精心设计的提示词生成图像。对五个医院相关职业（心脏科医生、医院院长、护士、急救人员、外科医生）和五类人像限定词（空、corporate、neutral、aesthetic、beautiful）组合，每组生成100张图像并标注性别分布进行统计分析。

Result: 结果：1) 护士被所有模型唯一生成为女性，外科医生多为男性；2) Qwen-Image与SDXL表现出强烈的男性主导，HiDream-I1-dev结果混合，FLUX.1-dev大多偏女性；3) HunyuanImage 2.1和SD3.5也存在刻板偏见，但对提示词敏感度不同；4) 肖像限定词会改变性别分布，如corporate强化男性形象，beautiful偏向女性；5) 提示词敏感性在模型间差异大。

Conclusion: 本文结论：现有开源文本到图像模型在医疗职业人物生成中普遍存在性别刻板偏见，且这种偏见具有模型特异性和对提示词敏感性。

Abstract: Text-to-image (TTI) models are increasingly used in professional,
educational, and creative contexts, yet their outputs often embed and amplify
social biases. This paper investigates gender representation in six
state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev,
Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL.
Using carefully designed prompts, we generated 100 images for each combination
of five hospital-related professions (cardiologist, hospital director, nurse,
paramedic, surgeon) and five portrait qualifiers ("", corporate, neutral,
aesthetic, beautiful).
  Our analysis reveals systematic occupational stereotypes: all models produced
nurses exclusively as women and surgeons predominantly as men. However,
differences emerge across models: Qwen-Image and SDXL enforce rigid male
dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in
most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce
gender stereotypes but with varying degrees of sensitivity to prompt
formulation. Portrait qualifiers further modulate gender balance, with terms
like corporate reinforcing male depictions and beautiful favoring female ones.
Sensitivity varies widely: Qwen-Image remains nearly unaffected, while
FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.
  These findings demonstrate that gender bias in TTI models is both systematic
and model-specific. Beyond documenting disparities, we argue that prompt
wording plays a critical role in shaping demographic outcomes. The results
underscore the need for bias-aware design, balanced defaults, and user guidance
to prevent the reinforcement of occupational stereotypes in generative AI.

</details>


### [7] [Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models](https://arxiv.org/abs/2510.00046)
*Xiaotian Zou*

Main category: cs.CV

TL;DR: 作者提出RLStealer，通过强化学习和相似度奖励从少量示例图像逆向恢复提示模板，攻击成本显著低于现有方法，揭示了提示交易中的重要安全威胁。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs推动设计工作流的发展，提示提示交易市场兴起，但交易的提示自身可能被窃取，带来知识产权和商业风险，需评估并展示该攻击的可行性。

Method: 将提示窃取建模为序列决策问题，使用强化学习在提示空间中探索；采用多种基于相似性的反馈信号作为奖励函数，以提升搜索效率和恢复质量。

Result: 在公开基准上，RLStealer取得了最先进的性能，并将总攻击成本降低到现有基线的13%以下；进一步分析显示其可跨不同图像风格泛化，高效窃取未见提示模板。

Conclusion: 本文揭示了提示（prompt）在多模态大型语言模型（MLLMs）驱动的图像生成产业中存在被盗用的安全风险，并提出了RLStealer，一种基于强化学习的提示逆向框架，能够用少量示例图像恢复出提示模板。

Abstract: Multimodal Large Language Models (MLLMs) have transformed text-to-image
workflows, allowing designers to create novel visual concepts with
unprecedented speed. This progress has given rise to a thriving prompt trading
market, where curated prompts that induce trademark styles are bought and sold.
Although commercially attractive, prompt trading also introduces a largely
unexamined security risk: the prompts themselves can be stolen.
  In this paper, we expose this vulnerability and present RLStealer, a
reinforcement learning based prompt inversion framework that recovers its
template from only a small set of example images. RLStealer treats template
stealing as a sequential decision making problem and employs multiple
similarity based feedback signals as reward functions to effectively explore
the prompt space. Comprehensive experiments on publicly available benchmarks
demonstrate that RLStealer gets state-of-the-art performance while reducing the
total attack cost to under 13% of that required by existing baselines. Our
further analysis confirms that RLStealer can effectively generalize across
different image styles to efficiently steal unseen prompt templates. Our study
highlights an urgent security threat inherent in prompt trading and lays the
groundwork for developing protective standards in the emerging MLLMs
marketplace.

</details>


### [8] [Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations](https://arxiv.org/abs/2510.00047)
*Sihao Ding,Santosh Vasa,Aditi Ramadwar*

Main category: cs.CV

TL;DR: 提出EDCT：把模型解释作为可检验假设，用生成式反事实编辑与LLM分析衡量解释的因果一致性，实验证明能发现VLM解释的不可信性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM生成的自然语言解释虽流畅可信，但可能与实际驱动预测的因果因素不一致，带来技术与治理风险，因此需要自动化、可审计的验证方法。

Method: EDCT流程包括：获取模型回答与NLE；将NLE解析为可测试的视觉概念；利用生成式修复(inpainting)制作针对性反事实编辑；用LLM辅助分析回答与NLE在反事实上的变化，计算Counterfactual Consistency Score (CCS)。

Result: 在120个精心挑选的OK-VQA样本和多款VLM上，EDCT发现了显著的可置信度缺口，并生成了对监管友好的审计证据，指出被引用概念在因果测试中失效的情况。

Conclusion: EDCT能有效检测VLM NLE的不可证伪性与不可信性，通过把模型自身解释当作可检验假设，提高了对因果驱动因素的审计能力。

Abstract: Vision-Language Models (VLMs) often produce fluent Natural Language
Explanations (NLEs) that sound convincing but may not reflect the causal
factors driving predictions. This mismatch of plausibility and faithfulness
poses technical and governance risks. We introduce Explanation-Driven
Counterfactual Testing (EDCT), a fully automated verification procedure for a
target VLM that treats the model's own explanation as a falsifiable hypothesis.
Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2)
parses the NLE into testable visual concepts, (3) generates targeted
counterfactual edits via generative inpainting, and (4) computes a
Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes
in both answers and explanations. Across 120 curated OK-VQA examples and
multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides
regulator-aligned audit artifacts indicating when cited concepts fail causal
tests.

</details>


### [9] [HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling](https://arxiv.org/abs/2510.00054)
*Xianjie Liu,Yiman Hu,Yixiong Zou,Liang Wu,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 作者指出高分辨率下性能下降源自背景干扰而非小目标，提出训练-free的HiDe框架通过注意力与布局解耦去噪，显著提升MLLMs在HR图像上的表现并节省内存。


<details>
  <summary>Details</summary>
Motivation: 现有方法多归因于感知限制即难以识别小物体而采用放大策略，但作者发现放大策略的治疗作用并非针对物体大小，而是为了避开复杂背景的干扰。因此提出通过解耦关键token与视觉区域来精确对齐并剔除背景干扰的替代方案。

Method: HiDe包含两步解耦：Token-wise Attention Decoupling (TAD)通过分解问题token来识别关键信息token并利用其注意力权重对齐目标视觉区域；Layout-Preserving Decoupling (LPD)将目标区域从背景中解耦并重构紧凑表示，保留必要空间布局同时去除背景干扰。该方法无需训练，且经过优化后比之前的无训练方法节省75%内存。

Result: HiDe在多个高分辨率基准（V*Bench, HRBench4K, HRBench8K）上取得新的SOTA，Qwen2.5-VL 7B和InternVL3 8B在V*Bench上分别达92.1%和91.6%，超越一些强化学习方法，并在内存使用上优于此前方法。代码已开源。

Conclusion: 本文认为高分辨率图像任务中MLLMs性能下降的主要原因是复杂背景干扰，而非小目标识别困难。提出了训练-free的分层解耦框架HiDe以缓解背景干扰，从而提升对目标区域的对齐与理解。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides in
visual understanding tasks. However, their performance on high-resolution
images remains suboptimal. While existing approaches often attribute this
limitation to perceptual constraints and argue that MLLMs struggle to recognize
small objects, leading them to use "zoom in" strategies for better detail, our
analysis reveals a different cause: the main issue is not object size, but
rather caused by complex background interference. We systematically analyze
this "zoom in" operation through a series of decoupling experiments and propose
the Hierarchical Decoupling Framework (HiDe), a training-free framework that
uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and
identify the key information tokens, then leverages their attention weights to
achieve precise alignment with the target visual regions. Subsequently, it
employs Layout-Preserving Decoupling (LPD) to decouple these regions from the
background and reconstructs a compact representation that preserves essential
spatial layouts while eliminating background interference. HiDe sets a new SOTA
on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B
to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After
optimization, HiDe uses 75% less memory than the previous training-free
approach. Code is provided in https://github.com/Tennine2077/HiDe.

</details>


### [10] [FSDENet: A Frequency and Spatial Domains based Detail Enhancement Network for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2510.00059)
*Jiahao Fu,Yinfeng Yu,Liejun Wang*

Main category: cs.CV

TL;DR: FSDENet通过融合空间多尺度特征、FFT全局频域信息与Haar小波分解的高低频特征，增强边界识别与灰度鲁棒性，在四个遥感分割数据集上达SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 遥感影像分割中边界模糊和灰度变化（阴影、低对比度）导致语义边界歧义，需同时利用空间细节与频域边缘敏感性以提升边界和过渡区域分割效果。

Method: 模型使用空间处理模块提取多尺度细粒度语义特征；在全局映射中引入FFT以增强在灰度变化下的全球表示能力；用Haar小波将特征分解为高低频分量，利用高频增强边界信息并融合回网络。

Result: 在LoveDA、Vaihingen、Potsdam和iSAID四个数据集上，FSDENet在边界区域与灰度过渡区表现优异，总体分割精度达到或超越现有SOTA方法（论文宣称）。

Conclusion: 本文提出FSDENet，结合空间域多尺度特征与频域信息（FFT与Haar小波）提升边界分割鲁棒性，尤其在灰度变化区域取得显著性能提升，宣称在四个数据集上达SOTA。

Abstract: To fully leverage spatial information for remote sensing image segmentation
and address semantic edge ambiguities caused by grayscale variations (e.g.,
shadows and low-contrast regions), we propose the Frequency and Spatial Domains
based Detail Enhancement Network (FSDENet). Our framework employs spatial
processing methods to extract rich multi-scale spatial features and
fine-grained semantic details. By effectively integrating global and
frequency-domain information through the Fast Fourier Transform (FFT) in global
mappings, the model's capability to discern global representations under
grayscale variations is significantly strengthened. Additionally, we utilize
Haar wavelet transform to decompose features into high- and low-frequency
components, leveraging their distinct sensitivity to edge information to refine
boundary segmentation. The model achieves dual-domain synergy by integrating
spatial granularity with frequency-domain edge sensitivity, substantially
improving segmentation accuracy in boundary regions and grayscale transition
zones. Comprehensive experimental results demonstrate that FSDENet achieves
state-of-the-art (SOTA) performance on four widely adopted datasets: LoveDA,
Vaihingen, Potsdam, and iSAID.

</details>


### [11] [Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving](https://arxiv.org/abs/2510.00060)
*Sheng Yang,Tong Zhan,Guancheng Chen,Yanfeng Lu,Jian Wang*

Main category: cs.CV

TL;DR: 将轨迹规划重构为“下一个航点”的语言生成问题，提出Max-V1单阶段端到端框架，利用视觉-语言模型直接从前视图生成航点序列并通过统计监督进行训练，在nuScenes上超越先前方法30%+并展现良好跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶通常将感知、预测、规划分成多个模块，效率或一致性受限；将驾驶视为语言生成问题可利用VLM的序列建模与生成优势，实现更自然的一体化端到端策略，并通过统计监督确保稳定性与可学习性。

Method: 提出Max-V1框架：将驾驶序列视为语言序列，采用单阶段单次生成的VLM基础模型从前视图直接生成未来航点序列；引入统计建模驱动的监督策略（可能是概率回归或分布拟合），用于定义清晰的损失函数以便模仿学习；训练基于大规模专家示范的数据集，实现端到端学习，从感知到决策统一输出轨迹。

Result: 在nuScenes上达到SOTA，整体提升超过30%；在不同车辆采集的跨域数据上泛化性能更好，显示出跨车辆鲁棒性和适配性强；代码将在发表后公开。

Conclusion: 本文将自动驾驶重新构想为一种广义语言，并将轨迹规划任务表述为下一个航点预测。提出Max-V1单阶段端到端框架，通过单次生成范式利用视觉-语言模型的生成能力，从前视相机直接预测轨迹。采用基于统计建模的监督策略，提供明确学习目标，使得通过模仿学习从大规模专家示范中掌握复杂驾驶策略成为可能。实验证明在nuScenes数据集上性能领先，较先前基线整体提升30%以上，并在跨域数据上表现出更好的泛化能力，展示出跨车辆鲁棒性和适应性潜力。

Abstract: In this work, we reconceptualize autonomous driving as a generalized language
and formulate the trajectory planning task as next waypoint prediction. We
introduce Max-V1, a novel framework for one-stage end-to-end autonomous
driving. Our framework presents a single-pass generation paradigm that aligns
with the inherent sequentiality of driving. This approach leverages the
generative capacity of the VLM (Vision-Language Model) to enable end-to-end
trajectory prediction directly from front-view camera input. The efficacy of
this method is underpinned by a principled supervision strategy derived from
statistical modeling. This provides a well-defined learning objective, which
makes the framework highly amenable to master complex driving policies through
imitation learning from large-scale expert demonstrations. Empirically, our
method achieves the state-of-the-art performance on the nuScenes dataset,
delivers an overall improvement of over 30% compared to prior baselines.
Furthermore, it exhibits superior generalization performance on cross-domain
datasets acquired from diverse vehicles, demonstrating notable potential for
cross-vehicle robustness and adaptability. Due to these empirical strengths,
this work introduces a model enabling fundamental driving behaviors, laying the
foundation for the development of more capable self-driving agents. Code will
be available upon publication.

</details>


### [12] [Efficient CNN Compression via Multi-method Low Rank Factorization and Feature Map Similarity](https://arxiv.org/abs/2510.00062)
*M. Kokhazadeh,G. Keramidas,V. Kelefouras*

Main category: cs.CV

TL;DR: 提出一个端到端DSE框架：用特征图相似性做秩选择，one-shot微调，按层混用3种Conv和3种FC低秩分解，集成在TensorFlow 2.x，能高效压缩CNN并在多模型多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统低秩分解方法面临秩选择困难、设计空间庞大、长时间微调、与不同层类型及分解方法兼容性差等问题，因此需要一种更高效且通用的LRF压缩DSE方法。

Method: 提出基于特征图相似性的秩选择策略以更好捕捉层输出的非线性交互；采用一次性（one-shot）微调以大幅缩短微调时间；框架兼容所有Conv和FC层，并在每层选择性集成三种Conv和三种FC的LRF技术，通过在单模型内混合多种LRF方法提升压缩效果；在TensorFlow 2.x中实现并对14个CNN模型与8个数据集进行评估。

Result: 在14个CNN模型和8个数据集上的实验证明，该方法在保持最低精度损失的同时实现了显著压缩，且优于若干最先进技术；并提供了六种LRF技术的全面比较与实用见解。

Conclusion: 该论文提出了一种端到端的设计空间探索（DSE）框架，用于对卷积神经网络（CNN）进行低秩分解（LRF）压缩，通过基于特征图相似性的秩选择、一键微调、按层选择多种LRF方法等策略，解决了传统LRF在秩选择、设计空间、微调时间及兼容性方面的不足，实验显示在14个模型和8个数据集上取得了明显压缩率且精度下降最小，优于若干现有方法。

Abstract: Low-Rank Factorization (LRF) is a widely adopted technique for compressing
deep neural networks (DNNs). However, it faces several challenges, including
optimal rank selection, a vast design space, long fine-tuning times, and
limited compatibility with different layer types and decomposition methods.
This paper presents an end-to-end Design Space Exploration (DSE) methodology
and framework for compressing convolutional neural networks (CNNs) that
addresses all these issues. We introduce a novel rank selection strategy based
on feature map similarity, which captures non-linear interactions between layer
outputs more effectively than traditional weight-based approaches. Unlike prior
works, our method uses a one-shot fine-tuning process, significantly reducing
the overall fine-tuning time. The proposed framework is fully compatible with
all types of convolutional (Conv) and fully connected (FC) layers. To further
improve compression, the framework integrates three different LRF techniques
for Conv layers and three for FC layers, applying them selectively on a
per-layer basis. We demonstrate that combining multiple LRF methods within a
single model yields better compression results than using a single method
uniformly across all layers. Finally, we provide a comprehensive evaluation and
comparison of the six LRF techniques, offering practical insights into their
effectiveness across different scenarios. The proposed work is integrated into
TensorFlow 2.x, ensuring compatibility with widely used deep learning
workflows. Experimental results on 14 CNN models across eight datasets
demonstrate that the proposed methodology achieves substantial compression with
minimal accuracy loss, outperforming several state-of-the-art techniques.

</details>


### [13] [Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry](https://arxiv.org/abs/2510.00067)
*Rafael da Silva Maciel,Lucio Veraldo Jr*

Main category: cs.CV

TL;DR: 本文开发了一种结合LLM与智能图像分析的自动化5S审计系统，验证显示在一致性、效率与成本方面显著优于传统人工审计，具备工业化推广潜力。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能提升传统5S审计的客观性、效率与一致性，使其符合工业4.0标准并适应汽车制造业中规模化、重复性高的审计场景。

Method: 基于LLM进行图像理解与评估，系统化地对拍摄的现场图像进行特征提取与5S规则匹配，采用统计指标（Cohen's kappa）验证与人工审核的一致性，并通过时间与成本对比评估效率提升与成本下降。

Result: 自动化系统与人工审计之间的Cohen's kappa为0.75（强一致），审计时间缩短50%，运营成本下降99.8%，为不同规模汽车工厂提供可扩展的实施方案。

Conclusion: 该论文提出并验证了一种基于大型语言模型（LLM）与智能图像分析的自动化5S审计系统，能够标准化评估Seiri、Seiton、Seiso、Seiketsu、Shitsuke五项内容，并在一致性和效率上显著优于人工审计。

Abstract: The evolution of the 5S methodology with the support of artificial
intelligence techniques represents a significant opportunity to improve
industrial organization audits in the automotive chain, making them more
objective, efficient and aligned with Industry 4.0 standards. This work
developed an automated 5S audit system based on large-scale language models
(LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu,
Shitsuke) in a standardized way through intelligent image analysis. The
system's reliability was validated using Cohen's concordance coefficient (kappa
= 0.75), showing strong alignment between the automated assessments and the
corresponding human audits. The results indicate that the proposed solution
contributes significantly to continuous improvement in automotive manufacturing
environments, speeding up the audit process by 50% of the traditional time and
maintaining the consistency of the assessments, with a 99.8% reduction in
operating costs compared to traditional manual audits. The methodology
presented establishes a new paradigm for integrating lean systems with emerging
AI technologies, offering scalability for implementation in automotive plants
of different sizes.

</details>


### [14] [OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding](https://arxiv.org/abs/2510.00069)
*Jiancong Xie,Wenjin Wang,Zhuomeng Zhang,Zihan Liu,Qi Liu,Ke Feng,Zixun Sun,Yuedong Yang*

Main category: cs.CV

TL;DR: 提出OIG-Bench与多智能体半自动注释管线，评估29个MLLMs，Qwen2.5-VL-72B最优但仍存在语义与推理缺陷；注释系统在图说生成上优于模型，可用于数据集构建。


<details>
  <summary>Details</summary>
Motivation: One-Image Guide是一种融合文本、图像和符号的可视化信息呈现形式，贴合人类感知与理解，但现有评估对MLLMs在该类格式上的理解能力不足，因此需专门基准进行测评。

Method: 构建半自动化多智能体注释管线，生成初步图像描述并辅助人工构建图文对；设计并发布OIG-Bench数据集；对29个主流MLLMs进行评测，比较表现并分析错误类型。

Result: 在29个模型中，Qwen2.5-VL-72B表现最佳总体准确率77%，但所有模型在复杂视觉-文本关系的理解上表现欠佳；多智能体注释系统在图像描述任务上超过所有被评测的MLLMs。

Conclusion: OIG-Bench定义了专注于One-Image Guide理解的基准，显示当前多模态大模型在处理该类任务上仍有显著弱点，尤其在语义理解与逻辑推理方面。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated
impressive capabilities. However, evaluating their capacity for human-like
understanding in One-Image Guides remains insufficiently explored. One-Image
Guides are a visual format combining text, imagery, and symbols to present
reorganized and structured information for easier comprehension, which are
specifically designed for human viewing and inherently embody the
characteristics of human perception and understanding. Here, we present
OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding
across diverse domains. To reduce the cost of manual annotation, we developed a
semi-automated annotation pipeline in which multiple intelligent agents
collaborate to generate preliminary image descriptions, assisting humans in
constructing image-text pairs. With OIG-Bench, we have conducted a
comprehensive evaluation of 29 state-of-the-art MLLMs, including both
proprietary and open-source models. The results show that Qwen2.5-VL-72B
performs the best among the evaluated models, with an overall accuracy of 77%.
Nevertheless, all models exhibit notable weaknesses in semantic understanding
and logical reasoning, indicating that current MLLMs still struggle to
accurately interpret complex visual-text relationships. In addition, we also
demonstrate that the proposed multi-agent annotation system outperforms all
MLLMs in image captioning, highlighting its potential as both a high-quality
image description generator and a valuable tool for future dataset
construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.

</details>


### [15] [Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning](https://arxiv.org/abs/2510.00072)
*Chenhui Xu,Fuxun Yu,Michael J. Bianco,Jacob Kovarskiy,Raphael Tang,Qi Zhang,Zirui Xu,Will LeVine,Brandon Dubbs,Heming Liao,Cassandra Burgess,Suvam Bag,Jay Patravali,Rupanjali Kukal,Mikael Figueroa,Rishi Madhok,Nikolaos Karianakis,Jinjun Xiong*

Main category: cs.CV

TL;DR: Geo-R1通过合成CoT微调+基于GRPO的弱监督强化学习，在后训练阶段赋予视觉-语言模型地理空间推理能力，避免昂贵标注并在多项基准上实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间建模主要依赖领域预训练或有监督微调，缺乏显式推理能力且人工标注成本高。作者旨在通过后训练阶段直接培养模型的推理能力，既降低标注成本又提升任务泛化与准确性。

Method: 两阶段流程：1）Scaffolding阶段：用合成的chain-of-thought示例进行有监督微调，建立“地理思维范式”，将视觉线索与地理先验连接起来；2）Elevating阶段：基于GRPO的强化学习，利用弱监督的跨视角配对作为代理任务提供可验证、可扩展的奖励信号，训练模型对齐跨模态特征并用推理提升预测精度。

Result: 在多个地理空间推理基准上取得SOTA表现；提供了可验证的奖励设计与可扩展训练方案，并已开源模型权重（Hugging Face链接）。

Conclusion: Geo-R1提出了一种以推理为核心的后训练框架，通过“搭建思维支架（scaffolding）”和“提升（elevating）”两阶段，使视觉-语言模型具备地理空间推理能力，避免昂贵人工推理标注，并在多项地理推理基准上达到了SOTA水平。

Abstract: We introduce Geo-R1, a reasoning-centric post-training framework that unlocks
geospatial reasoning in vision-language models by combining thinking
scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a
``geospatial thinking paradigm" via supervised fine-tuning on synthetic
chain-of-thought exemplars, enabling models to connect visual cues with
geographic priors without costly human reasoning annotations. In the elevating
stage, it uses GRPO-based reinforcement learning on a weakly-supervised
cross-view pairing proxy. This design supplies a verifiable and scalable reward
signal: teaching models to capture and reconcile features across modalities,
and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial
modeling from domain pretraining / supervised finetuning to reasoning-first
post-training, and achieves state-of-the-art performance across various
geospatial reasoning benchmarks. Our model is available at
https://huggingface.co/miniHui/Geo-R1.

</details>


### [16] [Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks](https://arxiv.org/abs/2510.00083)
*Hanjiang Hu,Bowei Li,Ziwei Wang,Tianhao Wei,Casidhe Hutchison,Eric Sample,Changliu Liu*

Main category: cs.CV

TL;DR: 提出USN指标指导剪枝并结合Wasserstein损失以提升在语义变换下的鲁棒性认证，实验证明对关键点检测任务有明显改进。


<details>
  <summary>Details</summary>
Motivation: 当前认证训练与鲁棒性认证在过参数化神经网络面前存在紧致性和可扩展性问题，作者希望通过删减低USN神经元保持表达能力的同时降低复杂度，从而改进认证紧致性与效率。

Method: 分析层与神经元对输入扰动的稳定性与方差，引入Unbiased and Smooth Neuron (USN)指标，根据USN进行神经元剪枝；同时设计Wasserstein距离损失以增强剪枝后神经元在层间的集中性。

Result: 在具有现实亮度与对比度扰动的关键点检测任务上，方法在鲁棒性认证性能和效率上均优于基线方法，实验证明其有效性。

Conclusion: 该论文提出了一种基于神经元度量USN的剪枝方法和Wasserstein距离损失，以提高在语义变换（亮度、对比度）下的鲁棒性认证效果。

Abstract: Deep neural networks have been widely adopted in many vision and robotics
applications with visual inputs. It is essential to verify its robustness
against semantic transformation perturbations, such as brightness and contrast.
However, current certified training and robustness certification methods face
the challenge of over-parameterization, which hinders the tightness and
scalability due to the over-complicated neural networks. To this end, we first
analyze stability and variance of layers and neurons against input
perturbation, showing that certifiable robustness can be indicated by a
fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce
a novel neural network pruning method that removes neurons with low USN and
retains those with high USN, thereby preserving model expressiveness without
over-parameterization. To further enhance this pruning process, we propose a
new Wasserstein distance loss to ensure that pruned neurons are more
concentrated across layers. We validate our approach through extensive
experiments on the challenging robust keypoint detection task, which involves
realistic brightness and contrast perturbations, demonstrating that our method
achieves superior robustness certification performance and efficiency compared
to baselines.

</details>


### [17] [Improved Hyperspectral Anomaly Detection via Unsupervised Subspace Modeling in the Signed Cumulative Distribution Transform Domain](https://arxiv.org/abs/2510.00148)
*Abu Hasnat Mohammad Rubaiyat,Jordan Vincent,Colin Olson*

Main category: cs.CV

TL;DR: 利用基于传输的模型和SCDT变换，将高光谱像素映射到便于子空间建模的域，从而实现更鲁棒的无监督异常检测，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实环境复杂且缺乏潜在目标光谱先验，传统HAD方法在复杂背景或未知异常上表现受限；通过引入变形/传输模型，提升对光谱形变的鲁棒性，并在SCDT域简化背景建模问题。

Method: 构建基于传输（变形）模型的数学表征：假设像素为模板经变形产生，应用SCDT将像素映射到域内；在SCDT域使用无监督子空间建模来表征背景（可能用PCA或子空间学习），通过计算重构误差或投影残差识别异常像元。

Result: 在五个不同数据集上的综合评估显示，该方法在检测性能上优于当前最先进的方法，说明SCDT+子空间建模在高光谱异常检测任务中具有优势。

Conclusion: 该文提出将高光谱像素视为模板模式经未知形变的观察，通过签名累积分布变换（SCDT）将像素映射到变换域，在变换域建立背景子空间模型，异常像元被视为偏离该模型的信号，从而实现无监督异常检测。

Abstract: Hyperspectral anomaly detection (HAD), a crucial approach for many civilian
and military applications, seeks to identify pixels with spectral signatures
that are anomalous relative to a preponderance of background signatures.
Significant effort has been made to improve HAD techniques, but challenges
arise due to complex real-world environments and, by definition, limited prior
knowledge of potential signatures of interest. This paper introduces a novel
HAD method by proposing a transport-based mathematical model to describe the
pixels comprising a given hyperspectral image. In this approach, hyperspectral
pixels are viewed as observations of a template pattern undergoing unknown
deformations that enables their representation in the signed cumulative
distribution transform (SCDT) domain. An unsupervised subspace modeling
technique is then used to construct a model of abundant background signals in
this domain, whereupon anomalous signals are detected as deviations from the
learned model. Comprehensive evaluations across five distinct datasets
illustrate the superiority of our approach compared to state-of-the-art
methods.

</details>


### [18] [MOLM: Mixture of LoRA Markers](https://arxiv.org/abs/2510.00293)
*Samar Fares,Nurbek Tastan,Noor Hussein,Karthik Nandakumar*

Main category: cs.CV

TL;DR: 提出基于LoRA适配器路由的水印框架MOLM，通过密钥驱动的参数扰动实现轻量、可更新且对多种攻击鲁棒的生成模型水印。


<details>
  <summary>Details</summary>
Motivation: 大规模生成逼真图像带来来源可追溯和鉴别的紧迫需求；现有水印方法在抗失真、抗自适应攻击和更新密钥成本方面存在不足。

Method: 提出Mixture of LoRA Markers (MOLM)：在残差块和注意力模块内插入轻量级LoRA适配器，并通过二进制路由密钥激活不同适配器组合以编码水印；设计包括编码器（模型参数扰动）与解码/验证器（密钥恢复）流程。

Result: 在Stable Diffusion和FLUX上实验表明，MOLM在保持图像质量的同时能在多种现实失真（压缩、再生成、均值攻击）和黑盒对抗攻击下实现稳健的密钥恢复。

Conclusion: 本文提出将水印编码视为对生成模型参数的与密钥相关的扰动，从而实现无需针对每个密钥重训练模型的可验证水印方法。

Abstract: Generative models can generate photorealistic images at scale. This raises
urgent concerns about the ability to detect synthetically generated images and
attribute these images to specific sources. While watermarking has emerged as a
possible solution, existing methods remain fragile to realistic distortions,
susceptible to adaptive removal, and expensive to update when the underlying
watermarking key changes. We propose a general watermarking framework that
formulates the encoding problem as key-dependent perturbation of the parameters
of a generative model. Within this framework, we introduce Mixture of LoRA
Markers (MOLM), a routing-based instantiation in which binary keys activate
lightweight LoRA adapters inside residual and attention blocks. This design
avoids key-specific re-training and achieves the desired properties such as
imperceptibility, fidelity, verifiability, and robustness. Experiments on
Stable Diffusion and FLUX show that MOLM preserves image quality while
achieving robust key recovery against distortions, compression and
regeneration, averaging attacks, and black-box adversarial attacks on the
extractor.

</details>


### [19] [Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection](https://arxiv.org/abs/2510.00303)
*Anay Majee,Amitesh Gangrade,Rishabh Iyer*

Main category: cs.CV

TL;DR: 提出CROWD：通过子模优化挑选代表性未知样本并用组合式表征学习解耦已知-未知，显著提高未知召回与已知精度，缓解语义混淆与遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有OWOD方法存在已知与未知语义混淆和灾难性遗忘，导致未知召回率低和已知类别精度下降，需一个统一的方法同时发现未知并保持已知性能。

Method: CROWD由两部分组成：CROWD-Discover使用最大化子模条件增益（SCG）函数的策略，从未标注数据中挑选与已知类别显著不同且具有代表性的未知实例；CROWD-Learn设计组合式目标，联合解纠缠已知与未知的表征，同时维护已知类别间的判别一致性，减轻语义混淆与灾难性遗忘。

Result: 在OWOD基准（M-OWODB和S-OWODB）上，CROWD使已知类别精度分别提升2.83%和2.05%，未知召回率接近现有最优方法的2.4倍。

Conclusion: 该论文提出了CROWD框架，通过组合式的数据发现与表征学习来提升开放世界目标检测中的未知对象发现和已知类别识别性能。

Abstract: Open-World Object Detection (OWOD) enriches traditional object detectors by
enabling continual discovery and integration of unknown objects via human
guidance. However, existing OWOD approaches frequently suffer from semantic
confusion between known and unknown classes, alongside catastrophic forgetting,
leading to diminished unknown recall and degraded known-class accuracy. To
overcome these challenges, we propose Combinatorial Open-World Detection
(CROWD), a unified framework reformulating unknown object discovery and
adaptation as an interwoven combinatorial (set-based) data-discovery
(CROWD-Discover) and representation learning (CROWD-Learn) task. CROWD-Discover
strategically mines unknown instances by maximizing Submodular Conditional Gain
(SCG) functions, selecting representative examples distinctly dissimilar from
known objects. Subsequently, CROWD-Learn employs novel combinatorial objectives
that jointly disentangle known and unknown representations while maintaining
discriminative coherence among known classes, thus mitigating confusion and
forgetting. Extensive evaluations on OWOD benchmarks illustrate that CROWD
achieves improvements of 2.83% and 2.05% in known-class accuracy on M-OWODB and
S-OWODB, respectively, and nearly 2.4x unknown recall compared to leading
baselines.

</details>


### [20] [Discrete Wavelet Transform as a Facilitator for Expressive Latent Space Representation in Variational Autoencoders in Satellite Imagery](https://arxiv.org/abs/2510.00376)
*Arpan Mahara,Md Rezaul Karim Khan,Naphtali Rishe,Wenjia Wang,Seyed Masoud Sadjadi*

Main category: cs.CV

TL;DR: 本文提出ExpDWT-VAE，在VAE中加入DWT频域分支以强化卫星影像潜在表征，实验证明能提升潜在空间质量，有利于LDM在遥感任务中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有LDM多依赖VAE构建潜在空间，但较少研究如何直接改进VAE的潜在表征。卫星影像包含丰富的频域信息，利用DWT可更好保留多尺度、边缘等特征，从而提升潜在空间质量。

Method: 设计双分支编码器：一支在空间域用卷积处理输入；另一支通过2D Haar小波分解提取频域子带，卷积处理后用逆DWT重构频域信息；两支融合后经卷积与对角高斯映射得到潜在表示。将该VAE与LDM流程结合用于遥感影像处理。

Result: 在TerraFly卫星影像数据集上进行实验，多个性能指标（如重建误差、下游任务效果、潜在空间可分性等）表明ExpDWT-VAE优于基线VAE，显示出更好的表征能力。

Conclusion: ExpDWT-VAE通过在VAE潜在空间中引入离散小波变换的频域分支，有效增强了卫星影像的潜在表征，提升了潜在空间质量，有利于后续LDM任务表现。

Abstract: Latent Diffusion Models (LDM), a subclass of diffusion models, mitigate the
computational complexity of pixel-space diffusion by operating within a
compressed latent space constructed by Variational Autoencoders (VAEs),
demonstrating significant advantages in Remote Sensing (RS) applications.
Though numerous studies enhancing LDMs have been conducted, investigations
explicitly targeting improvements within the intrinsic latent space remain
scarce. This paper proposes an innovative perspective, utilizing the Discrete
Wavelet Transform (DWT) to enhance the VAE's latent space representation,
designed for satellite imagery. The proposed method, ExpDWT-VAE, introduces
dual branches: one processes spatial domain input through convolutional
operations, while the other extracts and processes frequency-domain features
via 2D Haar wavelet decomposition, convolutional operation, and inverse DWT
reconstruction. These branches merge to create an integrated spatial-frequency
representation, further refined through convolutional and diagonal Gaussian
mapping into a robust latent representation. We utilize a new satellite imagery
dataset housed by the TerraFly mapping system to validate our method.
Experimental results across several performance metrics highlight the efficacy
of the proposed method at enhancing latent space representation.

</details>


### [21] [EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations](https://arxiv.org/abs/2510.00405)
*Jiayi Liu,Jiaming Zhou,Ke Ye,Kun-Yu Lin,Allan Wang,Junwei Liang*

Main category: cs.CV

TL;DR: 提出了真实自我视角基准EgoTraj-Bench和双流去噪预测模型BiFlow（含EgoAnchor），显著提高在有噪观测下的轨迹预测鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测多基于理想化观测历史，忽视第一视角感知噪声（遮挡、ID切换、跟踪漂移），导致部署时鲁棒性不足。需要在有噪的自我视角历史上学习能对应干净未来轨迹的模型。

Method: 构建EgoTraj-Bench基于真实第一视角的有噪观测和鸟瞰未来轨迹配对；提出BiFlow双流流匹配模型，同时去噪历史和预测未来，采用共享潜变量；引入EgoAnchor通过特征调制将历史精炼特征条件化到解码器中。

Result: 在EgoTraj-Bench上，BiFlow在minADE和minFDE上平均降低10-15%，展现更好鲁棒性并达到了SOTA性能。

Conclusion: 该文提出了面向自我视角的轨迹预测方法和基准，弥合了训练与部署中观测噪声差距，提升了鲁棒性。

Abstract: Reliable trajectory prediction from an ego-centric perspective is crucial for
robotic navigation in human-centric environments. However, existing methods
typically assume idealized observation histories, failing to account for the
perceptual artifacts inherent in first-person vision, such as occlusions, ID
switches, and tracking drift. This discrepancy between training assumptions and
deployment reality severely limits model robustness. To bridge this gap, we
introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy,
first-person visual histories in clean, bird's-eye-view future trajectories,
enabling robust learning under realistic perceptual constraints. Building on
this benchmark, we propose BiFlow, a dual-stream flow matching model that
concurrently denoises historical observations and forecasts future motion by
leveraging a shared latent representation. To better model agent intent, BiFlow
incorporates our EgoAnchor mechanism, which conditions the prediction decoder
on distilled historical features via feature modulation. Extensive experiments
show that BiFlow achieves state-of-the-art performance, reducing minADE and
minFDE by 10-15% on average and demonstrating superior robustness. We
anticipate that our benchmark and model will provide a critical foundation for
developing trajectory forecasting systems truly resilient to the challenges of
real-world, ego-centric perception.

</details>


### [22] [David and Goliath in Medical Vision: Convolutional Networks vs Biomedical Vision Language Models](https://arxiv.org/abs/2510.00411)
*Ran Tong,Jiaqi Liu,Su Liu,Jiexi Xu,Lanruo Wang,Tong Wang*

Main category: cs.CV

TL;DR: 通过对BiomedCLIP进行验证集上的阈值校准，零-shot VLM在胸片诊断任务上可显著提升性能，并能匹配或超越轻量级监督CNN基线。


<details>
  <summary>Details</summary>
Motivation: 探讨零-shot医用视觉-语言模型在胸片自动诊断中的实际效用，并验证简单的阈值校准能否释放其潜力，使其与高效的监督模型竞争。

Method: 在PneumoniaMNIST和Shenzhen TB两个数据集上比较了监督轻量级CNN与BiomedCLIP的零-shot性能；通过在验证集上寻找最优分类阈值对BiomedCLIP进行校准，并报告校准前后F1分数的变化。

Result: 经过阈值校准后，BiomedCLIP在PneumoniaMNIST的F1从未校准值提升至0.8841，超过监督CNN的0.8803；在Shenzhen TB上F1从0.4812提升至0.7684，接近监督CNN的0.7834。

Conclusion: Zero-shot VLMs (BiomedCLIP) 在默认阈值下性能逊于监督轻量级CNN，但通过在验证集上校准决策阈值，可显著提升VLM性能，使其在肺炎检测任务上略优于监督CNN，在结核检测任务上接近监督基线。

Abstract: The accurate interpretation of chest radiographs using automated methods is a
critical task in medical imaging. This paper presents a comparative analysis
between a supervised lightweight Convolutional Neural Network (CNN) and a
state-of-the-art, zero-shot medical Vision-Language Model (VLM), BiomedCLIP,
across two distinct diagnostic tasks: pneumonia detection on the PneumoniaMNIST
benchmark and tuberculosis detection on the Shenzhen TB dataset. Our
experiments show that supervised CNNs serve as highly competitive baselines in
both cases. While the default zero-shot performance of the VLM is lower, we
demonstrate that its potential can be unlocked via a simple yet crucial remedy:
decision threshold calibration. By optimizing the classification threshold on a
validation set, the performance of BiomedCLIP is significantly boosted across
both datasets. For pneumonia detection, calibration enables the zero-shot VLM
to achieve a superior F1-score of 0.8841, surpassing the supervised CNN's
0.8803. For tuberculosis detection, calibration dramatically improves the
F1-score from 0.4812 to 0.7684, bringing it close to the supervised baseline's
0.7834. This work highlights a key insight: proper calibration is essential for
leveraging the full diagnostic power of zero-shot VLMs, enabling them to match
or even outperform efficient, task-specific supervised models.

</details>


### [23] [PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents](https://arxiv.org/abs/2510.00413)
*Zikang Liu,Junyi Li,Wayne Xin Zhao,Dawei Gao,Yaliang Li,Ji-rong Wen*

Main category: cs.CV

TL;DR: 提出PAL-UI：结合双层摘要与主动检索历史截图，改善GUI智能体长时程规划与跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GUI智能体因记忆长度受限，依赖截断或简单文本摘要会遗失关键历史视觉信息，导致执行长时程任务失败；因此需要一种按需检索历史视觉观测的方法以支持规划。

Method: 提出PAL-UI框架：包含观察级与动作级的双重摘要器，以及一个检索工具用于在规划阶段按需回溯过去的视觉观测；在Qwen2.5-VL上用8.6K步级指令数据训练PAL-UI-3B与PAL-UI-7B。

Result: 在移动GUI导航任务中相较基线与先前方法取得显著提升，并在数据高效设置下仍表现出优势；在未经额外训练下对网页导航也有较好泛化能力。

Conclusion: PAL-UI通过主动回溯检索历史截图与双层摘要机制，有效缓解了长时程GUI任务中的记忆瓶颈，显著提升了移动端与跨域（网页）导航性能。

Abstract: Graphical User Interface (GUI) agents powered by Multimodal Large Language
Models (MLLMs) promise human-like interaction with software applications, yet
long-horizon tasks remain challenging due to memory limitations. Existing
approaches either truncate history or rely on simple textual summaries, which
risk losing critical information when past visual details become necessary for
future decisions. In this paper, we propose \textbf{PAL-UI} (\textbf{P}lanning
with \textbf{A}ctive \textbf{L}ook-back), a novel framework that enables GUI
agents to adaptively retrieve past observations when required. PAL-UI combines
a dual-level summarization agent, capturing both observation-level cues and
action-level outcomes, with a dedicated retrieval tool that allows the agent to
recall specific historical screenshots during planning. We curate a step-level
instruction dataset of 8.6K samples from mobile GUI navigation trajectories and
train \textbf{PAL-UI-3B} and \textbf{PAL-UI-7B} models based on Qwen2.5-VL.
Extensive experiments demonstrate that PAL-UI significantly outperforms
baseline models and prior methods in mobile GUI navigation tasks, even under
data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain
generalization, achieving notable improvements in web navigation without
additional training. Our work highlights the potential of active memory
retrieval for long-horizon planning capabilities of vision-based GUI agents.

</details>


### [24] [Domain-Specialized Interactive Segmentation Framework for Meningioma Radiotherapy Planning](https://arxiv.org/abs/2510.00416)
*Junhyeok Lee,Han Jang,Kyu Sung Choi*

Main category: cs.CV

TL;DR: 提出面向脑膜瘤放疗的交互式分割工具Interactive-MEN-RT，通过多种临床交互显著提升分割准确性（Dice最高77.6%、IoU64.8%），在放疗场景中更适用并已开源。


<details>
  <summary>Details</summary>
Motivation: 常规自动化深度学习分割因肿瘤异质性而难以稳定达到临床精度，放疗规划对边界精确性要求高，故需将临床专业知识通过交互方式融入分割流程以提高可靠性。

Method: 基于交互式医疗图像分割框架，集成多种临床交互方式（点标注、包围盒、套索工具和涂鸦/划痕）并结合深度学习分割模型进行迭代优化；在BraTS 2025脑膜瘤数据集（500例对比增强T1 MRI）上评估性能。

Result: 在500例对比增强T1加权MRI上，Interactive-MEN-RT的Dice最高可达77.6%，IoU最高为64.8%，在与其他方法比较中表现优越，验证了临床针对性工具的价值；代码已开源。

Conclusion: 本文提出了Interactive-MEN-RT，一种针对放疗规划的临床交互式3D脑膜瘤分割工具，证明了在临床交互下可显著提升分割性能，强调了针对性工具在关键临床应用中的必要性。

Abstract: Precise delineation of meningiomas is crucial for effective radiotherapy (RT)
planning, directly influencing treatment efficacy and preservation of adjacent
healthy tissues. While automated deep learning approaches have demonstrated
considerable potential, achieving consistently accurate clinical segmentation
remains challenging due to tumor heterogeneity. Interactive Medical Image
Segmentation (IMIS) addresses this challenge by integrating advanced AI
techniques with clinical input. However, generic segmentation tools, despite
widespread applicability, often lack the specificity required for clinically
critical and disease-specific tasks like meningioma RT planning. To overcome
these limitations, we introduce Interactive-MEN-RT, a dedicated IMIS tool
specifically developed for clinician-assisted 3D meningioma segmentation in RT
workflows. The system incorporates multiple clinically relevant interaction
methods, including point annotations, bounding boxes, lasso tools, and
scribbles, enhancing usability and clinical precision. In our evaluation
involving 500 contrast-enhanced T1-weighted MRI scans from the BraTS 2025
Meningioma RT Segmentation Challenge, Interactive-MEN-RT demonstrated
substantial improvement compared to other segmentation methods, achieving Dice
similarity coefficients of up to 77.6\% and Intersection over Union scores of
64.8\%. These results emphasize the need for clinically tailored segmentation
solutions in critical applications such as meningioma RT planning. The code is
publicly available at: https://github.com/snuh-rad-aicon/Interactive-MEN-RT

</details>


### [25] [BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration](https://arxiv.org/abs/2510.00438)
*Zhaoyang Li,Dongjun Qian,Kai Su,Qishuai Diao,Xiangyang Xia,Chang Liu,Wenfei Yang,Tianzhu Zhang,Zehuan Yuan*

Main category: cs.CV

TL;DR: BindWeave通过让多模态大模型解析并绑定提示中的实体与交互，生成主题感知条件供扩散变换器使用，从而显著提升多主体视频生成的主体一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以保持主体一致性，尤其是在解析描述复杂空间关系、时间逻辑和多主体交互的提示时表现欠佳。需要一种能将复杂提示语义准确绑定到具体视觉主体的方法。

Method: 提出MLLM-DiT框架：使用预训练的多模态大模型进行深度跨模态推理，将提示中的实体、角色、属性和交互解析并映射为主题感知隐藏状态，作为扩散变换器的视频生成条件。框架支持从单一主体到多主体异构实体场景的统一处理。

Result: 在OpenS2V基准上的实验结果显示，BindWeave在主体一致性、生成视频自然性和文本相关性方面优于现有开源与商业模型，证明了其在复杂多主体语义绑定与生成方面的有效性。

Conclusion: 该论文提出了一个名为BindWeave的框架，通过将多模态大语言模型（MLLM）与扩散变换器（DiT）结合，实现了复杂多主体视频生成中的主体一致性约束。实验表明，在OpenS2V基准上，BindWeave在主体一致性、自然性和文本相关性上均优于现有开源和商业模型。

Abstract: Diffusion Transformer has shown remarkable abilities in generating
high-fidelity videos, delivering visually coherent frames and rich details over
extended durations. However, existing video generation models still fall short
in subject-consistent video generation due to an inherent difficulty in parsing
prompts that specify complex spatial relationships, temporal logic, and
interactions among multiple subjects. To address this issue, we propose
BindWeave, a unified framework that handles a broad range of subject-to-video
scenarios from single-subject cases to complex multi-subject scenes with
heterogeneous entities. To bind complex prompt semantics to concrete visual
subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal
large language model performs deep cross-modal reasoning to ground entities and
disentangle roles, attributes, and interactions, yielding subject-aware hidden
states that condition the diffusion transformer for high-fidelity
subject-consistent video generation. Experiments on the OpenS2V benchmark
demonstrate that our method achieves superior performance across subject
consistency, naturalness, and text relevance in generated videos, outperforming
existing open-source and commercial models.

</details>


### [26] [Measuring and Controlling the Spectral Bias for Self-Supervised Image Denoising](https://arxiv.org/abs/2510.00454)
*Wang Zhang,Huaqiu Li,Xiaowan Hu,Tao Jiang,Zikang Chen,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出SCNet，通过频谱选择、Lipschitz约束和SSR模块实现自监督配对噪声图像的高频细节保留与噪声抑制，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 通过测量Image Pair Frequency-Band Similarity，发现现有将一张噪声图映射到另一张噪声图的自监督方法在频谱上存在偏差：高频结构细节保存差且在拟合高频细节时会学习到高频噪声，因此需要控制频谱学习过程以提升去噪效果。

Method: 提出三方面方法：1）频率带选择策略，用于在训练中选择图像的频谱分量以加速收敛；2）基于Lipschitz常数的参数优化，限制卷积核学习高频噪声的能力而不改动网络结构；3）SSR模块（频谱分离与低秩重建），在频域分离噪声与高频细节并通过低秩空间重建保留结构细节。整体构成Spectral Controlling Network (SCNet)。

Result: 在合成与真实世界数据集上的实验显示，SCNet在保留高频结构细节和抑制高频噪声方面优于现有自监督配对去噪方法，同时训练收敛更快。

Conclusion: 该论文提出了一种用于成对噪声图像的自监督去噪框架SCNet，通过频谱选择、基于Lipschitz常数的参数约束以及频域分离与低秩重建模块，解决了现有方法对高频细节保留不足且易拟合高频噪声的问题。实验验证表明方法在合成与真实数据集上均有效。

Abstract: Current self-supervised denoising methods for paired noisy images typically
involve mapping one noisy image through the network to the other noisy image.
However, after measuring the spectral bias of such methods using our proposed
Image Pair Frequency-Band Similarity, it suffers from two practical
limitations. Firstly, the high-frequency structural details in images are not
preserved well enough. Secondly, during the process of fitting high
frequencies, the network learns high-frequency noise from the mapped noisy
images. To address these challenges, we introduce a Spectral Controlling
network (SCNet) to optimize self-supervised denoising of paired noisy images.
First, we propose a selection strategy to choose frequency band components for
noisy images, to accelerate the convergence speed of training. Next, we present
a parameter optimization method that restricts the learning ability of
convolutional kernels to high-frequency noise using the Lipschitz constant,
without changing the network structure. Finally, we introduce the Spectral
Separation and low-rank Reconstruction module (SSR module), which separates
noise and high-frequency details through frequency domain separation and
low-rank space reconstruction, to retain the high-frequency structural details
of images. Experiments performed on synthetic and real-world datasets verify
the effectiveness of SCNet.

</details>


### [27] [VLOD-TTA: Test-Time Adaptation of Vision-Language Object Detectors](https://arxiv.org/abs/2510.00458)
*Atif Belal,Heitor R. Medeiros,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: VLOD-TTA 通过 IoU 权重熵与图像条件提示融合，在测试时自适应 VLOD，提高了跨域检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言目标检测器在域转移下性能下降，需在测试时无标签地适配以提高稳健性。

Method: 提出 IoU 加权熵目标以在空间上集中于重叠的 proposal 聚类并降低孤立框的确认偏差；引入图像条件的提示选择，根据图像兼容性对提示排序并与检测器的 logits 融合。

Result: 在风格化域、驾驶场景、低光照和常见腐败等多种分布移位基准上，对 YOLO-World 和 Grounding DINO 实现了一致改进，优于零样本和现有 TTA 基线。

Conclusion: VLOD-TTA 在多种分布移位下通过 IoU 权重熵目标和图像条件提示选择，提升了 VLOD 的零样本识别鲁棒性。

Abstract: Vision-language object detectors (VLODs) such as YOLO-World and Grounding
DINO achieve impressive zero-shot recognition by aligning region proposals with
text representations. However, their performance often degrades under domain
shift. We introduce VLOD-TTA, a test-time adaptation (TTA) framework for VLODs
that leverages dense proposal overlap and image-conditioned prompt scores.
First, an IoU-weighted entropy objective is proposed that concentrates
adaptation on spatially coherent proposal clusters and reduces confirmation
bias from isolated boxes. Second, image-conditioned prompt selection is
introduced, which ranks prompts by image-level compatibility and fuses the most
informative prompts with the detector logits. Our benchmarking across diverse
distribution shifts -- including stylized domains, driving scenes, low-light
conditions, and common corruptions -- shows the effectiveness of our method on
two state-of-the-art VLODs, YOLO-World and Grounding DINO, with consistent
improvements over the zero-shot and TTA baselines. Code :
https://github.com/imatif17/VLOD-TTA

</details>


### [28] [MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles](https://arxiv.org/abs/2510.00483)
*Yuheng Ji,Huajie Tan,Cheng Chi,Yijie Xu,Yuting Zhao,Enshen Zhou,Huaihai Lyu,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 提出MathSticks基准，通过火柴算式纠错测试视觉+符号组合推理（1.4M实例）；现有模型普遍不足，人类表现优异，数据开源。


<details>
  <summary>Details</summary>
Motivation: 衡量并推动视觉感知与符号算术操作的组合推理能力，弥补现有基准对视觉与符号一致性测试的缺失。

Method: 构建基于火柴拼接的纠错任务，规定严格守恒规则，生成1.4M实例并提供人工筛选的测试集；任务包括文本引导与纯视觉两种设置，并系统变化位数、移动复杂度、解的多样性与运算符。评估14个视-语言模型以测量性能。

Result: 闭源模型仅能处理简单情形，开源模型在纯视觉场景失败；人类准确率超过90%。数据集与代码已开源。

Conclusion: MathSticks 提供了一个统一视觉-符号-算术一致性的基准，发现当前多模态模型在组合推理上存在显著不足；人类表现远超模型。

Abstract: We introduce \textsc{MathSticks}, a benchmark for Visual Symbolic
Compositional Reasoning (VSCR), which unifies visual perception, symbolic
manipulation, and arithmetic consistency. Each task presents an incorrect
matchstick equation that must be corrected by moving one or two sticks under
strict conservation rules. The benchmark includes both text-guided and purely
visual settings, systematically covering digit scale, move complexity, solution
multiplicity, and operator variation, with 1.4M generated instances and a
curated test set. Evaluations of 14 vision--language models reveal substantial
limitations: closed-source models succeed only on simple cases, open-source
models fail in the visual regime, while humans exceed 90\% accuracy. These
findings establish \textsc{MathSticks} as a rigorous testbed for advancing
compositional reasoning across vision and symbols. Our code and dataset are
publicly available at https://github.com/Yuheng2000/MathSticks.

</details>


### [29] [Normal-Abnormal Guided Generalist Anomaly Detection](https://arxiv.org/abs/2510.00495)
*Yuexin Wang,Xiaolei Wang,Yizheng Gong,Jimin Xiao*

Main category: cs.CV

TL;DR: 提出NAGL框架，首次在通用异常检测中同时利用正常与异常参考，通过RM和AFL提取可迁移异常表示与实例感知异常特征，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有通用异常检测方法通常只利用正常样本作为参考，忽视了在真实场景中常可获得的异常样本信息；利用异常样本作为额外参考可以提供更丰富的异常模式，有助于提升跨域检测效果。

Method: 提出Normal-Abnormal Generalist Learning (NAGL)框架，包含Residual Mining (RM)和Anomaly Feature Learning (AFL)：RM从正常-异常参考残差中挖掘异常模式以构建可迁移的异常表示；AFL通过残差映射自适应学习查询图像中的异常特征以实现实例感知的异常检测。

Result: 在多个基准数据集上广泛实验，结果表明所提方法显著优于现有GAD方法，证明混合正常/异常参考在通用异常检测中的有效性。

Conclusion: 本文提出了一种更实际的通用异常检测范式——同时利用正常与异常样本作为参考以提升跨域异常检测性能。通过残差挖掘与异常特征学习两部分，能够提取可迁移的异常表示并在查询图像中识别实例感知的异常。实验显示在多个基准上优于现有方法。

Abstract: Generalist Anomaly Detection (GAD) aims to train a unified model on an
original domain that can detect anomalies in new target domains. Previous GAD
methods primarily use only normal samples as references, overlooking the
valuable information contained in anomalous samples that are often available in
real-world scenarios. To address this limitation, we propose a more practical
approach: normal-abnormal-guided generalist anomaly detection, which leverages
both normal and anomalous samples as references to guide anomaly detection
across diverse domains. We introduce the Normal-Abnormal Generalist Learning
(NAGL) framework, consisting of two key components: Residual Mining (RM) and
Anomaly Feature Learning (AFL). RM extracts abnormal patterns from
normal-abnormal reference residuals to establish transferable anomaly
representations, while AFL adaptively learns anomaly features in query images
through residual mapping to identify instance-aware anomalies. Our approach
effectively utilizes both normal and anomalous references for more accurate and
efficient cross-domain anomaly detection. Extensive experiments across multiple
benchmarks demonstrate that our method significantly outperforms existing GAD
approaches. This work represents the first to adopt a mixture of normal and
abnormal samples as references in generalist anomaly detection. The code and
datasets are available at https://github.com/JasonKyng/NAGL.

</details>


### [30] [Relative-Absolute Fusion: Rethinking Feature Extraction in Image-Based Iterative Method Selection for Solving Sparse Linear Systems](https://arxiv.org/abs/2510.00500)
*Kaiqi Zhang,Mingguan Yang,Dali Chang,Chun Chen,Yuxiang Zhang,Kexun He,Jing Zhao*

Main category: cs.CV

TL;DR: RAF通过融合图像的相对特征与矩阵的绝对数值特征，消除了图像表示的歧义性，显著提升了迭代方法选择的效果与求解速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的迭代求解器选择方法存在特征歧义问题——不同稀疏矩阵可能被映射为相同图像特征，导致选择错误和性能下降，因此需要一种能同时保留结构（相对）与数值（绝对）信息的表示。

Method: 提出Relative-Absolute Fusion (RAF)技术：并行提取矩阵的图像化相对特征与对应的数值绝对特征，并在特征层面融合，形成更全面且区分度更高的表示；在SuiteSparse和自建BMCMat数据集上进行实验比较。

Result: 在SuiteSparse和BMCMat上，RAF使求解稀疏线性系统的时间减少0.08s–0.29s，相比传统图像化选择方法加速5.86%–11.50%，达到了SOTA性能；并公开了BMCMat数据集。

Conclusion: RAF通过融合相对图像特征与绝对数值特征，解决了图像化矩阵表示中不同矩阵被映射为相同表示的问题，从而提升了迭代方法选择的准确性与鲁棒性。

Abstract: Iterative method selection is crucial for solving sparse linear systems
because these methods inherently lack robustness. Though image-based selection
approaches have shown promise, their feature extraction techniques might encode
distinct matrices into identical image representations, leading to the same
selection and suboptimal method. In this paper, we introduce RAF
(Relative-Absolute Fusion), an efficient feature extraction technique to
enhance image-based selection approaches. By simultaneously extracting and
fusing image representations as relative features with corresponding numerical
values as absolute features, RAF achieves comprehensive matrix representations
that prevent feature ambiguity across distinct matrices, thus improving
selection accuracy and unlocking the potential of image-based selection
approaches. We conducted comprehensive evaluations of RAF on SuiteSparse and
our developed BMCMat (Balanced Multi-Classification Matrix dataset),
demonstrating solution time reductions of 0.08s-0.29s for sparse linear
systems, which is 5.86%-11.50% faster than conventional image-based selection
approaches and achieves state-of-the-art (SOTA) performance. BMCMat is
available at https://github.com/zkqq/BMCMat.

</details>


### [31] [Affordance-Guided Diffusion Prior for 3D Hand Reconstruction](https://arxiv.org/abs/2510.00506)
*Naru Suzuki,Takehiko Ohkawa,Tatsuro Banno,Jihyun Lee,Ryosuke Furuta,Yoichi Sato*

Main category: cs.CV

TL;DR: 提出一种利用VLM推断的物体功能描述来条件化扩散生成的手部姿态先验，从而在严重遮挡下实现更准确且功能合理的三维手部姿态细化。


<details>
  <summary>Details</summary>
Motivation: 动机是人类在手部或物体严重遮挡时会依赖语义与功能上下文（如物体的形状与用途提示常见握持方式）来推断手部姿态，现有回归或无上下文的生成方法在主观合理性与功能一致性上不足。

Method: 方法包括：1) 使用大型视觉-语言模型（VLM）从图像中推断手-物体交互的功能性描述；2) 训练一个条件扩散生成模型，学习在给定功能描述下的合理手部姿态分布；3) 将该生成先验用于对初始手部姿态估计进行采样和细化，尤其修复被遮挡的关节位置。

Result: 在HOGraspNet数据集（包含严重遮挡的三维手-物体交互）上的大量实验显示，基于物体功能描述的扩散细化在关键点误差和功能一致性指标上均显著优于近期回归方法及不含语义条件的扩散细化方法。

Conclusion: 该论文提出利用物体功能感知（affordance-aware）文本描述作为先验，引导基于扩散模型的三维手部姿态细化，以解决严重遮挡情况下的手部姿态重建问题。

Abstract: How can we reconstruct 3D hand poses when large portions of the hand are
heavily occluded by itself or by objects? Humans often resolve such ambiguities
by leveraging contextual knowledge -- such as affordances, where an object's
shape and function suggest how the object is typically grasped. Inspired by
this observation, we propose a generative prior for hand pose refinement guided
by affordance-aware textual descriptions of hand-object interactions (HOI). Our
method employs a diffusion-based generative model that learns the distribution
of plausible hand poses conditioned on affordance descriptions, which are
inferred from a large vision-language model (VLM). This enables the refinement
of occluded regions into more accurate and functionally coherent hand poses.
Extensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe
occlusions, demonstrate that our affordance-guided refinement significantly
improves hand pose estimation over both recent regression methods and
diffusion-based refinement lacking contextual reasoning.

</details>


### [32] [Efficient Multi-modal Large Language Models via Progressive Consistency Distillation](https://arxiv.org/abs/2510.00515)
*Zichen Wen,Shaobo Wang,Yufa Zhou,Junyuan Zhang,Qintong Zhang,Yifeng Gao,Zhaorun Chen,Bin Wang,Weijia Li,Conghui He,Linfeng Zhang*

Main category: cs.CV

TL;DR: EPIC：一种分解特征扰动的渐进一致性蒸馏框架，通过token与layer两种一致性蒸馏，在教师指导下逐步适应视觉token压缩，从而提升多模态大模型的效率与稳健性。


<details>
  <summary>Details</summary>
Motivation: 视觉token在MLLMs中计算开销大，现有压缩方法在提升效率的同时增加了训练难度，因为模型参数空间难以快速适应压缩导致的特征空间大幅扰动。作者希望通过引入一种能够缓解学习难度并稳定压缩过程的方法来提升效率且保持性能。

Method: 提出渐进式学习框架（EPIC），包括两部分：token一致性蒸馏（在token维度对齐压缩前后或教师-学生之间的token特征）和layer一致性蒸馏（在层级维度保证中间表示的逐层一致性），通过逐步减小特征扰动并利用教师模型作为监管引导学生模型适应压缩带来的变化。

Result: 大量实验表明EPIC在效果、鲁棒性和泛化能力上优于现有方法，能够在保证或接近原始性能的前提下，显著提高推理效率并稳定训练过程。

Conclusion: 该论文提出了一种名为EPIC的渐进一致性蒸馏框架，通过在token维和层维上分解压缩引入的特征扰动，并分别设计token一致性蒸馏和layer一致性蒸馏，用教师模型的指导降低压缩带来的学习难度，提升多模态大模型在视觉token压缩场景下的效率。

Abstract: Visual tokens consume substantial computational resources in multi-modal
large models (MLLMs), significantly compromising their efficiency. Recent works
have attempted to improve efficiency by compressing visual tokens during
training, either through modifications to model components or by introducing
additional parameters. However, they often overlook the increased learning
difficulty caused by such compression, as the model's parameter space struggles
to quickly adapt to the substantial perturbations in the feature space induced
by token compression. In this work, we propose to develop Efficient MLLMs via
Progressive Consistency Distillation (EPIC), a progressive learning framework.
Specifically, by decomposing the feature space perturbations introduced by
token compression along the token-wise and layer-wise dimensions, we introduce
token consistency distillation and layer consistency distillation,
respectively, aiming to reduce the training difficulty by leveraging guidance
from a teacher model and following a progressive learning trajectory. Extensive
experiments demonstrate the superior effectiveness, robustness, and
generalization capabilities of our proposed framework.

</details>


### [33] [CardioBench: Do Echocardiography Foundation Models Generalize Beyond the Lab?](https://arxiv.org/abs/2510.00520)
*Darya Taratynova,Ahmed Aly,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: CardioBench为心脏超声基础模型提供了首个公开、统一的基准和评测流水线，揭示了时间信息、检索和领域文本表示等对性能的关键作用，并指出通用编码器虽强但在细粒度任务仍需领域适配。


<details>
  <summary>Details</summary>
Motivation: 现有心脏超声基础模型缺乏统一评测基准，且多数评估基于私有数据，导致可比性差；心脏超声有噪声大、帧冗余高、公开数据稀缺等挑战，亟需标准化基准来推动可重复和可比研究。

Method: 将8个公开数据集统一成标准化套件，涵盖4个回归和5个分类任务，设计零样本、探针（probing）与对齐（alignment）三种评估协议，对多类基础模型（心脏专用、生物医学、通用编码器）进行系统评估，同时公开预处理、数据拆分与评测流水线。

Result: 发现不同模型在任务上具有互补性：时间建模对功能性回归至关重要；检索方法在分布偏移下更稳健；领域文本编码器能捕捉生理学相关的语义轴。通用编码器迁移能力强，在许多任务上通过微调可接近专用模型，但在细粒度视图分类及微妙病变识别上表现不足。

Conclusion: CardioBench填补了心脏超声基础模型评估的空白，提供了统一、可复现的基准并揭示不同模型家族的互补优势和局限。

Abstract: Foundation models (FMs) are reshaping medical imaging, yet their application
in echocardiography remains limited. While several echocardiography-specific
FMs have recently been introduced, no standardized benchmark exists to evaluate
them. Echocardiography poses unique challenges, including noisy acquisitions,
high frame redundancy, and limited public datasets. Most existing solutions
evaluate on private data, restricting comparability. To address this, we
introduce CardioBench, a comprehensive benchmark for echocardiography FMs.
CardioBench unifies eight publicly available datasets into a standardized suite
spanning four regression and five classification tasks, covering functional,
structural, diagnostic, and view recognition endpoints. We evaluate several
leading FM, including cardiac-specific, biomedical, and general-purpose
encoders, under consistent zero-shot, probing, and alignment protocols. Our
results highlight complementary strengths across model families: temporal
modeling is critical for functional regression, retrieval provides robustness
under distribution shift, and domain-specific text encoders capture
physiologically meaningful axes. General-purpose encoders transfer strongly and
often close the gap with probing, but struggle with fine-grained distinctions
like view classification and subtle pathology recognition. By releasing
preprocessing, splits, and public evaluation pipelines, CardioBench establishes
a reproducible reference point and offers actionable insights to guide the
design of future echocardiography foundation models.

</details>


### [34] [Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation](https://arxiv.org/abs/2510.00527)
*Taeyun Woo,Jinah Park,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出首个粗到细级联扩散框架：关节扩散采样多假设，潜在空间Mesh LDM基于关节重建网格，提升3D手部重建精度并能建模姿态不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有确定性级联或单阶段概率方法要么无法建模姿态不确定性，要么缺乏级联细化导致精度不足。论文旨在将概率采样与级联细化结合，提高在遮挡与复杂构型下的重建准确性与不确定性表征能力。

Method: 构建两阶段级联扩散模型：第一阶段是关节扩散模型，用于从图像中采样多样化的3D关节假设；第二阶段是条件Mesh Latent Diffusion Model，在潜在空间中基于关节样本重建三维手网格。通过在潜在空间训练Mesh LDM并以多样关节样本作为条件，学习到关节-网格的分布关系与鲁棒手先验。

Result: 在FreiHAND和HO3Dv2数据集上，所提方法在保持姿态分布建模能力的同时，实现了最先进的性能，证明了级联扩散框架在准确性与不确定性建模上的优势。

Conclusion: 该论文提出了一个结合概率建模与粗到细级联细化的3D手部重建框架，成功解决了确定性方法对自遮挡和复杂关节姿态不确定性建模的不足。

Abstract: Deterministic models for 3D hand pose reconstruction, whether single-staged
or cascaded, struggle with pose ambiguities caused by self-occlusions and
complex hand articulations. Existing cascaded approaches refine predictions in
a coarse-to-fine manner but remain deterministic and cannot capture pose
uncertainties. Recent probabilistic methods model pose distributions yet are
restricted to single-stage estimation, which often fails to produce accurate 3D
reconstructions without refinement. To address these limitations, we propose a
coarse-to-fine cascaded diffusion framework that combines probabilistic
modeling with cascaded refinement. The first stage is a joint diffusion model
that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent
Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a
joint sample. By training Mesh LDM with diverse joint hypotheses in a learned
latent space, our framework learns distribution-aware joint-mesh relationships
and robust hand priors. Furthermore, the cascaded design mitigates the
difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy
through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate
that our method achieves state-of-the-art performance while effectively
modeling pose distributions.

</details>


### [35] [Forestpest-YOLO: A High-Performance Detection Framework for Small Forestry Pests](https://arxiv.org/abs/2510.00547)
*Aoduo Li,Peikai Lin,Jiancheng Li,Zhen Zhang,Shiting Wu,Zexiao Liang,Zhifa Jiang*

Main category: cs.CV

TL;DR: 针对林业遥感中小、遮挡、相似害虫难检问题，论文在YOLOv8上设计SPD-Conv、CSPOK和VarifocalLoss，构成Forestpest-YOLO，在自建数据集上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 林业遥感图像中的害虫目标通常体积极小、易被遮挡且与背景相似，导致传统检测器在细粒度特征保持与类别不均衡方面表现不佳，亟需针对这些挑战的专用检测框架。

Method: 基于YOLOv8，作者引入三项关键改进：1）SPD-Conv无损下采样模块，用于保留小目标的高分辨率细节；2）CSPOK跨阶段特征融合模块，动态增强多尺度表征并抑制背景噪声；3）引入VarifocalLoss作为训练目标，关注高质量与难样本。

Result: 在作者自建的ForestPest数据集上，Forestpest-YOLO取得了SOTA性能，尤其在小目标和遮挡场景下显著优于基线模型（具体数值未在摘要中给出）。

Conclusion: 该文提出的Forestpest-YOLO在林业遥感害虫检测任务上有效提升了小目标、遮挡和类内相似样本的检测性能，验证了其设计在保存细粒度特征与处理样本不均衡方面的有效性。

Abstract: Detecting agricultural pests in complex forestry environments using remote
sensing imagery is fundamental for ecological preservation, yet it is severely
hampered by practical challenges. Targets are often minuscule, heavily
occluded, and visually similar to the cluttered background, causing
conventional object detection models to falter due to the loss of fine-grained
features and an inability to handle extreme data imbalance. To overcome these
obstacles, this paper introduces Forestpest-YOLO, a detection framework
meticulously optimized for the nuances of forestry remote sensing. Building
upon the YOLOv8 architecture, our framework introduces a synergistic trio of
innovations. We first integrate a lossless downsampling module, SPD-Conv, to
ensure that critical high-resolution details of small targets are preserved
throughout the network. This is complemented by a novel cross-stage feature
fusion block, CSPOK, which dynamically enhances multi-scale feature
representation while suppressing background noise. Finally, we employ
VarifocalLoss to refine the training objective, compelling the model to focus
on high-quality and hard-to-classify samples. Extensive experiments on our
challenging, self-constructed ForestPest dataset demonstrate that
Forestpest-YOLO achieves state-of-the-art performance, showing marked
improvements in detecting small, occluded pests and significantly outperforming
established baseline models.

</details>


### [36] [Assessing Foundation Models for Mold Colony Detection with Limited Training Data](https://arxiv.org/abs/2510.00561)
*Henrik Pichler,Janis Keuper,Matthew Copping*

Main category: cs.CV

TL;DR: 使用5000张培养皿图像对比评估，MaskDINO在仅150张微调下即可接近YoloV9的性能，25张仍对约70%的样本可靠，说明数据高效的基础模型能显著减少标注需求并加速自动化微生物系统开发。


<details>
  <summary>Details</summary>
Motivation: To reduce the heavy annotation burden and long training times of traditional automated microbiology vision systems by showing that foundation models fine-tuned on very small datasets can perform reliably.

Method: Collected 5000 Petri dish images with bounding-box annotations and curated small subsets with instance-level masks; evaluated three vision foundation models and traditional baselines (including YoloV9) across full-data, few-shot (25–150 images), and low-shot experiments using task-specific metrics.

Result: MaskDINO fine-tuned on 150 images reaches near-parity with extensively trained YoloV9; with only 25 images it remains reliable on ~70% of samples. Overall foundation models achieve competitive performance with a small fraction of data and have higher upper-bound performance.

Conclusion: The paper demonstrates that data-efficient vision foundation models (e.g., MaskDINO) can approach or match traditional heavily-trained detection models (e.g., YoloV9) for mold colony quantification on Petri dish images, even with very few annotated samples, enabling faster and cheaper deployment.

Abstract: The process of quantifying mold colonies on Petri dish samples is of critical
importance for the assessment of indoor air quality, as high colony counts can
indicate potential health risks and deficiencies in ventilation systems.
Conventionally the automation of such a labor-intensive process, as well as
other tasks in microbiology, relies on the manual annotation of large datasets
and the subsequent extensive training of models like YoloV9. To demonstrate
that exhaustive annotation is not a prerequisite anymore when tackling a new
vision task, we compile a representative dataset of 5000 Petri dish images
annotated with bounding boxes, simulating both a traditional data collection
approach as well as few-shot and low-shot scenarios with well curated subsets
with instance level masks. We benchmark three vision foundation models against
traditional baselines on task specific metrics, reflecting realistic real-world
requirements. Notably, MaskDINO attains near-parity with an extensively trained
YoloV9 model while finetuned only on 150 images, retaining competitive
performance with as few as 25 images, still being reliable on $\approx$ 70% of
the samples. Our results show that data-efficient foundation models can match
traditional approaches with only a fraction of the required data, enabling
earlier development and faster iterative improvement of automated
microbiological systems with a superior upper-bound performance than
traditional models would achieve.

</details>


### [37] [Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning](https://arxiv.org/abs/2510.00570)
*Minghao Yang,Ren Togo,Guang Li,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出基于LoRA的自适应共享专家(ASE)的MoE-MTL方法，通过联合归一化的共享专家与更多低秩细粒度专家，实现更高效的STL到MTL过渡和知识共享，PASCAL-Context上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MoE-MTL方法依赖单任务预训练骨干，导致从单任务到多任务迁移时存在冗余适配与低效知识共享，需要一种高效的专家分配与共享机制。

Method: 在LoRA基础上设计MoE框架，提出自适应共享专家(ASE)，使用路由器计算的门控权重对共享专家进行联合归一化，与稀疏专家共同作用；同时通过增加LoRA专家数量并按比例降低其秩来构建细粒度专家。

Result: 在PASCAL-Context数据集的统一训练设置下，ASE在各种配置中均稳定提升了多任务性能，且验证了细粒度专家设计的有效性。

Conclusion: ASE在缓解从STL到MTL过渡期间的冗余微调和提升知识共享方面有效，且通过细粒度LoRA专家数量增加并降低秩的方案提升了性能。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task
learning (MTL). However, existing MoE-MTL methods often rely on single-task
pretrained backbones and suffer from redundant adaptation and inefficient
knowledge sharing during the transition from single-task to multi-task learning
(STL to MTL). To address these limitations, we propose adaptive shared experts
(ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are
assigned router-computed gating weights jointly normalized with sparse experts.
This design facilitates STL to MTL transition, enhances expert specialization,
and cooperation. Furthermore, we incorporate fine-grained experts by increasing
the number of LoRA experts while proportionally reducing their rank, enabling
more effective knowledge sharing under a comparable parameter budget. Extensive
experiments on the PASCAL-Context benchmark, under unified training settings,
demonstrate that ASE consistently improves performance across diverse
configurations and validates the effectiveness of fine-grained designs for MTL.

</details>


### [38] [Arbitrary Generative Video Interpolation](https://arxiv.org/abs/2510.00578)
*Guozhen Zhang,Haiguang Wang,Chunyu Wang,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: ArbInterp通过时间感知RoPE和按段外观-运动解耦条件生成，实现了任意时间戳与任意长度的高质量视频插帧，在多倍数插帧测试中明显优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成式插帧方法只能生成固定数量的中间帧，缺乏对生成帧率或序列总时长的灵活控制，无法实现任意时间戳或任意长度的插帧需求。

Method: 提出TaRoPE以在时间维度上调制RoPE，使生成帧与目标归一化时间戳对齐；将长序列分解为多个段进行合成，并设计外观-运动解耦的条件策略：使用前段端点保持外观一致性，利用时间语义维持运动连贯。

Result: 在构建的多尺度插帧基准（2x到32x）上，ArbInterp在保真度和时空连续性方面均优于之前的方法，展示了强大的任意因子插帧能力。

Conclusion: 该论文提出了一个支持任意时间戳和任意长度插帧的生成式VFI框架ArbInterp，通过时间感知旋转位置编码（TaRoPE）实现细粒度时间控制，并通过按段生成与外观-运动解耦条件策略保证长序列一致性，实验证明在多倍数插帧（2x-32x）上优于先前方法。

Abstract: Video frame interpolation (VFI), which generates intermediate frames from
given start and end frames, has become a fundamental function in video
generation applications. However, existing generative VFI methods are
constrained to synthesize a fixed number of intermediate frames, lacking the
flexibility to adjust generated frame rates or total sequence duration. In this
work, we present ArbInterp, a novel generative VFI framework that enables
efficient interpolation at any timestamp and of any length. Specifically, to
support interpolation at any timestamp, we propose the Timestamp-aware Rotary
Position Embedding (TaRoPE), which modulates positions in temporal RoPE to
align generated frames with target normalized timestamps. This design enables
fine-grained control over frame timestamps, addressing the inflexibility of
fixed-position paradigms in prior work. For any-length interpolation, we
decompose long-sequence generation into segment-wise frame synthesis. We
further design a novel appearance-motion decoupled conditioning strategy: it
leverages prior segment endpoints to enforce appearance consistency and
temporal semantics to maintain motion coherence, ensuring seamless
spatiotemporal transitions across segments. Experimentally, we build
comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to
assess generalizability across arbitrary interpolation factors. Results show
that ArbInterp outperforms prior methods across all scenarios with higher
fidelity and more seamless spatiotemporal continuity. Project website:
https://mcg-nju.github.io/ArbInterp-Web/.

</details>


### [39] [Color Models in Image Processing: A Review and Experimental Comparison](https://arxiv.org/abs/2510.00584)
*Muragul Muratbekova,Nuray Toganas,Ayan Igali,Maksat Shagyrov,Elnara Kadyrgali,Adilet Yerkin,Pakizar Shamoi*

Main category: cs.CV

TL;DR: 综述与实验证明HS*族最接近人类感知，其他模型各有利弊，论文提出了开放问题与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 颜色表示对计算机视觉和人机交互至关重要，选择合适的颜色模型影响应用效果，需系统评估现有模型并发现不足。

Method: 文献综述+理论分析+一系列实验评估（设备依赖性、色度一致性、计算复杂度等）比较不同模型，并分析感知一致性。

Result: 实验显示HS*族在感知对齐方面表现最好；同时揭示现有模型在设备依赖、色度一致性或计算开销上存在缺陷。

Conclusion: 作者总结了多种颜色模型的特点、优缺点，并指出HS*族最符合人类感知，提出未来研究方向与挑战。

Abstract: Color representation is essential in computer vision and human-computer
interaction. There are multiple color models available. The choice of a
suitable color model is critical for various applications. This paper presents
a review of color models and spaces, analyzing their theoretical foundations,
computational properties, and practical applications. We explore traditional
models such as RGB, CMYK, and YUV, perceptually uniform spaces like CIELAB and
CIELUV, and fuzzy-based approaches as well. Additionally, we conduct a series
of experiments to evaluate color models from various perspectives, like device
dependency, chromatic consistency, and computational complexity. Our
experimental results reveal gaps in existing color models and show that the HS*
family is the most aligned with human perception. The review also identifies
key strengths and limitations of different models and outlines open challenges
and future directions This study provides a reference for researchers in image
processing, perceptual computing, digital media, and any other color-related
field.

</details>


### [40] [Multi-level Dynamic Style Transfer for NeRFs](https://arxiv.org/abs/2510.00592)
*Zesheng Li,Shuaibo Li,Wei Ma,Jianwei Guo,Hongbin Zha*

Main category: cs.CV

TL;DR: MDS-NeRF通过多级特征适配器与动态风格注入，重构NeRF管线实现高质量、多视角一致的三维风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF风格迁移方法直接将风格统计整合进原始NeRF，导致内容保持和风格表现都不理想，需专门为风格化重构管线与注入机制。

Method: 提出多级特征适配器将内容辐射场转换为多级特征栅格，使用动态风格注入模块从风格参考中提取相关风格特征并自适应融合，最后通过多级级联解码器生成最终视图；并扩展支持基于三维风格参考的全视角（omni-view）风格迁移。

Result: 大量实验证明MDS-NeRF在保持多尺度空间结构的同时，能有效迁移风格特征，在主观视觉质量和多视角一致性方面表现优秀。

Conclusion: MDS-NeRF通过重构NeRF管线并引入动态风格注入模块，实现了在保持多尺度空间结构的同时进行高质量三维风格迁移。

Abstract: As the application of neural radiance fields (NeRFs) in various 3D vision
tasks continues to expand, numerous NeRF-based style transfer techniques have
been developed. However, existing methods typically integrate style statistics
into the original NeRF pipeline, often leading to suboptimal results in both
content preservation and artistic stylization. In this paper, we present
multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that
reengineers the NeRF pipeline specifically for stylization and incorporates an
innovative dynamic style injection module. Particularly, we propose a
multi-level feature adaptor that helps generate a multi-level feature grid
representation from the content radiance field, effectively capturing the
multi-scale spatial structure of the scene. In addition, we present a dynamic
style injection module that learns to extract relevant style features and
adaptively integrates them into the content patterns. The stylized multi-level
features are then transformed into the final stylized view through our proposed
multi-level cascade decoder. Furthermore, we extend our 3D style transfer
method to support omni-view style transfer using 3D style references. Extensive
experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D
style transfer, preserving multi-scale spatial structures while effectively
transferring stylistic characteristics.

</details>


### [41] [LVLMs as inspectors: an agentic framework for category-level structural defect annotation](https://arxiv.org/abs/2510.00603)
*Sheng Jiang,Yuanmin Ning,Bingxi Huang,Peiyin Chen,Zhaohui Chen*

Main category: cs.CV

TL;DR: ADPT利用LVLM与语义模式匹配及迭代自问答，无需人工监督即可实现高精度结构缺陷自动标注，适用于构建可扩展高质量数据集以支持下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决人工标注成本高、效率低、可靠性差的问题，构建可扩展的高保真结构缺陷标注管线，支持下游迁移学习和域适应任务。

Method: 结合领域提示工程、语义模式匹配模块与递归验证自问答机制，ADPT将原始图像转为语义标注；利用LVLM对图像进行分类/描述，匹配预定义缺陷模式，并通过多轮自核验提升标签可靠性。

Result: 在类平衡设置中，实现区分缺陷/非缺陷图片高达98%准确率；四类缺陷标注准确率85%-98%；在类不平衡数据上达到80%-92%准确率。

Conclusion: ADPT能在无需人工监督的情况下，通过LVLMs+语义模式匹配和迭代自问答优化实现高质量结构缺陷自动标注。

Abstract: Automated structural defect annotation is essential for ensuring
infrastructure safety while minimizing the high costs and inefficiencies of
manual labeling. A novel agentic annotation framework, Agent-based Defect
Pattern Tagger (ADPT), is introduced that integrates Large Vision-Language
Models (LVLMs) with a semantic pattern matching module and an iterative
self-questioning refinement mechanism. By leveraging optimized domain-specific
prompting and a recursive verification process, ADPT transforms raw visual data
into high-quality, semantically labeled defect datasets without any manual
supervision. Experimental results demonstrate that ADPT achieves up to 98%
accuracy in distinguishing defective from non-defective images, and 85%-98%
annotation accuracy across four defect categories under class-balanced
settings, with 80%-92% accuracy on class-imbalanced datasets. The framework
offers a scalable and cost-effective solution for high-fidelity dataset
construction, providing strong support for downstream tasks such as transfer
learning and domain adaptation in structural damage assessment.

</details>


### [42] [Disentangling Foreground and Background for vision-Language Navigation via Online Augmentation](https://arxiv.org/abs/2510.00604)
*Yunbo Xu,Xuesong Zhang,Jia Li,Zhenzhen Hu,Richang Hong*

Main category: cs.CV

TL;DR: 提出COFA：通过语义驱动的前/背景分离与共识驱动的在线特征增强，交替利用前景和背景信息以提升VLN在未见环境的泛化并在REVERIE与R2R上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是前景提供语义提示而背景提供空间连通性信息，两者对导航泛化能力至关重要，但在以往工作中没有得到充分利用，因此希望通过交替增强两类特征来提升在未见环境中的导航表现。

Method: 方法包括：1) 基于语义强化的地标识别，将观测分解为候选前景和背景特征；2) 设计共识驱动的在线增强策略，通过两阶段投票（基于指令与位置的多样性）决定特征偏好并选择交替增强前/背景特征以输入导航模型。

Result: 在REVERIE与R2R基准上，COFA在提升基线模型泛化能力方面表现显著，实验显示采用在线前后景增强后模型取得了比基线更好的导航成功率与指示遵循指标，并实现了SOTA性能。

Conclusion: 本文结论认为前景与背景的交替增强能提高VLN在未见环境的泛化能力，所提COFA方法通过候选前后景特征与共识驱动的在线增强机制，提升了基线性能并在REVERIE与R2R数据集上取得了SOTA。

Abstract: Following language instructions, vision-language navigation (VLN) agents are
tasked with navigating unseen environments. While augmenting multifaceted
visual representations has propelled advancements in VLN, the significance of
foreground and background in visual observations remains underexplored.
Intuitively, foreground regions provide semantic cues, whereas the background
encompasses spatial connectivity information. Inspired on this insight, we
propose a Consensus-driven Online Feature Augmentation strategy (COFA) with
alternative foreground and background features to facilitate the navigable
generalization. Specifically, we first leverage semantically-enhanced landmark
identification to disentangle foreground and background as candidate augmented
features. Subsequently, a consensus-driven online augmentation strategy
encourages the agent to consolidate two-stage voting results on feature
preferences according to diverse instructions and navigational locations.
Experiments on REVERIE and R2R demonstrate that our online
foreground-background augmentation boosts the generalization of baseline and
attains state-of-the-art performance.

</details>


### [43] [Robust Context-Aware Object Recognition](https://arxiv.org/abs/2510.00618)
*Klara Janouskova,Cristian Gavrus,Jiri Matas*

Main category: cs.CV

TL;DR: RCOR通过将定位与识别解耦并采用鲁棒融合，同时兼顾稳健性与上下文利用，改善了在不同背景下的视觉识别性能。


<details>
  <summary>Details</summary>
Motivation: 常规监督学习会导致模型对背景的过度依赖（捷径学习），虽有研究通过压制背景以提高泛化，但会丢失有用上下文信息；因此需要一种在保留上下文的同时提升稳健性的方案。

Method: 将定位作为识别过程的核心组成，先分离物体与背景的表征（物体中心化与上下文意识并行建模），随后采用鲁棒、非参数的融合策略整合两者信息。

Result: 在含不同背景分布的数据集上，RCOR提高了监督模型与视觉-语言模型（VLM）的表现，甚至无需微调，在复杂场景（如ImageNet-1k）中也能实现先定位后识别。

Conclusion: RCOR能同时实现对背景稳健性与保留上下文信息，不牺牲任一一方，从而提升识别模型在域内与域外背景数据上的性能。

Abstract: In visual recognition, both the object of interest (referred to as
foreground, FG, for simplicity) and its surrounding context (background, BG)
play an important role. However, standard supervised learning often leads to
unintended over-reliance on the BG, known as shortcut learning of spurious
correlations, limiting model robustness in real-world deployment settings. In
the literature, the problem is mainly addressed by suppressing the BG,
sacrificing context information for improved generalization.
  We propose RCOR -- Robust Context-Aware Object Recognition -- the first
approach that jointly achieves robustness and context-awareness without
compromising either. RCOR treats localization as an integral part of
recognition to decouple object-centric and context-aware modelling, followed by
a robust, non-parametric fusion. It improves the performance of both supervised
models and VLM on datasets with both in-domain and out-of-domain BG, even
without fine-tuning. The results confirm that localization before recognition
is now possible even in complex scenes as in ImageNet-1k.

</details>


### [44] [UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs](https://arxiv.org/abs/2510.00624)
*Mengfei Xia,Nan Xue,Jiapeng Zhu,Yujun Shen*

Main category: cs.CV

TL;DR: 去掉判别器的条件输入（UCD）能强迫判别器学习更全面的特征，从而改善GAN收敛与生成质量，实验证明在ImageNet-64上取得1.47 FID，优于多种强基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于对抗训练的一步生成（如GAN和扩散蒸馏）受限于GAN训练不收敛和模式坍缩问题；作者认为判别器接受条件作为输入会形成捷径，阻碍其学习有意义表示，进而影响监督生成器的能力。

Method: 通过理论分析证明条件注入会成为判别器的冗余捷径，提出在判别器中移除条件信息（UCD），保持生成器端带条件生成，且与GAN的经典理论兼容，可作为插件使用。并在ImageNet-64等数据集上大规模实验验证性能提升。

Result: 实验显示显著提升，例如在ImageNet-64上达到1.47 FID，超越StyleGAN-XL和若干先进的一步扩散模型；且理论保证了与vanilla GAN兼容，UCD可作为插件高效实现。

Conclusion: 本文提出无条件判别器（UCD），去除判别器对条件的输入，旨在避免GAN训练中判别器利用条件作为捷径，提升判别器提取有意义特征的能力，从而推动与生成器的纳什均衡，改善收敛与模式崩溃问题。

Abstract: Adversarial training turns out to be the key to one-step generation,
especially for Generative Adversarial Network (GAN) and diffusion model
distillation. Yet in practice, GAN training hardly converges properly and
struggles in mode collapse. In this work, we quantitatively analyze the extent
of Nash equilibrium in GAN training, and conclude that redundant shortcuts by
inputting condition in $D$ disables meaningful knowledge extraction. We thereby
propose to employ an unconditional discriminator (UCD), in which $D$ is
enforced to extract more comprehensive and robust features with no condition
injection. In this way, $D$ is able to leverage better knowledge to supervise
$G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee
on compatibility with vanilla GAN theory indicates that UCD can be implemented
in a plug-in manner. Extensive experiments confirm the significant performance
improvements with high efficiency. For instance, we achieved \textbf{1.47 FID}
on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art
one-step diffusion models. The code will be made publicly available.

</details>


### [45] [Virtual Fashion Photo-Shoots: Building a Large-Scale Garment-Lookbook Dataset](https://arxiv.org/abs/2510.00633)
*Yannick Hauri,Luca A. Lanzendörfer,Till Aczel*

Main category: cs.CV

TL;DR: 提出虚拟时尚拍摄任务并构建大规模分层服装—型录配对数据集，通过视觉-语言与定位的自动检索管道实现跨域配对，促进从目录式生成向富有叙事性的时尚影像转变。


<details>
  <summary>Details</summary>
Motivation: 现有时尚图像生成多聚焦于虚拟试穿等狭窄任务，缺乏编辑时尚中动态姿态、多样场景与叙事性的表达，因而提出新任务以填补电商与时尚媒体之间的鸿沟。

Method: 通过构建服装—型录（garment-lookbook）配对数据集，设计自动检索管道，将视觉-语言推理与目标级定位相结合，实现跨域服装对齐，并按质量分层（高、中、低）构建数据集。

Result: 构建了首个大规模服装—型录配对数据集，包含高质量1万对、中质量5万对、低质量30万对，为未来向富有创意和叙事性的时尚图像生成模型提供训练和评估基础。

Conclusion: 文章提出了“虚拟时尚拍摄”任务，将电商标准化服装图像转化为具有编辑感的时尚照片，丰富了时尚图像生成的应用场景。

Abstract: Fashion image generation has so far focused on narrow tasks such as virtual
try-on, where garments appear in clean studio environments. In contrast,
editorial fashion presents garments through dynamic poses, diverse locations,
and carefully crafted visual narratives. We introduce the task of virtual
fashion photo-shoot, which seeks to capture this richness by transforming
standardized garment images into contextually grounded editorial imagery. To
enable this new direction, we construct the first large-scale dataset of
garment-lookbook pairs, bridging the gap between e-commerce and fashion media.
Because such pairs are not readily available, we design an automated retrieval
pipeline that aligns garments across domains, combining visual-language
reasoning with object-level localization. We construct a dataset with three
garment-lookbook pair accuracy levels: high quality (10,000 pairs), medium
quality (50,000 pairs), and low quality (300,000 pairs). This dataset offers a
foundation for models that move beyond catalog-style generation and toward
fashion imagery that reflects creativity, atmosphere, and storytelling.

</details>


### [46] [LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection](https://arxiv.org/abs/2510.00634)
*Jiayao Jiang,Siran Peng,Bin Liu,Qi Chu,Nenghai Yu*

Main category: cs.CV

TL;DR: 该工作通过用可学习样条激活函数构建KAN并引入基于面部关键点的LAKAN模块，实现了针对面部伪造更灵活的表示与区域引导，从而在多个数据集上取得领先的检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和Transformer的方法对复杂、非线性的伪造伪迹建模仍有限，固定激活函数限制了网络表达能力；利用可学习激活和面部几何先验可更好捕捉伪造特征并引导网络关注重要区域。

Method: 用可学习的样条函数替换固定激活函数构建KAN；设计LAKAN模块以面部关键点作为结构先验，动态生成KAN的内部参数，形成实例特定信号以引导通用图像编码器关注含伪造伪迹的关键面部区域；在多个公开数据集上进行广泛实验验证。

Result: 在多个公开数据集上的实验显示所提方法优于现有方法，表现出更好的检测准确性和鲁棒性（文中宣称的结果）。

Conclusion: 该论文提出了一种基于Kolmogorov-Arnold Network(KAN)的面部伪造检测方法，并在此基础上设计了Landmark-assisted Adaptive KAN(LAKAN)模块，通过可学习样条激活函数和利用面部关键点生成实例相关参数，引导编码器关注重要面部区域，从而提升检测性能。

Abstract: The rapid development of deepfake generation techniques necessitates robust
face forgery detection algorithms. While methods based on Convolutional Neural
Networks (CNNs) and Transformers are effective, there is still room for
improvement in modeling the highly complex and non-linear nature of forgery
artifacts. To address this issue, we propose a novel detection method based on
the Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions
with learnable splines, our KAN-based approach is better suited to this
challenge. Furthermore, to guide the network's focus towards critical facial
areas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network
(LAKAN) module. This module uses facial landmarks as a structural prior to
dynamically generate the internal parameters of the KAN, creating an
instance-specific signal that steers a general-purpose image encoder towards
the most informative facial regions with artifacts. This core innovation
creates a powerful combination between geometric priors and the network's
learning process. Extensive experiments on multiple public datasets show that
our proposed method achieves superior performance.

</details>


### [47] [Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack](https://arxiv.org/abs/2510.00635)
*Nanxiang Jiang,Zhaoxin Fan,Enhan Kang,Daiheng Gao,Yun Zhou,Yanxia Chang,Zheng Zhu,Yeying Jin,Wenjun Wu*

Main category: cs.CV

TL;DR: ReFlux 提出反向注意力优化、速度引导动态和一致性保持目标，有效攻击 Flux 等 rectified flow T2I 模型上的概念抹除，提供了评估鲁棒性的可靠基准。


<details>
  <summary>Details</summary>
Motivation: 现有的概念抹除方法主要针对 Stable Diffusion，转移到基于 rectified flow 的 Transformer（如 Flux）时效果有限，原因在于这些方法在 Flux 上依赖注意力定位现象，因而需要专门针对该属性的攻击评估方法。

Method: 提出了反向注意力优化策略以重新激活被抑制信号并稳定注意力；引入速度引导动态（velocity-guided dynamic）以在配流过程（flow matching）中增强重激活的鲁棒性；并加入一致性保持目标以维持全局布局和保留无关内容。

Result: 大量实验表明 ReFlux 在有效性和效率上均优越，成为评估 rectified flow transformer 中概念抹除稳健性的可靠基准。

Conclusion: ReFlux 是专为最新的基于 rectified flow 的文本到图像模型（如 Flux）设计的概念攻击方法，能有效对抗现有概念抹除技术，重新激活被抑制的语义信号，同时保持图像全局布局和无关内容的一致性。

Abstract: Recent advances in text-to-image (T2I) diffusion models have enabled
impressive generative capabilities, but they also raise significant safety
concerns due to the potential to produce harmful or undesirable content. While
concept erasure has been explored as a mitigation strategy, most existing
approaches and corresponding attack evaluations are tailored to Stable
Diffusion (SD) and exhibit limited effectiveness when transferred to
next-generation rectified flow transformers such as Flux. In this work, we
present ReFlux, the first concept attack method specifically designed to assess
the robustness of concept erasure in the latest rectified flow-based T2I
framework. Our approach is motivated by the observation that existing concept
erasure techniques, when applied to Flux, fundamentally rely on a phenomenon
known as attention localization. Building on this insight, we propose a simple
yet effective attack strategy that specifically targets this property. At its
core, a reverse-attention optimization strategy is introduced to effectively
reactivate suppressed signals while stabilizing attention. This is further
reinforced by a velocity-guided dynamic that enhances the robustness of concept
reactivation by steering the flow matching process, and a
consistency-preserving objective that maintains the global layout and preserves
unrelated content. Extensive experiments consistently demonstrate the
effectiveness and efficiency of the proposed attack method, establishing a
reliable benchmark for evaluating the robustness of concept erasure strategies
in rectified flow transformers.

</details>


### [48] [FIN: Fast Inference Network for Map Segmentation](https://arxiv.org/abs/2510.00651)
*Ruan Bispo,Tim Brophy,Reenu Mohandas,Anthony Scanlan,Ciarán Eising*

Main category: cs.CV

TL;DR: 利用相机-雷达的BEV融合，配合改进损失与轻量化头，本文实现了高精度（53.5 mIoU）与超快推理（比最强基线快260%）的实时地图分割。


<details>
  <summary>Details</summary>
Motivation: 多传感器融合能把相机的语义信息和雷达的精距数据结合，提供成本与计算效率兼顾的感知方案。地图分割需兼顾高精度与实时性，但现有方法在这两方面难以同时满足。

Method: 在BEV表示下融合相机与雷达信息，使用一套‘高级损失’（可能包含类别平衡、边界/IoU相关项等）并设计新的轻量化分割头来提高每类性能和整体效率，从而兼顾精度与实时性。

Result: 提出方法在保持高精度（53.5 mIoU，与大型模型相近）同时大幅提升推理速度，比最强基线模型推理时间提高260%。

Conclusion: 本文提出了一种基于摄像头与雷达的实时BEV地图分割高效架构，通过引入改进损失函数与轻量化头部设计，在保证精度的同时显著提升推理速度，达到与大型模型可比的性能。

Abstract: Multi-sensor fusion in autonomous vehicles is becoming more common to offer a
more robust alternative for several perception tasks. This need arises from the
unique contribution of each sensor in collecting data: camera-radar fusion
offers a cost-effective solution by combining rich semantic information from
cameras with accurate distance measurements from radar, without incurring
excessive financial costs or overwhelming data processing requirements. Map
segmentation is a critical task for enabling effective vehicle behaviour in its
environment, yet it continues to face significant challenges in achieving high
accuracy and meeting real-time performance requirements. Therefore, this work
presents a novel and efficient map segmentation architecture, using cameras and
radars, in the \acrfull{bev} space. Our model introduces a real-time map
segmentation architecture considering aspects such as high accuracy, per-class
balancing, and inference time. To accomplish this, we use an advanced loss set
together with a new lightweight head to improve the perception results. Our
results show that, with these modifications, our approach achieves results
comparable to large models, reaching 53.5 mIoU, while also setting a new
benchmark for inference time, improving it by 260\% over the strongest baseline
models.

</details>


### [49] [OTTER: Open-Tagging via Text-Image Representation for Multi-modal Understanding](https://arxiv.org/abs/2510.00652)
*Jieer Ouyang,Xiaoneng Xiang,Zheng Wang,Yangkai Ding*

Main category: cs.CV

TL;DR: OTTER通过层次化多模态数据与多头注意力联合对齐固定与开放标签，成功在开放集多标签任务上兼顾稳定性与灵活性，取得显著且接近完美的开放标签性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多标签系统在预定义类别一致性与用户驱动开放标签灵活性之间的矛盾，提供一个既稳定又可扩展的标注方案。

Method: 构建大规模层次化多模态数据集，使用自动视觉-语言标注加人工精修的混合标注流程；采用多头注意力架构同时对齐视觉与文本表征，并与固定与开放集标签嵌入联合训练以实现动态语义一致的标注。

Result: 在两个基准数据集上超过竞争基线：Otter总体F1=0.81（领先0.10），Favorite总体F1=0.75（领先0.02）；开放集标签F1接近完美：Otter=0.99，Favorite=0.97；同时在预定义标签上保持竞争性准确率。

Conclusion: OTTER提出了一个结合预定义类别稳定性与开放标签适应性的统一多标签框架，能够在保持封闭集一致性的同时实现开放词汇灵活性。

Abstract: We introduce OTTER, a unified open-set multi-label tagging framework that
harmonizes the stability of a curated, predefined category set with the
adaptability of user-driven open tags. OTTER is built upon a large-scale,
hierarchically organized multi-modal dataset, collected from diverse online
repositories and annotated through a hybrid pipeline combining automated
vision-language labeling with human refinement. By leveraging a multi-head
attention architecture, OTTER jointly aligns visual and textual representations
with both fixed and open-set label embeddings, enabling dynamic and
semantically consistent tagging. OTTER consistently outperforms competitive
baselines on two benchmark datasets: it achieves an overall F1 score of 0.81 on
Otter and 0.75 on Favorite, surpassing the next-best results by margins of 0.10
and 0.02, respectively. OTTER attains near-perfect performance on open-set
labels, with F1 of 0.99 on Otter and 0.97 on Favorite, while maintaining
competitive accuracy on predefined labels. These results demonstrate OTTER's
effectiveness in bridging closed-set consistency with open-vocabulary
flexibility for multi-modal tagging applications.

</details>


### [50] [Weakly Supervised Cloud Detection Combining Spectral Features and Multi-Scale Deep Network](https://arxiv.org/abs/2510.00654)
*Shaocong Zhu,Zhiwei Li,Xinghua Li,Huanfeng Shen*

Main category: cs.CV

TL;DR: 针对薄云和样本质量问题，SpecMCD用光谱+多尺度场景级弱监督的渐进训练，生成像素级概率图并自适应阈值分割，F1提升>7.82%。


<details>
  <summary>Details</summary>
Motivation: 薄云特征不显著且训练样本质量低限制了深度学习云检测精度，需利用场景级弱监督与光谱信息弥补像素级标注不足。

Method: 提出渐进训练的多尺度场景级云检测网络，融合多尺度概率图与云厚度图生成像素级云概率图，并基于不同尺度场景级云掩模差异区域自适应阈值与距离加权优化得到二值云掩模。

Result: 在两个包含60幅高分一号多光谱图像的数据集（WDCD与GF1MS-WHU）上验证，SpecMCD相比其他弱监督方法（如WDCD与WSFNet）F1-score提升超过7.82%。

Conclusion: SpecMCD方法通过结合光谱特征与多尺度场景级深度网络的弱监督框架，实现了对像素级云掩模的显著改进，特别是在薄云和低质量训练样本场景下性能提升明显。

Abstract: Clouds significantly affect the quality of optical satellite images, which
seriously limits their precise application. Recently, deep learning has been
widely applied to cloud detection and has achieved satisfactory results.
However, the lack of distinctive features in thin clouds and the low quality of
training samples limit the cloud detection accuracy of deep learning methods,
leaving space for further improvements. In this paper, we propose a weakly
supervised cloud detection method that combines spectral features and
multi-scale scene-level deep network (SpecMCD) to obtain highly accurate
pixel-level cloud masks. The method first utilizes a progressive training
framework with a multi-scale scene-level dataset to train the multi-scale
scene-level cloud detection network. Pixel-level cloud probability maps are
then obtained by combining the multi-scale probability maps and cloud thickness
map based on the characteristics of clouds in dense cloud coverage and large
cloud-area coverage images. Finally, adaptive thresholds are generated based on
the differentiated regions of the scene-level cloud masks at different scales
and combined with distance-weighted optimization to obtain binary cloud masks.
Two datasets, WDCD and GF1MS-WHU, comprising a total of 60 Gaofen-1
multispectral (GF1-MS) images, were used to verify the effectiveness of the
proposed method. Compared to the other weakly supervised cloud detection
methods such as WDCD and WSFNet, the F1-score of the proposed SpecMCD method
shows an improvement of over 7.82%, highlighting the superiority and potential
of the SpecMCD method for cloud detection under different cloud coverage
conditions.

</details>


### [51] [Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents](https://arxiv.org/abs/2510.00658)
*Beomsu Kim,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 发现一致性模型训练末期切向量在流形上振荡；提出流形特征距离损失使切向量指向数据流形，从而大幅加速训练并提升样本质量，且适用于超小批量训练。


<details>
  <summary>Details</summary>
Motivation: 一致性模型能实现极少步数的高质量采样，但通常需要长时间训练和大批量以获得竞争性样本质量。论文旨在理解训练晚期 CM 的失败原因并提出能加速训练且对批量规模不敏感的优化方法。

Method: 论文提出 Align Your Tangent（AYT）方法：在训练一致性模型时加入 MFD 损失，该损失度量模型输出与数据流形特征之间的距离，从而引导模型更新朝向流形；理论上分析了 CM 切向量的振荡性并证明 MFD 使切向量与流形对齐；在多个数据集上进行实验，展示加速训练、在小批量下保持样本质量并在 LPIPS 指标上超越基线。

Result: 引入 MFD 的 AYT 方法能将 CM 训练速度提升数个数量级，并在主观/感知质量（如 LPIPS）上超过原始方法；还能在极小批量训练下维持或提升样本质量。实验代码公开。

Conclusion: 该论文通过分析一致性模型（CMs）训练收敛附近的动态，发现 CM 的切向量（输出更新方向）在数据流形附近呈现振荡，沿流形平行移动而非指向流形。为解决该问题，提出了一种新的损失函数“流形特征距离（MFD）”，使切向量与流形对齐并指向数据流形，从而显著加速训练并改善样本质量，同时支持极小批量训练。

Abstract: With diffusion and flow matching models achieving state-of-the-art generating
performance, the interest of the community now turned to reducing the inference
time without sacrificing sample quality. Consistency Models (CMs), which are
trained to be consistent on diffusion or probability flow ordinary differential
equation (PF-ODE) trajectories, enable one or two-step flow or diffusion
sampling. However, CMs typically require prolonged training with large batch
sizes to obtain competitive sample quality. In this paper, we examine the
training dynamics of CMs near convergence and discover that CM tangents -- CM
output update directions -- are quite oscillatory, in the sense that they move
parallel to the data manifold, not towards the manifold. To mitigate
oscillatory tangents, we propose a new loss function, called the manifold
feature distance (MFD), which provides manifold-aligned tangents that point
toward the data manifold. Consequently, our method -- dubbed Align Your Tangent
(AYT) -- can accelerate CM training by orders of magnitude and even out-perform
the learned perceptual image patch similarity metric (LPIPS). Furthermore, we
find that our loss enables training with extremely small batch sizes without
compromising sample quality. Code: https://github.com/1202kbs/AYT

</details>


### [52] [Unsupervised Unfolded rPCA (U2-rPCA): Deep Interpretable Clutter Filtering for Ultrasound Microvascular Imaging](https://arxiv.org/abs/2510.00660)
*Huaying Li,Liansheng Wang,Yinran Chen*

Main category: cs.CV

TL;DR: 提出一种无监督可解释的展开rPCA（U2-rPCA），通过IRLS展开与稀疏增强实现更好微血管杂波滤波，实验证明CNR显著提升且模块有效。


<details>
  <summary>Details</summary>
Motivation: 现有SVD和rPCA在特征建模与组织-血流分离上不足，监督学习方法虽有潜力但受可解释性和缺乏体外/体内标注限制；因此需要一个无监督且可解释的深度滤波器。

Method: 基于IRLS rPCA基线展开网络结构，引入低秩与稀疏正则化，并添加稀疏增强单元以强化微流信号提取；网络在序列的前几帧自适应训练后用于后续帧过滤。

Result: 在仿真及公开体内数据集上，U2-rPCA优于SVD、rPCA基线和另一深度滤波器，功率多普勒图像CNR提升2–10 dB；消融实验验证了各模块有效性。

Conclusion: 本文提出的U2-rPCA通过无监督深度展开的方式，实现了兼具可解释性和无需标注的超声微血管成像杂波滤波，显著提升了血流-组织分离性能。

Abstract: High-sensitivity clutter filtering is a fundamental step in ultrasound
microvascular imaging. Singular value decomposition (SVD) and robust principal
component analysis (rPCA) are the main clutter filtering strategies. However,
both strategies are limited in feature modeling and tissue-blood flow
separation for high-quality microvascular imaging. Recently, deep
learning-based clutter filtering has shown potential in more thoroughly
separating tissue and blood flow signals. However, the existing supervised
filters face the challenges of interpretability and lack of in-vitro and
in-vivo ground truths. While the interpretability issue can be addressed by
algorithm deep unfolding, the training ground truth remains unsolved. To this
end, this paper proposes an unsupervised unfolded rPCA (U2-rPCA) method that
preserves mathematical interpretability and is insusceptible to learning
labels. Specifically, U2-rPCA is unfolded from an iteratively reweighted least
squares (IRLS) rPCA baseline with intrinsic low-rank and sparse regularization.
A sparse-enhancement unit is added to the network to strengthen its capability
to capture the sparse micro-flow signals. U2-rPCA is like an adaptive filter
that is trained with part of the image sequence and then used for the following
frames. Experimental validations on a in-silico dataset and public in-vivo
datasets demonstrated the outperformance of U2-rPCA when compared with the
SVD-based method, the rPCA baseline, and another deep learning-based filter.
Particularly, the proposed method improved the contrastto-noise ratio (CNR) of
the power Doppler image by 2 dB to 10 dB when compared with other methods.
Furthermore, the effectiveness of the building modules of U2-rPCA was validated
through ablation studies.

</details>


### [53] [Multi-Domain Brain Vessel Segmentation Through Feature Disentanglement](https://arxiv.org/abs/2510.00665)
*Francesco Galati,Daniele Falcetta,Rosa Cortese,Ferran Prados,Ninon Burgos,Maria A. Zuluaga*

Main category: cs.CV

TL;DR: 提出一种基于解耦的图像到图像翻译域适配框架，通过保持空间信息并调整血管外观，实现跨模态、跨中心的脑血管分割，结果显示方法稳健且适应性强。


<details>
  <summary>Details</summary>
Motivation: 脑血管复杂形态导致自动分割困难，单一成像模态的模型泛化性差；临床需要跨模态、跨中心的统一血管树理解，促使开发不依赖域特定设计的数据适配方法。

Method: 基于图像属性的解耦（disentanglement），独立操纵外观属性（尤其是血管外观）而保留空间信息（形状和位置信息），在不改变标签的前提下将图像从源域迁移到目标域，从而实现域间迁移。

Result: 在跨医疗中心、图像模态和血管类型的大域差上取得了稳健的分割表现，并通过消融实验分析了所需标注数量及架构选择的影响，表现出框架的稳健性与通用性。

Conclusion: 该论文提出了一种通过图像到图像翻译的域自适应框架，用于对不同数据集、模态和医疗中心的脑动脉和静脉进行分割，避免了针对特定域的模型设计和数据统一化需求。

Abstract: The intricate morphology of brain vessels poses significant challenges for
automatic segmentation models, which usually focus on a single imaging
modality. However, accurately treating brain-related conditions requires a
comprehensive understanding of the cerebrovascular tree, regardless of the
specific acquisition procedure. Our framework effectively segments brain
arteries and veins in various datasets through image-to-image translation while
avoiding domain-specific model design and data harmonization between the source
and the target domain. This is accomplished by employing disentanglement
techniques to independently manipulate different image properties, allowing
them to move from one domain to another in a label-preserving manner.
Specifically, we focus on manipulating vessel appearances during adaptation
while preserving spatial information, such as shapes and locations, which are
crucial for correct segmentation. Our evaluation effectively bridges large and
varied domain gaps across medical centers, image modalities, and vessel types.
Additionally, we conduct ablation studies on the optimal number of required
annotations and other architectural choices. The results highlight our
framework's robustness and versatility, demonstrating the potential of domain
adaptation methodologies to perform cerebrovascular image segmentation in
multiple scenarios accurately. Our code is available at
https://github.com/i-vesseg/MultiVesSeg.

</details>


### [54] [A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models](https://arxiv.org/abs/2510.00666)
*Leah Bar,Liron Mor Yosef,Shai Zucker,Neta Shoham,Inbar Seroussi,Nir Sochen*

Main category: cs.CV

TL;DR: 将图像视为低维流形，结合核概率方法提出MPPM/LMPPM，将扩散模型解释为流形投影，LMPPM在恢复与生成上超越LDM。


<details>
  <summary>Details</summary>
Motivation: 当前生成图像方法忽视数据的流形几何结构，且潜空间分布常被视为不重要或预设；希望将几何和概率视角统一以更好刻画“好图像”流形并改进生成与恢复性能。

Method: 提出一个几何框架并结合核方法的概率建模，解释扩散模型为一种投影操作；构建在像素表示空间和潜变量空间同时工作的确定性模型MPPM与其潜变量版本LMPPM，并通过实验对比LDM。

Result: LMPPM在多个数据集上在图像恢复和生成任务中均优于Latent Diffusion Model (LDM)，展示了该框架的有效性。

Conclusion: 本文将几何观点与概率方法统一，提出了Manifold-Probabilistic Projection Model (MPPM)，并解释扩散模型为投影到“好图像”流形的机制，进而构建确定性模型LMPPM，在多个数据集上在图像恢复与生成任务优于LDM。

Abstract: The foundational premise of generative AI for images is the assumption that
images are inherently low-dimensional objects embedded within a
high-dimensional space. Additionally, it is often implicitly assumed that
thematic image datasets form smooth or piecewise smooth manifolds. Common
approaches overlook the geometric structure and focus solely on probabilistic
methods, approximating the probability distribution through universal
approximation techniques such as the kernel method. In some generative models,
the low dimensional nature of the data manifest itself by the introduction of a
lower dimensional latent space. Yet, the probability distribution in the latent
or the manifold coordinate space is considered uninteresting and is predefined
or considered uniform. This study unifies the geometric and probabilistic
perspectives by providing a geometric framework and a kernel-based
probabilistic method simultaneously. The resulting framework demystifies
diffusion models by interpreting them as a projection mechanism onto the
manifold of ``good images''. This interpretation leads to the construction of a
new deterministic model, the Manifold-Probabilistic Projection Model (MPPM),
which operates in both the representation (pixel) space and the latent space.
We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion
Model (LDM) across various datasets, achieving superior results in terms of
image restoration and generation.

</details>


### [55] [Beyond one-hot encoding? Journey into compact encoding for large multi-class segmentation](https://arxiv.org/abs/2510.00667)
*Aaron Kujawa,Thomas Booth,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出将one-hot替换为多种二进制编码以降低多类分割的计算/内存开销，但在108类3D脑分割任务上，所有二进制方案均未达到one-hot的分割质量（DSC显著下降），提示编码紧缩与性能保持间存在困难。


<details>
  <summary>Details</summary>
Motivation: 针对多类医学图像分割中类别数量庞大导致的线性增长的计算和内存开销，寻求编码策略将资源复杂度降为对数级，以便在大类数任务中提高可扩展性。

Method: 将传统的one-hot标签替换为多种二进制编码方案：基础二进制、带纠错的ECOC、类权重调整、硬/软解码、类到码字的映射设计以及标签嵌入树。评估指标为Dice Similarity Coefficient，在3D全脑108类分割任务上比较不同方法的性能与资源消耗。

Result: 二进制编码方法确实将复杂度从线性降为对数，但分割质量明显低于one-hot基线（one-hot DSC=82.4±2.8；二进制方法DSC范围39.3–73.8）。即使尝试ECOC、权重、软/硬解码和嵌入树等改进，仍未恢复到基线性能。作者公开这些负面结果以促进后续研究。

Conclusion: 使用二进制编码能显著降低计算复杂度和内存需求（从线性降到对数级），但在本文的3D脑分割（108类）任务中未能达到one-hot的分割性能，DSC显著下降。作者报告了多种编码变体均未达到基线，强调了负面结果的重要性。

Abstract: This work presents novel methods to reduce computational and memory
requirements for medical image segmentation with a large number of classes. We
curiously observe challenges in maintaining state-of-the-art segmentation
performance with all of the explored options. Standard learning-based methods
typically employ one-hot encoding of class labels. The computational complexity
and memory requirements thus increase linearly with the number of classes. We
propose a family of binary encoding approaches instead of one-hot encoding to
reduce the computational complexity and memory requirements to logarithmic in
the number of classes. In addition to vanilla binary encoding, we investigate
the effects of error-correcting output codes (ECOCs), class weighting,
hard/soft decoding, class-to-codeword assignment, and label embedding trees. We
apply the methods to the use case of whole brain parcellation with 108 classes
based on 3D MRI images. While binary encodings have proven efficient in
so-called extreme classification problems in computer vision, we faced
challenges in reaching state-of-the-art segmentation quality with binary
encodings. Compared to one-hot encoding (Dice Similarity Coefficient (DSC) =
82.4 (2.8)), we report reduced segmentation performance with the binary
segmentation approaches, achieving DSCs in the range from 39.3 to 73.8.
Informative negative results all too often go unpublished. We hope that this
work inspires future research of compact encoding strategies for large
multi-class segmentation tasks.

</details>


### [56] [Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation](https://arxiv.org/abs/2510.00681)
*Jinchang Zhang,Zijun Li,Jiakai Lin,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出利用图像教师（CLIP）对事件学生进行空间注意力蒸馏，并用自适应SNN进行事件分割的混合SNN-CNN框架，实现事件数据上的开放词汇检测。


<details>
  <summary>Details</summary>
Motivation: 事件相机具备高响应速度、低延迟和抗运动模糊等优点，但缺乏纹理和颜色信息，且现有事件检测方法通常仅限于预定义类别，难以推广到新颖对象；直接将CLIP应用到事件流效果差，因此需要桥接图像与事件模态之间的差距。

Method: 设计了空间注意力蒸馏策略，使学生网络能从原始事件输入中直接学习有意义的视觉特征；提出混合SNN-CNN架构，其中SNN自适应确定事件分割时刻以保留关键时序信息，CNN负责后续的特征提取与检测。

Result: 通过事件-图像知识蒸馏和混合SNN-CNN架构，本方法在事件数据上实现了开放词汇目标检测，能够继承CLIP的广泛视觉知识并保留事件流的关键时序信息，克服了固定分组事件分割造成的信息丢失问题。

Conclusion: 本论文提出了一种基于事件-图像知识蒸馏的开放词汇事件目标检测框架，通过使用图像输入的教师模型（利用CLIP语义能力）来指导事件流学生模型学习丰富的视觉表示，从而实现事件数据上的开放词汇检测。

Abstract: Event cameras offer advantages in object detection tasks due to high-speed
response, low latency, and robustness to motion blur. However, event cameras
lack texture and color information, making open-vocabulary detection
particularly challenging. Current event-based detection methods are typically
trained on predefined categories, limiting their ability to generalize to novel
objects, where encountering previously unseen objects is common.
Vision-language models (VLMs) have enabled open-vocabulary object detection in
RGB images. However, the modality gap between images and event streams makes it
ineffective to directly transfer CLIP to event data, as CLIP was not designed
for event streams. To bridge this gap, we propose an event-image knowledge
distillation framework that leverages CLIP's semantic understanding to achieve
open-vocabulary object detection on event data. Instead of training CLIP
directly on event streams, we use image frames as inputs to a teacher model,
guiding the event-based student model to learn CLIP's rich visual
representations. Through spatial attention-based distillation, the student
network learns meaningful visual features directly from raw event inputs while
inheriting CLIP's broad visual knowledge. Furthermore, to prevent information
loss due to event data segmentation, we design a hybrid spiking neural network
(SNN) and convolutional neural network (CNN) framework. Unlike fixed-group
event segmentation methods, which often discard crucial temporal information,
our SNN adaptively determines the optimal event segmentation moments, ensuring
that key temporal features are extracted. The extracted event features are then
processed by CNNs for object detection.

</details>


### [57] [ProtoMask: Segmentation-Guided Prototype Learning](https://arxiv.org/abs/2510.00683)
*Steffen Meinert,Philipp Schlinge,Nils Strodthoff,Martin Atzmueller*

Main category: cs.CV

TL;DR: ProtoMask用分割生成的语义补丁约束原型可视化区域，从而提高原型解释的真实性与质量，在细粒度分类任务上兼顾性能与更可信的解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的可解释方法常依赖后置显著性技术来解释原型语义，但这些技术的可靠性和质量受质疑。通过利用强大的分割基础模型，把显著性计算限制在语义补丁上，可以降低可视化的不确定性，提升映射真实性。

Method: 使用基础图像分割模型生成多个语义分割掩码，对每个掩码取边界框裁剪原图，得到多个输入分支；每个分支在基于原型的案例推理框架中学习原型并只在对应语义区域计算显著图，从而减少显著性映射与输入空间的歧义。模型命名为ProtoMask。

Result: 在三个常用细粒度分类数据集上，使用多种指标评估可解释性特征，ProtoMask在分类性能上与其他流行模型具有竞争力，同时在解释性指标（如显著性局部性、语义一致性等）上展现出独特优势。

Conclusion: 本文提出ProtoMask，通过用图像分割模型生成语义块并用其边界框裁剪输入来限制原型可视化范围，从而提高原型可解释性与可真实性。实验显示在细粒度分类数据集上具有竞争性能并带来独特解释性特征。

Abstract: XAI gained considerable importance in recent years. Methods based on
prototypical case-based reasoning have shown a promising improvement in
explainability. However, these methods typically rely on additional post-hoc
saliency techniques to explain the semantics of learned prototypes. Multiple
critiques have been raised about the reliability and quality of such
techniques. For this reason, we study the use of prominent image segmentation
foundation models to improve the truthfulness of the mapping between embedding
and input space. We aim to restrict the computation area of the saliency map to
a predefined semantic image patch to reduce the uncertainty of such
visualizations. To perceive the information of an entire image, we use the
bounding box from each generated segmentation mask to crop the image. Each mask
results in an individual input in our novel model architecture named ProtoMask.
We conduct experiments on three popular fine-grained classification datasets
with a wide set of metrics, providing a detailed overview on explainability
characteristics. The comparison with other popular models demonstrates
competitive performance and unique explainability features of our model.
https://github.com/uos-sis/quanproto

</details>


### [58] [Graph Integrated Multimodal Concept Bottleneck Model](https://arxiv.org/abs/2510.00701)
*Jiakai Lin,Jinchang Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出MoE-SGT：将Graph Transformer与Mixture of Experts引入概念瓶颈模型，建模多模态输入中结构化概念关系并通过动态专家分配提升推理能力与准确率。


<details>
  <summary>Details</summary>
Motivation: 传统CBM多为单模态且忽视概念间的结构性关系，导致在复杂概念推理任务上的表现受限，需一种能同时处理多模态输入并建模概念结构关系的框架。

Method: 构建答案-概念和答案-问题图以显式建模概念关系；采用结构注入的Graph Transformer捕获多层次依赖；将传统前馈层替换为Mixture of Experts模块以提升模型容量和动态任务分配能力。

Result: 在多个数据集上，MoE-SGT优于其他概念瓶颈网络，证明了结构化概念建模与动态专家选择对提升复杂概念推理效果的有效性。

Conclusion: MoE-SGT通过在概念瓶颈模型中引入图变换器和专家混合模块，显著提升了对多模态和结构化概念关系的建模能力，从而在多个数据集上实现了更高的准确率。

Abstract: With growing demand for interpretability in deep learning, especially in high
stakes domains, Concept Bottleneck Models (CBMs) address this by inserting
human understandable concepts into the prediction pipeline, but they are
generally single modal and ignore structured concept relationships. To overcome
these limitations, we present MoE-SGT, a reasoning driven framework that
augments CBMs with a structure injecting Graph Transformer and a Mixture of
Experts (MoE) module. We construct answer-concept and answer-question graphs
for multimodal inputs to explicitly model the structured relationships among
concepts. Subsequently, we integrate Graph Transformer to capture multi level
dependencies, addressing the limitations of traditional Concept Bottleneck
Models in modeling concept interactions. However, it still encounters
bottlenecks in adapting to complex concept patterns. Therefore, we replace the
feed forward layers with a Mixture of Experts (MoE) module, enabling the model
to have greater capacity in learning diverse concept relationships while
dynamically allocating reasoning tasks to different sub experts, thereby
significantly enhancing the model's adaptability to complex concept reasoning.
MoE-SGT achieves higher accuracy than other concept bottleneck networks on
multiple datasets by modeling structured relationships among concepts and
utilizing a dynamic expert selection mechanism.

</details>


### [59] [Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs](https://arxiv.org/abs/2510.00705)
*Sanghwan Kim,Rui Xiao,Stephan Alaniz,Yongqin Xian,Zeynep Akata*

Main category: cs.CV

TL;DR: 利用MLLM输出的不确定性（熵）作为无训练的提示，按低熵优先选择视觉候选输入，能显著提升细粒度多模态任务表现，且具普适性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在细粒度感知（小目标、高分辨率图像或长视频中关键时刻）表现不足，且现有方法依赖复杂且任务专用的微调，本工作希望找到一种无需训练即可提升通用性的方法。

Method: 通过计算候选视觉输入（区域、关键帧或时段）对应的语言响应不确定性（熵或置信度），对候选项进行打分并选择低熵（高确定性）的项喂入MLLM，以此指导模型注意力，应用于视觉搜索、长视频理解和时序定位三类任务。

Result: 在视觉搜索、长视频理解和时间定位任务上，基于不确定性的选择策略使得未经微调的现成MLLM能达到与专用微调方法相当的性能，证明了利用内在不确定性作为显著性指示的有效性。

Conclusion: 该论文提出了一种训练免费、基于不确定性的通用机制，通过利用多模态大模型输出的熵来选择最相关的视觉信息，从而提升模型在细粒度感知任务上的表现。

Abstract: Multimodal Large Language Models (MLLMs) often struggle with fine-grained
perception, such as identifying small objects in high-resolution images or
finding key moments in long videos. Existing works typically rely on
complicated, task-specific fine-tuning, which limits their generalizability and
increases model complexity. In this work, we propose an effective,
training-free framework that uses an MLLM's intrinsic uncertainty as a
proactive guidance signal. Our core insight is that a model's output entropy
decreases when presented with relevant visual information. We introduce a
unified mechanism that scores candidate visual inputs by response uncertainty,
enabling the model to autonomously focus on the most salient data. We apply
this simple principle to three complex visual tasks: Visual Search, Long Video
Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve
performance competitive with specialized, fine-tuned methods. Our work
validates that harnessing intrinsic uncertainty is a powerful, general strategy
for enhancing fine-grained multimodal performance.

</details>


### [60] [Deep learning motion correction of quantitative stress perfusion cardiovascular magnetic resonance](https://arxiv.org/abs/2510.00723)
*Noortje I. P. Schueler,Nathan C. K. Wong,Richard J. Crawley,Josien P. W. Pluim,Amedeo Chiribiri,Cian M. Scannell*

Main category: cs.CV

TL;DR: Unsupervised DL motion correction for stress perfusion CMR: one-shot, RPCA-based alignment of dynamic and auxiliary images; matches registration accuracy, improves temporal smoothness and perfusion consistency, and is ~15x faster, trained on multivendor data for generalizability.


<details>
  <summary>Details</summary>
Motivation: Traditional registration-based motion correction for quantitative stress perfusion CMR is slow and sensitive to acquisition variability, limiting robustness and scalability for clinical adoption.

Method: Unsupervised deep learning one-shot motion estimation in three steps, using robust PCA to mitigate contrast effects, aligning perfusion series and auxiliary images (AIF and proton density-weighted). Trained/validated on multivendor dataset (201 patients, 38 test) and compared against prior registration-based method.

Result: Significant improvement in temporal smoothness (p<0.001); myocardial Dice post-correction 0.92 and 0.91 vs 0.80 before registration; reduced myocardial perfusion SD (0.52 vs 0.55 ml/min/g); 15-fold faster processing.

Conclusion: The proposed unsupervised deep learning pipeline provides fast and robust motion correction for stress perfusion CMR, achieving comparable myocardial alignment and improved temporal smoothness and perfusion map consistency versus a registration-based baseline, while reducing processing time by ~15x and generalizing across multivendor data.

Abstract: Background: Quantitative stress perfusion cardiovascular magnetic resonance
(CMR) is a powerful tool for assessing myocardial ischemia. Motion correction
is essential for accurate pixel-wise mapping but traditional registration-based
methods are slow and sensitive to acquisition variability, limiting robustness
and scalability.
  Methods: We developed an unsupervised deep learning-based motion correction
pipeline that replaces iterative registration with efficient one-shot
estimation. The method corrects motion in three steps and uses robust principal
component analysis to reduce contrast-related effects. It aligns the perfusion
series and auxiliary images (arterial input function and proton
density-weighted series). Models were trained and validated on multivendor data
from 201 patients, with 38 held out for testing. Performance was assessed via
temporal alignment and quantitative perfusion values, compared to a previously
published registration-based method.
  Results: The deep learning approach significantly improved temporal
smoothness of time-intensity curves (p<0.001). Myocardial alignment (Dice =
0.92 (0.04) and 0.91 (0.05)) was comparable to the baseline and superior to
before registration (Dice = 0.80 (0.09), p<0.001). Perfusion maps showed
reduced motion, with lower standard deviation in the myocardium (0.52 (0.39)
ml/min/g) compared to baseline (0.55 (0.44) ml/min/g). Processing time was
reduced 15-fold.
  Conclusion: This deep learning pipeline enables fast, robust motion
correction for stress perfusion CMR, improving accuracy across dynamic and
auxiliary images. Trained on multivendor data, it generalizes across sequences
and may facilitate broader clinical adoption of quantitative perfusion imaging.

</details>


### [61] [DEAP DIVE: Dataset Investigation with Vision transformers for EEG evaluation](https://arxiv.org/abs/2510.00725)
*Annemarie Hoffsommer,Helen Schneider,Svetlana Pavlitska,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 通过将EEG信号转为尺度图并用ViT建模，作者证明了用12通道低成本EEG在DEAP数据集上可实现91.57%四象限情绪分类准确率，表明通道数可以大幅降低而维持高性能。


<details>
  <summary>Details</summary>
Motivation: 全套EEG测量复杂且成本高，低成本EEG设备通道受限，研究如何用减少的通道仍能准确预测情绪具有实际意义，可促进可穿戴设备和临床应用。

Method: 将原始EEG信号通过CWT转换为尺度图（scaleogram），作为图像输入给ViT模型进行训练；在不同通道子集上评估模型性能以确定低通道配置的可行性。

Result: 在只使用12个通道的情况下，模型在四象限情绪分类上达到了91.57%的准确率；相比32通道的最先进结果96.9%，通道显著减少仍能取得较高性能。

Conclusion: 本文提出使用低通道EEG（12通道）结合连续小波变换（CWT）与视觉Transformer（ViT），在DEAP数据集上实现对情绪（按唤醒度和愉悦度分为四象限）的高精度分类。

Abstract: Accurately predicting emotions from brain signals has the potential to
achieve goals such as improving mental health, human-computer interaction, and
affective computing. Emotion prediction through neural signals offers a
promising alternative to traditional methods, such as self-assessment and
facial expression analysis, which can be subjective or ambiguous. Measurements
of the brain activity via electroencephalogram (EEG) provides a more direct and
unbiased data source. However, conducting a full EEG is a complex,
resource-intensive process, leading to the rise of low-cost EEG devices with
simplified measurement capabilities. This work examines how subsets of EEG
channels from the DEAP dataset can be used for sufficiently accurate emotion
prediction with low-cost EEG devices, rather than fully equipped
EEG-measurements. Using Continuous Wavelet Transformation to convert EEG data
into scaleograms, we trained a vision transformer (ViT) model for emotion
classification. The model achieved over 91,57% accuracy in predicting 4
quadrants (high/low per arousal and valence) with only 12 measuring points
(also referred to as channels). Our work shows clearly, that a significant
reduction of input channels yields high results compared to state-of-the-art
results of 96,9% with 32 channels. Training scripts to reproduce our code can
be found here:
https://gitlab.kit.edu/kit/aifb/ATKS/public/AutoSMiLeS/DEAP-DIVE.

</details>


### [62] [Extreme Blind Image Restoration via Prompt-Conditioned Information Bottleneck](https://arxiv.org/abs/2510.00728)
*Hongeun Kim,Bryan Sangwoo Kim,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出将ELQ先投影到LQ流形再用冻结的BIR模型恢复的框架，基于信息瓶颈推导损失，支持推理时提示精炼和即插即用，能在极端退化下提升恢复效果。


<details>
  <summary>Details</summary>
Motivation: 直接从极端低质量图像恢复高质量图像难以学习，域差距大导致伪影和细节丢失，因此通过将问题分解为可处理的两步来缓解这一困难。

Method: 提出一个投影器Network，将E L Q映射到L Q域；同时保持下游B I R模型冻结不变。训练投影器使用信息论推导的目标函数，包含低质量重建项和高质量先验匹配项；推理时采用L F O（Look Forward Once）提示精炼，并支持即插即用增强已有模型。

Result: 理论上将图像恢复视为信息瓶颈问题并给出目标函数；在多个严重退化的实验设置下，方法能稳定训练并提升恢复质量，支持现有模型无微调增强。

Conclusion: 该论文提出通过将极端低质量图像(E L Q)先投影到中间低质量L Q流形，再用已训练的B I R模型恢复H Q，从而避免直接从E L Q到H Q的困难映射，实验显示在严重退化场景下有效。

Abstract: Blind Image Restoration (BIR) methods have achieved remarkable success but
falter when faced with Extreme Blind Image Restoration (EBIR), where inputs
suffer from severe, compounded degradations beyond their training scope.
Directly learning a mapping from extremely low-quality (ELQ) to high-quality
(HQ) images is challenging due to the massive domain gap, often leading to
unnatural artifacts and loss of detail. To address this, we propose a novel
framework that decomposes the intractable ELQ-to-HQ restoration process. We
first learn a projector that maps an ELQ image onto an intermediate,
less-degraded LQ manifold. This intermediate image is then restored to HQ using
a frozen, off-the-shelf BIR model. Our approach is grounded in information
theory; we provide a novel perspective of image restoration as an Information
Bottleneck problem and derive a theoretically-driven objective to train our
projector. This loss function effectively stabilizes training by balancing a
low-quality reconstruction term with a high-quality prior-matching term. Our
framework enables Look Forward Once (LFO) for inference-time prompt refinement,
and supports plug-and-play strengthening of existing image restoration models
without need for finetuning. Extensive experiments under severe degradation
regimes provide a thorough analysis of the effectiveness of our work.

</details>


### [63] [Defect Segmentation in OCT scans of ceramic parts for non-destructive inspection using deep learning](https://arxiv.org/abs/2510.00745)
*Andrés Laveda-Martínez,Natalia P. García-de-la-Puente,Fernando García-Torres,Niels Møller Israelsen,Ole Bang,Dominik Brouczek,Niels Benson,Adrián Colomer,Valery Naranjo*

Main category: cs.CV

TL;DR: 利用改进的U-Net对OCT图像进行训练和后处理，实现高精度（Dice=0.979）、可实际部署（18.98s/体积）的陶瓷内部缺陷自动检测系统。


<details>
  <summary>Details</summary>
Motivation: 在陶瓷制造中，需在不破坏部件完整性的前提下检测内部缺陷；OCT提供高分辨率内部成像，但需要自动化且可靠的算法来加速和标准化检测流程。

Method: 使用手工标注的OCT图像训练改进的U-Net网络，比较多种实验配置（例如数据增强、损失函数、网络变体和后处理方法），并对预测结果进行后处理以提取定量和定性评估指标。

Result: 模型在测试集上达到了0.979的Dice分数，推理时间约为18.98秒/体积，优于可比研究，能有效检测气孔、分层或包含物等缺陷。

Conclusion: 本文提出的基于U-Net的自动缺陷检测系统在陶瓷制造的OCT图像上表现优异，能高精度（Dice=0.979）分割内部缺陷，并具有足够的推理速度（约18.98s/体积）以支持实际质检应用。

Abstract: Non-destructive testing (NDT) is essential in ceramic manufacturing to ensure
the quality of components without compromising their integrity. In this
context, Optical Coherence Tomography (OCT) enables high-resolution internal
imaging, revealing defects such as pores, delaminations, or inclusions. This
paper presents an automatic defect detection system based on Deep Learning
(DL), trained on OCT images with manually segmented annotations. A neural
network based on the U-Net architecture is developed, evaluating multiple
experimental configurations to enhance its performance. Post-processing
techniques enable both quantitative and qualitative evaluation of the
predictions. The system shows an accurate behavior of 0.979 Dice Score,
outperforming comparable studies. The inference time of 18.98 seconds per
volume supports its viability for detecting inclusions, enabling more
efficient, reliable, and automated quality control.

</details>


### [64] [Multi-Objective Task-Aware Predictor for Image-Text Alignment](https://arxiv.org/abs/2510.00766)
*Eunki Kim,Na Min An,James Thorne,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出MULTI-TAP：基于冻结LVLM隐藏态加轻量回归层的插拔式多目标评分器，兼顾人类偏好对齐、长序列处理与高效推理，并发布EYE4ALL多模态偏好数据集。


<details>
  <summary>Details</summary>
Motivation: 现有图文对齐评估方法缺乏全面基准和评估器在一致性、人类偏好对齐、长序列处理、推理效率以及多目标评分适用性方面存在不足，需要一种既高效又能反映多维人类偏好的评估方法。

Method: 在冻结预训练LVLM的隐藏态上训练轻量级岭回归层，并可在LVLM之上添加奖励头产生总体分数。该方法适配不同LVLM架构，使用小模型（7-8B）即可达到与更大模型相当的性能。

Result: 在多目标基准测试以及新发布的数据集EYE4ALL上，MULTI-TAP 在性能和效率上优于现有方法（如VisionREWARD），与GPT-4o 基于的G-VEval相当，同时模型规模更小。新数据集包括EYE4ALLPref（选择/拒绝偏好）和EYE4ALLMulti（七个维度的细粒度人工标注分数）。

Conclusion: MULTI-TAP 是一种基于大视觉语言模型（LVLM）构建的插拔式多目标评分预测器，既能输出单一总体评分，也能生成多维度的细粒度评分，能更好地对齐人类偏好并兼顾效率与长序列处理能力。

Abstract: Evaluating image-text alignment while reflecting human preferences across
multiple aspects is a significant issue for the development of reliable
vision-language applications. It becomes especially crucial in real-world
scenarios where multiple valid descriptions exist depending on contexts or user
needs. However, research progress is hindered by the lack of comprehensive
benchmarks and existing evaluation predictors lacking at least one of these key
properties: (1) Alignment with human judgments, (2) Long-sequence processing,
(3) Inference efficiency, and (4) Applicability to multi-objective scoring. To
address these challenges, we propose a plug-and-play architecture to build a
robust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of
both multi and single-objective scoring. MULTI-TAP can produce a single overall
score, utilizing a reward head built on top of a large vision-language model
(LVLMs). We show that MULTI-TAP is robust in terms of application to different
LVLM architectures, achieving significantly higher performance than existing
metrics and even on par with the GPT-4o-based predictor, G-VEval, with a
smaller size (7-8B). By training a lightweight ridge regression layer on the
frozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained
scores for multiple human-interpretable objectives. MULTI-TAP performs better
than VisionREWARD, a high-performing multi-objective reward model, in both
performance and efficiency on multi-objective benchmarks and our newly released
text-image-to-text dataset, EYE4ALL. Our new dataset, consisting of
chosen/rejected human preferences (EYE4ALLPref) and human-annotated
fine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a
foundation for developing more accessible AI systems by capturing the
underlying preferences of users, including blind and low-vision (BLV)
individuals.

</details>


### [65] [ZQBA: Zero Query Black-box Adversarial Attack](https://arxiv.org/abs/2510.00769)
*Joana C. Costa,Tiago Roxo,Hugo Proença,Pedro R. M. Inácio*

Main category: cs.CV

TL;DR: ZQBA通过复用DNN的特征图零查询地生成可迁移且不可感知的对抗样本，克服了需大量查询或训练生成模型的限制，实验证明优于单次查询的黑盒攻击。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒攻击通常依赖大量查询或训练替代损失/扩散模型，限制了在实际场景的可用性；因此提出一种零查询、无需训练模型即可生成有效对抗样本的方法以提高实用性并展示DNN表示的脆弱性。

Method: 方法核心是从预训练的DNN中提取特征图（feature maps），对这些特征图进行处理并按一定权重叠加到原始图像，从而形成对抗样本；该方法不进行梯度估计或查询目标模型，也不训练生成模型，强调使用表示层信息实现跨模型、跨数据集的迁移性。

Result: 实验在CIFAR与Tiny ImageNet上进行，结果显示ZQBA相较于现有单次查询黑盒攻击具有更高的有效性且保持扰动不可感知（用SSIM定量评估并进行定性展示）；对抗样本可以在不同模型间迁移，证明了方法的普适性。

Conclusion: 本文提出了一种无需查询的黑盒对抗攻击方法ZQBA（Zero Query Black-box Adversarial），通过直接利用DNN的中间特征图并将其叠加到干净图像上，生成可迁移的对抗样本，从而在不依赖查询或训练替代模型/扩散模型的情况下使目标模型发生误判。

Abstract: Current black-box adversarial attacks either require multiple queries or
diffusion models to produce adversarial samples that can impair the target
model performance. However, these methods require training a surrogate loss or
diffusion models to produce adversarial samples, which limits their
applicability in real-world settings. Thus, we propose a Zero Query Black-box
Adversarial (ZQBA) attack that exploits the representations of Deep Neural
Networks (DNNs) to fool other networks. Instead of requiring thousands of
queries to produce deceiving adversarial samples, we use the feature maps
obtained from a DNN and add them to clean images to impair the classification
of a target model. The results suggest that ZQBA can transfer the adversarial
samples to different models and across various datasets, namely CIFAR and Tiny
ImageNet. The experiments also show that ZQBA is more effective than
state-of-the-art black-box attacks with a single query, while maintaining the
imperceptibility of perturbations, evaluated both quantitatively (SSIM) and
qualitatively, emphasizing the vulnerabilities of employing DNNs in real-world
contexts. All the source code is available at
https://github.com/Joana-Cabral/ZQBA.

</details>


### [66] [Uncertainty-Aware Concept Bottleneck Models with Enhanced Interpretability](https://arxiv.org/abs/2510.00773)
*Haifei Zhang,Patrick Barry,Eduardo Brandao*

Main category: cs.CV

TL;DR: 为CBM第二阶段设计基于二值类概念原型的可解释不确定性感知分类器，用原型距离同时做分类打分和不确定性估计，并用顺应预测处理异常/不确定样本，提升解释性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CBM在可解释性上有优势，但通常牺牲了端到端模型的性能，且概念预测的不确定性如何传播到最终标签决策尚未充分研究；因此需要一种既可解释又能评估不确定性的分类方法以提高鲁棒性与可信度。

Method: 学习一组二值（presence/absence）类级概念原型；将图像通过概念预测器映射为概念向量；计算该向量与每个类原型的距离，距离越小表明越可能属于该类，并用距离分布作为不确定性估计；将原型二值化以提供可解释规则；基于距离偏离程度应用顺应预测（conformal prediction）来标记不确定或异常样本。

Result: 提出的方法兼顾了可解释性与不确定性度量：通过类级二值概念原型提供直观的分类规则和判别依据；距离用于分类与不确定性评估，使得在概念预测不准确或遇到异常样本时能进行合理拒绝或多标签集合输出，从而增强鲁棒性。论文宣称该框架能实现顺应预测以控制错误率，但具体数值结果需看实验部分。

Conclusion: 该论文提出了用于CBM第二阶段的基于二值类级概念原型的不确定性感知可解释分类器，通过计算预测概念向量与每类原型的距离，既用于分类打分也用于不确定性度量；原型可作为可解释的分类规则，并结合顺应预测以处理不确定或异常输入，提升解释性和鲁棒性。

Abstract: In the context of image classification, Concept Bottleneck Models (CBMs)
first embed images into a set of human-understandable concepts, followed by an
intrinsically interpretable classifier that predicts labels based on these
intermediate representations. While CBMs offer a semantically meaningful and
interpretable classification pipeline, they often sacrifice predictive
performance compared to end-to-end convolutional neural networks. Moreover, the
propagation of uncertainty from concept predictions to final label decisions
remains underexplored. In this paper, we propose a novel uncertainty-aware and
interpretable classifier for the second stage of CBMs. Our method learns a set
of binary class-level concept prototypes and uses the distances between
predicted concept vectors and each class prototype as both a classification
score and a measure of uncertainty. These prototypes also serve as
interpretable classification rules, indicating which concepts should be present
in an image to justify a specific class prediction. The proposed framework
enhances both interpretability and robustness by enabling conformal prediction
for uncertain or outlier inputs based on their deviation from the learned
binary class-level concept prototypes.

</details>


### [67] [MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts](https://arxiv.org/abs/2510.00796)
*Yifan Shen,Yangyang Shu,Hye-young Paik,Yulei Sui*

Main category: cs.CV

TL;DR: 提出MetaLogic，通过生成语义等价的提示对并比较生成的图像对，提供一种无需真值、可扩展的评估方法，揭示并分类T2I模型在语义一致性方面的广泛失败。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型在输入提示有轻微语言变体时常产生语义不一致的图像，暴露出模型在逻辑推理和泛化能力上的不足，需要一个不依赖真实图像的评估方法来系统检测和诊断这些问题。

Method: 利用metamorphic testing生成语义等价但语法不同的提示对，生成对应图像并直接对比图像对以识别语义不一致。分类错误类型（实体遗漏、重复、位置错位等），并在多模型上进行大规模评估。无需真实图像作为参考，具有可扩展性。

Result: 在多种SOTA模型上，MetaLogic 发现了高频的对齐失败率：例如Flux.dev和DALLE-3的失配率分别为59%和71%。该框架有效揭示了被现有指标忽视的细粒度逻辑不一致。

Conclusion: MetaLogic 提出了一种无需真实图像、基于变形测试的评估框架，用于检测文本到图像生成模型在语义一致性上的失灵。该方法能发现并分类多种对齐错误，并能提供调试用的反例。评估显示多种 SOTA 模型在语义稳健性上存在显著缺陷。

Abstract: Recent advances in text-to-image (T2I) models, especially diffusion-based
architectures, have significantly improved the visual quality of generated
images. However, these models continue to struggle with a critical limitation:
maintaining semantic consistency when input prompts undergo minor linguistic
variations. Despite being logically equivalent, such prompt pairs often yield
misaligned or semantically inconsistent images, exposing a lack of robustness
in reasoning and generalisation. To address this, we propose MetaLogic, a novel
evaluation framework that detects T2I misalignment without relying on ground
truth images. MetaLogic leverages metamorphic testing, generating image pairs
from prompts that differ grammatically but are semantically identical. By
directly comparing these image pairs, the framework identifies inconsistencies
that signal failures in preserving the intended meaning, effectively diagnosing
robustness issues in the model's logic understanding. Unlike existing
evaluation methods that compare a generated image to a single prompt, MetaLogic
evaluates semantic equivalence between paired images, offering a scalable,
ground-truth-free approach to identifying alignment failures. It categorises
these alignment errors (e.g., entity omission, duplication, positional
misalignment) and surfaces counterexamples that can be used for model debugging
and refinement. We evaluate MetaLogic across multiple state-of-the-art T2I
models and reveal consistent robustness failures across a range of logical
constructs. We find that even the SOTA text-to-image models like Flux.dev and
DALLE-3 demonstrate a 59 percent and 71 percent misalignment rate,
respectively. Our results show that MetaLogic is not only efficient and
scalable, but also effective in uncovering fine-grained logical inconsistencies
that are overlooked by existing evaluation metrics.

</details>


### [68] [Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models](https://arxiv.org/abs/2510.00797)
*Ruyu Liu,Dongxu Zhuang,Jianhua Zhang,Arega Getaneh Abate,Per Sieverts Nielsen,Ben Wang,Xiufeng Liu*

Main category: cs.CV

TL;DR: SF-SPA通过图像纠正、零样本分割、LLM驱动的空间推理和能量仿真，实现了从街景图像到可部署光伏面积与能量产出的自动化评估，准确度高且效率显著提升。


<details>
  <summary>Details</summary>
Motivation: 在密集城市中，建筑立面是未被充分利用的太阳能资源，但因复杂几何形状和语义构件使得评估困难，现有方法依赖人工标注或难以处理街景图像，因此需要自动化、高效且鲁棒的评估工具。

Method: 方法包括(1) 图像几何纠正以消除透视失真并获得真实尺度；(2) 零样本语义分割识别窗户、门、阳台等不可用区域；(3) 利用大语言模型进行空间推理与PV板布局优化；(4) 能量产出仿真以估算年发电量。整个流程每栋建筑约耗时100秒。

Result: 在四个国家80栋建筑上的验证显示，与专家标注相比，面积估计平均误差为6.2% ±2.8%；评估速度显著优于人工方法；仿真结果表明能量产出预测可靠，可用于区域潜力评估与城市能源规划。

Conclusion: 该论文提出了名为SF-SPA的自动化框架，可从街景照片中评估建筑立面的光伏（PV）部署潜力。通过几何校正、零样本语义分割、LLM引导的空间推理和能量仿真四个阶段，实现了对立面可用面积的自动估计及能量产出预测。

Abstract: Building facades represent a significant untapped resource for solar energy
generation in dense urban environments, yet assessing their photovoltaic (PV)
potential remains challenging due to complex geometries and semantic com
ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an
automated framework that transforms street-view photographs into quantitative
PV deployment assessments. The approach combines com puter vision and
artificial intelligence techniques to address three key challenges: perspective
distortion correction, semantic understanding of facade elements, and spatial
reasoning for PV layout optimization. Our four-stage pipeline processes images
through geometric rectification, zero-shot semantic segmentation, Large
Language Model (LLM) guided spatial reasoning, and energy simulation.
Validation across 80 buildings in four countries demonstrates ro bust
performance with mean area estimation errors of 6.2% &#177; 2.8% compared to
expert annotations. The auto mated assessment requires approximately 100
seconds per building, a substantial gain in efficiency over manual methods.
Simulated energy yield predictions confirm the method's reliability and
applicability for regional poten tial studies, urban energy planning, and
building-integrated photovoltaic (BIPV) deployment. Code is available at:
https:github.com/CodeAXu/Solar-PV-Installation

</details>


### [69] [From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation](https://arxiv.org/abs/2510.00806)
*Fan Yang,Zhiyang Chen,Yousong Zhu,Xin Li,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出TrajVLM-Gen：先用VLM预测物理一致的运动轨迹，再用基于注意力的模块在生成中精化运动；在两大数据集上取得更好FVD。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型常产生不符合真实物理动力学的运动，导致生成视频在运动连贯性和物理合理性上失败；因此需要引入物理感知的运动先验来约束生成过程。

Method: 首先使用视觉-语言模型预测粗粒度的运动轨迹以保持物理一致性；其次将预测轨迹通过基于注意力的机制引导视频生成，以实现细粒度运动精化；并构建基于视频追踪数据的轨迹预测数据集用于训练与评估。

Result: 在UCF-101和MSR-VTT数据集上的实验表明TrajVLM-Gen优于现有方法，报告的FVD分别为545（UCF-101）和539（MSR-VTT），显示竞品比较优势。

Conclusion: TrajVLM-Gen通过两阶段框架有效改善图像到视频生成中的物理一致性问题；总体结论是其在运动一致性和生成质量上优于现有方法。

Abstract: Current video generation models produce physically inconsistent motion that
violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for
physics-aware image-to-video generation. First, we employ a Vision Language
Model to predict coarse-grained motion trajectories that maintain consistency
with real-world physics. Second, these trajectories guide video generation
through attention-based mechanisms for fine-grained motion refinement. We build
a trajectory prediction dataset based on video tracking data with realistic
motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that
TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of
545 on UCF-101 and 539 on MSR-VTT.

</details>


### [70] [What You See is What You Ask: Evaluating Audio Descriptions](https://arxiv.org/abs/2510.00808)
*Divy Kala,Eshika Khandelwal,Makarand Tapaswi*

Main category: cs.CV

TL;DR: 提出ADQA基准，量化AD写作的主观性并在更长的视频片段上以问答方式评估自动AD生成，显示现有方法远落后于人工水平。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要集中在几秒钟的裁剪片段并使用单一参考AD进行评估，而音频描述具有高度主观性且需要考虑更长时间的连贯性，因此需要新的评估基准和分析来推动更实用的自动AD生成。

Method: 通过对同一电影的两条独立人工AD轨进行对齐与分析，量化何时以及是否进行描述、以及描述内容和强调方式的主观性；构建ADQA数据集与问答式评估（包括视觉欣赏和叙事理解问题）；在该基准上评估现有自动AD生成方法并与人工AD对比。

Result: 通过对两条人工AD对齐分析，证明了描述时机与内容存在显著主观差异；ADQA提供了针对几分钟级视频片段的问答式评估，包括视觉事实（VA）和叙事理解（NU）问题；实验证明当前自动AD生成方法在ADQA上明显落后于人工AD。

Conclusion: 本文指出自动音频描述（AD）生成研究的不足，尤其是对主观性和长片段连贯性的忽视，提出ADQA基准用于评估几分钟级别视频片段的描述质量，并展示现有方法与人工AD之间存在较大差距。

Abstract: Audio descriptions (ADs) narrate important visual details in movies, enabling
Blind and Low Vision (BLV) users to understand narratives and appreciate visual
details. Existing works in automatic AD generation mostly focus on few-second
trimmed clips, and evaluate them by comparing against a single ground-truth
reference AD. However, writing ADs is inherently subjective. Through alignment
and analysis of two independent AD tracks for the same movies, we quantify the
subjectivity in when and whether to describe, and what and how to highlight.
Thus, we show that working with trimmed clips is inadequate. We propose ADQA, a
QA benchmark that evaluates ADs at the level of few-minute long, coherent video
segments, testing whether they would help BLV users understand the story and
appreciate visual details. ADQA features visual appreciation (VA) questions
about visual facts and narrative understanding (NU) questions based on the
plot. Through ADQA, we show that current AD generation methods lag far behind
human-authored ADs. We conclude with several recommendations for future work
and introduce a public leaderboard for benchmarking.

</details>


### [71] [PhraseStereo: The First Open-Vocabulary Stereo Image Segmentation Dataset](https://arxiv.org/abs/2510.00818)
*Thomas Campagnolo,Ezio Malis,Philippe Martinet,Gaetan Bahl*

Main category: cs.CV

TL;DR: PhraseStereo是将短语定位扩展到立体视觉的首个数据集，通过用GenStereo生成右视图对PhraseCut进行扩展，提供对齐的立体图像对、分割掩码及短语标注，促进语义与几何联合建模。


<details>
  <summary>Details</summary>
Motivation: 现有短语定位研究主要局限于单视图，忽视了立体视觉中蕴含的深度与几何线索。构建立体短语分割数据集能够让模型利用深度信息提升定位精度与上下文理解。

Method: 基于现有PhraseCut数据集，利用GenStereo从单视图生成准确的右视图，从而构建左右视图对；保持短语注释和分割掩码对齐，形成立体短语分割数据集。

Result: 构建了包含立体图像对、对齐分割掩码与短语注释的PhraseStereo数据集，为未来研究提供基准；数据集将在论文接受后公开。

Conclusion: 本工作提出了首个短语-立体图像对齐数据集PhraseStereo，将单视图短语定位扩展到立体视觉域，为联合语义与几何推理提供数据基础。

Abstract: Understanding how natural language phrases correspond to specific regions in
images is a key challenge in multimodal semantic segmentation. Recent advances
in phrase grounding are largely limited to single-view images, neglecting the
rich geometric cues available in stereo vision. For this, we introduce
PhraseStereo, the first novel dataset that brings phrase-region segmentation to
stereo image pairs. PhraseStereo builds upon the PhraseCut dataset by
leveraging GenStereo to generate accurate right-view images from existing
single-view data, enabling the extension of phrase grounding into the stereo
domain. This new setting introduces unique challenges and opportunities for
multimodal learning, particularly in leveraging depth cues for more precise and
context-aware grounding. By providing stereo image pairs with aligned
segmentation masks and phrase annotations, PhraseStereo lays the foundation for
future research at the intersection of language, vision, and 3D perception,
encouraging the development of models that can reason jointly over semantics
and geometry. The PhraseStereo dataset will be released online upon acceptance
of this work.

</details>


### [72] [NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution](https://arxiv.org/abs/2510.00820)
*Xiangtao Kong,Rongyuan Wu,Shuaizheng Liu,Lingchen Sun,Lei Zhang*

Main category: cs.CV

TL;DR: NSARM：基于next-scale自回归的两阶段训练框架，兼顾效率、质量与鲁棒性的真实图像超分方法。


<details>
  <summary>Details</summary>
Motivation: 克服基于预训练T2I扩散模型的Real-ISR方法在速度与质量之间的折衷、以及通过固定主模型并只训练附加模块导致的过度增强和幻觉问题；利用AR模型（如Infinity）的高效next-scale预测能力来提升效率与生成质量。

Method: 提出两阶段训练：先训练变换网络将低质输入映射到初步尺度，再进行端到端全模型微调；基于预训练的AR"next-scale"策略进行生成，保持模型生成能力同时提升鲁棒性。

Result: NSARM作为纯AR模型，在定量和定性评估中相较现有Real-ISR方法取得更优视觉效果、推理速度快，并在输入质量变化时表现出更强的鲁棒性和泛化能力。

Conclusion: NSARM通过结合下一尺度自回归建模与端到端微调，在真实图像超分任务中实现了更鲁棒、更高质量且高效的结果。

Abstract: Most recent real-world image super-resolution (Real-ISR) methods employ
pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality
image either from random Gaussian noise, which yields realistic results but is
slow due to iterative denoising, or directly from the input low-quality image,
which is efficient but at the price of lower output quality. These approaches
train ControlNet or LoRA modules while keeping the pre-trained model fixed,
which often introduces over-enhanced artifacts and hallucinations, suffering
from the robustness to inputs of varying degradations. Recent visual
autoregressive (AR) models, such as pre-trained Infinity, can provide strong
T2I generation capabilities while offering superior efficiency by using the
bitwise next-scale prediction strategy. Building upon next-scale prediction, we
introduce a robust Real-ISR framework, namely Next-Scale Autoregressive
Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation
network is first trained to map the input low-quality image to preliminary
scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive
fine-tuning enhances the robustness of NSARM in Real-ISR tasks without
compromising its generative capability. Extensive quantitative and qualitative
evaluations demonstrate that as a pure AR model, NSARM achieves superior visual
results over existing Real-ISR methods while maintaining a fast inference
speed. Most importantly, it demonstrates much higher robustness to the quality
of input images, showing stronger generalization performance. Project page:
https://github.com/Xiangtaokong/NSARM

</details>


### [73] [Feature Identification for Hierarchical Contrastive Learning](https://arxiv.org/abs/2510.00837)
*Julius Ott,Nastassia Vysotskaya,Huawei Sun,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 提出两种层级对比学习（G-HMLC 与 A-HMLC），通过高斯混合与注意力机制建模层级关系与不平衡性，在 CIFAR100 与 ModelNet40 上提升约2% 的线性评估准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法常忽略类别在不同层级间的内在关系与高层类别的不平衡性，导致在层级分类任务中错失重要监督信息和细粒度区分能力。作者旨在通过层级感知的对比学习补充这些监督信号。

Method: G-HMLC 利用高斯混合模型来表达不同层级的类别分布，从而在潜在空间中实现层级感知的类簇分离；A-HMLC 则采用注意力机制提取层级特定特征，模拟人类处理层级信息的方式。两者结合层级对比损失与不平衡处理策略进行训练。

Result: 在 CIFAR100 与 ModelNet40 上的线性评估中，所提方法相较现有层级对比学习方法在准确率上提升约2个百分点，并通过定量与定性分析展示了更细粒度的聚类效果和处理不平衡性的能力。

Conclusion: 本文提出两种层级对比学习方法（G-HMLC 与 A-HMLC），通过显式建模类间关系与高层不平衡分布，提升了多层级分类的聚类与分类性能。

Abstract: Hierarchical classification is a crucial task in many applications, where
objects are organized into multiple levels of categories. However, conventional
classification approaches often neglect inherent inter-class relationships at
different hierarchy levels, thus missing important supervisory signals. Thus,
we propose two novel hierarchical contrastive learning (HMLC) methods. The
first, leverages a Gaussian Mixture Model (G-HMLC) and the second uses an
attention mechanism to capture hierarchy-specific features (A-HMLC), imitating
human processing. Our approach explicitly models inter-class relationships and
imbalanced class distribution at higher hierarchy levels, enabling fine-grained
clustering across all hierarchy levels. On the competitive CIFAR100 and
ModelNet40 datasets, our method achieves state-of-the-art performance in linear
evaluation, outperforming existing hierarchical contrastive learning methods by
2 percentage points in terms of accuracy. The effectiveness of our approach is
backed by both quantitative and qualitative results, highlighting its potential
for applications in computer vision and beyond.

</details>


### [74] [Can World Models Benefit VLMs for World Dynamics?](https://arxiv.org/abs/2510.00855)
*Kevin Zhang,Kuangzhi Ge,Xiaowei Chi,Renrui Zhang,Shaojun Shi,Zhen Dong,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: Use a video diffusion model as a one-step generative encoder to create World-Language Models; this yields improved spatial and multi-frame reasoning (DyVA) outperforming many baselines, suggesting world-model priors are valuable for VLMs.


<details>
  <summary>Details</summary>
Motivation: Investigate whether powerful generative world (video) models can replace conventional vision encoders for general-purpose multimodal understanding, leveraging motion-consistent priors from large-scale video pretraining.

Method: Repurpose a video diffusion model as a generative encoder by performing a single denoising step to obtain latents used as visual embeddings; build World-Language Models (WorldLMs) and a leading variant named DyVA (Dynamic Vision Aligner). Conduct empirical evaluations on curated visual reasoning tasks and extensive model design explorations.

Result: DyVA surpasses open-source and proprietary baselines on curated spatial and temporal visual reasoning tasks, achieving state-of-the-art or comparable performance; generative encoders internalize motion consistency enabling single-image models to reason over multiple frames.

Conclusion: WorldLMs (using generative video diffusion models as encoders) can produce latent embeddings useful for multimodal vision-language tasks, offering distinct strengths compared to conventional encoders, notably improved spatial and multi-frame reasoning.

Abstract: Trained on internet-scale video data, generative world models are
increasingly recognized as powerful world simulators that can generate
consistent and plausible dynamics over structure, motion, and physics. This
raises a natural question: with the advent of strong video foundational models,
might they supplant conventional vision encoder paradigms for general-purpose
multimodal understanding? While recent studies have begun to explore the
potential of world models on common vision tasks, these explorations typically
lack a systematic investigation of generic, multimodal tasks. In this work, we
strive to investigate the capabilities when world model priors are transferred
into Vision-Language Models: we re-purpose a video diffusion model as a
generative encoder to perform a single denoising step and treat the resulting
latents as a set of visual embedding. We empirically investigate this class of
models, which we refer to as World-Language Models (WorldLMs), and we find that
generative encoders can capture latents useful for downstream understanding
that show distinctions from conventional encoders. Naming our best-performing
variant Dynamic Vision Aligner (DyVA), we further discover that this method
significantly enhances spatial reasoning abilities and enables single-image
models to perform multi-frame reasoning. Through the curation of a suite of
visual reasoning tasks, we find DyVA to surpass both open-source and
proprietary baselines, achieving state-of-the-art or comparable performance. We
attribute these gains to WorldLM's inherited motion-consistency internalization
from video pre-training. Finally, we systematically explore extensive model
designs to highlight promising directions for future work. We hope our study
can pave the way for a new family of VLMs that leverage priors from world
models and are on a promising path towards generalist vision learners.

</details>


### [75] [Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model](https://arxiv.org/abs/2510.00862)
*Hyun-kyu Ko,Youbin Kim,Jihyeon Park,Dongheok Park,Gyeongjin Kang,Wonjun Cho,Hyung Yi,Eunbyung Park*

Main category: cs.CV

TL;DR: 将Mamba选择性SSM与移位窗口自注意力结合，并提出对齐感知的Gather-Scatter Mamba以高效准确地进行视频超分辨率的时空信息传播与融合。


<details>
  <summary>Details</summary>
Motivation: RNN等状态空间模型在长序列建模上有优势但存在收敛和并行性问题；注意力机制虽能建模全局上下文但复杂度高，不适合长序列；Mamba作为选择性SSM具备线性复杂度与输入依赖的状态更新，但单独使用难以捕获精细空间依赖。

Method: 构建混合架构：使用移位窗口自注意力进行空间上下文聚合，使用Mamba选择性扫描进行线性时间的时序传播；引入GSM，在时间窗口内将特征按光流或位移向中心参考帧对齐（gather），经Mamba传播后再反向散开（scatter），以更好处理遮挡和信息重分布。

Result: 提出的混合模型和GSM在VSR任务上能提高时空信息融合效率、减轻遮挡伪影，并兼顾长程建模与空间细节恢复。作者提供了实现代码仓库。

Conclusion: 该论文提出将选择性状态空间模型（如Mamba）与移位窗口自注意力相结合，并引入对齐感知的Gather-Scatter Mamba（GSM），用于视频超分辨率任务，改善时序传播效率与空间细节建模，减少遮挡伪影。

Abstract: State Space Models (SSMs)-most notably RNNs-have historically played a
central role in sequential modeling. Although attention mechanisms such as
Transformers have since dominated due to their ability to model global context,
their quadratic complexity and limited scalability make them less suited for
long sequences. Video super-resolution (VSR) methods have traditionally relied
on recurrent architectures to propagate features across frames. However, such
approaches suffer from well-known issues including vanishing gradients, lack of
parallelism, and slow inference speed. Recent advances in selective SSMs like
Mamba offer a compelling alternative: by enabling input-dependent state
transitions with linear-time complexity, Mamba mitigates these issues while
maintaining strong long-range modeling capabilities. Despite this potential,
Mamba alone struggles to capture fine-grained spatial dependencies due to its
causal nature and lack of explicit context aggregation. To address this, we
propose a hybrid architecture that combines shifted window self-attention for
spatial context aggregation with Mamba-based selective scanning for efficient
temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an
alignment-aware mechanism that warps features toward a center anchor frame
within the temporal window before Mamba propagation and scatters them back
afterward, effectively reducing occlusion artifacts and ensuring effective
redistribution of aggregated information across all frames. The official
implementation is provided at: https://github.com/Ko-Lani/GSMamba.

</details>


### [76] [AI-CNet3D: An Anatomically-Informed Cross-Attention Network with Multi-Task Consistency Fine-tuning for 3D Glaucoma Classification](https://arxiv.org/abs/2510.00882)
*Roshan Kenia,Anfei Li,Rishabh Srivastava,Kaveri A. Thakoor*

Main category: cs.CV

TL;DR: 提出一种解剖学驱动的3D跨注意力+CNN模型（AI-CNet3D），通过CAREs与Grad-CAM一致性训练提升青光眼诊断的性能与可解释性，并实现参数高效。


<details>
  <summary>Details</summary>
Motivation: 传统将3D OCT体积压缩为2D报告会丢失关键结构信息，且需要利用半视网膜间的非对称性和眼底不同结构（ONH、黄斑）来改进青光眼检测与解释。

Method: 在3D CNN中引入跨注意力模块，对沿两个轴划分的体积（上/下半视网膜、视神经乳头和黄斑区）进行特征交互，生成通道注意表征（CAREs）；采用一致性多任务微调，将CAREs与最终卷积层的Grad-CAM对齐以增强性能与解读性。模型设计注重参数效率，参数量比现有注意力机制小百倍，同时保持相近GFLOPS。

Result: 在两个大型数据集上验证，AI-CNet3D在所有主要指标上超过现有卷积和注意力模型，同时显著减少参数数量并维持高诊断性能与计算复杂度相当。

Conclusion: 该论文提出了AI-CNet3D，一种在3D OCT体积上结合跨注意力机制与3D CNN的混合深度学习模型，旨在保留并利用视网膜与视神经关键结构信息以提高青光眼诊断的准确性与可解释性。

Abstract: Glaucoma is a progressive eye disease that leads to optic nerve damage,
causing irreversible vision loss if left untreated. Optical coherence
tomography (OCT) has become a crucial tool for glaucoma diagnosis, offering
high-resolution 3D scans of the retina and optic nerve. However, the
conventional practice of condensing information from 3D OCT volumes into 2D
reports often results in the loss of key structural details. To address this,
we propose a novel hybrid deep learning model that integrates cross-attention
mechanisms into a 3D convolutional neural network (CNN), enabling the
extraction of critical features from the superior and inferior hemiretinas, as
well as from the optic nerve head (ONH) and macula, within OCT volumes. We
introduce Channel Attention REpresentations (CAREs) to visualize
cross-attention outputs and leverage them for consistency-based multi-task
fine-tuning, aligning them with Gradient-Weighted Class Activation Maps
(Grad-CAMs) from the CNN's final convolutional layer to enhance performance,
interpretability, and anatomical coherence. We have named this model AI-CNet3D
(AI-`See'-Net3D) to reflect its design as an Anatomically-Informed
Cross-attention Network operating on 3D data. By dividing the volume along two
axes and applying cross-attention, our model enhances glaucoma classification
by capturing asymmetries between the hemiretinal regions while integrating
information from the optic nerve head and macula. We validate our approach on
two large datasets, showing that it outperforms state-of-the-art attention and
convolutional models across all key metrics. Finally, our model is
computationally efficient, reducing the parameter count by one-hundred--fold
compared to other attention mechanisms while maintaining high diagnostic
performance and comparable GFLOPS.

</details>


### [77] [Intuitions of Machine Learning Researchers about Transfer Learning for Medical Image Classification](https://arxiv.org/abs/2510.00902)
*Yucheng Lu,Hubert Dariusz Zając,Veronika Cheplygina,Amelia Jiménez-Sánchez*

Main category: cs.CV

TL;DR: 论文通过对机器学习从业者的任务导向调查，揭示了选择迁移学习源数据集的启发式：多因子驱动（任务、社区、数据属性、各种相似性），但“相似性更好”的传统观点不总适用，且需更清晰术语与工具支持。


<details>
  <summary>Details</summary>
Motivation: 目前源数据集选择多凭直觉，缺乏系统化原则，而源数据集对模型泛化和患者结果至关重要，故需理解从业者如何做出选择以改进实践。

Method: 通过对机器学习从业者进行基于任务的问卷调查（HCI视角），收集选择源数据集的决策过程与理由，并分析相似性评估与预期性能之间的关系。

Result: 发现选择受任务依赖、社区实践、数据集属性及计算/视觉/语义相似性影响；相似性评分与预期表现经常不一致；参与者用词模糊，表明需开发HCI工具与明确定义。

Conclusion: 该论文结论是，研究人员在选择迁移学习源数据集时依赖任务上下文、社区惯例、数据集属性以及计算或感知上的相似性，但“相似性越高越好”并不总成立，且术语使用常模糊，需更清晰定义与工具支持。

Abstract: Transfer learning is crucial for medical imaging, yet the selection of source
datasets - which can impact the generalizability of algorithms, and thus
patient outcomes - often relies on researchers' intuition rather than
systematic principles. This study investigates these decisions through a
task-based survey with machine learning practitioners. Unlike prior work that
benchmarks models and experimental setups, we take a human-centered HCI
perspective on how practitioners select source datasets. Our findings indicate
that choices are task-dependent and influenced by community practices, dataset
properties, and computational (data embedding), or perceived visual or semantic
similarity. However, similarity ratings and expected performance are not always
aligned, challenging a traditional "more similar is better" view. Participants
often used ambiguous terminology, which suggests a need for clearer definitions
and HCI tools to make them explicit and usable. By clarifying these heuristics,
this work provides practical insights for more systematic source selection in
transfer learning.

</details>


### [78] [PAL-Net: A Point-Wise CNN with Patch-Attention for 3D Facial Landmark Localization](https://arxiv.org/abs/2510.00910)
*Ali Shadman Yazdi,Annalisa Cappella,Benedetta Baldini,Riccardo Solazzo,Gianluca Tartaglia,Chiarella Sforza,Giuseppe Baselli*

Main category: cs.CV

TL;DR: 提出PAL-Net，用补丁注意力增强的点状CNN实现50个面部解剖标志自动定位，精度接近人工水平、泛化性好，是一种轻量可扩展的3D人类测量工具。


<details>
  <summary>Details</summary>
Motivation: 手工在3D面部扫描上标注解剖学标志既耗时又依赖专业知识，而现有深度学习方法常针对伪标志或需复杂输入表示，限制临床适用性，因此需要一种准确、轻量且易部署的自动化方案。

Method: 方法包含粗对齐、感兴趣区域过滤、初始标志点估计，以及基于补丁的点状CNN（pointwise CNN）并融合注意力机制来增强特征。训练与评估使用214个带注释的健康成人扫描，并在FaceScape 700名样本上做泛化测试。

Result: 在214个样本上，点位平均误差3.686 mm，保留结构距离的平均误差2.822 mm；在FaceScape上分别为0.41 mm和0.38 mm，表明较好泛化能力。与现有方法相比，在点位和结构评价上具有竞争力且计算开销较低。

Conclusion: PAL-Net是一种用于在立体光摄面部模型上自动定位50个解剖学标志点的轻量级深度学习流水线，具有接近人工重复标注误差的点位和结构距离误差，在多数面部区域表现稳定，但在耳部和发际线等网格质量差的区域性能下降。

Abstract: Manual annotation of anatomical landmarks on 3D facial scans is a
time-consuming and expertise-dependent task, yet it remains critical for
clinical assessments, morphometric analysis, and craniofacial research. While
several deep learning methods have been proposed for facial landmark
localization, most focus on pseudo-landmarks or require complex input
representations, limiting their clinical applicability. This study presents a
fully automated deep learning pipeline (PAL-Net) for localizing 50 anatomical
landmarks on stereo-photogrammetry facial models. The method combines coarse
alignment, region-of-interest filtering, and an initial approximation of
landmarks with a patch-based pointwise CNN enhanced by attention mechanisms.
Trained and evaluated on 214 annotated scans from healthy adults, PAL-Net
achieved a mean localization error of 3.686 mm and preserves relevant
anatomical distances with a 2.822 mm average error, comparable to
intra-observer variability. To assess generalization, the model was further
evaluated on 700 subjects from the FaceScape dataset, achieving a point-wise
error of 0.41\,mm and a distance-wise error of 0.38\,mm. Compared to existing
methods, PAL-Net offers a favorable trade-off between accuracy and
computational cost. While performance degrades in regions with poor mesh
quality (e.g., ears, hairline), the method demonstrates consistent accuracy
across most anatomical regions. PAL-Net generalizes effectively across datasets
and facial regions, outperforming existing methods in both point-wise and
structural evaluations. It provides a lightweight, scalable solution for
high-throughput 3D anthropometric analysis, with potential to support clinical
workflows and reduce reliance on manual annotation. Source code can be found at
https://github.com/Ali5hadman/PAL-Net-A-Point-Wise-CNN-with-Patch-Attention

</details>


### [79] [Equivariant Splitting: Self-supervised learning from incomplete data](https://arxiv.org/abs/2510.00929)
*Victor Sechaud,Jérémy Scanvic,Quentin Barthélemy,Patrice Abry,Julián Tachella*

Main category: cs.CV

TL;DR: 提出了基于重建网络等变性的自监督拆分损失方法，使得在单一不完全观测模型下也能进行无偏的自监督训练，并在多项逆问题任务中取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 当获取带标签的真值重建参考昂贵或不可能时，开发仅用噪声或不完整测量数据即可训练的学习方法。特别针对仅有单一不完整观测模型、前向算子高度秩亏的挑战场景，寻求可靠的自监督重建方法。

Method: 提出一种自监督学习策略：定义重建网络的等变性（equivariance），并设计自监督拆分损失（self-supervised splitting losses）。通过理论分析证明，在满足等变性条件下，所提出的自监督损失是对有监督损失的无偏估计。实现上在单一不完全观测模型下训练重建网络，并在多种任务上验证。

Result: 理论上证明结合等变性与拆分自监督损失可得到对有监督损失的无偏估计；实证上在图像修补、加速MRI与压缩感知等任务中，提出的方法在高度秩亏的前向模型下实现了最先进的性能。

Conclusion: 该论文提出了一种针对单一不完全观测模型下无监督重建的新策略，通过引入“重建网络的等变性”概念，并将其与自监督拆分损失结合，证明可得到有偏差补偿的无监督估计，从而在高度秩亏的前向模型情形中仍能逼近有监督损失。最终在图像修补、加速MRI与压缩感知任务中取得了优异表现。

Abstract: Self-supervised learning for inverse problems allows to train a
reconstruction network from noise and/or incomplete data alone. These methods
have the potential of enabling learning-based solutions when obtaining
ground-truth references for training is expensive or even impossible. In this
paper, we propose a new self-supervised learning strategy devised for the
challenging setting where measurements are observed via a single incomplete
observation model. We introduce a new definition of equivariance in the context
of reconstruction networks, and show that the combination of self-supervised
splitting losses and equivariant reconstruction networks results in unbiased
estimates of the supervised loss. Through a series of experiments on image
inpainting, accelerated magnetic resonance imaging, and compressive sensing, we
demonstrate that the proposed loss achieves state-of-the-art performance in
settings with highly rank-deficient forward models.

</details>


### [80] [Looking Alike From Far to Near: Enhancing Cross-Resolution Re-Identification via Feature Vector Panning](https://arxiv.org/abs/2510.00936)
*Zanwu Liu,Chao Yuan,Bo Li,Xiaowei Zhang,Guanglin Niu*

Main category: cs.CV

TL;DR: 发现ReID特征空间存在表示分辨率差异的语义方向，提出轻量VPFA通过建模分辨率特异性差异实现高效跨分辨率ReID，性能和效率均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是监控场景中摄像头距离引起行人图像分辨率差异，导致LR与HR图像难以匹配，现有依赖超分辨或联合学习的方法复杂且性能接近瓶颈，因此寻找更简单高效的特征补偿方式。

Method: 方法包括：1) 通过CCA和Pearson相关分析实证证明特征空间存在分辨率语义方向；2) 设计轻量的VPFA模块，用向量平移/对齐策略建模并补偿分辨率差异，无需复杂SR或联合学习；3) 在多种CR-ReID基准上训练和评估，比较与SOTA方法的性能与效率。

Result: 在多个跨分辨率ReID基准上，VPFA在准确率（例如mAP和rank-1）上显著优于先前SOTA，并在训练/推理效率方面表现更好，验证了基于分辨率语义方向建模的有效性。

Conclusion: 本文结论是：在ReID特征空间中存在表示分辨率差异的语义方向，基于该发现提出的Vector Panning Feature Alignment (VPFA)框架能有效建模分辨率特异性特征差异，显著提升CR-ReID性能并提高效率。

Abstract: In surveillance scenarios, varying camera distances cause significant
differences among pedestrian image resolutions, making it hard to match
low-resolution (LR) images with high-resolution (HR) counterparts, limiting the
performance of Re-Identification (ReID) tasks. Most existing Cross-Resolution
ReID (CR-ReID) methods rely on super-resolution (SR) or joint learning for
feature compensation, which increases training and inference complexity and has
reached a performance bottleneck in recent studies. Inspired by semantic
directions in the word embedding space, we empirically discover that semantic
directions implying resolution differences also emerge in the feature space of
ReID, and we substantiate this finding from a statistical perspective using
Canonical Correlation Analysis and Pearson Correlation Analysis. Based on this
interesting finding, we propose a lightweight and effective Vector Panning
Feature Alignment (VPFA) framework, which conducts CR-ReID from a novel
perspective of modeling the resolution-specific feature discrepancy. Extensive
experimental results on multiple CR-ReID benchmarks show that our method
significantly outperforms previous state-of-the-art baseline models while
obtaining higher efficiency, demonstrating the effectiveness and superiority of
our model based on the new finding in this paper.

</details>


### [81] [InfVSR: Breaking Length Limits of Generic Video Super-Resolution](https://arxiv.org/abs/2510.00948)
*Ziqing Zhang,Kai Liu,Zheng Chen,Xi Li,Yucong Chen,Bingnan Duan,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: InfVSR把VSR改成自回归单步扩散，改造DiT为因果结构并蒸馏为单步推理，支持流式长视频超分，质量优、语义一致性高、速度快。


<details>
  <summary>Details</summary>
Motivation: 现有VSR在处理数千帧长视频时效率低、扩展性差（时间分解导致伪影与断裂），需要一种既能流式推理又能利用预训练扩散先验的方案。

Method: 将预训练的DiT改造为因果结构（保留局部与全局一致性，使用滚动KV缓存和联合视觉引导），并通过补丁级像素监督与跨块分布匹配，将多步扩散蒸馏为单步高效推理。

Result: 在新构建的长视频基准上，InfVSR在语义一致性上显著提升，视觉质量达SOTA，且在速度上相比MGLD-VSR最高加速达58倍。

Conclusion: 该论文提出了InfVSR，通过将视频超分辨率重构为自回归单步扩散框架，实现了对超长视频的高效、可扩展处理。

Abstract: Real-world videos often extend over thousands of frames. Existing video
super-resolution (VSR) approaches, however, face two persistent challenges when
processing long sequences: (1) inefficiency due to the heavy cost of multi-step
denoising for full-length sequences; and (2) poor scalability hindered by
temporal decomposition that causes artifacts and discontinuities. To break
these limits, we propose InfVSR, which novelly reformulates VSR as an
autoregressive-one-step-diffusion paradigm. This enables streaming inference
while fully leveraging pre-trained video diffusion priors. First, we adapt the
pre-trained DiT into a causal structure, maintaining both local and global
coherence via rolling KV-cache and joint visual guidance. Second, we distill
the diffusion process into a single step efficiently, with patch-wise pixel
supervision and cross-chunk distribution matching. Together, these designs
enable efficient and scalable VSR for unbounded-length videos. To fill the gap
in long-form video evaluation, we build a new benchmark tailored for extended
sequences and further introduce semantic-level metrics to comprehensively
assess temporal consistency. Our method pushes the frontier of long-form VSR,
achieves state-of-the-art quality with enhanced semantic consistency, and
delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will
be available at https://github.com/Kai-Liu001/InfVSR.

</details>


### [82] [JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation](https://arxiv.org/abs/2510.00974)
*Siheng Wan,Zhengtao Yao,Zhengdao Li,Junhao Dong,Yanshu Li,Yikai Li,Linshan Li,Haoyan Xu,Yijiang Li,Zhikang Dong,Huacan Wang,Jifeng Shen*

Main category: cs.CV

TL;DR: JEPA-T通过晚期架构融合与目标级对齐，在离散token基础的T2I中实现了高效文本-视觉融合，提升了泛化和数据效率，并在ImageNet-1K上优于相关基线。


<details>
  <summary>Details</summary>
Motivation: 当前token为中心的T2I架构难以高效融合文本与视觉tokens，尤其要在保持主干网络任务无关性的同时增强条件化能力与对齐。

Method: 先将图像和caption分别编码为离散视觉与文本tokens；使用joint-embedding predictive Transformer（JEPA）预测特征；在预测器之后加入cross-attention用于条件去噪，同时在flow matching损失之前注入原始文本嵌入以增强对齐；推理时迭代去噪视觉tokens，支持class-conditional与自由文本生成。

Result: 在ImageNet-1K上显示出较强的数据效率、开放词汇泛化能力，且在多项指标上优于非融合和后融合基线，表明后期架构融合加目标级对齐在token-based T2I中是有效的折中。

Conclusion: 提出了JEPA-T，一种将图像和文本编码为离散视觉与文本tokens，并用联合嵌入预测Transformer进行处理的多模态框架，能在训练时通过后续的cross-attention实现条件去噪并在推理时进行文本条件的图像生成。

Abstract: Modern Text-to-Image (T2I) generation increasingly relies on token-centric
architectures that are trained with self-supervision, yet effectively fusing
text with visual tokens remains a challenge. We propose \textbf{JEPA-T}, a
unified multimodal framework that encodes images and captions into discrete
visual and textual tokens, processed by a joint-embedding predictive
Transformer. To enhance fusion, we incorporate cross-attention after the
feature predictor for conditional denoising while maintaining a task-agnostic
backbone. Additionally, raw texts embeddings are injected prior to the flow
matching loss to improve alignment during training. During inference, the same
network performs both class-conditional and free-text image generation by
iteratively denoising visual tokens conditioned on text. Evaluations on
ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency,
open-vocabulary generalization, and consistently outperforms non-fusion and
late-fusion baselines. Our approach shows that late architectural fusion
combined with objective-level alignment offers an effective balance between
conditioning strength and backbone generality in token-based T2I.The code is
now available: https://github.com/justin-herry/JEPA-T.git

</details>


### [83] [A Scene is Worth a Thousand Features: Feed-Forward Camera Localization from a Collection of Image Features](https://arxiv.org/abs/2510.00978)
*Axel Barroso-Laguna,Tommaso Cavallari,Victor Adrian Prisacariu,Eric Brachmann*

Main category: cs.CV

TL;DR: 提出FastForward：一种把映射图像特征锚定到3D并在单次前向传递中完成即时建图与重定位的方法，显著减少建图时间且保持甚至提升定位精度，且具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重定位方法即使在已知相机位姿的建图条件下也需要耗费数分钟到数小时进行地图构建，限制了实际部署的可行性，因而希望探索一种能在极短时间内完成建图并实现高精度重定位的方法。

Method: 将多幅映射图像的特征以3D锚点的形式存储，查询时利用这些3D特征直接预测图像到场景的像素级对应关系，并通过PnP或类似位姿求解器估计相机位姿；与图像检索结合以限定候选地图，提高效率与鲁棒性。

Result: FastForward在最小化地图准备时间的前提下，达到了与其他方法相当甚至更优的定位精度，同时在未知域（包括大规模室外场景）上表现出良好的泛化能力。

Conclusion: FastForward提出了一种将多张带已知位姿的地图图像特征锚定于三维空间的表示，并在单次前向传递中完成地图构建与查询图像的重定位，从而显著缩短了建图时间并在与检索结合时实现了与现有方法相当甚至更好的精度。

Abstract: Visually localizing an image, i.e., estimating its camera pose, requires
building a scene representation that serves as a visual map. The representation
we choose has direct consequences towards the practicability of our system.
Even when starting from mapping images with known camera poses,
state-of-the-art approaches still require hours of mapping time in the worst
case, and several minutes in the best. This work raises the question whether we
can achieve competitive accuracy much faster. We introduce FastForward, a
method that creates a map representation and relocalizes a query image
on-the-fly in a single feed-forward pass. At the core, we represent multiple
mapping images as a collection of features anchored in 3D space. FastForward
utilizes these mapping features to predict image-to-scene correspondences for
the query image, enabling the estimation of its camera pose. We couple
FastForward with image retrieval and achieve state-of-the-art accuracy when
compared to other approaches with minimal map preparation time. Furthermore,
FastForward demonstrates robust generalization to unseen domains, including
challenging large-scale outdoor environments.

</details>


### [84] [Visual Self-Refinement for Autoregressive Models](https://arxiv.org/abs/2510.00993)
*Jiamian Wang,Ziqi Zhou,Chaithanya Kumar Mummadi,Sohail Dianat,Majid Rabbani,Raghuveer Rao,Chen Qiu,Zhiqiang Tao*

Main category: cs.CV

TL;DR: 论文提出一个后处理的可插拔精炼模块，通过联合精炼自回归模型生成的所有视觉token，利用全局上下文缓解误差累积，从而提升视觉-语言生成的语义一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在处理视觉-语言数据时，视觉信号的空间特性与逐步下一token预测的序列依赖存在冲突，导致捕捉复杂空间对应关系困难和误差累积，影响生成结果的语义一致性。论文旨在通过一个后处理精炼模块解决这些问题。

Method: 在预训练后作为后处理步骤插入的精炼模块，对自回归模型生成的所有视觉token进行联合精炼。该模块利用全局上下文和token间关系，打破传统逐步依赖的限制，对生成序列做全局一致性调整，仍在共享的序列预测框架下运行，属于plug-and-play设计，可与现有模型配合使用。

Result: 实验表明，该方法能提升生成质量，使模型生成更语义一致的视觉结果。具体实验细节和量化指标在摘要中未给出。

Conclusion: 该论文提出了一个可插拔的后处理精炼模块，用于提升自回归视觉-语言模型对生成视觉序列中复杂空间对应关系的建模能力，缓解序列生成中的误差累积问题，从而在语义一致性和生成质量上取得提升。

Abstract: Autoregressive models excel in sequential modeling and have proven to be
effective for vision-language data. However, the spatial nature of visual
signals conflicts with the sequential dependencies of next-token prediction,
leading to suboptimal results. This work proposes a plug-and-play refinement
module to enhance the complex spatial correspondence modeling within the
generated visual sequence. This module operates as a post-pretraining step to
jointly refine all generated tokens of autoregressive model, enhancing
vision-language modeling under a shared sequential prediction framework. By
leveraging global context and relationship across the tokens, our method
mitigates the error accumulation issue within the sequential generation.
Experiments demonstrate that the proposed method improves the generation
quality, enhancing the model's ability to produce semantically consistent
results.

</details>


### [85] [SoftCFG: Uncertainty-guided Stable Guidance for Visual autoregressive Model](https://arxiv.org/abs/2510.00996)
*Dongli Xu,Aleksei Tiulpin,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: SoftCFG通过基于每个token的不确定性加权分配条件引导并用Step Normalization约束扰动累积，解决了AR模型中CFG的信号衰减与过导问题，无训练代价下提升了生成质量并在ImageNet 256上刷新自回归模型FID。


<details>
  <summary>Details</summary>
Motivation: 传统Classifier-Free Guidance在自回归图像生成中存在两个关键问题：随着解码推进，条件与无条件预测差异迅速消失（guidance diminishing），以及当放大条件强度时易造成视觉失真（over-guidance）。需要一种在整个序列上维持条件信号同时避免过强引导的方法。

Method: 提出SoftCFG：基于每个生成token的不确定性（certainty）计算权重，将带权重的条件-无条件logits差分按步加入所有token的采样过程中；引入Step Normalization对累计扰动进行规范化，防止长序列中扰动过大。方法无需训练，能无缝加到现有AR推理流程中。

Result: 在多项实验中，SoftCFG相较于标准CFG显著提升图像质量，并在ImageNet 256分辨率上取得了自回归模型的最优FID值。方法泛化且无需额外训练。

Conclusion: SoftCFG有效缓解了AR模型中CFG的guidance diminishing与over-guidance问题，通过在所有token上分配不确定性引导并用Step Normalization限制累积扰动，保持条件信号并提高视觉一致性，从而在训练自由、模型不可知的设置下显著提升图像质量，在ImageNet 256上达成了AR模型的最佳FID。

Abstract: Autoregressive (AR) models have emerged as powerful tools for image
generation by modeling images as sequences of discrete tokens. While
Classifier-Free Guidance (CFG) has been adopted to improve conditional
generation, its application in AR models faces two key issues: guidance
diminishing, where the conditional-unconditional gap quickly vanishes as
decoding progresses, and over-guidance, where strong conditions distort visual
coherence. To address these challenges, we propose SoftCFG, an
uncertainty-guided inference method that distributes adaptive perturbations
across all tokens in the sequence. The key idea behind SoftCFG is to let each
generated token contribute certainty-weighted guidance, ensuring that the
signal persists across steps while resolving conflicts between text guidance
and visual context. To further stabilize long-sequence generation, we introduce
Step Normalization, which bounds cumulative perturbations of SoftCFG. Our
method is training-free, model-agnostic, and seamlessly integrates with
existing AR pipelines. Experiments show that SoftCFG significantly improves
image quality over standard CFG and achieves state-of-the-art FID on ImageNet
256 among autoregressive models.

</details>


### [86] [TextCAM: Explaining Class Activation Map with Text](https://arxiv.org/abs/2510.01004)
*Qiming Zhao,Xingjian Li,Xiaoyu Cao,Xiaolong Wu,Min Xu*

Main category: cs.CV

TL;DR: TextCAM把CAM的空间热图与CLIP语义嵌入结合，通过LDA与通道加权生成图像区域对应的文本描述，并通过通道分组实现更细粒度的视觉-文本解释，在多数据集上验证了其忠实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: CAM虽能定位重要空间区域，但难以解释哪些视觉属性支撑模型预测，限制高风险场景的可信度。需要将空间注意力与自然语言语义结合以提升可解释性。

Method: 使用CLIP获得通道级语义表示，结合线性判别分析(LDA)提取区分性语义方向，并用CAM的通道权重加权聚合为文本描述；并进一步对通道进行语义分组以提供更细粒度解释。

Result: 在ImageNet、CLEVR和CUB上的实验表明，TextCAM能提供忠实且可解释的理由，提升人类理解、检测虚假相关性并保持模型保真度。

Conclusion: TextCAM通过将CAM的空间定位与VLM语义对齐相结合，生成可解释的视觉-文本证据，从而解决了CAM缺乏语义洞察的问题。

Abstract: Deep neural networks (DNNs) have achieved remarkable success across domains
but remain difficult to interpret, limiting their trustworthiness in
high-stakes applications. This paper focuses on deep vision models, for which a
dominant line of explainability methods are Class Activation Mapping (CAM) and
its variants working by highlighting spatial regions that drive predictions. We
figure out that CAM provides little semantic insight into what attributes
underlie these activations. To address this limitation, we propose TextCAM, a
novel explanation framework that enriches CAM with natural languages. TextCAM
combines the precise spatial localization of CAM with the semantic alignment of
vision-language models (VLMs). Specifically, we derive channel-level semantic
representations using CLIP embeddings and linear discriminant analysis, and
aggregate them with CAM weights to produce textual descriptions of salient
visual evidence. This yields explanations that jointly specify where the model
attends and what visual attributes likely support its decision. We further
extend TextCAM to generate feature channels into semantically coherent groups,
enabling more fine-grained visual-textual explanations. Experiments on
ImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and
interpretable rationales that improve human understanding, detect spurious
correlations, and preserve model fidelity.

</details>


### [87] [POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency](https://arxiv.org/abs/2510.01009)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CV

TL;DR: 通过每秒时间聚合成单帧并对QWEN-2.5-VL进行SFT+DPO，POVQA在ReasonVQA上大幅提高长视频VQA性能且对不同池化方法鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM长视频VQA受限于上下文窗口只能覆盖短时长（约50s），导致信息丢失。通过高效压缩每秒信息并针对性微调模型，能在有限上下文窗口内保留更多关键时序证据。

Method: 按1fps构建输入：Blend Blur with Last Frame、Weighted Average、Exponential、Ramp等时序池化（motion blur/加权平均），然后用QWEN-2.5-VL 7B进行SFT和DPO训练，监督目标为包含推理和最终答案的两轮文本。

Result: 在ReasonVQA（12部电影、239条人工标注QA+推理）上，F1从0.212提升到0.543，BLEU-4从0.031提升到0.291，ROUGE-L从0.196提升到0.528；推理质量也显著提高。跨池化函数的SFT+DPO评估显示效果稳健，TVQA零样本测试也有类似提升。

Conclusion: POVQA通过对视频按秒压缩成单帧并对大模型进行轻量监督微调，显著提升长视频VQA性能，尤其在ReasonVQA数据集上有大幅度收益，且对不同时序池化方法具有鲁棒性。

Abstract: Video Question Answering (VQA) with Large Vision Language Models (LVLMs) has
gained significant traction in research ever since the Flamingo was introduced
by Deepmind. Recent advancements in large context/long video question answering
have allowed VQA tasks to have context window of 1500+ frames. However, this
only leads to 50 seconds of video footage without losing any significant
information. We introduce POVQA, a data-efficient pipeline that compresses each
second of video into a single temporally pooled image (via motion blur and
weighted averaging variants) and then align LVLMs with lightweight supervision.
Concretely, we build 1 fps input sources using Blend Blur with Last Frame,
Weighted Average, Exponential and Ramp pooling and fine-tune QWEN-2.5-VL 7B
with supervised two turn target including reasoning and final answer. We apply
Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) on our
novel dataset ReasonVQA consisting of 12 movies with 239 human annotated
question-answer with reasoning prompts. On our ReasonVQA dataset, this method
dramatically improves performance over pooled baselines: F1 score improves from
0.212 to 0.543, BLEU-4 from 0.031 to 0.291, and ROUGE-L from 0.196 to 0.528.
Rationale quality also significantly increases. Cross-evaluation of SFT + DPO
on various pooling functions show that the gains persist regardless of the
pooling scheme used at train or test time, indicating strong robustness on
summarization of temporal evidence. Similar observations were made on zero-shot
in TVQA.

</details>


### [88] [ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning](https://arxiv.org/abs/2510.01010)
*Yuxiang Guo,Jiang Liu,Ze Wang,Hao Chen,Ximeng Sun,Yang Zhao,Jialian Wu,Xiaodong Yu,Zicheng Liu,Emad Barsoum*

Main category: cs.CV

TL;DR: ImageDoctor：一个具备可解释多维评分与像素级热图的 T2I 评价与奖励模型，采用“看-思考-预测”流程并结合 RL 优化，能更好对齐人类偏好并提升生成质量（≈+10%）。


<details>
  <summary>Details</summary>
Motivation: 现有 T2I 质量评价多用单一标量，缺乏可解释性和细粒度反馈，难以为偏好对齐提供有效密集奖励；因此需要多维、可解释且能输出像素级提示的评价器。

Method: 基于视-语模型，采用“看-思考-预测”范式：先定位潜在缺陷（生成热图），再生成推理理由，最后给出四个维度的量化分数；训练结合监督微调与强化学习（偏好优化）。

Result: 在多数据集上与人类偏好高度一致；作为奖励模型用于偏好微调时，相比标量奖励提升约10%的生成质量；在细节敏感性与推理能力上表现优异。

Conclusion: ImageDoctor 提出了一种多维评估框架，通过可解释的多方面评分与像素级热图，能更全面地评价 T2I 生成图像质量，并作为密集奖励用于偏好对齐，显著优于标量奖励方法。

Abstract: The rapid advancement of text-to-image (T2I) models has increased the need
for reliable human preference modeling, a demand further amplified by recent
progress in reinforcement learning for preference alignment. However, existing
approaches typically quantify the quality of a generated image using a single
scalar, limiting their ability to provide comprehensive and interpretable
feedback on image quality. To address this, we introduce ImageDoctor, a unified
multi-aspect T2I model evaluation framework that assesses image quality across
four complementary dimensions: plausibility, semantic alignment, aesthetics,
and overall quality. ImageDoctor also provides pixel-level flaw indicators in
the form of heatmaps, which highlight misaligned or implausible regions, and
can be used as a dense reward for T2I model preference alignment. Inspired by
the diagnostic process, we improve the detail sensitivity and reasoning
capability of ImageDoctor by introducing a "look-think-predict" paradigm, where
the model first localizes potential flaws, then generates reasoning, and
finally concludes the evaluation with quantitative scores. Built on top of a
vision-language model and trained through a combination of supervised
fine-tuning and reinforcement learning, ImageDoctor demonstrates strong
alignment with human preference across multiple datasets, establishing its
effectiveness as an evaluation metric. Furthermore, when used as a reward model
for preference tuning, ImageDoctor significantly improves generation quality --
achieving an improvement of 10% over scalar-based reward models.

</details>


### [89] [Towards Adversarial Training under Hyperspectral Images](https://arxiv.org/abs/2510.01014)
*Weihua Zhang,Chengze Jiang,Jie Gui,Lu Dong*

Main category: cs.CV

TL;DR: 论文提出将对抗训练应用于高光谱图像，针对频谱语义被噪声破坏的问题，通过数据增强与空间平滑形成AT-RA方法，显著提高了鲁棒性与洁净样本准确率。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在高光谱分类中易受对抗攻击；以往结构改动方法可扩展性差且对强攻击效果有限，需在高光谱域验证并改进对抗防御。

Method: 论文基于对抗训练，结合数据增强与空间平滑策略（AT-RA），提升频谱信息多样性并纠正/平滑对抗噪声导致的频谱语义破坏。

Result: AT-RA在实验中对AutoAttack提升鲁棒性21.34%、对PGD-50提升18.78%，并在无攻击下提高准确率2.68%。

Conclusion: 该论文将对抗训练引入高光谱分类领域，并提出了AT-RA方法以保留频谱语义并提高鲁棒性。

Abstract: Recent studies have revealed that hyperspectral classification models based
on deep learning are highly vulnerable to adversarial attacks, which pose
significant security risks. Although several approaches have attempted to
enhance adversarial robustness by modifying network architectures, these
methods often rely on customized designs that limit scalability and fail to
defend effectively against strong attacks. To address these challenges, we
introduce adversarial training to the hyperspectral domain, which is widely
regarded as one of the most effective defenses against adversarial attacks.
Through extensive empirical analyses, we demonstrate that while adversarial
training does enhance robustness across various models and datasets,
hyperspectral data introduces unique challenges not seen in RGB images.
Specifically, we find that adversarial noise and the non-smooth nature of
adversarial examples can distort or eliminate important spectral semantic
information. To mitigate this issue, we employ data augmentation techniques and
propose a novel hyperspectral adversarial training method, termed AT-RA. By
increasing the diversity of spectral information and ensuring spatial
smoothness, AT-RA preserves and corrects spectral semantics in hyperspectral
images. Experimental results show that AT-RA improves adversarial robustness by
21.34% against AutoAttack and 18.78% against PGD-50 while boosting benign
accuracy by 2.68%.

</details>


### [90] [Secure and reversible face anonymization with diffusion models](https://arxiv.org/abs/2510.01031)
*Pol Labarbarie,Vincent Itier,William Puech*

Main category: cs.CV

TL;DR: 提出一种将秘密密钥嵌入扩散潜空间并结合面部掩码与确定性扩散的可逆人脸匿名化方法，兼顾安全性、图像质量与可逆性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在图像质量、安全性（密钥机制）与可逆性之间取得良好权衡，扩散模型虽能生成高质量匿名图像但缺乏密钥机制保障只能被授权方逆转。

Method: 将秘密密钥与扩散模型的潜在人脸表示结合，通过面部掩码约束生成以保留与身份无关的特征，并采用确定性的前向和反向扩散过程确保在正确密钥下可恢复原始人脸。

Result: 提出的方法在保持高质量图像的同时，能够在给定密钥时可逆恢复原始人脸，且生成的匿名人脸在视觉上比以往方法更少与原始人脸相似。

Conclusion: 该论文提出了首个基于扩散模型的安全且可逆的人脸匿名化方法，实现了高质量图像生成与可逆性之间的平衡。

Abstract: Face images processed by computer vision algorithms contain sensitive
personal information that malicious actors can capture without consent. These
privacy and security risks highlight the need for effective face anonymization
methods. Current methods struggle to propose a good trade-off between a secure
scheme with high-quality image generation and reversibility for later person
authentication. Diffusion-based approaches produce high-quality anonymized
images but lack the secret key mechanism to ensure that only authorized parties
can reverse the process. In this paper, we introduce, to our knowledge, the
first secure, high-quality reversible anonymization method based on a diffusion
model. We propose to combine the secret key with the latent faces
representation of the diffusion model. To preserve identity-irrelevant
features, generation is constrained by a facial mask, maintaining high-quality
images. By using a deterministic forward and backward diffusion process, our
approach enforces that the original face can be recovered with the correct
secret key. We also show that the proposed method produces anonymized faces
that are less visually similar to the original faces, compared to other
previous work.

</details>


### [91] [Authentic Discrete Diffusion Model](https://arxiv.org/abs/2510.01047)
*Xiao Li,Jiaqi Zhang,Shuxiang Zhang,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: ADD是一种在one-hot离散空间上直接进行扩散的框架，通过timestep-conditioned交叉熵连接判别与生成，改善了分类与图像描述任务表现，并经消融验证了各部分贡献。


<details>
  <summary>Details</summary>
Motivation: 现有伪离散扩散方法通过在连续潜空间或掩码策略上操作，不能保留离散空间的核心扩散特性，且难以把判别信息直接融入生成模型。ADD意在在真实离散空间保留扩散属性并实现判别-生成协同。

Method: 直接对float编码的一热向量进行扩散，不依赖连续潜变量或mask策略；使用timestep-conditioned交叉熵作为训练目标，使模型输出与原始one-hot标签对齐。

Result: 在分类任务上，ADD优于基线方法；在图像描述（图像到文本生成）任务上表现出良好文本生成能力。消融实验显示各组件均带来可量化的提升。

Conclusion: ADD提出了在one-hot离散空间上直接进行扩散的新范式，通过时间步调条件交叉熵损失等机制在生成与判别之间建立联系，从而克服了伪离散方法的局限。

Abstract: We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally
redefines prior pseudo-discrete approaches by preserving core diffusion
characteristics directly in the one-hot space through a suite of coordinated
mechanisms. Unlike conventional "pseudo" discrete diffusion (PDD) methods, ADD
reformulates the diffusion input by directly using float-encoded one-hot class
data, without relying on diffusing in the continuous latent spaces or masking
policies. At its core, a timestep-conditioned cross-entropy loss is introduced
between the diffusion model's outputs and the original one-hot labels. This
synergistic design establishes a bridge between discriminative and generative
learning. Our experiments demonstrate that ADD not only achieves superior
performance on classification tasks compared to the baseline, but also exhibits
excellent text generation capabilities on Image captioning. Extensive ablations
validate the measurable gains of each component.

</details>


### [92] [KeySG: Hierarchical Keyframe-Based 3D Scene Graphs](https://arxiv.org/abs/2510.01049)
*Abdelrhman Werby,Dennis Rotondi,Fabio Scaparro,Kai O. Arras*

Main category: cs.CV

TL;DR: KeySG通过关键帧驱动的多模态分层3D场景图与层次RAG检索，提升了场景语义表达与LLM交互效率，解决了关系边限定和上下文窗口扩展问题，在多个基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图在关系类型上受限且在大型环境中序列化会超出LLM上下文，限制机器人在复杂人类环境中的推理與规划能力，因此需要一种更灵活、可扩展且能与LLM高效交互的表示方法。

Method: 构建包含楼层、房间、物体与功能元素的层次化图谱；使用优化几何与视觉覆盖的关键帧选择策略，从关键帧中调用视觉语言模型(VLM)提取多模态节点信息；省略显式对象关系边，转而通过关键帧信息与层次化检索增强生成(RAG)管线来回答复杂查询并支持任务无关推理。

Result: 在四个基准测试（包括3D物体分割与复杂查询检索）上，KeySG在大多数指标上优于先前方法，显示出更高的语义丰富性与效率。

Conclusion: KeySG提出了一种基于关键帧增强的分层3D场景图表示，克服了传统方法关系定义有限和上下文窗口受限的问题，通过关键帧结合多模态信息及层次化RAG检索，提升了语义丰富性与扩展性。

Abstract: In recent years, 3D scene graphs have emerged as a powerful world
representation, offering both geometric accuracy and semantic richness.
Combining 3D scene graphs with large language models enables robots to reason,
plan, and navigate in complex human-centered environments. However, current
approaches for constructing 3D scene graphs are semantically limited to a
predefined set of relationships, and their serialization in large environments
can easily exceed an LLM's context window. We introduce KeySG, a framework that
represents 3D scenes as a hierarchical graph consisting of floors, rooms,
objects, and functional elements, where nodes are augmented with multi-modal
information extracted from keyframes selected to optimize geometric and visual
coverage. The keyframes allow us to efficiently leverage VLM to extract scene
information, alleviating the need to explicitly model relationship edges
between objects, enabling more general, task-agnostic reasoning and planning.
Our approach can process complex and ambiguous queries while mitigating the
scalability issues associated with large scene graphs by utilizing a
hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant
context from the graph. Evaluated across four distinct benchmarks -- including
3D object segmentation and complex query retrieval -- KeySG outperforms prior
approaches on most metrics, demonstrating its superior semantic richness and
efficiency.

</details>


### [93] [Instant4D: 4D Gaussian Splatting in Minutes](https://arxiv.org/abs/2510.01119)
*Zhanpeng Luo,Haoxi Ran,Li Lu*

Main category: cs.CV

TL;DR: Instant4D: fast monocular 4D reconstruction for casual uncalibrated videos—deep visual SLAM + grid pruning + 4D Gaussians = 10x+ smaller models and 30x faster training, enabling per-video reconstruction in ~10 minutes.


<details>
  <summary>Details</summary>
Motivation: To overcome slow optimization and complex parameter estimation in dynamic view synthesis from uncalibrated casual videos by designing a fast, compact, and generalizable monocular reconstruction system.

Method: Pipeline: (1) geometric recovery via deep visual SLAM from monocular casual video, (2) grid pruning to remove redundant scene regions and reduce model size to <10% of original, (3) streamlined 4D Gaussian representation for temporal dynamics enabling ~30x speed-up and fast training (~2 minutes), overall reconstructing a 200-frame video within ~10 minutes. No calibrated cameras or depth sensors needed.

Result: Achieves ~30x speed-up versus previous methods, model size reduced to <10% after pruning, reconstructs a 200-frame video within ~10 minutes, competitive performance on several benchmarks (including Dycheck), and generalizes to in-the-wild videos.

Conclusion: Instant4D presents an efficient monocular dynamic scene reconstruction pipeline that uses native 4D representations and deep visual SLAM to reconstruct casual uncalibrated videos within minutes, achieving competitive quality while drastically reducing model size and training time.

Abstract: Dynamic view synthesis has seen significant advances, yet reconstructing
scenes from uncalibrated, casual video remains challenging due to slow
optimization and complex parameter estimation. In this work, we present
Instant4D, a monocular reconstruction system that leverages native 4D
representation to efficiently process casual video sequences within minutes,
without calibrated cameras or depth sensors. Our method begins with geometric
recovery through deep visual SLAM, followed by grid pruning to optimize scene
representation. Our design significantly reduces redundancy while maintaining
geometric integrity, cutting model size to under 10% of its original footprint.
To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian
representation, achieving a 30x speed-up and reducing training time to within
two minutes, while maintaining competitive performance across several
benchmarks. Our method reconstruct a single video within 10 minutes on the
Dycheck dataset or for a typical 200-frame video. We further apply our model to
in-the-wild videos, showcasing its generalizability. Our project website is
published at https://instant4d.github.io/.

</details>


### [94] [Strategic Fusion of Vision Language Models: Shapley-Credited Context-Aware Dawid-Skene for Multi-Label Tasks in Autonomous Driving](https://arxiv.org/abs/2510.01126)
*Yuxiang Feng,Keyang Zhang,Hassane Ouchouid,Ashwil Kaniamparambil,Ioannis Souflas,Panagiotis Angeloudis*

Main category: cs.CV

TL;DR: 提出了一种结合上下文和Shapley声誉的博弈论融合方法，对多VLM输出进行护栏对数似然融合，显著提升了自动驾驶视频多标签理解的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大VLM在自动驾驶堆栈中越来越被采用，但其幻觉行为在安全关键场景中限制了可靠性，因此需要一种能融合多模型、利用上下文和声誉信息、同时保留单一正确信号的鲁棒融合机制。

Method: 方法从带标签历史中学习每个模型、每个标签在不同上下文下的可靠性；推理时将模型报告转换为带“agreement”护栏的对数似然比，与上下文先验和基于Shapley贡献的公共声誉状态相结合，生成可校准的后验概率，并通过自动数据构建与LoRA微调的VLM专门化流程验证。

Result: 在构建的1000条真实行车记录仪视频数据集上，所提方法相比最优单模型实现了Hamming距离减少23%、Macro-F1提升55%、Micro-F1提升47%，并保持了可阈值化的校准后验和可解释性。

Conclusion: 本文提出了一种基于博弈论的多模型融合方法（Shapley-credited Context-Aware Dawid-Skene with Agreement），用于自动驾驶车载摄像头的视频多标签理解，旨在缓解大视觉语言模型（VLMs）的幻觉问题并提高决策可靠性。

Abstract: Large vision-language models (VLMs) are increasingly used in
autonomous-vehicle (AV) stacks, but hallucination limits their reliability in
safety-critical pipelines. We present Shapley-credited Context-Aware
Dawid-Skene with Agreement, a game-theoretic fusion method for multi-label
understanding of ego-view dashcam video. It learns per-model, per-label,
context-conditioned reliabilities from labelled history and, at inference,
converts each model's report into an agreement-guardrailed log-likelihood ratio
that is combined with a contextual prior and a public reputation state updated
via Shapley-based team credit. The result is calibrated, thresholdable
posteriors that (i) amplify agreement among reliable models, (ii) preserve
uniquely correct single-model signals, and (iii) adapt to drift. To specialise
general VLMs, we curate 1,000 real-world dashcam clips with structured
annotations (scene description, manoeuvre recommendation, rationale) via an
automatic pipeline that fuses HDD ground truth, vehicle kinematics, and YOLOv11
+ BoT-SORT tracking, guided by a three-step chain-of-thought prompt; three
heterogeneous VLMs are then fine-tuned with LoRA. We evaluate with Hamming
distance, Micro-Macro-F1, and average per-video latency. Empirically, the
proposed method achieves a 23% reduction in Hamming distance, 55% improvement
in Macro-F1, and 47% improvement in Micro-F1 when comparing with the best
single model, supporting VLM fusion as a calibrated, interpretable, and robust
decision-support component for AV pipelines.

</details>


### [95] [Code2Video: A Code-centric Paradigm for Educational Video Generation](https://arxiv.org/abs/2510.01174)
*Yanzhe Chen,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出 Code2Video：以可执行 Python 代码驱动、由 Planner/Coder/Critic 协同工作的多代理框架，面向专业教学视频生成；在自建 MMMC 基准和 TeachQuiz 教学有效性评估上优于直接代码生成，提升约40%。


<details>
  <summary>Details</summary>
Motivation: 像素级视频生成难以满足专业教学视频对学科知识、精确视觉结构及连贯过渡的高要求，使用可渲染环境并通过逻辑命令（代码）控制能更好地实现这些需求。

Method: 采用三代理协同：Planner 负责内容分段与视觉素材准备；Coder 将结构化指令转成可执行 Python 代码，并使用基于作用域的自动修复提升执行成功率；Critic 利用 VLM 与视觉锚点提示优化空间布局与清晰度。引入可执行代码驱动渲染环境以提高可控性与可解释性。

Result: 在 MMMC 基准上，Code2Video 在 VLM 审美得分、代码效率等多维度均表现良好；在 TeachQuiz 指标上相比直接代码生成提升约40%，并宣称生成的视频可与人工制作者的教程相当。

Conclusion: Code2Video 提出了一种以可执行 Python 代码为中心的多代理框架，通过 Planner、Coder、Critic 三个协同代理生成具备学科知识和精确视觉结构的教学视频。实验在 MMMC 基准上显示其在代码效率、视觉质量和教学有效性（TeachQuiz 指标）方面优于直接代码生成方法，声称可达到与人工教程相当的效果。

Abstract: While recent generative models advance pixel-space video synthesis, they
remain limited in producing professional educational videos, which demand
disciplinary knowledge, precise visual structures, and coherent transitions,
limiting their applicability in educational scenarios. Intuitively, such
requirements are better addressed through the manipulation of a renderable
environment, which can be explicitly controlled via logical commands (e.g.,
code). In this work, we propose Code2Video, a code-centric agent framework for
generating educational videos via executable Python code. The framework
comprises three collaborative agents: (i) Planner, which structures lecture
content into temporally coherent flows and prepares corresponding visual
assets; (ii) Coder, which converts structured instructions into executable
Python codes while incorporating scope-guided auto-fix to enhance efficiency;
and (iii) Critic, which leverages vision-language models (VLM) with visual
anchor prompts to refine spatial layout and ensure clarity. To support
systematic evaluation, we build MMMC, a benchmark of professionally produced,
discipline-specific educational videos. We evaluate MMMC across diverse
dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and
particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a
VLM, after unlearning, can recover knowledge by watching the generated videos.
Our results demonstrate the potential of Code2Video as a scalable,
interpretable, and controllable approach, achieving 40% improvement over direct
code generation and producing videos comparable to human-crafted tutorials. The
code and datasets are available at https://github.com/showlab/Code2Video.

</details>


### [96] [EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory](https://arxiv.org/abs/2510.01183)
*Jiahao Wang,Luoxin Ye,TaiMing Lu,Junfei Xiao,Jiahan Zhang,Yuxiang Guo,Xijun Liu,Rama Chellappa,Cheng Peng,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: EvoWorld结合可演化的显式3D重建与细粒度视角可控的视频生成，通过几何重投影为视频合成提供空间引导，从而实现长程、空间一致的全景视频生成与探索。


<details>
  <summary>Details</summary>
Motivation: 受人类在心理上回放并探索已见3D环境能力启发，旨在构建既能生成全景视频又能维护随时间演化的3D记忆的世界模型，以实现长程空间一致的探索与回放。

Method: 给定单帧全景图像，先用具有精细视角控制的视频生成器预测未来帧；再用前馈即插即用的变压器演化场景3D重建；最后基于该演化的显式3D记忆的几何重投影条件化合成未来帧，从而将几何引导融入视频合成。

Result: 引入首个覆盖合成室外、Habitat室内和真实场景的基准，重点测试闭环检测与长期轨迹的空间连贯性。实验证明，演化的3D记忆在视觉逼真度和空间一致性上均显著提升。

Conclusion: EvoWorld通过将演化的显式3D记忆与全景视频生成整合，实现了长时间空间一致性探索。该方法在视觉保真度和几何一致性上显著优于仅合成视频的方法，推动了长期空间一致世界建模的进展。

Abstract: Humans possess a remarkable ability to mentally explore and replay 3D
environments they have previously experienced. Inspired by this mental process,
we present EvoWorld: a world model that bridges panoramic video generation with
evolving 3D memory to enable spatially consistent long-horizon exploration.
Given a single panoramic image as input, EvoWorld first generates future video
frames by leveraging a video generator with fine-grained view control, then
evolves the scene's 3D reconstruction using a feedforward plug-and-play
transformer, and finally synthesizes futures by conditioning on geometric
reprojections from this evolving explicit 3D memory. Unlike prior
state-of-the-arts that synthesize videos only, our key insight lies in
exploiting this evolving 3D reconstruction as explicit spatial guidance for the
video generation process, projecting the reconstructed geometry onto target
viewpoints to provide rich spatial cues that significantly enhance both visual
realism and geometric consistency. To evaluate long-range exploration
capabilities, we introduce the first comprehensive benchmark spanning synthetic
outdoor environments, Habitat indoor scenes, and challenging real-world
scenarios, with particular emphasis on loop-closure detection and spatial
coherence over extended trajectories. Extensive experiments demonstrate that
our evolving 3D memory substantially improves visual fidelity and maintains
spatial scene coherence compared to existing approaches, representing a
significant advance toward long-horizon spatially consistent world modeling.

</details>


### [97] [IMAGEdit: Let Any Subject Transform](https://arxiv.org/abs/2510.01186)
*Fei Shen,Weihao Xu,Rui Yan,Dong Zhang,Xiangbo Shu,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出IMAGEdit：一个训练免费、兼容任意mask驱动视频生成模型的多主体视频编辑框架，通过提示引导的多模态对齐与先验mask重定向实现精确多主体编辑，在MSVBench上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑在多主体场景下受限于提示侧多模态条件不足、mask边界纠缠以及需要微调/重训练的问题，无法灵活、高质量地编辑任意数量的主体。IMAGEdit旨在解决这些限制，扩展多主体视频编辑的适用性。

Method: 方法包括两个主要模块：1) 基于提示的多模态对齐模块，利用大型模型生成多模态条件信息和多主体的mask运动序列；2) 基于先验的mask重定向模块，将先验mask序列优化后输入到预训练的mask驱动视频生成模型以合成编辑后的视频。整个流程无需微调，强调强泛化与对边界纠缠的纠正。

Result: 在作者构建的多主体基准MSVBench上，IMAGEdit在多项指标和定性结果上均显著优于最先进方法；且能与任何mask驱动视频生成模型兼容，提升整体性能。

Conclusion: IMAGEdit是一种无需训练的多目标视频主体编辑框架，能在不微调/重训练的前提下改变多个指定主体的外观并保持非目标区域不变，兼容任意基于mask的视频生成模型，且在新建的MSVBench基准上优于现有方法。

Abstract: In this paper, we present IMAGEdit, a training-free framework for any number
of video subject editing that manipulates the appearances of multiple
designated subjects while preserving non-target regions, without finetuning or
retraining. We achieve this by providing robust multimodal conditioning and
precise mask sequences through a prompt-guided multimodal alignment module and
a prior-based mask retargeting module. We first leverage large models'
understanding and generation capabilities to produce multimodal information and
mask motion sequences for multiple subjects across various types. Then, the
obtained prior mask sequences are fed into a pretrained mask-driven video
generation model to synthesize the edited video. With strong generalization
capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning
and overcomes mask boundary entanglement in videos with any number of subjects,
thereby significantly expanding the applicability of video editing. More
importantly, IMAGEdit is compatible with any mask-driven video generation
model, significantly improving overall performance. Extensive experiments on
our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit
consistently surpasses state-of-the-art methods. Code, models, and datasets are
publicly available at https://github.com/XWH-A/IMAGEdit.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [98] [AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents](https://arxiv.org/abs/2510.00039)
*Hossein Sholehrasa,Amirhossein Ghanaatian,Doina Caragea,Lisa A. Tell,Jim E. Riviere,Majid Jaberi-Douraki*

Main category: cs.DB

TL;DR: 提出AutoPK两阶段框架，结合LLM与混合相似度和验证，实现从复杂表格中高精度、低幻觉地提取并标准化PK参数，实验证明在多模型上大幅优于直接LLM基线。


<details>
  <summary>Details</summary>
Motivation: PK 数据通常嵌在结构多样、术语不一致的复杂表格中，自动检索与标准化困难，影响药物安全与有效性评估；因此需要一个高精度、可扩展的方法来提取和标准化PK表格数据。

Method: 第一阶段使用大语言模型（LLM）结合混合相似度度量和LLM验证来识别并抽取PK参数变体；第二阶段筛选相关表格行，将表转为键值文本格式，再用LLM重构标准化表格。

Result: 在605个真实世界PK表格数据集上，AutoPK 比直接使用LLM有显著提升。以 LLaMA 3.1-70B 为例，半衰期F1=0.92（提升0.10），清除率F1=0.91（提升0.21）；较小模型也获得2-7倍F1提升，并把Gemma的幻觉率从60-95%降到8-14%。部分开源模型在若干参数上超越GPT-4o Mini。

Conclusion: AutoPK 是一个两阶段框架，能显著提高从复杂科学表格中提取药代动力学（PK）数据的准确性与可扩展性，尤其在半衰期和清除率等关键参数上表现优异，并能使开源模型在若干指标上超越商用系统。

Abstract: Pharmacokinetics (PK) plays a critical role in drug development and
regulatory decision-making for human and veterinary medicine, directly
affecting public health through drug safety and efficacy assessments. However,
PK data are often embedded in complex, heterogeneous tables with variable
structures and inconsistent terminologies, posing significant challenges for
automated PK data retrieval and standardization. AutoPK, a novel two-stage
framework for accurate and scalable extraction of PK data from complex
scientific tables. In the first stage, AutoPK identifies and extracts PK
parameter variants using large language models (LLMs), a hybrid similarity
metric, and LLM-based validation. The second stage filters relevant rows,
converts the table into a key-value text format, and uses an LLM to reconstruct
a standardized table. Evaluated on a real-world dataset of 605 PK tables,
including captions and footnotes, AutoPK shows significant improvements in
precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA
3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance
parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and
0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with
AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's
hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled
open-source models like Gemma 3-27B to outperform commercial systems such as
GPT-4o Mini on several PK parameters. AutoPK enables scalable and
high-confidence PK data extraction, making it well-suited for critical
applications in veterinary pharmacology, drug safety monitoring, and public
health decision-making, while addressing heterogeneous table structures and
terminology and demonstrating generalizability across key PK parameters. Code
and data: https://github.com/hosseinsholehrasa/AutoPK

</details>


### [99] [Data Quality Taxonomy for Data Monetization](https://arxiv.org/abs/2510.00089)
*Eduardo Vyhmeister,Bastien Pietropoli,Andrea Visentin*

Main category: cs.DB

TL;DR: 通过系统综述提出基于平衡计分卡的全面数据质量分类法，将100+指标分为四类，连接财务、客户、流程和学习成长四个视角，帮助将数据质量管理纳入数据货币化战略。


<details>
  <summary>Details</summary>
Motivation: 弥合技术层面的细化数据质量评估与高层决策之间的鸿沟，为从业者和战略者提供可扩展的、基于证据的参考，以将数据质量管理与可持续价值创造对齐。

Method: 通过系统文献综述汇总并组织了100多个指标/KPI，将其分为四个子簇（基础、上下文、分辨率、专业化），并嵌入到BSC框架中，构建互联的“指标层”。

Result: 提出的分类法展示了数据质量如何支持估值准确性、客户信任、运营效率和创新能力，并通过互联的指标层实现维度间提升的级联效应。该框架可供数据管护者和决策者用于实践指导。

Conclusion: 本文提出了一个将数据质量纳入数据货币化（data monetisation）评估的全面分类法，强调数据质量在战略层面的连接作用，并将质量度量与平衡计分卡（BSC）的四个视角对齐。

Abstract: This chapter presents a comprehensive taxonomy for assessing data quality in
the context of data monetisation, developed through a systematic literature
review. Organising over one hundred metrics and Key Performance Indicators
(KPIs) into four subclusters (Fundamental, Contextual, Resolution, and
Specialised) within the Balanced Scorecard (BSC) framework, the taxonomy
integrates both universal and domain-specific quality dimensions. By
positioning data quality as a strategic connector across the BSC's Financial,
Customer, Internal Processes, and Learning & Growth perspectives, it
demonstrates how quality metrics underpin valuation accuracy, customer trust,
operational efficiency, and innovation capacity. The framework's interconnected
"metrics layer" ensures that improvements in one dimension cascade into others,
maximising strategic impact. This holistic approach bridges the gap between
granular technical assessment and high-level decision-making, offering
practitioners, data stewards, and strategists a scalable, evidence-based
reference for aligning data quality management with sustainable value creation.

</details>


### [100] [EMR-AGENT: Automating Cohort and Feature Extraction from EMR Databases](https://arxiv.org/abs/2510.00549)
*Kwanhyung Lee,Sungsoo Hong,Joonhyung Park,Jeonghyeop Lim,Juhwan Choi,Donghwee Yoon,Eunho Yang*

Main category: cs.DB

TL;DR: EMR-AGENT用语言模型驱动的多智能体替代手工规则，自动化EMR数据的队列与特征抽取及代码映射，并在MIMIC-III、eICU、SICdb上验证了较强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于EMR的临床预测模型高度依赖人为编写、数据库特定的管道（队列定义、特征选择、代码映射），这限制了可扩展性、可复现性和跨机构泛化，因此希望自动化并标准化该流程。

Method: 构建模块化代理(agent)体系，代理通过交互式查询数据库（主要以SQL为工具）观测结果、推理数据库schema与文档，迭代生成和调整查询以完成队列选择、特征提取与编码映射；并建立针对三种EMR数据库（MIMIC-III、eICU、SICdb）的基准评估，包括已见和未见schema情形。

Result: 在三个数据库上进行评估，结果显示框架在特征提取与跨schema泛化上表现良好，证明了自动化流程在无需专家手工设计的情况下也能实现可行性。作者将开源代码并提供在线演示。

Conclusion: 该论文提出的EMR-AGENT通过语言模型驱动的多智能体框架，自动化EMR中队列划定、特征抽取和代码映射，旨在替代人工、数据库特定的规则化流程，提高可扩展性和跨机构泛化能力。

Abstract: Machine learning models for clinical prediction rely on structured data
extracted from Electronic Medical Records (EMRs), yet this process remains
dominated by hardcoded, database-specific pipelines for cohort definition,
feature selection, and code mapping. These manual efforts limit scalability,
reproducibility, and cross-institutional generalization. To address this, we
introduce EMR-AGENT (Automated Generalized Extraction and Navigation Tool), an
agent-based framework that replaces manual rule writing with dynamic, language
model-driven interaction to extract and standardize structured clinical data.
Our framework automates cohort selection, feature extraction, and code mapping
through interactive querying of databases. Our modular agents iteratively
observe query results and reason over schema and documentation, using SQL not
just for data retrieval but also as a tool for database observation and
decision making. This eliminates the need for hand-crafted, schema-specific
logic. To enable rigorous evaluation, we develop a benchmarking codebase for
three EMR databases (MIMIC-III, eICU, SICdb), including both seen and unseen
schema settings. Our results demonstrate strong performance and generalization
across these databases, highlighting the feasibility of automating a process
previously thought to require expert-driven design. The code will be released
publicly at https://github.com/AITRICS/EMR-AGENT/tree/main. For a
demonstration, please visit our anonymous demo page:
https://anonymoususer-max600.github.io/EMR_AGENT/

</details>
