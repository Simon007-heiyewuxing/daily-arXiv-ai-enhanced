<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

TL;DR: Use CHAIR metric to create preference labels (non-hallucinated vs hallucinated) and apply DPO to fine-tune MLLMs, lowering hallucination rates


<details>
  <summary>Details</summary>
Motivation: Reduce hallucinations in MLLMs by aligning model outputs to visual evidence using CHAIR as preference signal without complex synthetic data pipelines

Method: Fine-tuning MLLMs with CHAIR-based DPO

Result: Fine-tuning with CHAIR-derived preferences via DPO reduces hallucinated answers across benchmarks; code and models released

Conclusion: CHAIR-DPO is an effective, simpler alignment strategy to decrease hallucinations in MLLMs by fine-tuning with CHAIR-based reward, avoiding reliance on proprietary models or complex pipelines.

Abstract: Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [2] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

TL;DR: 将SD3的多模态潜在空间与高频伪造残差作为显式模态融合用于伪造定位，保留语义潜特征，从而在多项基准上显著提升定位精度并具备较好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有伪造定位方法依赖大量人工标注数据且难以跟上以Stable Diffusion为代表的新一代多模态图像生成与篡改技术，需探索利用强大生成模型内在感知能力来提高取证效率与精度。

Method: 理论上证明SD的多模态架构可被条件化以输出伪造定位；在实践中使用SD3，将通过特定高通滤波提取的图像伪造残差作为一个显式模态，融合到SD3的潜在空间中训练，保留SD3提取的潜在特征并结合残差信息进行像素级伪造定位。

Result: 在常用伪造定位基准数据集上，相较于最先进方法性能提升最高达12%；模型在未见的真实文档伪造与自然场景伪造任务上也表现出稳健的泛化能力。

Conclusion: 本文提出将Stable Diffusion（尤其是SD3）的多模态架构用于图像取证的伪造定位，利用高频残差作为显式模态融合进潜在空间，从而在保留语义特征的同时提升定位性能。实验表明在多种基准上相比现有方法最高提升约12%，并在未见的真实文档与自然场景伪造图像上表现良好。

Abstract: Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [3] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

TL;DR: 将MLLM嵌入微调为预测病变定量属性，可在嵌入空间实现属性对齐，SLICE-3D上的基于属性的图像检索验证了此方法有助于可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 目前AI皮肤病诊断模型在性能上已取得进展，但可解释性不足限制了临床应用。结合MLLM的自然语言推理能力与可量化的病变属性有助于提供更具临床意义、可验证的解释。

Method: 通过微调MLLM的图像编码器以预测与病变外观相关的定量属性（如病变面积），并在嵌入空间中评估属性感知性。具体评估采用基于属性的内容检索（content-based image retrieval，CBIR）在SLICE-3D数据集上验证嵌入与属性的对应关系。

Result: 证明了通过微调，MLLM嵌入空间能编码定量属性信息，基于属性的检索结果显示嵌入对这些属性具有感知性，从而支持以属性为基础的可解释诊断流程。

Conclusion: 本文展示了将多模态大语言模型（MLLM）嵌入空间与定量病变属性对齐的可行性，为提高皮肤病诊断模型的可解释性提供了一条路径。

Abstract: Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [4] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

TL;DR: 将ViT编码器与轻量卷积解码器和线性分类器结合，结合重建与自监督目标进行预训练，并在少量标签下微调，显著提高AMR的标签效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提出一种在标签稀缺情况下提高自动调制识别(AMR)性能的方法，通过结合监督、自监督和重建目标，提升特征学习的鲁棒性和判别性。

Method: 使用ViT编码器、轻量卷积解码器和线性分类器；通过数据增强、重建损失和自监督/监督混合目标进行预训练，随后在部分标签上微调。

Result: 在RML2018.01A数据集上，所提ViT框架在低标签比例下优于监督CNN和ViT基线，使用仅15-20%标签即可接近ResNet水平的精度，并在不同SNR下保持良好性能。

Conclusion: 该统一框架简单且通用，通过重建约束保留I/Q细粒度结构，使得在标签不足时仍能学到判别性特征，从而实现高效的AMR。

Abstract: Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [5] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 提出基于姿态引导的粗到细框架InfinityHuman，通过稳定姿态与视觉锚点减少漂移，并用手部专项奖励提升手势真实感，在长时高分辨率音频驱动人物生成上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于音频驱动的人物动画在生成长时长高分辨率视频时存在误差累积导致的身份漂移、色彩和场景不稳定，以及手部动作建模不足的问题；因此需要一种能保持外观一致、改善手部表现并提高唇同步的方案。

Method: 采用粗到细流程：首先生成与音频同步的中间表示；然后使用姿态序列（作为稳定、不随外观下降的条件）和初始帧作为视觉锚点，输入到姿态引导的精细化网络迭代提升分辨率与帧一致性。此外设计手部专项奖励机制，使用高质量手部动作数据训练，以增强手势语义与细节。

Result: 在EMTD和HDTF数据集上，InfinityHuman在视频质量、身份保持、手部准确性和唇同步上均实现了SOTA水平。消融实验验证了姿态引导精细化器及手部奖励机制的有效性。

Conclusion: 该论文提出InfinityHuman框架，通过先生成音频同步的粗略表示再用姿态引导的精细化器逐步生成高分辨率、长时长视频，从而缓解时间累积误差导致的身份和颜色漂移问题，并改善手部动作和唇动同步。

Abstract: Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [6] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: 本文构建了含空间音频的360°视频眼动数据集YT360-EyeTracking，提出了球面几何感知的Vision Transformer模型SalViT360及其音频条件变体SalViT360-AV，实验证明音视频融合显著提升全景视频的显著性预测性能。


<details>
  <summary>Details</summary>
Motivation: 动机是当前缺乏针对360°视频（尤其包含空间音频）的全面显著性预测数据集与专门模型；传统2D显著性方法无法直接应对球面畸变与空间音频对视线引导的影响，因此需要新的数据与模型来研究音视融合对观众注意力的作用。

Method: 方法包括：1) 构建并公开YT360-EyeTracking数据集（81个ODV，包含不同音频-视觉条件）作为基准；2) 设计SalViT360，基于视觉Transformer并引入球面几何感知的时空注意力层以处理经度/纬度失真与全景特性；3) 设计SalViT360-AV，在SalViT360基础上加入音频条件的Transformer适配器以融合空间音频线索；4) 在多套基准数据集上训练与评估，对比现有方法并进行消融与解释性分析。

Result: 结果显示：SalViT360在仅视觉输入时就超过先前方法；SalViT360-AV在加入空间音频后进一步提升性能。两者在YT360-EyeTracking及其他基准上均取得较大幅度的改进（论文报告了多个评价指标的提升）。作者还通过消融分析表明球面几何感知注意力和音频适配器均为性能提升的关键组件。

Conclusion: 本文结论是：提出的SalViT360与SalViT360-AV在360°视频的显著性预测任务上显著优于现有方法，且结合空间音频的模型（SalViT360-AV）能进一步提升预测精度，表明空间音频对360°场景的注意力建模至关重要。

Abstract: Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [7] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

TL;DR: Proposes a VLM-based pipeline to explain vision models both per-sample and across datasets, enabling discovery of model trends and failures


<details>
  <summary>Details</summary>
Motivation: Existing xAI focuses on sample-level explanations; need dataset-level, general-behavior explanations to detect biases and patterns

Method: Pipeline-based explanation using Vision-Language Models

Result: A pipeline that explains vision models at sample and dataset levels, discovers failure cases, provides insights with minimal effort

Conclusion: Integrating VLMs into xAI enables scalable, dataset-level explanations and facilitates model debugging and fairness analysis

Abstract: The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [8] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

TL;DR: ATMS-KD通过自适应温度和混合样本增强，将MobileNetV3 Large知识有效蒸馏到三种轻量级残差CNN，显著优于直接训练和11种KD方法，在Damascena玫瑰数据集上最高达97.11%准确率且延迟低。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于资源受限农业环境的轻量级CNN训练框架，以在保持低延迟的同时提高模型性能。

Method: 结合自适应温度调度策略和混合样本数据增强，在训练过程中对教师-学生的软目标进行加权蒸馏，同时使用三种不同参数规模的残差CNN学生进行评估，并与11种现有KD方法和直接训练对比。

Result: 提出ATMS-KD框架（自适应温度调度+混合样本增强），在Damascena玫瑰成熟度分类数据集上，所有学生模型验证准确率均超过96.7%，在紧凑模型上达到97.11%，推理延迟最低72.19 ms，知识保留率均超99%。

Conclusion: ATMS-KD在不同容量的学生模型上均能高效转移教师知识，兼顾准确性与低延迟，适合部署于资源受限的农业场景。

Abstract: This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [9] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

TL;DR: 提出一种将微观结构图像与专家文本标准通过定制相似度表示相结合的混合视觉-语言框架，实现对增材制造复合材料的无监督零样本合格性判定，且支持可解释的人机协同


<details>
  <summary>Details</summary>
Motivation: Speed up and make reliable qualification of heterogeneous AM materials by linking microstructure images with expert textual criteria without retraining models

Method: Semantic hybrid VLRs for microstructure qualification

Result: Developed customized similarity-based VLR combining segmentation, CLIP and FLAVA, using positive/negative references and z-score normalization for zero-shot classification

Conclusion: 方法能在无需任务特定重训下，通过归一化相似度评分区分可接受与缺陷样本，增强追溯性与可解释性，适用于可扩展域自适应的工程定量化流程。

Abstract: Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [10] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

TL;DR: MedNeXt-L-k5 achieves high PVS segmentation accuracy on homogeneous T2w data (Dice ~0.88), struggles on T1w and across sites (LOSOCV voxel Dice ~0.35–0.38), and does not outperform nnU-Net.


<details>
  <summary>Details</summary>
Motivation: Manual PVS segmentation is slow and inconsistent; existing automated models generalize poorly across datasets. Aim to create an automated, generalizable model for PVS segmentation across diverse MRI data.

Method: Transformer-inspired 3D encoder-decoder convolutional network (MedNeXt-L-k5) adapted for PVS segmentation

Result: On homogeneous T2w HCP-Aging data: voxel-level Dice 0.88±0.06 (WM). On T1w HCP-Aging: Dice 0.58±0.09 (WM). LOSOCV across heterogeneous sites: voxel Dice 0.38±0.16 (WM), 0.35±0.12 (BG); cluster Dice 0.61±0.19 (WM), 0.62±0.21 (BG). MedNeXt-L-k5 matches or exceeds prior reports on T2w but not superior to nnU-Net.

Conclusion: MedNeXt-L-k5 is effective for PVS segmentation on quality T2w datasets, but transformer-inspired global attention isn't necessary for top performance and generalization remains limited across heterogeneous clinical datasets.

Abstract: Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [11] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 利用模型输出作为更强的空间一致性先验，训练-free地把输出层的patch语义反馈到中间注意力，提升CLIP在开放词汇分割中的定位与语义一致性。


<details>
  <summary>Details</summary>
Motivation: CLIP虽有强文本-视觉对齐但定位弱，现有通过修改中间注意力增强空间一致性的做法常因后续投影等操作导致一致性未能传递到最终输出；且中间注意力与文本表征交互不足，限制了CLIP潜力。本工作用最终输出的语义作为更可靠的空间先验反馈到中间表示以解决上述问题。

Method: 1) 计算基于最终输出的patch级对应关系作为空间一致性先验；2) 将该先验反向反馈到中间注意力，使用注意力隔离以避免干扰；3) 用置信度阈值稀疏化反馈（剪枝低置信patch）；4) 采用多次适应并集成结果以增强鲁棒性；5) 作为插件并兼容Q-K、自注意力及Proxy等注意力类型与多种预训练主干。

Result: 提出了一种无需训练的反馈驱动自适应框架，将最终输出的patch级别对应关系反馈到中间注意力以增强空间一致性和语义对齐。设计了注意力隔离、基于置信度的稀疏剪枝和自适应集成等模块，作为插件兼容多种方法、主干和注意力类型，在八个基准上稳定提升性能。

Conclusion: 通过输出驱动的反馈自适应，能显著改进中间注意力与最终预测之间的语义一致性，从而在多种方法和主干上稳定提升开放词汇分割性能，无需额外训练。

Abstract: CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [12] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

TL;DR: 提出一种轻量级、模型无关的分层探针方法，揭示MLLMs的三阶段层级组织（视觉对齐→语义整合→输出准备），并发现阶段位置会随基础模型架构改变。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在视觉语言任务上表现强劲，但其内部逐层处理动态缺乏系统性研究，作者希望理解模型如何在不同层次上完成视觉感知、语义整合与任务输出准备。

Method: 设计了一个分层探针框架：在每层提取token嵌入，使用标准化锚问题训练线性分类器预测细粒度视觉类别；通过三类受控提示变体（词汇变体、语义否定变体、输出格式变体）评估探针以分析各层功能。

Result: 在LLaVA-1.5、LLaVA-Next-LLaMA-3和Qwen2-VL上发现一致的阶段结构：早期层负责视觉对齐， 中间层进行词汇整合与语义推理，末端层负责生成任务特定输出；该结构对视觉token化、指令调优数据和预训练语料较鲁棒，但随基础LLM架构变化，阶段对应层位置发生明显移动。

Conclusion: 该论文揭示了多模态大模型在层级上呈现稳固的阶段性处理结构，并指出不同基础模型会改变阶段在层级上的分配。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [13] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 通过监督的分组稀疏非负字典学习，把视觉-文本嵌入解耦为概念子空间（SLiCS），可更精确地做概念过滤检索与条件生成，且在多种嵌入上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有共嵌入含语义信息但难以在复杂场景中分离出多个概念；希望将嵌入分解为概念特定成分，以便在检索和生成等下游任务中更精确地控制和过滤概念。

Method: 监督的分组字典学习：对嵌入进行线性合成建模，使用稀疏非负系数、分组原子与交替优化（收敛保证）；利用文本共嵌入找出概念相关词；在无标签场景用零样本文本分类产生实例级多标签用于无监督训练。

Result: 在多种嵌入（CLIP、TiTok压缩自编码器、DINOv2）上，SLiCS在概念过滤图像检索和基于图像提示的条件生成任务中显示出更高精度；定量与定性实验均支持其有效性。

Conclusion: 提出通过分组字典学习把CLIP等视觉-文本共嵌入空间解耦为多个概念子空间，取得更精确的概念过滤检索与条件生成等应用。

Abstract: Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [14] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: 该工作提出MedFoundationHub——一个本地化、隐私保护的图形界面工具，用于部署和评估医学视觉-语言模型(VLMs)。工具支持医生无代码选择模型、工程师即插即用部署、与Hugging Face模型无缝集成，并通过Docker实现跨操作系统的本地化推理，只需单卡NVIDIA A6000即可运行。作者邀请病理学家评估五个SOTA模型在结肠和肾脏病例上的表现（1015次评分），发现模型存在脱靶回答、推理模糊和病理术语不一致等问题。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在临床应用中潜力巨大，但存在PHI暴露、数据泄露与网络攻击等安全隐患；需要一个便于部署、隐私保护且能让临床专家参与评估的工具，降低使用风险。

Method: 构建一个基于GUI的工具，利用Docker编排实现操作系统无关的本地推理，整合Hugging Face开源模型，支持单卡A6000 GPU。并组织董事会认证的病理学家对五个VLM在结肠与肾脏病例上进行打分，收集1015次评分用于评估模型表现。

Result: 开发并发布MedFoundationHub，成功在本地单卡A6000上部署多款开源医学VLM；专家评估（1015次评分）显示模型在病理学应用中仍存在脱靶回答、推理模糊和术语不一致等缺陷。

Conclusion: MedFoundationHub能在资源有限的本地环境中安全部署医疗VLMs，便于临床专家参与评估；当前SOTA医学VLMs在病理任务上仍有明显可靠性和术语一致性问题，需进一步改进与风险控制。

Abstract: Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [15] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

TL;DR: 提出BIM，结合双向与多尺度扫描，在保证线性复杂度下实现高效且充分的跨任务交互，提升多任务密集预测性能。


<details>
  <summary>Details</summary>
Motivation: 充分的跨任务交互对多任务密集预测至关重要，但通常会带来高计算复杂度，现有方法在交互完整性和计算效率之间存在权衡。

Method: 方法包括两部分：1) 双向交互扫描（BI-Scan）：将任务优先和位置优先的扫描模式结合成双向序列表示，在统一的线性复杂度架构中高效保留跨任务信息；2) 多尺度扫描（MS-Scan）：通过不同粒度的扫描实现多尺度场景建模，满足不同任务的粒度需求并增强细粒度跨任务特征交互。

Result: 在NYUD-V2和PASCAL-Context两个基准上，BIM优于现有最先进方法，表明在效率与交互充分性上取得了较好折中。

Conclusion: 该论文提出了BIM（双向交互Mamba），通过双向扫描和多尺度扫描在保持线性复杂度的同时实现充分的跨任务交互，从而在多任务密集预测上提高性能。

Abstract: Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [16] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

TL;DR: 提出一个零训练的音频引导视觉编辑方法，结合预训练多模态编码器和空间对齐技术，并通过噪声分支与补丁选择支持多模态多提示复杂编辑，实验显示优于仅文本方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于文本引导的图像编辑在复杂场景下描述不足的问题，特别是需要额外的非文本编辑提示（如音频）来表达细粒度或多模态的编辑意图。

Method: 使用强零样本多模态编码器将音频嵌入到扩散模型提示空间；通过对齐策略减少音频编码与扩散提示编码间差距；设计独立噪声分支（separate noise branching）与自适应补丁选择机制，以支持多文本与多音频提示的并行编辑。

Result: 提出了一个无需额外训练的音频引导视觉编辑框架，利用预训练多模态编码器的零样本能力，通过弥合音频编码空间与扩散模型提示编码空间的差距，将多样化音频整合进视觉编辑。引入独立噪声分支和自适应补丁选择来处理多文本与多音频提示的复杂编辑场景。

Conclusion: 该框架能在复杂、多模态编辑任务中有效利用音频提供的丰富信息，超越纯文本引导方法，且不需额外训练，具有良好的泛化能力。

Abstract: Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [17] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

TL;DR: 提出GPR Loss和DAMP，构成AEVLP框架，有效利用并净化伪标签，在SPML场景下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 在单正例多标签学习(SPML)中，缺失标签造成误将正例视为负例或未知，且现有伪标签策略易引入噪声，需设计对噪声更鲁棒且能利用多样伪标签的方法。

Method: 提出Generalized Pseudo-Label Robust Loss (GPR Loss)以融合多种伪标签并抑制噪声；引入Dynamic Augmented Multi-focus Pseudo-labeling (DAMP)用于生成多样且动态的伪标签；两者结合形成Adaptive and Efficient Vision-Language Pseudo-Labeling (AEVLP)框架。

Result: 在四个基准数据集上的大量实验表明，AEVLP在多标签分类任务上显著优于现有方法，达到了最先进水平。

Conclusion: 该论文提出了用于单正例多标签学习的鲁棒伪标签损失函数和动态伪标签生成策略，构成AEVLP框架，声称在四个基准数据集上实现了最先进的结果。

Abstract: Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [18] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

TL;DR: 提出时序依赖IF（tdIF）与延迟脉冲策略，解决脉冲时序异质性与残余膜电位问题，使SNN在视觉检测任务中以<=5步时延达到SOTA性能，同时保持传统IF的能耗水平。


<details>
  <summary>Details</summary>
Motivation: 现有ANN→SNN转换在分类任务上能在超低时间步取得好效果，但在视觉检测任务（目标检测、车道线检测）上表现欠佳，原因在于脉冲时序异质性导致的残余膜电位和频率式表征不足。

Method: 提出delay-spike策略用于处理不同时序脉冲引起的残余膜电位；设计temporal-dependent Integrate-and-Fire (tdIF)神经元，使得IF神经元能根据时间步序动态调整积累与发放行为，保留与传统IF相当的能耗。

Result: 在目标检测与车道线检测两项关键视觉任务上，tdIF在<=5时间步实现了比现有转换方法更高的精度，证明了其在低时延条件下的高性能与能效优势。

Conclusion: tdIF方法通过引入时序依赖的IF神经元和延迟脉冲策略，有效缓解了异质脉冲模式引起的残余膜电位问题，从而在极低时间步下提高了视觉检测任务的性能，实验证明在5个时间步内可超越现有ANN-SNN转换方法，达到SOTA水平。

Abstract: Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [19] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

TL;DR: DUP-MCRNet通过动态不确定性传播和可学习多模态协同融合改善了显著目标检测的细节与边缘表现，在基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有SOD方法在复杂场景下易丢失细节、边缘模糊及单模态信息融合不足，需通过不确定性传播与多模态协同来提升鲁棒性与细节保留。

Method: 设计了动态不确定性图卷积模块（DUGC）通过基于空间语义距离的稀疏图在层间传播不确定性，结合通道自适应交互；提出可学习模态门控的多模态协同融合策略（MCF）用于加权融合RGB、深度和边缘注意力图；并以多尺度BCE与IoU损失、跨尺度一致性约束及不确定性引导监督优化像素与区域级性能。

Result: 在多组公开数据集上进行大量实验，DUP-MCRNet在整体性能、边缘清晰度和复杂背景鲁棒性方面优于现有方法。

Conclusion: 提出的DUP-MCRNet能在复杂场景下提高显著目标检测的边缘清晰度和小结构检测精度，且在公开基准上优于多种方法。

Abstract: In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [20] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

TL;DR: MSMVD projects per-view multi-scale image features into BEV at each scale and fuses them with a feature pyramid, improving detection of pedestrians with consistent or cross-view scale differences; +4.5 MODA on GMVD.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end MVPD methods fail on pedestrians that are consistently small/large in views or have large cross-view scale differences because they do not use multi-scale image features to build BEV features.

Method: Extract multi-scale image features from each view, project each scale into BEV space generating multi-scale BEV features, then fuse these using a feature pyramid network to output detections.

Result: The paper proposes MSMVD, a method that creates multi-scale BEV features by projecting multi-scale image features from each view into BEV space scale-by-scale, then fuses them with FPN to handle scale variations across and within views. It reports a 4.5-point MODA improvement on GMVD.

Conclusion: Exploiting multi-scale image features via multi-scale BEV features significantly boosts MVPD performance; MSMVD effectively handles small/large and cross-view scale variations.

Abstract: Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [21] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

TL;DR: 提出SFMFNet：轻量的空间-频率融合网络，通过门控融合、选择性交叉注意力和残差模糊池化实现实时高效深度伪造检测。


<details>
  <summary>Details</summary>
Motivation: 应对高逼真度深度伪造增长与现有检测器计算开销大、难以实时部署的问题，设计轻量且高效的检测模型。

Method: 提出空间-频率混合感知模块，通过门控机制融合空间纹理与频域伪影；引入token-selective交叉注意力用于多层特征交互；采用残差增强模糊池化在下采样时保留语义信息。

Result: 在多个基准数据集上验证，SFMFNet在准确率与效率之间取得良好平衡，具有较强的泛化能力，适合实时场景。

Conclusion: 该论文提出SFMFNet，一种轻量级实时深度伪造检测网络，在保持较高准确率的同时显著降低计算成本，适合实时应用场景。

Abstract: With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [22] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出将双模型权重选择与自我知识蒸馏结合的医学图像分类方法：从大型预训练模型初始化两个轻量模型、应用自我知识蒸馏并微调，以在受限计算资源下保持性能。


<details>
  <summary>Details</summary>
Motivation: 现实医疗场景中计算资源受限，难以部署大型模型，需要设计在保持性能的同时更高效的轻量化方案以便实用化。

Method: 1) 从大型预训练模型选择并初始化两个轻量模型的权重；2) 对这两模型施加自我知识蒸馏（SKD），允许多种初始权重配置共享知识，无需额外大计算成本；3) 对目标分类任务进行微调与联合训练。

Result: 在公开胸部X光、肺CT、脑MRI数据集上的广泛实验显示，所提方法在准确率、鲁棒性和参数效率上均优于现有轻量化与蒸馏方法。

Conclusion: 结合双模型权重选择与自我知识蒸馏能在多种医学影像数据集（胸部X光、肺CT、脑MRI）上显著提升轻量模型的性能与鲁棒性，比传统方法更能保留关键信息且计算开销可控。

Abstract: We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [23] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: 提出通过两模块生成紧凑特征以高效预测编码点云：几何再密化模块在更密尺度提取特征再稀化，跨尺度特征传播模块利用多分辨率占据信息指导特征传播。实现高压缩比和实时时间（12-bit下达26FPS）。


<details>
  <summary>Details</summary>
Motivation: 高精度LiDAR点云存储/传输代价高；现有八叉树/体素方法因几何极度稀疏导致上下文建模效率低，影响压缩与速度。故生成紧凑特征以提升效率。

Method: 两模块：1) 几何再密化模块（Geometry Re-Densification Module）：对稀疏几何先再密化，在密尺度提取特征后再稀化供预测头使用，减少昂贵的稀疏细节计算。2) 跨尺度特征传播模块（Cross-scale Feature Propagation Module）：利用多分辨率占据（occupancy）线索进行层次特征传播，减少冗余提取并为再密化模块提供丰富特征。二者结合生成紧凑特征用于预测编码。

Result: 在KITTI数据集上实现先进的压缩比，并在12-bit量化下编码和解码均达26FPS，支持实时性；代码已开源。

Conclusion: 提出的两模块协同生成紧凑特征，提升上下文建模效率并加速编码，实验证明在KITTI上达到最优压缩率和实时（12-bit下编码/解码26FPS）。

Abstract: LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [24] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

TL;DR: 利用含多视角与语义信息的视频作为监督来缓解3D数据稀缺，发布4M级多视角注释视频集并训练支持图像与密文本输入的生成模型，能生成空间一致且语义合理的3D内容，并可扩展到场景级。


<details>
  <summary>Details</summary>
Motivation: 3D领域缺乏大规模数据限制了生成模型的泛化，视频作为丰富的多视角和语义来源可作为替代监督以补足原生3D数据不足。

Method: 构建大规模多视角注释视频数据集Droplet3D-4M；训练一个可接受图像与密文本输入的生成模型Droplet3D，通过视频提供的空间一致性和语义监督来学习3D表示，评价包括空间一致性与语义对齐的实验。

Result: Introduces Droplet3D-4M video dataset and Droplet3D generative model leveraging video commonsense priors for improved 3D asset generation.

Conclusion: 视频中的多视角一致性和语义先验显著提升3D生成的空间一致性与语义逼真度，方法比传统3D方法更易扩展到场景级应用，并已开源数据与模型。

Abstract: Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [25] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

TL;DR: G^2Editor 使用3D高斯先验注入扩散生成，结合场景布局和分层特征，实现驾驶视频中高保真、姿态可控的目标编辑（重定位/插入/删除），在Waymo数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实世界角落案例难以获取且昂贵危险，通过编辑已采集传感器数据生成多样场景是一种可行替代，但现有3D高斯点溅或图像生成方法存在视觉保真度不足或姿态控制不精确的问题。

Method: 以目标的3D高斯表示作为密集先验，注入扩散去噪过程以保证姿态与空间一致性；利用场景级3D包围盒重建非目标对象的遮挡区域；在生成中加入分层细粒度特征作为外部条件以引导外观细节。

Result: 在Waymo Open Dataset上的实验表明，G^2Editor在姿态可控性和视觉质量上均优于现有方法，并能提升下游数据驱动任务的表现。

Conclusion: G^2Editor能在驾驶视频中实现高保真且姿态可控的目标编辑，统一支持目标重定位、插入与删除。

Abstract: Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [26] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

TL;DR: 通过在合成数据中嵌入CCD解剖先验，本文在无病变标注情形下显著提升了胎儿脑和胼胝体的分割与形态学量化，对CCD检测与亚型区分具有实际临床价值。


<details>
  <summary>Details</summary>
Motivation: CCD等罕见脑畸形标注样本稀少，导致基于深度学习的分割模型难以泛化至病理病例。通过合成具有病理特征的训练数据，可以在不依赖真实病变标注的前提下提升模型在罕见病变上的性能。

Method: 构建基于先验解剖知识的合成数据管道，对健康脑部影像施加多种模拟胼胝体缺失/异常的变换（例如尺寸缩减、形态扭曲或部分缺损），并将这些合成样本用于训练分割网络；评价包括对正常、CCD及其他病变组的分割性能及基于分割的形态学生物标志物（LCC长度与体积）分析。

Result: 在包含248例健康、26例CCD和47例其他病变的队列上验证，方法在CCD病例上显著提升分割性能，同时对健康及其他病变保持不退化；LCC估计误差在健康组从1.89 mm降至0.80 mm，在CCD组从10.9 mm降至0.7 mm；分割结果在拓扑一致性上也有所改善，有利于基于形状的分析与CCD亚型区分。

Conclusion: 本论文提出了一种病理感知的域随机化方法，通过将先验的CCD（胼胝体发育不全）解剖改变植入合成数据生成流程，从健康胎儿影像中合成多样化的病变样本，从而在缺乏病理标注的情况下提高分割模型对罕见畸形的泛化能力。

Abstract: Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [27] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出首个统一框架同时处理手语、唇读和音频，用于生成口语文本；架构模态无关，利用唇动作为手语非手势线索，性能达到或优于各专用任务SOTA。


<details>
  <summary>Details</summary>
Motivation: 音频ASR无法覆盖聋哑群体，视觉替代（手语、唇读）虽有进展但多被独立研究；整合这些视觉模态与音频以构建更普适的无声或多模态通信系统具有重要社会价值。

Method: 设计一个模态无关的统一架构，能接受手语视频（手部与身体）、唇动视频和音频的任意组合输入；在编码阶段分别提取各模态特征并进行对齐与融合，训练目标是直接生成书面语文本，可能采用共享解码器与跨模态注意力机制，并针对唇动提供专门分支。

Result: 在四类任务（SLT、VSR、ASR、AVSR）上，模型表现至少与各领域专家模型持平并在若干指标上超越；实验还显示当将唇动作为独立模态引入时，SLT性能有明显提升，并揭示模态间的互补性与鲁棒性提升。

Conclusion: 该统一框架在SLT、VSR、ASR和AVSR任务上达到了与或优于专用SOTA的性能；将唇动显式建模为独立模态可显著提升SLT效果，证明多模态融合的协同价值。

Abstract: Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [28] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

TL;DR: 提出Video-MTR：基于强化学习的多轮推理与门控双层奖励，实现端到端的迭代关键片段选择和问题理解，提升长视频理解的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 长视频理解需处理长时依赖与多事件，传统单轮静态推理或依赖外部VLM的做法存在复杂性高、无法端到端训练及性能受限等问题，需一种可迭代聚焦关键信息并能联合优化选择与理解的方案。

Method: 提出多轮（multi-turn）推理框架Video-MTR，采用逐步选择视频片段并根据先前理解更新片段选择；引入门控双层奖励（轨迹级基于答案正确性、回合级强调帧-查询相关性）以强化学习优化片段选择与问题理解，实现端到端训练，摒弃外部VLM依赖。

Result: 在VideoMME、MLVU和EgoSchema等基准上，Video-MTR在准确率和效率上均优于现有方法，推动长视频理解性能提升。

Conclusion: Video-MTR通过多轮强化推理实现对长视频的迭代关键片段选择和问题理解，从而在准确率和效率上优于现有方法。

Abstract: Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [29] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

TL;DR: 提出DUO：通过联合最小化语义与几何双重不确定性，并利用凸focal loss的无监督版本与语义感知法线场约束，实现对域偏移下的单目3D检测的有效测试时自适应。


<details>
  <summary>Details</summary>
Motivation: 动机是针对单目3D检测在真实世界域偏移（环境或传感器变化）下性能显著下降的问题，现有TTA方法仅关注单一不确定性或未区分语义与几何不确定性，导致适应效果有限。

Method: 方法包括两个核心模块：1）推导并利用具有凸结构的focal loss及其无监督版本，用于对高不确定性目标进行标签无关的权重调整和平衡学习；2）设计语义感知法线场约束，在具有明确语义线索的区域保持几何一致性，稳定3D表示。两者形成互补循环，增强空间感知和语义分类。

Result: 在多数据集和多种域偏移类型上的大量实验表明，DUO优于现有方法，在鲁棒性和检测性能上都有显著提升。

Conclusion: 该论文提出了一种名为DUO的测试时自适应（TTA）框架，通过同时优化语义不确定性和几何不确定性，提高单目三维目标检测在域偏移下的鲁棒性。

Abstract: Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [30] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

TL;DR: CaddieSet通过视频提取关节特征并结合15个专家定义指标，实现了对高尔夫球轨迹的可解释预测，为挥杆分析提供量化、可解释的工具。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽能提升击球精度，但未能量化建立挥杆姿态与球轨迹之间的关系，限制了对挥杆改进的可解释性与实用反馈能力。因此需构建带有姿态-轨迹关联和易解释指标的数据集。

Method: 使用基于计算机视觉的方法从单次挥杆视频中提取关节信息并按八个阶段分割，基于高尔夫专家知识定义15个关键挥杆指标，构建数据集并在多种基准模型上进行球轨迹预测实验，重点评估可解释模型与关节特征的关系。

Result: 构建了包含关节信息和多种球信息的CaddieSet，并证明在多个基准上能有效预测球轨迹；可解释模型的反馈与高尔夫领域知识一致，验证了定义的15个关键指标的有效性。

Conclusion: 该论文构建了CaddieSet数据集，通过从单次挥杆视频提取关节信息并划分为八个挥杆阶段，结合专家知识定义15个关键指标，实现了基于挥杆姿态对球轨迹的可解释预测。

Abstract: Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>


### [31] [IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection](https://arxiv.org/abs/2508.20492)
*Xuanming Cao,Chengyu Tao,Yifeng Cheng,Juan Du*

Main category: cs.CV

TL;DR: 提出IAENet，通过重要性感知融合2D预训练与3D专家，动态重权异常分数并用特定损失引导学习，在MVTec 3D-AD上达成SOTA且误报率显著降低。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云异常检测受限于缺乏与2D同等强大的预训练基础骨干，导致未能充分利用丰富的几何信息。通过引入2D预训练专家的知识并与3D专家协同，可提升检测性能。

Method: 提出重要性感知集成网络（IAENet）；由2D预训练专家与3D专家并行产生异常分数，设计IAF模块动态计算各专家重要性并重加权融合；引入多项定制损失（用于引导IAF既融合专家知识又保留各自优势）；在MVTec 3D-AD数据集上进行大规模对比实验。

Result: 在MVTec 3D-AD上实现新SOTA，显著降低误报率；定量指标与消融实验表明IAF与专门损失函数对性能提升有关键作用。

Conclusion: 该文提出将2D预训练模型与3D模型融合，以弥补3D缺乏强大预训练骨干的问题，通过重要性感知的动态加权融合模块（IAF）评估并重调各源异常分数，辅以专门设计的损失函数，引导IAF学习集成与保持源模型各自优势。实验在MVTec 3D-AD上取得了SOTA并显著降低误报率，适用于工业部署。

Abstract: Surface anomaly detection is pivotal for ensuring product quality in
industrial manufacturing. While 2D image-based methods have achieved remarkable
success, 3D point cloud-based detection remains underexplored despite its
richer geometric cues. We argue that the key bottleneck is the absence of
powerful pretrained foundation backbones in 3D comparable to those in 2D. To
bridge this gap, we propose Importance-Aware Ensemble Network (IAENet), an
ensemble framework that synergizes 2D pretrained expert with 3D expert models.
However, naively fusing predictions from disparate sources is non-trivial:
existing strategies can be affected by a poorly performing modality and thus
degrade overall accuracy. To address this challenge, We introduce an novel
Importance-Aware Fusion (IAF) module that dynamically assesses the contribution
of each source and reweights their anomaly scores. Furthermore, we devise
critical loss functions that explicitly guide the optimization of IAF, enabling
it to combine the collective knowledge of the source experts but also preserve
their unique strengths, thereby enhancing the overall performance of anomaly
detection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENet
achieves a new state-of-the-art with a markedly lower false positive rate,
underscoring its practical value for industrial deployment.

</details>


### [32] [Describe, Don't Dictate: Semantic Image Editing with Natural Language Intent](https://arxiv.org/abs/2508.20505)
*En Ci,Shanyan Guan,Yanhao Ge,Yilin Zhang,Wei Li,Zhenyu Zhang,Jian Yang,Ying Tai*

Main category: cs.CV

TL;DR: DescriptiveEdit把图像编辑当作带参考图像的文本到图像生成，新增Cross-Attentive UNet以在生成时注入参考特征，无需反演或改模型，可结合ControlNet/IP-Adapter，提升编辑效果


<details>
  <summary>Details</summary>
Motivation: 解决现有语义图像编辑中反演误差和指令式模型数据质量/规模受限的问题

Method: 以参考图像和文本提示为输入，设计Cross-Attentive UNet增加注意力桥以融合参考图像特征到提示驱动的图像生成；框架保持T2I性质，兼容各种扩展模块

Result: 提出DescriptiveEdit——将指令式图像编辑重新构造为参考图像驱动的文本到图像生成，通过Cross-Attentive UNet在生成过程中注入参考图像特征；与T2I模型兼容、支持ControlNet等扩展，在Emu Edit基准上提升编辑准确性和一致性

Conclusion: DescriptiveEdit在保留预训练T2I模型生成能力的同时，避免反演误差并突破指令数据局限，实验表明在基准上提升了编辑精度和一致性

Abstract: Despite the progress in text-to-image generation, semantic image editing
remains a challenge. Inversion-based algorithms unavoidably introduce
reconstruction errors, while instruction-based models mainly suffer from
limited dataset quality and scale. To address these problems, we propose a
descriptive-prompt-based editing framework, named DescriptiveEdit. The core
idea is to re-frame `instruction-based image editing' as `reference-image-based
text-to-image generation', which preserves the generative power of well-trained
Text-to-Image models without architectural modifications or inversion.
Specifically, taking the reference image and a prompt as input, we introduce a
Cross-Attentive UNet, which newly adds attention bridges to inject reference
image features into the prompt-to-edit-image generation process. Owing to its
text-to-image nature, DescriptiveEdit overcomes limitations in instruction
dataset quality, integrates seamlessly with ControlNet, IP-Adapter, and other
extensions, and is more scalable. Experiments on the Emu Edit benchmark show it
improves editing accuracy and consistency.

</details>


### [33] [DCFS: Continual Test-Time Adaptation via Dual Consistency of Feature and Sample](https://arxiv.org/abs/2508.20516)
*Wenting Yin,Han Sun,Xinru Meng,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: 提出DCFS方法通过双路径子特征一致性与置信度加权自监督来解决无源持续测试时域自适应中伪标签噪声与特征混淆问题，在CIFAR-C与ImageNet-C上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 在无源持续测试时域自适应中无法访问源数据，模型仅依赖目标域特征，易导致特征混淆与伪标签噪声，进而产生学习偏差与错误累积，需设计方法分离语义与域信息并抑制低置信伪标签的影响。

Method: 1) 使用双分类器结构将目标特征分解为语义相关子特征与域相关子特征；2) 强化子特征与整体特征之间的一致性（dual-path feature consistency）；3) 为每个样本计算自适应置信度分数并设阈值，按置信度加权自监督损失以降低伪标签噪声；4) 在无源持续测试时域自适应场景中在线更新模型参数。

Result: 在CIFAR10-C、CIFAR100-C与ImageNet-C等多个合成扰动数据集上的实验表明，DCFS在持续测试时域自适应任务中能稳定提升性能，减少伪标签噪声影响与误差累积。

Conclusion: DCFS通过将目标特征拆解为语义相关和域相关子特征、维持子特征与整体特征一致性，并对样本置信度进行自适应加权，从而减轻伪标签噪声和误差累积，实验显示在多个合成扰动数据集上有稳定性能提升。

Abstract: Continual test-time adaptation aims to continuously adapt a pre-trained model
to a stream of target domain data without accessing source data. Without access
to source domain data, the model focuses solely on the feature characteristics
of the target data. Relying exclusively on these features can lead to confusion
and introduce learning biases. Currently, many existing methods generate
pseudo-labels via model predictions. However, the quality of pseudo-labels
cannot be guaranteed and the problem of error accumulation must be solved. To
address these challenges, we propose DCFS, a novel CTTA framework that
introduces dual-path feature consistency and confidence-aware sample learning.
This framework disentangles the whole feature representation of the target data
into semantic-related feature and domain-related feature using dual classifiers
to learn distinct feature representations. By maintaining consistency between
the sub-features and the whole feature, the model can comprehensively capture
data features from multiple perspectives. Additionally, to ensure that the
whole feature information of the target domain samples is not overlooked, we
set a adaptive threshold and calculate a confidence score for each sample to
carry out loss weighted self-supervised learning, effectively reducing the
noise of pseudo-labels and alleviating the problem of error accumulation. The
efficacy of our proposed method is validated through extensive experimentation
across various datasets, including CIFAR10-C, CIFAR100-C, and ImageNet-C,
demonstrating consistent performance in continual test-time adaptation
scenarios.

</details>


### [34] [Adam SLAM - the last mile of camera calibration with 3DGS](https://arxiv.org/abs/2508.20526)
*Matthieu Gendrin,Stéphane Pateux,Xiaoran Jiang,Théo Ladune,Luce Morin*

Main category: cs.CV

TL;DR: 用3DGS反向传播微调相机标定，能显著提升新视图合成（数据集平均+0.4 dB PSNR），但微调耗时较长，适用于对训练时间不敏感的参考场景。


<details>
  <summary>Details</summary>
Motivation: 相机标定的像素级误差会显著影响新视图合成和重建质量，但实景缺乏真实标定，因此通过优化影响最终合成质量的指标来改进标定是必要的。

Method: 将预估相机参数引入可微的3DGS渲染流程，计算渲染图像与真实图像的颜色损失，再对相机内外参进行梯度下降优化（反向传播）。

Result: 使用3DGS模型通过对新视图颜色损失关于相机参数进行反向传播来微调相机标定，从而提高新视图合成质量。

Conclusion: 通过将相机参数作为可优化变量并利用3DGS对颜色重建误差反向传播进行微调，可以在不改变网络结构的情况下提升新视图合成质量，实测在基准数据集上平均提高约0.4 dB PSNR。

Abstract: The quality of the camera calibration is of major importance for evaluating
progresses in novel view synthesis, as a 1-pixel error on the calibration has a
significant impact on the reconstruction quality. While there is no ground
truth for real scenes, the quality of the calibration is assessed by the
quality of the novel view synthesis. This paper proposes to use a 3DGS model to
fine tune calibration by backpropagation of novel view color loss with respect
to the cameras parameters. The new calibration alone brings an average
improvement of 0.4 dB PSNR on the dataset used as reference by 3DGS. The fine
tuning may be long and its suitability depends on the criticity of training
time, but for calibration of reference scenes, such as Mip-NeRF 360, the stake
of novel view quality is the most important.

</details>


### [35] [Learning What is Worth Learning: Active and Sequential Domain Adaptation for Multi-modal Gross Tumor Volume Segmentation](https://arxiv.org/abs/2508.20528)
*Jingyun Yang,Guoqing Zhang,Jingge Wang,Yang Li*

Main category: cs.CV

TL;DR: 提出一种动态顺序域自适应主动学习框架，通过结合信息量与代表性选择多模态样本，减少负迁移并在有限标注下提升肿瘤分割性能，优于现有ADA方法


<details>
  <summary>Details</summary>
Motivation: Reduce annotation costs and avoid negative transfer in active domain adaptation for multi-modal medical image segmentation where source data access limited

Method: Active and sequential domain adaptation for multi-modal medical segmentation

Result: A query strategy prioritizing samples by informativeness and representativeness; dynamic sequential labeling and training; outperforms prior ADA methods on gross tumor volume segmentation tasks

Conclusion: 该方法能在多模态医疗图像ADA场景下，通过动态采样策略高效利用标注预算，显著提升分割效果并缓解负迁移问题

Abstract: Accurate gross tumor volume segmentation on multi-modal medical data is
critical for radiotherapy planning in nasopharyngeal carcinoma and
glioblastoma. Recent advances in deep neural networks have brought promising
results in medical image segmentation, leading to an increasing demand for
labeled data. Since labeling medical images is time-consuming and
labor-intensive, active learning has emerged as a solution to reduce annotation
costs by selecting the most informative samples to label and adapting
high-performance models with as few labeled samples as possible. Previous
active domain adaptation (ADA) methods seek to minimize sample redundancy by
selecting samples that are farthest from the source domain. However, such
one-off selection can easily cause negative transfer, and access to source
medical data is often limited. Moreover, the query strategy for multi-modal
medical data remains unexplored. In this work, we propose an active and
sequential domain adaptation framework for dynamic multi-modal sample selection
in ADA. We derive a query strategy to prioritize labeling and training on the
most valuable samples based on their informativeness and representativeness.
Empirical validation on diverse gross tumor volume segmentation tasks
demonstrates that our method achieves favorable segmentation performance,
significantly outperforming state-of-the-art ADA methods. Code is available at
the git repository: \href{https://github.com/Hiyoochan/mmActS}{mmActS}.

</details>


### [36] [Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection](https://arxiv.org/abs/2508.20530)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 通过在数据级早期融合RGB图像和LiDAR点云，并利用视觉基础模型和双向融合+噪声过滤+动态自我演化，显著提升无监督3D目标检测伪标签质量，nuScenes上mAP提升至28.4%。


<details>
  <summary>Details</summary>
Motivation: 现有无监督3D检测方法在生成伪标签时仅在标签层融合RGB与LiDAR，忽视两者在数据层的互补性，导致伪标签质量受限；通过早期数据级融合可更好利用图像的语义与深度信息提升点云稠密性与类别信息，从而改进伪标签质量和检测效果。

Method: 1) 用视觉基础模型对图像做实例分割与深度估计；2) 双向融合：将2D实例标签传给真实点云（点获类别），并将像素按深度反投影到3D以增加点密度；3) 局部半径过滤与全局统计过滤抑制深度/分割噪声；4) 基于密集表示的数据级动态自我演化，迭代优化伪盒子。

Result: This paper proposes a data-level fusion framework integrating RGB images and LiDAR for unsupervised 3D object detection, using vision foundation models (instance segmentation, depth estimation), bi-directional fusion (label transfer and pixel-to-point projection), local/global filtering to reduce noise, and a dynamic self-evolution strategy to iteratively refine pseudo-boxes, achieving strong improvements on nuScenes (28.4% mAP).

Conclusion: 数据级融合能更充分利用RGB与LiDAR的互补性，通过增强点云密度与类别标注、噪声过滤和迭代自我优化，可在无监督设置下显著提升3D检测性能，优于标签级简单融合方法。

Abstract: Existing LiDAR-based 3D object detectors typically rely on manually annotated
labels for training to achieve good performance. However, obtaining
high-quality 3D labels is time-consuming and labor-intensive. To address this
issue, recent works explore unsupervised 3D object detection by introducing RGB
images as an auxiliary modal to assist pseudo-box generation. However, these
methods simply integrate pseudo-boxes generated by LiDAR point clouds and RGB
images. Yet, such a label-level fusion strategy brings limited improvements to
the quality of pseudo-boxes, as it overlooks the complementary nature in terms
of LiDAR and RGB image data. To overcome the above limitations, we propose a
novel data-level fusion framework that integrates RGB images and LiDAR data at
an early stage. Specifically, we utilize vision foundation models for instance
segmentation and depth estimation on images and introduce a bi-directional
fusion method, where real points acquire category labels from the 2D space,
while 2D pixels are projected onto 3D to enhance real point density. To
mitigate noise from depth and segmentation estimations, we propose a local and
global filtering method, which applies local radius filtering to suppress depth
estimation errors and global statistical filtering to remove
segmentation-induced outliers. Furthermore, we propose a data-level fusion
based dynamic self-evolution strategy, which iteratively refines pseudo-boxes
under a dense representation, significantly improving localization accuracy.
Extensive experiments on the nuScenes dataset demonstrate that the detector
trained by our method significantly outperforms that trained by previous
state-of-the-art methods with 28.4$\%$ mAP on the nuScenes validation
benchmark.

</details>


### [37] [Digital Scale: Open-Source On-Device BMI Estimation from Smartphone Camera Images Trained on a Large-Scale Real-World Dataset](https://arxiv.org/abs/2508.20534)
*Frederik Rajiv Manichand,Robin Deuber,Robert Jakob,Steve Swerling,Jamie Rosen,Elgar Fleisch,Patrick Langer*

Main category: cs.CV

TL;DR: 作者构建了最大规模的手机全身图像BMI数据集，提出自动化过滤方法，训练出具有较好泛化性的深度学习模型并完成移动端部署与开源。


<details>
  <summary>Details</summary>
Motivation: 在无法使用传统体重身高测量的场景（如远程医疗或紧急情况）下，通过图像快速估计BMI具有实际需求，现有数据集规模有限且方法泛化性不足。

Method: 收集84,963张智能手机全身图像，使用人体检测和基于姿态的聚类自动过滤掉低质量图像，最终保留71,322张；基于深度学习模型进行回归训练，进行跨数据集评估，并在移动端通过CLAID框架部署。

Result: 在自主WayBED测试集上，全身图像MAPE为7.9%，在未见过的VisualBodyToBMI上为13%，经微调后在VisualBodyToBMI上达8.56%，并将完整流程部署到Android设备，代码开源。

Conclusion: 该论文通过构建大规模数据集和自动化过滤流程，显著提升了基于图像的BMI估计精度，并实现了移动端部署与开源。

Abstract: Estimating Body Mass Index (BMI) from camera images with machine learning
models enables rapid weight assessment when traditional methods are unavailable
or impractical, such as in telehealth or emergency scenarios. Existing computer
vision approaches have been limited to datasets of up to 14,500 images. In this
study, we present a deep learning-based BMI estimation method trained on our
WayBED dataset, a large proprietary collection of 84,963 smartphone images from
25,353 individuals. We introduce an automatic filtering method that uses
posture clustering and person detection to curate the dataset by removing
low-quality images, such as those with atypical postures or incomplete views.
This process retained 71,322 high-quality images suitable for training. We
achieve a Mean Absolute Percentage Error (MAPE) of 7.9% on our hold-out test
set (WayBED data) using full-body images, the lowest value in the published
literature to the best of our knowledge. Further, we achieve a MAPE of 13% on
the completely unseen~(during training) VisualBodyToBMI dataset, comparable
with state-of-the-art approaches trained on it, demonstrating robust
generalization. Lastly, we fine-tune our model on VisualBodyToBMI and achieve a
MAPE of 8.56%, the lowest reported value on this dataset so far. We deploy the
full pipeline, including image filtering and BMI estimation, on Android devices
using the CLAID framework. We release our complete code for model training,
filtering, and the CLAID package for mobile deployment as open-source
contributions.

</details>


### [38] [Domain Adaptation Techniques for Natural and Medical Image Classification](https://arxiv.org/abs/2508.20537)
*Ahmad Chaddad,Yihang Wu,Reem Kateb,Christian Desrosiers*

Main category: cs.CV

TL;DR: 对多数据集、多场景下的557次仿真表明，DSAN在医学图像域适应任务上具有显著优势，既提升准确率又带来更好可解释性。


<details>
  <summary>Details</summary>
Motivation: 评估域适应（DA）技术在自然图像与医学图像分类任务中的有效性，弥补以往研究偏向自然图像及常用数据集带来的偏差，探索在多种现实场景（分布外、动态数据流、有限样本）下的性能表现。

Method: 设计跨5个自然与8个医学数据集的实验，构造多种现实情形（分布外、动态数据流、样本不足），使用7种常见DA算法与基线模型（如ResNet50）比较，通过557次仿真实验评估分类精度与可解释性指标，重点分析DSAN性能。

Result: 进行了557次仿真实验，比较7种常用DA方法在5个自然数据集和8个医学数据集上的效果；发现Deep Subdomain Adaptation Network (DSAN)表现突出，在COVID-19数据集上（使用ResNet50）达到91.2%分类精度，并在动态数据流场景相较基线提升6.7%；DSAN在COVID-19和皮肤癌数据集上也表现出较好的可解释性。

Conclusion: 域适应技术对医学图像分类具有实际价值，尤其是DSAN在多种具有挑战性的场景中表现优异；研究为将DA方法应用于医疗影像提供了实证参考。

Abstract: Domain adaptation (DA) techniques have the potential in machine learning to
alleviate distribution differences between training and test sets by leveraging
information from source domains. In image classification, most advances in DA
have been made using natural images rather than medical data, which are harder
to work with. Moreover, even for natural images, the use of mainstream datasets
can lead to performance bias. {With the aim of better understanding the
benefits of DA for both natural and medical images, this study performs 557
simulation studies using seven widely-used DA techniques for image
classification in five natural and eight medical datasets that cover various
scenarios, such as out-of-distribution, dynamic data streams, and limited
training samples.} Our experiments yield detailed results and insightful
observations highlighting the performance and medical applicability of these
techniques. Notably, our results have shown the outstanding performance of the
Deep Subdomain Adaptation Network (DSAN) algorithm. This algorithm achieved
feasible classification accuracy (91.2\%) in the COVID-19 dataset using
Resnet50 and showed an important accuracy improvement in the dynamic data
stream DA scenario (+6.7\%) compared to the baseline. Our results also
demonstrate that DSAN exhibits remarkable level of explainability when
evaluated on COVID-19 and skin cancer datasets. These results contribute to the
understanding of DA techniques and offer valuable insight into the effective
adaptation of models to medical data.

</details>


### [39] [Contrastive Learning through Auxiliary Branch for Video Object Detection](https://arxiv.org/abs/2508.20551)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 在训练期间加入对比学习的辅助分支并动态调整损失权重，以增强骨干特征，从而在不增加推理成本的情况下提升视频目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 视频帧常受运动模糊、遮挡和形变等退化影响，传统通过特征聚合或复杂后处理提高性能但增加推理成本，作者希望在不增加推理负担情况下提升鲁棒性。

Method: 在检测器骨干后添加一个辅助分支，利用对比损失训练该分支以学习区分性特征；同时采用动态权重策略，训练初期重视对比损失，后期逐渐增加检测损失权重；推理时去掉辅助分支以保持计算开销不变。

Result: Paper proposes CLAB: Contrastive Learning through Auxiliary Branch to improve video object detection robustness without extra inference cost. Uses a contrastive auxiliary branch during training to enhance backbone features and a dynamic loss weighting shifting focus from auxiliary contrastive learning to detection loss as training progresses. Achieves state-of-the-art results for CNN-based models on ImageNet VID: 84.0% mAP (ResNet-101) and 85.2% mAP (ResNeXt-101) without post-processing.

Conclusion: CLAB在不增加推理计算的情况下，通过对比辅助分支和动态损失权重稳定提升了视频目标检测的鲁棒性并取得了SOTA结果。

Abstract: Video object detection is a challenging task because videos often suffer from
image deterioration such as motion blur, occlusion, and deformable shapes,
making it significantly more difficult than detecting objects in still images.
Prior approaches have improved video object detection performance by employing
feature aggregation and complex post-processing techniques, though at the cost
of increased computational demands. To improve robustness to image degradation
without additional computational load during inference, we introduce a
straightforward yet effective Contrastive Learning through Auxiliary Branch
(CLAB) method. First, we implement a constrastive auxiliary branch using a
contrastive loss to enhance the feature representation capability of the video
object detector's backbone. Next, we propose a dynamic loss weighting strategy
that emphasizes auxiliary feature learning early in training while gradually
prioritizing the detection task as training converges. We validate our approach
through comprehensive experiments and ablation studies, demonstrating
consistent performance gains. Without bells and whistles, CLAB reaches a
performance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,
respectively, on the ImageNet VID dataset, thus achieving state-of-the-art
performance for CNN-based models without requiring additional post-processing
methods.

</details>


### [40] [Towards Mechanistic Defenses Against Typographic Attacks in CLIP](https://arxiv.org/abs/2508.20570)
*Lorenz Hufe,Constantin Venhoff,Maximilian Dreyer,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.CV

TL;DR: 通过因果定位并消融负责排版信息的注意力头，提出了一种无需微调即可显著增强CLIP对图像中注入文本攻击的鲁棒性的防御方法，并公开了一组鲁棒的替代模型。


<details>
  <summary>Details</summary>
Motivation: 排版攻击通过在图像中注入文本引发目标误分类、恶意内容生成和VLM越狱，现有模型易受此类文字操控，亟需高效且无需大量重训练的防御手段。

Method: 对CLIP视觉编码器内部进行因果分析，识别模型后半层中特化的注意力头构成的“排版电路”，并通过选择性消融这些注意力头来阻断排版信息传递，形成无需训练/微调的防御方法；同时发布一组“失读症（dyslexic）”CLIP替代模型。

Result: 在ImageNet-100的排版变体上，方法提升高达19.6%的性能，而对标准ImageNet-100的准确率仅下降不到1%；与依赖微调的最先进防御方法相比，本方法在不训练的前提下仍具竞争力，并发布更鲁棒的dyslexic CLIP模型家族。

Conclusion: 通过定位并消融负责提取和传递排版信息的注意力头，可在不微调的情况下显著提高CLIP在含文本扰动图像上的鲁棒性，从而防止排版攻击对多模态系统的威胁。

Abstract: Typographic attacks exploit multi-modal systems by injecting text into
images, leading to targeted misclassifications, malicious content generation
and even Vision-Language Model jailbreaks. In this work, we analyze how CLIP
vision encoders behave under typographic attacks, locating specialized
attention heads in the latter half of the model's layers that causally extract
and transmit typographic information to the cls token. Building on these
insights, we introduce a method to defend CLIP models against typographic
attacks by selectively ablating a typographic circuit, consisting of attention
heads. Without requiring finetuning, our method improves performance by up to
19.6% on a typographic variant of ImageNet-100, while reducing standard
ImageNet-100 accuracy by less than 1%. Notably, our training-free approach
remains competitive with current state-of-the-art typographic defenses that
rely on finetuning. To this end, we release a family of dyslexic CLIP models
which are significantly more robust against typographic attacks. These models
serve as suitable drop-in replacements for a broad range of safety-critical
applications, where the risks of text-based manipulation outweigh the utility
of text recognition.

</details>


### [41] [GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition](https://arxiv.org/abs/2508.20579)
*Debasis Maji,Debaditya Barman*

Main category: cs.CV

TL;DR: 提出了GLaRE，一种基于图的面部关键点区域嵌入网络，通过3D人脸对齐提取关键点并用分层粗化构建商图以减少复杂性并保留空间结构，在AffectNet和FERG上分别达到了64.89%和94.24%的准确率，优于若干基线；消融实验显示区域级嵌入带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统FER受遮挡、表情多样性和可解释性差影响。利用GNN对关键点之间的关系建模可以获得结构化、可解释的表示，从而提高鲁棒性和性能。

Method: 首先用3D人脸对齐提取关键点，然后构建初始关键点图并通过层级图粗化生成商图以减少节点数并保留空间关系，接着在商图上学习区域级嵌入并通过GNN进行分类，最后在AffectNet和FERG上评估并做消融分析。

Result: 在AffectNet上取得64.89%准确率，在FERG上取得94.24%准确率，且消融实验证明区域级嵌入对性能有贡献。

Conclusion: GLaRE通过将面部关键点组织为商图并学习区域级嵌入，有效提升了表情识别性能和可解释性，在公开数据集上优于多种基线方法，表明区域化图表示对FER有显著帮助。

Abstract: Facial expression recognition (FER) is a crucial task in computer vision with
wide range of applications including human computer interaction, surveillance,
and assistive technologies. However, challenges such as occlusion, expression
variability, and lack of interpretability hinder the performance of traditional
FER systems. Graph Neural Networks (GNNs) offer a powerful alternative by
modeling relational dependencies between facial landmarks, enabling structured
and interpretable learning. In this paper, we propose GLaRE, a novel
Graph-based Landmark Region Embedding network for emotion recognition. Facial
landmarks are extracted using 3D facial alignment, and a quotient graph is
constructed via hierarchical coarsening to preserve spatial structure while
reducing complexity. Our method achieves 64.89 percentage accuracy on AffectNet
and 94.24 percentage on FERG, outperforming several existing baselines.
Additionally, ablation studies have demonstrated that region-level embeddings
from quotient graphs have contributed to improved prediction performance.

</details>


### [42] [FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models](https://arxiv.org/abs/2508.20586)
*Zheng Chong,Yanwei Lei,Shiyue Zhang,Zhuandi He,Zhen Wang,Xujie Zhang,Xiao Dong,Yiling Wu,Dongmei Jiang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 提出FastFit，一种基于可缓存扩散架构的高速多参考虚拟试衣框架，通过半注意力机制和类别嵌入替代时间步嵌入，实现参考特征与去噪过程解耦，节省大量计算并提升约3.5倍速度，同时构建了大规模数据集DressCode-MR，实验证明在保真度与效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟试衣方法在支持多参考物品组合和推理效率上存在两大瓶颈：一是难以处理多件服饰与配饰的组合；二是在扩散去噪过程中需要在每一步重复计算参考特征，导致计算冗余与低效率。

Method: 提出可缓存扩散架构：使用半注意力（Semi-Attention）机制，并用参考物品的类别嵌入替代传统时间步嵌入，从而将参考特征编码完全与去噪过程解耦，使参考特征只需计算一次并在所有去噪步中无损复用。构建DressCode-MR数据集通过专家模型流水线加人工反馈精炼，含28,179组高质量配对图像，覆盖上衣、下装、连衣裙、鞋、包五类。

Result: 在VITON-HD、DressCode及DressCode-MR上进行大量实验，结果显示FastFit在关键保真度指标上超过SOTA，同时在推理速度上平均加速约3.5倍。

Conclusion: FastFit在多参考虚拟试衣场景下既能显著提升推理效率（约3.5倍），又能在视觉保真度上超越当前最先进方法；同时，DressCode-MR为复杂多参考试衣研究提供了有价值的数据支持。

Abstract: Despite its great potential, virtual try-on technology is hindered from
real-world application by two major challenges: the inability of current
methods to support multi-reference outfit compositions (including garments and
accessories), and their significant inefficiency caused by the redundant
re-computation of reference features in each denoising step. To address these
challenges, we propose FastFit, a high-speed multi-reference virtual try-on
framework based on a novel cacheable diffusion architecture. By employing a
Semi-Attention mechanism and substituting traditional timestep embeddings with
class embeddings for reference items, our model fully decouples reference
feature encoding from the denoising process with negligible parameter overhead.
This allows reference features to be computed only once and losslessly reused
across all steps, fundamentally breaking the efficiency bottleneck and
achieving an average 3.5x speedup over comparable methods. Furthermore, to
facilitate research on complex, multi-reference virtual try-on, we introduce
DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of
high-quality, paired images covering five key categories (tops, bottoms,
dresses, shoes, and bags), constructed through a pipeline of expert models and
human feedback refinement. Extensive experiments on the VITON-HD, DressCode,
and our DressCode-MR datasets show that FastFit surpasses state-of-the-art
methods on key fidelity metrics while offering its significant advantage in
inference efficiency.

</details>


### [43] [UTA-Sign: Unsupervised Thermal Video Augmentation via Event-Assisted Traffic Signage Sketching](https://arxiv.org/abs/2508.20594)
*Yuqi Han,Songqian Zhang,Weijian Su,Ke Li,Jiayu Yang,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出UTA-Sign，通过双重增强机制将热成像帧与事件相机信号融合，利用热帧做时间对齐并用事件信号补充细节，实现低光交通标牌的无监督增强，实测提升标牌描绘质量和检测性能。


<details>
  <summary>Details</summary>
Motivation: Thermal cameras work well in low light but miss signage when materials similar to background; event cameras capture changes and work well in low-light/high-speed but are non-uniform. Combining them leverages complementary strengths.

Method: 开发双重增强机制：使用热帧提供准确运动线索作为时间参考以对齐事件信号，同时让事件信号为热帧补充细微标牌内容；无监督视频增强框架用于生成增强样本并在真实场景数据集上评估。

Result: Proposed UTA-Sign: unsupervised thermal-event video augmentation with dual-boosting fusing thermal frames and event signals, using thermal frames for temporal alignment and event signals to enrich signage content; validated on real-world datasets showing superior signage sketching and improved detection accuracy.

Conclusion: UTA-Sign有效缓解热像标牌盲点并弥补事件相机采样不均问题，通过互补融合提升了交通标牌的表征与检测，适用于夜间无人驾驶感知场景。

Abstract: The thermal camera excels at perceiving outdoor environments under low-light
conditions, making it ideal for applications such as nighttime autonomous
driving and unmanned navigation. However, thermal cameras encounter challenges
when capturing signage from objects made of similar materials, which can pose
safety risks for accurately understanding semantics in autonomous driving
systems. In contrast, the neuromorphic vision camera, also known as an event
camera, detects changes in light intensity asynchronously and has proven
effective in high-speed, low-light traffic environments. Recognizing the
complementary characteristics of these two modalities, this paper proposes
UTA-Sign, an unsupervised thermal-event video augmentation for traffic signage
in low-illumination environments, targeting elements such as license plates and
roadblock indicators. To address the signage blind spots of thermal imaging and
the non-uniform sampling of event cameras, we developed a dual-boosting
mechanism that fuses thermal frames and event signals for consistent signage
representation over time. The proposed method utilizes thermal frames to
provide accurate motion cues as temporal references for aligning the uneven
event signals. At the same time, event signals contribute subtle signage
content to the raw thermal frames, enhancing the overall understanding of the
environment. The proposed method is validated on datasets collected from
real-world scenarios, demonstrating superior quality in traffic signage
sketching and improved detection accuracy at the perceptual level.

</details>


### [44] [Disruptive Attacks on Face Swapping via Low-Frequency Perceptual Perturbations](https://arxiv.org/abs/2508.20595)
*Mengxiao Huang,Minglei Shu,Shuwang Zhou,Zhaoyang Liu*

Main category: cs.CV

TL;DR: 提出一种基于DWT的低频感知扰动主动防御方法，通过在低频引入伪影扰动并保留高频细节，能有效干扰人脸替换生成模型并保持图像自然性。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测方法多为被动检测，无法在攻击发生前或发生中有效阻止人脸替换等生成攻击；因此提出主动防御，通过在输入图像中引入针对生成模型的低频扰动，直接干扰其生成过程以保护隐私和安全。

Method: 设计了包含编码器、扰动生成器和解码器的完整网络结构，利用离散小波变换（DWT）提取低频分量，在低频子带上生成感知扰动并结合空域特征进行融合，以在不破坏高频细节的情况下引入可影响生成器的伪影。

Result: 在CelebA-HQ和LFW数据集上的实验证明，该方法显著降低了人脸替换模型的效果，提升了防御成功率，并在主观/客观评估上保持了较好的视觉质量。

Conclusion: 本文提出了一种基于低频感知扰动的主动防御方法，通过在频域和空域结合引入低频伪影来干扰人脸替换生成过程，从而降低Deepfake生成内容的真实性和效果。实验表明该方法在CelebA-HQ和LFW上能够显著减少人脸替换的成功率，同时在保持高频细节的前提下维持较好的视觉自然度。

Abstract: Deepfake technology, driven by Generative Adversarial Networks (GANs), poses
significant risks to privacy and societal security. Existing detection methods
are predominantly passive, focusing on post-event analysis without preventing
attacks. To address this, we propose an active defense method based on
low-frequency perceptual perturbations to disrupt face swapping manipulation,
reducing the performance and naturalness of generated content. Unlike prior
approaches that used low-frequency perturbations to impact classification
accuracy,our method directly targets the generative process of deepfake
techniques. We combine frequency and spatial domain features to strengthen
defenses. By introducing artifacts through low-frequency perturbations while
preserving high-frequency details, we ensure the output remains visually
plausible. Additionally, we design a complete architecture featuring an
encoder, a perturbation generator, and a decoder, leveraging discrete wavelet
transform (DWT) to extract low-frequency components and generate perturbations
that disrupt facial manipulation models. Experiments on CelebA-HQ and LFW
demonstrate significant reductions in face-swapping effectiveness, improved
defense success rates, and preservation of visual quality.

</details>


### [45] [Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion](https://arxiv.org/abs/2508.20604)
*Zheng Qin,Yabing Wang,Minghui Yang,Sanping Zhou,Ming Yang,Le Wang*

Main category: cs.CV

TL;DR: 通过将噪声作为多样性载体并在文本到动作的潜在空间中引入随机采样，Diverse-T2M实现了更高的动作生成多样性且保持文本一致性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到三维人体动作生成在文本一致性上已有进展，但生成多样性不足。作者希望设计一种简单有效的方法，使模型能显式建模不确定性，从而提升多样性同时保持语义一致性。

Method: 基于Transformer，作者将噪声信号作为多样性信息的载体，并构建连续潜在空间，将文本映射为可采样的表示；通过潜在空间采样器引入随机性，并在训练/采样过程中结合噪声与文本表示生成动作序列。

Result: 在HumanML3D与KIT-ML基准数据集上，Diverse-T2M在多样性指标上显著优于先前方法，同时在文本一致性（如R-Precision等）上仍保持或接近最先进水平。

Conclusion: 该文提出了Diverse-T2M，通过在生成过程中引入不确定性与随机采样，在保持文本-动作一致性的同时显著提升生成动作的多样性。

Abstract: Generating 3D human motions from text is a challenging yet valuable task. The
key aspects of this task are ensuring text-motion consistency and achieving
generation diversity. Although recent advancements have enabled the generation
of precise and high-quality human motions from text, achieving diversity in the
generated motions remains a significant challenge. In this paper, we aim to
overcome the above challenge by designing a simple yet effective text-to-motion
generation method, \textit{i.e.}, Diverse-T2M. Our method introduces
uncertainty into the generation process, enabling the generation of highly
diverse motions while preserving the semantic consistency of the text.
Specifically, we propose a novel perspective that utilizes noise signals as
carriers of diversity information in transformer-based methods, facilitating a
explicit modeling of uncertainty. Moreover, we construct a latent space where
text is projected into a continuous representation, instead of a rigid
one-to-one mapping, and integrate a latent space sampler to introduce
stochastic sampling into the generation process, thereby enhancing the
diversity and uncertainty of the outputs. Our results on text-to-motion
generation benchmark datasets~(HumanML3D and KIT-ML) demonstrate that our
method significantly enhances diversity while maintaining state-of-the-art
performance in text consistency.

</details>


### [46] [Optimization-Based Calibration for Intravascular Ultrasound Volume Reconstruction](https://arxiv.org/abs/2508.20605)
*Karl-Philippe Beaudet,Sidaty El Hadramy,Philippe C Cattin,Juan Verde,Stéphane Cotin*

Main category: cs.CV

TL;DR: 提出基于3D打印幻影的优化标定与3D IVUS体积重建方法，体内实验显示标定误差≈0.9–1.8 mm，注册误差≈3.4–5.7 mm，可用于CT与IVUS配准以支持肝手术导航。


<details>
  <summary>Details</summary>
Motivation: 术中超声视野受限且解剖复杂，直接解释困难。将术前CT与术中IVUS对齐能显著改善手术导航，但要求高精度的IVUS体积重建与标定。3D IVUS可以重建整个器官，从而为CT-IVUS配准提供手段。

Method: 构建3D打印幻影作为标定靶，采集带有光学/电磁跟踪信息的IVUS扫查数据，建立参数化的优化目标函数（最小化IVUS重建与CT目标点/表面之间的距离/误差），通过数值优化求解探头到探针坐标变换，实现标定并重建3D IVUS体积。

Result: 在猪肝体内数据上验证，标定误差为0.88–1.80 mm，CT与3D IVUS配准误差为3.40–5.71 mm，表明方法在体内环境中具有可用的精度，能用于术中图像配准与导航增强。

Conclusion: 本文提出了一种基于优化的标定方法，结合3D打印幻影实现对3D IVUS体积重建的精确标定，从而将跟踪的IVUS数据与术前CT对齐，以提升术中导航精度。

Abstract: Intraoperative ultrasound images are inherently challenging to interpret in
liver surgery due to the limited field of view and complex anatomical
structures. Bridging the gap between preoperative and intraoperative data is
crucial for effective surgical guidance. 3D IntraVascular UltraSound (IVUS)
offers a potential solution by enabling the reconstruction of the entire organ,
which facilitates registration between preoperative computed tomography (CT)
scans and intraoperative IVUS images. In this work, we propose an
optimization-based calibration method using a 3D-printed phantom for accurate
3D Intravascular Ultrasound volume reconstruction. Our approach ensures precise
alignment of tracked IVUS data with preoperative CT images, improving
intraoperative navigation. We validated our method using in vivo swine liver
images, achieving a calibration error from 0.88 to 1.80 mm and a registration
error from 3.40 to 5.71 mm between the 3D IVUS data and the corresponding CT
scan. Our method provides a reliable and accurate means of calibration and
volume reconstruction. It can be used to register intraoperative ultrasound
images with preoperative CT images in the context of liver surgery, and enhance
intraoperative guidance.

</details>


### [47] [Physics Informed Generative Models for Magnetic Field Images](https://arxiv.org/abs/2508.20612)
*Aye Phyu Phyu Aung,Lucas Lum,Zhansen Shi,Wen Qiu,Bernice Zee,JM Chin,Yeow Kheng Lim,J. Senthilnath*

Main category: cs.CV

TL;DR: 提出一种带物理约束的扩散生成模型PI-GenMFI，用于合成磁场成像数据以缓解真实数据稀缺，实验证明生成样本可用于缺陷定位且优于若干基线模型。


<details>
  <summary>Details</summary>
Motivation: 真实MFI数据由于专有性难以获取，限制了基于机器学习的缺陷定位模型训练。通过生成高质量合成MFI样本，可扩充训练数据以提升缺陷定位效率，减少大规模X射线扫描的成本和时间。

Method: 提出PI-GenMFI框架，在扩散模型中引入两类物理约束（文中未详述但可推测为基于电磁物理和传感器噪声/采样机制的约束），并针对常见的短路（power shorts）缺陷生成合成MFI图像，同时与VAE和其他扩散模型进行对比，使用多种图像生成与信号处理指标进行评估，并包含领域专家的主观评估。

Result: 通过定性（专家评估）和定量（图像生成与信号处理指标）评估，PI-GenMFI在生成质量和用于下游缺陷定位任务方面表现优于或接近SOTA的VAE与扩散基线，显示出在优化缺陷定位流程上的潜力。

Conclusion: 这篇论文提出了将物理先验融入扩散生成模型，用于合成磁场成像（MFI）数据，以解决真实MFI数据稀缺的问题，从而辅助半导体缺陷的定位与筛查。

Abstract: In semiconductor manufacturing, defect detection and localization are
critical to ensuring product quality and yield. While X-ray imaging is a
reliable non-destructive testing method, it is memory-intensive and
time-consuming for large-scale scanning, Magnetic Field Imaging (MFI) offers a
more efficient means to localize regions of interest (ROI) for targeted X-ray
scanning. However, the limited availability of MFI datasets due to proprietary
concerns presents a significant bottleneck for training machine learning (ML)
models using MFI. To address this challenge, we consider an ML-driven approach
leveraging diffusion models with two physical constraints. We propose Physics
Informed Generative Models for Magnetic Field Images (PI-GenMFI) to generate
synthetic MFI samples by integrating specific physical information. We generate
MFI images for the most common defect types: power shorts. These synthetic
images will serve as training data for ML algorithms designed to localize
defect areas efficiently. To evaluate generated MFIs, we compare our model to
SOTA generative models from both variational autoencoder (VAE) and diffusion
methods. We present a domain expert evaluation to assess the generated samples.
In addition, we present qualitative and quantitative evaluation using various
metrics used for image generation and signal processing, showing promising
results to optimize the defect localization process.

</details>


### [48] [Revisiting the Privacy Risks of Split Inference: A GAN-Based Data Reconstruction Attack via Progressive Feature Optimization](https://arxiv.org/abs/2508.20613)
*Yixiang Qiu,Yanhan Liu,Hongyao Yu,Hao Fang,Bin Chen,Shu-Tao Xia,Ke Xu*

Main category: cs.CV

TL;DR: 提出一种基于GAN的逐层特征优化攻击，通过层级生成器与L1-球约束提升在复杂模型和不同数据/分辨率下的重构效果。


<details>
  <summary>Details</summary>
Motivation: 现有的数据重构攻击在对浅层模型有效但在深层模型、高分辨率、迁移到不同数据集或网络架构时表现较差，且未充分利用语义先验。作者旨在提升重构质量和泛化能力，通过生成对抗网络和逐层优化中间特征来增强语义保真度。

Method: GAN-based Progressive Feature Optimization for Data Reconstruction Attacks

Result: 提出了将生成器分解为层级模块并逐步细化中间表示的PFO方法，同时引入L1-球约束以稳定优化并提高图像真实感。在各种设置（高分辨率、OOD、深层复杂网络）中，PFO显著优于先前方法。

Conclusion: PFO能显著提升数据重构攻击的语义保真度和泛化能力，凸显了分裂推理场景下的隐私风险。

Abstract: The growing complexity of Deep Neural Networks (DNNs) has led to the adoption
of Split Inference (SI), a collaborative paradigm that partitions computation
between edge devices and the cloud to reduce latency and protect user privacy.
However, recent advances in Data Reconstruction Attacks (DRAs) reveal that
intermediate features exchanged in SI can be exploited to recover sensitive
input data, posing significant privacy risks. Existing DRAs are typically
effective only on shallow models and fail to fully leverage semantic priors,
limiting their reconstruction quality and generalizability across datasets and
model architectures. In this paper, we propose a novel GAN-based DRA framework
with Progressive Feature Optimization (PFO), which decomposes the generator
into hierarchical blocks and incrementally refines intermediate representations
to enhance the semantic fidelity of reconstructed images. To stabilize the
optimization and improve image realism, we introduce an L1-ball constraint
during reconstruction. Extensive experiments show that our method outperforms
prior attacks by a large margin, especially in high-resolution scenarios,
out-of-distribution settings, and against deeper and more complex DNNs.

</details>


### [49] [EmoCAST: Emotional Talking Portrait via Emotive Text Description](https://arxiv.org/abs/2508.20615)
*Yiguo Jiang,Xiaodong Cun,Yong Zhang,Yudian Zheng,Fan Tang,Chi-Man Pun*

Main category: cs.CV

TL;DR: EmoCAST引入文本引导的解耦情感模块与情感化音频注意力模块，并配套情感数据集与渐进训练，显著提升了文本驱动情感口型视频的表达质量与唇同步，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在控制灵活性、运动自然性与表情质量上存在不足，且现有数据多为实验室采集，限制了真实场景应用，因而需要提高文本驱动情感合成的准确性和泛化能力。

Method: 提出一个基于扩散模型的框架EmoCAST，包含两个核心模块：1) 文本引导的解耦情感模块，将情感提示融入外观建模以增强空间情感知识；2) 情感化音频注意力模块，刻画受控情感与驱动音频之间的交互，生成情感感知特征以指导面部运动合成。并配套构建情感口型数据集，提出情感感知采样训练与渐进功能训练策略以提升细节和唇同步。

Result: 在提出的数据集与训练策略基础上，EmoCAST在多项主观与客观评价中优于现有方法，能生成更自然的面部运动、更准确的情感表达以及更精确的唇同步。

Conclusion: 该工作通过引入情感提示的解耦表征与情感-音频联合注意力模块，有效提升了文本驱动情感口型合成的表达控制性与自然度，构建的数据集与分步训练策略进一步增强了情感细节捕捉与唇同步性能。总体上，EmoCAST在生成真实、情感丰富且与音频同步的口型视频上达到了SOTA水平。

Abstract: Emotional talking head synthesis aims to generate talking portrait videos
with vivid expressions. Existing methods still exhibit limitations in control
flexibility, motion naturalness, and expression quality. Moreover, currently
available datasets are primarily collected in lab settings, further
exacerbating these shortcomings. Consequently, these limitations substantially
hinder practical applications in real-world scenarios. To address these
challenges, we propose EmoCAST, a diffusion-based framework with two key
modules for precise text-driven emotional synthesis. In appearance modeling,
emotional prompts are integrated through a text-guided decoupled emotive
module, enhancing the spatial knowledge to improve emotion comprehension. To
improve the relationship between audio and emotion, we introduce an emotive
audio attention module to capture the interplay between controlled emotion and
driving audio, generating emotion-aware features to guide more precise facial
motion synthesis. Additionally, we construct an emotional talking head dataset
with comprehensive emotive text descriptions to optimize the framework's
performance. Based on the proposed dataset, we propose an emotion-aware
sampling training strategy and a progressive functional training strategy that
further improve the model's ability to capture nuanced expressive features and
achieve accurate lip-synchronization. Overall, EmoCAST achieves
state-of-the-art performance in generating realistic, emotionally expressive,
and audio-synchronized talking-head videos. Project Page:
https://github.com/GVCLab/EmoCAST

</details>


### [50] [Mask-Guided Multi-Channel SwinUNETR Framework for Robust MRI Classification](https://arxiv.org/abs/2508.20621)
*Smriti Joshi,Lidia Garrucho,Richard Osuala,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: Developed a SwinUNETR-based ensemble model with preprocessing and heavy augmentation for multi-center breast MRI classification; ranked 2nd in ODELIA challenge; code open-sourced.


<details>
  <summary>Details</summary>
Motivation: Improve robustness and generalizability of AI-based breast MRI lesion classification across multi-center, multi-vendor data to aid early detection where mammography is less effective.

Method: SwinUNETR-based deep learning framework with breast region masking, extensive data augmentation, and ensemble learning for classification of left/right breast into no lesion/benign/malignant.

Result: Achieved second place on the ODELIA challenge leaderboard using the proposed framework on a dataset of 511 studies from six European centers; code publicly released.

Conclusion: The proposed approach is a robust, generalizable solution for breast MRI lesion classification across diverse scanners and centers, showing promise to support clinical interpretation and is available as open-source.

Abstract: Breast cancer is one of the leading causes of cancer-related mortality in
women, and early detection is essential for improving outcomes. Magnetic
resonance imaging (MRI) is a highly sensitive tool for breast cancer detection,
particularly in women at high risk or with dense breast tissue, where
mammography is less effective. The ODELIA consortium organized a multi-center
challenge to foster AI-based solutions for breast cancer diagnosis and
classification. The dataset included 511 studies from six European centers,
acquired on scanners from multiple vendors at both 1.5 T and 3 T. Each study
was labeled for the left and right breast as no lesion, benign lesion, or
malignant lesion. We developed a SwinUNETR-based deep learning framework that
incorporates breast region masking, extensive data augmentation, and ensemble
learning to improve robustness and generalizability. Our method achieved second
place on the challenge leaderboard, highlighting its potential to support
clinical breast MRI interpretation. We publicly share our codebase at
https://github.com/smriti-joshi/bcnaim-odelia-challenge.git.

</details>


### [51] [AvatarBack: Back-Head Generation for Complete 3D Avatars from Front-View Images](https://arxiv.org/abs/2508.20623)
*Shiqi Xin,Xiaolin Zhang,Yanbin Liu,Peng Zhang,Caifeng Shan*

Main category: cs.CV

TL;DR: AvatarBack fills missing back-head views by generating identity-consistent pseudo-images and aligning them to 3D Gaussians, boosting rear reconstruction quality while preserving frontal detail and animatability.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian Splatting based head-avatar reconstruction uses mainly frontal images, causing poor back-head reconstruction, geometric inconsistency, blurring, and reduced realism.

Method: Use Subject-specific Generator to synthesize back-view images from sparse frontal inputs; apply Adaptive Spatial Alignment with learnable transformation matrices during training to align synthetic views with 3D Gaussian representation; integrate into Gaussian Splatting pipeline; evaluate on benchmarks with geometric, photometric, and perceptual metrics.

Result: AvatarBack, a plug-and-play framework with Subject-specific Generator (SSG) and Adaptive Spatial Alignment (ASA), synthesizes plausible back-view pseudo-images and aligns them via learnable transformations, improving back-head geometry and maintaining frontal fidelity; validated on NeRSemble and K-hairstyle with multiple metrics.

Conclusion: AvatarBack effectively recovers consistent, realistic back-head geometry for Gaussian avatars using generative priors and spatial alignment; it improves metrics and preserves animatability.

Abstract: Recent advances in Gaussian Splatting have significantly boosted the
reconstruction of head avatars, enabling high-quality facial modeling by
representing an 3D avatar as a collection of 3D Gaussians. However, existing
methods predominantly rely on frontal-view images, leaving the back-head poorly
constructed. This leads to geometric inconsistencies, structural blurring, and
reduced realism in the rear regions, ultimately limiting the fidelity of
reconstructed avatars. To address this challenge, we propose AvatarBack, a
novel plug-and-play framework specifically designed to reconstruct complete and
consistent 3D Gaussian avatars by explicitly modeling the missing back-head
regions. AvatarBack integrates two core technical innovations,i.e., the
Subject-specific Generator (SSG) and the Adaptive Spatial Alignment Strategy
(ASA). The former leverages a generative prior to synthesize
identity-consistent, plausible back-view pseudo-images from sparse frontal
inputs, providing robust multi-view supervision. To achieve precise geometric
alignment between these synthetic views and the 3D Gaussian representation, the
later employs learnable transformation matrices optimized during training,
effectively resolving inherent pose and coordinate discrepancies. Extensive
experiments on NeRSemble and K-hairstyle datasets, evaluated using geometric,
photometric, and GPT-4o-based perceptual metrics, demonstrate that AvatarBack
significantly enhances back-head reconstruction quality while preserving
frontal fidelity. Moreover, the reconstructed avatars maintain consistent
visual realism under diverse motions and remain fully animatable.

</details>


### [52] [ArtFace: Towards Historical Portrait Face Identification via Model Adaptation](https://arxiv.org/abs/2508.20626)
*Francois Poh,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文探讨利用foundation模型提升历史绘画中人物面部识别。通过微调foundation模型并与传统人脸识别网络的嵌入向量融合，显著优于现有方法，在风格多样和领域转移下改善识别性能。


<details>
  <summary>Details</summary>
Motivation: 艺术史学家在识别历史坐像时面临主观性、数据稀缺与风格差异等挑战，传统人脸识别在照片上表现良好但在绘画上受域差影响较大，故探索foundation模型以提升跨域识别能力。

Method: 对foundation模型进行微调以适应艺术图像特征，提取其嵌入向量；同时从传统人脸识别网络提取嵌入；通过融合策略（如拼接或加权融合）结合两类嵌入，训练检索/分类模型并在历史绘画数据集上评估性能。

Result: 实验表明，微调后的foundation模型及其与传统网络嵌入的融合，在历史绘画人脸识别任务上显著超过现有最先进方法，验证了foundation模型在处理复杂艺术风格与高类内变异方面的优势。

Conclusion: 将foundation模型微调并与传统人脸识别特征结合，可以有效弥补绘画与照片间的域差，提升历史绘画中坐像识别的准确性与鲁棒性。

Abstract: Identifying sitters in historical paintings is a key task for art historians,
offering insight into their lives and how they chose to be seen. However, the
process is often subjective and limited by the lack of data and stylistic
variations. Automated facial recognition is capable of handling challenging
conditions and can assist, but while traditional facial recognition models
perform well on photographs, they struggle with paintings due to domain shift
and high intra-class variation. Artistic factors such as style, skill, intent,
and influence from other works further complicate recognition. In this work, we
investigate the potential of foundation models to improve facial recognition in
artworks. By fine-tuning foundation models and integrating their embeddings
with those from conventional facial recognition networks, we demonstrate
notable improvements over current state-of-the-art methods. Our results show
that foundation models can bridge the gap where traditional methods are
ineffective. Paper page at https://www.idiap.ch/paper/artface/

</details>


### [53] [CraftGraffiti: Exploring Human Identity with Custom Graffiti Art via Facial-Preserving Diffusion Models](https://arxiv.org/abs/2508.20640)
*Ayan Banerjee,Fernando Vilariño,Josep Lladós*

Main category: cs.CV

TL;DR: CraftGraffiti 提出“先风格化后保持身份”的涂鸦生成框架，结合 LoRA 微调扩散模型、面部一致自注意力和 CLIP 引导的无关键点重姿态，实现高艺术性下的身份保真与姿态定制。


<details>
  <summary>Details</summary>
Motivation: 涂鸦等高对比、抽象风格容易通过细微面部变形破坏可识别性，影响个人和文化真实性。目标是构建在强烈风格化下仍能保留被摄者身份特征的生成系统，以实现对艺术表达与身份尊重的平衡。

Method: 方法包括三部分：1) 使用 LoRA 微调预训练扩散变换器执行涂鸦风格迁移（风格优先）；2) 在注意力层中加入显式身份嵌入的面部一致自注意力机制，以增强面部特征保真度；3) 利用 CLIP 指导的提示扩展实现无关键点的姿态改变。作者还形式化证明“先风格化后身份恢复”的范式优于相反顺序，并在量化评价和用户偏好测试中验证。

Result: 实验表明该方法在面部特征一致性上具有竞争力，并在美学分数和人类偏好评估中达到或超越现有方法。定性结果与在 Cruilla 音乐节的现场部署展示了实际创作影响力。

Conclusion: CraftGraffiti 提出了一种先风格化后保持身份的生成流程，通过 LoRA 微调的扩散变换器进行涂鸦风格迁移，结合面部一致自注意力模块和 CLIP 引导的提示扩展实现无关键点的姿态定制，在保持艺术性同时显著降低身份漂移。

Abstract: Preserving facial identity under extreme stylistic transformation remains a
major challenge in generative art. In graffiti, a high-contrast, abstract
medium, subtle distortions to the eyes, nose, or mouth can erase the subject's
recognizability, undermining both personal and cultural authenticity. We
present CraftGraffiti, an end-to-end text-guided graffiti generation framework
designed with facial feature preservation as a primary objective. Given an
input image and a style and pose descriptive prompt, CraftGraffiti first
applies graffiti style transfer via LoRA-fine-tuned pretrained diffusion
transformer, then enforces identity fidelity through a face-consistent
self-attention mechanism that augments attention layers with explicit identity
embeddings. Pose customization is achieved without keypoints, using CLIP-guided
prompt extension to enable dynamic re-posing while retaining facial coherence.
We formally justify and empirically validate the "style-first, identity-after"
paradigm, showing it reduces attribute drift compared to the reverse order.
Quantitative results demonstrate competitive facial feature consistency and
state-of-the-art aesthetic and human preference scores, while qualitative
analyses and a live deployment at the Cruilla Festival highlight the system's
real-world creative impact. CraftGraffiti advances the goal of
identity-respectful AI-assisted artistry, offering a principled approach for
blending stylistic freedom with recognizability in creative AI applications.

</details>


### [54] [Improving Alignment in LVLMs with Debiased Self-Judgment](https://arxiv.org/abs/2508.20655)
*Sihan Yang,Chenhang Cui,Zihao Zhao,Yiyang Zhou,Weilong Yan,Ying Wei,Huaxiu Yao*

Main category: cs.CV

TL;DR: 提出一种内部自评去偏分数方法，无需外部数据即可提高视觉-语言模型对齐，减少幻觉并提升安全性与能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法依赖外部数据与人工标注，成本高且扩展性差；需一种可在模型内部生成评估信号以减少幻觉并提升安全性。

Method: 模型在生成输出时同时生成一个去偏自评分数，用于优化解码和偏好微调；该分数由模型内部计算，不依赖外部数据或人工标注。

Result: Debiased self-judgment score to align LVLMs, improving decoding and preference tuning; reduces hallucinations and enhances safety and capabilities.

Conclusion: 自评去偏分数可作为可扩展、节约成本的对齐策略，显著优于依赖外部注释或复杂后处理的传统方法。

Abstract: The rapid advancements in Large Language Models (LLMs) and Large
Visual-Language Models (LVLMs) have opened up new opportunities for integrating
visual and linguistic modalities. However, effectively aligning these
modalities remains challenging, often leading to hallucinations--where
generated outputs are not grounded in the visual input--and raising safety
concerns across various domains. Existing alignment methods, such as
instruction tuning and preference tuning, often rely on external datasets,
human annotations, or complex post-processing, which limit scalability and
increase costs. To address these challenges, we propose a novel approach that
generates the debiased self-judgment score, a self-evaluation metric created
internally by the model without relying on external resources. This enables the
model to autonomously improve alignment. Our method enhances both decoding
strategies and preference tuning processes, resulting in reduced
hallucinations, enhanced safety, and improved overall capability. Empirical
results show that our approach significantly outperforms traditional methods,
offering a more effective solution for aligning LVLMs.

</details>


### [55] ["Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection](https://arxiv.org/abs/2508.20670)
*Anastasios Skoularikis,Stefanos-Iordanis Papadopoulos,Symeon Papadopoulos,Panagiotis C. Petrantonakis*

Main category: cs.CV

TL;DR: introduced S-HArM dataset (9,576 real image-text pairs) labeled for intent; synthetic training data created via three prompting strategies; image/multimodal-guided synthetic data helps generalization but task remains challenging


<details>
  <summary>Details</summary>
Motivation: existing detection focuses on content authenticity but ignores intent; need dataset and methods to classify intent of AI-generated images

Method: multimodal dataset + synthetic training

Result: models trained on image- and multimodally-guided synthetic data generalize better to real-world data; overall performance limited

Conclusion: inferring intent in AI-generated images is hard; preserved visual context in synthetic data improves generalization; specialized architectures needed

Abstract: Recent advances in multimodal AI have enabled progress in detecting synthetic
and out-of-context content. However, existing efforts largely overlook the
intent behind AI-generated images. To fill this gap, we introduce S-HArM, a
multimodal dataset for intent-aware classification, comprising 9,576 "in the
wild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,
or Misinformation. Additionally, we explore three prompting strategies
(image-guided, description-guided, and multimodally-guided) to construct a
large-scale synthetic training dataset with Stable Diffusion. We conduct an
extensive comparative study including modality fusion, contrastive learning,
reconstruction networks, attention mechanisms, and large vision-language
models. Our results show that models trained on image- and multimodally-guided
data generalize better to "in the wild" content, due to preserved visual
context. However, overall performance remains limited, highlighting the
complexity of inferring intent and the need for specialized architectures.

</details>


### [56] [MobileCLIP2: Improving Multi-Modal Reinforced Training](https://arxiv.org/abs/2508.20691)
*Fartash Faghri,Pavan Kumar Anasosalu Vasu,Cem Koc,Vaishaal Shankar,Alexander Toshev,Oncel Tuzel,Hadi Pouransari*

Main category: cs.CV

TL;DR: 通过改进教师集合与captioner微调，MobileCLIP2在保持低延迟与小参数量的同时显著提升了ImageNet-1k零-shot准确率，部分型号达到了与更大模型相当的表现。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现高效且具强泛化能力的图文基础模型，通过改进训练数据和蒸馏策略在低延迟与小参数量条件下提升零-shot表现。

Method: 使用更优的CLIP教师集合（在DFN数据集上训练）、对caption生成模型进行在DFN上训练并微调于高质量数据集、在对比知识蒸馏中调优温度、以及合并多模型生成的合成caption来训练小型高效模型。

Result: This paper introduces MobileCLIP2, an improved family of low-latency image-text models that enhance zero-shot accuracy via better multi-modal reinforced training, improved teacher ensembles, and fine-tuned captioners, achieving state-of-the-art ImageNet-1k zero-shot performance while remaining efficient.

Conclusion: 改进的多模态增强训练、温度调优和多caption生成器合并带来了显著的零-shot性能提升，MobileCLIP2在速度与准确率间取得了更优的权衡，并开放了模型与数据生成工具。

Abstract: Foundation image-text models such as CLIP with zero-shot capabilities enable
a wide array of applications. MobileCLIP is a recent family of image-text
models at 3-15ms latency and 50-150M parameters with state-of-the-art zero-shot
accuracy. The main ingredients in MobileCLIP were its low-latency and light
architectures and a novel multi-modal reinforced training that made knowledge
distillation from multiple caption-generators and CLIP teachers efficient,
scalable, and reproducible. In this paper, we improve the multi-modal
reinforced training of MobileCLIP through: 1) better CLIP teacher ensembles
trained on the DFN dataset, 2) improved captioner teachers trained on the DFN
dataset and fine-tuned on a diverse selection of high-quality image-caption
datasets. We discover new insights through ablations such as the importance of
temperature tuning in contrastive knowledge distillation, the effectiveness of
caption-generator fine-tuning for caption diversity, and the additive
improvement from combining synthetic captions generated by multiple models. We
train a new family of models called MobileCLIP2 and achieve state-of-the-art
ImageNet-1k zero-shot accuracies at low latencies. In particular, we observe
2.2% improvement in ImageNet-1k accuracy for MobileCLIP2-B compared with
MobileCLIP-B architecture. Notably, MobileCLIP2-S4 matches the zero-shot
accuracy of SigLIP-SO400M/14 on ImageNet-1k while being 2$\times$ smaller and
improves on DFN ViT-L/14 at 2.5$\times$ lower latency. We release our
pretrained models (https://github.com/apple/ml-mobileclip) and the data
generation code (https://github.com/apple/ml-mobileclip-dr). The data
generation code makes it easy to create new reinforced datasets with arbitrary
teachers using distributed scalable processing.

</details>


### [57] [Learned Rate Control for Frame-Level Adaptive Neural Video Compression via Dynamic Neural Network](https://arxiv.org/abs/2508.20709)
*Chenhao Zhang,Wei Gao*

Main category: cs.CV

TL;DR: 提出动态路由自编码器配合比特率控制代理和联合路由优化，实现可变比特率下精确率控与优良RD性能，平均BD-Rate下降14.8%，平均比特率误差1.66%。


<details>
  <summary>Details</summary>
Motivation: Existing NVC struggles with precise rate control in variable bitrate scenarios; need flexible run-time bitrate adjustment while preserving RD performance

Method: Dynamic-Route Autoencoder; Rate Control Agent; Joint-Routes Optimization

Result: Achieves average BD-Rate reduction of 14.8% and BD-PSNR gain of 0.47dB over SOTA; bitrate error 1.66%; demonstrates RDCO across bitrates

Conclusion: Dynamic framework enables runtime bitrate control via route selection with collaborative training, yielding strong RD performance and low bitrate error for variable bitrate applications.

Abstract: Neural Video Compression (NVC) has achieved remarkable performance in recent
years. However, precise rate control remains a challenge due to the inherent
limitations of learning-based codecs. To solve this issue, we propose a dynamic
video compression framework designed for variable bitrate scenarios. First, to
achieve variable bitrate implementation, we propose the Dynamic-Route
Autoencoder with variable coding routes, each occupying partial computational
complexity of the whole network and navigating to a distinct RD trade-off.
Second, to approach the target bitrate, the Rate Control Agent estimates the
bitrate of each route and adjusts the coding route of DRA at run time. To
encompass a broad spectrum of variable bitrates while preserving overall RD
performance, we employ the Joint-Routes Optimization strategy, achieving
collaborative training of various routes. Extensive experiments on the HEVC and
UVG datasets show that the proposed method achieves an average BD-Rate
reduction of 14.8% and BD-PSNR gain of 0.47dB over state-of-the-art methods
while maintaining an average bitrate error of 1.66%, achieving
Rate-Distortion-Complexity Optimization (RDCO) for various bitrate and
bitrate-constrained applications. Our code is available at
https://git.openi.org.cn/OpenAICoding/DynamicDVC.

</details>


### [58] [CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network](https://arxiv.org/abs/2508.20734)
*Reza Akbari Movahed,Abuzar Rezaee,Arezoo Zakeri,Colin Berry,Edmond S. L. Ho,Ali Gooya*

Main category: cs.CV

TL;DR: 提出CardioMorphNet，一种基于递归贝叶斯变分自编码器的3D心脏形状引导变形配准方法，利用短轴CMR序列与分割图递归配准替代强度相似性损失，并能估计运动不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强度相似性的图像配准方法可能忽视心脏解剖区域，导致运动估计不准确，故提出一种形状引导且能建模时空依赖与不确定性的贝叶斯方法来提高准确性与可靠性。

Method: 构建一个递归变分自编码器来建模心动周期的时空依赖；设计两个后验模型分别用于双室分割和运动估计；通过贝叶斯推导得到损失函数，递归地对分割图进行配准以强调解剖区域，避免使用强度相似性损失；利用序列SAX体积输入并输出运动场与不确定性图。

Result: 在UK Biobank上，通过比较变形后的掩码与真值掩码，CardioMorphNet在运动估计指标上优于最先进方法，并在心脏区域的运动不确定性低于其他概率性配准方法。

Conclusion: CardioMorphNet在UK Biobank数据集上优于现有方法，能更准确地估计心脏运动且在心脏区域产生更低的不确定性，说明其预测更可靠。

Abstract: Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)
images is vital for assessing cardiac function and detecting its abnormalities.
Existing methods often struggle to capture heart motion accurately because they
rely on intensity-based image registration similarity losses that may overlook
cardiac anatomical regions. To address this, we propose CardioMorphNet, a
recurrent Bayesian deep learning framework for 3D cardiac shape-guided
deformable registration using short-axis (SAX) CMR images. It employs a
recurrent variational autoencoder to model spatio-temporal dependencies over
the cardiac cycle and two posterior models for bi-ventricular segmentation and
motion estimation. The derived loss function from the Bayesian formulation
guides the framework to focus on anatomical regions by recursively registering
segmentation maps without using intensity-based image registration similarity
loss, while leveraging sequential SAX volumes and spatio-temporal features. The
Bayesian modelling also enables computation of uncertainty maps for the
estimated motion fields. Validated on the UK Biobank dataset by comparing
warped mask shapes with ground truth masks, CardioMorphNet demonstrates
superior performance in cardiac motion estimation, outperforming
state-of-the-art methods. Uncertainty assessment shows that it also yields
lower uncertainty values for estimated motion fields in the cardiac region
compared with other probabilistic-based cardiac registration methods,
indicating higher confidence in its predictions.

</details>


### [59] [Mix, Align, Distil: Reliable Cross-Domain Atypical Mitosis Classification](https://arxiv.org/abs/2508.20745)
*Kaustubh Atey,Sameer Anand Jha,Gouranga Bala,Amit Sethi*

Main category: cs.CV

TL;DR: 本文提出一种用于MIDOG 2025任务2的训练时策略，提高异质来源下非典型有丝分裂（AMF）分类稳健性。方法通过在骨干网络早中阶段进行风格扰动增加特征多样性，利用弱域标签（扫描仪、来源、物种、肿瘤）和辅助对齐损失对注意力精炼特征进行跨域对齐，并通过带温度缩放KL散度的EMA教师蒸馏稳定预测。该方法在组织者预备排行榜上取得平衡准确率0.8762、灵敏度0.8873、特异性0.8651、ROC AUC0.9499，推理开销几乎为零，仅依赖粗糙域元数据，表现均衡且具竞争力。


<details>
  <summary>Details</summary>
Motivation: AMF作为重要的组织病理学标志，在不同扫描仪、染色和采集流程导致的域偏移下难以保持一致识别。需求是在仅有粗糙域元数据的现实条件下，设计一种训练策略提升模型在跨域环境中的泛化与稳健性，同时保持推理效率。

Method: 方法包括三个关键组件：1）在骨干网络的早期和中期阶段插入风格扰动模块以扩增特征风格多样性；2）使用弱域标签（Scanner、Origin、Species、Tumor）通过辅助对齐损失对注意力精炼后的特征在不同站点间进行对齐；3）使用EMA教师与温度缩放的KL散度进行知识蒸馏以稳定学生网络预测。训练阶段联合优化主任务损失、对齐损失与蒸馏损失。推理时无额外开销。

Result: 在组织者运行的MIDOG 2025预备排行榜上，该方法取得平衡准确率0.8762、灵敏度0.8873、特异性0.8651和ROC AUC0.9499；方法对推理时间几乎没有影响，且仅依赖粗略的域元数据，表现稳健且均衡。

Conclusion: 本文提出的训练时配方在域偏移情形下显著提升AMF分类的鲁棒性，兼顾高灵敏度与特异性，并在MIDOG 2025预赛上取得优异指标，且不会增加推理复杂度，适合实际部署与挑战赛提交。

Abstract: Atypical mitotic figures (AMFs) are important histopathological markers yet
remain challenging to identify consistently, particularly under domain shift
stemming from scanner, stain, and acquisition differences. We present a simple
training-time recipe for domain-robust AMF classification in MIDOG 2025 Task 2.
The approach (i) increases feature diversity via style perturbations inserted
at early and mid backbone stages, (ii) aligns attention-refined features across
sites using weak domain labels (Scanner, Origin, Species, Tumor) through an
auxiliary alignment loss, and (iii) stabilizes predictions by distilling from
an exponential moving average (EMA) teacher with temperature-scaled KL
divergence. On the organizer-run preliminary leaderboard for atypical mitosis
classification, our submission attains balanced accuracy of 0.8762, sensitivity
of 0.8873, specificity of 0.8651, and ROC AUC of 0.9499. The method incurs
negligible inference-time overhead, relies only on coarse domain metadata, and
delivers strong, balanced performance, positioning it as a competitive
submission for the MIDOG 2025 challenge.

</details>


### [60] [Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning](https://arxiv.org/abs/2508.20751)
*Yibin Wang,Zhimin Li,Yuhang Zang,Yujie Zhou,Jiazi Bu,Chunyu Wang,Qinglin Lu,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

TL;DR: Pref-GRPO replaces pointwise score maximization with pairwise preference fitting in GRPO for T2I, using win-rate rewards to avoid reward hacking; UniGenBench benchmark evaluates models with fine-grained criteria showing Pref-GRPO's benefits.


<details>
  <summary>Details</summary>
Motivation: Current GRPO-based RL methods for T2I use pointwise RMs that are vulnerable to reward hacking when normalization amplifies small score differences, causing over-optimization and instability.

Method: Paper analysis

Result: Proposed Pref-GRPO uses pairwise preference RMs and win-rate rewards, demonstrating more stable training, better differentiation of subtle image quality differences, and mitigation of reward hacking; also introduced UniGenBench benchmark revealing model strengths/weaknesses and validating Pref-GRPO.

Conclusion: Pref-GRPO stabilizes T2I RL training by fitting preferences instead of scores; UniGenBench provides comprehensive evaluation, confirming Pref-GRPO's advantages.

Abstract: Recent advancements highlight the importance of GRPO-based reinforcement
learning methods and benchmarking in enhancing text-to-image (T2I) generation.
However, current methods using pointwise reward models (RM) for scoring
generated images are susceptible to reward hacking. We reveal that this happens
when minimal score differences between images are amplified after
normalization, creating illusory advantages that drive the model to
over-optimize for trivial gains, ultimately destabilizing the image generation
process. To address this, we propose Pref-GRPO, a pairwise preference
reward-based GRPO method that shifts the optimization objective from score
maximization to preference fitting, ensuring more stable training. In
Pref-GRPO, images are pairwise compared within each group using preference RM,
and the win rate is used as the reward signal. Extensive experiments
demonstrate that PREF-GRPO differentiates subtle image quality differences,
providing more stable advantages and mitigating reward hacking. Additionally,
existing T2I benchmarks are limited by coarse evaluation criteria, hindering
comprehensive model assessment. To solve this, we introduce UniGenBench, a
unified T2I benchmark comprising 600 prompts across 5 main themes and 20
subthemes. It evaluates semantic consistency through 10 primary and 27
sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our
benchmarks uncover the strengths and weaknesses of both open and closed-source
T2I models and validate the effectiveness of Pref-GRPO.

</details>


### [61] [${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting](https://arxiv.org/abs/2508.20754)
*Yuxi Hu,Jun Zhang,Kuangyi Chen,Zhe Zhang,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: C3-GS adds three lightweight modules (context-aware, cross-dimension, cross-scale) to generalizable Gaussian Splatting to better learn multi-view consistent features from sparse inputs, enabling state-of-the-art novel view synthesis without per-scene optimization.


<details>
  <summary>Details</summary>
Motivation: improve feature learning for generalizable Gaussian Splatting to synthesize novel views for unseen scenes without per-scene optimization; address poor discriminative, multi-view consistent features from sparse views

Method: analysis of C3-GS

Result: proposed C3-GS with context-aware, cross-dimension, cross-scale modules integrated into pipeline; leads to improved feature fusion and photorealistic synthesis without extra supervision; achieves SOTA on benchmarks

Conclusion: C3-GS effectively enhances feature representation in feed-forward Gaussian parameter prediction, improving geometry and rendering quality under sparse views and generalizes well to unseen scenes.

Abstract: Generalizable Gaussian Splatting aims to synthesize novel views for unseen
scenes without per-scene optimization. In particular, recent advancements
utilize feed-forward networks to predict per-pixel Gaussian parameters,
enabling high-quality synthesis from sparse input views. However, existing
approaches fall short in encoding discriminative, multi-view consistent
features for Gaussian predictions, which struggle to construct accurate
geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a
framework that enhances feature learning by incorporating context-aware,
cross-dimension, and cross-scale constraints. Our architecture integrates three
lightweight modules into a unified rendering pipeline, improving feature fusion
and enabling photorealistic synthesis without requiring additional supervision.
Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS
achieves state-of-the-art rendering quality and generalization ability. Code is
available at: https://github.com/YuhsiHu/C3-GS.

</details>


### [62] [SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding](https://arxiv.org/abs/2508.20758)
*Jiawen Lin,Shiran Bian,Yihang Zhu,Wenbin Tan,Yachao Zhang,Yuan Xie,Yanyun Qu*

Main category: cs.CV

TL;DR: SeqVLM用多视图投影保留空间与上下文，结合VLM动态调度，实现更强的零样本3D视觉定位，在两个基准上显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有零-shot 3DVG依赖单视角导致空间推理受限且上下文或细节丢失，SeqVLM通过多视图和投影策略弥补这些不足，提高泛化和实用性。

Method: 方法包括：1) 用3D语义分割生成实例提议并语义过滤；2) 提案引导的多视图投影，将点云候选投影到图像序列以保留细节与空间关系；3) 动态调度机制按序列-查询迭代调用VLM进行跨模态推理以定位文本描述对象。

Result: SeqVLM提出了一种基于多视图图像序列与空间信息的零-shot 3D视觉定位方法，通过3D语义分割生成实例候选并进行语义过滤，再将候选投影到多视图真实场景图像上，结合动态调度的VLM推理实现跨模态检索。实验在ScanRefer和Nr3D上取得领先的Acc@0.25。

Conclusion: SeqVLM显著提升零-shot 3DVG性能，克服单视角限制与上下文丢失，展示了将真实场景多视图与VLM结合的有效性。

Abstract: 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes using
natural language descriptions. Although supervised methods achieve higher
accuracy in constrained settings, zero-shot 3DVG holds greater promise for
real-world applications since eliminating scene-specific training requirements.
However, existing zero-shot methods face challenges of spatial-limited
reasoning due to reliance on single-view localization, and contextual omissions
or detail degradation. To address these issues, we propose SeqVLM, a novel
zero-shot 3DVG framework that leverages multi-view real-world scene images with
spatial information for target object reasoning. Specifically, SeqVLM first
generates 3D instance proposals via a 3D semantic segmentation network and
refines them through semantic filtering, retaining only semantic-relevant
candidates. A proposal-guided multi-view projection strategy then projects
these candidate proposals onto real scene image sequences, preserving spatial
relationships and contextual details in the conversion process of 3D point
cloud to images. Furthermore, to mitigate VLM computational overload, we
implement a dynamic scheduling mechanism that iteratively processes
sequances-query prompts, leveraging VLM's cross-modal reasoning capabilities to
identify textually specified objects. Experiments on the ScanRefer and Nr3D
benchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scores
of 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,
respectively, which advance 3DVG toward greater generalization and real-world
applicability. The code is available at https://github.com/JiawLin/SeqVLM.

</details>


### [63] [Occlusion Robustness of CLIP for Military Vehicle Classification](https://arxiv.org/abs/2508.20760)
*Jan Erik van Woerden,Gertjan Burghouts,Lotte Nijskens,Alma M. Liezenga,Sabina van Rooij,Frank Ruis,Hugo J. Kuijf*

Main category: cs.CV

TL;DR: Transformer CLIP models beat CNNs; fine-grained dispersed occlusions hurt more than large contiguous ones; linear probes fail sharply at ~35% occlusion but fine-tuning backbone pushes failure to >60%; occlusion-aware augmentation and patch-level robustness needed.


<details>
  <summary>Details</summary>
Motivation: Assess CLIP's practical robustness in military scenarios with occlusion and low SNR where labeled data are scarce, to guide deployment and training strategies.

Method: Built a custom dataset of 18 military vehicle classes; applied controlled occlusions varying percentage and type (fine-grained dispersed vs large contiguous); evaluated CLIP variants (Transformer and CNN backbones) with zero-shot, linear-probe, and fine-tuned backbones; measured performance via Normalized AUC across occlusion percentages.

Result: Paper evaluates CLIP variants' robustness to occlusion on 18 military vehicle classes using NAUC across occlusion percentages.

Conclusion: Transformer backbones provide superior robustness; training with occlusion augmentations and fine-tuning backbone substantially improves tolerance to high occlusion; further work needed on patch sensitivity and architecture design for deployment.

Abstract: Vision-language models (VLMs) like CLIP enable zero-shot classification by
aligning images and text in a shared embedding space, offering advantages for
defense applications with scarce labeled data. However, CLIP's robustness in
challenging military environments, with partial occlusion and degraded
signal-to-noise ratio (SNR), remains underexplored. We investigate CLIP
variants' robustness to occlusion using a custom dataset of 18 military vehicle
classes and evaluate using Normalized Area Under the Curve (NAUC) across
occlusion percentages. Four key insights emerge: (1) Transformer-based CLIP
models consistently outperform CNNs, (2) fine-grained, dispersed occlusions
degrade performance more than larger contiguous occlusions, (3) despite
improved accuracy, performance of linear-probed models sharply drops at around
35% occlusion, (4) by finetuning the model's backbone, this performance drop
occurs at more than 60% occlusion. These results underscore the importance of
occlusion-specific augmentations during training and the need for further
exploration into patch-level sensitivity and architectural resilience for
real-world deployment of CLIP.

</details>


### [64] [SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer](https://arxiv.org/abs/2508.20762)
*Fachri Najm Noer Kartiman,Rasim,Yaya Wihardi,Nurul Hasanah,Oskar Natan,Bambang Wahono,Taufik Ibnu Salim*

Main category: cs.CV

TL;DR: 提出了一种名为SKGE-Swin的端到端自动驾驶模型，结合Swin Transformer与跳层机制以增强像素到像素的上下文感知，利用移位窗口自注意力捕获远距离像素关系并通过跳层保留关键信息。模型在CARLA对抗场景中评估，驾驶得分优于先前方法，并计划通过消融研究验证各组件贡献。


<details>
  <summary>Details</summary>
Motivation: 提升端到端自动驾驶模型的像素级上下文感知能力，能够捕获长距离像素依赖并保留关键特征，进而在复杂、对抗性环境中做出更稳健的驾驶决策。

Method: 引入Swin Transformer作为主干，利用其Shifted Window-MSA实现远距离像素关系建模；在网络不同阶段加入skip-stage（跳层）机制，保持早期至晚期特征的传递与融合；端到端训练在CARLA仿真平台的对抗场景中进行评估，采用Driving Score等指标比较性能并做消融分析。

Result: 在CARLA对抗场景下的实验表明，SKGE-Swin在Driving Score上优于先前方法；计划通过消融研究展示skip-stage与Swin Transformer各自对性能的贡献。

Conclusion: SKGE-Swin通过将Swin Transformer与跳层连接结合，显著提升了端到端自动驾驶模型对复杂环境的感知与驾驶性能，在CARLA对抗场景中取得优于既有方法的Driving Score，消融实验将进一步证实各模块的有效性。

Abstract: Focusing on the development of an end-to-end autonomous vehicle model with
pixel-to-pixel context awareness, this research proposes the SKGE-Swin
architecture. This architecture utilizes the Swin Transformer with a skip-stage
mechanism to broaden feature representation globally and at various network
levels. This approach enables the model to extract information from distant
pixels by leveraging the Swin Transformer's Shifted Window-based Multi-head
Self-Attention (SW-MSA) mechanism and to retain critical information from the
initial to the final stages of feature extraction, thereby enhancing its
capability to comprehend complex patterns in the vehicle's surroundings. The
model is evaluated on the CARLA platform using adversarial scenarios to
simulate real-world conditions. Experimental results demonstrate that the
SKGE-Swin architecture achieves a superior Driving Score compared to previous
methods. Furthermore, an ablation study will be conducted to evaluate the
contribution of each architectural component, including the influence of skip
connections and the use of the Swin Transformer, in improving model
performance.

</details>


### [65] [Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding](https://arxiv.org/abs/2508.20765)
*Gowreesh Mago,Pascal Mettes,Stevan Rudinac*

Main category: cs.CV

TL;DR: 该综述论文讨论了视频中抽象概念理解的挑战与研究进展，强调基础模型（foundation models）为解决该问题提供了新机遇，汇总了相关任务与数据集，并建议借鉴长期研究经验以避免重复劳动。


<details>
  <summary>Details</summary>
Motivation: 尽管当前模型能识别具体视觉元素，但缺乏对正义、自由等抽象概念的理解；随着基础模型发展，亟需将抽象概念理解纳入视频理解研究，使模型更符合人类推理与价值观。

Method: 论文以综述形式，梳理了用于视频抽象概念理解的任务与数据集，分析历史上研究方法的演进，并讨论基础模型如何应用于该领域的潜力与实践问题。

Result: 通过分析任务与数据集，作者发现研究周期性出现、方法不断利用当时可用工具；提出以基础模型为契机，整合社区经验，推动抽象概念理解研究发展。

Conclusion: 作者认为借助多模态基础模型，有望推进视频中抽象概念理解的研究；呼吁结合过去几十年的社区经验和现有工具，系统化地解决这一长期未决的挑战。

Abstract: The automatic understanding of video content is advancing rapidly. Empowered
by deeper neural networks and large datasets, machines are increasingly capable
of understanding what is concretely visible in video frames, whether it be
objects, actions, events, or scenes. In comparison, humans retain a unique
ability to also look beyond concrete entities and recognize abstract concepts
like justice, freedom, and togetherness. Abstract concept recognition forms a
crucial open challenge in video understanding, where reasoning on multiple
semantic levels based on contextual information is key. In this paper, we argue
that the recent advances in foundation models make for an ideal setting to
address abstract concept understanding in videos. Automated understanding of
high-level abstract concepts is imperative as it enables models to be more
aligned with human reasoning and values. In this survey, we study different
tasks and datasets used to understand abstract concepts in video content. We
observe that, periodically and over a long period, researchers have attempted
to solve these tasks, making the best use of the tools available at their
disposal. We advocate that drawing on decades of community experience will help
us shed light on this important open grand challenge and avoid ``re-inventing
the wheel'' as we start revisiting it in the era of multi-modal foundation
models.

</details>


### [66] [Safer Skin Lesion Classification with Global Class Activation Probability Map Evaluation and SafeML](https://arxiv.org/abs/2508.20776)
*Kuniko Paxton,Koorosh Aslansefat,Amila Akagić,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出一种对所有类别像素级概率化的CAM评估方法并结合SafeML报警，提升皮肤病变模型的可解释性与诊断安全性，在ISIC上用MobileNetV2和ViT验证并显示改进。


<details>
  <summary>Details</summary>
Motivation: 尽管皮肤病变分类模型准确率已接近或超过专家，但临床中仍对AI诊断存在不信任，原因在于现有可解释性方法可靠性不足（LIME不稳定，CAM未覆盖所有类别），从而需要更可靠、全面的可解释性与错误检测机制。

Method: 对所有类别的类激活图（CAM）进行像素级概率化建模与统一可视化，计算每个像素属于各个候选类别的激活概率分布，从而得到全局一致的解释图；同时引入SafeML对模型决策的不确定性/异常进行判断并生成报警机制。实验在ISIC数据集上，基于MobileNetV2和Vision Transformer两类主流架构进行对比验证。

Result: 方法在ISIC数据集上验证表明：1) 提供了更全面且像素级一致的可视化解释，能覆盖所有候选类别；2) 结合SafeML能更有效地识别错误预测并发出预警，提升诊断可靠性；3) 在MobileNetV2和ViT上均能适配，实验展示了可视化质量与误诊检测的改进。

Conclusion: 本文提出了一种基于概率化的全类激活图评估方法（Global Class Activation Probabilistic Map Evaluation，GCAP-ME），并结合SafeML检测机制，提高皮肤病变分类结果的可解释性与诊断可信度。

Abstract: Recent advancements in skin lesion classification models have significantly
improved accuracy, with some models even surpassing dermatologists' diagnostic
performance. However, in medical practice, distrust in AI models remains a
challenge. Beyond high accuracy, trustworthy, explainable diagnoses are
essential. Existing explainability methods have reliability issues, with
LIME-based methods suffering from inconsistency, while CAM-based methods
failing to consider all classes. To address these limitations, we propose
Global Class Activation Probabilistic Map Evaluation, a method that analyses
all classes' activation probability maps probabilistically and at a pixel
level. By visualizing the diagnostic process in a unified manner, it helps
reduce the risk of misdiagnosis. Furthermore, the application of SafeML
enhances the detection of false diagnoses and issues warnings to doctors and
patients as needed, improving diagnostic reliability and ultimately patient
safety. We evaluated our method using the ISIC datasets with MobileNetV2 and
Vision Transformers.

</details>


### [67] [Evaluating Compositional Generalisation in VLMs and Diffusion Models](https://arxiv.org/abs/2508.20783)
*Beth Pearson,Bilal Boulbarss,Michael Wray,Martha Lewis*

Main category: cs.CV

TL;DR: 研究比较扩散生成分类器与CLIP、ViLT的组合泛化能力：扩散分类器与ViLT能较好绑定属性与对象，但所有模型在关系性广义零样本任务中表现差，关系概念表示过于相似可能是原因。


<details>
  <summary>Details</summary>
Motivation: 探讨VLM是否能通过组合已知部分形成新意义（组合泛化），特别是比较生成式Diffusion Classifier与判别式模型在组合语义任务上的能力。

Method: 评估了Diffusion Classifier、CLIP和ViLT在概念绑定和关系性任务上的零样本与广义零样本性能，使用图像-文本匹配与分类指标，并分析CLIP嵌入的相似性以解释错误来源。

Result: Diffusion Classifier和ViLT在属性与对象绑定任务（如颜色-形状组合）上表现较好，但在需要关系推理的GZSL任务（如左右关系）上所有模型均失败，CLIP嵌入显示关系概念表示过于相似。

Conclusion: Diffusion Classifier在概念绑定任务上与ViLT表现良好，优于或可比于CLIP，但所有模型在关系性GZSL任务上都表现很差，表明VLM在关系推理上存在普遍挑战。

Abstract: A fundamental aspect of the semantics of natural language is that novel
meanings can be formed from the composition of previously known parts.
Vision-language models (VLMs) have made significant progress in recent years,
however, there is evidence that they are unable to perform this kind of
composition. For example, given an image of a red cube and a blue cylinder, a
VLM such as CLIP is likely to incorrectly label the image as a red cylinder or
a blue cube, indicating it represents the image as a `bag-of-words' and fails
to capture compositional semantics. Diffusion models have recently gained
significant attention for their impressive generative abilities, and zero-shot
classifiers based on diffusion models have been shown to perform competitively
with CLIP in certain compositional tasks. In this work we explore whether the
generative Diffusion Classifier has improved compositional generalisation
abilities compared to discriminative models. We assess three models --
Diffusion Classifier, CLIP, and ViLT -- on their ability to bind objects with
attributes and relations in both zero-shot learning (ZSL) and generalised
zero-shot learning (GZSL) settings. Our results show that the Diffusion
Classifier and ViLT perform well at concept binding tasks, but that all models
struggle significantly with the relational GZSL task, underscoring the broader
challenges VLMs face with relational reasoning. Analysis of CLIP embeddings
suggests that the difficulty may stem from overly similar representations of
relational concepts such as left and right. Code and dataset are available at:
https://github.com/otmive/diffusion_classifier_clip

</details>


### [68] [Surfel-based 3D Registration with Equivariant SE(3) Features](https://arxiv.org/abs/2508.20789)
*Xueyang Kang,Hang Zhao,Kourosh Khoshelham,Patrick Vandewalle*

Main category: cs.CV

TL;DR: 本文提出一种基于surfel的位姿回归方法，通过在SE(3)等变卷积核上学习位置与旋转的显式等变特征，从而在点云配准中提升对噪声和激烈旋转（如正交变换）的鲁棒性，能从Lidar点云使用虚拟透视相机初始化surfel，并通过等变编码器、交叉注意力相似性计算、全连接解码和Huber损失预测相对变换。实验在室内和室外数据集上显示优于现有方法并对真实扫描具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于点云的配准方法忽视了点的朝向与不确定性，因而对噪声和剧烈旋转敏感，需大量变换增强；为此作者引入surfel与SE(3)等变特征以直接建模旋转信息与不确定性，提升鲁棒性。

Method: 从Lidar点云用虚拟透视相机参数初始化surfel，使用SE(3)等变卷积编码器同时学习位置与旋转特征，接着用交叉注意力模块计算源/目标相似性，最后通过全连接解码器预测相对SE(3)变换并以非线性Huber损失优化。

Result: 在室内和室外数据集以及真实Lidar扫描上，提出的方法在配准精度和鲁棒性上优于现有非学习和学习方法，特别是在存在噪声与正交/剧烈旋转时表现更稳定。

Conclusion: 引入surfel表示与SE(3)等变特征学习能显著提升点云配准在噪声和大旋转下的鲁棒性，无需大量变换增强即可稳定预测源-目标之间的相对变换，在多数据集评估中优于现有最先进方法。

Abstract: Point cloud registration is crucial for ensuring 3D alignment consistency of
multiple local point clouds in 3D reconstruction for remote sensing or digital
heritage. While various point cloud-based registration methods exist, both
non-learning and learning-based, they ignore point orientations and point
uncertainties, making the model susceptible to noisy input and aggressive
rotations of the input point cloud like orthogonal transformation; thus, it
necessitates extensive training point clouds with transformation augmentations.
To address these issues, we propose a novel surfel-based pose learning
regression approach. Our method can initialize surfels from Lidar point cloud
using virtual perspective camera parameters, and learns explicit
$\mathbf{SE(3)}$ equivariant features, including both position and rotation
through $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relative
transformation between source and target scans. The model comprises an
equivariant convolutional encoder, a cross-attention mechanism for similarity
computation, a fully-connected decoder, and a non-linear Huber loss.
Experimental results on indoor and outdoor datasets demonstrate our model
superiority and robust performance on real point-cloud scans compared to
state-of-the-art methods.

</details>


### [69] [Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training](https://arxiv.org/abs/2508.20813)
*Tao Luo,Han Wu,Tong Yang,Dinggang Shen,Zhiming Cui*

Main category: cs.CV

TL;DR: 提出DVCTNet，一种模仿牙医诊断流程的双视图共训练网络：先做牙齿检测得到全局（全景）和局部（牙齿裁剪）两视图，分别预训练视觉基础模型，用门控跨视图注意力模块融合双视图特征并回写检测主干，提升了龋齿检测精度；在公共数据集与新构建的高精度数据集上均优于SOTA，代码和数据开源。


<details>
  <summary>Details</summary>
Motivation: 临床上牙医通常结合全景筛查与逐牙局部观察来做诊断。受此启发，利用全局与局部互补信息可改善因对比细微和病变形态多变导致的检测精度不足问题。

Method: 先用自动牙齿检测器构建两视图：全景图（global）与裁剪牙齿图（local）。分别在两视图上预训练两个视觉基础模型：global模型作为检测主干生成候选框与全局特征，local模型提取对应候选框匹配的牙齿局部特征。提出门控跨视图注意力（GCV-Atten）模块动态融合双视图特征，并将融合特征回写到检测模型以输出最终龋齿检测结果。

Result: 在公开数据集和作者新构建的双重验证（口内图+全景）高精度数据集上，DVCTNet在检测指标上均超过现有SOTA方法，证明了方法有效性与潜在临床可用性。

Conclusion: DVCTNet通过双视图共训练与门控跨视图注意力有效融合全局与局部信息，显著提升了全景X光片上的牙齿龋齿检测性能，在公共与高精度数据集上均优于现有方法，具备临床应用潜力。

Abstract: Accurate dental caries detection from panoramic X-rays plays a pivotal role
in preventing lesion progression. However, current detection methods often
yield suboptimal accuracy due to subtle contrast variations and diverse lesion
morphology of dental caries. In this work, inspired by the clinical workflow
where dentists systematically combine whole-image screening with detailed
tooth-level inspection, we present DVCTNet, a novel Dual-View Co-Training
network for accurate dental caries detection. Our DVCTNet starts with employing
automated tooth detection to establish two complementary views: a global view
from panoramic X-ray images and a local view from cropped tooth images. We then
pretrain two vision foundation models separately on the two views. The
global-view foundation model serves as the detection backbone, generating
region proposals and global features, while the local-view model extracts
detailed features from corresponding cropped tooth patches matched by the
region proposals. To effectively integrate information from both views, we
introduce a Gated Cross-View Attention (GCV-Atten) module that dynamically
fuses dual-view features, enhancing the detection pipeline by integrating the
fused features back into the detection model for final caries detection. To
rigorously evaluate our DVCTNet, we test it on a public dataset and further
validate its performance on a newly curated, high-precision dental caries
detection dataset, annotated using both intra-oral images and panoramic X-rays
for double verification. Experimental results demonstrate DVCTNet's superior
performance against existing state-of-the-art (SOTA) methods on both datasets,
indicating the clinical applicability of our method. Our code and labeled
dataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.

</details>


### [70] [FusionCounting: Robust visible-infrared image fusion guided by crowd counting via multi-task learning](https://arxiv.org/abs/2508.20817)
*He Li,Xinyu Liu,Weihang Kong,Xingchen Zhang*

Main category: cs.CV

TL;DR: 提出将人群计数与可见光-红外图像融合联合训练的FusionCounting，通过动态损失加权与对抗训练，低成本提升融合与计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有VIF方法以提升融合图像质量为主，部分工作引入语义任务（如分割/检测）但受标注成本或遮挡问题限制；人群计数在密集场景中标注成本低且能提供直接的密度信息，适合为VIF提供语义引导。

Method: 构建一个联合训练的多任务网络，输入可见光与红外图像并同时预测融合图像与人群密度图；采用动态损失权重策略平衡任务贡献；引入对抗训练以增强鲁棒性。

Result: 在公开数据集上的实验表明，FusionCounting在图像融合质量和人群计数精度上均优于对比方法，且对抗训练提升了模型稳定性和对抗鲁棒性。

Conclusion: 该文提出了FusionCounting，一种将可见光-红外图像融合（VIF）与人群计数联合的多任务框架，通过引入计数任务提供低成本的语义密度监督，从而提升融合质量与计数性能。

Abstract: Most visible and infrared image fusion (VIF) methods focus primarily on
optimizing fused image quality. Recent studies have begun incorporating
downstream tasks, such as semantic segmentation and object detection, to
provide semantic guidance for VIF. However, semantic segmentation requires
extensive annotations, while object detection, despite reducing annotation
efforts compared with segmentation, faces challenges in highly crowded scenes
due to overlapping bounding boxes and occlusion. Moreover, although RGB-T crowd
counting has gained increasing attention in recent years, no studies have
integrated VIF and crowd counting into a unified framework. To address these
challenges, we propose FusionCounting, a novel multi-task learning framework
that integrates crowd counting into the VIF process. Crowd counting provides a
direct quantitative measure of population density with minimal annotation,
making it particularly suitable for dense scenes. Our framework leverages both
input images and population density information in a mutually beneficial
multi-task design. To accelerate convergence and balance tasks contributions,
we introduce a dynamic loss function weighting strategy. Furthermore, we
incorporate adversarial training to enhance the robustness of both VIF and
crowd counting, improving the model's stability and resilience to adversarial
attacks. Experimental results on public datasets demonstrate that
FusionCounting not only enhances image fusion quality but also achieves
superior crowd counting performance.

</details>


### [71] [Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation](https://arxiv.org/abs/2508.20830)
*Krit Duangprom,Tryphon Lambrou,Binod Bhattarai*

Main category: cs.CV

TL;DR: 利用LoRA微调视觉-语言模型(VLM)进行手术器械2D关键点估计；通过精心设计的prompt构建指令微调数据集，只需两轮训练即优于基线，适合小样本场景，并可扩展到3D位姿估计。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/Transformer在小规模医疗数据上易过拟合，需利用预训练VLM的泛化能力在低资源下提升关键点检测。

Method: 将预训练VLM通过LoRA进行低秩适配；设计提示(prompt)生成指令微调数据，建立视觉特征与语义关键点描述的对齐；仅进行两轮epoch的微调并评估与基线模型对比。

Result: 在仅两轮微调后，适配后的VLM在2D手术器械关键点估计上优于基线模型，证明LoRA在小样本医疗场景的有效性，并指出可扩展应用于3D手部/器械位姿估计。

Conclusion: 基于LoRA的VLM微调能在低资源医疗图像上显著提升2D关键点检测性能，训练效率高且具良好泛化性，为3D手术位姿估计提供可行方向。

Abstract: This paper presents a novel pipeline for 2D keypoint estima- tion of surgical
tools by leveraging Vision Language Models (VLMs) fine- tuned using a low rank
adjusting (LoRA) technique. Unlike traditional Convolutional Neural Network
(CNN) or Transformer-based approaches, which often suffer from overfitting in
small-scale medical datasets, our method harnesses the generalization
capabilities of pre-trained VLMs. We carefully design prompts to create an
instruction-tuning dataset and use them to align visual features with semantic
keypoint descriptions. Experimental results show that with only two epochs of
fine tuning, the adapted VLM outperforms the baseline models, demonstrating the
ef- fectiveness of LoRA in low-resource scenarios. This approach not only
improves keypoint detection performance, but also paves the way for future work
in 3D surgical hands and tools pose estimation.

</details>


### [72] [PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification](https://arxiv.org/abs/2508.20835)
*Hao Yang,Qianyu Zhou,Haijia Sun,Xiangtai Li,Xuequan Lu,Lizhuang Ma,Shuicheng Yan*

Main category: cs.CV

TL;DR: 本文首次将RWKV应用于域泛化点云分类，针对令牌位移带来的空间失真和Bi-WKV注意力的跨域放大问题，提出自适应几何位移与键分布对齐两模块，兼顾效率与泛化，达成SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 提升点云分类模型在未见域上的泛化能力，利用RWKV架构的线性复杂度与全局感受野优势，克服现有基于卷积、Transformer或Mamba方法在感受野、计算成本与长程依赖建模上的不足。

Method: 在RWKV基础上引入自适应几何令牌位移以捕获局部邻域结构，修改位移以避免空间几何扭曲；设计跨域键特征分布对齐模块，通过对齐各域键特征的统计分布抑制Bi-WKV中指数加权放大的差异。

Result: 提出PointDGRWKV框架，通过自适应几何令牌位移（Adaptive Geometric Token Shift）和跨域键特征分布对齐（Cross-Domain key feature Distribution Alignment）两模块，增强局部空间建模与跨域鲁棒性，同时保持RWKV线性效率；在多个基准上实现了DG PCC的最新性能。

Conclusion: PointDGRWKV有效缓解了RWKV在无结构点云上出现的空间失真与注意力漂移问题，提高了模型的局部几何感知和跨域稳健性，且保持线性复杂度，实验证明其在DG PCC任务上优于现有方法。

Abstract: Domain Generalization (DG) has been recently explored to enhance the
generalizability of Point Cloud Classification (PCC) models toward unseen
domains. Prior works are based on convolutional networks, Transformer or Mamba
architectures, either suffering from limited receptive fields or high
computational cost, or insufficient long-range dependency modeling. RWKV, as an
emerging architecture, possesses superior linear complexity, global receptive
fields, and long-range dependency. In this paper, we present the first work
that studies the generalizability of RWKV models in DG PCC. We find that
directly applying RWKV to DG PCC encounters two significant challenges: RWKV's
fixed direction token shift methods, like Q-Shift, introduce spatial
distortions when applied to unstructured point clouds, weakening local
geometric modeling and reducing robustness. In addition, the Bi-WKV attention
in RWKV amplifies slight cross-domain differences in key distributions through
exponential weighting, leading to attention shifts and degraded generalization.
To this end, we propose PointDGRWKV, the first RWKV-based framework tailored
for DG PCC. It introduces two key modules to enhance spatial modeling and
cross-domain robustness, while maintaining RWKV's linear efficiency. In
particular, we present Adaptive Geometric Token Shift to model local
neighborhood structures to improve geometric context awareness. In addition,
Cross-Domain key feature Distribution Alignment is designed to mitigate
attention drift by aligning key feature distributions across domains. Extensive
experiments on multiple benchmarks demonstrate that PointDGRWKV achieves
state-of-the-art performance on DG PCC.

</details>


### [73] [PathMR: Multimodal Visual Reasoning for Interpretable Pathology Diagnosis](https://arxiv.org/abs/2508.20851)
*Ye Zhang,Yu Zhou,Jingwen Qi,Yongbing Zhang,Simon Puettmann,Finn Wichmann,Larissa Pereira Ferreira,Lara Sichward,Julius Keyl,Sylvia Hartmann,Shuo Zhao,Hongxiao Wang,Xiaowei Xu,Jianxu Chen*

Main category: cs.CV

TL;DR: PathMR: a cell-level multimodal visual reasoning model for pathology that outputs segmentation masks and expert diagnostic text, achieving SOTA on PathGen and GADVR


<details>
  <summary>Details</summary>
Motivation: Improve interpretability and traceability of deep learning for pathology by generating cell-level segmentation and expert-style text explanations in a unified multimodal framework

Method: Proposed method and components

Result: PathMR outperforms SOTA visual reasoning methods on PathGen and new GADVR in text quality, segmentation accuracy, and cross-modal alignment

Conclusion: PathMR improves interpretability for AI-driven pathological diagnosis by aligning pixel-level cell distributions with semantically rich textual explanations, demonstrating superior performance across multiple benchmarks

Abstract: Deep learning based automated pathological diagnosis has markedly improved
diagnostic efficiency and reduced variability between observers, yet its
clinical adoption remains limited by opaque model decisions and a lack of
traceable rationale. To address this, recent multimodal visual reasoning
architectures provide a unified framework that generates segmentation masks at
the pixel level alongside semantically aligned textual explanations. By
localizing lesion regions and producing expert style diagnostic narratives,
these models deliver the transparent and interpretable insights necessary for
dependable AI assisted pathology. Building on these advancements, we propose
PathMR, a cell-level Multimodal visual Reasoning framework for Pathological
image analysis. Given a pathological image and a textual query, PathMR
generates expert-level diagnostic explanations while simultaneously predicting
cell distribution patterns. To benchmark its performance, we evaluated our
approach on the publicly available PathGen dataset as well as on our newly
developed GADVR dataset. Extensive experiments on these two datasets
demonstrate that PathMR consistently outperforms state-of-the-art visual
reasoning methods in text generation quality, segmentation accuracy, and
cross-modal alignment. These results highlight the potential of PathMR for
improving interpretability in AI-driven pathological diagnosis. The code will
be publicly available in https://github.com/zhangye-zoe/PathMR.

</details>


### [74] [Deep Learning Framework for Early Detection of Pancreatic Cancer Using Multi-Modal Medical Imaging Analysis](https://arxiv.org/abs/2508.20877)
*Dennis Slobodzian,Karissa Tilbury,Amir Kordijazi*

Main category: cs.CV

TL;DR: 该论文利用自发荧光+SHG双模态成像与改进ResNet，在小样本、类别不平衡条件下通过冻结预训练层和类别加权训练实现PDAC检测准确率>90%，具有较强临床应用前景。


<details>
  <summary>Details</summary>
Motivation: PDAC诊断常因晚期发现导致极低的生存率，迫切需要一种能够在早期对病理切片进行自动且准确检测的方法以辅助病理学家提高诊断效率和准确性。

Method: 对来自40名患者的样本进行自发荧光和SHG成像，比较6种深度学习架构（包括传统CNN和ViT），最终选择修改的ResNet，使用冻结预训练层和类别加权训练来缓解小数据集和类别不平衡问题。

Result: 在40例样本上，最终模型在癌症识别任务上准确率超过90%，优于手工分析方法，并建立了一套可推广的自动化检测流程。

Conclusion: 该研究成功构建并验证了基于深度学习的双模态成像（自发荧光和二次谐波）PDAC早期检测框架，最终模型在癌症检测上准确率超过90%，具有临床应用潜力。

Abstract: Pacreatic ductal adenocarcinoma (PDAC) remains one of the most lethal forms
of cancer, with a five-year survival rate below 10% primarily due to late
detection. This research develops and validates a deep learning framework for
early PDAC detection through analysis of dual-modality imaging:
autofluorescence and second harmonic generation (SHG). We analyzed 40 unique
patient samples to create a specialized neural network capable of
distinguishing between normal, fibrotic, and cancerous tissue. Our methodology
evaluated six distinct deep learning architectures, comparing traditional
Convolutional Neural Networks (CNNs) with modern Vision Transformers (ViTs).
Through systematic experimentation, we identified and overcome significant
challenges in medical image analysis, including limited dataset size and class
imbalance. The final optimized framework, based on a modified ResNet
architecture with frozen pre-trained layers and class-weighted training,
achieved over 90% accuracy in cancer detection. This represents a significant
improvement over current manual analysis methods an demonstrates potential for
clinical deployment. This work establishes a robust pipeline for automated PDAC
detection that can augment pathologists' capabilities while providing a
foundation for future expansion to other cancer types. The developed
methodology also offers valuable insights for applying deep learning to
limited-size medical imaging datasets, a common challenge in clinical
applications.

</details>


### [75] [Understanding and evaluating computer vision models through the lens of counterfactuals](https://arxiv.org/abs/2508.20881)
*Pushkar Shukla*

Main category: cs.CV

TL;DR: 作者用反事实方法解释与缓解视觉分类器和文本到图像生成模型的偏见，提出CAVLI、ASAC、TIBET、BiasConnect和InterMit等工具，实现可扩展的审计与缓解策略。


<details>
  <summary>Details</summary>
Motivation: 提升对视觉分类器和生成模型中偏见和可解释性的理解，通过反事实分析揭示模型依赖的语义属性和因果关系，从而设计审计和缓解方法。

Method: 通过系统改变语义属性生成反事实样本：CAVLI结合LIME热图与TCAV计算概念依赖分；ASAC生成对抗反事实并用课程学习微调模型以避免刻板印象生成；TIBET批量替换提示中身份词并生成样本以评估提示敏感性；BiasConnect用因果图建模交叉群体属性的影响；InterMit基于因果敏感度分数及用户定义公平目标无训练地调整生成结果。

Result: 提出多种基于反事实的框架：CAVLI（结合LIME和TCAV的概念级归因与热图）、ASAC（基于对抗反事实与课程学习的公平微调）、TIBET（可扩展的提示敏感偏见评估流水线）、BiasConnect（构建交互偏见的因果图）、InterMit（无训练、模块化的交叉偏见缓解算法）。这些方法可发现背景等无关线索、评估提示中身份词的影响、诊断交叉偏见并在不同设定下缓解偏见。

Conclusion: 反事实分析是解释性、公平性与因果推断的统一视角，能为判别与生成模型提供可扩展且有原则的偏见评估与缓解方法，推动社会责任感强的模型开发。

Abstract: Counterfactual reasoning -- the practice of asking ``what if'' by varying
inputs and observing changes in model behavior -- has become central to
interpretable and fair AI. This thesis develops frameworks that use
counterfactuals to explain, audit, and mitigate bias in vision classifiers and
generative models. By systematically altering semantically meaningful
attributes while holding others fixed, these methods uncover spurious
correlations, probe causal dependencies, and help build more robust systems.
  The first part addresses vision classifiers. CAVLI integrates attribution
(LIME) with concept-level analysis (TCAV) to quantify how strongly decisions
rely on human-interpretable concepts. With localized heatmaps and a Concept
Dependency Score, CAVLI shows when models depend on irrelevant cues like
backgrounds. Extending this, ASAC introduces adversarial counterfactuals that
perturb protected attributes while preserving semantics. Through curriculum
learning, ASAC fine-tunes biased models for improved fairness and accuracy
while avoiding stereotype-laden artifacts.
  The second part targets generative Text-to-Image (TTI) models. TIBET provides
a scalable pipeline for evaluating prompt-sensitive biases by varying
identity-related terms, enabling causal auditing of how race, gender, and age
affect image generation. To capture interactions, BiasConnect builds causal
graphs diagnosing intersectional biases. Finally, InterMit offers a modular,
training-free algorithm that mitigates intersectional bias via causal
sensitivity scores and user-defined fairness goals.
  Together, these contributions show counterfactuals as a unifying lens for
interpretability, fairness, and causality in both discriminative and generative
models, establishing principled, scalable methods for socially responsible bias
evaluation and mitigation.

</details>


### [76] [To New Beginnings: A Survey of Unified Perception in Autonomous Vehicle Software](https://arxiv.org/abs/2508.20892)
*Loïc Stratil,Felix Fent,Esteban Rivera,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文综述并分类统一感知研究，定义Early/Late/Full三种范式，评估方法、数据与开源情况，旨在引导更鲁棒和可解释的自动驾驶感知研究。


<details>
  <summary>Details</summary>
Motivation: 传统模块化感知流水线虽具可解释性，但存在误差累积和子任务间协同不足的问题；为此需要统一感知范式来提升鲁棒性、上下文推理能力和效率，同时保持可解释输出。

Method: 作者通过构建一个基于任务整合方式、追踪表述和表示流的三维分类法，对现有统一感知方法进行系统性综述，并定义了Early、Late、Full三种统一范式，比较各类方法的架构、训练策略与数据集使用情况。

Result: 综述总结了统一感知领域的现有方法、公开数据集和开源实现，形成首个全面框架，指出研究空白与未来方向，如改进多任务融合策略、更有效的联合训练及可扩展的表示形式。

Conclusion: 本文提出的统一感知框架为自动驾驶感知任务提供了一种系统化的整合方法，能够缓解误差累积并增强任务间协同，同时保留可解释性，但在实现上仍需解决综合性能、训练标注以及推理效率等挑战。

Abstract: Autonomous vehicle perception typically relies on modular pipelines that
decompose the task into detection, tracking, and prediction. While
interpretable, these pipelines suffer from error accumulation and limited
inter-task synergy. Unified perception has emerged as a promising paradigm that
integrates these sub-tasks within a shared architecture, potentially improving
robustness, contextual reasoning, and efficiency while retaining interpretable
outputs. In this survey, we provide a comprehensive overview of unified
perception, introducing a holistic and systemic taxonomy that categorizes
methods along task integration, tracking formulation, and representation flow.
We define three paradigms -Early, Late, and Full Unified Perception- and
systematically review existing methods, their architectures, training
strategies, datasets used, and open-source availability, while highlighting
future research directions. This work establishes the first comprehensive
framework for understanding and advancing unified perception, consolidates
fragmented efforts, and guides future research toward more robust,
generalizable, and interpretable perception.

</details>


### [77] [Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation](https://arxiv.org/abs/2508.20909)
*Yifan Gao,Haoyue Li,Feng Yuan,Xiaosong Wang,Xin Gao*

Main category: cs.CV

TL;DR: 提出Dino U-Net，一种基于冻结DINOv3骨干的编码器-解码器架构，通过适配器和保真度感知投影模块（FAPM）融合高语义密集特征与低层空间细节，在7个公共医学影像分割数据集上取得SOTA，且随着骨干规模增大性能持续提升。


<details>
  <summary>Details</summary>
Motivation: 大规模自然图像上预训练的视觉基础模型（如DINOv3）拥有高保真的密集特征，如何有效迁移这些表征以满足医学图像分割对精细局部结构和临床精度的需求是关键挑战。

Method: 在冻结的DINOv3骨干上构建编码器，加入专用适配器以融合语义与低层空间信息；设计保真度感知投影模块(FAPM)在降维时保持特征质量并精炼后送入解码器；采用编码器-解码器结构进行端到端分割训练，评估7个公开医疗分割数据集并比较不同骨干规模。

Result: 在7个不同模态的医学分割数据集上，Dino U-Net超越之前方法成为SOTA，且随着DINOv3骨干参数量增加（最高到7B），分割准确率持续提升；实现参数高效且可扩展的迁移方案。

Conclusion: 利用DINOv3的高质量密集预训练特征并通过适配器与FAPM保真投影到解码器，可显著提升医学图像分割精度且参数高效，模型尺度增大会进一步带来性能提升。

Abstract: Foundation models pre-trained on large-scale natural image datasets offer a
powerful paradigm for medical image segmentation. However, effectively
transferring their learned representations for precise clinical applications
remains a challenge. In this work, we propose Dino U-Net, a novel
encoder-decoder architecture designed to exploit the high-fidelity dense
features of the DINOv3 vision foundation model. Our architecture introduces an
encoder built upon a frozen DINOv3 backbone, which employs a specialized
adapter to fuse the model's rich semantic features with low-level spatial
details. To preserve the quality of these representations during dimensionality
reduction, we design a new fidelity-aware projection module (FAPM) that
effectively refines and projects the features for the decoder. We conducted
extensive experiments on seven diverse public medical image segmentation
datasets. Our results show that Dino U-Net achieves state-of-the-art
performance, consistently outperforming previous methods across various imaging
modalities. Our framework proves to be highly scalable, with segmentation
accuracy consistently improving as the backbone model size increases up to the
7-billion-parameter variant. The findings demonstrate that leveraging the
superior, dense-pretrained features from a general-purpose foundation model
provides a highly effective and parameter-efficient approach to advance the
accuracy of medical image segmentation. The code is available at
https://github.com/yifangao112/DinoUNet.

</details>


### [78] [Classifying Mitotic Figures in the MIDOG25 Challenge with Deep Ensemble Learning and Rule Based Refinement](https://arxiv.org/abs/2508.20919)
*Sara Krauss,Ellena Spieß,Daniel Hieber,Frank Kramer,Johannes Schobel,Dominik Müller*

Main category: cs.CV

TL;DR: 基于ConvNeXt的模型集成能有效区分AMF与NMF，规则精化提高了特异性但代价是灵敏度下降，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 区分非典型和正常有丝分裂体以改进肿瘤分级的准确性，减轻人工标注的主观性和工作量。

Method: 训练ConvNeXtBase模型集成，基于AUCMEDI数据集；追加一个基于规则的精化模块用于调整模型预测。

Result: 使用ConvNeXtBase模型的深度学习集成在MIDOG25初步测试集上达到84.02%的平衡准确率；加入基于规则的精化模块提升了特异性但降低了灵敏度和整体表现。

Conclusion: 深度学习集成是区分AMF与NMF的有效方法；基于规则的精化在特定指标上有利但会牺牲敏感性，需优化或寻找替代策略。

Abstract: Mitotic figures (MFs) are relevant biomarkers in tumor grading.
Differentiating atypical MFs (AMFs) from normal MFs (NMFs) remains difficult,
as manual annotation is time-consuming and subjective. In this work an ensemble
of ConvNeXtBase models was trained with AUCMEDI and extend with a rule-based
refinement (RBR) module. On the MIDOG25 preliminary test set, the ensemble
achieved a balanced accuracy of 84.02%. While the RBR increased specificity, it
reduced sensitivity and overall performance. The results show that deep
ensembles perform well for AMF classification. RBR can increase specific
metrics but requires further research.

</details>


### [79] [COMETH: Convex Optimization for Multiview Estimation and Tracking of Humans](https://arxiv.org/abs/2508.20920)
*Enrico Martini,Ho Jin Choi,Nadia Figueroa,Nicola Bombieri*

Main category: cs.CV

TL;DR: 提出COMETH，一种轻量级实时多视角人体姿态融合方法，结合运动学/生物力学约束、凸优化逆运动学和状态观测器，在边缘分布式场景下提高定位、检测与跟踪精度，适用于工业安全。


<details>
  <summary>Details</summary>
Motivation: 在Industry 5.0中需要可扩展且实时的人体活动监测，但集中式多摄系统受带宽/计算限制，边缘分布式虽减负却带来精度与时空不一致，需一种轻量高效的融合算法。

Method: 方法结合三部分：1) 将运动学与生物力学约束加入到多视角融合中以提升关节定位精度；2) 使用基于凸优化的逆运动学进行空间融合，确保解的全局最优与稳定性；3) 设计状态观测器提高时间一致性与轨迹平滑性。系统面向实时操作并适配边缘设备分布式计算。

Result: 在多个公开和工业数据集上，COMETH在定位、检测和跟踪指标上均超越最先进方法，展示出更高的准确性、实时性与可扩展性；代码已开源。

Conclusion: COMETH通过引入生物力学与运动学约束、凸优化融合以及状态观测器，有效缓解了边缘设备带来的精度下降与时空不一致问题，在公开和工业数据集上均优于现有方法，适合可扩展的工业场景和安全关键应用。

Abstract: In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.

</details>


### [80] [Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement](https://arxiv.org/abs/2508.20954)
*Amir Jmal,Chaima Chtourou,Mahdi Louati,Abdelaziz Kallel,Houda Khmila*

Main category: cs.CV

TL;DR: Integrate SAM with field-alignment corrections and a learnable shape/size constraint to boost olive tree segmentation accuracy from 82% to 98% in satellite images


<details>
  <summary>Details</summary>
Motivation: Preserve olive biodiversity by early anomaly detection and management; improve olive tree segmentation in satellite imagery for remote sensing applications

Method: Segment Anything Model (SAM)-based olive tree segmentation with shape and alignment constraints

Result: Achieved 98% accuracy after applying SAM segmentation plus alignment corrections and a learnable shape/size constraint, improving from initial 82% SAM performance

Conclusion: Combining foundational segmentation models with domain-specific corrections and learnable geometric constraints greatly improves olive tree segmentation performance, aiding biodiversity monitoring

Abstract: In the context of proven climate change, maintaining olive biodiversity
through early anomaly detection and treatment using remote sensing technology
is crucial, offering effective management solutions. This paper presents an
innovative approach to olive tree segmentation from satellite images. By
leveraging foundational models and advanced segmentation techniques, the study
integrates the Segment Anything Model (SAM) to accurately identify and segment
olive trees in agricultural plots. The methodology includes SAM segmentation
and corrections based on trees alignement in the field and a learanble
constraint about the shape and the size. Our approach achieved a 98\% accuracy
rate, significantly surpassing the initial SAM performance of 82\%.

</details>


### [81] [E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections](https://arxiv.org/abs/2508.20955)
*Fang Wang,Huitao Li,Wenhan Chao,Zheng Zhuo,Yiran Ji,Chang Peng,Yupeng Sun*

Main category: cs.CV

TL;DR: 本文提出E-ConvNeXt，通过将CSPNet与ConvNeXt结合并优化Stem、Block结构及用通道注意力替代Layer Scale，大幅降低模型复杂度同时保持高精度，实验证明在ImageNet和目标检测任务上表现优异（如E-ConvNeXt-mini: 78.3% Top-1 @0.9GFLOPs）。


<details>
  <summary>Details</summary>
Motivation: 现有高性能网络对轻量级场景支持不足，需在尽量减少参数与运算的同时保持高精度，以扩大实际应用范围。

Method: 将Cross Stage Partial Connections（CSPNet）引入ConvNeXt并调整网络结构以减少重复计算，优化Stem和Block以提升特征表达和效率，并用通道注意力替代Layer Scale来增强通道间表示能力；构建不同复杂度配置并在ImageNet和检测任务上评估。

Result: E-ConvNeXt-mini在ImageNet上达到78.3% Top-1准确率（0.9GFLOPs），E-ConvNeXt-small达到81.9%（3.1GFLOPs），并在迁移到目标检测任务中表现出良好泛化。

Conclusion: E-ConvNeXt在保持准确率的同时显著降低了参数和计算复杂度，适合轻量级场景，并在分类与检测任务中展现出良好泛化能力。

Abstract: Many high-performance networks were not designed with lightweight application
scenarios in mind from the outset, which has greatly restricted their scope of
application. This paper takes ConvNeXt as the research object and significantly
reduces the parameter scale and network complexity of ConvNeXt by integrating
the Cross Stage Partial Connections mechanism and a series of optimized
designs. The new network is named E-ConvNeXt, which can maintain high accuracy
performance under different complexity configurations. The three core
innovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network
(CSPNet) with ConvNeXt and adjusting the network structure, which reduces the
model's network complexity by up to 80%; (2) Optimizing the Stem and Block
structures to enhance the model's feature expression capability and operational
efficiency; (3) Replacing Layer Scale with channel attention. Experimental
validation on ImageNet classification demonstrates E-ConvNeXt's superior
accuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at
0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transfer
learning tests on object detection tasks further confirm its generalization
capability.

</details>


### [82] [DrivingGaussian++: Towards Realistic Reconstruction and Editable Simulation for Surrounding Dynamic Driving Scenes](https://arxiv.org/abs/2508.20965)
*Yajiao Xiong,Xiaoyu Zhou,Yongtao Wan,Deqing Sun,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出DrivingGaussian++，用增量3D高斯和复合动态图高斯重建动态驾驶场景，结合LiDAR先验与LLM，实现高质量重建与训练自由的可控编辑（纹理、天气、物体移动），生成一致的多视角动态场景。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景重建或编辑方法在位置精确、遮挡处理、多视角一致性与可控性上存在不足，DrivingGaussian++旨在提供高效且可控的动态驾驶场景重建与编辑解决方案。

Method: 使用增量3D高斯来表征静态场景，复合动态图高斯图来建模移动物体，融合LiDAR深度先验以增强细节一致性；编辑通过训练-free的方法结合多视角图像与深度先验实现，LLM用于自动生成动态轨迹并在优化中增强其真实感。

Result: DrivingGaussian++提出了用于重建与可控编辑动态自动驾驶场景的新框架，结合增量3D高斯表示静态背景与复合动态图高斯表示移动物体，并引入LiDAR先验与LLM生成运动轨迹，实现训练免费编辑与多视角一致的真实感合成。

Conclusion: DrivingGaussian++在重建质量、物体位置与遮挡处理、以及可控编辑能力上优于现有方法，能够自动生成并优化动态物体轨迹，提升场景多样性与真实感。

Abstract: We present DrivingGaussian++, an efficient and effective framework for
realistic reconstructing and controllable editing of surrounding dynamic
autonomous driving scenes. DrivingGaussian++ models the static background using
incremental 3D Gaussians and reconstructs moving objects with a composite
dynamic Gaussian graph, ensuring accurate positions and occlusions. By
integrating a LiDAR prior, it achieves detailed and consistent scene
reconstruction, outperforming existing methods in dynamic scene reconstruction
and photorealistic surround-view synthesis. DrivingGaussian++ supports
training-free controllable editing for dynamic driving scenes, including
texture modification, weather simulation, and object manipulation, leveraging
multi-view images and depth priors. By integrating large language models (LLMs)
and controllable editing, our method can automatically generate dynamic object
motion trajectories and enhance their realism during the optimization process.
DrivingGaussian++ demonstrates consistent and realistic editing results and
generates dynamic multi-view driving scenarios, while significantly enhancing
scene diversity. More results and code can be found at the project site:
https://xiong-creator.github.io/DrivingGaussian_plus.github.io

</details>


### [83] [Webly-Supervised Image Manipulation Localization via Category-Aware Auto-Annotation](https://arxiv.org/abs/2508.20987)
*Chenfan Qu,Yiwu Zhong,Bin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 通过自动注释与质量过滤把海量网络伪造图像转为高质量像素掩码，构建MIMLv2并提出Web-IML，显著提升图像篡改定位性能


<details>
  <summary>Details</summary>
Motivation: 缓解图像篡改定位中标注数据稀缺问题，利用网络现成伪造图像与辅助任务生成像素级注释

Method: 利用受限图像篡改定位的辅助任务自动生成像素注释（CAAAv2），用QES筛除低质量注释，构建MIMLv2数据集；引入Object Jitter增强训练数据；设计Web-IML以利用网络级监督进行训练并在多基准上验证

Result: 提出CAAAv2自动像素级注释范式、QES质量过滤指标、构建大规模MIMLv2数据集（246,212张有掩码图像）、Object Jitter生成高质量篡改伪影，并设计Web-IML模型；在多项真实伪造基准上显著提升性能，平均IoU提升24.1点，相比无网络监督模型提升31%

Conclusion: 网络规模的伪造图像监督结合自动注释与过滤可有效缓解数据匮乏，显著提升篡改定位模型性能；公开MIMLv2和代码促进后续研究

Abstract: Images manipulated using image editing tools can mislead viewers and pose
significant risks to social security. However, accurately localizing the
manipulated regions within an image remains a challenging problem. One of the
main barriers in this area is the high cost of data acquisition and the severe
lack of high-quality annotated datasets. To address this challenge, we
introduce novel methods that mitigate data scarcity by leveraging readily
available web data. We utilize a large collection of manually forged images
from the web, as well as automatically generated annotations derived from a
simpler auxiliary task, constrained image manipulation localization.
Specifically, we introduce a new paradigm CAAAv2, which automatically and
accurately annotates manipulated regions at the pixel level. To further improve
annotation quality, we propose a novel metric, QES, which filters out
unreliable annotations. Through CAAA v2 and QES, we construct MIMLv2, a
large-scale, diverse, and high-quality dataset containing 246,212 manually
forged images with pixel-level mask annotations. This is over 120x larger than
existing handcrafted datasets like IMD20. Additionally, we introduce Object
Jitter, a technique that further enhances model training by generating
high-quality manipulation artifacts. Building on these advances, we develop a
new model, Web-IML, designed to effectively leverage web-scale supervision for
the image manipulation localization task. Extensive experiments demonstrate
that our approach substantially alleviates the data scarcity problem and
significantly improves the performance of various models on multiple real-world
forgery benchmarks. With the proposed web supervision, Web-IML achieves a
striking performance gain of 31% and surpasses previous SOTA TruFor by 24.1
average IoU points. The dataset and code will be made publicly available at
https://github.com/qcf-568/MIML.

</details>


### [84] [ExpertSim: Fast Particle Detector Simulation Using Mixture-of-Generative-Experts](https://arxiv.org/abs/2508.20991)
*Patryk Będkowski,Jan Dubiński,Filip Szatkowski,Kamil Deja,Przemysław Rokita,Tomasz Trzciński*

Main category: cs.CV

TL;DR: Use a Mixture-of-Generative-Experts model to simulate ALICE ZDC responses more accurately and much faster than Monte Carlo, addressing heterogeneous data distributions; code available online.


<details>
  <summary>Details</summary>
Motivation: Monte Carlo detector simulations are computationally expensive and data distributions vary across simulation conditions, making standard generative models struggle. A specialized, modular generative approach can better capture heterogeneous distributions and reduce computational cost.

Method: Design a Mixture-of-Generative-Experts deep model where each expert is trained/specialized on a subset of calorimeter response data; combine experts (likely via a gating network) to generate full-simulation outputs. Compare accuracy and runtime to Monte Carlo baselines.

Result: ExpertSim improves simulation accuracy for the ZDC and yields significant speedups over Monte Carlo; code is released on GitHub.

Conclusion: ExpertSim demonstrates that a Mixture-of-Generative-Experts architecture can produce accurate and faster simulations for the ALICE Zero Degree Calorimeter, offering a viable alternative to Monte Carlo methods.

Abstract: Simulating detector responses is a crucial part of understanding the inner
workings of particle collisions in the Large Hadron Collider at CERN. Such
simulations are currently performed with statistical Monte Carlo methods, which
are computationally expensive and put a significant strain on CERN's
computational grid. Therefore, recent proposals advocate for generative machine
learning methods to enable more efficient simulations. However, the
distribution of the data varies significantly across the simulations, which is
hard to capture with out-of-the-box methods. In this study, we present
ExpertSim - a deep learning simulation approach tailored for the Zero Degree
Calorimeter in the ALICE experiment. Our method utilizes a
Mixture-of-Generative-Experts architecture, where each expert specializes in
simulating a different subset of the data. This allows for a more precise and
efficient generation process, as each expert focuses on a specific aspect of
the calorimeter response. ExpertSim not only improves accuracy, but also
provides a significant speedup compared to the traditional Monte-Carlo methods,
offering a promising solution for high-efficiency detector simulations in
particle physics experiments at CERN. We make the code available at
https://github.com/patrick-bedkowski/expertsim-mix-of-generative-experts.

</details>


### [85] [ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering](https://arxiv.org/abs/2508.21010)
*Paritosh Parmar,Eric Peh,Basura Fernando*

Main category: cs.CV

TL;DR: 通过生成自然语言因果链并解耦推理与回答，论文在性能和可解释性上都有明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频因果问答模型为黑箱单体管道，难以解释且依赖浅表启发式，难以处理高阶推理。通过模仿人类认知中的因果链结构，显式中间表示可以将低层视频内容与高层因果推理连接起来，增强透明性和逻辑一致性。

Method: 两阶段框架：1) Causal Chain Extractor (CCE) 从视频-问题对生成自然语言因果链；2) Causal Chain-Driven Answerer (CCDA) 基于这些因果链生成答案。同时用大语言模型从现有数据集自动生成高质量因果链作为训练/评估数据，并提出新的评估指标CauCo。

Result: 在三大规模基准上的实验显示方法超越了最先进模型，并在可解释性、用户信任和泛化能力上取得显著增益，且CCE可作为跨领域可复用的因果推理引擎。

Conclusion: 该论文提出将因果推理与答案生成解耦，通过引入自然语言因果链作为可解释的中间表示，提高了可解释性和推理质量。实验证明在多个基准上性能优于现有方法，并提升了可解释性与泛化能力。

Abstract: Existing Causal-Why Video Question Answering (VideoQA) models often struggle
with higher-order reasoning, relying on opaque, monolithic pipelines that
entangle video understanding, causal inference, and answer generation. These
black-box approaches offer limited interpretability and tend to depend on
shallow heuristics. We propose a novel, modular framework that explicitly
decouples causal reasoning from answer generation, introducing natural language
causal chains as interpretable intermediate representations. Inspired by human
cognitive models, these structured cause-effect sequences bridge low-level
video content with high-level causal reasoning, enabling transparent and
logically coherent inference. Our two-stage architecture comprises a Causal
Chain Extractor (CCE) that generates causal chains from video-question pairs,
and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded in
these chains. To address the lack of annotated reasoning traces, we introduce a
scalable method for generating high-quality causal chains from existing
datasets using large language models. We also propose CauCo, a new evaluation
metric for causality-oriented captioning. Experiments on three large-scale
benchmarks demonstrate that our approach not only outperforms state-of-the-art
models, but also yields substantial gains in explainability, user trust, and
generalization -- positioning the CCE as a reusable causal reasoning engine
across diverse domains. Project page:
https://paritoshparmar.github.io/chainreaction/

</details>


### [86] [POSE: Phased One-Step Adversarial Equilibrium for Video Diffusion Models](https://arxiv.org/abs/2508.21019)
*Jiaxiang Cheng,Bing Ma,Xuhua Ren,Hongyi Jin,Kai Yu,Peng Zhang,Wenyue Li,Yuan Zhou,Tianxiang Zheng,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出了一种名为POSE的两阶段单步蒸馏框架，用于大规模视频扩散模型的加速，旨在实现单步生成高质量视频。方法包括稳定性预热、统一对抗均衡以及条件对抗一致性，以稳定单步对抗蒸馏并提升语义和帧一致性。实验在VBench-I2V上相比现有加速方法平均提升约7.15%并将延迟从1000秒降至10秒。


<details>
  <summary>Details</summary>
Motivation: 现有视频加速方法多沿用图像技术，忽视视频帧的时序相关性且缺乏针对大规模视频模型的单步蒸馏方案，导致采样效率低下，难以在长序列和大模型下实现实时生成。POSE旨在填补这一空白，实现高效且稳定的单步视频生成。

Method: POSE采用两阶段蒸馏：(1) 稳定性预热：在一阶到低信噪比区间引导高质量单步生成轨迹，稳定对抗蒸馏过程；(2) 统一对抗均衡：在高斯噪声空间中进行自对抗蒸馏，推动训练趋向纳什均衡，生成接近真实视频的单步结果；此外对条件生成引入(3) 条件对抗一致性以强化语义和帧间一致性。

Result: 在VBench-I2V基准上，POSE在语义对齐、时序一致性与帧质量三项指标上平均提升7.15%，并将预训练模型推理延迟从1000秒缩短到10秒（100×加速），同时保持具有竞争力的生成性能。

Conclusion: POSE能有效将大规模视频扩散模型的采样步骤压缩到单步，同时保持或提升视频质量与时序一致性，显著降低推理延迟，适用于条件与无条件的视频生成任务。

Abstract: The field of video diffusion generation faces critical bottlenecks in
sampling efficiency, especially for large-scale models and long sequences.
Existing video acceleration methods adopt image-based techniques but suffer
from fundamental limitations: they neither model the temporal coherence of
video frames nor provide single-step distillation for large-scale video models.
To bridge this gap, we propose POSE (Phased One-Step Equilibrium), a
distillation framework that reduces the sampling steps of large-scale video
diffusion models, enabling the generation of high-quality videos in a single
step. POSE employs a carefully designed two-phase process to distill video
models:(i) stability priming: a warm-up mechanism to stabilize adversarial
distillation that adapts the high-quality trajectory of the one-step generator
from high to low signal-to-noise ratio regimes, optimizing the video quality of
single-step mappings near the endpoints of flow trajectories. (ii) unified
adversarial equilibrium: a flexible self-adversarial distillation mechanism
that promotes stable single-step adversarial training towards a Nash
equilibrium within the Gaussian noise space, generating realistic single-step
videos close to real videos. For conditional video generation, we propose (iii)
conditional adversarial consistency, a method to improve both semantic
consistency and frame consistency between conditional frames and generated
frames. Comprehensive experiments demonstrate that POSE outperforms other
acceleration methods on VBench-I2V by average 7.15% in semantic alignment,
temporal conference and frame quality, reducing the latency of the pre-trained
model by 100$\times$, from 1000 seconds to 10 seconds, while maintaining
competitive performance.

</details>


### [87] [Reusing Computation in Text-to-Image Diffusion for Efficient Generation of Image Sets](https://arxiv.org/abs/2508.21032)
*Dale Decatur,Thibault Groueix,Wang Yifan,Rana Hanocka,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出一种无训练、聚类相似提示并在扩散早期共享计算的层级化扩散方法，通过利用扩散模型由粗到细的特性减少相似提示间的冗余计算，从而在保持或提升图像质量的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型在质量上表现优秀但计算代价高昂，先前工作多从单次推理优化入手；本文另辟蹊径，通过减少相似提示之间的冗余计算来提升大规模生成的整体效率。

Method: 对提示进行语义聚类，多个相似提示在扩散的早期共享计算（采用更少或合并的扩散步骤），随后在更细粒度的步骤中恢复各自独立处理；结合UnClip的text-to-image prior优化扩散步数分配，无需额外训练。

Result: 实验表明，对于基于图像嵌入条件的模型，该方法在降低计算成本的同时还能提升生成图像质量，且能与现有管线兼容并按提示集规模线性扩展。

Conclusion: 在使用以图像嵌入为条件的模型时，该方法能显著减少计算量并提升图像质量；它无缝集成到现有流水线、可扩展到大规模提示集合，并降低环境与成本开销。

Abstract: Text-to-image diffusion models enable high-quality image generation but are
computationally expensive. While prior work optimizes per-inference efficiency,
we explore an orthogonal approach: reducing redundancy across correlated
prompts. Our method leverages the coarse-to-fine nature of diffusion models,
where early denoising steps capture shared structures among similar prompts. We
propose a training-free approach that clusters prompts based on semantic
similarity and shares computation in early diffusion steps. Experiments show
that for models trained conditioned on image embeddings, our approach
significantly reduces compute cost while improving image quality. By leveraging
UnClip's text-to-image prior, we enhance diffusion step allocation for greater
efficiency. Our method seamlessly integrates with existing pipelines, scales
with prompt sets, and reduces the environmental and financial burden of
large-scale text-to-image generation. Project page:
https://ddecatur.github.io/hierarchical-diffusion/

</details>


### [88] [Mitosis detection in domain shift scenarios: a Mamba-based approach](https://arxiv.org/abs/2508.21033)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: Proposed Mamba-based VM-UNet with stain augmentation for mitosis detection under domain shift; preliminary MIDOG++ results show poor performance, indicating need for further work.


<details>
  <summary>Details</summary>
Motivation: Mitigate performance drop of mitosis detection models under domain shift by leveraging Mamba-inspired architecture and stain augmentations to improve robustness.

Method: VM-UNet with stain augmentation

Result: Submitted to MIDOG track 1; preliminary experiments on MIDOG++ show method needs significant improvements (large room for improvement).

Conclusion: Approach is promising but currently underperforms; future work should refine architecture, augmentation, and training to handle domain variability.

Abstract: Mitosis detection in histopathology images plays a key role in tumor
assessment. Although machine learning algorithms could be exploited for aiding
physicians in accurately performing such a task, these algorithms suffer from
significative performance drop when evaluated on images coming from domains
that are different from the training ones. In this work, we propose a
Mamba-based approach for mitosis detection under domain shift, inspired by the
promising performance demonstrated by Mamba in medical imaging segmentation
tasks. Specifically, our approach exploits a VM-UNet architecture for carrying
out the addressed task, as well as stain augmentation operations for further
improving model robustness against domain shift. Our approach has been
submitted to the track 1 of the MItosis DOmain Generalization (MIDOG)
challenge. Preliminary experiments, conducted on the MIDOG++ dataset, show
large room for improvement for the proposed method.

</details>


### [89] [A multi-task neural network for atypical mitosis recognition under domain shift](https://arxiv.org/abs/2508.21035)
*Gennaro Percannella,Mattia Sarno,Francesco Tortorella,Mario Vento*

Main category: cs.CV

TL;DR: 提出一种基于多任务学习的域泛化方法，通过利用与主分类相关的辅助任务，帮助模型忽略域变化的背景，从而在不同数据集上提升识别异常有丝分裂细胞的鲁棒性，初步评估结果令人鼓舞。


<details>
  <summary>Details</summary>
Motivation: Machine learning models for mitotic figure classification degrade under domain shift due to varying background and staining; using auxiliary tasks can force model to focus on object and ignore domain-specific background.

Method: Multi-task learning for domain generalization in mitosis detection

Result: Proposed multi-task approach achieved promising performance in preliminary evaluation on three datasets (MIDOG 2025 Atypical Training Set, Ami-Br, MIDOG25 preliminary test set).

Conclusion: 多任务学习可用于缓解域偏移对有丝分裂异常检测模型的影响，促使模型关注目标而非背景，初步实验在多个数据集上显示出潜力，未来需更大规模验证。

Abstract: Recognizing atypical mitotic figures in histopathology images allows
physicians to correctly assess tumor aggressiveness. Although machine learning
models could be exploited for automatically performing such a task, under
domain shift these models suffer from significative performance drops. In this
work, an approach based on multi-task learning is proposed for addressing this
problem. By exploiting auxiliary tasks, correlated to the main classification
task, the proposed approach, submitted to the track 2 of the MItosis DOmain
Generalization (MIDOG) challenge, aims to aid the model to focus only on the
object to classify, ignoring the domain varying background of the image. The
proposed approach shows promising performance in a preliminary evaluation
conducted on three distinct datasets, i.e., the MIDOG 2025 Atypical Training
Set, the Ami-Br dataset, as well as the preliminary test set of the MIDOG25
challenge.

</details>


### [90] [FW-GAN: Frequency-Driven Handwriting Synthesis with Wave-Modulated MLP Generator](https://arxiv.org/abs/2508.21040)
*Huynh Tong Dang Khoa,Dang Hoai Nam,Vo Nguyen Le Duy*

Main category: cs.CV

TL;DR: FW-GAN通过Wave-MLP生成器和频率引导鉴别器，结合频率分布损失，实现单样本风格一致的高质量手写合成，用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决标签书写数据稀缺问题，通过生成风格一致的人造样本来增强训练数据。

Method: 设计相位感知Wave-MLP用于捕捉长距空间关系，构建利用高频信息的频率引导鉴别器，并提出频率分布损失使生成样本与真实样本在频域对齐。

Result: 提出FW-GAN：包含相位感知Wave-MLP生成器、频率引导的鉴别器、频率分布损失，在越南语和英语数据集上实现高质量单样本风格一致的写作合成，可用于低资源HTR数据增强。

Conclusion: FW-GAN有效提升了合成样本的视觉逼真度与风格一致性，能在低资源手写识别任务中提供有价值的数据增强。

Abstract: Labeled handwriting data is often scarce, limiting the effectiveness of
recognition systems that require diverse, style-consistent training samples.
Handwriting synthesis offers a promising solution by generating artificial data
to augment training. However, current methods face two major limitations.
First, most are built on conventional convolutional architectures, which
struggle to model long-range dependencies and complex stroke patterns. Second,
they largely ignore the crucial role of frequency information, which is
essential for capturing fine-grained stylistic and structural details in
handwriting. To address these challenges, we propose FW-GAN, a one-shot
handwriting synthesis framework that generates realistic, writer-consistent
text from a single example. Our generator integrates a phase-aware Wave-MLP to
better capture spatial relationships while preserving subtle stylistic cues. We
further introduce a frequency-guided discriminator that leverages
high-frequency components to enhance the authenticity detection of generated
samples. Additionally, we introduce a novel Frequency Distribution Loss that
aligns the frequency characteristics of synthetic and real handwriting, thereby
enhancing visual fidelity. Experiments on Vietnamese and English handwriting
datasets demonstrate that FW-GAN generates high-quality, style-consistent
handwriting, making it a valuable tool for augmenting data in low-resource
handwriting recognition (HTR) pipelines. Official implementation is available
at https://github.com/DAIR-Group/FW-GAN

</details>


### [91] [MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs](https://arxiv.org/abs/2508.21044)
*Junpeng Ma,Qizhe Zhang,Ming Lu,Zhibin Wang,Qiang Zhou,Jun Song,Shanghang Zhang*

Main category: cs.CV

TL;DR: MMG-Vid是一种训练无关的两阶段视觉令牌剪枝方法，按段分配令牌预算并用时序感知DPC算法选择高边际增益令牌，显著降低计算量且几乎不损性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于令牌剪枝的方法忽略视频帧的动态特性和时序依赖，将视频视为多帧静态任务，导致剪枝效率和性能受限。需要在考虑时序性的前提下更合理地分配和选择视觉令牌。

Method: 先基于帧相似性将视频划分为若干段，对每段动态分配令牌预算以最大化段的边际增益；随后设计时序引导的DPC算法，联合建模帧间独特性和帧内多样性以最大化每个令牌的边际增益；二者结合以充分利用有限令牌预算。

Result: 在LLaVA-OneVision-7B上，MMG-Vid在保持>99.5%原始性能的情况下，减少了75%的视觉令牌，并将prefilling阶段加速3.9倍。大量实验验证了该方法的有效性。

Conclusion: 提出训练无关的视觉令牌剪枝框架MMG-Vid，通过最大化边际增益在段级和令牌级去冗余，从而在大幅降低视觉令牌数的同时保持性能。

Abstract: Video Large Language Models (VLLMs) excel in video understanding, but their
excessive visual tokens pose a significant computational challenge for
real-world applications. Current methods aim to enhance inference efficiency by
visual token pruning. However, they do not consider the dynamic characteristics
and temporal dependencies of video frames, as they perceive video understanding
as a multi-frame task. To address these challenges, we propose MMG-Vid, a novel
training-free visual token pruning framework that removes redundancy by
Maximizing Marginal Gains at both segment-level and token-level. Specifically,
we first divide the video into segments based on frame similarity, and then
dynamically allocate the token budget for each segment to maximize the marginal
gain of each segment. Subsequently, we propose a temporal-guided DPC algorithm
that jointly models inter-frame uniqueness and intra-frame diversity, thereby
maximizing the marginal gain of each token. By combining both stages, MMG-Vid
can maximize the utilization of the limited token budget, significantly
improving efficiency while maintaining strong performance. Extensive
experiments demonstrate that MMG-Vid can maintain over 99.5% of the original
performance, while effectively reducing 75% visual tokens and accelerating the
prefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.

</details>


### [92] [CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification](https://arxiv.org/abs/2508.21046)
*Wei Li,Renshan Zhang,Rui Shao,Jie He,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文提出CogVLA，一种模仿人类多模态认知的Vision-Language-Action框架，通过三阶段的指令驱动路由与稀疏化实现更高效的训练与推理。主要包括：在视觉编码器中注入指令以聚合压缩视觉tokens（EFA-Routing）；在语言模型中引入行动意图以剪枝无关视觉tokens（LFP-Routing）；以及结合因果视觉语言注意力与双向动作并行解码的V-L-A耦合注意力（CAtten）。在LIBERO与真实机器人任务上达到或超越最先进性能，同时显著降低训练开销与推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练VLM的VLA模型依赖大量后训练（post-training），导致计算开销大、可扩展性与部署受限。受人类多模态协同认知启发，提出指令驱动的稀疏化路由以提高效率与性能。

Method: 提出三阶段架构：1）EFA-Routing：在视觉编码器中用Encoder-FiLM注入指令，对双流视觉tokens进行选择性聚合与压缩，生成指令感知的潜表示；2）LFP-Routing：基于紧凑视觉编码，在LLM中用LLM-FiLM引入行动意图，剪枝与指令无关的视觉tokens以实现token级稀疏；3）CAtten：V-L-A耦合注意力，将因果视觉-语言注意力与双向动作并行解码结合，保证压缩感知输入下动作生成的准确性与一致性。

Result: 在LIBERO基准与真实机器人任务上，CogVLA分别达成97.4%与70.0%成功率，同时相比OpenVLA将训练成本降低2.5倍，推理延迟降低2.8倍。开源代码已公布。

Conclusion: CogVLA通过指令驱动的编码器与语言模型级别的路由与稀疏化，能在保持或提升任务成功率的同时，大幅降低训练成本与推理延迟，是面向大规模部署的高效VLA解决方案。

Abstract: Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.

</details>


### [93] [Veritas: Generalizable Deepfake Detection via Pattern-Aware Reasoning](https://arxiv.org/abs/2508.21048)
*Hao Tan,Jun Lan,Zichang Tan,Ajian Liu,Chuanbiao Song,Senyuan Shi,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 构建更具现实挑战性的HydraFake数据集，并提出引入planning与self-reflection推理模式的多模态大模型Veritas，显著增强对未见伪造和新域的检测泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基准与工业场景存在严重差距：训练数据同质、测试图像质量低，导致检测器在现实部署中表现不佳。因而需要更贴近真实的测试集与能进行类人推理的检测模型。

Method: 设计分层泛化测试的HydraFake数据集，包含多样化深度伪造技术和真实场景伪造；提出基于多模态大模型的Veritas，采用模式感知推理（含planning与self-reflection）并通过两阶段训练将推理能力内化入模型。

Result: 在HydraFake上实验表明，现有检测器在跨模型场景能泛化，但在未见伪造技术与新域上表现不足；Veritas在各类OOD场景取得显著提升，且能输出具有透明性和可信度的检测结果。

Conclusion: 本文提出了HydraFake数据集与Veritas检测器，目标弥合学术基准与工业实践之间的差距，提升对未见伪造与新域的泛化能力。

Abstract: Deepfake detection remains a formidable challenge due to the complex and
evolving nature of fake content in real-world scenarios. However, existing
academic benchmarks suffer from severe discrepancies from industrial practice,
typically featuring homogeneous training sources and low-quality testing
images, which hinder the practical deployments of current detectors. To
mitigate this gap, we introduce HydraFake, a dataset that simulates real-world
challenges with hierarchical generalization testing. Specifically, HydraFake
involves diversified deepfake techniques and in-the-wild forgeries, along with
rigorous training and evaluation protocol, covering unseen model architectures,
emerging forgery techniques and novel data domains. Building on this resource,
we propose Veritas, a multi-modal large language model (MLLM) based deepfake
detector. Different from vanilla chain-of-thought (CoT), we introduce
pattern-aware reasoning that involves critical reasoning patterns such as
"planning" and "self-reflection" to emulate human forensic process. We further
propose a two-stage training pipeline to seamlessly internalize such deepfake
reasoning capacities into current MLLMs. Experiments on HydraFake dataset
reveal that although previous detectors show great generalization on
cross-model scenarios, they fall short on unseen forgeries and data domains.
Our Veritas achieves significant gains across different OOD scenarios, and is
capable of delivering transparent and faithful detection outputs.

</details>


### [94] [FakeParts: a New Family of AI-Generated DeepFakes](https://arxiv.org/abs/2508.21052)
*Gaetan Brison,Soobash Daiboo,Samy Aimeur,Awais Hussain Sani,Xi Wang,Gianni Franchi,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出“部分深伪（FakeParts）”概念并公开FakePartsBench数据集（25K+视频、像素/帧级标注），证明其对人类与SOTA模型均有显著欺骗性，强调需要针对局部篡改的新检测方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造研究多集中于全局合成视频，而现实中更隐蔽的部分篡改（局部或时间段内修改）更具欺骗性，需专门数据与评估基准提高检测能力。

Method: 构建包含25K+视频的数据集FakePartsBench，提供像素级与帧级标注；通过用户研究和评估现有检测模型来验证部分篡改的欺骗性与检测难度。

Result: 用户研究显示与传统深伪比，FakeParts使人类检测准确率下降30%+；同样使现有SOTA检测模型性能大幅下降；公开了带细粒度注释的数据集以促进后续研究。

Conclusion: FakeParts揭示了部分篡改视频在检测方面的关键漏洞，现有方法在此类样本上效果显著下降；FakePartsBench为研究者提供了首个大规模基准，促进更健壮检测器的发展。

Abstract: We introduce FakeParts, a new class of deepfakes characterized by subtle,
localized manipulations to specific spatial regions or temporal segments of
otherwise authentic videos. Unlike fully synthetic content, these partial
manipulations, ranging from altered facial expressions to object substitutions
and background modifications, blend seamlessly with real elements, making them
particularly deceptive and difficult to detect. To address the critical gap in
detection capabilities, we present FakePartsBench, the first large-scale
benchmark dataset specifically designed to capture the full spectrum of partial
deepfakes. Comprising over 25K videos with pixel-level and frame-level
manipulation annotations, our dataset enables comprehensive evaluation of
detection methods. Our user studies demonstrate that FakeParts reduces human
detection accuracy by over 30% compared to traditional deepfakes, with similar
performance degradation observed in state-of-the-art detection models. This
work identifies an urgent vulnerability in current deepfake detection
approaches and provides the necessary resources to develop more robust methods
for partial video manipulations.

</details>


### [95] [Multi-View 3D Point Tracking](https://arxiv.org/abs/2508.21060)
*Frano Rajič,Haofei Xu,Marko Mihajlovic,Siyuan Li,Irem Demir,Emircan Gündoğdu,Lei Ke,Sergey Prokudin,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 该论文提出首个数据驱动的多视角3D点跟踪器，可在有限摄像头数量（例如4个）下对动态场景中的任意点进行在线追踪。方法将多视图特征融合为统一点云，利用k近邻相关与基于transformer的更新来估计鲁棒的长距离3D对应，能处理遮挡问题。在5K合成序列上训练，并在两项真实数据集（Panoptic Studio、DexYCB）上分别达成中位误差3.1 cm和2.0 cm，且能泛化到1-8视角和不同视频长度。作者将公开代码与数据集。


<details>
  <summary>Details</summary>
Motivation: 现有单目跟踪在深度歧义与遮挡下性能差，已有多摄像头方法要么需要大量（>20）摄像头，要么需要每个序列的昂贵优化；因此提出一个端到端、基于数据驱动且能在实用摄像头数量下工作的多视角3D追踪器。

Method: 方法输入已知相机位姿与多视角深度（传感器测量或估计深度），将多视图特征投影并融合为统一的点云；利用kNN相关计算局部匹配，再通过transformer式的更新模块输出3D点对应。训练使用5K合成Kubric序列，评估在Panoptic Studio与DexYCB数据集。

Result: 在两个真实基准上取得中位轨迹误差3.1 cm（Panoptic）和2.0 cm（DexYCB）；能泛化到1-8视角与不同视频长度，公开代码与数据。

Conclusion: 论文展示了一个实用且有效的多视角3D点跟踪方法，性能显著优于单目追踪，能在少量摄像头下稳健工作并对遮挡具有鲁棒性，适用于真实场景和多种相机配置。

Abstract: We introduce the first data-driven multi-view 3D point tracker, designed to
track arbitrary points in dynamic scenes using multiple camera views. Unlike
existing monocular trackers, which struggle with depth ambiguities and
occlusion, or prior multi-camera methods that require over 20 cameras and
tedious per-sequence optimization, our feed-forward model directly predicts 3D
correspondences using a practical number of cameras (e.g., four), enabling
robust and accurate online tracking. Given known camera poses and either
sensor-based or estimated multi-view depth, our tracker fuses multi-view
features into a unified point cloud and applies k-nearest-neighbors correlation
alongside a transformer-based update to reliably estimate long-range 3D
correspondences, even under occlusion. We train on 5K synthetic multi-view
Kubric sequences and evaluate on two real-world benchmarks: Panoptic Studio and
DexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.
Our method generalizes well to diverse camera setups of 1-8 views with varying
vantage points and video lengths of 24-150 frames. By releasing our tracker
alongside training and evaluation datasets, we aim to set a new standard for
multi-view 3D tracking research and provide a practical tool for real-world
applications. Project page available at https://ethz-vlg.github.io/mvtracker.

</details>


### [96] [OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning](https://arxiv.org/abs/2508.21066)
*Yuan Gong,Xionghui Wang,Jie Wu,Shiyin Wang,Yitong Wang,Xinglong Wu*

Main category: cs.CV

TL;DR: 提出OneReward：用一个VLM作为统一奖励进行多任务强化学习，得到的Seedream 3.0 Fill在掩码引导图像编辑任务上性能领先且免去任务特定SFT。


<details>
  <summary>Details</summary>
Motivation: 提出一种统一的强化学习框架OneReward，用单一奖励模型提升模型在多任务、多评估标准下的生成能力，解决任务间数据分布和评估差异带来的泛化问题。

Method: 采用单个视觉-语言模型判断生成结果优劣，作为reward信号对生成模型进行多任务强化学习训练，直接在预训练模型上微调，覆盖图像填充、扩展、对象移除与文字渲染等子任务。

Result: 基于单一视觉-语言模型作为生成奖励模型，构建Seedream 3.0 Fill，通过多任务强化学习在预训练基础模型上训练，无需任务特定的有监督微调，在多项评估维度上优于商用和开源对手。

Conclusion: OneReward证明了单一奖励模型可以在多任务生成中实现有效泛化；Seedream 3.0 Fill展示了在掩码编辑子任务上优越性，提升训练效率并简化流程。

Abstract: In this paper, we introduce OneReward, a unified reinforcement learning
framework that enhances the model's generative capabilities across multiple
tasks under different evaluation criteria using only \textit{One Reward} model.
By employing a single vision-language model (VLM) as the generative reward
model, which can distinguish the winner and loser for a given task and a given
evaluation criterion, it can be effectively applied to multi-task generation
models, particularly in contexts with varied data and diverse task objectives.
We utilize OneReward for mask-guided image generation, which can be further
divided into several sub-tasks such as image fill, image extend, object
removal, and text rendering, involving a binary mask as the edit area. Although
these domain-specific tasks share same conditioning paradigm, they differ
significantly in underlying data distributions and evaluation metrics. Existing
methods often rely on task-specific supervised fine-tuning (SFT), which limits
generalization and training efficiency. Building on OneReward, we develop
Seedream 3.0 Fill, a mask-guided generation model trained via multi-task
reinforcement learning directly on a pre-trained base model, eliminating the
need for task-specific SFT. Experimental results demonstrate that our unified
edit model consistently outperforms both commercial and open-source
competitors, such as Ideogram, Adobe Photoshop, and FLUX Fill [Pro], across
multiple evaluation dimensions. Code and model are available at:
https://one-reward.github.io

</details>


### [97] [Dress&Dance: Dress up and Dance as You Like It - Technical Preview](https://arxiv.org/abs/2508.21070)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出Dress&Dance：基于CondNet的多模态条件视频扩散框架，可用一张用户图像和参考视频生成高质量、短时长的虚拟试衣视频；通过多阶段训练融合图像与视频数据，提升配准与动作保真。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法在高分辨率、动作保真和多样服装支持上存在局限；数据量级上视频数据稀缺但图像数据丰富，因此通过多模态条件网络与分阶段训练来利用异构数据可提升质量与泛化能力。

Method: 基于视频扩散模型，设计CondNet作为多模态条件网络，利用注意力机制融合文本、图像与视频信息；采用分阶段训练，先在大规模图像数据上学习服装配准，再结合有限视频数据微调以提升运动一致性。生成端输入用户单张图像、指定服装与参考动作视频，输出5秒24FPS、1152x720的试穿视频。

Result: Dress&Dance提出了一种视频扩散框架，能够在1152x720分辨率下生成高质量的5秒、24FPS的虚拟试衣视频，用户仅需一张图片并可指定服装与参考视频动作。关键组件是CondNet，一个利用注意力机制统一文本、图像和视频等多模态输入的条件网络，改进了服装配准和动作一致性。CondNet通过在多阶段进阶训练中结合稀缺的视频数据与大量可用的图像数据进行训练。实验显示Dress&Dance在开放源代码与商业方案中表现更优，提供灵活高质量的试穿体验。

Conclusion: Dress&Dance通过CondNet实现了多模态条件统一和分阶段训练策略，显著提升了虚拟试穿视频的质量与动作一致性，优于现有开源和商业方法。

Abstract: We present Dress&Dance, a video diffusion framework that generates high
quality 5-second-long 24 FPS virtual try-on videos at 1152x720 resolution of a
user wearing desired garments while moving in accordance with a given reference
video. Our approach requires a single user image and supports a range of tops,
bottoms, and one-piece garments, as well as simultaneous tops and bottoms
try-on in a single pass. Key to our framework is CondNet, a novel conditioning
network that leverages attention to unify multi-modal inputs (text, images, and
videos), thereby enhancing garment registration and motion fidelity. CondNet is
trained on heterogeneous training data, combining limited video data and a
larger, more readily available image dataset, in a multistage progressive
manner. Dress&Dance outperforms existing open source and commercial solutions
and enables a high quality and flexible try-on experience.

</details>


### [98] [First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge](https://arxiv.org/abs/2508.21072)
*Fahad Shamshad,Tameem Bakr,Yahia Shaaban,Noor Hussein,Karthik Nandakumar,Nils Lukas*

Main category: cs.CV

TL;DR: 提出了针对水印的两类攻击：VAE为核心的beige-box自适应逃避策略与基于聚类+扩散模型的黑盒策略，能在保持图像质量的同时实现高成功率的水印去除（95.7%）。


<details>
  <summary>Details</summary>
Motivation: 探究现有内容水印在面对不同层次的对手知识时的鲁棒性，并通过参赛解决方案检验和挑战水印方法的安全性。

Method: 贝茨盒（beige-box）轨道采用基于VAE的自适应规避攻击，结合测试时优化和在CIELAB色彩空间的对比度恢复；黑盒轨道先基于空间或频域伪影对图像进行聚类，再对每个聚类应用受控噪声注入的图像到图像扩散模型，并利用ChatGPT生成的语义描述作为先验，同时对参数进行优化。

Result: 方法在挑战中取得优胜，黑盒与贝茨盒轨道均能高效去除水印；整体上报告95.7%的去除成功率，同时对残余图像质量影响极小。

Conclusion: 该论文展示了一种在多种对手知识水平下高效去除图像水印的攻击方法，实验证明在NeurIPS 2024挑战赛中取得了优胜并能在不显著损害图像质量的情况下实现高成功率的水印去除。

Abstract: Content watermarking is an important tool for the authentication and
copyright protection of digital media. However, it is unclear whether existing
watermarks are robust against adversarial attacks. We present the winning
solution to the NeurIPS 2024 Erasing the Invisible challenge, which
stress-tests watermark robustness under varying degrees of adversary knowledge.
The challenge consisted of two tracks: a black-box and beige-box track,
depending on whether the adversary knows which watermarking method was used by
the provider. For the beige-box track, we leverage an adaptive VAE-based
evasion attack, with a test-time optimization and color-contrast restoration in
CIELAB space to preserve the image's quality. For the black-box track, we first
cluster images based on their artifacts in the spatial or frequency-domain.
Then, we apply image-to-image diffusion models with controlled noise injection
and semantic priors from ChatGPT-generated captions to each cluster with
optimized parameter settings. Empirical evaluations demonstrate that our method
successfully achieves near-perfect watermark removal (95.7%) with negligible
impact on the residual image's quality. We hope that our attacks inspire the
development of more robust image watermarking methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [99] [Efficient Forkless Blockchain Databases](https://arxiv.org/abs/2508.20686)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 针对无分叉区块链设计专用StateDB，显著减少存储占用并提高吞吐量（100x存储提升，10x吞吐量提升）。


<details>
  <summary>Details</summary>
Motivation: 降低运行L1区块链节点的资源成本，尤其优化StateDB数据库在无分叉（forkless）区块链中的性能与资源使用。

Method: 分析传统forking数据库在无分叉环境下的冗余与低效，设计并实现一种无需保留分叉历史的数据库结构与协议优化，替换geth的StateDB并在Fantom客户端上进行基准测试和对比实验。

Result: 提出一种面向无分叉区块链的StateDB替代方案，实验结果显示相较于基于geth的Fantom客户端，在存储方面提升约100倍，吞吐量提升约10倍。

Conclusion: 使用为无分叉链优化的数据库能显著降低节点成本并提高性能；建议将此设计采纳到相关客户端以提升可扩展性和效率。

Abstract: Operating nodes in an L1 blockchain remains costly despite recent advances in
blockchain technology. One of the most resource-intensive components of a node
is the blockchain database, also known as StateDB, that manages balances,
nonce, code, and the persistent storage of accounts/smart contracts. Although
the blockchain industry has transitioned from forking to forkless chains due to
improved consensus protocols, forkless blockchains still rely on legacy forking
databases that are suboptimal for their purposes. In this paper, we propose a
forkless blockchain database, showing a 100x improvement in storage and a 10x
improvement in throughput compared to the geth-based Fantom Blockchain client.

</details>


### [100] [Research Challenges in Relational Database Management Systems for LLM Queries](https://arxiv.org/abs/2508.20912)
*Kerem Akillioglu,Anurag Chakraborty,Sairaj Voruganti,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 本文对SQL内调用LLM的三种实现进行早期评估，暴露功能性与性能瓶颈并提出初步优化，表明更紧密的LLM与DBMS整合可实现更可扩展高效的LLM查询


<details>
  <summary>Details</summary>
Motivation: 探讨将大型语言模型(LLMs)直接集成到关系型数据库(SQL)中的可行性与挑战，评估开源与企业平台在功能、性能与可扩展性方面的现状

Method: 设计五个代表性LLM查询并在两种开源系统与一企业平台上执行，测量功能正确性、性能与可扩展性瓶颈；针对发现的问题实现并评估初步优化方案，例如输出约束机制、资源使用优化与改进的查询规划策略。

Result: 通过评估两种开源系统与一种企业平台的五个代表性LLM-SQL查询，发现三大问题：结构化输出约束不足、资源利用率低及查询规划欠佳；并实现初步改进措施，提升处理能力

Conclusion: 现有SQL-invoked LLM集成存在显著限制，但通过加强结构化输出约束、优化资源调度与改进查询规划可取得实质改进；未来需在DBMS层面深度整合LLM以实现可扩展高效的处理

Abstract: Large language models (LLMs) have become essential for applications such as
text summarization, sentiment analysis, and automated question-answering.
Recently, LLMs have also been integrated into relational database management
systems to enhance querying and support advanced data processing. Companies
such as Amazon, Databricks, Google, and Snowflake offer LLM invocation directly
within SQL, denoted as LLM queries, to boost data insights. However,
open-source solutions currently have limited functionality and poor
performance. In this work, we present an early exploration of two open-source
systems and one enterprise platform, using five representative queries to
expose functional, performance, and scalability limits in today's SQL-invoked
LLM integrations. We identify three main issues: enforcing structured outputs,
optimizing resource utilization, and improving query planning. We implemented
initial solutions and observed improvements in accommodating LLM powered SQL
queries. These early gains demonstrate that tighter integration of LLM+DBMS is
the key to scalable and efficient processing of LLM queries.

</details>


### [101] [Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets](https://arxiv.org/abs/2508.20986)
*Lianpeng Qiao,Ziqi Cao,Kaiyu Feng,Ye Yuan,Guoren Wang*

Main category: cs.DB

TL;DR: ReCoGNN 自动将多表关系信号编码为图结构并通过GNN传播来自动生成增强特征，显著提升了下游预测性能。


<details>
  <summary>Details</summary>
Motivation: 动机是自动化特征增强以减少人工开销，挖掘关系数据库中与预测任务相关的表间与表内信号，从而提高AutoML在关系型数据上的效果。

Method: 方法包括三步：1) 在表内建模属性间语义依赖并将表划分为语义一致的子段；2) 构建表示跨段行间关系的异构加权图；3) 使用图神经网络进行消息传递，指导特征选择并生成增强特征用于原始数据。

Result: 在十个真实与合成数据集上的大量实验显示，ReCoGNN 在分类与回归任务上均优于现有方法，表现稳定且具有普适性。

Conclusion: ReCoGNN 提出了一种端到端自动化特征增强框架，能从多表关系数据中自动提取并选择有助于下游预测任务的特征，从而提高分类与回归任务性能。

Abstract: Data has become a foundational asset driving innovation across domains such
as finance, healthcare, and e-commerce. In these areas, predictive modeling
over relational tables is commonly employed, with increasing emphasis on
reducing manual effort through automated machine learning (AutoML) techniques.
This raises an interesting question: can feature augmentation itself be
automated and identify and utilize task-related relational signals?
  To address this challenge, we propose an end-to-end automated feature
augmentation framework, ReCoGNN, which enhances initial datasets using features
extracted from multiple relational tables to support predictive tasks. ReCoGNN
first captures semantic dependencies within each table by modeling intra-table
attribute relationships, enabling it to partition tables into structured,
semantically coherent segments. It then constructs a heterogeneous weighted
graph that represents inter-row relationships across all segments. Finally,
ReCoGNN leverages message-passing graph neural networks to propagate
information through the graph, guiding feature selection and augmenting the
original dataset. Extensive experiments conducted on ten real-life and
synthetic datasets demonstrate that ReCoGNN consistently outperforms existing
methods on both classification and regression tasks.

</details>
