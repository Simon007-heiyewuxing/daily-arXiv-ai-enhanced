<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 41]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Cropland Mapping using Geospatial Embeddings](https://arxiv.org/abs/2511.02923)
*Ivan Zvonkov,Gabriel Tseng,Inbal Becker-Reshef,Hannah Kerner*

Main category: cs.CV

TL;DR: 使用Presto与AlphaEarth地理空间嵌入在多哥进行农田制图，结果表明嵌入能简化流程并达到高精度的农田分类，利于评估土地利用变化及其气候影响。


<details>
  <summary>Details</summary>
Motivation: 高质量、及时的土地覆盖地图对于理解土地利用变化（气候变化的主要驱动力）至关重要；地理空间嵌入提供了一种更高效、易获取的景观特征映射方式，但其在实际制图应用中的效果尚未充分探讨。

Method: 使用Presto和AlphaEarth的地理空间嵌入生成特征，并基于这些嵌入构建分类模型来生成多哥的农田地图。

Result: 基于Presto和AlphaEarth嵌入的制图结果显示，这些嵌入能简化制图流程并实现高准确率的农田分类，从而支持更可靠的土地利用变化和气候影响评估。

Conclusion: 本文评估了地理空间嵌入在多哥农田制图中的应用，结论是嵌入方法能简化工作流并实现高精度的农田分类，有助于更好地评估土地利用变化及其气候影响。

Abstract: Accurate and up-to-date land cover maps are essential for understanding land
use change, a key driver of climate change. Geospatial embeddings offer a more
efficient and accessible way to map landscape features, yet their use in
real-world mapping applications remains underexplored. In this work, we
evaluated the utility of geospatial embeddings for cropland mapping in Togo. We
produced cropland maps using embeddings from Presto and AlphaEarth. Our
findings show that geospatial embeddings can simplify workflows, achieve
high-accuracy cropland classification and ultimately support better assessments
of land use change and its climate impacts.

</details>


### [2] [Generative Hints](https://arxiv.org/abs/2511.02933)
*Andy Dimnaku,Abdullah Yusuf Kavranoğlu,Yaser Abu-Mostafa*

Main category: cs.CV

TL;DR: 提出用生成模型生成虚拟未标注样本并以半监督方式施加已知不变性（generative hints），从而在整个输入空间更有效地学习不变性，优于传统数据增强，带来实质性的性能提升。


<details>
  <summary>Details</summary>
Motivation: 数据增强只能在训练样本的变换上教授不变性，无法保证模型在整个输入空间遵守已知性质。为此需要一种直接在输入空间施加不变性的训练机制。

Method: 首先训练一个生成模型以拟合训练集分布，生成大量虚拟示例；然后在半监督框架下，使用这些未标注虚拟示例同时优化分类损失和提示（hint）损失，使模型在整个输入空间学习期望的不变性/函数性质。

Result: 在细粒度视觉分类基准上，与带数据增强的微调模型相比，最高提升1.78%的top-1准确率，平均提升0.63%；在CheXpert X光数据集上平均提升1.286%。

Conclusion: 本文提出的generative hints方法，通过使用生成模型生成的虚拟未标注样本在整个输入空间强制执行已知的不变性，从而超越了仅靠数据增强在训练数据变换上的局限。实验表明，在多个数据集和架构上，generative hints显著超过标准数据增强。

Abstract: Data augmentation is widely used in vision to introduce variation and
mitigate overfitting, through enabling models to learn invariant properties,
such as spatial invariance. However, these properties are not fully captured by
data augmentation alone, since it attempts to learn the property on
transformations of the training data only. We propose generative hints, a
training methodology that directly enforces known invariances in the entire
input space. Our approach leverages a generative model trained on the training
set to approximate the input distribution and generate unlabeled images, which
we refer to as virtual examples. These virtual examples are used to enforce
functional properties known as hints. In generative hints, although the
training dataset is fully labeled, the model is trained in a semi-supervised
manner on both the classification and hint objectives, using the unlabeled
virtual examples to guide the model in learning the desired hint. Across
datasets, architectures, and loss functions, generative hints consistently
outperform standard data augmentation when learning the same property. On
popular fine-grained visual classification benchmarks, we achieved up to 1.78%
top-1 accuracy improvement (0.63% on average) over fine-tuned models with data
augmentation and an average performance boost of 1.286% on the CheXpert X-ray
dataset.

</details>


### [3] [ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology](https://arxiv.org/abs/2511.02946)
*Srikumar Sastry,Subash Khanal,Aayush Dhakal,Jiayu Lin,Dan Cher,Phoenix Jarosz,Nathan Jacobs*

Main category: cs.CV

TL;DR: 提出ProM3E：一种基于嵌入空间概率化掩码重构的任意模态生成模型，支持模态逆转，并通过混合跨/内模态相似性的检索方法和线性探针展示了优越表现。


<details>
  <summary>Details</summary>
Motivation: 生态学任务常涉及多模态（如图像、声音、环境传感器等），且实际场景下模态常缺失或需要互相预测，因而需要一种能在嵌入空间任意方向生成与推断模态的模型，同时希望量化融合不同模态对下游任务的贡献。

Method: 在嵌入空间对缺失模态进行掩码重构，训练模型以给定若干上下文模态推断缺失模态；采用概率模型表征嵌入的不确定性，并通过该不确定性分析哪些模态应被融合；提出了基于交叉模态和同模态相似性加权组合的检索策略；利用隐藏表示进行线性探针评估。

Result: 模型在跨模态检索任务上优于其他方法（文中声称在所有检索任务上取得更好性能），并且隐藏表示在线性探针任务上表现出更好的表征能力；同时提供了能评估模态融合可行性的概率性分析。

Conclusion: ProM3E通过在嵌入空间中进行概率化的掩码模态重构，实现任意模态到任意模态的表示生成，支持嵌入空间的模态逆转，并能评估不同模态融合在下游任务中的可行性；基于此提出了一种混合跨/内模态相似性的检索方法，在检索和线性探针任务上表现优越。

Abstract: We introduce ProM3E, a probabilistic masked multimodal embedding model for
any-to-any generation of multimodal representations for ecology. ProM3E is
based on masked modality reconstruction in the embedding space, learning to
infer missing modalities given a few context modalities. By design, our model
supports modality inversion in the embedding space. The probabilistic nature of
our model allows us to analyse the feasibility of fusing various modalities for
given downstream tasks, essentially learning what to fuse. Using these features
of our model, we propose a novel cross-modal retrieval approach that mixes
inter-modal and intra-modal similarities to achieve superior performance across
all retrieval tasks. We further leverage the hidden representation from our
model to perform linear probing tasks and demonstrate the superior
representation learning capability of our model. All our code, datasets and
model will be released at https://vishu26.github.io/prom3e.

</details>


### [4] [EvtSlowTV -- A Large and Diverse Dataset for Event-Based Depth Estimation](https://arxiv.org/abs/2511.02953)
*Sadiq Layi Macaulay,Nimet Kaygusuz,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出了大型无标注事件相机数据集EvtSlowTV（13B事件），并用自监督方法训练深度估计模型，改善了泛化能力并保留事件数据异步特性。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机深度估计受限于小规模带注释数据集，导致在真实世界场景中的泛化能力不足，因此需要大规模无标注、自然场景的数据来提升模型性能。

Method: 从YouTube公开视频收集并构建包含13B事件的数据集，保留事件数据的异步特性，使用自监督学习方法训练深度估计模型，避免依赖帧级标注。

Result: 构建了比现有事件数据集大一个数量级的EvtSlowTV，并证明在该数据集上训练能提升模型对复杂场景与运动的泛化能力，利用事件相机的HDR与低延迟特性进行自监督深度学习。

Conclusion: 作者提出了一个大规模事件相机数据集EvtSlowTV，并通过自监督学习框架利用原始事件流进行深度估计，提高了模型在复杂场景和运动下的泛化能力。

Abstract: Event cameras, with their high dynamic range (HDR) and low latency, offer a
promising alternative for robust depth estimation in challenging environments.
However, many event-based depth estimation approaches are constrained by
small-scale annotated datasets, limiting their generalizability to real-world
scenarios. To bridge this gap, we introduce EvtSlowTV, a large-scale event
camera dataset curated from publicly available YouTube footage, which contains
more than 13B events across various environmental conditions and motions,
including seasonal hiking, flying, scenic driving, and underwater exploration.
EvtSlowTV is an order of magnitude larger than existing event datasets,
providing an unconstrained, naturalistic setting for event-based depth
learning. This work shows the suitability of EvtSlowTV for a self-supervised
learning framework to capitalise on the HDR potential of raw event streams. We
further demonstrate that training with EvtSlowTV enhances the model's ability
to generalise to complex scenes and motions. Our approach removes the need for
frame-based annotations and preserves the asynchronous nature of event data.

</details>


### [5] [Hybrid Convolution and Vision Transformer NAS Search Space for TinyML Image Classification](https://arxiv.org/abs/2511.02992)
*Mikhael Djajapermana,Moritz Reiber,Daniel Mueller-Gritschneder,Ulf Schlichtmann*

Main category: cs.CV

TL;DR: 提出一个包含可搜索池化层的CNN-ViT混合NAS搜索空间，能在tinyML约束下得到比ResNet更优的CIFAR10模型。


<details>
  <summary>Details</summary>
Motivation: 现有CNN/ViT或其混合架构参数量大、计算开销高，不适合tinyML部署；因此需要在受限模型大小下寻找既高效又准确的混合架构。

Method: 设计了包含混合CNN块、ViT块和可搜索的池化块（用于高效特征图降采样）的NAS搜索空间，用以联合学习局部与全局信息并优化特征降采样策略。

Result: 在CIFAR10上搜索出的混合架构在有限模型大小约束下，较同等条件下的ResNet基tinyML模型表现出更高准确率与更快推理速度。

Conclusion: 提出的混合CNN-ViT搜索空间在CIFAR10上能在严格模型尺寸限制下找到在准确率和推理速度上优于ResNet-based tinyML模型的架构。

Abstract: Hybrids of Convolutional Neural Network (CNN) and Vision Transformer (ViT)
have outperformed pure CNN or ViT architecture. However, since these
architectures require large parameters and incur large computational costs,
they are unsuitable for tinyML deployment. This paper introduces a new hybrid
CNN-ViT search space for Neural Architecture Search (NAS) to find efficient
hybrid architectures for image classification. The search space covers hybrid
CNN and ViT blocks to learn local and global information, as well as the novel
Pooling block of searchable pooling layers for efficient feature map reduction.
Experimental results on the CIFAR10 dataset show that our proposed search space
can produce hybrid CNN-ViT architectures with superior accuracy and inference
speed to ResNet-based tinyML models under tight model size constraints.

</details>


### [6] [SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics](https://arxiv.org/abs/2511.02996)
*Ailar Mahdizadeh,Puria Azadi Moghadam,Xiangteng He,Shahriar Mirabbasi,Panos Nasiopoulos,Leonid Sigal*

Main category: cs.CV

TL;DR: SCALE-VLP将体积空间语义与领域知识融入软加权对比预训练，产生结构一致且语义 grounded 的表征，显著提高CT多任务与跨域表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLM多限于2D且使用二元监督，忽视体积医学影像连续结构与丰富临床语义，导致空间一致性差和语义利用不足；需构建能处理体积数据并结合医学知识的对比学习方法。

Method: 提出Soft-weighted contrastive VLP框架，利用体积空间语义保持解剖结构，并引入放射学本体等领域知识用于对齐；训练上采用软权重正负样本策略替代二元对比损失，保留切片间空间连贯性并注入语义先验。

Result: 在多任务上显著超越前人：CT-报告检索top-1提高最多4.3倍，异常分类提升10个百分点，报告生成达ROUGE-L 0.44和BERT-F1 0.89；在外部数据集零样本评价也有稳定增益。

Conclusion: SCALE-VLP通过软加权对比学习结合体积空间语义与领域知识语义，实现了在CT体数据上的结构一致且语义丰富的视-文表示，显著提升检索、报告生成与分类任务表现，并具备跨域零样本泛化能力。

Abstract: Vision-language models (VLMs) have demonstrated strong cross-modal
capabilities, yet most work remains limited to 2D data and assumes binary
supervision (i.e., positive vs. negative pairs), overlooking the continuous and
structured dependencies present in volumetric data such as CT. Existing
approaches often treat volumetric scans as independent 2D slices, compromising
spatial coherence and underutilizing rich clinical semantics. We propose
SCALE-VLP, a soft-weighted contrastive vision-language pre-training framework
that integrates (i) volumetric spatial semantics to preserve anatomical
structure and (ii) domain-aware, knowledge-infused semantics (e.g.,
radiological ontologies) to guide alignment. This yields structurally
consistent and semantically grounded representations under limited supervision,
demonstrating strong cross-task transferability (retrieval, report generation,
and classification), and cross-domain generalizability with consistent gains
without further fine-tuning. In particular, compared to the previous state of
the art, SCALE-VLP achieves up to 4.3x higher top-1 CT-report retrieval,
improves abnormality classification by 10 points, and reaches ROUGE-L 0.44 and
BERT-F1 0.89 for report generation. Further, in zero-shot evaluation on an
out-of-domain external dataset, we observe consistent gains, indicating the
cross-task and cross-domain generalization ability of SCALE-VLP.

</details>


### [7] [Learning with less: label-efficient land cover classification at very high spatial resolution using self-supervised deep learning](https://arxiv.org/abs/2511.03004)
*Dakota Hester,Vitor S. Martins,Lucas B. Ferreira,Thainara M. A. Lima*

Main category: cs.CV

TL;DR: Using BYOL pretraining on 377k unlabeled 1-m patches and only 1,000 labeled patches, transferring to segmentation models and fine-tuning (250–750 patches) produced an 87% overall accuracy, showing self-supervised learning greatly cuts annotation needs for high-res land cover mapping.


<details>
  <summary>Details</summary>
Motivation: Collecting large, representative annotated training data for meter-scale land cover mapping is costly and limits adoption of deep learning segmentation at large scales; goal is to reduce annotation requirements via self-supervised learning.

Method: Pretrain ResNet-101 encoder with BYOL on 377,921 unlabeled 256×256 color-infrared aerial patches; transfer weights to multiple segmentation architectures (FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN); fine-tune with 250/500/750 annotated patches and cross-validation; ensemble best U-Net models for final mapping.

Result: Ensemble of U-Net models achieved 87.14% overall accuracy and 75.58% macro F1 on statewide 1-m, 8-class map of Mississippi (123+ billion pixels); good mapping for water and forest, confusion among cropland, herbaceous, barren.

Conclusion: Self-supervised pretraining with BYOL enables accurate 1-m, 8-class statewide land cover mapping using only 1,000 annotated patches, substantially reducing annotation needs.

Abstract: Deep learning semantic segmentation methods have shown promising performance
for very high 1-m resolution land cover classification, but the challenge of
collecting large volumes of representative training data creates a significant
barrier to widespread adoption of such models for meter-scale land cover
mapping over large areas. In this study, we present a novel label-efficient
approach for statewide 1-m land cover classification using only 1,000 annotated
reference image patches with self-supervised deep learning. We use the
"Bootstrap Your Own Latent" pre-training strategy with a large amount of
unlabeled color-infrared aerial images (377,921 256x256 1-m pixel patches) to
pre-train a ResNet-101 convolutional encoder. The learned encoder weights were
subsequently transferred into multiple deep semantic segmentation architectures
(FCN, U-Net, Attention U-Net, DeepLabV3+, UPerNet, PAN), which were then
fine-tuned using very small training dataset sizes with cross-validation (250,
500, 750 patches). Among the fine-tuned models, we obtained the 87.14% overall
accuracy and 75.58% macro F1 score using an ensemble of the best performing
U-Net models for comprehensive 1-m, 8-class land cover mapping, covering more
than 123 billion pixels over the state of Mississippi, USA. Detailed
qualitative and quantitative analysis revealed accurate mapping of open water
and forested areas, while highlighting challenges in accurate delineation
between cropland, herbaceous, and barren land cover types. These results show
that self-supervised learning is an effective strategy for reducing the need
for large volumes of manually annotated data, directly addressing a major
limitation to high spatial resolution land cover mapping at scale.

</details>


### [8] [A Foundation Model for Brain MRI with Dynamic Modality Integration](https://arxiv.org/abs/2511.03014)
*Minh Sao Khue Luu,Bair N. Tuchinov*

Main category: cs.CV

TL;DR: 提出一个带可学习模态嵌入和条件层归一化的单编码器自监督基础模型，能处理缺失/未见MRI序列并通过方差-协方差正则提升表征多样性；在6万例数据上训练，初步验证可行，代码与模型开源。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常为每种模态组合训练独立模型，且难以处理缺失或未见模态；因此期望构建一个能共享参数、灵活适配不同模态组合的通用表征模型。

Method: 采用单一编码器+可学习模态嵌入、条件层归一化和针对缺失模态设计的掩码自编码目标；引入方差-协方差正则项以稳定特征学习并提高表征多样性；训练策略为自监督重建与模态插补，在约6万多中心MRI上训练。

Result: 作者计划在脑肿瘤与多发性硬化分割及病灶分类任务上评估，初步结果表明方法可行，完整性能和鲁棒性尚待更多实验验证。

Conclusion: 该工作提出了一个对多模态脑MRI具有适应性的基础模型，能在成像序列缺失或未见时仍进行表征学习，初步结果显示可行。

Abstract: We present a foundation model for brain MRI that can work with different
combinations of imaging sequences. The model uses one encoder with learnable
modality embeddings, conditional layer normalization, and a masked autoencoding
objective that accounts for missing modalities. A variance-covariance
regularizer is applied to stabilize feature learning and improve representation
diversity. This design removes the need for separate models for each modality
and allows the network to adapt when some sequences are missing or unseen. It
is trained on about 60,000 multi-center MRIs using self-supervised
reconstruction and modality imputation to learn flexible representations. A
learnable modality embedding guides feature extraction so the encoder can
adjust to different inputs. We describe our planned evaluation on brain tumor
and multiple sclerosis segmentation, as well as lesion classification, under
various modality settings. Preliminary results show that the method works
feasibly, and further experiments are planned to study its performance in more
detail. All code and pretrained models are available at
https://github.com/BrainFM/brainfm

</details>


### [9] [SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment](https://arxiv.org/abs/2511.03019)
*Wenbo Lu*

Main category: cs.CV

TL;DR: SLIP通过结构化对比损失利用实体关系图进行VLP，且在新构建的Amazon共购多模态图数据集上，在零/少样本跨模态任务上显著优于CLIP。


<details>
  <summary>Details</summary>
Motivation: 现有VLP方法把图像-文本对当作孤立样本训练，忽视了很多实际场景中天然存在的实体关系（如共购图、社交推荐图），而人类认知以关系认知地图编码知识，借鉴此点可改进跨模态表示。

Method: 在CLIP式的双塔框架基础上，增加一个结构对比损失，使得图中相邻节点的多模态表示在嵌入空间中更接近；并构建了大规模Amazon商品共购多模态图数据集，用于结构化跨模态监督。训练同时优化模态对齐和邻居关系保留。

Result: 在大规模共购图数据集上，SLIP在跨模态检索与分类任务（零样本与少样本）上持续优于CLIP，证明关系监督有助于提升跨模态对齐与下游性能。

Conclusion: 本文提出的SLIP通过引入结构感知的对比损失，将视觉-语言预训练从孤立的图像-文本对扩展到图结构的关系监督，理论上能更好捕捉实体间的语义关系，从而提升跨模态对齐效果。

Abstract: Vision-Language Pretraining (VLP) has achieved remarkable success across
various downstream tasks, but such gains are largely driven by scaling up on
training data. Yet, literature methods treat image-text pairs as isolated
training examples; this neglects the rich relational structure naturally
present in many domains, such as e-commerce product co-purchase graphs and
social recommendation networks. Inspired by neuroscientific evidence that human
encodes knowledge as relationship cognitive maps, we introduce Structure-aware
Language-Image Pretraining (SLIP). SLIP integrates a structural contrastive
loss to align modalities while also modeling relationships between neighboring
entities in a structured graph. To support this paradigm, we construct a
large-scale Amazon Product Co-purchase Multimodal Graph Dataset, enabling
structured cross-modality supervision at scale. Experiment results show that
SLIP consistently outperforms CLIP on cross-modal retrieval and classification
tasks in both zero-shot and few-shot settings, showing the value of relational
supervision for cross-modal alignment.

</details>


### [10] [From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth](https://arxiv.org/abs/2511.03053)
*Ziyang Xu,Olaf Wysocki,Christoph Holst*

Main category: cs.CV

TL;DR: 作者提出一个结合最优邻域估计和几何特征的学习框架，利用XGBoost预测MLS点云的点级不确定性（以C2C距离衡量），在真实数据上效果可行且效率高于Random Forest。


<details>
  <summary>Details</summary>
Motivation: 实际高精度应用（Scan-to-BIM、形变分析、三维建模）对MLS点云不确定性评估有强需求，但获取真实GT成本高且不可行，因而需要替代的评估方法。

Method: 提出一个学习框架：首先进行最优邻域估计，然后提取几何特征，最后使用XGBoost（并对比Random Forest）回归预测以点对点（C2C）距离量化的不确定性。

Result: 在真实数据集上的实验表明：框架可行，XGBoost在准确性上可与Random Forest媲美，但效率更高（约快3倍），证实几何特征可用于预测点级C2C不确定性。

Conclusion: 该研究表明，利用几何特征并结合最优邻域估计，可通过机器学习模型（如XGBoost）有效预测MLS点云的点级不确定性，从而减少对昂贵实测GT的依赖。

Abstract: Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning
(MLS) point clouds in many high-precision applications such as Scan-to-BIM,
deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)
for evaluation is often costly and infeasible in many real-world applications.
To reduce this long-standing reliance on GT in uncertainty evaluation research,
this study presents a learning-based framework for MLS point clouds that
integrates optimal neighborhood estimation with geometric feature extraction.
Experiments on a real-world dataset show that the proposed framework is
feasible and the XGBoost model delivers fully comparable accuracy to Random
Forest while achieving substantially higher efficiency (about 3 times faster),
providing initial evidence that geometric features can be used to predict
point-level uncertainty quantified by the C2C distance. In summary, this study
shows that MLS point clouds' uncertainty is learnable, offering a novel
learning-based viewpoint towards uncertainty evaluation research.

</details>


### [11] [A Plug-and-Play Framework for Volumetric Light-Sheet Image Reconstruction](https://arxiv.org/abs/2511.03093)
*Yi Gong,Xinyuan Zhang,Jichen Chai,Yichen Ding,Yifei Lou*

Main category: cs.CV

TL;DR: 把压缩感知与光片显微结合，借助DMD随机编码与PnP-ADMM恢复（含TV/BM3D等去噪器）并加时序平滑，实现高压缩、低光毒的快速心脏三维成像，斑马鱼实验表明效果良好。


<details>
  <summary>Details</summary>
Motivation: 传统光学成像在快速跳动的心脏组织上受限于时空分辨率的权衡，难以在低光照下获取动态细胞结构；因此需要一种既高效又低光毒的成像与恢复方法。

Method: 使用DMD进行随机二值掩码编码以实现压缩采集，构建基于ADMM求解的Plug-and-Play框架，集成多种去噪器（Tikhonov、TV、BM3D），并加入沿z轴相邻切片的时间/空时平滑正则化。

Result: 在斑马鱼心脏实验中，方法在高压缩比下仍能保持良好去噪和结构清晰度，验证了算法在真实生物高速低光成像场景下的有效性与鲁棒性。

Conclusion: 该论文提出的将压缩感知与光片显微结合的计算成像框架，在高压缩比、低光毒条件下能有效重建心脏细胞结构，方法鲁棒且适用于快速动态生物成像。

Abstract: Cardiac contraction is a rapid, coordinated process that unfolds across
three-dimensional tissue on millisecond timescales. Traditional optical imaging
is often inadequate for capturing dynamic cellular structure in the beating
heart because of a fundamental trade-off between spatial and temporal
resolution. To overcome these limitations, we propose a high-performance
computational imaging framework that integrates Compressive Sensing (CS) with
Light-Sheet Microscopy (LSM) for efficient, low-phototoxic cardiac imaging. The
system performs compressed acquisition of fluorescence signals via random
binary mask coding using a Digital Micromirror Device (DMD). We propose a
Plug-and-Play (PnP) framework, solved using the alternating direction method of
multipliers (ADMM), which flexibly incorporates advanced denoisers, including
Tikhonov, Total Variation (TV), and BM3D. To preserve structural continuity in
dynamic imaging, we further introduce temporal regularization enforcing
smoothness between adjacent z-slices. Experimental results on zebrafish heart
imaging under high compression ratios demonstrate that the proposed method
successfully reconstructs cellular structures with excellent denoising
performance and image clarity, validating the effectiveness and robustness of
our algorithm in real-world high-speed, low-light biological imaging scenarios.

</details>


### [12] [ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly](https://arxiv.org/abs/2511.03098)
*Miftahur Rahman,Samuel Adebayo,Dorian A. Acevedo-Mejia,David Hester,Daniel McPolin,Karen Rafferty,Debra F. Laefer*

Main category: cs.CV

TL;DR: ISC-Perception：首个针对 ISC 组件检测的混合数据集，自动标注合成数据，显著降低标注成本并提升检测性能，数据集可按需获取。


<details>
  <summary>Details</summary>
Motivation: 现场采集受限于安全、隐私与物流，缺少专门用于ISC组件检测的图像语料；需要一个可自动标注且高效的替代方案以推动ISC机器人感知发展。

Method: 构建混合数据集（程序渲染CAD、游戏引擎光真场景、少量真实图像），自动标注合成部分，计算并报告人工投入时间，对比训练不同数据组合的检测器性能，并在1200帧台架测试中评估。

Result: 在mAP@0.50上达到0.756，混合数据训练优于仅合成或仅光真数据；10,000幅图像的数据集生成耗时30.5小时，较每图60秒人工标注的166.7小时减少81.7%。台架测试mAP@0.50/mAP@[0.50:0.95]=0.943/0.823。

Conclusion: ISC-Perception 有效弥合了建筑机器人感知的数据缺口，通过混合合成渲染、游戏引擎光真场景和少量真实照片，显著降低人工标注成本并提升检测性能。

Abstract: The Intermeshed Steel Connection (ISC) system, when paired with robotic
manipulators, can accelerate steel-frame assembly and improve worker safety by
eliminating manual assembly. Dependable perception is one of the initial stages
for ISC-aware robots. However, this is hampered by the absence of a dedicated
image corpus, as collecting photographs on active construction sites is
logistically difficult and raises safety and privacy concerns. In response, we
introduce ISC-Perception, the first hybrid dataset expressly designed for ISC
component detection. It blends procedurally rendered CAD images, game-engine
photorealistic scenes, and a limited, curated set of real photographs, enabling
fully automatic labelling of the synthetic portion. We explicitly account for
all human effort to produce the dataset, including simulation engine and scene
setup, asset preparation, post-processing scripts and quality checks; our total
human time to generate a 10,000-image dataset was 30.5,h versus 166.7,h for
manual labelling at 60,s per image (-81.7%). A manual pilot on a representative
image with five instances of ISC members took 60,s (maximum 80,s), anchoring
the manual baseline. Detectors trained on ISC-Perception achieved a mean
Average Precision at IoU 0.50 of 0.756, substantially surpassing models trained
on synthetic-only or photorealistic-only data. On a 1,200-frame bench test, we
report mAP@0.50/mAP@[0.50:0.95] of 0.943/0.823. By bridging the data gap for
construction-robotics perception, ISC-Perception facilitates rapid development
of custom object detectors and is freely available for research and industrial
use upon request.

</details>


### [13] [DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs](https://arxiv.org/abs/2511.03099)
*Yiyi Miao,Taoyu Wu,Tong Chen,Sihao Li,Ji Jiang,Youpeng Yang,Angelos Stefanidis,Limin Yu,Jionglong Su*

Main category: cs.CV

TL;DR: 提出DentalSplat：通过先验引导的稠密立体初始化、尺度自适应剪枝、光流几何约束与梯度正则化，解决三视图无位姿正畸影像下的3DGS重建，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 正畸远程医疗中常仅能获取正中和双侧面（3张）稀疏影像，传统3DGS依赖密集多视角和精确相机位姿，导致在临床场景下难以应用；因此需要能在稀疏无位姿输入下仍能稳定重建的方法。

Method: 方法先用基于先验引导的稠密立体（dense stereo）模型生成初始点云，然后采用尺度自适应剪枝策略精简点云以提升3D Gaussian Splatting训练效率与质量；在极稀疏视角下引入光流作为几何约束，并结合梯度正则化以增强渲染保真度。

Result: 在950例临床病例和195例视频模拟测试集上验证，DentalSplat在稀疏视角和无位姿情形下优于现有方法，呈现更高的视角合成质量与重建精度。

Conclusion: 本文提出的DentalSplat在稀疏正畸影像的3D重建任务中有效，能在仅有三视图且无相机位姿信息条件下实现高质量的牙颌表面重建与新视角合成。

Abstract: In orthodontic treatment, particularly within telemedicine contexts,
observing patients' dental occlusion from multiple viewpoints facilitates
timely clinical decision-making. Recent advances in 3D Gaussian Splatting
(3DGS) have shown strong potential in 3D reconstruction and novel view
synthesis. However, conventional 3DGS pipelines typically rely on densely
captured multi-view inputs and precisely initialized camera poses, limiting
their practicality. Orthodontic cases, in contrast, often comprise only three
sparse images, specifically, the anterior view and bilateral buccal views,
rendering the reconstruction task especially challenging. The extreme sparsity
of input views severely degrades reconstruction quality, while the absence of
camera pose information further complicates the process. To overcome these
limitations, we propose DentalSplat, an effective framework for 3D
reconstruction from sparse orthodontic imagery. Our method leverages a
prior-guided dense stereo reconstruction model to initialize the point cloud,
followed by a scale-adaptive pruning strategy to improve the training
efficiency and reconstruction quality of 3DGS. In scenarios with extremely
sparse viewpoints, we further incorporate optical flow as a geometric
constraint, coupled with gradient regularization, to enhance rendering
fidelity. We validate our approach on a large-scale dataset comprising 950
clinical cases and an additional video-based test set of 195 cases designed to
simulate real-world remote orthodontic imaging conditions. Experimental results
demonstrate that our method effectively handles sparse input scenarios and
achieves superior novel view synthesis quality for dental occlusion
visualization, outperforming state-of-the-art techniques.

</details>


### [14] [Image-Intrinsic Priors for Integrated Circuit Defect Detection and Novel Class Discovery via Self-Supervised Learning](https://arxiv.org/abs/2511.03120)
*Botong. Zhao,Xubin. Wang,Shujing. Lyu,Yue. Lu*

Main category: cs.CV

TL;DR: 提出无支持集的IC DefectNCD，结合可学习正常特征重建、适应性二值化和软掩模注意力的师生分类策略，实现对IC SEM图像的鲁棒缺陷检测与新类别发现。


<details>
  <summary>Details</summary>
Motivation: IC制造流程复杂，缺陷类型繁多且存在稀有/新出现类别，监督方法依赖人工标注且难以扩展，聚类无监督方法缺乏先验导致不稳定。利用IC SEM图像内在先验可在无支持集条件下提升检测与新类发现性能。

Method: 方法包含三部分：1) Self Normal Information Guided IC Defect Detection：通过可学习的正常信息提取器聚合代表性正常特征，基于重建残差粗定位缺陷区域；2) 自适应二值化策略：对不同显著性缺陷生成稳定子图，聚焦核心缺陷区域；3) Self Defect Information Guided IC Defect Classification：在师生模型中引入软掩模引导的注意力机制，将空间缺陷先验注入以增强对缺陷区域的敏感性并抑制背景干扰，从而实现对未见缺陷的识别与分类。

Result: 在跨越三大关键制程、包含15类缺陷的真实数据集上验证，实验显示该方法在缺陷检测和未见缺陷分类上均取得稳健效果（文中报告了定量性能提升与鲁棒性）。

Conclusion: 本文提出的IC DefectNCD方法在真实IC SEM数据集上表现出对缺陷检测与新类别发现的鲁棒性，说明利用图像内在先验可在无监督/半监督场景下提高对罕见与未见缺陷的识别能力。

Abstract: Integrated circuit manufacturing is highly complex, comprising hundreds of
process steps. Defects can arise at any stage, causing yield loss and
ultimately degrading product reliability. Supervised methods require extensive
human annotation and struggle with emergent categories and rare, data scarce
defects. Clustering-based unsupervised methods often exhibit unstable
performance due to missing priors. We propose IC DefectNCD, a support set free
framework that leverages Image Intrinsic Priors in IC SEM images for defect
detection and novel class discovery. We first develop Self Normal Information
Guided IC Defect Detection, aggregating representative normal features via a
learnable normal information extractor and using reconstruction residuals to
coarsely localize defect regions. To handle saliency variations across defects,
we introduce an adaptive binarization strategy that produces stable subimages
focused on core defective areas. Finally, we design Self Defect Information
Guided IC Defect Classification, which incorporates a soft mask guided
attention mechanism to inject spatial defect priors into the teacher student
model. This enhances sensitivity to defective regions, suppresses background
interference, and enables recognition and classification of unseen defects. We
validate the approach on a real world dataset spanning three key fabrication
stages and covering 15 defect types. Experiments demonstrate robust performance
on both defect detection and unseen defect classification.

</details>


### [15] [Accelerating Physical Property Reasoning for Augmented Visual Cognition](https://arxiv.org/abs/2511.03126)
*Hongbo Lan,Zhenlin An,Haoyu Li,Vaibhav Singh,Longfei Shangguan*

Main category: cs.CV

TL;DR: 提出一套通过3D重建、语义融合与并行编码优化的系统，能将物理属性推理延迟从分钟级降到秒级，同时保持或提升精度，并能结合眼动定位在智能眼镜场景中实用。


<details>
  <summary>Details</summary>
Motivation: 目标是将原本耗时10-20分钟的物理属性推理管线加速到可实时/近实时的程度，以支持增强视觉认知与智能眼镜等实时应用。

Method: 通过快速的几何3D重建、高效的语义特征融合和并行视图编码等优化手段，\sysname使推理流程能够在低延迟下运行，并结合凝视追踪定位感兴趣物体以适配智能眼镜场景。

Result: 在ABO数据集上，相比两个SOTA基线，\sysname在速度上达到了62.9×—287.2×的加速，同时在物体级别质量估计上达到相当或略优的准确率，并在材质分割与体素级推理上表现更好；在Meta Aria智能眼镜的宜家门店实测中，对少量视图也能稳定估计属性。

Conclusion: 该论文提出了一个名为\sysname的系统，通过算法与系统级优化显著降低视觉引导物理属性推理的端到端延迟，并在准确率上不比现有方法差且在某些任务上更优。

Abstract: This paper introduces \sysname, a system that accelerates vision-guided
physical property reasoning to enable augmented visual cognition. \sysname
minimizes the run-time latency of this reasoning pipeline through a combination
of both algorithmic and systematic optimizations, including rapid geometric 3D
reconstruction, efficient semantic feature fusion, and parallel view encoding.
Through these simple yet effective optimizations, \sysname reduces the
end-to-end latency of this reasoning pipeline from 10--20 minutes to less than
6 seconds. A head-to-head comparison on the ABO dataset shows that \sysname
achieves this 62.9$\times$--287.2$\times$ speedup while not only reaching
on-par (and sometimes slightly better) object-level physical property
estimation accuracy(e.g. mass), but also demonstrating superior performance in
material segmentation and voxel-level inference than two SOTA baselines. We
further combine gaze-tracking with \sysname to localize the object of interest
in cluttered, real-world environments, streamlining the physical property
reasoning on smart glasses. The case study with Meta Aria Glasses conducted at
an IKEA furniture store demonstrates that \sysname achives consistently high
performance compared to controlled captures, providing robust property
estimations even with fewer views in real-world scenarios.

</details>


### [16] [Deploying Rapid Damage Assessments from sUAS Imagery for Disaster Response](https://arxiv.org/abs/2511.03132)
*Thomas Manzini,Priyankari Perali,Robin R. Murphy*

Main category: cs.CV

TL;DR: 首次将AI/ML建筑损伤评估从学术转向实际sUAS部署：用2万余标注训练模型，培训91名从业者，现场在18分钟评估415栋建筑，附部署经验与教训。


<details>
  <summary>Details</summary>
Motivation: 现场sUAS团队在灾后采集海量影像（每天47GB–369GB），超出专家解读能力与传输限制，延误救援决策，因此需要自动化的视觉评估以缓解数据洪流。

Method: 收集并构建了包含21,716栋建筑损伤标注的sUAS灾后航空影像数据集，训练多种计算机视觉/机器学习模型；对91名灾害从业者进行操作训练；将最佳模型集成到现场工作流程中并部署在飓风Debby和Helene响应期间。

Result: 最佳模型在两次飓风响应中约18分钟内评估了415栋建筑；展示了在真实灾害响应中使用AI/ML的可行性，并提供部署文档与经验总结。

Conclusion: 本文实现了首个面向实际联邦宣布灾难中的sUAS影像的建筑损伤自动评估AI/ML系统，并在两次飓风响应中部署验证。

Abstract: This paper presents the first AI/ML system for automating building damage
assessment in uncrewed aerial systems (sUAS) imagery to be deployed
operationally during federally declared disasters (Hurricanes Debby and
Helene). In response to major disasters, sUAS teams are dispatched to collect
imagery of the affected areas to assess damage; however, at recent disasters,
teams collectively delivered between 47GB and 369GB of imagery per day,
representing more imagery than can reasonably be transmitted or interpreted by
subject matter experts in the disaster scene, thus delaying response efforts.
To alleviate this data avalanche encountered in practice, computer vision and
machine learning techniques are necessary. While prior work has been deployed
to automatically assess damage in satellite imagery, there is no current state
of practice for sUAS-based damage assessment systems, as all known work has
been confined to academic settings. This work establishes the state of practice
via the development and deployment of models for building damage assessment
with sUAS imagery. The model development involved training on the largest known
dataset of post-disaster sUAS aerial imagery, containing 21,716 building damage
labels, and the operational training of 91 disaster practitioners. The best
performing model was deployed during the responses to Hurricanes Debby and
Helene, where it assessed a combined 415 buildings in approximately 18 minutes.
This work contributes documentation of the actual use of AI/ML for damage
assessment during a disaster and lessons learned to the benefit of the AI/ML
research and user communities.

</details>


### [17] [Finetuning-Free Personalization of Text to Image Generation via Hypernetworks](https://arxiv.org/abs/2511.03156)
*Sagar Shrestha,Gopal Sharma,Luowei Zhou,Suren Kumar*

Main category: cs.CV

TL;DR: 提出一种无微调超网络方法以预测LoRA权重，实现快速个性化；用输出正则化稳定训练；引入HM-CFG提高组合化能力；在多个数据集上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法（如DreamBooth）需要耗时的主题特定微调，计算成本高。现有的适配器或编码器方法仍需额外微调或依赖大型主干模型。因此，探索无微调且高效的个性化方法具有实际价值。

Method: 训练一个端到端的超网络，直接从主题图像预测LoRA适配权重；使用简单的输出正则化来稳定训练；在推断阶段无需对每个主题进行微调；引入HM-CFG在采样时将基础模型的组合性与个性化模型的主题保真度结合。

Result: 在CelebA-HQ、AFHQ-v2和DreamBench上进行的大量实验表明，该方法在个性化性能上表现强劲，兼顾了主题保真度和提示对齐，并且在组合生成任务上通过HM-CFG得到增强。

Conclusion: 该工作提出了一种基于超网络的无微调个性化方法，通过直接从主题图像预测LoRA权重，实现了在测试时无需每个主题的优化，同时保持主题保真度和提示对齐。方法还引入了混合模型无分类器引导（HM-CFG）来增强组合推广能力。整体结论是超网络为开放类别个性化提供了可扩展且有效的方向。

Abstract: Personalizing text-to-image diffusion models has traditionally relied on
subject-specific fine-tuning approaches such as
DreamBooth~\cite{ruiz2023dreambooth}, which are computationally expensive and
slow at inference. Recent adapter- and encoder-based methods attempt to reduce
this overhead but still depend on additional fine-tuning or large backbone
models for satisfactory results. In this work, we revisit an orthogonal
direction: fine-tuning-free personalization via Hypernetworks that predict
LoRA-adapted weights directly from subject images. Prior hypernetwork-based
approaches, however, suffer from costly data generation or unstable attempts to
mimic base model optimization trajectories. We address these limitations with
an end-to-end training objective, stabilized by a simple output regularization,
yielding reliable and effective hypernetworks. Our method removes the need for
per-subject optimization at test time while preserving both subject fidelity
and prompt alignment. To further enhance compositional generalization at
inference time, we introduce Hybrid-Model Classifier-Free Guidance (HM-CFG),
which combines the compositional strengths of the base diffusion model with the
subject fidelity of personalized models during sampling. Extensive experiments
on CelebA-HQ, AFHQ-v2, and DreamBench demonstrate that our approach achieves
strong personalization performance and highlights the promise of hypernetworks
as a scalable and effective direction for open-category personalization.

</details>


### [18] [Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation](https://arxiv.org/abs/2511.03163)
*Yun-Chen Lin,Jiayuan Huang,Hanyuan Zhang,Sergi Kavtaradze,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出SRFT-GaLore增强的双编码器（SAM2+DA2）与跨注意力融合方法，高效微调大模型并融合RGB与深度信息，实现肝脏标志物分割的精度与泛化提升。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中二维视频限制深度感知，导致标志物定位困难；现有工作在RGB与深度特征融合及大模型高效适配方面存在挑战，因此需要一个融合语义与几何信息且能高效微调大模型的框架。

Method: 采用SAM2提取RGB语义特征，DA2提取深度感知几何特征，设计跨注意力模块融合两类特征；为高效微调大尺度注意力层，提出SRFT-GaLore方法，用SRFT替代SVD实现低秩梯度投影，从而降低计算复杂度并保持表示能力。

Result: 在公开L3D数据集上，Dice提高4.85%，ASSD降低11.78点；在新构建的LLSD外部验证集上，显著优于SAM基线，表现稳定，证明了方法的跨数据集鲁棒性与实时深度受限场景适用性。

Conclusion: 本文提出的深度引导肝脏标志物分割框架，通过结合SAM2和DA2双编码器、SRFT-GaLore低秩梯度投影和跨注意力融合，实现了在腹腔镜视频中更精确和高效的标志物分割。实验表明在L3D和新建的LLSD数据集上均显著优于基线，具备良好的跨数据集泛化能力。

Abstract: Accurate detection and delineation of anatomical structures in medical
imaging are critical for computer-assisted interventions, particularly in
laparoscopic liver surgery where 2D video streams limit depth perception and
complicate landmark localization. While recent works have leveraged monocular
depth cues for enhanced landmark detection, challenges remain in fusing RGB and
depth features and in efficiently adapting large-scale vision models to
surgical domains. We propose a depth-guided liver landmark segmentation
framework integrating semantic and geometric cues via vision foundation
encoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGB
features and Depth Anything V2 (DA2) encoder to extract depth-aware features.
To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradient
projection method that replaces the computationally expensive SVD with a
Subsampled Randomized Fourier Transform (SRFT). This enables efficient
fine-tuning of high-dimensional attention layers without sacrificing
representational power. A cross-attention fusion module further integrates RGB
and depth cues. To assess cross-dataset generalization, we also construct a new
Laparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.
On the public L3D dataset, our method achieves a 4.85% improvement in Dice
Similarity Coefficient and a 11.78-point reduction in Average Symmetric Surface
Distance compared to the D2GPLand. To further assess generalization capability,
we evaluate our model on LLSD dataset. Our model maintains competitive
performance and significantly outperforms SAM-based baselines, demonstrating
strong cross-dataset robustness and adaptability to unseen surgical
environments. These results demonstrate that our SRFT-GaLore-enhanced
dual-encoder framework enables scalable and precise segmentation under
real-time, depth-constrained surgical settings.

</details>


### [19] [SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention](https://arxiv.org/abs/2511.03178)
*Shreyas C. Dhake,Jiayuan Huang,Runlong He,Danyal Z. Khan,Evangelos B. Mazomenos,Sophia Bano,Hani J. Marcus,Danail Stoyanov,Matthew J. Clarkson,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 论文提出首个面向未来推理的外科VQA数据集PitVQA-Anticipation和SurgAnt-ViVQA模型，通过GRU时间编码与门控跨模态注意力，实现从描述转向术中事件预测，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 内镜经蝶鞍垂体手术中能见度低且流程快速变化，主动预测下一步阶段、器械及剩余时间能提高实时辅助的可靠性。

Method: 构建PitVQA-Anticipation数据集（33.5小时视频，734,769 QA对），并提出SurgAnt-ViVQA模型：使用双向GRU进行时间编码，GRU门控的时间交叉注意力在token级注入视觉信息，结合参数高效微调LLM。

Result: 在PitVQA-Anticipation和EndoVis上优于图像/视频基线；消融实验表明时间递归与门控融合贡献最大；帧数实验显示8帧提升语言流畅性，32帧略降BLEU但改善时间估计。

Conclusion: 该论文成功从回顾性视觉问答扩展到前瞻性外科推理，提出新的数据集和模型，显著提升了术中预测任务表现。

Abstract: Anticipating forthcoming surgical events is vital for real-time assistance in
endonasal transsphenoidal pituitary surgery, where visibility is limited and
workflow changes rapidly. Most visual question answering (VQA) systems reason
on isolated frames with static vision language alignment, providing little
support for forecasting next steps or instrument needs. Existing surgical VQA
datasets likewise center on the current scene rather than the near future. We
introduce PitVQA-Anticipation, the first VQA dataset designed for forward
looking surgical reasoning. It comprises 33.5 hours of operative video and
734,769 question answer pairs built from temporally grouped clips and expert
annotations across four tasks: predicting the future phase, next step, upcoming
instrument, and remaining duration. We further propose SurgAnt-ViVQA, a video
language model that adapts a large language model using a GRU Gated Temporal
Cross-Attention module. A bidirectional GRU encodes frame to frame dynamics,
while an adaptive gate injects visual context into the language stream at the
token level. Parameter efficient fine tuning customizes the language backbone
to the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation and
EndoVis datasets, surpassing strong image and video based baselines. Ablations
show that temporal recurrence and gated fusion drive most of the gains. A frame
budget study indicates a trade-off: 8 frames maximize fluency, whereas 32
frames slightly reduce BLEU but improve numeric time estimation. By pairing a
temporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQA
advances surgical VQA from retrospective description to proactive anticipation.
PitVQA-Anticipation offers a comprehensive benchmark for this setting and
highlights the importance of targeted temporal modeling for reliable, future
aware surgical assistance.

</details>


### [20] [PETWB-REP: A Multi-Cancer Whole-Body FDG PET/CT and Radiology Report Dataset for Medical Imaging Research](https://arxiv.org/abs/2511.03194)
*Le Xue,Gang Feng,Wenbo Zhang,Yichi Zhang,Lanlan Li,Shuqi Wang,Liling Peng,Sisi Peng,Xin Gao*

Main category: cs.CV

TL;DR: PETWB-REP是一个包含490例配对全身FDG PET/CT影像与去标识化放射学报告的多肿瘤数据集，面向影像组学与多模态AI研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏同时包含功能（PET）与解剖（CT）影像并配有详尽临床放射学报告、覆盖多种癌症类型的大规模公开数据集，限制了跨模态AI方法和回顾性临床研究的发展。

Method: 整理并去标识化490例患者的全身18F-FDG PET/CT配对图像、结构化临床元数据和放射科文本报告，按常见癌种（如肺、肝、乳、前列腺、卵巢癌）进行分类与注释，构建可用于影像组学与多模态AI研究的数据集。

Result: 得到一个包含490名患者、配对PET/CT影像、去标识化文本报告和结构化临床信息的数据集，覆盖多种常见实体肿瘤，已整理为方便研究使用的格式，支持影像学、radiomics、人工智能和多模态学习研究。

Conclusion: PETWB-REP填补了多肿瘤类型、配对全身FDG PET/CT影像与放射学报告的公开数据空白，是多模态医学影像与临床文本研究的重要资源。

Abstract: Publicly available, large-scale medical imaging datasets are crucial for
developing and validating artificial intelligence models and conducting
retrospective clinical research. However, datasets that combine functional and
anatomical imaging with detailed clinical reports across multiple cancer types
remain scarce. Here, we present PETWB-REP, a curated dataset comprising
whole-body 18F-Fluorodeoxyglucose (FDG) Positron Emission Tomography/Computed
Tomography (PET/CT) scans and corresponding radiology reports from 490 patients
diagnosed with various malignancies. The dataset primarily includes common
cancers such as lung cancer, liver cancer, breast cancer, prostate cancer, and
ovarian cancer. This dataset includes paired PET and CT images, de-identified
textual reports, and structured clinical metadata. It is designed to support
research in medical imaging, radiomics, artificial intelligence, and
multi-modal learning.

</details>


### [21] [QG-CoC: Question-Guided Chain-of-Captions for Large Multimodal Models](https://arxiv.org/abs/2511.03206)
*Kuei-Chun Kao,Hsu Tzu-Yin,Yunqi Hong,Ruochen Wang,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出QG-CoC——一种问题引导的链式描述零样本提示法，提升MLLM在多图细粒度感知与跨图推理的能力，在多模型多任务评测中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法多聚焦单图或受限场景，无法在多图任务中实现精细感知与有效的多图信息融合；因此需要一种通用的零样本提示策略来提升多图推理能力。

Method: 提出零样本提示方法Question-Guided Chain-of-Captions（QG-CoC），通过问题引导的逐图描述链式生成，将多图信息串联并聚焦于与问题相关的细粒度线索，支持任意数量图片的输入，并在多种开源与闭源MLLMs上评测。

Result: 在多图与单图基准上，QG-CoC相比现有提示方法在困难场景下表现更稳健、性能具有竞争力，尤其在需要细粒度线索和跨图综合推理的任务中有明显改进。

Conclusion: QG-CoC方法在多图推理场景下能提高细粒度感知与推理整合，弥补现有提示方法的不足，展现出在多图与单图任务中的稳健性能提升。

Abstract: Recently, Multimodal Large Language Models (MLLMs) encounter two key issues
in multi-image contexts: (1) a lack of fine-grained perception across disparate
images, and (2) a diminished capability to effectively reason over and
synthesize information from multiple visual inputs. However, while various
prompting methods aim to describe visual content, many existing studies focus
primarily on single-image settings or specific, constrained scenarios. This
leaves a critical gap in understanding and addressing how MLLMs tackle more
general and complex multi-image reasoning tasks. Thus, we first extensively
investigate how current prompting methods perceive fine-grained visual details
and process visual information when dealing with multiple images. Our findings
reveal that existing prompting methods fall short in attending to needed clues
and seamlessly integrating perception and reasoning. Inspired by the findings,
we propose a new zero-shot prompting method, Question-Guided Chain-of-Captions
(QG-CoC), a generalized prompting approach that effectively handles problems
with an arbitrary number of images. We evaluate our method on various
open-source and closed-source MLLMs for multi-image and single-image
benchmarks. Experimental results indicate that QG-CoC demonstrates competitive
performance across tasks and exhibits robust improvements in the challenging
scenarios where existing prompting methods fail.

</details>


### [22] [MvBody: Multi-View-Based Hybrid Transformer Using Optical 3D Body Scan for Explainable Cesarean Section Prediction](https://arxiv.org/abs/2511.03212)
*Ruting Cheng,Boyuan Feng,Yijiang Zheng,Chuhui Qiu,Aizierjiang Aiersilan,Joaquin A. Calderon,Wentao Zhao,Qing Pan,James K. Hahn*

Main category: cs.CV

TL;DR: 本文用3D体型与自报数据训练多视角Transformer（MvBody）并结合度量学习，能在孕晚期实现有效剖宫产风险预测（AUC0.724），并用Integrated Gradients解释重要特征。


<details>
  <summary>Details</summary>
Motivation: 现有剖宫产风险预测模型多基于产时住院参数，难以应用于资源受限或居家环境；探索低成本通用设备获取的3D体型信息能否用于早期、可及的风险评估。

Method: 提出多视角Transformer网络MvBody，输入为31–38周孕期的3D光学体扫与自报医疗信息；在训练中加入度量学习损失以提升小样本下的训练效率与泛化性能，并采用Integrated Gradients解释模型预测。

Result: 在独立测试集上达成84.62%准确率与0.724 AUC-ROC；模型识别出的关键预测因子包括孕前体重、母亲年龄、产科史、既往剖宫产史及头肩部体型特征。

Conclusion: 研究表明，基于3D体型与自报医疗数据的多视角Transformer模型（MvBody）可在孕晚期预测剖宫产风险，且在独立测试集上优于传统机器学习与现有3D分析方法。

Abstract: Accurately assessing the risk of cesarean section (CS) delivery is critical,
especially in settings with limited medical resources, where access to
healthcare is often restricted. Early and reliable risk prediction allows
better-informed prenatal care decisions and can improve maternal and neonatal
outcomes. However, most existing predictive models are tailored for in-hospital
use during labor and rely on parameters that are often unavailable in
resource-limited or home-based settings. In this study, we conduct a pilot
investigation to examine the feasibility of using 3D body shape for CS risk
assessment for future applications with more affordable general devices. We
propose a novel multi-view-based Transformer network, MvBody, which predicts CS
risk using only self-reported medical data and 3D optical body scans obtained
between the 31st and 38th weeks of gestation. To enhance training efficiency
and model generalizability in data-scarce environments, we incorporate a metric
learning loss into the network. Compared to widely used machine learning models
and the latest advanced 3D analysis methods, our method demonstrates superior
performance, achieving an accuracy of 84.62% and an Area Under the Receiver
Operating Characteristic Curve (AUC-ROC) of 0.724 on the independent test set.
To improve transparency and trust in the model's predictions, we apply the
Integrated Gradients algorithm to provide theoretically grounded explanations
of the model's decision-making process. Our results indicate that pre-pregnancy
weight, maternal age, obstetric history, previous CS history, and body shape,
particularly around the head and shoulders, are key contributors to CS risk
prediction.

</details>


### [23] [Diffusion-Guided Mask-Consistent Paired Mixing for Endoscopic Image Segmentation](https://arxiv.org/abs/2511.03219)
*Pengyu Jie,Wanquan Liu,Rui He,Yihui Wen,Deyu Meng,Chenqiang Gao*

Main category: cs.CV

TL;DR: 通过在相同掩码下生成真实-合成图像对，并只混合外观（保留硬标签）再辅以自适应回归机制，论文实现了既保证像素级语义一致又提升多样性的分割增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合方法因掩码错位导致软标签模糊，而扩散合成虽增强多样性但忽视掩码条件结构并引入合成-真实域差异。需要一种兼顾标签保真性与样本多样性的增强策略，同时避免合成样本造成的域偏差。

Method: 对每个真实图像生成一个在相同掩码约束下的合成图像，形成配对样本；使用MCPMix只混合图像外观（不混掩码），训练时始终使用原始硬标签；引入RLA，在训练过程中自适应调节混合比例和混合样本的损失权重，逐步把学习锚定回真实数据。

Result: 在多个内窥镜和皮肤病变分割数据集（Kvasir-SEG, PICCOLO, CVC-ClinicDB, 私有NPC-LES, ISIC2017）上均取得了最先进的分割性能，并稳定优于基线方法，证明了该方法在增强鲁棒性和泛化性方面的有效性。

Conclusion: 该论文提出了将扩增混合与扩散生成相结合的配对范式，利用同一掩码下的真实-合成图像对，通过Mask-Consistent Paired Mixing在外观层面混合而保留硬掩码监督，从而在共享几何下生成从真实到合成的连续样本；同时提出Real-Anchored Learnable Annealing自适应调整混合强度和损失权重，逐步回归真实数据以缓解域偏差。

Abstract: Augmentation for dense prediction typically relies on either sample mixing or
generative synthesis. Mixing improves robustness but misaligned masks yield
soft label ambiguity. Diffusion synthesis increases apparent diversity but,
when trained as common samples, overlooks the structural benefit of mask
conditioning and introduces synthetic-real domain shift. We propose a paired,
diffusion-guided paradigm that fuses the strengths of both. For each real
image, a synthetic counterpart is generated under the same mask and the pair is
used as a controllable input for Mask-Consistent Paired Mixing (MCPMix), which
mixes only image appearance while supervision always uses the original hard
mask. This produces a continuous family of intermediate samples that smoothly
bridges synthetic and real appearances under shared geometry, enlarging
diversity without compromising pixel-level semantics. To keep learning aligned
with real data, Real-Anchored Learnable Annealing (RLA) adaptively adjusts the
mixing strength and the loss weight of mixed samples over training, gradually
re-anchoring optimization to real data and mitigating distributional bias.
Across Kvasir-SEG, PICCOLO, CVC-ClinicDB, a private NPC-LES cohort, and ISIC
2017, the approach achieves state-of-the-art segmentation performance and
consistent gains over baselines. The results show that combining
label-preserving mixing with diffusion-driven diversity, together with adaptive
re-anchoring, yields robust and generalizable endoscopic segmentation.

</details>


### [24] [Transformer-Progressive Mamba Network for Lightweight Image Super-Resolution](https://arxiv.org/abs/2511.03232)
*Sichen Guo,Wenjie Li,Yuanyang Liu,Guangwei Gao,Jian Yang,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 为解决Mamba方法缺乏尺度间细粒度过渡的问题，本文提出T-PMambaSR：窗口自注意力+Progressive Mamba并配合AHFRM，在线性复杂度下提升特征表示与高频细节恢复，实验性能优于现有方法且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba-based超分方法虽具线性复杂度但缺少不同建模尺度间的细粒度过渡，限制了特征表示效率；因此需要一种在保持线性复杂度下增强尺度间交互与细节恢复的轻量级框架。

Method: 提出T-PMambaSR框架：将窗口化自注意力与Progressive Mamba结合，允许不同尺度感受野间交互，形成细粒度的逐步建模。同时设计自适应高频细化模块(AHFRM)用于恢复Transformer和Mamba处理过程中丢失的高频细节。

Result: T-PMambaSR在一系列对比实验中表现出逐步增强的感受野与表达能力，在性能上优于近期的Transformer-或Mamba-based方法，同时计算代价更低。

Conclusion: T-PMambaSR通过逐步扩展感受野并结合窗口自注意力与渐进式Mamba机制，在保持线性复杂度的同时提升了特征表达能力和细节恢复，实验表明优于近期Transformer或Mamba方法且计算成本更低。

Abstract: Recently, Mamba-based super-resolution (SR) methods have demonstrated the
ability to capture global receptive fields with linear complexity, addressing
the quadratic computational cost of Transformer-based SR approaches. However,
existing Mamba-based methods lack fine-grained transitions across different
modeling scales, which limits the efficiency of feature representation. In this
paper, we propose T-PMambaSR, a lightweight SR framework that integrates
window-based self-attention with Progressive Mamba. By enabling interactions
among receptive fields of different scales, our method establishes a
fine-grained modeling paradigm that progressively enhances feature
representation with linear complexity. Furthermore, we introduce an Adaptive
High-Frequency Refinement Module (AHFRM) to recover high-frequency details lost
during Transformer and Mamba processing. Extensive experiments demonstrate that
T-PMambaSR progressively enhances the model's receptive field and
expressiveness, yielding better performance than recent Transformer- or
Mamba-based methods while incurring lower computational cost. Our codes will be
released after acceptance.

</details>


### [25] [Decoupled Multi-Predictor Optimization for Inference-Efficient Model Tuning](https://arxiv.org/abs/2511.03245)
*Liwei Luo,Shuaitengyuan Li,Dongwei Ren,Qilong Wang,Pengfei Zhu,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出DMPO，通过旁路模块、高阶统计预测器与两阶段损失权重分配，解耦早期层的表征与判别能力，显著提升早期退出场景下的推理效率与性能。


<details>
  <summary>Details</summary>
Motivation: 多阶段预测+早期退出结合参数高效微调能提高推理效率，但存在冲突：早期阶段需要既提供底层表征给深层，又需为自身预测器提供高层判别信息，二者难以兼得。

Method: 方法包括三部分：1) 在多阶段预测器中为浅层引入轻量级旁路模块（bypass）以实现浅层特征的功能分解；2) 为早期阶段设计基于高阶统计量的预测器以增强判别能力；3) 提出解耦优化策略，在微调阶段采用两阶段损失权重分配：初始阶段强调早期的表征能力以帮助深层获取判别能力，后续阶段则推动判别能力向更早的阶段迁移。

Result: 通过在多数据集和不同预训练骨干上的实验，DMPO在减少计算成本的同时，优于现有方法，验证了解耦架构设计与优化策略的有效性。

Conclusion: 该文提出DMPO方法，通过架构与优化双重手段在多阶段预测器中解耦浅层的表征能力与判别能力，从而提升早期退出的推理效率和准确性。实验证明在多数据集与多预训练骨干网络下，DMPO在降低计算成本时显著优于对比方法。

Abstract: Recently, remarkable progress has been made in large-scale pre-trained model
tuning, and inference efficiency is becoming more crucial for practical
deployment. Early exiting in conjunction with multi-stage predictors, when
cooperated with a parameter-efficient fine-tuning strategy, offers a
straightforward way to achieve an inference-efficient model. However, a key
challenge remains unresolved: How can early stages provide low-level
fundamental features to deep stages while simultaneously supplying high-level
discriminative features to early-stage predictors? To address this problem, we
propose a Decoupled Multi-Predictor Optimization (DMPO) method to effectively
decouple the low-level representative ability and high-level discriminative
ability in early stages. First, in terms of architecture, we introduce a
lightweight bypass module into multi-stage predictors for functional
decomposition of shallow features from early stages, while a high-order
statistics-based predictor is developed for early stages to effectively enhance
their discriminative ability. To reasonably train our multi-predictor
architecture, a decoupled optimization is proposed to allocate two-phase loss
weights for multi-stage predictors during model tuning, where the initial
training phase enables the model to prioritize the acquisition of
discriminative ability of deep stages via emphasizing representative ability of
early stages, and the latter training phase drives discriminative ability
towards earlier stages as much as possible. As such, our DMPO can effectively
decouple representative and discriminative abilities in early stages in terms
of architecture design and model optimization. Experiments across various
datasets and pre-trained backbones demonstrate that DMPO clearly outperforms
its counterparts when reducing computational cost.

</details>


### [26] [Generative deep learning for foundational video translation in ultrasound](https://arxiv.org/abs/2511.03255)
*Nikolina Tomic Roshni Bhatnagar,Sarthak Jain,Connor Lau,Tien-Yu Liu,Laura Gambini,Rima Arnaout*

Main category: cs.CV

TL;DR: 作者提出一个两网络、结合像素/对抗/感知损失的超声CFD-灰阶视频生成方法，在大规模数据上表现出高保真性和下游任务可替代性，可用于补齐子模态与数据增强。


<details>
  <summary>Details</summary>
Motivation: 超声影像包含多种子模态（如灰阶与CFD），在临床研究中常常不平衡或缺失；图像翻译可用于补齐与平衡数据集，但超声子模态翻译具有噪声高、组织纹理特殊等挑战。

Method: 基于两个网络的生成框架：一个用于重建解剖结构，另一个用于去噪以提升超声真实性；训练目标结合像素级损失、对抗损失与感知损失；在大量数据（训练54975，测试8368视频）上进行训练与评估。

Result: 生成视频在平均SSIM为0.91±0.04；在分类、分割等下游任务表现与真实视频无显著差异（F1:真实0.9, 合成0.89；Dice两者0.97）；临床专家辨别真实与合成准确率54±6%，显示高真实性；在其他超声领域也保持良好性能（平均SSIM 0.91±0.05）。

Conclusion: 该论文展示了一种高质量的超声彩色流（CFD）与灰阶视频相互翻译的生成方法，能生成在视觉质与下游任务表现上与真实视频难以区分的合成数据，扩展了医学影像数据增强与缺失子模态补齐的可能性。

Abstract: Deep learning (DL) has the potential to revolutionize image acquisition and
interpretation across medicine, however, attention to data imbalance and
missingness is required. Ultrasound data presents a particular challenge
because in addition to different views and structures, it includes several
sub-modalities-such as greyscale and color flow doppler (CFD)-that are often
imbalanced in clinical studies. Image translation can help balance datasets but
is challenging for ultrasound sub-modalities to date. Here, we present a
generative method for ultrasound CFD-greyscale video translation, trained on
54,975 videos and tested on 8,368. The method developed leveraged pixel-wise,
adversarial, and perceptual loses and utilized two networks: one for
reconstructing anatomic structures and one for denoising to achieve realistic
ultrasound imaging. Average pairwise SSIM between synthetic videos and ground
truth was 0.91+/-0.04. Synthetic videos performed indistinguishably from real
ones in DL classification and segmentation tasks and when evaluated by blinded
clinical experts: F1 score was 0.9 for real and 0.89 for synthetic videos; Dice
score between real and synthetic segmentation was 0.97. Overall clinician
accuracy in distinguishing real vs synthetic videos was 54+/-6% (42-61%),
indicating realistic synthetic videos. Although trained only on heart videos,
the model worked well on ultrasound spanning several clinical domains (average
SSIM 0.91+/-0.05), demonstrating foundational abilities. Together, these data
expand the utility of retrospectively collected imaging and augment the dataset
design toolbox for medical imaging.

</details>


### [27] [Enhancing Medical Image Segmentation via Heat Conduction Equation](https://arxiv.org/abs/2511.03260)
*Rong Wu,Yim-Sang Yu*

Main category: cs.CV

TL;DR: 提出U-Mamba+热传导的混合架构，利用状态空间模块处理长程依赖、瓶颈处的热传导算子进行全局语义扩散，在腹部CT/MRI分割任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net变体难以在受限计算预算下同时兼顾高效的全局上下文建模与长程依赖推理，作者希望通过结合状态空间动力学与热扩散机制来弥补这一空白。

Method: 设计了一种混合网络U-Mamba，将Mamba状态空间模块用于编码器/解码器中的长程推理，将Heat Conduction Operators置于瓶颈层以模拟频域热扩散，增强语义抽象。训练在多模态腹部CT和MRI上进行，和强基线模型比较。

Result: 在多模态腹部CT和MRI数据集上，所提模型稳定优于强基线，表明该方法在效果与泛化性上都有提升。

Conclusion: 该论文提出将Mamba基状态空间模块与热传导算子结合到U形分割架构中，以同时实现高效长程依赖建模和全局语义扩散，从而提升医学图像分割性能。

Abstract: Medical image segmentation has been significantly advanced by deep learning
architectures, notably U-Net variants. However, existing models struggle to
achieve efficient global context modeling and long-range dependency reasoning
under practical computational budgets simultaneously. In this work, we propose
a novel hybrid architecture utilizing U-Mamba with Heat Conduction Equation.
Our model combines Mamba-based state-space modules for efficient long-range
reasoning with Heat Conduction Operators (HCOs) in the bottleneck layers,
simulating frequency-domain thermal diffusion for enhanced semantic
abstraction. Experimental results on multimodal abdominal CT and MRI datasets
demonstrate that the proposed model consistently outperforms strong baselines,
validating its effectiveness and generalizability. It suggest that blending
state-space dynamics with heat-based global diffusion offers a scalable and
interpretable solution for medical segmentation tasks.

</details>


### [28] [IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection](https://arxiv.org/abs/2511.03267)
*Bingyang Guo,Hongjie Li,Ruiyun Yu,Hanzhe Liang,Jinbao Wang*

Main category: cs.CV

TL;DR: 提出面向工业设备部件的高保真点云数据集IEC3D-AD与生成式3D-AD范式GMANet，利用几何形态学生成异常样本并通过空间差异优化提升点级异常检测，实验证明效果明显。


<details>
  <summary>Details</summary>
Motivation: 现有3D-AD数据集无法充分反映真实工业现场中零部件的复杂缺陷与高分辨率点云特征，阻碍了对关键工业设备部件（如轴承、环、螺栓）异常检测的精确研究与应用。

Method: 构建高分辨率、细粒度标注的点云数据集（IEC3D-AD）；借鉴2D生成式异常检测思想，通过几何形态学生成合成异常点云样本；采用空间差异优化策略减少正常/异常点特征间距并增加重叠，从而提升点级异常判别能力。

Result: IEC3D-AD在分辨率和标注细粒度上优于现有数据集；GMANet通过生成合成异常与空间差异优化，在IEC3D-AD及其他公开数据集上取得了优越的检测性能，验证了方法与数据集的有效性。

Conclusion: 该论文提出了面向工业设备部件的高保真点云异常检测数据集IEC3D-AD，并在此基础上设计了基于几何形态学生成样本与空间差异优化的3D异常检测新范式GMANet，实验证明在IEC3D-AD及其他数据集上均有效。

Abstract: 3D anomaly detection (3D-AD) plays a critical role in industrial
manufacturing, particularly in ensuring the reliability and safety of core
equipment components. Although existing 3D datasets like Real3D-AD and MVTec
3D-AD offer broad application support, they fall short in capturing the
complexities and subtle defects found in real industrial environments. This
limitation hampers precise anomaly detection research, especially for
industrial equipment components (IEC) such as bearings, rings, and bolts. To
address this challenge, we have developed a point cloud anomaly detection
dataset (IEC3D-AD) specific to real industrial scenarios. This dataset is
directly collected from actual production lines, ensuring high fidelity and
relevance. Compared to existing datasets, IEC3D-AD features significantly
improved point cloud resolution and defect annotation granularity, facilitating
more demanding anomaly detection tasks. Furthermore, inspired by generative
2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. This
paradigm generates synthetic point cloud samples based on geometric
morphological analysis, then reduces the margin and increases the overlap
between normal and abnormal point-level features through spatial discrepancy
optimization. Extensive experiments demonstrate the effectiveness of our method
on both IEC3D-AD and other datasets.

</details>


### [29] [Unified Long Video Inpainting and Outpainting via Overlapping High-Order Co-Denoising](https://arxiv.org/abs/2511.03272)
*Shuangquan Lyu,Steven Mao,Yue Ma*

Main category: cs.CV

TL;DR: 利用LoRA微调与重叠混合时序去噪，高效扩展文本-视频扩散模型，实现可控、高质量且无缝的任意长度视频inpainting/outpainting。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时受限于固定长度窗口或存在拼接伪影、漂移问题；同时对可控性要求高的inpainting/outpainting任务需求尚未满足，故需设计可扩展且可控的长视频编辑方案。

Method: 基于大型预训练文本到视频扩散模型（如阿里巴巴的Wan 2.1），使用LoRA进行参数高效微调以适应掩码区域视频合成；引入重叠混合（overlap-and-blend）时序协同去噪策略并配合高阶求解器，确保长序列中的帧间一致性与无缝衔接。

Result: 在跨数百帧的inpainting/outpainting测试中，本方法在PSNR、SSIM、LPIPS等指标上均优于Wan 2.1和VACE等基线，且在主观真实感上表现更好，实现了低开销下的长距离视频编辑。

Conclusion: 该论文提出了一种统一的长视频修复（inpainting）与延展（outpainting）方法，能够实现对任意长度视频的空间编辑并保持高质量和一致性。

Abstract: Generating long videos remains a fundamental challenge, and achieving high
controllability in video inpainting and outpainting is particularly demanding.
To address both of these challenges simultaneously and achieve controllable
video inpainting and outpainting for long video clips, we introduce a novel and
unified approach for long video inpainting and outpainting that extends
text-to-video diffusion models to generate arbitrarily long, spatially edited
videos with high fidelity. Our method leverages LoRA to efficiently fine-tune a
large pre-trained video diffusion model like Alibaba's Wan 2.1 for masked
region video synthesis, and employs an overlap-and-blend temporal co-denoising
strategy with high-order solvers to maintain consistency across long sequences.
In contrast to prior work that struggles with fixed-length clips or exhibits
stitching artifacts, our system enables arbitrarily long video generation and
editing without noticeable seams or drift. We validate our approach on
challenging inpainting/outpainting tasks including editing or adding objects
over hundreds of frames and demonstrate superior performance to baseline
methods like Wan 2.1 model and VACE in terms of quality (PSNR/SSIM), and
perceptual realism (LPIPS). Our method enables practical long-range video
editing with minimal overhead, achieved a balance between parameter efficient
and superior performance.

</details>


### [30] [Diffusion-SDPO: Safeguarded Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2511.03317)
*Minghao Fu,Guo-Hua Wang,Tianyu Cui,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: 针对Diffusion-DPO会在放大偏好边际时恶化生成质量的问题，作者提出Diffusion-SDPO：通过闭式自适应缩放败者梯度来保障赢家误差不增，简单高效，并在多项基准上带来一致改进。


<details>
  <summary>Details</summary>
Motivation: 发现标准Diffusion-DPO在试图放大偏好边际时并不总能提升生成质量，反而可能通过增加赢家和/或败者的重建误差导致首选输出质量下降。需要一种保护赢家的更新策略以避免这种病态。

Method: 基于一阶分析推导出闭式缩放系数，根据败者梯度与赢家梯度的对齐程度调整败者梯度大小，确保每步优化中首选分支误差非增。该更新规则易于与现有DPO框架兼容，计算开销很小。

Result: 在标准文本到图像基准上，Diffusion-SDPO在自动偏好评分、美学和提示对齐等指标上均优于现有的偏好学习基线，且代码已开源。

Conclusion: 提出的Diffusion-SDPO通过自适应缩放败者梯度以保护赢家，能在优化过程中保证首选输出误差不增大，从而修复Diffusion-DPO在增大偏好边际时可能同时恶化赢家和败者重建误差的病态问题。

Abstract: Text-to-image diffusion models deliver high-quality images, yet aligning them
with human preferences remains challenging. We revisit diffusion-based Direct
Preference Optimization (DPO) for these models and identify a critical
pathology: enlarging the preference margin does not necessarily improve
generation quality. In particular, the standard Diffusion-DPO objective can
increase the reconstruction error of both winner and loser branches.
Consequently, degradation of the less-preferred outputs can become sufficiently
severe that the preferred branch is also adversely affected even as the margin
grows. To address this, we introduce Diffusion-SDPO, a safeguarded update rule
that preserves the winner by adaptively scaling the loser gradient according to
its alignment with the winner gradient. A first-order analysis yields a
closed-form scaling coefficient that guarantees the error of the preferred
output is non-increasing at each optimization step. Our method is simple,
model-agnostic, broadly compatible with existing DPO-style alignment frameworks
and adds only marginal computational overhead. Across standard text-to-image
benchmarks, Diffusion-SDPO delivers consistent gains over preference-learning
baselines on automated preference, aesthetic, and prompt alignment metrics.
Code is publicly available at https://github.com/AIDC-AI/Diffusion-SDPO.

</details>


### [31] [SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding](https://arxiv.org/abs/2511.03325)
*Mauro Orazio Drago,Luca Carlini,Pelinsu Celebi Balyemez,Dennis Pierantozzi,Chiara Lena,Cesare Hassan,Danail Stoyanov,Elena De Momi,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出面向手术场景的时序VideoQA模型SurgViVQA与REAL-Colon-VQA数据集，通过Mask视频-文本编码器和微调LLM提升动态事件理解与问句鲁棒性，在两数据集上明显优于图像级VQA基线。


<details>
  <summary>Details</summary>
Motivation: 现有外科VideoQA方法多依赖静态图像特征且数据集缺乏时序注释，导致无法有效推理手术过程中的动态事件；因此需要一个能建模时间一致性与交互的VideoQA框架和相应数据集。

Method: 提出Mask Video--Text Encoder以融合视频和提问特征，捕捉运动与器械-组织交互等时序线索；随后将编码特征交由微调的大型语言模型解码为答案。同时构建了REAL-Colon-VQA数据集，包含运动相关问题、诊断属性及重述/语义变体问题以测试鲁棒性。

Result: 在REAL-Colon-VQA上相比PitVQA关键词准确率提升约11%，在EndoVis18-VQA上提升约9%；在问题扰动实验中显示更好的泛化与鲁棒性。公开代码与数据集。

Conclusion: 本文提出的SurgViVQA通过将视频时序信息融入视觉问答，实现了对手术动态场景的更准确理解，并在新构建的REAL-Colon-VQA及公开数据集EndoVis18-VQA上显著优于基线，提升关键词准确率并增强对问题表述扰动的鲁棒性。

Abstract: Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (LLM) then
decodes into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.

</details>


### [32] [Multi-Object Tracking Retrieval with LLaVA-Video: A Training-Free Solution to MOT25-StAG Challenge](https://arxiv.org/abs/2511.03332)
*Yi Yang,Yiming Xu,Timo Kaiser,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: 两阶段零样本框架：FastTracker生成轨迹，LLaVA-Video进行文本-轨迹匹配，MOT25-StAG挑战中获第二名（m-HIoU 20.68，HOTA 10.73）。


<details>
  <summary>Details</summary>
Motivation: 复杂真实场景中需要同时完成基于自然语言的目标定位与跟踪，传统检测或单模态方法难以直接应对自由文本查询与多目标跟踪的结合，故提出结合跟踪器与多模态大模型的方案。

Method: 将任务视为视频检索问题：第一阶段用SOTA跟踪模型FastTracker生成轨迹候选；第二阶段用多模态大语言模型LLaVA-Video对轨迹与文本查询进行跨模态匹配，采用零样本策略完成匹配与筛选。

Result: 在MOT25-StAG测试集上获得m-HIoU=20.68和HOTA=10.73，排名挑战第二。

Conclusion: 该论文提出了一种两阶段零样本方法，将目标跟踪与多模态大模型结合，用于基于自由文本查询的时空动作定位任务，在MOT25-StAG挑战中取得了第二名的成绩。

Abstract: In this report, we present our solution to the MOT25-Spatiotemporal Action
Grounding (MOT25-StAG) Challenge. The aim of this challenge is to accurately
localize and track multiple objects that match specific and free-form language
queries, using video data of complex real-world scenes as input. We model the
underlying task as a video retrieval problem and present a two-stage, zero-shot
approach, combining the advantages of the SOTA tracking model FastTracker and
Multi-modal Large Language Model LLaVA-Video. On the MOT25-StAG test set, our
method achieves m-HIoU and HOTA scores of 20.68 and 10.73 respectively, which
won second place in the challenge.

</details>


### [33] [UniAVGen: Unified Audio and Video Generation with Asymmetric Cross-Modal Interactions](https://arxiv.org/abs/2511.03334)
*Guozhen Zhang,Zixiang Zhou,Teng Hu,Ziqiao Peng,Youliang Zhang,Yi Chen,Yuan Zhou,Qinglin Lu,Limin Wang*

Main category: cs.CV

TL;DR: UniAVGen通过双分支DiT、不对称跨模态交互与人脸感知调制，结合模态感知指导，在更少数据下实现高质量音视频联合生成与同步。


<details>
  <summary>Details</summary>
Motivation: 现有开源音视频生成方法在跨模态建模上不足，导致唇同步差、语义一致性不足，需要一个统一且高效的联合生成框架来提升时空同步与跨模态一致性，同时降低对大规模训练数据的依赖。

Method: 采用双并行Diffusion Transformers构建联合潜在空间；设计不对称跨模态交互机制以实现双向时序对齐的cross-attention；引入Face-Aware Modulation突出关键脸部区域；提出Modality-Aware Classifier-Free Guidance在推理时增强跨模态相关信号；单模型支持多项任务（联合生成/续写、视频到配音、音驱动视频）。

Result: 在更少（1.3M vs 30.1M）训练样本下，UniAVGen在音视频同步、音色一致性和情感一致性上取得总体优势，能完成多项音视频生成任务，验证了方法的有效性和数据效率。

Conclusion: UniAVGen提出了一个统一的音视频联合生成框架，通过双分支扩散Transformer和不对称跨模态交互实现更精确的时空同步与语义一致性，结合人脸感知调制和模态感知的无条件引导，能在较少训练数据下达成多任务音视频生成和续写，实验显示在唇同步、音色与情感一致性上具有优势。

Abstract: Due to the lack of effective cross-modal modeling, existing open-source
audio-video generation methods often exhibit compromised lip synchronization
and insufficient semantic consistency. To mitigate these drawbacks, we propose
UniAVGen, a unified framework for joint audio and video generation. UniAVGen is
anchored in a dual-branch joint synthesis architecture, incorporating two
parallel Diffusion Transformers (DiTs) to build a cohesive cross-modal latent
space. At its heart lies an Asymmetric Cross-Modal Interaction mechanism, which
enables bidirectional, temporally aligned cross-attention, thus ensuring
precise spatiotemporal synchronization and semantic consistency. Furthermore,
this cross-modal interaction is augmented by a Face-Aware Modulation module,
which dynamically prioritizes salient regions in the interaction process. To
enhance generative fidelity during inference, we additionally introduce
Modality-Aware Classifier-Free Guidance, a novel strategy that explicitly
amplifies cross-modal correlation signals. Notably, UniAVGen's robust joint
synthesis design enables seamless unification of pivotal audio-video tasks
within a single model, such as joint audio-video generation and continuation,
video-to-audio dubbing, and audio-driven video synthesis. Comprehensive
experiments validate that, with far fewer training samples (1.3M vs. 30.1M),
UniAVGen delivers overall advantages in audio-video synchronization, timbre
consistency, and emotion consistency.

</details>


### [34] [Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.03367)
*Gahyeon Kim,Sohee Kim,Seokju Lee*

Main category: cs.CV

TL;DR: 论文引入图像属性增强与对抗token嵌入（AAPL），在提示学习中解耦无关视觉扰动，提升对未见类别的泛化，并在多项任务上取得领先表现。


<details>
  <summary>Details</summary>
Motivation: 当前prompt learning在泛化至完全未见类别时性能欠佳，且主要侧重文本修改而忽视图像级增强；作者认为属性级图像增强能提供有助于学习更鲁棒提示的视觉变体。

Method: 提出AAPL方法：在soft prompt框架中结合属性相关的图像级数据增强，并引入对抗token嵌入以解耦由增强引入的表层视觉变化与类别相关的语义表示，使学习到的提示聚焦于判别性视觉特征。

Result: 在11个基准数据集的few-shot、zero-shot、跨数据集与领域泛化评估中，AAPL持续优于如CoOp、CoCoOp等现有方法，且代码已开源。

Conclusion: 该论文提出了在提示学习中加入基于图像属性增强和对抗token嵌入的策略，从而提升模型对未见类别的泛化能力，并在多项基准上优于现有方法。

Abstract: Recent advances in large-scale vision and language models have led to
significant progress in zero-shot learning tasks. Methods such as CoOp and
CoCoOp have shown that replacing handcrafted prompts with learnable vectors,
known as prompt learning, can result in improved performance. However, these
models often struggle to generalize to entirely unseen categories. While
traditional zero-shot learning techniques benefit from various data
augmentation strategies, prompt learning has primarily focused on text-based
modifications, leaving the potential of image-based augmentation largely
unexplored. In this work, we explore how image-level augmentations,
particularly those that introduce attribute-specific variations, can support
and enhance prompt learning. Our analysis examines the interaction between
these augmentations and soft prompt frameworks, revealing their potential to
improve generalization. We also identify a limitation in existing methods, such
as CoCoOp, which do not provide explicit guidance for learning prompts that
focus on semantically meaningful visual features. To address this, we propose
Adding Attributes to Prompt Learning, AAPL, a novel method that introduces
adversarial token embeddings to decouple superficial visual variations
introduced by augmentation from class-relevant semantic representations. This
decoupling enables the learned prompts to concentrate on visually
discriminative features that align with the target categories. We conduct
comprehensive experiments on eleven benchmark datasets, and AAPL consistently
outperforms existing methods across few-shot, zero-shot, cross-dataset, and
domain generalization settings. Our source code is publicly available at:
https://github.com/Gahyeonkim09/AAPL

</details>


### [35] [Robust Alignment of the Human Embryo in 3D Ultrasound using PCA and an Ensemble of Heuristic, Atlas-based and Learning-based Classifiers Evaluated on the Rotterdam Periconceptional Cohort](https://arxiv.org/abs/2511.03416)
*Nikolai Herrmann,Marcella C. Zijta,Stefan Klein,Régine P. M. Steegers-Theunissen,Rene M. H. Wijnen,Bernadette S. de Bakker,Melek Rousian,Wietske A. P. Bastiaansen*

Main category: cs.CV

TL;DR: 利用胚胎分割掩码做PCA提取主轴并生成四个候选朝向，结合Pearson启发式、图谱配准和随机森林三种选择策略（可多数投票融合），在首孕期3D超声大样本上实现约98%的一致对齐准确率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 统一胚胎在3D超声图像中的空间朝向有助于标准平面检测、解剖标志可视化和不同扫描间差异突出，从而利于产前生长监测的自动化与可扩展分析。

Method: 对分割后的胚胎掩码进行PCA以获得主轴，从中生成四个候选方向；随后使用三种不同策略选择标准方向：1) 基于Pearson相关系数的形状相似性启发式；2) 将图像与构建的图谱通过归一化互相关进行配准；3) 基于影像与形状特征训练的随机森林分类器。最终可选用多数投票融合三种策略结果。

Result: 在来自1043例妊娠的2166个纵向采集的3D超声图像（7+0至12+6周）上验证：PCA在99.0%的图像中正确提取主轴；Pearson启发式、图谱配准和随机森林分别在97.4%、95.8%和98.4%的图像中正确选取标准候选。三法多数投票准确率为98.5%。

Conclusion: 本文提出了一种基于胚胎分割掩码并结合PCA提取主轴的三维超声图像标准化对齐方法，通过三种候选方向选择策略（Pearson相关启发式、基于配准的图谱匹配和随机森林）以及多数投票进一步提高鲁棒性。整体流程在大样本首孕期数据上表现优异，能在临床与研究中实现可扩展的一致性对齐。

Abstract: Standardized alignment of the embryo in three-dimensional (3D) ultrasound
images aids prenatal growth monitoring by facilitating standard plane
detection, improving visualization of landmarks and accentuating differences
between different scans. In this work, we propose an automated method for
standardizing this alignment. Given a segmentation mask of the embryo,
Principal Component Analysis (PCA) is applied to the mask extracting the
embryo's principal axes, from which four candidate orientations are derived.
The candidate in standard orientation is selected using one of three
strategies: a heuristic based on Pearson's correlation assessing shape, image
matching to an atlas through normalized cross-correlation, and a Random Forest
classifier. We tested our method on 2166 images longitudinally acquired 3D
ultrasound scans from 1043 pregnancies from the Rotterdam Periconceptional
Cohort, ranging from 7+0 to 12+6 weeks of gestational age. In 99.0% of images,
PCA correctly extracted the principal axes of the embryo. The correct candidate
was selected by the Pearson Heuristic, Atlas-based and Random Forest in 97.4%,
95.8%, and 98.4% of images, respectively. A Majority Vote of these selection
methods resulted in an accuracy of 98.5%. The high accuracy of this pipeline
enables consistent embryonic alignment in the first trimester, enabling
scalable analysis in both clinical and research settings. The code is publicly
available at:
https://gitlab.com/radiology/prenatal-image-analysis/pca-3d-alignment.

</details>


### [36] [Generalizing Shape-from-Template to Topological Changes](https://arxiv.org/abs/2511.03459)
*Kevin Manogue,Tomasz M Schang,Dilara Kuş,Jonas Müller,Stefan Zachow,Agniva Sengupta*

Main category: cs.CV

TL;DR: 本文提出首个可处理拓扑变化的SfT框架，通过迭代分区模板并最小化物理与重投影联合能量，能够鲁棒地重建发生撕裂/切割的可变形表面，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SfT方法在形变伴随拓扑变化（如撕裂、切割）时失效，实际应用中常出现此类事件，因而需要一种能处理拓扑改变同时保持重投影一致性和物理合理性的SfT方法。

Method: 先用经典SfT获得初始解，然后通过基于能量最小化的迭代算法对模板空间域进行分区适配，能量项结合物理约束（例如弹性或不可穿透）与图像重投影一致性，分区允许局部独立变形以捕捉拓扑变化。

Result: 在合成和真实数据上均验证了方法在捕捉2D有界表面的撕裂和切割等拓扑事件方面的优越性，定量和定性结果均优于基线方法，证明了该方法的鲁棒性和广泛适用性。

Conclusion: 提出了一种将Shape-from-Template扩展到处理拓扑变化的鲁棒框架，通过初始化SfT解并迭代分割模板域来最小化联合物理合理性和重投影一致性的能量函数，从而在存在撕裂和切割等拓扑事件时也能重建可变形物体表面。

Abstract: Reconstructing the surfaces of deformable objects from correspondences
between a 3D template and a 2D image is well studied under Shape-from-Template
(SfT) methods; however, existing approaches break down when topological changes
accompany the deformation. We propose a principled extension of SfT that
enables reconstruction in the presence of such changes. Our approach is
initialized with a classical SfT solution and iteratively adapts the template
by partitioning its spatial domain so as to minimize an energy functional that
jointly encodes physical plausibility and reprojection consistency. We
demonstrate that the method robustly captures a wide range of practically
relevant topological events including tears and cuts on bounded 2D surfaces,
thereby establishing the first general framework for topological-change-aware
SfT. Experiments on both synthetic and real data confirm that our approach
consistently outperforms baseline methods.

</details>


### [37] [Human Mesh Modeling for Anny Body](https://arxiv.org/abs/2511.03589)
*Romain Brégier,Guénolé Fiche,Laura Bravo-Sánchez,Thomas Lucas,Matthieu Armando,Philippe Weinzaepfel,Grégory Rogez,Fabien Baradel*

Main category: cs.CV

TL;DR: Anny 是一个基于人体测量学的开源、可微分、无需扫描的人体模型，提供解释性表型参数与 WHO 校准分布，并通过生成的大规模合成数据证明其在 HMR 等任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数化人体模型依赖昂贵的 3D 扫描、受限的训练数据并且在人群代表性上存在偏差。作者希望构建一个开放、可解释、无需扫描且在人口统计学上更具代表性的模型。

Method: 基于 MakeHuman 社区的解剖与形态知识构建解释性 blendshape 空间，用表型参数（性别、年龄、身高、体重等）控制形状；用 WHO 人口统计数据校准分布；构建单一统一模型并生成 80 万张基于该模型的光照真实感人像集合（Anny-One）；对比评估 HMR 与扫描驱动模型的性能。

Result: Anny 能在毫米级精度上拟合扫描数据，支持可控的合成数据生成，并用于训练 HMR，其性能可与使用扫描数据训练的模型相匹配；模型及代码以 Apache 2.0 开源。

Conclusion: Anny 提供了一个简单、可微、无需扫描且基于人体测量学知识的开源人体参数模型，能生成跨年龄与体型的连续可解释形状空间，并在多项任务（扫描拟合、合成数据生成、HMR）中取得与基于扫描模型相当的效果。

Abstract: Parametric body models are central to many human-centric tasks, yet existing
models often rely on costly 3D scans and learned shape spaces that are
proprietary and demographically narrow. We introduce Anny, a simple, fully
differentiable, and scan-free human body model grounded in anthropometric
knowledge from the MakeHuman community. Anny defines a continuous,
interpretable shape space, where phenotype parameters (e.g. gender, age,
height, weight) control blendshapes spanning a wide range of human forms --
across ages (from infants to elders), body types, and proportions. Calibrated
using WHO population statistics, it provides realistic and demographically
grounded human shape variation within a single unified model. Thanks to its
openness and semantic control, Anny serves as a versatile foundation for 3D
human modeling -- supporting millimeter-accurate scan fitting, controlled
synthetic data generation, and Human Mesh Recovery (HMR). We further introduce
Anny-One, a collection of 800k photorealistic humans generated with Anny,
showing that despite its simplicity, HMR models trained with Anny can match the
performance of those trained with scan-based body models, while remaining
interpretable and broadly representative. The Anny body model and its code are
released under the Apache 2.0 license, making Anny an accessible foundation for
human-centric 3D modeling.

</details>


### [38] [Signal Intensity-weighted coordinate channels improve learning stability and generalisation in 1D and 2D CNNs in localisation tasks on biomedical signals](https://arxiv.org/abs/2511.03645)
*Vittal L. Rao*

Main category: cs.CV

TL;DR: 提出将坐标通道按局部信号强度加权，作为一种简单有效的输入表示，在ECG和细胞图像定位任务上均能加速收敛并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 在生物医学定位任务中，信号强度分布复杂，单纯坐标通道不能直接表征强度-位置关系；将强度与坐标耦合可为网络提供更有意义的先验，从而更容易学习绝对空间/时间位置相关的模式。

Method: 用强度加权坐标替换传统的纯坐标通道，构造输入时将每个坐标通道乘以对应位置的信号强度；在一维心电信号（20s两导联ECG的形态转变时刻预测）和二维细胞图像（SiPaKMeD细胞核中心回归）上训练并比较与常规模块（如CoordConv）的性能差异。

Result: 在两个不同模态和维度的任务上，强度加权坐标表示均表现出更快的训练收敛和更好的测试泛化性能，优于常规坐标通道方法。具体提高包括训练速度加快与回归/分类指标提升（摘要中未提供具体数值）。

Conclusion: 本文提出的信号强度加权坐标表示通过将坐标通道按局部信号强度缩放，将位置与强度耦合嵌入输入，作为一种简单、模态无关的归纳偏置，可提高定位任务的收敛速度和泛化能力。

Abstract: Localisation tasks in biomedical data often require models to learn
meaningful spatial or temporal relationships from signals with complex
intensity distributions. A common strategy, exemplified by CoordConv layers, is
to append coordinate channels to convolutional inputs, enabling networks to
learn absolute positions. In this work, we propose a signal intensity-weighted
coordinate representation that replaces the pure coordinate channels with
channels scaled by local signal intensity. This modification embeds an
intensity-position coupling directly in the input representation, introducing a
simple and modality-agnostic inductive bias. We evaluate the approach on two
distinct localisation problems: (i) predicting the time of morphological
transition in 20-second, two-lead ECG signals, and (ii) regressing the
coordinates of nuclear centres in cytological images from the SiPaKMeD dataset.
In both cases, the proposed representation yields faster convergence and higher
generalisation performance relative to conventional coordinate-channel
approaches, demonstrating its effectiveness across both one-dimensional and
two-dimensional biomedical signals.

</details>


### [39] [A Lightweight 3D-CNN for Event-Based Human Action Recognition with Privacy-Preserving Potential](https://arxiv.org/abs/2511.03665)
*Mehdi Sefidgar Dilmaghani,Francis Fowley,Peter Corcoran*

Main category: cs.CV

TL;DR: 使用事件相机数据和轻量3DCNN，结合focal loss与数据增强，在边缘可部署且隐私保护的人体动作识别任务上达到了优于常规模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机会采集可识别个人信息，存在隐私问题；事件相机仅记录像素强度变化，天然有利于隐私保护，适合家庭和监控场景下的动作识别。

Method: 设计了紧凑的3DCNN结构，采用焦点损失（focal loss）加类重加权处理类别不平衡，结合针对性的增强策略；在Toyota Smart Home和ETRI拼接数据集上训练与评估。

Result: 模型在复合数据集上取得F1-score 0.9415，准确率94.17%，相比C3D、ResNet3D和MC3_18最高提升约3%。

Conclusion: 该文提出了一种轻量级3D卷积神经网络用于处理事件相机数据的人体动作识别，兼顾时空建模与边缘部署需求，并在隐私保护场景下具有优势。

Abstract: This paper presents a lightweight three-dimensional convolutional neural
network (3DCNN) for human activity recognition (HAR) using event-based vision
data. Privacy preservation is a key challenge in human monitoring systems, as
conventional frame-based cameras capture identifiable personal information. In
contrast, event cameras record only changes in pixel intensity, providing an
inherently privacy-preserving sensing modality. The proposed network
effectively models both spatial and temporal dynamics while maintaining a
compact design suitable for edge deployment. To address class imbalance and
enhance generalization, focal loss with class reweighting and targeted data
augmentation strategies are employed. The model is trained and evaluated on a
composite dataset derived from the Toyota Smart Home and ETRI datasets.
Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy
of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D,
and MC3_18 by up to 3%. These results highlight the potential of event-based
deep learning for developing accurate, efficient, and privacy-aware human
action recognition systems suitable for real-world edge applications.

</details>


### [40] [Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection](https://arxiv.org/abs/2511.03666)
*Dongkeun Kim,Minsu Cho,Suha Kwak*

Main category: cs.CV

TL;DR: 论文提出一种利用身体部位及其人际关系进行自底向上的群体推理方法，能更好捕捉细粒度社交线索，从而提升社交互动与群体检测性能，在NVI数据集上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖个体整体表征，忽视面部表情、视线、手势等细粒度社交信号；并且直接检测群体而非基于个体间交互推理，导致在需要从细微线索推断群体配置时表现欠佳。

Method: 方法包括：1) 个体检测与部位感知特征增强——检测个体并提取身体各部位特征以捕获细粒度线索；2) 基于相似性的自底向上关联推理——通过计算个体及其部位特征的相似性，并结合空间关系，进行对等关联以推断群体配置；3) 交互关系建模——使用部位间的人际关系来识别交互类型。

Result: 在NVI数据集上进行实验，方法在社交互动检测指标上超过已有方法，取得新的最先进水平（具体数值未给出）。

Conclusion: 该论文提出的基于部位感知的自底向上团体推理框架，有效增强了对细粒度社交线索（如面部表情、视线、手势等）的建模能力，从而在社交互动检测任务上优于现有方法。

Abstract: Social interactions often emerge from subtle, fine-grained cues such as
facial expressions, gaze, and gestures. However, existing methods for social
interaction detection overlook such nuanced cues and primarily rely on holistic
representations of individuals. Moreover, they directly detect social groups
without explicitly modeling the underlying interactions between individuals.
These drawbacks limit their ability to capture localized social signals and
introduce ambiguity when group configurations should be inferred from social
interactions grounded in nuanced cues. In this work, we propose a part-aware
bottom-up group reasoning framework for fine-grained social interaction
detection. The proposed method infers social groups and their interactions
using body part features and their interpersonal relations. Our model first
detects individuals and enhances their features using part-aware cues, and then
infers group configuration by associating individuals via similarity-based
reasoning, which considers not only spatial relations but also subtle social
cues that signal interactions, leading to more accurate group inference.
Experiments on the NVI dataset demonstrate that our method outperforms prior
methods, achieving the new state of the art.

</details>


### [41] [Disentangled Concepts Speak Louder Than Words:Explainable Video Action Recognition](https://arxiv.org/abs/2511.03725)
*Jongseo Lee,Wooil Lee,Gyeong-Moon Park,Seong Tae Kim,Jinwoo Choi*

Main category: cs.CV

TL;DR: DANCE通过将动作预测分解为运动（姿态序列）、对象和场景三类概念，并采用ante-hoc概念瓶颈与LLM抽取语义概念，显著提高了视频动作识别的解释性且性能仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有可视化或语言化解释方法存在将运动与空间上下文混合的问题，且动作的动态性难以用语言直接表述。为了解耦运动与上下文并提升解释清晰度，提出了基于不同概念类型的解释框架。

Method: DANCE采用一种ante-hoc概念瓶颈设计，运动动态概念用人体姿态序列表示，对象与场景概念通过大语言模型自动抽取，模型必须通过这些概念来做动作预测。

Result: 在KTH、Penn Action、HAA500和UCF-101四个数据集上，DANCE在保持竞争性识别性能的同时显著提升了解释清晰度；用户研究验证了可解释性的提升，并展示了对模型调试、编辑和故障分析的帮助。

Conclusion: DANCE通过将动作识别的预测路径显式分为运动动态（人体姿态序列）、对象和场景三类概念，实现了更可解释且可操作的模型决策途径。

Abstract: Effective explanations of video action recognition models should disentangle
how movements unfold over time from the surrounding spatial context. However,
existing methods based on saliency produce entangled explanations, making it
unclear whether predictions rely on motion or spatial context. Language-based
approaches offer structure but often fail to explain motions due to their tacit
nature -- intuitively understood but difficult to verbalize. To address these
challenges, we propose Disentangled Action aNd Context concept-based
Explainable (DANCE) video action recognition, a framework that predicts actions
through disentangled concept types: motion dynamics, objects, and scenes. We
define motion dynamics concepts as human pose sequences. We employ a large
language model to automatically extract object and scene concepts. Built on an
ante-hoc concept bottleneck design, DANCE enforces prediction through these
concepts. Experiments on four datasets -- KTH, Penn Action, HAA500, and UCF-101
-- demonstrate that DANCE significantly improves explanation clarity with
competitive performance. We validate the superior interpretability of DANCE
through a user study. Experimental results also show that DANCE is beneficial
for model debugging, editing, and failure analysis.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [42] [Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering](https://arxiv.org/abs/2511.03393)
*Chiara Rucco,Motaz Saad,Antonella Longo*

Main category: cs.DB

TL;DR: 将实践中混合的ETL/ELT模式形式化为ETLT/ELTL设计模式，并通过ETLT++/ELTL++引入契约、版本、语义治理与监控，解决治理与可观测性不足，给出可复用实现路线。


<details>
  <summary>Details</summary>
Motivation: 传统ETL/ELT在可扩展性、治理和实时处理方面存在不足，实践中出现的混合模式缺乏文献级别的规范与最佳实践，因此需要形式化的设计模式来填补这一空白。

Method: 在设计模式框架下系统化定义ETLT与ELTL的结构、权衡与适用场景；在此基础上将强制性要素（契约、版本控制、语义管理、持续监控）嵌入，形成ETLT++/ELTL++；提供实现建议与操作性实践以支持多云与实时场景。

Result: 提出了正式的ETLT与ELTL设计模式定义，并扩展为ETLT++与ELTL++，明确了必需的设计义务和实现要点，提供了促进可审计性、可扩展性、成本效率和跨云/实时可用性的框架。

Conclusion: 该论文将ETLT和ELTL从实践中已有的混合模式上升为正式设计模式，并通过引入ETLT++和ELTL++强化治理、质量保证与可观测性，旨在为现代数据工程提供可重用的、可审计的流水线框架。

Abstract: Traditional ETL and ELT design patterns struggle to meet modern requirements
of scalability, governance, and real-time data processing. Hybrid approaches
such as ETLT (Extract-Transform-Load-Transform) and ELTL
(Extract-Load-Transform-Load) are already used in practice, but the literature
lacks best practices and formal recognition of these approaches as design
patterns. This paper formalizes ETLT and ELTL as reusable design patterns by
codifying implicit best practices and introduces enhanced variants, ETLT++ and
ELTL++, to address persistent gaps in governance, quality assurance, and
observability. We define ETLT and ELTL patterns systematically within a design
pattern framework, outlining their structure, trade-offs, and use cases.
Building on this foundation, we extend them into ETLT++ and ELTL++ by embedding
explicit contracts, versioning, semantic curation, and continuous monitoring as
mandatory design obligations. The proposed framework offers practitioners a
structured roadmap to build auditable, scalable, and cost-efficient pipelines,
unifying quality enforcement, lineage, and usability across multi-cloud and
real-time contexts. By formalizing ETLT and ELTL, and enhancing them through
ETLT++ and ELTL++, this work bridges the gap between ad hoc practice and
systematic design, providing a reusable foundation for modern, trustworthy data
engineering.

</details>


### [43] [HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics](https://arxiv.org/abs/2511.03437)
*Md Mizanur Rahaman Nayan,Zheyu Li,Flavio Ponzina,Sumukh Pinge,Tajana Rosing,Azad J. Naeemi*

Main category: cs.DB

TL;DR: 提出一种基于预聚类启发式的轻量级增量聚类+并行化数据库搜索硬件平台，显著降低资源与延迟，保持高搜索精度与能效。


<details>
  <summary>Details</summary>
Motivation: 传统蛋白质组学数据库搜索与聚类在资源与延迟上成本高，难以在边缘或受限设备上部署，故需设计轻量、增量且并行化的方案以降低能耗与延迟。

Method: 利用质谱特性进行bucket-wise并行化与查询调度，采用一次性硬件初始化加载预聚类数据并结合启发式增量聚类策略，采用3T 2M T J SOT-CAM 7nm计算在存内设计以提升能效与并行度。

Result: 增量聚类比重聚类加速约20倍，仅增加0.3%聚类误差；数据库搜索与最先进工具结果重合度96%；对人类基因组草案（131GB）的一次性设置为1.19mJ（2M光谱），1000查询消耗1.1μJ，bucket并行实现100x加速。

Conclusion: 本文提出的轻量级增量聚类与并行化数据库搜索平台，能在资源受限环境下实现低能耗、低延迟且保持高性能，是传统全量聚类与搜索的有效替代。

Abstract: Database (DB) search and clustering are fundamental in proteomics but
conventional full clustering and search approaches demand high resources and
incur long latency. We propose a lightweight incremental clustering and highly
parallelizable DB search platform tailored for resource-constrained
environments, delivering low energy and latency without compromising
performance. By leveraging mass-spectrometry insights, we employ bucket-wise
parallelization and query scheduling to reduce latency. A one-time hardware
initialization with pre-clustered proteomics data enables continuous DB search
and local re-clustering, offering a more practical and efficient alternative to
clustering from scratch. Heuristics from pre-clustered data guide incremental
clustering, accelerating the process by 20x with only a 0.3% increase in
clustering error. DB search results overlap by 96% with state-of-the-art tools,
validating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the
7nm node with a compute-in-memory design. For the human genome draft dataset
(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search
consumes 1.1{\mu}J. Bucket-wise parallelization further achieves 100x speedup.

</details>


### [44] [In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines](https://arxiv.org/abs/2511.03480)
*Khalid Belhajjame,Haroun Mezrioui,Yuyan Zhao*

Main category: cs.DB

TL;DR: 用张量来编码管道溯源，结合回溯与前瞻信息，低内存开销下支持细粒度高效溯源查询。


<details>
  <summary>Details</summary>
Motivation: 数据准备管道中错误定位、结果解释、公平性验证和数据质量问题识别都依赖精确且可查询的溯源信息。现有方法在细粒度、内存效率或查询性能上存在折中，推动了更高效索引机制的需求。

Method: 将管道中各处理操作的溯源用张量表示，回溯性张量记录输入输出记录间的关系，前瞻性信息描述输入输出模式（即输入/输出模式或模式映射）以实现属性级连接。在查询时同时利用两类张量进行组合计算，从而快速回答多种溯源查询。

Result: 通过在真实与合成数据上的评估，证明该方法在存储开销和查询性能上具有优势，能高效回答多类溯源查询（包括记录级和属性级），展示实用性与可扩展性。

Conclusion: 本文提出了一种基于张量的索引机制，用于高效捕获与查询数据处理管道的溯源信息。通过结合回溯性（retrospective）和前瞻性（prospective）溯源，能在记录级和属性级提供细粒度的溯源，并保持较低的内存开销。

Abstract: Data provenance has numerous applications in the context of data preparation
pipelines. It can be used for debugging faulty pipelines, interpreting results,
verifying fairness, and identifying data quality issues, which may affect the
sources feeding the pipeline execution. In this paper, we present an indexing
mechanism to efficiently capture and query pipeline provenance. Our solution
leverages tensors to capture fine-grained provenance of data processing
operations, using minimal memory. In addition to record-level lineage
relationships, we provide finer granularity at the attribute level. This is
achieved by augmenting tensors, which capture retrospective provenance, with
prospective provenance information, drawing connections between input and
output schemas of data processing operations. We demonstrate how these two
types of provenance (retrospective and prospective) can be combined to answer a
broad range of provenance queries efficiently, and show effectiveness through
evaluation exercises using both real and synthetic data.

</details>


### [45] [Analytical Queries for Unstructured Data](https://arxiv.org/abs/2511.03489)
*Daniel Kang*

Main category: cs.DB

TL;DR: 综述：针对用ML分析非结构化数据（尤其视频）的系统性挑战，本文总结了查询表达、计算优化（近似/分层）与错误/漂移检测三大方向的进展，展示了如何在性能与准确性间折衷以支持可部署的分析系统。


<details>
  <summary>Details</summary>
Motivation: 由于非结构化数据（文本、图像、视频、音频）增长迅猛且ML在理解此类数据方面能力提升，研究者希望将ML技术整合进数据管理系统以自动化“理解真实世界”，但面临表达查询、计算开销大、以及模型错误等挑战。

Method: 本文通过梳理和分类现有工作，提出了表达查询的多种方式（用户自定义函数、结构化模式、示例驱动）、基于近似和分层策略的查询优化框架，以及基于异常检测和概念漂移检测的错误处理策略。

Result: 近期工作已经开发出多种表达查询机制、近似/层次化的执行计划以及用于模型错误处理的检测技术，从而在实际系统中实现了更高的查询效率、灵活的查询表达能力和更稳健的结果。

Conclusion: 本文综述了在使用机器学习处理非结构化数据（尤其是视频分析）时，数据管理系统面临的关键挑战与进展，结论是通过表达查询接口、查询优化（近似与分层方法）和错误/漂移检测，可以在效率与准确性之间取得权衡，从而实用地部署ML驱动的分析系统。

Abstract: Unstructured data, in the form of text, images, video, and audio, is produced
at exponentially higher rates. In tandem, machine learning (ML) methods have
become increasingly powerful at analyzing unstructured data. Modern ML methods
can now detect objects in images, understand actions in videos, and even
classify complex legal texts based on legal intent. Combined, these trends make
it increasingly feasible for analysts and researchers to automatically
understand the "real world." However, there are major challenges in deploying
these techniques: 1) executing queries efficiently given the expense of ML
methods, 2) expressing queries over bespoke forms of data, and 3) handling
errors in ML methods.
  In this monograph, we discuss challenges and advances in data management
systems for unstructured data using ML, with a particular focus on video
analytics. Using ML to answer queries introduces new challenges.First, even
turning user intent into queries can be challenging: it is not obvious how to
express a query of the form "select instances of cars turning left." Second, ML
models can be orders of magnitude more expensive compared to processing
traditional structured data. Third, ML models and the methods to accelerate
analytics with ML models can be error-prone.
  Recent work in the data management community has aimed to address all of
these challenges. Users can now express queries via user-defined functions,
opaquely through standard structured schemas, and even by providing examples.
Given a query, recent work focuses on optimizing queries by approximating
expensive "gold" methods with varying levels of guarantees. Finally, to handle
errors in ML models, recent work has focused on applying outlier and drift
detection to data analytics with ML.

</details>
