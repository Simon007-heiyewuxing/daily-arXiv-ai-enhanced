<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 173]
- [cs.DB](#cs.DB) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: 基于自由能原理、仅用场景级表征，本文提出了可解释的广告视频情感估计方法，实验证明KLD/BS/UN分别对应愉悦/惊奇/不确定性驱动的情感，并展现良好稳健性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 情感反应是理解媒体效应的关键，但依赖生理信号或主观评分的研究在可解释性与可扩展性上受限；因此希望提出一种仅基于视频场景解析即可解释性估计情感的可行方法。

Method: 从场景级表征提取表达要素，利用Kullback-Leibler散度(KLD)表示预测误差、贝叶斯惊奇(BS)表示信念更新、不确定性(UN)表示先验模糊性，构成自由能(FE)的核心指标；在1059个15s食品广告视频上进行实验，并在九组超参数设置及六类日文广告（不同体裁与长度）上验证稳健性与泛化性。

Result: 实验证明KLD对应品牌呈现相关的愉悦性，BS对信息复杂性驱动的惊奇敏感，UN反映元素类型与空间布局不确定性及元素数量与变异性引起的惊奇；识别出不确定刺激、持续高情感、瞬时峰值与衰减三种情感模式，且在多种超参数与广告类型下结果稳定。

Conclusion: 本文建立了无需外部生理或主观评分的可解释情感估计方法，通过场景级表征并基于自由能原理量化愉悦性、惊奇性与习惯化，从而为广告视频情感分析提供了一套理论与实践相结合的框架。

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

</details>


### [2] [Can Generative Models Actually Forge Realistic Identity Documents?](https://arxiv.org/abs/2601.00829)
*Alexander Vinogradov*

Main category: cs.CV

TL;DR: 公开的扩散模型能生成看起来像证件的图片，但不能达到法证级真实度，文献中对风险的估计可能被高估。


<details>
  <summary>Details</summary>
Motivation: 评估公开可获取的扩散生成模型是否能伪造具备证件级真实性的身份文件，以回应公众对其潜在滥用的担忧。

Method: 使用多种公开的文本到图像与图像到图像生成管线（包括 Stable Diffusion、Qwen、Flux、Nano-Banana 等），对生成结果进行人类观察评估和与自动化证件验证工具的比对，重点检查结构完整性、印刷/微印特征、二维码/条码编码一致性和图像取证特征。

Result: 当前公开扩散模型能在表面上模拟证件外观（如布局、字体、纹理），但无法再现结构性和法证级别的真实性特征，生成的伪造证件难以通过专业的法证或严格的自动化验证。

Conclusion: 虽然生成模型在视觉上能制造欺骗性图像，但实现可通过法证检测的身份证伪造仍有显著技术鸿沟。建议机器学习研究者与证件法证专家合作以更现实地评估风险并制定应对策略。

Abstract: Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.

</details>


### [3] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: 在5216张小儿胸片上，对比多种模型和训练策略，微调的ResNet50达到几乎完美的检测性能，表明迁移学习微调在小儿肺炎筛查任务中具有显著优势，但需外部验证。


<details>
  <summary>Details</summary>
Motivation: 小儿肺炎致死率高，但放射科医生短缺且诊断存在主观差异，因而需要自动化且可靠的影像筛查方法。

Method: 使用包含5216张小儿胸片的数据集，按80/10/10拆分；比较7个模型：自定义CNNs从头训练，以及ResNet50、DenseNet121、EfficientNet-B0三种迁移学习（冻结骨干和微调两种策略）；评估指标为准确率、F1-score、AUC，并用Grad-CAM进行可解释性分析。

Result: 微调的ResNet50最佳：准确率99.43%，F1-score 99.61%，AUC 99.93%，测试集仅3例错误；整体微调策略比冻结骨干平均高出5.5个百分点；Grad-CAM显示模型关注的肺区与临床一致。

Conclusion: 迁移学习（微调）显著优于从头训练的自定义CNN，在小儿肺炎X光识别任务上表现接近完美，可作为资源受限环境的筛查工具，但需在多中心和成人数据集上进一步验证。

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

</details>


### [4] [Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS](https://arxiv.org/abs/2601.00839)
*Zahid Ullah,Muhammad Hilal,Eunsoo Lee,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 本文在CAMUS心脏超声数据集上对U-Net、Attention U-Net与TransUNet进行了统一且可复现的对比实验，强调保留原始图像强度、分辨率和对齐的重要性。U-Net在原生NIfTI上表现最好（94% Dice），TransUNet在自监督预训练下对难例泛化能力最强；伪标签能提升鲁棒性。提供了预处理指南与面向可扩展标注/自监督的展望。


<details>
  <summary>Details</summary>
Motivation: 尽管已有多篇综述总结了心脏影像和深度学习进展，但很少有工作将综述与统一、可复现的实验基准结合起来。作者旨在通过标准化预处理与评估，为研究者提供可比的性能基线，并探讨可扩展的标注与自监督策略以促进临床级别的心脏超声分割研究。

Method: 统一在CAMUS数据集上比较U-Net、Attention U-Net和TransUNet，控制变量包括预处理格式（NIfTI vs 16位PNG）、伪标签扩充策略、以及是否采用自监督预训练。所有模型在相同训练/验证划分、损失函数和评价指标下训练；评估使用平均Dice与边界错误分析。还测试了GPT辅助多边形标注的伪标签生成与置信度筛选策略。

Result: 在这项研究中，作者结合心脏超声分割文献的聚焦综述，并在CAMUS数据集上对U-Net、Attention U-Net和TransUNet三种代表性网络进行了可重复的受控对比实验。研究探讨了多种预处理管线（原生NIfTI体积、16位PNG导出、GPT辅助的多边形伪标签、自监督预训练等），并在统一训练划分、损失函数和评估标准下进行了比较。主要实验结果包括：在保留原始动态范围的NIfTI数据上训练的U-Net取得了94%平均Dice得分；使用16位PNG流程时为91%；Attention U-Net在小尺寸或低对比区域上缓解边界泄漏；TransUNet在具有自监督预训练初始化时对困难帧表现出最强的泛化能力；伪标签通过置信度筛选扩充训练集并提升鲁棒性。作者的贡献为：在标准化CAMUS预处理与评估下提供三种网络的可比基准；就保持强度保真、分辨率一致性和空间对齐给出实用建议；并展望了可扩展的自监督与GPT驱动的多模态标注流水线。

Conclusion: 对心脏超声分割任务而言，保留原始数据动态范围（NIfTI）与高分辨率输入对性能显著；复杂的注意力模块带来局部改进，但并不总是必需；带有自监督预训练的Transformer混合模型（TransUNet）在困难样本上最具泛化优势；伪标签与GPT辅助标注可在规模与效率上带来实用收益。

Abstract: Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.

</details>


### [5] [Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge](https://arxiv.org/abs/2601.00854)
*Igor Lodin,Sergii Filatov,Vira Filatova,Dmytro Filatov*

Main category: cs.CV

TL;DR: MCLSC在边缘设备上通过维护两层潜在语义画布（静态和动态），结合运动门控的稀疏重分割与视频稳定/运动补偿，实现高效一致的语义记忆。对480p测试视频，分割调用减少>30倍，端到端时间降低>20倍，同时保持静/动语义覆盖的一致性。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算受限，逐帧运行昂贵的全景分割不现实；需要一种既保持长时一致语义记忆又降低推理频率的方法，以支持实时/持续的视觉态势感知。

Method: 在稳定的基线坐标系中维护两层潜在语义画布（慢累积的静态层和快速更新的动态层）；使用运动检测作为门控，仅在检测到新信息时异步触发Mask2Former全景分割；对每帧进行视觉稳定和运动补偿以保持语义记忆一致。

Result: 在预录制的480p视频上，MCLSC将分割调用减少超过30倍，端到端平均处理时间减少超过20倍，同时维持一致的静态/动态语义覆盖。

Conclusion: MCLSC能在资源受限边缘设备上显著降低昂贵分割推理频率并加速处理，同时通过运动补偿在基线坐标系中维护连贯的静态与动态语义画布，适合持续视觉感知任务。

Abstract: We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.

</details>


### [6] [VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.00879)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: 将视觉-语言对齐与序数学习相结合的ViT框架（VLOrdinalFormer）有效改进了膝骨关节炎KL分级，特别是在早期（KL1/KL2）区分上表现突出，且具备较好可解释性。


<details>
  <summary>Details</summary>
Motivation: 临床上KOA早期分级（尤其KL1与KL2）在放射片上差异微小，放射科医师一致性差，需一种能结合影像与临床语义并考虑序数关系的自动化分级方法。

Method: 方法上采用ViT-L16作为视觉骨干，使用CORAL实现序数回归，并借助CLIP驱动的语义对齐模块将临床文本概念（如关节间隙变窄、骨赘形成、皮质下硬化）融入特征学习；训练时用分层五折交叉验证、类别加权和测试时增强及全局阈值优化以提高鲁棒性。

Result: 在公开的OAI kneeKL224数据集上，VLOrdinalFormer在macro F1和总体准确率上达到了state-of-the-art，特别提升了KL1和KL2的识别率，同时Grad-CAM与CLIP相似性可视化表明模型关注的解剖区域与临床关切一致。

Conclusion: VLOrdinalFormer通过将视觉语言对齐与序数回归相结合，显著提升了KOA分级的准确性与可解释性，尤其在KL1与KL2的区分上取得了明显改进，且在整体性能上优于CNN和普通ViT基线。

Abstract: Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.

</details>


### [7] [VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition](https://arxiv.org/abs/2601.00887)
*Hongbo Jin,Kuanwei Lin,Wenhao Zhang,Yichen Jin,Ge Li*

Main category: cs.CV

TL;DR: 提出VideoCuRL，沿视觉时序负荷和认知推理深度两轴构建2D课程，使用光流、关键帧熵和校准惊讶度作为无训练代理，采用对角波前调度，并用动态稀疏KL与结构化重访稳定训练，在多个基准上优于强RL基线。


<details>
  <summary>Details</summary>
Motivation: 当前RL课程多基于标量难度或随机打乱，无法区分视频理解中的时序视觉负荷与认知推理深度两类独立挑战，导致训练效率和泛化性不足。

Method: 将视频样本映射到视觉复杂度（光流、关键帧熵）和认知复杂度（校准惊讶度）两轴的课程网格，使用能力感知的对角波前（Diagonal Wavefront）按从易到难同时增加两轴复杂度进行样本调度；同时引入动态稀疏KL（Dynamic Sparse KL）和结构化重访（Structured Revisiting）来防止奖励崩塌与灾难性遗忘。

Result: 在VSI-Bench推理任务上提升约+2.5分，在VideoMME感知任务上提升约+2.9分，且相比生成式课程消除了高昂的推理开销，展示出更好的性能与可扩展性。

Conclusion: VideoCuRL通过在二维难度空间上进行能力感知的课程学习，并结合稳定训练的机制，显著提升视频LLM在感知和推理任务的性能，同时避免了生成式课程的推断开销，适用于可扩展的视频后训练。

Abstract: Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.

</details>


### [8] [Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study](https://arxiv.org/abs/2601.00888)
*Happy Gery Pangestu,Andi Prademon Yunus,Siti Khomsah*

Main category: cs.CV

TL;DR: 该研究比较了VGG16、VGG19、Inception V3、ResNet50和ResNet101五种CNN作为风格迁移骨干网络在印尼蜡染（batik）风格迁移任务上的表现，通过245次受控实验从结构保持、风格表现与计算效率三方面评估。结论为ResNet系列在保持类似感知相似度（LPIPS≈0.53）和结构相似度（SSIM无显著差异，p=0.83）的同时，收敛速度比VGG快5-6倍，FLOPs降低约16倍，适合资源受限与产业化部署。


<details>
  <summary>Details</summary>
Motivation: VGG系虽在风格表达上强，但计算和内存开销大，不利于资源受限环境和产业化应用；因此研究旨在寻找在保持风格与结构的同时更高效的骨干网络。

Method: 在受控设置下对五种主流CNN骨干进行245次实验，结合定量指标（SSIM、LPIPS、FLOPs、收敛速度等）、定性视觉评估和统计检验（ANOVA）来比较结构保存、风格表现及计算效率的权衡。

Result: ANOVA表明不同骨干在SSIM上的差异不显著（p=0.83）；ResNet在收敛速度上较VGG快约5-6倍，FLOPs显著更低（0.63 vs 10.12 GFLOPs），感知相似度LPIPS约0.53；定性上VGG产出更浓密的绘画质感，ResNet保留更多几何与canting笔触，Inception V3表现居中但噪声更多。

Conclusion: 骨干网络选择不会显著影响结构相似度（SSIM），但在收敛速度、计算量与风格质感上存在明显差异；ResNet系在效率与结构保留间提供最佳折衷，是面向可扩展产业应用的首选。

Abstract: Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.

</details>


### [9] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: 提出CornViT：三阶段CvT体系用于单粒玉米籽粒的纯度、形态与胚向判定，提供重标注数据集、显著优于ResNet/DenseNet的结果，并实现网页部署。


<details>
  <summary>Details</summary>
Motivation: 当前玉米籽粒分级主要依赖人工检查，效率和一致性不足；因此需要一个自动化、可部署且具有可解释性的单粒籽粒评估系统以支持种子认证与育种流程。

Method: 构建三阶段流水线：三路顺序CvT-13分类器对384x384 RGB图像依次判断纯度、形状（扁平/圆形）和纯扁种子的胚向（上/下）；使用ImageNet-22k预训练CvT-13，仅对头部进行微调；并重标注公共数据集得到三个阶段专用数据集，同时提供基于Flask的网页部署。

Result: 在重新标注的数据集上，CvT-13在三项任务上分别达到93.76%、94.11%、91.12%测试准确率，明显优于ResNet-50（76.56–81.02%）和DenseNet-121（86.56–89.38%）；并发布了数据集与代码以及Flask应用。

Conclusion: CornViT提出了一个三阶段的Convolutional Vision Transformer框架，能有效模拟人工层级判断，实现单颗玉米种子在纯净性、形态和胚向三个任务上的自动评估，且优于传统卷积网络。

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

</details>


### [10] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: 评估先进视觉-语言模型（GPT-4o, GPT-4o-mini, Claude 3.5）在判断常见垃圾是否可回收及其应投放哪类回收箱的表现，包含是否能适应地点回收规则、受污染/损坏情况及多材质物品。结果显示模型在上下文理解上有显著提升，但仍存在不足，需要进一步优化以改善公众回收行为。


<details>
  <summary>Details</summary>
Motivation: 普通公众难以准确判断物品是否可回收及正确分类；将先进视觉-语言模型应用于此可提升回收效率与环境可持续性。

Method: 使用精心挑选的图像数据集，输入到GPT-4o, GPT-4o-mini和Claude 3.5，评估它们的回收箱匹配准确性及体积适配判断。设置三类挑战场景：地点依赖规则、污染/损坏、混合材质。对比模型输出与人工标注的地面实况，并量化准确率、召回率与错误类型。

Result: 模型在标准场景下总体准确率显著高于早期模型，但在地点特定规则、污染识别及多材质物品上准确率下降明显。GPT-4o表现最好，Claude 3.5和GPT-4o-mini次之。建议结合规则数据库与额外检测模块来改进实用性。

Conclusion: 视觉-语言模型在识别物品并给出回收建议方面已取得可观进展，特别是在理解上下文（如地点规则、污染、体积适配）方面比早期模型更好，但对于模糊类别、多材质和细微污染判断仍不稳定，需结合本地规则数据库和检测污染/材质的模块来提升可靠性。

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

</details>


### [11] [Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting](https://arxiv.org/abs/2601.00913)
*Subhankar Mishra*

Main category: cs.CV

TL;DR: Clean-GS prunes spurious Gaussians from 3D Gaussian Splatting models using as few as 3 segmentation masks via whitelist spatial filtering, color-depth validation, and neighbor outlier removal, achieving major compression with minimal quality loss.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting reconstructions contain many spurious Gaussians (floaters) that obscure target objects and increase model size, making deployment in bandwidth-constrained settings difficult.

Method: Multi-stage pipeline: (1) whitelist filtering by projecting Gaussians into masked regions from few views, (2) depth-buffered color validation to confirm Gaussian belongs to object, (3) neighbor-based outlier removal to eliminate isolated floaters; uses color-guided validation and outlier detection rather than global importance metrics.

Result: Clean-GS removes background clutter and floaters using sparse semantic masks, achieving 60-80% model compression while preserving object quality; reduces file sizes (example: 125MB to 47MB) and maintains rendering quality on Tanks and Temples.

Conclusion: Semantic-guided, multi-stage filtering effectively isolates target objects and removes floaters, making 3DGS models suitable for web/AR/VR; code released.

Abstract: 3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs

</details>


### [12] [Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning](https://arxiv.org/abs/2601.00918)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出基于拓扑数据分析与集成学习的轻量级可解释框架TDA-Alz，用于OASIS-1四类阿尔茨海默病分期分类，表现优异且计算高效。


<details>
  <summary>Details</summary>
Motivation: 克服数据稀缺和模型可解释性差的问题，通过提取脑MRI的拓扑特征实现高效、可解释的AD分期分类。

Method: Topological data analysis + ensemble classification

Result: 在OASIS-1数据集上四类分类达到98.19%准确率和99.75% AUC，优于或匹配现有深度学习方法，同时无需数据增强、预训练或大算力。

Conclusion: TDA-Alz为MRI基础AD分期提供了一种强大、轻量且可解释的替代方案，适合临床辅助决策。

Abstract: Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.
  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.

</details>


### [13] [Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis](https://arxiv.org/abs/2601.00925)
*I-Hsien Ting,Yi-Jun Tseng,Yu-Sheng Lin*

Main category: cs.CV

TL;DR: 用于非造影CT的3D卷积神经网络可在无造影剂情况下自动分类肺栓塞，达到85%准确率和0.84 AUC，表明可行性


<details>
  <summary>Details</summary>
Motivation: Early detection of pulmonary embolism (PE) without relying on contrast-enhanced CT to avoid contrast-induced acute kidney injury and save time for acute cases

Method: 使用3D卷积神经网络对无造影CT图像进行自动分类，训练并评估模型性能，报告准确率和AUC指标

Result: A 3D convolutional neural network achieved 85% accuracy and 0.84 AUC for classifying PE on non-contrast CT images, demonstrating feasibility

Conclusion: 3D CNN可以在非造影CT上有效检测肺栓塞，提供无造影剂、快速的辅助诊断途径，适合不能接受造影剂或需紧急处置的患者

Abstract: Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.
  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.

</details>


### [14] [Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store](https://arxiv.org/abs/2601.00928)
*Luis Yoichi Morales,Francesco Zanlungo,David M. Woollard*

Main category: cs.CV

TL;DR: 用顶视3D轨迹检测顾客在货架前的停留（shelf visits）；在两家门店标注数据上校准并跨店验证，能识别浏览行为并用于分析浏览模式与购买的关联


<details>
  <summary>Details</summary>
Motivation: Enable autonomous understanding of shopper intent for robots/analytics by detecting shelf browsing from overhead 3D trajectories

Method: Trajectory-based shelf visit detection and cross-store validation

Result: Algorithm calibrated on two labeled trajectory sets (8138 and 15129), generalizes across stores; can extract browsing patterns linked to purchases

Conclusion: 模型在异店环境下具有迁移能力，货架浏览信息可用于零售规划与人机交互设计

Abstract: Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.

</details>


### [15] [ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery](https://arxiv.org/abs/2601.00939)
*Feng Luo,Hongbo Pan,Xiang Yang,Baoyu Jiang,Fengqing Liu,Tao Huang*

Main category: cs.CV

TL;DR: 引入基于物理的渲染方程与光线行进于3D高斯点云中以建模几何一致阴影，并加入阴影一致性约束和阴影图先验，提升多时相卫星影像的阴影分离与三维重建效果，训练耗时仅数分钟。


<details>
  <summary>Details</summary>
Motivation: 多时相卫星影像中由光照变化引起的阴影不一致性破坏三维重建质量，需要在表示与渲染层面精确建模阴影以提升重建与合成效果。

Method: 基于3D Gaussian Splatting，采用遥感物理渲染方程和高效的射线行进（ray marching）来计算几何一致的阴影；同时解耦照明分量与表观属性；引入阴影一致性约束和阴影图先验以强化稀视角性能。

Result: ShadowGS proposes integrating physics-based shadow modeling into 3D Gaussian Splatting for satellite imagery, improving shadow decoupling, geometry, and novel view synthesis with efficient training.

Conclusion: ShadowGS在阴影解耦、几何精度和新视角合成上均超越现有方法，并在稀视角与不同数据类型（RGB、锐化、稀视角）下表现稳健。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.

</details>


### [16] [Learning to Segment Liquids in Real-world Images](https://arxiv.org/abs/2601.00940)
*Jonas Li,Michelle Li,Luke Liu,Heng Fan*

Main category: cs.CV

TL;DR: 构建大规模液体分割数据集LQDS（5000张、14类），提出LQDM模型，通过边界分支与主分割分支的交叉注意力提升分割效果，优于现有方法


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect liquid segmentation; liquids are diverse, transparent/reflective, mimic backgrounds; need dataset and improved model

Method: dataset + cross-attention segmentation

Result: Created LQDS (5000 images, 14 classes) and LQDM model using cross-attention between boundary and main segmentation branches; outperforms SOTA and sets baseline

Conclusion: LQDS+LQDM有效提升液体语义分割性能，为该任务建立了强基线

Abstract: Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.

</details>


### [17] [PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education](https://arxiv.org/abs/2601.00943)
*Megha Mariam K. M,Aditya Arun,Zakaria Laskar,C. V. Jawahar*

Main category: cs.CV

TL;DR: Introduces a benchmark decomposing physics concepts into teaching points with prompts to assess T2V models; finds good visual quality but poor conceptual correctness in many areas; resource released publicly.


<details>
  <summary>Details</summary>
Motivation: Evaluate potential of Text-to-Video (T2V) models for creating physics educational videos by providing a benchmark.

Method: Constructed a benchmark by decomposing physics concepts into granular teaching points and crafting prompts for each; evaluated T2V models on generated videos for visual coherence and conceptual accuracy across topics.

Result: Current T2V models produce visually coherent videos but often lack conceptual accuracy, performing better on mechanics, fluids, optics and worse on electromagnetism, thermodynamics. Benchmark and code released.

Conclusion: There is a gap between visual quality and conceptual correctness; benchmark aims to help close gap and enable curriculum-aligned T2V educational content.

Abstract: Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.

</details>


### [18] [Deep Clustering with Associative Memories](https://arxiv.org/abs/2601.00963)
*Bishwajit Saha,Dmitry Krotov,Mohammed J. Zaki,Parikshit Ram*

Main category: cs.CV

TL;DR: DCAM: a unified energy-based associative-memory loss that tightly couples representation learning and clustering, yielding better clustering across models and modalities.


<details>
  <summary>Details</summary>
Motivation: Existing deep clustering pipelines separate differentiable representation learning from discrete clustering, requiring approximations that weaken integration; use associative memory/energy dynamics to create a unified objective tying them together.

Method: Energy-based associative memory loss for integrated deep clustering

Result: Proposed DCAM method improves clustering quality across architectures (conv, res, FC) and modalities (images, text) by integrating representation and clustering via an energy-based associative memory loss.

Conclusion: Energy-based associative memory loss effectively unifies representation learning and clustering, giving robust improvements in clustering performance for diverse architectures and data types.

Abstract: Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).

</details>


### [19] [A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI](https://arxiv.org/abs/2601.00964)
*Md. Maksudul Haque,Rahnuma Akter,A S M Ahsanul Sarkar Akib,Abdul Hasib*

Main category: cs.CV

TL;DR: Proposed data balancing, augmentation, hybrid EfficientNetV2-L with channel attention and three-stage progressive learning plus Grad-CAM/saliency; achieved high accuracy and AUC on HAM10000 with interpretability.


<details>
  <summary>Details</summary>
Motivation: Improve multi-class skin lesion classification on HAM10000, address data imbalance and need for interpretability

Method: Deep hybrid EfficientNetV2-L with channel attention and progressive learning

Result: Achieved 91.15% accuracy, 85.45% macro F1, 99.33% micro AUC; strong performance across seven classes, especially melanoma and melanocytic nevi; XAI provided visual explanation

Conclusion: The combined approach yields high classification performance and improved diagnostic transparency via XAI, supporting clinical trust.

Abstract: Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\% and micro-average AUC of 99.33\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.

</details>


### [20] [Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss](https://arxiv.org/abs/2601.00988)
*Lin Xi,Yingliang Ma,Xiahai Zhuang*

Main category: cs.CV

TL;DR: 提出一种基于方向采样的本地匹配FSVOS方法，采用非参数动态采样和时空对比监督，并发布MOSXAV数据集，在多个血管影像数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: Reduce search space and avoid costly/impractical implementations (im2col-like ops, CUDA kernels) by using direction-based sampling to adapt sampling regions dynamically and portable across devices; improve temporal feature coherence for VOS and clinical applicability

Method: Local-directional non-parametric sampling + supervised spatio-temporal contrastive learning

Result: Design of a FSVOS model with local matching via direction-based non-parametric sampling, supervised spatio-temporal contrastive loss, and MOSXAV dataset; achieves SOTA accuracy and generalization on CADICA, XACV, MOSXAV

Conclusion: 方法在提高分割精度和跨类泛化能力方面表现优越，且计算和部署更灵活，适合临床应用

Abstract: We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.

</details>


### [21] [UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data](https://arxiv.org/abs/2601.00991)
*Joshua Kawaguchi,Saad Manzur,Emily Gao Wang,Maitreyi Sinha,Bryan Vela,Yunxi Wang,Brandon Vela,Wayne B. Hayes*

Main category: cs.CV

TL;DR: They built an Unreal Engine 5 renderer to produce UnrealPose-1M, a ~1M-frame synthetic human-pose dataset with full 3D/2D annotations and release the generation pipeline; they validate with downstream task experiments.


<details>
  <summary>Details</summary>
Motivation: Provide large-scale, diverse, accurately labeled 3D human pose data without expensive studio capture by using synthetic rendering in Unreal Engine 5.

Method: Use Unreal Engine 5 Movie Render Queue to render eight sequences (five coherent, three randomized) across multiple scenes and subjects, produce per-frame annotations (3D joints in world/camera coords, 2D projections/COCO keypoints with occlusion/visibility flags, bboxes, camera intrinsics/extrinsics), assemble UnrealPose-1M, and evaluate on image-to-3D, 2D keypoint detection, lifting, and detection/segmentation tasks.

Result: UnrealPose-Gen pipeline and UnrealPose-1M dataset (≈1M frames) with rich annotations (3D joints, 2D keypoints, visibility, bboxes, camera intrinsics/extrinsics) across multiple sequences and scenes; reported fidelity checks on four tasks and public release of pipeline and dataset.

Conclusion: Unreal synthetic data can supplement real data for pose tasks; the released pipeline enables community expansion of labeled 3D pose data.

Abstract: Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted "coherent" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.

</details>


### [22] [WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift](https://arxiv.org/abs/2601.00993)
*Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo*

Main category: cs.CV

TL;DR: WildIng fuses textual species descriptions with image embeddings to reduce sensitivity to background/environment shifts, achieving large gains in cross-region accuracy


<details>
  <summary>Details</summary>
Motivation: Address geographic domain shift in wildlife camera-trap image classification by integrating text descriptions with image features to form invariant representations

Method: Introduce WildIng and evaluate

Result: WildIng improves generalization across regions; boosts BioCLIP accuracy by ~30% under domain shift (example: CLIP adapter drops 84.77%→16.17% across Africa→America)

Conclusion: Text-augmented visual representations provide more robust ecological species classification under geographical distribution shifts; WildIng shows substantial improvements on Africa and America datasets and code is publicly available.

Abstract: Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.

</details>


### [23] [DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models](https://arxiv.org/abs/2601.00998)
*Yue Zhou,Jue Chen,Zilun Zhang,Penghui Huang,Ran Ding,Zhentao Zou,PengFei Gao,Yuchen Wei,Ke Li,Xue Yang,Xue Jiang,Hongxin Yang,Jonathan Li*

Main category: cs.CV

TL;DR: 该文提出DVGBench：首个针对无人机场景的隐式视觉指向基准，涵盖交通、灾害等六大场景，并为每个目标提供显式与隐式查询；提出DroneVG-R1模型，在强化学习框架下引入隐式到显式的链式思维（I2E-CoT），将隐式描述转为显式指令以简化定位；基准测试显示主流LVLM在隐式推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉指向数据集以显式描述为主，无法评估模型在需场景知识与复杂推理的隐式指向任务上能力，迫切需要高质量基准与新方法以提升LVLM在真实无人机任务中的实用性。

Method: 构建覆盖六大应用场景的DVGBench数据集，为每个对象配备显式与隐式查询；设计DroneVG-R1，将I2E-CoT嵌入强化学习训练流程，使模型先生成显式中间解释再进行定位，从而降低直接从隐式到定位的难度；在显式与隐式任务上对比主流LVLM并进行错误分析。

Result: DVGBench数据集公开并覆盖六类场景；DroneVG-R1在隐式VG任务上较无I2E-CoT方法表现更优；主流LVLM在隐式推理上的失败率高，展示了明显的局限性。

Conclusion: DVGBench填补了无人机隐式视觉指向评测的空白，DroneVG-R1通过I2E-CoT显著提升隐式VG表现，但整体评测揭示当前LVLM在场景特定推理与多模态理解上仍有较大改进空间。

Abstract: Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench

</details>


### [24] [Lightweight Channel Attention for Efficient CNNs](https://arxiv.org/abs/2601.01002)
*Prem Babu Kanaparthi,Tulasi Venkata Sri Varshini Padamata*

Main category: cs.CV

TL;DR: 提出轻量通道注意力LCA，使用自适应一维卷积与分组减少参数；在CIFAR-10上与SE、ECA比较，取得接近或优于ECA的准确率及相似参数和延迟，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有通道注意力机制（如SE、ECA）在提升性能上有效，但不同设计在效率与准确率之间的权衡尚未被系统比较；需提出更轻量的注意力模块以适配资源受限设备。

Method: 设计LCA模块，使用自适应一维卷积和分组操作以减少参数；在ResNet18和MobileNetV2上将SE、ECA和LCA分别集成；在CIFAR-10上进行训练与评估，记录准确率、FLOPs、参数量和GPU推理延迟。

Result: LCA在ResNet18上达到94.68%准确率，在MobileNetV2上达到93.10%；参数量与ECA相当，FLOPs和GPU延迟保持有利，证明了在尽量少的额外开销下能保留注意力效果。

Conclusion: LCA在参数效率和延迟方面与ECA相当，同时在ResNet18和MobileNetV2上实现了有竞争力的准确率，适合在资源受限场景中部署注意力模块。

Abstract: Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.

</details>


### [25] [Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2601.01022)
*Shiao Wang,Xiao Wang,Haonan Zhao,Jiarui Xu,Bo Jiang,Lin Zhu,Xin Zhao,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出在频域进行幅相级注意力融合与基于事件运动的空间稀疏化的RGB-Event跟踪框架，实现高频事件信息的有效利用与计算加速，在多个基准上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-Event目标跟踪方法在特征级融合方面的局限，充分利用事件相机的高动态范围与对运动的高灵敏度，减少对低信息区域的冗余计算。

Method: 方法包括：1) 将RGB和事件数据用FFT变换到频域，解耦幅度与相位；2) 设计幅度与相位注意力模块，有选择地将高频事件信息注入RGB特征；3) 引入运动引导的空间稀疏化模块，基于事件的运动敏感性预测目标相关空间概率，过滤低信息区域；4) 将稀疏特征送入轻量主干网络并通过跟踪头预测目标位置；在FE108、FELT、COESOT上进行实验验证。

Result: 提出一种在频域进行早期融合的跟踪框架，通过FFT将RGB与事件模态转到频域并解耦幅度与相位，用幅相注意力有选择地将高频事件信息融合到RGB中；同时引入运动引导的空间稀疏化模块，过滤低信息区域，将稀疏的目标相关特征送入主干网络，从而在保留性能的同时显著降低计算量。在FE108、FELT、COESOT三个数据集上验证了方法的高效性与性能提升。

Conclusion: 频域早期融合结合幅相注意力与运动引导的空间稀疏化能有效提升RGB-Event跟踪的性能并显著减少主干计算，为事件相机在目标跟踪中的利用提供了新的思路。

Abstract: Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

</details>


### [26] [ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval](https://arxiv.org/abs/2601.01024)
*Tien-Huy Nguyen,Huu-Loc Tran,Thanh Duc Ngo*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

</details>


### [27] [Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation](https://arxiv.org/abs/2601.01026)
*Douglas Costa Braga,Daniel Oliveira Dantas*

Main category: cs.CV

TL;DR: 提出一个可复现的注意力卷积神经网络（EfficientNetV2-B3 + SE），通过数据增强、focal loss 与病人级划分，在C-NMC 2019数据集上达到97.89% F1/accuracy，参数量远小于VGG16，并通过100次蒙特卡洛实验验证显著性。


<details>
  <summary>Details</summary>
Motivation: Improve automated leukemic cell classification with a reproducible, robust, and computationally efficient deep learning pipeline suitable for clinical use.

Method: 使用EfficientNetV2-B3与Squeeze-and-Excitation构建注意力CNN；应用广泛的数据增强、focal loss处理类别不平衡；采用病人级数据划分与100次蒙特卡洛随机实验进行统计显著性验证；可视化注意力映射以解释模型决策。

Result: An attention-based CNN combining EfficientNetV2-B3 and Squeeze-and-Excitation achieves 97.89% F1 and accuracy on C-NMC 2019, outperforms prior methods by up to 4.67%, uses 89% fewer parameters than VGG16, and provides interpretable attention visualizations.

Conclusion: 现代注意力机制结合高效网络架构能在保证可解释性和计算效率的前提下，显著提升白血病细胞分类性能并具备临床部署潜力。

Abstract: We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.

</details>


### [28] [Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising](https://arxiv.org/abs/2601.01036)
*Kiet Dang Vu,Trung Thai Tran,Kien Nguyen Do Trung,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: 提出Mono3DV：通过3D感知匈牙利匹配+3D去噪+变分查询去噪，稳定引入3D几何信息到DETR匹配中，在KITTI上实现SOTA单目3D检测。


<details>
  <summary>Details</summary>
Motivation: 现有DETR类单目3D目标检测在匈牙利匹配中仅使用2D属性，导致高质量的3D预测被2D匹配条件错误抑制。由于单目3D估计本质上的不确定性，直接将3D属性用于匹配会引发训练不稳定，因此需要一种既能利用3D信息又能稳定训练的策略。

Method: 提出了3个关键模块：1) 3D感知双向匹配（3D-Aware Bipartite Matching），在匹配代价中直接加入3D几何信息；2) 训练阶段的3D去噪（3D-DeNoising）以稳定包含3D属性的匹配过程；3) 变分查询去噪（Variational Query DeNoising），解决传统去噪梯度消失，从而增强性能。整体基于Transformer架构，不依赖外部数据。

Result: 在不使用外部数据的条件下，Mono3DV在KITTI 3D目标检测基准上达到了最先进的结果，表明3D感知匹配与变分去噪等策略对提升单目3D检测性能具有显著作用。

Conclusion: 本文提出的Mono3DV通过将3D信息引入DETR类的双向匹配并结合去噪与变分查询去噪，有效解决了单目3D检测中2D匹配抑制高质量3D预测的问题，显著提升了KITTI基准上性能，验证了方法的有效性。

Abstract: While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.

</details>


### [29] [Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking](https://arxiv.org/abs/2601.01041)
*Xiang Zhang,Wenliang Weng,Daoyong Fu,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出MASM方法，将预训练权重用奇异值分解划分为稳定语义主子空间和多个可学习伪造工件子空间，通过选择性层掩码调节层更新，并加入正交与谱一致性约束，旨在提高跨数据集泛化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 不同伪造方法产生的工件分布差异大，直接微调整个模型会破坏预训练模型的语义结构，导致跨域泛化差。需要在保持语义稳定性的同时建模多样化的工件特征。

Method: 对预训练权重做SVD，划分为语义主子空间与多个可学习工件子空间；针对各子空间引入选择性层掩码以自适应控制层参数更新；对工件子空间施加正交约束与谱一致性约束以保证互补性和稳定的谱结构。

Result: 提出的方法应能在跨数据集和复杂真实场景中提高检测鲁棒性，减少对某一伪造特征的过拟合，提升泛化性能（论文摘要中未给出具体实验数值）。

Conclusion: MASM通过子空间分离与选择性更新，有效保护语义结构并促使多样化、互补的工件表示，降低对单一伪造特征的过拟合，从而提升跨数据集深度伪造检测的泛化性能。

Abstract: Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.

</details>


### [30] [Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data](https://arxiv.org/abs/2601.01044)
*Jin Wang,Angelo De Castro,Yuxi Zhang,Lucas Basolli Borsatto,Yuechen Guo,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 针对奶牛体重预测，迁移学习可在数据稀缺的小农场显著提升性能，且深度图与点云两种模态表现相当；迁移学习因只需共享模型权重而适合隐私/政策受限的跨农场场景。


<details>
  <summary>Details</summary>
Motivation: 探究在牲畜应用中迁移学习的有效性与最佳微调策略，特别是当可用的预训练权重不仅限于ImageNet/COCO时；并比较深度图像与点云两种模态在奶牛体重预测中的表现差异。

Method: 从大、中、小三种规模奶牛场采集顶视深度图像与点云数据，使用ConvNeXt、MobileViT处理深度图，PointNet与DGCNN处理点云，在单源学习、联合学习与迁移学习三种设计下比较模型性能。

Result: 迁移学习在小型农场上显著优于单源学习，并与或优于联合学习，表明仅使用预训练模型权重即可带来跨农场的性能提升；深度图像和点云模型间无稳定差异。

Conclusion: 研究表明在奶牛体重预测任务中，迁移学习能显著提升小型农场的数据表现，且预训练表示在不同农场与成像条件下具有良好泛化能力。

Abstract: Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.

</details>


### [31] [EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos](https://arxiv.org/abs/2601.01050)
*Hongming Fu,Wenjia Wang,Xiaozhen Qiao,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: EgoGrasp reconstructs global, temporally consistent hand-object interactions from egocentric videos using spatial intelligence pre-processing, a decoupled diffusion-based HOI prior, and test-time optimization, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild, addressing limitations of prior methods that handle only single images or camera coordinates and fail under severe camera motion and occlusions.

Method: Multi-stage pipeline: (1) robust pre-processing with spatial intelligence models to handle camera motion and occlusion, (2) whole-body HOI prior learned via decoupled diffusion models that's template-free and multi-object scalable, (3) multi-objective test-time optimization to refine reconstructions.

Result: Proposed EgoGrasp: multi-stage framework including robust pre-process pipeline with spatial intelligence models, whole-body HOI prior via decoupled diffusion models (template-free, multi-object scalable), and multi-objective test-time optimization; achieves state-of-the-art W-HOI reconstruction in experiments.

Conclusion: EgoGrasp effectively handles dynamic cameras, occlusions, and multiple objects to produce accurate world-space HOI reconstructions, setting a new state-of-the-art.

Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.

</details>


### [32] [Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance](https://arxiv.org/abs/2601.01056)
*Ifeanyi Ezuma,Ugochukwu Ugwu*

Main category: cs.CV

TL;DR: 本研究在LC25000病理图像数据集上比较了深度学习与传统机器学习的分类性能。微调的InceptionResNet-v2作为分类器和特征提取器分别评估：微调分类器达96.01%准确率与96.8%平均AUC；基于其深度特征训练的模型表现更优，神经网络达99.84%准确率与99.99%AUC。基于深度特征的模型在不同信噪比下更鲁棒（尤其是GBM与KNN）。HOG与深度特征结合能提升性能，但在噪声环境下提升有限。


<details>
  <summary>Details</summary>
Motivation: 随着数字病理学发展，自动化图像分析在临床中日益重要。研究旨在评估深度学习与传统机器学习在病理图像分类任务的表现及鲁棒性，探索深度特征与手工特征的结合是否能进一步提升性能。

Method: 对LC25000五类组织病理图像数据集，使用微调的InceptionResNet-v2进行两种方式：1) 直接微调网络作为端到端分类器；2) 用微调网络提取深度特征，结合传统机器学习（NN、GBM、KNN等）与手工特征（HOG）训练分类器。比较准确率、平均AUC与信噪比（SNR）变化下的鲁棒性。

Result: 微调InceptionResNet-v2端到端分类准确率96.01%，平均AUC96.8%。使用其深度特征训练的模型表现更好：神经网络达到99.84%准确率与99.99%AUC。基于深度特征的GBM与KNN在不同SNR下展现更强鲁棒性。HOG与深度特征结合提升了性能，但在噪声环境中提升有限。

Conclusion: 使用微调的InceptionResNet-v2提取的深度特征能显著提升传统机器学习模型的分类性能和鲁棒性；直接微调的网络已具备高性能，但将其作为特征提取器并配合机器学习分类器（如NN、GBM、KNN）在准确率和AUC上可取得更优结果。HOG特征有助于进一步提升性能，但易受噪声影响。

Abstract: The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\% and an average AUC of 96.8\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\% and accuracy of 99.84\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.

</details>


### [33] [Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers](https://arxiv.org/abs/2601.01064)
*Jianan Li,Wangcai Zhao,Tingfa Xu*

Main category: cs.CV

TL;DR: LSST separates spectral and spatial processing using grouped self-attention, spectrum shuffle, and depth-wise separable convolutions, plus a focal spectrum loss to boost hard spectral bands, delivering superior, efficient HSI reconstruction.


<details>
  <summary>Details</summary>
Motivation: Improve efficiency and reconstruction quality for HSI from CS measurements by exploiting spectral/spatial properties and reducing computation.

Method: Proposed model and techniques

Result: LSST with separate spectral transformer blocks and lightweight spatial convolutions achieves better reconstruction with fewer FLOPs/params; introduced Focal Spectrum Loss; code released.

Conclusion: The divide-and-conquer LSST design effectively balances performance and efficiency for hyperspectral reconstruction, improving accuracy on complex spectral bands while reducing computational cost.

Abstract: Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.

</details>


### [34] [A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields](https://arxiv.org/abs/2601.01084)
*Adari Rama Sukanya,Puvvula Roopesh Naga Sri Sai,Kota Moses,Rimalapudi Sarvendranath*

Main category: cs.CV

TL;DR: 作者构建并公开了一个在印度稻田全生长期采集的高分辨率RGB与多光谱无人机影像数据集（42k张、1cm GSD、含NDVI/NDRE与完整元数据），可用于精准农业相关研究。


<details>
  <summary>Details</summary>
Motivation: 弥补现有公开数据集中缺乏覆盖全部生育阶段、具有高空间分辨率与丰富元数据的稻田遥感影像的空白，以支持精准喷洒、病害分析与产量估算等应用研究。

Method: 使用20MP RGB相机与5MP四波段（R,G,RE,NIR）多光谱相机，通过制定SOP与检查清单在安得拉邦Vijayawada地区5英亩稻田上采集影像，飞行获得1cm/pixel GSD，共42,430张原始图像（415GB），并利用Pix4D Fields生成正射影像与NDVI/NDRE植被指数图。

Result: 生成包含42,430张高分辨率影像及GPS、飞行高度、环境条件等元数据的公开数据集，并发布于IEEE DataPort（带DOI）；验证了使用Pix4D处理可生成正射与植被指数图，适合下游农学与遥感任务。

Conclusion: 该论文提供了一个覆盖稻田全生育期的大规模无人机RGB与多光谱影像数据集，为印度稻作研究提供高分辨率、多波段和完整元数据支持。

Abstract: We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.

</details>


### [35] [Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models](https://arxiv.org/abs/2601.01085)
*Jiayi Xu,Zhang Zhang,Yuanrui Zhang,Ruitao Chen,Yixian Xu,Tianyu He,Di He*

Main category: cs.CV

TL;DR: Luminark通过补丁亮度阈值与二进制模式定义无训练可认证水印，并用watermark guidance实现跨模型注入，实验证明检测准确、鲁棒且不显著损害图像质量。


<details>
  <summary>Details</summary>
Motivation: 提供一种无需额外训练即可对通用视觉生成模型注入、且具备概率保证的水印方法，以便服务方能识别其合成内容并控制误报。

Method: 定义二进制目标模式和对应补丁阈值；检测时对图像分补丁比较亮度与阈值生成二进制模式并比对；通过统计分析控制误报率；利用通用的指导技术（watermark guidance）作为插拔式注水模块，无需训练即可在扩散、自动回归与混合模型中应用。

Result: 在九种不同模型（扩散、自动回归、混合框架）上验证，Luminark在检测准确率、对常见图像变换的鲁棒性和视觉质量方面表现良好。

Conclusion: Luminark提出了一种基于补丁亮度统计的训练-free且可概率认证的水印方法，能在多种视觉生成模型中注入且保持高检测准确率与鲁棒性。

Abstract: In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.

</details>


### [36] [600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script](https://arxiv.org/abs/2601.01088)
*Haq Nawaz Malik*

Main category: cs.CV

TL;DR: 发布了一个包含约60.2万单词级图像的合成Kashmiri OCR数据集（256x64像素），使用三种字体、数据增强和多背景纹理，分十个档案约10.6GB，许可为CC-BY-4.0，目标是促进Kashmiri低资源OCR研究。


<details>
  <summary>Details</summary>
Motivation: Kashmiri为濒危Dardic语言，使用改良的波斯-阿拉伯字母系统，缺乏足够的OCR训练/评估资源；作者旨在通过公开大规模数据集促进该语言的OCR研究与模型鲁棒性。

Method: 通过合成渲染约602,000个单词级图像（256x64像素），采用三种传统Kashmiri字体、丰富的数据增强（模拟文档退化）和多样背景纹理生成数据；提供多种格式的标注以兼容CRNN、TrOCR等模型，并将数据分成十个压缩包分发。

Result: 释放了约602,000张单词级图像，数据集大小约10.6GB，采用CC-BY-4.0许可，提升了研究可重现性并方便在低资源OCR任务中训练与评估模型。

Conclusion: 该论文发布了一个大规模合成Kashmiri光学字符识别（OCR）数据集，填补了该语言在OCR资源上的空白，对低资源语言识别研究具有重要价值。

Abstract: This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.

</details>


### [37] [NarrativeTrack: Evaluating Video Language Models Beyond the Frame](https://arxiv.org/abs/2601.01095)
*Hyeonjeong Ha,Jinjin Ge,Bo Feng,Kaixin Ma,Gargi Chakraborty*

Main category: cs.CV

TL;DR: 提出NarrativeTrack与CRP评估MLLM的视频叙事理解，发现模型在跨场景追踪实体身份和上下文时常发生幻觉，表明需要把感知与时序推理结合起来。


<details>
  <summary>Details</summary>
Motivation: 分析并评估多模态大语言模型在视频叙事理解上的能力，尤其是实体级别的时序追踪与推理缺陷。

Method: 构建Compositional Reasoning Progression（CRP）从实体存在、实体变化、实体模糊度三维递进复杂性；使用全自动实体中心管线从视频中抽取时序实体表示并生成评测题目，对多种开源与视频专用MLLM进行评估。

Result: 提出了NarrativeTrack基准与CRP评估框架，并通过自动化实体中心管线生成时序化实体表示，揭示现有MLLM在感知扎根与时间一致性之间的权衡与局限。

Conclusion: 现有模型在感知与时间推理上存在互补但未整合的强弱点；NarrativeTrack为诊断与改进时序化叙事理解提供了系统化工具。

Abstract: Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.

</details>


### [38] [Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks](https://arxiv.org/abs/2601.01099)
*Mahmudul Hasan,Mabsur Fatin Bin Hossain*

Main category: cs.CV

TL;DR: 本文比较了自定义CNN架构与预训练/迁移学习模型在5个真实图像数据集上的表现，发现深层网络对细粒度多分类有显著提升，而轻量预训练模型适合简单二分类任务，并将架构扩展至目标检测用于识别未授权三轮车。


<details>
  <summary>Details</summary>
Motivation: 为实际应用提供如何根据任务复杂度和资源约束选择CNN架构的实证指导，填补关于自定义架构与预训练模型在多种场景下系统比较的空白。

Method: 构建自定义CNN变体（不同深度、残差连接、特征提取策略），与多种预训练/迁移学习模型在5个真实数据集上训练对比，评估分类和定位指标，并把最佳结构用于基于检测器的扩展案例。

Result: Comparative study of custom CNN vs pretrained/transfer models across 5 datasets, analyzing depth, residuals, feature extraction, showing deeper nets help fine-grained tasks, lightweight pretrained models work for binary tasks, extended to object detection for unauthorized auto-rickshaw detection; provides guidance for model selection.

Conclusion: 根据任务复杂度和资源限制，应在细粒度识别场景首选深层/残差网络，而在资源受限或二分类任务可优先使用轻量级预训练/迁移模型；所提架构也可用于目标检测场景。

Abstract: This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.

</details>


### [39] [Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization](https://arxiv.org/abs/2601.01103)
*Abhinav Attri,Rajeev Ranjan Dwivedi,Samiran Das,Vinod Kumar Kurmi*

Main category: cs.CV

TL;DR: HAQAGen: aligns global color stats via differentiable histogram matching, injects local hue-sat priors with SPADE, uses Mamba backbone for textures, and adaptive-resolution inference for high-res outputs—improves perceptual metrics across datasets.


<details>
  <summary>Details</summary>
Motivation: To achieve high-quality, high-resolution NIR-to-RGB colorization that preserves both natural colors and fine structural details across diverse scenes and resolutions.

Method: Combined loss with differentiable histogram matching + perceptual and feature losses; SPADE-based hue-saturation prior injection; Mamba backbone for texture-aware supervision; adaptive-resolution inference engine.

Result: HAQAGen proposes a resolution-invariant NIR-to-RGB generative model combining global and local chromatic constraints with texture-aware backbone and adaptive-resolution inference.

Conclusion: HAQAGen effectively balances color realism and structural fidelity, scales to native resolutions, and outperforms baselines in perceptual metrics and texture preservation.

Abstract: We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/

</details>


### [40] [Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation](https://arxiv.org/abs/2601.01167)
*Tianheng Cheng,Xinggang Wang,Junchao Liao,Wenyu Liu*

Main category: cs.CV

TL;DR: GAI通过学习像素的空间与语义关系，自适应地将低分辨率语义信息插值到高分辨率特征，提升分割精度并保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 提高上采样的语义对齐与上下文信息，解决双线性等坐标插值造成的特征错位与语义贫乏问题，同时满足低延迟推理要求。

Method: Guided Attentive Interpolation (GAI)

Result: 提出GAI模块来自适应插值低分辨率到高分辨率特征，通过学习像素间的空间和语义关系来生成富语义的高分辨率特征；构建了基于GAI的分割网络GAIN，在Cityscapes达到78.8 mIoU且22.3 FPS，在CamVid达到80.6 mIoU且64.5 FPS，达成低延迟语义分割的新状态。

Conclusion: GAI有效改善插值特征的语义对齐与上下文丰富性，可无缝集成到现有卷积网络，实验证明在低延迟场景下取得了最先进的分割性能。

Abstract: Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.

</details>


### [41] [CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops](https://arxiv.org/abs/2601.01176)
*Andrés Bell-Navas,Jesús Garicano-Mena,Antonella Ausiello,Soledad Le Clainche,María Villalba-Orero,Enrique Lara-Pezzi*

Main category: cs.CV

TL;DR: 提出CardioMOD-Net：用HODMD提取时间动态+Vision Transformer，从小鼠超声cine loop进行多类HFpEF表型诊断（4类）和连续发病时间回归，诊断准确率65%，发病时间RMSE 21.72周，OB和SAH表现较好。


<details>
  <summary>Details</summary>
Motivation: 当前AI超声研究多聚焦于二分类HFpEF检测，缺乏合并共病表型的多类分型和连续时间预后估计；研究旨在填补这一空白，推动前临床HFpEF诊断与进展预测。

Method: 使用小鼠长轴二维cine loop，应用高阶动态模态分解（HODMD）提取时间序列特征，构建共享潜空间，分别训练Vision Transformer分类器（多类）和回归器（预测HFpEF发生年龄）。

Result: The paper proposes CardioMOD-Net, a unified AI framework for multiclass diagnosis and continuous prediction of HFpEF onset from echocardiography cine loops in mice. It decomposes parasternal long-axis cine loops using HODMD to extract temporal features, uses a shared latent space and Vision Transformers for classification and regression, achieving 65% diagnostic accuracy across four groups and RMSE 21.72 weeks for onset prediction, with better performance for OB and SAH. The study indicates feasibility of combined diagnostic/prognostic modelling from single cine loops in small-data preclinical settings.

Conclusion: 在小样本前临床模型中，单个二维心脏超声cine loop经HODMD和Transformer处理，可同时实现HFpEF多类分型和发病时间预测，为诊断+预后一体化研究提供可行框架。

Abstract: Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.
  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.
  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.
  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.

</details>


### [42] [GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation](https://arxiv.org/abs/2601.01181)
*Chenglizhao Chen,Shaojiang Yuan,Xiaoxue Lu,Mengke Song,Jia Song,Zhenyu Wu,Wenfeng Song,Shuai Li*

Main category: cs.CV

TL;DR: 作者提出使用生成模型合成伪装场景数据：构建GenCAMO-DB并设计无掩码的环境感知生成框架GenCAMO，用于提升伪装密集预测任务的表现。


<details>
  <summary>Details</summary>
Motivation: 真实世界伪装数据的采集与密集标注成本高，数据规模小且多模态标注稀缺，阻碍了复杂伪装场景下密集预测模型的发展。用生成模型合成高质量多模态数据可作为低成本替代，提升模型泛化与推理能力。

Method: 提出GenCAMO框架：环境感知（scene context）驱动、无需显式目标掩码的生成流程，产出图像及对应深度图、场景图、属性描述和文本提示。利用这些合成样本与真实数据混合训练，增强模型的细粒度表征与先验推理能力。

Result: GenCAMO-DB: large-scale multi-modal synthetic camouflage dataset (depth, scene graphs, attributes, prompts) generated via environment-aware, mask-free generative framework GenCAMO. Improves training for RGB-D COD and open-vocabulary COS. Experiments show significant performance gains.

Conclusion: 通过高质量多模态合成数据，能够有效缓解真实标注稀缺问题，并在复杂伪装场景的密集预测任务（如RGB-D伪装目标检测与开放词汇伪装分割）上显著提升性能。

Abstract: Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.

</details>


### [43] [Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors](https://arxiv.org/abs/2601.01192)
*Hao Lu,Xuhui Zhu,Wenjing Zhang,Yanan Li,Xiang Bai*

Main category: cs.CV

TL;DR: 构建拥挤场景VIC数据集并提出结合社会分组与位移先验的O2M匹配基线OMAN++，显著提升拥挤场景个体计数性能。


<details>
  <summary>Details</summary>
Motivation: 解决拥挤场景下视频个体计数(VIC)在识别帧间对应关系上的困难，构建更具代表性的数据集并引入社会分组与时空位移先验以改进匹配策略与特征学习。

Method: 论文方法

Result: 提出WuhanMetroCrowd数据集和新基线OMAN++，包含隐式上下文生成器、O2M匹配器和位移先验注入模块；在多个基准上超越现有方法，在拥挤场景上显著降低错误（在新数据集上错误减少38.12%）。

Conclusion: 引入一对多匹配及位移先验能更好地适应拥挤动态人群的帧间对应任务，OMAN++在多数据集上证明了有效性，尤其在高密度场景中效果提升明显。

Abstract: Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.

</details>


### [44] [MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity](https://arxiv.org/abs/2601.01200)
*Zhang Chen,Shuai Wan,Yuezhe Zhang,Siyu Ren,Fuzheng Yang,Junhui Hou*

Main category: cs.CV

TL;DR: Paper uses RBF to model local point-cloud features implicitly and compares coefficients across scales; introduces grouped residual MLP with channel attention; shows better performance


<details>
  <summary>Details</summary>
Motivation: Address point cloud quality assessment challenges by avoiding point-to-point matching errors using continuous RBF-based feature representation and a grouped ResGrouped-MLP network to map multi-scale differences to scores

Method: Read abstract and produce concise analysis

Result: Proposed MS-ISSM and ResGrouped-MLP outperform prior metrics on benchmarks; code released

Conclusion: MS-ISSM avoids matching errors, captures multi-scale perceptual distortions, and combined with ResGrouped-MLP yields improved PCQA metrics with good generalization

Abstract: The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.

</details>


### [45] [RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models](https://arxiv.org/abs/2601.01202)
*Jiazhu Dai,Huihui Jiang*

Main category: cs.CV

TL;DR: 提出一种对参考图像的对抗攻击RefSR-Adv，能有效破坏RefSR重建，跨架构跨数据集生效；发现模型对参考图像过度依赖是漏洞，应关注RefSR鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注RefSR的后门攻击，然而对抗攻击对RefSR的脆弱性未被充分研究；作者旨在填补该空白并促使社区重视RefSR的鲁棒性。

Method: 提出RefSR-Adv攻击：在参考图像上优化对抗扰动，目标是最大化对抗输出与原始输出之间的差异，从而在不修改低分辨率输入的情况下破坏高频纹理重建。

Result: 在CUFED5、WR-SR和DRefSR数据集上，RefSR-Adv在CNN、Transformer和Mamba等多种架构上均能引入严重伪影并显著下降性能；实验还发现低分辨率输入与参考图像相似度越高，攻击越有效，表明模型对参考特征的过度依赖是主要安全缺陷。

Conclusion: 本文揭示了RefSR（参考图像超分辨）系统存在的安全漏洞，即通过仅对参考图像施加对抗扰动即可显著恶化重建结果。

Abstract: Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.

</details>


### [46] [XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression](https://arxiv.org/abs/2601.01204)
*Zunhai Su,Weihao Ye,Hansen Feng,Keyu Fan,Jing Zhang,Dahai Yu,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: 提出XStreamVGGT：对StreamVGGT的KV缓存进行重要性识别剪枝与量化，达到≈4.4×更少内存和≈5.5×更快推理，性能损失极小。


<details>
  <summary>Details</summary>
Motivation: 解决StreamVGGT因KV缓存无界增长导致的内存和延迟问题，使流式3D重建在长序列输入下可扩展且实用。

Method: 方法包含两步：1) 通过高效token重要性识别对多视角产生的冗余KVs进行剪枝，确保固定内存预算；2) 基于KV张量分布特性进行量化以进一步减少内存占用；全流程无需微调。

Result: XStreamVGGT提出了一种无须微调的KV缓存压缩方法，通过联合剪枝与量化在流式Transformer中显著降低内存占用并加速推理。

Conclusion: XStreamVGGT在保持重建性能的前提下，通过KV剪枝+量化实现了极高的内存与速度提升，适用于大规模流式3D视觉几何应用。

Abstract: Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>


### [47] [Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission](https://arxiv.org/abs/2601.01210)
*Kazuhiko Murasaki,Shunsuke Konagai,Masakatsu Aoki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: Propose a CNN implementing joint bilateral filtering to fuse multi-LiDAR and high-res RGB to produce real-time (30 fps) full-HD dense depth maps, much faster than prior training-based methods, yielding accurate, artifact-free dense point clouds


<details>
  <summary>Details</summary>
Motivation: Need low-latency dense 3D reconstruction for immersive telepresence: LiDAR is real-time but sparse, so require fast on-the-fly depth completion without multiview artifacts

Method: CNN-based joint bilateral filtering for real-time LiDAR densification

Result: Full-HD dense depth maps at 30 fps, >15x faster than recent learning-based method; dense point clouds with accurate geometry and no multiview inconsistencies/ghosting

Conclusion: Effective high-speed densification via CNN-implemented joint bilateral filtering enabling real-time immersive telepresence without ghosting or multiview inconsistency

Abstract: To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.

</details>


### [48] [Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation](https://arxiv.org/abs/2601.01213)
*Riccardo Gelato,Carlo Sgaravatti,Jakob Grahn,Giacomo Boracchi,Filippo Maria Bianchi*

Main category: cs.CV

TL;DR: 本文将Segment Anything Model (SAM) 移植并适配到Sentinel-1 SAR影像，采用adapter、多编码器、提示工程和限制编码器训练时间的算法，从而提高对小型、低对比雪崩目标的分割与标注效率，并集成到标注工具，显著加速标注流程。


<details>
  <summary>Details</summary>
Motivation: 高质量雪崩分割模型需要大量专家标注，但SAR影像标注耗时昂贵。将通用分割基础模型（SAM）适配至SAR可显著降低人工标注成本，加速雪崩制图与风险管理。

Method: 使用领域适配器来缓解SAM的域迁移问题；用多编码器结构支持超过三通道的SAR输入；设计提示工程以增强对不精确提示的鲁棒性；限制对主编码器的训练时间以提高训练效率；将模型嵌入标注工具并进行用户实验评估。

Result: SAR影像雪崩分割与标注加速方法

Conclusion: 通过组合adapter、多通道编码器、鲁棒提示策略和高效训练算法，改进后的SAM能更好处理SAR多通道输入与域差异，在标注工具中能降低人工标注时间，提高小目标雪崩的检测与分割质量。

Abstract: Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.

</details>


### [49] [UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass](https://arxiv.org/abs/2601.01222)
*Mengfei Li,Peng Li,Zheng Zhang,Jiahao Lu,Chengfeng Zhao,Wei Xue,Qifeng Liu,Sida Peng,Wenxiao Zhang,Wenhan Luo,Yuan Liu,Yike Guo*

Main category: cs.CV

TL;DR: UniSH提出一种在一次前向传递中联合恢复度量尺度场景与高保真人体的框架，核心在于利用无标注野外数据的蒸馏与两阶段监督来缩小sim-to-real差距。


<details>
  <summary>Details</summary>
Motivation: 真实标注数据稀缺导致对合成数据依赖，从而产生严重sim-to-real域差，使得模型在人类几何细节和野外视频对齐上表现差。为缩小域差并利用大量无标注野外数据，提出了蒸馏与两阶段监督训练范式。

Method: 模型融合场景重建与HMR先验，训练包含（1）从专家深度模型蒸馏高频表面细节的鲁棒蒸馏；（2）先在合成数据学习粗定位，再在真实无标注数据上通过直接优化SMPL与点云几何对应的两阶段监督微调。模型为一次前向传递同时输出场景几何、人点云、相机参数与度量尺度SMPL。

Result: 提出了 UniSH，一个统一的前馈框架，可联合恢复具有度量尺度的3D场景与人体。为应对真实标注数据稀缺导致的sim-to-real差距，论文提出了利用无标签野外数据的训练范式，融合场景重建与HMR的强先验。核心组件包括：1）通过从专家深度模型蒸馏高频细节以精炼人表面细节的鲁棒蒸馏策略；2）先在合成数据上学习粗定位，再在真实数据上通过优化SMPL网格与人体点云的几何对应进行微调的两阶段监督方案。最终模型在一次前向传播中同时输出高保真场景几何、人类点云、相机参数和度量一致的SMPL人体。实验显示在以人为中心的场景重建任务上达到了最先进性能，并在全局人体运动估计上与基于优化的方法和仅HMR方法相比表现具竞争力。

Conclusion: 通过将专家深度模型的高频细节蒸馏到前馈网络并在合成->真实的两阶段监督下微调，UniSH成功提高了人体几何细节与场景-人体对齐，达到了SOTA表现。

Abstract: We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/

</details>


### [50] [Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment](https://arxiv.org/abs/2601.01224)
*Bac Nguyen,Yuhta Takida,Naoki Murata,Chieh-Hsin Lai,Toshimitsu Uesaka,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: CODA adds register slots and contrastive loss to improve slot-image alignment and reduce entanglement, boosting OCL performance


<details>
  <summary>Details</summary>
Motivation: explain why proposed approach helps slot attention issues

Method: analysis of method

Result: improved FG-ARI etc

Conclusion: CODA is efficient and effective for real-world OCL

Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.

</details>


### [51] [HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training](https://arxiv.org/abs/2601.01228)
*Markus Haltmeier,Lukas Neumann,Nadja Gruber,Johannes Schwab,Gyeongha Hwang*

Main category: cs.CV

TL;DR: Proposes HyDRA, a measurement-only DEQ framework using adaptive denoising regularization and data-driven early stopping to train DEQ models from measurements only, achieving competitive sparse-view CT results.


<details>
  <summary>Details</summary>
Motivation: Overcome ill-posedness and lack of supervised (x,y) pairs by training DEQ models using only measurements y via combining measurement consistency and adaptive denoising regularization and data-driven early stopping

Method: HyDRA: measurement-only DEQ training for image reconstruction

Result: HyDRA enables competitive sparse-view CT reconstructions with fast inference without supervised pairs

Conclusion: HyDRA shows that DEQs can be trained without paired ground truth by combining measurement consistency, adaptive denoising regularization, and learned stopping, yielding strong reconstruction results and efficient inference.

Abstract: Solving image reconstruction problems of the form \(\mathbf{A} \mathbf{x} = \mathbf{y}\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \((\mathbf{x},\mathbf{y})\). In many practical settings, only measurements \(\mathbf{y}\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.

</details>


### [52] [RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection](https://arxiv.org/abs/2601.01240)
*Ziqian Guan,Xieyi Fu,Yuting Wang,Haowen Xiao,Jiarui Zhu,Yingying Zhu,Yongtao Liu,Lin Gu*

Main category: cs.CV

TL;DR: RFAssigner通过点先验+高斯感受野距离，从未分配位置自适应挑选补充正样本，改善小目标正样本不足，实现多尺度平衡训练并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 训练密集检测器时正负样本分配不平衡，尤其对小目标正样本不足，导致模型多尺度学习受限。

Method: 基于点先验构建初始正样本集合；定义高斯感受野(GRF)并计算候选点与GT的GRF距离；基于GRF相似度自适应选择额外正样本，平衡各尺度样本数量；在多个数据集和单模型FCOS-ResNet-50上验证性能。

Result: 提出RFAssigner：先用点先验确定初始正样本，再用高斯感受野(GRF)距离衡量未分配候选位置与GT的相似度，自适应选择额外正样本，实现多尺度更平衡的学习。在三个具有不同尺度分布的数据集上，单一FCOS-ResNet-50配RFAssigner在所有尺度上取得SOTA且优于现有方法，无需额外模块或启发式策略。

Conclusion: RFAssigner有效缓解尺度不均衡问题，提升不同尺度目标的检测性能，简单且具通用性，可直接集成到现有密集检测器中。

Abstract: Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.

</details>


### [53] [MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance](https://arxiv.org/abs/2601.01260)
*Hamad Khan,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 设计了MambaFormer：以轻量级路由器在Token级别动态分配定制Transformer(ET5)或状态空间模型(EMamba)专家，使用多目标效用损失平衡延迟与准确率，在牙科与医学QA任务上达成高效且低延迟的预测。


<details>
  <summary>Details</summary>
Motivation: 提高在临床场景中部署大语言模型(LLMs)的效率，解决计算成本与线性时间模型效率之间的权衡。

Method: 构建轻量级门控器进行token级动态路由；定制并微调ET5与EMamba专家以适配序列长度和任务；利用上下文复杂度、归一化序列长度和领域特征驱动路由；引入效用引导的多目标损失以联合优化路由与计算成本。

Result: 提出名为MambaFormer的基于LLM的混合Mixture-of-Experts(MoE)框架，在DentalQA和PubMedQA数据集上取得良好性能（BERTScore=0.9180），并显著降低延迟（0.077s），比T5-Large快24.4倍。

Conclusion: MambaFormer在保证准确率的同时显著降低推理延迟，为资源受限的临床部署提供可扩展解决方案。

Abstract: The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

</details>


### [54] [AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures](https://arxiv.org/abs/2601.01281)
*Sifatullah Sheikh Urmi,Kirtonia Nuzath Tabassum Arthi,Md Al-Imran*

Main category: cs.CV

TL;DR: 评估了三种CNN和一种ViT，使用数据预处理与增强，VFDNET+MobileNetV3表现最佳，说明AI可用于可靠的深偽检测。


<details>
  <summary>Details</summary>
Motivation: The paper addresses the growing problem of AI-generated deepfakes threatening digital authenticity and the need for reliable detection methods.

Method: 比较四种模型（3个CNN+1个ViT），在大规模人脸图像数据集上进行训练与测试，采用数据预处理与增强来提升性能。

Result: VFDNET with MobileNetV3 achieved superior accuracy and efficient performance among the evaluated models.

Conclusion: 结合数据预处理和增强技术，VFDNET在MobileNetV3上表现最优，证明了该方法在深偽检测任务中的有效性和效率。

Abstract: The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.

</details>


### [55] [S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss](https://arxiv.org/abs/2601.01285)
*Md. Sanaullah Chowdhury Lameya Sabrin*

Main category: cs.CV

TL;DR: S2M-Net通过频谱截断FFT和形态自适应损失以低成本获得全局上下文并自动调整损失权重，在多模态医学分割上用更少参数达到了或超过最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割需在边界局部精度、解剖学全局一致性和计算效率之间取得平衡，现有卷积或Transformer方法各有短板：卷积局部但感受野受限，Transformer全局但成本/过拟合问题严重。

Method: 提出了Spectral-Selective Token Mixer (SSTM)——基于截断二维FFT并加入可学习频率过滤和内容门控的空间投影，以O(HW log HW)代价获得全局上下文；以及Morphology-Aware Adaptive Segmentation Loss (MASL)——通过自动分析目标形态（紧凑性、管状性、不规则性、尺度）以受约束的可学习权重自适应地组合五类互补损失，免去手动调参。

Result: 在16个跨8种模态的数据集上进行全面评估，在息肉分割Dice 96.12%、外科器械83.77%（较先前工作提升17.85%）、脑肿瘤80.90%，并在多项任务上比专用基线提升3–18%，参数量仅4.7M，较Transformer方法少3.5–6×。

Conclusion: S2M-Net在保持计算高效性的同时，通过频谱选择的token混合器和形态感知自适应损失实现了全局感受野和边界精度的平衡，显著提升了多种医学图像分割任务的性能。

Abstract: Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\mathcal{O}(HW \log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\% Dice on polyp segmentation, 83.77\% on surgical instruments (+17.85\% over the prior art) and 80.90\% on brain tumors, with consistent 3-18\% improvements over specialized baselines while using 3.5--6$\times$ fewer parameters than transformer-based methods.

</details>


### [56] [VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results](https://arxiv.org/abs/2601.01312)
*Kailash A. Hambarde,Hugo Proença,Md Rashidunnabi,Pranita Samale,Qiwei Yang,Pingping Zhang,Zijing Gong,Yuhao Wang,Xi Zhang,Ruoshui Qu,Qiaoyun He,Yuhang Zhang,Thi Ngoc Ha Nguyen,Tien-Dung Mai,Cheng-Jun Kang,Yu-Fan Lin,Jin-Hui Jiang,Chih-Chung Hsu,Tamás Endrei,György Cserey,Ashwat Rajbhandari*

Main category: cs.CV

TL;DR: 提出并发布VReID-XFD极远距离空地行人再识别视频基准与挑战，数据规模大、元数据丰富，系统评测显示高度/距离增大会显著降性能，正视准直视角劣势，最好方法在空对地场景仅得43.93% mAP


<details>
  <summary>Details</summary>
Motivation: Existing ReID systems assume good appearance cues; extreme far-distance aerial-to-ground scenarios break these assumptions due to severe resolution loss, extreme viewpoints, unstable motion, clothing variation; need a dedicated benchmark and analysis

Method: benchmark dataset creation and challenge organization; systematic empirical evaluation across altitudes and view angles using video-based ReID pipelines

Result: VReID-XFD dataset: 371 identities, 11,288 tracklets, 11.75M frames; covers altitudes 5.8–120m, angles 30–90°, distances up to 120m; supports multiple evaluation modes; challenge attracted 10 teams; findings: performance degrades with altitude/distance, nadir views worse, trade-off between peak and robustness; best mAP 43.93% for aerial-to-ground

Conclusion: VReID-XFD揭示极远距离空地ReID的困难与研究方向：需设计对低分辨率、极端视角和不稳定运动鲁棒的新方法，并利用物理元数据和视频时序信息提升性能。

Abstract: Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .

</details>


### [57] [LinMU: Multimodal Understanding Made Linear](https://arxiv.org/abs/2601.01322)
*Hongjie Wang,Niraj K. Jha*

Main category: cs.CV

TL;DR: 提出 LinMU，一种全线性复杂度的视觉-语言模型设计，替换自注意力为 M-MATE 双分支模块（Flex-MA 状态空间模型 + Local-Swin 窗口注意力），并用三阶段蒸馏将预训练模型转换为该架构，在多项基准上达到教师模型性能同时显著提升长视频/高分辨率处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有 VLM 受自注意力二次复杂度限制，无法高效处理高分辨率图像与长上下文视频，也难以部署在边缘设备；目标是实现线性复杂度同时保持全局注意力性能。

Method: 关键方法是将每个自注意力层替换为 M-MATE 模块：一条基于双向状态空间模型的 Flex-MA 分支用于全局上下文，另一条 Local-Swin 分支用于局部相关性；并设计三阶段蒸馏流程（初始化并仅训练 Flex-MA、解冻并联合训练 Local-Swin、解冻剩余块并用 LoRA 回归隐藏态与 logits）以从预训练 VLM 有效迁移。

Result: 在 MM-VU（文中 MMMU?）、TextVQA、LongVideoBench、Video-MME 等基准上，LinMU 达到与教师模型相当的性能，同时在分钟级视频上将 TTFT 最低减少至 2.7 倍、token 吞吐率最高提高 9.0 倍；消融实验验证三阶段蒸馏和双分支设计的必要性。

Conclusion: LinMU 在不使用任何二次复杂度模块下，能够实现与全局自注意力 VLM 相当的多模态理解性能，并大幅降低推理时延和提高长序列吞吐量，证明了无二次复杂度的 VLM 在高分辨率图像和长视频任务中的可行性。

Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.

</details>


### [58] [Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning](https://arxiv.org/abs/2601.01339)
*Weihang You,Hanqi Jiang,Yi Pan,Junhao Chen,Tianming Liu,Fei Dou*

Main category: cs.CV

TL;DR: 提出NeuroAlign，通过NTCL和增强向量量化模拟视觉系统的层次与时序，辅以DynaSyncMM-EMA动态多模态融合，实现更精细的fMRI与视频对齐，显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: Reduce modality gap and better reflect hierarchical and temporal processing in brain for fMRI-video alignment.

Method: Two-stage: 1) NTCL for global semantic + temporal bidirectional prediction; 2) enhanced vector quantization for fine-grained pattern matching; DynaSyncMM-EMA for adaptive multimodal fusion.

Result: NeuroAlign: two-stage framework with NTCL (Neural-Temporal Contrastive Learning) for temporal bidirectional prediction and enhanced vector quantization for fine-grained matching; DynaSyncMM-EMA for dynamic multimodal fusion; outperforms baselines in cross-modal retrieval.

Conclusion: NeuroAlign establishes a new paradigm for modeling hierarchical temporal neural-visual alignment, improving cross-modal retrieval and advancing understanding of visual cognitive mechanisms.

Abstract: Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.

</details>


### [59] [Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding](https://arxiv.org/abs/2601.01352)
*Yixuan Lai,He Wang,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: 用短参考视频学习紧凑的动态身份tokens，通过Sinkhorn路由将动态信息作为轻量级条件融入扩散-Transformer视频生成器，从而在大视角和表情变化下更好地保留身份，同时保持自然性和提示忠实度。


<details>
  <summary>Details</summary>
Motivation: 单张图像作为参考缺乏时间信息导致生成的视频出现姿态锁定、非自然变形和平均人脸；短视频包含个体特定的动态模式（如微笑方式），可用于更好地保持身份。

Method: 提出了一个基于扩散-Transformer的视频生成器的身份条件变体，使用Sinkhorn路由编码器从短视频中学习紧凑的身份动态tokens，与预训练主干兼容，并作为轻量级条件加入生成器。

Result: 在各种主体和提示下，方法在保持提示忠实度和视觉真实感的同时，显著提高了在大姿态变化和富表情行为时的身份保持。

Conclusion: 该工作提出了用短参考视频而不是单张图像来进行身份条件的视频生成，通过引入参考中的动态信息来提升身份保持能力，尤其在大视角变化和丰富表情下效果显著。

Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.

</details>


### [60] [Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance](https://arxiv.org/abs/2601.01356)
*Dang H. Pham,Tu N. Nguyen,Hoa N. Nguyen*

Main category: cs.CV

TL;DR: 本文提出三种行人重识别（ReID）方法：SCM-ReID（监督对比+混合损失）、IQAGA/DAPRH（基于GAN的域自适应与伪标签优化）和ViTC-UReID（基于ViT与相机感知代理学习）。在多个数据集和设置下均显著提升性能。


<details>
  <summary>Details</summary>
Motivation: ReID在跨摄像头环境中受外观变化、域差异与标注不足影响，需提升特征判别性、域泛化能力与对噪声伪标签的鲁棒性。

Method: 1) SCM-ReID：监督对比分支+分类、中心、三元组和质心三元组混合损失以增强类内紧凑性与类间分离；2) IQAGA/DAPRH：GAN生成目标风格图像、域不变映射网络与基于图/邻居的伪标签精炼策略；3) ViTC-UReID：利用Vision Transformer提取全局与局部注意力特征，结合相机感知代理学习以缓解相机偏差。

Result: 在Market-1501、CUHK03、DukeMTMC-reID和MSMT17等数据集上，提出方法在不同设置下均带来显著提升，UDA场景中mAP和Rank-1最多提升约12%。

Conclusion: 提出的方法分别在有监督、UDA和完全无监督三类设置中提高了ReID性能；在标准基准上取得了显著的mAP和Rank-1提升，证明了在特征判别性、域不变性和伪标签噪声处理上的有效性。

Abstract: Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.

</details>


### [61] [Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser](https://arxiv.org/abs/2601.01360)
*Jiawei Fang,Ruonan Zheng,Xiaoxia Gao,Shifan Jiang,Anjun Chen,Qi Ye,Shihui Guo*

Main category: cs.CV

TL;DR: 提出GID，一种轻量级可插拔Transformer，用位置感知专家分头+跨部位融合+通用姿态预测，专门去噪松散衣物中IMU的位移伪影；并提供GarMoCap数据集，能在单用户训练下实时泛化并提升现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是解决将IMU嵌入宽松衣物时传感器相对身体位移导致的结构化、位置相关噪声，因紧贴式传感器对日常使用而言侵入性强，需设计能在宽松服饰下恢复精确MoCap的轻量泛化方法和数据集。

Method: 方法将问题分解为三个阶段：位置特定去噪（每个IMU对应专家头学习局部服装动态）、自适应跨部位融合（轻量级模块保持一致性）、通用姿态预测（共享时空骨干提取全局运动）；基于Transformer结构和位置感知专家架构训练，利用配对松紧IMU数据进行监督。

Result: GID (Garment Inertial Denoiser) introduces a transformer-based denoiser that corrects garment-induced IMU noise, using location-specific expert heads, spatio-temporal backbone, and fusion module; trained on GarMoCap dataset; improves downstream inertial MoCap methods and generalizes across users/motions/garments.

Conclusion: GID能有效恢复松衣着用场景下的IMU信号，提升惯性MoCap精度，具有实时性和跨用户/动作/服装的泛化能力，可作为现有惯性管线的drop-in模块。

Abstract: Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.

</details>


### [62] [Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography](https://arxiv.org/abs/2601.01364)
*Mostofa Rafid Uddin,Mahek Vora,Qifeng Wu,Muyuan Chen,Min Xu*

Main category: cs.CV

TL;DR: 提出一种将SE(3)变换与形态学内容分离的深度表示学习框架，包含多选学习模块以应对高噪声，能生成模板形态并在仿真与真实数据上优于现有方法，发现新形态。


<details>
  <summary>Details</summary>
Motivation: This paper aims to improve morphology inference in cryo-electron tomography (cryo-ET) by addressing limitations of existing EM-based methods, specifically their failure to capture rare morphologies and sensitivity to hyperparameters.

Method: Use a disentangled representation with separate latent spaces for SE(3) transformations and morphological content; introduce a multi-choice learning module to robustly assign subvolumes to morphological templates under high noise; train end-to-end to generate denoised template morphologies.

Result: They develop a disentangled deep representation learning framework that separates SE(3) transformations from morphological content, with a novel multi-choice learning module to handle high noise. The learned content is used to generate template morphologies, showing improved performance and discovery of new morphologies on simulated and real datasets.

Conclusion: The framework effectively disentangles pose and morphology, reduces reliance on EM and manual tuning, improves detection of rare morphologies, and aids discovery of previously unidentified structures.

Abstract: Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.

</details>


### [63] [ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking](https://arxiv.org/abs/2601.01386)
*Xiaobao Wei,Zhangjie Ye,Yuxiang Gu,Zunjie Zhu,Yunfei Guo,Yingying Shen,Shan Zhao,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Rongfeng Lu,Hangjun Ye*

Main category: cs.CV

TL;DR: 提出ParkRecon3D数据集和ParkGaussian方法，基于3D Gaussian Splatting并引入slot-aware策略，提升停车场景重建质量和与车位感知的一致性


<details>
  <summary>Details</summary>
Motivation: Existing parking reconstruction work focuses on 2D perception; 3D reconstruction needed for complex spatial geometry and alignment with parking slot perception

Method: 3D Gaussian Splatting with slot-aware reconstruction

Result: ParkGaussian achieves SOTA reconstruction and better preserves perception consistency for parking slot detection on ParkRecon3D

Conclusion: 方法在ParkRecon3D上证明有效，将开源代码和数据集

Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian

</details>


### [64] [Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets](https://arxiv.org/abs/2601.01393)
*Shamik Shafkat Avro,Nazira Jesmin Lina,Shahanaz Sharmin*

Main category: cs.CV

TL;DR: CustomCNN通过若干架构改进在多领域图像分类任务上实现了高效且有竞争力的性能，适用于智能城市与农业应用。


<details>
  <summary>Details</summary>
Motivation: 研究架构设计如何影响多领域图像分类性能，特别在智能城市和农业场景中实现高效、泛化的模型。

Method: 设计轻量CustomCNN，使用残差连接稳定训练、SE模块提升通道注意力、逐层增加通道数并用Kaiming初始化，最后在五个公开数据集上训练评估并与流行CNN比较。

Result: 提出一种集成残差连接、SE注意力、渐进通道扩展和Kaiming初始化的CustomCNN，在五个公开数据集上表现出计算高效且性能具竞争力的结果。

Conclusion: 有针对性的网络设计（残差+SE+渐进通道扩展+Kaiming初始化）能在保持计算效率的同时提升多领域分类效果，适合实际应用。

Abstract: This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.

</details>


### [65] [SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution](https://arxiv.org/abs/2601.01406)
*Habiba Kausar,Saeed Anwar,Omar Jamal Hammad,Abdul Bais*

Main category: cs.CV

TL;DR: Introduce SwinIFS: integrates landmark heatmaps with Swin Transformer for identity-preserving face SR up to 8x, achieving sharper, photorealistic results and good efficiency on CelebA.


<details>
  <summary>Details</summary>
Motivation: Face super-resolution suffers from loss of fine structural details and identity-specific features when upscaling severely degraded inputs.

Method: Input concatenation of RGB and dense Gaussian landmark heatmaps fed into a compact Swin Transformer with hierarchical attention; trained and evaluated on CelebA for multiple upscaling factors.

Result: SwinIFS: landmark-guided framework using dense Gaussian heatmaps and compact Swin Transformer backbone to achieve identity-preserving SR with strong 8x performance, better perceptual quality and efficiency.

Conclusion: SwinIFS recovers finer facial details and preserves identity across large upscaling factors, balancing accuracy and efficiency for practical applications.

Abstract: Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.

</details>


### [66] [Mask-Guided Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01408)
*Gong Gao,Zekai Wang,Jian Zhao,Ziqi Xie,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 提出MGMTN：用AML生成局部组掩码定位关键面部区域，并通过G2FF融合组与全局特征，显著改善面部属性识别效果。


<details>
  <summary>Details</summary>
Motivation: 动机是传统多任务面部属性识别方法依赖全局特征，容易包含冗余信息并导致不同属性之间的负迁移，故提出选择性关注局部有效区域以提高特征学习效率。

Method: 方法包括两部分：Adaptive Mask Learning (AML)利用预训练关键点标注模型和全卷积网络精确定位关键面部部位并生成组掩码；Group-Global Feature Fusion (G2FF)将组特征与全局特征融合用于属性分类。

Result: 在两个具有挑战性的面部属性识别数据集上进行的大量实验表明，MGMTN在提高识别性能方面优于基线方法。

Conclusion: 该论文提出的MGMTN通过引入AML和G2FF，有效缓解了全局特征带来的冗余和负迁移问题，从而提高了面部属性识别性能。

Abstract: Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.

</details>


### [67] [AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval](https://arxiv.org/abs/2601.01416)
*Yue Zhou,Ran Ding,Xue Yang,Xue Jiang,Xingzhao Liu*

Main category: cs.CV

TL;DR: 该论文提出了针对无人机车辆图像的空间感知数据集AirSpatial（含206K+指令），并设置了空间定位和空间问答两项任务，首次在遥感配准数据集中提供3D边界框（3DBB）。采用“图像理解预训练 + 空间理解微调”的两阶段训练策略，构建了具备精细车辆属性识别与检索能力的空中代理AirSpatialBot，集成任务规划、图像理解、空间理解与执行。实验展示了现有视觉语言模型在空间理解上的局限，并给出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有远程感知VLM在空间理解上表现不足，尤其在无人机视角的车辆图像中。为提升实际应用能力，需要专门的数据集、任务与训练方法来增强空间感知能力。

Method: 构建大规模标注数据集（206K+指令），包含二维/三维标注和两类任务；采用先在图像理解任务上预训练再在空间任务上微调的两阶段训练；开发一个集成的代理系统用于调度VLM完成识别、检索与执行等任务。

Result: 见下文

Conclusion: AirSpatial数据集与两阶段训练策略显著提升了VLM在遥感车辆空间理解任务上的能力，AirSpatialBot能实现细粒度属性识别和动态任务适应，证实了该方向的可行性并为后续研究提供了资源。

Abstract: Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot

</details>


### [68] [DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer](https://arxiv.org/abs/2601.01425)
*Xu Guo,Fulong Ye,Xinghui Li,Pengqi Tu,Pengze Zhang,Qichao Sun,Songtao Zhao,Xiangwang Hou,Qian He*

Main category: cs.CV

TL;DR: 本文提出SyncID-Pipe数据流水线与DreamID-V（扩散Transformer），结合合成-真实课程与身份连贯强化学习，构建IDBench-V基准，显著提升视频人脸换脸的身份一致性、属性保真与时序一致性，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VFS方法很难在保持身份相似性的同时保留姿态、表情、光照、背景与时间动态一致性；因此作者希望把在图像换脸领域表现优越的模型和训练策略迁移/拓展到视频换脸，从而提升整体质量与一致性。

Method: 1) 设计SyncID-Pipe：预训练Identity-Anchored Video Synthesizer并与IFS模型结合，构造双向ID四元组用于显式监督。2) 提出DreamID-V：首个扩散Transformer框架，核心为Modality-Aware Conditioning模块用于判别式注入多模态条件。3) 引入Synthetic-to-Real Curriculum和Identity-Coherence RL策略以提升真实感和身份一致性。4) 构建IDBench-V基准并在多场景下验证。

Result: 提出的方法在自建基准和对比实验中显著优于现有方法，在身份相似度、属性保留、时序一致性与视觉真实感上都有提升，且方法通用性强，可扩展至多种相关换脸任务。

Conclusion: 该论文提出了将图像人脸替换优势迁移到视频领域的全面框架，以提升视频人脸换脸的身份相似性与属性保真、以及时序一致性。通过构建SyncID-Pipe数据流水线预训练身份锚定的视频合成器，结合图像换脸模型生成成对四元组监督；并提出基于扩散-Transformer的DreamID-V框架，采用模态感知条件注入模块、合成到真实的课程机制与身份连贯强化学习来增强视觉真实感与身份一致性。同时构建了包含多样场景的IDBench-V基准。实验显示方法在多项指标上优于SOTA，并具备良好的适配性。

Abstract: Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.

</details>


### [69] [EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views](https://arxiv.org/abs/2601.01431)
*Weiqi Yu,Yiyang Yao,Lin He,Jianming Lv*

Main category: cs.CV

TL;DR: 提出一种基于图像边缘的深度与法线正则化策略用于稀视图NeRF，非边缘区域约束深度/法线，边缘区域保留细节，实验显示在LLFF/DTU上能更好保持边界并抑制伪影；模块可插拔使用。


<details>
  <summary>Details</summary>
Motivation: 现有在稀视图下的NeRF通过全局深度正则化降低伪影，但会模糊几何边界。利用边缘先验有望在保持边界清晰的同时抑制伪影。

Method: 先从输入图像提取边缘（基于强度/梯度或现有边缘检测器），然后在非边缘区域施加深度与法线正则化约束以增强几何一致性，同时在边缘区域放宽或不施加这些正则化以保留高频几何细节。该边缘引导模块可作为plug-and-play组件整合到其他NeRF变体中。

Result: 在LLFF和DTU数据集上，EdgeNeRF在保持尖锐几何边界与抑制伪影方面表现优异，并能显著提升其他方法的效果且训练时间开销有限。

Conclusion: EdgeNeRF通过边缘引导的深度与法线正则化，在稀视图条件下有效减少了几何伪影并保留边界细节，优于现有全局深度约束方法。

Abstract: Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.

</details>


### [70] [In defense of the two-stage framework for open-set domain adaptive semantic segmentation](https://arxiv.org/abs/2601.01439)
*Wenqi Ren,Weijie Wang,Meng Zheng,Ziyan Wu,Yang Tang,Zhun Zhong,Nicu Sebe*

Main category: cs.CV

TL;DR: Proposes SATS: separate known/unknown then adapt with unknown-aware DA and hard unknown augmentation, yielding significant H-Score gains on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: OSDA-SS needs to adapt known classes across domains while detecting unknown classes; unified-stage methods suffer from annotation imbalance causing negative transfer and underfitting of unknowns.

Method: Two sequential steps: (1) known/unknown separation to produce accurate unknowns, (2) unknown-aware domain adaptation; also introduces hard unknown exploration data augmentation to expose model to challenging unknowns.

Result: SATS, a two-step Separating-then-Adapting Training Strategy, first separates known/unknown then performs unknown-aware domain adaptation, plus hard unknown exploration augmentation; achieves +3.85% and +18.64% H-Score improvements on benchmarks.

Conclusion: Two-stage strategy with hard unknown exploration yields better balance between known and unknown learning, improving OSDA-SS performance significantly.

Abstract: Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.

</details>


### [71] [PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations](https://arxiv.org/abs/2601.01454)
*Xiao Li,Zilong Liu,Yining Liu,Zhuhong Li,Na Dong,Sitian Qin,Xiaolin Hu*

Main category: cs.CV

TL;DR: 构建覆盖ImageNet-1K的细粒度部件标注数据集PIN++（100K图），并提出利用伪标签的多尺度部件监督模型（MPM），在分类、分割和少样本任务上显著提升表现，展示部件标注对下游任务的价值。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中缺乏覆盖全面且高质量的部件标注，限制了基于部件的研究与更鲁棒的视觉识别模型发展，因此构建一个覆盖ImageNet-1K的细粒度部件标注集并研究如何利用这些标注以提升下游任务性能。

Method: 1) 构建PIN++：为ImageNet-1K每类收集并手工标注100张图的部件掩码；2) 用这些标注训练部件分割网络，推理无标注图像以生成伪部件标签；3) 设计MPM：在传统分类网络中加入多尺度辅助旁路层（bypass layers），用伪标签和真实部件标注对旁路层进行联合监督，提升特征的部件感知；4) 在分割、目标分割、少样本学习等任务上进行广泛实验并对比分析。

Result: 建立了PartImageNet++（PIN++）数据集，为ImageNet-1K所有类别提供细粒度零件标注，共100K张图像（每类100张）；提出多尺度部件监督识别模型（MPM），先用PIN++训练部件分割网络生成其余图像的伪标签，再将传统识别网络与辅助旁路层结合，采用伪标签和原始标注联合监督；在分割、目标分割、少样本学习等下游任务上进行了广泛实验，结果显示提升了基于部件的鲁棒识别并为多项任务建立了强基线。

Conclusion: PIN++大幅扩展了可用的部件标注资源，通过MPM和伪标签策略可有效提升图像识别与分割等任务的性能，证明部件级监督在下游任务中具有重要价值并提供了强基线。

Abstract: To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.

</details>


### [72] [Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration](https://arxiv.org/abs/2601.01456)
*Wentao Bian,Fenglei Xu*

Main category: cs.CV

TL;DR: DA-FSS decouples semantic and geometric paths, uses arbitration modules to avoid CLIP confusion and balance plasticity/stability, improving few-shot 3D point cloud segmentation


<details>
  <summary>Details</summary>
Motivation: Address conflicts in multimodal FS-PCS, specifically Plasticity-Stability Dilemma and CLIP confusion

Method: Paper analysis

Result: Proposed DA-FSS with decoupled semantic and geometric experts, Parallel Expert Refinement, SAM, DAM; outperforms MM-FSS on S3DIS and ScanNet

Conclusion: DA-FSS effectively leverages multimodal info, reduces semantic confusion, and yields better boundaries, completeness, texture differentiation; validated on S3DIS and ScanNet

Abstract: In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.

</details>


### [73] [Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation](https://arxiv.org/abs/2601.01457)
*Mingxing Zhan,Li Zhang,Beibei Wang,Yingjie Wang,Zenglin Shi*

Main category: cs.CV

TL;DR: Calibrate frozen relative-depth models to metric depth by predicting an uncertainty envelope from captions and selecting image-specific affine inverse-depth transforms using frozen visual features, supervised by a least-squares oracle; yields better in-domain and zero-shot metric depth.


<details>
  <summary>Details</summary>
Motivation: Monocular metric depth is ill-posed due to global scale ambiguity and domain-shift sensitivity; captions give noisy but useful scale cues, so combining language and frozen visual features can calibrate relative-depth models to metric depth without finetuning the backbone.

Method: Freeze relative-depth backbone and CLIP text encoder; predict an uncertainty-aware envelope of affine calibration parameters in inverse depth from captions; select image-specific calibration within envelope using pooled multi-scale frozen visual features; supervise with closed-form least-squares oracle in inverse depth; train only calibration heads.

Result: Improved in-domain accuracy on NYUv2 and KITTI, and better zero-shot transfer to SUN-RGBD and DDAD compared to language-only baselines.

Conclusion: The paper proposes recovering metric depth from relative-depth foundation models by learning image-specific affine transforms in inverse depth via lightweight calibration heads, using language to predict an uncertainty-aware envelope of feasible calibrations and selecting a calibration using frozen visual features, improving in-domain and zero-shot transfer performance.

Abstract: Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.

</details>


### [74] [Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network](https://arxiv.org/abs/2601.01460)
*Mohd Usama,Belal Ahmad,Christer Gronlund,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: 提出一种基于GAN的图像到图像翻译方法进行超声图像域自适应，通过修改纹理和去除混响噪声使源域图像与目标域一致，在两组颈动脉数据集上表现优于无自适应且优于CycleGAN比较结果。


<details>
  <summary>Details</summary>
Motivation: Different ultrasound devices/settings produce images with varying textures and reverberation noise, breaking training-test distribution assumption and hurting model performance; retraining per device is costly.

Method: GAN-based domain adaptation for ultrasound

Result: Proposed GAN translates test images' texture and removes reverberation noise to match target domain while preserving content; applied to two carotid ultrasound datasets from three domains; outperforms no adaptation and compared with CycleGAN.

Conclusion: 模型能有效改变纹理并去噪，实现域对齐，提升直方图相似性与降低Bhattacharyya距离，相比无适应获得更好统计指标。

Abstract: Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.

</details>


### [75] [Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm](https://arxiv.org/abs/2601.01481)
*Mohammad Hassan Saghafi,Seyed Majid Noorhosseini,Seyed Abolfazl Seyed Javadein,Hadi Khalili*

Main category: cs.CV

TL;DR: 改进ViBe背景建模＋回波消除方法，实现对海岸视频中船舶的鲁棒、实时检测与跟踪


<details>
  <summary>Details</summary>
Motivation: 提高海岸视频序列中船舶检测与跟踪在复杂动态场景下的鲁棒性和实时性

Method: 在ViBe基础上修改背景更新与像素更新策略以减少目标丢失；增加基于几何属性的目标筛选与利用亮度失真（brightness distortion）判断并抑制回波区域；综合这些模块构成检测与跟踪流水线，保证实时处理能力。

Result: 提出改进的ViBe背景建模用于移动目标检测，结合基于几何特性与亮度失真概念的回波（backwash）消除方法，实验显示在实时性与精度上性能优异

Conclusion: 改进后的ViBe降低了船舶漏检率，对海浪与光照变化具有鲁棒性，并能快速更新背景；结合几何与亮度失真策略的回波消除有效减少误检，整体方法在实时检测与跟踪上表现优良。

Abstract: In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.

</details>


### [76] [Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization](https://arxiv.org/abs/2601.01483)
*Xinyu Qiu,Heng Jia,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: ADPO jointly trains generation and verification with a preference verification reward and decoupled advantage optimization, improving verification AUC, reducing inference time, and boosting task accuracies.


<details>
  <summary>Details</summary>
Motivation: Parallel test-time scaling with separate generation and verification models is costly; unify both capabilities into one efficient policy to cut cost and improve alignment between answers and verification.

Method: Introduce preference verification reward using mean verification scores of positive/negative samples as threshold; compute separate advantages for generation and verification, apply token masks to isolate gradients, and combine masked GRPO objectives for joint optimization.

Result: ADPO is a unified RL framework combining generation and self-verification in one policy, reducing training/inference cost and improving verification calibration.

Conclusion: ADPO effectively aligns verification with answer correctness while preserving generation quality, yielding strong empirical gains across reasoning and control benchmarks.

Abstract: Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.

</details>


### [77] [Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease](https://arxiv.org/abs/2601.01485)
*Zobia Batool,Diala Lteif,Vijaya B. Kolachalama,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 提出一种名为Extended MixStyle（EM）的算法，通过混合特征的高阶矩（偏度和峰度）来模拟不同域间的分布变化，从而提升基于sMRI的阿尔茨海默病（AD）分类模型的单域泛化性能。在NACC数据集上训练并在三个未见队列上测试，平均macro-F1提高约2.4个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有sMRI上用于AD分类的深度学习模型在跨队列应用中表现下降显著，原因是扫描器、协议和样本特征差异导致的域偏移。单域泛化在临床现实中尤为重要，因为数据通常碎片化且难以获得多域训练集。

Method: 在特征层面计算并混合均值、方差及高阶矩（偏度、峰度），作为训练时的数据增强/域自适应策略。EM在sMRI卷积/注意力模型中插入作为正则化模块，通过在训练批次内随机配对并线性混合这些统计量来生成伪域分布。

Result: 在NACC（n=4,647）训练，测试于三个未见队列（总n=3,126），EM相比最先进的单域泛化基线平均提高macro-F1约2.4个百分点，显示出更稳定的跨域分类性能。

Conclusion: EM通过扩展MixStyle到高阶统计量，有效缓解了扫描仪、协议和人口学差异导致的域变化，从而提高了模型在未见队列上的鲁棒性和泛化能力，可推动更可靠的AD临床筛查。

Abstract: Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.

</details>


### [78] [DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion](https://arxiv.org/abs/2601.01487)
*Ziyue Zhang,Luxi Lin,Xiaolin Hu,Chao Chang,HuaiXi Wang,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: DeepInv: first trainable stepwise inversion solver using self-supervised pseudo-noise generation and iterative multi-scale training; much better SSIM and speed.


<details>
  <summary>Details</summary>
Motivation: current inversion lacks supervision, approximations hurt performance/efficiency

Method: self-supervised trainable inversion solver

Result: trainable solver DeepInv produces high-quality pseudo-noises via self-supervised objective and augmentation; iterative multi-scale training yields fast accurate mapping, large SSIM and speed gains over EasyInv/ReNoise

Conclusion: trainable, self-supervised inversion solver improves accuracy and efficiency for diffusion inversion; provides community insights; code to be released.

Abstract: Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.

</details>


### [79] [DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation](https://arxiv.org/abs/2601.01507)
*Tao Li,Qing Li,Na Li,Hui Xie*

Main category: cs.CV

TL;DR: 该论文提出DiffKD-DCIS框架，结合条件扩散模型用于超声图像增强和教师-学生知识蒸馏以提升DCIS升级为IDC的预测性能。通过三阶段流程（条件扩散生成、教师网络特征提取、学生网络蒸馏），在多中心1435例数据集上验证，生成图像质量良好，学生网络参数更少、推理更快，在外部测试集上优于部分组合模型，预测准确度与资深放射医师相当、优于初级医师，具有临床潜力。


<details>
  <summary>Details</summary>
Motivation: 超声数据量有限且跨中心分布差异导致传统深度学习模型泛化性差，迫切需要数据增强与模型压缩结合的方法来提高DCIS升级预测的性能与临床可用性。

Method: 三阶段方法：1) 条件扩散模型以多模态条件（可能包括病理、临床和影像特征）生成高保真合成超声图像进行数据扩增；2) 使用大型教师网络在原始与合成数据上学习鲁棒特征表示；3) 通过知识蒸馏将教师知识迁移到参数更少的学生网络，兼顾泛化能力与计算效率。

Result: 在多中心1435例数据集上，生成图像质量良好；学生网络参数更少、推理更快；在外部测试集上整体表现优于仅使用部分模块的组合，准确率可与资深放射医师相比，优于初级放射医师。

Conclusion: DiffKD-DCIS能在有限超声数据下通过条件扩散增强和知识蒸馏提升模型泛化性，同时保持轻量化和高效推理，达到或接近资深放射科医师的预测水平，适合实际临床部署。

Abstract: Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.
  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.
  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.

</details>


### [80] [A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI](https://arxiv.org/abs/2601.01512)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.CV

TL;DR: GBU-Net, a group-batch-normalized U-Net variant, enhances contextual feature capture for left ventricle segmentation in cine MRI, achieving state-of-the-art performance (97% dice with ensemble).


<details>
  <summary>Details</summary>
Motivation: Improve accuracy of left ventricle segmentation in cine MRI by leveraging group normalization within a U-Net architecture to better capture contextual information needed for cardiac MRI.

Method: Modified U-Net with group/batch normalization, tailored down-sampling and up-sampling pathways, contextual enhancement techniques; trained/tested on 805 scans from 45 patients; evaluated with dice coefficient and mean perpendicular distance; ensemble used for final performance.

Result: GBU-Net significantly improves segmentation accuracy, outperforming existing methods; ensemble achieves 97% dice on SunnyBrook dataset; better dice and mean perpendicular distance metrics.

Conclusion: GBU-Net provides enhanced precision and contextual understanding, outperforming standard CNN-based segmentation for left ventricle, useful for surgical robotics and medical analysis.

Abstract: This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.

</details>


### [81] [FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01513)
*Gen Li,Peiyu Liu*

Main category: cs.CV

TL;DR: 提出VideoSpeculateRAG：在VLM-RAG任务中用轻量“草稿模型”生成候选答案，再由重模型验证，结合相似性过滤检索知识中的实体，达到约2倍加速且保持或提升准确性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在视觉推理中难以整合外部知识，传统RAG方法效率低且易损失答案质量，需一种既高效又可靠的检索增强生成策略。

Method: 两大技术：1) 预测性解码管线——轻量草稿模型快速生成多个答案候选，重型号验证并精炼，减少推理时间；2) 相似性基的过滤策略——对检索到的知识进行实体相似性筛选，改善实体对齐，降低检索噪声对最终答案的负面影响。

Result: 实验表明与标准RAG相比，VideoSpeculateRAG在保持或提升准确性的同时，推理速度约加速2倍。

Conclusion: VideoSpeculateRAG在效率与准确性间取得平衡，通过预测性解码与实体相似性过滤显著降低延迟并提升答案质量，是面向复杂多模态知识推理的有效框架。

Abstract: Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.

</details>


### [82] [BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding](https://arxiv.org/abs/2601.01526)
*Hongbing Li,Linhui Xiao,Zihan Zhao,Qi Shen,Yixiang Huang,Bo Xiao,Zhanyu Ma*

Main category: cs.CV

TL;DR: BARE reduces modality bias and strengthens referential reasoning in one-tower visual grounding via three modules, improving accuracy and efficiency across benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in one-tower visual grounding models, specifically over-entangled multimodal representations causing modality biases and weak semantic reasoning for referential cues.

Method: One-tower framework that preserves modality-specific features and constructs referential semantics using: (i) language salience modulator to highlight important text cues; (ii) visual bias correction to remove deceptive visual biases; (iii) referential relationship enhancement to model relationships; integrated training and evaluation on five benchmarks.

Result: Proposes BARE: preserves modality-specific features and builds referential semantics via three modules—language salience modulator, visual bias correction, referential relationship enhancement—achieving SOTA and better efficiency on five benchmarks; code released.

Conclusion: BARE effectively mitigates multimodal distractions and enhances referential comprehension, leading to state-of-the-art performance and improved computational efficiency.

Abstract: Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.

</details>


### [83] [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528)
*Yang Zhou,Hao Shao,Letian Wang,Zhuofan Zong,Hongsheng Li,Steven L. Waslander*

Main category: cs.CV

TL;DR: 提出 DrivingGen：一个多样化数据集 + 新的多维度指标，用于全面评估生成式驾驶世界模型，发现通用与驾驶专用模型的关键权衡。


<details>
  <summary>Details</summary>
Motivation: 现有评估缺乏与驾驶安全相关的量化标准，且数据集不够多样，无法有效促进行业可部署世界模型的发展，因此需要一个更全面的基准。

Method: 收集多源多样化视频数据；设计视觉真实度、轨迹合理性、时间一致性、可控性等评估指标；在 14 个模型上统一评测并分析其性能与缺陷。

Result: DrivingGen 构建了一个用于评估生成式驾驶世界模型的全面基准。通过从现有驾驶数据集和互联网视频中收集多样化评估集，并设计联合评估视觉真实感、轨迹合理性、时间一致性和可控性的新指标，DrivingGen 能更系统地衡量模型在自动驾驶场景下的表现。基准评测 14 个最先进模型，揭示了通用视频模型与专用驾驶模型之间的权衡：前者视觉效果更好但物理一致性差，后者运动捕捉更真实但视觉质量较低。

Conclusion: DrivingGen 为生成式驾驶世界模型提供了统一且更贴近实际需求的评估框架，有助于推动可控、可靠、可部署的驾驶世界模型研究与应用。

Abstract: Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.

</details>


### [84] [Improving Flexible Image Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.01535)
*Zixuan Fu,Lanqing Guo,Chong Wang,Binbin Song,Ding Liu,Bihan Wen*

Main category: cs.CV

TL;DR: ReToK通过在训练时增加尾部token激活频率并用分层语义正则化保持前端token语义一致性，解决了传统尾截断策略导致的信息过度集中问题，提升了256×256 ImageNet上的生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统嵌套丢弃在训练时通过截断尾部token使得前端token承担大部分信息，随着token长度增加，后续token参与度低，限制了自回归生成模型的效果，因而需要一种能均衡利用所有token的分词器。

Method: 1) 冗余token填充（Redundant Token Padding）：故意激活并训练尾部token，防止信息都集中在前端。2) 分层语义正则化（Hierarchical Semantic Regularization）：用预训练视觉模型的解码特征对前端token进行强正则，向尾部逐步减弱以保留低级细节。3) 结合上述策略训练可变长度tokenizer并用于AR生成。

Result: ReToK提出了一种改进的可变长度图像分词器，通过冗余token填充和分层语义正则化，缓解了嵌套丢弃导致的信息集中问题，从而提高了AR图像生成的性能。

Conclusion: ReToK能更充分利用所有token进行潜在建模，相比现有可变和定长分词器，在图像生成任务上表现更佳，尤其在生成质量和token使用均衡上有显著改进。

Abstract: Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}

</details>


### [85] [FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01537)
*Gong Gao,Zekai Wang,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: 提出FAR-AMTN：通过共享权重的组特定注意力、跨组特征融合与动态加权实现更少参数下更高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统多任务网络高层独立导致参数爆炸并限制高层特征交互，阻碍属性语义关系建模，影响泛化性能；需在低复杂度下增强组间信息共享与交互。

Method: 引入WSGSA模块共享参数以减少复杂度并提升组特征，CGFF促进组间交互，DWS实现任务同步收敛；实验验证。

Result: FAR-AMTN proposes WSGSA, CGFF, DWS to improve FAR MTN generalization with fewer parameters.

Conclusion: FAR-AMTN在CelebA与LFWA上优于现有方法，参数显著减少且准确率提高。

Abstract: To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.

</details>


### [86] [EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding](https://arxiv.org/abs/2601.01547)
*Tianjun Gu,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: 论文提出TSI范式和EscherVerse基准，首次在大规模真实视频上系统评估物理动态与人类意图联合推理，促进面向目的的空间智能研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了人类意图在空间变化中的作用，且多集中于受限场景；作者希望推动从被动场景描述向具有目的性的人类中心空间理解转变。

Method: 通过构建EscherVerse，包括Escher-Bench、Escher-35k数据集和Escher系列模型，从真实视频中提取场景，设计评测任务来考察物体永久性、状态转变、轨迹预测及意图推断能力，并提出了新的数据整理流程。

Result: 构建了首个系统性评估意图驱动推理的开源基准和数据集，提供评测套件与模型实现，为研究社区提供基础资源，推动相关研究。

Conclusion: 该论文提出了Teleo-Spatial Intelligence (TSI)范式，强调结合物理动态推理与意图驱动推理以提升空间智能。

Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.

</details>


### [87] [Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation](https://arxiv.org/abs/2601.01593)
*Haonan Cai,Yuxuan Luo,Zhouhui Lian*

Main category: cs.CV

TL;DR: 提出GAR-Font：基于自回归的多模态少样本字体生成框架，包含全局感知分词器、轻量语言风格适配器和后处理精修，能更好保持全局风格与结构并支持文本引导。


<details>
  <summary>Details</summary>
Motivation: 传统FFG在仅依赖图像参考时难以保留全局风格和结构，自回归方法受限于局部块级分词无法建模全局依赖；同时忽略文本在传达设计意图中的作用。

Method: 使用全局感知分词器捕捉局部与全局依赖，设计多模态风格编码器结合视觉与文本风格输入（通过轻量语言-风格适配器），并在生成后加入精修模块以提升细节与一致性，整体在自回归框架下训练和推理。

Result: GAR-Font introduces a global-aware tokenizer, a multimodal style encoder with a language-style adapter, and a post-refinement pipeline to improve few-shot font generation. It outperforms prior methods in preserving global style and structural fidelity, enables textual style control, and avoids heavy multimodal pretraining.

Conclusion: GAR-Font有效提升了FFG在全局风格一致性和结构保真度方面的表现，同时通过轻量级语言适配器实现了可控的文本风格指导，无需大量多模态预训练。

Abstract: Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.

</details>


### [88] [Guiding Token-Sparse Diffusion Models](https://arxiv.org/abs/2601.01608)
*Felix Krause,Stefan Andreas Baumann,Johannes Schusterbauer,Olga Grebenkova,Ming Gui,Vincent Tao Hu,Björn Ommer*

Main category: cs.CV

TL;DR: Sparse Guidance applies token-level sparsity during inference to restore classifier-free guidance effectiveness in sparsely trained diffusion models, yielding better fidelity and large FLOP savings.


<details>
  <summary>Details</summary>
Motivation: Sparse training reduces compute but harms classifier-free guidance response at inference; need method to retain high-variance conditional predictions and good CFG behavior while using sparsity to save compute.

Method: Token-level sparse guidance for diffusion models

Result: Proposed Sparse Guidance (SG) uses token-level sparsity instead of conditional dropout to preserve high-variance conditional prediction; achieves 1.58 FID on ImageNet-256 with 25% fewer FLOPs and up to 58% FLOP savings at matched quality; in 2.5B text-to-image model SG improves composition, human preference and throughput.

Conclusion: SG enables sparsely trained diffusion models to retain effective CFG-driven sampling, improving image quality, composition and efficiency; it allows substantial inference compute reductions while maintaining or improving fidelity and user preference.

Abstract: Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.

</details>


### [89] [CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment](https://arxiv.org/abs/2601.01613)
*Kazi Ramisa Rifa,Jie Zhang,Abdullah Imran*

Main category: cs.CV

TL;DR: CAP-IQA通过提示与因果去偏将医学先验与实例上下文结合，提升了CT图像质量评估的准确性与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法能引入医学先验但常带来理想化偏差，难以应对噪声、运动伪影和扫描器差异等真实退化，需将通用先验与实例级信息结合并去偏。

Method: 提出CAP-IQA框架：使用CNN视觉编码器与领域文本编码器，融合放射学风格提示与上下文提示，通过因果去偏机制将理想化知识与图像特有退化分离；简化为编码器-only结构，并做提示引导的特征融合与联合训练。

Result: 在2023 LDCTIQA挑战基准上，CAP-IQA总体相关性得分为2.8590，较榜首2.7427提升4.24%；在9.15万张儿科CT的内部数据上也展示了良好泛化；消融实验表明提示引导融合与简化编码器设计共同提升了特征对齐与可解释性。

Conclusion: CAP-IQA有效地将文本先验与实例上下文结合，通过因果去偏实现对CT图像质量评估的改进，能够在公开基准和大规模门诊数据上均表现出较好泛化性。

Abstract: Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.

</details>


### [90] [An Empirical Study of Monocular Human Body Measurement Under Weak Calibration](https://arxiv.org/abs/2601.01639)
*Gaurav Sekar*

Main category: cs.CV

TL;DR: 对三种弱校准单目人体测量策略（基于关键点的几何、基于姿态的回归、基于物体校准的轮廓）在半受限消费级相机条件下的系统性实证比较；重点分析校准假设如何影响测量稳定性、鲁棒性与失败模式；发现用户校准努力与围度稳定性之间存在明显权衡，为轻量级单目人体测量系统设计提供实证参考。


<details>
  <summary>Details</summary>
Motivation: 单目RGB设备（手机、笔记本）普及但缺乏深度信息，现有方法在实际消费级场景下受限。研究目标是系统化比较不同弱校准策略，揭示它们在真实可部署设置中的行为，帮助设计低成本、高可用性的测量系统。

Method: 在半受限设置下使用消费级单目相机采集多视角/多体型数据，比较三类方法：1) 基于关键点的几何重建与尺寸估计；2) 基于人体姿态参数回归到围度的学习模型；3) 利用场景中已知尺寸物体对轮廓进行尺度校准并测量围度。通过对噪声、视角、遮挡与体型变化的消融实验，评估各方法的鲁棒性与失败模式。

Result: 实验表明：物体校准轮廓方法在有可靠场景参考时对围度估计最稳定；姿态回归在受限训练分布内表现平衡且对关键点噪声容忍度较高；关键点几何方法易受视角与关键点定位误差影响，围度波动最大。总体存在用户校准努力与测量稳定性之间的清晰权衡。

Conclusion: 不同弱校准策略在测量精度与稳健性上存在权衡：关键点几何方法受视角与关键点噪声敏感但实现成本低；姿态驱动回归在训练数据匹配时可获得更稳定的围度估计但受泛化限制；物体校准轮廓方法在有明确尺度参考时围度稳定性最好但对遮挡和背景复杂度敏感。总体上，增加用户校准努力能提高稳定性，但会增加使用门槛。

Abstract: Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.

</details>


### [91] [Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows](https://arxiv.org/abs/2601.01660)
*Aymen Mir,Riza Alp Guler,Jian Wang,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 提出面向3D Gaussian Splatting的DGSM与SH重光照方法，在不网格化的情况下实现实时一致阴影与重光照，适用于动态角色与场景交互。


<details>
  <summary>Details</summary>
Motivation: 目标是在3DGS表示下实现与场景或插入对象交互时的光照与阴影一致性，解决传统方法需要网格化或离线优化的问题，使实时渲染中的虚拟角色与场景光照一致。

Method: 提出DGSM，将经典深度阴影映射思想推广至体表示，利用3DGS沿光线的闭式光累积计算体积透射率；对每个光源在同心径向壳上表格化透射率并存储于八面体纹理图集中，GPU可实时采样以衰减受影响的高斯元，从而实现体积阴影。此外，用HDRI探针用球面谐波（SH）表示近场环境照明，并对每个高斯进行快速辐射传输近似以重光照，避免显式BRDF估计或离线优化。

Result: 在AvatarX与ActorsHQ的动态角色、以及ScanNet++、DL3DV和SuperSplat的场景合成实验中，DGSM与SH重光照在单人和多人场景中均能在体表示内生成连贯的阴影与光照效果，支持场景中对象的交互而无需网格化。

Conclusion: 本文提出了针对3D Gaussian Splatting（3DGS）体表示的实时一致光照与阴影方法，通过Deep Gaussian Shadow Maps（DGSM）在不进行显式网格化的前提下，实现在动态交互场景中投射与接收一致阴影并实现角色重光照。

Abstract: We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.

</details>


### [92] [LabelAny3D: Label Any Object 3D in the Wild](https://arxiv.org/abs/2601.01676)
*Jin Yao,Radowan Mahmud Redoy,Sebastian Elbaum,Matthew B. Dwyer,Zezhou Cheng*

Main category: cs.CV

TL;DR: 提出LabelAny3D，用analysis-by-synthesis从单张图像重建3D场景以自动生成3D边框标注，并发布COCO3D开放词汇单目3D检测基准；生成的标注提升检测性能并优于先前自动标注方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测在室内和自动驾驶上进展显著，但缺乏覆盖自然场景与开放词汇的大规模3D标注数据，人工3D标注成本高、困难大；因此需要自动化、高质量的标注方法以扩展3D检测到in-the-wild图像与更多类别。

Method: 通过analysis-by-synthesis框架，先用基础模型/重建模块从单张图像恢复场景的整体3D表示（可能包括实例分割、尺度/深度估计、语义提示、形状/姿态重建等），再通过渲染或优化生成一致的3D边界框作为标注。将该自动标注应用于MS-COCO产生COCO3D并用于训练和评估开域单目3D检测器。

Result: LabelAny3D提出一种基于analysis-by-synthesis的管线，通过从单张2D图像重建整体3D场景来自动生成高质量的3D边界框标注，并基于此构建了COCO3D数据集，用于开放词汇的单目3D检测。实验显示，LabelAny3D生成的标注可以提升单目3D检测在多基准上的表现，优于已有的自动标注方法，表明基于大模型的标注有望推动真实开放世界场景的3D识别扩展。

Conclusion: LabelAny3D能高质量自动生成三维边界框标注，推动开放词汇单目3D检测的发展；基于该方法构建的COCO3D丰富了3D检测的类别覆盖，标注对改善模型性能有实际效果。

Abstract: Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.

</details>


### [93] [Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada](https://arxiv.org/abs/2601.01677)
*Zhengsen Xu,Lanying Wang,Sibo Cheng,Xue Rui,Kyle Gao,Yimin Zhu,Mabel Heffring,Zack Dewis,Saeid Taleghanidoozdoozan,Megan Greenwood,Motasem Alkayid,Quinn Ledingham,Hongjie He,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出一种可置信赖的长序列多尺度时序模型，结合不确定性量化与SHAP解释，在2023-2024年加拿大西部火险预测上显著优于现有方法（F1 0.90，PR-AUC 0.98），并揭示温度为主的驱动机制及2024年水分限制的增强作用


<details>
  <summary>Details</summary>
Motivation: Wildfire risk prediction is difficult due to stochastic ignition/spread and nonlinear interactions among factors; purely data-driven models lack reliability and interpretability

Method: Long-sequence, multi-scale temporal modeling integrating heterogeneous drivers with uncertainty quantification and SHAP interpretation

Result: Model outperforms existing time-series approaches on western Canada 2023-2024, achieving F1 0.90 and PR-AUC 0.98; shows spatial/seasonal uncertainty patterns; SHAP indicates temperature drivers dominate, moisture constraints more important in 2024

Conclusion: 该框架在准确性、效率、不确定性表征和过程解释性方面均表现良好，为火险预测与管理提供了有力数据驱动工具；代码开源

Abstract: In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.

</details>


### [94] [Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages](https://arxiv.org/abs/2601.01680)
*Afzal Hossain,Mst Rumana Sumi,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 见下文TL;DR节


<details>
  <summary>Details</summary>
Motivation: 见下文动机节

Method: 见下文方法节

Result: 见下文结果节

Conclusion: 见下文结论节

Abstract: Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.

</details>


### [95] [FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation](https://arxiv.org/abs/2601.01687)
*Abdur R. Fayjie,Pankhi Kashyap,Jutika Borah,Patrick Vandewalle*

Main category: cs.CV

TL;DR: FALCON：一种基于2D切片的跨域少样本分割框架，元训练+对抗性微调+边界感知学习，能以更少标注和更低计算成本实现更精确的边界分割。


<details>
  <summary>Details</summary>
Motivation: 3D医学分割受限于注释稀缺、隐私、病人间差异和高计算成本，需一种能用少量标签并降低计算开销的高精度方法。

Method: 先在自然图像上进行元训练以学习可泛化的分割先验，再通过对抗性微调和边界感知学习将模型迁移到医学域。采用任务感知推理，基于支持样例动态适应患者特异性解剖差异。处理方式以2D切片为主，减少计算开销。

Result: 在四个基准数据集上，FALCON在Hausdorff Distance上持续最低，表明边界更精确；Dice得分与现有最优方法可比。同时在标注量、数据增强使用和计算开销上有显著节省。

Conclusion: FALCON通过2D切片处理和跨域少样本学习实现了高精度3D医学图像分割，特别在边界精度（Hausdorff Distance）上优于现有方法，同时保持与SOTA相当的Dice得分。

Abstract: Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.

</details>


### [96] [Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data](https://arxiv.org/abs/2601.01689)
*Afzal Hossain,Stephanie Schuckers*

Main category: cs.CV

TL;DR: 在Young Face Aging数据集上，向训练集中加入经过过滤的StyleGAN2生成的人脸样本用于对MagFace微调，可明显降低6–36个月跨度内的验证错误率，相较于预训练及仅用真实数据微调均有提升。


<details>
  <summary>Details</summary>
Motivation: 儿童面部随年龄快速且非线性变化导致模板漂移与随时间增加的识别错误，探索合成数据是否能作为长期稳定器以提升模型对时间变化的鲁棒性。

Method: 使用身份不重叠协议在YFA数据集上比较三种设置：不微调的MagFace；仅用真实人脸微调；用真实+过滤后的StyleGAN2生成样本微调。生成样本仅用于训练同一身份，且做后处理过滤以减少身份泄露和伪影样本。评估指标为6–36个月的注册-验证间隔误差率。

Result: Synthetic augmentation improves temporal robustness in child face recognition

Conclusion: 经过身份隔离协议与生成样本过滤后，合成数据增强的微调策略能提高儿童人脸识别模型的时间稳健性，但仍需权衡身份泄露与伪影风险。

Abstract: Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.

</details>


### [97] [Learnability-Driven Submodular Optimization for Active Roadside 3D Detection](https://arxiv.org/abs/2601.01695)
*Ruiyu Mao,Baoming Zhang,Nicholas Ruozzi,Yunhui Guo*

Main category: cs.CV

TL;DR: 提出一种以可学习性为驱动的主动学习方法LH3D，用于道路侧单目3D检测，通过避免本质不可判定样本来节省标注预算，在低标注比例下仍能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 道路侧数据在实际部署时常常只能获取道路侧单目数据，缺乏车端配对信息，导致许多样本对3D属性存在本质歧义，人工标注成本高且难以获得可靠标签。为减少在这些不可判定样本上的浪费，需要按可学习性选择样本。

Method: LH3D通过学习可判定性度量（learnability）来筛选样本，结合信息量和可标注性两方面的评分，抑制远距、模糊或遮挡导致的不可判定样本，同时保证覆盖性；在DAIR-V2X-I数据集上与不确定性驱动的主动学习基线比较。

Result: 在DAIR-V2X-I数据集上，使用25%标注预算时，LH3D分别达到车辆86.06%、行人67.32%、骑行者78.67%的全量性能，显著优于基于不确定性的基线方法。

Conclusion: 本文提出了面向道路侧单目3D目标检测的主动学习框架LH3D，通过评估场景“可学习性”而非仅依赖不确定性来选择标注样本，从而避免对本质不可判定（inherently ambiguous）的样本浪费标注预算。

Abstract: Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.

</details>


### [98] [Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems](https://arxiv.org/abs/2601.01696)
*Yian Liu,Xiong Wang,Ping Xu,Lei Zhu,Ming Yan,Linyun Xue*

Main category: cs.CV

TL;DR: 提出CDO训练时模块，通过对齐特征与标签的协方差分布提升车道检测精度，适用于分割/锚点/曲线方法，跨6模型和3数据集测试，准确率提升0.01%~1.5%，无需增加推理计算。


<details>
  <summary>Details</summary>
Motivation: Improve lane detection accuracy on low-power embedded systems without increasing runtime complexity by optimizing feature distributions using training-time module compatible with various model types.

Method: Covariance Distribution Optimization (CDO) module; applied as plug-in during training, aligns feature distributions to labels via covariance/mean matching; lightweight, no extra inference compute.

Result: Applied CDO to 6 models (2 real-time, 4 SOTA) across CULane, TuSimple, LLAMAS; accuracy gains 0.01%–1.5%; no extra inference cost; easy integration with existing parameters; benefits for power efficiency and flexibility.

Conclusion: CDO是一个轻量级、易集成的训练时优化模块，通过调整特征分布以接近真实标签分布，能够在嵌入式系统上提升车道检测准确性且不增加运行开销，适合实时、低功耗应用。

Abstract: Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.

</details>


### [99] [FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing](https://arxiv.org/abs/2601.01720)
*Xijie Huang,Chengming Xu,Donghao Luo,Xiaobin Hu,Peng Tang,Xu Peng,Jiangning Zhang,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出通过数据和模型改进实现无需运行时引导的First-Frame Propagation（FFP）视频编辑：构建了大规模高质量数据集FFP-300K（300K对、720p、81帧），并提出Adaptive Spatio-Temporal RoPE（AST-RoPE）与自蒸馏的训练目标，平衡首帧外观保持与源视频运动保留。实验在EditVerseBench上显示相较于现有方法在PickScore和VLM得分上分别提高约0.2和0.3。


<details>
  <summary>Details</summary>
Motivation: 现有FFP方法依赖运行时复杂引导，主要源于训练数据集短、低分辨率、任务多样性不足，无法学习稳健的时间先验；为实现无引导的高质量视频编辑需补齐数据和方法上的缺陷。

Method: （1）构建FFP-300K：两轨管线生成多样的局部与全局编辑视频对，720p、81帧，300K对；（2）模型架构：提出AST-RoPE，动态重映射位置编码以解耦外观与运动参考；（3）训练目标：引入自蒸馏，使用身份传播任务作为正则以保证长期时间一致性并防止语义漂移。

Result: 在EditVerseBench上，比现有学术与商业模型提升约0.2 PickScore和0.3 VLM得分，表明在保持首帧外观与源视频运动方面取得显著改进。

Conclusion: 通过高质量数据集和架构+目标级改进，方法实现了真正的无引导FFP并显著优于现有学术与商业模型，提升了外观一致性与时间稳定性。

Abstract: First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.

</details>


### [100] [Point-SRA: Self-Representation Alignment for 3D Representation Learning](https://arxiv.org/abs/2601.01746)
*Lintong Wei,Jian Lu,Haozhe Cheng,Jihua Zhu,Kaibing Zhang*

Main category: cs.CV

TL;DR: Point-SRA通过双重自表示对齐（MAE层与MFT层）、MeanFlow Transformer的条件概率重建及流条件微调，显著提升点云重建、分类、分割与检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法使用固定掩码比率与点到点重建假设，忽视多层表示间的互补性与点云多样性，導致重建目标与真实分布不匹配，因而提出Point-SRA以解决这些问题。

Method: 方法包括：1）在MAE中使用多掩码比率并进行自蒸馏对齐；2）提出MeanFlow Transformer以条件流模型实现多样化重建；3）在MFT不同时间步间进行自表示对齐；4）设计基于流模型的微调架构以利用学习到的点云分布。

Result: Point-SRA提出了一种结合自蒸馏与概率建模的3D表示学习方法，通过在MAE中使用多掩码比率和在MFT中引入条件流式生成，实现多层次互补表征的对齐与多样化重建。

Conclusion: Point-SRA在不同任务上均优于现有方法（如Point-MAE、MaskPoint等），证明了多掩码比率、自蒸馏以及概率重建在捕获几何与语义信息方面的有效性。

Abstract: Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.

</details>


### [101] [MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement](https://arxiv.org/abs/2601.01749)
*Lei Zhu,Lijian Lin,Ye Zhu,Jiahao Wu,Xuehan Hou,Yu Li,Yunfei Liu,Jie Chen*

Main category: cs.CV

TL;DR: 提出MANGO，两阶段纯图像监督的双向对话3D头生成框架；第一阶段用扩散-转换器+双音频交互建模多说话人3D运动；第二阶段用快速3D高斯渲染器生成图像并通过交替训练提供2D光度监督；并发布MANGO-Dialog数据集。


<details>
  <summary>Details</summary>
Motivation: 当前方法侧重单人且依赖伪3D标签，难以捕捉细粒度面部动态与听说状态流畅转换，需要纯图像级监督与更好交互建模。

Method: 两阶段方法：1) 扩散式Transformer结合双音频交互模块，从多说话人音频生成自然3D运动；2) 利用快速3D高斯渲染器渲染高保真图像，并在交替训练中用2D光度损失监督3D运动。

Result: 在MANGO-Dialog数据集（50+小时、500+身份）上，大量实验证明方法在准确性与真实感上优于现有方法，能更好地建模双人对话3D动作。

Conclusion: MANGO显著提升了双人3D对话动作的准确性与真实感，通过交替训练避免伪3D标签噪声，增强了可控性与保真度。

Abstract: Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.

</details>


### [102] [CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology](https://arxiv.org/abs/2601.01769)
*Hao Lu,Ziniu Qian,Yifu Li,Yang Zhou,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: 提出基于临床诊断模板的管线（CPRT）构建大规模切片-描述数据集和临床VQA基准，并提出双流Slide-level QA模型CTIS-QA，在多项任务上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: To systematically collect and structure pathological information from pathology reports for improved vision-language tasks and clinically relevant VQA.

Method: 基于CAP协议设计CPRT从报告提取特征；用提取出的要素构建CTIS-Align数据集和CTIS-Bench问答基准；提出双流模型CTIS-QA：一流通过聚类聚合获得全局上下文，另一流通过注意力引导的补丁感知模块关注局部关键区域。

Result: Designed CPRT, created CTIS-Align (80k pairs) and CTIS-Bench (977 WSIs, 14,879 QA), proposed CTIS-QA model that outperforms SOTA on multiple tasks; code/data released.

Conclusion: CPRT有助于标准化病理信息抽取，CTIS系列数据集与CTIS-QA模型促进了临床相关的WSI视觉-语言对齐与问答，提升了病理影像理解的准确性和可用性。

Abstract: In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.

</details>


### [103] [Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery](https://arxiv.org/abs/2601.01781)
*Lakshay Sharma,Alex Marin*

Main category: cs.CV

TL;DR: Introduce Subimage Overlap Prediction pretraining: predict location mask of extracted sub-image within original image; improves convergence and segmentation mIoU with much less pretraining data


<details>
  <summary>Details</summary>
Motivation: reduce need for large pretraining datasets for SSL in remote sensing segmentation

Method: analysis of paper's abstract

Result: faster convergence and equal or better mIoU on downstream segmentation, robust when labeled data reduced, works across architectures and datasets, outperforms other SSL with less pretraining data

Conclusion: SOP is an effective SSL pretext for remote sensing semantic segmentation, enabling faster convergence and competitive/better performance while requiring less pretraining data

Abstract: Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.

</details>


### [104] [DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization](https://arxiv.org/abs/2601.01784)
*Boyang Zhao,Xin Liao,Jiaxin Chen,Xiaoshuai Wu,Yufeng Wu*

Main category: cs.CV

TL;DR: 提出DDNet：双流图学习+解缠与层次融合，有效结合局部与全局线索，AP@0.95提升约9%，跨域表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统TFL方法关注局部视野，难以捕捉全局异常，且本地平滑性会掩盖全局线索，因此需要兼顾局部与全局信息的框架。

Method: 提出双流框架：Temporal Distance Stream处理局部伪造痕迹，Semantic Content Stream捕获长程语义关联；引入Trace Disentanglement and Adaptation(TDA)提取通用伪造指纹，及Cross-Level Feature Embedding(CLFE)进行层次特征深度融合；整体基于图学习协调两条流。

Result: 在ForgeryNet和TVIL数据集上，DDNet在AP@0.95上相比最先进方法提升约9%，并在跨域测试中表现出更强鲁棒性。

Conclusion: DDNet通过双流图学习与特征解缠技术，实现了更精确的时序伪造段定位，显著提升了跨域鲁棒性。

Abstract: The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \emph{local view}, failing to capture global anomalies. To address this, we propose a \underline{d}ual-stream graph learning and \underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \emph{Temporal Distance Stream} for local artifacts and a \emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\% in AP@0.95, with significant improvements in cross-domain robustness.

</details>


### [105] [VerLM: Explaining Face Verification Using Natural Language](https://arxiv.org/abs/2601.01798)
*Syed Abdul Hannan,Hazim Bukhari,Thomas Cantalapiedra,Eman Ansar,Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.CV

TL;DR: 该论文提出一种用于人脸验证的视觉-语言模型（VLM），在判定两张人脸是否为同一人同时生成可解释说明。模型训练使用两种解释风格：简洁总结和详尽差异描述；并将一种用于音频区分的先进方法跨模态改造成适用于视觉输入，从而提升准确性与可解释性。整合高级特征提取与推理能力，实验显示优于基线与现有方法，提升透明性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前人脸验证系统缺乏可解释性，用户与监管方难以理解模型决策。作者希望通过引入能同时给出判定与解释的视觉-语言模型，提升系统透明度与可信度。

Method: 基于一种用于音频差异判断的最先进模型，改造其编码与对齐模块以处理视觉特征；采用两种解释风格的监督训练（简洁总结与详尽差异描述）；结合强大的特征提取器与多模态融合推理模块，输出验证结果与文本解释。

Result: 模型在标准人脸验证基准上表现超过基线，且生成的解释在可读性和相关性上优于对照方法，证明方法有效。

Conclusion: 提出的VLM在准确率和解释性上均优于基线方法，证明了跨模态迁移（从音频到视觉）和双风格解释训练能有效提高人脸验证系统的透明性、可靠性和可解释性。

Abstract: Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.

</details>


### [106] [Causality-Aware Temporal Projection for Video Understanding in Video-LLMs](https://arxiv.org/abs/2601.01804)
*Zhengjian Kang,Qi Chen,Rui Liu,Kangtong Mo,Xingyu Zhang,Xiaoyu Deng,Ye Zhang*

Main category: cs.CV

TL;DR: 提出V-CORE，通过LSA和CATP在视频建模中引入显式单向时间约束，参数高效且显著提升时间/因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLM常用无约束的双向投影器建模帧间交互，可能模糊时间顺序，影响需要严格时间因果关系的视频理解任务。提出V-CORE以显式维护时间有序性。

Method: V-CORE包含两部分：可学习空间聚合（LSA）用于自适应选择关键空间token以减少冗余；以及因果感知时间投影器（CATP），通过块因果注意力和终端动态摘要token（作为因果汇）实现单向有序的信息流。采用4-bit QLoRA和冻结LLM骨干实现参数高效训练。

Result: 在NExT-QA上达成61.2%准确率，在MSVD-QA、MSRVTT-QA和TGIF-QA上也保持竞争力；在时间和因果推理子类上分别提升约+3.5%和+5.2%。

Conclusion: V-CORE通过在时间维度引入显式的因果约束，显著提升了视频中时间顺序与因果推理相关任务的性能，尤其在NExT-QA等基准上表现优秀。

Abstract: Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.

</details>


### [107] [Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification](https://arxiv.org/abs/2601.01807)
*Ubaidullah,Muhammad Abid Hussain,Mohsin Raza Jafri,Rozi Khan,Moid Sandhu,Abd Ullah Khan,Hyundong Shin*

Main category: cs.CV

TL;DR: 提出LUMPNet：结合YOLOv11检测、EfficientNet分类与新型自适应混合优化器的混合深度学习框架，用于基于图像的LSD早期检测；在公开数据集上表现优异（训练99%、验证98%）。


<details>
  <summary>Details</summary>
Motivation: LSD传播快且对畜牧业影响重大，早期准确检测皮肤结节对防止疫情扩散和及时干预至关重要，因此需要高效且准确的自动化图像识别方法。

Method: 框架先用YOLOv11检测并定位皮肤结节区域，随后裁剪并输入EfficientNet（带compound scaling）进行二分类（LSD/健康）。训练过程中采用作者提出的自适应混合优化器以稳定和加速收敛。对比实验包含与AdamW训练的EfficientNet-B0。

Result: LUMPNet提出了一种基于深度学习的混合方法，用于通过图像检测和定位牛皮肤结节，以实现早期Lumpy Skin Disease (LSD) 的识别。方法包含YOLOv11进行目标检测、基于EfficientNet且采用compound scaling的CNN进行分类，并引入一种新的自适应混合优化器以稳固和加速训练。作者在公开数据集上评估了模型，报告了训练精度99%、验证精度98%，并通过与使用AdamW优化器训练的EfficientNet-B0进行对比，表明LUMPNet在性能上更优。

Conclusion: LUMPNet在公开数据集上对LSD皮肤结节的检测和分类表现出高精度，优于基线EfficientNet-B0+AdamW方案，表明所提架构和优化器对提高检测、分类性能有积极作用。

Abstract: Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.

</details>


### [108] [Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning](https://arxiv.org/abs/2601.01818)
*Sungjune Park,Hongda Mao,Qingshuang Chen,Yong Man Ro,Yelin Kim*

Main category: cs.CV

TL;DR: 利用语言描述引导的场景感知器生成上下文感知视频表示，并通过聚焦目标与抑制干扰的两项训练目标，显著提升了自我中心视觉注意力预测的性能和鲁棒性，在Ego4D与AEA数据集上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 动态自我中心场景复杂且存在歧义，场景语境信息对调节人类注意力起关键作用，故通过语言引导的语境建模来提升注意力预测的鲁棒性和准确性。

Method: 设计了一个语言引导的Context Perceiver，用语言场景描述来汇总并生成上下文感知的视频表示；并引入两项训练目标：一是促使模型聚焦于目标兴趣点区域，二是抑制来自不相关区域的干扰。整体框架用于强化自我中心视觉注意力预测。

Result: 在Ego4D和Aria Everyday Activities数据集上进行的大量实验显示，该方法在多样动态的自我中心场景中取得了SOTA性能并增强了鲁棒性。

Conclusion: 提出了基于语言引导的场景语境感知学习框架，旨在提高自我中心视觉注意力预测的鲁棒性。该方法通过语言描述引导的上下文感知器生成视频语境表示，并通过两项训练目标提升关注目标兴趣点区域与抑制无关区域的能力。实验在Ego4D和AEA数据集上取得了最先进的性能。

Abstract: As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.

</details>


### [109] [RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images](https://arxiv.org/abs/2601.01835)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出RSwinV2：结合SwinTransformer的shifted-window注意力与卷积逆残差块，兼顾全局与局部特征；在Kaggle数据集上达96.21%准确率、95.62% F1，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 提高对Mpox皮肤病变与类似疾病（如水痘、麻疹、牛痘）间差异化识别能力，减少类内变异性并增强判别性，辅助临床或远程诊断。

Method: 基于SwinTransformerV2，输入图像切分为非重叠patch并采用shifted window多头注意力，同时定制输入维度、嵌入结构和输出层，加入patch/position嵌入与多头注意力，以及引入Inverse Residual Block（卷积残差跳跃连接）以缓解梯度消失并融合局部与全局特征。

Result: 在公开Kaggle数据集上，RSwinV2达到了96.21%准确率与95.62% F1，优于传统CNN和原始SwinTransformer变体，证明了在Mpox病变识别上的有效性。

Conclusion: 该论文提出了一种名为RSwinV2的模型，通过在SwinTransformer基础上定制层次结构并加入逆残差块来提升痘类病变（Mpox）图像分类性能，实验在Kaggle数据集上实现了96.21%准确率和95.62% F1。

Abstract: In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.

</details>


### [110] [ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting](https://arxiv.org/abs/2601.01847)
*Chuhang Ma,Shuai Tan,Ye Pan,Jiaolong Yang,Xin Tong*

Main category: cs.CV

TL;DR: 提出ESGaussianFace，结合3D Gaussian Splatting与情感-音频空间注意力和双重高斯变形预测器，通过多阶段训练实现情感与风格驱动的高质量三维一致性口型动画


<details>
  <summary>Details</summary>
Motivation: Existing methods lack emotional and stylistic richness and 3D consistency; need efficient high-quality emotional+style audio-driven talking heads

Method: 3D Gaussian Splatting, emotion-audio-guided spatial attention, dual 3D Gaussian deformation predictors, multi-stage training

Result: ESGaussianFace produces efficient, high-quality, 3D-consistent talking head videos with improved lip accuracy, expression variation, and style expressiveness; validated by extensive experiments

Conclusion: 方法有效融合情感与风格特征，提升口型准确性与表情多样性，生成高效且三维一致的动画，优于现有技术

Abstract: Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.

</details>


### [111] [GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection](https://arxiv.org/abs/2601.01856)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: GCR通过在共享的冻结patch嵌入空间按最近原型累积距离路由输入，分离跨头决策与头内评分，解决跨专家分数不可比问题，从而在持续扩展类别下实现稳定且高效的异常检测。


<details>
  <summary>Details</summary>
Motivation: 在任务不可知且类别持续扩展的场景中，传统方法需先将输入路由到某个类别专家再进行头内评分，但不同专家的异常分数分布在尺度与尾部行为上差异很大，直接比较不可比，导致路由不稳定并引发性能崩溃。作者认为许多被归因于表示遗忘的失败，其实源于跨头决策规则不稳定。

Method: GCR在冻结的patch-embedding空间维护类别特定的原型库（prototype banks）。对每张测试图像，计算到每个类别原型库的累积最近原型距离，选择距离最小的类别作为路由目标（几何一致性路由），然后只在该专家头内使用标准基于原型的异常评分生成异常图。整个流程不需要端到端微调表示。

Result: 在MVTec AD和VisA数据集上，GCR显著提升了路由稳定性，缓解持续学习中的性能坍塌，实现近零遗忘（near-zero forgetting），同时保持竞争性的检测和定位性能。

Conclusion: 该论文提出GCR（geometry-consistent routing），通过在共享的冻结patch嵌入空间中基于最近原型距离累积的路由规则，将输入路由到合适的类别专家，分离跨头决策与头内异常评分，从而稳定任务不可知的持续异常检测并避免跨头评分不可比的问题。

Abstract: Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.
  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.
  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR

</details>


### [112] [RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations](https://arxiv.org/abs/2601.01865)
*Wenlong Yang,Canran Jin,Weihang Yuan,Chao Wang,Lifeng Sun*

Main category: cs.CV

TL;DR: RRNet通过对象感知、虚拟光源参数估计和深度感知渲染，在不需像素对齐训练数据下实现实时高质量视频重光照与曝光控制，适用于视频会议与移动摄影


<details>
  <summary>Details</summary>
Motivation: 解决实时视频增强中在不均匀光照下平衡速度与曝光控制的问题

Method: 使用精简编码器与轻量预测头估计少量虚拟光源参数，结合深度感知渲染模块进行局部重光照；并用生成式AI合成多样光照数据进行训练

Result: 提出RRNet，通过估计少量虚拟光源参数和深度感知渲染模块实现局部重光照，实时高分辨率表现，并提供生成式数据合成管线

Conclusion: RRNet在低光增强、局部照明调整和眩光去除上优于先前方法，架构轻量可解释，适合实际应用

Abstract: With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.

</details>


### [113] [Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.01870)
*Wenyu Shao,Hongbo Liu,Yunchuan Ma,Ruili Wang*

Main category: cs.CV

TL;DR: 本文提出EGMT，一种实体引导的多任务红外-可见图像融合方法：从大模态模型生成的图注中提取实体级文本信息，作为多标签分类的伪标签，构建并行多任务框架并设计实体引导的跨模态交互模块，从而增强语义监督与视觉-文本细粒度交互，提升融合图的目标保留、纹理细节和语义一致性；并发布四个数据集的实体注释版本。


<details>
  <summary>Details</summary>
Motivation: 句子级文本容易包含冗余语义噪声，限制对文本深层语义价值的利用；通过提取实体级信息并将其用于多任务语义监督与跨模态交互，可减少噪声并增强融合图的语义表达。

Method: (1) 使用大视觉-语言模型生成图像描述；(2) 设计规则或模型从句子中提取实体级信息作为伪标签；(3) 构建并行多任务网络，一支用于图像融合，一支用于多标签分类以提供语义监督；(4) 设计实体引导的跨模态交互模块，在视觉特征与实体特征间进行细粒度交互；(5) 在实体注释的四个数据集上训练与评估，比较主观与客观指标。

Result: 在TNO、RoadScene、M3FD、MSRS四个实体注释数据集上的大量实验显示，EGMT在保留显著目标、纹理细节和语义一致性方面优于现有最先进方法，证明了实体级文本监督与交互模块的有效性。

Conclusion: EGMT通过实体级文本提取、多任务语义监督与实体引导的跨模态交互，显著提升了红外-可见图像融合的语义一致性与图像质量，在保留显著目标与细节方面优于现有方法，并公开了实体注释数据集供研究使用。

Abstract: Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.

</details>


### [114] [CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving](https://arxiv.org/abs/2601.01874)
*Shuhang Chen,Yunqiu Xu,Junjie Xie,Aojun Lu,Tao Feng,Zeying Huang,Ning Zhang,Yi Sun,Yi Yang,Hangjie Yuan*

Main category: cs.CV

TL;DR: 本文提出CogFlow，一个受人类认知启发的三阶段框架（感知→内化→推理），通过协同视觉奖励、知识内化奖励和视觉门控策略优化提升视觉数学问题求解。并构建了包含12万多对齐标注的MathCog数据集，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 观察到现有工作仅提升视觉输入提取，而忽视了视觉线索是否被忠实整合进后续推理，导致模型寻找视觉不相关的“捷径”。因此提出模仿人类分层推理流程的框架以解决该问题。

Method: 提出三阶段流程：1) 感知：使用协同视觉奖励在参数和语义空间优化视觉信息提取；2) 内化：设计知识内化奖励模型以保证视觉线索被忠实整合；3) 推理：采用视觉门控策略优化算法，强制推理过程以视觉知识为基础。并构建MathCog数据集用于训练。

Result: 在常用视觉数学推理基准上，CogFlow优于现有方法；并通过大规模对齐标注数据MathCog支持模型训练，实验与消融分析显示各模块均带来提升。

Conclusion: CogFlow通过增强感知、引入知识内化以及视觉约束的推理策略，显著提高了多模态大模型在视觉数学推理任务中的准确性和视觉对齐性，在多个基准上表现优异。

Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.

</details>


### [115] [Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems](https://arxiv.org/abs/2601.01891)
*Niloufar Alipour Talemi,Julia Boone,Fatemeh Afghah*

Main category: cs.CV

TL;DR: 综述agentic AI在遥感中的应用，提出分类、分析体系结构、评测基准并指出挑战与路线图


<details>
  <summary>Details</summary>
Motivation: Existing vision FMs and multimodal LLMs advance representations but lack sequential planning and tool orchestration for complex geospatial tasks

Method: Survey and taxonomy of agentic AI for remote sensing

Result: Unified taxonomy (single-agent copilots vs multi-agent systems), analysis of architectures (planning, RAG, memory), review of benchmarks (trajectory-aware reasoning), critique of grounding, safety, orchestration, roadmap for robust autonomous geospatial intelligence

Conclusion: 提出研究方向以提升遥感领域中自治智能的规划、工具协同、可靠性与安全性

Abstract: The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.

</details>


### [116] [Forget Less by Learning from Parents Through Hierarchical Relationships](https://arxiv.org/abs/2601.01892)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: 在超曲率（洛伦兹）空间建立父子概念结构，让已学概念作为新概念学习的指导，从而减少灾难性遗忘并促进新概念整合。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于减少概念间干扰，却忽视了概念间可能的正向交互；利用层级关系可让已学概念帮助学习新概念，从而减轻遗忘并提高整体性能。

Method: 将概念嵌入洛伦兹流形（超曲率空间），定义父子关系并在微调新概念时以父概念作为指导信号，结合定制的损失项和训练策略以减少参数覆盖和干扰。

Result: FLLP在连续学习Custom Diffusion Models时，通过在洛伦兹流形中构建父子概念关系，利用父概念指导新概念的学习以减少遗忘。实验在三套公共数据集和一套合成基准上验证，显示在稳健性和泛化性能上都有一致提升。

Conclusion: FLLP通过在超曲率流形中显式建模概念层次的父子关系，既能保留先前知识又能顺利吸收新概念，实验证明能提高鲁棒性和泛化能力。

Abstract: Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.

</details>


### [117] [Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection](https://arxiv.org/abs/2601.01908)
*Jingjing Wang,Qianglin Liu,Zhuo Xiao,Xinning Yao,Bo Liu,Lu Li,Lijuan Niu,Fugen Zhou*

Main category: cs.CV

TL;DR: 提出Nodule-DETR：通过MSFCA、HFF和MSDA三大模块增强超声中低对比、边界模糊甲状腺结节的检测，在真实临床数据上显著优于基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 超声图像中常见低对比度和结节边界模糊问题限制了甲状腺结节检测的准确性，因此需要一种能增强微弱特征并处理多尺度、形状多样性的检测方法。

Method: 本文提出了一种基于DETR的检测架构Nodule-DETR，包含三个关键模块：多谱频域通道注意力（MSFCA）利用频域分析增强低对比结节特征；层次特征融合（HFF）用于高效的多尺度特征整合；多尺度可变形注意（MSDA）用于灵活捕捉小且形状不规则的结节。

Result: 在真实临床甲状腺超声图像数据集上进行的大量实验表明，Nodule-DETR在mAP@0.5:0.95上比基线提高了0.149，达到SOTA性能。

Conclusion: Nodule-DETR能够有效提高甲状腺结节在超声图像中的检测性能，显著优于基线模型，具备良好临床应用潜力。

Abstract: Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.

</details>


### [118] [Learning Action Hierarchies via Hybrid Geometric Diffusion](https://arxiv.org/abs/2601.01914)
*Arjun Ramesh Kaushik,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: HybridTAS uses hyperbolic-guided denoising in diffusion models to exploit hierarchical action structure, leading to SOTA temporal action segmentation.


<details>
  <summary>Details</summary>
Motivation: Temporal action segmentation needs to assign labels per frame; existing iterative refinement methods ignore hierarchical structure of actions.

Method: Design a diffusion-based segmentation model with hybrid Euclidean-hyperbolic embeddings; map high-level classes to hyperbolic root nodes for early timesteps and fine classes to leaves for later timesteps; perform denoising conditioned on hierarchical embeddings; evaluate on GTEA, 50Salads, Breakfast.

Result: HybridTAS integrates Euclidean and hyperbolic geometries into diffusion model denoising, using hyperbolic space to model hierarchical coarse-to-fine denoising. Achieves SOTA on GTEA, 50Salads, Breakfast.

Conclusion: Incorporating hyperbolic geometry into the denoising process allows coarse-to-fine label refinement and improves segmentation performance across benchmarks.

Abstract: Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.

</details>


### [119] [TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing](https://arxiv.org/abs/2601.01915)
*Yujie Hu,Zecheng Tang,Xu Jiang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: 提出TalkPhoto：无训练、对话式图像编辑框架，通过提示模板让开源LLM解析需求并分层调用现有编辑工具，支持即插即用方法接入，提升调用准确性与编辑质量同时降低tokens消耗。


<details>
  <summary>Details</summary>
Motivation: Improve controllability and flexibility of image editing by using LLMs to interpret instructions and manage edits without time-consuming multi-instruction dataset training.

Method: Design prompt template to guide LLM to analyze instructions and decompose tasks; hierarchical invocation of existing advanced editing methods in a plug-and-play manner; efficient token usage and no model retraining.

Result: TalkPhoto: a training-free framework using prompt-engineered open-source LLMs to analyze user needs and hierarchically invoke existing editing methods; plug-and-play integration of methods; better invocation accuracy, lower token use, and higher editing quality across tasks.

Conclusion: TalkPhoto enables precise, flexible, and high-quality image editing without additional training by leveraging prompt-engineered LLMs and modular invocation of advanced editing tools.

Abstract: Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.

</details>


### [120] [AR-MOT: Autoregressive Multi-object Tracking](https://arxiv.org/abs/2601.01925)
*Lianjie Jia,Yuhan Wu,Binghao Ran,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: AR-MOT reframes MOT as LLM-based sequence generation, enabling flexible, head-free tracking via object tokenizer, RAA, and TMF, showing competitive results on MOT17/DanceTrack


<details>
  <summary>Details</summary>
Motivation: Existing MOT methods are rigid, task-specific, hard to extend to multi-modal/instruction-driven tasks due to fixed heads and pipelines

Method: autoregressive paradigm for MOT

Result: AR-MOT formulates MOT as sequence generation in LLM; introduces Object Tokenizer, Region-Aware Alignment, Temporal Memory Fusion; achieves comparable SOTA on MOT17 and DanceTrack

Conclusion: AR-MOT provides a flexible, extensible MOT framework that supports new modalities/instructions by changing output sequence format without architecture changes, validated by experiments

Abstract: As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.

</details>


### [121] [MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering](https://arxiv.org/abs/2601.01926)
*Zhifei Li,Yiran Wang,Chenyi Xiong,Yujing Xia,Xiaoju Hou,Yue Zhao,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.CV

TL;DR: 提出MacVQA：结合噪声过滤与原型记忆分配，改善持续VQA中的知识保留与新任务适应，实验证明优于现有基线（平均准确率约43%）。


<details>
  <summary>Details</summary>
Motivation: 当前持续VQA方法难以在保留旧知识、适应新知识和保持鲁棒特征之间取得平衡，故提出结合噪声过滤与原型内存分配的方案以提升整体性能。

Method: 模型通过融合视觉与问题信息，先用全局噪声过滤模块去除噪声以提高表示质量，再用基于原型的内存分配策略选择性存储样本特征，优化内存使用和特征质量，从而支持持续学习。

Result: MacVQA提出了一种用于持续视觉问答的新框架，通过自适应原型内存分配和全局噪声过滤来平衡知识保留与适应性，并提升特征表示鲁棒性。

Conclusion: MacVQA在十个持续VQA任务上显著优于基线，兼顾准确率与遗忘度，表明其在知识保持和组合泛化方面更有效。

Abstract: Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.

</details>


### [122] [Face Normal Estimation from Rags to Riches](https://arxiv.org/abs/2601.01950)
*Meng Wang,Wenjing Dai,Jiawan Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 提出用于面部法线估计的粗到细方法：先用小数据训练粗估计器生成示例，再用自注意力和精细化网络结合图像与示例获得高质量法线，显著降低配对数据需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: Reduce dependence on large-scale paired datasets for face normal estimation by splitting task into coarse guidance and fine refinement to save training data and resources.

Method: 步骤：1) 在小数据集上训练简洁模型得到粗糙面部法线作为示例；2) 使用自注意力模块捕捉长程依赖，修复局部伪影；3) 定制精细化网络，将输入图像与示例一起映射为高质量细颗粒法线；4) 进行大量实验和消融验证性能和训练成本。

Result: A coarse-to-fine pipeline: small-data-trained coarse normal estimator produces exemplars; self-attention refines to remove local artifacts; a refinement network maps image+exemplar to high-quality normals. Shows better quality and lower training expense vs SOTA; code released.

Conclusion: 方法通过功能分离（粗估计+精细化）有效减少对大规模配对训练数据和计算资源的需求，在多项实验中优于现有方法，且开源代码模型便于复现。

Abstract: Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.

</details>


### [123] [MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization](https://arxiv.org/abs/2601.01955)
*Zhexin Zhang,Yifeng Zhu,Yangyang Xu,Long Chen,Yong Du,Shengfeng He,Jun Yu*

Main category: cs.CV

TL;DR: MotionAdapter disentangles motion via cross-frame attention, customizes with DINO correspondences, guides DiT denoising to transfer motion while preserving appearance; outperforms SOTA


<details>
  <summary>Details</summary>
Motivation: extract motion and adapt to target content

Method: analysis of method

Result: better motion transfer and editing like zooming

Conclusion: MotionAdapter enables robust semantically aligned motion transfer and supports editing tasks

Abstract: Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \romannumeral1) explicit disentanglement of motion from appearance and \romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.

</details>


### [124] [AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing](https://arxiv.org/abs/2601.01957)
*Tianbo Wang,Yuqing Ma,Kewei Liao,Zhange Zhang,Simin Li,Jinyang Guo,Xianglong Liu*

Main category: cs.CV

TL;DR: 提出AFTER，通过事实增强的一般引导和查询自适应偏移，对LVLM内部激活进行细粒度编辑，有效降低类别/属性/关系类幻觉，AMBER基准上可降约16.3%。


<details>
  <summary>Details</summary>
Motivation: 现有激活编辑方法虽能缓解幻觉但忽视事实文本语义的引导，无法显式纠正语言偏见；因此需要一种结合事实语义与查询自适应能力的激活编辑策略以更精确地抑制不同类型的幻觉。

Method: 方法包含两部分：1) FAS：构建事实增强的通用激活引导向量，显式编码视觉-文本事实关联以指导激活编辑；2) QAO：针对不同问题查询，设计偏移估计器从通用引导向量中生成查询特定的编辑偏移，提升编辑的针对性与多样性。

Result: AFTER提出了一种自适应事实引导的视觉-文本激活编辑方法，旨在缓解大视觉-语言模型的幻觉问题。该方法由Factual-Augmented Activation Steering (FAS)和Query-Adaptive Offset Optimization (QAO)组成：FAS提供事实增强的一般引导向量以建立精确的视觉-文本关联，QAO根据具体查询自适应地从该引导向量生成查询专属的编辑偏移，从而提高编辑的多样性与粒度。实验显示在三种主流LVLM和多个基准上效果显著，在AMBER基准上可使幻觉率相比基线降低最多16.3%。

Conclusion: AFTER能显著缓解LVLM的语言偏见导致的幻觉问题，通过结合事实语义和查询自适应编辑改善视觉-文本对齐，实验证明在多模型多基准上普适有效并且成本较低。

Abstract: Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.

</details>


### [125] [Forget Less by Learning Together through Concept Consolidation](https://arxiv.org/abs/2601.01963)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: FL2T: order-agnostic concurrent concept learning for custom diffusion models using set-invariant inter-concept proxies to reduce catastrophic forgetting, achieving ~2%+ CLIP alignment gains.


<details>
  <summary>Details</summary>
Motivation: Mitigate catastrophic forgetting in Custom Diffusion Models when learning new concepts incrementally and address limitations of existing methods that use fixed sequential learning and ignore inter-concept interactions.

Method: Introduce FL2T with a set-invariant inter-concept learning module where learnable proxies guide selective feature aggregation across concepts to enable concurrent, order-agnostic learning and improved retention; evaluated on three datasets in 10-task settings using CLIP Image Alignment metrics.

Result: Proposes FL2T framework enabling concurrent and order-agnostic concept learning using a set-invariant inter-concept learning module with proxies for feature selection, showing improved knowledge retention and transfer and at least 2% average gain in CLIP Image Alignment across three datasets in 10-task incremental learning.

Conclusion: Inter-concept guidance via set-invariant proxies helps preserve old concepts and incorporate new ones, significantly mitigating catastrophic forgetting in incremental concept learning for CDMs.

Abstract: Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.

</details>


### [126] [Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation](https://arxiv.org/abs/2601.01984)
*Weijian Ma,Shizhao Sun,Tianyu Yu,Ruiyu Wang,Tat-Seng Chua,Jiang Bian*

Main category: cs.CV

TL;DR: 通过在 VLM 中构建 JSON 式对象蓝图并结合监督、蓝图感知奖励与反捷径增强，论文提升了模型的空间推理能力与答案的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么聚焦局部图像补丁改善精细感知但削弱整体空间感知，或标注孤立坐标忽视整体组织。作者受认知学中面向对象蓝图概念启发，旨在同时捕捉对象位置与整体组织以增强空间语义理解。

Method: 首先由模型根据图像与问题生成对象中心的蓝图；在训练阶段结合(1)用于微调的蓝图嵌入推理轨迹监督，(2)在强化学习中使用蓝图感知奖励以鼓励合适数量对象并使答案与蓝图因果一致，以及(3)反捷径数据增强通过针对性扰动阻止模型依赖表层视觉/语言线索。

Result: 在多项基准上，该方法持续优于现有通用 VLM 与专门的空间推理模型，表明蓝图式对象表征与相应训练策略能提升模型的空间推理与因果一致性。

Conclusion: 该论文提出在视觉-语言模型中引入面向对象的蓝图表征，通过生成记录相关对象位置、大小和属性的 JSON 式蓝图并在其上推理，显著提升空间推理能力。

Abstract: Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.

</details>


### [127] [VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis](https://arxiv.org/abs/2601.01989)
*Aly R. Elkammar,Karim M. Gamaleldin,Catherine M. Elias*

Main category: cs.CV

TL;DR: 该论文提出使用Transformer/Video Vision Transformer模型和多模态输入预测行人过街意图，在JAAD数据集上达到并超越SOTA，涉及准确率、AUC、F1-score等指标，并通过消融研究分析设计选择的影响。


<details>
  <summary>Details</summary>
Motivation: 为实现从L3到L4自动驾驶的过渡，需要准确预测行人过街意图以提高道路安全，故探索更强的模型和多模态特征来理解行人行为。

Method: 提出多种规模的Transformer/Video Vision Transformer模型，输入不同数据模态（可能包括视频帧、光流、轨迹、场景上下文等），通过训练优化在JAAD数据集上进行预测。使用多模型设计并进行消融实验比较各设计和模态的贡献。

Result: 在JAAD数据集上，提出的方法在Accuracy、AUC、F1-score等指标上超过现有SOTA，并通过广泛的消融实验验证不同设计选择的优势。

Conclusion: 基于不同规模的Transformer/ViViT模型并融合多模态特征，能显著提升行人意图预测性能，在JAAD上取得SOTA结果，消融实验说明模型架构和模态输入对性能有重要影响。

Abstract: Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.

</details>


### [128] [API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning](https://arxiv.org/abs/2601.01992)
*Chen Zhu,Huiwen Zhang,Yujie Li,Mu He,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 提出API框架：AHG用于现实感生成增强，DHR进行密度感知的补丁重要性处理，MNCD损失利用多负样本；在多基准上取得SOTA且泛化强。


<details>
  <summary>Details</summary>
Motivation: Improve generalization of dehazing models on complex real-world haze distributions by addressing limited training data and varying haze density.

Method: AHG生成多样且真实的合成雾图作为训练数据；DHR按补丁重要性与雾密度自适应处理图像区域；MNCD在空间与频域采集多重负样本进行对比学习，减小细节模糊和歧义。

Result: Proposed API framework with AHG for realistic haze augmentation, DHR for adaptive patch importance-aware removal, and MNCD loss using multiple negatives across spatial/frequency domains; achieves SOTA on benchmarks and robust generalization.

Conclusion: API框架通过数据增强、密度感知去雾与多负对比损失，显著提升了去雾性能与泛化能力，适应多样化雾分布。

Abstract: Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.

</details>


### [129] [Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors](https://arxiv.org/abs/2601.01998)
*Chen Zhu,Huiwen Zhang,Mu He,Yujie Li,Xiaotian Qiao*

Main category: cs.CV

TL;DR: 提出多专家+频率感知路由器的逐步互补强化框架，用以同时处理夜间雾和低照度退化，显著提升可视性并具备泛化能力。


<details>
  <summary>Details</summary>
Motivation: 夜间图像同时存在雾与低光等多重退化，单一退化建模效果有限；观察到雾与低光先验可相互补充，因而提出互相强化的联合恢复策略以提升可视性。

Method: 构建图像/块/像素级专家，分别在视觉与频率域重建不同尺度信息；设计频率感知路由器，基于频率特征动态分配专家权重；采用逐步级联策略强化先验一致性，训练在夜间去雾数据集并验证在其他任务上的泛化。

Result: 提出一种通过互补强化雾与低照度先验一致性的夜间有雾图像可视性增强框架；使用图像/块/像素级多专家在视觉与频率域逐步恢复结构、区域和细节；引入频率感知路由器自适应融合专家贡献；在夜间去雾基准上效果优越，并推广到白天去雾与低光增强任务。

Conclusion: 通过在多尺度、多域上互相强化雾与低光先验并自适应融合，各专家协同恢复全局结构、区域模式与细节，实现稳健且泛化的夜间去雾和低光增强。

Abstract: Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.

</details>


### [130] [Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach](https://arxiv.org/abs/2601.02016)
*Matthias Bartolo,Dylan Seychell,Gabriel Hili,Matthew Montebello,Carl James Debono,Saviour Formosa,Konstantinos Makantasis*

Main category: cs.CV

TL;DR: 该论文将LUPI（使用特权信息学习）引入目标检测，提出一种模型无关的教师-学生框架，在训练时利用诸如边界框掩码、显著性图和深度线索等特权信息，提高检测精度且不增加推理开销。实验覆盖五种主流检测器与多个数据集，结果表明学生模型稳定优于基线，尤其在中大目标上提升明显，且中间权重平衡最优。


<details>
  <summary>Details</summary>
Motivation: 利用训练阶段可得但推理时不可用的细粒度描述性信息（如掩码、显著图、深度）作为特权信息，提升检测器的学习效率与泛化能力，同时保持推理效率。

Method: 提出一个通用的教师-学生方法：教师网络在训练时接收常规图像输入加特权信息，学生只接收常规输入；通过中间特征对齐和损失加权（包含蒸馏损失与检测任务损失）引导学生学习。进行了模型无关实现，适配五种检测器并在多个数据集上验证。

Result: 在多个公共基准（含无人机垃圾检测数据集与Pascal VOC 2012）上，LUPI训练的学生模型在mAP等指标上稳定超越基线，尤其对中大目标改进明显；消融研究显示中间权重设置能最优平衡特权引导与标准输入学习。

Conclusion: LUPI框架能在不增加推理复杂度的前提下，通过在训练阶段注入特权信息显著提升目标检测性能，特别对中大目标与泛化能力有利。

Abstract: This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.

</details>


### [131] [Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement](https://arxiv.org/abs/2601.02018)
*Guangqian Guo,Aixi Ren,Yong Guo,Xuehui Yu,Jiacheng Tian,Wenli Li,Yaoxing Wang,Shan Gao*

Main category: cs.CV

TL;DR: GleSAM++ enhances SAMs for degraded images via latent-space generative enhancement, feature alignment (FDA, CRE), and degradation-aware adaptive enhancement (DAE) for two-stage reconstruction, boosting robustness with few extra params.


<details>
  <summary>Details</summary>
Motivation: Improve SAM robustness on low-quality/degraded images where performance drops, enabling real-world applicability.

Method: Generative Latent space Enhancement to boost features; Feature Distribution Alignment and Channel Replication and Expansion to adapt diffusion model to segmentation framework; Degradation-aware Adaptive Enhancement to predict degradation level and perform degradation-aware reconstruction in two stages; plug-in to pre-trained SAM/SAM2.

Result: GleSAM++ uses Generative Latent space Enhancement plus FDA and CRE to align diffusion model with segmentation, and DAE to predict degradation level and perform degradation-aware reconstruction; applies to pre-trained SAM/SAM2 with minimal extra parameters and improves segmentation robustness across degradations, including unseen ones.

Conclusion: GleSAM++ effectively improves segmentation robustness on complex and unseen degradations while retaining performance on clear images, using minimal additional parameters and mechanisms for alignment and degradation-aware processing.

Abstract: Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.

</details>


### [132] [Adapting Depth Anything to Adverse Imaging Conditions with Events](https://arxiv.org/abs/2601.02020)
*Shihan Peng,Yuyang Xiong,Hanyu Zhou,Zhiwei Shi,Haoyue Liu,Gang Chen,Luxin Yan,Yi Chang*

Main category: cs.CV

TL;DR: ADAE fuses event camera data with frame-based Depth Anything using entropy-based spatial fusion and motion-guided temporal correction to handle extreme illumination and motion blur, improving depth estimation robustness.


<details>
  <summary>Details</summary>
Motivation: Frame-based depth models fail under extreme illumination and motion blur because visual signals are corrupted. Event cameras provide complementary high dynamic range and temporal resolution. ADAE aims to fuse both sources while inheriting foundation model knowledge to robustly estimate depth in adverse conditions.

Method: Designs two modules: 1) Entropy-Aware Spatial Fusion computes information entropy to detect illumination degradation and adaptively fuse frame and event features. 2) Motion-Guided Temporal Correction uses event-derived motion cues to recalibrate features in blurred regions. Integrates into Depth Anything framework and trains/evaluates on degraded datasets.

Result: The paper proposes ADAE, an event-guided spatiotemporal fusion framework to improve Depth Anything for depth estimation under adverse imaging (extreme illumination, motion blur). It introduces entropy-aware spatial fusion to weight event vs frame features based on information entropy indicating illumination degradation, and motion-guided temporal correction using event motion cues to recalibrate blurred regions. The two modules complement to enhance robustness; extensive experiments validate superiority.

Conclusion: ADAE effectively enhances Depth Anything in degraded scenes by adaptively combining event and frame features and using event-based motion cues for temporal correction, leading to improved depth estimation under adverse conditions.

Abstract: Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.

</details>


### [133] [Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding](https://arxiv.org/abs/2601.02029)
*Toshihiko Nishimura,Hirofumi Abe,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: 无训练、无配对图像，通过虚拟相机投影+语言引导的2D基础分割模型、多视角加权投票回投，实现大规模点云的开放词汇3D语义分割，性能接近监督方法。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵的3D标注数据与配对RGB图像的依赖，同时借助强大的2D基础模型与语言引导实现开放词汇的对象识别，拓展分割能力到任意文本查询。

Method: 采用虚拟相机将点云投影为多视角2D图像，利用自然语言提示引导的2D基础分割模型进行每视角语义预测，最终通过加权投票将多视角2D预测聚合回3D点云以实现3D分割。

Result: 在无训练的设定下，方法优于现有训练免费方法，并达到与监督方法可比的分割精度，同时支持开放词汇识别能力。

Conclusion: 该论文提出了一种无需3D标注或配对RGB图像、基于2D基础模型与自然语言提示实现大规模点云3D语义分割的训练免费方法。

Abstract: This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.

</details>


### [134] [AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off](https://arxiv.org/abs/2601.02038)
*Yihan Zhu,Mengying Ge*

Main category: cs.CV

TL;DR: 提出AlignVTOFF，一种并行U-Net框架（Reference U-Net + TSFA）用于虚拟试穿平铺服装生成，重点保留几何结构和高频纹理，通过混合注意力把参考服装特征注入冻结的去噪U-Net，从而改善纹理衰减。


<details>
  <summary>Details</summary>
Motivation: 现有轻量特征提取模块难以保留服饰的结构化图案与细粒度细节，导致生成过程中纹理衰减，需更强的模块保证几何变形建模与高频信息保留。

Method: 构建并行U-Net：Reference U-Net用于多尺度特征提取与几何保真；Texture-Spatial Feature Alignment（TSFA）通过可训练的跨注意力和冻结的自注意力将参考特征注入冻结的去噪U-Net，实现纹理与空间信息对齐。

Result: 在大量实验中，AlignVTOFF产生的平铺服装在结构真实性和高频细节保真方面均领先于最先进方法。

Conclusion: AlignVTOFF在多种设置下均优于现有方法，能生成结构更真实、细节保真度更高的平铺服装图像。

Abstract: Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.

</details>


### [135] [Agentic Retoucher for Text-To-Image Generation](https://arxiv.org/abs/2601.02046)
*Shaocheng Shen,Jianfeng Liang. Chunlei Cai,Cong Geng,Huiyu Duan,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Agentic Retoucher：一个感知—推理—动作的层级主体化修复框架，并构建GenBlemish-27K数据集；在失真定位、感知质量和用户偏好对齐上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型尽管具备惊人的逼真度，但肢体、面部、文字等局部失真普遍存在；现有修复方法要么代价高（反复生成），要么依赖空间定位弱的VLM导致语义漂移，因此需要一个可控、语义稳健且高效的后处理框架。

Method: 提出三类代理：感知代理学习文本-图像一致性的上下文显著性以定位细粒度失真；推理代理通过渐进式偏好对齐实施人类对齐的病因诊断；动作代理依据用户偏好自适应规划局部修补（inpainting）。整体是决策回路而非一次性重生成。

Result: Agentic Retoucher提出通过感知-推理-动作的层级决策框架，对T2I生成结果进行自我修正，提高局部失真修复能力。构建了GenBlemish-27K数据集用于细粒度标注和评估。

Conclusion: Agentic Retoucher为T2I后处理提供了一种自我纠错且具人类式决策流程的解决方案，显著改善局部失真并可量化评估，推动更可靠的生成模型部署。

Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.

</details>


### [136] [PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction](https://arxiv.org/abs/2601.02088)
*Jiahao Bao,Huazhen Liu,Yu Zhuang,Leran Tao,Xinyu Xu,Yongtao Shi,Mengjia Cheng,Yiming Wang,Congshuang Ku,Ting Zeng,Yilang Du,Siyi Chen,Shunyao Shen,Suncheng Xiang,Hongbo Yu*

Main category: cs.CV

TL;DR: 提出PhysSFI-Net，通过将头颅-面部交互编码、LSTM增量变形预测与生物力学重建相结合，实现对术后面部高分辨率、可解释预测；在135例患者上表现优于ACMT-Net，点云误差约1.07 mm，表面偏差约1.30 mm，标志点误差约2.45 mm。


<details>
  <summary>Details</summary>
Motivation: 解决传统生物力学模型计算成本高、几何深度学习缺乏可解释性的矛盾，提供一种既高效又具有生物物理解释的术后面部形态预测方法。

Method: 框架由三部分组成：1）分层图模块：包含颅面与手术计划编码器及注意力机制，用以提取骨骼-面部交互特征；2）LSTM顺序预测器：对软组织变形进行增量式时间序列预测；3）生物力学启发重建模块：实现高分辨率面部表面重建。使用点云形状误差（Hausdorff）、表面偏差与标志点欧氏距离评估。

Result: PhysSFI-Net是一种用于预测颌骨整形术后软组织变形的物理引导几何深度学习框架，结合了分层图模块、LSTM顺序预测器和生物力学启发的重建模块。

Conclusion: PhysSFI-Net在预测颌骨整形术后面部形态方面表现优异，兼顾准确性与可解释性，具有临床规划与模拟的应用潜力。

Abstract: Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.

</details>


### [137] [MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation](https://arxiv.org/abs/2601.02091)
*Zhehuan Cao,Fiseha Berhanu Tesema,Ping Fu,Jianfeng Ren,Ahmed Nasr*

Main category: cs.CV

TL;DR: 提出了首个大规模仅基于光学影像的冰碛堆分割数据集（3340张Google Earth高分辨率人工标注图像），并设计轻量级网络MCD-Net（MobileNetV2+CBAM+DeepLabV3+），在保证计算成本降低60%以上的同时取得62.3% mIoU和72.8% Dice的分割性能，证明了光学影像可用于冰碛体分割，数据代码已公开。


<details>
  <summary>Details</summary>
Motivation: 高分辨率数字高程模型（DEM）稀缺且光学影像对比度弱，阻碍自动冰碛分割；因此希望构建大规模光学仅数据集并设计轻量、可部署的分割模型，以支持高海拔冰川监测与重建。

Method: 构建3340张人工标注的高分辨率Google Earth影像数据集（四川、云南地区）；提出MCD-Net：采用MobileNetV2作为轻量编码器，加入CBAM注意力模块，并使用DeepLabV3+解码器；与ResNet152、Xception等深背骨网络对比，评估mIoU和Dice等指标，同时报告计算成本。

Result: 在公开数据集上，MCD-Net达到62.3% mIoU和72.8% Dice，计算开销较深网络降低超过60%；但脊线细节受限，未来需结合更高分辨率数据或多源信息提升精度。

Conclusion: 研究表明，尽管冰碛脊线受亚像素宽度和光谱歧义限制难以精确描绘，但光学影像足以实现可靠的冰碛体分割；所提数据集与MCD-Net为冰碛分割提供了可复现的基准和可部署的轻量级方案。

Abstract: Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\% mean Intersection over Union (mIoU) and 72.8\% Dice coefficient while reducing computational cost by more than 60\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.

</details>


### [138] [InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting](https://arxiv.org/abs/2601.02098)
*Jinlong Fan,Shanshan Zhao,Liang Zheng,Jing Zhang,Yuxiang Yang,Mingming Gong*

Main category: cs.CV

TL;DR: InpaintHuman: multi-scale UV features + identity-preserving diffusion inpainting yield better reconstruction of occluded monocular video human avatars, improving geometry and temporal consistency over prior 3D Gaussian Splatting and SDS-based methods


<details>
  <summary>Details</summary>
Motivation: Solve incomplete/occluded monocular video reconstruction producing complete animatable avatars with photorealism and temporal coherence

Method: Paper analysis from provided abstract

Result: Introduces multi-scale UV-parameterized representation with hierarchical coarse-to-fine interpolation and identity-preserving diffusion inpainting using textual inversion and semantic-conditioned guidance; uses pixel-level supervision; shows improvements on synthetic and real datasets

Conclusion: Method achieves more complete, identity-faithful, and temporally consistent animatable avatars from occluded monocular videos, validated on PeopleSnapshot, ZJU-MoCap and OcMotion datasets

Abstract: Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.

</details>


### [139] [360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images](https://arxiv.org/abs/2601.02102)
*Jiaqi Yao,Zhongmiao Yan,Jingyi Xu,Songpengcheng Xia,Yan Xiang,Ling Pei*

Main category: cs.CV

TL;DR: 提出一种面向360图像的前馈3D Gaussian Splatting方法，通过深度-法线几何正则化同时提升渲染质量和几何一致性，显著改善点云与表面重建精度。


<details>
  <summary>Details</summary>
Motivation: 3D scene reconstruction from images is crucial for AR, robotics, and digital twins. Existing methods either fail under sparse views/low-texture or are slow due to per-scene optimization. Feed-forward 3D Gaussian Splatting offers efficiency but lacks geometric consistency for accurate surfaces.

Method: 基于Feed-forward 3DGS，设计Depth-Normal几何正则化，将渲染深度梯度与法线信息耦合，用于监督高斯基元的旋转、尺度与位置，从而改善几何重建。

Result: A feed-forward 3DGS framework for 360 images that produces geometrically consistent Gaussian primitives with high rendering quality. Introduces Depth-Normal geometric regularization to supervise Gaussian rotation, scale, and position, improving point cloud and surface accuracy while maintaining rendering quality.

Conclusion: 该方法在保持高质量渲染的同时，显著提高了几何一致性，适用于需要可靠空间感知的3D重建任务。

Abstract: 3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.

</details>


### [140] [HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures](https://arxiv.org/abs/2601.02103)
*Yating Wang,Yuan Sun,Xuan Wang,Ran Yi,Boyao Zhou,Yipengjing Sun,Hongyu Liu,Yinuo Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: HeadLighter通过双分支架构、渐进式监督训练与法线蒸馏，在受控光照数据上实现了物理可解释的光照—外观解耦，支持实时且可控的头部重光照与视点编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D Gaussian Splatting的人头生成模型虽能实时且视图一致地合成高逼真头部，但光照与内在外观深度耦合，难以进行可控重光照；已有解耦方法依赖强假设，无法处理复杂光照。

Method: 提出双分支架构：一支学习与光照不变的人头属性（外观分支），另一支学习物理基渲染组件（渲染分支）；采用渐进式解耦训练，在受控光照的多视角光台数据上进行监督学习；引入蒸馏策略以生成高质量法线用于真实感渲染。

Result: 在光台采集的多视角受控光照数据上，HeadLighter在保持生成质量和实时渲染的同时，实现了显式光照与视点编辑；实验表明解耦效果优于弱监督方法，法线质量提高，渲染真实感增强。

Conclusion: HeadLighter成功实现了在保持实时渲染与高质量生成的同时，对人头模型进行物理可解释的光照与外观解耦，从而支持明确的光照与视点编辑。

Abstract: Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.

</details>


### [141] [MagicFight: Personalized Martial Arts Combat Video Generation](https://arxiv.org/abs/2601.02107)
*Jiancheng Huang,Mingfu Yan,Songyan Chen,Yi Huang,Shifeng Chen*

Main category: cs.CV

TL;DR: 提出首个个性化双人武术对抗视频生成任务与方法MagicFight，并发布Unity合成数据集，显著改善了身份保持和动作连贯性问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化人类视频生成多聚焦于单人场景，难以处理双人互动场景尤其是武术对抗，导致身份混淆、肢体错误与动作不连贯。为此提出新的任务并创建合成数据集，推动该方向研究。

Method: MagicFight基于现有单人舞蹈生成模型进行改造：1) 使用Unity物理引擎合成大规模双人武术数据集（多角色、多动作、多场景）；2) 在模型层面引入双人交互建模与身份保持机制（如双通道条件编码、交互约束损失、身份一致性正则化）；3) 采用动作对齐与时间一致性优化以保持连贯战斗流程。

Result: 在合成数据集上，MagicFight相比基线模型在身份保真度、动作一致性和视觉质量上均有显著提升；生成视频在两人交互连贯性和身份保持方面表现优良，展示出高保真、低异常的战斗序列。数据集已公开以促进后续研究。

Conclusion: 本文提出了首个针对双人武术对抗的个性化视频生成任务，方法MagicFight通过定制合成数据集和模型改进，有效解决了身份混淆、肢体异常和动作不匹配等问题，推动了交互式视频生成的研究。

Abstract: Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.
  Website: https://MingfuYAN.github.io/MagicFight/
  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta

</details>


### [142] [Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model](https://arxiv.org/abs/2601.02112)
*Utkarsh Singh,Absaar Ali,Adarsh Roy*

Main category: cs.CV

TL;DR: 提出基于按来流方向切片的PointNet2D+双向LSTM的轻量级Cd预测替代模型，速度快、精度高且具可解释性，适合早期汽车气动设计探索。


<details>
  <summary>Details</summary>
Motivation: 传统CFD和风洞测试计算与资源开销大，不利于早期快速迭代；现有机器学习替代模型往往计算复杂或可解释性差，难以对详细几何输入保持高精度。本研究旨在提出一个高效、可解释且精确的替代模型。

Method: 将3D车辆点云沿来流方向分解为有序的二维横截面序列。每个切片由轻量级PointNet2D编码器提取特征，随后使用双向LSTM处理切片特征序列以捕捉纵向几何变化，最终预测Cd。模型在DrivAerNet++数据集上训练与评估。

Result: 在DrivAerNet++数据集上，模型取得R^2>0.9528和MAE≈6.046×10^{-3}的Cd预测性能，并在消费级GPU上实现约0.025秒/样本的推理速度，证明了该方法在精度、速度与可解释性之间的良好平衡。

Conclusion: 本文提出了一种轻量级的基于切片序列处理的气动阻力系数预测替代模型，能够在保持高精度（R^2>0.9528，MAE≈6.046e-3）的同时实现快速推理（约0.025 s/样本），适用于早期汽车设计的快速迭代和可解释性分析。

Abstract: The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.

</details>


### [143] [Remote Sensing Change Detection via Weak Temporal Supervision](https://arxiv.org/abs/2601.02126)
*Xavier Bou,Elliot Vincent,Gabriele Facciolo,Rafael Grompone von Gioi,Jean-Michel Morel,Thibaud Ehret*

Main category: cs.CV

TL;DR: 提出一种无需新增注释的弱时序监督策略，通过扩展单时相数据并假设同址多时相多为无变化、不同地点配对生成变化，结合目标感知变化图和迭代精化来处理噪声标签，在多个数据集上取得良好表现


<details>
  <summary>Details</summary>
Motivation: 针对像素级标注昂贵的问题，通过利用单时相数据的额外时间观测进行弱时序监督，避免新增标注

Method: 分析该论文的方法

Result: 在扩展的FLAIR和IAILD数据集上实现了在零样本和小样本情形下的强性能，并展示了法国大范围应用结果

Conclusion: 该方法通过利用现有单时相数据的多时间观测，提供了一种可扩展且成本低廉的语义变化检测途径，适用于大范围应用

Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.

</details>


### [144] [Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery](https://arxiv.org/abs/2601.02139)
*Chenyang Lai,Shuaiyu Chen,Tianjin Huang,Siyang Song,Guangliang Cheng,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: 作者提出OSCD（双时相溢油变化检测）任务与TAHI合成框架，通过生成灾前影像实现时序比对，从而显著减少误报并提升溢油检测精度，适用于实际SAR监测场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于单张SAR图像的分割方法难以区分溢油与外观相似的海况特征，在样本匮乏与泛化能力差的场景下误报率高，亟需引入时序信息以提高可靠性。

Method: 提出TAHI框架：包含高保真混合修复（用于从溢油后影像重建无油灾前影像）和时序真实感增强（用于保证辐射与海况一致性），并据此合成灾前影像以构建OSCD数据集并基准测试多种变化检测模型。

Result: 基于TAHI合成的灾前影像构建的OSCD数据集上，现有变化检测模型在降低误报和提升检测准确率方面优于传统单帧分割方法，证明时序感知方法在实际海上溢油监测中更具可行性和扩展性。

Conclusion: 提出的OSCD任务通过比对灾前灾后SAR影像，能更有效区分真实溢油与类似海洋现象，降低误报率，提升检测鲁棒性。

Abstract: Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.

</details>


### [145] [Efficient Unrolled Networks for Large-Scale 3D Inverse Problems](https://arxiv.org/abs/2601.02141)
*Romain Vo,Julián Tachella*

Main category: cs.CV

TL;DR: 本文提出一种域划分与正规算子近似方法，使得在单GPU上也能将大型成像算子嵌入到深度学习重建网络中，从而实现端到端训练并在3D锥束CT与3D多线圈加速MRI上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的逆问题方法在大规模（如3D）成像中，因全局向前算子的内存开销极大而无法将算子嵌入网络中，导致不能端到端训练。作者寻求一种能在有限内存下保持算子信息、实现可扩展端到端训练的方案。

Method: 提出将全局正向算子拆分为多个子域处理的策略，并引入对正规算子的近似以保持边界一致性与重建精度，从而允许使用patching技术在训练时加入算子模块，实现端到端反演网络的内嵌式算子。

Result: 在3D锥束CT与3D多线圈加速MRI数据集上，所提方法以单GPU训练与推理实现了最先进的重建质量，优于无法嵌入算子的传统patching方法和部分基于近似算子的替代方案。

Conclusion: 通过域划分和近似正规算子，作者成功在单张GPU上训练并推理包含任意大规模正向算子的端到端深度重建模型，并在3D X射线锥束断层与3D多线圈加速MRI上实现了最先进的结果，证明了方法的有效性和可扩展性。

Abstract: Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.

</details>


### [146] [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/abs/2601.02147)
*Sunny Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: Proposes BiPrompt, a test-time bilateral prompt optimization that removes spurious cues in images and re-centers text prompts to improve robustness and worst-group performance of vision-language models without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing debiasing targets single modality; goal to jointly mitigate spurious correlations in both visual and textual modalities during test-time without retraining or domain supervision.

Method: Bilateral prompt optimization (BiPrompt): test-time debiasing for vision-language models

Result: BiPrompt uses attention-guided erasure and orthogonal prediction consistency on visual side, and balanced prompt normalization on textual side, minimizing conditional mutual information between spurious cues and predictions; yields consistent improvements in average and worst-group accuracies over prior methods.

Conclusion: BiPrompt is a lightweight, effective method to steer vision-language models toward causal, domain-invariant reasoning by jointly addressing visual and textual spurious correlations during test-time adaptation.

Abstract: Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.

</details>


### [147] [Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32](https://arxiv.org/abs/2601.02177)
*Oliver Custance,Saad Khan,Simon Parkinson*

Main category: cs.CV

TL;DR: 本文评估了使用廉价ESP32 WiFi传感器进行多人物体步态识别的可行性，比较六种信号分离方法（FastICA、SOBI、PCA、NMF、Wavelet、张量分解）在1~10人场景下的表现，提出诊断指标并得出结论：所有方法在多人人数下精度低且无显著差异，主要受限于硬件信号质量而非算法。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦单人步态识别并取得高准确率，但多人识别研究稀少且多依赖复杂昂贵的硬件或固件修改；作者旨在弄清多人识别表现差是算法问题还是廉价硬件引起的限制。

Method: 使用ESP32采集1~10人场景的WiFi CSI数据，应用六种信号分离技术（FastICA、SOBI、PCA、NMF、Wavelet、Tensor Decomposition），并引入三项诊断指标（主体内变异、主体间可区分性、性能退化率）评估分离效果与识别精度。

Result: 在7个场景（1~10人）上，六种方法的识别率集中在45%-56%区间（均值、标准差σ=3.74%），方法间差异无统计学显著性（p>0.05）；最佳方法NMF也仅56%准确率；诊断指标显示高主体内变异、低主体间可区分性和随人数增加的严重性能下降，归因于ESP32传感器的信噪比和分辨能力不足。

Conclusion: 基于ESP32的 commodity WiFi CSI 无法为可靠的多人物体分离提供足够的信号质量，导致高主体内变异、低主体间可区分性和随人数增加而严重性能退化；因此不是算法的主要瓶颈。

Abstract: WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\%, $σ$=3.74\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.

</details>


### [148] [QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition](https://arxiv.org/abs/2601.02189)
*Cheng Ying Wu,Yen Jui Chang*

Main category: cs.CV

TL;DR: 提出QuIC：一种轻量的量子启发二阶交互分类器，替代GAP以捕捉通道协方差，稳定训练且显著提升浅层网络的FGVC性能。


<details>
  <summary>Details</summary>
Motivation: 传统GAP只捕获一阶统计量，无法区分细粒度相似子类；Bilinear方法虽能捕获高阶信息但维度高且训练不稳定，需要一种轻量且稳定的二阶交互建模方法。

Method: 将特征通道视为相互作用的量子态，构建可学习的observable算符来捕捉二阶特征协方差，作为插拔式头部替换GAP；保持低维输出并支持稳定端到端训练。

Result: 在多个实验上，QuIC使VGG16的Top-1提升近20%，在ResNet18上优于SE-Block；t-SNE可视化显示QuIC增强了类内紧凑性并解决了歧义样本。

Conclusion: QuIC模块能在保持轻量和单阶段训练的前提下，通过量子态启发的可学习算符捕捉二阶通道协方差，从而显著提升浅层骨干网络在FGVC任务上的表现。

Abstract: Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.

</details>


### [149] [Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models](https://arxiv.org/abs/2601.02198)
*Alexander Möllers,Julius Hense,Florian Schulz,Timo Milbich,Maximilian Alber,Lukas Ruff*

Main category: cs.CV

TL;DR: They model magnification sampling as domain adaptation, show discrete uniform sampling causes gaps at intermediate magnifications, propose continuous sampling and optimized distributions, create TCGA-MS and BRACS-MS benchmarks, and demonstrate improved accuracy (up to 4 pts) at intermediate magnifications.


<details>
  <summary>Details</summary>
Motivation: To understand how magnification sampling during training affects pathology foundation models across different magnifications and to improve performance at intermediate scales.

Method: The paper models magnification sampling as a multi-source domain adaptation problem and develops a theoretical framework.

Result: Continuous magnification sampling and optimized sampling distributions improve performance, especially at intermediate magnifications; benchmarks TCGA-MS and BRACS-MS created; magnification is a major source of performance variation across models.

Conclusion: Continuous sampling and optimized distributions yield more reliable performance across magnifications; magnification is a key driver of model variation; their work enables more robust pathology foundation models.

Abstract: In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.

</details>


### [150] [Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules](https://arxiv.org/abs/2601.02203)
*Oliver Custance,Saad Khan,Simon Parkinson,Quan Z. Sheng*

Main category: cs.CV

TL;DR: 提出基于CSI-ResNet-A的两阶段框架，通过自监督对比学习预训练和Adapter轻量微调解决域移问题，并用有状态计数器生成稳定占用估计。在WiFlow数据集上10-shot无监督场景MAE=0.44，提出Generalisation Index(GI)量化鲁棒性并在WiAR基准获得98.8%准确率；Adapter微调仅训练97.2%参数且性能仅比全微调低约1%。


<details>
  <summary>Details</summary>
Motivation: 解决WiFi CSI人群计数在不同环境间的域移问题，提升模型在少样本或无标签目标域中的泛化能力，满足隐私保护的IoT应用需求。

Method: 构建CSI-ResNet-A，先用自监督对比学习进行预训练以获得域不变特征，加入轻量Adapter模块用于少量样本（10-shot）高效微调，最后用有状态计数机将事件序列转换为稳定人数估计。

Result: 在WiFlow数据集10-shot无监督场景MAE=0.44；在WiAR公共基准达成98.8%准确率；Adapter微调仅训练3.8%参数（节省97.2%）且性能接近全微调（98.84% vs 99.67%）。

Conclusion: 该框架通过学习域不变表示与Adapter微调，有效缓解域移，实现了高精度、低成本的设备无关WiFi人群计数，适合实际IoT部署。

Abstract: Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.

</details>


### [151] [NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation](https://arxiv.org/abs/2601.02204)
*Huichao Zhang,Liao Qu,Yiheng Liu,Hang Chen,Yangyang Song,Yongsheng Dong,Shikun Sun,Xian Li,Xu Wang,Yi Jiang,Hu Ye,Bo Chen,Yiming Gao,Peng Liu,Akide Liu,Zhipeng Yang,Qili Deng,Linjie Xing,Jiyang Liu,Zhao Wang,Yang Zhou,Mingcong Liu,Yi Zhang,Qian He,Xiwei Hu,Zhongqi Qi,Jie Shao,Zhiye Fu,Shuai Wang,Fangmin Chen,Xuezhi Chai,Zhihua Wu,Yitong Wang,Zehuan Yuan,Daniel K. Du,Xinglong Wu*

Main category: cs.CV

TL;DR: NextFlow是训练在6万亿交错文本-图像token上的统一自回归transformer，采用文本逐token与图像多尺度预测结合策略，实现高速高质量图像及多模态生成，训练配方和前缀调优强化学习策略解决稳定性与策略优化问题。


<details>
  <summary>Details</summary>
Motivation: 提出统一解码器自回归Transformer，处理6万亿交错文本-图像离散token，目的是实现原生多模态理解与生成，包括图像编辑、交错内容与视频生成。

Method: 使用统一视觉表示的解码器自回归Transformer，文本采用传统下一token预测，图像采用下一尺度预测以多尺度生成图像；训练中采用稳健配方解决多尺度生成不稳定，并引入前缀调优用于强化学习优化策略。

Result: NextFlow在统一架构下通过保持文本的逐token预测和图像的多尺度(next-scale)预测，实现生成1024x1024图像仅需5秒，并以统一方式激活多模态能力；在统一模型中达到SOTA，并在视觉质量上可与专用扩散模型相媲美。

Conclusion: 通过统一视觉表示和多尺度生成策略，NextFlow在保持统一解码器架构下同时实现高效和高质量的多模态生成，展示了统一模型可替代部分专用视觉生成器的潜力。

Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.

</details>


### [152] [Seeing the Unseen: Zooming in the Dark with Event Cameras](https://arxiv.org/abs/2601.02206)
*Dachun Kai,Zeyu Xiao,Huyue Zhu,Jiaxiao Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出RetinexEVSR，一种首个事件驱动的低光视频超分框架，结合高对比度事件信号与Retinex先验，通过双向跨模态融合、光照引导的事件增强和事件引导的反射率增强模块在低光条件下恢复细节并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有低光视频超分方法因对比度低、高频信息缺失而难以恢复细节；事件相机在低光下具有高对比度与高时空分辨率，可补充传统RGB的不足，因此将事件信号与Retinex先验结合用于提升低光视频超分性能。

Method: 提出双向跨模态融合策略；光照引导的事件增强模块：利用Retinex模型生成的光照图逐步净化事件特征；事件引导的反射率增强模块：利用增强的事件特征通过多尺度融合动态恢复反射率细节；整体为事件驱动的低光视频超分管线。

Result: 在三套数据集上取得SOTA，尤其在SDSD上最高提升2.95 dB和65%运行时间降低；代码已开源。

Conclusion: RetinexEVSR在三套数据集上实现了最先进性能，在SDSD基准上相比此前事件方法提高最多2.95 dB并降低65%运行时间，证明事件信号与Retinex先验在低光视频超分任务中的有效性与实用性。

Abstract: This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.

</details>


### [153] [Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion](https://arxiv.org/abs/2601.02211)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: Systematically intervened on blocks of Multimodal Diffusion Transformers by removing/disabling/enhancing textual hidden-states to reveal block roles; early blocks encode semantics, late blocks refine details; selective text enhancement aids alignment; propose training-free methods improving metrics on SD3.5


<details>
  <summary>Details</summary>
Motivation: Existing component-level analyses (e.g., positional encoding, attention) are insufficient to explain how different blocks and their interaction with text conditions shape generation; need systematic block-level causal study

Method: Blockwise causal analysis of MMDiT models

Result: Found semantic content emerges in early blocks, fine details in late blocks; removing blocks less harmful than disabling text conditions; enhancing text conditions in selective blocks boosts semantic attributes; derived training-free strategies that improve text alignment, editing precision, and speed

Conclusion: Blockwise analysis yields mechanistic insights enabling practical, training-free improvements for alignment, editing, and acceleration across T2I tasks

Abstract: Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.

</details>


### [154] [Prior-Guided DETR for Ultrasound Nodule Detection](https://arxiv.org/abs/2601.02212)
*Jingjing Wang,Zhuo Xiao,Xinning Yao,Bo Liu,Lijuan Niu,Xiangzhi Bai,Fugen Zhou*

Main category: cs.CV

TL;DR: Paper proposes prior-guided DETR for ultrasound nodules: spatially-adaptive deformable FFN with prior regularization, multi-scale spatial-frequency mixer, and dense feature interaction to inject geometric and structural priors; shows strong performance on thyroid and breast datasets


<details>
  <summary>Details</summary>
Motivation: Improve ultrasound nodule detection by injecting priors to handle irregular shapes, indistinct boundaries, scale variation, speckle noise

Method: Analyze methods and contributions

Result: Proposed prior-guided DETR with SDFPR, MSFFM, DFI; outperforms 18 detectors on four datasets

Conclusion: Incorporating geometric and structural priors at multiple stages stabilizes detection of challenging ultrasound nodules and boosts performance over existing detectors.

Abstract: Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.

</details>


### [155] [FMVP: Masked Flow Matching for Adversarial Video Purification](https://arxiv.org/abs/2601.02228)
*Duoxun Tang,Xueyi Zhang,Chak Hin Wang,Xi Xiao,Dasen Dai,Xinhang Jiang,Wentao Shi,Rui Li,Qing Li*

Main category: cs.CV

TL;DR: FMVP通过掩膜物理打碎对抗结构并用条件流匹配+修复重建视频，再配合频率门控损失和双训练范式，有效净化对抗视频并能检测对抗样本，实验证明其在多种攻击下均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基净化采样低效且轨迹弯曲；直接回归难以恢复细微被扰结构，需物理摧毁对抗结构并重建视频动态以提升净化效果。

Method: 提出了Flow Matching for Adversarial Video Purification (FMVP)：1) 用掩膜策略物理打碎全局对抗结构；2) 采用条件流匹配（CFM）与视频修复（inpainting）重建干净视频动力学；3) 设计频率门控损失（FGL）在频域抑制高频对抗残差、保留低频内容；4) 引入Attack-Aware和Generalist训练范式分别应对已知/未知威胁。

Result: 在UCF-101和HMDB-51上，FMVP相较于DiffPure、Defense Patterns、Temporal Shuffling和FlowPure显著提升鲁棒性：对PGD攻击鲁棒准确率>87%，对CW>89%；对自适应攻击DiffHammer也表现优越；作为零样本检测器对PGD/CW的检测准确率分别为98%/79%。

Conclusion: FMVP通过掩膜打碎对抗结构并结合条件流匹配与修复目标，实现了视频对抗样本的高效净化，使得防御模型在标准对抗攻击和自适应攻击下表现优越，并能作为零样本对抗检测器使用。

Abstract: Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.

</details>


### [156] [VIBE: Visual Instruction Based Editor](https://arxiv.org/abs/2601.02242)
*Grigorii Alekseenko,Aleksandr Gordeev,Irina Tolstykh,Bulat Suleimanov,Vladimir Dokholyan,Georgii Fedorov,Sergey Yakubson,Aleksandra Tsybina,Mikhail Chernyshov,Maksim Kuprashevich*

Main category: cs.CV

TL;DR: 本文提出了一个紧凑、高吞吐的基于指令的图像编辑管线，使用2B参数的Qwen3-VL引导和1.6B参数的扩散模型Sana1.5生成图像，在保持输入一致性的同时实现低成本推理，与更大型基线相比在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前开源指令式图像编辑虽发展迅速，但达到实用质量的模型寥寥且扩散骨干通常参数量大、计算成本高，阻碍部署与研究；因此研究小规模且高效的替代方案。

Method: 结合现代2B参数的Qwen3-VL作为编辑引导器和1.6B参数的Sana1.5扩散生成器，通过在架构、数据处理、训练配置及评估上做出专门设计，实现低成本推理与高源一致性；在ImgEdit和GEdit基准上进行评估。

Result: 在ImgEdit和GEdit基准上，该紧凑管线的表现能匹配甚至超越参数量和推理成本更高的基线模型；可在24GB GPU内存中运行，于NVIDIA H100上以BF16在约4秒内生成2K分辨率编辑图像。

Conclusion: 在保持严格源图像一致性和低推理成本的前提下，采用小规模视觉语言模型（2B）与轻量扩散模型（1.6B）能实现与大模型相当或更好的图像编辑效果，尤其在属性调整、对象移除、背景编辑和目标替换等需保留输入的编辑任务上表现突出。

Abstract: Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.

</details>


### [157] [A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets](https://arxiv.org/abs/2601.02246)
*Annoor Sharara Akhand*

Main category: cs.CV

TL;DR: Across five real-world datasets, fine-tuning pre-trained CNNs gives best predictive performance; compact custom CNNs trained from scratch give good efficiency with reasonable accuracy; using pre-trained models only as fixed feature extractors is less effective.


<details>
  <summary>Details</summary>
Motivation: Practitioners need guidance on choosing between training compact CNNs, using pre-trained CNNs as feature extractors, or fine-tuning pre-trained backbones for varied, small-to-medium real-world image datasets; objective is to quantify trade-offs in predictive performance and computational efficiency.

Method: Controlled empirical comparison of CNN paradigms for specialized image classification

Result: Transfer learning (partial/full fine-tuning) consistently achieved highest accuracy and macro F1 across five diverse datasets. Custom compact CNNs trained from scratch offered favorable efficiency–accuracy trade-offs with much lower parameter counts and faster per-epoch training, while fixed feature extractors were intermediate but less competitive than transfer learning.

Conclusion: For small-to-medium specialized image classification tasks, prefer transfer learning when predictive performance is primary; choose compact custom CNNs when compute/memory constraints demand efficiency; fixed pre-trained feature extraction can be a quick but suboptimal alternative.

Abstract: Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.

</details>


### [158] [SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection](https://arxiv.org/abs/2601.02249)
*Xiantai Xiang,Guangyao Zhou,Zixiao Wen,Wenshuai Li,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuhan Liu,Zongxu Pan,Yuxin Hu*

Main category: cs.CV

TL;DR: 本文提出SLGNet，一种在冻结ViT基础模型上通过结构感知适配器和语言引导调制实现高效多模态（RGB+红外）目标检测的方法。该方法同时解决了跨模态结构一致性丧失和缺乏环境感知的静态融合问题，在多个数据集上达到最先进性能，同时大幅减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的方法虽然高效，但在跨模态结构一致性和环境自适应方面存在不足，导致在域差异大（如夜间、高对比）场景下结构信息丢失和融合失效，因此需要一种兼顾效率与鲁棒性的解决方案。

Method: 在冻结的ViT骨干上插入Structure-Aware Adapter提取并注入RGB和IR的分层结构表示；设计Language-Guided Modulation模块，利用视觉语言模型生成的结构化caption作为环境先验，动态重校准视觉特征；整体采用参数高效的适配器式微调策略。

Result: 在LLVIP、FLIR、KAIST、DroneVehicle数据集上进行大量实验，SLGNet在LLVIP上达到66.1 mAP，并相比完全微调减少约87%可训练参数，整体性能优于现有方法并证明了其效率和鲁棒性。

Conclusion: SLGNet通过结构感知适配器注入分层结构先验和通过VLM生成的结构化文本进行语言引导调制，显著改善了在高对比和夜间等复杂场景下的检测表现，并在LLVIP等数据集上刷新SOTA，同时将可训练参数减少约87%。

Abstract: Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.

</details>


### [159] [VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation](https://arxiv.org/abs/2601.02256)
*Shikun Sun,Liao Qu,Huichao Zhang,Yiheng Liu,Yangyang Song,Xian Li,Xu Wang,Yi Jiang,Daniel K. Du,Xinglong Wu,Jia Jia*

Main category: cs.CV

TL;DR: Introduce conflict-aware GRPO with intermediate rewards, time-step reweighting, and ReFL-based mask propagation to stabilize RL training for VARs, yielding better sample quality and alignment.


<details>
  <summary>Details</summary>
Motivation: VAR models face asynchronous policy conflicts due to heterogeneous input structures across generation steps, causing unstable RL training and poor alignment.

Method: Combine stabilizing intermediate rewards, dynamic time-step reweighting for credit assignment, and a mask propagation algorithm inspired by Reward Feedback Learning to isolate optimization effects spatially and temporally.

Result: The proposed framework enhances GRPO using stabilizing intermediate rewards, dynamic time-step reweighting, and mask propagation via ReFL, improving sample quality and alignment over vanilla GRPO.

Conclusion: The method resolves asynchronous policy conflicts and achieves robust, effective optimization for VAR models, outperforming baseline GRPO.

Abstract: Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.

</details>


### [160] [DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies](https://arxiv.org/abs/2601.02267)
*Renke Wang,Zhenyu Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: DiffProxy利用扩散生成先验生成多视角一致且像素对齐的人体代理，配合手部细化和不确定性感知测试时调整，在仅用合成数据训练下实现对真实数据的SOTA零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 真实数据的标注不完美会在训练时引入偏差，而合成数据虽有精确监督但存在域差异。需要一种方法既能利用合成数据的精确标签，又能在真实世界中泛化良好。

Method: 采用扩散模型作为生成先验，设计了多条件机制以生成像素对齐的多视角一致代理；加入手部细化模块以视觉提示增强局部细节；以及不确定性感知的测试时尺度调整以在优化时提高鲁棒性。训练完全基于合成数据并用于引导网格恢复。

Result: 在五个真实世界基准上达到最先进性能，特别是在遮挡和部分视角等挑战性场景下展现出强大的零样本（zero-shot）泛化能力。

Conclusion: DiffProxy通过扩散模型生成多视角一致的人体代理（proxy），在仅用合成数据训练的情况下实现对真实世界数据的强泛化，尤其在遮挡和部分视角等困难场景表现优异。

Abstract: Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html

</details>


### [161] [TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation](https://arxiv.org/abs/2601.02273)
*Salim Khazem*

Main category: cs.CV

TL;DR: TopoLoRA-SAM 使用 LoRA + 空间卷积适配器并结合可微 clDice，实现拓扑感知的参数高效 SAM 适配，在薄结构和噪声模态分割上以低参数成本超越或匹配完全微调的专用模型。


<details>
  <summary>Details</summary>
Motivation: SAM 零样本泛化能力强，但在领域特定分割（尤其细长结构与高噪声模态）上表现不足；完全微调代价高且易遗忘，需低成本且保拓扑信息的适配方法。

Method: 在冻结的 ViT 编码器中注入 LoRA，同时加入轻量级空间卷积适配器，并可选地使用可微的 clDice 作为拓扑监督，形成参数高效的二值语义分割适配框架。

Result: 在五个基准（DRIVE、STARE、CHASE_DB1、Kvasir-SEG、SL-SSDD）上对比 U-Net 等主流模型，TopoLoRA-SAM 在视网膜数据集上取得最佳平均 Dice，并在整体数据集上达成最好平均 Dice，训练仅调优约 5.2%（~4.9M）参数，在 CHASE_DB1 上提升明显。

Conclusion: TopoLoRA-SAM 在保持参数高效的同时，针对薄结构与噪声模态显著提升了分割性能，尤其在视网膜血管数据集上达到最优或接近最优结果，说明拓扑感知与 LoRA 结合的适配策略可替代完全微调的专用模型。

Abstract: Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git

</details>


### [162] [InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams](https://arxiv.org/abs/2601.02281)
*Shuai Yuan,Yantai Yang,Xiaotian Yang,Xupeng Zhang,Zhonghao Zhao,Lingming Zhang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: 提出InfiniteVGGT：一种因果视觉几何Transformer，使用有界可适应的KV缓存作为滚动记忆，并引入无训练、与注意力无关的裁剪策略实现长时稳定的无限输入流处理；同时发布Long3D基准用于约10k帧的连续3D几何评估。


<details>
  <summary>Details</summary>
Motivation: 解决离线大模型（如VGGT）无法实时应用与现有流式方法无法支持真正无限时域或在长序列上出现灾难性漂移的矛盾，追求可扩展且长期稳定的实时3D几何理解系统。

Method: 提出基于有界但自适应的KV缓存的因果视觉几何Transformer，结合训练免费且注意力无关的键值裁剪策略（基于信息陈旧度判定）实现记忆前滚；与FlashAttention兼容以保障计算效率。

Result: 实现了在无限时域流处理下优于现有流式方法的长时稳定性；发布Long3D数据集以评估约10k帧的连续性能；代码开源。

Conclusion: InfiniteVGGT通过滚动KV缓存与智能裁剪，在保证与FlashAttention兼容的同时，实现了无限时域的流式3D几何估计，并在长序列稳定性上优于现有流式方法；Long3D基准填补了长期连续评估的空白。

Abstract: The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

</details>


### [163] [Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2601.02289)
*Tom Burgert,Leonard Hackel,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 提出GeoRank，通过最小化球面距离正则化将地理相似性嵌入对比SSL表征，能提升多光谱遥感表征并改进多种SSL算法；同时分析了关键实验变量对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 遥感影像存在显著的地理和时间变化，现有SSL方法未充分利用地理元数据或不能直接在表征空间体现地理相似性，因此提出GeoRank以显式编码地理关系。

Method: 在对比自监督框架中引入基于球面距离的正则项（GeoRank），直接对地理位置相似性进行约束，能与多种对比SSL算法（如BYOL、DINO）结合。

Result: GeoRank在多个设置中优于或不逊于已有整合地理元数据的方法，并在不同对比SSL算法上带来一致提升；论文还系统研究了数据增强、数据集规模、图像尺寸与时序视图对多光谱遥感SSL的影响。

Conclusion: GeoRank通过优化球面距离将地理关系嵌入对比自监督表征空间，从而在多光谱遥感图像上有效提升表征质量。

Abstract: Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.

</details>


### [164] [SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting](https://arxiv.org/abs/2601.02299)
*Sara Inácio,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 引入真实回收厂密集标注数据集SortWaste与ClutterScore难度度量，并通过基准测试揭示现有检测器在高杂乱环境下性能不足，推动更具挑战性的数据与算法研究。


<details>
  <summary>Details</summary>
Motivation: 缺乏真实废物分拣数据集与客观难度度量，导致自动化分拣算法在高杂乱、遮挡和视觉复杂性场景下表现不足。

Method: 在真实回收分拣厂采集图像并进行密集标注，定义ClutterScore指标（结合物体数量、类别与尺寸熵、空间重叠等代理量）来量化场景难度；对多种先进目标检测模型在不同难度场景上进行系统评测。

Result: 发布了SortWaste数据集并提出ClutterScore，基准测试显示在塑料检测任务上最高mAP为59.7%，但在高杂乱场景性能显著下降，表明当前方法在复杂实际场景中仍需改进。

Conclusion: 本文提出了SortWaste数据集和ClutterScore难度度量，并通过基准评测展示了现有目标检测器在废物分拣场景的性能与局限。

Abstract: The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.

</details>


### [165] [360DVO: Deep Visual Odometry for Monocular 360-Degree Camera](https://arxiv.org/abs/2601.02309)
*Xiaopeng Guo,Yinzhe Xu,Huajian Huang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 本文提出360DVO，一种首个基于深度学习的单目全景视觉里程计框架，包含DAS-Feat畸变感知球面特征提取器和可微分全景束束调整(ODBA)。在新建真实数据集及合成数据上优于现有方法，稳健性和精度显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于手工特征或光度目标的方法在激烈运动和光照变化等困难场景下鲁棒性不足，需引入学习型特征并结合全景几何约束提升性能。

Method: 提出畸变感知球面特征提取器（DAS-Feat）以自适应学习对全景畸变鲁棒的稀疏特征补丁；利用这些特征在新颖的可微分全景束束调整(ODBA)模块中建立约束以估计位姿。整体构成端到端学习的OVO框架360DVO。

Result: 在作者新建的真实全景VO基准以及TartanAir V2和360VO合成数据集上，360DVO相比最优基线提高稳健性约50%、位姿精度约37.5%。

Conclusion: 360DVO在真实与合成基准上均显著超越现有全景VO方法（如360VO、OpenVSLAM），提高稳健性50%和精度37.5%，证明深度学习特征+ODBA对全景VO的有效性。

Abstract: Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage

</details>


### [166] [Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping](https://arxiv.org/abs/2601.02315)
*Saurabh Kaushik,Lalit Maurya,Beth Tellman*

Main category: cs.CV

TL;DR: 本文提出Prithvi-CAFE，一种将预训练Geo-Foundation模型Prithvi的编码器与并行CNN残差分支（含卷积注意力模块）融合的方法，旨在在洪水遥感分割任务中补偿GFM在局部细节上的不足，通过多尺度多层特征融合与适配器实现高效微调。在Sen1Flood11与FloodPlanet数据集上均取得SOTA成绩，尤其在保留长程依赖同时增强局部细节方面明显优于原Prithvi、U-Net等基线。


<details>
  <summary>Details</summary>
Motivation: 尽管Geo-Foundation Models能捕捉长程依赖和语义信息，但在需要精细局部分辨率的任务（如洪水边界分割）上往往不如专门的U-Net等CNN结构，作者希望通过结合两者优势提升性能与泛化。

Method: 在Prithvi编码器旁并行加入一个CNN残差分支，残差分支通过Convolutional Attention Modules(CAM)增强局部表示；在Prithvi中使用适配器实现快速高效微调；对多尺度、多层特征进行融合（CAFE模块），综合利用长程上下文与局部细节用于分割预测。

Result: 在Sen1Flood11测试集上，Prithvi-CAFE IoU为83.41，优于Prithvi(82.50)、TerraMind(82.90)、DOFA(81.54)、spectralGPT(81.02)。在hold-out测试站点上，Prithvi-CAFE IoU为81.37，明显超过U-Net(70.57)与Prithvi(72.42)。在FloodPlanet上，Prithvi-CAFE IoU为64.70，也优于U-Net(60.14)、Terramind(62.33)、DOFA(59.15)和Prithvi 2.0(61.91)。

Conclusion: Prithvi-CAFE能显著改善GFM在洪水分割中的局部细节建模，提升IoU与泛化能力，在两个数据集上均优于原Prithvi和U-Net，证明将GFMs与轻量CNN分支结合并进行多尺度融合是有效策略。

Abstract: Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}

</details>


### [167] [Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching](https://arxiv.org/abs/2601.02318)
*Roja Sahoo,Anoop Namboodiri*

Main category: cs.CV

TL;DR: 提出F2P：融合闪光与非闪光两种无接触指纹，利用注意力融合+U-Net增强并训练跨域嵌入，显著提升脊线可视性与识别性能。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹在卫生和便捷性上有优势，但受照明、皮下色素和反射影响导致脊线模糊。闪光与非闪光各有利弊，融合两者可同时保留脊线细节并抑制噪声，从而提升识别性能。

Method: 1) 构建FNF成对闪光/非闪光数据集；2) 手动相减得到脊线保留信号作为参考；3) 轻量注意力融合网络整合两模态特征；4) U-Net增强模块生成最优灰度图；5) 跨域兼容深度嵌入用于验证比对。

Result: F2P提出通过融合同一手指的闪光与非闪光无接触指纹图像以增强脊线清晰度并提高识别性能。作者构建了专门的数据集FNF Database，并通过手工闪光-非闪光相减隔离出脊线保留信号。方法包括轻量注意力融合网络和U-Net增强模块，最终生成最优灰度图像；再经跨域兼容的深度嵌入模型产生统一嵌入用于与接触式指纹比对。实验显示F2P在脊线清晰度和识别指标上优于单一采集（AUC=0.999，EER=1.12%）。

Conclusion: F2P通过系统采集并融合闪光/非闪光图像，能更好保留脊线细节并生成与接触式指纹兼容的判别嵌入，从而在验证任务中显著优于传统单模态方法。

Abstract: Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).

</details>


### [168] [BEDS: Bayesian Emergent Dissipative Structures](https://arxiv.org/abs/2601.02329)
*Laurent Caraffa*

Main category: cs.CV

TL;DR: 将学习等同于耗散结构形成，贝叶斯推断与耗散过程同构；并声称基本常数可由贝叶斯固定点导出，提出哥德尔不完备性与耗散缺陷的类比；实证上给出P2P架构节能6个数量级并支持持续学习。


<details>
  <summary>Details</summary>
Motivation: 寻求将物理学、信息论和机器学习统一，解释学习为何能在开放耗散系统中发生并设计更高效可持续的学习系统。

Method: 通过建立热力学耗散结构与贝叶斯更新的形式同构，导出固定点解并用架构实现做为工程验证，同时提出理论性推测连接逻辑不完备性与耗散限制。

Result: BEDS提出将学习视为通过熵输出将能流转化为结构的过程，结合不可逆热力学与贝叶斯更新构建统一框架。

Conclusion: 论文提供了一个大胆跨学科假设框架，若严格成立将影响我们对学习、计算及物理系统限制的理解，但当前论证在数学严谨性、推导细节和实证可复现性方面存疑，需要补强证明与实验复现。

Abstract: We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.

</details>


### [169] [Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding](https://arxiv.org/abs/2601.02339)
*Jingming He,Chongyi Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出一种联合增强框架：使用基于Laplace-Beltrami的各向异性三维高斯Chebyshev描述子捕获细致形状；根据局部语义和形状信号自适应分配高斯与SH；跨场景知识迁移加速收敛，提升分割与渲染


<details>
  <summary>Details</summary>
Motivation: Address shortcomings of 3DGS semantic rendering methods that separate semantic and rendering branches and rely on 2D supervision and rendering gradients, leading to issues in subtle/textureless regions

Method: Analyze method and contributions

Result: Anisotropic 3D Gaussian Chebyshev descriptor using Laplace-Beltrami, adaptive allocation of Gaussians and spherical harmonics guided by semantic and shape signals, cross-scene knowledge transfer module; shows better segmentation and rendering with high FPS

Conclusion: 该方法通过结合语义与形状信息改进高斯建模与资源分配，并利用知识迁移提升效率与鲁棒性，在多数据集上提高分割精度与渲染质量，同时保持高渲染帧率。

Abstract: Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.

</details>


### [170] [Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices](https://arxiv.org/abs/2601.02353)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.CV

TL;DR: Proposes DACIS to score channel importance for disease discrimination within a PMP pipeline, enabling large pruning while supporting few-shot adaptation; achieves strong compression and near-original accuracy on plant disease datasets, feasible for Raspberry Pi deployment.


<details>
  <summary>Details</summary>
Motivation: Enable accurate, efficient plant disease classification on low-cost edge devices with limited labeled data by reducing model size while preserving task-relevant features.

Method: Prune-then-Meta-Learn-then-Prune (PMP) pipeline combining DACIS

Result: 78% model size reduction, retaining 92.3% accuracy; compressed model runs at 7 FPS on Raspberry Pi 4; validated on PlantVillage and PlantDoc.

Conclusion: DACIS-guided PMP achieves high compression with minimal performance loss, making on-device, few-shot plant disease diagnosis practical for resource-constrained farmers.

Abstract: Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.

</details>


### [171] [Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes](https://arxiv.org/abs/2601.02356)
*Jing Tan,Zhaoyang Zhang,Yantao Shen,Jiarui Cai,Shuo Yang,Jiajun Wu,Wei Xia,Zhuowen Tu,Stefano Soatto*

Main category: cs.CV

TL;DR: Talk2Move uses RL and diffusion to perform text-instructed translation, rotation, and scaling of objects without paired data, leveraging group policy optimization and spatial rewards for accurate, interpretable edits.


<details>
  <summary>Details</summary>
Motivation: This paper aims to enable object-level geometric transformations in images driven by natural language, addressing limitations of current multimodal generation methods that focus on appearance or style changes and lack supervision for geometry edits.

Method: Combines diffusion-based generative model with RL (GRPO) to explore geometric action space via diverse rollouts from images and textual variations; uses spatial reward model, off-policy step evaluation, active step sampling, and object-centric displacement/rotation/scale rewards.

Result: Introduces Talk2Move, an RL-based diffusion framework using Group Relative Policy Optimization, spatial reward guided model, off-policy step evaluation and active step sampling, plus object-centric spatial rewards, achieving improved spatial accuracy and scene coherence on benchmarks.

Conclusion: Talk2Move effectively performs semantically faithful object-level spatial transformations from text, outperforming prior methods in spatial accuracy and scene coherence, while reducing need for paired data.

Abstract: We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.

</details>


### [172] [VINO: A Unified Visual Generator with Interleaved OmniModal Context](https://arxiv.org/abs/2601.02358)
*Junyi Chen,Tong He,Zhoujie Fu,Pengfei Wan,Kun Gai,Weicai Ye*

Main category: cs.CV

TL;DR: 提出VINO，一个在单一框架下进行图像和视频生成/编辑的统一模型，通过将视觉语言模型与多模态扩散Transformer结合，并用交错条件令牌指导扩散，支持多模态输入输出与多参考、多身份控制。


<details>
  <summary>Details</summary>
Motivation: 减少针对每种模态训练不同模型或独立模块的需求，构建一个能同时处理文本、图像和视频并执行生成与编辑任务的统一系统，以提高可扩展性与多任务一致性。

Method: 将视觉语言模型与多模态扩散变换器(MMDiT)耦合，使用交错的条件令牌表示文本、图像和视频输入；引入多阶段训练流程，从视频生成基模型逐步扩展为支持图像/视频输入输出的多任务生成器。

Result: VINO: unified visual generator combining image/video generation and editing in one model; uses shared diffusion backbone conditioned on text/images/videos; couples VLM with Multimodal Diffusion Transformer; interleaved conditioning tokens; supports multi-reference grounding, long-form instruction following, identity preservation; multi-stage training pipeline expanding video base to unified generator; shows strong visual quality and controllability across benchmarks.

Conclusion: VINO展示了通过交错的、上下文式多模态条件来实现可扩展的统一视觉生成，能够在不依赖模态专用结构的前提下实现高质量生成和更可控的多身份编辑。

Abstract: We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.

</details>


### [173] [ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors](https://arxiv.org/abs/2601.02359)
*Kaede Shiohara,Toshihiko Yamasaki,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 提出基于扩散模型且完全自监督的面部伪造检测方法，通过个性化参考集的扩散重建误差来衡量身份距离，从而实现对未知伪造的泛化检测；在多数据集上优于现有最优方法，并对新生成器和图像扰动表现鲁棒。


<details>
  <summary>Details</summary>
Motivation: 当前监督方法依赖已有伪造样本导致对未知伪造泛化差；自监督方法潜力大但难以学到判别性表示。作者通过将扩散生成的音频驱动表情合成与个性化重建误差结合，提出了一种无需伪造样本即可检测未知伪造的思路。

Method: 使用基于扩散的生成模型从音频合成面部表情序列，并对模型进行个性化微调（或使用参考集）以学习特定主体的表达特征。通过计算在给定主体参考集下重建输入视频时的扩散重建误差，得到身份距离，用于判定是否为该主体的真实视频。整个流程为完全自监督，无需伪造样本训练。

Result: ExposeAnyone提出了一种完全自监督的基于扩散模型的方法，通过从音频生成面部表情序列并利用针对特定主体的个性化参考集进行重建误差比较，实现对人脸伪造（face forgery）的检测。该方法在四个数据集（DF-TIMIT、DFDCP、KoDF、IDForge）上平均AUC提高了4.22个百分点，并能检测到Sora2生成的视频，同时对模糊和压缩等常见扰动具有较强鲁棒性。

Conclusion: ExposeAnyone能在无需伪造样本监督的条件下，通过个性化扩散重建误差有效区分真实与伪造视频，显著提升对未知伪造的泛化能力，并具备较强的现实世界鲁棒性。

Abstract: Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [174] [A formal query language and automata model for aggregation in complex event recognition](https://arxiv.org/abs/2601.00967)
*Pierre Bourhis,Cristian Riveros,Amaranta Salas*

Main category: cs.DB

TL;DR: 本文将聚合引入复杂事件逻辑，给出新的袋语义，定义ACEL语言和ACEA自动机，证明可表达性和模型更强的表达能力。


<details>
  <summary>Details</summary>
Motivation: 当前CER查询语言对聚合支持语法受限且语义未定义，现实应用（如金融、传感器网络）中聚合是关键需求，故需要一个具有明确定义语义与良好表达与计算模型的聚合扩展。

Method: 通过将CEL的语义从位置集合改为元组袋，引入对任意交换幺半群的聚合算子构建ACEL；设计扩展的自动机模型ACEA以支持聚合与过滤，并形式化证明了从ACEL到ACEA的可翻译性与表达性比较。

Result: 提出ACEL语言与ACEA自动机模型；确立基于元组袋的新语义；证明ACEL可用ACEA表达且ACEA表达能力强于ACEL；并通过示例展示ACEL对实际查询的自然性。

Conclusion: 本文提出并形式化了在复杂事件识别（CER）中引入聚合操作的查询语言扩展，给出新的基于元组袋（bag of tuples）的语义，定义了ACEL语言并构造了相应的自动机模型ACEA，证明了ACEL可被ACEA表示且ACEA在表达能力上超出ACEL。

Abstract: Complex Event Recognition (CER) systems are used to identify complex patterns in event streams, such as those found in stock markets, sensor networks, and other similar applications. An important task in such patterns is aggregation, which involves summarizing a set of values into a single value using an algebraic function, such as the maximum, sum, or average, among others. Despite the relevance of this task, query languages in CER typically support aggregation in a restricted syntactic form, and their semantics are generally undefined.
  In this work, we present a first step toward formalizing a query language with aggregation for CER. We propose to extend Complex Event Logic (CEL), a formal query language for CER, with aggregation operations. This task requires revisiting the semantics of CEL, using a new semantics based on bags of tuples instead of sets of positions. Then, we present an extension of CEL, called Aggregation CEL (ACEL), which introduces an aggregation operator for any commutative monoid operation. The operator can be freely composed with previous CEL operators, allowing users to define complex queries and patterns. We showcase several queries in practice where ACEL proves to be natural for specifying them. From the computational side, we present a novel automata model, called Aggregation Complex Event Automata (ACEA), that extends the previous proposal of Complex Event Automata (CEA) with aggregation and filtering features. Moreover, we demonstrate that every query in ACEL can be expressed in ACEA, illustrating the effectiveness of our computational model. Finally, we study the expressiveness of ACEA through the lens of ACEL, showing that the automata model is more expressive than ACEL.

</details>


### [175] [Grain-Aware Data Transformations: Type-Level Formal Verification at Zero Computational Cost](https://arxiv.org/abs/2601.00995)
*Nikos Karayannidis*

Main category: cs.DB

TL;DR: 这篇论文把“粒度”（数据颗粒）引入类型理论，给出数学定义和三种关系（相等、可比较、不可比较），并证明了一个可推断连接后输出粒度的定理，从而在编译时验证数据转换正确性，防止fan/chasm陷阱并用Lean4形式化证明，显著降低验证成本。


<details>
  <summary>Details</summary>
Motivation: 传统数据工程在验证数据转换正确性时依赖昂贵的测试、数据物化和人工排错，缺乏形式化的粒度推理方法导致fan trap和chasm trap等问题。需要一种能够在编译时证明转换正确性的通用方法以降低成本并提升自动化与可信度。

Method: 提出粒度的类型理论定义，定义三种粒度关系（相等、排序、不可比较），给出一组关系代数和推断规则，特别证明了一个一般的粒度推断定理用于等值连接的输出，通过模式化的类型级操作和模式匹配覆盖所有连接场景；同时在Lean4中给出机器可检验的形式证明和自动化工具链。

Result: 实现了类型级的粒度推断与验证规则，能在不执行数据的情况下通过模式分析发现粒度相关错误；在Lean4中机器证明并实验证明可将验证成本降低约98-99%，并展示LLM可生成证明草案以进一步降低人工负担。

Conclusion: 将粒度作为类型信息嵌入类型系统并用类型级运算推断连接输出粒度，可以在编译时验证数据管道的正确性，检测fan trap、chasm trap和聚合问题，结合Lean4机器检查证明和LLM辅助证明生成，显著降低验证成本并提高部署信心。

Abstract: Data transformation correctness is a major challenge in data engineering: how to verify pipeline accuracy before deployment. Traditional methods involve costly iterative testing, data materialization, and manual error detection, due to the lack of formal approaches to reasoning about data granularity (grain), which can shift during transformations, causing issues like fan traps (metrics duplication) and chasm traps (data loss). We introduce the first formal, mathematical definition of grain, extending it from an informal concept in dimensional modeling to a universal, type-theoretic framework applicable to any data type. Encoding grain into the type system allows compile-time verification of transformation correctness, shifting validation from runtime. We define three core grain relations-equality, ordering, and incomparability-and prove a general grain inference theorem that computes the output grain of equi-joins from input grains using type-level operations. This covers all join scenarios, including comparable and incomparable keys. Together with inference rules for relational operations, this enables verification through schema analysis alone, at zero cost. Our approach allows engineers to verify that entire pipeline DAGs maintain correctness properties, detecting grain-related errors such as fan traps, chasm traps, and aggregation issues before data processing. It emphasizes the importance of grain, focusing on critical characteristics rather than all data details. We provide machine-checked formal proofs in Lean 4, reducing verification costs by 98-99%. Additionally, large language models can automatically generate correctness proofs, shifting human effort from proof writing to proof verification, thus democratizing formal methods in data engineering and supporting confident deployment of AI-generated pipelines with machine-checkable guarantees.

</details>


### [176] [Entity-Aware and Secure Query Optimization in Database Using Named Entity Recognition](https://arxiv.org/abs/2601.01254)
*Azrin Sultana,Hasibur Rashid Chayon*

Main category: cs.DB

TL;DR: 提出将NER用于查询敏感信息检测，结合DBN-LSTM模型、AES+盲索引对敏感数据加密并优化加密搜索，同时对非敏感数据用K-means分组和排序检索，显著提升隐私保护与检索效率（DBN-LSTM准确率93%）


<details>
  <summary>Details</summary>
Motivation: Automatic detection of sensitive information in queries before encryption to reduce manual errors, speed up query processing, and lower privacy risks in cloud storage

Method: intelligent privacy-preserving query optimization with NER and encryption

Result: DBN-LSTM NER achieved ~93% accuracy and 94% precision; AES with blind indexing enabled fast encrypted search; K-means clustering and rank search improved non-sensitive data retrieval compared to traditional methods

Conclusion: 结合敏感检测、加密与并行查询优化，可在云环境中实现高效且隐私保护的数据库检索，特别是DBN-LSTM用于NER和盲索引加速的加密搜索表现突出。

Abstract: Cloud storage has become the backbone of modern data infrastructure, yet privacy and efficient data retrieval remain significant challenges. Traditional privacy-preserving approaches primarily focus on enhancing database security but fail to address the automatic identification of sensitive information before encryption. This can dramatically reduce query processing time and mitigate errors during manual identification of sensitive information, thereby reducing potential privacy risks. To address this limitation, this research proposes an intelligent privacy-preserving query optimization framework that integrates Named Entity Recognition (NER) to detect sensitive information in queries, utilizing secure data encryption and query optimization techniques for both sensitive and non-sensitive data in parallel, thereby enabling efficient database optimization. Combined deep learning algorithms and transformer-based models to detect and classify sensitive entities with high precision, and the Advanced Encryption Standard (AES) algorithm to encrypt, with blind indexing to secure search functionality of the sensitive data, whereas non-sensitive data was divided into groups using the K-means algorithm, along with a rank search for optimization. Among all NER models, the Deep Belief Network combined with Long Short-Term Memory (DBN-LSTM) delivers the best performance, with an accuracy of 93% and precision (94%), recall, and F1 score of 93%, and 93%, respectively. Besides, encrypted search achieved considerably faster results with the help of blind indexing, and non-sensitive data fetching also outperformed traditional clustering-based searches. By integrating sensitive data detection, encryption, and query optimization, this work advances the state of privacy-preserving computation in modern cloud infrastructures.

</details>


### [177] [Curator: Efficient Vector Search with Low-Selectivity Filters](https://arxiv.org/abs/2601.01291)
*Yicheng Jin,Yongji Wu,Wenjun Hu,Bruce M. Maggs,Jun Yang,Xiao Zhang,Danyang Zhuo*

Main category: cs.DB

TL;DR: 提出了 Curator：一种与图索引互补的分区索引，针对低选择性带过滤的近似最近邻搜索（ANNS）场景，通过为不同标签构建共享聚类树下的专用索引来避免图索引在稀疏子集上的连通性崩溃，同时支持增量更新和临时索引以处理复杂谓词。评测显示，与预过滤回退相比，可将延迟最多降低20.9倍，构建时间和内存仅分别增加5.5%和4.3%。


<details>
  <summary>Details</summary>
Motivation: 图索引在无过滤或高选择性过滤的ANNS中表现优越，但在低选择性过滤下因合格向量稀疏导致图结构碎片化、连通性崩溃；已有方案通过增加图度数来恢复连通性，但带来高昂的构建成本。因此需要一种与图索引互补且成本更低的方案。

Method: 提出双索引架构：保留现有图索引用于高选择性查询，新增基于分区的Curator索引用于低选择性过滤查询。Curator在共享的聚类树上为每个标签构建专用索引，依据各标签的向量分布自适应索引结构，支持增量更新并能在运行时高效构建临时索引以处理复杂谓词。

Result: 与现有图索引结合时，Curator在低选择性过滤查询上可将延迟降低最多20.9倍，构建时间增加5.5%，内存占用增加4.3%。

Conclusion: Curator能显著提升低选择性过滤查询的召回效率和延迟表现，且仅带来较小的构建与内存开销，作为图索引的补充方案具有较高实用价值。

Abstract: Embedding-based dense retrieval has become the cornerstone of many critical applications, where approximate nearest neighbor search (ANNS) queries are often combined with filters on labels such as dates and price ranges. Graph-based indexes achieve state-of-the-art performance on unfiltered ANNS but encounter connectivity breakdown on low-selectivity filtered queries, where qualifying vectors become sparse and the graph structure among them fragments. Recent research proposes specialized graph indexes that address this issue by expanding graph degree, which incurs prohibitively high construction costs. Given these inherent limitations of graph-based methods, we argue for a dual-index architecture and present Curator, a partition-based index that complements existing graph-based approaches for low-selectivity filtered ANNS. Curator builds specialized indexes for different labels within a shared clustering tree, where each index adapts to the distribution of its qualifying vectors to ensure efficient search while sharing structure to minimize memory overhead. The system also supports incremental updates and handles arbitrary complex predicates beyond single-label filters by efficiently constructing temporary indexes on the fly. Our evaluation demonstrates that integrating Curator with state-of-the-art graph indexes reduces low-selectivity query latency by up to 20.9x compared to pre-filtering fallback, while increasing construction time and memory footprint by only 5.5% and 4.3%, respectively.

</details>


### [178] [A Tool for Semantic-Aware Spatial Corpus Construction](https://arxiv.org/abs/2601.01415)
*Wei Huang,Xieyang Wang,Jianqiu Xu,Guidong Zhang*

Main category: cs.DB

TL;DR: 提出SSCC工具，通过基于空间关系的知识库构建与模板增强的查询对生成，实现高效、语义一致的空间自然语言查询语料构建。


<details>
  <summary>Details</summary>
Motivation: 现有空间自然语言查询语料稀缺，手工构建或基于模板的动态生成效率低且质量不稳定，迫切需要一种高效且保持语义一致性的语料构建工具。

Method: 方法包括两个模块：1) 基于空间关系的知识库构建模块，从数据集中提取并判定空间关系；2) 模板增强的查询对生成模块，通过模板匹配与参数替换生成自然语言查询与可执行查询对，并保证几何一致性与空间逻辑。

Result: 实验表明SSCC在知识库构建上效率提升53倍，在查询对语料生成上效果提升2.5倍，能显著减少时间和人工成本。

Conclusion: SSCC在保证几何一致性与空间逻辑正确性的前提下，大幅提升了知识库构建和语料生成效率，为空间自然语言接口训练提供高质量语料，显著降低时间与人工成本。

Abstract: Spatial natural language interface to database systems provide non-expert users with convenient access to spatial data through natural language queries. However, the scarcity of high-quality spatial natural language query corpora limits the performance of such systems. Existing methods rely on manual knowledge base construction and template-based dynamic generation, which suffer from low construction efficiency and unstable corpus quality. This paper presents semantic-aware spatial corpus construction (SSCC), a tool designed for constructing high-quality spatial natural language query and executable language query pair corpora. SSCC consists of two core modules: (i) a knowledge base construction module based on spatial relations, which extracts and determines spatial relations from datasets, and (ii) a template-augmented query pair corpus generation module, which produces query pairs via template matching and parameter substitution. The tool ensures geometric consistency and adherence to spatial logic in the generated spatial relations. Experimental results demonstrate that SSCC achieves (i) a 53x efficiency improvement for knowledge base construction and (ii) a 2.5x effectiveness improvement for query pair corpus. SSCC provides high-quality corpus support for spatial natural language interface training, substantially reducing both time and labor costs in corpus construction.

</details>


### [179] [RadixGraph: A Fast, Space-Optimized Data Structure for Dynamic Graph Storage (Extended Version)](https://arxiv.org/abs/2601.01444)
*Haoxuan Xie,Junfeng Liu,Siqiang Luo,Kai Wang*

Main category: cs.DB

TL;DR: RadixGraph通过基数树顶点索引和快照-日志混合边存储，实现了百万级并发更新吞吐和约40%内存节省，比最优基线最高快16.27倍。


<details>
  <summary>Details</summary>
Motivation: 随着真实世界动态图规模增长，需高效的存储与更新机制以支持大规模并发写入并控制内存使用。

Method: 采用指针-数组风格的基数树实现顶点索引，优化了查询与空间的权衡；边存储利用快照-日志混合结构以实现均摊O(1)更新时间。

Result: RadixGraph提出了一种基于基数树的顶点索引和混合快照-日志边存储结构，旨在在大规模动态图中实现高吞吐更新与低内存开销。

Conclusion: RadixGraph在保持图分析性能的同时，显著提高了更新吞吐并降低内存占用，适合需要高频更新与紧凑存储的动态图场景。

Abstract: Dynamic graphs model many real-world applications, and as their sizes grow, efficiently storing and updating them becomes critical. We present RadixGraph, a fast and memory-efficient data structure for dynamic graph storage. RadixGraph features a carefully designed radix-tree-based vertex index that strikes an optimal trade-off between query efficiency and space among all pointer-array-based radix trees. For edge storage, it employs a hybrid snapshot-log architecture that enables amortized $O(1)$ update time. RadixGraph supports millions of concurrent updates per second while maintaining competitive performance for graph analytics. Experimental results show that RadixGraph outperforms the most performant baseline by up to $16.27\times$ across various datasets in ingesting graph updates, and reduces memory usage by an average of $40.1\%$. RadixGraph is open-source at https://github.com/ForwardStar/RadixGraph.

</details>


### [180] [SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses](https://arxiv.org/abs/2601.01888)
*Yifan Wu,Yuhan Li,Zhenhua Wang,Zhongle Xie,Dingyu Yang,Ke Chen,Lidan Shou,Bo Tang,Liang Lin,Huan Li,Gang Chen*

Main category: cs.DB

TL;DR: 本文提出SafeLoad，一种针对云数据仓库中内存过载（MO）查询的入院控制框架，并发布了包含1.5亿条真实查询的开源基准SafeBench。SafeLoad先用可解释的判别规则过滤内存安全查询，再结合全局模型与簇级模型的混合架构及误判修正模块识别MO查询，并通过自调节配额机制按簇动态调整预测配额以提高精度。实验表明，SafeLoad在保持低在线和离线开销的同时，精度较最佳基线提升最多66%，并将CPU浪费减少最多8.09倍。


<details>
  <summary>Details</summary>
Motivation: 数据库查询因内存过载失败会浪费CPU并中断业务流程，现有入院控制方法精度不足且缺乏公开MO标注数据集，导致难以有效预防MO查询造成的资源浪费与失败。

Method: SafeLoad采用三步流程：1）可解释的判别规则用作前置过滤器，快速识别并剔除内存安全查询；2）混合模型架构由一个全局模型与多个簇级模型组成，结合误判修正模块进行精细分类；3）自调节配额管理动态按簇调整预测配额以权衡召回与精度，同时降低在线/离线开销。

Result: 在包含1.5亿条查询的SafeBench上，SafeLoad相较于最优基线最高提升66%精度，并在无需SafeLoad时最多将CPU浪费降低8.09倍；同时保持低延迟的在线预测与合理的离线训练开销。

Conclusion: SafeLoad能显著提高MO查询识别精度与资源利用率，具有可解释性、效率与自适应性，并配套发布了大规模实测数据集SafeBench，便于社区复现与比较。

Abstract: Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.

</details>


### [181] [Vector Search for the Future: From Memory-Resident, Static Heterogeneous Storage, to Cloud-Native Architectures](https://arxiv.org/abs/2601.01937)
*Yitong Song,Xuanhe Zhou,Christian S. Jensen,Jianliang Xu*

Main category: cs.DB

TL;DR: Paper surveys vector search evolution focusing on storage architectures: in-memory methods, SSD-based heterogeneous systems, and emerging cloud-native tiered designs; discusses index/layout/query/update techniques and future research directions for scalable, cost-efficient retrieval.


<details>
  <summary>Details</summary>
Motivation: Vector search must scale to billion/trillion vectors while keeping low latency and cost; storage architecture evolution drives solutions from in-memory to heterogeneous and cloud-native tiers.

Method: Review from storage-architecture perspective

Result: Survey of memory-resident methods (IVF, hashing, quantization, graph), heterogeneous storage designs (SSD offload, block layouts, I/O-efficient query/update), and cloud-native memory-SSD-object architectures; identification of open problems for trillion-scale retrieval.

Conclusion: Heterogeneous and cloud-native architectures are key for scaling VS to trillion vectors; future work needed on tiering strategies, consistency, cost-latency tradeoffs, and system-level integration.

Abstract: Vector search (VS) has become a fundamental component in multimodal data management, enabling core functionalities such as image, video, and code retrieval. As vector data scales rapidly, VS faces growing challenges in balancing search, latency, scalability, and cost. The evolution of VS has been closely driven by changes in storage architecture. Early VS methods rely on all-in-memory designs for low latency, but scalability is constrained by memory capacity and cost. To address this, recent research has adopted heterogeneous architectures that offload space-intensive vectors and index structures to SSDs, while exploiting block locality and I/O-efficient strategies to maintain high search performance at billion scale. Looking ahead, the increasing demand for trillion-scale vector retrieval and cloud-native elasticity is driving a further shift toward memory-SSD-object storage architectures, which enable cost-efficient data tiering and seamless scalability.
  In this tutorial, we review the evolution of VS techniques from a storage-architecture perspective. We first review memory-resident methods, covering classical IVF, hash, quantization, and graph-based designs. We then present a systematic overview of heterogeneous storage VS techniques, including their index designs, block-level layouts, query strategies, and update mechanisms. Finally, we examine emerging cloud-native systems and highlight open research opportunities for future large-scale vector retrieval systems.

</details>


### [182] [AeroSketch: Near-Optimal Time Matrix Sketch Framework for Persistent, Sliding Window, and Distributed Streams](https://arxiv.org/abs/2601.02019)
*Hanyan Yin,Dongxie Wen,Jiajun Li,Zhewei Wei,Xiao Zhang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.DB

TL;DR: 提出AeroSketch，一种基于随机数值线性代数的矩阵草图框架，在持续、滑动窗口和分布式流场景下，在保持最优空间和通信代价的同时，将因子分解从立方时间降到近二次时间（仅差对数因子），并在更新吞吐量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高通量向量流形式的矩阵数据难以完整存储或处理，现有的流式矩阵草图方法虽然在空间和通信上最优，但因频繁且昂贵的矩阵分解导致更新效率低下，尤其在严格误差要求下复杂度为立方，限制了其实时分析能力。

Method: 利用RandNLA的随机化技术对矩阵流进行低秩近似更新，替代频繁的精确矩阵分解；通过设计近似更新算法，使得每次更新时间接近二次复杂度并引入对数因子以控制误差与通信成本，在持久、滑动窗口和分布式设置中分别构造相应的协议以维持最优资源使用。

Result: 在合成与真实数据集上的大量实验表明，AeroSketch在更新吞吐量上持续优于最先进方法，在严格误差约束下把每次矩阵分解的计算从O(n^3)降为O(n^2 polylog n)，同时近似误差、空间和通信成本保持可比。

Conclusion: AeroSketch在严格误差约束下显著提高了更新效率（将立方时间复杂度降至二次级别），同时保持与现有最优方法相当的近似质量和最优的空间/通信开销，适用于多种流式场景。

Abstract: Many real-world matrix datasets arrive as high-throughput vector streams, making it impractical to store or process them in their entirety. To enable real-time analytics under limited computational, memory, and communication resources, matrix sketching techniques have been developed over recent decades to provide compact approximations of such streaming data. Some algorithms have achieved optimal space and communication complexity. However, these approaches often require frequent time-consuming matrix factorization operations. In particular, under tight approximation error bounds, each matrix factorization computation incurs cubic time complexity, thereby limiting their update efficiency.
  In this paper, we introduce AeroSketch, a novel matrix sketching framework that leverages recent advances in randomized numerical linear algebra (RandNLA). AeroSketch achieves optimal communication and space costs while delivering near-optimal update time complexity (within logarithmic factors) across persistent, sliding window, and distributed streaming scenarios. Extensive experiments on both synthetic and real-world datasets demonstrate that AeroSketch consistently outperforms state-of-the-art methods in update throughput. In particular, under tight approximation error constraints, AeroSketch reduces the cubic time complexity to the quadratic level. Meanwhile, it maintains comparable approximation quality while retaining optimal communication and space costs.

</details>


### [183] [Octopus: A Lightweight Entity-Aware System for Multi-Table Data Discovery and Cell-Level Retrieval](https://arxiv.org/abs/2601.02304)
*Wen-Zhi Li,Sainyam Galhotra*

Main category: cs.DB

TL;DR: Octopus利用LLM解析细粒度实体并结合紧凑表头嵌入与表内扫描，实现多表和单元格级检索，无需大量离线预处理且成本低、效果好。


<details>
  <summary>Details</summary>
Motivation: 现有系统假定单表可解并依赖昂贵离线处理，无法高效应对多表（独立或join）和单元格级查询需求，因而需要一个轻量、无训练且能处理多表与单元格检索的新方案。

Method: 用LLM解析自然语言查询以识别列提及和值提及；将列提及匹配到表头通过紧凑嵌入索引；对值提及直接扫描表内容以定位单元格；对检索结果进行精简的NL2SQL执行以减少token和LLM调用。

Result: Octopus提出了一种轻量、无训练、实体感知的多表数据发现与单元格级检索系统，通过基于LLM解析器识别查询中的列与数值实体，并将其匹配到表头和直接扫描表内容，实现不依赖重型离线索引的高效检索。

Conclusion: 细粒度实体对齐与直接内容扫描能显著提升多表检索与单元格查找的准确性，同时大幅减少计算与token开销，适用于独立与基于连接的多表场景。

Abstract: Tabular data constitute a dominant form of information in modern data lakes and repositories, yet discovering the relevant tables to answer user questions remains challenging. Existing data discovery systems assume that each question can be answered by a single table and often rely on resource-intensive offline preprocessing, such as model training or large-scale content indexing. In practice, however, many questions require information spread across multiple tables -- either independently or through joins -- and users often seek specific cell values rather than entire tables. In this paper, we present Octopus, a lightweight, entity-aware, and training-free system for multi-table data discovery and cell-level value retrieval. Instead of embedding entire questions, Octopus identifies fine-grained entities (column mentions and value mentions) from natural-language queries using an LLM parser. It then matches these entities to table headers through a compact embedding index and scans table contents directly for value occurrences, eliminating the need for heavy content indexing or costly offline stages. The resulting fine-grained alignment not only improves table retrieval accuracy but also facilitates efficient downstream NL2SQL execution by reducing token usage and redundant LLM calls. To evaluate Octopus, we introduce a new benchmark covering both table- and cell-level discovery under multi-table settings, including five datasets for independent discovery and two for join-based discovery. Experimental results show that Octopus consistently outperforms existing systems while achieving substantially lower computational and token costs. Code is available at https://github.com/wenzhilics/octopus.

</details>
