<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation](https://arxiv.org/abs/2510.05266)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 该论文提出一种用于检查下水道/涵洞缺陷的少样本语义分割框架E-FPN，基于原型学习，结合高效多尺度编码器（InceptionSepConv与深度可分离卷积）、掩膜平均池化生成原型，以及全局/局部/交叉自注意力强化特征。实验在复杂基础设施数据集上表现优异，最佳8-way 5-shot训练在2-way测试达82.55% F1和72.26% mIoU，自注意力带来最大提升约2.57% F1和2.9% mIoU。


<details>
  <summary>Details</summary>
Motivation: 基础设施检测中标注稀缺且昂贵，传统深度学习需大量数据且难以快速学习新缺陷类别。提出少样本分割框架以在有限标注下实现可靠缺陷识别。

Method: 使用改进的FPN编码器（InceptionSepConv块+深度可分离卷积）提取多尺度特征；在原型学习框架下用掩膜平均池化从少量支持样本生成类别原型；引入全局自注意力、局部自注意力与跨注意力模块增强查询与支持之间的特征交互；在不同way/shot设置下训练并在2-way分类任务上测试。

Result: 在挑战性基础设施检查数据集上，最佳配置（8-way 5-shot训练，在2-way测试）达82.55% F1与72.26% mIoU。自注意力模块相比基线分别提升约2.57% F1与2.9% mIoU；整体在不同设置均显示出稳健的少样本分割能力。

Conclusion: E-FPN在少样本基础设施缺陷分割上显著提升了性能，尤其自注意力模块效果明显，框架可用于快速适应新的缺陷类别，降低数据标注成本，提升维护效率。

Abstract: Few-shot semantic segmentation is vital for deep learning-based
infrastructure inspection applications, where labeled training examples are
scarce and expensive. Although existing deep learning frameworks perform well,
the need for extensive labeled datasets and the inability to learn new defect
categories with little data are problematic. We present our Enhanced Feature
Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert
and sewer defect categories using a prototypical learning framework. Our
approach has three main contributions: (1) adaptive E-FPN encoder using
InceptionSepConv blocks and depth-wise separable convolutions for efficient
multi-scale feature extraction; (2) prototypical learning with masked average
pooling for powerful prototype generation from small support examples; and (3)
attention-based feature representation through global self-attention, local
self-attention and cross-attention. Comprehensive experimentation on
challenging infrastructure inspection datasets illustrates that the method
achieves excellent few-shot performance, with the best configuration being
8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way
classification testing. The self-attention method had the most significant
performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over
baselines. Our framework addresses the critical need to rapidly respond to new
defect types in infrastructure inspection systems with limited new training
data that lead to more efficient and economical maintenance plans for critical
infrastructure systems.

</details>


### [2] [SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography](https://arxiv.org/abs/2510.05296)
*Zahra Maleki,Amirhossein Akbari,Amirhossein Binesh,Babak Khalaj*

Main category: cs.CV

TL;DR: 作者提出一种优先化肤色分割方法以增强rPPG信号提取，能在身体大范围运动与复杂光照下保持较好心率估计性能，并发布SYNC-rPPG数据集；在公开数据集与新数据集上均表现优于其它方法，尤其在说话与头转等挑战情景中。


<details>
  <summary>Details</summary>
Motivation: 现有rPPG方法对光照变化、运动与非皮肤干扰敏感，未充分利用全身肤色区域并排除易干扰部位；通过优先化皮肤分割可以提高rPPG信号质量，从而增强在真实世界条件下的心率预测准确性。

Method: 设计了一种皮肤区域检测与优先排序的分割网络，输出去除嘴、眼、头发等干扰区域的肤色掩码，并根据肤色质量对区域赋予优先级以引导rPPG信号提取。训练与评估在公开数据集和新建SYNC-rPPG数据集上进行，比较心率MAE等指标，展示在动态场景（说话、头转）下的鲁棒性。

Result: 提出一种用于远程光电容积描记(rPPG)的肤色分割优先化模型，通过检测全身可见肤色区域并排除干扰区域（如口腔、眼睛、头发）来提高rPPG信号质量与鲁棒性。评估包括公开数据集和新提供的SYNC-rPPG数据集，展示在说话与头部旋转等复杂场景下仍能维持较低的心率平均绝对误差(MAE)，并对各类肤色表现出高准确率。

Conclusion: 优先化的肤色分割能够显著提升rPPG在真实世界复杂条件下的稳健性和准确性，尤其在处理多部位皮肤、遮挡与动态场景时；该方法具有对多种肤色的良好适应性，适合实际部署于远程健康监测与智能场景。

Abstract: Remote photoplethysmography (rPPG) is an innovative method for monitoring
heart rate and vital signs by using a simple camera to record a person, as long
as any part of their skin is visible. This low-cost, contactless approach helps
in remote patient monitoring, emotion analysis, smart vehicle utilization, and
more. Over the years, various techniques have been proposed to improve the
accuracy of this technology, especially given its sensitivity to lighting and
movement. In the unsupervised pipeline, it is necessary to first select skin
regions from the video to extract the rPPG signal from the skin color changes.
We introduce a novel skin segmentation technique that prioritizes skin regions
to enhance the quality of the extracted signal. It can detect areas of skin all
over the body, making it more resistant to movement, while removing areas such
as the mouth, eyes, and hair that may cause interference. Our model is
evaluated on publicly available datasets, and we also present a new dataset,
called SYNC-rPPG, to better represent real-world conditions. The results
indicate that our model demonstrates a prior ability to capture heartbeats in
challenging conditions, such as talking and head rotation, and maintain the
mean absolute error (MAE) between predicted and actual heart rates, while other
methods fail to do so. In addition, we demonstrate high accuracy in detecting a
diverse range of skin tones, making this technique a promising option for
real-world applications.

</details>


### [3] [DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology](https://arxiv.org/abs/2510.05315)
*Yousef Yeganeh,Maximilian Frantzen,Michael Lee,Kun-Hsing Yu,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: DeepAf 用单张图像的空间-光谱混合特征预测最佳焦点，显著加速自动对焦并保持与多图像方法相当的精度，具有良好跨实验室泛化并在4x 放大下实现0.90 AUC 的癌症分类。


<details>
  <summary>Details</summary>
Motivation: 当前WSI扫描器昂贵，现有低成本自动聚焦或深度学习方法要么耗时（焦点堆栈）、要么需要多张输入或泛化性差；因此需要一种快速、单镜头、可跨样本泛化的自动聚焦解决方案。

Method: 提出一种混合网络，融合空间与光谱特征，通过回归预测离最佳焦点的距离（单次成像），并将预测用于调整显微镜控制参数；在实验与临床样本上评估聚焦精度、泛化能力及下游分类任务。

Result: 提出了 DeepAf 单次成像自动对焦框架，结合空间与光谱特征的混合架构，实现从单张图像回归到最佳焦点距离并控制显微镜，从而将传统显微镜转为高效切片扫描仪。与堆栈聚焦方法相比，聚焦时间减少约80%；在同实验室样本上聚焦精度为0.18 μm；与双图像方法（0.19 μm）性能接近但输入需求减半。跨实验室泛化良好，错误聚焦率仅0.72%，90%预测在景深内。在536例脑组织的临床研究中，4x放大下癌症分类AUC达0.90，在低倍放大下仍能维持诊断性能，适用于资源受限环境的实时数字病理。

Conclusion: DeepAf 可以将常规显微镜升级为低成本、高效的数字病理扫描系统，显著减少聚焦时间并在资源受限环境中保持诊断准确性。

Abstract: While Whole Slide Imaging (WSI) scanners remain the gold standard for
digitizing pathology samples, their high cost limits accessibility in many
healthcare settings. Other low-cost solutions also face critical limitations:
automated microscopes struggle with consistent focus across varying tissue
morphology, traditional auto-focus methods require time-consuming focal stacks,
and existing deep-learning approaches either need multiple input images or lack
generalization capability across tissue types and staining protocols. We
introduce a novel automated microscopic system powered by DeepAf, a novel
auto-focus framework that uniquely combines spatial and spectral features
through a hybrid architecture for single-shot focus prediction. The proposed
network automatically regresses the distance to the optimal focal point using
the extracted spatiospectral features and adjusts the control parameters for
optimal image outcomes. Our system transforms conventional microscopes into
efficient slide scanners, reducing focusing time by 80% compared to stack-based
methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples,
matching the performance of dual-image methods (0.19 {\mu}m) with half the
input requirements. DeepAf demonstrates robust cross-lab generalization with
only 0.72% false focus predictions and 90% of predictions within the depth of
field. Through an extensive clinical study of 536 brain tissue samples, our
system achieves 0.90 AUC in cancer classification at 4x magnification, a
significant achievement at lower magnification than typical 20x WSI scans. This
results in a comprehensive hardware-software design enabling accessible,
real-time digital pathology in resource-constrained settings while maintaining
diagnostic accuracy.

</details>


### [4] [Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection](https://arxiv.org/abs/2510.05326)
*Jalal Ahmmed,Faruk Ahmed,Rashedul Hasan Shohan,Md. Mahabub Rana,Mahdi Hasan*

Main category: cs.CV

TL;DR: 通过迁移学习微调五种预训练CNN，DenseNet201以99.33%准确率实现了高效、稳健的多类芒果叶病检测。


<details>
  <summary>Details</summary>
Motivation: 提高芒果叶病识别准确率，利用预训练CNN和迁移学习应对多类病害检测问题，从而改善产量与质量。

Method: 使用迁移学习对DenseNet201、InceptionV3、ResNet152V2、SeResNet152和Xception进行微调，按训练/验证分割训练模型，使用准确率、精确率、召回率、F1分数和混淆矩阵评估性能，并分析训练/验证曲线收敛性。

Result: 在五种预训练模型中，DenseNet201表现最佳，准确率99.33%；ResNet152V2和SeResNet152也表现良好；InceptionV3与Xception在外观相似类别上表现较差。

Conclusion: 微调的迁移学习模型能在智能农业中实现高精度、多类芒果叶病自动识别，DenseNet201最为优越，但仍需注意相似病害的混淆问题与数据泛化能力。

Abstract: Mango is an important fruit crop in South Asia, but its cultivation is
frequently hampered by leaf diseases that greatly impact yield and quality.
This research examines the performance of five pre-trained convolutional neural
networks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for
multi-class identification of mango leaf diseases across eight classes using a
transfer learning strategy with fine-tuning. The models were assessed through
standard evaluation metrics, such as accuracy, precision, recall, F1-score, and
confusion matrices. Among the architectures tested, DenseNet201 delivered the
best results, achieving 99.33% accuracy with consistently strong metrics for
individual classes, particularly excelling in identifying Cutting Weevil and
Bacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong
outcomes, whereas InceptionV3 and Xception exhibited lower performance in
visually similar categories like Sooty Mould and Powdery Mildew. The training
and validation plots demonstrated stable convergence for the highest-performing
models. The capability of fine-tuned transfer learning models, for precise and
dependable multi-class mango leaf disease detection in intelligent agricultural
applications.

</details>


### [5] [Mitigating Diffusion Model Hallucinations with Dynamic Guidance](https://arxiv.org/abs/2510.05356)
*Kostas Triaridis,Alexandros Graikos,Aggelina Chatziagapi,Grigorios G. Chrysos,Dimitris Samaras*

Main category: cs.CV

TL;DR: Dynamic Guidance在生成时有选择地锐化会导致伪影的方向，减少扩散模型的幻觉，同时保持语义多样性，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 扩散模型常因在不同数据模态之间过度平滑而产生结构性幻觉，需要一种在生成阶段直接纠正而非后处理的细粒度方法，以在减少伪影的同时保留语义插值带来的多样性。

Method: 提出在预定会导致伪影的方向上选择性地增强（sharpen）扩散模型的score函数，而在其它方向保持原始平滑，以此抑制模式间过度平滑引起的幻觉。

Result: 在受控合成和自然图像数据集上，Dynamic Guidance显著降低了生成幻觉率，表现优于现有基线方法。

Conclusion: Dynamic Guidance在生成时通过有选择地锐化评分函数以减少幻觉，同时保留语义变异，从而显著减少图像生成中的结构性不一致。

Abstract: Diffusion models, despite their impressive demos, often produce hallucinatory
samples with structural inconsistencies that lie outside of the support of the
true data distribution. Such hallucinations can be attributed to excessive
smoothing between modes of the data distribution. However, semantic
interpolations are often desirable and can lead to generation diversity, thus
we believe a more nuanced solution is required. In this work, we introduce
Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates
hallucinations by selectively sharpening the score function only along the
pre-determined directions known to cause artifacts, while preserving valid
semantic variations. To our knowledge, this is the first approach that
addresses hallucinations at generation time rather than through post-hoc
filtering. Dynamic Guidance substantially reduces hallucinations on both
controlled and natural image datasets, significantly outperforming baselines.

</details>


### [6] [LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation](https://arxiv.org/abs/2510.05367)
*Yang Xiao,Gen Li,Kaiyuan Deng,Yushu Wu,Zheng Zhan,Yanzhi Wang,Xiaolong Ma,Bo Hui*

Main category: cs.CV

TL;DR: 本文针对基于扩散模型的视频生成推理中的训练免费加速问题，提出将推理分为编码、去噪和解码三阶段，并为后两阶段设计缓存减少策略：异步缓存交换、特征分块、解码时切片潜变量。这些策略在保证时间开销低于加速收益的前提下，减少内存峰值并提升推理速度，同时保持质量下降在可接受范围。代码开源。


<details>
  <summary>Details</summary>
Motivation: 扩散模型视频生成推理中潜变量冗余导致缓存激增，特别是在去噪和解码阶段，现有缓存加速方法会引起内存峰值，限制可扩展性，因此需要阶段性内存优化策略以实现训练免费加速。

Method: 将推理流程分解为编码、去噪、解码三部分，针对去噪和解码阶段的缓存爆发问题，分别采用：1) 异步缓存交换（将不活跃缓存异步搬移至主存/磁盘以减小GPU内存占用）；2) 特征分块（按通道或空间切分特征以降低单次内存占用并分块处理）；3) 解码时切片潜变量（将潜变量分片逐段解码以避免一次性解码大尺寸数据）。同时设计调度以确保这些操作引入的时间开销小于加速收益。

Result: 相比基线方法，LightCache 在推理速度和内存占用上有显著改善，质量下降在可接受范围内（具体数值在论文与代码仓库中）。

Conclusion: 提出的分阶段内存优化策略（异步缓存交换、特征分块、切片解码）能在不显著增加时间开销的情况下，降低去噪和解码阶段的内存峰值并加速推理，保持生成质量基本稳定。

Abstract: Training-free acceleration has emerged as an advanced research area in video
generation based on diffusion models. The redundancy of latents in diffusion
model inference provides a natural entry point for acceleration. In this paper,
we decompose the inference process into the encoding, denoising, and decoding
stages, and observe that cache-based acceleration methods often lead to
substantial memory surges in the latter two stages. To address this problem, we
analyze the characteristics of inference across different stages and propose
stage-specific strategies for reducing memory consumption: 1) Asynchronous
Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same
time, we ensure that the time overhead introduced by these three strategies
remains lower than the acceleration gains themselves. Compared with the
baseline, our approach achieves faster inference speed and lower memory usage,
while maintaining quality degradation within an acceptable range. The Code is
available at https://github.com/NKUShaw/LightCache .

</details>


### [7] [See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models](https://arxiv.org/abs/2510.05408)
*Kebin Contreras,Luis Toscano-Palomino,Mauro Dalla Mura,Jorge Bacca*

Main category: cs.CV

TL;DR: 利用热成像中的残留热痕作为被动时间码，结合两个VLM和受约束扩散模型，从配对热图与RGB图重建出最多120秒前的场景帧，为热痕迹驱动的时光逆转成像提供初步验证。


<details>
  <summary>Details</summary>
Motivation: 利用人体与环境的温差导致的残余热印记作为时间信息载体，恢复几秒至数分钟以前的交互事件，这是RGB相机难以实现的，从而推动取证与场景分析应用。

Method: 方法将两个视觉语言模型与受约束的扩散重建过程耦合：一个VLM负责生成场景描述（语义/文本提示），另一个VLM在扩散过程中引导图像重建以保证语义和结构一致性。输入为配对的热图和RGB图，输出为重建的过去帧。

Result: 在三种受控场景下评估，结果显示可以重建出在语义与结构上合理的过去帧，最长期限可达120秒，证明了方法的可行性。

Conclusion: 该论文提出了一种基于热成像的时光逆转重建框架，能从当前热图和RGB配对图像推断几秒到两分钟前的场景状态，展示了热痕迹可作为被动时间编码的可行性。

Abstract: Recovering the past from present observations is an intriguing challenge with
potential applications in forensics and scene analysis. Thermal imaging,
operating in the infrared range, provides access to otherwise invisible
information. Since humans are typically warmer (37 C -98.6 F) than their
surroundings, interactions such as sitting, touching, or leaning leave residual
heat traces. These fading imprints serve as passive temporal codes, allowing
for the inference of recent events that exceed the capabilities of RGB cameras.
This work proposes a time-reversed reconstruction framework that uses paired
RGB and thermal images to recover scene states from a few seconds earlier. The
proposed approach couples Visual-Language Models (VLMs) with a constrained
diffusion process, where one VLM generates scene descriptions and another
guides image reconstruction, ensuring semantic and structural consistency. The
method is evaluated in three controlled scenarios, demonstrating the
feasibility of reconstructing plausible past frames up to 120 seconds earlier,
providing a first step toward time-reversed imaging from thermal traces.

</details>


### [8] [Personalizing Retrieval using Joint Embeddings or "the Return of Fluffy"](https://arxiv.org/abs/2510.05411)
*Bruno Korbar,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出pi-map：一个将实例图像嵌入映射为文本token的可训练网络，结合冻结的CLIP编码器，实现更好的个性化复合图像检索。


<details>
  <summary>Details</summary>
Motivation: 解决用复合查询（图像实例+文本描述）检索图像的问题，例如用一张目标物体图像指定实例并结合动作/位置等自然语言描述进行检索。

Method: 设计一个映射网络，将目标实例的局部图像嵌入转换为文本token，保留CLIP的图像与文本编码器冻结；对每个实例仅需一次简单训练以生成token，检索时将该token与文本查询拼接后输入CLIP文本编码器实现匹配。

Result: 在两个个性化检索基准上，pi-map方法超过了先前的最先进方法，证明了将实例图像嵌入转为文本token并与自然语言拼接的有效性。

Conclusion: 本文提出通过可训练的映射网络(pi-map)将局部图像嵌入映射为文本token，并与自然语言查询结合，以实现基于实例的复合检索；在两个个性化检索基准上优于现有方法。

Abstract: The goal of this paper is to be able to retrieve images using a compound
query that combines object instance information from an image, with a natural
text description of what that object is doing or where it is. For example, to
retrieve an image of "Fluffy the unicorn (specified by an image) on someone's
head". To achieve this we design a mapping network that can "translate" from a
local image embedding (of the object instance) to a text token, such that the
combination of the token and a natural language query is suitable for CLIP
style text encoding, and image retrieval. Generating a text token in this
manner involves a simple training procedure, that only needs to be performed
once for each object instance. We show that our approach of using a trainable
mapping network, termed pi-map, together with frozen CLIP text and image
encoders, improves the state of the art on two benchmarks designed to assess
personalized retrieval.

</details>


### [9] [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](https://arxiv.org/abs/2510.05488)
*Peizhi Yan,Rabab Ward,Qiang Tang,Shan Du*

Main category: cs.CV

TL;DR: 提出了ArchitectHead，一种基于UV特征场的3D Gaussian头像框架，实现了训练后可连续调整细节等级（LOD）；通过在2D UV空间对高斯点特征进行多层可学习特征图编码，并用轻量解码器映射到3D高斯属性，动态重采样特征图改变高斯数量，无需重训练即可高效调节LOD。实验证明在最高LOD达SOTA，自我与跨身份重现效果好；在最低LOD仅用6.2%高斯，质量适度下降但渲染速度近乎翻倍。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS头像使用固定且大量的高斯点，不便于在不同场景中平衡渲染效率与视觉质量；需要一种能在推理阶段连续控制细节等级的机制。

Method: 在UV二维特征空间构建多层可学习特征图的UV特征场；通过从不同分辨率的特征图动态重采样得到所需数量的特征点；使用轻量神经解码器将这些UV特征解码为3D Gaussian属性以供渲染，支持无需重训练的LOD调节。

Result: 在最高LOD下达到SOTA质量；在最低LOD仅使用6.2%高斯时，L1+7.9%、PSNR下降0.97%、SSIM下降0.6%、LPIPS+24.1%，渲染速度接近提升2倍。

Conclusion: ArchitectHead成功实现了训练后可连续控制LOD的3DGS头部化身，在保持高质量渲染的同时显著提升低细节下的效率，是可调复杂度渲染的有效方案。

Abstract: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.

</details>


### [10] [Human Action Recognition from Point Clouds over Time](https://arxiv.org/abs/2510.05506)
*James Dickens*

Main category: cs.CV

TL;DR: 提出点云流水线与混合点+稀疏卷积骨干，用多模态点特征并支持传感器和单目深度，NTU120上交主体89.3%。


<details>
  <summary>Details</summary>
Motivation: 目前HAR研究主要集中在骨架和视频方法，随着消费级深度传感器和Lidar的普及，利用稠密3D数据作为第三类输入源有潜力提升动作识别；此外单目深度估计的发展使得点云方法更易获得。

Method: 构建一条流水线：场景中先分割出人体点云并跟踪个体，进行身体部位分割；将点云序列体素化后，融合点级特征（法线、颜色、红外强度、部位标签），并采用一种混合骨干——点基方法与稀疏卷积网络相结合进行特征提取与分类；同时支持传感器深度与单目估计深度，最终还用集成不同深度来源提升性能。

Result: 在NTU RGB-D 120上评估，所提方法在交叉主体设置下达到89.3%准确率，优于先前点云动作识别方法，并与骨架方法竞争；融合传感器与估计深度的集成进一步提升了性能。

Conclusion: 该论文提出了一种基于点云的3D视频动作识别新范式，通过点云分割、跟踪、人体部位解析并结合Point-based方法与稀疏卷积的骨干网络，实现了对深度传感器和单目估计深度的通用支持。

Abstract: Recent research into human action recognition (HAR) has focused predominantly
on skeletal action recognition and video-based methods. With the increasing
availability of consumer-grade depth sensors and Lidar instruments, there is a
growing opportunity to leverage dense 3D data for action recognition, to
develop a third way. This paper presents a novel approach for recognizing
actions from 3D videos by introducing a pipeline that segments human point
clouds from the background of a scene, tracks individuals over time, and
performs body part segmentation. The method supports point clouds from both
depth sensors and monocular depth estimation. At the core of the proposed HAR
framework is a novel backbone for 3D action recognition, which combines
point-based techniques with sparse convolutional networks applied to
voxel-mapped point cloud sequences. Experiments incorporate auxiliary point
features including surface normals, color, infrared intensity, and body part
parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D
120 dataset demonstrates that the method is competitive with existing skeletal
action recognition algorithms. Moreover, combining both sensor-based and
estimated depth inputs in an ensemble setup, this approach achieves 89.3%
accuracy when different human subjects are considered for training and testing,
outperforming previous point cloud action recognition methods.

</details>


### [11] [Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models](https://arxiv.org/abs/2510.05509)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TL;DR: 通过在噪声空间引入基于score雅可比的黎曼度量，求测地线以获得与数据流形对齐的插值路径，从而在扩散模型中实现更自然的图像插值。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量样本，但缺乏显式、可解析的低维潜空间来参数化数据流形，导致现有插值方法沿高密度路径但不一定与流形对齐，产生感知上不自然的过渡。

Method: 基于最近发现：score函数（扩散模型中的得分网络）的雅可比矩阵包含数据流形的切空间信息，论文在噪声空间构造以该雅可比为基础的黎曼度量，并在该度量下计算测地线作为插值路径。

Result: 在图像插值任务上，基于该黎曼度量的测地线比基于密度的和简单基线方法产生更自然、更忠实的过渡。

Conclusion: 该论文提出在噪声空间上定义一种新的黎曼度量，利用score函数雅可比矩阵刻画局部数据流形的切空间，从而使噪声空间上的测地线更贴合或平行于学习到的数据流形，实现更自然的插值效果。

Abstract: Diffusion models are powerful deep generative models (DGMs) that generate
high-fidelity, diverse content. However, unlike classical DGMs, they lack an
explicit, tractable low-dimensional latent space that parameterizes the data
manifold. This absence limits manifold-aware analysis and operations, such as
interpolation and editing. Existing interpolation methods for diffusion models
typically follow paths through high-density regions, which are not necessarily
aligned with the data manifold and can yield perceptually unnatural
transitions. To exploit the data manifold learned by diffusion models, we
propose a novel Riemannian metric on the noise space, inspired by recent
findings that the Jacobian of the score function captures the tangent spaces to
the local data manifold. This metric encourages geodesics in the noise space to
stay within or run parallel to the learned data manifold. Experiments on image
interpolation show that our metric produces perceptually more natural and
faithful transitions than existing density-based and naive baselines.

</details>


### [12] [Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation](https://arxiv.org/abs/2510.05532)
*Sam Sartor,Pieter Peers*

Main category: cs.CV

TL;DR: Teamwork uses multiple coordinated copies of a pretrained diffusion model plus a modified LoRA to expand input/output channels and adapt to new graphics tasks efficiently without changing the original model architecture.


<details>
  <summary>Details</summary>
Motivation: Provide a unified, flexible way to expand input/output channels and adapt pretrained diffusion models for diverse graphics tasks without changing model architecture.

Method: Coordinate several copies of the base diffusion model (teammates) and apply a novel variant of Low Rank-Adaptation to jointly adapt and synchronize them for channel expansion and task adaptation; allow dynamic activation/deactivation of teammates.

Result: Teamwork coordinates multiple instances of a base diffusion model (teammates) and uses a novel LoRA variation to adapt and coordinate them, enabling channel expansion and dynamic (de)activation; demonstrated on tasks like inpainting, SVBRDF estimation, intrinsic decomposition, neural shading, intrinsic image synthesis.

Conclusion: Teamwork is a flexible, efficient method to reuse pretrained diffusion models for varied generative and inverse graphics tasks by coordinating multiple model instances with adapted low-rank updates, supporting dynamic teammate activation and delivering strong results across tasks.

Abstract: Large pretrained diffusion models can provide strong priors beneficial for
many graphics applications. However, generative applications such as neural
rendering and inverse methods such as SVBRDF estimation and intrinsic image
decomposition require additional input or output channels. Current solutions
for channel expansion are often application specific and these solutions can be
difficult to adapt to different diffusion models or new tasks. This paper
introduces Teamwork: a flexible and efficient unified solution for jointly
increasing the number of input and output channels as well as adapting a
pretrained diffusion model to new tasks. Teamwork achieves channel expansion
without altering the pretrained diffusion model architecture by coordinating
and adapting multiple instances of the base diffusion model (\ie, teammates).
We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address
both adaptation and coordination between the different teammates. Furthermore
Teamwork supports dynamic (de)activation of teammates. We demonstrate the
flexibility and efficiency of Teamwork on a variety of generative and inverse
graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic
decomposition, neural shading, and intrinsic image synthesis.

</details>


### [13] [Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work](https://arxiv.org/abs/2510.05538)
*Owen Henkel,Bill Roberts,Doug Jaffe,Laurence Holt*

Main category: cs.CV

TL;DR: 本文评估多模态大模型在批改手写数学作业上的能力。对加纳中学算术题（有客观答案）模型表现接近人类；对美国小学生的数学图画（主观、需教学判断）模型直接判读表现差，但在给出人工描述后显著改进，说明视觉理解仍是瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探讨MLLMs在教育评估中对手写作业的应用潜力，特别是在小学和初中数学中大量手写工作导致教师批改负担重，自动化评估若可靠可节省大量时间并提供教学洞见。

Method: 进行两组实验：A组用288份加纳中学学生手写算术答案测试模型客观评分准确性并与人工对比；B组用150份美国小学生数学图画测试模型直接评分与在提供详细人工图像描述后评分的差异，计算与人工标注之间的kappa一致性。

Result: 实验A：模型在客观算术答案上达到95%准确率，k=0.90，但存在少数非人类式错误。实验B：直接分析学生图画时模型与人工标签k=0.20；在获得详细人工描述后k提升到0.47，达到人类间一致性水平。

Conclusion: MLLMs在识别手写算术答案方面接近人类，但在直接解读学生数学图画方面存在明显不足；提供人工描述能显著提升一致性，表明当前挑战主要在视觉表征与理解。

Abstract: Recent advances in multimodal large language models (MLLMs) raise the
question of their potential for grading, analyzing, and offering feedback on
handwritten student classwork. This capability would be particularly beneficial
in elementary and middle-school mathematics education, where most work remains
handwritten, because seeing students' full working of a problem provides
valuable insights into their learning processes, but is extremely
time-consuming to grade. We present two experiments investigating MLLM
performance on handwritten student mathematics classwork. Experiment A examines
288 handwritten responses from Ghanaian middle school students solving
arithmetic problems with objective answers. In this context, models achieved
near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human
educators would be unlikely to make. Experiment B evaluates 150 mathematical
illustrations from American elementary students, where the drawings are the
answer to the question. These tasks lack single objective answers and require
sophisticated visual interpretation as well as pedagogical judgment in order to
analyze and evaluate them. We attempted to separate MLLMs' visual capabilities
from their pedagogical abilities by first asking them to grade the student
illustrations directly, and then by augmenting the image with a detailed human
description of the illustration. We found that when the models had to analyze
the student illustrations directly, they struggled, achieving only k = 0.20
with ground truth scores, but when given human descriptions, their agreement
levels improved dramatically to k = 0.47, which was in line with human-to-human
agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic
work relatively well, but still struggle to "see" student mathematical
illustrations.

</details>


### [14] [Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics](https://arxiv.org/abs/2510.05558)
*Christopher Hoang,Mengye Ren*

Main category: cs.CV

TL;DR: Midway Network extends latent dynamics modeling to natural videos with a midway inference path and dense forward prediction, enabling joint self-supervised learning of recognition and motion; yields strong segmentation and optical flow results and reveals high-level correspondences.


<details>
  <summary>Details</summary>
Motivation: The paper targets learning visual representations that jointly capture static object semantics and dynamic motion using only unlabeled natural videos, addressing limitations of prior self-supervised methods that focus on either recognition or motion but not both.

Method: Introduce a hierarchical latent dynamics model with a midway top-down inference path between frames to infer motion latents; use a dense forward prediction loss across multiple levels to handle complex multi-object scenes; pretrain on large-scale natural video datasets and evaluate on semantic segmentation, optical flow, and a forward feature perturbation analysis to probe learned correspondences.

Result: They propose Midway Network, which uses a midway top-down path to infer motion latents between frames, a dense forward prediction objective, and hierarchical structure; after pretraining on large video datasets it outperforms prior self-supervised methods on semantic segmentation and optical flow, and shows that learned dynamics capture high-level correspondence via forward feature perturbation analysis.

Conclusion: Midway Network successfully learns joint representations for object recognition and motion from natural videos by combining hierarchical latent dynamics, midway inference, and dense prediction; it improves downstream segmentation and flow performance and captures high-level correspondence.

Abstract: Object recognition and motion understanding are key components of perception
that complement each other. While self-supervised learning methods have shown
promise in their ability to learn from unlabeled data, they have primarily
focused on obtaining rich representations for either recognition or motion
rather than both in tandem. On the other hand, latent dynamics modeling has
been used in decision making to learn latent representations of observations
and their transformations over time for control and planning tasks. In this
work, we present Midway Network, a new self-supervised learning architecture
that is the first to learn strong visual representations for both object
recognition and motion understanding solely from natural videos, by extending
latent dynamics modeling to this domain. Midway Network leverages a midway
top-down path to infer motion latents between video frames, as well as a dense
forward prediction objective and hierarchical structure to tackle the complex,
multi-object scenes of natural videos. We demonstrate that after pretraining on
two large-scale natural video datasets, Midway Network achieves strong
performance on both semantic segmentation and optical flow tasks relative to
prior self-supervised learning methods. We also show that Midway Network's
learned dynamics can capture high-level correspondence via a novel analysis
method based on forward feature perturbation.

</details>


### [15] [HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video](https://arxiv.org/abs/2510.05560)
*Hongchi Xia,Chih-Hao Lin,Hao-Yu Hsu,Quentin Leboutet,Katelyn Gao,Michael Paulitsch,Benjamin Ummenhofer,Shenlong Wang*

Main category: cs.CV

TL;DR: HoloScene builds simulation-ready digital twins by optimizing a scene-graph that encodes geometry, appearance, physics and relationships, using energy-based objective and hybrid optimization to achieve complete geometry, physical realism, and photorealistic rendering.


<details>
  <summary>Details</summary>
Motivation: Digitize physical world into accurate simulation-ready virtual environments, overcoming shortcomings of existing 3D reconstruction methods (geometry completeness, interactivity, physical plausibility, photorealism, physical properties).

Method: Interactive scene-graph representation; energy-based optimization combining observational data, physical constraints, generative priors; hybrid optimization combining sampling-based exploration and gradient-based refinement.

Result: HoloScene: interactive 3D reconstruction framework using comprehensive scene-graph representation encoding geometry, appearance, physical properties, relationships. Formulates reconstruction as energy-based optimization integrating observations, physical constraints, generative priors. Uses hybrid optimization (sampling + gradient refinement). Produces complete precise geometry, physical stability, realistic rendering. Demonstrated superior performance on benchmarks and practical interactive applications.

Conclusion: HoloScene successfully unifies reconstruction and physical/visual realism, enabling interactive, simulation-ready digital twins with strong benchmark performance and practical applications.

Abstract: Digitizing the physical world into accurate simulation-ready virtual
environments offers significant opportunities in a variety of fields such as
augmented and virtual reality, gaming, and robotics. However, current 3D
reconstruction and scene-understanding methods commonly fall short in one or
more critical aspects, such as geometry completeness, object interactivity,
physical plausibility, photorealistic rendering, or realistic physical
properties for reliable dynamic simulation. To address these limitations, we
introduce HoloScene, a novel interactive 3D reconstruction framework that
simultaneously achieves these requirements. HoloScene leverages a comprehensive
interactive scene-graph representation, encoding object geometry, appearance,
and physical properties alongside hierarchical and inter-object relationships.
Reconstruction is formulated as an energy-based optimization problem,
integrating observational data, physical constraints, and generative priors
into a unified, coherent objective. Optimization is efficiently performed via a
hybrid approach combining sampling-based exploration with gradient-based
refinement. The resulting digital twins exhibit complete and precise geometry,
physical stability, and realistic rendering from novel viewpoints. Evaluations
conducted on multiple benchmark datasets demonstrate superior performance,
while practical use-cases in interactive gaming and real-time digital-twin
manipulation illustrate HoloScene's broad applicability and effectiveness.
Project page: https://xiahongchi.github.io/HoloScene.

</details>


### [16] [CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval](https://arxiv.org/abs/2510.05586)
*Bin Kang,Bin Chen,Junjie Wang,Yulin Li,Junzhi Zhao,Zhuotao Tian*

Main category: cs.CV

TL;DR: CalibCLIP是一个训练-free的方法，通过视觉的Contrastive Visual Enhancer和文本的Discriminative Concept Calibrator来抑制主导低信息tokens并增强判别性概念，从而提高CLIP类VLM在图像检索任务中的辨别力，七个基准上均有改进。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs存在结构性问题，少数低贡献但主导的tokens会在信息聚合中占主导，抑制文本驱动图像检索任务中的判别特征，需设计无需训练的校准策略以恢复判别性。

Method: 视觉：Contrastive Visual Enhancer(CVE)将视觉特征分解为目标与低信息区域，识别主导tokens并动态抑制其表示，从而保留判别特征；文本：Discriminative Concept Calibrator(DCC)区分通用与判别性概念，抑制通用概念影响，增强判别概念表示。两者整合后无需训练即可应用于已有VLM。

Result: CalibCLIP提出了一种无训练的校准方法，通过在视觉和文本空间分别抑制和增强信息来改善文本驱动图像检索表现。视觉上用CVE将特征分解为目标与低信息区域，识别并动态压制主导（低贡献）tokens；文本上用DCC区分通用与判别概念，抑制通用概念并提升判别概念表示。实验在七个基准上显示一致提升。

Conclusion: CalibCLIP能有效缓解少量低贡献tokens主导全局语义的问题，通过视觉与文本双向校准提高检索区分性，且不需模型再训练，在多项检索基准上表现优异。

Abstract: Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP

</details>


### [17] [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
*Zeqi Gu,Markos Georgopoulos,Xiaoliang Dai,Marjan Ghazvininejad,Chu Wang,Felix Juefei-Xu,Kunpeng Li,Yujun Shi,Zecheng He,Zijian He,Jiawei Zhou,Abe Davis,Jialiang Wang*

Main category: cs.CV

TL;DR: 提出ShortCoTI，用自适应长度奖励在强化学习框架下压缩CoT提示，显著减少推理长度（约54%），并在T2I-CompBench和GenEval上保持或稍提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 近来自回归式多模态大模型在图像生成任务中采用CoT以增强对齐与细节，但CoT常带来冗余且与原提示冲突的细化（visual overthinking），增加计算成本并可能降低一致性，因此需要生成更简洁的CoT序列以提高效率与保持质量。

Method: 在强化学习范式中加入一个自适应的简洁性奖励函数，该函数根据每个任务的估计难度调整奖励规模，鼓励生成更短的CoT提示，同时与原有质量目标平衡；通过在多模态模型上训练并评估，在T2I-CompBench与GenEval数据集上验证效果。

Result: ShortCoTI是一种用于缩减生成式多模态大模型中链式思维（CoT）冗余的方法，旨在减少视觉过度思考（visual overthinking）带来的计算与不一致问题；通过在强化学习中引入自适应长度奖励，ShortCoTI在多项基准上将推理提示长度减少约54%，同时保持或略微提升图像质量指标；定性分析表明，该方法消除了冗长与重复的细化过程，生成更简洁且语义丰富的提示，从而提高生成效率而不损失图像保真度与视觉吸引力。

Conclusion: ShortCoTI能够在不降低图像质量的前提下显著压缩CoT推理长度，提高计算效率，并减少引入与原始提示相矛盾的多余细节。

Abstract: Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

</details>


### [18] [HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](https://arxiv.org/abs/2510.05609)
*Junwen Chen,Peilin Xiong,Keiji Yanai*

Main category: cs.CV

TL;DR: Propose HOI-R1: train MLLM with RL and text-based HOI reasoning and rewards to perform HOI detection without detectors, achieving 2x baseline on HICO-DET


<details>
  <summary>Details</summary>
Motivation: Simplify HOI detection by leveraging MLLM reasoning abilities without extra detectors, avoiding complex VLM-HOI integration

Method: Reinforcement learning fine-tuning of MLLM for HOI detection

Result: On HICO-DET, HOI-R1 doubles baseline accuracy and shows strong generalization; code released

Conclusion: Language models, when RL-fine-tuned with task-specific reasoning and rewards, can perform HOI detection purely via text, offering simpler pipelines and good generalization

Abstract: Recent Human-object interaction detection (HOID) methods highly require prior
knowledge from VLMs to enhance the interaction recognition capabilities. The
training strategies and model architectures for connecting the knowledge from
VLMs to the HOI instance representations from the object detector are
challenging, and the whole framework is complex for further development or
application. On the other hand, the inherent reasoning abilities of MLLMs on
human-object interaction detection are under-explored. Inspired by the recent
success of training MLLMs with reinforcement learning (RL) methods, we propose
HOI-R1 and first explore the potential of the language model on the HOID task
without any additional detection modules. We introduce an HOI reasoning process
and HOID reward functions to solve the HOID task by pure text. The results on
the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline
with great generalization ability. The source code is available at
https://github.com/cjw2021/HOI-R1.

</details>


### [19] [Efficient Conditional Generation on Scale-based Visual Autoregressive Models](https://arxiv.org/abs/2510.05610)
*Jiaqi Liu,Tao Huang,Chang Xu*

Main category: cs.CV

TL;DR: 提出ECM，轻量控制模块+早期集中采样，显著减少微调成本并提升AR模型的受控图像生成效果与效率。


<details>
  <summary>Details</summary>
Motivation: 动机是减少在复杂空间条件化图像生成任务中对预训练AR模型进行昂贵微调的需求，提供一个可插拔、轻量且高效的替代方案。

Method: 方法包括：1) 分布式架构的控制模块，含上下文感知注意力层以利用实时生成的token改进条件特征；2) 共享门控FFN以在参数受限下保持一致性学习；3) 早期集中采样策略减少每次迭代训练token数，配合推理时温度调度弥补晚期token训练不足。

Result: ECM提出了一个轻量级可插拔控制模块，通过分布式架构将控制信号注入自回归生成模型；使用上下文感知注意力层和共享门控前馈网络以高效利用有限参数；引入早期集中采样策略和推理时温度调度以在降低训练成本的同时保持生成质量。

Conclusion: ECM在保持或提升生成质量与多样性的同时，大幅降低训练与推理开销，尤其适合空间条件化的图像生成任务。

Abstract: Recent advances in autoregressive (AR) models have demonstrated their
potential to rival diffusion models in image synthesis. However, for complex
spatially-conditioned generation, current AR approaches rely on fine-tuning the
pre-trained model, leading to significant training costs. In this paper, we
propose the Efficient Control Model (ECM), a plug-and-play framework featuring
a lightweight control module that introduces control signals via a distributed
architecture. This architecture consists of context-aware attention layers that
refine conditional features using real-time generated tokens, and a shared
gated feed-forward network (FFN) designed to maximize the utilization of its
limited capacity and ensure coherent control feature learning. Furthermore,
recognizing the critical role of early-stage generation in determining semantic
structure, we introduce an early-centric sampling strategy that prioritizes
learning early control sequences. This approach reduces computational cost by
lowering the number of training tokens per iteration, while a complementary
temperature scheduling during inference compensates for the resulting
insufficient training of late-stage tokens. Extensive experiments on
scale-based AR models validate that our method achieves high-fidelity and
diverse control over image generation, surpassing existing baselines while
significantly improving both training and inference efficiency.

</details>


### [20] [PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction](https://arxiv.org/abs/2510.05613)
*Ziqiao Meng,Qichao Wang,Zhiyang Dou,Zixing Song,Zhipeng Zhou,Irwin King,Peilin Zhao*

Main category: cs.CV

TL;DR: 生成一段过短的摘要


<details>
  <summary>Details</summary>
Motivation: 请描述该论文提出方法的动机

Method: 请概述该论文采用的方法

Result: 请总结该论文的主要实验结果

Conclusion: 请给出该论文的结论

Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based
approaches in quality. The performance gap stems from the fact that
autoregressive models impose an artificial ordering on inherently unordered
point sets, forcing shape generation to proceed as a sequence of local
predictions. This sequential bias emphasizes short-range continuity but
undermines the model's capacity to capture long-range dependencies, hindering
its ability to enforce global structural properties such as symmetry,
consistent topology, and large-scale geometric regularities. Inspired by the
level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a
coarse-to-fine generative framework that preserves global shape structure at
low resolutions and progressively refines fine-grained geometry at higher
scales through a next-scale prediction paradigm. This multi-scale factorization
aligns the autoregressive objective with the permutation-invariant nature of
point sets, enabling rich intra-scale interactions while avoiding brittle fixed
orderings. Experiments on ShapeNet show that PointNSP establishes
state-of-the-art (SOTA) generation quality for the first time within the
autoregressive paradigm. In addition, it surpasses strong diffusion-based
baselines in parameter, training, and inference efficiency. Finally, in dense
generation with 8,192 points, PointNSP's advantages become even more
pronounced, underscoring its scalability potential.

</details>


### [21] [TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation](https://arxiv.org/abs/2510.05615)
*Guangrong Wan,Jun liu,Tang tang,Lianghao Shi,Wenjun Luo,TingTing Xu*

Main category: cs.CV

TL;DR: 作者构建了包含帧分类、Placido环检测与像素级TFBU分割的TFM数据集，提出轻量高效的TF-Net及端到端实时流水线TF-Collab，实现自动化泪膜破裂分析并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 自动化泪膜破裂分析受限于标注数据缺乏和缺少集成方案，阻碍临床化部署与实时分析需求，需构建公开数据集并提出高效模型与端到端流程。

Method: 构建包含15个高分辨率视频、6247帧，标注帧级分类、Placido环检测与像素级破裂区域的多任务TFM数据集；设计轻量化TF-Net（MobileOne-mini骨干+重参数化+增强特征金字塔）作为基线分割模型；比较多种SOTA医学分割模型；设计TF-Collab流水线按序调用帧分类、瞳孔定位和TFBU分割以实现自动化分析。

Result: 提供了首个公开的多任务TFM数据集与代码；TF-Net在准确性与计算效率间取得平衡，适合实时应用；TF-Collab实现了基于三任务模型的全自动实时分析流程，实验验证其有效性。

Conclusion: 本文提出了首个多任务泪膜分析数据集TFM，并在此基础上提出高效分割模型TF-Net及整合实时流程TF-Collab，展示了可用于临床实时干眼诊断的可行方案。

Abstract: Tear film break-up (TFBU) analysis is critical for diagnosing dry eye
syndrome, but automated TFBU segmentation remains challenging due to the lack
of annotated datasets and integrated solutions. This paper introduces the Tear
Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task
tear film analysis, comprising 15 high-resolution videos (totaling 6,247
frames) annotated with three vision tasks: frame-level classification ('clear',
'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area
segmentation. Leveraging this dataset, we first propose TF-Net, a novel and
efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini
backbone with re-parameterization techniques and an enhanced feature pyramid
network to achieve a favorable balance between accuracy and computational
efficiency for real-time clinical applications. We further establish benchmark
performance on the TFM segmentation subset by comparing TF-Net against several
state-of-the-art medical image segmentation models. Furthermore, we design
TF-Collab, a novel integrated real-time pipeline that synergistically leverages
models trained on all three tasks of the TFM dataset. By sequentially
orchestrating frame classification for BUT determination, pupil region
localization for input standardization, and TFBU segmentation, TF-Collab fully
automates the analysis. Experimental results demonstrate the effectiveness of
the proposed TF-Net and TF-Collab, providing a foundation for future research
in ocular surface diagnostics. Our code and the TFM datasets are available at
https://github.com/glory-wan/TF-Net

</details>


### [22] [InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment](https://arxiv.org/abs/2510.05617)
*Ibrahim Salihu Yusuf,Iffanice Houndayi,Rym Oualha,Mohamed Aziz Cherif,Kobby Panford-Quainoo,Arnu Pretorius*

Main category: cs.CV

TL;DR: InstaGeo是一个开源端到端地理空间机器学习框架，自动化数据预处理、模型蒸馏与部署，能把大模型压缩至约1/8体积并快速生成可交互web地图应用。重现三项研究数据集时mIoU损失极小，且在扩充农作物分割数据集后达到60.65% mIoU，显著优于先前基线。


<details>
  <summary>Details</summary>
Motivation: 尽管开源多光谱影像与地理基础模型已普及，但缺乏完整的自动化数据流水线和轻量化下游模型阻碍了实际部署，尤其在人道或环境场景中需要低延迟、低碳与易部署的解决方案。

Method: 框架包括(1)自动化数据编排管道将原始卫星影像转为模型输入数据集；(2)任务特定的模型蒸馏以得到小型高效模型；(3)一键部署为交互式web地图应用。作者重现了三项工作的数据集并训练模型，比较了蒸馏前后mIoU与模型大小、FLOPs和碳排放。

Result: 成功重现三项研究并在精度上保持微小差距（-0.73、-0.20、+1.79 pp），将模型缩小至最多8倍，降低FLOPs和CO2排放；扩展农作物分割数据集获得60.65% mIoU，比基线高12 pp；从原始数据到部署可在一天内完成。

Conclusion: InstaGeo统一了数据准备、模型压缩和部署流程，将研究级地理空间基础模型转化为低碳、实用的实时大规模地球观测工具，推动从模型规模竞争转向数据质量与应用导向的创新。

Abstract: Open-access multispectral imagery from missions like Landsat 8-9 and
Sentinel-2 has fueled the development of geospatial foundation models (GFMs)
for humanitarian and environmental applications. Yet, their deployment remains
limited by (i) the absence of automated geospatial data pipelines and (ii) the
large size of fine-tuned models. Existing GFMs lack workflows for processing
raw satellite imagery, and downstream adaptations often retain the full
complexity of the original encoder.
  We present InstaGeo, an open-source, end-to-end framework that addresses
these challenges by integrating: (1) automated data curation to transform raw
imagery into model-ready datasets; (2) task-specific model distillation to
derive compact, compute-efficient models; and (3) seamless deployment as
interactive web-map applications. Using InstaGeo, we reproduced datasets from
three published studies and trained models with marginal mIoU differences of
-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for
desert locust prediction. The distilled models are up to 8x smaller than
standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal
accuracy loss.
  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger
crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp
improvement over prior baselines. Moreover, InstaGeo enables users to progress
from raw data to model deployment within a single working day.
  By unifying data preparation, model compression, and deployment, InstaGeo
transforms research-grade GFMs into practical, low-carbon tools for real-time,
large-scale Earth observation. This approach shifts geospatial AI toward data
quality and application-driven innovation. Source code, datasets, and model
checkpoints are available at:
https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git

</details>


### [23] [Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection](https://arxiv.org/abs/2510.05633)
*Sara Mandelli,Diego Vila-Portela,David Vázquez-Padín,Paolo Bestagini,Fernando Pérez-González*

Main category: cs.CV

TL;DR: 本文研究频域“谱峰”是否真为生成图像的重要指征。作者提出去除谱峰的策略并评估多种检测器，还设计了仅基于谱峰的线性可解释检测器。结果显示多数检测器并不依赖谱峰，质疑了领域中普遍假设。


<details>
  <summary>Details</summary>
Motivation: 频域周期性峰值被广泛认为是生成模型图像的强指征，但现有深度检测器作为黑盒，无法确认其是否真的依赖此类特征，影响解释性与可信度。

Method: 提出一种从图像中移除频域周期性峰值的操作（保留图像其它信息），并用该操作测试多种检测器对性能的影响；同时构建一个只使用谱峰信息的线性检测器作为可解释基线。

Result: 在移除谱峰后，许多检测器性能未显著下降；线性谱峰检测器虽可辨别部分合成图像，但并不能完全替代深度模型，表明谱峰并非多数检测器的核心判据。

Conclusion: 大多数现有检测器并不根本依赖频域谱峰来判断图像是否合成，意味着谱峰并非通用或唯一的证据来源，需要更可解释、稳健的取证方法。

Abstract: Over the years, the forensics community has proposed several deep
learning-based detectors to mitigate the risks of generative AI. Recently,
frequency-domain artifacts (particularly periodic peaks in the magnitude
spectrum), have received significant attention, as they have been often
considered a strong indicator of synthetic image generation. However,
state-of-the-art detectors are typically used as black-boxes, and it still
remains unclear whether they truly rely on these peaks. This limits their
interpretability and trust. In this work, we conduct a systematic study to
address this question. We propose a strategy to remove spectral peaks from
images and analyze the impact of this operation on several detectors. In
addition, we introduce a simple linear detector that relies exclusively on
frequency peaks, providing a fully interpretable baseline free from the
confounding influence of deep learning. Our findings reveal that most detectors
are not fundamentally dependent on spectral peaks, challenging a widespread
assumption in the field and paving the way for more transparent and reliable
forensic tools.

</details>


### [24] [Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning](https://arxiv.org/abs/2510.05643)
*Shozo Saeki,Minoru Kawahara,Hirohisa Aman*

Main category: cs.CV

TL;DR: 提出CHEST损失，将超曲率空间与欧氏空间的代理式Soft Triple损失结合，并加上基于超曲率层次聚类的正则化，从而在大规模数据集上实现更高精度和更稳定的深度度量学习


<details>
  <summary>Details</summary>
Motivation: Enable efficient supervised proxy-based deep metric learning in hyperbolic space to leverage hierarchical data structures while retaining computational efficiency for large-scale datasets

Method: Combine hyperbolic and Euclidean proxy-based losses with hyperbolic clustering regularization

Result: Proposed CHEST loss combining hyperbolic and Euclidean Soft Triple proxy losses plus hyperbolic hierarchical clustering regularizer; improves accuracy and stability

Conclusion: 在四个基准数据集上，CHEST损失达到了新的最先进性能，表明结合超曲率和欧氏表征以及层次化正则化能提升超曲率空间中的代理式监督DML的有效性。

Abstract: Deep metric learning (DML) aims to learn a neural network mapping data to an
embedding space, which can represent semantic similarity between data points.
Hyperbolic space is attractive for DML since it can represent richer
structures, such as tree structures. DML in hyperbolic space is based on
pair-based loss or unsupervised regularization loss. On the other hand,
supervised proxy-based losses in hyperbolic space have not been reported yet
due to some issues in applying proxy-based losses in a hyperbolic space.
However, proxy-based losses are attractive for large-scale datasets since they
have less training complexity. To address these, this paper proposes the
Combined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is
composed of the proxy-based losses in hyperbolic and Euclidean spaces and the
regularization loss based on hyperbolic hierarchical clustering. We find that
the combination of hyperbolic and Euclidean spaces improves DML accuracy and
learning stability for both spaces. Finally, we evaluate the CHEST loss on four
benchmark datasets, achieving a new state-of-the-art performance.

</details>


### [25] [Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation](https://arxiv.org/abs/2510.05649)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 该论文提出两种深度学习框架：AHP-CADNet用于基于眼部关键点、头姿态与结构化临床属性的多级注意力融合可解释AHP诊断；基于课程学习的缺失数据插补框架采用结构化变量与非结构化临床笔记逐步增强鲁棒性。两框架在PoseGaze-AHP数据集上表现优异，分类准确率96.9-99.0%，连续变量MAE 0.103-0.199，R2>0.93；插补准确率93.46-99.78%，且建模临床依赖显著提升（p<0.001）。


<details>
  <summary>Details</summary>
Motivation: AHP诊断依赖主观临床评估且医疗记录常不完整，导致误诊与并发症风险。作者旨在通过自动化且可解释的诊断工具与稳健的缺失数据插补方法，提高诊断准确性并应对现实世界临床数据的不完整性。

Method: 1) AHP-CADNet：融合眼部关键点、头部姿态特征与结构化临床属性，采用多级注意力机制实现特征交互并输出可解释预测；2) 插补框架：基于课程学习，逐步从结构化变量到非结构化临床笔记进行缺失值预测，利用PubMedBERT等预训练模型建模文本和临床依赖关系以提升插补质量。

Result: 在PoseGaze-AHP数据集上，AHP-CADNet分类任务准确率96.9-99.0%，连续变量预测MAE 0.103-0.199，R2>0.93；课程学习插补在所有临床变量上达93.46-99.78%准确率（使用PubMedBERT），建模临床依赖带来统计学上显著改进（p<0.001）。

Conclusion: AHP-CADNet和课程学习插补框架在AHP自动诊断与缺失数据恢复上均有效，能提供高准确性与可解释性，有助于临床早期诊断并减轻因不完整病历导致的评估困难。

Abstract: Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that
arises from ocular misalignment conditions, such as strabismus, enabling
patients to reduce diplopia and preserve binocular vision. Early diagnosis
minimizes morbidity and secondary complications such as facial asymmetry;
however, current clinical assessments remain largely subjective and are further
complicated by incomplete medical records. This study addresses both challenges
through two complementary deep learning frameworks. First, AHP-CADNet is a
multi-level attention fusion framework for automated diagnosis that integrates
ocular landmarks, head pose features, and structured clinical attributes to
generate interpretable predictions. Second, a curriculum learning-based
imputation framework is designed to mitigate missing data by progressively
leveraging structured variables and unstructured clinical notes to enhance
diagnostic robustness under realistic data conditions. Evaluation on the
PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet
achieves 96.9-99.0 percent accuracy across classification tasks and low
prediction errors for continuous variables, with MAE ranging from 0.103 to
0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy
across all clinical variables (93.46-99.78 percent with PubMedBERT), with
clinical dependency modeling yielding significant improvements (p < 0.001).
These findings confirm the effectiveness of both frameworks for automated
diagnosis and recovery from missing data in clinical settings.

</details>


### [26] [EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario](https://arxiv.org/abs/2510.05650)
*Yiping Ma,Shiyu Hu,Buyuan Zhu,Yipei Wang,Yaxuan Kang,Shiqing Liu,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduVerse是首个支持用户自定义、多代理与人机融合的教育仿真平台，基于CIE架构实现课堂长期演化仿真，验证显示教学与互动指标与真实课堂相符并能捕捉跨会话学习轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有教育AI多局限于短期或单代理设置，难以同时重现开放式认知、动态社交、情感与长期发展，故需可自定义的多代理仿真平台。

Method: 提出分层CIE（认知-互动-演化）架构与人机交互接口，支持环境、代理和会话的用户自定义，多会话、多代理、以及真实用户参与。

Result: 在中学语文课堂（多文本体裁、多环境、多会话）验证，IRF率、网络密度与跨会话正向过渡率R+等指标显示模拟与真实课堂相近并能捕捉纵向行为/情感/认知变化。

Conclusion: EduVerse实现了课堂认知、互动与长期演化的联合仿真，展示了较高的教学逼真度、群体互动与跨会话发展能力。

Abstract: Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.

</details>


### [27] [SD-MVSum: Script-Driven Multimodal Video Summarization Method and Datasets](https://arxiv.org/abs/2510.05652)
*Manolis Mylonas,Charalampia Zerva,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出SD-MVSum：利用加权跨模态注意力将脚本与视频帧及转录文本的语义相似性融合用于脚本驱动多模态视频摘要，并扩展了两个数据集；在实验中性能具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 原方法仅利用视觉信息，忽视了视频中的语音/转录内容与用户脚本之间的重要语义对应；通过加入转录文本并显式建模与脚本的相关性，可生成更符合用户意图的摘要。

Method: 引入一种加权跨模态注意力机制，分别计算脚本-视频和脚本-转录文本对的相似性权重，以突出与脚本语义最相关的视频片段；并在原始单模态方法基础上将转录文本作为额外模态融合进模型，同时扩展并使用S-VideoXum与MrHiSum数据集进行训练与评估。

Result: SD-MVSum在脚本驱动的视频摘要任务中，通过加权跨模态注意力同时建模脚本-视频和脚本-转录文本的相关性，从而利用多模态语义相似性提升摘要对用户脚本的响应性。作者还扩展了两个大型数据集（S-VideoXum, MrHiSum）以支持多模态脚本驱动摘要的训练与评估，并在实验中展示了方法在与SOTA方法比较中的竞争性。

Conclusion: SD-MVSum通过显式建模脚本与视频与转录文本间的语义相似性，有效提升了脚本驱动视频摘要的相关性，并且在扩展数据集上表现出与现有SOTA方法相当的性能，具有实用价值与可复现性（代码与数据已发布）。

Abstract: In this work, we extend a recent method for script-driven video
summarization, originally considering just the visual content of the video, to
take into account the relevance of the user-provided script also with the
video's spoken content. In the proposed method, SD-MVSum, the dependence
between each considered pair of data modalities, i.e., script-video and
script-transcript, is modeled using a new weighted cross-modal attention
mechanism. This explicitly exploits the semantic similarity between the paired
modalities in order to promote the parts of the full-length video with the
highest relevance to the user-provided script. Furthermore, we extend two
large-scale datasets for video summarization (S-VideoXum, MrHiSum), to make
them suitable for training and evaluation of script-driven multimodal video
summarization methods. Experimental comparisons document the competitiveness of
our SD-MVSum method against other SOTA approaches for script-driven and generic
video summarization. Our new method and extended datasets are available at:
https://github.com/IDT-ITI/SD-MVSum.

</details>


### [28] [A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer](https://arxiv.org/abs/2510.05657)
*Anwen Lu,Mingxin Liu,Yiping Jiao,Hongyi Gong,Geyang Xu,Jun Chen,Jun Xu*

Main category: cs.CV

TL;DR: ARGUS通过微几何、层级视野对齐与几何先验融合，捕获WSIs的宏-中-微层次信息，显著提升肝癌组织学亚型判别性能并达成SOTA


<details>
  <summary>Details</summary>
Motivation: 提升肝癌（HCC与ICC）在全切片图像（WSIs）上的组织学亚型判别精度，通过更好地表征宏中微层次结构、肿瘤微环境（TME）和几何信息

Method: 构建细胞核几何结构的微几何特征；设计Hierarchical FoVs Alignment模块建模宏/中层交互；采用Geometry Prior Guided Fusion将微几何与FoVs特征融合形成整体表型表示

Result: 提出ARGUS方法：构建微几何特征（基于细胞核几何结构）、设计层级视野对齐模块（Hierarchical FoVs Alignment）以捕获宏/中层交互，并通过几何先验引导融合策略将微几何与FoVs特征联合表示

Conclusion: ARGUS在公开和私有队列上实现了SOTA肝癌组织学亚型分类，展示了对临床诊断的潜在价值

Abstract: Primary liver malignancies are widely recognized as the most heterogeneous
and prognostically diverse cancers of the digestive system. Among these,
hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge
as the two principal histological subtypes, demonstrating significantly greater
complexity in tissue morphology and cellular architecture than other common
tumors. The intricate representation of features in Whole Slide Images (WSIs)
encompasses abundant crucial information for liver cancer histological
subtyping, regarding hierarchical pyramid structure, tumor microenvironment
(TME), and geometric representation. However, recent approaches have not
adequately exploited these indispensable effective descriptors, resulting in a
limited understanding of histological representation and suboptimal subtyping
performance. To mitigate these limitations, ARGUS is proposed to advance
histological subtyping in liver cancer by capturing the macro-meso-micro
hierarchical information within the TME. Specifically, we first construct a
micro-geometry feature to represent fine-grained cell-level pattern via a
geometric structure across nuclei, thereby providing a more refined and precise
perspective for delineating pathological images. Then, a Hierarchical
Field-of-Views (FoVs) Alignment module is designed to model macro- and
meso-level hierarchical interactions inherent in WSIs. Finally, the augmented
micro-geometry and FoVs features are fused into a joint representation via
present Geometry Prior Guided Fusion strategy for modeling holistic phenotype
interactions. Extensive experiments on public and private cohorts demonstrate
that our ARGUS achieves state-of-the-art (SOTA) performance in histological
subtyping of liver cancer, which provide an effective diagnostic tool for
primary liver malignancies in clinical practice.

</details>


### [29] [Teleportraits: Training-Free People Insertion into Any Scene](https://arxiv.org/abs/2510.05660)
*Jialu Gao,K J Joseph,Fernando De La Torre*

Main category: cs.CV

TL;DR: 作者首次提出基于预训练扩散模型的训练-free人物插入方法，结合反演、无分类器引导与掩码自注意力，在单张参考图像下实现可用性感知的场景级编辑与高质量个性化，达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将人物定位/姿态确定和个性化处理视为独立问题并依赖训练，而忽视两者间的联系；作者认为预训练扩散模型已隐含场景放置能力，故探索无需额外训练即可完成高质量插入。

Method: 结合反演技术（inversion）与无分类器引导（classifier-free guidance）实现符合场景可用性的全局编辑；并提出基于掩码的自注意力（mask-guided self-attention）机制以在单张参考图像条件下实现高质量个性化（保持人物身份、服饰和身体特征）。

Result: 在多样的合成场景中取得了最先进的性能，能在保持背景一致性的同时高质量保留被插入人物的身份信息，达到了逼真的人物插入效果。

Conclusion: 该论文提出了一个无训练（training-free）的统一管线，利用预训练的文本到图像扩散模型实现将参考图像中的人物逼真插入背景场景。

Abstract: The task of realistically inserting a human from a reference image into a
background scene is highly challenging, requiring the model to (1) determine
the correct location and poses of the person and (2) perform high-quality
personalization conditioned on the background. Previous approaches often treat
them as separate problems, overlooking their interconnections, and typically
rely on training to achieve high performance. In this work, we introduce a
unified training-free pipeline that leverages pre-trained text-to-image
diffusion models. We show that diffusion models inherently possess the
knowledge to place people in complex scenes without requiring task-specific
training. By combining inversion techniques with classifier-free guidance, our
method achieves affordance-aware global editing, seamlessly inserting people
into scenes. Furthermore, our proposed mask-guided self-attention mechanism
ensures high-quality personalization, preserving the subject's identity,
clothing, and body features from just a single reference image. To the best of
our knowledge, we are the first to perform realistic human insertions into
scenes in a training-free manner and achieve state-of-the-art results in
diverse composite scene images with excellent identity preservation in
backgrounds and subjects.

</details>


### [30] [When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach](https://arxiv.org/abs/2510.05661)
*Daniel Gonzálbez-Biosca,Josep Cabacas-Maso,Carles Ventura,Ismael Benito-Altamirano*

Main category: cs.CV

TL;DR: 提出用于古典音乐多机位视频自动剪辑的多模态架构：音频（log-mel）、可选图像嵌入和时间标量特征，通过轻量级卷积-Transformer进行“何时切换”分割；“如何切换”使用CLIP编码器并限制干扰片段为同一音乐会片段；采用伪标注聚类构建数据集。模型在切点检测和画面选择上优于基线。


<details>
  <summary>Details</summary>
Motivation: 多机位古典音乐会剪辑需要在音频与视觉之间做出精细决策，但现有工作多聚焦生成与理解，未充分研究自动剪辑。作者目标是分解任务（何时/如何切）并利用多模态信号与现代编码器提升性能。

Method: 1) 时间分割（何时切换）：输入log-mel频谱、可选图像嵌入和时间标量特征，使用轻量级卷积层提取局部特征，再用Transformer建模长时依赖，输出切点概率；2) 画面选择（如何切换）：用CLIP图像编码器提取视觉嵌入，限制候选干扰片段来自同一音乐会，进行相似度或学习式排序选择；3) 数据集：通过自动聚类将原始视频切分为镜头伪标签，用于训练与评估。

Result: 模型在检测剪辑时间点上优于以往基线，视觉选镜也达到具有竞争力的效果；伪标注的数据集证明了自动聚类方法可行。

Conclusion: 本文在多模态自动视频剪辑任务上取得进展：提出的时间分割网络和基于CLIP的画面选择策略在切点检测与画面选择上提升了性能，数据集构建的伪标注方法有效。

Abstract: Automated video editing remains an underexplored task in the computer vision
and multimedia domains, especially when contrasted with the growing interest in
video generation and scene understanding. In this work, we address the specific
challenge of editing multicamera recordings of classical music concerts by
decomposing the problem into two key sub-tasks: when to cut and how to cut.
Building on recent literature, we propose a novel multimodal architecture for
the temporal segmentation task (when to cut), which integrates log-mel
spectrograms from the audio signals, plus an optional image embedding, and
scalar temporal features through a lightweight convolutional-transformer
pipeline. For the spatial selection task (how to cut), we improve the
literature by updating from old backbones, e.g. ResNet, with a CLIP-based
encoder and constraining distractor selection to segments from the same
concert. Our dataset was constructed following a pseudo-labeling approach, in
which raw video data was automatically clustered into coherent shot segments.
We show that our models outperformed previous baselines in detecting cut points
and provide competitive visual shot selection, advancing the state of the art
in multimodal automated video editing.

</details>


### [31] [Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis](https://arxiv.org/abs/2510.05668)
*M. Torrente,A. Follador,A. Calcante,P. Casati,R. Oberti*

Main category: cs.CV

TL;DR: 提出一种时序融合的低成本图像分析流程，能在叶片重叠情况下准确计数并评估生长力，验证R. solani显著抑制菊苣类(Lactuca sativa)种子萌发与幼苗活力，方法表现(R2=0.98, RMSE=1.12)可靠


<details>
  <summary>Details</summary>
Motivation: Assess impact of R. solani on lettuce germination and early growth using low-cost continuous imaging and robust analysis to handle overlapping seedlings

Method: image-based time-integrated seedling phenotyping with R. solani inoculation

Result: Developed temporal-integrated image analysis combining morphological and spatial features, enabling accurate counting and vigor metrics under challenging overlap; found R. solani reduces germination and vigor; achieved R^2=0.98 and RMSE=1.12

Conclusion: 时序整合的图像分析可在非破坏性、可扩展的低成本监测系统中准确量化萌发率与出苗时点，尤其在后期重叠严重时优于传统分割技术，适用于病原影响表型研究

Abstract: The study investigates the effects of R. solani inoculation on the
germination and early development of Lactuca sativa L. seeds using a low-cost,
image-based monitoring system. Multiple cameras were deployed to continuously
capture images of the germination process in both infected and control groups.
The objective was to assess the impact of the pathogen by analyzing germination
dynamics and growth over time. To achieve this, a novel image analysis pipeline
was developed. The algorithm integrates both morphological and spatial features
to identify and quantify individual seedlings, even under complex conditions
where traditional image analyses fails. A key innovation of the method lies in
its temporal integration: each analysis step considers not only the current
status but also their developmental across prior time points. This approach
enables robust discrimination of individual seedlings, especially when
overlapping leaves significantly hinder object separation. The method
demonstrated high accuracy in seedling counting and vigor assessment, even in
challenging scenarios characterized by dense and intertwined growth. Results
confirm that R. solani infection significantly reduces germination rates and
early seedling vigor. The study also validates the feasibility of combining
low-cost imaging hardware with advanced computational tools to obtain
phenotyping data in a non-destructive and scalable manner. The temporal
integration enabled accurate quantification of germinated seeds and precise
determination of seedling emergence timing. This approach proved particularly
effective in later stages of the experiment, where conventional segmentation
techniques failed due to overlapping or intertwined seedlings, making accurate
counting. The method achieved a coefficient of determination of 0.98 and a root
mean square error (RMSE) of 1.12, demonstrating its robustness and reliability.

</details>


### [32] [Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension](https://arxiv.org/abs/2510.05674)
*Jike Zhong,Yuxiang Lai,Xiaofeng Yang,Konstantinos Psounis*

Main category: cs.CV

TL;DR: 用对象而非patch作为视觉token进行MIM可显著增强视觉模型的语义理解与推理能力，对多模态任务有明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前ViT基于空间patch的训练缺乏语义与上下文引导，导致无法获得类似语言模型的推理与in-context学习能力；而语言模型通过词作为基本语义单元自然学习真实分布，启发在视觉端以对象作为“词”进行建模。

Method: 在掩码图像建模（MIM）框架中，以对象为单位而非随机空间patch进行掩码与重建，构建语义驱动的目标；并通过定性与定量实验、以及与多模态大模型在VQA等任务上的评测验证方法有效性。

Result: 实验表明：对象级表示能学习更接近真实世界的分布，避免像素平均等捷径；在VQA、GQA、ScienceQA等多模态问答任务上，与MLLM联合评估显示显著提升的推理与语境理解能力。

Conclusion: 本文提出通过以“对象”为视觉等价词替代像素patch进行掩码建模，从而弥合视觉模型与语言模型在语义与上下文学习上的差距，显著提升视觉编码器的推理与上下文理解能力。

Abstract: Recent advances in language modeling have witnessed the rise of highly
desirable emergent capabilities, such as reasoning and in-context learning.
However, vision models have yet to exhibit comparable progress in these areas.
In this paper, we argue that this gap could stem from the lack of semantic and
contextual guidance in current vision transformer (ViT) training schemes, and
such a gap can be narrowed through the design of a semantic-grounded objective.
Specifically, we notice that individual words in natural language are
inherently semantic, and modeling directly on word tokens naturally learns a
realistic distribution. In contrast, ViTs rely on spatial patchification, which
inevitably lacks semantic information. To bridge this gap, we propose to
directly model "object" as the visual equivalence of "word," pushing the model
to learn the global context and semantics among visual elements. We investigate
our hypotheses via masked image modeling (MIM), a framework where our approach
can be readily tested by applying masks to visual objects rather than random
patches. Considerable evidence from qualitative and quantitative evaluations
reveals a key finding: object-level representation alone helps to learn a
real-world distribution, whereas pixel-averaging shortcuts are often learned
without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual
question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning
and contextual understanding gained with this simple objective. We hope our
study highlights the effectiveness of object-level encoding and provides a
plausible direction for developing stronger vision encoders and tokenizers.
Code and model will be publicly released. Keywords: Semantic Visual Tokenizer,
Vision Reasoning, In-context Learning, Multimodal Reasoning

</details>


### [33] [AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models](https://arxiv.org/abs/2510.05715)
*Shihao Zhu,Bohan Cao,Ziheng Ouyang,Zhen Li,Peng-Tao Jiang,Qibin Hou*

Main category: cs.CV

TL;DR: AgeBooth利用年龄条件prompt混合与SVDMix的LoRA融合，在无需大量年龄标注数据情况下，增强基于adapter的身份个性化扩散模型的年龄控制能力，能从单张照片生成不同年龄且保持身份一致的高质量人像。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在保持身份一致性的同时难以精确控制年龄，且微调通常需要昂贵的跨年龄成对数据。为此提出无需大量年龄标注的高效微调方法，改善年龄编辑的精度和视觉质量，同时节省数据和计算成本。

Method: 方法包括：1）利用年龄条件的prompt混合（age-conditioned prompt blending）来线性结合不同年龄的语义控制；2）提出年龄特定的LoRA融合策略，采用SVDMix矩阵融合技术对多个LoRA进行融合以生成中间年龄效果；3）基于adapter/LoRA个性化微调流程对单张参考图进行身份保持微调，减少对大规模年龄标注数据的依赖。

Result: AgeBooth提出了一种针对年龄控制的微调方法，基于adapter/LoRA个性化模型，通过年龄条件的prompt混合和基于SVDMix的LoRA融合来生成不同年龄的高质量、保身份一致性的面部图像，从单张参考图生成多年龄段人像，不依赖大量带年龄标签的数据。

Conclusion: AgeBooth在无需昂贵年龄成对数据的前提下，通过年龄条件的prompt混合和年龄特定的LoRA融合有效提升了年龄控制能力，实验显示在年龄控制精度和视觉质量上优于现有编辑方法。

Abstract: Recent diffusion model research focuses on generating identity-consistent
images from a reference photo, but they struggle to accurately control age
while preserving identity, and fine-tuning such models often requires costly
paired images across ages. In this paper, we propose AgeBooth, a novel
age-specific finetuning approach that can effectively enhance the age control
capability of adapterbased identity personalization models without the need for
expensive age-varied datasets. To reduce dependence on a large amount of
age-labeled data, we exploit the linear nature of aging by introducing
age-conditioned prompt blending and an age-specific LoRA fusion strategy that
leverages SVDMix, a matrix fusion technique. These techniques enable
high-quality generation of intermediate-age portraits. Our AgeBooth produces
realistic and identity-consistent face images across different ages from a
single reference image. Experiments show that AgeBooth achieves superior age
control and visual quality compared to previous state-of-the-art editing-based
methods.

</details>


### [34] [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/abs/2510.05722)
*Jiaojiao Ye,Jiaxing Zhong,Qian Xie,Yuzhou Zhou,Niki Trigoni,Andrew Markham*

Main category: cs.CV

TL;DR: 训练-free地结合ControlNet与VLM生成带像素标签的高质量合成数据，提升一-shot语义分割表现。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强难以操控高层语义（材质、纹理），现有扩散模型方法要么计算开销大，要么性能受限；因此提出一个高效、训练-free的方案，以自动生成带标签的合成数据提高下游任务表现。

Method: 利用预训练ControlNet做条件图像生成，结合VLM进行多向文本提示扩展以增加语义多样性；通过掩码生成器获得像素级标签，并用高质量图像选择模块筛除低质样本；整个流程无需额外训练。

Result: 提出了一个无训练管线，结合预训练ControlNet与视觉语言模型生成带像素级标签的合成图像，用于数据增强以提升少样本语义分割。加入多向提示生成器、掩码生成器和高质量图像选择模块以提升多样性与质量，在PASCAL-5i和COCO-20i上进行一-shot分割任务评估，超越同时期工作。

Conclusion: 方法能在不需人工标注的情况下生成高保真、多样性的带标签合成图像，显著改善一-shot语义分割，在PASCAL-5i与COCO-20i上优于同期方法。

Abstract: Generating enough and diverse data through augmentation offers an efficient
solution to the time-consuming and labour-intensive process of collecting and
annotating pixel-wise images. Traditional data augmentation techniques often
face challenges in manipulating high-level semantic attributes, such as
materials and textures. In contrast, diffusion models offer a robust
alternative, by effectively utilizing text-to-image or image-to-image
transformation. However, existing diffusion-based methods are either
computationally expensive or compromise on performance. To address this issue,
we introduce a novel training-free pipeline that integrates pretrained
ControlNet and Vision-Language Models (VLMs) to generate synthetic images
paired with pixel-level labels. This approach eliminates the need for manual
annotations and significantly improves downstream tasks. To improve the
fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and
High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i
present promising performance and outperform concurrent work for one-shot
semantic segmentation.

</details>


### [35] [Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect](https://arxiv.org/abs/2510.05740)
*Amirtaha Amanzadi,Zahra Dehghanian,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 构建了包含12个生成器的OmniGen基准，并提出基于CLIP与DINOv2特征融合的FusionDetect，实现更好的跨生成器与跨域合成图像检测性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前工作侧重于跨生成器泛化，但忽视了视觉域（图像内容/风格）间的泛化，需更现实的基准与方法以提高检测器的普适性。

Method: 提出FusionDetect，基于两个冻结的基础模型CLIP与DINOv2提取互补特征，构建统一特征空间并进行检测；并引入OmniGen基准包含12个生成器用于评估。

Result: 在既有基准上平均精度/准确度分别提升约6.13%和3.87%，在OmniGen上准确度提升4.48%，且对常见图像扰动表现出较强鲁棒性。

Conclusion: 本文提出的FusionDetect在跨生成器与跨视觉域的合成图像检测问题上取得了显著进展，达到了新的SOTA并提升了鲁棒性。

Abstract: The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect

</details>


### [36] [ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving](https://arxiv.org/abs/2510.05752)
*Yongxuan Lyu,Guangfeng Jiang,Hongsi Liu,Jun Liu*

Main category: cs.CV

TL;DR: 引入ALISE：利用视觉基础模型生成伪标签，结合时空投票和2D/3D语义损失以及原型对比损失，实现无监督LiDAR实例分割并取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 手工标注室外LiDAR点云用于实例分割代价高且耗时，目标是完全去除对人工标注的依赖，通过多模态和时空一致性自动生成高质量伪标签以实现无监督学习。

Method: 1) 用视觉基础模型（文本+图像）生成初始2D/3D伪标签；2) 时空投票模块融合2D和3D语义对伪标签进行离线和在线优化；3) 设计2D先验损失将视觉知识注入3D网络；4) 提出基于原型的对比损失利用3D语义一致性构建区分性特征空间。

Result: ALISE提出了一个无标注的LiDAR实例分割框架，通过VFM生成初始伪标签，结合时空投票与2D/3D语义优化，及两种语义监督（2D先验损失与基于原型的对比损失）提升特征学习，最终在无监督设置下达到新的SOTA并优于部分有2D监督方法。

Conclusion: ALISE能在完全无标注下通过多模态伪标签生成与语义一致性优化，显著提高无监督3D实例分割性能，并能超越某些使用2D标注的基线。

Abstract: The manual annotation of outdoor LiDAR point clouds for instance segmentation
is extremely costly and time-consuming. Current methods attempt to reduce this
burden but still rely on some form of human labeling. To completely eliminate
this dependency, we introduce ALISE, a novel framework that performs LiDAR
instance segmentation without any annotations. The central challenge is to
generate high-quality pseudo-labels in a fully unsupervised manner. Our
approach starts by employing Vision Foundation Models (VFMs), guided by text
and images, to produce initial pseudo-labels. We then refine these labels
through a dedicated spatio-temporal voting module, which combines 2D and 3D
semantics for both offline and online optimization. To achieve superior feature
learning, we further introduce two forms of semantic supervision: a set of 2D
prior-based losses that inject visual knowledge into the 3D network, and a
novel prototype-based contrastive loss that builds a discriminative feature
space by exploiting 3D semantic consistency. This comprehensive design results
in significant performance gains, establishing a new state-of-the-art for
unsupervised 3D instance segmentation. Remarkably, our approach even
outperforms MWSIS, a method that operates with supervision from ground-truth
(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).

</details>


### [37] [OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search](https://arxiv.org/abs/2510.05759)
*Zexin Zheng,Huangyu Dai,Lingtao Mao,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.CV

TL;DR: 提出了OneVision端到端生成框架，通过VRQ（视觉对齐残差量化编码）和多阶段语义对齐实现检索与个性化统一，线上A/B显示CTR、CVR和订单量显著提升，并提升推理效率21%。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段级联架构在多视图表示差异和分阶段优化目标冲突下难以同时兼顾用户体验和转化，需要一种能够统一检索与个性化并简化服务路径的端到端方案。

Method: 基于VRQ构建视觉对齐的残差量化编码以统一多视图表示，随后采用多阶段语义对齐方案在不同服务阶段维持视觉相似性并融入用户偏好，采用动态剪枝提升推理效率。

Result: 离线表现与线上多阶段架构相当，推理效率提升21%；A/B测试显示+2.15% item CTR、+2.27% CVR、+3.12% 订单量，证明方法有效。

Conclusion: OneVision通过视觉对齐的残差量化编码和多阶段语义对齐，在保持视觉相似性先验的同时引入个性化信息，实现了检索与个性化的统一，线上实验显示显著提升实际业务指标，且推理更高效。

Abstract: Traditional vision search, similar to search and recommendation systems,
follows the multi-stage cascading architecture (MCA) paradigm to balance
efficiency and conversion. Specifically, the query image undergoes feature
extraction, recall, pre-ranking, and ranking stages, ultimately presenting the
user with semantically similar products that meet their preferences. This
multi-view representation discrepancy of the same object in the query and the
optimization objective collide across these stages, making it difficult to
achieve Pareto optimality in both user experience and conversion. In this
paper, an end-to-end generative framework, OneVision, is proposed to address
these problems. OneVision builds on VRQ, a vision-aligned residual quantization
encoding, which can align the vastly different representations of an object
across multiple viewpoints while preserving the distinctive features of each
product as much as possible. Then a multi-stage semantic alignment scheme is
adopted to maintain strong visual similarity priors while effectively
incorporating user-specific information for personalized preference generation.
In offline evaluations, OneVision performs on par with online MCA, while
improving inference efficiency by 21% through dynamic pruning. In A/B tests, it
achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and
+3.12% order volume. These results demonstrate that a semantic ID centric,
generative architecture can unify retrieval and personalization while
simplifying the serving pathway.

</details>


### [38] [A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data](https://arxiv.org/abs/2510.05760)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 通过估计并嵌入弱标注源的标签转移矩阵，在梯度层面按来源与类别对样本加权，提出一种能利用大量不可靠标注提升遥感场景分类性能的鲁棒训练策略。


<details>
  <summary>Details</summary>
Motivation: 深度网络在遥感场景分类上对大量高质量标注高度依赖，而高质量标注获取成本高且数量有限；同时存在大量低成本但不可靠的标注来源。如何利用这些弱标注来扩展训练集并同时减轻标签噪声对模型的负面影响，是该工作的主要动机。

Method: 方法包括：1) 将多个弱来源（例如过时数字地图）与少量高质量标注合并生成多来源标注数据集；2) 估计或利用每个弱来源的标签错误转移矩阵，描述真实标签到观测标签的概率；3) 在训练过程中将转移矩阵嵌入标签并用于为每个样本和每个类别计算梯度权重，使每个实例对不同类别的损失贡献不同；4) 以该加权梯度机制训练深度网络。

Result: 在多组数据集上的实验表明，所提方法在存在噪声标签的情形下具有更好的鲁棒性和性能提升，能有效利用不可靠标注源提升分类效果。

Conclusion: 该论文提出了一种在遥感影像场景分类中利用弱标注源与少量可靠标注结合的训练策略，通过引入和嵌入每个弱标注源的错误统计（转移矩阵）来按来源对标签加权，从而在梯度层面实现按类别的加权优化，提高了对噪声标签的鲁棒性。

Abstract: Deep learning has gained broad interest in remote sensing image scene
classification thanks to the effectiveness of deep neural networks in
extracting the semantics from complex data. However, deep networks require
large amounts of training samples to obtain good generalization capabilities
and are sensitive to errors in the training labels. This is a problem in remote
sensing since highly reliable labels can be obtained at high costs and in
limited amount. However, many sources of less reliable labeled data are
available, e.g., obsolete digital maps. In order to train deep networks with
larger datasets, we propose both the combination of single or multiple weak
sources of labeled data with a small but reliable dataset to generate
multisource labeled datasets and a novel training strategy where the
reliability of each source is taken in consideration. This is done by
exploiting the transition matrices describing the statistics of the errors of
each source. The transition matrices are embedded into the labels and used
during the training process to weigh each label according to the related
source. The proposed method acts as a weighting scheme at gradient level, where
each instance contributes with different weights to the optimization of
different classes. The effectiveness of the proposed method is validated by
experiments on different datasets. The results proved the robustness and
capability of leveraging on unreliable source of labels of the proposed method.

</details>


### [39] [Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection](https://arxiv.org/abs/2510.05782)
*I. M. De la Jara,C. Rodriguez-Opazo,D. Teney,D. Ranasinghe,E. Abbasnejad*

Main category: cs.CV

TL;DR: 本文指出仅使用预训练模型的最后一层进行OOD检测不够，发现残差连接下的中间层表示能提供丰富多样的OOD信号。提出一种基于熵的无训练方法，自动选择具有互补信息的中间层（无需OOD数据）。在多种模型和任务上，该方法在远/OOD和近/OOD基准上分别可提升最多10%和7%的检测精度，拓展了OOD检测研究方向，并分析了训练目标与模型架构对基于置信度的OOD方法的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖预训练模型最后一层，忽视中间层可能携带的有助于检测分布偏移的信号；希望在不需额外训练或OOD数据的前提下，利用这些潜在信号提高检测性能。

Method: 分析残差连接如何在中间层生成多样化表示；提出一个基于熵的训练-free层选择准则来挑选互补的中间层表示，并将这些表示结合到现有置信度/OOD评分方法中进行评估。

Result: 在多种模型架构和训练目标上，所提方法在远O0D基准上最多提升10%，近OOD提升超7%，优于当前最先进的训练-free方法，并揭示训练目标和架构对结果的影响。

Conclusion: 中间层表示在OOD检测中能提供重要且互补的信息；通过基于熵的自动层选择，在无需额外训练或OOD数据的条件下，能显著提升检测性能。

Abstract: Out-of-distribution (OOD) detection is essential for reliably deploying
machine learning models in the wild. Yet, most methods treat large pre-trained
models as monolithic encoders and rely solely on their final-layer
representations for detection. We challenge this wisdom. We reveal the
\textit{intermediate layers} of pre-trained models, shaped by residual
connections that subtly transform input projections, \textit{can} encode
\textit{surprisingly rich and diverse signals} for detecting distributional
shifts. Importantly, to exploit latent representation diversity across layers,
we introduce an entropy-based criterion to \textit{automatically} identify
layers offering the most complementary information in a training-free setting
-- \textit{without access to OOD data}. We show that selectively incorporating
these intermediate representations can increase the accuracy of OOD detection
by up to \textbf{$10\%$} in far-OOD and over \textbf{$7\%$} in near-OOD
benchmarks compared to state-of-the-art training-free methods across various
model architectures and training objectives. Our findings reveal a new avenue
for OOD detection research and uncover the impact of various training
objectives and model architectures on confidence-based OOD detection methods.

</details>


### [40] [Rasterized Steered Mixture of Experts for Efficient 2D Image Regression](https://arxiv.org/abs/2510.05814)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Mårten Sjöström*

Main category: cs.CV

TL;DR: 提出将 Steered Mixture of Experts（SMoE）与光栅化高斯核渲染相结合，通过边缘感知门控和光栅化优化替代全局迭代，显著加速二维图像回归并节省内存，同时保留稀疏性与重建质量，支持原生超分与去噪等应用。


<details>
  <summary>Details</summary>
Motivation: 传统 SMoE 虽性能好，但因全局迭代优化代价高、内存占用大，限制了实际应用；因此需要一种既保留边缘感知稀疏性又能显著加速并节省内存的优化方法。

Method: 将 SMoE 的高斯裸核渲染离散化为光栅化过程：使用光栅化的高斯核渲染快速计算像素贡献，结合基于图像边缘的门控网络决定专家权重；用局部/像素级的光栅化更新替代全局迭代优化以加速训练并减少内存。

Result: 相比标准 SMoE 或纯光栅化高斯核方法，该方法在相似重建质量下实现更快的参数更新、更低的内存占用，并能原生支持超分与去噪等任务。

Conclusion: 光栅化驱动的优化在保持 SMoE 的边缘感知稀疏结构下，能大幅提升参数更新速度与内存效率，并扩展到超分和去噪任务，从而在效率与重建精度间取得更好平衡。

Abstract: The Steered Mixture of Experts regression framework has demonstrated strong
performance in image reconstruction, compression, denoising, and
super-resolution. However, its high computational cost limits practical
applications. This work introduces a rasterization-based optimization strategy
that combines the efficiency of rasterized Gaussian kernel rendering with the
edge-aware gating mechanism of the Steered Mixture of Experts. The proposed
method is designed to accelerate two-dimensional image regression while
maintaining the model's inherent sparsity and reconstruction quality. By
replacing global iterative optimization with a rasterized formulation, the
method achieves significantly faster parameter updates and more
memory-efficient model representations. In addition, the proposed framework
supports applications such as native super-resolution and image denoising,
which are not directly achievable with standard rasterized Gaussian kernel
approaches. The combination of fast rasterized optimization with the edge-aware
structure of the Steered Mixture of Experts provides a new balance between
computational efficiency and reconstruction fidelity for two-dimensional image
processing tasks.

</details>


### [41] [Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images](https://arxiv.org/abs/2510.05819)
*Sven Koehler,Sarah Kaye Mueller,Jonathan Kiekenap,Gerald Greil,Tarique Hussain,Samir Sarikouch,Florian André,Norbert Frey,Sandy Engelhardt*

Main category: cs.CV

TL;DR: 本文提出一种自监督深度学习方法，通过从SAX和4CH cine CMR图像生成稠密形变配准场并计算一维运动描述子，进而基于简单规则检测五个或更多心脏关键帧（含ED、ES及三处中间帧），在多中心、多病种公开数据集上相比基于容积曲线的方法显著提高检测精度（cFD降低30–51%在SAX，11–47%在4CH），实现帧级对齐以便跨个体和个体内的心动学分析。


<details>
  <summary>Details</summary>
Motivation: 单个心动周期差异使得自动时序对比与子相分析变得困难；仅从左室容积曲线推断ED/ES不能充分刻画心肌运动，需更精细的关键帧检测以支持更深入的动力学分析。

Method: 先对cine CMR图像进行密集可变形配准以得到位移场，从位移场提取一维全局运动描述曲线（反映收缩-舒张模式），然后用一组简单规则从曲线中定位关键帧（ED、ES及三处中间帧）；模型自监督训练并在多个公开数据集上评估。

Result: 在M&Ms-2训练/评估集（n=360）以及M&Ms（n=345）、ACDC（n=100）和GCN先天性心脏病集合上独立验证，cFD平均低于1.31帧（SAX）和1.73帧（LAX）；相对于容积曲线方法，ED/ES检测cFD提升30–51%（SAX）和11–47%（4CH）。

Conclusion: 自监督形变场驱动的一维运动描述子可可靠检测多视图心脏关键帧，优于传统基于左室容积曲线的方法，并在多数据集、多病种上展示了良好泛化性，可用于精确的时序对齐与心肌运动分析。

Abstract: Cardiovascular magnetic resonance (CMR) is the gold standard for assessing
cardiac function, but individual cardiac cycles complicate automatic temporal
comparison or sub-phase analysis. Accurate cardiac keyframe detection can
eliminate this problem. However, automatic methods solely derive end-systole
(ES) and end-diastole (ED) frames from left ventricular volume curves, which do
not provide a deeper insight into myocardial motion. We propose a
self-supervised deep learning method detecting five keyframes in short-axis
(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable
registration fields are derived from the images and used to compute a 1D motion
descriptor, which provides valuable insights into global cardiac contraction
and relaxation patterns. From these characteristic curves, keyframes are
determined using a simple set of rules. The method was independently evaluated
for both views using three public, multicentre, multidisease datasets. M&Ms-2
(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC
(n=100) datasets for repeatability control. Furthermore, generalisability to
patients with rare congenital heart defects was tested using the German
Competence Network (GCN) dataset. Our self-supervised approach achieved
improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED
and ES, as measured by cyclic frame difference (cFD), compared with the
volume-based approach. We can detect ED and ES, as well as three additional
keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for
SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and
intra-patient analysis of cardiac dynamics, irrespective of cycle or phase
lengths. GitHub repository:
https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git

</details>


### [42] [Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow](https://arxiv.org/abs/2510.05836)
*Ruyang Liu,Shangkun Sun,Haoran Tang,Ge Li,Wei Gao*

Main category: cs.CV

TL;DR: 本文创新性地将光流运动先验引入长视频理解，通过TGO和MTP两步时空冗余压缩，显著提升了多模态LLM在小时级视频任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖CLIP等语义先验来提取关键视觉信息，但忽视了运动信息的重要性；光流作为运动先验能更好地指出时序与局部运动变化，帮助压缩冗余并保留关键信息。

Method: 提出了两个核心模块：Temporal Granularity Optimization (TGO)利用粗粒度光流进行帧分组并结合语义先验筛除不相关场景；Motion Token Pruning (MTP)基于细粒度光流对帧内视觉token进行冗余剪枝。

Result: 在多项长视频多模态LLM基准上取得领先性能，尤其在小时级视频理解任务上表现显著：Video-MME 64.7%、MLVU 71.4%、LongVideoBench 60.4%。

Conclusion: Flow4Agent通过引入光流运动先验，有效缓解了长视频中时空冗余问题，从而提升了基于LLM的长视频理解性能。

Abstract: Long-form video understanding has always been a challenging problem due to
the significant redundancy in both temporal and spatial contents. This
challenge is further exacerbated by the limited context length of Multimodal
Large Language Models (MLLMs). To address this issue, many previous works have
attempted to extract key video information, where the "key" is typically
semantic-aware and heavily dependent on the CLIP model as prior. In this paper,
we propose Flow4Agent, a novel framework that pioneeringly incorporates motion
priors from optical flow to facilitate LLM-based long video understanding.
Flow4Agent mitigates the redundancy in long videos at both temporal and spatial
levels through two core modules: Temporal Granularity Optimization (TGO)
adaptively refines framelevel hierarchies, which first leverages coarse flow
priors to group similar visual contents and then applies semantic priors to
filter out highly irrelevant scene information. Motion Token Pruning (MTP)
further refines the intra-frame visual representations, pruning high-redundancy
video tokens using fine-grained optical flow information. Extensive experiments
demonstrate that our Flow4Agent outperforms existing methods across a wide
range of video MLLM benchmarks, especially for hour-level video understanding
tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.

</details>


### [43] [acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows](https://arxiv.org/abs/2510.05886)
*Johannes Seiffarth,Keitaro Kasahara,Michelle Bund,Benita Lückel,Richard D. Paul,Mathias Pesch,Lennart Witting,Michael Bott,Dietrich Kohlheyer,Katharina Nöh*

Main category: cs.CV

TL;DR: acia-workflows将深度学习分割/跟踪工具整合入用户友好的、可复现的Jupyter工作流，支持高通量微流控活细胞成像的数据分析并提供多种示例。


<details>
  <summary>Details</summary>
Motivation: 自动化处理大规模活细胞成像(LCI)数据，以便在单细胞水平实现时空表征，支持生物学研究的常规应用。

Method: 开发acia Python库集成八种深度学习分割与跟踪方法；使用Jupyter Notebook打包管道、依赖与可视化，构建可复现工作流；提供多种针对微流控LCI实验的应用示例。

Result: 提出acia-workflows平台：包括acia Python库（8种深度学习分割与跟踪方法）、将管道与依赖整合为Jupyter Notebook的工作流，以及多个示例应用工作流。公开提供10+个应用案例。

Conclusion: 该平台降低了复杂LCI数据分析的进入门槛，促进可复现、高通量的单细胞动力学研究，并通过开源工作流推动社区采纳与定制。

Abstract: Live-cell imaging (LCI) technology enables the detailed spatio-temporal
characterization of living cells at the single-cell level, which is critical
for advancing research in the life sciences, from biomedical applications to
bioprocessing. High-throughput setups with tens to hundreds of parallel cell
cultivations offer the potential for robust and reproducible insights. However,
these insights are obscured by the large amount of LCI data recorded per
experiment. Recent advances in state-of-the-art deep learning methods for cell
segmentation and tracking now enable the automated analysis of such large data
volumes, offering unprecedented opportunities to systematically study
single-cell dynamics. The next key challenge lies in integrating these powerful
tools into accessible, flexible, and user-friendly workflows that support
routine application in biological research. In this work, we present
acia-workflows, a platform that combines three key components: (1) the
Automated live-Cell Imaging Analysis (acia) Python library, which supports the
modular design of image analysis pipelines offering eight deep learning
segmentation and tracking approaches; (2) workflows that assemble the image
analysis pipeline, its software dependencies, documentation, and visualizations
into a single Jupyter Notebook, leading to accessible, reproducible and
scalable analysis workflows; and (3) a collection of application workflows
showcasing the analysis and customization capabilities in real-world
applications. Specifically, we present three workflows to investigate various
types of microfluidic LCI experiments ranging from growth rate comparisons to
precise, minute-resolution quantitative analyses of individual dynamic cells
responses to changing oxygen conditions. Our collection of more than ten
application workflows is open source and publicly available at
https://github.com/JuBiotech/acia-workflows.

</details>


### [44] [BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data](https://arxiv.org/abs/2510.05888)
*Arefin Ittesafun Abian,Debopom Sutradhar,Md Rafi Ur Rashid,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Kheng Cher Yeo,Sami Azam*

Main category: cs.CV

TL;DR: 提出首个基于多模态（图像+元数据）的BioAutoML-NAS，结合可微NAS与多模态融合，在两大昆虫数据集上显著提升分类性能，适用于可持续农业监测。


<details>
  <summary>Details</summary>
Motivation: 昆虫分类对农业和生态研究至关重要，但因昆虫复杂特征、类不平衡和大规模数据集而具有挑战性。因此需自动化、可扩展且能融合视觉与元数据的模型以提升效果并适应大规模生物影像数据。

Method: 基于可堆叠的可微NAS单元（cells），在每个连接处自动学习最优操作；使用交替的双层优化联合更新网络权重与架构参数；引入零操作以稀疏化连接；通过多模态融合模块将图像嵌入与元数据（类别性生物信息）融合用于分类。

Result: 在BIOSCAN-5M上：准确率96.81%、精确率97.46%、召回96.81%、F1 97.05%，相较于最先进的方法提升约16%、10%和8%（不明确对应指标）；在Insects-1M上：准确率93.25%、精确率93.71%、召回92.74%、F1 93.22%。

Conclusion: 本文提出的BioAutoML-NAS通过在图像子网络中引入神经网络结构搜索（NAS）并结合元数据的多模态融合，实现了高精度的昆虫分类。实验表明，在BIOSCAN-5M和Insects-1M数据集上均显著优于现有迁移学习、Transformer、AutoML和其他NAS方法，证明了方法在生物监测和农业管理中的实用性。

Abstract: Insect classification is important for agricultural management and ecological
research, as it directly affects crop health and production. However, this task
remains challenging due to the complex characteristics of insects, class
imbalance, and large-scale datasets. To address these issues, we propose
BioAutoML-NAS, the first BioAutoML model using multimodal data, including
images, and metadata, which applies neural architecture search (NAS) for images
to automatically learn the best operations for each connection within each
cell. Multiple cells are stacked to form the full network, each extracting
detailed image feature representations. A multimodal fusion module combines
image embeddings with metadata, allowing the model to use both visual and
categorical biological information to classify insects. An alternating bi-level
optimization training strategy jointly updates network weights and architecture
parameters, while zero operations remove less important connections, producing
sparse, efficient, and high-performing architectures. Extensive evaluation on
the BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%
accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming
state-of-the-art transfer learning, transformer, AutoML, and NAS methods by
approximately 16%, 10%, and 8% respectively. Further validation on the
Insects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,
and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides
accurate, confident insect classification that supports modern sustainable
farming.

</details>


### [45] [$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://arxiv.org/abs/2510.05891)
*Yanran Zhang,Bingyao Yu,Yu Zheng,Wenzhao Zheng,Yueqi Duan,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 该论文提出D^3QE方法，利用视觉自回归（AR）模型在向量量化表示上存在的离散分布和codebook频率偏差，通过融入动态codebook频率统计到Transformer注意力机制，从语义特征与量化误差潜变量联合检测AR生成图像。构建ARForensics数据集并在7个主流视觉AR模型上验证，结果显示检测准确率高、泛化性强并对真实世界扰动鲁棒。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型通过离散token预测生成图像，其向量量化表示在真实与生成图像间展现独特的模式与codebook频率偏差，这些差异可作为判别真实与伪造图像的线索。

Method: 提出离散分布差异感知的量化误差（D^3QE），设计一种将动态codebook频率统计注入注意力机制的Transformer，将语义特征与量化误差潜变量融合用于检测；并构建ARForensics数据集进行训练和评估。

Result: 在包括7个主流视觉AR模型的ARForensics数据集上，D^3QE达到了更高的检测精度与更好的跨模型泛化能力，同时对常见现实扰动（如压缩、噪声等）表现出鲁棒性。

Conclusion: D^3QE在检测视觉自回归生成图像方面表现优越，能利用codebook频率偏差和量化误差特征实现高准确率与较强泛化性，对常见扰动具有鲁棒性。

Abstract: The emergence of visual autoregressive (AR) models has revolutionized image
generation while presenting new challenges for synthetic image detection.
Unlike previous GAN or diffusion-based methods, AR models generate images
through discrete token prediction, exhibiting both marked improvements in image
synthesis quality and unique characteristics in their vector-quantized
representations. In this paper, we propose to leverage Discrete Distribution
Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated
image detection that exploits the distinctive patterns and the frequency
distribution bias of the codebook existing in real and fake images. We
introduce a discrete distribution discrepancy-aware transformer that integrates
dynamic codebook frequency statistics into its attention mechanism, fusing
semantic features and quantization error latent. To evaluate our method, we
construct a comprehensive dataset termed ARForensics covering 7 mainstream
visual AR models. Experiments demonstrate superior detection accuracy and
strong generalization of D$^3$QE across different AR models, with robustness to
real-world perturbations. Code is available at
\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.

</details>


### [46] [Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning](https://arxiv.org/abs/2510.05899)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Jinyan Zhou,Jianfeng Cao,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: 作者提出WS-ICL，一种用弱标注（如框或点）替代稠密像素标签的in-context learning方法，用于医学图像分割，显著降低注释成本，在三项测试集上性能接近常规ICL，且与交互式方法也具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 常规模型（ICL或交互式）泛化强但标注成本高：交互式需对每张图多次提示，ICL需大量像素级标签。为降低注释负担，作者探索用弱提示替代稠密标签。

Method: 提出在ICL框架中以弱提示（边界框、点）作为上下文而非像素级掩码，训练和推理时利用这些弱提示指导模型生成精细分割，从而减少每张图像所需的人工交互或密集标签需求。

Result: 在三个保留基准上实验证明WS-ICL在指标上接近常规ICL且注释成本显著降低，同时在交互式范式下也表现出竞争力。

Conclusion: WS-ICL在注释成本大幅降低的情况下，能达到与常规ICL相当的分割性能，并在交互式场景中也表现有竞争力，展示了面向更高效统一医疗图像分割模型的可行方向。

Abstract: Universal models for medical image segmentation, such as interactive and
in-context learning (ICL) models, offer strong generalization but require
extensive annotations. Interactive models need repeated user prompts for each
image, while ICL relies on dense, pixel-level labels. To address this, we
propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that
leverages weak prompts (e.g., bounding boxes or points) instead of dense labels
for context. This approach significantly reduces annotation effort by
eliminating the need for fine-grained masks and repeated user prompting for all
images. We evaluated the proposed WS-ICL model on three held-out benchmarks.
Experimental results demonstrate that WS-ICL achieves performance comparable to
regular ICL models at a significantly lower annotation cost. In addition,
WS-ICL is highly competitive even under the interactive paradigm. These
findings establish WS-ICL as a promising step toward more efficient and unified
universal models for medical image segmentation. Our code and model are
publicly available at https://github.com/jiesihu/Weak-ICL.

</details>


### [47] [Kaputt: A Large-Scale Dataset for Visual Defect Detection](https://arxiv.org/abs/2510.05903)
*Sebastian Höfer,Dorian Henning,Artemij Amiranashvili,Douglas Morrison,Mariliza Tzes,Ingmar Posner,Marc Matvienko,Alessandro Rennola,Anton Milan*

Main category: cs.CV

TL;DR: 作者发布了一个面向零售物流的超大规模缺陷检测数据集，显著增加了物体和姿态多样性，现有方法在此基准上表现大幅下降，显示出新问题的挑战性。


<details>
  <summary>Details</summary>
Motivation: 提出适用于物流场景的缺陷检测大规模数据集，弥补制造场景数据集（如MVTec-AD、VisA）在物体姿态与外观多样性方面的不足。

Method: 收集并标注来自零售物流场景的大规模图像数据，统计并构建基准数据集，对多种最先进异常检测方法进行大规模评估和定量/定性分析。

Result: 构建了一个包含超过230,000张图像、29,000多个缺陷实例、48,000多个不同物体的大规模基准数据集；评估表明现有最先进方法在该数据集上的AUROC不超过56.96%。

Conclusion: 现有工业异常检测方法难以适应零售物流中高姿态和外观变异性的挑战，需要新方法和研究；该数据集为此提供了标准化评估平台。

Abstract: We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.

</details>


### [48] [Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging](https://arxiv.org/abs/2510.05971)
*Ron Keuth,Paul Kaftan,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 在医疗影像上系统比较MetaFormer的token mixer：分类用低复杂度（分组卷积/池化）够用，分割需卷积局部偏置，分组卷积在效率与性能上最佳；预训练权重对新mixer仍有利。


<details>
  <summary>Details</summary>
Motivation: 尽管MetaFormer在自然图像中得到广泛研究，但在医疗影像领域缺乏系统比较，不同token mixer之间的适用性尚不明确，且预训练迁移的影响不清楚。

Method: 在MetaFormer框架下，对pooling、卷积（包括标准与分组）、注意力等token mixer进行横向对比，覆盖分类与语义分割两类任务，使用8个涵盖多模态与挑战的医疗影像数据集，并评估从自然图像预训练权重向新mixer迁移的效果。

Result: 分类任务上低复杂度mixer表现良好，且可迁移预训练权重；分割任务上需卷积的局部偏置，分组卷积在参数与速度上优于标准卷积，而Channel-MLP已能提供足够的跨通道交互。

Conclusion: 本论文通过在医疗影像任务上系统比较不同token mixer，证明了在分类任务中低复杂度的mixer（分组卷积、池化）已足够，而在分割任务中局部归纳偏置（卷积）不可或缺，分组卷积在效率与性能之间表现最优。

Abstract: The generalization of the Transformer architecture via MetaFormer has
reshaped our understanding of its success in computer vision. By replacing
self-attention with simpler token mixers, MetaFormer provides strong baselines
for vision tasks. However, while extensively studied on natural image datasets,
its use in medical imaging remains scarce, and existing works rarely compare
different token mixers, potentially overlooking more suitable designs choices.
In this work, we present the first comprehensive study of token mixers for
medical imaging. We systematically analyze pooling-, convolution-, and
attention-based token mixers within the MetaFormer architecture on image
classification (global prediction task) and semantic segmentation (dense
prediction task). Our evaluation spans eight datasets covering diverse
modalities and common challenges in the medical domain. Given the prevalence of
pretraining from natural images to mitigate medical data scarcity, we also
examine transferring pretrained weights to new token mixers. Our results show
that, for classification, low-complexity token mixers (e.g. grouped convolution
or pooling) are sufficient, aligning with findings on natural images.
Pretrained weights remain useful despite the domain gap introduced by the new
token mixer. For segmentation, we find that the local inductive bias of
convolutional token mixers is essential. Grouped convolutions emerge as the
preferred choice, as they reduce runtime and parameter count compared to
standard convolutions, while the MetaFormer's channel-MLPs already provide the
necessary cross-channel interactions. Our code is available on GitHub.

</details>


### [49] [Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis](https://arxiv.org/abs/2510.05976)
*Eashan Adhikarla,Yixin Liu,Brian D. Davison*

Main category: cs.CV

TL;DR: Survey analyzes diffusion models for LLIE, compares to other SOTA, proposes six-category taxonomy, examines practical challenges and future directions including foundation models.


<details>
  <summary>Details</summary>
Motivation: Assess diffusion models for low-light image enhancement and compare them to GAN and Transformer methods, examine deployment challenges, and explore future paradigms like foundation models.

Method: Survey paper

Result: Proposed multi-perspective taxonomy of six categories; evaluated failure modes, benchmark issues, trade-offs, and deployment constraints; highlighted open research questions.

Conclusion: Diffusion models are promising for LLIE but face challenges in efficiency, benchmarking, and deployment; future work should explore novel conditioning, real-time adaptation, and foundation models.

Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.

</details>


### [50] [A Dynamic Mode Decomposition Approach to Morphological Component Analysis](https://arxiv.org/abs/2510.05977)
*Owen T. Huber,Raghu G. Raj,Tianyu Chen,Zacharie I. Idriss*

Main category: cs.CV

TL;DR: 提出一种基于场景动态变化自适应视频表示方法：通过对动态模态分解（DMD）特征值聚类得到数据驱动字典，扩展形态成分分析（MCA）形成动态形态成分分析（DMCA），用于视频形态分离、去噪和目标增强，实验包括静态图像示例、Adobe 240fps 视频去噪、海面弱目标增强与逆合成孔径雷达风杂波去除。


<details>
  <summary>Details</summary>
Motivation: 标准 MCA 依赖预定义且不可适应的视频字典，难以捕捉视频中随时间变化的动态结构；利用 DMD 的频率/增长率信息并对其本征值聚类，可以学习更适配视频场景的字典，从而更好分离不同形态分量并提升去噪与检测效果。

Method: 先对视频序列做动态模态分解（DMD），提取本征值和模态；对特征值进行聚类以识别动态相似的模态组；基于聚类结果构建数据驱动字典替代预设字典；在形态成分分析框架下采用稀疏分解（即 DMCA）进行分离、重构与去噪。

Result: 在静态图像示例中展示 DMCA 的分离能力；在 Adobe 240fps 数据集上对比去噪表现良好；成功增强海面上弱目标的信噪比；在逆合成孔径雷达图像中实现自行车与风杂波的分离。

Conclusion: DMCA 能利用 DMD 特征值聚类构建自适应字典，有效分离结构不同的形态分量，提升视频去噪与弱目标检测性能，并可应用于 SAR 风杂波抑制。

Abstract: This paper introduces a novel methodology of adapting the representation of
videos based on the dynamics of their scene content variation. In particular,
we demonstrate how the clustering of dynamic mode decomposition eigenvalues can
be leveraged to learn an adaptive video representation for separating
structurally distinct morphologies of a video. We extend the morphological
component analysis (MCA) algorithm, which uses multiple predefined incoherent
dictionaries and a sparsity prior to separate distinct sources in signals, by
introducing our novel eigenspace clustering technique to obtain data-driven MCA
dictionaries, which we call dynamic morphological component analysis (DMCA).
After deriving our novel algorithm, we offer a motivational example of DMCA
applied to a still image, then demonstrate DMCA's effectiveness in denoising
applications on videos from the Adobe 240fps dataset. Afterwards, we provide an
example of DMCA enhancing the signal-to-noise ratio of a faint target summed
with a sea state, and conclude the paper by applying DMCA to separate a bicycle
from wind clutter in inverse synthetic aperture radar images.

</details>


### [51] [Diffusion-Based Image Editing for Breaking Robust Watermarks](https://arxiv.org/abs/2510.05978)
*Yunyi Ni,Finn Carter,Ze Niu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 扩散模型可通过图像再生和引导攻击有效移除鲁棒隐形水印，理论与实验均表明现有水印方法易受生成模型攻击，需要新策略。


<details>
  <summary>Details</summary>
Motivation: 评估并揭示扩散/生成模型对现有鲁棒隐形水印的威胁，推动为生成式AI环境设计更强的水印技术。

Method: 提出扩散驱动的图像再生流程与显式针对水印信号的引导扩散攻击，并通过信息论证明扩散足够深时互信息趨近于零。实验在三种现代水印方法上评估，测量恢复率与视觉质量。

Result: Diffusion模型能破坏鲁棒隐形水印；提出针对水印的引导扩散攻击；理论证明扩散变换使水印与图像互信息消失；在StegaStamp、TrustMark、VINE上实验证明几乎零恢复率且图像感知质量高。

Conclusion: 在生成模型时代，现有鲁棒不可见水印方法对扩散模型攻击脆弱，需要设计对抗生成过程的新型水印算法。

Abstract: Robust invisible watermarking aims to embed hidden information into images
such that the watermark can survive various image manipulations. However, the
rise of powerful diffusion-based image generation and editing techniques poses
a new threat to these watermarking schemes. In this paper, we present a
theoretical study and method demonstrating that diffusion models can
effectively break robust image watermarks that were designed to resist
conventional perturbations. We show that a diffusion-driven ``image
regeneration'' process can erase embedded watermarks while preserving
perceptual image content. We further introduce a novel guided diffusion attack
that explicitly targets the watermark signal during generation, significantly
degrading watermark detectability. Theoretically, we prove that as an image
undergoes sufficient diffusion-based transformation, the mutual information
between the watermarked image and the embedded watermark payload vanishes,
resulting in decoding failure. Experimentally, we evaluate our approach on
multiple state-of-the-art watermarking schemes (including the deep
learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate
near-zero watermark recovery rates after attack, while maintaining high visual
fidelity of the regenerated images. Our findings highlight a fundamental
vulnerability in current robust watermarking techniques against generative
model-based attacks, underscoring the need for new watermarking strategies in
the era of generative AI.

</details>


### [52] [Detection and Measurement of Hailstones with Multimodal Large Language Models](https://arxiv.org/abs/2510.06008)
*Moritz Alker,David C. Schedl,Andreas Stöckl*

Main category: cs.CV

TL;DR: 使用预训练多模态大模型对474张社交媒体/新闻冰雹图片进行直径估计，最佳无微调模型MAE=1.12cm；两阶段提示（借助参考物）优于单阶段；方法能补充传统传感器，但需自动化图片采集以实现实时应用。


<details>
  <summary>Details</summary>
Motivation: 利用现成多模态大模型快速、低成本地从社交媒体图片中提取冰雹空间信息，以补充传统雷暴/冰雹传感器并实现更快更详尽的灾害评估。

Method: 收集474张奥地利真实记录的冰雹图片（2022-01至2024-09，直径2–11cm），比较四种预训练模型在单阶段与两阶段提示策略下的表现，两阶段提示利用图中参考物（如手）提供尺码线索。

Result: 最佳模型在无微调情况下实现1.12cm的平均绝对误差；两阶段提示提升了大多数模型的可靠性；显示该方法可为传统传感器提供空间密集的补充数据，自动化实时图片采集仍是未来工作。

Conclusion: 该研究证明了预训练多模态大语言模型可用于从社交媒体/新闻图片中估计冰雹直径，最佳模型平均绝对误差为1.12cm，且两阶段提示通常优于单阶段提示。

Abstract: This study examines the use of social media and news images to detect and
measure hailstones, utilizing pre-trained multimodal large language models. The
dataset for this study comprises 474 crowdsourced images of hailstones from
documented hail events in Austria, which occurred between January 2022 and
September 2024. These hailstones have maximum diameters ranging from 2 to 11cm.
We estimate the hail diameters and compare four different models utilizing
one-stage and two-stage prompting strategies. The latter utilizes additional
size cues from reference objects, such as human hands, within the image. Our
results show that pretrained models already have the potential to measure
hailstone diameters from images with an average mean absolute error of 1.12cm
for the best model. In comparison to a single-stage prompt, two-stage prompting
improves the reliability of most models. Our study suggests that these
off-the-shelf models, even without fine-tuning, can complement traditional hail
sensors by extracting meaningful and spatially dense information from social
media imagery, enabling faster and more detailed assessments of severe weather
events. The automated real-time image harvesting from social media and other
sources remains an open task, but it will make our approach directly applicable
to future hail events.

</details>


### [53] [Continual Learning for Image Captioning through Improved Image-Text Alignment](https://arxiv.org/abs/2510.06009)
*Bertram Taetz,Gal Bordelius*

Main category: cs.CV

TL;DR: 本文提出一种无推理开销的多损失渐进图像描述方法，在预训练ViT-GPT-2基础上整合提示式余弦相似损失、CLIP式损失与语言引导三元组损失，以缓解灾难性遗忘并提升语义对齐。


<details>
  <summary>Details</summary>
Motivation: 解决连续学习中图像描述任务的两大困难：灾难性遗忘与随时间演进的视觉概念与语言对齐困难，利用语义提示与对比学习稳定语义表示并保留已有知识。

Method: 基于预训练ViT-GPT-2，主损失为交叉熵，附加三种损失：提示式余弦相似损失（用合成提示编码对象/属性/动作并与图像嵌入对齐）、CLIP式图像-目标描述对齐损失、以及语言引导的三元组对比损失以增强类级可分性。训练时使用提示和对比学习，但生成阶段不需要提示。

Result: 实验显示方法在缓解遗忘和语义对齐方面优于现有最先进方法，且不增加推理负担；代码已开源。

Conclusion: 所提方法在连续学习场景下能减少遗忘并提升语义一致性，且推理时无额外开销，较现有方法在语义对齐方面表现更好。

Abstract: Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.

</details>


### [54] [Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context](https://arxiv.org/abs/2510.06026)
*An Thi Nguyen,Radina Stoykova,Eric Arazo*

Main category: cs.CV

TL;DR: Models trained for generic instance search may overlearn to identify people despite no human data; combining index exclusion and confusion loss mitigates this but is vulnerable to partial-person circumvention; regulatory standards needed.


<details>
  <summary>Details</summary>
Motivation: Urgent need to understand unintended person identification in models trained without human data and evaluate mitigations to align with privacy regulations.

Method: We trained generic instance search models on large diverse datasets (no human labels) and discovered emergent person re-identification capabilities via overlearning.

Result: Combining index exclusion and confusion loss lowers person re-ID accuracy to <2% while retaining 82% retrieval for non-persons; but mitigations can be bypassed via partial-person images.

Conclusion: Technical mitigations can reduce emergent identification but are not foolproof; policy and technical standards required to prevent identification-capable systems from arising in benign contexts.

Abstract: Generic instance search models can dramatically reduce the manual effort
required to analyze vast surveillance footage during criminal investigations by
retrieving specific objects of interest to law enforcement. However, our
research reveals an unintended emergent capability: through overlearning, these
models can single out specific individuals even when trained on datasets
without human subjects. This capability raises concerns regarding
identification and profiling of individuals based on their personal data, while
there is currently no clear standard on how de-identification can be achieved.
We evaluate two technical safeguards to curtail a model's person
re-identification capacity: index exclusion and confusion loss. Our experiments
demonstrate that combining these approaches can reduce person re-identification
accuracy to below 2% while maintaining 82% of retrieval performance for
non-person objects. However, we identify critical vulnerabilities in these
mitigations, including potential circumvention using partial person images.
These findings highlight urgent regulatory questions at the intersection of AI
governance and data protection: How should we classify and regulate systems
with emergent identification capabilities? And what technical standards should
be required to prevent identification capabilities from developing in seemingly
benign applications?

</details>


### [55] [Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between](https://arxiv.org/abs/2510.06035)
*Ondřej Týbl,Lukáš Neumann*

Main category: cs.CV

TL;DR: UniNAS提出了一个将卷积、Transformer及混合结构统一到同一搜索空间的图结构化NAS框架，并给出搜索算法、验证其能搜出性能优异的架构，以及提供统一工具包以保障可重复性。


<details>
  <summary>Details</summary>
Motivation: 提出一个统一的神经架构搜索空间，能够覆盖卷积网络、Transformer及混合架构，以便在同一框架内发现和分析各种网络结构。

Method: 基于图结构构建通用的架构空间，设计遍历该空间的新型搜索算法，并在统一的训练评估协议下对发现的架构进行验证。

Result: 提出了UniNAS统一架构空间和一种新的搜索算法；在相同训练设置下，搜索到的架构优于部分手工设计的最先进模型；提供了统一工具包和标准化训练评估协议以促进可重复性和公平比较。

Conclusion: UniNAS为系统性探索整个神经网络架构谱系提供了统一的图形化NAS视角，能发现新架构并促进研究中的公平比较与可重复性。

Abstract: We introduce Universal Neural Architecture Space (UniNAS), a generic search
space for neural architecture search (NAS) which unifies convolutional
networks, transformers, and their hybrid architectures under a single, flexible
framework. Our approach enables discovery of novel architectures as well as
analyzing existing architectures in a common framework. We also propose a new
search algorithm that allows traversing the proposed search space, and
demonstrate that the space contains interesting architectures, which, when
using identical training setup, outperform state-of-the-art hand-crafted
architectures. Finally, a unified toolkit including a standardized training and
evaluation protocol is introduced to foster reproducibility and enable fair
comparison in NAS research. Overall, this work opens a pathway towards
systematically exploring the full spectrum of neural architectures with a
unified graph-based NAS perspective.

</details>


### [56] [VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization](https://arxiv.org/abs/2510.06040)
*Xinye Cao,Hongcan Guo,Jiawen Qian,Guoshun Nan,Chao Wang,Yuqi Pan,Tianhao Hou,Xiaojuan Wang,Yutong Gao*

Main category: cs.CV

TL;DR: 提出VideoMiner，迭代分段-生成描述-聚类构建树状层次结构，并用专门的强化学习算法T-GRPO在树上选择关键帧，解决长视频冗余与动态适应层次结构问题，提升长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 直接均匀抽帧使LLM在长视频中被大量冗余信息淹没，且现有层级关键帧提取方法难以同时抑制冗余并动态适应复杂层次结构，因此需要一种能保持时间一致性并精确定位关键帧的分层方法。

Method: VideoMiner通过迭代地对长视频进行事件分段、为每段生成描述（caption），并聚类这些描述形成层次化树结构（视频→事件→关键帧）；为在树上高效定位关键帧，提出T-GRPO（tree-based group relative policy optimization），一种考虑事件级时空信息并受问题引导的强化学习策略。此外设计树增长辅助机制（auxin）动态调整扩展深度。

Result: 在所有长视频理解任务上优于基线，T-GRPO促成了推理链生成，树增长auxin带来精度与效率的权衡收益；论文并公开了实现代码。

Conclusion: VideoMiner结合分层树结构与树型强化学习（T-GRPO），在长视频理解任务上取得优于现有方法的性能；T-GRPO还能促使模型自发生成推理链，树结构增长策略在精度和效率间取得平衡。

Abstract: Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.

</details>


### [57] [GLVD: Guided Learned Vertex Descent](https://arxiv.org/abs/2510.06046)
*Pol Caselles Rico,Francesc Moreno Noguer*

Main category: cs.CV

TL;DR: GLVD extends Learned Vertex Descent by adding per-vertex neural optimization plus dynamic 3D keypoint guidance, enabling expressive, efficient few-shot 3D face reconstruction without heavy 3D supervision.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of 3DMM-based methods (limited expressiveness) and heavy compute of pure optimization approaches by combining learned descent with lightweight per-vertex optimization and keypoint-driven global structure.

Method: Start from LVD framework; add per-vertex neural fields that are optimized iteratively using relative spatial encodings; predict dynamic 3D keypoints for global structural guidance; perform few-shot (single or multi-view) optimization without dense 3D labels.

Result: GLVD: a hybrid 3D face reconstruction method combining per-vertex neural field optimization with global guidance from predicted 3D keypoints; uses relative spatial encoding to iteratively refine vertices without dense 3D supervision; achieves SOTA single-view and competitive multi-view performance with much faster inference.

Conclusion: GLVD successfully balances expressiveness and efficiency: it avoids fixed 3DMM constraints, reduces inference time, and attains SOTA single-view results while being competitive in multi-view.

Abstract: Existing 3D face modeling methods usually depend on 3D Morphable Models,
which inherently constrain the representation capacity to fixed shape priors.
Optimization-based approaches offer high-quality reconstructions but tend to be
computationally expensive. In this work, we introduce GLVD, a hybrid method for
3D face reconstruction from few-shot images that extends Learned Vertex Descent
(LVD) by integrating per-vertex neural field optimization with global
structural guidance from dynamically predicted 3D keypoints. By incorporating
relative spatial encoding, GLVD iteratively refines mesh vertices without
requiring dense 3D supervision. This enables expressive and adaptable geometry
reconstruction while maintaining computational efficiency. GLVD achieves
state-of-the-art performance in single-view settings and remains highly
competitive in multi-view scenarios, all while substantially reducing inference
time.

</details>


### [58] [Medical Vision Language Models as Policies for Robotic Surgery](https://arxiv.org/abs/2510.06064)
*Akshay Muppidi,Martin Radfar*

Main category: cs.CV

TL;DR: 本文提出将医疗领域视觉语言模型MedFlamingo与PPO结合，用于内镜视觉驱动的腹腔镜手术强化学习任务，在LapGym五个任务上显著提高成功率并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 视觉输入高维、手术环境奖励稀疏且从原始视觉数据中提取任务相关特征困难，因此通过注入医用领域的视觉语言知识以改善学习效率和策略质量。

Method: 在每个episode只对观测和指令进行一次前处理，使用MedFlamingo生成高层次规划token并与实时视觉反馈结合，作为PPO输入；对比基线包括标准视觉PPO和通用OpenFlamingo PPO，在LapGym五个任务环境上评估。

Result: 在五个LapGym任务中，MedFlamingo PPO收敛更快、成功率均超过70%，相对基线提升66.67%至1114.29%，显示显著性能提升与样本效率改进。

Conclusion: 引入医用领域知识的Vision-Language模型（MedFlamingo）能有效提升基于视觉的PPO在腹腔镜手术任务中的表现，显著加快训练收敛并提高成功率，表明专科知识对手术规划与决策有重要价值。

Abstract: Vision-based Proximal Policy Optimization (PPO) struggles with visual
observation-based robotic laparoscopic surgical tasks due to the
high-dimensional nature of visual input, the sparsity of rewards in surgical
environments, and the difficulty of extracting task-relevant features from raw
visual data. We introduce a simple approach integrating MedFlamingo, a medical
domain-specific Vision-Language Model, with PPO. Our method is evaluated on
five diverse laparoscopic surgery task environments in LapGym, using only
endoscopic visual observations. MedFlamingo PPO outperforms and converges
faster compared to both standard vision-based PPO and OpenFlamingo PPO
baselines, achieving task success rates exceeding 70% across all environments,
with improvements ranging from 66.67% to 1114.29% compared to baseline. By
processing task observations and instructions once per episode to generate
high-level planning tokens, our method efficiently combines medical expertise
with real-time visual feedback. Our results highlight the value of specialized
medical knowledge in robotic surgical planning and decision-making.

</details>


### [59] [Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA](https://arxiv.org/abs/2510.06067)
*Python Song,Luke Tenyi Chang,Yun-Yun Tsai,Penghui Li,Junfeng Yang*

Main category: cs.CV

TL;DR: 提出CAPTCHA-X和agent式推理框架，强调逐步推理对提升VLM解决高难度CAPTCHA的关键作用，方法将准确率从~21.9%提升到83.9%。


<details>
  <summary>Details</summary>
Motivation: 当前商业VLM在解决需要空间推理的CAPTCHA任务上表现不佳（基线准确率约21.9%），需要系统化评估并验证推理对性能提升的作用，同时提供有推理标注的基准促进研究。

Method: 构建了CAPTCHA-X基准数据集，包含七类真实世界CAPTCHA，附带逐步行动解与定位标注；定义五项面向推理的评估指标；提出一种整合模型内在推理能力的agent式框架，通过强制模型先进行逐步推理再输出坐标来提升性能。

Result: 在五类高难度CAPTCHA上方法达成平均83.9%求解准确率，显著优于现有基线，证明了逐步推理能大幅提升VLM在视觉空间任务上的能力。

Conclusion: 本文证明逐步推理对视觉-语言模型（VLM）解决高难度空间推理任务（如CAPTCHA）至关重要，并提出了能显著提升求解准确率的代理式VLM框架。

Abstract: CAPTCHA, originally designed to distinguish humans from robots, has evolved
into a real-world benchmark for assessing the spatial reasoning capabilities of
vision-language models. In this work, we first show that step-by-step reasoning
is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent
high-difficulty spatial reasoning tasks, and that current commercial
vision-language models still struggle with such reasoning. In particular, we
observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to
effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).
However, our findings indicate that requiring the model to perform step-by-step
reasoning before generating the final coordinates can significantly enhance its
solving accuracy, underscoring the severity of the gap. To systematically study
this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with
reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,
etc.) with step-by-step action solutions and grounding annotations. We further
define five reasoning-oriented metrics that enable a comprehensive evaluation
of models reasoning capabilities. To validate the effectiveness of reasoning,
we also propose a general agentic VLM-based framework that incorporates the
models inherent reasoning abilities. Our method achieves state-of-the-art
performance across five high-difficulty CAPTCHA types, with an average solving
accuracy of 83.9 percent, substantially surpassing existing baselines. These
results reveal the limitations of current models and highlight the importance
of reasoning in advancing visual-spatial challenges in the future.

</details>


### [60] [There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers](https://arxiv.org/abs/2510.06070)
*Meghna P Ayyar,Jenny Benois-Pineau,Akka Zemmari*

Main category: cs.CV

TL;DR: Use statistical filtering on ViT attention (plus class-specific variant) to remove noise and produce faithful, human-aligned explanations, outperforming many SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Attention maps in ViTs are noisy; prior CNN filtering methods can help; need class-discriminative and human-aligned explanations

Method: Combine attention with statistical filtering for ViT explanations

Result: Sharper, more interpretable maps; competitive or superior to SOTA on perturbation faithfulness and human gaze alignment; efficient

Conclusion: Filtering attention yields efficient, discriminative, and human-plausible explanations for ViTs, matching or exceeding SOTA performance.

Abstract: Explainable AI (XAI) has become increasingly important with the rise of large
transformer models, yet many explanation methods designed for CNNs transfer
poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on
attention weights, which tend to yield noisy maps as they capture
token-to-token interactions within each layer.While attribution methods
incorporating MLP blocks have been proposed, we argue that attention remains a
valuable and interpretable signal when properly filtered. We propose a method
that combines attention maps with a statistical filtering, initially proposed
for CNNs, to remove noisy or uninformative patterns and produce more faithful
explanations. We further extend our approach with a class-specific variant that
yields discriminative explanations. Evaluation against popular state-of-the-art
methods demonstrates that our approach produces sharper and more interpretable
maps. In addition to perturbation-based faithfulness metrics, we incorporate
human gaze data to assess alignment with human perception, arguing that human
interpretability remains essential for XAI. Across multiple datasets, our
approach consistently outperforms or is comparable to the SOTA methods while
remaining efficient and human plausible.

</details>


### [61] [When Thinking Drifts: Evidential Grounding for Robust Video Reasoning](https://arxiv.org/abs/2510.06077)
*Mi Luo,Zihui Xue,Alex Dimakis,Kristen Grauman*

Main category: cs.CV

TL;DR: CoT can harm video reasoning by producing misleading, ungrounded internal monologues ("visual thinking drift"); Video-VER trains models to produce visually grounded reasoning traces via a reward for evidence alignment, improving accuracy across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Investigate why Chain-of-Thought helps in text but often degrades video reasoning, identify the "visual thinking drift" problem where internal monologues are ungrounded, and develop methods to encourage grounded, evidence-based reasoning in multimodal models.

Method: Analyze failure modes of CoT in video via empirical study and Bayesian framing; introduce VER, a reinforcement learning reward that scores reasoning traces by how well they align with verifiable visual evidence; train multimodal models (Video-VER) with this reward and evaluate on 10 benchmarks.

Result: The paper identifies a failure mode when applying Chain-of-Thought (CoT) to video reasoning: CoT can cause "visual thinking drift" where generated reasoning traces diverge from actual visual evidence and lead to hallucinations and errors. It proposes Visual Evidence Reward (VER), a reinforcement learning approach that rewards traces grounded in visual evidence, improving performance across benchmarks. Empirical results show Video-VER achieves top performance on 10 video understanding datasets.

Conclusion: Chain-of-Thought without visual grounding can degrade video reasoning due to hallucination and bias amplification; explicitly rewarding visually grounded traces (VER) mitigates this and yields state-of-the-art performance across diverse video benchmarks.

Abstract: Video reasoning, the task of enabling machines to infer from dynamic visual
content through multi-step logic, is crucial for advanced AI. While the
Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,
its application to video understanding remains underexplored. This paper
presents a systematic analysis revealing that CoT often degrades performance in
video reasoning, generating verbose but misleading internal monologues, and
leading to hallucinated visual details and overridden correct intuitions - a
phenomenon we term "visual thinking drift". We explain this drift through a
Bayesian lens, positing that CoT traces often diverge from actual visual
evidence, instead amplifying internal biases or language priors, causing models
to storytell rather than engage in grounded reasoning. To counteract this, we
introduce Visual Evidence Reward (VER), a novel reinforcement learning
framework that explicitly rewards the generation of reasoning traces that are
verifiably grounded in visual evidence. Comprehensive evaluation across 10
diverse video understanding benchmarks demonstrates that our Video-VER
consistently achieves top performance. Our work sheds light on the distinct
challenges of video-centric reasoning and encourages the development of AI that
robustly grounds its inferences in visual evidence - for large multimodal
models that not only "think before answering", but also "see while thinking".

</details>


### [62] [A public cardiac CT dataset featuring the left atrial appendage](https://arxiv.org/abs/2510.06090)
*Bjoern Hansen,Jonas Pedersen,Klaus F. Kofoed,Oscar Camara,Rasmus R. Paulsen,Kristine Soerensen*

Main category: cs.CV

TL;DR: 本文发布了首个开源、高分辨率且解剖一致的左心耳(LAA)、冠状动脉(CA)和肺静脉(PV)分割数据集，基于ImageCAS 1000例CCTA，通过专门的高分辨率LAA分割模型和对CA、PV的修正与精炼生成标签，并标注了常见数据缺陷样本。


<details>
  <summary>Details</summary>
Motivation: 尽管像TotalSegmentator这样的先进框架在全心结构分割上表现良好，但对LAA、CA和PV的准确细分仍有挑战；因此需要一个高质量、解剖一致且开源的数据集来推动算法和形态学研究。

Method: 作者使用在大型私人手工标注数据集上训练的高分辨率LAA分割网络，将其迁移到ImageCAS上生成LAA标签；对原始ImageCAS的CA标签进行了改进，并基于TotalSegmentator的输出对PV进行了精细化处理；同时整理并标注了含有步进伪影、部分结构超出视野等问题的扫描。

Result: 在1000例CCTA的ImageCAS上生成并公开了高分辨率的LAA、改进的CA和精细化的PV分割标签，并提供了常见数据缺陷清单，支持后续研究与方法开发。

Conclusion: 该数据集填补了现有分割数据的空白，为LAA形态学研究和相关算法开发提供了可靠的、高质量的公开资源，同时通过标注数据缺陷提升了数据使用的透明性与实用性。

Abstract: Despite the success of advanced segmentation frameworks such as
TotalSegmentator (TS), accurate segmentations of the left atrial appendage
(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant
challenge in medical imaging. In this work, we present the first open-source,
anatomically coherent dataset of curated, high-resolution segmentations for
these structures, supplemented with whole-heart labels produced by TS on the
publicly available ImageCAS dataset consisting of 1000 cardiac computed
tomography angiography (CCTA) scans. One purpose of the data set is to foster
novel approaches to the analysis of LAA morphology.
  LAA segmentations on ImageCAS were generated using a state-of-the-art
segmentation framework developed specifically for high resolution LAA
segmentation. We trained the network on a large private dataset with manual
annotations provided by medical readers guided by a trained cardiologist and
transferred the model to ImageCAS data. CA labels were improved from the
original ImageCAS annotations, while PV segmentations were refined from TS
outputs. In addition, we provide a list of scans from ImageCAS that contains
common data flaws such as step artefacts, LAAs extending beyond the scanner's
field of view, and other types of data defects.

</details>


### [63] [Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2510.06098)
*Yinjian Wang,Wei Li,Yuanyuan Gui,Gemine Vivone*

Main category: cs.CV

TL;DR: 本文提出一种新的张量框架下的高光谱图像超分辨率融合模型，通过块项分解将高光谱影像分解为谱子空间与空间图谱，再将空间图谱堆叠为高阶空间张量，提出了非凸的“模式重排张量相关全变差”以共同建模高阶空间低秩与多尺度平滑先验，并设计基于线性化ADMM的高效优化算法，证明了KKT收敛性，实验验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有张量方法一般只能在一到两层同时利用有限先验，难以兼顾多层多先验，导致权重平衡与多块变量优化困难，因此需要一种能够紧凑表达并高效优化多级先验的模型。

Method: 方法基于块项分解将高光谱融合问题分解为谱基与空间图谱两部分；空间图谱组成一个高阶空间张量，采用提出的非凸“模式重排张量相关全变差”来联合建模高阶空间低秩与多尺度平滑；使用线性化ADMM设计优化算法，并证明KKT收敛性。

Result: 在多个数据集上的实验表明，所提模型在恢复精度和视觉质量上均优于现有方法，且算法收敛性得到理论证明。

Conclusion: 本文所提方法有效融合了多级空间先验与谱低秩结构，在多个数据集上提升了高光谱超分辨率质量，同时算法收敛性有理论保证，代码将开源。

Abstract: Fusing a hyperspectral image with a multispectral image acquired over the
same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a
popular computational way to access the latent high-spatial-spectral-resolution
image. To date, a variety of fusion methods have been proposed, among which the
tensor-based ones have testified that multiple priors, such as multidimensional
low-rankness and spatial total variation at multiple levels, effectively drive
the fusion process. However, existing tensor-based models can only effectively
leverage one or two priors at one or two levels, since simultaneously
incorporating multi-level priors inevitably increases model complexity. This
introduces challenges in both balancing the weights of different priors and
optimizing multi-block structures. Concerning this, we present a novel
hyperspectral super-resolution model compactly characterizing these multi-level
priors of hyperspectral images within the tensor framework. Firstly, the
proposed model decouples the spectral low-rankness and spatial priors by
casting the latent high-spatial-spectral-resolution image into spectral
subspace and spatial maps via block term decomposition. Secondly, these spatial
maps are stacked as the spatial tensor encoding the high-order spatial
low-rankness and smoothness priors, which are co-modeled via the proposed
non-convex mode-shuffled tensor correlated total variation. Finally, we draw
inspiration from the linearized alternating direction method of multipliers to
design an efficient algorithm to optimize the resulting model, theoretically
proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments
on multiple datasets demonstrate the effectiveness of the proposed algorithm.
The code implementation will be available from https://github.com/WongYinJ.

</details>


### [64] [Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction](https://arxiv.org/abs/2510.06113)
*Shuo Jiang,Zhuwen Chen,Liaoman Xu,Yanming Zhu,Changmiao Wang,Jiong Zhang,Feiwei Qin,Yifei Chen,Zhu Zhu*

Main category: cs.CV

TL;DR: 提出FeatProto：一个将WSI全局/局部特征与基因组数据对齐的多模态原型框架，通过EMA ProtoUp和层级匹配提升生存预测性能与可解释性，在四个癌症数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生存分析模型可解释性差，传统原型学习过分依赖局部相似性且与基因组语义对齐不足，需一个可追溯且兼顾全局/局部信息的多模态原型框架。

Method: 构建统一特征原型空间，结合全局与局部WSI特征与基因组画像；提出EMA ProtoUp指数移动平均原型更新策略并引入漫游机制；采层级原型匹配捕捉全局中心性、局部典型性和队列趋势。

Result: 在四个公开癌症数据集上，FeatProto在准确性和可解释性上均优于现有领先的单模态与多模态生存预测方法。

Conclusion: FeatProto将病理图像与基因组数据在统一的原型空间中融合，提升了生存预测的准确性与可解释性，适用于多种癌症数据集。

Abstract: Survival analysis plays a vital role in making clinical decisions. However,
the models currently in use are often difficult to interpret, which reduces
their usefulness in clinical settings. Prototype learning presents a potential
solution, yet traditional methods focus on local similarities and static
matching, neglecting the broader tumor context and lacking strong semantic
alignment with genomic data. To overcome these issues, we introduce an
innovative prototype-based multimodal framework, FeatProto, aimed at enhancing
cancer survival prediction by addressing significant limitations in current
prototype learning methodologies within pathology. Our framework establishes a
unified feature prototype space that integrates both global and local features
of whole slide images (WSI) with genomic profiles. This integration facilitates
traceable and interpretable decision-making processes. Our approach includes
three main innovations: (1) A robust phenotype representation that merges
critical patches with global context, harmonized with genomic data to minimize
local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that
sustains stable cross-modal associations and employs a wandering mechanism to
adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype
matching scheme designed to capture global centrality, local typicality, and
cohort-level trends, thereby refining prototype inference. Comprehensive
evaluations on four publicly available cancer datasets indicate that our method
surpasses current leading unimodal and multimodal survival prediction
techniques in both accuracy and interoperability, providing a new perspective
on prototype learning for critical medical applications. Our source code is
available at https://github.com/JSLiam94/FeatProto.

</details>


### [65] [Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework](https://arxiv.org/abs/2510.06123)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: SSGNet用StyleGAN3生成类特异性图像并配合迭代伪标签扩充与精炼数据，能在有限标注下稳定提升医学影像分类与分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像标注稀缺且类别不平衡，限制深度学习模型性能；通过生成高质量合成数据并利用半监督伪标签可以缓解标注瓶颈并提升鲁棒性。

Method: 在已有基线模型上引入StyleGAN3生成的类平衡合成图像以扩充训练集，并采用迭代伪标签策略对未标注或合成样本的标签进行逐步精炼，将生成模型与伪标注循环集成到训练流程。

Result: 在多项医学影像基准上，SSGNet在分类与分割任务上均有稳定提升；FID评估表明生成样本质量高，支持合成数据的有效性。

Conclusion: SSGNet通过将基于StyleGAN3的类特异性生成与迭代半监督伪标签结合，能有效扩充训练数据并提升分类与分割性能。

Abstract: Deep learning in medical imaging is often limited by scarce and imbalanced
annotated data. We present SSGNet, a unified framework that combines class
specific generative modeling with iterative semisupervised pseudo labeling to
enhance both classification and segmentation. Rather than functioning as a
standalone model, SSGNet augments existing baselines by expanding training data
with StyleGAN3 generated images and refining labels through iterative pseudo
labeling. Experiments across multiple medical imaging benchmarks demonstrate
consistent gains in classification and segmentation performance, while Frechet
Inception Distance analysis confirms the high quality of generated samples.
These results highlight SSGNet as a practical strategy to mitigate annotation
bottlenecks and improve robustness in medical image analysis.

</details>


### [66] [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/abs/2510.06131)
*Jiawei Mao,Yuhan Wang,Lifeng Chen,Can Zhao,Yucheng Tang,Dong Yang,Liangqiong Qu,Daguang Xu,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出MeDiM，一种无模态特异性组件的医疗离散扩散模型，在共享概率空间中联合生成图像与文本，利用多模态大语言模型作为扩散主干并通过移除因果注意力掩码与注入连续时间步嵌入实现双向上下文与扩散感知。实验在MIMIC-CXR与PathGen上显示高保真生成与报告质量，且联合生成的图像-报告对能提升下游指标。


<details>
  <summary>Details</summary>
Motivation: 当前医疗生成模型受限于模态特异性，无法整合影像、病理与临床笔记等互补证据，阻碍成为跨模态医学基础模型的发展；因此需要一种统一的生成框架以学习跨模态共享分布并支持多样化生成任务。

Method: 基于离散扩散框架，使用MLLM作为共享生成骨干，去除因果注意力掩码以实现双向上下文建模，并注入连续时间步嵌入以赋予扩散时间意识；模型可以在同一概率空间内对图像和文本进行采样与互译，并支持提示驱动的联合图像-报告生成。

Result: 在MIMIC-CXR上FID 16.60，PathGen上FID 24.19，报告生成METEOR分别为0.2650与0.2580；使用联合图像-报告生成的数据增强下游报告生成指标显著提升（BLEU-1 +6.43%，BLEU-2 +18.57%，BLEU-3 +31.58%，METEOR +4.80%）。

Conclusion: MeDiM能在单一框架下实现跨模态医学数据的统一生成，生成质量高且有助于下游任务，展示了将MLLM与离散扩散结合以实现通用医学基础模型的可行性。

Abstract: Recent advances in generative medical models are constrained by
modality-specific scenarios that hinder the integration of complementary
evidence from imaging, pathology, and clinical notes. This fragmentation limits
their evolution into foundation models that can learn and reason across the
full spectrum of biomedical data. We propose MeDiM, the first medical discrete
diffusion model that learns shared distributions across modalities without
modality-specific components. MeDiM unifies multiple generative tasks:
translating between images and text, and jointly producing image-report pairs
across domains in response to prompts. Built on a discrete diffusion framework,
MeDiM bridges vision and language representations through a shared
probabilistic space. To enable unified and flexible medical generation, we
employ a multimodal large language model (MLLM) as the diffusion backbone,
leveraging its prior knowledge and cross-modal reasoning. Two key designs are
introduced: (1) removing the causal attention mask for bidirectional context,
and (2) injecting continuous timestep embeddings for diffusion awareness.
Experiments demonstrate high-fidelity medical generation (FID 16.60 on
MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR
0.2650 and 0.2580). Jointly generated image-report pairs further enhance
downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,
plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports
coherent and clinically grounded multimodal outputs.

</details>


### [67] [Deforming Videos to Masks: Flow Matching for Referring Video Segmentation](https://arxiv.org/abs/2510.06139)
*Zanyi Wang,Dengyang Jiang,Liuzhuozheng Li,Sizhe Dang,Chengzu Li,Harry Yang,Guang Dai,Mengmeng Wang,Jingdong Wang*

Main category: cs.CV

TL;DR: 将RVOS视为条件连续形变问题，提出FlowRVS，一阶段、语言引导的生成式方法，借助预训练T2V模型实现细粒度像素控制与时间一致性，在多个数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统“先定位后分割”管线将语义简化为粗糙的几何提示（如点），导致信息瓶颈并难以维持时间一致性；因此需要一种能保持细粒度语义并兼具时序连贯性的端到端方法。

Method: 引入一种一阶段生成式框架，基于预训练文本到视频模型（T2V），采用细粒度像素控制和文本-视频语义对齐，学习从视频全局表示到目标掩码的直接变形，而非从噪声生成或直接预测掩码。

Result: 在主要RVOS基准上达成SOTA：MeViS上的J&F为51.1（比此前SOTA提升1.6），Ref-DAVIS17零-shot为73.3（提升2.7），显示出建模为连续形变过程的有效性。

Conclusion: 本文提出FlowRVS，通过将RVOS重新建模为条件连续流（conditional continuous flow）来直接学习从视频整体表示到目标掩码的语言引导形变，克服了先前“定位-分割”级联方法的信息瓶颈和时间一致性问题，并在多个基准上实现了新的SOTA。

Abstract: Referring Video Object Segmentation (RVOS) requires segmenting specific
objects in a video guided by a natural language description. The core challenge
of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels
and continuously segment them through the complex dynamics of a video. Faced
with this difficulty, prior work has often decomposed the task into a pragmatic
`locate-then-segment' pipeline. However, this cascaded design creates an
information bottleneck by simplifying semantics into coarse geometric prompts
(e.g, point), and struggles to maintain temporal consistency as the segmenting
process is often decoupled from the initial language grounding. To overcome
these fundamental limitations, we propose FlowRVS, a novel framework that
reconceptualizes RVOS as a conditional continuous flow problem. This allows us
to harness the inherent strengths of pretrained T2V models, fine-grained pixel
control, text-video semantic alignment, and temporal coherence. Instead of
conventional generating from noise to mask or directly predicting mask, we
reformulate the task by learning a direct, language-guided deformation from a
video's holistic representation to its target mask. Our one-stage, generative
approach achieves new state-of-the-art results across all major RVOS
benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in
MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),
demonstrating the significant potential of modeling video understanding tasks
as continuous deformation processes.

</details>


### [68] [Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images](https://arxiv.org/abs/2510.06145)
*Aditya Prakash,David Forsyth,Saurabh Gupta*

Main category: cs.CV

TL;DR: 用扩散模型将2D手关键点序列提升为4D运动标签，并用扩散式预测模型从单张图像预测双手3D运动，借助补标签的多样数据显著提升泛化和准确性。


<details>
  <summary>Details</summary>
Motivation: 缺乏多样化的3D手注释限制了从单图像预测双手3D运动的泛化，需通过补标签与能建模多模态未来运动的预测模型来解决。

Method: 1) 注释管线：用扩散模型将2D关键点序列提升到4D手运动以为多样数据补标签；2) 预测模型：对未来手运动使用扩散损失以建模多模态分布；3) 在6个数据集上训练与评估，包括零样本日常场景测试。

Result: They propose lifting 2D hand keypoint sequences to 4D (3D+time) using a diffusion model and use a diffusion-based forecasting model to predict future bimanual 3D hand motion from a single image; they create an annotation pipeline to impute 3D labels for diverse datasets, train on combined data, and evaluate on 6 datasets with strong gains in zero-shot generalization.

Conclusion: 通过扩散式提升与预测，并在多数据集上用补标签训练，能显著提高从单图像预测双手三维动作的准确性与泛化能力。

Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation
from a single image in everyday settings. To address the lack of 3D hand
annotations in diverse settings, we design an annotation pipeline consisting of
a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the
forecasting model, we adopt a diffusion loss to account for the multimodality
in hand motion distribution. Extensive experiments across 6 datasets show the
benefits of training on diverse data with imputed labels (14% improvement) and
effectiveness of our lifting (42% better) & forecasting (16.4% gain) models,
over the best baselines, especially in zero-shot generalization to everyday
images.

</details>


### [69] [ShapeGen4D: Towards High Quality 4D Shape Generation from Videos](https://arxiv.org/abs/2510.06208)
*Jiraphon Yenphraphai,Ashkan Mirzaei,Jianqi Chen,Jiaxu Zou,Sergey Tulyakov,Raymond A. Yeh,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: 提出基于大规模预训练3D模型的原生视频到4D生成框架，包含时序注意力、时序采样与4D潜变量锚定、跨帧噪声共享，增强时间一致性与稳健性


<details>
  <summary>Details</summary>
Motivation: improve video-to-4D shape generation using pretrained 3D models and temporal mechanisms

Method: analysis of methods

Result: end-to-end single dynamic 3D representation with better temporal stability, handles non-rigid, volume and topology changes, outperforms baselines

Conclusion: 方法无需逐帧优化即可稳健恢复时变几何与一致外观，在多样真实视频上提高感知保真并减少失败

Abstract: Video-conditioned 4D shape generation aims to recover time-varying 3D
geometry and view-consistent appearance directly from an input video. In this
work, we introduce a native video-to-4D shape generation framework that
synthesizes a single dynamic 3D representation end-to-end from the video. Our
framework introduces three key components based on large-scale pre-trained 3D
models: (i) a temporal attention that conditions generation on all frames while
producing a time-indexed dynamic representation; (ii) a time-aware point
sampling and 4D latent anchoring that promote temporally consistent geometry
and texture; and (iii) noise sharing across frames to enhance temporal
stability. Our method accurately captures non-rigid motion, volume changes, and
even topological transitions without per-frame optimization. Across diverse
in-the-wild videos, our method improves robustness and perceptual fidelity and
reduces failure modes compared with the baselines.

</details>


### [70] [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209)
*Jiahao Wang,Zhenpei Yang,Yijing Bai,Yingwei Li,Yuliang Zou,Bo Sun,Abhijit Kundu,Jose Lezama,Luna Yue Huang,Zehao Zhu,Jyh-Jing Hwang,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.CV

TL;DR: 本文提出将视频生成模型与端到端（E2E）驾驶模型结合（Drive&Gen），提出用E2E驾驶器作为判别器来评估生成视频的真实度，并利用可控生成进行定向实验找出分布差异以提升E2E规划器泛化性。还展示合成数据能作为低成本替代，提升模型在超出原始运营设计域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型和E2E驾驶模型兴起，如何将二者结合用于仿真评估与改进E2E规划器成为关键问题：判断生成视频是否符合条件并足够真实用于评估；以及如何用生成数据理解并改善E2E模型的偏差与泛化能力。

Method: 提出基于E2E驾驶器的统计评估指标评判生成视频的真实度；运用可控视频生成器构造定向分布干预实验以识别影响E2E性能的因素；将生成的视频作为合成训练集，验证其提升E2E模型在OOD场景下的泛化效果。

Result: 通过设计的统计量和定向实验，作者证明生成视频在服从条件性与可用于E2E评估方面具有可行性；发现若干导致性能下降的分布差异；并展示用生成数据增量训练能显著提升E2E模型在新运营设计域下的表现。

Conclusion: Drive&Gen 框架表明：可控视频生成能提供足够信息用于评估E2E驾驶器，利用生成数据可诊断并缩小分布差距，且合成数据能有效提升E2E模型的泛化，降低真实数据采集成本。

Abstract: Recent advances in generative models have sparked exciting new possibilities
in the field of autonomous vehicles. Specifically, video generation models are
now being explored as controllable virtual testing environments.
Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined
alternative to conventional modular autonomous driving systems, gaining
popularity for their simplicity and scalability. However, the application of
these techniques to simulation and planning raises important questions. First,
while video generation models can generate increasingly realistic videos, can
these videos faithfully adhere to the specified conditions and be realistic
enough for E2E autonomous planner evaluation? Second, given that data is
crucial for understanding and controlling E2E planners, how can we gain deeper
insights into their biases and improve their ability to generalize to
out-of-distribution scenarios? In this work, we bridge the gap between the
driving models and generative world models (Drive&Gen) to address these
questions. We propose novel statistical measures leveraging E2E drivers to
evaluate the realism of generated videos. By exploiting the controllability of
the video generation model, we conduct targeted experiments to investigate
distribution gaps affecting E2E planner performance. Finally, we show that
synthetic data produced by the video generation model offers a cost-effective
alternative to real-world data collection. This synthetic data effectively
improves E2E model generalization beyond existing Operational Design Domains,
facilitating the expansion of autonomous vehicle services into new operational
contexts.

</details>


### [71] [Fine-grained Defocus Blur Control for Generative Image Models](https://arxiv.org/abs/2510.06215)
*Ayush Shrivastava,Connelly Barnes,Xuaner Zhang,Lingzhi Zhang,Andrew Owens,Sohrab Amirghodsi,Eli Shechtman*

Main category: cs.CV

TL;DR: 在文本到图像扩散模型中引入EXIF元数据，通过仿真成像流程（生成全对焦图→单目深度估计→焦距预测→可微镜头模糊）实现可控散景/虚化，支持精细的光圈/焦距控制并保持场景内容不变。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型难以整合精细的相机参数如光圈，导致无法高保真可控地生成镜头虚化；利用EXIF可提升物理一致性和用户交互式控制能力。

Method: 生成全对焦图像→单目深度估计→焦距/聚焦距离变换器预测聚焦距离→可微分镜头模糊模块形成虚化图；整个流程可逆传播梯度以在无直接虚化标注下学习。

Result: The paper presents a framework that integrates EXIF camera metadata into text-to-image diffusion models to enable controllable lens blur (defocus) effects. It generates an all-in-focus image, estimates depth, predicts focus distance via a focus distance transformer, and applies a differentiable lens blur, training end-to-end without explicit defocus supervision.

Conclusion: 该方法能在不改变场景内容的前提下，基于EXIF实现细粒度可控的虚化效果，优于现有扩散模型在可控性与真实感上的表现。

Abstract: Current text-to-image diffusion models excel at generating diverse,
high-quality images, yet they struggle to incorporate fine-grained camera
metadata such as precise aperture settings. In this work, we introduce a novel
text-to-image diffusion framework that leverages camera metadata, or EXIF data,
which is often embedded in image files, with an emphasis on generating
controllable lens blur. Our method mimics the physical image formation process
by first generating an all-in-focus image, estimating its monocular depth,
predicting a plausible focus distance with a novel focus distance transformer,
and then forming a defocused image with an existing differentiable lens blur
model. Gradients flow backwards through this whole process, allowing us to
learn without explicit supervision to generate defocus effects based on content
elements and the provided EXIF data. At inference time, this enables precise
interactive user control over defocus effects while preserving scene contents,
which is not achievable with existing diffusion models. Experimental results
demonstrate that our model enables superior fine-grained control without
altering the depicted scene.

</details>


### [72] [Dropping the D: RGB-D SLAM Without the Depth Sensor](https://arxiv.org/abs/2510.06216)
*Mert Kiray,Alican Karaomer,Benjamin Busam*

Main category: cs.CV

TL;DR: 用单目深度估计、关键点检测和实例分割三模块生成有度量尺度的稀疏 3D 特征，结合未改动的 RGB-D 后端，在 TUM 数据集上达到了与 RGB-D 方法可比甚至更好性能，同时保持实时性（22 FPS）。


<details>
  <summary>Details</summary>
Motivation: 降低系统复杂度与成本，验证预训练视觉模型能否替代主动深度传感器提供实时且有尺度的深度信息。

Method: 系统用单目度量深度预测给静态关键点赋深度，并用实例分割掩码（膨胀后）去除动态物体，随后把带尺度的关键点反投影为稀疏 3D 特征，直接喂入未改动的 RGB-D SLAM 后端进行跟踪与建图。

Result: DropD-SLAM 用预训练视觉模型替代深度传感器，在实时单目 SLAM 中实现了接近或优于 RGB-D 级别的绝对尺度精度。

Conclusion: 现代预训练视觉模型已能在实时条件下提供可靠的度量尺度信息，使得无需主动深度传感器的 SLAM 成为可行且高效的替代方案。

Abstract: We present DropD-SLAM, a real-time monocular SLAM system that achieves
RGB-D-level accuracy without relying on depth sensors. The system replaces
active depth input with three pretrained vision modules: a monocular metric
depth estimator, a learned keypoint detector, and an instance segmentation
network. Dynamic objects are suppressed using dilated instance masks, while
static keypoints are assigned predicted depth values and backprojected into 3D
to form metrically scaled features. These are processed by an unmodified RGB-D
SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM
attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,
matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS
on a single GPU. These results suggest that modern pretrained vision models can
replace active depth sensors as reliable, real-time sources of metric scale,
marking a step toward simpler and more cost-effective SLAM systems.

</details>


### [73] [EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark](https://arxiv.org/abs/2510.06218)
*Deheng Zhang,Yuqian Fu,Runyi Yang,Yang Miao,Tianwen Qian,Xu Zheng,Guolei Sun,Ajad Chhatkuli,Xuanjing Huang,Yu-Gang Jiang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 构建首个夜间第一视角VQA基准EgoNight，利用昼夜对齐视频（合成+真实）和自动标注+人工校验得到3658 QA，展示白天到夜间性能显著下降，并提供检索与深度估计任务。


<details>
  <summary>Details</summary>
Motivation: 现有第一视角视觉基准多聚焦白天场景，忽视低光条件；现实应用不可避免含夜间场景，需建立夜间基准以评估与推动模型在低照度下的表现改进。

Method: 收集昼夜对齐的合成（Blender）与真实视频；提出昼增强夜间自动标注引擎并结合多轮人工校验；构建3658 QA对并设计12类问题；评估现有MLLMs并新增两个辅助任务。

Result: EgoNight introduces the first comprehensive nighttime egocentric VQA benchmark, using day-night aligned videos (synthetic and real) to improve annotations and analyze performance gaps; contains 3658 QA pairs across 90 videos, 12 QA types, extensive human verification, and auxiliary tasks (correspondence retrieval, depth estimation).

Conclusion: EgoNight-VQA揭示现有多模态模型在低照度下推理能力严重下降，强调需发展跨光照域泛化技术，数据集可促进夜间第一视角视觉研究。

Abstract: Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.

</details>


### [74] [Human3R: Everyone Everywhere All at Once](https://arxiv.org/abs/2510.06219)
*Yue Chen,Xingyu Chen,Yuxuan Xue,Anpei Chen,Yuliang Xiu,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: Human3R 提出一个统一的前馈框架，能从单目视频实时在世界坐标系中重建多人人体（SMPL-X）、稠密场景和相机轨迹，消除多阶段与迭代优化依赖，训练成本低，速度高（15 FPS，8 GB），在多个任务上达到或超过现有方法性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多阶段流水线、迭代接触优化和外部模块（人体检测、深度估计、SLAM），复杂且不利于实时在线应用；因此需要一个统一且高效的单目视频 4D 重建框架，能一次性恢复“everyone、everywhere、all-at-once”。

Method: 基于CUT3R 的 4D 在线重建模型，采用参数高效的视觉 prompt tuning 保留时空先验，并在模型中引入多人体 SMPL-X 的直接读取模块，实现全局多人体、场景和相机轨迹的联合回归；在 BEDLAM 合成数据上以单 GPU 一天训练完成。

Result: 在多个任务（全局人体运动估计、局部人体网格恢复、视频深度估计、相机位姿估计）上，Human3R 在实时（15 FPS）和低内存（8 GB）条件下取得了 SOTA 或有竞争力的性能，证明了其高效性与实用性。

Conclusion: Human3R 是一个轻量且高效的统一模型，能在单阶段、一次前向推理中同时恢复全局多人体姿态、局部人体网格、视频深度和相机位姿，摆脱繁琐依赖和迭代接触优化，具备实时性能与良好精度，可作为强基线并方便扩展。

Abstract: We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [75] [Redefining Cost Estimation in Database Systems: The Role of Execution Plan Features and Machine Learning](https://arxiv.org/abs/2510.05612)
*Utsav Pathak,Amit Mankodi*

Main category: cs.DB

TL;DR: 用PostgreSQL执行计划特征+查询语义训练模型预测真实查询运行时。构建自动化采集管道，比较多种模型，XGBoost表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统成本模型基于静态启发式，难以反映复杂/动态工作负载下的真实表现，机器学习方法可用于改进成本估计并提升查询优化效果。

Method: 从PostgreSQL的EXPLAIN ANALYZE提取细粒度计划统计和基于执行痕迹的查询嵌入，使用参数化TPC-H生成 >1000 条查询，构建特征集并训练不同回归模型（基线回归、改进XGBoost、LSTM序列模型），评估MSE和误差分布。

Result: The paper proposes an ML-based framework to predict SQL query runtimes using execution plan features and query semantics extracted from PostgreSQL. It builds automated pipeline with TPC-H parameterized queries and compares models (baselines, XGBoost, LSTM). XGBoost performs best with MSE 0.3002 and >65% predictions within 10% error.

Conclusion: 结合执行计划的标量/结构特征与查询语义，基于树的模型（XGBoost）能显著提升实际运行时预测精度，有望改进查询优化中的成本估计。

Abstract: Accurate query runtime prediction is a critical component of effective query
optimization in modern database systems. Traditional cost models, such as those
used in PostgreSQL, rely on static heuristics that often fail to reflect actual
query performance under complex and evolving workloads. This remains an active
area of research, with recent work exploring machine learning techniques to
replace or augment traditional cost estimators. In this paper, we present a
machine learning-based framework for predicting SQL query runtimes using
execution plan features extracted from PostgreSQL. Our approach integrates
scalar and structural features from execution plans and semantic
representations of SQL queries to train predictive models. We construct an
automated pipeline for data collection and feature extraction using
parameterized TPC-H queries, enabling systematic evaluation of multiple
modeling techniques. Unlike prior efforts that focus either on cardinality
estimation or on synthetic cost metrics, we model the actual runtimes using
fine-grained plan statistics and query embeddings derived from execution
traces, to improve the model accuracy. We compare baseline regressors, a
refined XGBoost model, and a sequential LSTM-based model to assess their
effectiveness in runtime prediction. Our dataset includes over 1000 queries
generated from TPC-H query templates executed in PostgreSQL with EXPLAIN
ANALYZE. Experimental results show that the XGBoost model significantly
outperforms others, achieving a mean squared error of 0.3002 and prediction
accuracy within 10% of the true runtime in over 65% of cases. The findings
highlight the potential of tree-based learning combined with execution plan
features for improving cost estimation in query optimizers.

</details>


### [76] [Speeding up SQL subqueries via decoupling of non-correlated predicate (extended version)](https://arxiv.org/abs/2510.05907)
*Dmitrii Radivonchik,Yakov Kuzin,Anton Chizhov,Dmitriy Shcheka,Mikhail Firsov,Kirill Smirnov,George Chernishev*

Main category: cs.DB

TL;DR: 把相关子查询中与外部无关的谓词提取出来，重写查询并在基于块的执行模型与位置支持列存中利用延迟物化来减少相关谓词的评估，成本模型帮助判断何时有利，实测在合适场景可达5倍加速


<details>
  <summary>Details</summary>
Motivation: Reduce repeated evaluation of correlated subquery predicates by factoring out non-correlated parts to cut computation and I/O

Method: Predicate isolation + rewrite + cost model

Result: Proposed rewrites, block-based Volcano adaptation, benefit in position-enabled column-store with late materialization; cost model; experiments show up to 5x speedup under suitable conditions

Conclusion: 技术在选取性合适时显著减少相关谓词评估次数，能在支持位置的列存和延迟物化的系统中带来明显加速，但在不满足前提或在其他系统中可能不占优

Abstract: In this paper, we discuss a novel technique for processing correlated
subqueries in SQL. The core idea is to isolate the non-correlated part of the
predicate and use it to reduce the number of evaluations of the correlated
part. We begin by providing an overview of several classes of queries that may
benefit from this technique. For each class, we propose a potential rewrite and
discuss the conditions under which it is advantageous. Next, we address the
evaluation aspects of the proposed rewrites: 1) we describe our approach to
adapting the block-based Volcano query processing model, and 2) we discuss the
benefits of implementing that technique within a position-enabled column-store
with late materialization support. Finally, we present a simple cost model that
allows estimation of the benefits of said rewrites.
  Our evaluation has a quantitative part and a qualitative part. The former
focuses on studying the impact of non-correlated predicate selectivity on our
technique. The latter identifies the limitations of our approach by comparing
it with alternative approaches available in existing systems. Overall,
experiments conducted using PosDB (a position-enabled column-store) and
PostgreSQL demonstrated that, under suitable conditions, our technique can
achieve a 5x improvement.

</details>
