<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 100]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

TL;DR: MultiFoodChat通过VLM与LLM驱动的多轮对话和多智能体协同推理，实现零样本食品识别，在准确性与可解释性上优于现有非监督/少样本方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督模型依赖大规模标注数据且在未见食物类别上泛化能力差，迫切需要无监督或零样本方案以减少标注成本并提升鲁棒性。

Method: 结合视觉-语言模型（VLMs）与大型语言模型（LLMs），通过多轮视文对话实现协同推理。引入Object Perception Token（OPT）用于捕捉细粒度视觉属性，引入Interactive Reasoning Agent（IRA）用于动态解读上下文线索以精炼预测。整体为多智能体设计，允许无需额外训练或人工标注的灵活推理。

Result: 在多个公开食品数据集上，MultiFoodChat在识别精度和可解释性上均优于现有无监督和少样本方法，展示了作为食品质检与分析新范式的潜力。

Conclusion: 该论文提出了MultiFoodChat，一个基于对话的多智能体推理框架，用于实现零样本食品图像识别，旨在提升对未见食物类别的泛化能力。

Abstract: Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [2] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

TL;DR: 该演示系统使用训练的分割模型在腹腔镜视频中检测并用多色覆盖标注深色子宫内膜异位灶，并生成检测摘要以辅助医生浏览。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症外观多样且分布在体内不同位置，非专家难以准确识别；因此开发自动化视频辅助标注工具可以降低漏诊与误诊、提高医生效率。

Method: 基于训练的图像分割模型（可能为深度学习分割网络）对手术视频逐帧或帧序列进行分析，检测并用多色覆盖标注识别出的异位灶区域，生成检测摘要以改善视频浏览。

Result: 系统能够在腹腔镜手术视频中识别并以多色覆盖展示常见的深色内膜异位灶，并提供检测摘要用于改善视频回放与浏览体验。

Conclusion: 论文提出了一个用于在腹腔镜手术视频中识别和分割深色子宫内膜异位灶的演示系统，有助于辅助妇科医生浏览和标注病灶。

Abstract: Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [3] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 将YOLO与多种VLM结合，通过视觉候选+语言推理提升遥感影像中飞机检测与场景理解，在退化与少样本情形下表现稳健，MAE显著下降，CLIPScore小幅提升。


<details>
  <summary>Details</summary>
Motivation: 传统视觉模型依赖大量领域标注数据且缺乏对复杂场景的语义理解，VLM提供视觉-文本融合能力，可补足语义与上下文信息，从而在遥感领域提高检测与解释性能。

Method: 基于传统目标检测模型YOLO进行初步候选框生成，再将检测结果与图像一并输入多模态VLM（LLaVA、ChatGPT、Gemini）进行上下文理解与验证；在有标签与无标签数据上进行评估，并引入图像退化情景测试鲁棒性。

Result: 在飞机检测与计数任务上，模型平均MAE降低48.46%；在全景理解评价（CLIPScore）上提升6.17%；在退化图像与少样本/无标签设置中仍维持较好表现。

Conclusion: 结合YOLO与VLM（如LLaVA、ChatGPT、Gemini）能显著提升遥感影像中飞机检测与场景理解的精度，尤其在低质量或退化图像与少样本场景下效果明显。

Abstract: Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [4] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

TL;DR: 作者开发了基于EfficientNetV2-S和MIL的AI模型，对前列腺癌cribriform形态检测表现接近或优于病理专家，内外部验证均显示高AUC并在人机对比中取得最高平均一致性。


<details>
  <summary>Details</summary>
Motivation: cribriform形态提示较差预后且会影响是否适合主动监测，但该形态在病理报告中易被漏报且病理学家间一致性低，因此需要一种能提高检测准确性和一致性的辅助工具。

Method: 基于EfficientNetV2-S编码器并结合多实例学习（MIL）进行端到端全片分类的深度学习模型。训练集为640张来自430名患者的针吸活检切片；内部验证集和外部独立验证集分别用于评估泛化性。三位高一致性泌尿病理学家提供参考标注，并与九位专家进行了人机对比。

Result: 内部验证AUC 0.97（95% CI 0.95-0.99），Cohen's kappa 0.81（95% CI 0.72-0.89）；外部验证AUC 0.90（95% CI 0.86-0.93），Cohen's kappa 0.55（95% CI 0.45-0.64）。在人机对比的88张切片中，模型平均Kappa为0.66（95% CI 0.57-0.74），优于九位病理学家（0.35-0.62）。

Conclusion: 本研究表明所开发的AI模型在识别前列腺癌的cribriform形态方面达到了接近病理学专家水平，能提高诊断一致性并有助于临床决策。

Abstract: Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [5] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

TL;DR: NAPPure通过对抗图像生成建模与最大似然解耦，扩展了净化方法以有效抵抗非加性扰动，实验证明在GTSRB与CIFAR-10上性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有去噪/对抗净化方法多基于加性扰动假设，难以应对现实中常见的非加性扰动（如模糊、遮挡、畸变），因此需要扩展的净化框架以覆盖更广泛的扰动类型。

Method: 首先建立对抗图像的生成过程模型，将观察到的图像视为由干净图像与一组扰动参数通过非加性变换合成；然后基于该生成模型，通过最大化观测图像的似然来联合估计干净图像和扰动参数，完成去污（purification）。

Result: 在GTSRB和CIFAR-10数据集上实验表明，NAPPure在应对非加性扰动时显著提升了图像分类模型的鲁棒性，相比现有方法具有明显优势。

Conclusion: 提出的NAPPure框架能够有效处理非加性对抗扰动，通过建立对抗图像生成过程并最大似然分解干净图像与扰动参数，从而提高分类模型在模糊、遮挡和畸变等非加性扰动下的鲁棒性。

Abstract: Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [6] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: Vgent以图结构化视频并加入中间结构化推理，解决RAG在长视频中时序破碎与噪声问题，显著提升LVLM的长视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM难以处理超出上下文窗口的大量视频标记且容易丢失长程时序信息；直接将RAG应用于长视频会破坏时间依赖并引入无关信息，影响推理准确性。

Method: 构建视频片段的语义图（节点为片段，边为语义关系）；基于图的检索选择相关片段作为上下文；在生成前引入结构化验证和推理模块以聚合和筛选检索信息；最后在LVLM上进行生成回答。

Result: 在三项长视频理解基准上对多种开源LVLM评测，Vgent较基线在MLVU上提升3.0%~5.4%，并比现有视频RAG方法高出8.6%。

Conclusion: Vgent通过将长视频表示为结构化图并引入中间推理步骤，有效提升了LVLM在长视频理解任务中的检索与推理能力，显著提高了多项基准测试的性能。

Abstract: Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [7] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

TL;DR: 提出基于原型的1D共享表示TPL，通过学习统一原型序列来对齐不同场景与生成式AI视频，显著改善多视频同步的准确性与效率，并提供代码与新数据集。


<details>
  <summary>Details</summary>
Motivation: 传统同步仅适用于相同场景的简单时间偏移；不同场景或生成式AI视频存在主题、背景差异和非线性时间错位，需更鲁棒的跨场景同步方法。

Method: 从预训练模型提取高维时空嵌入，TPL学习一个统一的原型序列（1D），将各视频的帧嵌入映射到该原型，利用原型对齐而非两两帧匹配来同步视频；可兼容多种预训练特征，应用于帧检索与阶段分类。

Result: 实验表明TPL在多数据集上提升同步精度、效率和鲁棒性，能处理细粒度帧检索与阶段分类任务，并首次缓解多条生成式AI视频的同步问题；同时作者发布了代码与新数据集。

Conclusion: 本文提出的Temporal Prototype Learning (TPL)能在不同场景与生成式AI视频间实现稳健同步，通过学习共享的紧凑1D原型序列来锚定关键动作阶段，避免逐对匹配，从而提升准确性、效率与鲁棒性。

Abstract: Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [8] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出Capture, Canonicalize, Splat零样本流水线：用生成式规范化统一多视图，再用在穹顶捕捉构建的高保真Gaussian splatting数据集上训练的transformer生成高保真、保留身份的静态四分身头像。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么单视图导致几何不一致和虚构细节影响身份保留，要么在合成数据上训练无法捕捉皮肤皱纹和细微头发等高频细节，限制真实感。需要一种能从少量无结构手机图像生成高质量保真头像的零样本方法。

Method: 提出Capture, Canonicalize, Splat三阶段管道：1) generative canonicalization模块将多视图无结构图像生成标准化一致的表征；2) 基于transformer的模型在新建的大规模高保真Gaussian splatting头像数据集上训练（数据来自穹顶捕捉的真实人物）；3) 使用该模型从输入照片生成静态四分身avatar。

Result: 该管道能在零样本条件下从无结构手机照片生成具有强真实感和稳健身份保留的静态四分身头像，弥补了单视图和合成数据训练方法的不足。

Conclusion: 方法实现了从少量无结构手机照片生成高保真、保留身份的静态四分身头像，克服了单视图几何不一致和合成数据训练缺陷，生成效果逼真并保留个体特征。

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [9] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

TL;DR: cubic是一个对SciPy/scikit-image API做GPU透明替换的开源Python库，利用CuPy和cuCIM在2D/3D生物图像处理上实现自动调度与显著加速，兼容现有工作流并提升可扩展性与效率。


<details>
  <summary>Details</summary>
Motivation: 现代显微镜产生的2D/3D大规模数据对现有工具的可扩展性、效率和与现代科学计算工作流的集成提出挑战，很多工具缺乏API、GPU加速、全面的3D处理能力或互操作性，阻碍高通量和交互式分析。

Method: 通过对SciPy和scikit-image的API进行扩展，集成CuPy与RAPIDS cuCIM的GPU加速实现，提供设备无关的API，依据数据所在设备自动调度到GPU或CPU，覆盖从预处理到分割与特征提取的2D/3D图像处理流程。

Result: 对单个操作和现有去卷积与分割流水线进行基准测试与复现，cubic在保持算法一致性的前提下实现了显著加速，支持交互式探索与自动化高通量分析，并与Python科学计算生态系统良好集成。

Conclusion: cubic使得现有Python生物图像分析工作流能够在GPU上透明加速，在保持算法一致性的同时显著提升了处理速度，从而为可扩展、可重复的生物图像分析提供了稳健基础。

Abstract: Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [10] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

TL;DR: 通过用4D Gaussian Splatting和视频重光照合成多样化训练数据，并在其上微调视频扩散模型，论文实现了多视角身份保持、精确3D相机控制和光照适应，且支持高效多主体合成与虚拟制作工作流。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成难以同时保持人物跨视角身份一致、精确控制相机运动与适应灯光变化，尤其在虚拟制作场景中需要更强的多视角和相机可控性。

Method: 使用采集的体积捕捉表演，通过4D Gaussian Splatting对不同相机轨迹重渲染，并用视频重光照模型加入光照多样性；在这些数据上微调开源视频扩散模型；此外通过联合训练与噪声混合两种方式实现多主体合成。

Result: 实验表明生成视频质量提升，个性化准确度更高，并增强了相机控制与光照适应性；方法也支持场景和真实视频定制以及在定制过程中对运动和空间布局的控制。

Conclusion: 该论文提出了一个用于视频扩散模型的定制化管线，能同时实现多视角角色一致性与3D相机控制，并支持灯光变化、多主体生成和虚拟制作相关能力。

Abstract: We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [11] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

TL;DR: 本文首次尝试将Big Five与HEXACO联合建模进行多模态表观人格识别，提出多任务/共享表示方法，并在自我介绍视频数据集上证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前大多数工作只关注Big Five，缺乏对HEXACO（包含Honesty-Humility）的表观识别研究，且未阐明Big Five与HEXACO在机器学习建模下的关系。联合建模可提升对多模态行为的感知。

Method: 提出一种联合学习框架，同时训练预测Big Five和HEXACO特质，利用自我介绍视频数据集，可能通过共享表示或多任务损失来捕捉两者关系。

Result: 在自我介绍视频数据集上的实验表明，所提方法在识别Big Five与HEXACO上均取得有效性能（论文宣称效果优异）。

Conclusion: 本研究提出将Big Five与HEXACO联合建模以从多模态人类行为中自动识别表观人格特质，结论是联合优化能有效同时识别两套人格维度。

Abstract: This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [12] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

TL;DR: 利用位平面提取低位噪声并结合最大梯度块选择与轻量分类器，本文实现了快速且高精度的AI生成图像检测，具备强泛化性和极高效率。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建误差的方法计算量大且难以捕捉原始图像中的固有噪声特征，作者观察到低位平面更能反映噪声模式，进而提出更高效的错误提取方案。

Method: 方法包括：1) 使用位平面分解提取低位平面的噪声信号并进行归一化（缩放和阈值化）；2) 通过多方向梯度计算噪声得分，设计最大梯度块选择策略以放大噪声信号；3) 设计轻量级分类头，比较两种结构：纯噪声分类器与噪声引导分类器。

Result: 在GenImage基准上取得平均准确率98.9%，比对比方法提升11.9%，显示优良的跨生成器泛化（GAN→Diffusion>98.2%，Diffusion→GAN>99.2%），并将错误提取速度提升到毫秒级，接近快100倍。

Conclusion: 本文提出基于位平面(bit-plane)处理的噪声提取与快速检测方法，可高效区分AI生成图像与真实图像，具有显著提升的准确率和跨生成器泛化能力。

Abstract: The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [13] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

TL;DR: PIA是一种融合音素、嘴唇几何与身份嵌入的多模态时序检测方法，专门用于识别由先进生成模型制造的高质量深度伪造，通过跨模态一致性检测弥补传统方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型能生成几乎完美的独立帧，传统基于帧级一致性或单一模态的检测方法难以发现细微的时间错配或身份动态异常，因而需要结合语音-嘴形的音素-视形对齐以及身份动态信息的多模态分析来提升鲁棒性。

Method: 构建多模态特征管线：从音频获取音素序列（或语音特征），从视频提取嘴唇几何特征和面部动态序列，并计算高质量的面部身份嵌入；在时间维度上对这些模态进行对齐和建模（如使用时序网络或跨模态注意力），并输出不一致评分用于判定伪造。

Result: 该方法在论文中宣称显著提高了检测性能，特别是在捕捉微小时序差异和身份动态异常方面优于传统基准方法；并且作者开源了代码（给出GitHub链接）。

Conclusion: 该论文提出了一种多模态音视频检测框架PIA，通过结合语音（音素序列）、嘴唇运动几何、以及身份嵌入来捕捉深度伪造中的细微时序与身份动态不一致，从而提升对现代生成模型（GAN、扩散、神经渲染）生成的高质量深度伪造的检测能力。

Abstract: The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [14] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 提出EIM——利用事件间隔编码以发挥EVS时序能力，优化传感器参数并通过实验实现室内10 m处28 kbps和50 m处8.4 kbps，显著提升事件基OCC的比特率。


<details>
  <summary>Details</summary>
Motivation: 现有基于帧的摄像头OCC系统存在比特率低、处理负担高等限制，事件驱动视觉传感器具备高时效性、低延迟和高动态范围，但现有事件基OCC多使用传统调制方式（如OOK、PPM），尚未有专门利用EVS独特特性的调制方法。论文旨在提出一种充分利用EVS异步高时序分辨率的调制方案以提升传输速率。

Method: 提出EIM调制理论模型；调整和定制EVS的参数以优化其对EIM的频率响应；实验测定可用的最大调制阶数；基于这些参数进行传输实验并测量不同距离下的比特率和误码性能。

Result: 开发并验证了EIM方案，实验结果表明在室内环境可达10 m: 28 kbps，50 m: 8.4 kbps，证明EIM在事件基OCC中能显著提升比特率并为该领域树立新的性能基准。

Conclusion: 该论文提出了一种针对事件驱动视觉传感器(EVS)的专用调制方案——事件间隔调制(EIM)，通过利用事件之间的时间间隔进行信息编码，从而提高了事件基可见光通信的传输速率。通过调整EVS参数以优化频率响应并实验确定最大调制阶数，作者在室内环境下实现了10 m处28 kbps和50 m处8.4 kbps的成功传输，创下事件基OCC系统的比特率新高。

Abstract: Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [15] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

TL;DR: 提出MACE：使用门控的混合专家网络与ALF-LB策略，在大场景定位与渲染中实现低成本高精度，且可在10分钟内完成高质量渲染训练。


<details>
  <summary>Details</summary>
Motivation: 单一网络容量有限，导致场景坐标回归方法扩展到大规模场景时精度和效率下降；需要在保持定位与渲染质量的前提下降低计算与训练成本。

Method: 引入基于MOE的混合专家架构：一个门控网络对输入隐式分类并路由到单个子网络，使每次推理仅激活一个子网络；提出ALF-LB（Auxiliary-Loss-Free Load Balancing）策略用于均衡专家负载以提高大场景定位精度；在渲染模块结合快速训练策略，在剑桥数据集上实现了仅10分钟训练即可得到高质量渲染。

Result: 在大场景定位与渲染任务上，MACE显著降低计算开销，同时在定位精度和渲染质量上优于或至少与现有方法持平；在Cambridge测试集上仅用10分钟训练便得到高质量渲染结果（具体数值未在摘要给出）。

Conclusion: MACE通过混合专家结构与门控网络、以及无辅助损失的负载均衡策略，在大场景定位与渲染任务中实现了高效推理与高质量重建，显著降低计算成本并提升精度。

Abstract: Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [16] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

TL;DR: IPRO用基于人脸身份评分的奖励优化扩散模型，通过在采样末端反传奖励、多角度人脸特征池与KL正则化，有效提升图像到视频生成的人物身份一致性，兼具快速收敛与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V模型难以在大幅表情或动作变化下保持输入人脸与生成视频的人物身份一致，尤其当人脸在图像中占比很小时，人类对身份差异高度敏感，故需提出无需改动模型结构即可增强身份保持的优化方法。

Method: 在预训练I2V扩散模型上引入基于面部身份打分器的奖励函数，使用强化学习思路将奖励信号在采样链的末端步骤中反向传播以获取更丰富的梯度反馈，构建以真实视频人脸为特征池的打分机制以增强泛化，并加入KL散度正则化防止过拟合。

Result: 在Wan 2.2 I2V模型和作者自研I2V模型上的大量实验表明，IPRO在身份保持上有明显提升，且训练更稳定、收敛更快。代码和项目已公开。

Conclusion: 提出的IPRO方法通过在扩散模型训练中引入基于人脸身份打分器的强化学习奖励优化，能有效提升图像到视频生成中人物身份一致性，并通过在采样链后端反向传播奖励、采用多角度人脸特征池以及KL散度正则化来加速收敛并稳定训练。

Abstract: Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [17] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

TL;DR: 提出将人类偏好驱动的GRPO用于多人物身份保持的视频生成，借助大规模偏好数据训练视频奖励模型，显著提升身份一致性，最高改进18.9%。


<details>
  <summary>Details</summary>
Motivation: 现有专注于单一主体或场景的生成方法在涉及多人物互动时难以保持一致身份，亟需引入人为偏好反馈以优化多人物身份保持。

Method: 构建了一个以人为注释和合成失真数据为基础的大规模偏好数据集并训练视频打分模型；在此基础上提出针对多人物一致性的GRPO变体用于策略优化，并对VACE与Phantom进行微调。

Result: 在多项消融实验和比较中，Identity-GRPO在保持人类一致性指标上相较基线方法提升最多达18.9%，并提供了注释质量与设计选择对策略优化影响的定量分析。

Conclusion: Identity-GRPO通过引入以人为中心的反馈优化流程，有效提升了多人物视频生成中身份一致性的问题，显著优于现有方法。

Abstract: While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [18] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

TL;DR: 提出基于相对位置动态采样的MatchAttention与BilinearSoftmax，结合分层MatchDecoder及遮挡处理策略，实现高分辨率实时高精度跨视图匹配，多个基准上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有跨注意力在高分辨率图像上计算复杂度呈二次增长，且缺乏显式匹配约束，导致难以扩展到高分辨率或处理遮挡。论文旨在设计一种显式对齐相对位置的注意力机制，以降低复杂度并提高匹配精度和鲁棒性。

Method: 核心方法为MatchAttention：1) 用相对位置决定key-value的注意力采样中心；2) 提出BilinearSoftmax，实现连续可微的滑动窗口注意力采样；3) 将相对位置嵌入到通道并通过残差跨层迭代更新；4) 以此为核心设计分层MatchDecoder进行高效跨视图解码；5) 为处理遮挡引入门控cross-MatchAttention和一致性约束损失。

Result: 在多个基准数据集上达到了领先或竞争的性能：MatchStereo-B在Middlebury平均误差排名第1，且在KITTI分辨率上推理只需29ms；MatchStereo-T能在0.1s内处理4K图像且仅用3GB显存。此外在KITTI 2012/2015、ETH3D和Spring flow上也达到SOTA/接近SOTA水平，兼顾高精度和低计算开销。

Conclusion: 本文提出了MatchAttention，通过动态匹配相对位置并用BilinearSoftmax实现连续可微的滑动窗口采样，从而在高分辨率下实现高效准确的跨视图匹配。结合层间残差更新的相对位置嵌入、分层解码器MatchDecoder、门控机制和一致性约束损失，模型在处理遮挡和提高匹配鲁棒性方面效果显著。最终在多个基准（Middlebury、KITTI、ETH3D、Spring）上达到或领先于SOTA，并在高分辨率实时推理上表现优异。

Abstract: Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [19] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

TL;DR: 利用事件相机、OOK+toggle解调与数字锁相环，作者在户外实现了200m@60kbps和400m@30kbps下BER<10^{-3}的稳健OCC链路。


<details>
  <summary>Details</summary>
Motivation: 动机是提高基于事件相机的OCC在复杂户外环境下的鲁棒性和通信距离/速率，克服传统帧式相机在时序分辨率与抗干扰能力上的不足，扩展OCC在远距离和高速率场景下的可用性。

Method: 方法上，作者使用事件相机接收光信号，采用OOK（开关键控）进行调制；接收端通过toggle解调减少光强波动与背景干扰影响，同时引入数字锁相环来实时跟踪信号相位/时钟，实现稳健的比特恢复。实验包含不同距离与速率的户外测试，评估误码率。

Result: 结果显示，在户外实验中，该方案在200 m、60 kbps和400 m、30 kbps条件下均可达到误码率BER < 10^{-3}，表明方法在远距离和不同速率下具有较高的可靠性。

Conclusion: 该文提出了一种基于事件相机的光学相机通信（OCC）稳健解调方案，结合了OOK调制、切换（toggle）解调与数字锁相环（DPLL），并在户外实验中在200米、60 kbps及400米、30 kbps条件下实现了BER < 10^{-3}，首次达到该性能。

Abstract: We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [20] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: GauSSmart用2D分割和DINO特征监督增强高斯Splatting，改善稀疏区域的重建并在多个数据集上取得优于基线的结果。


<details>
  <summary>Details</summary>
Motivation: 高斯Splatting在大尺度数据上表现良好，但在覆盖稀疏或细节区域时存在局限，2D基础模型可提供丰富的语义与结构先验以弥补这一不足

Method: 在高斯Splatting基础上引入2D分割先验、凸滤波和来自基础模型（如DINO）的语义特征监督，用于引导高斯斑点的密化与细化

Result: 在三个数据集上的多数场景里，GauSSmart优于现有高斯Splatting方法，表现为更好的覆盖率与细节重建

Conclusion: GauSSmart通过将2D基础模型与3D高斯Splatting结合，有效提升了稀疏区域的重建质量和细节保留

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [21] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

TL;DR: 利用前门因果框架与显式图像变换，把语义特征作为中介以减弱混杂，实现病理图像跨域泛化，实验证明在两个数据集上有约7%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法多通过对齐统计分布或引入统计变异来应对域偏移，但忽视了潜在的因果关系和混杂影响，导致泛化受限。

Method: 提出基于前门准则的变换策略：将语义特征作为中介变量，构造对观测切片的显式转换以阻断混杂路径，实现因果效应估计；在训练中融合该变换以提升模型对未见域的稳健性。

Result: 在CAMELYON17和一私有病理数据集上验证，方法在未见域上均取得性能提升，最高可达约7%的改进，优于现有基线方法。

Conclusion: 本文提出了基于因果推断的框架，通过前门准则设计显式中介变量和图像变换策略，利用语义特征并削弱混杂影响，从而提升病理切片跨域泛化能力。

Abstract: Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [22] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: 提出一种训练-free的三层对比解码+水印方法，通过在解码层中选择成熟/业余/枢纽层并基于水印问题判断视觉落地性，从而显著降低LVLMs的幻觉并提升视觉对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs易产生幻觉，可能过分依赖单一模态或记忆训练数据，缺乏对视觉信息的有效落地。提出无需再训练即可增强模型对视觉内容的依赖，从而降低幻觉。

Method: 方法包含三步：1) 在解码层中选取一个成熟层和一个业余层；2) 通过一个与水印相关的问题评估各层的视觉落地性并确定枢纽层；3) 基于这三层进行三层对比解码生成最终输出。整个方法无需额外训练并结合水印检测判断层级的视觉依赖。

Result: 在POPE、MME和AMBER等公开基准上，该方法在减少幻觉和生成更具视觉落地性的回答方面达到了最先进的性能。

Conclusion: 该论文提出了一种训练-free的方法，通过三层（成熟层、业余层、枢纽层）对比解码与水印检测来减少大视觉-语言模型(LVLMs)的幻觉问题，从而提高模型的视觉落地性。

Abstract: Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [23] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

TL;DR: 提出MID-StyleGAN，融合扩散模型与GAN进行多域眼部图像生成与翻译，缓解数据稀缺并显著提升PAD系统在LivDet2020等数据集上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有虹膜PAD受训练数据不足限制，构造与采集PAs（人工眼、打印图、隐形眼镜）困难，导致PAD泛化性差，需合成高质量多域数据。

Method: 提出结合扩散模型与StyleGAN的多域图像翻译框架，通过条件/多域架构实现在不同呈现攻击域与真实域间的互译；引入适配眼部数据的自适应损失以保持域一致性并增强多样性与真实感。

Result: 生成图像质量优于现有方法；用合成数据增强训练后，LivDet2020上TDR@1%FDR从93.41%提升至98.72%，证明合成数据能显著提高PAD系统性能。

Conclusion: MID-StyleGAN可有效生成多域（真实、印刷、彩色隐形眼镜）眼部合成图像，从而缓解数据匮乏，提高PAD性能。

Abstract: An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [24] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: VaCo通过MTQs、VALs和TGM，将多源视觉模型的任务感知特征引入MLLM训练，实施视觉判别对齐与表示协调，从而显著增强模型的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM仅以文本的下一个token预测为监督，忽略了对视觉信息的显式优化，导致模型在需要视觉分析的任务上能力不足。作者希望通过引入视觉导向的监督和多源视觉模型的协同来提升视觉理解能力。

Method: 在MLLM内部加入可学习的模块化任务查询（MTQs）和视觉对齐层（VALs），并利用多个视觉基础模型（VFMs）提供任务感知的感知特征作为监督，通过视觉判别对齐优化视觉与文本输出的统一；采用Token Gateway Mask（TGM）限制不同MTQ组之间的信息流以协调多VFM间的表示冲突。

Result: 在多项基准测试中，VaCo显著提升了不同MLLM的性能，尤其在视觉理解相关任务中展示出更优的能力。

Conclusion: VaCo通过引入视觉判别对齐和多源VFM协调机制，解决了MLLM仅靠文本预测监督导致的视觉信息忽视问题，最终提升了模型在视觉理解任务上的表现。

Abstract: Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [25] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

TL;DR: 用循环一致性关键点和结合GRU的变换同步位姿块进行自监督RGB-D配准，显著提升匹配与配准性能。


<details>
  <summary>Details</summary>
Motivation: 随着消费级深度相机大量未标注RGB-D数据的出现，研究如何利用这些数据进行场景几何推理；寻求在缺乏监督标签下提升配准精度的自监督方法。

Method: 使用循环一致性关键点作为显著点以在匹配时强制空间一致性约束，从而提高对应点精度；提出包含GRU循环单元与变换同步的位姿模块，将历史信息与多视角数据融合；将这些组件集成至现有方法以评估通用性。

Result: 在ScanNet和3DMatch数据集上，方法优于之前的自监督配准方法，并在某些情况下超过早期监督方法；将提出的组件集成到现有方法亦带来性能提升。

Conclusion: 该论文提出通过循环一致性关键点和融合历史及多视角信息的新型位姿模块来提升RGB-D配准的自监督性能，实验证明在ScanNet和3DMatch上优于先前自监督方法并超过部分早期监督方法。

Abstract: With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [26] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 提出SPR：通过语义与定位评分对MLLM描述进行重写与偏好优化，以奖励精确本地化的细粒度描述，从而显著提升模型的空间理解与定位表现，训练成本低。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过调整MLLM以拟合预标注指令数据来注入空间知识，但缺乏对模型实际输出的直接监督，导致模型在细粒度空间感知（如精确描述或定位）和满足用户期望方面表现不足。

Method: 随机选取图像区域与MLLM生成的区域描述，计算语义分数与定位分数来评估文本与定位质量；对描述进行细化以提高定位准确性；将得分最高的细化结果与最低得分的初始描述配对，进行偏好优化（奖励更精确的本地化描述）；在标准指代与定位基准上进行评估。

Result: 在标准的指代与定位基准上，SPR在提升MLLM的空间理解能力方面取得明显效果，且训练开销很小。

Conclusion: SPR方法通过对MLLM生成的区域描述进行语义与定位评分，并对低质量描述进行精细化重写与偏好优化，从而提升模型的细粒度空间理解能力。

Abstract: Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [27] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出DOS，通过对CLIP文本嵌入进行方向性调整，显著改善多对象文本到图像生成中的对象区分问题，实验和人类评估均证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在处理包含多个对象的提示时常出现漏检或目标混合；通过研究发现四类易出错场景（相似形状、相似纹理、不同背景偏差、多个对象），并利用CLIP嵌入的性质提出解决方案。

Method: 基于对CLIP文本嵌入的两点观察，DOS在输入T2I模型前对三类CLIP文本嵌入进行修改（方向性分离），以增强不同目标在嵌入空间的可区分性。

Result: 在多项基准测试中，DOS持续提高生成多目标图像的成功率并降低对象混合；主观评估中比四种对比方法获得更多投票（高出26.24%~43.04%）。

Conclusion: DOS能显著提升多目标图像生成的成功率并减少目标混合问题，是一种实用有效的方法。

Abstract: Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [28] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

TL;DR: DRBD-Mamba通过空间填充曲线、双向门控融合与特征量化，提出一种高效且鲁棒的3D肿瘤分割方法，在多个评测划分上提升了肿瘤子区分割性能并大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于Mamba的状态空间模型虽能捕捉长程依赖，但在3D医学图像上因跨多轴序列计算导致计算开销大，且在不同BraTS数据划分上的稳健性未被充分评估。

Method: 利用空间填充曲线将3D特征映射为1D以保留空间局部性，减少多轴序列扫描；采用双向（前向和反向）Mamba特征并通过门控融合模块自适应整合；加入量化模块离散化特征以提升鲁棒性；采用双分辨率结构捕捉多尺度长程依赖。

Result: 在公开的20%测试集上，Whole Tumor Dice提高0.10%，Tumor Core提高1.75%，Enhancing Tumor提高0.93%；在作者提出的五折系统划分上，平均Tumor Core Dice提升0.86%，Enhancing Tumor提升1.45%；模型在效率上提升约15倍，同时维持高分割精度。

Conclusion: 提出的DRBD-Mamba在保持高精度分割的同时显著提升计算效率，尤其在肿瘤核心和增强肿瘤分割上有明显增益，并在多折评估中表现稳健。

Abstract: Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [29] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

TL;DR: BoardVision通过数据集基准、模型比较与轻量集成（CTV Voter），并评估扰动稳健性，推动主板组装级缺陷检测从研究向实际部署转化。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于裸板或线路级缺陷检测，组装级全主板检测（如螺丝丢失、风扇线松动、表面划痕）尚未充分探索，而这些直接影响大规模电子制造的可靠性。

Method: 基于MiracleFactory主板数据集，在受控条件下比较YOLOv7和Faster R-CNN两类代表性检测器；提出轻量级集成策略Confidence-Temporal Voting（CTV Voter）以平衡精度与召回；并通过GUI工具实现可部署的检验流程。

Result: 提供了对两种检测器的系统性比较，展示YOLOv7在精度上优于Faster R-CNN但召回不足；提出的CTV Voter在平衡精度和召回方面取得改进；并评估了在清晰度、亮度、方向等扰动下的稳健性，最后发布了可操作的GUI检验工具。

Conclusion: 本文构建了一个名为BoardVision的可复现框架，用于主板组装级缺陷检测，提供数据集基准、模型比较、轻量集成方法以及可部署工具，推动视觉算法向实际质量检验过渡。

Abstract: Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [30] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

TL;DR: DCMIL通过easy-to-hard双课程对比多实例学习，高效处理多放大倍数WSI，提升癌症预后预测性能并增强可解释性，无需密集注释。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于千兆像素大小的计算瓶颈和稀缺的密集人工标注，且常忽视多放大倍数WSI的细粒度信息和肿瘤微环境变化，影响预后模型的效果与可解释性。

Method: 提出了一种名为双课程对比多实例学习（DCMIL）的进阶式表征学习方法，通过easy-to-hard进程在多放大倍数WSI上进行对比学习与多实例学习结合，处理千兆像素输入并估计实例不确定性，模型不依赖密集标注。

Result: 在12种癌症（5954名患者，1254万张切片）上广泛实验证明DCMIL在预后预测任务上优于标准WSI方法，并能识别关键预后区域、估计实例不确定性、区分正常与肿瘤组织的形态差异。代码已开源。

Conclusion: 本文提出的DCMIL模型能在无需密集标注的情况下，有效将WSI直接转化为预后预测，且在12类癌症的大规模数据集上优于标准方法，能定位预后相关的细粒度区域并提供不确定性估计，有助于发现新的生物学见解。

Abstract: The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [31] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

TL;DR: 提出单模型统一的帧内/帧间神经视频压缩并附加同时双帧压缩，改善失遮挡与误差累积问题，较DCVC-RT有约10.7% BD-rate增益且保持实时性。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法在处理失遮挡、新出现内容和帧间误差传播方面存在不足，且误差会累积，影响稳定性与质量。借鉴传统视频编码允许在帧间使用帧内编码的策略以克服这些缺陷。

Method: 使用单模型处理每一帧并训练其在帧内/帧间编码间自适应切换；加入同时两帧压缩方案以向前和向后利用帧间冗余；沿用传统视频编码中允许在帧间帧内编码的思想以截断误差传播，无需手动刷新机制。

Result: 相较DCVC-RT平均实现约10.7% BD-rate降低，帧间比特率与质量更稳定，并保持实时编解码性能。

Conclusion: 提出统一的帧内/帧间编码神经视频压缩框架，通过单一模型自适应执行帧内或帧间编码，并引入同时双帧压缩设计以双向利用帧间冗余，从而解决失遮挡、新内容、误差累积等问题，显著优于DCVC-RT。

Abstract: Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [32] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

TL;DR: 提出基于核范数正则化与自适应乐观指数梯度优化的最小失真视频通用对抗攻击，效果优于现有基线且更隐蔽。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的视频目标检测器在安全应用中易受对抗样本影响，尤其是通用扰动更具实用威胁；因此希望生成在多帧上有效且隐蔽的通用扰动，且扰动结构集中于背景以减少可见性。

Method: 在损失函数中加入核范数正则项以鼓励低秩/结构化扰动，并设计一种自适应的乐观指数化梯度优化器（AO-Exp）用于高效求解；同时与低秩投影梯度下降和Frank-Wolfe方法进行对比。

Result: 在视频目标检测模型上，所提方法在攻击成功率和隐蔽性上均优于低秩投影梯度方法与Frank-Wolfe攻击，且优化过程中表现出更好的可扩展性和收敛速度。代码和数据已开源。

Conclusion: 该论文提出了一种针对视频目标检测的最小失真通用对抗攻击，利用核范数正则化诱导背景中结构化的扰动，并采用自适应乐观指数梯度法提高可扩展性和收敛性。

Abstract: Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [33] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: 综述49项研究：无监督生成模型能在脑影像异常检测中提供可解释的伪健康重建，对大病灶效果好，微小病灶仍难点；需更好的解剖感知建模、评估与临床验证以推进临床落地。


<details>
  <summary>Details</summary>
Motivation: 由于监督方法依赖大量体素级注释且受限于已知病变类型，研究者希望利用仅用健康数据训练的生成模型，通过重构偏差检测异常，扩展到未知或异质病变，支持结构可解释的伪健康重建。

Method: 采用PRISMA引导的范围性综述方法，梳理2018–2025年间49篇研究，涵盖AE、VAE、GAN和去噪扩散模型在脑MRI（及少量CT）上的应用，比较性能指标与网络设计选择。

Result: 生成模型在大体积局灶性病变（如肿瘤、梗死）表现良好，对微小或弥散性病变进展显著但仍具挑战。伪健康重建提供直观的异常定位与可解释性，模型可用于半监督学习和跨病种偏差映射。建议未来工作聚焦解剖感知模型、构建基础模型、制定任务相关评估标准并进行临床验证。

Conclusion: 本文综述表明无监督深度生成模型在脑影像异常检测与分割上具有很大潜力，尤其适用于缺乏标注的数据场景与复杂、罕见病变。要实现临床应用仍需解决解剖感知建模、标准化评估指标、跨病种泛化和严格临床验证等挑战。

Abstract: Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [34] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

TL;DR: 作者用剪枝+重置的迭代策略在多任务图像修复模型中找到了稀疏但高性能的子网络（MIR-L），在10%参数量下仍维持或优于密集模型性能。


<details>
  <summary>Details</summary>
Motivation: 多任务（all-in-one）图像修复模型参数量大、计算高，需寻找更小且高效的子网络以减少计算和存储开销。

Method: 提出MIR-L方法：基于迭代剪枝（剪除低幅值权重）与重置剩余权重到原始初始化的循环，寻找‘winning tickets’子网络。

Result: 在去雨、去雾、去噪基准数据集上，MIR-L在仅保留10%可训练参数的情况下仍保持高恢复性能，甚至达到或超过最新方法。代码与预训练模型已公开。

Conclusion: 该论文提出了一种针对多任务图像修复模型的剪枝压缩策略，能够在高稀疏率下保持或超过密集模型性能。

Abstract: Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [35] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

TL;DR: 利用Sentinel-2时间序列与CNN-LSTM集成模型，可在地块级实现高召回的季节性放牧检测，能显著提升有限巡查资源的效率，适合用于合规与保护监测。


<details>
  <summary>Details</summary>
Motivation: 放牧同时影响农业产出与生物多样性，但缺乏可扩展的放牧活动空间监测方法。作者旨在用免费、全球覆盖的卫星影像实现可操作的放牧检测以支持合规与保护工作。

Method: 使用每个地块的4-10月Sentinel-2 L2A时间序列多谱反射率作为输入，训练CNN-LSTM模型集成，通过五折验证评估性能，主要指标为F1和召回率；并开展基于模型预测的巡查优先级模拟。

Result: 在五次验证拆分上平均F1=77%，放牧地块召回率达90%。在资源受限（每年巡查≤4%地点）的运营情境下，若优先巡查模型预测为未放牧的地块，可比随机巡查多确认约17.2倍的未放牧地点。代码与模型已公开。

Conclusion: 本文表明可利用Sentinel-2多时相数据和CNN-LSTM集成模型，在地块级别实现季节性放牧（二分类）检测，达到较高召回率和实用的检测性能，可用于指导巡查资源分配。

Abstract: Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [36] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

TL;DR: 本文首次将Vision Mamba用于三维多孔介质渗透率预测，证明其在效率与精度上的优势，并提供了对组件影响的消融分析与开源实现。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba相较于ViT具有线性尺度增长的网络规模和更少的可训练参数，能在高分辨率三维图像任务中显著降低计算与内存开销，因此将其应用于三维多孔介质渗透率预测以提高效率与可扩展性。

Method: 构建基于Vision Mamba的渗透率预测神经网络，并与ViT和CNN基线模型在相同数据集和任务上比较；进行消融研究以评估各组件对准确率的影响；公开源代码以保证可复现性。

Result: 实验结果显示Vision Mamba在内存占用和计算需求上明显优于ViT（因其从二次尺度降为线性），参数量少于CNN；在预测精度上与或超过ViT和CNN。消融研究揭示了关键组件对性能的贡献。源代码已公开。

Conclusion: 本文提出将Vision Mamba作为骨干网络首次应用于三维多孔介质渗透率预测，实验证明其在计算与内存效率上优于ViT和传统CNN，同时保持或提升预测精度。

Abstract: Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [37] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

TL;DR: SurgScan为外科器械制造的自动化质量控制提供了一个高精度、实时且可扩展的AI解决方案，显著减少了人工检测依赖。


<details>
  <summary>Details</summary>
Motivation: 人工检测存在误差和不一致性，医疗器械缺陷影响无菌性和患者安全，需自动化、高精度、实时的检测方法。

Method: 基于YOLOv8的实时目标检测框架，在高分辨率的102,876张图像数据集上训练，结合对比度增强的预处理，并与多种CNN架构进行比较评估。

Result: 在11种器械、5类缺陷上达到99.3%准确率，推理速度4.2-5.8ms/图，且对比度增强显著提升检测效果，满足ISO 13485和FDA相关要求。

Conclusion: SurgScan显著提升了外科器械缺陷检测的准确性和实时性，具备工业部署潜力。

Abstract: Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [38] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

TL;DR: 提出Prompt-aware Noise Projector，通过VLM反馈训练奖励模型并优化噪声投影器，将随机高斯噪声映射为提示感知噪声，提升Stable Diffusion的文本-图像对齐，推断仅需一次前向。


<details>
  <summary>Details</summary>
Motivation: 观察到训练时的提示条件噪声分布与推断时从高斯先验抽取噪声存在不匹配，导致部分初始噪声引导的去噪路径生成的图像与提示不匹配。为提高文本-图像对齐，需要将推断噪声调整为与训练中提示相关的分布。

Method: 设计了一个文本条件的噪声投影器：先采样多噪声，生成对应图像并用视觉-语言模型（VLM）获取逐token反馈，蒸馏成奖励模型，然后通过准直接偏好优化（quasi-direct preference optimization）训练投影器，使其将高斯噪声映射为更符合训练时分布的提示感知噪声，在不修改Stable Diffusion本体的情况下工作，推断时只需一次前向传递。

Result: 实验证明，该方法在不同提示下都能提升文本-图像对齐性，且不需参考图像或人工先验，推断开销小，替代了多样本后选择的策略，实现单次前向即可得到更契合提示的生成结果。

Conclusion: 该论文提出了在Stable Diffusion推断前对初始化噪声进行文本条件化修正（noise projector），以解决训练-推断分布不匹配导致的文本与图像对齐问题。

Abstract: In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [39] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

TL;DR: 提出PaddleOCR-VL-0.9B，一种融合NaViT式视觉编码器与ERNIE语言模型的轻量级VLM，实现多语言、复杂元素的高效文档解析，达成SOTA表现并适用于实际部署。


<details>
  <summary>Details</summary>
Motivation: 提升文档解析的准确性与效率，使模型在多语言与复杂元素（表格、公式、图表等）识别方面达到SOTA，同时保持低资源占用以利于工程部署。

Method: 构建PaddleOCR-VL-0.9B，将NaViT风格的动态分辨率视觉编码器与ERNIE-4.5-0.3B语言模型融合，形成紧凑的跨模态VLM，支持多类元素识别并优化推理速度与资源占用。

Result: 在公开与内部基准测试上，PaddleOCR-VL在页面级解析与元素级识别上均达到或优于SOTA，明显超越已有方案，与顶尖VLM竞争力强，且推理速度快、资源占用低。

Conclusion: PaddleOCR-VL提出了一种高效、性能领先的文档解析模型，兼顾多语言与复杂元素识别，同时资源消耗低，适合实际部署。

Abstract: In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [40] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

TL;DR: 提出DentVFM——基于ViT的2D/3D牙科视觉基础模型，在约160万多模态牙科影像上自监督预训练，并通过DentBench评估，显著提升泛化与标注效率，支持跨模态诊断，推动智能牙科医疗发展。


<details>
  <summary>Details</summary>
Motivation: 牙颌面放射学图像解读受训练专业人员短缺限制，现有牙科AI系统多为单模态、任务专用且依赖昂贵标注，难以在多样临床场景下泛化，亟需通用、可扩展且标注高效的牙科视觉模型。

Method: 基于Vision Transformer的2D和3D模型变体，使用自监督学习在约160万张多模态放射影像上进行预训练；构建DentBench涵盖八个牙科子专科、更多疾病与成像模态进行评估；在多项下游任务（疾病诊断、治疗分析、生物标志物识别、解剖标志点检测与分割）上进行微调与零/少样本测试，并与监督、自监督及弱监督基线比较；实现跨模态诊断能力。

Result: DentVFM在DentBench及多项下游任务上均显著优于各类基线，展示出更好的泛化性、标签效率和跨模态诊断性能，且在缺乏常规成像的情形下比经验丰富的牙医更可靠。

Conclusion: DentVFM是首个面向牙科的视觉基础模型系列，通过在大规模多模态牙科影像数据集DentVista上采用自监督学习，生成任务无关的视觉表征，显著提升了在多种牙科任务上的泛化能力、标签效率和可扩展性。

Abstract: Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [41] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出 PL-SE-ADA：通过双编码器、对抗训练与子重建相加实现可解释的域谐波，兼顾疾病信息保留与域信息分离，实验显示优于或等同现有方法并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学影像在不同采集站点存在显著域偏移，影响下游疾病分类性能；现有的域不变/可分离表示方法虽取得效果，但缺乏对所学特征的可解释性，限制临床可用性。

Method: 设计两个编码器分别提取域不变表征 z_u 和域特异表征 z_d，一个解码器将两部分重构并相加复原图像，域判别器与编码器之间采用对抗训练以去除 z_u 中的域信息，同时通过分别从 z_u 和 z_d 解码得到的子重建项相加以保证两者均保留信息；训练目标包含重建损失、对抗损失和分类损失等。

Result: 在脑 MR 图像上的实验证明，PL-SE-ADA 在图像重建、疾病分类和域识别任务上达到相当或更好的性能，并能可视化域不变与域特异成分，提供更直观的解释。

Conclusion: PL-SE-ADA 提供了一种兼顾域谐波与可解释性的表征学习框架，通过分解潜在表征为域不变和域特异两部分并重建图像，既保持疾病相关信息，又恢复域特异性成分。实验证明其在重建、分类和域识别任务上与或优于现有方法，并能可视化两类特征，提升医学应用的可解释性。

Abstract: Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [42] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

TL;DR: VisualSplit显式将图像分解为可解释的经典描述子并用重建预训学习，每个描述子可独立控制，提升了图像生成与编辑的属性可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代深度模型表示不透明，难以解释；而经典视觉描述子直观且可解释。探讨是否能把经典视觉线索融入现代学习，以获得既可解释又有效的图像表示。

Method: 设计VisualSplit框架，将输入图像分解为若干独立的经典描述子通道；采用重建驱动的预训练目标，使模型学习每个描述子的本质表示；训练后将各描述子作为可控组件用于图像生成、编辑等任务。

Result: 实验表明，VisualSplit在保持描述子可解释性的同时，能用于属性可控的图像生成与编辑，性能优于不进行显式分解的基线方法，展示了其在视觉理解与控制上的优势。

Conclusion: VisualSplit通过将图像显式分解为经典可解释的描述子（边缘、颜色、强度分布等），并采用重建驱动的预训练，能同时保持可解释性和表达力，从而在图像生成与编辑等任务上实现更好的属性控制，验证了将传统视觉线索融入现代学习有助于视觉理解。

Abstract: Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [43] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: FMA通过学习跨模态速度场实现多步特征校正，相比传统单步PEFT方法在复杂跨模态对齐任务上更精确、鲁棒并有效提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法均为单步调整，对于模态间高度纠缠的复杂数据集不足以实现精确对齐，故提出多步校正以逐步纠正异构特征差异。

Method: 提出Flow Matching Alignment (FMA)：使用固定耦合策略保持训练类别对应，采用噪声增强缓解数据稀缺，并设计早停求解器提前终止变换以提升效率与准确性。FMA作为模型不可知的方法，可在不同预训练视觉-语言模型上进行多步特征校正。

Result: 在多种基准和不同背骨模型上，FMA相比单步PEFT方法稳定带来显著性能提升，尤其在困难数据集上效果明显。

Conclusion: 本文提出了一种基于流匹配的跨模态多步校正方法FMA，通过学习跨模态速度场对图像与文本特征进行多步变换，从而比单步PEFT方法获得更精确鲁棒的对齐效果。

Abstract: Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [44] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

TL;DR: 提出一种无训练的提示嵌入编辑SDeC，通过SVD抑制ID嵌入中的场景相关方向，有效缓解ID漂移，且无需预先知道所有目标场景，实用且高效。


<details>
  <summary>Details</summary>
Motivation: 观察到T2I模型在拟合大规模自然图像分布时会自发学到‘场景-身份’相关性，导致跨场景保持身份一致性失败；且先前方法依赖预知所有目标场景这一不现实假设。

Method: 提出训练免费（training-free）的提示嵌入编辑方法，通过对ID嵌入做SVD分析、量化方向稳定性，并自适应重加权特征值以抑制与场景相关的成分，从而实现反向的场景去语境化。

Result: SDeC在不需要预先获取所有目标场景的情况下，允许按场景单独使用，并在实验中显著提升了身份保持性能，同时保持场景多样性。

Conclusion: SDeC通过在ID提示嵌入中识别并抑制场景-身份相关性，有效减少了T2I生成中的身份漂移问题。

Abstract: Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [45] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

TL;DR: 论文提出了主动回答随时间演进问题的egocentric任务、相应基准ESTP-Bench与评估指标ESTP-F1，并通过包含数据引擎、多阶段训练与动态压缩的流水线实现并验证了方法的有效性与优势。


<details>
  <summary>Details</summary>
Motivation: 实现能够在类人场景中主动理解、预测并及时响应事件的AI，以满足同步感知与推理的需求。

Method: 构建ESTP-Bench和ESTP-F1评估框架；设计包含数据引擎、多阶段训练策略和主动动态压缩的技术流水线；并与多种基线对比。

Result: 在多种在线和离线基准上优于多种基线，并展示了在时机把握、连贯性和效率方面的提升。

Conclusion: 该论文提出了一种面向egocentric视频的主动式辅助任务，并设计了评估基准和技术流水线，验证了方法有效性。

Abstract: Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [46] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

TL;DR: BalanceGS通过密度控制、相似性采样/合并与内存重排三重优化，使3D Gaussian Splatting训练更均衡高效，达成1.44×加速且质量几乎不变。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS训练流水线存在三大低效：高斯密度分配偏斜、投影阶段计算负载失衡和颜色splatting时内存访问碎片，从而浪费计算与带宽。BalanceGS旨在通过算法与系统协同优化解决这些瓶颈。

Method: 提出三方面改进：1) 基于启发式负载感知的高斯密度控制，自动去除高密度区域的冗余高斯并填补稀疏区域；2) 基于相似性的高斯采样与合并，将静态一对一线程-像素映射替换为自适应负载分配，使线程处理可变数量的高斯；3) 基于重排的内存访问映射，重构RGB存储并在共享内存中批量载入以减少碎片化访问。

Result: 在NVIDIA A100 GPU上，BalanceGS相比原始3DGS实现了约1.44×的训练加速，同时重建质量仅有可忽略的退化。

Conclusion: BalanceGS通过算法与系统协同设计，在不显著降低重建质量的前提下，有效提升了3D Gaussian Splatting训练效率。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [47] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

TL;DR: CALM-Net通过多分支融合edge conv、点注意力和曲率嵌入，从点云中学习更丰富的几何上下文特征，在nuScenes上将重识别准确率提升约1.97%并验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现有点云重识别方法在捕捉局部几何细节（如表面曲率）方面不足，难以区分外观相近或被遮挡的车辆；因此引入曲率信息和多分支设计以增强判别特征。

Method: 提出一种多分支神经网络：分别基于edge convolution、point attention以及曲率嵌入模块提取互补特征；随后将各分支特征融合用于车辆重识别。

Result: 在nuScenes大规模数据集上实验，CALM-Net相比最强基线平均重识别准确率提升约1.97个百分点，验证了曲率信息和多分支学习的有效性。

Conclusion: CALM-Net通过融合曲率嵌入、边卷积和点注意力的多分支结构，有效提升了基于LiDAR点云的车辆重识别性能，特别是曲率信息能补充局部几何变化特征，从而提高判别能力。

Abstract: This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [48] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

TL;DR: 提出了一个由Point Descriptor和Point Localizer组成的双向像素级关键点理解框架，创建了20K+合成数据集，并通过GRPO优化泛化性，实验显示优于基线，支持语言↔像素精确定位。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型仅限于目标/区域级别定位，缺乏通过自然语言实现像素级关键点理解与精确定位的能力，作者旨在弥补这一空白并支持双向（描述↔坐标）交互。

Method: 构建Point Descriptor生成自适应粗到细的关键点自然语言描述；Point Localizer根据描述回归精确像素坐标；构建LlamaPointInPart 数据集（20K+三元组）并用GRPO在AP-10K上优化描述以提高泛化和定位准确性；设计新的评估协议通过Localizer判断描述定位距离而非直接文本比对。

Result: 实验表明在LlamaPointInPart上优于基线方法；框架在跨类别泛化和定位精度方面表现良好。

Conclusion: 该论文提出了一个面向像素级关键点理解的双向框架，包含Point Descriptor和Point Localizer，能从自然语言生成精确像素坐标并反向用坐标生成描述。

Abstract: Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [49] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: 提出STANC E框架：用像素对齐的2.5D实例运动场与在token级保留空间位置信息的Dense RoPE，联合RGB与辅助图预测，从而提高图像到视频生成的运动一致性与时序连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成在保持对象运动和交互一致性方面困难，原因在于人类提供的稀疏运动提示在编码后信息塌缩为过少有效token，以及在同一head中同时优化外观与运动易偏向纹理而牺牲时间一致性。

Method: 提出两个组件：1) Instance Cues：对每个实例在掩码内平均光流并结合单目深度，生成像素级2.5D运动场；2) Dense RoPE：对少量运动token（固定于首帧）施加空间可寻址的旋转位置编码，并结合联合预测RGB与辅助图（分割或深度）。

Result: 通过Instance Cues与Dense RoPE，模型在不依赖逐帧轨迹脚本下改善时间一致性与结构锚定，稳定优化，降低深度歧义并提升运动指导效果。

Conclusion: STANCE通过将稀疏可编辑运动提示转为像素对齐的2.5D运动场（Instance Cues）并在token空间保留这些提示（Dense RoPE），有效提升了视频生成中的运动一致性与交互连贯性。

Abstract: Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [50] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 该工作通过结合SpeciesNet预测、CLIP嵌入和三元组损失度量学习，构建五阶段层级再分类系统，显著将高层级标签细化为物种，实现高准确率和较高物种识别覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有模型对数千物种做预测但采取保守的汇总策略，导致大量检测仅标注到高层分类（如动物、哺乳动物），需要一种方法将这些高层标签细化到物种以提升生态监测与保护的实用价值。

Method: 五阶段流水线：高置信度接受、鸟类覆盖策略、质心构建、基于三元组损失的度量学习、以及自适应余弦距离评分；使用SpeciesNet EfficientNetV2-M作为初始预测，并利用CLIP嵌入进行特征对齐与相似度计算。

Result: 在LILA BC Desert Lion Conservation数据集的一部分（4018张图像，15031个检测）上实验：从“空白”和“动物”标签中恢复了761个鸟类检测，重新分类456个被标注为animal、mammal或blank的检测，准确率为96.5%，实现64.9%的物种级识别率。

Conclusion: 该论文提出了一种层级再分类系统，在Animal Detect平台上将SpeciesNet的高层级分类结果细化到物种级别，通过结合CLIP嵌入和度量学习提高物种识别率。

Abstract: State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [51] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

TL;DR: 使用DINOv2特征+UMAP降维+GMM聚类与1D相似度排序，可高效组织无标签相机陷阱图像，实验表明在多类别上表现良好，并已投入生产以加速生物多样性监测的人工标注。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱产生大量未标注图像，且存在训练集以外的物种，需一种无需额外标注即可组织和加速人工审核的自动化方法。

Method: 比较了多种自监督特征提取器（CLIP、DINOv2、MegaDescriptor）、降维（PCA、UMAP）与无监督聚类（DBSCAN、GMM），并通过t-SNE实现一维相似性排序，在包含5个物种的测试集上评估性能。

Result: 在5类测试集上，DINOv2+UMAP+GMM达成88.6%准确率（macro-F1=0.874）；1D排序在1500张图像上对哺乳类与鸟类一致性为88.2%，对鱼类为95.2%，并已在生产环境中部署，改善了探索分析与标注效率。

Conclusion: 本论文展示了在无标签野生动物相机陷阱图像上，基于自监督视觉变换器的零样本组织方法具有实用性，并成功将连续相似度排序部署到生产以加速标注流程。

Abstract: Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [52] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

TL;DR: 提出Wiki-PRF：动态视觉工具提取多模态查询+多模态检索融合+检索结果过滤，并用强化学习优化VLM，显著提升KB-VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG在KB-VQA中多模态查询质量不足、检索相关性差，导致检索结果无法充分支持答案生成，需改善查询构造与检索结果过滤。

Method: 提出三阶段流程：1) Processing阶段动态调用视觉工具提取多模态检索信息；2) Retrieval阶段将视觉与文本特征融合以进行多模态检索；3) Filtering阶段对检索结果进行相关性过滤与聚合。同时用强化学习训练视觉语言模型，以答案准确率与格式一致性作为奖励信号，增强推理、工具调用与过滤能力。

Result: 在E-VQA与InfoSeek基准上分别获得显著提升（36.0与42.8分提升或得分，文中表述需核实），并宣称达到SOTA。代码开源于给定仓库。

Conclusion: 该论文提出的Wiki-PRF方法通过三阶段（Processing、Retrieval、Filtering）改进KB-VQA任务的检索质量与答案准确性，实验在E-VQA与InfoSeek上取得了显著提升，宣称达到SOTA。

Abstract: Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [53] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

TL;DR: 提出Shot2Tactic-Caption：用于羽毛球的视频多尺度（击球/战术）字幕生成框架与数据集，采用双分支Transformer结构与击球级提示机制，能建模战术的中断与恢复，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 传统视频描述多关注动作或整体事件，缺乏对竞技运动中战术层面随时间展开的建模。羽毛球战术具有动作序列与中断/恢复等复杂时序特征，需专门方法进行语义与时序多尺度描述。

Method: 双分支设计（击球分支与战术分支），每分支含视觉编码器、时空Transformer编码器与Transformer解码器；引入Tactic Unit Detector以识别战术单元、类型与状态；对战术解码器采用基于击球的提示（将预测的战术类型与状态嵌入为提示并通过交叉注意力注入）。

Result: 构建Shot2Tactic-Caption数据集（5,494条击球字幕、544条战术字幕），实验证明该框架能生成准确的击球与战术字幕；消融实验显示ResNet50为更优的时空编码器，且基于击球的提示结构提升了战术字幕的一致性与准确性。

Conclusion: 该论文提出了一种用于羽毛球的多尺度语义与时序视频描述框架，能够生成击球级与战术级字幕，并构建了首个相关数据集。

Abstract: Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [54] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

TL;DR: EVS通过在推理阶段裁剪连续帧中不变的空间补丁，显著降低视频令牌冗余，最高可将TTFT加速4x；上训练使模型对各种压缩率鲁棒，从而实现高效、可扩展的视频-语言理解。


<details>
  <summary>Details</summary>
Motivation: 现有VLM处理长视频时面临令牌数二次增长导致的上下文限制和延迟问题，长视频常超出LLM的令牌预算，需一种高效的帧内冗余压缩方法。

Method: 提出了一种推理时可插拔的方法EVS，基于检测并剪除时间上静态的图像补丁以减少冗余令牌；同时引入一种上训练(uptraining)阶段，使用随机剪枝率让模型在不同压缩率下保持鲁棒性。

Result: 在不显著损失准确度的情况下，EVS在推理时可将LLM的首令牌时间缩短最多4倍；结合上训练后，模型在激进剪枝下仍能保持全性能，整体提升效率-精度折中。

Conclusion: EVS能在不改变模型结构或重新训练的情况下，通过剔除在连续帧中保持不变的空间补丁来显著减少视频令牌数，从而提升视频-语言模型的推理效率并延长可处理的序列长度。

Abstract: Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [55] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

TL;DR: RepTok用一个微调的SSL语义token加flow matching解码器，保持SSL几何正则化，实现紧凑高效的图像生成与重建，表现与效率兼备。


<details>
  <summary>Details</summary>
Motivation: 利用预训练SSL表示作为紧凑有效的潜在空间，避免二维潜空间的空间冗余并降低训练成本，同时保持生成质量。

Method: 基于预训练自监督视觉Transformer（SSL encoder），只微调语义token embedding，并训练一个生成解码器（flow matching objective）。加入余弦相似度损失以保持SSL空间几何性质。

Result: 在ImageNet类别条件生成和极低训练预算下的MS-COCO零样本文生中表现具有竞争力，且训练更高效。

Conclusion: RepTok提出用单一连续语义token作为图像潜变量，通过微调预训练SSL编码器的token embedding并结合基于flow matching的生成解码器，实现高效重建与生成。

Abstract: We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [56] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

TL;DR: An inference-only diffusion input adaptation method (SteeringTTA) uses Feynman-Kac steering with pseudo-label rewards and particle trajectories to better handle corruptions, outperforming baselines on ImageNet-C.


<details>
  <summary>Details</summary>
Motivation: Existing input-only diffusion TTA methods rely on gradient guidance, which limits exploration and generalization across distortion types; SteeringTTA aims to overcome these limits with a steering approach derived from Feynman-Kac formulation.

Method: Maintain multiple particle trajectories through diffusion, apply steering using cumulative top-K class probabilities and an entropy schedule as rewards, enabling exploration and confidence balance during inference-only input adaptation.

Result: Consistent improvement over baseline on ImageNet-C under corruptions using no model updates or source data.

Conclusion: SteeringTTA is an effective inference-only TTA method that uses Feynman-Kac steering in diffusion-based input adaptation, guided by pseudo-label rewards, and it outperforms baselines on ImageNet-C without model updates or source data.

Abstract: Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [57] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

TL;DR: 通过在约1M无配对视频上预训练并在<150k配对样本上微调，本文以低成本实现了强指令驱动的视频编辑，指令对齐和编辑质量分别提升约12%和15%。


<details>
  <summary>Details</summary>
Motivation: 构建大规模配对视频编辑数据成本高、复杂，阻碍了指令驱动视频编辑的发展；因此希望用低成本的无配对视频预训练来获取通用编辑能力，再用少量配对数据微调。

Method: 基于HunyuanVideoT2V，先对约100万真实视频片段进行预训练以学习基本编辑概念（如添加、替换、删除），随后用少于15万经过策划的配对编辑样本进行微调以扩展任务和提高质量；预训练利用无配对视频的上下文学习来构建指令条件编辑能力。

Result: 在比较实验中，相较于现有指令驱动视频编辑方法，本方法在指令对齐率提升约12%，编辑质量提升约15%，同时在视觉保真度和指令遵从性上表现更好。

Conclusion: 本文提出一种低成本的预训练策略，通过对大量无配对视频片段进行上下文学习，使基础视频生成模型获得通用的指令驱动编辑能力，随后用少量高质量配对编辑数据微调以提升任务覆盖与质量。

Abstract: Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [58] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

TL;DR: 在MAE预训练中对编码器使用DBP可加速收敛、降低能耗并提升下游分割性能，适用于工业场景。


<details>
  <summary>Details</summary>
Motivation: MAE预训练在低标注样本情形下效果良好但计算开销大，不适合工业场景。目标是减少预训练时间和能耗，同时不损失甚至提升下游任务性能，使大规模ViT预训练在资源受限环境中更实用。

Method: 在ViT的MAE预训练过程中，对编码器层应用DBP——一种通过迭代降低每层输入相关性的优化方法，以加快收敛。只在编码器上使用DBP以避免不稳定性，同时保持解码器和整体MAE框架不变。

Result: 在ImageNet-1K上预训练并在ADE20K上微调，DBP-MAE将达到基线性能所需的壁钟时间减少21.1%，碳排放减少21.4%，并在分割任务上mIoU提升1.1个百分点。对专有工业数据的预训练和微调也获得了类似收益。

Conclusion: DBP-MAE在保持稳定性的前提下，通过在编码器中选择性集成Decorrelated Backpropagation来加速MAE预训练，从而在减少计算时间和碳排放的同时提升下游任务性能。

Abstract: Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [59] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

TL;DR: EuroMineNet是首个覆盖欧盟的2015-2024年多时序Sentinel-2矿业监测基准，含133个矿区的年度专家标注并提出CA-TIoU评估，20种模型基线显示长期变化检测可行但短期动态检测仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在时间深度或地理覆盖上有限，难以支持长期监测矿业引起的土地表面变化；需要一个覆盖整个欧盟、具有长时间序列和专家标注的基准数据集以推动可持续监测与治理。

Method: 构建包含133个矿区的多时序数据集，使用Sentinel-2多光谱影像提取年度影像并由专家进行验证标注；提出Change-Aware Temporal IoU(CA-TIoU)用于评估时序一致性；设计两项任务（多时序足迹制图与跨时序变化检测），并在20个深度学习模型上进行基线测试。

Result: 发布了EuroMineNet数据集与基线代码，基准测试表明当前GeoAI模型能较好识别长期变化但对短期动态（及时缓解所需）检测仍存在困难；提出的CA-TIoU和两项任务为后续研究提供评估标准。

Conclusion: EuroMineNet填补了欧洲范围内多时序矿业足迹监测的数据空白，提供2015-2024年基于Sentinel-2的年度观测与专家标注，支持时序一致性的土地利用绘制与跨时间变化检测，推动GeoAI在可持续资源管理中的应用。

Abstract: Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [60] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Sami Azam,Asif Karim,Jemima Beissbarth,Amanda Leach*

Main category: cs.CV

TL;DR: WeCKD提出链式弱监督蒸馏：分段训练并逐步精化知识，显著提升有限标注下的医学影像任务性能，最高累计增益+23%。


<details>
  <summary>Details</summary>
Motivation: 传统教师-学生单步KD在小样本和弱监督场景下易出现知识退化与监督效率低下，无法充分利用有限标注资源，亟需一种结构化、渐进的知识传递机制。

Method: 提出了链式级联蒸馏结构：多个模型按序排列，每一阶段仅用部分数据训练，既从前一模型学习又对传递的知识进行细化和增强；训练策略包含部分数据分配、逐步蒸馏损失和弱监督标签利用。

Result: 在四个耳镜图像数据集上，WeCKD在多数情况下优于现有监督方法；在两种其它医学影像模态（显微和MRI）上也展示了良好泛化性；与单一骨干在同等有限数据上相比，累计精度提升可达+23%。

Conclusion: WeCKD通过链式、弱监督的知识蒸馏框架，有效缓解了单步KD的知识退化与对大标注集/强教师的依赖，从而在有限数据条件下显著提升模型性能。

Abstract: Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.

</details>


### [61] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

TL;DR: VTimeCoT通过引入进度条集成与高效高亮两类视觉工具，并结合visuotemporal链式思维，实现了无需训练即可显著提升MLLM在视频时序定位与推理任务的性能，同时提高了推理可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在视频时序定位与时序推理方面表现欠佳，限制了实际视频理解系统的应用；受人类使用视频播放器进度条进行视频理解的启发，提出利用进度条作为交互式视觉工具改善模型对时间信息的建模与推理。

Method: 引入两个可视化工具：1) 插件式进度条集成工具，将进度条信息作为模型可交互输入；2) 高效高亮工具，用于快速定位关键时间段；并提出visuotemporal CoT——跨模态的时序链式思维，将视频与文本的推理过程结合，从而引导模型在时间维度上进行逐步推理。该框架为训练免疫（training-free），即无需或少量额外训练即可插入现有MLLM。

Result: 在视频时序定位与基于推理的问题回答任务上，相较于Qwen2VL-7B和GPT4o基线，VTimeCoT取得了显著性能提升；此外展示了可组合的推理步骤与可解释的中间高亮/进度条输出。

Conclusion: 该文提出了VTimeCoT框架，基于可视化进度条工具与visuotemporal CoT，实现无训练或少训练的高效视频时序定位与推理，显著提升了在Qwen2VL-7B和GPT4o基线上的性能，并具备可组合与可解释性。

Abstract: In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [62] [Leveraging Learned Image Prior for 3D Gaussian Compression](https://arxiv.org/abs/2510.14705)
*Seungjoo Shin,Jaesik Park,Sunghyun Cho*

Main category: cs.CV

TL;DR: 利用图像恢复网络和粗渲染残差信息，对已压缩的3D高斯进行图像空间修复与微调，从而在更小存储开销下实现更高质量渲染，兼容多种压缩基线并显著改善率失真表现。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩虽大幅减小存储但缺乏学习先验，导致率失真权衡受限。引入学习型图像先验可以恢复压缩带来的质量下降，并进一步压缩表示。

Method: 先对高斯球体进行现有压缩方法得到压缩高斯；训练一个恢复网络在图像空间学习从压缩渲染到原始渲染的残差，输入包含粗渲染残差作为侧信息；在恢复图像的监督下微调压缩高斯以得到更紧凑且质量更高的表示。兼容多种高斯压缩基线。

Result: 在多组实验中，该框架在相同或更低存储下优于现有最先进的3DGS压缩方法，取得更好的率失真曲线和渲染质量。

Conclusion: 该论文提出将学习的图像先验用于3D Gaussian Splatting (3DGS) 压缩重建，通过在图像空间建模压缩伪影并利用粗渲染残差作为侧信息来提升率失真性能，最终实现更紧凑的表示与更高的渲染质量。

Abstract: Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

</details>


### [63] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: 提出一种基于统计异常检测的半自动流水线来在VHR卫星影像中初筛鲸鱼候选点，效果高召回且极大降低专家检查负担，无需训练数据并已开源。


<details>
  <summary>Details</summary>
Motivation: 传统鲸类监测方法成本高且难以扩展。尽管有研究在超高分辨率卫星影像中识别鲸鱼，但由于带标注影像稀缺、影像质量与环境条件多变，以及在大规模遥感档案上构建鲁棒机器学习管道成本高，故需要一种可扩展且不依赖大量标注数据的初筛方法。

Method: 使用基于统计的异常检测器来标记空间上的异常点（“interesting points”），然后通过网页端标注工具由专家对这些点进行快速查看和标注；评估在三个含已知鲸鱼标注的基准场景上，报告召回率为90.3%–96.4%，并将需要专家检查的面积缩减多达99.8%。

Result: 在三个基准场景上实现高召回（90.3%–96.4%），并大幅减少需要人工检查的面积（最高减少99.8%），方法已开源。

Conclusion: 论文提出了一种无需训练数据的半自动方法，通过统计异常检测找出卫星影像中的空间异常点（可能是鲸鱼），并结合网页标注界面让专家高效标注，从而在大范围影像中显著减少人工检查面积。

Abstract: Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [64] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 论文评估现有深度CMC模型在二战档案影片上的表现，Video Swin Transformer在HISTORIAN上达80.25%准确率，显示潜力与挑战，强调需要多模态与时序方法改进。


<details>
  <summary>Details</summary>
Motivation: 摄像机运动携带重要的空间与叙事信息，但当前CMC模型主要在现代数据集上验证，其在历史档案影片（噪声、低分辨率、剪辑风格不同）上的表现未知，需系统评估以指导应用与模型改进。

Method: 整理并比较了代表性的方法与数据集，对模型设计和标签定义差异进行分析；在HISTORIAN（含二战专家注释镜头）上评估了五种标准视频分类模型（包括Video Swin Transformer等），使用标准训练和评估流程进行对比实验。

Result: 在HISTORIAN数据集上，Video Swin Transformer表现最佳，准确率为80.25%，表明尽管训练数据有限，但模型能较好收敛；同时指出现有模型在低质量视频上的适应性问题，建议结合多模态输入与改进的时序架构以提升泛化。

Conclusion: 该论文首次系统评估了深度视频摄像机运动分类(CMC)模型在历史档案影片上的泛化能力，发现现有模型在低质量、有限标注的历史影像上仍能取得较好表现但面临若干挑战。

Abstract: Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [65] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

TL;DR: 提出CFSAM，一种同时建模多尺度特征局部与全局跨层依赖的自注意力模块，显著提升SSD300在PASCAL VOC和COCO上的检测性能，并加速训练收敛，计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有方法多仅在单层或双层特征上进行精细化或融合，忽视了多尺度表示间的丰富跨层依赖，从而限制了对大尺度变化目标的上下文捕获能力。

Method: CFSAM包括三部分：卷积局部特征提取器（捕捉局部细节）、基于Transformer的全局建模单元（高效建模跨层交互），以及特征融合机制（恢复并增强原始表示）。该模块集成到SSD300并用于端到端训练。

Result: 在SSD300上加入CFSAM后，PASCAL VOC mAP从75.5%提升到78.6%；COCO mAP从43.1%提升到52.1%。此外，模块加快了训练收敛且未带来显著计算开销。

Conclusion: 该论文引入了跨层自注意力模块（CFSAM），通过在多尺度特征图上同时建模局部与全局依赖，提升了目标检测性能，尤其在处理尺度变化大的目标时更有效。

Abstract: Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [66] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

TL;DR: 构建ImageNet-F以模拟现实中混合粒度标注，提出free-grain learning并结合CLIP伪属性与半监督学习，改善层级分类表现。


<details>
  <summary>Details</summary>
Motivation: 现实世界的标注粒度不一致（由图像质量、标注者经验等因素影响），现有层级分类方法假设细粒度标注完备不切实际，需一个处理混合粒度监督的范式与基准。

Method: 构建了ImageNet-F基准并模拟混合粒度的标注；提出free-grain learning范式；利用CLIP估计语义歧义；引入来自视觉-语言模型的伪属性用于语义引导，并结合半监督学习增强视觉引导；同时提供强基线作为对比。

Result: 在ImageNet-F上，所提方法（伪属性+半监督）在混合监督下显著提升了层级分类性能，相对于强基线有明显提升，证明了方法在真实受限监督情形下的有效性。

Conclusion: 本论文提出并验证了在具有混合标注粒度的真实场景下进行层级图像分类的有效方法。

Abstract: Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [67] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

TL;DR: DEXTER是一个无数据的解释框架，结合扩散生成与大语言模型，通过优化提示生成能激活分类器的合成图像，再用语言模型撰写类级解释，能在多数据集与任务上有效发现模型决策模式与偏差。


<details>
  <summary>Details</summary>
Motivation: 传统解释方法依赖训练数据或标签，限制了在数据不可用或敏感场景下的可解释性需求；因此提出一种数据独立的方法来理解模型内在机制。

Method: 通过优化文本提示（prompt）以生成类条件的合成图像，使目标分类器达到高激活；随后用大语言模型对这些合成样本生成详细的自然语言报告，描述每个类的决策特征与偏差。

Result: 在激活最大化、切片发现与去偏、偏差解释三项任务上均展示出良好性能；在ImageNet、Waterbirds、CelebA与FairFaces数据集的实验以及用户研究中，DEXTER在全局模型解释和类级偏差报告方面优于现有方法。

Conclusion: DEXTER能够在无训练数据的条件下，利用扩散模型与大语言模型生成对视觉分类器的全局文本解释，揭示分类器的决策模式与偏差。

Abstract: Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>


### [68] [LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement](https://arxiv.org/abs/2510.14753)
*Xu Wu,Zhihui Lai,Xianxu Hou,Jie Zhou,Ya-nan Zhang,Linlin Shen*

Main category: cs.CV

TL;DR: LightQANet通过光量化模块与光感提示模块，分别实现静态光因子建模与动态光照适配，从而提升低光图像的纹理与色彩恢复，达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLIE方法在弱光下像素信息严重退化，导致特征表示不可靠，进而造成纹理恢复差、颜色不一致和伪影。需引入能够分离照明因素并动态适配光照变化的机制以提高鲁棒性和一致性。

Method: 提出Light Quantization Module (LQM)以显式提取并量化照明相关因素，强制结构化光因子学习，获得光照不变表征；提出Light-Aware Prompt Module (LAPM)，将照明先验编码为可学习提示，引导特征学习以动态适应连续变化的光照。两者结合构成LightQANet，用以增强特征提取与恢复能力。

Result: 在多个低照度数据集的广泛实验中，LightQANet在主观视觉效果和客观指标（如PSNR/SSIM或其他论文采用的指标）上均优于现有方法，在复杂光照场景下表现出更好的色彩保真与细节恢复，且适应性强。

Conclusion: LightQANet通过光量化与光感提示模块，有效分离并建模照明因素，从而提升低照度图像的光照不变表征和动态适应能力，显著改善纹理恢复与色彩一致性，降低伪影，实验表明在多个数据集上达到或超过SOTA性能。

Abstract: Low-light image enhancement (LLIE) aims to improve illumination while
preserving high-quality color and texture. However, existing methods often fail
to extract reliable feature representations due to severely degraded
pixel-level information under low-light conditions, resulting in poor texture
restoration, color inconsistency, and artifact. To address these challenges, we
propose LightQANet, a novel framework that introduces quantized and adaptive
feature learning for low-light enhancement, aiming to achieve consistent and
robust image quality across diverse lighting conditions. From the static
modeling perspective, we design a Light Quantization Module (LQM) to explicitly
extract and quantify illumination-related factors from image features. By
enforcing structured light factor learning, LQM enhances the extraction of
light-invariant representations and mitigates feature inconsistency across
varying illumination levels. From the dynamic adaptation perspective, we
introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors
into learnable prompts to dynamically guide the feature learning process. LAPM
enables the model to flexibly adapt to complex and continuously changing
lighting conditions, further improving image enhancement. Extensive experiments
on multiple low-light datasets demonstrate that our method achieves
state-of-the-art performance, delivering superior qualitative and quantitative
results across various challenging lighting scenarios.

</details>


### [69] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: 提出用无条件扩散模型填补火星高度图空洞，通过非均匀多尺度重缩放在128x128分辨率训练，在1000个样本上优于IDW、克里金和Navier-Stokes，RMSE降低4-15%，LPIPS提升29-81%。


<details>
  <summary>Details</summary>
Motivation: 火星高度图常有缺失值，地形重建对任务如虚拟现实、任务规划和训练至关重要，而现有条件方法依赖地球上更完整的数据集，无法直接用于火星；传统插值方法在几何连续性上表现不足。

Method: 在12000张HiRISE来源的火星高度图上训练无条件扩散模型；训练前采用非均匀重缩放以在多尺度上保留地形特征，再统一调整为128x128分辨率输入；与IDW、克里金和Navier-Stokes等方法在1000张样本上比较。

Result: 在1000个评估样本上，该方法在RMSE上比基线低4-15%，在LPIPS感知相似度上提高29-81%，说明在数值精度与视觉一致性上均有显著提升。

Conclusion: 该论文提出了一种基于无条件扩散模型的火星地形重建方法，能比传统插值与修复算法更准确地填补高度图空洞。

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [70] [MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks](https://arxiv.org/abs/2510.14770)
*Zhang Nengbo,Hann Woei Ho,Ye Zhou*

Main category: cs.CV

TL;DR: 受蜜蜂舞启发，论文提出用MAV运动作为信号，事件相机捕获并用分割+轻量SNN解码，达到低功耗且鲁棒的群体通信。


<details>
  <summary>Details</summary>
Motivation: 在频谱拥塞、干扰、以及无线电高能耗等受限环境下，传统无线通信不足以保障MAV群间可靠通信，受蜜蜂摆尾舞启发尝试用运动视觉通信以降低功耗并提高对干扰的鲁棒性。

Method: 设计了四种运动基元构成的视觉代码本，通过事件相机生成事件帧，采用基于事件帧的分割模型定位运动段，再用轻量级脉冲神经网络(SNN)进行动作识别，最后通过集成解码算法结合分割与分类结果解码序列信息。

Result: 实验表明该方法能准确解码位置类信息（航向、距离等），在低功耗条件下保持高识别率，验证了事件相机+SNN在MAV视觉通信场景的可行性与能效优势。

Conclusion: 该论文提出了一种受蜜蜂舞启发的基于视觉的MAV群体通信框架，通过运动编码与事件相机结合，实现低功耗、鲁棒的信息传输。

Abstract: Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in
environments, where conventional radio-based methods suffer from spectrum
congestion, jamming, and high power consumption. Inspired by the waggle dance
of honeybees, which efficiently communicate the location of food sources
without sound or contact, we propose a novel visual communication framework for
MAV swarms using motion-based signaling. In this framework, MAVs convey
information, such as heading and distance, through deliberate flight patterns,
which are passively captured by event cameras and interpreted using a
predefined visual codebook of four motion primitives: vertical (up/down),
horizontal (left/right), left-to-up-to-right, and left-to-down-to-right,
representing control symbols (``start'', ``end'', ``1'', ``0''). To decode
these signals, we design an event frame-based segmentation model and a
lightweight Spiking Neural Network (SNN) for action recognition. An integrated
decoding algorithm then combines segmentation and classification to robustly
interpret MAV motion sequences. Experimental results validate the framework's
effectiveness, which demonstrates accurate decoding and low power consumption,
and highlights its potential as an energy-efficient alternative for MAV
communication in constrained environments.

</details>


### [71] [CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection](https://arxiv.org/abs/2510.14792)
*Hojun Choi,Youngsun Lim,Jaeyo Shin,Hyunjung Shim*

Main category: cs.CV

TL;DR: 通过视觉链式思维分解目标理解并结合对比背景学习，CoT-PL能生成更鲁棒的伪标签，在拥挤/遮挡场景显著提升开放词汇目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于图文匹配的伪标签方法忽略了中间推理步骤，面对拥挤或遮挡的复杂场景鲁棒性不足，因而需要可解释的多步骤推理来改进伪标签质量。

Method: 提出结构化视觉链式思维（分为区域感知、零-shot类别识别、背景定位）与对比背景学习（CBL）相结合的管线，用预先计算的背景线索作为负样本，促进特征解纠缠。

Result: 在拥挤和遮挡情形下，新类伪标签质量分别较最好先前工作提升103.4%和168.4%；在open-vocabulary COCO上novel +7.7 AP50，在LVIS上novel +2.9 mask AP，达成新的SOTA。

Conclusion: CoT-PL通过将链式思维推理引入伪标签生成流程，提高了在拥挤或遮挡场景下的开放词汇目标检测鲁棒性，并通过对比背景学习增强物体与背景特征分离，显著提升新类伪标签质量与检测性能。

Abstract: Open-vocabulary object detection (OVD) seeks to recognize and localize object
categories beyond those seen during training. Recent approaches typically
leverage vision-language models (VLMs) to generate pseudo-labels using
image-text alignment, allowing detectors to generalize to unseen classes
without explicit supervision. However, these methods depend heavily on direct
image-text matching, neglecting the intermediate reasoning steps essential for
interpreting semantically complex scenes. This results in limited robustness
when confronted with crowded or occluded visual contexts. In this paper, we
introduce CoT-PL, a new framework that employs structured visual
chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL
decomposes object understanding into three interpretable steps: (1) region
perception even for unseen objects, (2) category recognition via zero-shot
reasoning, and (3) background grounding to separate semantically complex
objects. Crucially, the third step naturally motivates our contrastive
background learning (CBL) that uses the pre-computed background cues as
negatives to promote feature disentanglement between objects and background. In
this way, CoT reasoning and CBL form an integrated pipeline tailored to robust
pseudo-labeling in crowded or occluded scenes. Notably, in these two settings,
our novel-class pseudo-label quality achieves relative improvements of 103.4%
and 168.4% over the best prior, respectively. Our extensive experiments
demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9
mask AP on LVIS for novel classes, setting a new state of the art.

</details>


### [72] [Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images](https://arxiv.org/abs/2510.14800)
*Usama Sajjad,Abdul Rehman Akbar,Ziyu Su,Deborah Knight,Wendy L. Frankel,Metin N. Gurcan,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出PRISM：一种通过捕捉形态连续谱的可解释AI模型，在III期CRC五年生存预测上显著优于现有方法且具备跨亚组稳健性。


<details>
  <summary>Details</summary>
Motivation: 当前基于任务不可知的基础模型可能忽视器官特异的重要形态学模式，这些模式反映不同的生物学过程并影响肿瘤行为、治疗反应和患者结局。因此需要一个可解释且考虑形态连续性与表型多样性的CRC专用模型。

Method: 构建PRISM模型以整合空间形态学的连续变异谱；在424例III期CRC患者的手术切除标本中提取8.74百万张组织学图像进行训练；评估模型在五年OS预测的AUC、准确率、风险比等指标，并与现有CRC特异模型及基础模型比较，同时做亚组分析（性别、不同化疗方案）。

Result: PRISM在五年总体生存预测上达到AUC=0.70±0.04，准确率68.37%±4.75%，HR=3.34（95% CI 2.28-4.90，p<0.0001）；相比现有CRC特异方法提高约15%准确率，相比AI基础模型提高约23%准确率；在性别及临床病理亚组间表现稳定，性别相关AUC与准确率差异极小，化疗方案间准确率波动仅1.44%，复现了Alliance队列关于不同方案无生存差异的发现。

Conclusion: PRISM提出了一种可解释的、考虑形态连续变异谱的AI模型，用于预测III期结直肠癌患者的预后。该模型在五年总体生存预测上表现优于现有CRC特异方法和通用基础模型，并在性别与不同临床病理亚组间具有稳健性。

Abstract: Colorectal cancer (CRC) remains the third most prevalent malignancy globally,
with approximately 154,000 new cases and 54,000 projected deaths anticipated
for 2025. The recent advancement of foundation models in computational
pathology has been largely propelled by task agnostic methodologies that can
overlook organ-specific crucial morphological patterns that represent distinct
biological processes that can fundamentally influence tumor behavior,
therapeutic response, and patient outcomes. The aim of this study is to develop
a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated
Spatial Morphology), that incorporates a continuous variability spectrum within
each distinct morphology to characterize phenotypic diversity and reflecting
the principle that malignant transformation occurs through incremental
evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained
on 8.74 million histological images extracted from surgical resection specimens
of 424 patients with stage III CRC. PRISM achieved superior prognostic
performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%;
HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific
methods by 15% and AI foundation models by ~23% accuracy. It showed
sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable
performance across clinicopathological subgroups, with minimal accuracy
fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens,
replicating the Alliance cohort finding of no survival difference between
treatments.

</details>


### [73] [Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks](https://arxiv.org/abs/2510.14803)
*Pedro R. A. S. Bassi,Xinze Zhou,Wenxuan Li,Szymon Płotka,Jieneng Chen,Qi Chen,Zheren Zhu,Jakub Prządo,Ibrahim E. Hamacı,Sezgin Er,Yuhan Wang,Ashwin Kumar,Bjoern Menze,Jarosław B. Ćwikła,Yuyin Zhou,Akshay S. Chaudhari,Curtis P. Langlotz,Sergio Decherchi,Andrea Cavalli,Kang Wang,Yang Yang,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 利用海量临床报告而非依赖大量人工掩膜训练分割模型，成本低、可扩展，能在多肿瘤检测上达到或超越传统掩膜监督的性能。


<details>
  <summary>Details</summary>
Motivation: 手工绘制肿瘤体素级掩膜成本极高且难以大规模获取，而医疗报告普遍存在且包含丰富肿瘤信息，若能利用报告监督训练分割模型可大幅降低成本并扩展肿瘤类型覆盖。

Method: 从大规模配有医学报告的CT库中提取描述性信息，设计损失/监督策略使模型学习将报告中的肿瘤描述映射到图像上的分割预测；同时可以结合少量人工掩膜进行混合训练以提升性能。

Result: 在101,654份报告上训练的模型性能可比拟使用723个掩膜训练的模型；将报告与掩膜结合训练可使敏感性提升+13%、特异性提升+8%，并在7种肿瘤中击败放射科医师中的5种类型；实现了对若干此前无公开掩膜的数据集（如脾、胆囊、前列腺、膀胱、子宫、食管）的分割。

Conclusion: R-Super 表明利用医疗报告训练分割模型可显著减少对人工肿瘤掩膜的依赖，且在多种肿瘤检测任务中表现可与少量人工掩膜训练的模型相当或更优。

Abstract: Early tumor detection save lives. Each year, more than 300 million computed
tomography (CT) scans are performed worldwide, offering a vast opportunity for
effective cancer screening. However, detecting small or early-stage tumors on
these CT scans remains challenging, even for experts. Artificial intelligence
(AI) models can assist by highlighting suspicious regions, but training such
models typically requires extensive tumor masks--detailed, voxel-wise outlines
of tumors manually drawn by radiologists. Drawing these masks is costly,
requiring years of effort and millions of dollars. In contrast, nearly every CT
scan in clinical practice is already accompanied by medical reports describing
the tumor's size, number, appearance, and sometimes, pathology
results--information that is rich, abundant, and often underutilized for AI
training. We introduce R-Super, which trains AI to segment tumors that match
their descriptions in medical reports. This approach scales AI training with
large collections of readily available medical reports, substantially reducing
the need for manually drawn tumor masks. When trained on 101,654 reports, AI
models achieved performance comparable to those trained on 723 masks. Combining
reports and masks further improved sensitivity by +13% and specificity by +8%,
surpassing radiologists in detecting five of the seven tumor types. Notably,
R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate,
bladder, uterus, and esophagus, for which no public masks or AI models
previously existed. This study challenges the long-held belief that
large-scale, labor-intensive tumor mask creation is indispensable, establishing
a scalable and accessible path toward early detection across diverse tumor
types.
  We plan to release our trained models, code, and dataset at
https://github.com/MrGiovanni/R-Super

</details>


### [74] [Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning](https://arxiv.org/abs/2510.14819)
*Ji Cao,Yu Wang,Tongya Zheng,Zujie Ren,Canghong Jin,Gang Chen,Mingli Song*

Main category: cs.CV

TL;DR: PRTraj结合环境语义（POI）与显式路线选择建模，生成环境感知、路线选择敏感的轨迹表示，在多任务和少样本条件下均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有TRL方法忽视外部环境和内部路线选择行为，把轨迹视为孤立的时空序列，导致表示缺乏语义和行为信息。

Method: 提出两阶段框架：Environment Perception Module利用周边POI分布增强道路网络表征；Route Choice Encoder将轨迹的路段转移建模为决策序列，生成路线选择感知的路段表示并聚合为轨迹向量。

Result: 在3个真实数据集、5个下游任务上，PRTraj展现出优于基线的方法性能与更好的数据效率，少样本场景下仍保持鲁棒性。

Conclusion: PRTraj通过融合环境感知与显式路径选择建模，显著提升了轨迹表示的质量与下游任务表现。

Abstract: Trajectory Representation Learning (TRL) aims to encode raw trajectories into
low-dimensional vectors, which can then be leveraged in various downstream
tasks, including travel time estimation, location prediction, and trajectory
similarity analysis. However, existing TRL methods suffer from a key oversight:
treating trajectories as isolated spatio-temporal sequences, without
considering the external environment and internal route choice behavior that
govern their formation. To bridge this gap, we propose a novel framework that
unifies comprehensive environment \textbf{P}erception and explicit
\textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation
learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an
Environment Perception Module to enhance the road network by capturing
multi-granularity environmental semantics from surrounding POI distributions.
Building on this environment-aware backbone, a Route Choice Encoder then
captures the route choice behavior inherent in each trajectory by modeling its
constituent road segment transitions as a sequence of decisions. These
route-choice-aware representations are finally aggregated to form the global
trajectory embedding. Extensive experiments on 3 real-world datasets across 5
downstream tasks validate the effectiveness and generalizability of PRTraj.
Moreover, PRTraj demonstrates strong data efficiency, maintaining robust
performance under few-shot scenarios. Our code is available at:
https://anonymous.4open.science/r/PRTraj.

</details>


### [75] [FraQAT: Quantization Aware Training with Fractional bits](https://arxiv.org/abs/2510.14823)
*Luca Morreale,Alberto Gil C. P. Ramos,Malcolm Chadwick,Mehid Noroozi,Ruchika Chavhan,Abhinav Mehrotra,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: 提出FBQ：一种渐进式分数位量化，通过逐步从32位降到4位并利用分数位优化，能在多款扩散模型上比标准QAT降低4-7% FID，并实现手机端部署。


<details>
  <summary>Details</summary>
Motivation: 当前先进生成模型参数量大、计算资源需求高，难以直接部署到手机等受限设备上。尽管8位量化能提高效率，但更激进的低位量化（如4位）会显著损害模型质量，因此需要新的量化方法在极端压缩下仍能保持高质量生成。

Method: FBQ通过逐层或逐参数地渐进减小位宽，并在训练/微调过程中允许使用分数位表示和优化权重，以平滑量化引入的误差。相比传统的QAT（量化感知训练），FBQ在优化过程中保留了更多细粒度的表示能力，降低了量化带来的性能损失。

Result: 在多个扩散模型（SD3.5-Medium、Sana、pixart、FLUX.1-schnell）上，FBQ在保持4位量化的同时，比标准QAT在FID指标上低4-7%的值，表现出更好的生成质量。同时，FBQ已成功部署在三星S25U的Qualcomm SM8750-AB Snapdragon 8 Elite HTP上。

Conclusion: 本文提出了一种渐进式量化方法（Fractional Bits Quantization，简称FBQ），通过逐步将模型精度从32位降低到4位，并在优化过程中利用分数位来维持生成质量，从而在激进量化下保持较好性能。

Abstract: State-of-the-art (SOTA) generative models have demonstrated impressive
capabilities in image synthesis or text generation, often with a large capacity
model. However, these large models cannot be deployed on smartphones due to the
limited availability of on-board memory and computations. Quantization methods
lower the precision of the model parameters, allowing for efficient
computations, \eg, in \INT{8}. Although aggressive quantization addresses
efficiency and memory constraints, preserving the quality of the model remains
a challenge. To retain quality in previous aggressive quantization, we propose
a new fractional bits quantization (\short) approach. The novelty is a simple
yet effective idea: we progressively reduce the model's precision from 32 to 4
bits per parameter, and exploit the fractional bits during optimization to
maintain high generation quality. We show that the \short{} yields improved
quality on a variety of diffusion models, including SD3.5-Medium, Sana,
\pixart, and FLUX.1-schnell, while achieving $4-7\%$ lower FiD than standard
QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the
Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).

</details>


### [76] [Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data](https://arxiv.org/abs/2510.14831)
*Qi Chen,Xinze Zhou,Chen Liu,Hao Chen,Wenxuan Li,Zekun Jiang,Ziyan Huang,Yuxuan Zhao,Dexin Yu,Junjun He,Yefeng Zheng,Ling Shao,Alan Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: 合成数据能显著提升数据利用效率；AbdomenAtlas 2.0提供大规模、多器官、逐体素标注的CT肿瘤数据集，显著改善分割性能。


<details>
  <summary>Details</summary>
Motivation: 真实、逐体素标注的肿瘤数据稀缺且标注成本高，限制AI分割性能随数据扩展的提升。探索合成数据是否能提高数据扩展效率，同时提供大规模多器官标注数据集以推动研究。

Method: 在JHH 3,000例胰腺肿瘤CT数据集上做数据规模实验，比较不同真实样本量下模型性能并引入合成数据补充；构建并人工逐体素标注的AbdomenAtlas 2.0（10,135 CT，15,130个肿瘤实例），使用23名放射科专家进行标注，并在内外部分布测试中评估模型表现。

Result: 在JHH数据上，模型在1,500真实扫描后性能趋于平台期，但加入合成数据可在仅500真实扫描时达到相同性能；AbdomenAtlas 2.0在内部分布测试上带来+7% DSC提升，在外部分布测试上带来+16% DSC提升。

Conclusion: 合成数据能显著减少对真实标注样本的需求，提升肿瘤分割模型的数据效率；AbdomenAtlas 2.0 在规模和多器官覆盖上显著优于现有公开数据集，并带来分割性能提升。

Abstract: AI for tumor segmentation is limited by the lack of large, voxel-wise
annotated datasets, which are hard to create and require medical experts. In
our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found
that AI performance stopped improving after 1,500 scans. With synthetic data,
we reached the same performance using only 500 real scans. This finding
suggests that synthetic data can steepen data scaling laws, enabling more
efficient model training than real data alone. Motivated by these lessons, we
created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130
tumor instances per-voxel manually annotated in six organs (pancreas, liver,
kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23
expert radiologists, it is several orders of magnitude larger than existing
public tumor datasets. While we continue expanding the dataset, the current
version of AbdomenAtlas 2.0 already provides a strong foundation--based on
lessons from the JHH dataset--for training AI to segment tumors in six organs.
It achieves notable improvements over public datasets, with a +7% DSC gain on
in-distribution tests and +16% on out-of-distribution tests.

</details>


### [77] [QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models](https://arxiv.org/abs/2510.14836)
*Yixuan Li,Yuhui Chen,Mingcai Zhou,Haoran Li*

Main category: cs.CV

TL;DR: 提出QDepth-VLA：通过VQ-VAE量化深度token与深度专家辅助预测，增强VLA模型的深度感知与空间推理，实验证明能提升操作任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA方法缺乏对三维结构的理解与推理能力，导致在需要精细空间感知与控制的操作任务上表现受限。通过引入深度监督，期望补足几何信息不足的问题。

Method: 使用VQ-VAE对深度图进行编码，得到量化的潜在tokens；设计一个深度专家模块负责预测这些量化深度tokens，作为辅助任务与原有VLA任务共同训练，以此增强几何线索的学习。

Result: 在仿真基准和真实世界任务上，QDepth-VLA在空间推理能力上表现显著提升，并在若干操作任务上取得具有竞争力的性能。

Conclusion: QDepth-VLA通过引入辅助的深度预测任务与专门的深度专家模块，能让VLA模型学习到深度感知表示，从而改善三维结构理解与精细控制能力。

Abstract: Spatial perception and reasoning are crucial for Vision-Language-Action (VLA)
models to accomplish fine-grained manipulation tasks. However, existing
approaches often lack the ability to understand and reason over the essential
3D structures necessary for precise control. To address this limitation, we
propose QDepth-VLA, a general framework that augments VLA models with an
auxiliary depth prediction task. A dedicated depth expert is designed to
predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder,
enabling the model to learn depth-aware representations that capture critical
geometric cues. Experimental results on the simulation benchmarks and
real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning
and competitive performance on manipulation tasks.

</details>


### [78] [ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints](https://arxiv.org/abs/2510.14847)
*Meiqi Wu,Jiashu Zhu,Xiaokun Feng,Chubin Chen,Chen Zhu,Bingze Song,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: ImagerySearch：一种根据提示语义动态调整搜索空间和奖励的自适应测试时策略，配合新建的LDT-Bench，可显著提升想象性视频生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在现实场景表现良好，但面对长距离语义关系或少见概念组合的想象性提示时性能显著下降，且现有测试时缩放方法固定搜索空间与静态奖励限制了适应性。

Method: 提出基于提示的自适应测试时搜索策略：动态修改搜索空间与奖励函数，结合语义关系解析以引导推理过程；并构建LDT-Bench用于评估长距离语义提示下的生成表现。

Result: 在LDT-Bench上，ImagerySearch持续优于强基线与现有测试时缩放方法，并在VBench上也取得竞争性提升，证明对多样提示类型均有效。

Conclusion: ImagerySearch通过根据提示中的语义关系动态调整推理搜索空间和奖励函数，有效提升了在想象力场景下的视频生成质量。

Abstract: Video generation models have achieved remarkable progress, particularly
excelling in realistic scenarios; however, their performance degrades notably
in imaginative scenarios. These prompts often involve rarely co-occurring
concepts with long-distance semantic relationships, falling outside training
distributions. Existing methods typically apply test-time scaling for improving
video quality, but their fixed search spaces and static reward designs limit
adaptability to imaginative scenarios. To fill this gap, we propose
ImagerySearch, a prompt-guided adaptive test-time search strategy that
dynamically adjusts both the inference search space and reward function
according to semantic relationships in the prompt. This enables more coherent
and visually plausible videos in challenging imaginative settings. To evaluate
progress in this direction, we introduce LDT-Bench, the first dedicated
benchmark for long-distance semantic prompts, consisting of 2,839 diverse
concept pairs and an automated protocol for assessing creative generation
capabilities. Extensive experiments show that ImagerySearch consistently
outperforms strong video generation baselines and existing test-time scaling
approaches on LDT-Bench, and achieves competitive improvements on VBench,
demonstrating its effectiveness across diverse prompt types. We will release
LDT-Bench and code to facilitate future research on imaginative video
generation.

</details>


### [79] [A Multi-Task Deep Learning Framework for Skin Lesion Classification, ABCDE Feature Quantification, and Evolution Simulation](https://arxiv.org/abs/2510.14855)
*Harsha Kotla,Arun Kumar Rajasekaran,Hannah Rana*

Main category: cs.CV

TL;DR: 提出一个同时分类与量化ABCD特征的深度学习框架，并模拟特征随时间演化；在HAM10000上分类准确89%，黑色素瘤AUC0.96，边界特征最难预测。


<details>
  <summary>Details</summary>
Motivation: 大多数深度学习皮肤病变分类器作为黑盒，缺乏对临床可解释特征（ABCD）量化与解释；作者希望建立可提供可解释ABCD评分并模拟演变的模型，以便更好地辅助临床决策并研究病变进程。

Method: 基于深度学习的多任务框架，输出病变类别同时回归/量化A、B、C、D四项特征分值，并通过时间演化模拟生成E（Evolving）表示；在潜在表征空间可视化ABCD特征轨迹；在HAM10000数据集上训练与评估，使用分类指标（精度、AUC）和回归/分类指标评估每个ABCD特征表现。

Result: 分类准确率约89%，黑色素瘤AUC=0.96；对不对称性、颜色变化、直径的特征预测效果良好，边界不规则性仍难以建模；在潜在空间中展示了从良性到恶性的特征演变轨迹。

Conclusion: 该论文提出了一个能够同时进行皮肤病变分类并量化ABCD特征分数的深度学习框架，且通过模拟特征随时间演化来表示E（Evolving），在可视化潜在空间中展示病变从良性到恶性的演变轨迹。实验在HAM10000数据集上进行，分类准确率约89%，黑色素瘤AUC为0.96；在ABCD指标中，对不对称性、颜色变化和直径的预测表现较好，但边界不规则性较难建模。总体上，该框架有助于将机器学习诊断结果与临床可解释特征挂钩，促进对皮肤癌进展的理解。

Abstract: Early detection of melanoma has grown to be essential because it
significantly improves survival rates, but automated analysis of skin lesions
still remains challenging. ABCDE, which stands for Asymmetry, Border
irregularity, Color variation, Diameter, and Evolving, is a well-known
classification method for skin lesions, but most deep learning mechanisms treat
it as a black box, as most of the human interpretable features are not
explained. In this work, we propose a deep learning framework that both
classifies skin lesions into categories and also quantifies scores for each
ABCD feature. It simulates the evolution of these features over time in order
to represent the E aspect, opening more windows for future exploration. The A,
B, C, and D values are quantified particularly within this work. Moreover, this
framework also visualizes ABCD feature trajectories in latent space as skin
lesions evolve from benign nevuses to malignant melanoma. The experiments are
conducted using the HAM10000 dataset that contains around ten thousand images
of skin lesions of varying stages. In summary, the classification worked with
an accuracy of around 89 percent, with melanoma AUC being 0.96, while the
feature evaluation performed well in predicting asymmetry, color variation, and
diameter, though border irregularity remains more difficult to model. Overall,
this work provides a deep learning framework that will allow doctors to link ML
diagnoses to clinically relevant criteria, thus improving our understanding of
skin cancer progression.

</details>


### [80] [Multi-modal video data-pipelines for machine learning with minimal human supervision](https://arxiv.org/abs/2510.14862)
*Mihai-Cristian Pîrvu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 用预训练专家和自动化流水线在视频上生成多模态监督，基于PHG-MAE训练并蒸馏出<1M模型，可在低端设备上实时做语义分割和深度估计，性能接近大型模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界本质上是多模态的，但现有模型多为单模态或双模态，限制了对真实世界复杂性的理解。故希望通过整合尽可能多的视觉模态、并减少人工监督来提升模型的泛化与部署能力。

Method: 作者构建了一个全自动数据流水线，利用预训练专家模型在原始视频上生成多模态监督信号，并使用PHG-MAE架构进行训练，随后将大模型有效蒸馏为<1M参数的轻量模型；同时开放源码并在手持设备或网络摄像头上部署进行实时语义分割和使用DPT进行近实时深度估计。

Result: 在低参数量(<1M)的模型下，论文报告与约300M参数规模模型竞争的性能，并成功实现了在普通硬件上实时语义分割和近实时深度估计的部署；同时作者开源了数据流水线和部署框架。

Conclusion: 本文提出了一种在几乎无人工监督下整合多视觉模态的方法，使用预训练专家模型与程序化组合在原始视频上自动构建多模态数据流水线，并基于PHG-MAE训练出参数量低于1M的高效模型，能在实时语义分割和深度估计等任务上在低端设备上达到与~300M参数模型媲美的效果。

Abstract: The real-world is inherently multi-modal at its core. Our tools observe and
take snapshots of it, in digital form, such as videos or sounds, however much
of it is lost. Similarly for actions and information passing between humans,
languages are used as a written form of communication. Traditionally, Machine
Learning models have been unimodal (i.e. rgb -> semantic or text ->
sentiment_class). Recent trends go towards bi-modality, where images and text
are learned together, however, in order to truly understand the world, we need
to integrate all these independent modalities. In this work we try to combine
as many visual modalities as we can using little to no human supervision. In
order to do this, we use pre-trained experts and procedural combinations
between them on top of raw videos using a fully autonomous data-pipeline, which
we also open-source. We then make use of PHG-MAE, a model specifically designed
to leverage multi-modal data. We show that this model which was efficiently
distilled into a low-parameter (<1M) can have competitive results compared to
models of ~300M parameters. We deploy this model and analyze the use-case of
real-time semantic segmentation from handheld devices or webcams on commodity
hardware. Finally, we deploy other off-the-shelf models using the same
framework, such as DPT for near real-time depth estimation.

</details>


### [81] [Benchmarking Multimodal Large Language Models for Face Recognition](https://arxiv.org/abs/2510.14866)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文系统评估了开源多模态大模型在多个人脸识别基准上的零样本表现：虽然语义能力强，但在高精度场景仍落后于专用人脸识别模型，基准代码已开源。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在视觉-语言任务上表现优异，但其在人脸识别领域的能力尚未被充分评估；需要在相同评测协议下比较开源MLLM与专用模型的性能，发现差距并为改进提供方向。

Method: 对若干开源最先进的多模态大模型在统一协议下进行系统基准测试，评估其在LFW、CALFW、CPLFW、CFP、AgeDB和RFW等标准人脸识别数据集上的零样本识别性能，并与专用人脸识别模型进行比较。

Result: 实验表明MLLMs能捕捉到对人脸相关任务有用的丰富语义信息，但在零样本高精度识别上不及专用模型。基准测试揭示了性能差距并指出了提升方向。

Conclusion: MLLMs在面部识别任务中展示了语义理解能力，但在零样本高精度识别场景下仍落后于专用模型。Benchmark为未来提升MLLM在人脸识别上的性能与泛化能力提供了基础。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable performance
across diverse vision-and-language tasks. However, their potential in face
recognition remains underexplored. In particular, the performance of
open-source MLLMs needs to be evaluated and compared with existing face
recognition models on standard benchmarks with similar protocol. In this work,
we present a systematic benchmark of state-of-the-art MLLMs for face
recognition on several face recognition datasets, including LFW, CALFW, CPLFW,
CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich
semantic cues useful for face-related tasks, they lag behind specialized models
in high-precision recognition scenarios in zero-shot applications. This
benchmark provides a foundation for advancing MLLM-based face recognition,
offering insights for the design of next-generation models with higher accuracy
and generalization. The source code of our benchmark is publicly available in
the project page.

</details>


### [82] [TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions](https://arxiv.org/abs/2510.14874)
*Guangyi Han,Wei Zhai,Yuhang Yang,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该工作构建大规模野外3D HOI数据集并提出TOUCH扩散框架，通过接触建模与物理细化，实现基于细粒度意图的自由形手物交互生成。


<details>
  <summary>Details</summary>
Motivation: 现有HOI生成受限于固定抓握模式与稳定抓握的强先验，无法覆盖日常生活中推、戳、转等自由交互的多样性；需要细粒度意图控制与更真实的物理接触建模。

Method: 构建WildO2真实世界3D HOI数据集（4.4k交互，92意图，610物品类），并提出TOUCH三阶段框架：以多层次扩散模型为核心进行细粒度语义控制，结合显式接触建模作为条件，随后通过接触一致性和物理约束进行细化。

Result: 实验表明方法能在WildO2数据集上生成可控、多样且物理合理的手部交互，超越仅有抓握先验的现有方法。

Conclusion: 该论文提出了Free-Form HOI Generation任务，突破仅限抓握的限制，实现基于细粒度意图的自由形交互生成，兼顾可控性、多样性与物理合理性。

Abstract: Hand-object interaction (HOI) is fundamental for humans to express intent.
Existing HOI generation research is predominantly confined to fixed grasping
patterns, where control is tied to physical priors such as force closure or
generic intent instructions, even when expressed through elaborate language.
Such an overly general conditioning imposes a strong inductive bias for stable
grasps, thus failing to capture the diversity of daily HOI. To address these
limitations, we introduce Free-Form HOI Generation, which aims to generate
controllable, diverse, and physically plausible HOI conditioned on fine-grained
intent, extending HOI from grasping to free-form interactions, like pushing,
poking, and rotating. To support this task, we construct WildO2, an in-the-wild
diverse 3D HOI dataset, which includes diverse HOI derived from internet
videos. Specifically, it contains 4.4k unique interactions across 92 intents
and 610 object categories, each with detailed semantic annotations. Building on
this dataset, we propose TOUCH, a three-stage framework centered on a
multi-level diffusion model that facilitates fine-grained semantic control to
generate versatile hand poses beyond grasping priors. This process leverages
explicit contact modeling for conditioning and is subsequently refined with
contact consistency and physical constraints to ensure realism. Comprehensive
experiments demonstrate our method's ability to generate controllable, diverse,
and physically plausible hand interactions representative of daily activities.
The project page is $\href{https://guangyid.github.io/hoi123touch}{here}$.

</details>


### [83] [BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data](https://arxiv.org/abs/2510.14876)
*Roni Goldshmidt,Hamish Scott,Lorenzo Niccolini,Shizhan Zhu,Daniel Moura,Orly Zvitia*

Main category: cs.CV

TL;DR: 提出以自车为中心的碰撞预测模型BADAS（基于V-JEPA2），通过重标注与合成负样本改进评估并在多数据集上实现SOTA，释放部分模型与数据。


<details>
  <summary>Details</summary>
Motivation: 现有碰撞预测方法无法区分威胁是否针对自车，导致真实部署中误报过多，需要以自车为中心的评估基准与模型来提升实用性。

Method: 提出基于V-JEPA2的端到端训练骨干，训练数据包括公开1.5k视频（BADAS-Open）和40k专有视频（BADAS1.0）；对现有基准数据集进行重标注以区分是否涉及自车、增加一致的报警时间标签并合成必要的负样本，从而支持AP/AUC与时间性评估。

Result: 在DAD、DADA-2000、DoTA和Nexar等数据集上，BADAS在AP/AUC指标上达到最先进水平，优于基线前向碰撞ADAS，并给出更现实的事故发生时间估计；公开了BADAS-Open权重、代码与重标注数据。

Conclusion: BADAS针对以自车为中心的碰撞预测问题取得了显著进展，能有效降低与自车无关事故的误报，并在多个数据集上达到或超过现有最优指标，同时开放部分模型与重标注数据以促进研究。

Abstract: Existing collision prediction methods often fail to distinguish between
ego-vehicle threats and random accidents not involving the ego vehicle, leading
to excessive false alerts in real-world deployment. We present BADAS, a family
of collision prediction models trained on Nexar's real-world dashcam collision
dataset -- the first benchmark designed explicitly for ego-centric evaluation.
We re-annotate major benchmarks to identify ego involvement, add consensus
alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC
and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and
comes in two variants: BADAS-Open (trained on our 1.5k public videos) and
BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and
Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a
forward-collision ADAS baseline while producing more realistic time-to-accident
estimates. We release our BADAS-Open model weights and code, along with
re-annotations of all evaluation datasets to promote ego-centric collision
prediction research.

</details>


### [84] [ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention](https://arxiv.org/abs/2510.14882)
*Keli Liu,Zhendong Wang,Wengang Zhou,Shaodong Xu,Ruixiao Dong,Houqiang Li*

Main category: cs.CV

TL;DR: ScaleWeaver 是一个在视觉自回归模型上通过参数高效微调实现可控文本到图像生成的框架，核心为去除冗余注意力的 Reference Attention 和零初始化投影，兼顾质量、控制性与效率。


<details>
  <summary>Details</summary>
Motivation: 当前可控生成研究多集中于扩散模型，VAR 在高保真与推理效率上有优势，但缺乏灵活精确的控制机制，ScaleWeaver 旨在填补该空白。

Method: 通过在 VAR 主干上进行参数高效微调，引入改进的 MMDiT 块与 Reference Attention：去除图像->条件的不必要注意力以降低计算，强调参数复用并加入零初始化线性投影以稳定控制信号注入。

Result: ScaleWeaver 在大量实验中展示了高质量生成与精确控制能力，且在效率上优于基于扩散的方法，证明了其在 VAR 范式下的实用性与有效性。

Conclusion: ScaleWeaver 提出了一种针对视觉自回归(VAR)模型的高效可控生成框架，通过改进的 MMDiT 块和 Reference Attention 模块，实现了条件信息的高效注入与参数高复用，从而在保持基线生成能力的同时提供精确控制。

Abstract: Text-to-image generation with visual autoregressive~(VAR) models has recently
achieved impressive advances in generation fidelity and inference efficiency.
While control mechanisms have been explored for diffusion models, enabling
precise and flexible control within VAR paradigm remains underexplored. To
bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel
framework designed to achieve high-fidelity, controllable generation upon
advanced VAR models through parameter-efficient fine-tuning. The core module in
ScaleWeaver is the improved MMDiT block with the proposed Reference Attention
module, which efficiently and effectively incorporates conditional information.
Different from MM Attention, the proposed Reference Attention module discards
the unnecessary attention from image$\rightarrow$condition, reducing
computational cost while stabilizing control injection. Besides, it
strategically emphasizes parameter reuse, leveraging the capability of the VAR
backbone itself with a few introduced parameters to process control
information, and equipping a zero-initialized linear projection to ensure that
control signals are incorporated effectively without disrupting the generative
capability of the base model. Extensive experiments show that ScaleWeaver
delivers high-quality generation and precise control while attaining superior
efficiency over diffusion-based methods, making ScaleWeaver a practical and
effective solution for controllable text-to-image generation within the visual
autoregressive paradigm. Code and models will be released.

</details>


### [85] [You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction](https://arxiv.org/abs/2510.14885)
*Logan Lawrence,Oindrila Saha,Megan Wei,Chen Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 提出简单高效的两阶段方法（开放式生成 + 受限解码）和检索场景下的早停概率估算，解决了 MLLM 在大规模多项选择FGVC评估中的效率与准确性问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法多针对语言任务或仅限小规模（≤5选项）MCQ，无法处理FGVC中数百至数千且高度相关的候选选项，且检索设置下对全集合逐项概率计算成本高。

Method: nlg2choice：先让 MLLM 生成开放式回答（最小约束），再用文本级受限解码对候选选项进行选择；检索任务中采用基于受限响应概率的早停策略来加速计算。

Result: 在七个细粒度视觉数据集上，nlg2choice在分类与检索评估上均优于对比方法，并且对不同自然语言实现方式鲁棒。

Conclusion: 该论文提出 nlg2choice，两阶段方法能有效评估自回归多模态模型在大规模多选题（数百到数千选项）FGVC任务中的表现，通过先开放式生成再受限解码选择，提高分类与检索性能。

Abstract: Despite the renewed interest in zero-shot visual classification due to the
rise of Multimodal Large Language Models (MLLMs), the problem of evaluating
free-form responses of auto-regressive models remains a persistent challenge.
Most existing works focus on language-only tasks or don't consider Multiple
Choice Questions (MCQs) beyond 5-way options, both of which are critical
capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where
choice counts are in the hundreds to thousands and the choices are highly
related. Furthermore, in this highly multi-way MCQ setting it is not clear how
to extend LLM choice extraction to retrieval-based problems, where computing
probabilities over the choice set is computationally costly. In this work we
investigate nlg2choice, a simple two-stage method which first asks the MLLM an
open-ended question for the task with minimal constraints, then uses text-only
constrained decoding to predict the most likely choice. In retrieval settings,
we compute the probability of the constrained response taking that choice with
an early stopping method to significantly improve throughput. Our results show
improvement over a suite of seven fine-grained visual datasets when evaluating
in terms of classification and retrieval, and show that this performance holds
over the various ways that users of LLMs can implement tasks in natural
language.

</details>


### [86] [Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](https://arxiv.org/abs/2510.14896)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 本文利用MLLM将物体对的时序活动与交互转化为文本表示，用于半监督VAD，既提高了对交互异常的检测能力，又提供了可解释性，并在多个基准上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有半监督VAD方法难以识别涉及物体交互的复杂异常，且缺乏解释性；利用MLLM能将视觉交互转换为可理解的文本，从而改进检测性能并增强可解释性。

Method: 首先在标注为正常的视频中，对检测到的物体对在不同时间点的视觉输入进行查询，利用MLLM生成描述物体活动与交互的文本；这些文本作为高层表示保存在数据库；测试时对视频中的物体对同样生成文本并与训练集文本进行相似度比对以判定异常。该方法也可与传统VAD方法结合以提高可解释性。

Result: 在包含交互异常的基准数据集上，该方法有效检测复杂交互异常；在不含交互异常的数据集上也达到了最先进的性能，且结果具有可解释性。

Conclusion: 该论文提出了一种基于多模态大语言模型（MLLM）的半监督视频异常检测（VAD）新框架，通过生成与物体对在不同时刻的活动及交互相关的文本描述，将视频中的高层语义信息用于异常检测，提升了对复杂交互异常的检测能力并提供了可解释性。

Abstract: Existing semi-supervised video anomaly detection (VAD) methods often struggle
with detecting complex anomalies involving object interactions and generally
lack explainability. To overcome these limitations, we propose a novel VAD
framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous
MLLM-based approaches that make direct anomaly judgments at the frame level,
our method focuses on extracting and interpreting object activity and
interactions over time. By querying an MLLM with visual inputs of object pairs
at different moments, we generate textual descriptions of the activity and
interactions from nominal videos. These textual descriptions serve as a
high-level representation of the activity and interactions of objects in a
video. They are used to detect anomalies during test time by comparing them to
textual descriptions found in nominal training videos. Our approach inherently
provides explainability and can be combined with many traditional VAD methods
to further enhance their interpretability. Extensive experiments on benchmark
datasets demonstrate that our method not only detects complex interaction-based
anomalies effectively but also achieves state-of-the-art performance on
datasets without interaction anomalies.

</details>


### [87] [MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos](https://arxiv.org/abs/2510.14904)
*Gabriel Fiastre,Antoine Yang,Cordelia Schmid*

Main category: cs.CV

TL;DR: 通过用VLM生成时空实体的合成描述扩充数据，并训练端到端的MaskCaptioner，实现了检测、分割、跟踪与轨迹级描述的联合优化，在多项基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: DVOC任务需同时掌握视频中对象的空间、时间细节并用自然语言描述，但人工标注代价高昂且现有方法多采用分离训练，导致性能受限，故引入合成字幕扩充数据并实现联合训练以提升性能。

Method: 使用VLM为标注好的物体实例生成合成字幕，构建扩增数据集LVISCap与LV-VISCap；基于这些数据对MaskCaptioner进行预训练，并设计其端到端框架以同时完成检测、分割、跟踪与轨迹级别的字幕生成。

Result: 在三个DVOC基准（VidSTG, VLN, BenSMOT）上，经过在LVISCap与LV-VISCap上的预训练，MaskCaptioner取得了最新的最优（state-of-the-art）结果。并且公开了数据集与代码。

Conclusion: 本文提出通过利用先进的视觉语言模型（VLM）生成时空定位实体的合成描述，并基于此扩展LVIS和LV-VIS数据集（得到LVISCap与LV-VISCap），从而训练一个端到端模型MaskCaptioner，实现检测、分割、跟踪与描述对象轨迹的联合学习，缓解了以往分离训练的不足。

Abstract: Dense Video Object Captioning (DVOC) is the task of jointly detecting,
tracking, and captioning object trajectories in a video, requiring the ability
to understand spatio-temporal details and describe them in natural language.
Due to the complexity of the task and the high cost associated with manual
annotation, previous approaches resort to disjoint training strategies,
potentially leading to suboptimal performance. To circumvent this issue, we
propose to generate captions about spatio-temporally localized entities
leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets
with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an
end-to-end model capable of jointly detecting, segmenting, tracking and
captioning object trajectories. Moreover, with pretraining on LVISCap and
LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three
existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are
available at https://www.gabriel.fiastre.fr/maskcaptioner/.

</details>


### [88] [3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation](https://arxiv.org/abs/2510.14945)
*JoungBin Lee,Jaewoo Jung,Jisang Han,Takuya Narihira,Kazumi Fukuda,Junyoung Seo,Sunghwan Hong,Yuki Mitsufuji,Seungryong Kim*

Main category: cs.CV

TL;DR: 通过动态SLAM分离静态场景并构建3D场景记忆，结合双时空条件提示，实现长序列且可控的视角一致视频生成，显著提升场景一致性与相机控制能力。


<details>
  <summary>Details</summary>
Motivation: 解决基于单帧或短片段条件的视频生成在长范围推断时的场景不一致和相机控制不足问题，尤其避免将过去的动态元素错误保留到未来视图。

Method: 提出双时空条件（邻近时间帧保证运动连贯，邻近空间视图保证场景一致性）并引入3D静态场景记忆，使用动态SLAM和动态掩码分离静态几何与动态元素；将静态表示投影到目标视点作为3D空间提示，同时保留时间上下文处理动态区域。

Result: 在场景一致性、相机可控性和生成质量上明显优于现有方法，同时兼顾计算效率与运动真实性。

Conclusion: 该论文提出了3DScenePrompt，通过结合三维静态场景记忆与双时空条件，实现了长序列视频生成中的相机精确控制与场景一致性。

Abstract: We present 3DScenePrompt, a framework that generates the next video chunk
from arbitrary-length input while enabling precise camera control and
preserving scene consistency. Unlike methods conditioned on a single image or a
short clip, we employ dual spatio-temporal conditioning that reformulates
context-view referencing across the input video. Our approach conditions on
both temporally adjacent frames for motion continuity and spatially adjacent
content for scene consistency. However, when generating beyond temporal
boundaries, directly using spatially adjacent frames would incorrectly preserve
dynamic elements from the past. We address this by introducing a 3D scene
memory that represents exclusively the static geometry extracted from the
entire input video. To construct this memory, we leverage dynamic SLAM with our
newly introduced dynamic masking strategy that explicitly separates static
scene geometry from moving elements. The static scene representation can then
be projected to any target viewpoint, providing geometrically consistent warped
views that serve as strong 3D spatial prompts while allowing dynamic regions to
evolve naturally from temporal context. This enables our model to maintain
long-range spatial coherence and precise camera control without sacrificing
computational efficiency or motion realism. Extensive experiments demonstrate
that our framework significantly outperforms existing methods in scene
consistency, camera controllability, and generation quality. Project page :
https://cvlab-kaist.github.io/3DScenePrompt/

</details>


### [89] [OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression](https://arxiv.org/abs/2510.14954)
*Zhe Li,Weihao Yuan,Weichao Shen,Siyu Zhu,Zilong Dong,Chang Xu*

Main category: cs.CV

TL;DR: 提出基于连续masked自回归transformer和DiT条件扩散、多模态融合策略的全身动作生成框架，在多模态生成任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 应对全身多模态人体动作生成的两大挑战：有效的动作生成机制和多模态融合。

Method: 设计连续masked自回归transformer，采用因果注意、门控线性注意（gated linear attention）、RMSNorm，结合DiT条件扩散结构，使用AdaLN和cross-attention融合文本、语音、音乐。

Result: 在文本到动作、语音到手势、音乐到舞蹈等任务上均超过以往方法，且代码将公开。

Conclusion: 提出了一种连续的masked autoregressive motion transformer，结合门控线性注意和RMSNorm，并使用DiT结构扩散条件，通过AdaLN和跨注意力融合多模态，实现对全身多模态动作生成的改进。

Abstract: Whole-body multi-modal human motion generation poses two primary challenges:
creating an effective motion generation mechanism and integrating various
modalities, such as text, speech, and music, into a cohesive framework. Unlike
previous methods that usually employ discrete masked modeling or autoregressive
modeling, we develop a continuous masked autoregressive motion transformer,
where a causal attention is performed considering the sequential nature within
the human motion. Within this transformer, we introduce a gated linear
attention and an RMSNorm module, which drive the transformer to pay attention
to the key actions and suppress the instability caused by either the abnormal
movements or the heterogeneous distributions within multi-modalities. To
further enhance both the motion generation and the multimodal generalization,
we employ the DiT structure to diffuse the conditions from the transformer
towards the targets. To fuse different modalities, AdaLN and cross-attention
are leveraged to inject the text, speech, and music signals. Experimental
results demonstrate that our framework outperforms previous methods across all
modalities, including text-to-motion, speech-to-gesture, and music-to-dance.
The code of our method will be made public.

</details>


### [90] [RealDPO: Real or Not Real, that is the Preference](https://arxiv.org/abs/2510.14955)
*Guo Cheng,Danni Yang,Ziqi Huang,Jianlou Si,Chenyang Si,Ziwei Liu*

Main category: cs.CV

TL;DR: RealDPO用真实视频作为偏好正样本并结合定制DPO损失与RealAction-5K数据集，通过迭代对比学习显著提升复杂动作的视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在复杂运动合成上表现不足，生成运动常不自然、不平滑或与语境不一致，限制实用性。通过引入真实数据作为偏好信号，希望缩小与真实世界运动的差距。

Method: 提出RealDPO：用真实世界视频作为正样本进行偏好学习，采用Direct Preference Optimization（DPO）并定制损失函数，通过对比真实视频与模型错误输出，进行迭代自我修正；同时构建RealAction-5K数据集用于后训练。

Result: 大量实验表明，RealDPO在视频质量、文本对齐性和运动真实感方面优于最先进模型和现有偏好优化技术。

Conclusion: RealDPO通过把真实视频作为偏好学习的正样本，并结合专门设计的DPO损失，能显著提升生成视频的运动真实性、平滑度和文本对齐性，相较于SFT和现有偏好优化方法具有明显优势。

Abstract: Video generative models have recently achieved notable advancements in
synthesis quality. However, generating complex motions remains a critical
challenge, as existing models often struggle to produce natural, smooth, and
contextually consistent movements. This gap between generated and real-world
motions limits their practical applicability. To address this issue, we
introduce RealDPO, a novel alignment paradigm that leverages real-world data as
positive samples for preference learning, enabling more accurate motion
synthesis. Unlike traditional supervised fine-tuning (SFT), which offers
limited corrective feedback, RealDPO employs Direct Preference Optimization
(DPO) with a tailored loss function to enhance motion realism. By contrasting
real-world videos with erroneous model outputs, RealDPO enables iterative
self-correction, progressively refining motion quality. To support
post-training in complex motion synthesis, we propose RealAction-5K, a curated
dataset of high-quality videos capturing human daily activities with rich and
precise motion details. Extensive experiments demonstrate that RealDPO
significantly improves video quality, text alignment, and motion realism
compared to state-of-the-art models and existing preference optimization
techniques.

</details>


### [91] [MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2510.14958)
*Weikang Shi,Aldrich Yu,Rongyao Fang,Houxing Ren,Ke Wang,Aojun Zhou,Changyao Tian,Xinyu Fu,Yuxuan Hu,Zimu Lu,Linjiang Huang,Si Liu,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: MathCanvas提供数据集、训练流程与基准，通过大规模图示生成/编辑预训练和策略性视觉推理微调，使统一多模态模型能生成时机恰当且高保真的图示，从而显著提升多步数学题目的视觉辅助解题能力（BAGEL-Canvas在专用基准上大幅优于基线）。


<details>
  <summary>Details</summary>
Motivation: LLMs在纯文本推理上已很强，但几何等数学领域依赖视觉辅助，现有VCoT方法受限于外部工具或无法生成高保真、策略性生成图示，难以支持复杂问题；因此需要一种内在的、统一的视觉-文本推理框架。

Method: 两阶段框架：1) 视觉操作预训练——构建15.2M对数据集（10M caption-to-diagram MathCanvas-Imagen，5.2M 编辑轨迹 MathCanvas-Edit）预训练模型以掌握图示生成与编辑；2) 策略性视觉辅助推理微调——在219K示例的MathCanvas-Instruct上训练，学习在何时以及如何交替使用视觉与文本步骤。并构建MathCanvas-Bench（3K问题）评估模型的交互式视觉-文本解题能力。

Result: 提出的模型BAGEL-Canvas在MathCanvas-Bench上相比强LMM基线取得86%相对提升，并在公开数学基准上展现良好泛化能力，证明方法有效。

Conclusion: MathCanvas通过预训练与微调两阶段，使统一大多模态模型具备内在视觉推理能力，在生成与编辑数学图示及策略性何时使用图示方面显著优于基线，能在专门基准上取得大幅提升，提升了视觉辅助数学推理的可行性与通用性。

Abstract: While Large Language Models (LLMs) have excelled in textual reasoning, they
struggle with mathematical domains like geometry that intrinsically rely on
visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often
limited by rigid external tools or fail to generate the high-fidelity,
strategically-timed diagrams necessary for complex problem-solving. To bridge
this gap, we introduce MathCanvas, a comprehensive framework designed to endow
unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for
mathematics. Our approach consists of two phases. First, a Visual Manipulation
stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M
caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing
trajectories (MathCanvas-Edit), to master diagram generation and editing.
Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on
MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual
reasoning paths, teaching it when and how to leverage visual aids. To
facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging
benchmark with 3K problems that require models to produce interleaved
visual-textual solutions. Our model, BAGEL-Canvas, trained under this
framework, achieves an 86% relative improvement over strong LMM baselines on
MathCanvas-Bench, demonstrating excellent generalization to other public math
benchmarks. Our work provides a complete toolkit-framework, datasets, and
benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project
Page: https://mathcanvas.github.io/

</details>


### [92] [C4D: 4D Made from 3D through Dual Correspondences](https://arxiv.org/abs/2510.14960)
*Shizun Wang,Zhenxiang Jiang,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: C4D adds short-term optical flow and long-term point tracking to pointmap-based reconstruction, uses a dynamic-aware tracker and motion masks, and jointly optimizes per-frame geometry and camera poses to recover smooth 3D trajectories for dynamic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing pointmap-based methods perform well for static scenes but fail on dynamic scenes because moving objects break multi-view geometry; leveraging temporal correspondences can provide motion information to separate dynamic from static elements.

Method: C4D predicts pointmaps and two correspondence types (short-term optical flow and long-term point tracking), trains a dynamic-aware point tracker to estimate motion masks, and optimizes dynamic scene objectives to jointly recover per-frame geometry and camera parameters while lifting 2D trajectories to 3D.

Result: The framework achieves complete 4D recovery, strong performance in depth estimation, camera pose estimation, and point tracking across experiments.

Conclusion: C4D successfully extends pointmap-based 3D reconstruction to dynamic 4D scenes by integrating temporal correspondences, achieving improved depth, pose, and tracking results.

Abstract: Recovering 4D from monocular video, which jointly estimates dynamic geometry
and camera poses, is an inevitably challenging problem. While recent
pointmap-based 3D reconstruction methods (e.g., DUSt3R) have made great
progress in reconstructing static scenes, directly applying them to dynamic
scenes leads to inaccurate results. This discrepancy arises because moving
objects violate multi-view geometric constraints, disrupting the
reconstruction. To address this, we introduce C4D, a framework that leverages
temporal Correspondences to extend existing 3D reconstruction formulation to
4D. Specifically, apart from predicting pointmaps, C4D captures two types of
correspondences: short-term optical flow and long-term point tracking. We train
a dynamic-aware point tracker that provides additional mobility information,
facilitating the estimation of motion masks to separate moving elements from
the static background, thus offering more reliable guidance for dynamic scenes.
Furthermore, we introduce a set of dynamic scene optimization objectives to
recover per-frame 3D geometry and camera parameters. Simultaneously, the
correspondences lift 2D trajectories into smooth 3D trajectories, enabling
fully integrated 4D reconstruction. Experiments show that our framework
achieves complete 4D recovery and demonstrates strong performance across
multiple downstream tasks, including depth estimation, camera pose estimation,
and point tracking. Project Page: https://littlepure2333.github.io/C4D

</details>


### [93] [RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion](https://arxiv.org/abs/2510.14962)
*Thao Nguyen,Jiaqi Ma,Fahad Shahbaz Khan,Souhaib Ben Taieb,Salman Khan*

Main category: cs.CV

TL;DR: 提出一种在像素空间中原生集成Token-wise Attention的扩散模型和时空编码器，免去独立潜空间模块，提升降水即时预报的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在降雨即时预报中要么依赖潜空间自编码器（增加复杂度并限制泛化），要么直接在像素空间操作（计算开销大且常缺少注意力模块，难以建模远程时空依赖）。论文旨在结合注意力与像素级扩散以获得更好效果且资源可控的方案。

Method: 在像素空间的扩散模型架构中设计Token-wise Attention模块，并将其嵌入U-Net的不同尺度和一个专门的时空编码器中，以动态建模空间和时间依赖；该方法避免使用单独训练的潜在自动编码器，从而减轻复杂度，同时通过结构性注意力保持计算资源可控。

Result: 在多个数据集上的大量实验和可视化结果表明，该方法在局部细节保真、泛化能力和复杂降水场景下的鲁棒性方面显著优于现有最先进方法。

Conclusion: 该论文提出将Token-wise Attention同时集成到U-Net扩散模型与时空编码器中，以原生方式在像素空间引入注意力机制，从而在不依赖独立潜空间自编码器的情况下捕捉多尺度空间交互与时间演化，提高降雨即时预报的局部保真度、泛化性与鲁棒性。

Abstract: Precipitation nowcasting, predicting future radar echo sequences from current
observations, is a critical yet challenging task due to the inherently chaotic
and tightly coupled spatio-temporal dynamics of the atmosphere. While recent
advances in diffusion-based models attempt to capture both large-scale motion
and fine-grained stochastic variability, they often suffer from scalability
issues: latent-space approaches require a separately trained autoencoder,
adding complexity and limiting generalization, while pixel-space approaches are
computationally intensive and often omit attention mechanisms, reducing their
ability to model long-range spatio-temporal dependencies. To address these
limitations, we propose a Token-wise Attention integrated into not only the
U-Net diffusion model but also the spatio-temporal encoder that dynamically
captures multi-scale spatial interactions and temporal evolution. Unlike prior
approaches, our method natively integrates attention into the architecture
without incurring the high resource cost typical of pixel-space diffusion,
thereby eliminating the need for separate latent modules. Our extensive
experiments and visual evaluations across diverse datasets demonstrate that the
proposed method significantly outperforms state-of-the-art approaches, yielding
superior local fidelity, generalization, and robustness in complex
precipitation forecasting scenarios.

</details>


### [94] [ChangingGrounding: 3D Visual Grounding in Changing Scenes](https://arxiv.org/abs/2510.14965)
*Miao Hu,Zhiwei Huang,Tai Wang,Jiangmiao Pang,Dahua Lin,Nanning Zheng,Runsen Xu*

Main category: cs.CV

TL;DR: 引入面向动态场景的3D视觉定位基准ChangingGrounding，并提出零样本Mem-ChangingGrounder，通过检索记忆+轻量多视角融合，实现更少探索、更高精度的目标定位。


<details>
  <summary>Details</summary>
Motivation: 现实机器人在不断变化的场景中需要根据自然语言指令定位物体，但传统3DVG方法假设有最新的重建点云，导致频繁重扫和部署困难，因此需要一个能利用历史观察、只在必要时探索并在动态场景中仍保持高精度的方案与评测。

Method: 提出Mem-ChangingGrounder：结合跨模态检索以识别查询对象类型，检索相关记忆指导动作策略，进行目标导向的高效探索；在探索失败时回退；对目标执行多视角扫描并融合证据，最终将多视角信息投影以生成精确3D包围盒。

Result: 构建了ChangingGrounding基准并评估多种基线；Mem-ChangingGrounder在定位准确性上取得最高，同时显著降低了探索成本，证明了记忆与主动多视角策略的有效性。

Conclusion: 本文提出了ChangingGrounding基准，重新定义3D视觉定位为一个主动、记忆驱动的问题，并验证了记忆和多视角轻量融合在变化场景中对定位效率与精度的提升。

Abstract: Real-world robots localize objects from natural-language instructions while
scenes around them keep changing. Yet most of the existing 3D visual grounding
(3DVG) method still assumes a reconstructed and up-to-date point cloud, an
assumption that forces costly re-scans and hinders deployment. We argue that
3DVG should be formulated as an active, memory-driven problem, and we introduce
ChangingGrounding, the first benchmark that explicitly measures how well an
agent can exploit past observations, explore only where needed, and still
deliver precise 3D boxes in changing scenes. To set a strong reference point,
we also propose Mem-ChangingGrounder, a zero-shot method for this task that
marries cross-modal retrieval with lightweight multi-view fusion: it identifies
the object type implied by the query, retrieves relevant memories to guide
actions, then explores the target efficiently in the scene, falls back when
previous operations are invalid, performs multi-view scanning of the target,
and projects the fused evidence from multi-view scans to get accurate object
bounding boxes. We evaluate different baselines on ChangingGrounding, and our
Mem-ChangingGrounder achieves the highest localization accuracy while greatly
reducing exploration cost. We hope this benchmark and method catalyze a shift
toward practical, memory-centric 3DVG research for real-world applications.
Project page: https://hm123450.github.io/CGB/ .

</details>


### [95] [WithAnyone: Towards Controllable and ID Consistent Image Generation](https://arxiv.org/abs/2510.14975)
*Hengyuan Xu,Wei Cheng,Peng Xing,Yixiao Fang,Shuhan Wu,Rui Wang,Xianfang Zeng,Daxin Jiang,Gang Yu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 通过大规模配对数据和对比身份损失，WithAnyone有效解决了重建训练下的copy-paste问题，实现身份保持与生成多样性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有重建式训练在缺乏大规模同人多图配对数据下容易直接复制参考人脸（copy-paste），从而牺牲可控性和表现力；需要在保真与多样性间取得平衡。

Method: 构造MultiID-2M数据集、设计衡量copy-paste与身份-多样性权衡的基准，并在扩散模型中引入对比身份损失进行训练。

Result: 提出的WithAnyone在质/量评测和用户研究中均显著降低copy-paste伪影，提升对姿态/表情的可控性，同时保持较高的身份相似度和感知质量。

Conclusion: 该工作构建了大规模配对数据集并提出对比身份损失，通过平衡身份保持与多样性，有效缓解了copy-paste问题。

Abstract: Identity-consistent generation has become an important focus in text-to-image
research, with recent models achieving notable success in producing images
aligned with a reference identity. Yet, the scarcity of large-scale paired
datasets containing multiple images of the same individual forces most
approaches to adopt reconstruction-based training. This reliance often leads to
a failure mode we term copy-paste, where the model directly replicates the
reference face rather than preserving identity across natural variations in
pose, expression, or lighting. Such over-similarity undermines controllability
and limits the expressive power of generation. To address these limitations, we
(1) construct a large-scale paired dataset MultiID-2M, tailored for
multi-person scenarios, providing diverse references for each identity; (2)
introduce a benchmark that quantifies both copy-paste artifacts and the
trade-off between identity fidelity and variation; and (3) propose a novel
training paradigm with a contrastive identity loss that leverages paired data
to balance fidelity with diversity. These contributions culminate in
WithAnyone, a diffusion-based model that effectively mitigates copy-paste while
preserving high identity similarity. Extensive qualitative and quantitative
experiments demonstrate that WithAnyone significantly reduces copy-paste
artifacts, improves controllability over pose and expression, and maintains
strong perceptual quality. User studies further validate that our method
achieves high identity fidelity while enabling expressive controllable
generation.

</details>


### [96] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: Ponimator以近接交互姿态为核心，借助时空条件扩散模型把高质量mocap交互知识迁移到图像/文本驱动的交互动画生成，支持多任务并表现良好。


<details>
  <summary>Details</summary>
Motivation: 观察到近距离人际交互姿态蕴含强交互先验，人类可据此推断过去和未来动态；借鉴此先验构建动画生成框架以扩展mocap知识至开放世界。

Method: 使用两个条件扩散模型：姿态动画器（基于时间先验生成动态序列）和姿态生成器（基于空间先验从单帧或文本合成交互姿态）；训练数据为近接两人姿态及其时间上下文。

Result: 在多数据集与多任务上实验表明姿态先验具有普适性，框架在图像驱动交互动画、反应动画、文本到交互合成等任务中表现有效且鲁棒。

Conclusion: Ponimator提出基于近距离交互姿态的通用交互动画框架，能有效从mocap数据迁移交互知识并支持多种任务。

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>


### [97] [Terra: Explorable Native 3D World Model with Point Latents](https://arxiv.org/abs/2510.14977)
*Yuanhui Huang,Weiliang Chen,Wenzhao Zheng,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Terra是一个原生3D世界模型，结合P2G-VAE和SPFlow在3D潜空间表示与生成上实现高一致性与高效可探索场景生成，ScanNet v2实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 多数现有世界模型依赖像素对齐表示，忽视物理世界的三维本质，导致三维一致性差和建模效率低。为此提出原生3D潜空间表示以提升一致性与效率。

Method: 方法包括：1) P2G-VAE：将3D输入编码为潜在点表示，解码为3D高斯基元同时建模几何与外观；2) SPFlow：在潜在点空间进行稀疏点流匹配，联合去噪点的位置与特征；3) 逐步生成（progressive generation）用于可探索场景建模，并支持单次生成多视角渲染。

Result: 在ScanNet v2室内场景上，Terra在重建与生成任务上取得了最先进的性能，并展示了高3D一致性与从任意视角灵活渲染能力。

Conclusion: 提出了一种面向本征三维潜空间的世界模型Terra，通过点到高斯（P2G）变分自编码器与稀疏点流匹配网络实现3D表示与生成，提升多视图一致性与建模效率，实验在ScanNet v2上取得优异重建与生成表现。

Abstract: World models have garnered increasing attention for comprehensive modeling of
the real world. However, most existing methods still rely on pixel-aligned
representations as the basis for world evolution, neglecting the inherent 3D
nature of the physical world. This could undermine the 3D consistency and
diminish the modeling efficiency of world models. In this paper, we present
Terra, a native 3D world model that represents and generates explorable
environments in an intrinsic 3D latent space. Specifically, we propose a novel
point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into
a latent point representation, which is subsequently decoded as 3D Gaussian
primitives to jointly model geometry and appearance. We then introduce a sparse
point flow matching network (SPFlow) for generating the latent point
representation, which simultaneously denoises the positions and features of the
point latents. Our Terra enables exact multi-view consistency with native 3D
representation and architecture, and supports flexible rendering from any
viewpoint with only a single generation process. Furthermore, Terra achieves
explorable world modeling through progressive generation in the point latent
space. We conduct extensive experiments on the challenging indoor scenes from
ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction
and generation with high 3D consistency.

</details>


### [98] [Learning an Image Editing Model without Image Editing Pairs](https://arxiv.org/abs/2510.14978)
*Nupur Kumari,Sheng-Yu Wang,Nanxuan Zhao,Yotam Nitzan,Yuheng Li,Krishna Kumar Singh,Richard Zhang,Eli Shechtman,Jun-Yan Zhu,Xun Huang*

Main category: cs.CV

TL;DR: 无需配对数据，通过展开少步扩散模型并用VLM提供的指令遵循与保持不变性反馈进行端到端训练，辅以分布匹配损失以保证视觉质量，达到了与有监督训练相当或更优的编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型依赖大量输入-目标图像配对进行监督微调，但这类自然配对难以规模化收集；使用合成配对会放大预训练模型的伪影。目标是消除对配对数据的依赖，直接利用VLM做为评价器进行训练。

Method: 在训练中对少步扩散编辑模型进行反向传播，使用VLM评估生成图像是否遵循编辑指令并保持未改动内容，从而获得直接梯度进行端到端优化；同时引入分布匹配损失（DMD）以保持生成图像的视觉真实度，使其留在预训练模型学习到的图像流形内。

Result: 在标准基准和消融研究中验证，在无任何配对数据条件下，本方法在少步设置下与使用大量监督配对数据训练的扩散图像编辑模型表现相当；并在给定相同VLM作为奖励模型的前提下，优于基于强化学习的方法如Flow-GRPO。

Conclusion: 该论文提出了一种无配对数据训练范式，通过在训练时展开（unroll）少步扩散模型并利用视觉-语言模型（VLM）提供的反馈来直接优化编辑过程。

Abstract: Recent image editing models have achieved impressive results while following
natural language editing instructions, but they rely on supervised fine-tuning
with large datasets of input-target pairs. This is a critical bottleneck, as
such naturally occurring pairs are hard to curate at scale. Current workarounds
use synthetic training pairs that leverage the zero-shot capabilities of
existing models. However, this can propagate and magnify the artifacts of the
pretrained model into the final trained model. In this work, we present a new
training paradigm that eliminates the need for paired data entirely. Our
approach directly optimizes a few-step diffusion model by unrolling it during
training and leveraging feedback from vision-language models (VLMs). For each
input and editing instruction, the VLM evaluates if an edit follows the
instruction and preserves unchanged content, providing direct gradients for
end-to-end optimization. To ensure visual fidelity, we incorporate distribution
matching loss (DMD), which constrains generated images to remain within the
image manifold learned by pretrained models. We evaluate our method on standard
benchmarks and include an extensive ablation study. Without any paired data,
our method performs on par with various image editing diffusion models trained
on extensive supervised paired data, under the few-step setting. Given the same
VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.

</details>


### [99] [From Pixels to Words -- Towards Native Vision-Language Primitives at Scale](https://arxiv.org/abs/2510.14979)
*Haiwen Diao,Mingxuan Li,Silei Wu,Linjun Dai,Xiaohua Wang,Hanming Deng,Lewei Lu,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出NEO，一种遵循若干原语构建的原生视觉-语言模型；使用3.9亿图文对训练的单一密集架构能在多任务上接近模块化VLM表现，同时提供可复用组件与开源实现以推动研究民主化。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决两个问题：1) 明确原生VLM相较于模块化VLM的基本局限与可克服程度；2) 让原生VLM的研究更加可及与民主化，从而加速该领域进展。作者旨在提出清晰指导原则并提供一个可扩展、成本有效且可重用的原生VLM基石。

Method: 作者首先明确原生VLM的设计原则：在共享语义空间中对齐像素与词表示；无缝整合视觉与语言模块的优势；内在具备支持统一编码、对齐和推理的跨模态特性。在此基础上，设计并实现了NEO系列模型，一个单一密集的端到端架构，采用精心设计的原语（具体细节摘要中未给出）来缓解视觉-语言冲突。使用约3.9亿图文对进行训练，并提供配套的可复用组件与开源实现。

Result: 在论文中，作者报告NEO在多种实际场景下能与顶级模块化模型竞争，表明在仅使用390M图文对的情况下，NEO能从头高效学习视觉感知并缓解视觉-语言冲突。并且开源了代码与模型以促进社区复现。具体的量化指标、基线对照、任务与测试集未在摘要中阐明。

Conclusion: 该论文提出了NEO，一种从原理出发构建的原生视觉-语言模型（native VLM），旨在缩小与模块化VLMs的差距并推动其民主化。作者声称NEO通过一些原语在单一密集模型中高效对齐像素与词表示、集成视觉与语言模块的优势，并具备支持统一编码、对齐和推理的跨模态性质。使用390M图文对训练，NEO可从零开始学习视觉感知，表现可与顶级模块化模型媲美，并提供可复用组件与开源代码模型。

Abstract: The edifice of native Vision-Language Models (VLMs) has emerged as a rising
contender to typical modular VLMs, shaped by evolving model architectures and
training paradigms. Yet, two lingering clouds cast shadows over its widespread
exploration and promotion: (-) What fundamental constraints set native VLMs
apart from modular ones, and to what extent can these barriers be overcome? (-)
How to make research in native VLMs more accessible and democratized, thereby
accelerating progress in the field. In this paper, we clarify these challenges
and outline guiding principles for constructing native VLMs. Specifically, one
native VLM primitive should: (i) effectively align pixel and word
representations within a shared semantic space; (ii) seamlessly integrate the
strengths of formerly separate vision and language modules; (iii) inherently
embody various cross-modal properties that support unified vision-language
encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of
native VLMs built from first principles, capable of rivaling top-tier modular
counterparts across diverse real-world scenarios. With only 390M image-text
examples, NEO efficiently develops visual perception from scratch while
mitigating vision-language conflicts inside a dense and monolithic model
crafted from our elaborate primitives. We position NEO as a cornerstone for
scalable and powerful native VLMs, paired with a rich set of reusable
components that foster a cost-effective and extensible ecosystem. Our code and
models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.

</details>


### [100] [Coupled Diffusion Sampling for Training-Free Multi-View Image Editing](https://arxiv.org/abs/2510.14981)
*Hadi Alzayer,Yunzhi Zhang,Chen Geng,Jia-Bin Huang,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出耦合扩散采样，在推理时并行从多视图分布和2D编辑分布采样并通过耦合项约束，以隐式3D正则化实现多视图一致性编辑，无需显式3D优化，兼容多模型。


<details>
  <summary>Details</summary>
Motivation: 现有2D编辑模型能单张产生高质量编辑，但无法保证多视图一致性；已有方法通过显式3D表示进行优化，但代价高、在稀疏视角下不稳定。因此需要一种无需显式3D优化、在推理阶段即可保证多视图一致性的通用方案。

Method: 方法是耦合扩散采样：在采样时同时从预训练的多视图图像分布和2D编辑图像分布并行采样两条轨迹，利用耦合项将生成的2D图像序列约束到多视图一致性，从而作为隐式3D正则化。该方法为推理时算法，无需训练或昂贵的优化，兼容不同2D编辑模型。

Result: 论文在三类多视图图像编辑任务上验证了方法的有效性与通用性，展示了适用于多种模型架构的潜力，能在不同任务上提高视图一致性同时保持编辑质量。

Conclusion: 该论文提出了一种在推理阶段利用扩散模型进行多视图一致性图像编辑的方法，通过耦合扩散采样在多视图图像分布和2D编辑图像分布之间并行采样并加入耦合项，从而在不显式构建3D表示或昂贵优化的情况下实现视图一致性。

Abstract: We present an inference-time diffusion sampling method to perform multi-view
consistent image editing using pre-trained 2D image editing models. These
models can independently produce high-quality edits for each image in a set of
multi-view images of a 3D scene or object, but they do not maintain consistency
across views. Existing approaches typically address this by optimizing over
explicit 3D representations, but they suffer from a lengthy optimization
process and instability under sparse view settings. We propose an implicit 3D
regularization approach by constraining the generated 2D image sequences to
adhere to a pre-trained multi-view image distribution. This is achieved through
coupled diffusion sampling, a simple diffusion sampling technique that
concurrently samples two trajectories from both a multi-view image distribution
and a 2D edited image distribution, using a coupling term to enforce the
multi-view consistency among the generated images. We validate the
effectiveness and generality of this framework on three distinct multi-view
image editing tasks, demonstrating its applicability across various model
architectures and highlighting its potential as a general solution for
multi-view consistent editing.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [101] [Towards a Multimodal Stream Processing System](https://arxiv.org/abs/2510.14631)
*Uélison Jean Lopes dos Santos,Alessandro Ferri,Szilard Nistor,Riccardo Tommasini,Carsten Binnig,Manisha Luthra*

Main category: cs.DB

TL;DR: 将MLLM作为流处理算子并通过逻辑/物理/语义层优化显著提升多模态流查询性能，原型验证性能提升>10×，并列出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有将MLLMs集成进数据库以支持多模态查询的工作并不能直接应用于流处理场景，因为流处理对延迟和吞吐量有严格要求，需专门的设计与优化。

Method: 通过在逻辑、物理和语义层面引入一系列新颖优化与查询变换，减少对模型的调用负载以提高吞吐量，同时保持查询准确性；并实现了原型系统（文中称为\system{}）来验证这些优化。

Result: 原型系统在采用这些优化后性能提高超过一个数量级（即十倍以上），并提出了一个研究路线图，指出构建可扩展高效的多模态流处理系统的若干开放挑战。

Conclusion: 本文提出在流处理系统中将多模态大型语言模型（MLLMs）作为一等算子，从而支持跨模态的实时查询处理，表明这是可行且能显著提升性能。

Abstract: In this paper, we present a vision for a new generation of multimodal
streaming systems that embed MLLMs as first-class operators, enabling real-time
query processing across multiple modalities. Achieving this is non-trivial:
while recent work has integrated MLLMs into databases for multimodal queries,
streaming systems require fundamentally different approaches due to their
strict latency and throughput requirements. Our approach proposes novel
optimizations at all levels, including logical, physical, and semantic query
transformations that reduce model load to improve throughput while preserving
accuracy. We demonstrate this with \system{}, a prototype leveraging such
optimizations to improve performance by more than an order of magnitude.
Moreover, we discuss a research roadmap that outlines open research challenges
for building a scalable and efficient multimodal stream processing systems.

</details>
