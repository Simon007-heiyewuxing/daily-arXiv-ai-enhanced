<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 55]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Text-Driven 3D Hand Motion Generation from Sign Language Data](https://arxiv.org/abs/2508.15902)
*Léore Bensabath,Mathis Petrovich,Gül Varol*

Main category: cs.CV

TL;DR: 作者通过将大规模手语视频的伪标签经LLM和词典/脚本转换为手部动作描述，构建训练集并训练扩散模型HandMDM，实现从文本生成鲁棒的3D手部动作，具备跨类别和跨语言的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模带文本描述的3D手部动作数据，限制了从自然语言生成精细手部运动的能力。作者希望通过自动化标注管线扩大数据规模，以便训练能够理解复杂手部属性与运动的生成模型。

Method: 作者利用大规模手语视频数据集及其嘈杂的伪注释类别，结合手语词典中手部属性与运动脚本提示，通过大型语言模型将类别标签翻译为手部动作描述，从而自动构建训练数据。基于此数据训练了文本条件的手部动作扩散模型HandMDM，并在多域（同一手语中未见类别、其他手语及非手语动作）上评估其泛化性。

Result: 利用自动构建的数据训练的HandMDM在多个场景下表现出对未见类别和跨语言、跨域手部动作的鲁棒性。论文提供了详尽的实验分析，并计划公开模型与数据以推动该领域研究。

Conclusion: 本文提出了一个基于文本条件的3D手部动作生成方法，通过构建大规模的“手部动作–文本标签”对并训练扩散模型，实现从自然语言描述生成多样且鲁棒的手部动作。

Abstract: Our goal is to train a generative model of 3D hand motions, conditioned on
natural language descriptions specifying motion characteristics such as
handshapes, locations, finger/hand/arm movements. To this end, we automatically
build pairs of 3D hand motions and their associated textual labels with
unprecedented scale. Specifically, we leverage a large-scale sign language
video dataset, along with noisy pseudo-annotated sign categories, which we
translate into hand motion descriptions via an LLM that utilizes a dictionary
of sign attributes, as well as our complementary motion-script cues. This data
enables training a text-conditioned hand motion diffusion model HandMDM, that
is robust across domains such as unseen sign categories from the same sign
language, but also signs from another sign language and non-sign hand
movements. We contribute extensive experimental investigation of these
scenarios and will make our trained models and data publicly available to
support future research in this relatively new field.

</details>


### [2] [VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos](https://arxiv.org/abs/2508.15903)
*Kaining Li,Shuwei He,Zihan Xu*

Main category: cs.CV

TL;DR: 提出VT-LVLM-AR：用VTEM将长视频转为语义事件序列，结合冻结的LLaVA-1.5与P-Tuning实现高效、可解释的细粒度动作识别，并在NTU数据集上达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度模型在长时序视频上计算开销大、难以捕捉长距离时序依赖且语义理解有限；LVLM具备强大多模态理解与推理能力，但直接应用于连续视频的细粒度动作识别仍存在瓶颈，本工作旨在弥合视频与LVLM的语义鸿沟。

Method: 设计并实现了Video-to-Event Mapper(VTEM)进行轻量时空特征提取、自适应时间池化与概念量化（带事件一致性偏置），输出视觉事件序列；随后将序列输入冻结的LLaVA-1.5并用P-Tuning v2进行参数高效的Prompt微调用于动作分类。

Result: 在NTU RGB+D与NTU RGB+D 120数据集上取得SOTA成绩（例如NTU RGB+D X-Sub上94.1%），消融实验表明VTEM各组件及P-Tuning对性能贡献显著，人类评估支持视觉事件表示的可解释性。

Conclusion: VT-LVLM-AR通过将视频转为语义丰富的视觉事件序列，并利用冻结的LVLM（LLaVA-1.5）结合P-Tuning进行微调，有效提升了长视频中细粒度动作识别的性能与可解释性。

Abstract: Human action recognition in long-term videos, characterized by complex
backgrounds and subtle action differences, poses significant challenges for
traditional deep learning models due to computational overhead, difficulty in
capturing long-range temporal dependencies, and limited semantic understanding.
While Large Language Models (LLMs) and Large Vision-Language Models (LVLMs)
have shown remarkable capabilities in multi-modal understanding and reasoning,
their direct application to continuous video streams for fine-grained action
recognition remains an open problem. This paper introduces VT-LVLM-AR
(Video-Temporal Large Vision-Language Model Adapter for Action Recognition), a
novel framework designed to bridge this gap. VT-LVLM-AR comprises a
Video-to-Event Mapper (VTEM) that efficiently transforms raw video into
compact, semantically rich, and temporally coherent "visual event sequences"
through lightweight spatio-temporal feature extraction, adaptive temporal
pooling, and conceptual quantization with an event coherence bias. These visual
event sequences are then fed into an LVLM-based Action Reasoning module,
specifically a frozen LLaVA-1.5 model, adapted using parameter-efficient Prompt
Tuning (P-Tuning v2) for action classification. Comprehensive evaluations on
the NTU RGB+D and NTU RGB+D 120 datasets demonstrate that VT-LVLM-AR
consistently achieves state-of-the-art performance, surpassing existing methods
(e.g., 94.1% accuracy on NTU RGB+D X-Sub). Ablation studies confirm the
critical contributions of VTEM's components and the efficacy of Prompt Tuning,
while human evaluations underscore the interpretability of our visual event
representations. This work highlights the immense potential of leveraging LVLMs
for robust and interpretable video action understanding through effective
video-to-language translation and efficient model adaptation.

</details>


### [3] [Boosting Pathology Foundation Models via Few-shot Prompt-tuning for Rare Cancer Subtyping](https://arxiv.org/abs/2508.15904)
*Dexuan He,Xiao Zhou,Wenbin Guan,Liyuan Zhang,Xiaoman Zhang,Sinuo Xu,Ge Wang,Lifeng Wang,Xiaojun Yuan,Xin Sun,Yanfeng Wang,Kun Sun,Ya Zhang,Weidi Xie*

Main category: cs.CV

TL;DR: PathPT通过将VL模型零样本能力与空间感知聚合和提示调优结合，能在少样本条件下为稀有癌症分型与病灶定位提供可扩展、高效且更可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 稀有癌症样本稀缺且专家有限，传统仅基于视觉的MIL方法忽视跨模态知识和可解释性，导致在稀有亚型分型和病灶定位上性能不足。利用VL基础模型的零样本能力和提示策略可以在少样本场景下改善分型与定位。

Method: 提出PathPT框架：1) 使用视觉-语言（VL）病理基础模型的零样本推理把WSI的弱监督标签传播到图像瓷砖，生成细粒度伪标签；2) 对视觉特征进行空间感知聚合，保留病灶位置信息；3) 采用任务特定的提示调优（prompt tuning），将语义级提示与病理学特征对齐，实现跨模态推理；4) 在多模态下结合改进的多实例学习（MIL）策略进行训练与推理。

Result: 在8个稀有癌症数据集（4个成人、4个儿科，56个亚型，2,910张WSI）及3个常见癌症数据集上进行基准测试。在三种少样本设置下，PathPT相比四种MIL基线和四个SOTA VL模型，持续实现更高的亚型分类准确率以及更强的病变区域定位性能，显示出显著提升。

Conclusion: PathPT在稀有癌症组织病理学分型任务中表现优异，通过引入基于视觉-语言基础模型的零样本能力与提示调优，将WSI级别监督转化为瓷砖级别的细粒度引导，显著提升了亚型识别准确率与病变定位能力。

Abstract: Rare cancers comprise 20-25% of all malignancies but face major diagnostic
challenges due to limited expert availability-especially in pediatric oncology,
where they represent over 70% of cases. While pathology vision-language (VL)
foundation models show promising zero-shot capabilities for common cancer
subtyping, their clinical performance for rare cancers remains limited.
Existing multi-instance learning (MIL) methods rely only on visual features,
overlooking cross-modal knowledge and compromising interpretability critical
for rare cancer diagnosis. To address this limitation, we propose PathPT, a
novel framework that fully exploits the potential of vision-language pathology
foundation models through spatially-aware visual aggregation and task-specific
prompt tuning. Unlike conventional MIL, PathPT converts WSI-level supervision
into fine-grained tile-level guidance by leveraging the zero-shot capabilities
of VL models, thereby preserving localization on cancerous regions and enabling
cross-modal reasoning through prompts aligned with histopathological semantics.
We benchmark PathPT on eight rare cancer datasets(four adult and four
pediatric) spanning 56 subtypes and 2,910 WSIs, as well as three common cancer
datasets, evaluating four state-of-the-art VL models and four MIL frameworks
under three few-shot settings. Results show that PathPT consistently delivers
superior performance, achieving substantial gains in subtyping accuracy and
cancerous region grounding ability. This work advances AI-assisted diagnosis
for rare cancers, offering a scalable solution for improving subtyping accuracy
in settings with limited access to specialized expertise.

</details>


### [4] [Semantic-Aware Ship Detection with Vision-Language Integration](https://arxiv.org/abs/2508.15930)
*Jiahao Li,Jiancheng Pan,Yuze Sun,Xiaomeng Huang*

Main category: cs.CV

TL;DR: 提出SASD：将VLM与多尺度自适应滑动窗口结合，配套ShipSem-VL细粒度数据集，通过三项任务验证，在复杂遥感船舶检测中提升语义感知与检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以获取细粒度语义信息，影响复杂场景下的船舶检测效果，因此引入VLM以利用语言表达的语义能力并结合多尺度窗口提高局部细节捕获能力。

Method: 构建了一个结合VLM和多尺度自适应滑动窗口的检测框架；构建并标注ShipSem-VL数据集以提供丰富的细粒度船舶属性标签；设计三项任务（可能为检测、属性识别和语义检索或问答）来评估框架。

Result: 在三个任务上的实验表明，该框架在细粒度语义识别和检测准确性方面有明显提升，证明ShipSem-VL数据集和所提方法能有效推进语义感知的船舶检测。

Conclusion: 该论文提出将视觉-语言模型与多尺度自适应滑动窗口结合，用于提高遥感影像中船舶检测的语义感知能力，能在复杂场景下捕获细粒度属性，较现有方法性能更优。

Abstract: Ship detection in remote sensing imagery is a critical task with wide-ranging
applications, such as maritime activity monitoring, shipping logistics, and
environmental studies. However, existing methods often struggle to capture
fine-grained semantic information, limiting their effectiveness in complex
scenarios. To address these challenges, we propose a novel detection framework
that combines Vision-Language Models (VLMs) with a multi-scale adaptive sliding
window strategy. To facilitate Semantic-Aware Ship Detection (SASD), we
introduce ShipSem-VL, a specialized Vision-Language dataset designed to capture
fine-grained ship attributes. We evaluate our framework through three
well-defined tasks, providing a comprehensive analysis of its performance and
demonstrating its effectiveness in advancing SASD from multiple perspectives.

</details>


### [5] [Automatic Retrieval of Specific Cows from Unlabeled Videos](https://arxiv.org/abs/2508.15945)
*Jiawen Lyu,Manu Ramesh,Madison Simonds,Jacquelyn P. Boerman,Amy R. Reibman*

Main category: cs.CV

TL;DR: 提出一套由AutoCattloger、非深度学习识别器与CowFinder构成的自动化牛只编目与识别系统，能用每头牛一段视频建立目录并在无约束视频流中识别个体，适用于挤奶场景。


<details>
  <summary>Details</summary>
Motivation: 现有公开文献中鲜有自动化视频系统能实现免手动干预的牛只编目与识别；因此提出一个无需深度学习、能用单视频样本建立样本库并在连续视频流中识别牛只的系统。

Method: 系统由三部分组成：AutoCattloger负责从每头牛的单个视频片段构建牛只目录（Cattlog）；eidetic cow recognizer基于非深度学习的方法进行识别；CowFinder在连续未标注、未分割的视频流中定位并识别牛只。

Result: 系统在实际挤奶等场景中对未标注、未分割的视频流表现出能够发现并识别个体的能力，证明其在田间应用的可行性。

Conclusion: 该系统实现了基于单个视频样本的无监督牛只编目与识别，且在非约束视频流中能成功定位和识别个体，展示了在挤奶场景下的实用性。

Abstract: Few automated video systems are described in the open literature that enable
hands-free cataloging and identification (ID) of cows in a dairy herd. In this
work, we describe our system, composed of an AutoCattloger, which builds a
Cattlog of dairy cows in a herd with a single input video clip per cow, an
eidetic cow recognizer which uses no deep learning to ID cows, and a CowFinder,
which IDs cows in a continuous stream of video. We demonstrate its value in
finding individuals in unlabeled, unsegmented videos of cows walking
unconstrained through the holding area of a milking parlor.

</details>


### [6] [Investigating Different Geo Priors for Image Classification](https://arxiv.org/abs/2508.15946)
*Angela Zhu,Christian Lange,Max Hamilton*

Main category: cs.CV

TL;DR: 研究用SINR作为地理先验来辅助iNaturalist图像分类，评估不同模型配置及对训练外物种的处理，发现提升分类的关键因素与制作精确范围图的要求不同，且如何处理未训练物种对性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 利用物种发生的空间分布作为先验，可以提升基于图像的物种识别，特别是在具有地理位置信息的观察（如iNaturalist）中。探索SINR作为隐式地理先验的可行性及其限制。

Method: 评估多种SINR模型配置，比较不同超参、特征表示与训练方式；并对如何对待不在Geo Prior训练集内的物种（如平滑、退化或基于距离的分配）做实验。采用iNaturalist观测数据，将地理先验与视觉分类模型结合，测量分类性能提升和范围图准确性。

Result: 发现一些模型配置在提升视觉分类性能方面效果显著，但这些成功因素并非完全等同于制作精确分布图所需的因素；另外，对训练外物种的预测处理策略对分类性能影响较大。

Conclusion: 这些SINR模型可作为视觉物种分类的有效地理先验，但其表现依赖于模型配置、训练物种覆盖度以及对训练外物种的预测处理策略。

Abstract: Species distribution models encode spatial patterns of species occurrence
making them effective priors for vision-based species classification when
location information is available. In this study, we evaluate various SINR
(Spatial Implicit Neural Representations) models as a geographical prior for
visual classification of species from iNaturalist observations. We explore the
impact of different model configurations and adjust how we handle predictions
for species not included in Geo Prior training. Our analysis reveals factors
that contribute to the effectiveness of these models as Geo Priors, factors
that may differ from making accurate range maps.

</details>


### [7] [Representation Learning with Adaptive Superpixel Coding](https://arxiv.org/abs/2508.15959)
*Mahmoud Khalil,Ahmad Khalil,Alioune Ngom*

Main category: cs.CV

TL;DR: ASC replaces fixed patches with adaptive superpixel tokenization in self-supervised transformers, yielding better performance on image benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing vision models rely on grid structures and fixed patches which are not adaptive to image content; need for modality-agnostic, content-aware tokenization to improve representation.

Method: Introduce adaptive superpixel layers to replace fixed-size patch partitioning; use self-supervised training on transformer backbone; analyze properties and evaluate on downstream image tasks.

Result: ASC outperforms widely-used alternatives on standard image downstream task benchmarks, demonstrating effectiveness of adaptive superpixel coding.

Conclusion: ASC shows that adaptive, content-aware tokenization improves transformer-based vision models, outperforming fixed-grid patch methods on standard benchmarks, and suggesting a promising direction for modality-agnostic design.

Abstract: Deep learning vision models are typically tailored for specific modalities
and often rely on domain-specific assumptions, such as the grid structures used
by nearly all existing vision models. In this work, we propose a
self-supervised model based on Transformers, which we call Adaptive Superpixel
Coding (ASC). The key insight of our model is to overcome the limitations of
traditional Vision Transformers, which depend on fixed-size and non-adaptive
patch partitioning. Instead, ASC employs adaptive superpixel layers that
dynamically adjust to the underlying image content. We analyze key properties
of the approach that make it effective, and find that our method outperforms
widely-used alternatives on standard image downstream task benchmarks.

</details>


### [8] [Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification](https://arxiv.org/abs/2508.15960)
*Zhenhao Guo,Rachit Saluja,Tianyuan Yao,Quan Liu,Yuankai Huo,Benjamin Liechty,David J. Pisapia,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: 本文提出Glo-VLMs，通过图文联合学习与少样本适配策略，将大预训练VLMs成功应用于细粒度肾小球分类，在每类仅8个样本时仍取得较高准确率和宏AUC。


<details>
  <summary>Details</summary>
Motivation: 细粒度肾小球亚型间形态差异细微且与临床术语难以精确对齐，在标注稀缺的医学场景中探究如何用大预训练模型提高诊断效果。

Method: 提出Glo-VLMs框架，结合病理图像与临床文本提示进行联合图文表示学习，比较多种VLM架构与适配策略，在few-shot（每类8-shot等）设定下进行系统评估与多类指标对比。

Result: 在8-shot每类的设置下，微调VLMs可实现0.7416准确率、0.9045宏AUC、0.5277 F1-score，表明在高度受限的监督下基础模型能被有效适配用于细粒度医学图像分类。

Conclusion: 基于大规模预训练视觉-语言模型（VLMs）在有限标注下可用于细粒度肾小球分类，能显著提升少样本学习场景下的诊断性能。

Abstract: Vision-language models (VLMs) have shown considerable potential in digital
pathology, yet their effectiveness remains limited for fine-grained,
disease-specific classification tasks such as distinguishing between glomerular
subtypes. The subtle morphological variations among these subtypes, combined
with the difficulty of aligning visual patterns with precise clinical
terminology, make automated diagnosis in renal pathology particularly
challenging. In this work, we explore how large pretrained VLMs can be
effectively adapted to perform fine-grained glomerular classification, even in
scenarios where only a small number of labeled examples are available. In this
work, we introduce Glo-VLMs, a systematic framework designed to explore the
adaptation of VLMs to fine-grained glomerular classification in
data-constrained settings. Our approach leverages curated pathology images
alongside clinical text prompts to facilitate joint image-text representation
learning for nuanced renal pathology subtypes. By assessing various VLMs
architectures and adaptation strategies under a few-shot learning paradigm, we
explore how both the choice of method and the amount of labeled data impact
model performance in clinically relevant scenarios. To ensure a fair
comparison, we evaluate all models using standardized multi-class metrics,
aiming to clarify the practical requirements and potential of large pretrained
models for specialized clinical research applications. As a result, fine-tuning
the VLMs achieved 0.7416 accuracy, 0.9045 macro-AUC, and 0.5277 F1-score with
only 8 shots per class, demonstrating that even with highly limited
supervision, foundation models can be effectively adapted for fine-grained
medical image classification.

</details>


### [9] [Contributions to Label-Efficient Learning in Computer Vision and Remote Sensing](https://arxiv.org/abs/2508.15973)
*Minh-Tan Pham*

Main category: cs.CV

TL;DR: 该手稿系统综述并贡献一系列面向计算机视觉与遥感的标注高效学习方法，涵盖弱监督、多任务、自监督对比学习与少样本层次建模，实验证明在多数据集上均有效，并展望其向大规模与实际应用的延展。


<details>
  <summary>Details</summary>
Motivation: 现实应用中标注成本高且数据往往部分标注或未标注，尤其是遥感领域存在多模态、分辨率差异与场景异质性，迫切需要标注高效的学习方法以充分利用大量未标注数据并适配领域特性。

Method: 提出并集成多种标注高效学习方法：1) 基于异常感知表征的弱监督目标发现与检测，从大量背景图像中学习区分异常实例；2) 多任务学习框架，在不同数据集具有不重叠注释的情况下联合训练以提升目标检测与语义分割性能；3) 基于自监督与有监督对比学习的多模态表征学习，增强遥感场景分类；4) 结合显式与隐式层次建模的少样本学习用于层次化场景分类。

Result: 在多个人工与遥感数据集上通过大量实验验证：弱监督异常检测提高对象发现召回与定位准确性；多任务联合训练在检测与分割任务上带来稳定增益；多模态对比学习提升场景分类鲁棒性与泛化；少样本层次分类在类间层次结构上获得更好样本效率。

Conclusion: 该手稿围绕在计算机视觉与遥感中实现标注高效学习的方法展开，通过弱监督、多任务、自监督与少样本学习等策略，提高在标注稀缺或部分标注场景下的性能，并针对地球观测数据的多模态、分辨率与场景异质性提出领域适配方案。综合实验证明所提方法在自然图像与遥感数据集上均有显著提升，未来将朝着大规模扩展与实际应用部署方向发展。

Abstract: This manuscript presents a series of my selected contributions to the topic
of label-efficient learning in computer vision and remote sensing. The central
focus of this research is to develop and adapt methods that can learn
effectively from limited or partially annotated data, and can leverage abundant
unlabeled data in real-world applications. The contributions span both
methodological developments and domain-specific adaptations, in particular
addressing challenges unique to Earth observation data such as multi-modality,
spatial resolution variability, and scene heterogeneity. The manuscript is
organized around four main axes including (1) weakly supervised learning for
object discovery and detection based on anomaly-aware representations learned
from large amounts of background images; (2) multi-task learning that jointly
trains on multiple datasets with disjoint annotations to improve performance on
object detection and semantic segmentation; (3) self-supervised and supervised
contrastive learning with multimodal data to enhance scene classification in
remote sensing; and (4) few-shot learning for hierarchical scene classification
using both explicit and implicit modeling of class hierarchies. These
contributions are supported by extensive experimental results across natural
and remote sensing datasets, reflecting the outcomes of several collaborative
research projects. The manuscript concludes by outlining ongoing and future
research directions focused on scaling and enhancing label-efficient learning
for real-world applications.

</details>


### [10] [Panoptic Segmentation of Environmental UAV Images : Litter Beach](https://arxiv.org/abs/2508.15985)
*Ousmane Youme,Jean Marie Dembélé,Eugene C. Ezin,Christophe Cambier*

Main category: cs.CV

TL;DR: 针对无人机沙滩图像中复杂背景带来的误检问题，作者用实例分割与全景分割替代简单CNN分类，少样本下提高了海洋垃圾检测的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统分类型CNN在沙滩场景下易受反光、纹理等干扰，导致误判；需要更精细的像素级识别以准确定位和计数塑料垃圾，且无人机图像分辨率高、适用于局部监测。

Method: 采用基于实例分割和全景分割的CNN模型，强调在少样本情况下保持较好准确率，利用分割方法减少由沙子反光、脚印、阴影、藻类、沙丘等干扰带来的误检。

Result: 模型在少量训练样本下能取得较好准确率，表现出更强的鲁棒性和减少误检，便于无人机进行海滩垃圾定位与计数。

Conclusion: 该论文提出在无人机图像上使用实例分割和全景分割方法以提高海洋垃圾（沙滩垃圾）检测的鲁棒性与精度。

Abstract: Convolutional neural networks (CNN) have been used efficiently in several
fields, including environmental challenges. In fact, CNN can help with the
monitoring of marine litter, which has become a worldwide problem. UAVs have
higher resolution and are more adaptable in local areas than satellite images,
making it easier to find and count trash. Since the sand is heterogeneous, a
basic CNN model encounters plenty of inferences caused by reflections of sand
color, human footsteps, shadows, algae present, dunes, holes, and tire tracks.
For these types of images, other CNN models, such as CNN-based segmentation
methods, may be more appropriate. In this paper, we use an instance-based
segmentation method and a panoptic segmentation method that show good accuracy
with just a few samples. The model is more robust and less

</details>


### [11] [Automated Multi-label Classification of Eleven Retinal Diseases: A Benchmark of Modern Architectures and a Meta-Ensemble on a Large Synthetic Dataset](https://arxiv.org/abs/2508.15986)
*Jerry Cao-Xue,Tien Comlekoglu,Keyi Xue,Guanliang Wang,Jiang Li,Gordon Laurie*

Main category: cs.CV

TL;DR: 利用百万级高保真合成视网膜图像训练的多标签模型能高精度分类多种病变，并在真实临床数据上展现良好泛化，证明合成数据可作为开发眼科AI系统的可行替代。


<details>
  <summary>Details</summary>
Motivation: 临床大规模标注视网膜图像稀缺且成本高，合成大规模数据（SynFundus-1M）可解决隐私与标注问题，需要建立基线以验证其在多标签病变分类与泛化能力。

Method: 构建端到端深度学习流水线，使用5折多标签分层交叉验证训练六种现代网络（ConvNeXtV2、SwinV2、ViT、ResNet、EfficientNetV2、RETFound），并采用堆叠外折预测与XGBoost的元集成器。评估包括内部验证与在三个真实临床数据集上的外部测试。

Result: 最终元集成模型在内部验证上取得宏AUC 0.9973；在真实数据集上表现为：综合DR数据集AUC 0.7972，AIROGS青光眼数据集AUC 0.9126，RFMiD多标签宏AUC 0.8800。

Conclusion: 作者证明了使用合成视网膜图像（SynFundus-1M）训练的多标签深度学习模型能够高精度分类多种视网膜疾病，并能较好泛化到真实临床数据集，提供了用于合成数据研究的基线。

Abstract: The development of multi-label deep learning models for retinal disease
classification is often hindered by the scarcity of large, expertly annotated
clinical datasets due to patient privacy concerns and high costs. The recent
release of SynFundus-1M, a high-fidelity synthetic dataset with over one
million fundus images, presents a novel opportunity to overcome these barriers.
To establish a foundational performance benchmark for this new resource, we
developed an end-to-end deep learning pipeline, training six modern
architectures (ConvNeXtV2, SwinV2, ViT, ResNet, EfficientNetV2, and the
RETFound foundation model) to classify eleven retinal diseases using a 5-fold
multi-label stratified cross-validation strategy. We further developed a
meta-ensemble model by stacking the out-of-fold predictions with an XGBoost
classifier. Our final ensemble model achieved the highest performance on the
internal validation set, with a macro-average Area Under the Receiver Operating
Characteristic Curve (AUC) of 0.9973. Critically, the models demonstrated
strong generalization to three diverse, real-world clinical datasets, achieving
an AUC of 0.7972 on a combined DR dataset, an AUC of 0.9126 on the AIROGS
glaucoma dataset and a macro-AUC of 0.8800 on the multi-label RFMiD dataset.
This work provides a robust baseline for future research on large-scale
synthetic datasets and establishes that models trained exclusively on synthetic
data can accurately classify multiple pathologies and generalize effectively to
real clinical images, offering a viable pathway to accelerate the development
of comprehensive AI systems in ophthalmology.

</details>


### [12] [Diverse Signer Avatars with Manual and Non-Manual Feature Modelling for Sign Language Production](https://arxiv.org/abs/2508.15988)
*Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 提出LDM驱动的手语生成框架与手/非手特征聚合模块，提升了手语生成的多样性和视觉质量，在YouTube-SL-25上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成模型难以在保持视觉质量的同时捕捉表现多样性和非手部属性（如情感），尤其在使用不同种族背景的参考图像时容易丢失语言内容或视觉一致性。

Method: 首先使用LDM从参考图像生成高质量面部/全身图像作为头像，再设计了一个新的手语特征聚合模块，分别显式建模非手部特征（面部表情等）和手部特征，最后将聚合特征用于驱动数字化身合成。

Result: 在YouTube-SL-25数据集上的实验显示，提出的方法在感知质量指标上显著优于现有SOTA方法，同时能够利用不同族裔的参考图像产生多样化且保持语言内容的输出。

Conclusion: 本论文提出了一种基于潜在扩散模型（LDM）的手语生成流水线，通过生成参考图像并合成逼真的数字化身，以提升手语生成的多样性与视觉质量。研究表明，在YouTube-SL-25数据集上，该方法在感知质量指标上显著优于现有方法。

Abstract: The diversity of sign representation is essential for Sign Language
Production (SLP) as it captures variations in appearance, facial expressions,
and hand movements. However, existing SLP models are often unable to capture
diversity while preserving visual quality and modelling non-manual attributes
such as emotions. To address this problem, we propose a novel approach that
leverages Latent Diffusion Model (LDM) to synthesise photorealistic digital
avatars from a generated reference image. We propose a novel sign feature
aggregation module that explicitly models the non-manual features
(\textit{e.g.}, the face) and the manual features (\textit{e.g.}, the hands).
We show that our proposed module ensures the preservation of linguistic content
while seamlessly using reference images with different ethnic backgrounds to
ensure diversity. Experiments on the YouTube-SL-25 sign language dataset show
that our pipeline achieves superior visual quality compared to state-of-the-art
methods, with significant improvements on perceptual metrics.

</details>


### [13] [DRespNeT: A UAV Dataset and YOLOv8-DRN Model for Aerial Instance Segmentation of Building Access Points for Post-Earthquake Search-and-Rescue Missions](https://arxiv.org/abs/2508.16016)
*Aykut Sirma,Angelos Plastropoulos,Argyrios Zolotas,Gilbert Tang*

Main category: cs.CV

TL;DR: 提出DRespNeT：一个针对地震后航拍实例分割的1080p高分辨率数据集，含28类细粒度多边形标注；配套优化的YOLOv8-DRN达到92.7% mAP50和27 FPS，促进实时搜救决策。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖卫星影像或粗粒度语义标签，无法满足搜救任务对可通行入口、障碍物、残骸等级等精细信息的需求，因此需要高分辨率、实例级别、操作性强的数据集以支持实时决策。

Method: 构建1080p灾区航拍视频数据集（包括2023土耳其地震等），进行逐帧多边形级实例标注（28类）；使用YOLOv8-seg并提出YOLOv8-DRN优化模型进行训练和推理评估，测量mAP50与FPS。

Result: DRespNeT包含28个关键类的细粒度多边形实例标注；YOLOv8-DRN在RTX-4090上实现92.7% mAP50和27 FPS的多目标检测速度，证明了数据集与模型在实时SAR场景中的可用性。

Conclusion: DRespNeT通过高分辨率航拍实例分割数据集填补了灾后城市环境感知的空白，提供了细粒度多类别多实例标注，支持实时模型部署以提升搜救效能。

Abstract: Recent advancements in computer vision and deep learning have enhanced
disaster-response capabilities, particularly in the rapid assessment of
earthquake-affected urban environments. Timely identification of accessible
entry points and structural obstacles is essential for effective
search-and-rescue (SAR) operations. To address this need, we introduce
DRespNeT, a high-resolution dataset specifically developed for aerial instance
segmentation of post-earthquake structural environments. Unlike existing
datasets, which rely heavily on satellite imagery or coarse semantic labeling,
DRespNeT provides detailed polygon-level instance segmentation annotations
derived from high-definition (1080p) aerial footage captured in disaster zones,
including the 2023 Turkiye earthquake and other impacted regions. The dataset
comprises 28 operationally critical classes, including structurally compromised
buildings, access points such as doors, windows, and gaps, multiple debris
levels, rescue personnel, vehicles, and civilian visibility. A distinctive
feature of DRespNeT is its fine-grained annotation detail, enabling
differentiation between accessible and obstructed areas, thereby improving
operational planning and response efficiency. Performance evaluations using
YOLO-based instance segmentation models, specifically YOLOv8-seg, demonstrate
significant gains in real-time situational awareness and decision-making. Our
optimized YOLOv8-DRN model achieves 92.7% mAP50 with an inference speed of 27
FPS on an RTX-4090 GPU for multi-target detection, meeting real-time
operational requirements. The dataset and models support SAR teams and robotic
systems, providing a foundation for enhancing human-robot collaboration,
streamlining emergency response, and improving survivor outcomes.

</details>


### [14] [NeuralMeshing: Complete Object Mesh Extraction from Casual Captures](https://arxiv.org/abs/2508.16026)
*Floris Erich,Naoya Chiba,Abdullah Mustafa,Ryo Hanai,Noriaki Ando,Yusuke Yoshiyasu,Yukiyasu Domae*

Main category: cs.CV

TL;DR: 利用每段视频中的一个已知点和SfM技术，该工作实现了多视频自动对齐与融合，从而生成无需洞填充的完整物体网格，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 目标是在没有商业3D扫描仪的情况下，利用普通摄像设备从多段视频中重建日常物体的完整几何模型。

Method: 系统要求每个视频至少有一帧标定出一个已知点（通过棋盘格或AR标记自动确定），其余帧通过传统Structure-from-Motion对齐到世界坐标，随后进行多视角点云/网格合并和优化以生成完整物体网格。

Result: 系统能够从多个视频中合并重建，生成完整对象网格而不依赖洞填充；并已开源代码于给定GitHub地址。

Conclusion: 该论文提出了一个从两个或更多视频自动生成物体几何模型的系统，利用已知点和SfM对帧进行世界坐标定位，通过多视频融合得到完整网格，避免了洞填充。

Abstract: How can we extract complete geometric models of objects that we encounter in
our daily life, without having access to commercial 3D scanners? In this paper
we present an automated system for generating geometric models of objects from
two or more videos. Our system requires the specification of one known point in
at least one frame of each video, which can be automatically determined using a
fiducial marker such as a checkerboard or Augmented Reality (AR) marker. The
remaining frames are automatically positioned in world space by using
Structure-from-Motion techniques. By using multiple videos and merging results,
a complete object mesh can be generated, without having to rely on hole
filling. Code for our system is available from
https://github.com/FlorisE/NeuralMeshing.

</details>


### [15] [CoVeRaP: Cooperative Vehicular Perception through mmWave FMCW Radars](https://arxiv.org/abs/2508.16030)
*Jinyue Song,Hansol Ku,Jayneel Vora,Nelson Lee,Ahmad Kamari,Prasant Mohapatra,Parth Pathak*

Main category: cs.CV

TL;DR: 发布了21k帧多车协同FMCW雷达-相机-GPS数据集CoVeRaP，提出中/后融合的协同感知网络，使用PointNet式多分支编码器与自注意力融合多模态雷达特征，显著提升3D检测，最高在IoU0.9下mAP提高9x，优于单车方法并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: FMCW雷达在雨天和强光等条件下稳定，但本身点云稀疏且噪声高，单车雷达检测性能受限。通过多车共享雷达点云，可补足视角与覆盖不足，提高检测鲁棒性。

Method: 构建了包含21k帧时间对齐雷达、相机、GPS的协同数据集CoVeRaP；提出统一的协同感知框架，包含中融合和后融合策略。基线网络使用多分支PointNet风格编码器，结合自注意力融合空间、速度（Doppler）和强度信息到共同潜在空间，解码器输出3D边界框和点级深度置信度。

Result: 在实验中，中融合并结合强度编码在IoU 0.9处将mAP提升最多达9倍，并在各项设置下稳步优于单车基线。公开数据集与代码，成为首个可复现的多车FMCW雷达感知基准。

Conclusion: 该论文展示了通过多车协同共享FMCW雷达数据显著提升三维目标检测精度，尤其在高IoU要求下改善明显，首次提出了可复现的多车雷达基准CoVeRaP。

Abstract: Automotive FMCW radars remain reliable in rain and glare, yet their sparse,
noisy point clouds constrain 3-D object detection. We therefore release
CoVeRaP, a 21 k-frame cooperative dataset that time-aligns radar, camera, and
GPS streams from multiple vehicles across diverse manoeuvres. Built on this
data, we propose a unified cooperative-perception framework with middle- and
late-fusion options. Its baseline network employs a multi-branch PointNet-style
encoder enhanced with self-attention to fuse spatial, Doppler, and intensity
cues into a common latent space, which a decoder converts into 3-D bounding
boxes and per-point depth confidence. Experiments show that middle fusion with
intensity encoding boosts mean Average Precision by up to 9x at IoU 0.9 and
consistently outperforms single-vehicle baselines. CoVeRaP thus establishes the
first reproducible benchmark for multi-vehicle FMCW-radar perception and
demonstrates that affordable radar sharing markedly improves detection
robustness. Dataset and code are publicly available to encourage further
research.

</details>


### [16] [Wavelet-Enhanced PaDiM for Industrial Anomaly Detection](https://arxiv.org/abs/2508.16034)
*Cory Gardner,Byungseok Min,Tae-Hyuk Ahn*

Main category: cs.CV

TL;DR: WE-PaDiM在PaDiM基础上引入2D小波变换对多层CNN特征按频带选择并拼接建模，替代随机通道选择。该方法在MVTec AD上对检测与定位均表现优异，且小波子带选择在检测/定位间存在明显权衡。


<details>
  <summary>Details</summary>
Motivation: PaDiM通过随机通道降维可能丢失有结构的频谱信息；引入小波变换可基于频率成分系统性选择特征，利用多尺度频率线索提升异常检测/定位性能同时保持高效。

Method: 对预训练CNN的多层特征图先进行2D DWT，选择若干感兴趣子带（如LL、LH、HL），对齐不同层的空间分辨率后按通道拼接，再用PaDiM框架对拼接特征建模（估计多元高斯分布并用马氏距离计算异常得分）。评估使用ResNet-18与EfficientNet B0–B6在MVTec AD上的性能，并进行小波基与子带选择分析。

Result: 在MVTec AD 15类上，按类优化配置下，平均Image-AUC为99.32%，Pixel-AUC为92.10%。分析发现：Haar等简单小波结合细节子带（HL或LH/HL/HH）常改善定位；LL近似带有助图像级检测。方法在效率上与PaDiM可比，并提供更可解释的特征选择策略。

Conclusion: WE-PaDiM通过在CNN多层特征上引入二维离散小波变换，并基于频带有选择地拼接子带特征，再用PaDiM的多元高斯建模，替代随机通道选择，从而更有结构性地保留与异常相关的多尺度频率信息。实验在MVTec AD上显示方法在图像与像素级异常检测/定位上都取得了强性能，且不同小波子带的选择在检测与定位间存在权衡：低频分量有利整体图像判别，高频细节子带提升定位精度。

Abstract: Anomaly detection and localization in industrial images are essential for
automated quality inspection. PaDiM, a prominent method, models the
distribution of normal image features extracted by pre-trained Convolutional
Neural Networks (CNNs) but reduces dimensionality through random channel
selection, potentially discarding structured information. We propose
Wavelet-Enhanced PaDiM (WE-PaDiM), which integrates Discrete Wavelet Transform
(DWT) analysis with multi-layer CNN features in a structured manner. WE-PaDiM
applies 2D DWT to feature maps from multiple backbone layers, selects specific
frequency subbands (e.g., LL, LH, HL), spatially aligns them, and concatenates
them channel-wise before modeling with PaDiM's multivariate Gaussian framework.
This DWT-before-concatenation strategy provides a principled method for feature
selection based on frequency content relevant to anomalies, leveraging
multi-scale wavelet information as an alternative to random selection. We
evaluate WE-PaDiM on the challenging MVTec AD dataset with multiple backbones
(ResNet-18 and EfficientNet B0-B6). The method achieves strong performance in
anomaly detection and localization, yielding average results of 99.32%
Image-AUC and 92.10% Pixel-AUC across 15 categories with per-class optimized
configurations. Our analysis shows that wavelet choices affect performance
trade-offs: simpler wavelets (e.g., Haar) with detail subbands (HL or LH/HL/HH)
often enhance localization, while approximation bands (LL) improve image-level
detection. WE-PaDiM thus offers a competitive and interpretable alternative to
random feature selection in PaDiM, achieving robust results suitable for
industrial inspection with comparable efficiency.

</details>


### [17] [Expandable Residual Approximation for Knowledge Distillation](https://arxiv.org/abs/2508.16050)
*Zhaoyi Yan,Binghui Chen,Yunfan Liu,Qixiang Ye*

Main category: cs.CV

TL;DR: 通过残差分解和教师权重重用，ERA以逐步逼近的方式简化知识蒸馏过程，在ImageNet和COCO上带来显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 教师与学生模型间的学习能力差距导致知识难以充分迁移，需将复杂的知识传递问题简化为可逐步逼近的子问题。

Method: 提出Expandable Residual Approximation (ERA)，使用Multi-Branched Residual Network (MBRNet)将残差知识逐步近似；引入Teacher Weight Integration (TWI)策略重用教师头部权重以缓解容量差距。

Result: 在ImageNet上Top-1准确率提升1.41%，在MS COCO上AP提升1.40，且在多项视觉任务中达到领先性能。

Conclusion: ERA通过将残差知识分解为多个步骤，降低学生拟合教师表示的难度，从而缓解了教师-学生间容量差距问题。结合教师权重重用策略，能更有效地传递高级语义信息。实验表明ERA在ImageNet和COCO等视觉任务上实现了显著提升。

Abstract: Knowledge distillation (KD) aims to transfer knowledge from a large-scale
teacher model to a lightweight one, significantly reducing computational and
storage requirements. However, the inherent learning capacity gap between the
teacher and student often hinders the sufficient transfer of knowledge,
motivating numerous studies to address this challenge. Inspired by the
progressive approximation principle in the Stone-Weierstrass theorem, we
propose Expandable Residual Approximation (ERA), a novel KD method that
decomposes the approximation of residual knowledge into multiple steps,
reducing the difficulty of mimicking the teacher's representation through a
divide-and-conquer approach. Specifically, ERA employs a Multi-Branched
Residual Network (MBRNet) to implement this residual knowledge decomposition.
Additionally, a Teacher Weight Integration (TWI) strategy is introduced to
mitigate the capacity disparity by reusing the teacher's head weights.
Extensive experiments show that ERA improves the Top-1 accuracy on the ImageNet
classification benchmark by 1.41% and the AP on the MS COCO object detection
benchmark by 1.40, as well as achieving leading performance across computer
vision tasks. Codes and models are available at
https://github.com/Zhaoyi-Yan/ERA.

</details>


### [18] [Advances and Trends in the 3D Reconstruction of the Shape and Motion of Animals](https://arxiv.org/abs/2508.16062)
*Ziqi Li,Abderraouf Amrani,Shri Rai,Hamid Laga*

Main category: cs.CV

TL;DR: 论文综述了利用RGB图像/视频进行动物3D几何、姿态与运动重建的深度学习方法，分类讨论输入模态、表示形式、重建技术与训练策略，总结优劣并指出数据匮乏、泛化能力弱及精细动态建模是未来研究重点。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描不可行或成本高昂，且难以在动物自然栖息环境中部署，因而兴起基于RGB图像/视频的非侵入性深度学习方法，应用广泛包括生物学研究、畜牧管理、动物保护以及数字内容创作等。

Method: 文章按输入模态（单目图像、多视、视频、含深度/多谱）、形状/运动表示（显式网格、参数化模板、体素、隐式神经场、骨骼+皮肤）、重建技术（监督学习、基于优化的推理、生成模型、基于物理的约束）、训练机制（合成数据、弱监督、无监督、跨域适配）进行分类与综述，并选取若干关键方法做定量/定性分析。

Result: 综述总结了现有方法在不同输入与表示下的性能差异，指出基于模板的方法在可解释性与稳定性上有优势，而隐式表示在细节恢复和拓扑灵活性上更强；同时指出大多数方法在跨物种泛化、稀缺标注数据和动态毛发/软组织建模上仍存在显著不足。

Conclusion: 该综述系统性梳理了基于深度学习的动物三维重建研究进展，强调从RGB图像/视频非侵入式重建的可行性与多样方法，并指出当前受限于数据、泛化性和精细形变建模等挑战，需要跨学科、更丰富数据与弱监督/自监督方法推动发展。

Abstract: Reconstructing the 3D geometry, pose, and motion of animals is a
long-standing problem, which has a wide range of applications, from biology,
livestock management, and animal conservation and welfare to content creation
in digital entertainment and Virtual/Augmented Reality (VR/AR). Traditionally,
3D models of real animals are obtained using 3D scanners. These, however, are
intrusive, often prohibitively expensive, and difficult to deploy in the
natural environment of the animals. In recent years, we have seen a significant
surge in deep learning-based techniques that enable the 3D reconstruction, in a
non-intrusive manner, of the shape and motion of dynamic objects just from
their RGB image and/or video observations. Several papers have explored their
application and extension to various types of animals. This paper surveys the
latest developments in this emerging and growing field of research. It
categorizes and discusses the state-of-the-art methods based on their input
modalities, the way the 3D geometry and motion of animals are represented, the
type of reconstruction techniques they use, and the training mechanisms they
adopt. It also analyzes the performance of some key methods, discusses their
strengths and limitations, and identifies current challenges and directions for
future research.

</details>


### [19] [A Unified Voxel Diffusion Module for Point Cloud 3D Object Detection](https://arxiv.org/abs/2508.16069)
*Qifeng Liu,Dawei Zhao,Yabo Dong,Linzhi Shang,Liang Xiao,Juan Wang,Kunkong Zhao,Dongming Lu,Qi Zhu*

Main category: cs.CV

TL;DR: 提出可插拔的Voxel Diffusion Module，通过稀疏3D卷积与submanifold卷积扩散并增强体素特征，计算高效并能提升Transformer/SSM点云检测器的精度，在多个基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和SSM的点云检测器采用体素化表示但序列化处理要求输入输出维度一致，缺乏卷积所带来的空间扩散能力，导致检测精度受限。作者受CNN检测结构启发，提出VDM以弥补该不足。

Method: VDM由稀疏3D卷积、submanifold稀疏卷积和残差连接组成，输出特征图下采样到原始输入分辨率的四分之一，以保证计算效率；其功能包括通过稀疏3D卷积扩散前景体素特征以丰富空间上下文，以及聚合细粒度空间信息以强化体素级特征表达。VDM可插拔地集成到Transformer或SSM检测框架中。

Result: 将VDM嵌入Transformer和SSM模型后，在多个基准数据集上均有稳定提升：VDM-SSM在Waymo上达74.7 mAPH (L2)，在nuScenes上72.9 NDS，在Argoverse 2上42.3 mAP，在ONCE上67.6 mAP，均为SOTA。

Conclusion: 本文提出的Voxel Diffusion Module (VDM)通过在体素层面引入稀疏3D卷积、子流形稀疏卷积及残差连接，有效增强了点云目标检测模型的空间扩散与特征表达能力，从而提升了Transformer与SSM为基础模型的检测性能。

Abstract: Recent advances in point cloud object detection have increasingly adopted
Transformer-based and State Space Models (SSMs), demonstrating strong
performance. However, voxelbased representations in these models require strict
consistency in input and output dimensions due to their serialized processing,
which limits the spatial diffusion capability typically offered by
convolutional operations. This limitation significantly affects detection
accuracy. Inspired by CNN-based object detection architectures, we propose a
novel Voxel Diffusion Module (VDM) to enhance voxel-level representation and
diffusion in point cloud data. VDM is composed of sparse 3D convolutions,
submanifold sparse convolutions, and residual connections. To ensure
computational efficiency, the output feature maps are downsampled to one-fourth
of the original input resolution. VDM serves two primary functions: (1)
diffusing foreground voxel features through sparse 3D convolutions to enrich
spatial context, and (2) aggregating fine-grained spatial information to
strengthen voxelwise feature representation. The enhanced voxel features
produced by VDM can be seamlessly integrated into mainstream Transformer- or
SSM-based detection models for accurate object classification and localization,
highlighting the generalizability of our method. We evaluate VDM on several
benchmark datasets by embedding it into both Transformerbased and SSM-based
models. Experimental results show that our approach consistently improves
detection accuracy over baseline models. Specifically, VDM-SSMs achieve 74.7
mAPH (L2) on Waymo, 72.9 NDS on nuScenes, 42.3 mAP on Argoverse 2, and 67.6 mAP
on ONCE, setting new stateof-the-art performance across all datasets. Our code
will be made publicly available.

</details>


### [20] [Ensemble learning of foundation models for precision oncology](https://arxiv.org/abs/2508.16085)
*Xiangde Luo,Xiyue Wang,Feyisope Eweje,Xiaoming Zhang,Sen Yang,Ryan Quinton,Jinxi Xiang,Yuchen Li,Yuanfeng Ji,Zhe Li,Yijiang Chen,Colin Bergstrom,Ted Kim,Francesca Maria Olguin,Kelley Yuan,Matthew Abikenari,Andrew Heider,Sierra Willens,Sanjeeth Rajaram,Robert West,Joel Neal,Maximilian Diehn,Ruijiang Li*

Main category: cs.CV

TL;DR: ELF通过集成五个病理基础模型并在大规模WSI上训练，得到统一切片级表示，能更稳健、有效地完成多种临床病理任务，特别适合数据有限的应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型训练数据和策略不一致，导致性能波动和泛化性受限；因此提出将多模型集成以获得更一致且通用的切片级表示，利于临床任务特别是样本稀缺的情境。

Method: 构建包含五个最先进病理基础模型的集成框架，使用53,699张覆盖20个解剖部位的WSI进行训练，采用滑片级（slide-level）表示学习，利用集成学习策略整合不同模型的互补信息并提高数据使用效率。

Result: 在病种分类、生物标志物检测和多类抗癌疗法（化疗、靶向、免疫疗法）反应预测等任务上，ELF优于其组成模型和现有切片级模型，表现出更高的准确性和稳健性。

Conclusion: ELF通过融合多种病理基础模型，生成统一的切片级表示，显著提升了在多任务和多癌种场景下的准确性与鲁棒性，尤其在有限数据的临床情境中具有优势。

Abstract: Histopathology is essential for disease diagnosis and treatment
decision-making. Recent advances in artificial intelligence (AI) have enabled
the development of pathology foundation models that learn rich visual
representations from large-scale whole-slide images (WSIs). However, existing
models are often trained on disparate datasets using varying strategies,
leading to inconsistent performance and limited generalizability. Here, we
introduce ELF (Ensemble Learning of Foundation models), a novel framework that
integrates five state-of-the-art pathology foundation models to generate
unified slide-level representations. Trained on 53,699 WSIs spanning 20
anatomical sites, ELF leverages ensemble learning to capture complementary
information from diverse models while maintaining high data efficiency. Unlike
traditional tile-level models, ELF's slide-level architecture is particularly
advantageous in clinical contexts where data are limited, such as therapeutic
response prediction. We evaluated ELF across a wide range of clinical
applications, including disease classification, biomarker detection, and
response prediction to major anticancer therapies, cytotoxic chemotherapy,
targeted therapy, and immunotherapy, across multiple cancer types. ELF
consistently outperformed all constituent foundation models and existing
slide-level models, demonstrating superior accuracy and robustness. Our results
highlight the power of ensemble learning for pathology foundation models and
suggest ELF as a scalable and generalizable solution for advancing AI-assisted
precision oncology.

</details>


### [21] [Two-flow Feedback Multi-scale Progressive Generative Adversarial Network](https://arxiv.org/abs/2508.16089)
*Sun Weikai,Song Shijie,Chi Wenjie*

Main category: cs.CV

TL;DR: 提出MSPG-SEN：两流反馈+多尺度渐进GAN，包含APFL、两流动态残差网络和DEMA，并在五个数据集上声称实现SOTA性能，但论文中若干描述模糊、指标来源和实验细节需核实。


<details>
  <summary>Details</summary>
Motivation: 针对扩散模型在图像生成领域的竞争，重申GAN因其独特优势仍有发展空间，目标是提升GAN的图像质量、训练稳定性和效率。

Method: 提出两流反馈多尺度渐进生成对抗网络(MSPG-SEN)，包含自适应感知-行为反馈回路(APFL)、全连接两流动态残差网络以及动态嵌入注意力机制(DEMA)。通过消融实验和在五个数据集上的测试评估效果。

Result: 作者报告在五个数据集上取得高性能指标（如INJK 89.7%、AWUN 78.3%、IONJ 85.5%、POKL 88.7%、OPIN 96.4%），并通过消融实验验证各模块贡献。

Conclusion: 本文提出的MSPG-SEN在多个数据集上取得了优异结果，声称在图像质量、训练稳定性和计算效率方面有提升，但部分细节不足以判断其广泛有效性。

Abstract: Although diffusion model has made good progress in the field of image
generation, GAN\cite{huang2023adaptive} still has a large development space due
to its unique advantages, such as WGAN\cite{liu2021comparing},
SSGAN\cite{guibas2021adaptive} \cite{zhang2022vsa} \cite{zhou2024adapt} and so
on. In this paper, we propose a novel two-flow feedback multi-scale progressive
generative adversarial network (MSPG-SEN) for GAN models. This paper has four
contributions: 1) : We propose a two-flow feedback multi-scale progressive
Generative Adversarial network (MSPG-SEN), which not only improves image
quality and human visual perception on the basis of retaining the advantages of
the existing GAN model, but also simplifies the training process and reduces
the training cost of GAN networks. Our experimental results show that, MSPG-SEN
has achieved state-of-the-art generation results on the following five
datasets,INKK The dataset is 89.7\%,AWUN The dataset is 78.3\%,IONJ The dataset
is 85.5\%,POKL The dataset is 88.7\%,OPIN The dataset is 96.4\%. 2) : We
propose an adaptive perception-behavioral feedback loop (APFL), which
effectively improves the robustness and training stability of the model and
reduces the training cost. 3) : We propose a globally connected two-flow
dynamic residual network(). After ablation experiments, it can effectively
improve the training efficiency and greatly improve the generalization ability,
with stronger flexibility. 4) : We propose a new dynamic embedded attention
mechanism (DEMA). After experiments, the attention can be extended to a variety
of image processing tasks, which can effectively capture global-local
information, improve feature separation capability and feature expression
capabilities, and requires minimal computing resources only 88.7\% with INJK
With strong cross-task capability.

</details>


### [22] [Domain Adaptation via Feature Refinement](https://arxiv.org/abs/2508.16124)
*Savvas Karatsiolis,Andreas Kamilaris*

Main category: cs.CV

TL;DR: 提出DAFR2：通过BN统计适配、特征蒸馏和假设迁移的组合实现简单高效的无监督领域自适应，在多项腐败基准上提升鲁棒性并提供理论与实验支持。


<details>
  <summary>Details</summary>
Motivation: 在分布偏移和图像腐败场景下，希望用简单有效的方法提升模型在目标域的泛化能力，而避免依赖目标标签或复杂训练目标。

Method: 结合三部分：利用目标无标签数据更新BatchNorm统计量、从源模型蒸馏特征、以及基于假设迁移(hypothesis transfer)的策略来微调目标模型。

Result: 在CIFAR10-C、CIFAR100-C、MNIST-C和PatchCamelyon-C等基准上优于先前方法，理论与实证分析显示更好的特征对齐、域间互信息提升及对输入扰动的敏感性降低。

Conclusion: DAFR2通过统计对齐、蒸馏和假设迁移联合提升无监督领域自适应的鲁棒性，得出结论为在不需目标标签或复杂架构下可提升特征对齐和稳健性。

Abstract: We propose Domain Adaptation via Feature Refinement (DAFR2), a simple yet
effective framework for unsupervised domain adaptation under distribution
shift. The proposed method synergistically combines three key components:
adaptation of Batch Normalization statistics using unlabeled target data,
feature distillation from a source-trained model and hypothesis transfer. By
aligning feature distributions at the statistical and representational levels,
DAFR2 produces robust and domain-invariant feature spaces that generalize
across similar domains without requiring target labels, complex architectures
or sophisticated training objectives. Extensive experiments on benchmark
datasets, including CIFAR10-C, CIFAR100-C, MNIST-C and PatchCamelyon-C,
demonstrate that the proposed algorithm outperforms prior methods in robustness
to corruption. Theoretical and empirical analyses further reveal that our
method achieves improved feature alignment, increased mutual information
between the domains and reduced sensitivity to input perturbations.

</details>


### [23] [4D Virtual Imaging Platform for Dynamic Joint Assessment via Uni-Plane X-ray and 2D-3D Registration](https://arxiv.org/abs/2508.16138)
*Hao Tang,Rongxi Yi,Lei Li,Kaiyi Cao,Jiapeng Zhao,Yihan Xiao,Minghai Shi,Peng Yuan,Yan Xi,Hui Tang,Wei Li,Zhan Wu,Yixin Zhou*

Main category: cs.CV

TL;DR: 提出一种基于双臂机器人CBCT的4D关节成像平台，通过深度学习和3D-2D融合实现低剂量、亚体素精度的动态负重关节成像，仿真与临床验证均显示良好性能，适用于生物力学研究和个性化骨科诊疗。


<details>
  <summary>Details</summary>
Motivation: 传统CT无法进行负重下的动态关节成像，而现有4D成像方法要么辐射剂量过高，要么仅提供二维信息，难以满足术后功能评估和精准诊疗的需求，因此需要一种低剂量、高精度并能在自然站立姿态下获取完整三维时序信息的成像方法。

Method: 方法包括：1）设计并使用双机器人臂球管束CBCT系统，支持可编程、无机架的直立扫描轨迹；2）构建混合成像流程，将静态3D CBCT与动态2D X光图像融合，采用深度学习预处理、3D-2D投影和迭代优化进行配准和重建；3）开发临床验证的定量运动学评估框架，用于分析关节术后运动学变化。作者在仿真和临床数据上评估方法性能。

Result: 仿真研究显示该方法实现了0.235 mm的亚体素配准精度，成功率为99.18%，优于传统和最新配准方法。临床评估表明，该平台能准确量化TKA患者胫骨平台运动及内外侧变异，验证了其实用性。

Conclusion: 该论文提出了一种集成化的四维膝关节分析平台，解决了传统CT无法在负重情况下捕捉关节动态运动的问题。通过结合双臂机器人CBCT、混合成像管线和临床验证的运动学评估框架，实现了低剂量、快速且亚体素精度的动态关节成像，具有明显的临床与研究价值。

Abstract: Conventional computed tomography (CT) lacks the ability to capture dynamic,
weight-bearing joint motion. Functional evaluation, particularly after surgical
intervention, requires four-dimensional (4D) imaging, but current methods are
limited by excessive radiation exposure or incomplete spatial information from
2D techniques. We propose an integrated 4D joint analysis platform that
combines: (1) a dual robotic arm cone-beam CT (CBCT) system with a
programmable, gantry-free trajectory optimized for upright scanning; (2) a
hybrid imaging pipeline that fuses static 3D CBCT with dynamic 2D X-rays using
deep learning-based preprocessing, 3D-2D projection, and iterative
optimization; and (3) a clinically validated framework for quantitative
kinematic assessment. In simulation studies, the method achieved sub-voxel
accuracy (0.235 mm) with a 99.18 percent success rate, outperforming
conventional and state-of-the-art registration approaches. Clinical evaluation
further demonstrated accurate quantification of tibial plateau motion and
medial-lateral variance in post-total knee arthroplasty (TKA) patients. This 4D
CBCT platform enables fast, accurate, and low-dose dynamic joint imaging,
offering new opportunities for biomechanical research, precision diagnostics,
and personalized orthopedic care.

</details>


### [24] [High-Precision Mixed Feature Fusion Network Using Hypergraph Computation for Cervical Abnormal Cell Detection](https://arxiv.org/abs/2508.16140)
*Jincheng Li,Danyang Dong,Menglin Zheng,Jingbo Zhang,Yueqin Hang,Lichi Zhang,Lili Zhao*

Main category: cs.CV

TL;DR: 提出了MLF-SNet与CLFFS-HC模块，通过超图融合空间与判别特征，显著提升TCT图像中异常宫颈细胞检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效建模视觉特征的空间相关性，且缺乏将细胞间关联特征与细胞内部判别特征融合到端到端检测模型的策略。

Method: 构建多层次融合子网络(MLF-SNet)以增强特征提取，随后引入交叉层特征融合策略与超图计算模块(CLFFS-HC)用于融合空间相关与深层判别特征，实现端到端检测。

Result: 在三个公开数据集上的实验表明，该方法显著提高了宫颈异常细胞检测性能。

Conclusion: 本文提出了基于超图的细胞检测网络，通过融合空间相关特征与深层判别特征，提升了宫颈细胞异常检测性能。

Abstract: Automatic detection of abnormal cervical cells from Thinprep Cytologic Test
(TCT) images is a critical component in the development of intelligent
computer-aided diagnostic systems. However, existing algorithms typically fail
to effectively model the correlations of visual features, while these spatial
correlation features actually contain critical diagnostic information.
Furthermore, no detection algorithm has the ability to integrate
inter-correlation features of cells with intra-discriminative features of
cells, lacking a fusion strategy for the end-to-end detection model. In this
work, we propose a hypergraph-based cell detection network that effectively
fuses different types of features, combining spatial correlation features and
deep discriminative features. Specifically, we use a Multi-level Fusion
Sub-network (MLF-SNet) to enhance feature extractioncapabilities. Then we
introduce a Cross-level Feature Fusion Strategy with Hypergraph Computation
module (CLFFS-HC), to integrate mixed features. Finally, we conducted
experiments on three publicly available datasets, and the results demonstrate
that our method significantly improves the performance of cervical abnormal
cell detection.

</details>


### [25] [Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection](https://arxiv.org/abs/2508.16157)
*Pi-Wei Chen,Jerry Chun-Wei Lin,Wei-Han Chen,Jia Ji,Zih-Ching Chen,Feng-Hao Yeh,Chao-Chun Chen*

Main category: cs.CV

TL;DR: 提出APT：通过自生成带噪声的合成异常样本和SMGS语义对齐，进行可学习提示调优，实现无先验少样本的VLM异常检测并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM的异常检测依赖人工设计提示和难以获取的异常样本，导致在特定场景下对异常语义理解不足；因此提出一个无先验、少样本但能自适应学习场景相关提示的方法以提升检测性能。

Method: APT利用带噪声扰动的自生成异常样本训练可学习提示，并提出自优化元提示引导方案（SMGS）通过迭代语义对齐与多样化合成异常融合，避免对合成噪声的过拟合；该方法同时适用于像素级异常检测并结合预训练VLM进行联合训练或微调。

Result: 在多个基准数据集上，APT在像素级异常检测任务中取得了最先进的效果，且不需要手工提示设计或先验异常知识，显示出良好的鲁棒性和通用性。

Conclusion: 该论文提出一种无需先验知识的少样本自适应提示调优框架（APT），通过自生成异常样本与语义对齐策略提高视觉-语言大模型在异常检测任务中的场景相关能力，解决了手工提示与异常样本缺乏带来的局限。

Abstract: Pre-trained Vision-Language Models (VLMs) have recently shown promise in
detecting anomalies. However, previous approaches are fundamentally limited by
their reliance on human-designed prompts and the lack of accessible anomaly
samples, leading to significant gaps in context-specific anomaly understanding.
In this paper, we propose \textbf{A}daptive \textbf{P}rompt \textbf{T}uning
with semantic alignment for anomaly detection (APT), a groundbreaking prior
knowledge-free, few-shot framework and overcomes the limitations of traditional
prompt-based approaches. APT uses self-generated anomaly samples with noise
perturbations to train learnable prompts that capture context-dependent
anomalies in different scenarios. To prevent overfitting to synthetic noise, we
propose a Self-Optimizing Meta-prompt Guiding Scheme (SMGS) that iteratively
aligns the prompts with general anomaly semantics while incorporating diverse
synthetic anomaly. Our system not only advances pixel-wise anomaly detection,
but also achieves state-of-the-art performance on multiple benchmark datasets
without requiring prior knowledge for prompt crafting, establishing a robust
and versatile solution for real-world anomaly detection.

</details>


### [26] [RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution](https://arxiv.org/abs/2508.16158)
*Haodong He,Yancheng Bai,Rui Lan,Xu Duan,Lei Sun,Xiangxiang Chu,Gui-Song Xia*

Main category: cs.CV

TL;DR: RAGSR通过区域化的细粒度文本描述和区域引导注意力，将区域-文本对与扩散模型注意力有效结合，提升多物体场景下的超分辨率细节恢复与视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于VLM和T2I的SISR方法在多物体或复杂提示下难以生成准确的区域细节，原因是缺乏细粒度区域描述和模型对复杂提示的捕捉能力不足。

Method: RAGSR首先对图像进行目标区域定位，并为每个区域生成细粒度描述，形成区域-文本对作为T2I模型的文本先验；然后在扩散模型的注意力模块中引入区域引导注意力，保证对应区域-文本对的注意力集中，同时抑制无关区域间的交互。

Result: 在基准数据集上，RAGSR在视觉细节生成的感知质量和语境一致性方面优于现有方法，能够更好地恢复局部真实细节。

Conclusion: 本文提出RAGSR方法，通过区域化文本引导的注意力机制改善图像超分辨率时的局部细节恢复，尤其在多物体场景下效果更好。

Abstract: The rich textual information of large vision-language models (VLMs) combined
with the powerful generative prior of pre-trained text-to-image (T2I) diffusion
models has achieved impressive performance in single-image super-resolution
(SISR). However, existing methods still face significant challenges in
generating clear and accurate regional details, particularly in scenarios
involving multiple objects. This challenge primarily stems from a lack of
fine-grained regional descriptions and the models' insufficient ability to
capture complex prompts. To address these limitations, we propose a Regional
Attention Guided Super-Resolution (RAGSR) method that explicitly extracts
localized fine-grained information and effectively encodes it through a novel
regional attention mechanism, enabling both enhanced detail and overall
visually coherent SR results. Specifically, RAGSR localizes object regions in
an image and assigns fine-grained caption to each region, which are formatted
as region-text pairs as textual priors for T2I models. A regional guided
attention is then leveraged to ensure that each region-text pair is properly
considered in the attention process while preventing unwanted interactions
between unrelated region-text pairs. By leveraging this attention mechanism,
our approach offers finer control over the integration of text and image
information, thereby effectively overcoming limitations faced by traditional
SISR techniques. Experimental results on benchmark datasets demonstrate that
our approach exhibits superior performance in generating perceptually authentic
visual details while maintaining contextual consistency compared to existing
approaches.

</details>


### [27] [Through the Looking Glass: A Dual Perspective on Weakly-Supervised Few-Shot Segmentation](https://arxiv.org/abs/2508.16159)
*Jiaqi Ma,Guo-Sen Xie,Fang Zhao,Zechao Li*

Main category: cs.CV

TL;DR: 提出TLG：通过HA、HT模块和异构CLIP文本信息，实现支持-查询对的异构建模，在WFSS任务上以极少参数显著超越现有方法并超越同架构的全监督模型。


<details>
  <summary>Details</summary>
Motivation: 传统元学习中对支持-查询对使用相同网络导致语义过度同质化，限制了模型提取更丰富、互补的表征能力。

Method: 将支持-查询对视为双重视角，设计异构视觉聚合（HA）模块以增强互补性并保留语义共性；设计异构传递（HT）模块以抑制语义噪声并放大异构语义的独特性；引入异构CLIP（HC）文本信息用于增强多模态模型的泛化能力。

Result: 在WFSS任务上，TLG以仅1/24参数量实现对Pascal-5i提升13.2%和对COCO-20i提升9.7%，并且首次实现弱监督（图像级）模型超越同骨干的全监督（像素级）模型。

Conclusion: 本文提出通过引入异构但同源的网络结构（TLG），在弱监督少样本语义分割任务中显著提升性能，且参数量远小于现有方法，甚至超越了同骨干的全监督模型。

Abstract: Meta-learning aims to uniformly sample homogeneous support-query pairs,
characterized by the same categories and similar attributes, and extract useful
inductive biases through identical network architectures. However, this
identical network design results in over-semantic homogenization. To address
this, we propose a novel homologous but heterogeneous network. By treating
support-query pairs as dual perspectives, we introduce heterogeneous visual
aggregation (HA) modules to enhance complementarity while preserving semantic
commonality. To further reduce semantic noise and amplify the uniqueness of
heterogeneous semantics, we design a heterogeneous transfer (HT) module.
Finally, we propose heterogeneous CLIP (HC) textual information to enhance the
generalization capability of multimodal models. In the weakly-supervised
few-shot semantic segmentation (WFSS) task, with only 1/24 of the parameters of
existing state-of-the-art models, TLG achieves a 13.2\% improvement on
Pascal-5\textsuperscript{i} and a 9.7\% improvement on
COCO-20\textsuperscript{i}. To the best of our knowledge, TLG is also the first
weakly supervised (image-level) model that outperforms fully supervised
(pixel-level) models under the same backbone architectures. The code is
available at https://github.com/jarch-ma/TLG.

</details>


### [28] [FTIO: Frequent Temporally Integrated Objects](https://arxiv.org/abs/2508.16183)
*Mohammad Mohammadzadeh Kalati,Farhad Maleki,Ian McQuillan*

Main category: cs.CV

TL;DR: FTIO通过频繁出现对象选择和三阶段时间修正作为后处理，解决了UVOS中对象选择不确定和时间不一致问题，达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: UVOS中初始显著对象分割的不确定性、对象小尺度或结构复杂导致的选择失败，以及变形和快速运动引起的时间不一致，都是影响性能的关键问题。

Method: FTIO是一个后处理框架，包含两部分：1) 结合标准构建一个综合判据，筛选频繁出现的显著对象以提高对象选择鲁棒性；2) 一个三阶段流程用于修正时间不一致，整合并填补缺失的对象掩码区域。

Result: 在多目标UVOS任务上，FTIO在实验中取得了最先进的性能，显示出对小目标和结构复杂目标的改进与时间一致性的提升。

Conclusion: 提出的FTIO框架通过频繁出现的显著对象选择和三阶段时间一致性修正，有效提升了多目标无监督视频对象分割（UVOS）的性能。

Abstract: Predicting and tracking objects in real-world scenarios is a critical
challenge in Video Object Segmentation (VOS) tasks. Unsupervised VOS (UVOS) has
the additional challenge of finding an initial segmentation of salient objects,
which affects the entire process and keeps a permanent uncertainty about the
object proposals. Moreover, deformation and fast motion can lead to temporal
inconsistencies. To address these problems, we propose Frequent Temporally
Integrated Objects (FTIO), a post-processing framework with two key components.
First, we introduce a combined criterion to improve object selection,
mitigating failures common in UVOS--particularly when objects are small or
structurally complex--by extracting frequently appearing salient objects.
Second, we present a three-stage method to correct temporal inconsistencies by
integrating missing object mask regions. Experimental results demonstrate that
FTIO achieves state-of-the-art performance in multi-object UVOS. Code is
available at: https://github.com/MohammadMohammadzadehKalati/FTIO

</details>


### [29] [SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning](https://arxiv.org/abs/2508.16201)
*Yicheng Ji,Jun Zhang,Heming Xia,Jinpeng Chen,Lidan Shou,Gang Chen,Huan Li*

Main category: cs.CV

TL;DR: 提出无训练的两阶段视频token剪枝的推测解码框架SpecVLM，能在保留准确率的前提下显著加速Vid-LLMs解码，最高达2.68×加速。


<details>
  <summary>Details</summary>
Motivation: 现有Vid-LLMs依赖密集视频token表示，导致预填充和解码阶段内存与计算开销大；既有token减少方法会损失信息，因此需要一种在不降低准确性的前提下加速解码的方法。

Method: 提出训练无关的SpecVLM框架：两阶段视频token剪枝。阶段一基于验证器（目标模型）的注意力信号选择高信息量token；阶段二对剩余token进行空间均匀剪枝。然后使用剪枝后的轻量草稿模型进行推测解码，最后由目标模型验证并补全。

Result: 在四个视频理解基准上，SpecVLM在不同大模型上实现了显著解码加速：LLaVA-OneVision-72B最高2.68×，Qwen2.5-VL-32B最高2.11×，在剪枝高达90%视频token的情况下仍保持鲁棒性和效果。

Conclusion: SpecVLM通过在推测解码中分阶段剪枝视频token，实现了在不牺牲精度的前提下大幅加速Vid-LLMs的解码过程。

Abstract: Video large language models (Vid-LLMs) have shown strong capabilities in
understanding video content. However, their reliance on dense video token
representations introduces substantial memory and computational overhead in
both prefilling and decoding. To mitigate the information loss of recent video
token reduction methods and accelerate the decoding stage of Vid-LLMs
losslessly, we introduce SpecVLM, a training-free speculative decoding (SD)
framework tailored for Vid-LLMs that incorporates staged video token pruning.
Building on our novel finding that the draft model's speculation exhibits low
sensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,
enabling efficient speculation without sacrificing accuracy. To achieve this,
it performs a two-stage pruning process: Stage I selects highly informative
tokens guided by attention signals from the verifier (target model), while
Stage II prunes remaining redundant ones in a spatially uniform manner.
Extensive experiments on four video understanding benchmarks demonstrate the
effectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$
decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup for
Qwen2.5-VL-32B.

</details>


### [30] [\textsc{T-Mask}: Temporal Masking for Probing Foundation Models across Camera Views in Driver Monitoring](https://arxiv.org/abs/2508.16207)
*Thinesh Thiyakesan Ponbagavathi,Kunyu Peng,Alina Roitberg*

Main category: cs.CV

TL;DR: tldr：论文在驾驶监测场景用图像基础模型做轻量探测，提出T-Mask时序token掩码方法，提高跨视角和少样本下的识别性能，优于PEFT并不增加额外参数。


<details>
  <summary>Details</summary>
Motivation: 动机是解决摄像头视角变化对驾驶员监测系统的负面影响，探索在只有单一训练视角并且不对未知视角做额外适配时，基础模型的鲁棒性与轻量级适配策略能否提升跨视角泛化性。

Method: 方法包括：将图像基础模型（DINOv2、CLIP）用于视频驾驶监测，比较线性探测、进阶探测策略、参数高效微调（PEFT）与全参数微调；提出T-Mask，一种基于时间维度的token掩码方法，强调动态区域并将图像模型转为视频探测器，不增加额外参数。

Result: 结果：在Drive&Act数据集上，T-Mask在跨视角top-1准确率上比强基线探测方法高出+1.23%，比PEFT方法高出+8.0%；对次要活动，在训练视角下提升+5.42%，跨视角提升+1.36%；不增加参数。

Conclusion: 论文结论：在驾驶员监测任务中，通过轻量级的探测（probing）方法适配图像基础模型，并结合时序token掩码策略（T-Mask），可以显著提升跨视角（cross-view）和低数据环境下的识别鲁棒性，尤其对次要活动（underrepresented secondary activities）改善效果明显。

Abstract: Changes of camera perspective are a common obstacle in driver monitoring.
While deep learning and pretrained foundation models show strong potential for
improved generalization via lightweight adaptation of the final layers
('probing'), their robustness to unseen viewpoints remains underexplored. We
study this challenge by adapting image foundation models to driver monitoring
using a single training view, and evaluating them directly on unseen
perspectives without further adaptation. We benchmark simple linear probes,
advanced probing strategies, and compare two foundation models (DINOv2 and
CLIP) against parameter-efficient fine-tuning (PEFT) and full fine-tuning.
Building on these insights, we introduce \textsc{T-Mask} -- a new
image-to-video probing method that leverages temporal token masking and
emphasizes more dynamic video regions. Benchmarked on the public Drive\&Act
dataset, \textsc{T-Mask} improves cross-view top-1 accuracy by $+1.23\%$ over
strong probing baselines and $+8.0\%$ over PEFT methods, without adding any
parameters. It proves particularly effective for underrepresented secondary
activities, boosting recognition by $+5.42\%$ under the trained view and
$+1.36\%$ under cross-view settings. This work provides encouraging evidence
that adapting foundation models with lightweight probing methods like
\textsc{T-Mask} has strong potential in fine-grained driver observation,
especially in cross-view and low-data settings. These results highlight the
importance of temporal token selection when leveraging foundation models to
build robust driver monitoring systems. Code and models will be made available
at https://github.com/th-nesh/T-MASK to support ongoing research.

</details>


### [31] [Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion Transformers](https://arxiv.org/abs/2508.16211)
*Shikang Zheng,Liang Feng,Xinyu Wang,Qinming Zhou,Peiliang Cai,Chang Zou,Jiacheng Liu,Yuqi Lin,Junjie Chen,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出基于ODE的FoCa（先预测后校准）特征缓存方法，在激进加速下比现有方法更稳健，在多数据集上实现数倍无损推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法在高加速比下表现下降严重，主要因长期跳跃间隔导致的预测不稳定与历史特征难以鲁棒集成，需一种能稳定长步预测并校正误差的策略。

Method: 从ODE视角对隐藏特征序列建模，提出Forecast-then-Calibrate流程：先基于历史特征进行长步预测（Forecast），再通过校准步骤融入当前信息以矫正误差（Calibrate），实现稳定的高倍率特征缓存。

Result: 在图像合成、视频生成与超分辨率等任务上，无需额外训练下取得近无损加速：FLUX 5.50×、HunyuanVideo 6.45×、Inf-DiT 3.17×、DiT 4.53×。

Conclusion: FoCa通过将特征缓存问题视为特征-ODE求解问题，有效缓解了长步预测导致的误差累积，在不额外训练的情况下，在多项任务上实现了显著的加速同时保持高质量。

Abstract: Diffusion Transformers (DiTs) have demonstrated exceptional performance in
high-fidelity image and video generation. To reduce their substantial
computational costs, feature caching techniques have been proposed to
accelerate inference by reusing hidden representations from previous timesteps.
However, current methods often struggle to maintain generation quality at high
acceleration ratios, where prediction errors increase sharply due to the
inherent instability of long-step forecasting. In this work, we adopt an
ordinary differential equation (ODE) perspective on the hidden-feature
sequence, modeling layer representations along the trajectory as a feature-ODE.
We attribute the degradation of existing caching strategies to their inability
to robustly integrate historical features under large skipping intervals. To
address this, we propose FoCa (Forecast-then-Calibrate), which treats feature
caching as a feature-ODE solving problem. Extensive experiments on image
synthesis, video generation, and super-resolution tasks demonstrate the
effectiveness of FoCa, especially under aggressive acceleration. Without
additional training, FoCa achieves near-lossless speedups of 5.50 times on
FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high
quality with a 4.53 times speedup on DiT.

</details>


### [32] [OmniCache: A Trajectory-Oriented Global Perspective on Training-Free Cache Reuse for Diffusion Transformer Models](https://arxiv.org/abs/2508.16212)
*Huanpeng Chu,Wei Wu,Guanyu Fen,Yutao Zhang*

Main category: cs.CV

TL;DR: OmniCache通过从采样轨迹角度进行全局缓存重用并结合动态噪声估计与滤波，在无需训练的情况下显著加速Diffusion-Transformer推理，同时维持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion-Transformer推理成本高（步骤多、每步计算复杂），现有缓存策略通常基于步间相似性且偏好重用后期步骤，未充分利用整个采样轨迹的全局冗余，难以满足实时部署需求。

Method: 从采样视角分析Diffusion-Transformer的轨迹，设计全局缓存重用策略，将缓存分配在整个采样过程而非集中在后期步骤；在重用缓存时，动态估计并过滤对应噪声以减少对采样方向的影响。该方法无需额外训练，可直接应用于现有模型。

Result: 实验表明，OmniCache在加速采样的同时保持了竞争性的生成质量，证明了其在实际部署中提高效率的可行性。

Conclusion: 该论文提出了OmniCache，一种无训练的加速方法，通过跨整个采样过程全球性地重用缓存计算来降低Diffusion-Transformer的推理开销，同时利用动态噪声估计与滤波缓解缓存重用带来的噪声误差，使生成质量保持竞争力。

Abstract: Diffusion models have emerged as a powerful paradigm for generative tasks
such as image synthesis and video generation, with Transformer architectures
further enhancing performance. However, the high computational cost of
diffusion Transformers-stemming from a large number of sampling steps and
complex per-step computations-presents significant challenges for real-time
deployment. In this paper, we introduce OmniCache, a training-free acceleration
method that exploits the global redundancy inherent in the denoising process.
Unlike existing methods that determine caching strategies based on inter-step
similarities and tend to prioritize reusing later sampling steps, our approach
originates from the sampling perspective of DIT models. We systematically
analyze the model's sampling trajectories and strategically distribute cache
reuse across the entire sampling process. This global perspective enables more
effective utilization of cached computations throughout the diffusion
trajectory, rather than concentrating reuse within limited segments of the
sampling procedure.In addition, during cache reuse, we dynamically estimate the
corresponding noise and filter it out to reduce its impact on the sampling
direction.Extensive experiments demonstrate that our approach accelerates the
sampling process while maintaining competitive generative quality, offering a
promising and practical solution for efficient deployment of diffusion-based
generative models.

</details>


### [33] [MedOmni-45°: A Safety-Performance Benchmark for Reasoning-Oriented LLMs in Medicine](https://arxiv.org/abs/2508.16213)
*Kaiyuan Ji,Yijin Guo,Zicheng Zhang,Xiangyang Zhu,Yuan Tian,Ning Liu,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出MedOmni-45 Degrees基准，通过操控提示测试医疗LLM的准确性、推理忠实性和反阿谀能力，发现存在不可避免的安全-性能权衡，提供用于改进安全性评估的工具。


<details>
  <summary>Details</summary>
Motivation: 现有基准往往将推理不可靠性（如CoT不忠实和阿谀随从现象）简化为单一准确率，掩盖了模型在安全性与性能之间的权衡；需要一个更细化的评估框架来揭示推理脆弱性并指导更安全的模型开发。

Method: 构建了包含1,804个面向推理的医疗问题（跨六个专科和三类任务，包含500个MedMCQA题目）的数据集；为每题配对七种操控性提示和无提示基线，生成约27K个输入；评估七个LLM（覆盖开源/封闭、通用/医学、基础/增强推理），共计约189K次推理；设计三项指标（Accuracy、CoT-Faithfulness、Anti-Sycophancy），并结合为一个可视化的45度复合得分。

Result: 通过45度图展示，所有模型均位于对角线下方，表明存在安全-性能权衡；QwQ-32B最接近均衡（43.81度），但没有模型在两者上都领先。

Conclusion: 该论文提出了一个名为MedOmni-45 Degrees的基准和评估流程，用于在受操控提示条件下衡量医疗大模型的安全-性能权衡，发现没有模型在安全性与准确性上同时领先，开放模型QwQ-32B表现最接近均衡。

Abstract: With the increasing use of large language models (LLMs) in medical
decision-support, it is essential to evaluate not only their final answers but
also the reliability of their reasoning. Two key risks are Chain-of-Thought
(CoT) faithfulness -- whether reasoning aligns with responses and medical facts
-- and sycophancy, where models follow misleading cues over correctness.
Existing benchmarks often collapse such vulnerabilities into single accuracy
scores. To address this, we introduce MedOmni-45 Degrees, a benchmark and
workflow designed to quantify safety-performance trade-offs under manipulative
hint conditions. It contains 1,804 reasoning-focused medical questions across
six specialties and three task types, including 500 from MedMCQA. Each question
is paired with seven manipulative hint types and a no-hint baseline, producing
about 27K inputs. We evaluate seven LLMs spanning open- vs. closed-source,
general-purpose vs. medical, and base vs. reasoning-enhanced models, totaling
over 189K inferences. Three metrics -- Accuracy, CoT-Faithfulness, and
Anti-Sycophancy -- are combined into a composite score visualized with a 45
Degrees plot. Results show a consistent safety-performance trade-off, with no
model surpassing the diagonal. The open-source QwQ-32B performs closest (43.81
Degrees), balancing safety and accuracy but not leading in both. MedOmni-45
Degrees thus provides a focused benchmark for exposing reasoning
vulnerabilities in medical LLMs and guiding safer model development.

</details>


### [34] [PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting](https://arxiv.org/abs/2508.16217)
*Hohyun Na,Seunghoo Hong,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出PromptFlare：通过在cross-attention中对无信息共享token注入对抗噪声，阻断prompt引导的扩散式图像篡改，结果表明在性能和资源消耗上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型使得高质量图像修改变得简便，从而带来被恶意利用的风险；传统基于图像级不一致性的对抗方法在应对文本提示影响上存在根本性局限，需提出针对prompt层面的保护策略。

Method: 分析prompt嵌入的固有属性，识别并针对prompt中不变且语义上无信息量的共享tokens，在cross-attention中注入作为诱饵的对抗噪声，使采样过程偏离有意义的prompt-image对齐，从而中和prompt效应。

Result: 在EditBench上，PromptFlare在多个指标上达到或超过现有方法，同时计算开销和GPU显存需求明显降低，实验证明其在防止未经授权图像编辑方面既有效又高效。

Conclusion: PromptFlare通过对prompt的跨注意力机制注入干扰噪声，有效削弱扩散模型基于prompt的恶意图像修改能力，在EditBench数据集上表现出色，同时显著降低计算和显存开销，证明其为鲁棒且高效的图像保护方法。

Abstract: The success of diffusion models has enabled effortless, high-quality image
modifications that precisely align with users' intentions, thereby raising
concerns about their potential misuse by malicious actors. Previous studies
have attempted to mitigate such misuse through adversarial attacks. However,
these approaches heavily rely on image-level inconsistencies, which pose
fundamental limitations in addressing the influence of textual prompts. In this
paper, we propose PromptFlare, a novel adversarial protection method designed
to protect images from malicious modifications facilitated by diffusion-based
inpainting models. Our approach leverages the cross-attention mechanism to
exploit the intrinsic properties of prompt embeddings. Specifically, we
identify and target shared token of prompts that is invariant and semantically
uninformative, injecting adversarial noise to suppress the sampling process.
The injected noise acts as a cross-attention decoy, diverting the model's focus
away from meaningful prompt-image alignments and thereby neutralizing the
effect of prompt. Extensive experiments on the EditBench dataset demonstrate
that our method achieves state-of-the-art performance across various metrics
while significantly reducing computational overhead and GPU memory usage. These
findings highlight PromptFlare as a robust and efficient protection against
unauthorized image manipulations. The code is available at
https://github.com/NAHOHYUN-SKKU/PromptFlare.

</details>


### [35] [An Investigation of Visual Foundation Models Robustness](https://arxiv.org/abs/2508.16225)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 论文综述了视觉基础模型的鲁棒性问题及现有防御方法，指出在实际安全敏感应用中需更系统的训练/评估方法来应对分布偏移、噪声与对抗攻击。


<details>
  <summary>Details</summary>
Motivation: VFMs已广泛部署于关键与安全敏感领域，但实际环境中光照、天气与传感器差异导致性能下降，且存在对抗威胁，迫切需要研究提升视觉模型鲁棒性的策略与评估方法以建立可信系统。

Method: 通过回顾历史与现代深度学习架构（如LeNet-5、AlexNet、ResNet、VGG、Inception、DenseNet、YOLO、ViT），分析这些模型在不同任务中的适配性；系统总结了现有经验性防御与鲁棒训练方法，并比较其在对抗样本、分布移位与自然扰动下的效果；最后提出网络属性、组件及评估指标供消融研究与基准测试使用。

Result: 归纳出主要鲁棒性挑战类别（分布移位、噪声与空间畸变、对抗攻击），总结了主流防御方法（数据增强、对抗训练、输入变换、正则化、鲁棒架构设计等）的优劣，并列出用于消融与基准测试的网络属性与评价指标框架。

Conclusion: 本文综述了视觉基础模型（VFMs）在计算机视觉任务中的广泛应用与面临的鲁棒性挑战，强调在安全敏感场景中建立可靠性的必要性；认为需要针对分布偏移、噪声、空间畸变与对抗攻击等问题改进训练与防御策略，并提出了评估与消融研究的方向。

Abstract: Visual Foundation Models (VFMs) are becoming ubiquitous in computer vision,
powering systems for diverse tasks such as object detection, image
classification, segmentation, pose estimation, and motion tracking. VFMs are
capitalizing on seminal innovations in deep learning models, such as LeNet-5,
AlexNet, ResNet, VGGNet, InceptionNet, DenseNet, YOLO, and ViT, to deliver
superior performance across a range of critical computer vision applications.
These include security-sensitive domains like biometric verification,
autonomous vehicle perception, and medical image analysis, where robustness is
essential to fostering trust between technology and the end-users. This article
investigates network robustness requirements crucial in computer vision systems
to adapt effectively to dynamic environments influenced by factors such as
lighting, weather conditions, and sensor characteristics. We examine the
prevalent empirical defenses and robust training employed to enhance vision
network robustness against real-world challenges such as distributional shifts,
noisy and spatially distorted inputs, and adversarial attacks. Subsequently, we
provide a comprehensive analysis of the challenges associated with these
defense mechanisms, including network properties and components to guide
ablation studies and benchmarking metrics to evaluate network robustness.

</details>


### [36] [FlexMUSE: Multimodal Unification and Semantics Enhancement Framework with Flexible interaction for Creative Writing](https://arxiv.org/abs/2508.16230)
*Jiahao Chen,Zhiyong Ma,Wenbiao Du,Qingyuan Chuai*

Main category: cs.CV

TL;DR: 作者提出FlexMUSE，通过msaGate、跨模态注意力融合和mscDPO，并配合可选T2I模块，实现经济灵活的多模态创作写作；并发布了约3k条的ArtMUSE数据集，展示了较好的创造性与一致性。


<details>
  <summary>Details</summary>
Motivation: MMCW要求生成带插图的文章，但文本与图像之间语义并非严格相关，现有方法需要特定模态输入或昂贵训练且常有语义不一致问题。因此需要一种经济、可交互且能提高模态间语义一致性的解决方案，促进创造性写作。

Method: 提出FlexMUSE，包括：1) 可选的文本到图像(T2I)模块以支持可选视觉输入；2) modality semantic alignment gating(msaGate)用于限制并对齐文本输入；3) 基于注意力的跨模态融合模块增强输入特征；4) modality semantic creative direct preference optimization(mscDPO)通过扩展被拒样本促进写作创造性。训练/优化上可能结合对比或偏好学习。发布了约3k校准文本-图像对的ArtMUSE数据集。

Result: 实验表明FlexMUSE在一致性、创造性和连贯性方面取得了有希望的结果。作者还提供了ArtMUSE数据集（约3k对）以推进该领域研究。

Conclusion: 该论文提出了FlexMUSE框架，通过可选的T2I模块和多种对齐与优化机制来实现灵活的多模态创作写作（MMCW），在一致性、创造性和连贯性方面取得了良好效果，并发布了ArtMUSE数据集以促进研究。

Abstract: Multi-modal creative writing (MMCW) aims to produce illustrated articles.
Unlike common multi-modal generative (MMG) tasks such as storytelling or
caption generation, MMCW is an entirely new and more abstract challenge where
textual and visual contexts are not strictly related to each other. Existing
methods for related tasks can be forcibly migrated to this track, but they
require specific modality inputs or costly training, and often suffer from
semantic inconsistencies between modalities. Therefore, the main challenge lies
in economically performing MMCW with flexible interactive patterns, where the
semantics between the modalities of the output are more aligned. In this work,
we propose FlexMUSE with a T2I module to enable optional visual input. FlexMUSE
promotes creativity and emphasizes the unification between modalities by
proposing the modality semantic alignment gating (msaGate) to restrict the
textual input. Besides, an attention-based cross-modality fusion is proposed to
augment the input features for semantic enhancement. The modality semantic
creative direct preference optimization (mscDPO) within FlexMUSE is designed by
extending the rejected samples to facilitate the writing creativity. Moreover,
to advance the MMCW, we expose a dataset called ArtMUSE which contains with
around 3k calibrated text-image pairs. FlexMUSE achieves promising results,
demonstrating its consistency, creativity and coherence.

</details>


### [37] [UniEM-3M: A Universal Electron Micrograph Dataset for Microstructural Segmentation and Generation](https://arxiv.org/abs/2508.16239)
*Nan wang,Zhiyi Xia,Yiming Li,Shi Tang,Zuxin Fan,Xi Fang,Haoyi Tao,Xiaochen Cai,Guolin Ke,Linfeng Zhang,Yanhui Hong*

Main category: cs.CV

TL;DR: 该工作发布了首个大规模多模态EM实例级数据集UniEM-3M（~300万个实例标签）并训练了文本-图像扩散模型，提出UniEM-Net作为强基线，在综合基准上表现最好，将推动材料微观结构自动分析研究。


<details>
  <summary>Details</summary>
Motivation: 深度学习在EM图像微观结构表征上的发展受限于缺乏大规模、多样且有专家注释的数据集，因获取成本高、隐私和标注复杂等原因难以获得。作者希望通过构建公开的数据集和生成模型来推动自动化材料分析研究。

Method: 构建并发布包含5091张高分辨率EM图像、约300万实例分割标签和图像级属性解耦文本描述的多模态数据集；在整个集合上训练文本到图像的扩散生成模型用于数据增强和分布代理；对多种实例分割方法进行基准测试，并提出基于流（flow-based）的UniEM-Net作为强基线。

Result: 发布了名为 UniEM-3M 的大规模多模态数据集（部分公开）、一个文本到图像扩散生成模型，以及完整基准。基于流的 UniEM-Net 在该基准上优于其他先进方法。

Conclusion: 该论文介绍了 UniEM-3M 数据集及相关基准，旨在解决电子显微镜（EM）图像实例级标注数据稀缺问题。作者发布了大规模多模态数据集、训练好的文本到图像扩散模型，并提出了强基线 UniEM-Net，实验证明其在基准上表现优越。

Abstract: Quantitative microstructural characterization is fundamental to materials
science, where electron micrograph (EM) provides indispensable high-resolution
insights. However, progress in deep learning-based EM characterization has been
hampered by the scarcity of large-scale, diverse, and expert-annotated
datasets, due to acquisition costs, privacy concerns, and annotation
complexity. To address this issue, we introduce UniEM-3M, the first large-scale
and multimodal EM dataset for instance-level understanding. It comprises 5,091
high-resolution EMs, about 3 million instance segmentation labels, and
image-level attribute-disentangled textual descriptions, a subset of which will
be made publicly available. Furthermore, we are also releasing a text-to-image
diffusion model trained on the entire collection to serve as both a powerful
data augmentation tool and a proxy for the complete data distribution. To
establish a rigorous benchmark, we evaluate various representative instance
segmentation methods on the complete UniEM-3M and present UniEM-Net as a strong
baseline model. Quantitative experiments demonstrate that this flow-based model
outperforms other advanced methods on this challenging benchmark. Our
multifaceted release of a partial dataset, a generative model, and a
comprehensive benchmark -- available at huggingface -- will significantly
accelerate progress in automated materials analysis.

</details>


### [38] [Structuring GUI Elements through Vision Language Models: Towards Action Space Generation](https://arxiv.org/abs/2508.16271)
*Yi Xu,Yesheng Zhang,jiajia Liu,Jingdong Chen*

Main category: cs.CV

TL;DR: Propose IAML: augment coordinate data via IoU-based sampling and fine-tune MLLMs to reduce exposure bias, yielding better UI coordinate prediction.


<details>
  <summary>Details</summary>
Motivation: Numerical UI coordinates lack semantic representation in language models and next-token training causes exposure bias, so models need augmented data reflecting coordinate variations and training that accounts for IoU-based quality.

Method: They design an IoU-based coordinate sampling pipeline to generate augmented training data near ground-truth coordinates and fine-tune MLLMs with an IAML paradigm to reduce exposure bias in next-token prediction.

Result: Extensive experiments show IAML leads to superior performance compared to traditional training paradigms in predicting UI element coordinates.

Conclusion: The paper concludes that IoU-Augmented Maximum Likelihood (IAML) training with IoU-based coordinate sampling effectively improves MLLMs' ability to predict UI element coordinates, outperforming traditional maximum likelihood training.

Abstract: Multimodal large language models (MLLMs) have emerged as pivotal tools in
enhancing human-computer interaction. In this paper we focus on the application
of MLLMs in the field of graphical user interface (GUI) elements structuring,
where they assist in processing user instructions based on screen contents.
Despite the promise of MLLMs, their performance in precisely generating UI
element coordinates, a critical aspect of GUI understanding, is hindered by the
nature of next-token prediction training. This challenge arises from the
semantic void surrounding numerical UI coordinates in language representation
spaces, necessitating a substantial and diverse dataset to bolster visual
module capabilities. To address these limitations, we introduce an
IoU-Augmented Maximum Likelihood (IAML) training paradigm. Specifically, our
approach involves a novel pipeline for IoU-based coordinate sampling to augment
the training data, which considers the proximity to ground truth coordinates.
This data augmentation strategy is then employed to fine-tune MLLMs under the
IAML paradigm, which is designed to mitigate the exposure bias problem inherent
in traditional maximum likelihood estimation. Through extensive experiments, we
demonstrate the superior performance of our IAML training approach over
traditional training paradigms.

</details>


### [39] [IRSAMap:Towards Large-Scale, High-Resolution Land Cover Map Vectorization](https://arxiv.org/abs/2508.16272)
*Yu Meng,Ligao Deng,Zhihao Xi,Jiansheng Chen,Jingbo Chen,Anzhi Yue,Diyou Liu,Kai Li,Chenhao Wang,Kaiyu Li,Yupeng Deng,Xian Sun*

Main category: cs.CV

TL;DR: IRSAMap：首个全球性高分辨率、多要素、面向对象的遥感矢量数据集，含180万+实例、10类地物，覆盖79区域、6大洲，支持多任务基准，推动从像素级到对象级的地理特征自动化建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在类别标注有限、数据规模小及缺乏空间结构信息等方面不足，无法满足高精度对象边界与拓扑一致性需求，因此需要一个更大规模、更精细、更具结构化信息的全球性矢量数据集。

Method: 通过构建一个包含超过180万实例、10类地物的全球性矢量注记体系，并结合人工+AI的智能注记流程生成数据；同时提供多任务标注以支持像素级分类、建筑轮廓提取、道路中心线提取及全景分割等多种任务。

Result: 构建了覆盖79个区域、6大洲、总面积>1000平方公里的IRSAMap数据集，包含10类典型地物的矢量注记与多任务标签，提升注记效率与一致性，支持多种下游任务，已公开发布。

Conclusion: 该论文提出了IRSAMap数据集，推动遥感影像从像素级分割向面向对象的矢量建模转变，为土地覆盖大规模高分辨率矢量制图提供了基础资源。

Abstract: With the enhancement of remote sensing image resolution and the rapid
advancement of deep learning, land cover mapping is transitioning from
pixel-level segmentation to object-based vector modeling. This shift demands
more from deep learning models, requiring precise object boundaries and
topological consistency. However, existing datasets face three main challenges:
limited class annotations, small data scale, and lack of spatial structural
information. To overcome these issues, we introduce IRSAMap, the first global
remote sensing dataset for large-scale, high-resolution, multi-feature land
cover vector mapping. IRSAMap offers four key advantages: 1) a comprehensive
vector annotation system with over 1.8 million instances of 10 typical objects
(e.g., buildings, roads, rivers), ensuring semantic and spatial accuracy; 2) an
intelligent annotation workflow combining manual and AI-based methods to
improve efficiency and consistency; 3) global coverage across 79 regions in six
continents, totaling over 1,000 km; and 4) multi-task adaptability for tasks
like pixel-level classification, building outline extraction, road centerline
extraction, and panoramic segmentation. IRSAMap provides a standardized
benchmark for the shift from pixel-based to object-based approaches, advancing
geographic feature automation and collaborative modeling. It is valuable for
global geographic information updates and digital twin construction. The
dataset is publicly available at https://github.com/ucas-dlg/IRSAMap

</details>


### [40] [Robust Small Methane Plume Segmentation in Satellite Imagery](https://arxiv.org/abs/2508.16282)
*Khai Duc Minh Tran,Hoa Van Nguyen,Aimuni Binti Muhammad Rawi,Hareeshrao Athinarayanarao,Ba-Ngu Vo*

Main category: cs.CV

TL;DR: 提出基于ResNet34编码器的U-Net并结合Varon比率与Sanchez回归双谱增强，在Sentinel-2影像上实现对小尺度甲烷羽流（单像素级）高效检测，验证集F1=78.39%。


<details>
  <summary>Details</summary>
Motivation: 甲烷是强效温室气体，现有遥感方法对小尺度（<~400 m2）泄漏检测能力有限，利用Sentinel-2广覆蓋与更细空间分辨率实现自动化、广域且高灵敏度的甲烷监测具有重大气候与环境管理价值。

Method: 使用基于U-Net的语义分割网络，编码器采用ResNet34；预处理阶段对多光谱波段进行Varon比率和Sanchez回归双谱增强以突出甲烷特征；训练过程中采用常见的分割损失（如交叉熵与Dice混合）并进行数据增强以提升泛化性。

Result: 在验证集上实现78.39% F1分数，能够检测到单像素（20 m）尺度的甲烷羽流（约400 m2），在灵敏度与精确度上优于现有自动化遥感检测技术。

Conclusion: 本文提出将Sentinel-2影像与改进的U-Net（ResNet34编码器）结合，通过双谱增强（Varon比率和Sanchez回归）优化输入特征，实现对小尺度甲烷羽流的高灵敏度检测，显著优于传统方法。

Abstract: This paper tackles the challenging problem of detecting methane plumes, a
potent greenhouse gas, using Sentinel-2 imagery. This contributes to the
mitigation of rapid climate change. We propose a novel deep learning solution
based on U-Net with a ResNet34 encoder, integrating dual spectral enhancement
techniques (Varon ratio and Sanchez regression) to optimise input features for
heightened sensitivity. A key achievement is the ability to detect small plumes
down to 400 m2 (i.e., for a single pixel at 20 m resolution), surpassing
traditional methods limited to larger plumes. Experiments show our approach
achieves a 78.39% F1-score on the validation set, demonstrating superior
performance in sensitivity and precision over existing remote sensing
techniques for automated methane monitoring, especially for small plumes.

</details>


### [41] [EdgeDoc: Hybrid CNN-Transformer Model for Accurate Forgery Detection and Localization in ID Documents](https://arxiv.org/abs/2508.16284)
*Anjith George,Sebastien Marcel*

Main category: cs.CV

TL;DR: EdgeDoc用轻量卷积-Transformer加噪声印记特征做文档伪造检测与定位，在FantasyID上超越基线并在ICCV2025竞赛中获第三名。


<details>
  <summary>Details</summary>
Motivation: 数字图像与文档编辑工具普及导致伪造门槛下降，给KYC和远程开户等流程带来安全风险，需要高效、轻量且能定位篡改的检测方法以保障服务完整性。

Method: 设计了一个轻量级卷积-Transformer混合网络作为主干，辅以从图像中提取的噪声印记（noiseprint）特征作为辅助输入，网络同时进行篡改检测和像素级定位；训练与评估在FantasyID数据集上进行，并参与ICCV 2025 DeepID挑战。

Result: 在FantasyID上优于基线方法，在ICCV 2025 DeepID挑战中取得第三名，表现证明了结合noiseprint特征对检测细微篡改的有效性与实用性。

Conclusion: EdgeDoc提出了一种结合轻量级卷积变换器与噪声印记特征的文档篡改检测与定位方法，能有效识别细微伪造并在ICCV 2025 DeepID挑战中获第三名，实验显示在FantasyID数据集上优于基线方法。

Abstract: The widespread availability of tools for manipulating images and documents
has made it increasingly easy to forge digital documents, posing a serious
threat to Know Your Customer (KYC) processes and remote onboarding systems.
Detecting such forgeries is essential to preserving the integrity and security
of these services. In this work, we present EdgeDoc, a novel approach for the
detection and localization of document forgeries. Our architecture combines a
lightweight convolutional transformer with auxiliary noiseprint features
extracted from the images, enhancing its ability to detect subtle
manipulations. EdgeDoc achieved third place in the ICCV 2025 DeepID Challenge,
demonstrating its competitiveness. Experimental results on the FantasyID
dataset show that our method outperforms baseline approaches, highlighting its
effectiveness in realworld scenarios. Project page : https://www.idiap.
ch/paper/edgedoc/

</details>


### [42] [Learning Long-Range Action Representation by Two-Stream Mamba Pyramid Network for Figure Skating Assessment](https://arxiv.org/abs/2508.16291)
*Fengshun Wang,Qiurui Wang,Peilin Zhao*

Main category: cs.CV

TL;DR: 本文提出的双流Mamba金字塔网络，通过分离TES与PCS评估流、在PCS中跨层融合视听信息及在TES中用多尺度金字塔定位动作元素，解决了特征混用、动作元素评分缺失和长视频建模三大问题，在FineFS上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有工作没有遵循花样滑冰的评判先验（TES基于动作元素，PCS基于视听整体），同时没有对分离的动作元素逐个评分且难以处理赛事长视频，因而亟需一种能分离评估流并高效建模长时依赖的方法。

Method: 提出两条并行流：视觉特征专注的TES评估流和视听融合的PCS评估流；在PCS流中采用多层融合以在各金字塔层次上融合音频与视频，同时避免视频特征被影响；在TES流中使用多尺度Mamba金字塔捕捉不同时间尺度的动作元素并配以TES头进行局部元素打分；整体模型利用Mamba架构线性复杂度的长程依赖建模能力。

Result: 在FineFS基准上取得了最先进的成绩，实验证明在TES与PCS预测上均优于现有方法，并且对长视频的建模效率与元素定位精度有明显提升。

Conclusion: 该论文提出的双流Mamba金字塔网络与花样滑冰评分标准高度一致，通过将TES与PCS评估流分离、在PCS流中引入多层融合机制，以及在TES流中设计多尺度金字塔和TES头部，有效解决了现有方法在特征使用、动作元素局部化与长视频建模方面的不足。

Abstract: Technical Element Score (TES) and Program Component Score (PCS) evaluations
in figure skating demand precise assessment of athletic actions and artistic
interpretation, respectively. Existing methods face three major challenges.
Firstly, video and audio cues are regarded as common features for both TES and
PCS predictions in previous works without considering the prior evaluation
criterion of figure skating. Secondly, action elements in competitions are
separated in time, TES should be derived from each element's score, but
existing methods try to give an overall TES prediction without evaluating each
action element. Thirdly, lengthy competition videos make it difficult and
inefficient to handle long-range contexts. To address these challenges, we
propose a two-stream Mamba pyramid network that aligns with actual judging
criteria to predict TES and PCS by separating visual-feature based TES
evaluation stream from audio-visual-feature based PCS evaluation stream. In the
PCS evaluation stream, we introduce a multi-level fusion mechanism to guarantee
that video-based features remain unaffected when assessing TES, and enhance PCS
estimation by fusing visual and auditory cues across each contextual level of
the pyramid. In the TES evaluation stream, the multi-scale Mamba pyramid and
TES head we proposed effectively address the challenges of localizing and
evaluating action elements with various temporal scales and give score
predictions. With Mamba's superior ability to capture long-range dependencies
and its linear computational complexity, our method is ideal for handling
lengthy figure skating videos. Comprehensive experimentation demonstrates that
our framework attains state-of-the-art performance on the FineFS benchmark. Our
source code is available at
https://github.com/ycwfs/Figure-Skating-Action-Quality-Assessment.

</details>


### [43] [Enhanced Hybrid Technique for Efficient Digitization of Handwritten Marksheets](https://arxiv.org/abs/2508.16295)
*Junaid Ahmed Sifat,Abir Chowdhury,Hasnat Md. Imtiaz,Md. Irtiza Hossain,Md. Imran Bin Azad*

Main category: cs.CV

TL;DR: 提出OpenCV+PaddleOCR+（改进）YOLOv8的混合方案，高效检测复杂表格并识别手写文本；在自建数据集上改进YOLOv8达92.72%准确率，优于其他方法，适合文档数字化应用。


<details>
  <summary>Details</summary>
Motivation: 手写成绩单存在多样的书写风格与复杂表格结构，传统OCR或单一方法难以兼顾表格结构检测与手写文本识别的鲁棒性，因此提出混合方法以兼顾效率与精度，减少人工干预。

Method: 使用OpenCV的图像处理技术检测表格的行、列以实现轻量且准确的表格检测；在检测到的表格单元区域内，比较使用PaddleOCR、YOLOv8和改进YOLOv8进行手写文本识别；改进YOLOv8通过架构或训练的调整提升识别准确率；在自定义、多样化的手写数据集上进行实验与对比。

Result: 在自建数据集上，改进YOLOv8准确率为92.72%，PaddleOCR为91.37%，YOLOv8为88.91%；整体系统在表格检测与手写识别方面表现良好，具有实际可用性与部署价值。

Conclusion: 该论文提出了一个混合方法，将OpenCV用于表格检测，PaddleOCR用于顺序手写文本识别，并引入YOLOv8与改进的YOLOv8以提升识别性能；在自建数据集上，改进的YOLOv8取得最佳准确率，从而能有效减少人工干预，适用于学术与行政文件的数字化。

Abstract: The digitization of handwritten marksheets presents huge challenges due to
the different styles of handwriting and complex table structures in such
documents like marksheets. This work introduces a hybrid method that integrates
OpenCV for table detection and PaddleOCR for recognizing sequential handwritten
text. The image processing capabilities of OpenCV efficiently detects rows and
columns which enable computationally lightweight and accurate table detection.
Additionally, YOLOv8 and Modified YOLOv8 are implemented for handwritten text
recognition within the detected table structures alongside PaddleOCR which
further enhance the system's versatility. The proposed model achieves high
accuracy on our custom dataset which is designed to represent different and
diverse handwriting styles and complex table layouts. Experimental results
demonstrate that YOLOv8 Modified achieves an accuracy of 92.72 percent,
outperforming PaddleOCR 91.37 percent and the YOLOv8 model 88.91 percent. This
efficiency reduces the necessity for manual work which makes this a practical
and fast solution for digitizing academic as well as administrative documents.
This research serves the field of document automation, particularly handwritten
document understanding, by providing operational and reliable methods to scale,
enhance, and integrate the technologies involved.

</details>


### [44] [A Multimodal-Multitask Framework with Cross-modal Relation and Hierarchical Interactive Attention for Semantic Comprehension](https://arxiv.org/abs/2508.16300)
*Mohammad Zia Ur Rehman,Devraj Raghuvanshi,Umang Jain,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: MM-ORIENT通过跨模态关系图在隐空间无显式交互地重构单模态特征并用分层单模态注意力提取判别信息，实现对噪声鲁棒且对多任务有效的多模态表示。


<details>
  <summary>Details</summary>
Motivation: 动机是两点：一是单模态中存在噪声，显式模态交互会把噪声带入联合表示，降低性能；二是多数融合方法重视联合表示，但可能忽视单模态中有价值的判别信息，尤其在多任务场景下，需要兼顾单模态判别能力与跨模态关系。

Method: 方法包括两部分：1) 跨模态关系图（Cross-modal relation graphs）：基于不同模态特征决定节点邻居，用邻居信息重构单模态特征以获得跨模态表示，避免显式模态间交互带来的噪声传播。2) 分层交互单模态注意力（HIMA）：在多层次上对单模态内部信息进行注意力加权，提取与任务相关的判别性特征，随后进行晚期融合并用于多任务学习。

Result: 作者在三个数据集上的广泛实验表明，MM-ORIENT在多任务设置下优于基线方法，能更好地理解多模态内容并提升任务性能（论文声称的定量和定性结果支持该结论）。

Conclusion: 该论文提出了MM-ORIENT框架，通过跨模态关系图和分层交互单模态注意力（HIMA）在多任务设置下获得鲁棒的多模态表示。核心思想是在隐空间进行无显式模态间交互，通过利用其他模态决定邻居来重构单模态特征，从而减轻噪声影响，并在融合前通过HIMA提取单模态判别信息。实验在三个数据集上验证了方法的有效性。

Abstract: A major challenge in multimodal learning is the presence of noise within
individual modalities. This noise inherently affects the resulting multimodal
representations, especially when these representations are obtained through
explicit interactions between different modalities. Moreover, the multimodal
fusion techniques while aiming to achieve a strong joint representation, can
neglect valuable discriminative information within the individual modalities.
To this end, we propose a Multimodal-Multitask framework with crOss-modal
Relation and hIErarchical iNteractive aTtention (MM-ORIENT) that is effective
for multiple tasks. The proposed approach acquires multimodal representations
cross-modally without explicit interaction between different modalities,
reducing the noise effect at the latent stage. To achieve this, we propose
cross-modal relation graphs that reconstruct monomodal features to acquire
multimodal representations. The features are reconstructed based on the node
neighborhood, where the neighborhood is decided by the features of a different
modality. We also propose Hierarchical Interactive Monomadal Attention (HIMA)
to focus on pertinent information within a modality. While cross-modal relation
graphs help comprehend high-order relationships between two modalities, HIMA
helps in multitasking by learning discriminative features of individual
modalities before late-fusing them. Finally, extensive experimental evaluation
on three datasets demonstrates that the proposed approach effectively
comprehends multimodal content for multiple tasks.

</details>


### [45] [Exploiting Information Redundancy in Attention Maps for Extreme Quantization of Vision Transformers](https://arxiv.org/abs/2508.16311)
*Lucas Maisonnave,Karim Haroun,Tom Pegeot*

Main category: cs.CV

TL;DR: TL;DR：用香农熵识别低信息量的注意力头，冻结并低精度量化这些注意力图（EAM），可在保持准确率的前提下加速Transformer推理并降低内存占用，尤其在≤20%稀疏率下效果最好。


<details>
  <summary>Details</summary>
Motivation: 动机：Transformer的多头自注意力带来高昂的计算和内存开销，尤其在边缘设备部署时受限。通过识别并去除注意力图中的信息冗余，可减少计算量并保留模型性能。

Method: 方法：首先计算每个注意力头的熵以评估信息量；将低熵头视为冗余，冻结其权重并对对应的注意力图进行低精度量化（EAM）；在推理时跳过对这些头的重复计算以节省资源。

Result: 结果：在ImageNet-1k上，针对DeiT和Swin两种Transformer结构，EAM在注意力图稀疏率≤20%时能够实现相同或更高的精度，并在更高稀疏率下仍保持竞争性能。

Conclusion: 论文结论：通过测量和利用注意力头的香农熵，可以识别冗余的低熵注意力头，对其权重进行冻结并低精度量化，从而在保持精度的同时显著降低MHSA的计算和内存开销。

Abstract: Transformer models rely on Multi-Head Self-Attention (MHSA) mechanisms, where
each attention head contributes to the final representation. However, their
computational complexity and high memory demands due to MHSA hinders their
deployment at the edge. In this work, we analyze and exploit information
redundancy in attention maps to accelerate model inference. By quantifying the
information captured by each attention head using Shannon entropy, our analysis
reveals that attention heads with lower entropy, i.e., exhibiting more
deterministic behavior, tend to contribute less information, motivating
targeted compression strategies. Relying on these insights, we propose Entropy
Attention Maps (EAM), a model that freezes the weights of low-entropy attention
maps and quantizes these values to low precision to avoid redundant
re-computation. Empirical validation on ImageNet-1k shows that EAM achieves
similar or higher accuracy at $\leq$20\% sparsity in attention maps and
competitive performance beyond this level for the DeiT and Swin Transformer
models.

</details>


### [46] [Vision encoders should be image size agnostic and task driven](https://arxiv.org/abs/2508.16317)
*Nedyalko Prisadnikov,Danda Pani Paudel,Yuqian Fu,Luc Van Gool*

Main category: cs.CV

TL;DR: 主张构建对图像尺寸不敏感且任务驱动的动态视觉编码器，并以图像分类的概念验证展示了可行性和前景。


<details>
  <summary>Details</summary>
Motivation: 自然视觉系统在处理大量视觉数据时基于任务选择性分配注意和计算资源，而当前视觉编码器通常对图像尺寸敏感且计算固定，效率低下。

Method: 提出受生物视觉中行为层面效率启发的设计理念，并给出一个针对图像分类的概念验证实现，展示任务依赖型计算复杂度的可行性。

Result: 通过概念验证的分类模型证明了动态、任务驱动的编码器在性能和资源利用上具有潜力，表明该方向可行且值得进一步研究。

Conclusion: 本文主张下一代视觉编码器应对图像尺寸不敏感并以任务为驱动，通过动态分配计算资源实现更高效的视觉处理。

Abstract: This position paper argues that the next generation of vision encoders should
be image size agnostic and task driven. The source of our inspiration is
biological. Not a structural aspect of biological vision, but a behavioral
trait -- efficiency. We focus on a couple of ways in which vision in nature is
efficient, but modern vision encoders not. We -- humans and animals -- deal
with vast quantities of visual data, and need to be smart where we focus our
limited energy -- it depends on the task. It is our belief that vision encoders
should be dynamic and the computational complexity should depend on the task at
hand rather than the size of the image. We, also, provide concrete first steps
towards our vision -- a proof-of-concept solution for image classification.
Despite classification being not very representative for what we are trying to
achieve, it shows that our approach is feasible and promising.

</details>


### [47] [Attention Mechanism in Randomized Time Warping](https://arxiv.org/abs/2508.16366)
*Yutaro Hiraoka,Kazuya Okamura,Kota Suto,Kazuhiro Fukui*

Main category: cs.CV

TL;DR: RTW权重与自注意力权重高度相似，但RTW的全局序列视角使其在动作识别上优于基于局部自注意力的Transformer，实验提升约5%。


<details>
  <summary>Details</summary>
Motivation: 揭示RTW与自注意力之间的内在联系，解释为何传统时序比对方法在现代深度学习架构中仍有优势，并探索两者的互补性以改进动作识别。

Method: 将RTW与Transformer的自注意力在权重空间进行比较，使用典型角（canonical angles）分析两者权重矩阵的相似性，并在Something-Something V2数据集上进行性能对比实验。

Result: 两种方法产生的权重模式高度相关（平均相关系数0.80），实验表明RTW比Transformer在Something-Something V2上高约5%的性能。

Conclusion: RTW的权重可被解释为自注意力权重，两者在权重模式上高度相似（平均相关为0.80），但RTW以全局视角操作序列而自注意力通常是局部视角，这使得RTW在动作识别任务上（如Something-Something V2）比Transformer表现更好，提升约5%。

Abstract: This paper reveals that we can interpret the fundamental function of
Randomized Time Warping (RTW) as a type of self-attention mechanism, a core
technology of Transformers in motion recognition. The self-attention is a
mechanism that enables models to identify and weigh the importance of different
parts of an input sequential pattern. On the other hand, RTW is a general
extension of Dynamic Time Warping (DTW), a technique commonly used for matching
and comparing sequential patterns. In essence, RTW searches for optimal
contribution weights for each element of the input sequential patterns to
produce discriminative features. Although the two approaches look different,
these contribution weights can be interpreted as self-attention weights. In
fact, the two weight patterns look similar, producing a high average
correlation of 0.80 across the ten smallest canonical angles. However, they
work in different ways: RTW attention operates on an entire input sequential
pattern, while self-attention focuses on only a local view which is a subset of
the input sequential pattern because of the computational costs of the
self-attention matrix. This targeting difference leads to an advantage of RTW
against Transformer, as demonstrated by the 5\% performance improvement on the
Something-Something V2 dataset.

</details>


### [48] [A Lightweight Group Multiscale Bidirectional Interactive Network for Real-Time Steel Surface Defect Detection](https://arxiv.org/abs/2508.16397)
*Yong Zhang,Cunjian Chen,Qiang Gao,Yi Wang,Bin Fang*

Main category: cs.CV

TL;DR: 本文提出GMBINet，通过GMBI模块（group-wise多尺度、BPFI、EWMS）有效增强多尺度特征提取与交互，实现极低参数量下的实时高效表面缺陷检测，适合工业场景。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习表面缺陷检测方法计算复杂、推理慢，不利于工业实时部署；现有轻量化多分支方法虽用DSConv但计算开销仍高且跨尺度交互不足。

Method: 提出了GMBI模块，包含group-wise多尺度特征提取、BPFI双向渐进特征交互和无参数EWMS乘加操作，实现跨尺度交互且不增加计算开销；整体网络轻量化，基于DSConv多分支但采用分组策略降低复杂度。

Result: 在SD-Saliency-900和NRSD-MN上，GMBINet在512分辨率下实现GPU 1048 FPS、CPU 16.53 FPS，仅0.19 M参数并保持竞争性精度；在NEU-CLS上也展示了良好泛化能力。

Conclusion: GMBINet在保持极低参数量的同时实现了高帧率与竞争性精度，适合资源受限的工业部署。

Abstract: Real-time surface defect detection is critical for maintaining product
quality and production efficiency in the steel manufacturing industry. Despite
promising accuracy, existing deep learning methods often suffer from high
computational complexity and slow inference speeds, which limit their
deployment in resource-constrained industrial environments. Recent lightweight
approaches adopt multibranch architectures based on depthwise separable
convolution (DSConv) to capture multiscale contextual information. However,
these methods often suffer from increased computational overhead and lack
effective cross-scale feature interaction, limiting their ability to fully
leverage multiscale representations. To address these challenges, we propose
GMBINet, a lightweight framework that enhances multiscale feature extraction
and interaction through novel Group Multiscale Bidirectional Interactive (GMBI)
modules. The GMBI adopts a group-wise strategy for multiscale feature
extraction, ensuring scale-agnostic computational complexity. It further
integrates a Bidirectional Progressive Feature Interactor (BPFI) and a
parameter-free Element-Wise Multiplication-Summation (EWMS) operation to
enhance cross-scale interaction without introducing additional computational
overhead. Experiments on SD-Saliency-900 and NRSD-MN datasets demonstrate that
GMBINet delivers competitive accuracy with real-time speeds of 1048 FPS on GPU
and 16.53 FPS on CPU at 512 resolution, using only 0.19 M parameters.
Additional evaluations on the NEU-CLS defect classification dataset further
confirm the strong generalization ability of our method, demonstrating its
potential for broader industrial vision applications beyond surface defect
detection. The dataset and code are publicly available at:
https://github.com/zhangyongcode/GMBINet.

</details>


### [49] [SAMFusion: Sensor-Adaptive Multimodal Fusion for 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2508.16408)
*Edoardo Palladin,Roland Dietze,Praveen Narayanan,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 在恶劣天气中，新增NIR门控相机和雷达，并以深度引导的注意力融合和BEV细化、Transformer解码器的可见度/距离加权，显著提升自动驾驶目标检测鲁棒性（雾天脆弱行人远距提升约17.2 AP）。


<details>
  <summary>Details</summary>
Motivation: 动机是现有多模态融合方法在理想条件下表现良好，但在恶劣天气或低能见度场景（如浓雾、降雪、传感器污染）时性能下降，需要引入更鲁棒的感知链路与跨模态协同策略。

Method: 方法包含将RGB、LiDAR、近红外门控相机（NIR gated）和雷达四种模态通过注意力引导的深度基融合策略，在BEV平面上进一步进行学习式细化，并由一个Transformer解码器根据距离与能见度动态加权各模态输出。

Result: 在具有挑战性的雾天场景下，本方法在远距离且脆弱行人检测上相比次优方法提升了约17.2 AP，整体提升了在边缘天气条件下的检测可靠性。

Conclusion: 本论文提出了一种面向恶劣天气的多传感器融合方法，能够在浓雾、降雪、低光及传感器遮挡等边缘场景中提升自动驾驶目标检测的鲁棒性。

Abstract: Multimodal sensor fusion is an essential capability for autonomous robots,
enabling object detection and decision-making in the presence of failing or
uncertain inputs. While recent fusion methods excel in normal environmental
conditions, these approaches fail in adverse weather, e.g., heavy fog, snow, or
obstructions due to soiling. We introduce a novel multi-sensor fusion approach
tailored to adverse weather conditions. In addition to fusing RGB and LiDAR
sensors, which are employed in recent autonomous driving literature, our sensor
fusion stack is also capable of learning from NIR gated camera and radar
modalities to tackle low light and inclement weather. We fuse multimodal sensor
data through attentive, depth-based blending schemes, with learned refinement
on the Bird's Eye View (BEV) plane to combine image and range features
effectively. Our detections are predicted by a transformer decoder that weighs
modalities based on distance and visibility. We demonstrate that our method
improves the reliability of multimodal sensor fusion in autonomous vehicles
under challenging weather conditions, bridging the gap between ideal conditions
and real-world edge cases. Our approach improves average precision by 17.2 AP
compared to the next best method for vulnerable pedestrians in long distances
and challenging foggy scenes. Our project page is available at
https://light.princeton.edu/samfusion/

</details>


### [50] [HAMSt3R: Human-Aware Multi-view Stereo 3D Reconstruction](https://arxiv.org/abs/2508.16433)
*Sara Rojas,Matthieu Armando,Bernard Ghamen,Philippe Weinzaepfel,Vincent Leroy,Gregory Rogez*

Main category: cs.CV

TL;DR: HAMSt3R使用蒸馏得到的DUNE编码器并增加分割、DensePose与深度头，实现无需优化的前馈联合人体与场景三维重建，在人体重建与通用三维重建任务上均表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法在户外静态场景表现良好，但难以在以人为中心、场景与人体交互复杂的场景中准确重建；因此希望设计能同时恢复人体与场景的统一三维表示。

Method: 方法基于DUNE编码器（通过蒸馏自MASt3R与multi-HMR等模型）来强化对场景与人体的理解，并增添了多人分割、DensePose稠密对应与深度预测等网络头，从而输出包含人体语义信息的稠密三维点图。模型为端到端前馈结构，无需复杂优化流程。

Result: 在EgoHumans与EgoExo4D两个包含多样人体场景的基准上，方法能够有效重建人体，同时在传统多视图立体与多视图位姿回归任务上依然保持较强性能，体现了良好的泛化能力。

Conclusion: 本文提出了HAMSt3R，一种针对稀疏、未校准多视图图像的人体与场景联合三维重建方法，继承并扩展了MASt3R以更好地处理以人为中心的场景。

Abstract: Recovering the 3D geometry of a scene from a sparse set of uncalibrated
images is a long-standing problem in computer vision. While recent
learning-based approaches such as DUSt3R and MASt3R have demonstrated
impressive results by directly predicting dense scene geometry, they are
primarily trained on outdoor scenes with static environments and struggle to
handle human-centric scenarios. In this work, we introduce HAMSt3R, an
extension of MASt3R for joint human and scene 3D reconstruction from sparse,
uncalibrated multi-view images. First, we exploit DUNE, a strong image encoder
obtained by distilling, among others, the encoders from MASt3R and from a
state-of-the-art Human Mesh Recovery (HMR) model, multi-HMR, for a better
understanding of scene geometry and human bodies. Our method then incorporates
additional network heads to segment people, estimate dense correspondences via
DensePose, and predict depth in human-centric environments, enabling a more
comprehensive 3D reconstruction. By leveraging the outputs of our different
heads, HAMSt3R produces a dense point map enriched with human semantic
information in 3D. Unlike existing methods that rely on complex optimization
pipelines, our approach is fully feed-forward and efficient, making it suitable
for real-world applications. We evaluate our model on EgoHumans and EgoExo4D,
two challenging benchmarks con taining diverse human-centric scenarios.
Additionally, we validate its generalization to traditional multi-view stereo
and multi-view pose regression tasks. Our results demonstrate that our method
can reconstruct humans effectively while preserving strong performance in
general 3D reconstruction tasks, bridging the gap between human and scene
understanding in 3D vision.

</details>


### [51] [HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images](https://arxiv.org/abs/2508.16465)
*Anilkumar Swamy,Vincent Leroy,Philippe Weinzaepfel,Jean-Sébastien Franco,Grégory Rogez*

Main category: cs.CV

TL;DR: HOSt3R提出了一种无关键点、无需模板与相机内参的单目手-物体3D变换估计并结合多视图重建，能在复杂场景和未见物体上实现SOTA级别的重建与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法依赖关键点检测（SfM、手关键点），在复杂物体几何、弱纹理和遮挡下表现差，影响泛化与可扩展性；因此需要一种稳健、无关键点依赖的方法。

Method: HOSt3R通过直接从单目运动视频/图像估计手-物体三维变换，避免使用SfM或手关键点优化；将该变换估计与多视角重建管线结合，实现手物体形状恢复。

Result: 在SHOWMe基准上达到了目标任务（无物体模板的手物体3D变换与形状估计）的SOTA；在HO3D数据上的实验表明对未见过的物体类别具有泛化能力。

Conclusion: 该文提出了一种无需关键点检测的单目视频手物体三维变换估计与多视角重建方法HOSt3R，实现了无模板、无相机内参限制的通用手物体3D重建。

Abstract: Hand-object 3D reconstruction has become increasingly important for
applications in human-robot interaction and immersive AR/VR experiences. A
common approach for object-agnostic hand-object reconstruction from RGB
sequences involves a two-stage pipeline: hand-object 3D tracking followed by
multi-view 3D reconstruction. However, existing methods rely on keypoint
detection techniques, such as Structure from Motion (SfM) and hand-keypoint
optimization, which struggle with diverse object geometries, weak textures, and
mutual hand-object occlusions, limiting scalability and generalization. As a
key enabler to generic and seamless, non-intrusive applicability, we propose in
this work a robust, keypoint detector-free approach to estimating hand-object
3D transformations from monocular motion video/images. We further integrate
this with a multi-view reconstruction pipeline to accurately recover
hand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely
on pre-scanned object templates or camera intrinsics, and reaches
state-of-the-art performance for the tasks of object-agnostic hand-object 3D
transformation and shape estimation on the SHOWMe benchmark. We also experiment
on sequences from the HO3D dataset, demonstrating generalization to unseen
object categories.

</details>


### [52] [Arbitrary-Scale 3D Gaussian Super-Resolution](https://arxiv.org/abs/2508.16467)
*Huimin Zeng,Yue Bai,Yun Fu*

Main category: cs.CV

TL;DR: 提出一个集成的、支持任意尺度的3D Gaussian超分框架，实现高质量、结构一致且实时的HR渲染，单模型可覆盖整数与非整数放大倍数。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS超分方法只能对固定倍数进行高分辨率渲染或通过后处理上采样，前者不灵活、后者增加复杂性并降低效率；因此需要一种能在单模型下高效渲染任意尺度HR视图且避免混叠的方案。

Method: 方法包括三个核心模块：尺度感知渲染（scale-aware rendering）、基于生成式先验的优化（generative prior-guided optimization）和渐进超分（progressive super-resolving）；同时支持整数与非整数倍尺度渲染。

Result: 在大量实验中，所提方法在任意尺度HR渲染上表现优异，相比原始3DGS提升约6.59 dB PSNR，保持结构一致性并在1080p下实现85 FPS实时渲染。

Conclusion: 该论文提出了一种集成框架，实现了单个3D Gaussian模型支持任意尺度的超分渲染，兼顾了渲染质量、结构一致性与实时性。

Abstract: Existing 3D Gaussian Splatting (3DGS) super-resolution methods typically
perform high-resolution (HR) rendering of fixed scale factors, making them
impractical for resource-limited scenarios. Directly rendering arbitrary-scale
HR views with vanilla 3DGS introduces aliasing artifacts due to the lack of
scale-aware rendering ability, while adding a post-processing upsampler for
3DGS complicates the framework and reduces rendering efficiency. To tackle
these issues, we build an integrated framework that incorporates scale-aware
rendering, generative prior-guided optimization, and progressive
super-resolving to enable 3D Gaussian super-resolution of arbitrary scale
factors with a single 3D model. Notably, our approach supports both integer and
non-integer scale rendering to provide more flexibility. Extensive experiments
demonstrate the effectiveness of our model in rendering high-quality
arbitrary-scale HR views (6.59 dB PSNR gain over 3DGS) with a single model. It
preserves structural consistency with LR views and across different scales,
while maintaining real-time rendering speed (85 FPS at 1080p).

</details>


### [53] [Seeing Clearly, Forgetting Deeply: Revisiting Fine-Tuned Video Generators for Driving Simulation](https://arxiv.org/abs/2508.16512)
*Chun-Peng Chang,Chen-Yu Wang,Julian Schmidt,Holger Caesar,Alain Pagani*

Main category: cs.CV

TL;DR: Fine-tuning for better visuals on driving data can hurt dynamic accuracy; simple continual learning with domain replay balances both.


<details>
  <summary>Details</summary>
Motivation: To assess whether recent high-quality video generation models remain reliable for safety-critical driving applications by evaluating trade-offs between visual realism and dynamic/spatial accuracy when fine-tuned on driving data.

Method: Empirical analysis: fine-tuning existing video generation approaches on driving datasets; measuring visual fidelity and spatial accuracy; analyzing correlations between scene regularity and objective alignment; testing continual learning with replay from diverse domains.

Result: Fine-tuning increased visual realism but reduced fine-grained dynamic/spatial accuracy; continual learning via replay from diverse domains mitigated the trade-off, preserving spatial accuracy while keeping good visual quality.

Conclusion: Fine-tuning video generation models on structured driving datasets can improve visual fidelity but may degrade spatial accuracy of dynamic elements due to a misalignment between visual quality and dynamic understanding objectives.

Abstract: Recent advancements in video generation have substantially improved visual
quality and temporal coherence, making these models increasingly appealing for
applications such as autonomous driving, particularly in the context of driving
simulation and so-called "world models". In this work, we investigate the
effects of existing fine-tuning video generation approaches on structured
driving datasets and uncover a potential trade-off: although visual fidelity
improves, spatial accuracy in modeling dynamic elements may degrade. We
attribute this degradation to a shift in the alignment between visual quality
and dynamic understanding objectives. In datasets with diverse scene structures
within temporal space, where objects or perspective shift in varied ways, these
objectives tend to highly correlated. However, the very regular and repetitive
nature of driving scenes allows visual quality to improve by modeling dominant
scene motion patterns, without necessarily preserving fine-grained dynamic
behavior. As a result, fine-tuning encourages the model to prioritize
surface-level realism over dynamic accuracy. To further examine this
phenomenon, we show that simple continual learning strategies, such as replay
from diverse domains, can offer a balanced alternative by preserving spatial
accuracy while maintaining strong visual quality.

</details>


### [54] [Towards Open World Detection: A Survey](https://arxiv.org/abs/2508.16527)
*Andrei-Stefan Bulzan,Cosmin Cernazanu-Glavan*

Main category: cs.CV

TL;DR: 本文综述视觉领域多个子任务，提出Open World Detection作为统一术语，讨论子领域的历史、方法、数据集与相互融合趋势，展望最终向“感知”领域的整合。


<details>
  <summary>Details</summary>
Motivation: 随着各个视觉任务的成功与复杂化，研究趋向于把多个专门化子领域整合，提出一个通用的、类无关的检测框架以应对开放世界中的未知对象检测与理解的需求。

Method: 通过回顾视觉研究史，梳理关键概念、方法与数据集，覆盖从显著性检测、前景/背景分割、OOD检测到开放世界目标检测、零样本检测与视觉大语言模型等多个子领域，分析它们的重叠与趋同。

Result: 文章构建了一张当前视觉检测领域的发展路线图，阐明了不同子领域的交叉点与发展趋势，并提出OWD作为统领未来感知研究的潜在统一范式。

Conclusion: 本文提出Open World Detection(OWD)作为统一术语，将类无关且通用的检测模型在视觉领域中归为一类，主张各视觉子领域正在趋于融合，并最终可能统一为“感知”这一更广的领域。

Abstract: For decades, Computer Vision has aimed at enabling machines to perceive the
external world. Initial limitations led to the development of highly
specialized niches. As success in each task accrued and research progressed,
increasingly complex perception tasks emerged. This survey charts the
convergence of these tasks and, in doing so, introduces Open World Detection
(OWD), an umbrella term we propose to unify class-agnostic and generally
applicable detection models in the vision domain. We start from the history of
foundational vision subdomains and cover key concepts, methodologies and
datasets making up today's state-of-the-art landscape. This traverses topics
starting from early saliency detection, foreground/background separation, out
of distribution detection and leading up to open world object detection,
zero-shot detection and Vision Large Language Models (VLLMs). We explore the
overlap between these subdomains, their increasing convergence, and their
potential to unify into a singular domain in the future, perception.

</details>


### [55] [MV-RAG: Retrieval Augmented Multiview Diffusion](https://arxiv.org/abs/2508.16577)
*Yosef Dayani,Omer Benishu,Sagie Benaim*

Main category: cs.CV

TL;DR: MV-RAG通过检索真实2D图像并用混合训练的多视角扩散模型进行条件生成，解决了文本到3D在域外/稀有概念上的失败，显著提升3D一致性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D扩散先验的文本到3D方法在处理域外或稀有概念时表现欠佳，生成结果容易不一致或不准确；引入真实世界检索图像并让多视角扩散模型以其为条件，可弥补先验缺陷，提高生成质量。

Method: 提出了MV-RAG流水线：先从大规模2D数据库检索相关图像，再用检索到的图像作为条件输入到多视角扩散模型。训练策略为混合训练：一是用带增强条件视图的结构化多视角数据模拟检索时的视图多样性以训练视图特定重建；二是用检索到的真实2D图像集合并采用‘保留视图预测’任务，让模型从其他视图预测被保留视图以从2D数据中学习3D一致性。

Result: 在新构建的OOD提示集合上，MV-RAG相比最先进的文本到3D、图像到3D和个性化基线显著提高了3D一致性、照片真实感和文本遵从性，同时在标准基准上仍保持有竞争力的性能。

Conclusion: MV-RAG能有效提升文本到3D生成在域外或稀有概念上的一致性和准确性，通过检索相关真实2D图像并以多视角扩散模型为条件生成多视图输出，实现更稳定的3D重建。

Abstract: Text-to-3D generation approaches have advanced significantly by leveraging
pretrained 2D diffusion priors, producing high-quality and 3D-consistent
outputs. However, they often fail to produce out-of-domain (OOD) or rare
concepts, yielding inconsistent or inaccurate results. To this end, we propose
MV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images
from a large in-the-wild 2D database and then conditions a multiview diffusion
model on these images to synthesize consistent and accurate multiview outputs.
Training such a retrieval-conditioned model is achieved via a novel hybrid
strategy bridging structured multiview data and diverse 2D image collections.
This involves training on multiview data using augmented conditioning views
that simulate retrieval variance for view-specific reconstruction, alongside
training on sets of retrieved real-world 2D images using a distinctive held-out
view prediction objective: the model predicts the held-out view from the other
views to infer 3D consistency from 2D data. To facilitate a rigorous OOD
evaluation, we introduce a new collection of challenging OOD prompts.
Experiments against state-of-the-art text-to-3D, image-to-3D, and
personalization baselines show that our approach significantly improves 3D
consistency, photorealism, and text adherence for OOD/rare concepts, while
maintaining competitive performance on standard benchmarks.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [56] [NeuroKoop: Neural Koopman Fusion of Structural-Functional Connectomes for Identifying Prenatal Drug Exposure in Adolescents](https://arxiv.org/abs/2508.16414)
*Badhan Mazumder,Aline Kotoski,Vince D. Calhoun,Dong Hye Ye*

Main category: q-bio.NC

TL;DR: NeuroKoop 用神经 Koopman 算子在潜在空间融合结构与功能脑网络，通过图神经网络提升对产前药物暴露的分类与生物学解释性。


<details>
  <summary>Details</summary>
Motivation: 传统多模态脑网络分析方法难以充分融合结构与功能互补信息，限制了对产前暴露影响的生物学理解与预测性能，因而需要一种能在潜在空间统一表示并捕捉跨模态动力学关系的新方法。

Method: 构建以源基形态测量(SBM)和功能网络连接(FNC)为输入的结构化脑图，使用图神经网络学习节点嵌入，再通过神经 Koopman 算子在潜在空间中对两模态嵌入进行对齐与融合，最后基于融合表示进行 PDE 状态分类并解释重要连接。

Result: 在 ABCD 大样本队列上，NeuroKoop 在分类性能上优于若干基线模型，并识别出若干显著的结构-功能连接通路，指示 PDE 可能影响的脑区与网络。

Conclusion: NeuroKoop 提出并验证了一种基于 Koopman 算子和图神经网络的多模态脑网络融合框架，可提高对产前药物暴露 (PDE) 青少年分类的表现并揭示结构-功能连接的关键模式。

Abstract: Understanding how prenatal exposure to psychoactive substances such as
cannabis shapes adolescent brain organization remains a critical challenge,
complicated by the complexity of multimodal neuroimaging data and the
limitations of conventional analytic methods. Existing approaches often fail to
fully capture the complementary features embedded within structural and
functional connectomes, constraining both biological insight and predictive
performance. To address this, we introduced NeuroKoop, a novel graph neural
network-based framework that integrates structural and functional brain
networks utilizing neural Koopman operator-driven latent space fusion. By
leveraging Koopman theory, NeuroKoop unifies node embeddings derived from
source-based morphometry (SBM) and functional network connectivity (FNC) based
brain graphs, resulting in enhanced representation learning and more robust
classification of prenatal drug exposure (PDE) status. Applied to a large
adolescent cohort from the ABCD dataset, NeuroKoop outperformed relevant
baselines and revealed salient structural-functional connections, advancing our
understanding of the neurodevelopmental impact of PDE.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [57] [Combined Approximations for Uniform Operational Consistent Query Answering](https://arxiv.org/abs/2508.15814)
*Marco Calautti,Ester Livshits,Andreas Pieris,Markus Schneider*

Main category: cs.DB

TL;DR: 在满足自连接自由和广义超树宽有界的连接查询下，修复比例的近似计数在组合复杂度下可用高效近似方案（通过引入SpanTL并利用树自动机近似计数），但放宽这些限制则难以近似。


<details>
  <summary>Details</summary>
Motivation: 扩展现有仅对固定查询（数据复杂度）可近似的结果到查询作为输入（组合复杂度）的情况，以理解在更一般情形下能否有效近似CQA的修复比例。

Method: 引入计数复杂度类SpanTL，利用树自动机的近似计数结果来构造高效近似方案，并把问题归约或放入该复杂度类；证明在放宽自连接自由或无界广义超树宽时出现困难，给出不可能性的证据。

Result: 证明了对于自连接自由、广义超树宽有界的情形存在高效近似方案，并证明一旦放宽任一语法限制，近似方案不可行；构造了SpanTL并证明问题属于该类。

Conclusion: 本文证明在自连接自由且广义超树宽有界的布尔连接查询下，对于主键一致性修复问题，可以对“有多少修复使得查询成立”的比例给出高效近似方案；放宽这些语法限制后，近似方案不存在（或不太可能存在）。

Abstract: Operational consistent query answering (CQA) is a recent framework for CQA
based on revised definitions of repairs, which are built by applying a sequence
of operations (e.g., fact deletions) starting from an inconsistent database
until we reach a database that is consistent w.r.t. the given set of
constraints. It has been recently shown that there is an efficient
approximation for computing the percentage of repairs that entail a given query
when we focus on primary keys, conjunctive queries, and assuming the query is
fixed (i.e., in data complexity). However, it has been left open whether such
an approximation exists when the query is part of the input (i.e., in combined
complexity). We show that this is the case when we focus on self-join-free
conjunctive queries of bounded generelized hypertreewidth. We also show that it
is unlikely that efficient approximation schemes exist once we give up one of
the adopted syntactic restrictions, i.e., self-join-freeness or bounding the
generelized hypertreewidth. Towards the desired approximation, we introduce a
counting complexity class, called $\mathsf{SpanTL}$, show that each problem in
it admits an efficient approximation scheme by using a recent approximability
result about tree automata, and then place the problem of interest in
$\mathsf{SpanTL}$.

</details>


### [58] [MAAdvisor: Zero-Shot Index Advisor using Multi-Agent LLMs](https://arxiv.org/abs/2508.16044)
*Zhaodonghui Li,Haitao Yuan,Jiachen Shi,Hao Zhang,Yu Rong,Gao Cong*

Main category: cs.DB

TL;DR: 提出基于多智能体的零样本LLM索引推荐系统MAAdvisor，通过任务分解与多代理协作，在效率、泛化性及性能上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有启发式方法计算开销大，学习型方法泛化性差，基于提示的调优方法无法达到SOTA且示例准备代价高。需要一种高效、通用且无需示例的索引推荐方法。

Method: 将整体任务解构为全局代理负责流程控制（规划、组合、反思）与局部代理负责具体索引选择与修订，利用大型语言模型通过多轮交互协同工作，进行零样本推理。

Result: 实验显示MAAdvisor在性能上优于传统启发式方法和学习/提示型方法，且在效率和零样本推理能力上具有显著优势。

Conclusion: MAAdvisor提出了一种多智能体的零样本索引顾问，通过将索引推荐分解为规划、选择、组合、修订与反思等子步骤，并为每个子步骤设计LLM嵌入的专用代理，从而在不需示例训练的情况下实现高效、可推广的索引选择。

Abstract: Index recommendation is one of the most important problems in database
management system (DBMS) optimization. Given queries and certain index-related
constraints, traditional methods rely on heuristic optimization or
learning-based models to select effective indexes and improve query
performance. However, heuristic optimization suffers from high computation
time, and learning-based models lose generalisability due to training for
different workloads and database schemas. With the recent rapid development of
large language models (LLMs), methods using prompt tuning have been proposed to
enhance the efficiency of index selection. However, such methods still can not
achieve the state-of-the-art (SOTA) results, and preparing the index selection
demonstrations is also resource-intensive. To address these issues, we propose
MAAdvisor, a zero-shot LLM-based index advisor with a multi-agent framework. We
decompose the index recommendation problem into sub-steps, including planning,
selection, combination, revision, and reflection. A set of LLM-embedded agents
is designed to handle each one of the different sub-steps. Our method utilizes
global agents to control the index selection process and local agents to select
and revise indexes. Through extensive experiments, we show that our proposed
MAAdvisor not only achieves the SOTA performance compared to the heuristic
methods, but also outperforms learning-based and prompt-based methods with
higher efficiency and better zero-shot inference ability.

</details>


### [59] [Attribute Filtering in Approximate Nearest Neighbor Search: An In-depth Experimental Study](https://arxiv.org/abs/2508.16263)
*Mocheng Li,Xiao Yan,Baotong Lu,Yue Zhang,James Cheng,Chenhao Ma*

Main category: cs.DB

TL;DR: 这篇工作构建了一个统一的Filtering ANN框架，系统分类并比较了多种算法，在大规模数据上进行广泛实验，分析关键组件影响，给出实践建议并开源代码。


<details>
  <summary>Details</summary>
Motivation: 随着结构化与非结构化数据融合，需在向量相似搜索中同时满足属性约束，涌现出多种Filtering ANN算法，但缺乏统一分析与比较，难以选择合适方法。

Method: 构建统一接口并基于属性类型与过滤策略设计分类体系；分解分析算法的关键组件（索引结构、剪枝策略、入口点选择）；在4个数据集（最大1000万条）上实验比较10种算法与12种配置，覆盖合成与真实属性以及不同选择性；并做深入组件级消融分析。

Result: 提出统一接口与分类、广泛实验比较了现有方法的优势与局限，揭示剪枝、入口点选择与边过滤开销对性能的显著影响，给出实际选型指南并开源实现。

Conclusion: 该论文提出了一个统一的Filtering ANN接口，对现有算法进行系统分类与比较；通过实验验证了不同方法在各类数据和选择性下的性能差异，并分析了关键组件对整体性能的影响，最后给出实践建议与未来研究方向。

Abstract: With the growing integration of structured and unstructured data, new methods
have emerged for performing similarity searches on vectors while honoring
structured attribute constraints, i.e., a process known as Filtering
Approximate Nearest Neighbor (Filtering ANN) search. Since many of these
algorithms have only appeared in recent years and are designed to work with a
variety of base indexing methods and filtering strategies, there is a pressing
need for a unified analysis that identifies their core techniques and enables
meaningful comparisons.
  In this work, we present a unified Filtering ANN search interface that
encompasses the latest algorithms and evaluate them extensively from multiple
perspectives. First, we propose a comprehensive taxonomy of existing Filtering
ANN algorithms based on attribute types and filtering strategies. Next, we
analyze their key components, i.e., index structures, pruning strategies, and
entry point selection, to elucidate design differences and tradeoffs. We then
conduct a broad experimental evaluation on 10 algorithms and 12 methods across
4 datasets (each with up to 10 million items), incorporating both synthetic and
real attributes and covering selectivity levels from 0.1% to 100%. Finally, an
in-depth component analysis reveals the influence of pruning, entry point
selection, and edge filtering costs on overall performance. Based on our
findings, we summarize the strengths and limitations of each approach, provide
practical guidelines for selecting appropriate methods, and suggest promising
directions for future research. Our code is available at:
https://github.com/lmccccc/FANNBench.

</details>
