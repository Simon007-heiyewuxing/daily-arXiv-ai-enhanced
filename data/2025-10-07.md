<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: SoC-DT: a differentiable, personalized reaction-diffusion framework with an IMEX-SoC solver that models surgery/chemo/radiotherapy to predict post-treatment tumor imaging, outperforming classical and neural baselines.


<details>
  <summary>Details</summary>
Motivation: Enable accurate prediction of tumor trajectories under standard-of-care therapies by integrating treatment effects and patient variability into mechanistic models to improve treatment planning and progression forecasting.

Method: Unified reaction-diffusion PDE model with discrete modules for surgery, chemotherapy, radiotherapy, personalized parameters from genomics/demographics; solved using an implicit-explicit exponential time-differencing (IMEX-SoC) solver ensuring stability and positivity; trained/evaluated on synthetic and glioma datasets against baselines.

Result: Proposed SoC-DT, a differentiable framework combining reaction-diffusion tumor growth models, discrete SoC interventions, and personalization via genomics/demographics; introduced IMEX-SoC solver; showed superior performance on synthetic and glioma data versus PDE baselines and neural models.

Conclusion: SoC-DT bridges mechanistic interpretability and differentiable solvers to enable biologically consistent, patient-specific digital twins for oncology, improving tumor dynamics prediction under SoC treatments.

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 该论文提出一个结合分布式多GPU推理框架与交互式可视化平台的混合系统，用于分析视频集中名人动态。推理端通过优化的ONNX模型、异构批量推理和高吞吐并行处理实现可扩展的带时间戳的出现记录生成；可视化端将记录转为多种图表（出现频率、时长、共现矩阵、网络图、堆叠面积图、热力图等），提供名人重要性、屏幕时间分布、时序动态和共现关系的多维洞察，支持交互式探索并助力娱乐分析与内容策略。


<details>
  <summary>Details</summary>
Motivation: 视频内容量剧增，需自动化解析人物结构与互动关系以支持内容制作、受众分析与媒体研究。

Method: 构建多GPU分布式推理引擎（ONNX优化模型、异构批量和高吞吐并行调度）生成带时间戳的人物出现记录；将记录输入交互式可视化模块，提供多种视图（频率、时长、饼图、共现矩阵、网络图、堆叠面积、季节对比、热力图），并支持动态过滤和时序查询。

Result: 实现可扩展的视频人物识别与分析流水线，能处理大量视频并产出多维可视化分析，帮助发现名人屏幕时间分布、共同出现模式与季节性变化。

Conclusion: 该系统有效整合分布式识别和结构化可视化，能在大规模视频数据上高效生成精细的名人出现记录，并通过丰富的可视化工具揭示人物 prominence、共现与时序模式，满足娱乐分析与受众研究需求。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 在跨域海洋塑料检测任务中，轻量级CNN（MobileNetV2）经监督微调后表现最佳（F1=0.97），比大型模型更稳健。所有微调模型Precision≈99%，Recall差异较大；零样本CLIP召回较高但误报多，Gemini精确但召回也高。错误多由珊瑚纹理、悬浮颗粒和高光引起。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染严峻，需要可靠的水下垃圾自动检测。但模型在不同数据源间常因域移性能下降，因此评估跨域鲁棒性并比较微调模型与零样本预训练模型的表现很重要。

Method: 训练并微调多种模型（MobileNetV2, ResNet-18, EfficientNet-B0, DeiT-Tiny, ViT-B16）在一个带标签的水下数据集；构建平衡的跨域测试集（正例来自不同来源，负例来自训练域）；评估性能（Precision, Recall, F1）；并测试零样本模型（CLIP ViT-L14, Gemini 2.0 Flash）；进行错误分析以识别常见混淆源。

Result: Compact CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision transformers (DeiT-Tiny, ViT-B16) were trained on one underwater dataset and evaluated cross-domain on a balanced test set with plastic-positive images from a different source and negatives from the training domain. Two zero-shot models (CLIP ViT-L14, Gemini 2.0 Flash) were also evaluated. MobileNetV2 achieved best cross-domain F1 (0.97); all fine-tuned models had ~99% Precision but varied Recall. CLIP showed higher Recall (~80%) but lower Precision (~56%); Gemini showed high Precision (~99%) and Recall (~81%). Common errors involved coral textures, suspended particulates, and specular glare.

Conclusion: 经过监督训练的小型卷积网络能在跨域水下塑料检测中实现优秀泛化，而大规模预训练的视觉语言模型在零样本设置下展现互补优势。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP uses CLIP label retrieval (mCLIP, AraCLIP, Jina V4) + 21K translated Visual Genome labels to create Arabic prompts; passes labels+image to Qwen-VL/Gemini Pro Vision; mCLIP+Gemini highest BLEU-1(5.34%) and cosine(60.01%), AraCLIP+Qwen highest LLM-judge(36.33%)


<details>
  <summary>Details</summary>
Motivation: improve Arabic image captioning by grounding in interpretable visual labels and using CLIP-based retrieval with multimodal LLMs

Method: analysis of methods

Result: mCLIP+Gemini best BLEU-1 and cosine; AraCLIP+Qwen best LLM-judge; pipeline yields culturally coherent captions

Conclusion: Interpretable, label-grounded pipeline improves cultural/contextual Arabic captions though automatic metrics are low; different encoders excel on different metrics.

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 在SpaceNet地物分类任务上，比较EfficientNet-B0（CNN）与ViT-Base（Transformer）。在自然不平衡和人工平衡两种标签分布下，使用统一预处理和训练设定。结论为：在不平衡数据上两者在准确率上相近（约93%），但EfficientNet在宏F1与延迟上更优；在平衡数据上两者均表现更好且更接近，EfficientNet仍在效率上占优。公开了重现所需资源。


<details>
  <summary>Details</summary>
Motivation: 评估在遥感地物分类场景中，现代CNN与Vision Transformer在不同标签分布下的相对表现与实际部署效率，以指导模型选择与复现研究。

Method: 在SpaceNet数据集上进行受控对比：统一输入尺寸224×224、ImageNet归一化、轻量数据增强、40个epoch、单卡NVIDIA P100，比较指标包括准确率、macro-F1、balanced accuracy、per-class recall及模型大小与延迟。两种标签分布（不平衡五类与每类700张的平衡重采样）分别训练评估。

Result: 不平衡分割：EfficientNet-B0在测试集达93%准确率，宏F1更高且延迟更低；ViT-Base在准确率上接近93%但参数和运行时间更大。平衡分割：两模型性能提升，EfficientNet-B0达99%准确率，ViT-Base同样竞争，表明平衡样本减少了架构差距。已公开清单、训练日志与逐图预测以便复现。

Conclusion: 平衡数据会缩小CNN与ViT的性能差距，但EfficientNet-B0在效率（参数量、推理延迟）上保持优势；在不平衡任务上，CNN在宏F1和部分类别召回上更稳健。

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: Survey covers camera-based AI methods for VRU safety beyond detection, reviewing detection, tracking, trajectory and intent tasks, discussing datasets, models, deployment challenges, and future directions


<details>
  <summary>Details</summary>
Motivation: Existing surveys focus mainly on detection; need comprehensive coverage of vision-based tasks for proactive VRU protection in dynamic urban environments

Method: State-of-the-art review of camera-based AI sensing systems for VRU safety

Result: Systematic examination of four core tasks: detection/classification, tracking/reidentification, trajectory prediction, intent recognition; highlighted recent 5-year developments and trends

Conclusion: Provides foundational reference linking visual AI advances with practical deployment considerations, identifies open challenges in data, models, and deployment to guide future research

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: EOFM embeddings vary substantially with sensor architecture; this impacts cross-modal use and benchmarking; designers/users should account for sensor-specific factors


<details>
  <summary>Details</summary>
Motivation: Examine how different sensor architectures affect EOFM representations and highlight implications for design and use

Method: Representation analysis and evaluation

Result: Found high sensitivity of EOFM representation spaces to sensor architecture; differences can cause mismatches when using embeddings across modalities

Conclusion: Need for modality-aware training, standardized evaluation across sensors, and development of cross-sensor alignment techniques to ensure robust EOFM embeddings

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: Use inpainting-based, SAM-refined perturbations to generate realistic, localized edits that reveal morphological cues driving detection and traits, improving interpretability and trust in ecological vision models


<details>
  <summary>Details</summary>
Motivation: Opaque vision-model predictions limit trust and field adoption in ecological monitoring; need in-distribution, localized, photorealistic interventions that reveal fine-grained cues

Method: Inpainting-guided, perturbation-based explanation

Result: Photorealistic mask-localized edits (object removal/replacement and background replacement) using SAM-refined masks and inpainting; evaluated via flip rate, confidence drop, and expert review; localize diagnostic structures and avoid deletion artifacts

Conclusion: Approach produces domain-relevant, interpretable explanations suitable for expert validation and more trustworthy AI deployment in ecology.

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 综述医用图像分割（MIS）方法，从传统方法到深度学习，涵盖U-Net、注意力机制、GAN、Transformer及新兴趋势并以腰椎分割为案例。


<details>
  <summary>Details</summary>
Motivation: 系统梳理MIS领域发展，连接传统与现代技术，识别挑战与未来研究方向，并通过腰椎分割案例展示特定解剖区域的研究需求。

Method: 回顾传统分割（阈值、边缘、区域、聚类、模型驱动）和深度学习方法（CNN/FCN、U-Net及其变体），还讨论注意力机制、半监督、GAN、Transformer、混合架构与分布式学习框架。

Result: 总结现有方法优缺点，指出在小样本、跨域泛化、计算开销与可解释性方面的不足，提出融合策略、数据共享与新学习范式以改善临床实用性。

Conclusion: 尽管方法丰富且性能提高，但仍面临标签不足、域适应、可解释性和临床整合等挑战；混合方法、联邦学习、跨模态与主动学习是未来方向。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: 提出DECOR，一种对方向鲁棒的深度聚类方法，用于在有旋转/对齐变化和噪声的晶圆缺陷图中发现一致簇；在MixedWM38数据集上优于基线，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 晶圆质量测试数据复杂、无标签、类别不平衡且单片晶圆可含多种缺陷，传统聚类在存在方向变化与噪声时不可靠，故需设计对方向变化稳健的聚类方法。

Method: 构建一个深度聚类框架，显式建模方向鲁棒性（对旋转和对齐不变），使用神经网络学习缺陷图的特征并聚类；可能包含数据增强（旋转）、特征对齐模块或方向不变的表示学习，并在MixedWM38上无监督评估。

Result: 在MixedWM38数据集上，DECOR在聚类一致性和鲁棒性指标上优于现有基线，能在无需手动调参下稳定发现缺陷簇。

Conclusion: DECOR能在有方向变化和复杂缺陷的晶圆图像中稳定发现聚类，优于现有基线方法，适合自动视觉检测系统。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 论文提出用LSTM+注意力在不平衡表情分类上通过训练6类子集并对第7类进行纠错来恢复被排除类别，实验证明对小类识别有提升潜力，适合稀有事件检测任务。


<details>
  <summary>Details</summary>
Motivation: 应对人脸表情识别中类别严重不平衡的问题，希望通过模型的纠错能力提高对小众情绪（稀有事件）的识别，适用于反欺诈等需要检测少数类的应用场景。

Method: 构建LSTM神经网络并引入注意力机制聚焦面部关键区域，训练时对所有可能的6类子集进行训练，并对第7类（在训练中被排除）进行后续的错误修正实验。

Result: 实验表明对被排除的第7类存在可变程度的恢复能力：部分情绪类别恢复效果较好，部分较差；在测试集上对某些小类校正后关键指标（如精度/召回/其他）有所提高，证明方法在检测稀有类上有潜力。

Conclusion: 该文提出了基于LSTM+注意力机制的纠错方法，可在类别不平衡的人脸表情多分类任务中对被排除的类进行恢复，显示对小类识别性能有提升潜力。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: 本文构建了首个动态图表生成基准DCG-Bench与数据集DCG-8K，提出两阶段训练与联合代码-视觉奖励策略，训练出Qwen2.5-VL-DCG-3B，在三类任务上平均领先开源模型8.31%。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在静态图表生成与理解上已有进展，但在动态（动画）图表生成及理解方面研究不足，需建立基准和方法推动该方向发展。

Method: 构建三类任务（简单文本->图表、详细文本->图表、视频->图表）与包含指令-代码-视频三元组的DCG-8K；提出两阶段训练流程并采用Group Relative Policy Optimization结合Joint-Code-Visual Reward训练MLLM（Qwen2.5-VL-DCG-3B）。

Result: 在DCG-Bench上，提出模型比最佳开源MLLM平均提升8.31%，在3B参数规模下与专有模型表现接近；同时暴露现有模型在视觉到图表任务上的短板。

Conclusion: 本文提出了DCG-Bench评估框架和高质量数据集DCG-8K，并通过两阶段训练与联合代码-视觉奖励的策略生成了Qwen2.5-VL-DCG-3B模型，实验证明在动态图表生成任务上优于现有开源模型并接近专有模型性能。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出VoT——一个端到端的视觉里程计Transformer，直接回归相机位姿，免去稠密重建与BA，泛化性与速度优异。


<details>
  <summary>Details</summary>
Motivation: 现有单目视觉里程计依赖复杂的预训练组件与优化模块，受相机标定和超参敏感，且在真实场景泛化能力差；大型多模态3D模型虽能通用重建与位姿估计，但难以处理长视频和精确逐帧位姿。作者希望通过端到端学习简化管线并提升泛化与速度。

Method: VoT以可插拔的预训练编码器提取单目帧特征，随后用时空注意力模块建模全局关联，最后直接预测相邻帧间的相机运动，训练仅使用相机位姿作为监督，不依赖相机内参或稠密几何。

Result: 实验证明VoT能随数据量和更强的预训练主干扩展而提升性能，在不同相机运动和标定条件下表现鲁棒，精度超越传统方法且速度提高3倍以上。

Conclusion: 本文提出了一个端到端的单目视觉里程计方法VoT，利用Transformer在时空域建模帧间关系，直接回归相机位姿，省略了稠密重建与传统优化模块，具有更好的泛化性与计算效率。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 本文提出一种在推断时用侧信息指导采样的搜索算法，平衡探索与利用，提升扩散模型在多种逆问题中的重建效果，并优于梯度奖励方法。


<details>
  <summary>Details</summary>
Motivation: 利用扩散模型作为先验解决逆问题时常忽视侧信息，尤其在病态问题中侧信息能显著提升重建质量。论文动机在于设计一种能在推断时利用侧信息以平衡探索与利用的搜索算法，避免基于梯度的指导造成的作弊伪影。

Method: 设计了一种在采样过程中引入侧信息的搜索算法，该算法在探索（多样性）与利用（保留侧信息一致性）之间做平衡，替代或补充梯度基的奖励引导；方法可嵌入多种扩散重建管线，并通过大量任务实验证明其有效性。

Result: 提出了一种新的推断时搜索算法，用侧信息引导采样过程，能无缝集成到现有扩散模型重建流水线，并在多种逆问题（box掩膜修复、超分、运动/高斯/非线性/盲去模糊）上显著提升定性和定量表现，优于基于梯度的奖励指导等基线。代码开源。

Conclusion: 使用侧信息的搜索式推断能稳定提高扩散模型逆问题重建的质量，避免梯度引导的伪影问题，且易于集成到现有方法中，实验表明该方法在多种任务和基线下具有优势。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 综述声纳图像数据集现状，涵盖SSS、FLS、SAS、MBES、DIDSON等模态，分析分类/检测/分割/三维重建等应用，提供主表与时间线，指出数据稀缺与注释不全为主要瓶颈并给出改进建议。


<details>
  <summary>Details</summary>
Motivation: 声纳图像对水下探测与自治导航关键，但受限于可用训练/测试数据；综述并整理现有数据集以降低入门门槛、指引研究方向和促进数据共享。

Method: 系统性文献与数据集搜寻、模态分类与任务映射、构建主表与时间线、比较集特征（模态、规模、注释类型、公开可用性）并识别空白与趋势。

Result: 整理出多模态公开数据集清单，比较其大小和标注详情，发现大多数数据集样本少、注释不一致、任务覆盖不均；提出推动数据标准化、注释工具、跨域迁移学习与合成数据的建议。

Conclusion: 当前公开、标注良好的声纳图像数据集仍然稀缺，限制了水下声学视觉算法的发展；需要更多多样化、大规模且高质量注释的数据集，统一标准、跨模态融合和合成/仿真数据是未来方向。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出一种结合无镜头相机与隐式神经表示的系统，能在约46.6°×37.6°视锥内无专用设备地重建显示发射光场，推动便捷显示校准。


<details>
  <summary>Details</summary>
Motivation: 传统显示器校准需要专用测量设备和暗室，门槛高且不便普及；目标是去除专用硬件约束，使内容创作者可在日常环境中高效完成显示校准。

Method: 设计并共构造一台透镜缺失（lensless）相机并基于隐式神经表示（Implicit Neural Representation，INR）开发重建算法；采集不同视点的数据并在视锥约46.6°×37.6°范围内重建显示光场。

Result: 在无需专用仪器和暗室的条件下，提出的系统能够从宽视角范围内重建显示光场，为无障碍显示校准提供初步可行性证明。

Conclusion: 本文提出通过无镜头相机与隐式神经表示联合设计，实现无需专用设备即可从大视角范围重建显示器发出的光场，为显示器校准和表征提供便捷方法。

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: Provenance networks embed a learned KNN-like retrieval into model architecture to justify outputs by concrete training exemplars, improving explainability, robustness, and trust at the cost of extra computation and limited scale


<details>
  <summary>Details</summary>
Motivation: Make model decisions traceable to training exemplars to improve transparency, detect memorization/hallucination, and attribute credit to data

Method: Analyze provenance networks: architecture and training-data-driven explainability

Result: Provenance networks link predictions to weighted training examples, enabling verification of inclusion, anomaly detection, robustness to perturbations, and study of memorization-generalization trade-offs; incurs extra compute and scales to moderate datasets

Conclusion: Provenance networks offer a complementary, end-to-end explainability approach that uncovers data influence on predictions, helping address opaqueness, hallucination, and data-credit assignment, though further work needed for scaling and efficiency

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 作者从匹配视角统一单模与多模UAD，提出UCF对匹配代价体进行学习滤波以降噪和突出异常，广泛提升性能并在多场景创SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法依赖图像或特征层面的匹配，但匹配噪声被忽视，限制检测能力；同时多模态（如RGB-3D、RGB-Text）扩展导致方法孤立，缺乏统一理解与迁移。

Method: 先对测试样本与正常样本（同模或异模）进行匹配构建代价体，再用一个可学习的滤波模块对代价体进行多层注意力引导的精炼，输出更清晰的异常图。

Result: 提出Unified Cost Filtering (UCF)，通用后处理框架对任意UAD方法的异常代价体进行可学习滤波，利用多层注意力从测试样本引导，抑制匹配噪声并强化微小异常；在22个基准上提升多种方法并创SOTA。

Conclusion: UCF作为通用后处理模块能显著改善不同UAD方法在单模及多模场景的表现，证明从匹配噪声出发的统一视角有效，代码将开源。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 利用视觉-语言模型自动评估并修正工业图纸目标检测结果，提升检测质量并提供可解释反馈。


<details>
  <summary>Details</summary>
Motivation: 工业图纸数字化需要高精度目标检测，目前检测算法虽有进步，但缺乏自动化质量评估工具，无法有效发现漏检或错检。借助VLM的多模态推理能力，可弥补传统评估方法的不足，实现自动化、语义化的质量判断与优化引导。

Method: 利用VLM的多模态理解能力，输入图像和检测结果（bounding boxes与类别），通过自然语言提示或问答机制让VLM识别缺失、冗余或类别不一致的检测项，输出修正建议或置信度评估；并将VLM反馈用于迭代改进检测模型或后处理策略。

Result: 框架能够在复杂工业图纸上检测并指出缺失和不一致的目标，提升整体检测性能；实验证明在若干P&ID子集上，结合VLM反馈后检测精度和召回率均有明显提高，且能生成可解释的修正建议。

Conclusion: 该论文提出一种基于视觉-语言模型（VLM）的评估与反馈框架，用于自动评估工业图纸（如P&ID）中目标检测结果的质量，并能引导检测结果的修正与优化。

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 通过多任务学习将深度、3D坐标和边缘特征融入VLM，提出全对象与遮挡对象两种策略及其集成，显著提升VSR数据集的空间推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在空间推理、尤其三维场景和复杂物体布局上的表现不足，需引入显式空间信息以提升多模态模型对真实场景空间关系的理解。 

Method: 构建融合空间特征（深度图、3D坐标、边缘图）的多模态编码器；设计两种训练策略：对完整物体区域（SpatialViLT）与对被遮挡/掩码区域（MaskedSpatialViLT）；最后通过SpatialEnsemble集成两者预测。使用VSR数据集在方向、拓扑、接近性等子任务上评估。

Result: SpatialViLT通过将深度图、3D坐标和边缘图等空间特征融入视觉-语言模型，提升了对三维场景和复杂物体配置的空间推理能力。提出了两种变体（SpatialViLT和MaskedSpatialViLT）以及结合二者的SpatialEnsemble，在VSR数据集上的方向性、拓扑性和接近性关系等空间推理类别上取得了SOTA表现。

Conclusion: SpatialViLT系列模型通过显式空间特征和多任务训练增强了VLM的空间理解，尤其在方向、拓扑和接近关系判断上表现优异，为真实场景多模态理解提供了重要进展。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 通过将真实伪影场叠加到合成图像进行监督训练，DAE和U-Net都能有效减少两相OS-SI伪影，改善成像质量，且各有侧长。


<details>
  <summary>Details</summary>
Motivation: 两相OS-SI通过减少相位数加快采集但产生残余伪影，传统去噪方法难以消除这些伪影；受限于缺乏无伪影的真实光学切片作为监督标签，研究探索用合成训练对解决这一问题。

Method: 使用真实采集到的伪影场叠加到合成图像上生成训练对，训练两种网络：不对称去噪自编码器（DAE）和U-Net，然后在真实的两相OS-SI图像上评估它们的去伪影性能。

Result: 两种网络均能在真实OS-SI图像上提升图像清晰度，但对不同种类的伪影各有优势，表明基于合成数据的监督训练是可行且有效的。

Conclusion: 本文表明，通过在合成图像上添加真实的条纹伪影场进行有监督训练，编码器-解码器网络可以有效降低两相光学切片结构照明（OS-SI）中由减少采集相位数引起的伪影，从而改善图像清晰度并简化重建流程。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: 提出PEaRL：用通路激活分数替代基因水平表示，结合transformer与对比学习对齐组织学图像，比SOTA在三个癌症ST数据集上在基因和通路表达预测上显著更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖少量高变基因，范围受限且忽略协调的生物学程序。通过以通路为单位的表示，可捕获生物学程序并改善可解释性与跨模态对齐。

Method: 用ssGSEA计算每个空间点的通路激活分数；用transformer编码这些通路表示；用对比学习将通路表示与组织学图像特征对齐；在三个癌症ST数据集上评估基因与通路表达预测性能，与SOTA方法比较。

Result: PEaRL将转录组表示为基于ssGSEA计算的通路激活分数，并使用transformer与组织学特征通过对比学习对齐，从而降低维度、提高可解释性并增强跨模态对应性。

Conclusion: 以通路为基础的表示比基因水平嵌入更具生物学一致性和可解释性，能提高跨模态预测性能并推进计算病理学。

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS通过分层语义提示与双提示架构，实现了细粒度、可扩展的多模态医学影像分析，兼具分割与预后能力，并在多数数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像深度学习模型要么为特定任务而设计、缺乏泛化与预后能力，要么通用模型对任务条件化过于简单、缺乏医学语义理解。因此需要一个既能细粒度控制又具可扩展性的通用框架。

Method: 设计了分层文本控制与双提示（dual-prompt）架构，将视觉编码与层级语义提示融合；针对分割任务在多模态、多器官数据集上训练与评估；并通过将EHR信息以文本形式输入，实现影像+临床的预后预测。采用参数高效微调策略以适配新任务/中心。

Result: 在分割任务上涵盖三种成像模态、十个数据集、30多种器官/肿瘤类型；在10个数据集中8个超过task-specific和现有通用模型的性能；在头颈癌的预后预测上，融合EHR后CI=0.69；并展示了从不同中心/模态快速微调能力。

Conclusion: DuPLUS提出了一种基于视觉-语言的多模态医学影像分析框架，通过分层语义提示与双提示机制实现细粒度任务控制，并能扩展到预后预测，展示了较强的泛化性和参数高效微调能力。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出线程序列化YOLOv10与MobileSAM并行运行的移动优化两阶段检测+分割框架，在Houbara数据集上实现高精度与实时性能，并公开代码与40k标注数据集。


<details>
  <summary>Details</summary>
Motivation: Enable real-time, resource-efficient detection and segmentation of cryptic wildlife (Houbara Bustard) on edge/mobile devices using parallelized YOLOv10 detection and MobileSAM segmentation.

Method: Mobile-optimized two-stage threaded detection-segmentation

Result: Achieved strong detection (mAP50 0.9627, mAP75 0.7731, mAP95 0.7178) and segmentation (MobileSAM mIoU 0.7421); YOLOv10 runs at 43.7 ms/frame; released 40k annotated Houbara dataset and code.

Conclusion: 线程化并行化两阶段框架在有限计算资源下能显著降低延迟并保持高精度，适用于野生动物实时监测。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: 通过将注意力相对于柏拉图立体对称参考系来定义，Platonic Transformer在保持标准Transformer架构和成本的同时实现了对平移和柏拉图群对称的等变性，并等价于动态群卷积，带来可扩展的线性卷积变体和多任务上的竞争性能。


<details>
  <summary>Details</summary>
Motivation: 现有等变方法通常通过复杂且计算密集的设计牺牲Transformer的效率与灵活性；因此需要一种既保留Transformer效率又能引入几何对称性的方案。

Method: 在标准Transformer架构不变且计算开销相同的前提下，引入基于柏拉图立体对称群的参考系来定义注意力，从而实现参数共享。证明该注意力等价于动态群卷积，推导出可学习的自适应几何滤波器，并提出线性时间的可扩展卷积变体。

Result: 在CIFAR-10、ScanObjectNN、QM9和OMol25等视觉、三维点云和分子性质预测任务上，Platonic Transformer在不增加计算代价的情况下，通过几何约束取得了有竞争力的性能。

Conclusion: 本文提出了Platonic Transformer，通过将注意力机制相对于由柏拉图立体对称群定义的参考系来实现几何对称性的归纳偏置，平衡了等变性与Transformer效率之间的权衡。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 本文综述了域泛化语义分割领域，聚类评述各类方法，指出了基于foundation model的范式转变，并通过性能对比强调foundation model的重要性。


<details>
  <summary>Details</summary>
Motivation: 对提升深度神经网络在未知域上的泛化能力的需求，特别是语义分割任务在生物医学和自动驾驶等领域的重要性，推动了领域泛化（DG）研究的发展。本综述旨在系统整理该方向的研究进展，分析方法，识别最新的发展趋势并比较性能。

Method: 通过系统分类现有方法、综述每类方法的核心思想与技术路线，并进行全面的性能对比分析以评估各方法在不同数据集和设置下的表现。

Result: 对域泛化语义分割的现有方法进行了聚类和综述，指出了向基于foundation model的方法转变的范式转移，并通过广泛的性能比较展示了foundation model对域泛化性能的显著影响。

Conclusion: 基于foundation model的方法正在成为域泛化语义分割的主流方向，其对提高泛化性能具有显著作用；未来研究应进一步探索该范式及其与传统方法的结合。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出了一个先在图像-文本caption对预训练、再在图像-报告对微调的Transformer视觉-语言两阶段训练框架，用于自动生成内镜检查报告，旨在减少文书负担并提高临床效率。


<details>
  <summary>Details</summary>
Motivation: 内镜检查（EGD和结肠镜）在胃肠疾病诊断与管理中至关重要，但关联的文书工作繁重，影响临床效率并导致医师倦怠，因此需要自动化生成报告以减轻负担。

Method: 两阶段训练：第一阶段在通用图像-文本caption对上进行预训练，以获取通用的视觉-语言特征；第二阶段在图像-报告对上微调，使模型能够生成临床相关的检查发现。模型采用Transformer视觉编码器和文本解码器的架构。

Result: 方法可自动生成临床意义的内镜检查发现报告，有望在减轻医师工作量和提升患者护理方面发挥作用。

Conclusion: 该研究提出了一个基于Transformer的视觉编码器和文本解码器的两阶段训练框架，用于自动生成内镜（EGD和结肠镜）检查报告，旨在缓解胃肠科医师的文书负担。

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan maps sketches to 2D projected paths then uses diffusion to produce 3D trajectories from depth images; trained on synthetic and some human-labeled data; good real-world results


<details>
  <summary>Details</summary>
Motivation: make human-drawn sketches usable for drone path planning in real scenes

Method: analyze methods and approach

Result: zero-shot sim-to-real transfer; strong sim and real performance

Conclusion: modular design and mixed training data enable effective interpretation of human intent and robust 3D path generation

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 为防止基于潜变量的实时身份劫持，作者提出在潜变量上进行姿态条件大间隔对比学习以解缠身份信息，并用余弦相似度检测身份替换，效果优异且实时。


<details>
  <summary>Details</summary>
Motivation: AI视频会议通过发送紧凑的姿态-表情潜变量节省带宽，但该潜变量可被操纵导致实时冒名顶替，且生成的每帧为合成图像，使传统深伪检测失效，因此需要直接在潜变量层面检测身份劫持。

Method: 设计了一个基于姿态条件的大间隔对比编码器（pose-conditioned, large-margin contrastive encoder），对潜变量进行解缠，得到恒定的身份嵌入；使用简单的余弦相似度阈值检测非法身份替换。

Result: 在多个talking-head生成模型上，该方法优于现有傀儡式攻击防御，能实时运行，并对分布外场景具有强泛化能力。

Conclusion: 提出了一种无需查看重建视频的生物特征泄露防御方法，通过在传输的潜变量中分离出恒定身份线索并消除瞬时姿态和表情，从而检测身份劫持。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了REVEL任务：任意时间对任意目标通过交互拖拽精细控制自回归视频扩散模型生成的视频。发现拖拽导致潜在空间分布漂移与上下文干扰，提出无训练的DragStream方法：自适应分布自整策略和时空-频率选择性优化，能与现有模型集成并有效缓解问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型难以实现流式、对生成中视频在任意时间对任意对象作细粒度、可交互的拖拽控制；拖拽操作在潜在空间引起累积扰动并被上下文帧干扰，导致不可控或不自然的生成结果，需新任务定义与方法解决。

Method: 提出两大模块：1）自适应分布自整策略：利用邻帧统计量约束潜在嵌入的漂移，动态校正潜在分布；2）时空-频率选择性优化机制：在空间频率域有选择地传播视觉提示，保留有用上下文信息同时抑制干扰。整体为训练自由的后处理插入式方案，兼容已有模型。

Result: 在多组实验（包括DragVideo和SG-I2V等场景）中，DragStream有效限制潜在漂移、减少视觉伪影并提升拖拽交互的自然性与稳健性，证明其能增强视频扩散模型的可控性与交互性。

Conclusion: DragStream能在无训练条件下显著抑制拖拽引起的潜在分布漂移并减少上下文干扰，从而实现流式、细粒度的拖拽交互视频编辑和动画化，且可与现有自回归视频扩散模型无缝结合。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: GAS-MIL是一个可扩展的集合式MIL框架，用于高效整合多个Foundation Models的特征，避免手动特征选择与广泛微调，在多项病理图像分类任务中提升或保持性能，便于临床部署与后续多模态扩展。


<details>
  <summary>Details</summary>
Motivation: 解决将多个Foundation Models (FMs) 集成到病理学诊断任务中的选择与适配成本高且耗时的问题，通过无需手工挑选特征或大量任务特定微调的框架，利用多个FM的互补性提升诊断性能。

Method: 提出基于组聚合选择的多实例学习方法，自动融合多个FM的特征（可能包括分组策略、加权或选择机制）并在Bag级别进行聚合和分类，从而无需对每个FM进行任务特定微调。

Result: 提出了Group-Aggregative Selection Multi-Instance Learning (GAS-MIL) 框架，能无缝整合来自多个FM的特征，保留互补优势。在PANDA、UBC-OCEAN和TCGA-BrCa三项癌症分类任务上，GAS-MIL表现优于或等同于单一FM和现有MIL方法，展现出稳健性和泛化能力。

Conclusion: GAS-MIL 简化了异构FM的整合流程，降低部署成本，提升分类任务性能，为病理学领域的多模态和精准肿瘤学应用提供了可扩展的基础。

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 构建首个模拟OOE的人机协作视频数据集DANDSD，并提出基于图嵌入+变换器的实时旁观者情境感知评估方法，显著优于FINCH基线，推动无人机辅助救援的自适应决策。


<details>
  <summary>Details</summary>
Motivation: 利用无人机快速送达纳洛酮可在EMS到达前挽救生命，而旁观者的情境感知决定能否正确使用药物；目前缺少用于实时评估旁观者SA的标注数据与方法。

Method: 构建DANDSD数据集（模拟OOE场景中的大学生旁观者行为视频），提取几何、运动、交互图特征，采用图嵌入与变换器模型进行融合，进行实时SA预测与时间分割评估。

Result: 提出的数据集DANDSD和模型在SA预测上表现优异，在MoF上比FINCH高9%，在IoU上高5%，实现高精度的实时SA评估与时间分割。

Conclusion: 本研究表明基于视频的图嵌入+变换器框架能够实时评估非专业旁观者在无人机辅助纳洛酮投放场景中的情境感知（SA），并在时间分割任务上超过现有FINCH基线。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 在真实食品包装图像基准上，Tesseract字符错误最低，EasyOCR多语与准确性平衡，PaddleOCR覆盖率高但慢，TrOCR最差；建议发展布局感知与更好的文本定位。


<details>
  <summary>Details</summary>
Motivation: 食品包装文本识别对于合规和营养监测至关重要，但包装场景文本多语种、布局密集、字体多样并伴随光反射与曲面变形，给OCR带来挑战。需要评估现有开源OCR在真实包装图像上的实际效果以建立基准。

Method: 收集231种产品共1,628张包装图片，所有模型评估速度与覆盖率；从中挑选113张图（60种产品）作为带标注的ground truth用于准确率评测。采用CER、WER、BLEU、ROUGE-L、F1、coverage与执行时间等指标比较四个开源OCR（Tesseract、EasyOCR、PaddleOCR、TrOCR）。

Result: 在标注子集上，Tesseract取得最低CER（0.912）与最高BLEU（0.245）；EasyOCR在准确性与多语支持上表现均衡；PaddleOCR覆盖率接近完整但因仅在CPU上运行速度较慢；TrOCR尽管使用GPU加速但效果最弱。研究指出需发展布局感知方法与改进文本定位以提高包装OCR性能。

Conclusion: 该研究在真实食品包装图像上比较了四种开源OCR系统的表现，发现Tesseract在字符层面表现最优，EasyOCR在准确性与多语支持之间平衡较好，PaddleOCR覆盖最全但因CPU运行较慢，TrOCR表现最差。研究为包装文本识别提供了基线并指出需朝布局感知与文本本地化改进。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: Proposes FrameOracle, a lightweight plug-and-play module trained with a four-stage curriculum and a new FrameOracle-41K dataset of keyframe annotations to predict which and how many frames to use, achieving significant reduction in frames and improved or maintained accuracy across VLMs and benchmarks.


<details>
  <summary>Details</summary>
Motivation: VLMs limited by fixed or non-adaptive frame sampling; need adaptive selection and budget prediction to save computation and preserve accuracy

Method: FrameOracle keyframe selection via curriculum and dataset supervision

Result: Reduces 16-frame inputs to 10.4 frames avg with no accuracy loss; from 64 to 13.9 frames avg with 1.4% accuracy improvement; state-of-the-art efficiency-accuracy trade-offs

Conclusion: FrameOracle effectively adapts frame selection and budget for VLMs, backed by a curated dataset, yielding strong efficiency gains without harming and sometimes improving accuracy.

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出CFT混合方法，结合目标游戏标注、共域标注与未标注数据提升视觉缺陷检测，减少对目标标注的依赖，在多游戏环境下优于基线并在半标注情况下仍表现良好。


<details>
  <summary>Details</summary>
Motivation: Labeled visual bug data in games is scarce due to rarity of bugs; need to leverage unlabeled and co-domain labeled data to reduce dependence on target-game labels.

Method: Co-FineTuning (CFT)

Result: CFT integrates labeled target-game and co-domain data plus unlabeled data to improve feature learning, yielding superior detection across games and competitive performance with only 50% target labels.

Conclusion: CFT提升了可扩展性和适应性，能在标注有限时保持良好检测性能，适合跨游戏视觉缺陷检测。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: 在无增强、统一优化设置下，HRM对小图像数据缺乏图像特定归纳偏置，导致在CIFAR-10/100上落后于简单CNN基线；在MNIST上表现接近98%。


<details>
  <summary>Details</summary>
Motivation: 检验带有Transformer式模块和DEQ训练的层级推理模型在纯粹、未增强的小分辨率图像分类任务上的实用性和泛化能力。

Method: 比较了HRM与简单Conv--BN--ReLU基线在相同训练协议下的性能，使用一阶DEQ风格单步训练、深度监督、旋转位置编码和RMSNorm，记录训练/测试准确率与损失曲线并分析错误。

Result: HRM在MNIST上训练稳定且表现良好，但在小分辨率自然图像（CIFAR-10/100）上高度过拟合，泛化能力差。

Conclusion: 当前HRM结构在小分辨率无增强设置中并不具备与简单卷积网络竞争的泛化能力，但通过引入图像相关的归纳偏置或模型修改，仍有改进空间。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: DINOv2通过多视图增强与带动量教师的自蒸馏，在Transformer上学得兼顾语义与空间的通用特征，已在多数基准上超越传统SSL和部分弱监督方法，但仍受训练数据/计算以及部分任务泛化的限制。


<details>
  <summary>Details</summary>
Motivation: 希望在无需大规模标注的情况下，获得兼具高层语义理解与细粒度空间信息的通用视觉特征，推动SSL在各类下游任务上与弱监督方法抗衡甚至超越。

Method: 采用多裁剪（multi-crop）视图增强结合带动量教师模型的自蒸馏（self-distillation with mean teacher），利用大规模未标注图像和适当的变换策略训练ViT骨干，学习高质量特征表征。

Result: DINO/DINOv2在多项下游任务（分类、检索、分割等）上优于多数SSL方法，并在某些基准上超过OpenCLIP等弱监督方法；表现出如类聚类、空间敏感性及可迁移性等显著特性。

Conclusion: DINOv2通过无监督自蒸馏与多视图裁剪在Transformer骨干上学习了兼顾语义与空间细节的通用视觉特征，性能已超越部分弱监督方法，但仍在特定任务和训练资源/数据依赖上存在局限。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: DCS通过分类器驱动的多层次奖励引导扩散模型生成语义一致且有判别力的样本，进而增强FSCIL的增量学习性能，形成互惠提升循环，取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: FSCIL在顺序学习新类时受限于数据稀缺和稳塑性困境，现有方法泛化能力不足。直接使用扩散模型增强数据会带来语义错配或无效指导，故需设计与分类器状态对齐的生成策略。

Method: 提出一种基于奖励对齐的联合训练策略，利用分类器状态动态构建多层次奖励信号驱动扩散模型生成图像；奖励在特征层采用原型锚定的最大均值差异和逐维方差匹配确保语义一致性与多样性，在logits层采用置信度重校准与跨阶段混淆感知机制促进探索性生成与类间判别性；生成样本用于增量微调分类器，形成协同进化。

Result: 在多个FSCIL基准数据集上，DCS显著提升了新类学习能力与历史知识保持，达到或超过当前最先进方法的准确率和鲁棒性。

Conclusion: 本文提出的DCS框架通过扩散模型与FSCIL分类器的互相促进循环，有效缓解了新类学习时的数据稀缺和遗忘问题，实现了SOTA性能。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: 提出MonitorVLM，一种用于矿业监控视频的视觉-语言违章检测框架，包含9k VQA数据集、动态条款筛选CF模块和行为放大BM模块，并通过轻量接口实现自动化上报。


<details>
  <summary>Details</summary>
Motivation: 传统人工巡检在矿业等高危领域效率低、误检率高，难以覆盖大规模动态场景，故需智能化、自动化的安全监测方案。

Method: 构建以VQA形式的领域数据集并加入增强/辅助检测标注；设计CF模块动态选择Top-K相关条款以减低推理开销；设计BM模块放大工人区域以提升细粒度动作识别；将视觉-语言模型在该数据上训练/微调并部署于轻量Web界面实现实时上报与打点。

Result: 通过在9k样本数据集上评估，MonitorVLM相较未微调的72B视觉-语言基线模型，在精确率提升22.01%、召回率提升34.22%、F1提升28.37%；CF模块降低13.56%推理延迟，BM模块分别提升3.45%精确率和8.62%召回率。

Conclusion: MonitorVLM在矿区违章检测任务上显著优于基线模型，在精确率、召回率和F1上分别提升约22.01%、34.22%和28.37%，并能在保持准确度的前提下降低推理时延，具备实际部署价值。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 结合引导分类与扩散模型，利用ExceptionNet提取的特征作为条件输入，通过时间与图像协变量嵌入调制线性投影，提出云端可扩展的混合扩散分类框架，用于交通事故图像检测；通过消融研究优化时间步调度与编码等设计，在公开数据集上达97.32%准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在建模复杂数据分布与细微事故特征时表现受限，扩散模型能够更好地捕捉复杂分布，结合引导分类可提升事故检测的鲁棒性与准确性。

Method: 先用微调的ExceptionNet提取特征（图像张量）作为条件输入，构建多个条件模块用时间嵌入与图像协变量嵌入去调制网络的线性投影；采用云端实现以提升可扩展性，并通过不同时间步调度、编码方法、步数与结构变体进行消融分析。

Result: 在公开数据集上，所提条件扩散模型在图像级事故检测任务上取得97.32%准确率，优于所比较的基线模型；并通过消融实验展示时间步相关设计对性能的影响。

Conclusion: 提出的基于条件扩散的混合模型在图像事故检测上优于基线，达到97.32%的准确率，证明扩散模型在复杂数据分布建模与分类任务中的有效性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: SAMSOD通过单模态监督、梯度去冲突和双解耦适配器，有效解决多模态收敛不平衡和激活梯度差异，提升RGB-T及相关任务的显著性检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Segment Anything Model微调的RGB-T SOD方法忽视了两模态收敛不平衡与高/低激活之间的梯度差异，导致性能仍有提升空间。

Method: SAMSOD包括三部分：1) 单模态监督（unimodal supervision）强化弱势模态学习；2) 梯度去冲突（gradient deconfliction）用于缓解多模态训练中冲突梯度对收敛的负面影响；3) 两个解耦适配器分别屏蔽高、低激活神经元以突出前景并增强背景学习。

Result: 在RGB-T SOD基准数据集以及带涂鸦弱监督的RGB-T SOD、完全监督的RGB-D SOD和RGB-D轨道表面缺陷检测任务上，SAMSOD均体现出性能提升与良好泛化性。

Conclusion: 提出的SAMSOD通过单模态监督与梯度冲突缓解以及双解耦适配器设计，有效提升RGB-T显著性目标检测性能，并在多种基准与任务上展示出泛化能力。

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: Introduces SOREC, a 100k driving-scene small-object REC dataset, and PIZA, a parameter-efficient adapter that progressively zooms to localize small objects; applying PIZA to GroundingDINO significantly improves performance.


<details>
  <summary>Details</summary>
Motivation: Localizing extremely small objects in referring expression comprehension is challenging; need dataset and method to improve localization in driving scenarios

Method: Progressive-iterative zooming adapter (PIZA) with small object REC dataset (SOREC)

Result: Created SOREC dataset (100k pairs) and proposed PIZA adapter applied to GroundingDINO, showing significant accuracy improvement on SOREC; released code/models

Conclusion: SOREC and PIZA effectively tackle small-object REC; resources are publicly available

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: Paper introduces Attention-WNet—WNet augmented with attention—for artery/vein segmentation; reports superior results on HRF and DRIVE datasets


<details>
  <summary>Details</summary>
Motivation: Improve artery-vein segmentation accuracy by integrating attention mechanisms into WNet to better capture vessel-specific features and contextual dependencies

Method: Analyze Attention-WNet for artery-vein segmentation

Result: Attention-WNet evaluated on HRF and DRIVE shows improved performance over prior state-of-the-art models

Conclusion: Attention integrated into WNet improves segmentation accuracy for retinal artery-vein classification, outperforming existing models on tested datasets

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 为LAION-400M构建大规模人像及感知人口学标注，发现数据不平衡与有害关联，并证明大部分性别偏差可由训练数据共现解释。


<details>
  <summary>Details</summary>
Motivation: 现有大规模网络图像文本数据集缺乏人口学标注，阻碍理解训练数据构成如何导致视觉-语言模型的群体偏差。

Method: 构建基于目标检测、多模态生成和微调分类器的自动化标注流水线，为LAION-400M生成人像边界框、感知性别与种族/族裔标签及自动化描述；利用这些注释分析数据不平衡并与CLIP和Stable Diffusion的偏差进行关联分析与回归解释。

Result: 提供了包含2.76亿+人像边界框及感知人口学标签的标注资源；发现男性与被感知为黑人或中东人的图像更常与犯罪及负面内容共同出现；实证证明60-70%的性别偏差可由数据共现线性解释。

Conclusion: 数据集成分对视觉-语言模型的偏差具有重要影响；通过构建大规模人像标注数据集，可实证数据构成与模型偏差之间的线性关联。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较两类预训练网络（通用大数据预训练 vs 卫星影像专用预训练）在里约贫民窟检测上的表现，研究目标是确定任务专用性与预训练数据量哪个更重要。


<details>
  <summary>Details</summary>
Motivation: 尽管已有深度学习方法用于非正规定居点检测，但近期预训练网络的潜力尚未被充分利用。研究动机是探究在特定的遥感目标（如贫民窟检测）中：更多的通用预训练数据量是否比任务专用的预训练更有利。

Method: 选择两个预训练模型类别：1）在大规模通用图像数据上预训练的网络；2）在卫星影像上专门预训练的网络。对两类模型在里约热内卢贫民窟检测任务上进行微调并评估性能差异，比较精度、召回率、F1等指标，并控制模型容量与训练设置以消除混淆变量。

Result: 未给出具体结果，研究设计用于揭示两种预训练策略在城区非正规定居点检测任务中的相对优劣。

Conclusion: 结论尚未给出（需要实验结果）。研究将为是否应优先使用大规模通用预训练模型或专用卫星预训练模型提供实证依据。

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: 作者发现预防性对抗扰动易被绕过，提出LoRA patching在生成器上注入LoRA补丁和MMFA损失，以低样本低计算成本有效绕过防御，并提出可视化警示的防御补丁作为缓解


<details>
  <summary>Details</summary>
Motivation: 证明并利用现有预防性抗Deepfake防御的脆弱性，展示通过对生成器进行LoRA微调可以绕过或中和这些防御

Method: 将低秩适配(LoRA)作为补丁注入到Deepfake生成器，同时引入可学习门控以避免梯度爆炸，并设计多模态特征对齐(MMFA)损失以在语义层面对齐生成输出；少量数据和短训练即可实现攻击或嵌入可见警示

Result: 提出了LoRA patching方法，通过向Deepfake生成器注入可插拔LoRA补丁并配合可学习门控和MMFA损失，在只用1000张人脸和1个epoch下成功绕过多种主动防御；并提出防御性LoRA用于在输出里嵌入可见警告

Conclusion: 当前基于预先在图像上嵌入扰动的防护范式存在严重弱点，需发展更鲁棒的Deepfake防御；LoRA patching既能绕过这些防御也能被用作警示性防御

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 利用测试时可用的参考地图对VPR模型做简单微调，可可靠提升在域差异大的基准上的检索性能（约+2.3% Recall@1），且不损害泛化。


<details>
  <summary>Details</summary>
Motivation: 现代VPR方法在大规模通用数据上表现良好，但当测试环境与训练域差异大时性能下降。许多应用场景在部署前已拥有目标环境的参考集，因此可利用该信息作为未探索的数据源来减小域差距。

Method: 在测试前对模型在参考集（包含目标域图像与位姿）上进行简单的微调，不改变模型架构或训练数据来源，仅使用目标环境的参考图像作为额外的微调数据。评估使用多个挑战性数据集，比较微调前后的Recall@1等指标。

Result: 在多个具有域差异的挑战性基准上，采用RSF能普遍提升性能，平均Recall@1约提升2.3%，并且微调后的模型仍保留对其他数据集的泛化能力。

Conclusion: 该论文提出在测试前利用参考图像集（地图）对VPR模型进行微调（RSF），以缩小训练与测试域差距，从而提升在具有显著域差异的挑战性基准上的性能。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: Decompose SAM gradient into SGD gradient + PSF; reuse PSF adaptively to cut computation ~40% while maintaining generalization


<details>
  <summary>Details</summary>
Motivation: Reduce SAM computational overhead while keeping its generalization benefits

Method: Decompose SAM gradient into SGD term and Projection of Second-order onto First-order (PSF); adaptively sample, reuse, and periodically update PSF during training to reduce gradient calculations

Result: ARSAM reuses and adaptively updates decomposed PSF component to achieve ~40% speedup with comparable accuracy to SAM across tasks

Conclusion: ARSAM maintains SAM's generalization with significant speedup and broad applicability

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA通过每层概念嵌入生成与提示调优、以及多层视觉聚合，解决细粒度概念捕捉不足，提升了概念和疾病预测效果


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型只从最后一层提取特征，忽略浅层和多尺度信息，且缺乏有效的概念编码引导，导致难以提取细粒度概念

Method: Concept Prompting and Aggregating (CoPA)

Result: 提出CEG和CPT，从视觉编码器每层提取概念表示并作为提示引导，同时多层视觉表示聚合以与文本概念对齐，显著提升概念与病症预测性能，在三公开数据集上优于SOTA

Conclusion: 通过多层概念提取与提示引导并结合多层聚合对齐，CoPA能更有效捕获图像中有价值的概念信息，改善概念和疾病预测，代码已开源

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: 在大规模3D血管分割数据上，ZFP能在不显著降低自动分割性能的前提下实现高达约23:1的数据压缩，适合促进数据共享与协作研究。


<details>
  <summary>Details</summary>
Motivation: 大规模3D医学影像数据占用巨大存储、传输成本高，阻碍跨机构合作与可重复研究；探索无显著损失下的高效压缩方法可以降低这些壁垒，提升数据可访问性与研究可扩展性。

Method: 在带有真实血管分割标注的大规模3D医学数据集上，分别对原始体积施加ZFP的误差容忍（accuracy）模式和固定速率（fixed-rate）模式压缩；对压缩后数据运行既定的自动脑血管分割算法，计算并对比Dice系数与未压缩基线；汇报压缩比与分割指标的统计汇总。

Result: ZFP压缩在医学图像数据集（尤其是3D血管分割任务）中能显著减少存储和传输成本，同时保持分割性能几乎不变。本文在一个大规模带有真实血管分割标注的3D数据集上评估了ZFP的两种模式（误差容忍和固定速率），对比基线未压缩的Dice约0.8774；在误差容忍模式下实现了高达22.89:1的数据压缩比，平均Dice仍保持在0.87656，说明压缩对分割质量影响极小。

Conclusion: ZFP是一个可行且强大的工具，可在保持高分割准确率的同时大幅减少医学3D图像数据的存储和传输需求，从而支持更广泛的合作研究。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出三支路混合编码器（CNN、Transformer、Mamba注意力融合）+多尺度注意力CNN解码器和共注意门，兼顾局部、全局与长程依赖，提升分割精度与泛化且复杂度可控。


<details>
  <summary>Details</summary>
Motivation: 现有模型任务专一、跨模态/解剖区域表现不稳；需在临床场景平衡准确率与计算效率，提升泛化能力。

Method: 三支路编码器并行提取局部（CNN）、全局（Transformer）和长程（MAF）特征；多尺度注意力CNN解码器融合上下文重建细粒度分割；共注意门在编码/解码阶段跨尺度强化语义/空间相关特征选择。

Result: Proposed hybrid segmentation architecture (CNN+Transformer+MAF) with co-attention gate and multi-scale CNN decoder; claims superior accuracy/generalization and comparable complexity.

Conclusion: 方法在多个基准数据集上优于现有方法，兼顾效率与效果，适用于临床多模态医学影像分割。代码与模型将开源。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出一种基于YOLOv9并使用多边形标注的道路破损与人孔检测方法，构建并使用来自达卡的千余张图像数据集训练三类模型（破损、未破损、人孔），整体图像级准确率78.1%。模型对破损与未破损类别表现良好（F1分别为86.7%与89.2%），但在人孔检测上受类不平衡影响表现较差（F1为18.2%）。该方法在发展中国家城市基础设施监测方面具有高效、可扩展性。


<details>
  <summary>Details</summary>
Motivation: 人工道路检测耗时、成本高且易出错，尤其在发展中国家。使用自动化深度学习方法并采用更精确的多边形标注，可提高检测精度并提高维护效率与可扩展性。

Method: 收集并标注超过1000张主要来自达卡的道路图像，使用多边形注释替代传统边界框以精确定位瑕疵；将数据标注为三类（Broken、Not Broken、Manhole）；基于YOLOv9模型进行训练与评估，报告图像级准确率及各类别F1分数。

Result: 在构建的数据集上，模型总体图像级准确率为78.1%；Broken类F1为86.7%，Not Broken类F1为89.2%，Manhole类F1为18.2%；表明方法在识别道路破损方面效果显著，但在人孔检测上因样本不足表现差，需要针对数据不平衡进行改进。

Conclusion: 采用YOLOv9与多边形标注能显著提升道路破损的定位精度并在破损/未破损分类上取得较高性能，但数据集类不平衡导致人孔检测效果明显低于其他类别，需要通过扩充人孔样本或使用类不平衡处理策略改善。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出在时间维度上用SimCLR进行对比学习，学到域不变特征，用以指导预训练SDE进行无配对图像翻译；效果可比SOTA且更快、更高效


<details>
  <summary>Details</summary>
Motivation: Leverage strengths of score-based diffusion models for high-fidelity generation and contrastive learning for semantic consistency in unpaired I2I

Method: time-dependent contrastive learning with SimCLR guiding pretrained SDE

Result: Contrastive-SDE obtains comparable results to SOTA on several metrics, converges faster, requires no labels or classifier training

Conclusion: 方法在三个无配对I2I任务上表现良好，训练收敛更快且不依赖标签或分类器，提供了一种高效替代方案。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO评测存在误导性，LIBERO-PRO通过四维度扰动暴露模型依赖记忆的弱点，呼吁社区采用更严谨的泛化评估。


<details>
  <summary>Details</summary>
Motivation: 现有LIBERO评测容易被模型记忆训练集中的动作/布局等表面模式所利用，造成虚高性能；需要更合理的评估以衡量模型的泛化和理解能力。

Method: 提出LIBERO-PRO，在四个维度（被操纵对象、初始状态、任务指令、环境）对模型进行系统扰动评估，并基于此扩展基准进行实验比较。

Result: 实验显示常用模型在原评测下>90%准确率，但在LIBERO-PRO泛化设置下可降至0.0%，并揭示模型在对象替换、指令破坏等场景中的失败模式。

Conclusion: LIBERO当前评测设置存在严重漏洞，导致模型通过记忆动作序列和场景布局获得高分，无法体现真实理解和泛化能力。LIBERO-PRO揭示了这一问题并提供更严格的干扰评估框架。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 提出 Mirage 数据集以暴露当前检测器的盲点；LVLM 对明显伪影有效，但对无明显伪影图像检测能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型能产出对 AI 检测器难以识别但对人类仍可辨认的图像，迫切需要新的数据集与检测方法来评估并提升检测器在这些情况下的鲁棒性，同时探索 LVLM 可否提供可解释的判断。

Method: 作者构建了包含多样显著伪影的 Mirage 数据集；评估了当前最先进的检测器和若干 LVLM 在 Mirage 与现有基准上的性能；通过实验比较 LVLM 在有/无可见伪影图像上的检测差异，并分析可解释性表现。

Result: 该论文构建了一个名为 Mirage 的数据集，包含多样化且具有人类可见伪影的 AI 生成图像，并发现现有最先进的检测方法在该数据集上表现不佳。研究还探索了大型视觉语言模型（LVLM）在可解释 AI 图像检测中的应用，结果表明 LVLM 在检测带明显伪影的 AI 图像时效果较好，但面对缺乏此类线索的图像时性能下降。

Conclusion: Mirage 揭示了现有 AI 图像检测器在真实可见伪影场景下的脆弱性；LVLM 可作为可解释检测工具，但不能依赖于其在难例上的判断，需要结合其他方法或改进模型以提升稳健性。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround提出一种统一的视觉定位（visual grounding）范式，通过在展开的transformer不同中间层动态选择“mask as prompt”，替代固定使用最后一层的“<SEG> as prompt”。核心为Policy-Prompted Masking，包含随机跳跃连接（SSC）和Mask as Prompt（MasP），前者使用强化学习策略在层间抽样以实现跳接，后者将<SEG> token与图像token相似度图作为软logit掩码提示SAM生成掩码。实验首次在属性维度上统一传统指代表达分割到推理分割、多目标与空目标等场景，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法固定使用最后隐藏层作为<SEG> prompt，会放大逐层传播的累积误差且缺乏显式空间提示（如坐标），因此提出动态层选择和显式相似度掩码以改进定位性能。

Method: 提出Policy-Prompted Masking，包括：1) Stochastic Skip Connection（SSC）：用强化学习策略对<SEG> token在unrolled transformer层间进行随机采样，实现动态跳接到视觉模型；2) Mask as Prompt（MasP）：把<SEG>与图像tokens的相似度图作为软logit掩码输入SAM以提供显式空间信息。

Result: 在多个任务和设置上验证了UGround的有效性，首次从属性角度在同一框架下覆盖传统指代表达分割、推理分割、单/多目标以及正/空查询等情形，代码与模型公开。

Conclusion: UGround通过动态选择连接层并显式利用相似度掩码作为提示，能缓解固定最后层带来的累积误差与缺乏空间提示的问题，从而提升视觉定位的鲁棒性与泛化性，并可统一多种视觉定位任务。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: 提出OMG4，通过采样、剪枝、合并三阶段减少关键高斯基元并结合隐式外观压缩与4D子向量量化，实现在基准数据集上在保持质量的同时将模型体积缩小60%以上


<details>
  <summary>Details</summary>
Motivation: Reduce storage overhead of 4D Gaussian Splatting while maintaining visual fidelity for real-time dynamic scene rendering

Method: Progressive pruning + compression for compact 4D Gaussian models

Result: OMG4 prunes and merges Gaussians and applies implicit appearance compression plus generalized SVQ to cut model sizes >60% with preserved reconstruction quality

Conclusion: OMG4 有效压缩4D Gaussian Splatting 模型，显著降低存储需求并保持重建质量，为紧凑的4D场景表示提供可行方案。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出通过图像-图像对齐与多实例词表关联的结构化域对齐方法，将地面预训练的开域表征有效迁移到航拍目标检测，实现多数据集零样本性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测局限于固定类别，难以扩展新类；利用在大规模地面图像-文本对上预训练的对比模型作为开域检测基础，但直接迁移到航拍领域受域差异、视角变化和尺度极端差异影响，需要专门的适配策略。

Method: 方法包括两部分：1) 对比的图像-图像对齐（contrastive image-to-image alignment），通过对齐航拍与地面图像的嵌入空间以缓解视角与域差异；2) 多实例词表关联（multi-instance vocabulary associations），将航拍图像与文本嵌入进行多实例层面的匹配以支持开放词汇检测。训练基于预训练的对比图像-文本模型，并在不同航拍数据集上无监督或零样本评估。

Result: 在多个航拍数据集上进行实验，零样本设置下相比于在目标数据集上微调的封闭词汇模型，分别在DOTAv2上提升+6.32 mAP、VisDrone(Images)上+4.16 mAP、HRRSD上+3.46 mAP，并在xView和DIOR等数据集上也获得性能改进。

Conclusion: 本文提出了一种用于将地面视角开域表征适配到航拍图像的结构化领域对齐框架，通过对比的图像-图像对齐和多实例词表关联进行域间知识转移，实现了在多数据集上的开域目标检测性能提升。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 综述指出深度学习在皮肤癌检测具大潜力，但需通过数据增强、混合模型、特征融合及严格临床验证来克服噪声、类内差异和数据不平衡等挑战。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌发病率高且早期诊断直接影响预后，传统诊断依赖人工经验，自动化、准确的诊断工具可提升检测效率与可及性，因此评估深度学习在该领域的应用与挑战具有重要意义。

Method: 基于PRISMA框架的系统综述方法，筛选并综合最近有关深度学习用于皮肤病诊断的研究，比较不同技术（数据增强、模型架构、混合方法、特征融合等）的优劣与适用场景。

Result: 总结出多项应对挑战的策略：1）数据增强与生成人工样本以缓解数据不平衡；2）利用混合模型（例如结合CNN与Transformer或传统特征方法）提高鲁棒性；3）多尺度与多模态特征融合改善区分相似类别的能力；4）强调模型可解释性、外部验证与临床试验以促进临床落地。

Conclusion: 本文综述认为深度学习在皮肤癌诊断中具有显著潜力，但仍受数据质量、多样性及模型泛化能力限制，需结合数据增强、混合模型和特征融合等方法，并推动临床流程整合与规范化验证。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出SDAKD：通过学生判别器与三阶段训练＋特征图蒸馏，有效缓解容量不匹配，提升超分GAN蒸馏效果，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 教师判别器往往比学生生成器容量更大，导致在知识蒸馏中判别信号对学生生成器不友好，影响学习效果。引入学生判别器以减少这种容量差异，从而稳定且有效地把教师生成器/判别器的知识迁移给更小的学生模型。

Method: 引入一个与学生生成器配套的学生判别器（student discriminator），采用三阶段训练策略：第一阶段可能预训练学生生成器/判别器，后两阶段整合特征图蒸馏（adapted feature map distillation）与对抗训练以加强学生生成器学习。

Result: 在GCFSR与Real-ESRGAN两种高性能超分辨率GAN上进行评估，实验显示SDAKD在生成质量指标和主观视觉效果上均超越基线方法与现有SOTA的GAN蒸馏方法。

Conclusion: 本文提出SDAKD，通过引入学生判别器缓解教师判别器与学生生成器间的容量不匹配，从而提升GAN蒸馏效果。三阶段训练结合改进的特征图蒸馏，在超分辨率GAN剪枝/压缩上优于基线与现有方法。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [61] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 提出PoseGaze-AHP：首个同步头姿与凝视的3D合成数据集（7920张图），由LLM抽取临床记录并经NHA生成，抽取准确率91.92%，有助于隐私合规的AI眼源性AHP诊断工具开发。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头部姿态或眼动，缺乏同时包含两者的同步数据，限制了联合诊断模型与AI辅助工具的发展。

Method: 通过使用Claude 3.5 Sonnet模型对医学文献进行分步、分层和复杂提示的迭代抽取，获取临床记录；对缺失数据进行系统插补；使用Neural Head Avatar框架将结构化数据转化为3D表示并生成图像素材。

Result: 构建了PoseGaze-AHP数据集，包含7920张由两种头部纹理生成的图像，覆盖多种眼科病情；信息抽取方法整体准确率为91.92%。

Conclusion: PoseGaze-AHP提供了首个同步头部姿态与凝视运动的3D数据集，能够推动基于AI的眼源性异常头位（AHP）诊断研究与算法开发。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [62] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文提出了大型4D动态人体网格主观质量评估数据集DHQA-4D（32个实扫序列、1920个失真样本、11类失真、含有有无纹理MOS），并设计了多模态模型DynaMesh-Rater，通过提取投影视频视觉特征、剪裁视频运动特征和四维几何特征，利用大模型（LMM）+LoRA指令微调融合多维特征进行质量评分预测。实验表明方法在DHQA-4D上优于已有评估方法。


<details>
  <summary>Details</summary>
Motivation: 4D数字人网格在采集、压缩和传输过程中会产生多种噪声，影响用户体验，因此需要可靠的客观质量评估方法与数据集以指导重建、压缩与传输优化。

Method: 收集32个高质量4D人体网格序列，生成1920个失真样本并进行主观打分；从投影2D视频提取视觉特征，从裁剪视频片段提取运动特征，从4D网格提取几何特征；将多维特征输入大多模态模型（LMM），采用LoRA进行指令微调以预测质量分数。

Result: 提供了包含主观MOS的公开大型数据集DHQA-4D，并验证DynaMesh-Rater在该数据集上的性能优于现有质量评估方法。

Conclusion: 构建的DHQA-4D和提出的DynaMesh-Rater能有效评估有纹理与无纹理的动态4D人体网格质量，且在基准数据集上表现优越。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [63] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 本文在ResNet-50上引入ASFF双分支自适应融合高层语义与中层细节特征，提高了皮肤病变分类的准确性与鲁棒性，实验在ISIC子集上取得优异指标并通过Grad-CAM验证了关注病变区域。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像存在类间相似性高、类内差异大和噪声等挑战，传统单尺度或单通道特征提取易过拟合与误判，需更有效的多尺度特征自适应融合策略以提高鲁棒性与判别能力。

Method: 基于ResNet-50引入双分支自适应空间特征融合（ASFF）：高层语义分支与中层细节分支分别经过全局平均池化和全连接层生成自适应权重，再对多尺度特征加权融合用于分类，结合Grad-CAM可视化验证关注病变区域。

Result: 在ISIC 2020子集（3297张图像）上，与5种经典CNN比较，模型在准确率（93.18%）、精确率、召回率、特异性、F1和AUC（P-R:0.9670, ROC:0.9717）上表现最好；Grad-CAM显示模型能突出病变区域、抑制背景。

Conclusion: 提出的ASFF增强ResNet-50模型在皮肤病变分类任务上能更好地融合多尺度语义与细节特征，从而提升特征表征能力并抑制噪声影响，最终提高分类性能。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [64] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 提出基于DenseNet-121的多模态加权集成模型，融合临床、影像学和病理图像以提高口鳞癌早期识别，综合验证集准确率84.58%。


<details>
  <summary>Details</summary>
Motivation: Improve early detection of OSCC to reduce high mortality and late-stage diagnosis by integrating multimodal imaging data.

Method: 使用公开数据集的三种模态，针对每种模态基于迁移学习训练DenseNet-121，进行模态特定预处理与数据增强，最后采用基于验证集性能加权的集成策略对预测进行融合，并用准确率、精确率、召回率、F1评估。

Result: A weighted ensemble of DenseNet-121 models trained on clinical, radiological, and histopathological images achieved overall accuracy 84.58% on multimodal validation; modality accuracies: radiological 100%, histopathological 95.12%, clinical 63.10%.

Conclusion: 该多模态集成框架可作为非侵入性AI辅助分诊工具，增强高危病变早期识别，支持临床决策，有望减少诊断延迟并改善患者结局。

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [65] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 选择性使用高质量子集能替代大规模指令调优数据；IQA-Select通过聚类抽样用10%数据达到或超越全量训练效果，显著降算力开销。


<details>
  <summary>Details</summary>
Motivation: 现有做法通过扩大指令调优数据规模来增强多模态大模型的图像质量感知，但大规模数据带来计算成本高且含冗余，作者质疑规模法则并探究数据质量与选择对性能的影响。

Method: 提出三阶段聚类数据选择框架：1) 聚类特征提取（比较不同特征表示）；2) 集群配额分配（根据重要性/多样性定配额）；3) 集群内采样策略（随机或基于代表性选择），最终实现简单高效的IQA-Select。

Result: The paper investigates data quality vs quantity in instruction tuning datasets for explainable image quality assessment (IQA), finding redundancy and proposing a clustering-based data selection method (IQA-Select) that uses only 10% of data to surpass full-data fine-tuning performance.

Conclusion: IQA-Select表明当前可解释IQA指令调优数据存在大量冗余；通过合理的聚类特征、配额分配与采样策略，用小比例数据可获得更好或相当的性能。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [66] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 该论文提出一种将大规模视觉语言模型（VLM）规划能力与通用动作专家（Action Expert）结合的框架：用稀疏3D轨迹作为中间表示，VLM生成粗略3D航路点，动作专家基于实时点云将其精化为可执行的动作序列，并采用“动作预训练 + 点云微调”范式以提升泛化能力与训练效率。


<details>
  <summary>Details</summary>
Motivation: 传统单体VLA模型因训练数据稀缺、领域狭窄而泛化差；现有双系统方法虽拆分思考與行动，但动作模块语义模糊，阻碍大规模跨任务训练，且协同机制不明。需一种能在不同环境下通用、且易于大规模训练的动作表示与模块。

Method: 框架分两阶段：1）规划阶段：VLM基于视觉语言输入生成稀疏3D航路点（waypoints）；2）执行阶段：通用动作专家读取实时点云，将稀疏航路点细化为密集、可执行的动作序列。训练范式为“Action Pre-training, Pointcloud Fine-tuning”，先在大规模多域动作数据上预训练动作专家，再在目标环境点云数据上微调。

Result: 方法能将VLM生成的高层途径有效转化为实际可执行动作，减少在新环境的微调需求，提升跨任务和跨环境泛化；‘动作预训练+点云微调’提高训练效率和鲁棒性（论文宣称在多任务/多环境场景下显著优于传统VLA与现有双系统方法）。

Conclusion: 通过引入稀疏3D轨迹作为桥梁并构建可泛化的动作专家，论文成功将VLM的高层计划能力与低层物理动作执行相结合，减轻了语义歧义、降低跨任务训练难度，并提升了在新环境中的零或少样本迁移能力。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [67] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 将零-shot细粒度分类变为视觉问答并加入注意力干预，配合更精确的类描述集，在多项基准上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 探索大型视觉-语言模型(LVLMs)在零样本细粒度图像分类任务中的潜力，解决现有方法依赖直接类名生成的局限，通过将分类问题转化为视觉问答并引入注意力干预来提升模型区分类间细微差异的能力。同时构建更精确的类描述基准数据集。

Method: 将图像分类任务表述为视觉问答，设计问题模板和答案判断策略；提出注意力干预以增强模型关注细微特征；构建或扩展类描述基准以提供更全面的类别描述；在多种细粒度数据集上进行零-shot评估并与SOTA比较。

Result: 提出将零样本细粒度分类转为视觉问答的框架，结合新的注意力干预技术以及更完善的类描述数据集，在多个细粒度基准上进行大量实验证明，比当前SOTA方法表现更好。公开代码和数据集。

Conclusion: 方法有效提升LVLM在零样本细粒度分类的表现，证明了LVLM用于此类任务的潜力；改进的数据集有助于更准确评估模型能力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [68] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 该文研究雾天下自动驾驶感知系统的鲁棒性，系统性比较传统滤波、深度去雾网络、链式处理（滤波→模型、模型→滤波）及基于视觉-语言的大模型编辑方法，在Foggy Cityscapes上以图像质量与检测(mAP)/分割(PQ,RQ,SQ)为评估指标，发现何时去雾有益、何时链式处理产生协同或退化，并且视觉-语言评分与任务指标（尤其mAP）高度相关。


<details>
  <summary>Details</summary>
Motivation: 现实雾天会显著损害自动驾驶感知性能，但现有去雾方法在图像质量上提升并不总能转化为更好的检测或分割，且多数评估依赖合成数据，难以反映真实场景效能，因此需要系统性研究不同去雾策略对下游任务的实际影响。

Method: 构建系统化基准：选取多类去雾算法（传统滤波与现代学习方法）及基于VLM的编辑方法，设计单步与链式流水线（filter→model, model→filter），在Foggy Cityscapes数据集上同时测量图像重建质量指标与下游检测(mAP)与分割(PQ,RQ,SQ)性能，并用VLM作为评判者给出定性打分以研究其与任务指标的相关性。

Result: 通过综合评估发现：1) 某些去雾方法能在雾天显著提升mAP和PQ等指标；2) 链式处理有时带来协同增益，但也可能因信息丢失或伪影引入导致性能下降；3) VLM图像编辑能在某些情形替代专用去雾模型但不稳定；4) VLM评判分与mAP相关性强，适合作为补充评估手段。

Conclusion: 去雾并非总能提升下游感知性能：在某些条件下能显著提高检测与分割，但错误的链式顺序或不适当方法会导致性能下降；视觉-语言模型提出的定性评分与检测mAP有较强的一致性，证明其在评估中的潜力。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [69] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO: a cascaded pipeline connecting Text-to-Motion and conditional video diffusion with prepared textual/visual conditioning and automatic camera viewpoint selection for coherent general human motion video generation


<details>
  <summary>Details</summary>
Motivation: Enable general-purpose human video generation by bridging T2M models and video diffusion models, addressing limitations of prior works restricted to image-to-video or narrow domains

Method: Cascaded Text-to-Video via Text-to-Motion and Video Diffusion

Result: CAMEO framework with prompt and condition preparation, camera-aware conditioning module, demonstrated strong performance on MovieGen and a new benchmark for T2M-VDM combination

Conclusion: CAMEO effectively integrates T2M and VDMs with specially designed training/inference components, improving coherence and versatility across benchmarks and use cases.

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [70] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: 提出OpenFLAME——一种联邦化的视觉定位系统（VPS）后端，允许不同组织为其私有空间构建并维护独立的3D扫描与定位服务，从而解决集中式VPS在隐私、覆盖与维护上的不足。论文讨论了分片VPS带来的挑战（定位一致性、质量控制、服务选择等），并提出联邦图像定位的概念与参考解决方案，以在不共享私有数据的前提下管理和合并地图数据。


<details>
  <summary>Details</summary>
Motivation: 集中式VPS（如Google、Niantic）难以覆盖私人室内空间并伴随隐私、法规与维护成本问题，因此需要一种能让独立组织保有与维护自己VPS服务，同时又能实现跨空间一致定位的联邦化方案。

Method: 引入联邦图像式定位框架，设计地图分片、访问控制与数据合并策略；提出服务发现与路由机制用于选择合适VPS；用隐私保留的数据交换与验证（如摘要、局部描述子或安全聚合）保持地图一致性与质量控制；并提供参考实现细节以支持跨服务定位与结果融合。

Result: 提出OpenFLAME架构与参考方案，展示如何在不直接共享私有3D数据的情况下管理、验证和合并地图，解决服务选择与质量控制问题，从而支持广泛且隐私友好的6DoF室内外定位。

Conclusion: OpenFLAME展示了联邦化VPS可行且有益，允许组织保有私有室内扫描并参与分布式定位生态，同时通过协议与策略解决一致性、质量、服务路由与数据合并等挑战，从而扩大覆盖并保护隐私。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [71] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 提出一个基于CNN-LSTM从动作数据提取生物力学特征并用LLM生成可操作反馈的框架，旨在将可解释AI与体育生物力学结合，使用THETIS数据集进行验证


<details>
  <summary>Details</summary>
Motivation: Current systems classify strokes but lack actionable, biomechanically grounded language feedback for coaches/players; bridging explainable AI and usability

Method: CNN-LSTM based biomechanical feature extraction and LLM feedback

Result: Framework extracts joint angles, limb velocities, kinetic chain patterns via CNN-LSTM, analyzes relationships to stroke effectiveness/injury risk, and uses LLMs to generate actionable feedback; evaluated on THETIS dataset for classification and interpretability

Conclusion: 该方法能提升分类性能并生成技术上准确、以生物力学为依据的可操作反馈，但需关注数据多样性、评价主观性和实时性部署问题

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [72] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp通过合成针对性时序样本来细化Video-LLMs，使其更关注视频的视觉与时间信息，从而在多个基准上显著提升时序理解性能。


<details>
  <summary>Details</summary>
Motivation: 短语：现有Video-LLMs在细粒度时序理解任务上表现不足，原因是微调数据缺乏时序复杂性，模型依赖语言推理而非视频动态理解。TimeWarp旨在构建合成时序数据集以纠正这一问题。

Method: 提出TimeWarp方法系统生成合成时序样本并构建大规模偏好数据集，用于对现有Video-LLMs进行微调，以强化模型对时序动态的敏感性和对视觉信息的依赖。

Result: 通过TimeWarp生成的大规模偏好数据集可以显著提升模型在七个时序理解基准上的表现，推动Video-LLMs的时序理解能力。

Conclusion: TimeWarp构建的合成数据集能有效引导Video-LLMs关注视觉时序细节，显著提升细粒度时序任务表现，验证了数据引导策略在增强模型时序理解方面的有效性。

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [73] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Extending VLM text encoder context to 512 tokens for biomedical captions improves downstream retrieval and classification; BIOMEDICA-LongCAP dataset and BMC-LongCLIP model demonstrate substantial gains.


<details>
  <summary>Details</summary>
Motivation: Investigate effects of pretraining VLMs with longer text context for biomedical captions, since current models use short windows (<77 tokens) causing truncation of long captions common in biomedical literature.

Method: Assembled 1M image-caption pairs with context-aware descriptions from full-text articles (BIOMEDICA-LongCAP); trained a long-context VLM (BMC-LongCLIP) with text encoder supporting up to 512 tokens; evaluated on long-caption retrieval and classification benchmarks.

Result: Longer text context improves retrieval and classification; introduced BIOMEDICA-LongCAP (1M pairs) and trained BMC-LongCLIP with 512-token text encoder, achieving up to +30% Recall@1 and +2% classification avg, and faster convergence; token waste reduced from 55% to 2.2%.

Conclusion: Long-context modeling is beneficial for biomedical VLMs, providing better performance and efficiency; future work should explore longer contexts and integration strategies.

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [74] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出CPG框架，通过可控伪标签筛选将未标注样本逐步加入已标注集以形成已知分布，结合对数概率校正构建贝叶斯最优分类器并迭代优化，同时用类别感知增强和辅助分支提升少数类表现，理论证明能减少泛化误差，实验证明显著优于前沿方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法假设未标注数据分布已知（长尾/均匀/逆长尾），但实际常未知且任意，需一种不依赖未标注分布的稳健半监督长尾学习策略。

Method: (1) 动态可控筛选：每步从未标注集中挑选可靠伪标签，保证更新后标注集具有已知分布；(2) 对数调整构建贝叶斯最优分类器以校正类别偏差；(3) 迭代自增强：更好的分类器用于下一步伪标签识别；(4) 类别感知自适应增强提升少数类表示；(5) 辅助分支最大化数据利用率。

Result: Proposes CPG (Controllable Pseudo-label Generation) for long-tailed semi-supervised learning where unlabeled distribution unknown; expands labeled set with reliable pseudo-labels to create known distribution and trains Bayes-optimal classifier via logit adjustment; iterative self-reinforcing cycle; class-aware adaptive augmentation and auxiliary branch; theoretical generalization error reduction; strong empirical gains up to 15.97%.

Conclusion: CPG通过构建已知分布的增强标注集并基于该分布进行对数校正，形成可控自增强循环，能在未知未标注分布情形下提高半监督长尾分类性能，理论和实验证明其有效性。

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [75] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 通过对PaddleOCRv5微调并使用古代越南汉文手稿数据，本文将字符识别准确率从37.5%提升到50%，并提供完整训练流水线与可交互演示以促进历史文本的数字化与语义研究。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统在古籍扫描件中受降解扫描、非标准字形与手写变体影响表现不佳，限制了越南历史文献数字化与跨语言语义研究的开展。

Method: 对PaddleOCRv5的文本识别模块进行再训练，使用精心挑选的古代越南汉文手稿数据集；构建完整训练流水线，包括预处理、LMDB转换、评估与可视化；并开发交互式演示以比较微调前后结果。

Result: 在所用数据与设置下，将基线模型的精确识别率从37.5%提升到50.0%，在噪声条件下提升尤为明显；并提供了一个在线可视化演示以支持下游任务。

Conclusion: 本文提出通过对PaddleOCRv5进行微调来提升汉喃（古代越南汉字）文献的字符识别效果，并在噪声图像下显著提高了精确识别率。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [76] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: 提出MetaSeg，一种将隐式神经表示（INRs）与元学习结合用于医学图像分割的方法。INR同时预测像素强度和类别，通过元学习找到在训练集上优化的初始参数，使其能通过微调拟合未知图像并解码标签。在2D和3D脑MRI分割上，Dice分数与U-Net相当，但参数量减少约90%。


<details>
  <summary>Details</summary>
Motivation: 传统INR在表达单一信号方面优秀，但不擅长分布级的预测任务如分割；因此将元学习用于学习跨样本的可迁移初始化，使INR能高效适应新图像并输出语义标签。

Method: 构建一个同时预测像素强度和分类标签的隐式神经表示网络；使用元学习（可能为MAML或类似方法）在训练集上学习一个好的初始化；在测试时对单幅图像进行微调以拟合像素并输出标签。评估在2D和3D脑MRI上并与U-Net比较。

Result: 在2D和3D脑MRI分割任务上，MetaSeg取得与常用U-Net可比的Dice分数，同时参数数目减少约90%，表明其在参数效率与性能间取得良好平衡。

Conclusion: MetaSeg能以远少于传统模型（如U-Net）的参数实现可比的分割性能，证明INR结合元学习是医学图像分割的可行而高效的替代方案。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [77] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: ViTL localizes relevant intervals with low-fps skim then reallocates tokens to spans for detailed answering, trained with interleaved objective; paired with new span-grounded MC dataset, improves accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Scale long-video QA under fixed token budgets while maintaining interpretability and temporal grounding

Method: Two-stage long-video QA with localization and span-aware token reallocation

Result: Up to 8.6% improvement with 50% less frame input on benchmarks; span-aware reallocation outperforms uniform sampling

Conclusion: ViTL and the new dataset offer an interpretable, compute-efficient approach for scalable long-video QA, enabling better accuracy under constrained token budgets and yielding improved temporal grounding.

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [78] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 通过LLM驱动的四类伪造管线生成多样化假新闻视频并配合不确定性主动学习筛选样本，实现对短视频假新闻检测器的有效数据增强与性能提升。


<details>
  <summary>Details</summary>
Motivation: 现实中视频片段与虚假事件之间存在多对多映射关系，现有数据集难以覆盖这些复杂关系导致学习偏倚，故需通过模拟多样化伪造流程进行数据增强并有选择地加入训练以提升泛化。

Method: 设计四类新闻伪造流水线（通过视频片段重组、配音/字幕篡改、上下文替换、混合拼接等策略），使用大语言模型生成对应的文本叙述与编辑指令，再由模态处理器生成合成视频样本；结合基于模型不确定性的主动学习在增强样本池中选择最有利训练的样本迭代加入训练集。

Result: AgentAug提出通过模拟真实创作流程、使用多条由大语言模型驱动的生成管线来扩增短视频假新闻数据，并结合基于不确定性采样的主动学习在训练中筛选有价值的增强样本，从而缓解现有数据稀疏与偏倚问题，提升检测器性能。

Conclusion: AgentAug能显著提高短视频假新闻检测性能，尤其在数据稀缺场景下，通过多元化合成与主动筛选减少模式偏倚并扩展训练覆盖。

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [79] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: Study optimizes hyperparameters and attention re-weighting for more consistent text-driven image edits, introducing CL P2P to fix cycle inconsistencies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve precision and reliability of prompt-to-prompt image editing using stable diffusion by optimizing hyperparameters and attention mechanisms.

Method: Analyzes word swap, develops attention re-weighting, and introduces CL P2P framework to tune cross-attention for stable diffusion models.

Result: Proposes a study of word swap method, an attention re-weight method, and the CL P2P framework to handle limitations like cycle inconsistency.

Conclusion: Optimizing hyperparameters and re-weighting attention improves edit consistency; CL P2P addresses cycle inconsistency and enhances adaptability of P2P methods.

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [80] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight通过迭代调用专用工具逐步聚焦屏幕区域，大幅提升GUI视觉定位准确率；用少量数据就超越了更大规模模型


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在GUI系统中视觉定位（visual grounding）不可靠的问题，使其能进行精确的指针级操作（点击、拖拽）

Method: 训练一个图像引导推理模型，能够在推理时动态选择并调用多种工具，逐步缩小关注区域以改进视觉定位；在ScreenSpot-Pro基准上进行评估，与V2P-7B和GTA-1-7B比较样本效率和准确率。

Result: 提出GUI-Spotlight：一种通过动态调用多种专用工具、迭代缩小屏幕关注区域的图像引导推理模型；在ScreenSpot-Pro基准上以仅18.5K训练样本达成52.8%准确率，优于V2P-7B（50.6%，9.6M样本）和GTA-1-7B（50.1%，1.56M样本）

Conclusion: GUI-Spotlight显著提高了视觉定位精度，证明了使用专用工具组合和迭代聚焦策略在低样本条件下比单一大模型更高效；有望推动MLLM在真实GUI交互中的可靠应用

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [81] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 提出REQuant：通过层级局部最小化的范围估计与高效搜索算法优化后训练量化，能在8/6/4-bit场景下更好保持分类精度，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在后训练量化中，如何在低比特宽度下尽量保持模型精度是主要挑战，现有方法在极低比特（如4-bit）精度下降明显。提出通过更准确的范围估计来减少量化误差。

Method: 将每层的量化范围估计为最小化量化误差的优化问题，证明问题局部凸，设计逐层高效搜索算法并在权重变换空间中应用搜索以进一步改进。

Result: 在ImageNet图像分类上，ResNet系列和Inception-v3的8-bit和6-bit量化几乎无top-1精度损失，4-bit相比现有方法有显著提升。提供了开源代码。

Conclusion: 提出了一种基于范围估计的后训练量化方法，通过将范围估计建模为层级局部极小化的优化问题，证明局部凸性并给出高效搜索算法，在变换权重空间中应用以进一步提升性能。实验在ResNet系列和Inception-v3上显示，在8-bit和6-bit几乎不损失top-1精度，4-bit也有显著提升。

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [82] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind通过三模态组合检索与等变布局编码器ESSGNN，实现了对场景感知的3D资产检索，显著提高了检索的空间和风格一致性，支持迭代场景构建。


<details>
  <summary>Details</summary>
Motivation: 当前3D资产检索忽视空间约束与风格一致性，且缺乏专门针对3D素材检索的标准范式，主流方法依赖通用3D形状表示，导致检索结果与目标场景不匹配。

Method: 提出可插拔的等变布局编码器ESSGNN，联合建模物体级外观特征与场景级布局结构，支持任意文本/图像/3D组合查询及迭代式场景构建。

Result: 在多种检索任务中，MetaFind在空间一致性和风格一致性方面均优于基线方法，能够在场景更新中持续调整检索结果以保持上下文协调。

Conclusion: MetaFind有效提升了面向元宇宙场景生成的3D资产检索质量，通过三模态（文本、图像、3D）可组合查询和基于场景的布局编码，解决了检索与场景空间/语义/风格不一致的问题。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [83] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 通过将子类有序信息融入二元交叉熵损失，提出了一种加权正则化方法，使模型在训练时对阈值附近的错误判决给予更大惩罚，从而改善太阳耀斑的二分类预测，特别是降低阈值附近的混淆。


<details>
  <summary>Details</summary>
Motivation: 传统二分类方法忽视了子类之间的序关系，导致模型在阈值附近容易混淆；实证研究显示最常见的误判发生在阈值附近，因此利用子类的有序信息可提升判别能力。

Method: 在标准的BCE损失上叠加一个基于子类间有序距离的权重函数。具体地，为每个真实子类与预测阈值的距离分配权重，距离越小（即越接近阈值）权重越大；最终损失为加权BCE，促使模型更关注阈值附近样本的判别。

Result: 摘要中未给出具体实验结果或数值，但预计该正则化策略会减少阈值附近的错误率并提升整体预测性能，具体提升幅度需通过实验验证。

Conclusion: 通过在二元交叉熵损失中引入基于子类有序性的加权项，作者提出的方法可以在训练时对靠近分类阈值的错误预测施加更大惩罚，从而缓解模型在阈值附近区分相近强度事件的困难。该方法本质上是一种数据驱动的有序性正则化，有望提高预测性能，尤其是减少阈值附近的错误分类。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [84] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire: post-training quantization framework for demoiréing that handles outliers and frequency sensitivity to maintain quality; outperforms prior methods significantly


<details>
  <summary>Details</summary>
Motivation: Existing quantization methods degrade demoiréing performance due to activation outliers and weakened smooth-region representations

Method: Post-training quantization for demoiréing models

Result: Proposed QuantDemoire with outlier-aware quantizer and frequency-aware calibration; keeps some extreme weights in FP16 and emphasizes low/mid frequencies during calibration; achieves large reductions in params and computation and >4 dB improvement on W4A4 vs existing methods

Conclusion: QuantDemoire effectively quantizes demoiréing models with minimal quality loss by combining outlier handling and frequency-focused calibration, enabling deployment on edge devices

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [85] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: proposes TV-LoRA: diffusion prior + anisotropic TV + low-rank nuclear norm (LoRA) in ADMM for efficient, robust sparse-view CT; outperforms benchmarks


<details>
  <summary>Details</summary>
Motivation: address ill-posedness and texture loss in extreme sparse-view CT by combining generative priors and structured regularization

Method: analysis of methods

Result: superior SSIM, texture recovery, edge clarity, artifact suppression across datasets at N_view=8,4,2; FFT-PCG speedup; ablation confirms complementary effects

Conclusion: TV-LoRA enables high-fidelity, efficient 3D reconstruction with strong robustness and clinical applicability in low-dose sparse-sampling CT

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [86] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 本工作提出拓扑建图评测的规范化协议：用定位精度评估拓扑一致性、首次量化数据集模糊性，构建校准基准数据集并开放基线代码，揭示当前方法在感知混淆下的短板。


<details>
  <summary>Details</summary>
Motivation: 当前拓扑建图领域缺乏统一评测度量、数据集与协议，使得不同方法难以公平比较；感知混淆（perceptual aliasing）是影响性能的关键问题但未被充分量化。

Method: 形式化拓扑一致性、证明定位精度是可解释代理指标；提出首个定量数据集模糊性测量；构建基准数据集并实现深度学习与经典方法作为基线，进行定量评估。

Result: 给出了一个可解释的评测协议，包括定位精度代理指标和数据集模糊性度量；通过基准数据集与实验展示现有方法在感知混淆下的局限性；并发布所有数据与工具以促进统一评测。

Conclusion: 作者认为拓扑地图的一致性是核心属性，定位精度可作为代理指标，并提出了数据集的“模糊性（ambiguity）”量化方法，搭建了具有校准模糊级别的基准数据集，开放基线实现和评估工具，以推动可复现比较。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [87] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 提出高分辨率事件网格流数据集HREM/HREM+，轻量级EEMFlow模型及CDC和ADM模块，在准确性和速度（30x）上显著优于现有方法，并增强对不同事件密度的鲁棒性（性能提升8-10%）。


<details>
  <summary>Details</summary>
Motivation: Introduce event-based meshflow estimation, address lack of datasets and methods, and handle event data density issue.

Method: 生成1280x720高分辨率HREM/HREM+数据集；设计轻量编码-解码EEMFlow网络；CDC模块用于基于置信度的细节补全获得密集光流；ADM用于自适应调整事件密度；大量实验证明效果。

Result: Created HREM and HREM+ datasets, proposed EEMFlow network with CDC module for dense flow, and ADM for density adaptation; achieved superior accuracy and 30x runtime speed vs SOTA; ADM improves performance by 8-10%.

Conclusion: HREM/HREM+和EEMFlow系列为事件相机的网格流与光流估计提供了高分辨率数据与高效鲁棒的方法，尤其通过CDC和ADM提升细节保持与跨密度泛化能力，实验证明显著性能和效率提升。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [88] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 通过回归预训练+联合训练以及时变得分缩放的采样引导，提出了一种更快更精简的扩散模型6D姿态估计管线，省去评估网络并在多个数据集上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的6D姿态估计方法训练收敛慢（端到端学习编码器与扩散网络）且需额外的评估网络来筛选采样结果，增加复杂度和计算开销。

Method: 1) 对编码器进行回归头预训练，然后同时用回归头和扩散去噪头联合训练；2) 在采样阶段引入基于时间的得分缩放作为指导，以平衡探索与利用并去除额外的评估网络；3) 最终通过单次姿态推断实现高质量姿态估计。

Result: 方法显著加快训练收敛并提升精度，采样引导保持对称物体的多模态特性同时在终止步骤得到高质量姿态；在REAL275、HouseCat6D和ROPE等基准上实现SOTA表现，并在训练和推理上更高效。

Conclusion: 该论文提出通过回归头预训练编码器并联合训练回归与扩散去噪头，加上时变得分缩放的采样引导，实现了训练更快、推理更高效且无需额外评估网络的6D物体姿态估计方法；实验在多个基准上达到了最先进的性能。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [89] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 提出基于概念漂移视角的蒸馏方法（APO），通过学习-比较-批判范式消减多教师推理漂移导致的偏差，提升学生模型一致性、鲁棒性和泛化，并发布大规模CXR-MAX数据集。


<details>
  <summary>Details</summary>
Motivation: 观察到从多个MLLM教师蒸馏时，教师间推理轨迹存在不可预测的变化（概念漂移），这些漂移会将偏差传递给学生，损害学生性能，需要机制来检测并抵消这种漂移以获得稳健的蒸馏效果。

Method: 将多教师非平稳推理轨迹建模为多流次令牌预测，提出"learn, compare, critique"范式，并设计了自主偏好优化（APO）：学生在教师引导下先学习并自蒸馏偏好推理，通过比较多教师输出进行偏好学习，然后对教师的漂移性推理进行批判性反思并通过APO执行概念对齐。

Result: 大量实验显示该方法在一致性、鲁棒性和泛化能力上超过基线方法。并构建并公开了大型数据集CXR-MAX（170,982条蒸馏推理轨迹，基于MIMIC-CXR），实现和数据已公开。

Conclusion: 该论文提出并验证了一种针对多模态大语言模型（MLLM）教师产生的概念漂移问题的蒸馏方法，最终获得了在一致性、鲁棒性和泛化性上表现更优的学生模型。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [90] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: SiteShield是一个结合视觉与音频输入并使用RAG的LVLM系统，可显著提高施工安全检查报告的准确性与效率，实验结果证明其优于无RAG的单模态方法。


<details>
  <summary>Details</summary>
Motivation: 传统施工安全检查信息量大且检索低效，现有LVLM应用存在响应不相关、模态受限和幻觉问题，LLM缺乏训练数据及实时适应能力，从而需要一个多模态且具检索增强的自动化系统。

Method: 构建基于LVLM的RAG框架，整合图像和音频数据，利用检索库提供外部知识支持并融合生成模型输出，评估指标包括F1、hamming loss、precision和recall。

Result: 在真实世界数据上，SiteShield优于未使用RAG的单模态LLM：F1=0.82，hamming loss=0.04，precision=0.76，recall=0.96，显示检索增强与多模态融合对安全检查有显著提升。

Conclusion: SiteShield通过将多模态（视觉+音频）输入与检索增强生成（RAG）结合，显著提高了施工安全检查报告的准确性与实时性。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [91] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE利用生成式图像翻译与自适应融合，在无需偏见先验或反偏样本的情况下，通过对齐任务相关但偏见不同的样本并错配同偏样本来实现强鲁棒性，实验证明显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有去偏方法通常依赖强先验（如已知偏见或偏见冲突样本），但现实中这种信息难以获取，因此需要一种无需这些先验信息的通用去偏方法。

Method: BLADE首先训练生成模型进行偏见域间的图像翻译，保持任务相关特征不变；随后对每个图像与其生成的反偏样本按偏见敏感性进行自适应融合；在表示学习阶段通过对齐（与反偏样本）和错配（与同偏样本）来鼓励鲁棒表征。

Result: 在多个基准上显著优于最先进方法，特别是在受污染的CIFAR-10最坏组上取得约18%的绝对提升，表明该方法能在无监督或弱监督条件下有效减轻数据偏见影响。

Conclusion: 该论文提出了BLADE，一种无需先验偏见信息或反偏样本的生成式去偏框架，通过生成模型在偏见域间翻译图像并基于样本对偏见敏感性的自适应融合，进而在表征学习阶段对齐任务相关但偏见不同的样本并错配具有相同偏见的样本，从而抑制模型对表面相关性的依赖。实验证明在多个基准数据集上优于现有方法，在损坏的CIFAR-10最差组设置下比最近的基线提升约18%。

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [92] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: 提出一种不用额外概念标注即可生成空间定位概念解释的模型（SEG-MIL-CBM），通过对语义分割区域做注意力加权的多实例学习来提升鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 提高神经网络在安全关键场景下的可解释性和可靠性，避免模型利用虚假或误导性特征并提供空间定位的概念级解释。

Method: 使用图像分割得到语义区域作为实例，输入到注意力机制的MIL模块中学习区域级概念与分类器的关联，并通过注意力权重实现空间与概念级的解释性；模型训练不需概念标注，依赖分割和弱监督聚合。

Result: 提出SEG-MIL-CBM框架：将概念引导的图像分割融入基于注意力的多实例学习（MIL），将每个分割区域视为实例并在无需概念或组合标注下学习聚合证据，实现空间定位的概念级可解释性；在存在虚假相关性、输入破坏和大规模基准上表现稳健。

Conclusion: SEG-MIL-CBM在不依赖概念注释的情况下，能提供语义区域级的概念解释，减少对无关提示的依赖并在多种扰动和数据偏差情景下表现稳健，为可解释视觉模型提供实用路径。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [93] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa models hidden features as a mixture of ODEs per dimension and applies dimension-wise hybrid caching, enabling large sampling speedups (5x-6x) for diffusion transformers without retraining while maintaining fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing feature-caching methods use uniform strategies across feature dimensions despite heterogeneous dynamics; modeling hidden feature evolution per-dimension can allow better caching and faster sampling without retraining.

Method: Dimension-wise hybrid ODE caching for diffusion transformers

Result: HyCa applies a mixture-of-ODEs perspective and uses hybrid caching per-dimension, achieving near-lossless acceleration across models and domains, e.g., 5.55x on FLUX, 5.56x on HunyuanVideo, 6.24x on Qwen-Image and Qwen-Image-Edit.

Conclusion: Dimension-wise hybrid ODE-inspired caching effectively balances forecasting and reuse of hidden representations, providing substantial, near-lossless acceleration for diffusion transformer sampling across tasks and models.

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [94] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: 提出World-To-Image框架，通过代理动态检索网络图像并进行多模态提示优化，使文本到图像生成能处理模型知识缺口的未知实体；在NICE基准上显著提升语义对齐和美学评分。


<details>
  <summary>Details</summary>
Motivation: T2I模型对新颖或训练时未见的实体生成效果差，原因在于模型知识截止，需引入外部、实时的世界知识以弥补缺失。

Method: 设计一个能动态在网络上检索目标实体图像的代理，将检索到的多模态信息输入到多模态提示优化模块，对基础生成模型的提示进行优化引导；使用LLMGrader和ImageReward等现代评估指标评估语义保真度与美学。

Result: 在作者构建的NICE基准上，World-To-Image在准确度-提示（accuracy-to-prompt）上比现有最先进方法提高了8.1%，且在语义对齐和视觉美学方面均有显著提升，通常在三次迭代内完成优化。

Conclusion: World-To-Image通过利用基于代理的在线图像检索与多模态提示优化，显著提高了T2I模型对新实体的生成准确性和视觉美感，实验在NICE基准上表现优越，且在少于三次迭代内高效收敛。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [95] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC构建基于几何感知距离和密度驱动聚合的层次语义树，将平坦的视觉token词表转为结构化预测空间，显著加速训练并提升图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统AR模型将视觉token视为平坦、无结构的词表，忽视了嵌入空间中邻近token通常语义相似的本质，导致预测任务复杂、训练效率低且生成质量受限。

Method: MASC采用几何感知距离度量和密度驱动的凝聚式聚类，从codebook的嵌入分布中构造语义树；在训练/推理时将flat token预测转为逐层树上分类。该模块为plug-and-play，可与现有AR框架结合。

Result: MASC提出通过在视觉token的嵌入空间上构建层次语义树来简化自回归(AR)图像生成模型的预测任务，从而提高训练效率和生成质量。

Conclusion: 通过将高维无结构的token预测任务转为层次化结构，MASC为AR模型提供了有效的先验，显著提升训练速度和生成性能，使AR方法在图像生成上更具竞争力。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [96] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn scans images to find suspicious regions then zooms in for focused analysis; trained on MagniFake dataset of 20k annotated images; achieves high accuracy and interpretability


<details>
  <summary>Details</summary>
Motivation: AI-generated imagery blurs real/synthetic boundary; need accurate detection and interpretable explanations

Method: Two-stage forensic framework using VLMs

Result: 96.39% accuracy, robust generalization, grounded human-understandable explanations

Conclusion: ZoomIn improves forensic accuracy and interpretability by combining coarse scanning and focused VLM-based analysis using MagniFake dataset

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [97] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 一个用很少代码、少量训练数据就能端到端训练的图像配准算法，适合训练资源受限的应用，示例在立体视觉上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决图像配准问题，同时应对训练数据稀缺、训练时间短以及代码复杂度低的实际需求，提供一个易实现的基线方法。

Method: 基于少量训练数据和简短Python实现，作者设计了一个端到端可训练的模型（细节抽象），并在立体视觉任务上以19x15窗口、74张图像进行训练验证。实现重心是简洁性和高效训练。

Result: 在少量训练数据（如74张图像）和小输入窗口（19x15）下，算法实现了准确的配准结果，代码行数仅数十行，适合作为资源受限场景的起点。

Conclusion: 本文提出了一个简单、端到端可训练且代码简洁的图像配准算法，适用于训练数据和训练时间受限的场景，在某些设置下能达到较高精度。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [98] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出ArConv卷积层并构建轻量级CNN（1.3M参数），在眼底影像数据集RfMiD上以0.9328准确率优于MobileNetV2，适合移动端眼病诊断应用。


<details>
  <summary>Details</summary>
Motivation: 提升深度神经网络的可访问性，使其能在资源受限设备（如手机）上高效运行，便于眼病早期筛查与诊断。

Method: 在卷积层级别进行重构与优化，提出ArConv层并构建包含该层的新型轻量级CNN，比较并在相同条件下与MobileNetV2在RfMiD数据集上训练评估。

Result: 最终模型仅含1.3M参数，在RfMiD测试集上准确率为0.9328，优于MobileNetV2的0.9266（2.2M参数），显示出更好的性能-复杂度权衡。

Conclusion: 本文通过重新设计卷积层（提出ArConv）提高模型可用性，在保持高准确率的同时显著减少参数量，适合移动设备部署。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [99] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Generate a too long; didn't read summary


<details>
  <summary>Details</summary>
Motivation: What problem does this paper try to solve and why is it important?

Method: Please summarize the method in this paper

Result: What's the main result?

Conclusion: What's the conclusion of the paper

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [100] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: CoSSeg-TTA: nnU-Netv2 + mean-teacher semi-supervision, randomized histogram style transfer + trainable contrast-aware network for domain augmentation, and continual test-time adaptation; improves liver segmentation on GED4 across centers


<details>
  <summary>Details</summary>
Motivation: Address limited annotated data and cross-center domain shifts in contrast-enhanced liver MRI (GED4) for robust segmentation

Method: Semi-supervised domain-adaptive segmentation with test-time adaptation

Result: Outperforms nnU-Netv2 baseline; better Dice and Hausdorff, generalizes to unseen domains under low-annotation settings

Conclusion: Combining semi-supervised learning, randomized style-based domain augmentation, and test-time adaptation yields a compact, robust liver segmentation pipeline that handles domain variability and low annotations better than baseline.

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [101] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: Proposes a patch-agnostic, concept-based defense that identifies and suppresses influential concept activation vectors to neutralize adversarial patches; outperforms PatchCleanser on Imagenette with ResNet-50


<details>
  <summary>Details</summary>
Motivation: patch attacks are localized perturbations that cause targeted misclassification; existing defenses need patch size/location prior, limiting applicability; desire a patch-agnostic defense using interpretability

Method: concept-based explanation suppression of influential concept activation vectors

Result: on Imagenette with ResNet-50, achieves higher robust and clean accuracy than PatchCleanser, strong across varying patch sizes/locations

Conclusion: Combining interpretability (concepts) with robustness is promising; concept-driven defenses are scalable strategy against adversarial patch attacks.

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [102] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: 提出Adapt-STformer，通过Recurrent Deformable Transformer Encoder（Recurrent-DTE）用迭代递归机制融合多帧信息，实现可变序列长度、快速推理和低内存。实验表明在多个数据集上相比次优方法提升召回率并降低时间和内存。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的Seq-VPR方法牺牲灵活性和效率以追求性能，缺乏对可变序列长度、实时推理和低内存的兼顾。

Method: 设计Recurrent-DTE，用递归迭代机制在编码器中逐帧融合特征，支持可变seq-length；整体模型为Adapt-STformer，重点在序列特征提取的效率与内存优化。

Result: 在Nordland、Oxford、NuScenes数据集上，召回率最多提升17%，序列提取时间减少36%，内存使用降低35%，优于次佳基线。

Conclusion: Adapt-STformer在保持或提升表现的同时显著提高了灵活性和效率，适用于实时Seq-VPR应用。

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [103] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit将图像编辑问题视为视频生成，利用预训练视频生成模型和推理时的时序推理token生成物理一致的编辑路径，从而提升编辑后视觉与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法常忽视物理一致性，导致编辑结果在物体位置、相互作用与动力学上不连贯；在需要世界模拟的任务中这种一致性至关重要。

Method: 利用大规模预训练视频生成模型作为底座；把输入和编辑后图像当作视频第一与最后帧；在推理过程中加入时序推理token与目标帧联合去噪以想象编辑轨迹，随后丢弃token以降低计算开销。

Result: 在新建的PBench-Edit基准测试上，ChronoEdit在视觉保真度与物理合理性上均超过最先进基线；并将开放14B与2B模型与代码。

Conclusion: ChronoEdit通过把输入与目标图像看作视频首尾帧并在推理时加入时序推理token，共同去噪生成可行的编辑轨迹，显著优于现有方法并在PBench-Edit基准上表现更好。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [104] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD is a large, multi-site SMPL mesh dataset for PD gait enabling better 3D reconstruction and clinical score prediction; pretraining on it substantially improves accuracy and generalization


<details>
  <summary>Details</summary>
Motivation: Lack of large, diverse, clinically annotated motion datasets for objective gait assessment in Parkinson's Disease

Method: Introduce dataset and benchmarks

Result: Created CARE-PD: largest public 3D mesh gait archive for PD, multi-site, harmonized SMPL meshes, supports supervised UPDRS gait score prediction and unsupervised pretext tasks; pretraining improves MPJPE and PD severity macro-F1

Conclusion: Clinically curated, diverse motion data like CARE-PD significantly improve motion reconstruction and PD severity prediction; dataset and benchmarks released for research

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [105] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR通过离散化计数与分层自回归建模基因共表达，提升了从H&E图像预测空间转录组的准确性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 降低ST测序成本，通过H&E图像预测空间基因表达，解决现有方法忽视基因共表达和将表达视为连续值的问题。

Method: 对基因进行层级聚类以捕获依赖性；将表达视为离散token并不用码本直接生成原始计数；采用多尺度自回归从粗到细因式分解；结合组织图像与空间嵌入作为条件信息进行解码。

Result: 提出GenAR：多尺度自回归框架，基因层级聚类、无码本离散token生成预测计数、融合组织与空间嵌入的解码，实现从粗到细的预测。

Conclusion: GenAR在四个ST数据集上达到最先进性能，能提供成本效益高的分子表型预测，对精准医疗有潜在影响。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [106] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: RAP uses semantic 3D rasterization plus feature-space alignment to cheaply generate useful augmented training data, improving closed-loop robustness and generalization without photorealistic rendering.


<details>
  <summary>Details</summary>
Motivation: End-to-end driving policies trained with imitation learning on expert demonstrations fail in closed-loop due to lack of recovery data; photorealistic rendering for augmentation is costly. The paper argues photorealism unnecessary; semantic fidelity and scalability are sufficient.

Method: Replace photorealistic rendering with rasterization of annotated primitives to synthesize counterfactual views and maneuvers; apply Raster-to-Real feature-space alignment to bridge sim-to-real gap; integrate into RAP pipeline for training planners.

Result: Proposes 3D Rasterization to generate synthetic views via lightweight rasterization of annotated primitives and introduces Raster-to-Real feature-space alignment. Combined as Rasterization Augmented Planning (RAP), achieving state-of-the-art on NAVSIM v1/v2, Waymo ODE E2E Driving, and Bench2Drive.

Conclusion: Lightweight rasterization with feature alignment suffices to scale end-to-end driving training as a practical alternative to photorealistic rendering, yielding SOTA robustness and long-tail generalization.

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [107] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 提出 Diffusion^2：两个串联扩散模型（先生成历史再预测未来），并通过双头不确定性估计与时序自适应噪声模块缓解生成噪声，显著提升瞬时轨迹预测性能，达成 SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机在于现实场景中常存在观测不足的瞬时轨迹问题（如盲区突然出现行人），传统依赖充分历史观测的方法在此类极端情况下性能下降，因而需要专门为瞬时轨迹预测设计的方法以提升交通安全。

Method: 方法由两个串联的扩散模型组成：第一阶段为向后扩散（backward）生成未观测到的历史轨迹，第二阶段为向前扩散（forward）预测未来轨迹。为应对生成历史轨迹带来的噪声，设计了双头参数化机制估计不可约（aleatoric）不确定性，并引入时间自适应噪声模块以在前向扩散过程中动态调节噪声尺度。

Result: 在常用数据集 ETH/UCY 与 Stanford Drone 上进行的实验显示，Diffusion^2 在瞬时轨迹预测任务上达到了新的最先进水平，验证了其在处理观测稀缺与不确定性方面的有效性。

Conclusion: Diffusion^2 提出了一种针对瞬时轨迹预测的双阶段扩散模型框架，通过先向后生成缺失历史轨迹再向前预测未来轨迹，能够在稀缺观测条件下提升预测性能。实验在 ETH/UCY 与 Stanford Drone 数据集上取得了最先进结果，表明方法在极端场景下有效。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [108] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出了MorphoSim，一种从自然语言生成4D可编辑多视角场景的框架，支持对象级控制（引导、重色、移除）与交互式编辑，结合轨迹引导生成与特征场蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型主要受限于2D视角和交互性低，无法满足机器人等需要可控、可编辑、多视角一致性的世界模型需求；因此需要生成可从任意视点观察并支持对象级交互的4D场景。

Method: 系统将自然语言指令解析为对象级编辑和轨迹指令，使用轨迹引导的生成（trajectory-guided generation）创建动态物体运动，并通过feature field distillation将多视角一致性与可编辑性结合，允许局部编辑而无需重生成整个场景。

Result: 实验证明MorphoSim在场景保真度、可控性和编辑性之间取得平衡，能够实现对象重定向、重着色和移除等编辑，并保持多视角一致性；代码已公开。

Conclusion: MorphoSim在保持高场景保真度的同时，实现多视角一致性的动态场景生成与交互式对象编辑，优于仅2D文本到视频方法。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [109] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 在受控的几何形状计数基准上，VLMs能处理单形状计数，但在组合多形状时失败，暴露出组合性计数的根本弱点。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在最简化、受控场景下的计数能力，以排除其他因素干扰，揭示模型在基本视觉-语言任务（计数）上的局限性，特别是组合性计数。

Method: 构建VLMCountBench基准，使用极简几何形状与组合，严格控制独立变量（颜色、大小、提示词细化等），并对模型在不同设置下的计数能力进行系统消融实验。

Result: 实验显示：当只出现一种形状时，VLMs计数可信；但当出现多种形状组合时，计数性能显著下降，提示模型难以进行组合性计数。

Conclusion: VLMs在纯单一形状计数任务上表现良好，但在组合多种形状的计数任务上表现显著不足，表明当前VLMs在组合性计数方面存在根本性局限。

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [110] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++提出三模块框架：可变形配准、纹理引导恢复和深度度量学习样本生成，从而在提高图像质量的同时保持身份一致性，实验验证性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成的方法在视觉质量与身份保真之间存在权衡，容易导致身份扭曲或退化去除不完全。旨在充分利用生成先验实现高质量且保持身份的盲人脸恢复。

Method: 采用可学习的可变形人脸配准模块进行语义对齐，设计纹理引导恢复网络动态提取并迁移生成脸部的纹理，同时通过生成正样本与困难负样本并结合深度度量学习来更好地融合集成身份保持和生成特征。

Result: 在真实世界和合成数据集上的大量实验表明，CodeFormer++在视觉保真度和身份一致性方面均优于现有方法。

Conclusion: CodeFormer++通过将生成先验与身份保持任务分解为对齐、纹理引导恢复和深度度量学习融合三部分，能够在保持身份一致性的同时提升视觉质量，解决了视觉质量与身份保真度之间的权衡问题。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [111] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出A.I.R.：一种训练免费、基于VLM语义推理的自适应迭代关键帧选择方法，兼顾精度与效率，能在VideoQA任务上提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级相似性方法（如CLIP）无法充分反映复杂query与帧的真实相关性，而直接用VLM全面分析全视频又计算代价高昂。需要一种兼顾语义理解能力与计算效率的帧选择方案。

Method: 使用强大的VLM进行对复杂问题的深度语义分析，并将该分析嵌入到一个成本敏感的迭代循环中：每次只处理一小批高潜力帧，通过相似性与推理引导的打分与筛选迭代更新候选集，直至得到最终少量关键帧供下游回答模块使用。该方法无需额外训练（training-free），避免了昂贵的模型微调。

Result: 在多种VideoQA基准上，A.I.R.优于现有帧选择方法，显著提升了基础VLM的问答性能，同时在计算效率上相较于其他基于VLM的方法有明显节省。

Conclusion: A.I.R.通过引入训练-free的自适应、迭代、基于推理的关键帧选择机制，在保证精度的同时显著降低了计算开销，为将强大的视觉语言模型高效应用于VideoQA提供了可行路径。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [112] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: 提出reAR，一种在自回归视觉生成器中加入逐token正则化的简单训练策略，通过在预测下一个token时同时恢复当前token的视觉嵌入并在噪声上下文中预测目标token的嵌入，解决生成器与分词器不一致问题，无需修改分词器或推理流程，显著提升ImageNet上的gFID和IS指标。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视觉生成性能落后于扩散模型，归因于分词器和栅格化顺序问题，而作者认为核心瓶颈是生成器生成的token无法被分词器良好解码（生成器-分词器不一致），需要在训练中显式对齐。

Method: 在标准因果Transformer训练中，加入两个额外的回归任务：1) 在预测下一个token时恢复当前token的视觉嵌入；2) 在噪声上下文下预测目标token的视觉嵌入。该策略不改分词器、生成顺序或推理流程。

Result: 在ImageNet上，使用标准栅格化分词器时，gFID从3.02降至1.86，IS提升至316.9；在先进分词器下，177M参数模型达到gFID 1.42，匹配675M参数的扩散模型性能。

Conclusion: reAR通过引入token级别的视觉嵌入恢复与预测正则化，有效缓解生成器-分词器不一致，显著提升自回归视觉生成质量，在参数更小的模型上达到或匹配更大扩散模型性能。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [113] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: 提出SPEGNet，一种统一的伪装目标检测网络，通过通道校准和空间增强融合多尺度特征，直接从上下文丰富的表征中恢复边界，并以渐进精细化实现尺度自适应的边缘调制，在保持边界精度与区域一致性间取得平衡。相比累积复杂组件的现有方法，SPEGNet计算效率高、实时推理，并在CAMO、COD10K、NC4K数据集上取得优秀指标。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法通过独立加入边界模块、注意力机制和多尺度处理等复杂组件来提升性能，导致计算开销增加且常常需要降分辨率处理，丢失伪装目标的细节；因此需要一种统一且高效的方法来在不同尺度间保持语义-空间对齐并恢复精细边界。

Method: 提出一种将多尺度特征通过通道校准（channel calibration）与空间增强（spatial enhancement）融合的网络，并在中间分辨率阶段以渐进精细化（progressive refinement）实现尺度自适应的边缘调制，使边界信息直接从上下文丰富的表征中生成，减少独立边界模块与复杂注意力机制的使用。

Result: 在CAMO上S_alpha=0.887，在COD10K上0.890，在NC4K上0.895；同时实现实时推理速度，对小型、复杂和大尺度相似纹理目标均表现良好，且能处理遮挡和边界模糊情况。

Conclusion: SPEGNet通过统一设计避免多余组件堆叠，利用通道校准与空间增强结合多尺度信息并在中等分辨率处进行最强边缘调制，从而在边界精度和区域一致性上取得良好平衡，实现了在多个伪装目标检测基准上的领先性能并保留实时速度。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [114] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM把检测数据连同器官分割和结构化理由转成带CoT推理的医学VQA数据，并用分层的CoT课程策略训练模型，在多项基准上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 弥合临床诊断推理与AI之间的差距，需要可扩展且具有临床对齐能力的数据和训练策略，使医学视觉-语言模型能生成带有临床推理步骤的回答。

Method: 构建MedCLM管道：1) 将检测数据中病灶边框与器官分割和结构化理由关联，生成含CoT的问答对；2) 设计Integrated CoT-Curriculum Strategy，分为Easy（显式病灶框）、Medium（鼓励隐式定位）、Hard（弱监督推理）三个训练阶段，以逐步增强视觉-语言模型的定位与推理能力。

Result: 在多个医学VQA基准上取得了最先进的性能，证明MedCLM能有效扩展用于临床对齐的医学视觉-语言模型的数据和训练方法。

Conclusion: MedCLM提出了一种将检测数据集自动转换为带有Chain-of-Thought (CoT)推理的大规模医学视觉问答数据的管道，并通过结合病灶框与器官分割及结构化理由来提升模型生成逐步推理的能力。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [115] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 本文构建首个古希腊陶罐3D VQA数据集VaseVQA-3D并提出VaseVLM通过领域自适应训练显著提升模型在3D陶罐理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统VLM在通用图像说明和视觉推理上表现良好，但在文化遗产等专业领域（如3D陶罐）由于训练数据稀缺和领域知识不足，导致识别与理解能力不足，亟需专门数据集与领域适配方法。

Method: 构建包含664个古希腊陶罐三维模型的VaseVQA-3D数据集，配套问答对并建立完整数据构建流程；在模型方面，通过领域自适应训练（fine-tuning或适配策略）将基础视觉语言模型调整到陶罐领域，提升特征对齐和语义理解能力。

Result: 在VaseVQA-3D上，VaseVLM相比先前最先进方法在R@1指标上提升12.8%，在词汇相似度（lexical similarity）上提升6.6%，显著改善对3D陶罐的识别与理解。

Conclusion: 本文提出了用于古希腊陶罐分析的首个3D视觉问答数据集VaseVQA-3D，并基于此进行领域自适应训练，得到名为VaseVLM的模型，在3D陶罐识别与理解上显著提升。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [116] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: 提出面向电商的图像编辑模型TBStar-Edit，通过数据工程、层次化模型与两阶段训练提升编辑精度与一致性，在自建电商基准上优于通用编辑模型。


<details>
  <summary>Details</summary>
Motivation: 通用图像编辑模型在电商场景中存在一致性限制（商品外观或布局变化不稳定），需要定制化方法以保证商品图像的高保真和一致性。

Method: 构建了完善的数据工程流程（采集、构建、过滤、增强），设计层次化模型框架（基础模型、pattern shifting模块、consistency enhancement模块），并采用两阶段训练策略：第一阶段训练编辑模式迁移，第二阶段训练一致性增强；不同模块在不同数据上训练。

Result: 在作者自建的电商基准上，TBStar-Edit在VIE Score等客观指标和用户主观偏好上均优于现有通用域编辑模型。

Conclusion: 该论文针对电商场景提出了TBStar-Edit，一种注重保留商品外观与排版一致性的图像编辑模型，声称在客观和主观评估上优于通用编辑模型。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [117] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出了"异步扩散模型"，给不同像素分配不同去噪时间步，使与提示相关的区域更慢去噪以利用更清晰的上下文，从而提升文本到图像的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 同步去噪导致所有像素在同一噪声水平上演化，使提示相关区域无法参考更清晰的邻域，降低文本到图像对齐度。

Method: 在扩散生成过程中为不同像素动态分配不同的去噪时间步，相关区域采用更慢的去噪进度，修改像素级去噪过程以允许利用更清晰的邻域上下文。

Result: 大量实验表明该方法能显著提高文本到图像的对齐效果，且适用于多种提示。

Conclusion: 异步扩散模型通过像素级时间步调度，显著改善了文本与生成图像的对齐性，在各种提示下均有提升，并公开了代码。

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [118] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: TAG是一种基于中间样本投影放大切向分量的高效扩散引导方法，通过一阶泰勒展开理论证明可将采样引向更高概率区域，从而提升生成图像的语义一致性与质量，且为架构无关、计算开销低的插件式模块。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在生成图像时常出现语义不一致或幻觉，现有引导方法要么依赖外部信号要么需架构改动并增加计算开销，需一种更高效、直接且无架构依赖的引导方法。

Method: 利用中间样本作为投影基，计算估计得分向量在该基上的切向分量，并对该分量进行放大，结合一阶泰勒展开分析，证明放大切向分量能修正采样轨迹而无需修改模型结构。

Result: TAG作为即插即用模块，能在不改动扩散模型的前提下提高采样保真度，减少不一致性，且计算开销极小。

Conclusion: TAG通过放大与中间样本正交的切向分量，引导扩散采样轨迹朝更高概率区域移动，从而减少语义不一致和幻觉问题。

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [119] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: CRL uses LLM-generated descriptive words to define a semantic basis, then projects VLM image features into this conditional space to produce task-specific embeddings without supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Universal representations capture dominant semantics that may not align with specific downstream needs; CRL aims to provide customizable embeddings for arbitrary user-specified criteria without expensive supervised fine-tuning.

Method: Given a user criterion, use an LLM to generate descriptive texts forming a semantic basis; construct basis vectors via VLM embeddings of these texts; project image features into this conditional space to obtain tailored embeddings.

Result: Proposes Conditional Representation Learning (CRL) to tailor image embeddings to user-specified criteria using LLMs to generate descriptive texts approximating a semantic basis and VLMs to project images into that conditional space.

Conclusion: CRL effectively produces customized representations that improve performance on classification and retrieval for various user-defined criteria, offering a parameter-free alternative to fine-tuning.

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [120] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 通过记录病理学家日常视图导航并生成行为驱动的标注（Pathology-CoT），构建了Pathologist-o3智能体，实现了可解释的区域建议与基于行为的推理，在淋巴结转移检测上超越现有o3模型并具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有病理学基础模型虽然能力强，但缺乏决定下一步查看位置、调整放大倍数并输出可解释诊断的代理系统，主要原因是缺乏可扩展且临床对齐的专家视图行为监督数据。

Method: 1) 开发AI Session Recorder在标准WSI查看器中记录视图导航并转换为离散行为命令和边界框；2) 人工轻量复核将AI草拟的理由整理为Pathology-CoT数据集；3) 基于该数据训练两阶段Pathologist-o3，第一阶段生成候选ROI，第二阶段进行行为引导的因果推理和诊断。

Result: The paper introduces the AI Session Recorder to capture pathologist viewing behavior and transforms logs into standardized commands and bounding boxes; creates Pathology-CoT dataset via lightweight human-in-the-loop review; builds Pathologist-o3 two-stage agent that proposes ROIs and conducts behavior-guided reasoning; shows improved performance on GI lymph-node metastasis detection over OpenAI o3 and generalizes across backbones.

Conclusion: 行为记录结合轻量人工复核能产生高效、可扩展的专家监督，从而使基于行为的病理学智能体成为可行且更具解释性的临床工具。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [121] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出S2Fin，通过空间、光谱、频率域交互与高频稀疏增强变换器及两级频率融合，有效提取结构与细节特征，在四个基准多模态数据集上优于现有方法


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing fusion methods which fail to extract structural and detail features from heterogeneous and redundant multimodal images by introducing frequency domain learning for key and sparse detail features

Method: Read and analyze paper abstract

Result: Proposed S2Fin network with pairwise fusion modules across spatial, spectral, and frequency domains; introduced high-frequency sparse enhancement transformer with sparse spatial-spectral attention; two-level spatial-frequency fusion (adaptive frequency channel module and high-frequency resonance mask); spatial-spectral attention fusion module; superior classification performance on four benchmark multimodal datasets with limited labeled data

Conclusion: S2Fin成功将频域学习与空间-光谱特征融合，增强高频细节与边缘信息，在有限标注下实现多模态遥感图像分类的性能提升。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [122] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 本文提出一种融合Swin/ViT和基于纹理方法的集成框架，通过数据切分、顺序训练、频域分割、patch注意力和人脸分割等策略，提高深度伪造检测的准确性与泛化能力，在DFWild-Cup数据集上达到SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同数据集和生成技术间泛化能力不足，且难以兼顾全局结构与局部纹理信息，因此需要一种结合二者优点的鲁棒检测框架。

Method: 提出将多模型集成：Transformer（Swin、ViT）用于全局特征，纹理方法用于局部/频域特征；引入数据切分与顺序训练缓解数据不平衡，频域分割和patch级注意力强调高影响区域，并利用人脸分割集中判别区域。最终通过集成预测输出得到决策。

Result: 在DFWild-Cup（一套涵盖八个深假数据集的多样化子集）上达到最新最优性能；实验证明Transformer擅长全局特征，纹理方法提升可解释性和局部细节捕捉，二者互补。

Conclusion: 混合模型结合Transformer的全局特征提取与纹理方法的可解释性，能显著提升深度伪造检测的稳健性和泛化能力，适用于多样化、现实场景的检测任务。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [123] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 单模型差异有限；结合更好的分割方法与分类器融合可提升疏伐/毁林检测性能


<details>
  <summary>Details</summary>
Motivation: SLIC常用但未必最优，探索其他超像素方法在林地变化检测任务中是否能提升分类性能

Method: 实验比较五种分割方法对林地变化检测的影响

Result: 单模型下不同分割方法差异小；采用分类器融合后，某些分割方法能显著提升平衡准确率

Conclusion: 分割方法与模型融合均对遥感森林变化检测有正向影响，建议在部署前评估多种超像素方法并使用模型融合来稳健提升性能

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [124] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: 本工作提出EduPersona：一个面向课堂的主观能力评估基准，包含两种语言、三门学科、十种人格（基于大五人格），原始对话1,308轮、扩展到128k问答轮。基于此构建三层评估任务：TASK1（一致性）、TASK2（学生真实感）、TASK3（长期人格一致性）。对三款代表性大模型和其10种人格微调版本评测，取得显著提升（TASK1+33.6%、TASK2+30.6%、TASK3+14.9%）。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在教育场景中被广泛采用，但其课堂导向的主观能力（行为/情感/表达与人格一致性）缺乏系统评估，影响理解模型边界与可信部署，因此需建立专门基准与可验证评估范式。

Method: 构建跨语种、多学科、基于大五人格的人格化课堂对话数据集（1,308轮原始对话，扩展到≈128k轮），并定义三层递进评估任务。利用数据对三款LLM进行人格微调，比较原始与微调模型在三个任务上的表现。

Result: 微调后模型在三项任务上平均显著提升：TASK1+33.6%、TASK2+30.6%、TASK3+14.9%，并揭示人格建模的难度存在异质性；数据集与评估框架将开源支持社区研究。

Conclusion: EduPersona是首个聚焦课堂主观能力的基准与评估框架，能有效提升并量化模型的课堂行为、情感、表达与人格一致性，为可信、类人教育AI研究提供数据与方法支持。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [125] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 提出一种分层多阶段的步态专家混合模型（MoME），通过阶段化处理和任务门控在PsyMo数据集上提升心理特质多任务预测性能，且辅助任务有助于提升结果。


<details>
  <summary>Details</summary>
Motivation: 从步态中提取心理特征具有潜力，但难点在于如何表示并利用复杂时序运动信息用于多任务学习。作者希望通过分层的专家模型和多阶段处理提高对心理属性的预测能力。

Method: 将2D姿态序列按行走周期分为四个复杂度阶段；每阶段使用多个轻量级专家网络提取时空特征；对每个任务在每个阶段使用门控模块为专家加权；多任务学习框架并可加入身份、性别、BMI等辅助任务以提升主任务表现。

Result: 提出了Multi-Stage Mixture of Movement Experts (MoME)架构：将步态分为四个运动复杂度阶段，使用轻量专家模型提取时空特征，并通过任务特定的门控模块在特征和阶段间自适应加权。对PsyMo基准（17个心理特质）上进行评估，分别在运行层和主体层获得37.47%和44.6%的加权F1，优于现有步态分析模型；加入身份识别、性别、BMI等辅助任务进一步提升效果。

Conclusion: 分层多阶段的专家混合架构有效利用步态时空信息以改善多任务心理学属性预测，证明步态可用于心理特质估计并为未来研究提供基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [126] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出ConceptSplit：训练时用ToVA仅调整value投影，推理时用LODA优化潜变量，从而缓解多概念个性化中的概念混合问题并取得良好效果。


<details>
  <summary>Details</summary>
Motivation: 动机是多概念个性化（在T2I扩散模型中同时表示多个主题）常遭遇概念混合（多个学习到的概念相互干扰或混合），需要一种在训练和推理层面分离概念的方案。

Method: 方法包含两部分：1) Token-wise Value Adaptation (ToVA)：训练阶段仅适配cross-attention的value投影，避免修改key导致的注意力机制破坏；2) Latent Optimization for Disentangled Attention (LODA)：推理阶段通过优化输入潜变量以缓解注意力纠缠。

Result: 通过大量定性与定量实验，ConceptSplit 在减少不期望的概念干扰方面表现出鲁棒性，实验代码已开源。

Conclusion: 本论文提出ConceptSplit，通过训练和推理阶段分别优化以分离多概念个性化中出现的概念混合问题，验证了方法的有效性。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [127] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出一种结合3D基础模型微调与交叉伪监督协同训练的标签高效肝脏分割方法，在无配准、多相、多厂商MRI下对有标签和无标签域均表现稳健


<details>
  <summary>Details</summary>
Motivation: Labeled hepatobiliary-phase (GED4) MRI are scarce while non-contrast phases are unlabeled; spatial misalignment and missing phases complicate registration-based methods; need label-efficient cross-modality generalization across vendors

Method: Fine-tuning a 3D foundation segmentation model + co-training with cross pseudo supervision

Result: Model fine-tuned from foundation 3D backbone, co-trained with CPS on unlabeled T1WI/T2WI/DWI, standard preprocessing, learns without registration and generalizes across phases/vendors with robust segmentation performance

Conclusion: 结合基础模型适配与协同训练能在现实临床多相多厂商MRI条件下实现标签高效且鲁棒的肝脏分割

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [128] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 提出基于FLAME参数引导的表达交叉注意力Adapter与Reference Adapter，结合ID一致基础扩散模型，实现身份一致且可控的细粒度面部表情生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法虽在面部身份保持上进展显著，但难以在不损害身份一致性的情况下实现对细粒度表情（包含微表情和过渡）的精确控制；因此需要一个在保持身份一致的前提下，对表情进行精细、可控编辑的生成框架。

Method: 基于ID一致的面部基础扩散模型，设计了一个使用FLAME blendshape参数引导的表达交叉注意力模块（Adapter），通过混合图像和视频数据训练，以学习从基本情绪到微表情及表情过渡的细粒度控制；另有一个可插拔的Reference Adapter实现从参考帧迁移外观用于真实图像编辑。

Result: 所提模型在量化指标和视觉效果上均优于现有方法，能够生成带有细微表达与表达过渡的身份一致性面部图像，并支持将参考帧的外观迁移到真实图像中；论文提供代码和模型。

Conclusion: 该论文提出了一个在保持身份一致性前提下实现细粒度面部表情可控生成的扩散框架，包含表达交叉注意力模块和可插拔参考适配器，能在真实图像上进行表达编辑并在定量与定性评测中优于现有方法。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [129] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: 提出Temporal diffusion模型ReactDiff，结合时间行为运动学和面部动作单元依赖约束，生成更平滑、多样且符合人体面部结构的反应。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时兼顾随机性与时序连贯性，常导致抖动、不稳定过渡和非自然表情，因而需要将生理解剖学和时序动力学引入生成过程以逼近真实人类反应流形。

Method: 在条件扩散框架中引入两种先验：1）时间面部行为运动学（鼓励轨迹平滑与动力学一致性）；2）面部动作单元依赖（保持解剖学一致的表达组合）。模型以对话上下文为条件，进行带约束的反向扩散采样以生成表情序列。

Result: ReactDiff通过在扩散过程融入时空面部运动学先验，显著提高了对话中面部反应的连贯性、多样性和自然度。

Conclusion: ReactDiff在REACT2024数据集上实现了领先的反应质量，尤其在多样性和语义适配性上有明显优势，且有效抑制抖动和不自然过渡。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [130] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 提出了基于对比预训练的高判别性物体特征编码器，解耦特征学习与场景图预测，并融合几何与语义信息，显著提升3DSSG上物体与关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 3D Semantic Scene Graph Prediction suffers from poor object and relationship feature representations; existing methods over-rely on GNNs and lack discriminative object features, limiting scene graph accuracy.

Method: 设计判别性强的物体特征编码器，使用对比学习进行预训练来分离物体表征学习与场景图任务，且在关系预测中融合几何与语义特征；将预训练编码器直接替换进现有框架进行评估。

Result: A contrastive-pretrained, highly discriminative object feature encoder that decouples object representation learning from scene graph prediction, improving object classification and relationship prediction; combining geometric and semantic features for better relation prediction; substantial performance gains on 3DSSG and improvements when plugged into existing frameworks.

Conclusion: 高质量的物体表征是提升3D语义场景图性能的关键；通过对比预训练和结合几何语义特征，可以显著提升物体分类与关系预测，且能直接增强现有方法的表现。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [131] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 首次针对相机陷阱场景建立单目绝对深度估计基准，Depth Anything V2在精度-速度上表现最佳，ZoeDepth最快但不准；推荐使用中位数提取深度，并提供了面向野生动物监测的实用指导。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱广泛用于野生动物监测，但单目图像缺乏深度信息，现有MDE方法在自然野外环境下的性能未被系统评估，需要建立基准以指导保护监测系统中深度估计的实际应用。

Method: 使用已标定的ChARUCO棋盘获得图像中的真实距离，并对Depth Anything V2、ML Depth Pro、ZoeDepth、Metric3D及一个几何基线方法进行评测；比较指标包含平均绝对误差(MAE)、相关性、以及提取深度时的均值/中位数策略的影响，同时统计每张图像的推理时间。

Result: Depth Anything V2表现最好(平均绝对误差0.454m，相关系数0.962，推理时长0.22s/图像)；ZoeDepth速度最快(0.17s/图像)但精度最差(MAE 3.087m)；在所有深度学习方法中，基于中位数的深度提取优于均值策略。

Conclusion: 本文首次建立了野外监测条件下的单目绝对深度估计基准，对四种SOTA方法与几何基线在93张带有ChARUCO标定的相机陷阱图像上进行评测，发现Depth Anything V2在精度与速度上最优，ZoeDepth在自然户外环境中表现显著下降。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [132] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: ExposureEngine使用定向边界框替代水平框，显著改善体育转播中赞助商logo的检测与曝光度量，并提供可交互的分析与审计工具。


<details>
  <summary>Details</summary>
Motivation: 传统基于HBB的方法在logo旋转或透视变形时会导致曝光度量不准确，需要一种旋转感知且可审计的自动化分析系统。

Method: 构建OBB检测器（训练数据集：1,103帧、670个唯一logo的OBB标注），模型输出带角度的有向边界框并融合到分析管道中计算曝光时长与覆盖面积，且加入语言驱动的交互层生成报告。

Result: OBB检测器在数据集上达到mAP@0.5=0.859，精度0.96，召回0.87；系统能输出精确的可见性指标并支持自然语言查询与报告生成。

Conclusion: 提出了ExposureEngine，一个基于OBB的端到端系统，实现旋转感知的赞助商可见性分析，在足球转播场景中获得高精度检测和可解释的曝光度量。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [133] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: 将统计异常检测嵌入YOLO检测头，Treat小目标为背景中的异常，从而控制误报率并提升在少样本、噪声和域偏移下的鲁棒性，结构通用且适配多种YOLO变体。


<details>
  <summary>Details</summary>
Motivation: 红外小目标在复杂背景和微小尺度下容易被常规检测器误检，且真实场景中训练样本有限、存在噪声与域偏移，需一种能控制误报率并具备高鲁棒性的方案。

Method: 在YOLO检测头集成统计异常检测检验，将小目标视为相对于背景的异常模式；仅修改检测头以保持对不同YOLO骨干的兼容性，并可扩展至实例分割模型。

Result: 在多个IRSTD基准上取得有竞争力的检测性能，尤其在训练数据受限、噪声污染和域转移场景中表现稳健；设计轻量且对多种YOLO骨干及实例分割模型均适用。

Conclusion: AA-YOLO通过在YOLO检测头中引入统计异常检测，有效将小目标检测问题转化为异常检测，从而显著降低误报率并提升在小样本、噪声和域偏移情况下的鲁棒性。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [134] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: Transformers on COCO WholeBody keypoints from CANDOR conversations: spatial stream best, temporal useful, fusion gives 98.03%.


<details>
  <summary>Details</summary>
Motivation: Evaluate transformer-based models for person identification during natural conversations using body keypoints.

Method: Two-stream framework modeling spatial (configs) and temporal (motion) of 133 keypoints; compare pre-trained vs scratch, test velocity features, and propose multi-scale temporal transformer for hierarchical motion modeling.

Result: Domain-specific training outperforms transfer learning; spatial information more discriminative than temporal; spatial transformer 95.74% accuracy; multi-scale temporal transformer 93.90%; fusion achieves 98.03%.

Conclusion: Transformer architectures are effective for person identification in natural interactions; both posture and motion are complementary; future work should explore multimodal and cross-cultural datasets.

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [135] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: 提出PG-Occ，一种渐进式高斯Transformer框架，通过在线密化和各向异性采样的时空融合，在保持计算效率下增强3D高斯表示以捕捉小物体，实现开放词汇的3D占据预测。相较于现有方法，mIoU提升14.3%。


<details>
  <summary>Details</summary>
Motivation: 传统固定语义类别的3D占据预测无法应对开放词汇查询；而现有稀疏高斯难以表示小物体、稠密表示又计算昂贵，需要一种在效率与细节捕捉之间折衷的新表示与策略。

Method: 核心方法包括：1) 渐进在线密化（progressive online densification），在前向过程中逐步增加3D高斯的稠密度以恢复细节；2) 各向异性感受野采样与时空融合，按尺度和阶段自适应分配感受域以更好聚合特征；3) 基于Transformer的架构用于跨尺度信息交互并输出文本对齐的占据表示，支持开放词汇查询。

Result: 在公开基准上PG-Occ取得SOTA，avg mIoU相较最优方法提升14.3%，同时在细小目标的检测和语义对齐能力上表现更好。

Conclusion: PG-Occ在开放词汇3D占据预测上显著优于先前方法，兼顾细节捕捉与计算效率，证明渐进密化和各向异性采样时空融合有效。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [136] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 本文关注开放词汇学习中的分布估计问题，提出通过生成未见类数据来缩小估计误差。建立理论上界表明生成未见类数据能上界估计误差，并设计了基于层次语义树与领域信息的未见类数据生成流水线和一个分布对齐算法。实验证明在11个数据集上最多提升14%。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅用已见类估计开放环境分布，因缺乏未见类样本导致估计误差不可识别。作者认为生成未见类数据可帮助学习超出已见类，从而使分布估计有界。

Method: 提出了两个核心模块：1）类-域级的数据生成流水线，借助层次语义树和从已见类推断的域信息合成未见类样本；2）分布对齐算法，利用生成数据估计并最大化后验概率以提高泛化。

Result: 在11个数据集上的大量实验显示，该方法较基线最高提升约14%，证明了生成未见类数据并进行分布对齐在开放词汇学习上的有效性。

Conclusion: 通过为开放环境生成未见类样本并进行分布对齐，可以有效估计整体数据分布，从而提升开放词汇学习的泛化性能，实验结果支持该方法的优越性。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [137] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg benchmark shows limited cross-center generalization; fine-tuning helps; ViViT + spatiotemporal modeling promising; challenges: imbalance, tuning


<details>
  <summary>Details</summary>
Motivation: assess FL generalization and adaptation for surgical video classification

Method: analysis of methods

Result: ViViT-based approach best; fine-tuning improves per-center performance; overall generalization limited

Conclusion: benchmark identifies trade-offs between personalization and global robustness; points to need for imbalance-aware and adaptive FL methods

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [138] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出一个两机器人协调的自动高分辨率3D扫描系统，通过将扫描空间参数化为区域、优化轨迹与航点分配，实现更全面覆盖、降低遮挡并提高重建精度与效率。


<details>
  <summary>Details</summary>
Motivation: 减少对人工或半自动操作的依赖，使文化遗产文物的高保真3D扫描过程自动化，提升保存和记录的可扩展性与一致性。

Method: 将扫描空间划分为多个区域，使用带扫描仪的机器人和托盘机器人协同规划运动；通过优化轨迹和航点分配以最大化覆盖并最小化遮挡，同时在精度与效率之间做平衡。

Result: Automated two-robot high-fidelity 3D scanning system for cultural heritage artifacts.

Conclusion: 该系统在Chamfer Distance和F-score上均优于基线方法，证明在几何精度、数字化效率和减少对专家依赖方面具有显著优势。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [139] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: TL;DR: ViT在数据充足时微调优于CNN，跨域泛化更强；但在少量数据时，CNN的归纳偏置让其表现更稳健。建议开发混合架构以平衡局部与全局表征，提升几何估计任务表现。


<details>
  <summary>Details</summary>
Motivation: 动机：尽管ViT和大规模CNN在视觉表示学习上取得成功，但它们作为几何估计任务（特别是在低数据量下、涉及图像变形）主干的效率和适应性尚不清楚；这些任务需要不同程度的局部和全局信息平衡。

Method: 方法：系统比较了多种预训练主干网络（ResNet, EfficientNet, CLIP-ResNet, CLIP-ViT variants, DINO），在不同数据量设置（包括few-shot）下微调用于两个几何估计任务：1) 估计图像对之间的2D刚性变换；2) 预测立体图像对的基础矩阵。评估包括同域和跨域性能，侧重分析模型的局部/全局表征能力与归纳偏置影响。

Result: 结果：1) 在大下游数据量时，ViT微调表现最好；2) 在小数据量时，CNN由于归纳偏置和较小模型容量表现更稳健，可与ViT匹敌；3) ViT在跨域（数据分布变化）泛化能力更强；4) 强调需要为几何任务考虑混合或调整架构以兼顾局部与全局特征。

Conclusion: 作者结论：在大规模下游数据中，ViT在微调上优于CNN；在小数据（few-shot）场景下，具有归纳偏置和较小容量的CNN可匹配ViT；ViT在跨域泛化上更强；建议为几何估计任务慎重选择或设计混合架构以平衡局部与全局特征。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [140] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出基于Diffusion Transformer的DiT-VTON，通过多种条件整合与大规模、多样化训练，实现更鲁棒、保细节且支持广泛商品类别与图像编辑功能的虚拟试穿/试用系统。


<details>
  <summary>Details</summary>
Motivation: 现有VTO模型在细粒度细节保留、真实场景鲁棒性、采样效率、图像编辑与通用化能力方面不足，需一种既能处理多品类又具编辑能力的统一方案。

Method: 在DiT架构上比较多种图像条件接入方式（in-context token拼接、通道拼接、ControlNet集成），并通过扩充训练数据（多背景、非服装类别、非结构化参考图）进行数据放缩训练，直接以图像条件驱动生成，无需额外条件编码器。

Result: 在VITON-HD上细节保留与鲁棒性超越现有最先进方法；在大规模千类商品数据集上，也优于具VTA及编辑能力的模型，并展示姿势保持、局部编辑、纹理迁移与目标层级定制等功能。

Conclusion: DiT-VTON通过将Diffusion Transformer（DiT）适配为图像条件的虚拟试穿框架，有效提升了细节保留、鲁棒性与编辑能力，构建了更通用的虚拟试穿/试用系统（VTA）。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [141] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: 该论文提出EgoSurg，一种从固定挂墙摄像头视频重建手术室中任意人员的第一视角回放的框架，实现无需干预临床流程即可合成高保真、任意视角的动态3D记录。


<details>
  <summary>Details</summary>
Motivation: 传统固定摄像头或口述回忆无法记录手术团队成员的主观视觉视角，限制了对决策过程（影响手术安全、培训和流程优化）的理解。

Method: 结合几何驱动的神经渲染与基于扩散模型的视图增强，从房间尺度的固定摄像头视频中重建动态场景并合成任意时间点的第一视角影像；在多站点手术案例与受控实验中进行评估。

Result: 在多站点真实手术和受控研究中，EgoSurg成功重建出具高视觉质量和保真度的个体特定视野及任意视点重放。

Conclusion: EgoSurg能将现有手术室摄像头基础设施转换为可导航的动态三维记录，重构个体特定视觉视野并合成任意视点的高质量回放，为沉浸式手术数据科学和手术可视化、分析提供新基础。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [142] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 研究显示主流多模态大模型中，图像value tokens在LM内部仍保留可用于零-shot感知任务的信息，但经过MLM微调后通常比未微调的视觉编码器（如SigLIP）含信息更少；key tokens在后层包含输入无关的伪影，削弱感知能力；通过在图像输入加文本前缀等方法可部分恢复和改善感知信息。


<details>
  <summary>Details</summary>
Motivation: 尽管有针对ViT编码器和Transformer激活的可解释性研究，仍缺乏对多模态大模型为何在感知密集任务上表现不佳的机制性理解；本工作通过检查视觉key-value tokens在LM内部的流动与变形，提供新的视角以揭示和改进MLM的感知能力。

Method: 分析了LLaVA-OneVision、Qwen2.5-VL、Llama-3-LLaVA-NeXT的视觉key-value tokens，测量视觉value tokens在LM中对多项感知任务（分割、语义/时间对应、指代表达检测等）的零-shot表现，与未微调编码器SigLIP对比；检测input-agnostic key tokens在后层的伪影并评估文本前缀对视觉信息保留的作用；在BLINK基准上量化信息未被输出利用的比例（如艺术风格问题中33.3%未被利用）。

Result: MLMs的视觉键值tokens承载重要但受限的感知信息

Conclusion: MLMs当前在感知重任务上受限主要因语言模型对视觉信息的处理（信息损失与伪影引入）以及键token的有害特性。改进方向包括更好地训练视觉编码器与语言模型的接口、增强语言模型对视觉信息的控制，如使用提示或结构化训练来保留并利用关键视觉信息。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [143] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON is a first 4D virtual try-on system from single garment images, using prior-free flow correction and non-linear deformation to produce temporally coherent, pose- and view-controllable realistic try-on results; validated with extended baselines.


<details>
  <summary>Details</summary>
Motivation: Enable 4D virtual try-on from single in-shop garment images supporting pose/view control without multi-view data or physics priors.

Method: Paper analysis

Result: Introduces Reciprocal Flow Rectifier for temporal coherence and Non-Linear Deformer for decomposing Gaussian maps to handle view-pose-specific deformations; establishes benchmark and shows high fidelity/diversity.

Conclusion: AvatarVTON effectively enables dynamic, realistic, and diverse garment try-on under single-view supervision, suitable for AR/VR and digital humans.

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [144] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: Use 3D Flow Matching conditioned on MRI/CBCT features to generate sCT; good global structure but misses fine details because of low resolution constraints; propose patch-based and latent-space approaches for future work


<details>
  <summary>Details</summary>
Motivation: Enable MRI-only and CBCT-based adaptive radiotherapy by generating sCTs with high-quality anatomical fidelity while reducing radiation exposure

Method: Fully 3D Flow Matching (FM) for sCT generation

Result: FM model accurately reconstructs global anatomical structures across abdomen, head & neck, thorax; limited preservation of fine details due to low training resolution

Conclusion: 3D FM is promising for sCT generation; improving training resolution (patch-based or latent-space flows) should enhance local detail preservation and clinical applicability

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [145] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: AT-BPTT通过阶段感知的时间步选择、自适应窗口和低秩Hessian近似，使数据蒸馏的内循环更高效、更准确，兼具性能提升与计算资源节约。


<details>
  <summary>Details</summary>
Motivation: 现有数据蒸馏的内循环截断通常采用随机截断，忽略了神经网络在不同训练阶段具有不同学习动力学的事实，导致截断不合理且效果受限。作者观察到早中晚期的学习行为差异，提出自适应截断以提升效率和性能。

Method: AT-BPTT包含三部分：1) 基于概率的阶段感知时间步选择机制，用于根据训练阶段概率采样截断位置；2) 基于梯度变化的自适应窗口大小策略，根据梯度方差/变化调整反向传播时间步长度；3) 低秩Hessian近似，用以降低计算和内存开销。

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上，AT-BPTT平均比基线方法提高6.16%准确率；内循环优化加速3.9倍，内存节省63%。

Conclusion: 本文提出的AT-BPTT通过动态调整截断位置和窗口大小，针对模型在不同训练阶段的梯度行为，实现了更有效的内循环优化，从而显著提升了数据蒸馏性能。

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [146] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 提出一种基于航拍总览图像的光伏电站自动建模方法，通过视觉分割识别光伏组件并推断结构信息（排、列、支架），利用布局相关关键点在多图间合并检测，生成带语义的三维地理参考模型，便于运维。


<details>
  <summary>Details</summary>
Motivation: 缺乏准确、最新的电站模型会影响运维效率，且现有方法依赖第三方数据。本工作旨在通过仅基于航拍图像的自动化映射方法，去除对外部数据的依赖，达到组件级别的详细建模。

Method: 首先对航拍总览图像进行光伏组件的视觉分割；然后识别与布局相关的视觉关键点，推断每个组件所属的支架、行、列；接着基于这些关键点将来自多张图像的检测结果合并，保持结构完整性；最后融合三维位置与语义结构，生成地理参考模型。

Result: 在两个不同电站的实验验证中，方法成功生成了包含组件级语义结构的三维模型，证明了该方法的可行性与实用性。

Conclusion: 该方法能在组件级别实现电站自动化建模，保留了结构完整性并生成紧凑的三维地理参考模型，实验在两个电站上验证，证明适用于运维。

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [147] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 本文提出一种从3D骨骼关节数据识别肢体交流功能（kinesics）的框架，结合ST-GCN和CNN并使用迁移学习，兼顾可扩展性与隐私保护，在DUET数据集上表现良好，可用于强化学习的模拟与人机环境交互建模。


<details>
  <summary>Details</summary>
Motivation: 现有理论模型或问卷法难以普适、静态且耗时，且难以兼顾隐私；需要一种从低维匿名化的骨骼数据中自动推断心理状态相关肢体信息的方法。

Method: 使用空间-时间图卷积网络（ST-GCN）对骨骼时空结构建模，结合CNN和迁移学习以避免人工定义动作到心理类别的映射，输入为3D骨骼关节序列，输出为肢体交流功能类标。

Result: 在DUET数据集上的实验显示，该方法能准确识别肢体交流功能，具有可扩展性并能揭示反映认知与情绪状态的潜在动作结构。

Conclusion: 所提框架能在不泄露身份的前提下，从骨骼数据中学习到与认知、情绪相关的肢体交流模式，提升行为建模的规模化和准确性，为RL驱动的人-环境交互仿真提供新路径。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [148] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 该论文关注将网络物理系统与社会利益对齐，提出CPSIS框架，并针对社交行为测量的基础任务——双人互动识别展开研究。作者在真实场景深度骨架数据上，比较五种基于骨架的互动识别算法，使用12类交流型互动（包括文化与情感相关类别），以兼顾隐私和语义深度。


<details>
  <summary>Details</summary>
Motivation: 传统CPS侧重经济目标，忽视了社会/人本益处。为实现对社会目标（如增进互助或减少冲突）的测量与驱动，需要可隐私保护且能捕捉语义的人际交互识别方法。因此研究骨架数据在双人互动识别中的效用与局限。

Method: 在真实世界采集的深度传感器骨架数据集上，将12类双人互动按通信类型（如标志性手势、情感展示等）标注。实现并比较五种基于骨架的互动识别算法，评估指标包括分类准确率、混淆矩阵分析及隐私/可解释性讨论。

Result: 实验表明骨架方法能在一定程度上识别12类双人互动，但对文化/情感类互动的召回与精确度较低。不同算法在时间建模与跨主体交互建模上的性能差异显著，提示未来应结合更强的语义/上下文信息与更多样化数据集进行改进。

Conclusion: 基于深度传感器的骨架方法在保护隐私的同时可实现对复杂双人互动的识别，但现有五种算法在处理文化与情感丰富的互动（如示意动作与情感展示）时仍有局限，需要更多语义化建模和更大多样性数据提升性能。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [149] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 提出了一种将早退(exit)结构与知识蒸馏结合的方法，训练一个较小的学生早退模型；对教师错误分类样本引入基于熵的新损失，提高在保证准确率的同时显著降低计算量。实验证明在CIFAR10/100和SVHN上有效。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用（实时与边缘设备）受限于计算资源，需要在不显著牺牲精度下减小模型计算量；动态早退结构允许在运行时调节模型复杂度，但现有蒸馏方法未针对早退网络的错误预测样本进行专门处理。

Method: 从复杂的教师早退模型蒸馏到较小的学生早退模型，除传统KD损失外，对教师预测错误的样本加入基于预测熵的损失（鼓励学生对这些样本具有更高不确定性或合适分布），并联合训练早退分支以在不同退出点实现效率/精度折衷。

Result: 在CIFAR10、CIFAR100和SVHN上实验显示，与常规KD和基线早退模型相比，提出方法在保持或略增准确率的同时，显著降低浮点运算量（FLOPs）与延迟，证明了方法有效性。

Conclusion: 该方法通过对教师错误预测样本使用熵损失，有效平衡了精度与效率，实现了在多个数据集上的计算复杂度显著降低且分类性能不退化，拓展了知识蒸馏在早退网络中的应用。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [150] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 将自然图像IQA深度网络迁移并重训到显微镜图像，得到快速、泛化性强且支持patch可视化的μDeepIQA，用于稳健的显微镜图像质量评估。


<details>
  <summary>Details</summary>
Motivation: 传统显微镜图像质量指标实现简单但计算开销大、对大数据集耗时且对非理想图像不稳定；深度学习方法可提高性能、泛化性并加速预测，适合在显微镜成像数据上应用。

Method: 采用现有用于自然图像的深度卷积神经网络架构，针对显微镜图像重训练模型以回归各项质量指标与全局质量分数；模型提供每个patch的质量预测以实现空间化的质量映射。

Result: μDeepIQA在显微镜图像上实现了快速、稳定的质量估计，能在超出传统方法理想范围的输入上保持泛化能力，支持patch级别的质量评估与可视化，并在处理异常值和小区域质量评估上表现优越。

Conclusion: 本文提出将用于自然图像IQA的深度卷积网络架构迁移并重训练到光学显微镜图像质量评估，得到的μDeepIQA能快速稳定地预测单个质量指标和全局质量分数，能在标准方法外的情况中更好泛化，并支持基于patch的空间质量可视化。

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [151] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出一个结合果穗检测/重量估计与光照不变高光谱品质评估（LISA）的IoT机器人系统，有效应对光照域偏移，实验上检测召回0.82、果重R^2=0.76，品质泛化提升>20%，可产出空间分辨的产量与品质地图。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱田间应用因自然光照变化导致域偏移，影响品质预测的泛化性；同时缺乏将产量（果穗检测与重量）与品质（Brix、酸度）在空间上同时高分辨率映射的端到端系统。论文旨在填补这一空白，为精准葡萄栽培提供可操作的数据产品。

Method: 构建端到端机器人平台采集图像与高光谱数据；开发高性能的果穗检测与重量估计模型；设计Light-Invariant Spectral Autoencoder（LISA），采用域对抗思想学习光照不变特征以应对高光谱数据的域偏移；将检测、估计和品质预测模块集成，并生成地理参考的高分辨率产量与品质图。

Result: 整套流程在构建的数据集上取得果穗检测召回率0.82，果重预测R^2为0.76；LISA相比基线方法提升品质预测泛化能力超过20%；系统能生成带地理参考的高分辨率葡萄产量与品质地图。

Conclusion: 该论文提出了一个端到端的、物联网驱动的田间机器人系统，可实现葡萄产量和品质（Brix、酸度）的无损、实时和空间分辨率测量。系统集成了果穗检测与果重估计模型以及基于高光谱数据的品质评估新框架（LISA），并通过在实验室与早/下午自然光三种光照域上的数据验证，显示出在检测召回率、果重R^2和品质预测泛化能力上的优异表现。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [152] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 本文提出并公开了一个大规模的多模态海底栖息地数据集（约100万SSS切片，36000个有分割标注），并提供配套光学图像、水深图、工具与基准，旨在推动水下栖息地映射与多传感器融合研究。


<details>
  <summary>Details</summary>
Motivation: 构建大规模、多模态的海底栖息地数据集以推动水下遥感与机器学习研究，解决现有标注数据稀缺的问题。

Method: 采集和预处理SSS、测深和AUV光学图像；手动标注约36000个SSS切片的分割掩码；空间配准光学图像与SSS切片以便跨模态学习；发布原始数据、马赛克和开源工具以支持社区使用。

Result: 发布了包含约一百万侧扫声纳(SSS)切片、配套的水深图和约36000个带分割掩码的人工标注样本的多模态数据集，并提供原始传感器数据、马赛克图像和开源预处理与标注工具。

Conclusion: 该数据集将成为统一的基准资源，促进自监督与跨模态表示学习、AUV多传感器融合和海底分类算法的发展，降低研究门槛并提升可重复性。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [153] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 在基加利采集的198张摩托车图像上比较YOLOv5、Faster R-CNN、SSD和RetinaNet，发现轻量YOLOv5更适合资源受限的实时部署，而更复杂模型精度更高但难以实时部署；数据集规模与实现复杂性是主要瓶颈，推荐采用更简化的架构和扩充数据。


<details>
  <summary>Details</summary>
Motivation: 基加利等发展中国家城市中，摩托车作为主要交通工具行为不可预测且常违规，给自动驾驶系统带来重大挑战。本研究旨在评估现有目标检测模型在此类场景的适用性，以指导面向资源受限环境的可行解决方案。

Method: 使用PyTorch实现迁移学习，对YOLOv5、Faster R-CNN、SSD和RetinaNet四种目标检测模型在自采集的198张基加利摩托车图像数据集上进行训练与评估，比较精度（mAP/IOU）、定位性能和推理速度（FPS），并分析数据集与实现过程中的挑战。

Result: 在小规模数据集上，基于YOLOv5的轻量版本在推理速度与部署便利性上表现最好，但精度受限；Faster R-CNN在精度和定位上更好但推理慢且模型大；SSD和RetinaNet表现居中但同样受数据规模影响。总体表现受限于数据多样性、标注质量与计算资源，建议使用轻量化模型并侧重数据扩充与简化实现。

Conclusion: 在真实、资源受限的环境（如卢旺达基加利）中进行摩托车检测时，复杂的目标检测模型（如YOLOv5、Faster R-CNN、SSD、RetinaNet）各有优劣，但数据集规模小与实现复杂性显著限制了性能与可部署性。为提高可行性，应优先采用轻量级、简化的模型架构并扩充多样化标注数据，同时优化训练策略与推理效率。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [154] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出SAHC：用层次化分类头、可训练层次矩阵和层次一致性机制，增强遥感影像的层次特征学习与分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感影像分类多聚焦于细粒度类别，常忽视预定义的标签层次关系。作者希望利用这些层次信息改善特征学习与分类性能。

Method: 在深度网络中集成多个专门处理不同粗细粒度的分类头，使用可训练的层次矩阵自监督地学习层次结构，并引入层次一致性机制（加权集成）以保持不同层级概率分布一致。

Result: 在三个具有不同层次复杂度的基准数据集上、采用不同骨干网络进行评估，实验结果表明SAHC在引导网络学习层次结构和实现层次一致性方面表现有效且稳健。

Conclusion: 该论文提出的SAHC方法通过层次化分类头和可训练层次矩阵，结合层次一致性机制，有效地学习类别层次结构并提升遥感影像分类的鲁棒性。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [155] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: REN 是首个面向医学影像的解剖感知 MoE 框架，使用放射组学与多种深度特征的多模态门控训练区域专家，在 ILD 分类上显著提高了性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统 MoE 未嵌入医学图像特有的解剖与病变区域异质性，故提出 REN 以利用解剖先验提升区域性病变建模与临床可解释性。

Method: 构建了 7 个区域专家（各肺叶与双侧组合），利用解剖先验进行区域化训练；引入多模态门控机制，将放射组学生物标志物与 CNN/ViT/Mamba 特征融合以决定各专家权重；在病人级交叉验证下评估模型表现。

Result: 在 ILD 分类任务上，放射组学引导的 REN 集成平均 AUC 为 0.8646±0.0467，较 SwinUNETR 基线提升 12.5%，下叶专家 AUC 达 0.88-0.90，显著优于常规模型（CNN 0.76-0.79），并通过病人级交叉验证证明了泛化性。

Conclusion: REN 提出了基于解剖结构的 MoE 框架，通过多模态门控将放射组学与深度学习特征动态融合，从而提升了肺部疾病（尤其是 ILD）分类性能并增强可解释性。

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [156] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 提出NFPF：利用SFLM和重构差异进行渐进式无监督主动学习，显著提升样本选择质量，达到接近监督AL的效果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督主动学习依赖局部梯度评分和一次性线性选择，易受噪声和模糊数据影响，且难以覆盖整体数据分布，需一种能在无标签条件下更准确衡量样本重要性的方法。

Method: NFPF使用SFLM来学习样本对模型性能的贡献，基于SFLM构建重构差异(Reconstruction Difference)进行初始样本选择，避免了基于局部梯度的评分方法，并采用渐进式特征选择策略覆盖全局数据分布。

Result: 在多个视觉数据集上，NFPF明显优于所有被比较的无监督主动学习方法，并达到与监督主动学习方法接近的性能。消融实验和可视化结果支持其更强的鲁棒性和更好的数据分布覆盖。

Conclusion: NFPF提出了一种新的无监督主动学习框架，通过特定特征学习机(SFLM)和重构差异指标来衡量样本重要性，显著提升了对样本代表性和鲁棒性的选择能力，实验表明在视觉数据集上优于现有UAL方法并接近有监督AL性能。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [157] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出CA3D-Diff：列感知交叉注意力+隐式3D扩散模型，解决CC/MLO视图间的非刚性错位与组织重叠，能生成结构一致且有助于下游诊断的合成视图。


<details>
  <summary>Details</summary>
Motivation: 临床流程中常出现缺失或退化的双视图乳腺X线片，传统像素级或基于2D的方法难以处理严重的非刚性变形和组织重叠，需要一种能利用投影几何和跨视图列对应性的生成方法来恢复缺失视图并改善下游诊断。

Method: 基于条件扩散模型的双向视图翻译；设计列感知交叉注意力（引入高斯衰减列偏置以强调局部列对应关系）；构建隐式3D重建模块，将2D噪声潜表示按乳腺投影几何反投影到3D特征体并精炼注入UNet。

Result: CA3D-Diff提出了一种用于双视图乳腺X线片（CC<->MLO）转换的条件扩散框架，结合列感知交叉注意力和隐式3D重建模块，通过高斯衰减的列偏置强化局部列相关性，并将噪声2D潜表示反投影为粗糙3D特征体以增强解噪UNet的结构意识。实验显示在视觉保真度和结构一致性上优于现有方法，并能提升单视图恶性肿瘤分类性能。

Conclusion: CA3D-Diff能有效恢复缺失或退化视图，增强跨视图对齐与结构一致性，且在实际筛查中的单视图分类任务带来性能提升，具有实际诊断价值。

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [158] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: 提出SSDD—一种基于像素扩散且结合Transformer组件的解码器，并通过蒸馏得到高效单步解码器，实现无GAN训练下更高质量、更快的替代KL-VAE


<details>
  <summary>Details</summary>
Motivation: 替代KL-VAE作为tokenizer的解码器，消除对对抗损失的依赖，同时提供更好重建质量和更快采样

Method: Pixel diffusion decoder SSDD

Result: 单步蒸馏得到的单步解码器在不使用GAN的情况下，比KL-VAE在重建FID上从0.87提升到0.50，吞吐量提升1.4倍，且保持DiTs生成质量并将采样速度提升3.8倍

Conclusion: SSDD可作为KL-VAE的即插即用替代，提供更好的重建质量与更快采样，同时去除了对对抗训练的需求。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [159] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出在VFM的内部表征中嵌入数字水印，通过微调少量层和小的编码-解码器网络，使水印在功能性拷贝（如微调后的模型）中也可检测，从而实现模型所有权验证；理论与实验表明误检率和漏检率很低。


<details>
  <summary>Details</summary>
Motivation: Protect intellectual property of VFMs by embedding watermarks that survive downstream fine-tuning and enable ownership verification

Method: Fine-tune small layers + encoder-decoder watermarking

Result: Low false detection and low false misdetection probabilities; watermarks remain detectable after fine-tuning for downstream tasks

Conclusion: 方法在理论与实验上均证明能有效嵌入在微调后仍可检测的水印，能区分被盗重分发的模型与独立模型，适合VFM所有权验证。

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [160] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 该论文提出在预训练DNN后添加转换层以生成多重潜在表示来估计不确定性（LUR），并在此基础上提出带排斥训练的RLUR。与八种末层概率深度学习方法在四个视频驾驶动作/意图识别数据集上比较，LUR/RLUR在分类性能与校准上与其他方法相当；在基于不确定性检测OOD方面，LUR表现与最优方法匹配，同时训练更高效、调参更容易。作者还为NuScenes提供了新增标注。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限且需安全保障的视频驾驶任务，末层概率方法虽能检测OOD但性能不稳且有些方法训练复杂或成本高，故提出更高效、易调的潜在表示方法来估计模型不确定性。

Method: 在预训练DNN上追加转换层（产生多个隐表示）用于不确定性估计；实现两种变体：标准LUR和通过排斥训练增强表征多样性的RLUR；与八种末层概率深度学习（LL-PDL）方法在四个驾驶视频数据集上比较分类、校准和基于不确定性的OOD检测性能；并为NuScenes添加了大量帧级和视频级标注。

Result: LUR和RLUR在分布内分类和校准上与LL-PDL可比；在基于不确定性的OOD检测上，LUR可与最优方法匹配，并且训练效率更高、调参更容易；作者同时发布了NuScenes的额外标注数据。

Conclusion: LUR和RLUR在不降低原模型分类性能的前提下，可提供有效的不确定性估计；LUR在OOD检测上与最优LL-PDL方法持平，但训练更高效且调参简单；总体可作为末层方法的高效替代。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [161] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 用手绘螺旋和波形图结合迁移学习与注意力机制的CNN，数据增强与硬投票集成可在早期帕金森病检测中取得约93.3%总体准确率。


<details>
  <summary>Details</summary>
Motivation: 提升早期帕金森病诊断的便捷性与可及性，寻找非侵入性廉价生物标志物（手绘运动图像）。

Method: 使用预训练CNN（迁移学习）+自定义卷积层+注意力机制，进行数据增强，训练多模型并用硬投票集成。

Result: Models show promising diagnostic potential using hand-drawn spiral and wave images for PD detection, achieving high weighted metrics and improved ensemble accuracy.

Conclusion: 基于手绘图像的机器学习方法可作为一种无创、廉价的PD早筛手段，但需更大样本、多中心验证与对临床场景的鲁棒性测试。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [162] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 系统综述Video-LMM的后训练技术（SFT、RL、TTS），分析视频特有问题并提供评估资源与未来方向


<details>
  <summary>Details</summary>
Motivation: 提供视频大模型后训练方法的全面框架，厘清SFT/RL/TTS等技术在视频理解中的适配与挑战

Method: Survey

Result: 构建分类体系，总结代表性方法、设计原则、评估协议，整理基准与数据集，指出奖励设计、可扩展性与成本-性能的开放问题

Conclusion: 本综述提供统一的理论与实践框架，促进Video-LMM从感知向推理能力的跃迁，呼吁在奖励、扩展性和高效部署上加强研究

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [163] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 利用3D基础模型的空间先验，提出针对极端视角变化的分割匹配方法，在公开数据集上显著优于现有方法，并带来下游任务的实用提升。


<details>
  <summary>Details</summary>
Motivation: 传统的关键点匹配在处理遮挡、光照与大视角变化时表现有限，而分割匹配能捕捉更大范围结构化区域。利用已训练的3D基础模型的空间知识，有望在宽基线匹配任务中获得更强的鲁棒性。

Method: 设计了基于3D基础模型空间理解的匹配网络，利用模型的几何与语义先验对图像分割区域进行表征和配对；训练与评估在ScanNet++和Replica数据集上进行，并与SAM2视频传播器及局部特征匹配方法对比。

Result: 在AUPRC指标上，相较于最先进方法（包括SAM2视频传播器和局部特征匹配），在ScanNet++与Replica数据集上最高提升约30%；此外在3D实例分割和图像目标导航等下游任务中也展示了性能增益。

Conclusion: 本文提出利用3D基础模型的空间理解能力，解决极端视角变化下的分割匹配问题，通过构建具有3D诱导偏差的匹配架构，在宽基线（最多180°视角差）场景中显著提升了分割区域对应的鲁棒性与准确性。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [164] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 通过训练分类器选择合适的对比度增强算法生成伪参考，将无参考评估转为全参考评估，在三数据库上显示出对比度失真图像质量评估的良好效果。


<details>
  <summary>Details</summary>
Motivation: 传统IQA方法多关注模糊与噪声等失真，而对比度失真视觉特性不同且被忽视，且无参考场景下缺乏可靠质量度量，故通过伪参考生成将NR问题转为FR以提升评估性能。

Method: 构建一组对比度增强算法生成伪参考图像；制作大规模对比度增强图像数据集；训练分类网络根据图像内容和失真类型选择最合适的增强算法以生成伪参考图；最后以全参考方式评估伪参考与失真图像之间的质量差异。

Result: 在三个包含对比度失真的数据库（CCID2014、TID2013、CSIQ）上进行评估，结果显示所提出方法具有较好性能和竞争力。

Conclusion: 提出了一种用于对比度失真图像的无参考图像质量评估（NR-IQA）方法，通过生成伪参考图像并转化为全参考评估来提高准确性。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [165] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 融合ResNet-50、ViT與FAISS的神經可塑性模組化分類器，通過動態擴展模塊提升在垃圾分類與工業缺陷檢測任務上的性能與適應性。


<details>
  <summary>Details</summary>
Motivation: 在垃圾分類與工業表面缺陷檢測領域，環境和資料分佈動態變化且樣本不均衡，需一種既能捕捉局部細節又能獲取全局語義、並具備記憶與線上適應能力的模型。

Method: Neuroplastic Modular Classifier: hybrid ResNet-50 + ViT + FAISS + dynamic modular growth

Result: 提出將ResNet-50作為局部特徵提取器、ViT獲取全局語義、FAISS實現檢索型記憶支持，並設計可擴展的神經可塑性模塊塊，在訓練中於性能停滯時動態增長。實驗顯示在垃圾分類與KolektorSDD2缺陷檢測上，相較於靜態模型有較高的準確率與適應性。

Conclusion: 提出的模型提供一種可擴展且更具適應性的圖像分類方案，在環境與工業應用中展現優勢，特別在面對動態資料分佈與複雜樣本時能持續學習與改善。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [166] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出从数据、模型、训练到评估的结构化视觉生成系统：1.3M数据集、统一VLM+FLUX模型、三阶段训练与推理时外部推理、以及StructBench/StructScore评估；结果显示现有模型仍不理想，所提方法在编辑上效果好


<details>
  <summary>Details</summary>
Motivation: 该领域现有视觉生成模型在结构化视觉（图表、图解、数学图形）上表现欠佳，需要组合规划、文本渲染和多模态推理以保证事实准确性

Method: 从摘要看方法

Result: 构建了1.3M对高质量结构化图像数据，带有链式思考注释；训练了结合VLM和FLUX.Kontext的统一模型，使用轻量连接器；三阶段训练课程；推理时使用外部推理器；提出StructBench基准和StructScore评估方式；模型在编辑任务上表现强，推理增强带来一致收益

Conclusion: 论文通过释放数据、模型和基准，推动结构化视觉的统一多模态基础研究与评估

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [167] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 通过CCE和CCA，作者实现了不同世界角色之间的自然视频交互，同时减少风格混淆，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 目标是生成在不同世界（如真人与卡通）之间自然互动的视频，解决角色从未共存导致的交互不连贯和风格混淆（style delusion）问题。

Method: 提出Cross-Character Embedding (CCE)学习多模态下的角色身份与行为逻辑，并引入Cross-Character Augmentation (CCA)通过合成共存与混合风格的数据增强训练集。

Result: 在包含10个角色的卡通与真人剧集基准上，方法在身份保持、交互质量和抗风格混淆方面均有明显提升，支持新的生成叙事形式。

Conclusion: 该论文提出了一种用于跨角色交互的文本到视频生成框架，旨在在不同风格世界中保持角色身份与行为一致性，同时实现自然交互。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [168] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain在推理时利用大多模态模型生成关键帧并对预训练视频生成器进行稀疏微调，低成本显著改善了复杂场景下的视频连贯性与因果建模。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在连贯建模复杂动态和因果后果链方面能力有限，而大语言/多模态模型在视觉状态推理和未来预测上表现出色，故将两者结合以弥补短板。

Method: 构建一个推理阶段的chain-of-visual-thought框架：先用大多模态模型生成稀疏关键帧作为视觉状态快照，再在这些关键帧处对预训练视频生成器进行稀疏的推理时微调，以关键帧指导生成过程。

Result: 在复杂多步情境的广泛实验中，VChain明显提升了生成视频的质量，且方法调优高效、开销小、避免了密集监督。

Conclusion: VChain通过在推理时注入多模态模型的视觉推理信号，有效提升了视频生成模型在复杂多步场景中对连锁视觉因果关系的建模能力。

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [169] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker provides dataset, evaluation, and a multi-agent system to automatically generate faithful and informative academic presentation videos from papers, improving over prior baselines


<details>
  <summary>Details</summary>
Motivation: Automate and scale creation of academic presentation videos from research papers to reduce labor and ensure faithfulness to paper content; handle multi-modal dense inputs and coordinate slides, subtitles, speech, and talking head

Method: multi-agent academic presentation video generation framework; dataset construction; evaluation metrics design; experiments on Paper2Video benchmark

Result: Released PaperTalker dataset of 101 papers with videos/slides/speaker metadata; proposed four evaluation metrics and a multi-agent system integrating slide generation, layout refinement via tree search visual choice, cursor grounding, subtitling, TTS, talking-head rendering; demonstrated improved faithfulness and informativeness over baselines on Paper2Video benchmark

Conclusion: PaperTalker is a practical step toward automated academic video generation by combining tailored dataset, metrics, and a multi-agent generation pipeline; resources released publicly for future research

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [170] [Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads](https://arxiv.org/abs/2510.03386)
*Zixuan Yi,Sami Abu-el-Haija,Yawen Wang,Teja Vemparala,Yannis Chronis,Yu Gan,Michael Burrows,Carsten Binnig,Bryan Perozzi,Ryan Marcus,Fatma Ozcan*

Main category: cs.DB

TL;DR: 针对重复子查询模式在线学习大量局部简单回归器，低开销提升基数估计精度并在PostgreSQL中显著加速查询，兼顾效果与可运维性。


<details>
  <summary>Details</summary>
Motivation: 传统数据库依赖启发式基数估计，随着查询复杂度增加误差显著扩大；尽管学习型估计器能提高精度，但其部署和运维开销阻碍了实际采用。由于查询中存在大量重复子查询模式，利用这些重复性可在低成本下提升估计质量。

Method: 识别查询工作负载中的高重复子查询模式；针对每个模式在线训练一个简单回归器（局部化模型）；使用子查询图结构的哈希作为索引实现随机访问回归器；将回归器嵌入执行时，开销微小，训练和推理在查询执行过程中并行进行。

Result: 在合成JOB-lite与IMDb工作负载的模拟实验中，方法在误差上接近最先进学习方法；将该方法整合进PostgreSQL后，运行时间相比传统方法加速7.5分钟（>30%），仅花费37秒的在线学习开销；操作成本远低于其他学习型估计器。

Conclusion: 本文提出一种在线学习多简单回归器的方法，通过将回归器与查询子图模式绑定并用图结构哈希随机访问，实现低开销的基数估计改进。该方法在误差指标上可与最先进学习方法竞争，并在PostgreSQL中显著提升准确性与查询运行时，同时将操作成本大幅降低，适合实际部署。

Abstract: DB engines produce efficient query execution plans by relying on cost models.
Practical implementations estimate cardinality of queries using heuristics,
with magic numbers tuned to improve average performance on benchmarks.
Empirically, estimation error significantly grows with query complexity.
Alternatively, learning-based estimators offer improved accuracy, but add
operational complexity preventing their adoption in-practice. Recognizing that
query workloads contain highly repetitive subquery patterns, we learn many
simple regressors online, each localized to a pattern. The regressor
corresponding to a pattern can be randomly-accessed using hash of graph
structure of the subquery. Our method has negligible overhead and competes with
SoTA learning-based approaches on error metrics. Further, amending PostgreSQL
with our method achieves notable accuracy and runtime improvements over
traditional methods and drastically reduces operational costs compared to other
learned cardinality estimators, thereby offering the most practical and
efficient solution on the Pareto frontier. Concretely, simulating JOB-lite
workload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only
37 seconds overhead for online learning.

</details>


### [171] [Dual Pruning and Sorting-Free Overestimation for Average-Utility Sequential Pattern Mining](https://arxiv.org/abs/2510.04014)
*Kai Cao,Yucong Duan,Wensheng Gan*

Main category: cs.DB

TL;DR: HAUSP-PG通过前缀与剩余序列的独立处理实现双重剪枝，并设计无需排序的平均效用上界，提升HAUSPM在长序列应用中的效率与资源利用。


<details>
  <summary>Details</summary>
Motivation: 解决高平均效用序列模式挖掘(HAUSPM)在长序列场景下效率低、剪枝不足和内存/时间开销大的问题，提供更公平的模式度量。

Method: 设计两套互补策略分别处理模式前缀和剩余序列以实现双向剪枝；提出无需排序的平均效用上界计算方法以节省计算与内存；在多数据集上进行实验评估。

Result: 提出HAUSP-PG算法：对模式前缀和剩余序列独立处理实现双重剪枝；无需对项排序即可计算平均效用上界，降低时空开销；在真实与合成数据集上验证了算法的满意性能。

Conclusion: HAUSP-PG在效率和资源消耗上较现有方法有明显改进，适合长序列场景（如网络安全、AI应用）中的高平均效用序列模式挖掘。

Abstract: In a quantitative sequential database, numerous efficient algorithms have
been developed for high-utility sequential pattern mining (HUSPM). HUSPM
establishes a relationship between frequency and significance in the real world
and reflects more crucial information than frequent pattern mining. However,
high average-utility sequential pattern mining (HAUSPM) is deemed fairer and
more valuable than HUSPM. It provides a reasonable measure for longer patterns
by considering their length. In contrast to scenarios in retail business
analysis, some pattern mining applications, such as cybersecurity or artificial
intelligence (AI), often involve much longer sequences. Consequently, pruning
strategies can exert a more pronounced impact on efficiency. This paper
proposes a novel algorithm named HAUSP-PG, which adopts two complementary
strategies to independently process pattern prefixes and remaining sequences,
thereby achieving a dual pruning effect. Additionally, the proposed method
calculates average utility upper bounds without requiring item sorting,
significantly reducing computational time and memory consumption compared to
alternative approaches. Through experiments conducted on both real-life and
synthetic datasets, we demonstrate that the proposed algorithm could achieve
satisfactory performance.

</details>


### [172] [Ambidextrous Degree Sequence Bounds for Pessimistic Cardinality Estimation](https://arxiv.org/abs/2510.04249)
*Yu-Ting Lin,Hsin-Po Wang*

Main category: cs.DB

TL;DR: 本文提出一种改进的悲观基数估计框架：在以信息熵上界连接基数的dexterous方法中，引入“ambidextrous”约束，改为计数“claw pairs”而非单一“claws”，从而得到更紧的上界。理论上新的约束不弱于旧约束，实证在数据集上显著降低过估计量级（如从x到x^{3/4}的改善），示例中将估计从1.2e9降到5.1e8，更接近真实1.8e7。


<details>
  <summary>Details</summary>
Motivation: 数据库连接基数估计在查询优化中至关重要。已有dexterous框架通过信息熵和图的p-范数给出悲观（上界）估计，但在某些稀疏/结构化图中对claw计数导致严重过估，作者提出更精细的结构计数以减小过估。

Method: 在信息论框架下，将连接行的熵H(X_1,...,X_n)用Shannon型不等式上界（如分解为边条件熵），并在对单项项H(X_i)+p H(X_j|X_i)的上界阶段，用基于图的度序列的p-范数估计原来计数"claws"，而新方法改为计数"claw pairs"（ambidextrous结构），并据此导出更紧的p-范数基上界。

Result: 理论上证明ambidextrous约束不劣于旧约束，并在实证（如com-Youtube数据集的friend triples计数）中显著降低上界，从1.2e9降至5.1e8，理论估计过度仍存在但量级更小（从x到x^{3/4}）。

Conclusion: 引入ambidextrous（计数claw pairs）约束能严格改进或保持不劣于现有dexterous上界，在理论与实证上都给出更紧的悲观连接基数上界。

Abstract: In a large database system, upper-bounding the cardinality of a join query is
a crucial task called $\textit{pessimistic cardinality estimation}$. Recently,
Abo Khamis, Nakos, Olteanu, and Suciu unified related works into the following
dexterous framework. Step 1: Let $(X_1, \dotsc, X_n)$ be a random row of the
join, equating $H(X_1, \dotsc, X_n)$ to the log of the join cardinality. Step
2: Upper-bound $H(X_1, \dotsc, X_n)$ using Shannon-type inequalities such as
$H(X, Y, Z) \le H(X) + H(Y|X) + H(Z|Y)$. Step 3: Upper-bound $H(X_i) + p H(X_j
| X_i)$ using the $p$-norm of the degree sequence of the underlying graph of a
relation.
  While old bound in step 3 count "claws $\in$" in the underlying graph, we
proposed $\textit{ambidextrous}$ bounds that count "claw pairs
${\ni}\!{-}\!{\in}$". The new bounds are provably not looser and empirically
tighter: they overestimate by $x^{3/4}$ times when the old bounds overestimate
by $x$ times. An example is counting friend triples in the
$\texttt{com-Youtube}$ dataset, the best dexterous bound is $1.2 \cdot 10^9$,
the best ambidextrous bound is $5.1 \cdot 10^8$, and the actual cardinality is
$1.8 \cdot 10^7$.

</details>
