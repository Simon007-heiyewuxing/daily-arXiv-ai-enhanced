<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 提出一種結合Transformer增強的場景圖生成模型與多種AI技術，自動從P&ID（pdf）抽取設計資料與工廠拓撲，達到高準確率


<details>
  <summary>Details</summary>
Motivation: 自動化對天然氣廠的文檔與P&ID圖紙的資訊抽取，以加速數位化流程並減輕MGM用戶的日常負擔

Method: Scene Graph Generation with Transformer-enhanced relational reasoning

Result: 使用OCR、Vision LLM、物體偵測、關係推理與優化演算法，文字設計資料抽取準確率91%，元件識別率93%，階層結構抽取約80%準確率

Conclusion: 結合多種AI技術及改良的Transformer模型，可有效應對資料異質性並高精度完成天然氣設施的數位化資訊抽取，提升效率並減少人工負擔。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [2] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 提出模型无关的评估框架与新指标，系统检测inpainting-based talking face中的唇部泄露问题，并分析参考图像选择的影响，为未来基准提供工具。


<details>
  <summary>Details</summary>
Motivation: 当前inpainting-based方法为保持视频细节通常使用身份参考图像，但该机制会引入lip leaking——参考图像影响生成唇形，而非仅由驱动音频决定；而现有指标和测试设置难以检测该问题。

Method: 设计三种互补测试设置：静音输入生成（检测模型是否在无声情况下仍生成唇动）、不匹配音视频对（评估参考图像对唇动的影响）、匹配音视频合成（用于对比与基线）；并提出派生指标如lip-sync discrepancy和基于静音音频的lip-sync分数，同时研究不同身份参考图像选择对leakage的影响。

Result: 提出的评估方法能够系统地揭示和量化lip leaking现象，衍生指标提供了更敏感的检测手段，且展示了不同参考图像选择会显著影响leakage，为参考设计提供指导。

Conclusion: 本文提出了一个系统化评估框架，用于检测和量化inpainting-based talking face生成中的lip leaking问题，强调模型无关性并为未来研究提供可靠基准。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [3] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: 提出MATRIX数据集和一个针对动态无人机群的实时多视角检测与跟踪框架，在复杂遮挡和动态摄像机下仍表现稳健并展示迁移学习与容错性。


<details>
  <summary>Details</summary>
Motivation: 提升多无人机系统在复杂城市环境下对行人跟踪的覆盖性和鲁棒性，解决现有方法在动态摄像机位置和遮挡下性能下降的问题。

Method: 实时相机标定、基于关键点的图像配准、在鸟瞰图(BEV)中进行特征融合，以及多视角检测与关联策略；并通过迁移学习与相机掉线实验验证稳健性。

Result: 构建了MATRIX数据集（8架无人机同步飞行、连续变化位置、包含40名行人与建筑遮挡），并提出了一个实时相机标定、基于特征的图像配准与BEV多视角特征融合的深度学习跟踪框架，在复杂环境下仍能维持约90%检测和跟踪准确率并跟踪约80%轨迹。

Conclusion: MATRIX数据集与所提框架为动态多视角监控研究提供了新的基准：在复杂城市遮挡与摄像机故障场景下，实时相机校准与BEV特征融合能显著提高检测与跟踪鲁棒性，且预训练模型有良好泛化性。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [4] [Learning Topology-Driven Multi-Subspace Fusion for Grassmannian Deep Network](https://arxiv.org/abs/2511.08628)
*Xuan Yu,Tianyang Xu*

Main category: cs.CV

TL;DR: 提出拓扑驱动的Grassmannian多子空间融合框架，通过自适应子空间选择与Fréchet均值交互提高非欧几里得表示的判别性与鲁棒性，理论保证收敛并在多个数据集上领先。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用静态单子空间表示，忽视多子空间间动态交互，难以捕捉复杂几何结构。提出自适应多子空间协作以增强表示能力。

Method: Topology-driven multi-subspace fusion on Grassmannian

Result: 提出基于Kolmogorov-Arnold定理的自适应多子空间建模与Fréchet均值优化的多子空间交互块；证明在投影度量拓扑下自适应子空间收敛并适配梯度优化；集成Riemannian BN与互信息正则化并在多个任务上达SOTA。

Conclusion: 工作将多通道交互思想推广到非欧几里得域，实现稳定可优化的多子空间协作，提升判别性与可解释性，并在动作识别、EEG和图任务上取得优异表现。

Abstract: Grassmannian manifold offers a powerful carrier for geometric representation learning by modelling high-dimensional data as low-dimensional subspaces. However, existing approaches predominantly rely on static single-subspace representations, neglecting the dynamic interplay between multiple subspaces critical for capturing complex geometric structures. To address this limitation, we propose a topology-driven multi-subspace fusion framework that enables adaptive subspace collaboration on the Grassmannian. Our solution introduces two key innovations: (1) Inspired by the Kolmogorov-Arnold representation theorem, an adaptive multi-subspace modelling mechanism is proposed that dynamically selects and weights task-relevant subspaces via topological convergence analysis, and (2) a multi-subspace interaction block that fuses heterogeneous geometric representations through Fréchet mean optimisation on the manifold. Theoretically, we establish the convergence guarantees of adaptive subspaces under a projection metric topology, ensuring stable gradient-based optimisation. Practically, we integrate Riemannian batch normalisation and mutual information regularisation to enhance discriminability and robustness. Extensive experiments on 3D action recognition (HDM05, FPHA), EEG classification (MAMEM-SSVEPII), and graph tasks demonstrate state-of-the-art performance. Our work not only advances geometric deep learning but also successfully adapts the proven multi-channel interaction philosophy of Euclidean networks to non-Euclidean domains, achieving superior discriminability and interpretability.

</details>


### [5] [Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising](https://arxiv.org/abs/2511.08633)
*Assaf Singer,Noam Rotstein,Amir Mann,Ron Kimmel,Or Litany*

Main category: cs.CV

TL;DR: TTM利用用户友好的粗略动画作为运动提示，结合图像条件与区域依赖的双时钟去噪，无需额外训练即可实现高质量、可控的图像到视频生成


<details>
  <summary>Details</summary>
Motivation: 解决图像/文本条件在扩散视频生成中对精确运动控制的不足，以及减少基于模型微调的高成本和限制性

Method: 使用粗略的参考动画（如剪切拖拽或基于深度的重投影）作为运动线索，采用类似SDEdit的策略在视频采样过程中通过区域依赖的双时钟去噪保持运动区强对齐、其他区域灵活，同时通过图像条件保持外观一致性

Result: 提出Time-to-Move (TTM)，一个无需训练、即插即用的框架，通过粗略参考动画和双时钟去噪在图像到视频扩散模型中实现精确的运动与外观控制，兼容任意主干并在现实性与运动控制上匹配或超越训练型基线

Conclusion: TTM在不增加训练或运行时成本的前提下提供精确运动与像素级外观控制，兼容各种扩散骨干，并在对象与相机运动基准上达到或超越已有训练方法

Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.

</details>


### [6] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 提出统一共享的固定大小coreset用于持续异常检测，训练阶段增量更新嵌入，推理用最近邻匹配，性能优于基线并在多个数据集上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入式CAD方法需为每个任务构建类/任务专属子内存，导致内存碎片化、扩展性差和管理复杂。作者希望设计一个共享内存且固定大小的增量更新策略，使模型能在序列任务中持续学习正常模式而不遗忘。

Method: 在训练阶段维护一个统一且固定大小的coreset，基于新任务的数据增量更新该coreset中的样本嵌入；推理阶段采用最近邻匹配机制计算异常分数。系统不需要任务特定的子内存，从而简化内存管理并节省存储开销。

Result: 在MVTec AD和Visa数据集上进行了全面实验，分别取得平均图像级AUROC 0.972和0.891；在一个真实电子纸数据集上实现对异常样本检出100%准确率，显示在实际场景中的鲁棒性。作者表示将开源实现。

Conclusion: 本论文提出了一个共享内存库的持续异常检测（CAD）框架，通过在固定大小的核集（coreset）中增量更新嵌入实现连续知识获取，避免了以往方法为每个任务构建任务专属子内存导致的灵活性和可扩展性问题。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [7] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 该文提出一个将扩散去噪与时间感知的actor-critic决策结合的统一框架，用于提前预测交通事故，提高在降质传感输入下的鲁棒性并平衡提前报警与误报率。


<details>
  <summary>Details</summary>
Motivation: 解决实际自主驾驶中传感器输入噪声/退化导致的鲁棒性问题，以及需要在提前预警和误报抑制之间作出权衡的时机决策问题。

Method: 框架包含两部分：1) 扩散模块对图像与物体特征进行迭代去噪与重建，从而保留运动和交互信息；2) 时间感知的actor-critic模型用长时序推理与时间加权奖励学习何时发出警报，实现早期检测与可靠性之间的权衡。

Result: 在三数据集上取得SOTA准确率，并在mean time-to-accident上有显著提升；在加入高斯与脉冲噪声的情况下性能下降小，定性结果显示预测更早且稳定。

Conclusion: 方法在DAD、CCD、A3D三数据集上实现了最先进的准确率并显著提升平均提前事故时间（mean time-to-accident），在高斯噪声与脉冲噪声下仍保持稳健，同时在定性分析中展示了更早、更稳定且符合人类判断的预测。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [8] [RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation](https://arxiv.org/abs/2511.08651)
*Hae-Won Jo,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: RS-Net通过空间/时间上下文评分对象对重要性，增强三元组评分，能插拔到现有模型，提升Recall/Precision并改善长尾关系识别。


<details>
  <summary>Details</summary>
Motivation: 现有DSGG方法仅在有标注的对象对上训练，缺乏对非相关对象对的指导，难以在推理时区分有意义的关系。通过对对象对的上下文重要性打分，引入空间交互和长程时间上下文以增强关系识别。

Method: Relation Scoring Network (RS-Net)

Result: 提出了模块化的RS-Net，包含可学习的空间上下文编码器和聚合视频级信息的时间编码器，将关系分数整合入三元组评分机制。可无缝集成到现有DSGG模型中。在Action Genome数据集上提升了Recall和Precision，显著提高mean Recall，对长尾关系分布有效，并在参数增加的同时保持竞争性效率，超越SOTA方法。

Conclusion: RS-Net有效利用空间交互和长程时间信息为对象对打分，提升DSGG模型在关系预测上的表现，尤其在长尾类别上有明显改进，同时保持效率。

Abstract: Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.

</details>


### [9] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出AAM在潜在空间匿名化视频特征，通过三种训练目标在冻结编码器上可插拔训练，显著降低隐私泄露约35%且保持近乎基线的下游任务性能，同时有助于减轻性别偏差。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要在像素级做匿名化，需对整个视频模型重新训练且往往是任务专用的，无法适配大规模、冻结的视频基础模型。作者希望在不破坏通用视频特征效用的情况下有效去除敏感个人信息，并降低计算开销。

Method: 通过设计AAM并在冻结的视频编码器特征上训练，使用三种损失：1）片段级自监督隐私损失以减少静态片段间的互信息；2）协同训练目标以保留在见过任务上的效用；3）潜在一致性损失以增强对未见任务的泛化。AAM为小参数开销、即插即用的模块，避免了重训练整个模型和重提特征的昂贵开销。

Result: 在多项下游任务上，方法实现了约35%的隐私泄露降低，同时保持接近基线的效用表现（动作识别：Kinetics400, UCF101, HMDB51；时序动作检测：THUMOS14；异常检测：UCF-Crime）。还对敏感时序属性识别做了分析，并提出评估动作识别中性别偏差的新协议，结果显示方法能缓解性别偏差并促进更公平的模型。

Conclusion: 该论文提出了一种在潜伏空间（latent space）进行视频隐私保护的新范式，通过在冻结的视频编码器上插入轻量级的匿名化适配模块（AAM），在不影响下游任务效用的前提下移除敏感信息。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [10] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 本文研究了自回归逐像素预测在视觉模型中的扩展性，使用不同算力预算训练Transformer并评估预测目标、分类准确率和生成质量。发现最优扩展策略与任务强相关：在固定低分辨率下，分类和生成的最优扩展不同；提高分辨率时模型规模需比数据增长更快。推断主要瓶颈是算力而非数据，预测在未来五年内逐像素建模可行。


<details>
  <summary>Details</summary>
Motivation: 探究自回归逐像素预测作为统一视觉模型框架的扩展规律及其在分类与生成任务上的差异，以指导未来模型和数据规模配置。

Method: 在32x32图像上，按IsoFlops曲线训练一系列Transformer模型，算力预算覆盖到7e19 FLOPs，评估三种目标：下一个像素预测损失、ImageNet分类准确率、以及使用Fréchet距离的生成质量，并分析不同预算下模型/数据/算力的最优分配。

Result: 发现任务对最优扩展策略有显著影响；生成任务要求数据规模增长比分类快3-5倍；随着分辨率提高，模型规模需比数据增长更快；主要瓶颈是算力，预计未来五年逐像素建模可行。

Conclusion: 最优扩展策略依赖任务与分辨率，生成任务需要更快增长的数据量，而随分辨率提高模型规模需更快扩展。总体瓶颈是算力而非数据，逐像素建模在未来五年有望可行。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [11] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 本文探讨通过对Stable Diffusion进行微调（如LoRA、DreamBooth）以生成更能代表各训练组的图像，进一步按组平衡数据用于预训练并在真实数据上微调，从而缓解分类器中的群体偏差。


<details>
  <summary>Details</summary>
Motivation: 训练数据中群体不均衡导致分类器继承社会偏见；现有用Stable Diffusion生成平衡数据的方法未能很好保留原数据分布，故通过微调扩散模型以更好拟合各组样本。

Method: 对每个群体样本使用多种扩散微调方法：LoRA（低秩适配）与DreamBooth。为应对群体内高度变异，先对每组内图像进行聚类，再对每个簇训练独立DreamBooth模型。使用这些模型生成组内平衡的合成数据，用于模型预训练，随后在真实数据上微调并评估。

Result: 在多个基准数据集上，微调策略整体优于未微调的Stable Diffusion，达到或接近Group-DRO等SOTA去偏方法，并且在偏差强烈的场景中表现更佳。

Conclusion: 微调过的扩散模型（LoRA、DreamBooth及按簇训练的DreamBooth）比原始Stable Diffusion更能生成代表性组图像，生成的数据用于预训练可在多个基准上提升公平性，平均性能接近SOTA方法（如Group-DRO），在偏差严重时优于它们。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [12] [WiCV at CVPR 2025: The Women in Computer Vision Workshop](https://arxiv.org/abs/2511.08748)
*Estefania Talavera,Deblina Bhattacharjee,Himangi Mittal,Mengwei Ren,Karen Sanchez,Carla Muntean,JungEun Kim,Mona Jalal*

Main category: cs.CV

TL;DR: WiCV@CVPR2025第16届通过论文与海报展示、导师配对及资助支持，继续增强女性与代表性不足群体在计算机视觉领域的可见性和职业发展。


<details>
  <summary>Details</summary>
Motivation: 记录并评估WiCV对提高女性及代表性不足群体在计算机视觉社区可见性、包容性和职业发展影响，供未来届和类似倡议参考与改进。

Method: 通过统计与分析会议数据（投稿与录取率、口头/海报报告分配、导师配对情况、现场参与人数及资助金额），并与历届WiCV数据做对比，评估活动影响与发展趋势。

Result: 2025年第16届WiCV接收14篇长文（32投稿，接受率43.8%），其中5篇作口头报告；另有36篇短文海报。导师计划促成80名受导师与37名导师配对；现场参会者超过100人；获得约4.4万美元资助。整体显示活动规模与影响力稳步增长。

Conclusion: WiCV@CVPR 2025 在推动计算机视觉领域的多样性与包容性方面继续发挥重要作用，具有稳健的参与度、有效的导师计划和可观的资助支持，为未来活动提供可借鉴的经验。

Abstract: The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.

</details>


### [13] [Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation](https://arxiv.org/abs/2511.08809)
*Abu Taib Mohammed Shahjahan,A. Ben Hamza*

Main category: cs.CV

TL;DR: 提出PoseKAN：将可学习边函数的Kolmogorov-Arnold Network引入骨架图，用多跳聚合、残差块和全局响应归一化提升2D-3D姿态恢复的表达力与长程依赖建模，实验证明性能优越。


<details>
  <summary>Details</summary>
Motivation: GCN局限在局部感受野和频谱偏差，难以建模远程依赖、遮挡和深度模糊以及高频细节，需一种更自适应、有更强表达能力的图学习方法。

Method: 在骨架图上替换传统GCN为可学习的边变换函数（KAN），使用多跳特征聚合聚合远邻信息，堆叠残差PoseKAN块以深化网络并引入全局响应归一化以增强特征对比和选择性。

Result: 在标准基准数据集上进行大量实验，PoseKAN在多项指标上达到与最新方法竞争的性能，展示了更强的高频细节建模和对远程依赖的捕捉能力。

Conclusion: PoseKAN提出将Kolmogorov-Arnold Network扩展到图结构用于2D到3D人体姿态提升，通过可学习的边函数、多跳聚合、残差块和全局响应归一化提高表达能力和空间感知，克服GCN局部感受野和频谱偏差的限制。

Abstract: Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.

</details>


### [14] [SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph](https://arxiv.org/abs/2511.08810)
*Jingjie He,Weijie Liang,Zihan Shan,Matthew Caesar*

Main category: cs.CV

TL;DR: 该工作提出SIFT-Graph防御框架，通过结合SIFT关键点与图注意力网络(GAT)，将结构稳健的局部特征与传统视觉模型（如ViT、CNN）融合，以提高对梯度白盒对抗攻击的鲁棒性，同时仅带来极小的干净样本精度下降。


<details>
  <summary>Details</summary>
Motivation: 现代视觉模型依赖像素级密集表征，这对不可感知扰动极其敏感。通过引入基于结构的、尺度与旋转不变的局部特征，可提供与像素域互补的稳健信息，从而增强对抗攻击的防御能力。

Method: 从原始图像提取SIFT关键点及其描述子，构建以关键点为节点的图结构并用图注意力网络编码节点关系，得到鲁棒的特征嵌入；再将这些嵌入与传统视觉模型（ViT/CNN）的特征进行融合（如拼接或注意力融合），联合训练或作为防御模块集成到分类器中；在白盒梯度攻击场景下评估鲁棒性和干净样本性能。

Result: 初步实验表明，在梯度基白盒攻击下，SIFT-Graph显著提高了模型鲁棒性，同时仅带来很小的干净准确率下降。

Conclusion: SIFT-Graph通过聚合尺度与旋转不变的局部结构性特征并与主流视觉模型融合，能够有效提升模型对梯度型白盒对抗攻击的防御能力，且在干净样本上只造成边际损失，显示出将手工与学习型特征结合用于对抗防御的可行性与潜力。

Abstract: Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.

</details>


### [15] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: 提出DT-NVS，一种基于transformer的3D感知扩散模型，通过自注意力改进、相机条件化和参考帧角色交换训练策略，在未对齐多类别真实视频上训练，能从单张图生成多样且优于现有方法的新视图。


<details>
  <summary>Details</summary>
Motivation: Single-view novel view synthesis for real-world everyday scenes (unaligned, multi-category) is under-explored; existing diffusion methods limited to small camera shifts or object-centric scenes. Need a 3D-aware diffusion approach trained on large-scale real videos with image-only losses.

Method: 基于transformer骨干的3D-aware扩散模型；对transformer和自注意力进行修改以将图像映射至3D表示；提出相机条件化策略以支持未对齐数据；引入参考帧交换的训练范式；在大规模真实视频数据上用图像级损失训练。

Result: DT-NVS: a transformer-based 3D-aware diffusion model with novel transformer/self-attention modifications, camera conditioning strategies, and a training paradigm swapping reference roles; outperforms state-of-the-art 3D diffusion and deterministic methods for single-image generalized novel view synthesis, producing diverse outputs.

Conclusion: DT-NVS有效扩展了扩散模型在真实日常场景单视图新视图合成的应用，通过架构与训练策略创新，在多类别、未对齐真实视频数据上实现更好和更具多样性的结果。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [16] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 提出Shadow-informed Pose Feature (SiPF) 和 RIAttnConv，利用共享学习旋转生成全局一致“shadow”参考点，解决旋转不变方法丢失全局位姿信息导致的对称结构区分失败问题，在3D分类和分割上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变方法虽然保证了任意旋转下鲁棒性，但会丢失全局位姿信息，导致在对称或相似局部几何体之间无法区分（如左右机翼），源于感受野受限引发的Wing-tip feature collapse。

Method: 1) 定义Shadow-informed Pose Feature：从学习的共享旋转中导出一个全局一致的参考点（shadow），将其与局部RI描述子结合；2) 设计RIAttnConv：基于注意力的卷积算子，利用SiPF在聚合特征时增强区分相似结构能力；3) 构建任务自适应shadow定位模块：用quaternion上的Bingham分布学习最优全局旋转以生成一致的shadow；4) 在分类与分割数据集上进行大规模实验对比。

Result: 在多个3D分类和part segmentation基准上，尤其是需要细粒度空间区分的场景，所提方法明显优于现有旋转不变方法，验证了SiPF+RIAttnConv的有效性和Bingham-based shadow定位的实用性。

Conclusion: SiPF通过为局部旋转不变描述子注入来自学习到的全局参考点的位姿信息，恢复全局位姿意识，同时保持旋转不变性。结合RIAttnConv和基于Bingham分布的动态shadow定位模块，模型在需细粒度空间区分的任务上显著优于现有RI方法。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [17] [SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation](https://arxiv.org/abs/2511.08872)
*Hu Cui,Wenqiang Hua,Renjing Huang,Shurui Jia,Tessai Hayama*

Main category: cs.CV

TL;DR: 本文提出SAS-SSM与SasMamba：通过结构感知时空卷积+步幅扫描构建多尺度结构表征，在维持线性复杂度下更好保留骨骼空间结构，实现参数更少但性能竞争力强的3D姿态估计。


<details>
  <summary>Details</summary>
Motivation: 动机是现有基于SSM的方法在将2D关键点序列展平为时间序列时采用的手工扫描操作破坏了骨骼的空间结构并混淆时空特征，导致难以捕捉复杂的姿态依赖，因此需要一种既能保留空间结构又能高效建模长程依赖的方案。

Method: 方法包括两部分：1) 结构感知时空卷积，用于动态捕获关节之间的重要局部交互；2) 基于步幅的扫描策略（stride-based scan），用于构建多尺度的全局结构表征，并与SSM结合以保持线性复杂度。整体模型SasMamba基于SAS-SSM设计，参数更少但性能优越。

Result: 本论文提出了一种名为SAS-SSM的骨骼结构感知步幅状态空间模型，并在此基础上构建了SasMamba，用于3D人体姿态估计。作者指出以往基于SSM的方法在把2D关键点时间序列展平时采用了人工设计的扫描操作，破坏了人体的空间结构并混淆了时空特征，从而难以捕捉复杂的姿态依赖关系。为此，SAS-SSM先通过结构感知的时空卷积动态捕捉关节间的局部交互，然后采用基于步幅的扫描策略构建多尺度的全局结构表征，兼顾局部与全局信息，同时保持线性计算复杂度。实验表明，基于SAS-SSM的SasMamba在参数量明显更少的情况下仍取得了有竞争力的3D姿态估计性能。

Conclusion: SAS-SSM有效缓解了传统SSM方法破坏空间结构的问题，兼顾局部与全局建模，在参数更少的情况下仍能达到竞争性的3D姿态估计性能，表明该方法在效率与表达力上具有优势。

Abstract: Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.

</details>


### [18] [Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks](https://arxiv.org/abs/2511.08883)
*Cheng Wang,Shuisheng Zhou,Fengjiao Peng,Jin Sheng,Feng Ye,Yinli Dong*

Main category: cs.CV

TL;DR: 作者设计了MFAVBs：先用两路共享权重ViT提取正样本对特征，融合后输入更大ViT，再拆分为新正样本对重复多次融合/增广，最终在实例级和聚类级计算交叉熵损失；使用CLIP特征预处理，提升聚类效果并取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习网络通过参数共享或动量更新让两编码器隐式交互，可能未充分利用正样本对的互补性与相似性；因此需要显式融合正样本对特征并进行多次增广以提取更有利于聚类的表征。

Method: 方法包括：1) 两路共享权重的ViT分别输入两种预处理增强，提取特征；2) 将两路特征融合后输入更大ViT；3) 将输出特征拆分成新的正样本对，循环进入MFAVBs，实现多次融合和增广；4) 将最终特征投影到实例级和聚类级空间，计算交叉熵损失并反向更新；5) 使用CLIP预训练特征作为输入增强区分能力。

Result: 提出了一种基于ViT的多重融合增强块（MFAVBs）的对比聚类方法，通过显式融合正样本对的特征并进行多次增广融合，提高聚类特征提取和区分相似图像的能力；在七个公开数据集上优于现有方法。

Conclusion: 通过多次显式融合正样本对特征并结合CLIP预处理，MFAVBs显著提升了对比聚类性能，具备更好的相似图像区分能力并在多个数据集上达到或超过现有最优结果。

Abstract: In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.

</details>


### [19] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 提出EfficientNet-based四步深学习方法用于6类GBM组织学区域分类，训练集表现极佳但验证/测试上泛化显著下降，提示需改进数据多样性与泛化策略。


<details>
  <summary>Details</summary>
Motivation: 通过自动、稳健且精确地识别GBM组织学切片中的不同子区域，帮助实现大规模形态学理解，辅助临床诊断与研究。

Method: 使用BraTS-Path 2024挑战的公开训练集，构建四步深度学习管线并评估EfficientNet-B0到B4多种变体。采用5折交叉验证以选取最佳模型（B1和B4），并在保留验证集与最终隐藏测试集上进行评估。

Result: 在训练集5折交叉验证中EfficientNet-B1/B4达F1=0.98；但在BraTS-Path保留验证集与隐藏测试集上，EfficientNet-B1分别只达到F1=0.546与0.517，表明训练表现与实际泛化存在大幅差距。

Conclusion: 该研究提出了基于EfficientNet系列的四步深度学习流程，用于对GBM的6类组织学区域进行分类。尽管在训练集上表现优异（交叉验证F1达0.98），但在验证集和隐藏测试集上的F1分别仅为0.546和0.517，显示出显著的泛化能力不足。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [20] [Improving VisNet for Object Recognition](https://arxiv.org/abs/2511.08897)
*Mehdi Fatan Serj,C. Alejandro Parraga,Xavier Otazu*

Main category: cs.CV

TL;DR: 将RBF、马氏距离和视网膜预处理等生物启发机制加入VisNet，结合Hebbian与时间连续学习，可显著提高对物体识别与对称性分类的性能，并增强生物学可解释性。


<details>
  <summary>Details</summary>
Motivation: 模拟人类视觉在物体识别上对变换（位置、尺度、旋转）具备的不变性和高效性，提升人工系统在多变环境下的识别能力，同时保持模型的生物可解释性。

Method: 基于VisNet的多层生物启发网络，采用Hebbian学习和时间连续性约束，通过将时间上相邻视图关联以形成变换不变表示。扩展变体包括：在某些层替换为RBF神经元，用马氏距离替代欧氏度量用于学习/匹配，以及引入类视网膜预处理（例如局部对比增强、空间滤波）以模拟早期视觉处理。实验在MNIST、CIFAR-10及自定义对称物体集上评估性能。

Result: 在多个数据集上，扩展后的VisNet变体较基线模型在识别准确率上有显著提升；对称性检测任务中也表现更好，表明RBF单元和马氏距离等机制有助于捕获类别内部分布与对称结构。

Conclusion: 该论文表明，将径向基函数（RBF）神经元、马氏距离学习和类视网膜预处理等机制引入VisNet能显著提升对物体识别和对称性分类的鲁棒性与不变性，从而增强模型的生物学相关性和可解释性。

Abstract: Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.
  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations

</details>


### [21] [Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency](https://arxiv.org/abs/2511.08901)
*Riling Wei,Kelu Yao,Chuanguang Yang,Jin Wang,Zhuoyan Gao,Chao Li*

Main category: cs.CV

TL;DR: 提出面向弱语义一致性的ACKD与SemBridge框架，通过自监督匹配和语义感知的最优传输对齐，有效降低知识传输成本，在MS与RGB不对称模态的遥感分类上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统对称跨模态知识蒸馏依赖语义强相关的配对模态，在真实场景中配对模态稀缺，需研究能在弱语义一致性下传递知识的方法。

Method: 设计SemBridge包含两个模块：Student-Friendly Matching（自监督学习动态选择相关教师样本，为每个学生样本提供个性化指导）和Semantic-aware Knowledge Alignment（基于拉格朗日优化的最优传输路径寻求）；并在MS与RGB遥感分类数据集上构建基准进行评估。

Result: 在多个数据集和6种模型架构上，与7个对比方法相比，SemBridge在遥感场景分类任务上达到SOTA性能，验证了针对弱语义一致性的有效性。

Conclusion: 提出ACKD以应对弱语义一致性下的跨模态知识蒸馏问题，通过SemBridge框架缓解知识传输成本，提高教师与学生间匹配与对齐效果，实现了在遥感MS与RGB不对称模态上的最优表现。

Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.

</details>


### [22] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 将视觉检测器输出与文本预训练LLM提供的结构先验通过逆方差加权融合，改进半监督文档版式检测。OCR+LLM推断层次区域，与教师模型输出融合生成伪标签；在轻量与预训练模型上均有稳定提升，且可用开源LLM替代云API。


<details>
  <summary>Details</summary>
Motivation: 现有半监督方法仍需大量数据或昂贵多模态预训练；文本预训练LLM具结构理解能力，可作为可用的结构先验，用于增强视觉模型的伪标签质量，从而减少标注依赖并保护隐私。

Method: 对未标注文档，先用OCR提取文本并用LLM推断层次化区域（表格、段落等）；同时用教师检测器预测实例和不确定性；通过逆方差（置信度）融合视觉与LLM的位置信息，生成伪标签训练学生模型；引入学习的实例自适应门控调整权重并用PAC边界分析收敛性；评估在PubLayNet上不同模型与LLM配置的性能与成本。

Result: The paper proposes fusing visual detector outputs with structural priors from text-pretrained LLMs via probabilistic inverse-variance weighting to improve semi-supervised document layout detection. They use an OCR-LLM pipeline to infer hierarchical regions from unlabeled documents and combine these with teacher detector outputs to produce refined pseudo-labels. Experiments show consistent gains across model scales: a lightweight SwiftFormer backbone achieves 88.2±0.3 AP with 5% labels on PubLayNet, and integrating with LayoutLMv3 reaches 89.7±0.4 AP, outperforming standard semi-supervised LayoutLMv3 and matching UDOP while avoiding large-scale multimodal pretraining. Key findings include benefits of learned instance-adaptive gating (+0.9 AP), viability of open-source LLMs like Llama-3-70B for privacy-preserving deployment with minimal performance loss, LLMs' role in semantic disambiguation (+3.8 AP in 18.7% cases), and modest system costs ($12 API or 17 GPU-hours per 50K pages).

Conclusion: 利用LLM的结构先验与视觉预测进行概率层面融合，可在低标签设置下提升文档布局检测性能，且兼顾隐私与计算成本，适配不同规模模型。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [23] [Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection](https://arxiv.org/abs/2511.08904)
*Yating Liu,Yan Lu*

Main category: cs.CV

TL;DR: CCDF通过引入循环一致性和语义一致性模块，减少生成器过拟合并增强细节重建，显著优于现有无监督遥感变化检测方法。


<details>
  <summary>Details</summary>
Motivation: 解决基于生成器的重构方法因过拟合导致的变化检测性能下降问题，通过一致性约束改善重构质量与细节保留。

Method: 在生成器重构基础上增加两致性模块：Cycle Consistency用于约束重构网络避免过拟合，Semantic Consistency用于保持语义/细节信息以便更准确地捕捉不可重构区域作为变化。

Result: 提出了一种新的未监督变化检测框架（CCDF），通过循环一致性（CC）模块缓解生成器重构过拟合问题，并引入语义一致性（SC）模块以增强细节重建，从而提高变化检测性能。

Conclusion: 结合循环一致性和语义一致性可以有效抑制生成器过拟合并提升细节恢复能力，从而提高无监督遥感变化检测精度。

Abstract: Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.

</details>


### [24] [HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing](https://arxiv.org/abs/2511.08908)
*Shuji Ono*

Main category: cs.CV

TL;DR: 基于衣物光谱特性的HitoMi-Cam能在无GPU的边缘设备上实时且鲁棒地检测人员，在形状不可预测的场景（如搜救）中优于传统CNN检测器。


<details>
  <summary>Details</summary>
Motivation: 传统CNN对人体姿态和形状敏感，导致在训练未覆盖的体态下性能下降；利用衣物的光谱特性可绕过形状依赖，提高在异常场景中的检测可靠性。

Method: 利用衣物的光谱反射率特征进行检测，设计轻量级算法在资源受限边缘设备上运行，实现实时帧率（23.2 fps，253x190）并在多场景下评估AP与误报率。

Result: HitoMi-Cam采用基于光谱反射率的衣物识别，应对CNN对形状依赖的问题。作者在无GPU的边缘设备上实现系统，实时处理23.2 fps，并在模拟搜救场景中AP=93.5%，优于比较的CNN模型（最佳53.8%）；误报率低，定位为在特定条件下补充CNN的工具。

Conclusion: 光谱感知的人体检测在边缘设备上可行且对形状变化具有较好鲁棒性，可作为CNN检测器的补充，特别适用于形状不可预期的场景如灾难搜救。

Abstract: While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.

</details>


### [25] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: 提出Negative Entity Suppression (NES)，通过合成图像一致检索、过滤检索内容中的负实体、以及用负实体在注意力层面抑制，减少文本训练下零样本图像描述的幻觉问题并提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 纯文本训练的零样本图像描述避免了配对数据成本，但在新视觉域上易产生幻觉，即生成了输入中不存在的实体，传统检索虽引入外部知识却可能把不相关实体带入，导致问题加重。

Method: 三阶段：1) 用合成图像保证训练与推理时一致的图到文本检索；2) 识别并过滤检索到的负实体（在生成中出现但不在输入图像中的对象）；3) 利用这些负实体在注意力层面进行抑制以减少模型对幻觉特征的依赖。

Result: 在多项基准上，NES在跨域泛化和幻觉率控制上优于此前方法，并取得新的最优成绩，同时域内表现保持有竞争力。

Conclusion: NES在保持域内性能的同时显著降低幻觉率并提高跨域迁移能力，在多个基准上达成ZIC新SOTA。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [26] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: 针对1B-2B参数VLM在边缘设备上的部署，SPEED-Q通过分阶段敏感度调节与蒸馏稳定训练，实现了在2-bit/4-bit权重量化下的高准确率与训练稳定性，2-bit下精度比现有方法高出最多6倍。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署VLM需要极高的内存与带宽节约，而1B-2B参数量级的VLM更适合边缘设备，但现有研究很少探索对这类模型的激进低比特量化，且视觉与语言模块对量化的敏感度不同，训练在低精度下容易不稳定，需提出专门的方法解决这些挑战。

Method: SPEED-Q包含两部分：1) 分阶段敏感度自适应机制，针对视觉（ViT）与语言（LLM）模块在量化敏感度上的显著差异，采用不同阶段和不同策略逐步量化以平衡两者性能；2) 蒸馏增强的量化策略，通过引入蒸馏损失稳定训练过程并降低对大量数据的依赖，从而提升低比特量化下的精度和鲁棒性。

Result: SPEED-Q提出了一种面向边缘设备部署的小型亿级参数视觉-语言模型(VLM)的低比特仅权重量化框架，通过分阶段敏感度自适应和蒸馏增强策略解决视觉与语言模块量化敏感性差异及数值精度下降引发的训练不稳定问题，显著提升2位/4位量化下的性能与稳定性。

Conclusion: SPEED-Q是首个专门用于将整个小型亿级参数VLM量化到低比特的框架，通过敏感度自适应与蒸馏增强，能在低数据量和低算力条件下实现准确且稳定的量化，实验证明其在多基准上优于现有方法。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [27] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出Diff-FCHM，一种以机器视觉为主导的协同压缩方法：先对机器任务进行面向机器的可变比特率压缩并逐步聚合语义特征，再利用扩散先验恢复人类感知所需的高保真细节，从而在机器视觉和人类视觉压缩上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人机协同压缩多沿用面向人类视觉的流程，导致在结合机器视觉时出现复杂度高、比特率冗余等问题；而机器视觉只需核心区域、信息量更少，故以机器为中心的压缩更高效。

Method: 提出基于机器视觉的可插拔可变比特率压缩策略，渐进聚合来自机器压缩的语义特征，并结合扩散模型先验（diffusion prior）用于细节恢复；整体框架命名为Diff-FCHM。

Result: 在多项机器视觉任务与人类视觉重建评测中，Diff-FCHM表现 consistently superior，且在比特率和复杂度上具有明显优势；作者计划在接受后开源代码。

Conclusion: Diff-FCHM实现了以机器视觉为基础的人机协同压缩，显著降低复杂性与比特率，同时在机器任务性能和人类主观质量上取得明显提升。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [28] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 该论文提出层次蒸馏（HD）框架，将轨迹蒸馏用于生成结构“草图”，并用分布蒸馏与对抗训练细化细节；引入自适应加权判别器（AWD）聚焦局部缺陷，从而实现高质量单步生成。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹蒸馏保结构但损失高频细节，分布蒸馏细节好但易模式崩溃与训练不稳，二者各有利弊。作者将二者结合以兼顾全局结构与细节保真，克服单独方法的局限。

Method: 提出Hierarchical Distillation框架：第一阶段用轨迹蒸馏生成全局结构草图，作为初始化分布；第二阶段用分布蒸馏结合改进的对抗训练（AWD判别器）对细节进行精细化。AWD根据token局部缺陷动态加权，使判别器专注难以修复区域，从而提高细节质量和训练稳定性。

Result: 在多项任务上达到SOTA：ImageNet 256×256单步FID=2.26（可与250步教师媲美），在MJHQ高分辨率文图任务中也表现出色，证明方法在不同分辨率与任务上的泛化能力。

Conclusion: 将轨迹与分布蒸馏协同组合、并引入AWD后，单步扩散模型在图像质量上显著提升，在ImageNet 256×256上单步FID达2.26，接近250步教师模型，并在MJHQ高分辨率文图生成上表现良好，证明方法通用且有效。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [29] [Boosting Adversarial Transferability via Ensemble Non-Attention](https://arxiv.org/abs/2511.08937)
*Yipeng Zou,Qin Liu,Jie Wu,Yu Peng,Guo Chen,Hui Zhou,Guanghui Ye*

Main category: cs.CV

TL;DR: 通过把不同模型的非注意力区域梯度与注意力区域梯度分离并基于元学习融合，NAMEA显著提高了跨架构对抗样本的可迁移性，ImageNet上优于AdaEA与SMER。


<details>
  <summary>Details</summary>
Motivation: 异构模型（如CNN与ViT）在注意力分布上差异显著，导致直接集成其梯度时方向差异大、方差高，从而降低迁移性。将非注意力区域的梯度纳入并与注意力区域梯度解耦融合，可补充跨架构的有用扰动信息，提高迁移攻击效果。

Method: 1) 识别与分离每个子模型的注意力区域与非注意力区域；2) 从两类区域分别提取梯度，并通过元学习机制对两类梯度进行加权或融合；3) 在迭代优化过程中同时利用融合后的梯度指导对抗扰动更新。

Result: NAMEA提出了一种新的集成对抗攻击方法，通过整合模型注意力区域与非注意力区域的梯度信息来提升跨架构（CNN与ViT）迁移性。方法利用观察到的不同架构间注意力差异，把非注意力区域的梯度解耦并与注意力区域梯度基于元学习进行融合，从而降低集成模型的梯度方差并保留个体模型优势。实验在ImageNet上显示，NAMEA相比最先进方法AdaEA和SMER平均分别提升约15.0%和9.6%。

Conclusion: 将非注意力区域梯度纳入集成攻击，并通过解耦与元学习融合策略，可有效提升跨架构对抗攻击的迁移性能，为利用模型注意力差异提供了新思路。

Abstract: Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.

</details>


### [30] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: BRHVC通过BMC和BCF协调双向参考的运动与上下文，实现更高效的NBVC，实验证明性能优于现有神经压缩方法并超越VTM-RA。


<details>
  <summary>Details</summary>
Motivation: NBVC相比P帧压缩尚未充分研究，但可利用双向参考帧提升压缩效果；然而层次化编码导致在较大帧间距层次上对两个参考帧的贡献不平衡，需优化参考信息利用来 harmonize 双向参考。

Method: 提出Bi-directional Motion Converge (BMC)用于在运动压缩阶段合并多路光流以获得更精确的运动补偿；提出Bi-directional Contextual Fusion (BCF)在运动补偿精度指导下显式建模双向参考上下文的权重，从而更有效地利用参考信息。

Result: 在HEVC数据集上，BRHVC在实验中优于之前的最先进神经视频压缩方法，甚至在随机访问配置下超越传统编码器VTM-RA。源码已开源。

Conclusion: 该论文提出了一种针对双向参考帧的神经B帧视频压缩方法BRHVC，通过协调双向参考的运动与上下文信息来提升编码性能。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [31] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: 提出通过可学习的Hausdorff维估计和基于HD的训练与推理策略，提升FGM的生成多样性，同时保持图像质量，ImageNet上多样性提升39%


<details>
  <summary>Details</summary>
Motivation: FGMs produce high-quality images but suffer low diversity due to self-similarity; HD quantifies structural complexity and can guide generation diversity enhancement

Method: Use Hausdorff Dimension to increase diversity in Fractal Generative Models

Result: Learnable HD estimator from embeddings; HD-based loss with monotonic momentum scheduling during training; HD-guided rejection sampling during inference; 39% diversity improvement on ImageNet with comparable image quality

Conclusion: 首次将Hausdorff维引入FGM，通过训练阶段的动量调度HD损失和推理阶段的HD拒绝采样，有效提升生成多样性且不降低视觉质量

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [32] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: AuthSig通过在生成签名风格嵌入中隐式编码水印位，并结合关键点驱动的数据增强，实现了对静态签名的稳健绑定与高精度提取（>98%），可抵抗数字与物理降质。


<details>
  <summary>Details</summary>
Motivation: 静态扫描签名因便捷仍被广泛使用，但缺乏认证属性，易被复制和重用；因此需要一种能在图像层面绑定认证信息且对常见降质与打印扫描鲁棒的方法。

Method: 利用生成模型（可能是风格化或条件生成网络）精细调制风格嵌入以嵌入水印比特；提出基于关键点的增强策略以扩展手写签名风格多样性，支持稳健的水印嵌入与提取。实验在数字域变换、签名特有降质和印刷-扫描场景下进行了评估。

Result: 在多个破坏场景下，AuthSig的水印提取准确率超过98%，在印刷-扫描等现实场景依然有效，表明该方法具有很高的鲁棒性与实用价值。

Conclusion: AuthSig提出了一种基于生成模型与水印结合的静态电子签名认证框架，通过在签名图像的风格嵌入中隐式编码水印位，实现“一签一用”绑定，从而增强静态签名的不可重用性与可验证性。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [33] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: 提出CoDR，通过簇剪枝构造多样化coreset并结合双重检索实现高效全局示例选择，显著提升LVLMs的ICL表现。


<details>
  <summary>Details</summary>
Motivation: The paper addresses inefficiencies and suboptimal performance in demonstration selection for in-context learning (ICL) in large visual language models. Existing methods (random, similarity-based, infoscore) struggle to balance efficiency and effectiveness; selection is NP-hard. The proposed approach aims to improve selection by constructing diverse coresets and enhancing retrieval to achieve globally effective selection efficiently.

Method: Construct diverse coreset via cluster-pruning to increase expected mutual information, then apply dual retrieval mechanism to select global demonstrations efficiently while preserving diversity.

Result: The proposed CoDR (Coreset-based Dual Retrieval) framework significantly improves ICL performance over existing strategies by selecting diverse subsets with higher expected mutual information and using cluster-pruning and dual retrieval for efficient global demonstration selection.

Conclusion: CoDR offers a robust and efficient demonstration selection framework that balances diversity and relevance, leading to better ICL outcomes in LVLMs compared to prior methods.

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [34] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: WDT-MD结合噪声扰动条件、基于修复的像素级监督与小波扩散Transformer，多方面解决扩散模型的恒等映射、误报及正常结构重建问题，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 检测糖尿病视网膜病变中最早的病理征——微动脉瘤（MA），自动化筛查以减少人工劳动并提高准确率

Method: 引入噪声编码图像条件避免恒等映射；利用修复生成伪正常样式作为像素级监督以降低假阳性；设计融合多尺度小波分析的扩散Transformer以更好重建正常视网膜特征

Result: 提出了WDT-MD框架，通过噪声编码条件、伪正常图案修复及小波扩散Transformer结构，显著提升了在IDRiD与e-ophtha MA数据集上的像素级与图像级检测性能

Conclusion: WDT-MD有效缓解了扩散异常检测在微动脉瘤筛查中的三大问题，提升了重建质量与区分能力，有望应用于早期DR筛查

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [35] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 提出在ICTM框架下融合I散度和自适应TV正则、空间自适应权重与偏置场估计的分割模型，并用RMSAV优化，在有噪声和强非匀质性图像上性能优越。


<details>
  <summary>Details</summary>
Motivation: Improve segmentation robustness for images with heavy noise and intensity inhomogeneity by integrating denoising into an ICTM-based variational model.

Method: Variational segmentation with denoising in ICTM

Result: A model combining I-divergence, adaptive TV regularizer, spatially adaptive weight, and bias-field estimation within ICTM, optimized via RMSAV; experiments show superior accuracy and robustness.

Conclusion: 该方法通过联合去噪和偏置校正显著提升了在多种噪声与强度非均匀图像上的分割效果，优化效率也得到保证。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [36] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: 本文引入负向视觉提示到开放集检测，设计统一提示编码器、NNC计算模块与NNH损失，在零样本与长尾场景显著提升性能，将负提示作为开放集识别新维度。


<details>
  <summary>Details</summary>
Motivation: 当前开放集目标检测方法仅依赖正向提示（如文本描述或视觉示例），面对视觉上相似但语义不同的干扰样本时脆弱。引入负向视觉提示可以抵消这些难负样本，提高识别鲁棒性。

Method: 构建联合处理正负提示的视觉提示编码器；在概率计算阶段引入训练自由的NNC模块动态抑制对负样本的响应；设计NNH损失在微调时强制正负嵌入之间的判别边际；支持自动或手动生成负样本并在推理时灵活使用。

Result: 提出T-Rex-Omni框架：包含统一视觉提示编码器、训练自由的Negating Negative Computing(NNC)模块和用于微调的Negating Negative Hinge(NNH)损失。支持仅正向或正负联合推理，能使用用户指定或自动生成的负样本。实验显示显著提升零样本检测性能，尤其在长尾场景（LVIS-minival上51.2 AP_r）。

Conclusion: 负向视觉提示是提高开放集视觉识别系统鲁棒性的关键，新框架T-Rex-Omni在多种设置下证明了其有效性，缩小了视觉提示与文本提示方法的差距。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [37] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: 提出Owl，一个基于因果视角的双模注意力重权框架，通过VTACR量化并基于其动态干预注意力、辅以双路径对比解码，有效抑制LVLMs对象幻觉并达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有缓解对象幻觉的方法通常独立调控视觉或文本注意力，忽视两者交互的因果影响；通过因果建模和度量两类注意力的相对贡献，能更精细地识别并抑制由文本先验主导的幻觉生成。

Method: 基于结构因果图将视觉注意力和文本注意力视为中介；提出VTACR量化解码过程中模态贡献比；设计逐层逐token的注意力干预机制基于VTACR动态重权注意力；引入双路径对比解码，一条路径强化视觉锚定预测，另一条放大幻觉使其被对比抑制。

Result: 在POPE和CHAIR基准上显著降低幻觉发生，提升了可靠性并在faithfulness指标上达成新的SOTA，同时保留了视觉-语言理解能力。

Conclusion: Owl通过建模视觉与文本注意力作为中介变量的因果图，提出VTACR度量并基于其动态调整注意力，结合双路径对比解码，有效减少LVLMs的对象幻觉，同时保持视觉-语言理解性能。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [38] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出一种稠密跨尺度、带完全空间相关模块并结合JND损失的可调精度/效率图像对齐方法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像对齐方法精度有限且计算复杂度高。通过利用跨尺度特征相关性与空间相关性，并聚焦于人类对失真敏感区域，可提升对齐精度并降低计算成本。

Method: 提出稠密跨尺度框架，计算并融合不同尺度特征间的相关性；设计可调节尺度数目的机制以控制计算量；引入完全空间相关模块计算像素级或局部窗口的空间相似度；将JND纳入损失函数，使训练更注重可觉察区域；通过定量指标（如对齐误差、运行时间）和定性可视化评估性能。

Result: 与最先进方法相比，在多个数据集上取得更低的对齐误差和更快的运行速度；定性结果显示更少的可见错配与更自然的对齐；模型允许在速度与精度间灵活权衡。

Conclusion: 本论文提出了一种稠密跨尺度图像对齐模型，通过考虑跨尺度特征间的相关性来降低对齐难度；引入可调尺度数量以在精度和效率间平衡；增加了完全空间相关模块以在低计算代价下提高精度；采用刚刚可觉差（JND）损失使模型聚焦于对失真更敏感的区域，从而消除明显对齐误差。实验表明方法优于现有最先进方法。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [39] [USF-Net: A Unified Spatiotemporal Fusion Network for Ground-Based Remote Sensing Cloud Image Sequence Extrapolation](https://arxiv.org/abs/2511.09045)
*Penghui Niu,Taotao Cai,Jiashuai She,Yajuan Zhang,Junhua Gua,Ping Zhanga,Jungong Hane,Jianxin Li*

Main category: cs.CV

TL;DR: 提出USF-Net，通过自适应大核卷积和低复杂度时间注意力模块实现高效的地基云图像序列外推，并发布新数据集ASI-CIS，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高地基云图像序列外推预测的准确性和效率，解决现有方法在动态多尺度特征提取、长期时序建模、以及注意力机制计算复杂度方面的不足。

Method: 编码器使用三层基础层提取特征；USTM模块包含SiB（带SSM的自适应多尺度空间建模）和TiB（带TAM的低复杂度时序注意力）；DSM+TGM用于统一时序指导下的时空依赖建模；解码端DUM利用初始时间状态作为注意算子抑制ghosting。

Result: 提出USF-Net：融合自适应大核卷积与低复杂度注意力机制的统一时空融合网络，在编码器-解码器框架中结合时间流信息。引入SiB+SSM、TiB+TAM、DSM+TGM及DUM模块，并发布ASI-CIS数据集。实验表明USF-Net在准确性与计算效率上显著优于现有方法。

Conclusion: USF-Net在保持计算效率的同时，能更好地建模多尺度空间上下文与长距离时间依赖，显著改善云外推质量并降低伪影（ghosting），适合光伏预测场景。

Abstract: Ground-based remote sensing cloud image sequence extrapolation is a key research area in the development of photovoltaic power systems. However, existing approaches exhibit several limitations:(1)they primarily rely on static kernels to augment feature information, lacking adaptive mechanisms to extract features at varying resolutions dynamically;(2)temporal guidance is insufficient, leading to suboptimal modeling of long-range spatiotemporal dependencies; and(3)the quadratic computational cost of attention mechanisms is often overlooked, limiting efficiency in practical deployment. To address these challenges, we propose USF-Net, a Unified Spatiotemporal Fusion Network that integrates adaptive large-kernel convolutions and a low-complexity attention mechanism, combining temporal flow information within an encoder-decoder framework. Specifically, the encoder employs three basic layers to extract features. Followed by the USTM, which comprises:(1)a SiB equipped with a SSM that dynamically captures multi-scale contextual information, and(2)a TiB featuring a TAM that effectively models long-range temporal dependencies while maintaining computational efficiency. In addition, a DSM with a TGM is introduced to enable unified modeling of temporally guided spatiotemporal dependencies. On the decoder side, a DUM is employed to address the common "ghosting effect." It utilizes the initial temporal state as an attention operator to preserve critical motion signatures. As a key contribution, we also introduce and release the ASI-CIS dataset. Extensive experiments on ASI-CIS demonstrate that USF-Net significantly outperforms state-of-the-art methods, establishing a superior balance between prediction accuracy and computational efficiency for ground-based cloud extrapolation. The dataset and source code will be available at https://github.com/she1110/ASI-CIS.

</details>


### [40] [4KDehazeFlow: Ultra-High-Definition Image Dehazing via Flow Matching](https://arxiv.org/abs/2511.09055)
*Xingchi Chen,Pu Wang,Xuerui Li,Chaopeng Li,Juxiang Zhou,Jianhou Gan,Dianjie Lu,Guijuan Zhang,Wenqi Ren,Zhuoran Zheng*

Main category: cs.CV

TL;DR: 本文提出4KDehazeFlow，一种基于流匹配与雾感知向量场的UHD图像去雾方法，通过连续向量场的渐进优化实现数据驱动的非线性色彩变换，兼容多种网络并引入可学习3D查找表与RK4求解器以提高效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于先验的方法在场景适应性上受限，深度学习方法在超高清图像上计算复杂且易产生色偏；因此需要一种既高效又能保持色彩与细节的UHD去雾方案。

Method: 将去雾建模为连续向量场的流匹配问题，设计雾感知向量场作为动力学场；引入可学习的3D LUT将雾变换参数压缩为映射矩阵以加速推理；使用四阶Runge-Kutta（RK4）ODE求解器对流场进行稳健迭代求解，减少伪影并保证数值稳定性。

Result: 在大规模实验中，4KDehazeFlow对比七种最先进方法平均提升约2dB PSNR，视觉与定量指标在浓雾和色彩保真任务上均有显著改进，且推理速度和显存开销适合4K图像处理。

Conclusion: 4KDehazeFlow在多项指标上超越七个最先进方法，平均提高约2dB PSNR，且在浓雾场景和色彩保真方面表现更佳，适用于4K及更高分辨率图像的去雾任务。

Abstract: Ultra-High-Definition (UHD) image dehazing faces challenges such as limited scene adaptability in prior-based methods and high computational complexity with color distortion in deep learning approaches. To address these issues, we propose 4KDehazeFlow, a novel method based on Flow Matching and the Haze-Aware vector field. This method models the dehazing process as a progressive optimization of continuous vector field flow, providing efficient data-driven adaptive nonlinear color transformation for high-quality dehazing. Specifically, our method has the following advantages: 1) 4KDehazeFlow is a general method compatible with various deep learning networks, without relying on any specific network architecture. 2) We propose a learnable 3D lookup table (LUT) that encodes haze transformation parameters into a compact 3D mapping matrix, enabling efficient inference through precomputed mappings. 3) We utilize a fourth-order Runge-Kutta (RK4) ordinary differential equation (ODE) solver to stably solve the dehazing flow field through an accurate step-by-step iterative method, effectively suppressing artifacts. Extensive experiments show that 4KDehazeFlow exceeds seven state-of-the-art methods. It delivers a 2dB PSNR increase and better performance in dense haze and color fidelity.

</details>


### [41] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: PAN unifies LLM-based latent autoregressive dynamics with video diffusion decoding to create a general, action-conditioned, long-horizon world model capable of realistic video simulation and reasoning.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between realistic video generation and controllable, interactive, long-horizon world models; enable action-conditioned simulation across diverse domains

Method: Generative Latent Prediction (GLP) with LLM-grounded autoregressive latent dynamics and a video diffusion decoder

Result: PAN can generate high-quality, temporally coherent videos conditioned on history and natural language actions, supporting long-horizon forecasting and simulative reasoning; outperforms prior video generators and domain-specific world models

Conclusion: PAN is a step toward general, interactive world models by combining language-grounded latent reasoning and high-fidelity video decoding, enabling open-domain action-conditioned simulation with coherent long-term dynamics.

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [42] [VietMEAgent: Culturally-Aware Few-Shot Multimodal Explanation for Vietnamese Visual Question Answering](https://arxiv.org/abs/2511.09058)
*Hai-Dang Nguyen,Minh-Anh Dang,Minh-Tan Le,Minh-Tuan Le*

Main category: cs.CV

TL;DR: 提出VietMEAgent，一种面向越南文化的多模态可解释VQA框架，结合检测、结构化程序生成与双模态解释，使用文化知识库并构建数据集，增强了对文化内容的理解与可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前VQA系统在处理具有文化特定性的内容时性能欠佳且缺乏可解释性，训练语料中文化知识不足，推理过程不透明。

Method: 结合文化物体检测骨干、结构化程序生成、基于注意力的视觉证据和文本化推理，使用越南文化实体知识库与双模态解释模块。

Result: 构建越南文化VQA数据集并在其上验证，系统在可解释性与文化敏感性方面表现良好，证明了编程式方法在文化AI中的可行性。

Conclusion: VietMEAgent有效提升越南文化内容的VQA表现，并提供可解释性输出，能支持教育与文化保护。

Abstract: Contemporary Visual Question Answering (VQA) systems remain constrained when confronted with culturally specific content, largely because cultural knowledge is under-represented in training corpora and the reasoning process is not rendered interpretable to end users. This paper introduces VietMEAgent, a multimodal explainable framework engineered for Vietnamese cultural understanding. The method integrates a cultural object detection backbone with a structured program generation layer, yielding a pipeline in which answer prediction and explanation are tightly coupled. A curated knowledge base of Vietnamese cultural entities serves as an explicit source of background information, while a dual-modality explanation module combines attention-based visual evidence with structured, human-readable textual rationales. We further construct a Vietnamese Cultural VQA dataset sourced from public repositories and use it to demonstrate the practicality of programming-based methodologies for cultural AI. The resulting system provides transparent explanations that disclose both the computational rationale and the underlying cultural context, supporting education and cultural preservation with an emphasis on interpretability and cultural sensitivity.

</details>


### [43] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: DOC通过正交梯度与动量增加反攻多样性，并用方向敏感性自适应控制强度，从而在多数据集上提高测试时对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于测试时对抗防御（如TTC）通过对抗样本梯度生成反向扰动，但因优化目标不同导致搜索空间受限，反攻易过拟合有限对抗模式，缺乏多样性和覆盖性，难以抵消广泛扰动。

Method: Directional Orthogonal Counterattack (DOC)

Result: 提出DOC，通过引入正交梯度方向和基于动量的更新来扩展反攻搜索空间并提升扰动多样性；设计基于平均余弦相似度的方向敏感性分数以自适应调节反攻强度。实验证明在16个数据集上在各种攻击下提升了对抗鲁棒性且保持了竞争性的干净精度。

Conclusion: 增强反攻多样性与覆盖可显著提升测试时防御效果，DOC为提高VLP模型的对抗鲁棒性提供了一种有效且实用的方法。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [44] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出在组合零样本学习（CZSL）中引入组合增量学习（CompIL），建立两个基准（MIT-States-CompIL 和 C-GQA-CompIL），并提出伪重放+视觉合成器与语言原语蒸馏的框架，以在不断学习新组合时保持已学组合表示与原语对齐。实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现实世界组合几乎无限且长期存在长尾分布，模型需要在不断出现新组合的情况下逐步提升组合泛化能力，传统一次性训练不能满足持续学习需求。

Method: 构建CompIL基准；提出伪重放框架：用视觉合成器合成已学组合的视觉表示作为回放样本；设计语言原语蒸馏机制以维护原语表示一致性；在增量学习设置中交替训练新组合和合成回放样本。

Result: 在MIT-States-CompIL和C-GQA-CompIL两个基准上的大量实验表明，所提框架在防止遗忘和提升未见组合识别上均优于基线方法。

Conclusion: 提出的伪重放框架和语言原语蒸馏机制能有效缓解增量学习过程中的灾难性遗忘，提升模型的组合泛化能力，在两个构建的CompIL基准上取得了显著性能改善。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [45] [Ultra-Light Test-Time Adaptation for Vision--Language Models](https://arxiv.org/abs/2511.09101)
*Byunghyun Kim*

Main category: cs.CV

TL;DR: UL-TTA: fully training-free, logit-level Bayesian test-time adaptation for VLMs that updates prototypes, priors, and temperatures online to boost accuracy and calibration under domain shift, efficiently and robustly without backbone updates.


<details>
  <summary>Details</summary>
Motivation: VLMs like CLIP suffer under domain shift (feature drift, class-prior mismatch, miscalibration), and existing TTA methods are costly or unsuitable for streaming/edge; need a lightweight, backprop-free adaptation.

Method: Freeze backbone; perform online EM-style updates of class prototypes, class priors, and temperatures using selective filtering, closed-form Bayesian updates anchored by text and Dirichlet priors, decoupled temperatures, and lightweight guards (norm clipping, prior KL constraints, smoothed temperature).

Result: Consistent improvements across large benchmarks (~726K samples), e.g., +4.7% top-1 over zero-shot CLIP and 20–30% ECE reduction, with <8% latency overhead and stable long-stream behavior up to 200K samples.

Conclusion: UL-TTA shows that logit-level, training-free Bayesian adaptation can effectively handle domain shift for VLMs, improving accuracy and calibration without backbone updates.

Abstract: Vision-Language Models (VLMs) such as CLIP achieve strong zero-shot recognition by comparing image embeddings to text-derived class prototypes. However, under domain shift, they suffer from feature drift, class-prior mismatch, and severe miscalibration. Existing test-time adaptation (TTA) methods often require backpropagation through large backbones, covariance estimation, or heavy memory/state, which is problematic for streaming and edge scenarios. We propose Ultra-Light Test-Time Adaptation (UL-TTA), a fully training-free and backprop-free framework that freezes the backbone and adapts only logit-level parameters: class prototypes, class priors, and temperature. UL-TTA performs an online EM-style procedure with (i) selective sample filtering to use only confident predictions, (ii) closed-form Bayesian updates for prototypes and priors anchored by text and Dirichlet priors, (iii) decoupled temperatures for prediction vs. calibration, and (iv) lightweight guards (norm clipping, prior KL constraints, smoothed temperature) to prevent drift in long streams. Across large-scale cross-domain and OOD benchmarks (PACS, Office-Home, DomainNet, Terra Incognita, ImageNet-R/A/V2/Sketch; ~726K test samples) and strong TTA baselines including Tent, T3A, CoTTA, SAR, Tip-Adapter, and FreeTTA, UL-TTA consistently improves top-1 accuracy (e.g., +4.7 points over zero-shot CLIP on average) while reducing ECE by 20-30%, with less than 8% latency overhead. Long-stream experiments up to 200K samples show no collapse. Our results demonstrate that logit-level Bayesian adaptation is sufficient to obtain state-of-the-art accuracy-calibration trade-offs for VLMs under domain shift, without updating any backbone parameters.

</details>


### [46] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 本文提出DKDS数据集，包含带印章与退化的古文献草书（Kuzushiji），用于文本与印章检测及文档二值化两大基准任务，并给出YOLO系列与多种二值化方法的基线结果。


<details>
  <summary>Details</summary>
Motivation: 现有Kuzushiji OCR方法在干净文献上表现良好，但未充分考虑文档退化与印章等噪声，导致实际识别性能下降，且缺乏专门数据集评测此类问题。

Method: 构建包含印章和退化的Kuzushiji文档数据集（由专家标注），定义两个任务：文本与印章检测（用YOLO多个版本做基线）和文档二值化（传统算法、K-means结合的传统算法及GAN方法作为基线）。

Result: 提供了两个任务的基线实验：YOLO系列在文本与印章检测上给出初步结果；传统、聚类增强与GAN方法在二值化任务上分别给出基线效果。数据集与代码公开。

Conclusion: DKDS填补了现有数据集中对退化与印章干扰场景的空白，为Kuzushiji识别与文档二值化提供了新的评测基准和基线实现，数据与代码已开源。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [47] [PIFF: A Physics-Informed Generative Flow Model for Real-Time Flood Depth Mapping](https://arxiv.org/abs/2511.09130)
*ChunLiang Wu,Tsunhua Yang,Hungying Chen*

Main category: cs.CV

TL;DR: 提出PIFF，将地形与简化水淹模型及时间序列降雨通过生成式网络融合，实现近实时洪深估计，替代昂贵模拟，实验证明在台南26 km区域上的182种降雨情景中性能有效。


<details>
  <summary>Details</summary>
Motivation: 传统数值模拟与航拍方法效率或可靠性不足，需一种既快速又能体现物理因果关系的洪水预测方法，支持近实时应急响应。

Method: 基于image-to-image生成网络，输入为DEM和SPM条件图，降雨时序通过Transformer编码器处理；训练时引入物理约束损失以嵌入水动力学先验，输出为洪深图像。

Result: PIFF: physics-informed flow-based generative network mapping DEM to flood depth; conditioned on simplified inundation model (SPM) and transformer rainfall encoder; tested on 26 km area, 182 scenarios.

Conclusion: 通过将流体物理先验（SPM）与数据驱动生成网络和降雨时间编码结合，PIFF能在保持准确性的同时显著提高洪深估计的实时性，适用于快速响应和洪灾评估。

Abstract: Flood mapping is crucial for assessing and mitigating flood impacts, yet traditional methods like numerical modeling and aerial photography face limitations in efficiency and reliability. To address these challenges, we propose PIFF, a physics-informed, flow-based generative neural network for near real-time flood depth estimation. Built on an image-to-image generative framework, it efficiently maps Digital Elevation Models (DEM) to flood depth predictions. The model is conditioned on a simplified inundation model (SPM) that embeds hydrodynamic priors into the training process. Additionally, a transformer-based rainfall encoder captures temporal dependencies in precipitation. Integrating physics-informed constraints with data-driven learning, PIFF captures the causal relationships between rainfall, topography, SPM, and flooding, replacing costly simulations with accurate, real-time flood maps. Using a 26 km study area in Tainan, Taiwan, with 182 rainfall scenarios ranging from 24 mm to 720 mm over 24 hours, our results demonstrate that PIFF offers an effective, data-driven alternative for flood prediction and response.

</details>


### [48] [MACEval: A Multi-Agent Continual Evaluation Network for Large Models](https://arxiv.org/abs/2511.09139)
*Zijian Chen,Yuze Sun,Yuan Tian,Wenjun Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: MACEval is an autonomous multi-agent continuous evaluation system for large models that reduces human effort, mitigates contamination risks, and offers efficient, scalable, longitudinal benchmarking across open-ended tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address shortcomings of existing benchmarks for evaluating large language models: closed-ended designs, data contamination/overfitting, heavy human curation, and poor adaptability to evolving model capabilities. It introduces a dynamic, autonomous evaluation framework to provide longitudinal, sustainable metrics.

Method: Builds a cascaded multi-agent network with role assignment for agents, on-the-fly data generation, and evaluation routing; develops new metrics for longitudinal/sustainable performance measurement; validates on 9 tasks and 23 models comparing efficiency, automation, and scalability.

Result: Introduces MACEval, a Multi-Agent Continual Evaluation network that uses role assignment, in-process data generation, and cascaded agent evaluation routing; defines new longitudinal metrics; demonstrates capability across 9 open-ended tasks and 23 models, showing human-free automation, efficiency, and scalability.

Conclusion: MACEval provides an automatic, efficient, and flexible evaluation framework reducing human work and data overhead while offering sustainable longitudinal metrics; it's effective across diverse tasks and models and can integrate existing benchmarks.

Abstract: Hundreds of benchmarks dedicated to evaluating large models from multiple perspectives have been presented over the past few years. Albeit substantial efforts, most of them remain closed-ended and are prone to overfitting due to the potential data contamination in the ever-growing training corpus of large models, thereby undermining the credibility of the evaluation. Moreover, the increasing scale and scope of current benchmarks with transient metrics, as well as the heavily human-dependent curation procedure, pose significant challenges for timely maintenance and adaptation to gauge the advancing capabilities of large models. In this paper, we introduce MACEval, a \Multi-Agent Continual Evaluation network for dynamic evaluation of large models, and define a new set of metrics to quantify performance longitudinally and sustainably. MACEval adopts an interactive and autonomous evaluation mode that employs role assignment, in-process data generation, and evaluation routing through a cascaded agent network. Extensive experiments on 9 open-ended tasks with 23 participating large models demonstrate that MACEval is (1) human-free and automatic, mitigating laborious result processing with inter-agent judgment guided; (2) efficient and economical, reducing a considerable amount of data and overhead to obtain similar results compared to related benchmarks; and (3) flexible and scalable, migrating or integrating existing benchmarks via customized evaluation topologies. We hope that MACEval can broaden future directions of large model evaluation.

</details>


### [49] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: 提出PressTrack-HMR，用tracking-by-detection分离多人体压力信号并分别进行HMR，同时构建MIP数据集，显著推进了基于压力的多人体姿态恢复研究。


<details>
  <summary>Details</summary>
Motivation: 视觉方法在真实场景中受遮挡、照明和隐私限制，地面触觉（压力）交互提供了一种无遮挡且更隐私友好的替代手段。但多人体同时在垫上行走时，压力信号会互相交织，难以分离并获得个体时间序列，阻碍了压力驱动的多人体HMR扩展。

Method: 采用tracking-by-detection策略：首先对原始压力图进行检测与分割以区分不同个体的压力信号，然后对每个个体的时间序列压力信号单独进行单人HMR恢复。并构建了多人体交互压力数据集MIP用于训练和评估。

Result: 在提出的方法上，实验显示在压力数据驱动的多人体HMR任务中取得了优秀性能：MPJPE为89.2 mm，WA-MPJPE_{100}为112.6 mm。该结果证明触觉垫在普适且保护隐私的多人体动作识别场景中具有潜力。

Conclusion: 本文提出了PressTrack-HMR，一种基于压感垫的自顶向下多人体全局人体网格恢复方法，能在无视觉信息下实现多人体姿态重建。

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [50] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++ uses an octree-based transformer to extract multi-scale local descriptors and a learnable geometric verification for re-ranking, yielding major gains in LiDAR place recognition and fast accurate 6-DoF localisation in forests and urban scenes.


<details>
  <summary>Details</summary>
Motivation: Improve LiDAR place recognition and 6-DoF localisation in challenging forest and urban scenarios by handling clutter, self-similarity, and viewpoint changes using hierarchical descriptors and efficient registration.

Method: Octree-based transformer extracts hierarchical descriptors; learnable multi-scale geometric verification for re-ranking; coarse-to-fine registration for efficient 6-DoF pose estimation, replacing RANSAC for dense clouds.

Result: HOTFLoc++ achieves SOTA performance: Recall@1 90.7% on CS-Wild-Places (+29.6%), 91.7% on Wild-Places, 96.0% on MulRan; 97.2% of registrations under 2m & 5°. Multi-scale re-ranking halves localisation errors and registration runs two orders faster than RANSAC for dense clouds.

Conclusion: Hierarchical multi-scale descriptors plus learnable re-ranking and coarse-to-fine registration produce robust, fast LiDAR localisation in challenging environments, outperforming baselines significantly.

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [51] [DBINDS -- Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: 提出DBINDS：利用扩散模型反演提取初始噪声差异序列（INDS），并结合多域多尺度特征与LightGBM分类器，在跨生成器检测任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素的检测器对未知生成器泛化能力差，转而利用隐空间动态（扩散反演的初始噪声差异）以提高跨生成器鲁棒性和在少量样本下的效果。

Method: 对视频进行扩散模型反演以恢复初始噪声序列，构造INDS并提取时域/频域/尺度等多域特征，进行特征优化后用LightGBM（贝叶斯调参）分类。

Result: DBINDS通过扩散模型反演获取视频初始噪声序列，并基于噪声序列差异进行鉴别，强调潜在空间动态而非像素级特征。

Conclusion: 通过分析扩散反演得到的初始噪声序列差异，DBINDS能在有限训练数据和单一生成器训练下实现良好泛化，提升生成视频检测的稳健性。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [52] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 提出DermBench与DermEval，结合基准数据与参考无关评估器，为皮肤科多模态模型提供可重现、可扩展且与专家对齐的评估方法。


<details>
  <summary>Details</summary>
Motivation: 提高多模态大语言模型在皮肤病学诊断叙述生成上的评估可靠性与可扩展性。

Method: 收集4000张真实皮肤病图片并配专家认证诊断叙述；使用LLM作为裁判为候选叙述评分；训练参考无关的多模态评估器DermEval以对单个病例生成结构化评论及评分。

Result: 构建DermBench基准与DermEval自动评估器，实现与专家评分高度一致的评估体系。

Conclusion: DermBench和DermEval能以接近专家的评分误差可靠衡量模型诊断能力与可信度，支持细粒度个案分析并揭示模型局限与偏差。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [53] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: TACO通过原子化查询+释义+自我聚合置信度估计，显著减少MLLM幻觉并提升置信度校准，无需外部视觉专家。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型常出现关于对象存在、属性或关系的幻觉，影响可靠性。需要一种无需外部视觉专家的自我验证与置信度估计方法来提升模型可信性。

Method: TACO包含四步：1) 将回答拆成原子查询；2) 对每个原子查询生成多种释义；3) 对释义结果采用自洽（黑盒）或自信（灰盒）聚合以估计置信度；4) 基于置信度结果让语言模型修正或重写答案。无需外部视觉专家。

Result: 在五个基准（POPE、MME、HallusionBench、AMBER、MM-Hal Bench）和两种模型（LLaVA-1.5-7B、CogVLM2）上，TACO优于直接提示和Visual Contrastive Decoding，减少系统偏差并改善置信度校准。

Conclusion: 本文提出了TACO框架，通过将多模态大模型的回答分解为原子查询、进行释义以降低措辞敏感性，并使用自洽或自信聚合估计置信度，最终由语言模型精炼答案，从而减少幻觉并提升置信度校准。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [54] [Spatial Information Bottleneck for Interpretable Visual Recognition](https://arxiv.org/abs/2511.09239)
*Kaixiang Shu,Kai Meng,Junqin Luo*

Main category: cs.CV

TL;DR: VJP during backprop are minimal sufficient statistics; using an information bottleneck on VJP spatially disentangles foreground/background, improving explanations and accuracy across datasets and methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks conflate discriminative foreground features with spurious background correlations, hurting interpretability and robustness; need framework to understand and improve gradient-based attribution.

Method: Theoretical proof of VJP as minimal sufficient statistics; propose encoding-decoding view; design Spatial Information Bottleneck that maximizes mutual information between foreground VJP and inputs and minimizes it in background regions; train networks with S-IB to shape VJP spatially; evaluate across six explanation methods and five benchmarks.

Result: Proved VJP from backprop are minimal sufficient statistics of inputs for class labels; proposed S-IB to maximize mutual information between foreground VJP and inputs while minimizing background; improves visualization quality across explanation methods and boosts classification accuracy on five benchmarks.

Conclusion: Optimizing spatial structure of VJP via Spatial Information Bottleneck yields better foreground-focused explanations and suppresses background, improving both interpretability and classification performance universally across explanation methods.

Abstract: Deep neural networks typically learn spatially entangled representations that conflate discriminative foreground features with spurious background correlations, thereby undermining model interpretability and robustness. We propose a novel understanding framework for gradient-based attribution from an information-theoretic perspective. We prove that, under mild conditions, the Vector-Jacobian Products (VJP) computed during backpropagation form minimal sufficient statistics of input features with respect to class labels. Motivated by this finding, we propose an encoding-decoding perspective : forward propagation encodes inputs into class space, while VJP in backpropagation decodes this encoding back to feature space. Therefore, we propose Spatial Information Bottleneck (S-IB) to spatially disentangle information flow. By maximizing mutual information between foreground VJP and inputs while minimizing mutual information in background regions, S-IB encourages networks to encode information only in class-relevant spatial regions. Since post-hoc explanation methods fundamentally derive from VJP computations, directly optimizing VJP's spatial structure during training improves visualization quality across diverse explanation paradigms. Experiments on five benchmarks demonstrate universal improvements across six explanation methods, achieving better foreground concentration and background suppression without method-specific tuning, alongside consistent classification accuracy gains.

</details>


### [55] [GRACE: Designing Generative Face Video Codec via Agile Hardware-Centric Workflow](https://arxiv.org/abs/2511.09272)
*Rui Wan,Qi Zheng,Ruoyu Zhang,Bu Chen,Jiaming Liu,Min Li,Minge Jing,Jinjia Zhou,Yibo Fan*

Main category: cs.CV

TL;DR: 本文提出首个面向FPGA的动画生成编解码（AGC）边缘部署方案：通过量化、层融合压缩网络，设计软硬协同、重叠加速器并在PYNQ-Z1上实现，能耗效率分别较CPU和GPU提升24.9×和4.1×，每像素重建仅耗11.7μJ。


<details>
  <summary>Details</summary>
Motivation: 传统AGC解码器参数多、算法更新快、计算与传输能耗高，不利于部署在边缘受限设备；因此需要将算法压缩并在FPGA上设计高效可重构的加速器以降低能耗与资源占用。

Method: 对AGC进行后训练静态量化与层融合；设计协处理器范式的软硬件协同架构；实现卷积、grid sampling、上采样等硬件引擎；使用重叠执行、双缓冲流水线与循环展开进行并行优化；在PYNQ-Z1上实现原型并进行能效评估。

Result: FPGA部署面向边缘的AGC实现与加速器设计

Conclusion: 通过算法级压缩和软硬件协同的加速器设计，AGC可高效部署于资源受限的FPGA边缘平台，显著提升能耗效率并满足实时视频重建需求。

Abstract: The Animation-based Generative Codec (AGC) is an emerging paradigm for talking-face video compression. However, deploying its intricate decoder on resource and power-constrained edge devices presents challenges due to numerous parameters, the inflexibility to adapt to dynamically evolving algorithms, and the high power consumption induced by extensive computations and data transmission. This paper for the first time proposes a novel field programmable gate arrays (FPGAs)-oriented AGC deployment scheme for edge-computing video services. Initially, we analyze the AGC algorithm and employ network compression methods including post-training static quantization and layer fusion techniques. Subsequently, we design an overlapped accelerator utilizing the co-processor paradigm to perform computations through software-hardware co-design. The hardware processing unit comprises engines such as convolution, grid sampling, upsample, etc. Parallelization optimization strategies like double-buffered pipelines and loop unrolling are employed to fully exploit the resources of FPGA. Ultimately, we establish an AGC FPGA prototype on the PYNQ-Z1 platform using the proposed scheme, achieving \textbf{24.9$\times$} and \textbf{4.1$\times$} higher energy efficiency against commercial Central Processing Unit (CPU) and Graphic Processing Unit (GPU), respectively. Specifically, only \textbf{11.7} microjoules ($\upmu$J) are required for one pixel reconstructed by this FPGA system.

</details>


### [56] [Deep Learning for Metabolic Rate Estimation from Biosignals: A Comparative Study of Architectures and Signal Selection](https://arxiv.org/abs/2511.09276)
*Sarvenaz Babakhani,David Remy,Alina Roitberg*

Main category: cs.CV

TL;DR: 本工作系统性比较了经典回归与深度学习模型在不同生理信号组合下对能量消耗（EE）估计的表现，发现呼吸分钟通气量最具预测力，Transformer在所有活动上达到了最低RMSE 0.87 W/kg；多信号组合（如Hexoskin五信号）适合更快的CNN/ResNet+注意力模型；低强度活动的误差显著较低，高强度活动标准化误差相近；个体间差异显著，建议采用自适应建模。


<details>
  <summary>Details</summary>
Motivation: 现有研究多用经典回归估计能量消耗，少有研究分离神经网络架构效应与信号选择影响。本研究旨在系统评估两者，从而为选择模型与传感器组合提供指导。

Method: 系统性实验：比较经典基线（回归）与多种深度学习架构（包括Transformer、CNN、ResNet+注意力等），在单一信号、信号对、以及分组传感器输入（例如Hexoskin五信号）上进行训练与评估，按活动类型和受试者分组报告RMSE与NRMSE。

Result: 核心结果包括：1) 单信号中分钟通气量最有预测力；2) Transformer总体最优（RMSE 0.87 W/kg）；3) 多信号输入可使轻量模型（CNN/ResNet+注意力）在精度上接近更复杂模型；4) 低强度活动RMSE最低（下至0.29 W/kg，NRMSE=0.04），高强度活动RMSE更大但NRMSE相对接近；5) 个体间差异大，建议自适应建模。

Conclusion: 呼吸分钟通气量为最有效的单一信号，Transformer在总体上表现最好；但在需要更快或更低复杂度的场景下，结合多信号的CNN或ResNet+注意力是可行替代；不同活动强度和个体差异影响模型性能，需考虑按活动或按个体自适应策略。

Abstract: Energy expenditure estimation aims to infer human metabolic rate from physiological signals such as heart rate, respiration, or accelerometer data, and has been studied primarily with classical regression methods. The few existing deep learning approaches rarely disentangle the role of neural architecture from that of signal choice. In this work, we systematically evaluate both aspects. We compare classical baselines with newer neural architectures across single signals, signal pairs, and grouped sensor inputs for diverse physical activities. Our results show that minute ventilation is the most predictive individual signal, with a transformer model achieving the lowest root mean square error (RMSE) of 0.87 W/kg across all activities. Paired and grouped signals, such as those from the Hexoskin smart shirt (five signals), offer good alternatives for faster models like CNN and ResNet with attention. Per-activity evaluation revealed mixed outcomes: notably better results in low-intensity activities (RMSE down to 0.29 W/kg; NRMSE = 0.04), while higher-intensity tasks showed larger RMSE but more comparable normalized errors. Finally, subject-level analysis highlights strong inter-individual variability, motivating the need for adaptive modeling strategies. Our code and models will be publicly available at https://github.com/Sarvibabakhani/deeplearning-biosignals-ee .

</details>


### [57] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 提出RichKD，通过融合常规模型教师与CLIP视觉-语言知识进行知识蒸馏，改进蒸馏质量、类间一致性与鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有多教师知识蒸馏缺乏跨模态多样性，仅依赖视觉信息，忽视CLIP的视觉-语言表示作为互补监督源

Method: Proposed method

Result: 融合常规模型教师与CLIP的logits和特征，利用CLIP多提示文本引导，提升了准确率、置信度一致性和对非目标类的语义概率分布，在多个基准上表现优于大多数基线，并提高了鲁棒性

Conclusion: 简单的融合策略即可显著提升蒸馏效果和鲁棒性，CLIP提供的语义丰富监督改善了模型置信度和对非目标类概率分布，使学生更可靠

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [58] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: DensiCrafter通过对Trellis生成的体素密度场进行可微物理约束优化，实现了高达43%材料减量的轻量自支撑3D中空结构，兼容预训练模型并通过3D打印验证。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成模型缺乏物理约束和可制造性考虑，生成的几何可能在制造或实际使用中不稳定或浪费材料，因此需要一种生成既轻量又自支撑且可打印的三维设计方法。

Method: 在Treliis生成的粗糙体素网格基础上，将其视为连续密度场并进行优化。引入三项可微的、基于物理约束且无仿真的损失项，以及质量正则化和受限优化域，以保留外表面并惩罚不必要材料。方法可与预训练的Trellis型模型无缝集成。

Result: 在文本到三维任务上，材料质量可减少最多43%，在与最先进基线比较时提高了稳定性并保持高几何保真度。真实3D打印实验验证了中空结构的可制造性和自支撑性。

Conclusion: 该论文提出了一种在不改变预训练模型架构的前提下，通过优化密度场生成轻量且自支撑的三维中空结构的方法，可显著减少材料用量并保持几何保真度，且在真实3D打印中可行。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [59] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: 在教师-学生半监督分割中加入学生→教师反馈（通过反馈归因器和接收器），并扩展为双教师反馈模型，有助于纠正伪标签错误、抑制误差自我强化并提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中图像本身的不确定性导致伪标签错误，学生在迭代中反复确认这些错误导致自我强化偏差。现有方法多依赖外部改动，忽视框架内部的自我修正潜力。

Method: 设计反馈归因器（标识触发学生更新的伪标签）和反馈接收器（确定反馈作用位置），并提出双教师反馈模型，通过跨教师监督解决分歧以避免一致性错误。学生向教师反馈其受伪标签影响的变化，教师据此修正伪标签。

Result: 在三个医学图像基准数据集上的全面评估表明，该方法能有效抑制错误传播，提升半监督医学图像分割性能，双教师结构带来更多增益。

Conclusion: 本文提出在教师-学生半监督分割框架中引入反馈机制，以避免学生对错误伪标签的自我强化，从而降低误导性监督带来的偏差。

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [60] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: 提出FQ-PETR：通过QFPE、DULUT和QANS实现PETR系列模型的高保真全量化，在W8A8下保持接近浮点精度并大幅降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有PETR模型在部署端受限于高计算和内存开销，直接量化会导致严重精度下降，主要由于图像特征与位置嵌入幅值差异及对非线性算子量化效率低下。

Method: 设计QFPE替换非线性位置嵌入，提出DULUT用两级线性LUT近似非线性算子，采用QANS在softmax数值稳定化后再进行量化；在多种PETR模型上进行W8A8量化评估并与基线比较。

Result: FQ-PETR提出了一种针对PETR系列模型的全量化框架，通过三项关键创新解决量化导致的精度下降问题：1）QFPE：用LiDAR先验引导的单点采样和基于锚点的嵌入替代多点采样，消除非线性并对齐尺度；2）DULUT：用两级线性查找表近似复杂非线性函数，条目少且无需专用硬件；3）QANS：在数值稳定化后再量化，减轻大输入导致的注意力畸变。实验表明在W8A8下几乎无精度损失（约1%下降），并显著降低延迟最高达75%。

Conclusion: FQ-PETR在保证精度的同时实现了高效量化，适用于多种PETR变体，并显著优于现有PTQ和QAT方法。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [61] [Spatio-Temporal Context Learning with Temporal Difference Convolution for Moving Infrared Small Target Detection](https://arxiv.org/abs/2511.09352)
*Houzhang Fang,Shukai Guo,Qiuhuan Chen,Yi Chang,Luxin Yan*

Main category: cs.CV

TL;DR: 提出TDCNet：用重参数化的时差卷积模块融合时间差和3D卷积以捕获多尺度运动信息，并通过TDC引导的跨主干时空注意力精炼帧特征，从而显著提升移动红外小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 移动红外小目标在弱目标特征与复杂背景下难以检测，时间差能显式建模运动但空间表征弱，3D卷积空间表征强但对时间运动不显式，需要融合两者以获得更准确的时空表征以提升检测性能。

Method: 设计TDC重参数化模块：由三并行TDC块组成，每块将时间差与3D卷积融合为统一的时空卷积表示；同时构建并行3D主干并提出TDC引导的时空跨注意力，用于在全局语义层面融合两路特征以精炼当前帧表示。

Result: 提出了一种用于移动红外小目标检测的新网络TDCNet，核心是引入了时差卷积（TDC）重参数化模块和TDC引导的时空注意力机制，通过并行的TDC块融合时间差和3D卷积，捕获不同时间尺度的运动上下文，并与并行3D主干网络进行跨注意力交互，提升了时空特征表达，抑制伪运动干扰，在IRSTD-UAV和公共红外数据集上达到了最先进性能。

Conclusion: TDCNet通过在同一模块内融合时间差信息与3D卷积并引入跨主干时空注意力，增强了运动感知和多尺度时空表示，有效抑制复杂背景下的伪运动，显著提升了移动红外小目标检测的准确性。

Abstract: Moving infrared small target detection (IRSTD) plays a critical role in practical applications, such as surveillance of unmanned aerial vehicles (UAVs) and UAV-based search system. Moving IRSTD still remains highly challenging due to weak target features and complex background interference. Accurate spatio-temporal feature modeling is crucial for moving target detection, typically achieved through either temporal differences or spatio-temporal (3D) convolutions. Temporal difference can explicitly leverage motion cues but exhibits limited capability in extracting spatial features, whereas 3D convolution effectively represents spatio-temporal features yet lacks explicit awareness of motion dynamics along the temporal dimension. In this paper, we propose a novel moving IRSTD network (TDCNet), which effectively extracts and enhances spatio-temporal features for accurate target detection. Specifically, we introduce a novel temporal difference convolution (TDC) re-parameterization module that comprises three parallel TDC blocks designed to capture contextual dependencies across different temporal ranges. Each TDC block fuses temporal difference and 3D convolution into a unified spatio-temporal convolution representation. This re-parameterized module can effectively capture multi-scale motion contextual features while suppressing pseudo-motion clutter in complex backgrounds, significantly improving detection performance. Moreover, we propose a TDC-guided spatio-temporal attention mechanism that performs cross-attention between the spatio-temporal features from the TDC-based backbone and a parallel 3D backbone. This mechanism models their global semantic dependencies to refine the current frame's features. Extensive experiments on IRSTD-UAV and public infrared datasets demonstrate that our TDCNet achieves state-of-the-art detection performance in moving target detection.

</details>


### [62] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: 提出Flora方法解决零样本骨架动作识别中语义对齐脆弱和分类器僵化的问题，采用邻近类语义调节与点到区域对齐、无噪流匹配与对比正则化构建分布感知分类器，在少量训练数据下也有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有“先对齐再分类”方法受限于点对点对齐易受语义不完善影响和静态分类器导致决策粗糙，需更鲁棒的对齐策略和更灵活的分类器以提升零样本骨架动作识别。

Method: 核心包括：1) 灵活邻居感知的文本语义调谐，将相邻类别上下文融入形成方向感知的区域语义；2) 引入跨模态几何一致性损失，保证点到区域对齐稳健；3) 采用无噪声流匹配（noise-free flow matching）缩小语义与骨架潜空间分布差异；4) 条件无依赖的对比正则化增强可分性；5) 基于token级速度预测构建分布感知的开放式流分类器，得到细粒度决策边界。

Result: 在三个基准数据集上做大量实验，结果显示Flora在总体性能和低资源（仅10%见类数据）下均有显著提升，验证了方法的有效性；并已发布代码。

Conclusion: Flora通过邻近类感知的语义调谐、跨模态几何一致性和无噪声流匹配联合优化，实现了更稳健的点到区域对齐与更细粒度的决策边界，显著提升零样本骨架动作识别性能，尤其在仅用10%见类数据时仍表现优异。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [63] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: 基于高斯基元物理参数与渲染雅可比传播协方差，结合语义分割，提出目标感知不确定性用于更有效的主动视角选择，提升3DGS对象重建质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有主动重建方法使用场景级不确定性，易受背景干扰，无法高效聚焦物体目标；因此需要面向对象且物理可解释的不确定性定义以改进视角选择与重建效率。

Method: 从高斯基元参数（位置、尺度、旋转）建立参数协方差，通过渲染雅可比将参数协方差传播到像素/观测不确定性，结合语义分割掩码计算目标加权不确定度，基于该不确定度设计主动视角选择策略以优先采集对提升目标质量最有利的视角。

Result: OUGS提出了一种基于3D Gaussian Splatting的面向对象的不确定性定义，通过从高斯基元的物理参数（位置、尺度、旋转）导出协方差并经渲染雅可比传播，获得可解释的不确定性模型，并结合语义分割得到目标感知不确定度，进而用于主动视角选择以提升目标重建效率与精度。

Conclusion: OUGS在公共数据集上显著提高了针对对象的重建效率与质量，同时对全局场景提供稳健的不确定性估计，优于现有方法。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [64] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 提出一种域不变视觉位姿优化方法和首个合成基准数据集，实现鲁棒的2D-3D帧级配准，合成上误差小并能泛化到真实数据。


<details>
  <summary>Details</summary>
Motivation: 解决支气管镜手术中插入器定位困难，受呼吸运动、解剖变异和CT与体内视图不一致影响，现有视觉方法缺乏跨域泛化。

Method: （1）构建模态与域不变的特征编码器，将真实RGB帧与CT渲染深度图映射到相同表征空间；（2）使用相似性度量进行2D-3D匹配；（3）引入可微渲染模块，根据深度一致性迭代优化相机位姿；（4）构造并发布首个配对合成CT-内镜数据基准，训练在不同合成环境下实现泛化。

Result: 提出基于视觉的位姿优化框架、训练域不变编码器、可微渲染迭代精化位姿，并发布首个公共合成基准数据集；在合成基准上达到平均平移误差2.65 mm、旋转误差0.19 rad，并在真实病人数据上表现出良好跨域泛化。

Conclusion: 通过域不变编码器与可微渲染的迭代优化，本工作实现了稳定的支气管镜实时定位，并通过公开合成基准促进领域可重复评估与发展。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [65] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 作者创建了一个专门的数据集并微调检测与再识别模型，使得在拥挤且遮挡严重的美式足球场景中实现了更高的检测与跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集多为行人或特定运动（如足球、篮球），缺少针对美式足球的标准数据集；高遮挡与身体接触使美式足球跟踪更具挑战，需要专门的数据与评测以推动方法发展。

Method: 收集并标注美式足球比赛视频构建数据集，基于现有检测模型（预训练）进行微调，训练再识别(re-id)模型，并将微调后的检测器和re-id模型整合进跟踪系统进行比较评估。

Result: 构建了首个美式足球运动员检测与跟踪数据集，并评估了多种检测与跟踪方法。

Conclusion: 通过构建数据集和微调模型，能显著提升美式足球球员在高密度场景下的检测与跟踪性能，从而填补了该领域数据与评测的空白。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [66] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: Use two teachers (ViT and CNN). Fuse their outputs by discrepancy-aware weighting and teach student residual features through an auxiliary branch, improving lightweight CNN performance significantly.


<details>
  <summary>Details</summary>
Motivation: Bridge the performance gap between efficient lightweight CNNs and high-performing ViTs by leveraging both heterogeneous ViT and homogeneous CNN teachers to provide complementary guidance, addressing architectural mismatch and underutilization of strong CNN teachers.

Method: Dual-Teacher Knowledge Distillation with Discrepancy-Aware strategies

Result: Proposed framework dynamically fuses teacher predictions using adaptive weights based on confidence and student-teacher discrepancy, and introduces learning of residual features via an auxiliary branch; achieves consistent improvements over SOTAs on HMDB51, EPIC-KITCHENS-100, Kinetics-400, with up to 5.95% gain on HMDB51.

Conclusion: Dual-teacher framework with discrepancy-aware weighting and structure-aware residual distillation effectively transfers complementary knowledge from ViT and CNN teachers to a lightweight CNN student, reducing architectural mismatch and improving action recognition accuracy across benchmarks.

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


### [67] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: DreamPose3D结合动作感知的条件化扩散、关节亲和力编码与幻觉式时间解码，显著改善3D人体姿态估计的准确性与时间连贯性，应对模糊与噪声场景表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖几何线索、独立预测每帧3D姿态，难以处理运动模糊、2D噪声及跨帧一致性问题。受人类理解与预测动作的启发，通过引入动作理解和时间想象来增强模型决策。

Method: 提出了基于扩散模型的DreamPose3D框架：在去噪过程中加入从2D序列提取的动作提示（action prompts）进行条件化；引入在注意力机制中融合运动学关节亲和力的表征编码器；训练时使用幻觉式姿态解码器生成时间一致的3D序列以模拟人类的运动重建。

Result: 在Human3.6M和MPI-3DHP基准上，在所有指标上均达到或超过现有最优方法；在广播棒球数据集上展示了对含噪2D输入和模糊动作场景的强鲁棒性与时间一致性处理能力。

Conclusion: DreamPose3D能通过结合动作感知提示、关节亲和力编码和时间一致性的幻觉式解码器，有效提升3D人体姿态估计的准确性与鲁棒性，在多个数据集上实现了SOTA性能。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>


### [68] [vMFCoOp: Towards Equilibrium on a Unified Hyperspherical Manifold for Prompting Biomedical VLMs](https://arxiv.org/abs/2511.09540)
*Minye Shao,Sihan Guo,Xinrun Li,Xingyu Miao,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 提出 vMFCoOp，在高维超球面上用 von Mises-Fisher 分布对齐 LLM 与 CLIP 的语义先验，通过统一语义锚点和三条约束，提升生物医学提示学习的鲁棒性和小样本分类性能，在14个医学数据集和多种模态上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于 LLM 蒸馏的语义先验用于提示学习时，存在 LLM 与 CLIP 语义不对齐、欧氏优化无法建模统一表示及局部几何约束，导致模态间差距放大和小样本适配不稳定，需要一种在流形上进行概率对齐的方案。

Method: 在共享的超球流形上以逆估计方式拟合 vMF 分布，构建统一语义锚点以桥接任意 LLM 与 CLIP 骨干之间的语义偏差，并在训练时引入三条互补约束（未详细说明但涉及局部几何约束、对比对齐和分布一致性），用于稳定提示优化与小样本分类。

Result: 在14个医学数据集、12种医学成像模态和13个解剖区域上，vMFCoOp 在准确率、泛化性和临床适用性上均优于现有最先进方法，并展示了稳定的少样本适配性能。

Conclusion: vMFCoOp 在多数据集、多模态、多解剖区域的医学图像小样本任务中，能稳定提升准确率、泛化性和临床适用性，优于现有 CoOp 和基于欧氏对齐的方法，适合扩展到更多下游应用。

Abstract: Recent advances in context optimization (CoOp) guided by large language model (LLM)-distilled medical semantic priors offer a scalable alternative to manual prompt engineering and full fine-tuning for adapting biomedical CLIP-based vision-language models (VLMs). However, prompt learning in this context is challenged by semantic misalignment between LLMs and CLIP variants due to divergent training corpora and model architectures; it further lacks scalability across continuously evolving families of foundation models. More critically, pairwise multimodal alignment via conventional Euclidean-space optimization lacks the capacity to model unified representations or apply localized geometric constraints, which tends to amplify modality gaps in complex biomedical imaging and destabilize few-shot adaptation. In this work, we propose vMFCoOp, a framework that inversely estimates von Mises-Fisher (vMF) distributions on a shared Hyperspherical Manifold, aligning semantic biases between arbitrary LLMs and CLIP backbones via Unified Semantic Anchors to achieve robust biomedical prompting and superior few-shot classification. Grounded in three complementary constraints, vMFCoOp demonstrates consistent improvements across 14 medical datasets, 12 medical imaging modalities, and 13 anatomical regions, outperforming state-of-the-art methods in accuracy, generalization, and clinical applicability. This work will be continuously expanded to encompass more downstream applications, and the corresponding resources are intended to be shared through https://github.com/VinyehShaw/UniEqui.

</details>


### [69] [RF-DETR: Neural Architecture Search for Real-Time Detection Transformers](https://arxiv.org/abs/2511.09554)
*Isaac Robinson,Peter Robicheaux,Matvei Popov,Deva Ramanan,Neehar Peri*

Main category: cs.CV

TL;DR: RF-DETR：对目标域进行轻量微调并用权重共享NAS无须重训练地搜索准确性-延迟权衡，显著提升实时检测性能与速度。


<details>
  <summary>Details</summary>
Motivation: 解决开集目标检测在真实世界数据集上对域外类别泛化差的问题，通过轻量专用检测器和权重共享神经架构搜索（NAS）在目标数据集上发现准确率-延迟的最优折衷曲线。

Method: 基于预训练骨干网络在目标数据集上微调；采用权重共享神经架构搜索评估大量候选配置（不同的分辨率、层数、头数等“可调节旋钮”）以无重训练方式找到准确率-延迟非支配前沿；并调整NAS中的可调项以提高DETR向新域的迁移能力。

Result: 提出RF-DETR，一种轻量级专用检测Transformer，能在无需重训练的情况下通过权重共享NAS评估千余网络配置并提高在COCO和Roboflow100-VL上的实时检测性能；RF-DETR(nano)在COCO上达48.0 AP，比D-FINE(nano)高5.3 AP，RF-DETR(2x-large)在Roboflow100-VL上比GroundingDINO(tiny)高1.2 AP且快20倍，且首个在COCO实时检测超过60 AP的检测器。

Conclusion: 通过结合轻量专用DETR与可迁移的NAS设计（可调节控制项），RF-DETR在多个真实世界数据集上实现了显著更好的准确率-延迟折中，是实时检测器的新基准。

Abstract: Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the "tunable knobs" for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves on prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is at https://github.com/roboflow/rf-detr

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [FlashMap: A Flash Optimized Key-Value Store](https://arxiv.org/abs/2511.08826)
*Zonglin Guo,Tony Givargis*

Main category: cs.DB

TL;DR: FlashMap is a flash-optimized key-value store achieving ~20M inserts/s and ~24M lookups/s on one server by using SSD-specific optimizations.


<details>
  <summary>Details</summary>
Motivation: Leverage the performance and scalability needs of modern applications and exploit characteristics of Flash SSDs to build a high-performance key-value store.

Method: Design and implement FlashMap with optimizations for Flash SSDs (likely including write amplification reduction, efficient indexing, batching, and concurrency); evaluate experimentally on data-center-grade server with 100-byte payloads.

Result: Achieves average throughput of 19.8M inserts/sec and 23.8M random lookups/sec for 100-byte payloads on a single server.

Conclusion: FlashMap is an SSD-optimized key-value store that delivers very high throughput on a single server, demonstrating that tailored designs for flash can greatly improve performance.

Abstract: Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.

</details>


### [71] [Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration](https://arxiv.org/abs/2511.09001)
*Yuka Haruki,Shigeru Ishikura,Kazuya Demachi,Teruaki Hayashi*

Main category: cs.DB

TL;DR: 文章提出一种将结构与上下文融合的图嵌入方法用于表格匹配，实验显示在多种困难场景下优于现有方法，但在某些语义微妙区分的案例上仍失败。


<details>
  <summary>Details</summary>
Motivation: 现有自动化表格匹配方法未充分考虑数据集特征对匹配效果的影响，且少有方法将多种上下文信息与结构信息有效结合，限制了在真实企业场景下的应用效果。

Method: 构建融合结构信息（如表格的列-单元格关系）与上下文信息（列描述、外部知识等）的图表示，并在该图上训练图嵌入模型用于列匹配和实体解析。

Result: 在多样化数据集（不同领域专属性、数据量、缺失率、重叠率）上实验表明，该方法普遍优于现有图基方法，尤其在数值比例高或缺失严重的困难情形下表现更好，但对语义相近却需区分的列表现不佳。

Conclusion: 基于上下文的图嵌入方法能提升表格匹配任务的鲁棒性，但仍存在语义相近却需区分的列等失败案例，表明该领域还有改进空间。

Abstract: As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.

</details>


### [72] [Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking](https://arxiv.org/abs/2511.09052)
*Yu Wang,Hui Wang,Jiake Ge,Xin Wang*

Main category: cs.DB

TL;DR: 提出将GNN-PE扩展到分布式系统的三项核心创新：动态相关感知负载均衡与热迁移、多GPU协作动态缓存策略以及基于支配嵌入剪枝潜力的查询计划排序（PE-score），并结合METIS划分与轻量元数据管理，实现边切最小、负载均衡和查询不中断的分布式精确子图匹配。


<details>
  <summary>Details</summary>
Motivation: 现有GNN-PE在单机上能高效完成精确子图匹配，但缺乏对分布式系统的扩展性和优化，面临负载不均、GPU资源利用低和查询计划次优等问题。

Method: （1）设计轻量动态相关感知负载均衡和热迁移机制，融合CPU、通信和内存等多维度指标并保证索引一致性；（2）提出基于在线增量学习的多GPU协作动态缓存策略，支持异构GPU并考虑图结构的替换策略；（3）引入基于支配嵌入剪枝潜力（PE-score）的查询计划排序方法，结合METIS划分、并行离线预处理与轻量元数据管理实现整体优化。

Result: 通过METIS划分、并行预处理和轻量元数据管理，提出的方法在数十台机器的分布式场景下实现了边切最小、负载均衡和查询不中断，显著提升了分布式子图匹配的效率与稳定性。

Conclusion: 该方法通过综合负载均衡、GPU缓存与查询计划优化，在分布式（数十台机器）环境下显著提高了精确子图匹配的效率和稳定性，解决了现有GNN-PE在扩展性和分布式优化方面的不足。

Abstract: Exact subgraph matching on large-scale graphs remains a challenging problem due to high computational complexity and distributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed environments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, memory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves "minimum edge cut + load balancing + non-interruptible queries" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.

</details>


### [73] [CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System](https://arxiv.org/abs/2511.09262)
*Jiaping Cao,Ting Sun,Man Lung Yiu,Xiao Yan,Bo Tang*

Main category: cs.DB

TL;DR: 提出CheetahGIS：基于Flink StateFun的模块化流式空间查询系统，通过网格索引、元数据同步与负载均衡等优化，支持对象查询、范围计数与k近邻查询，具可扩展性与高效性


<details>
  <summary>Details</summary>
Motivation: Existing spatial data analytics systems struggle with massive moving objects and real-time queries; need scalable, efficient processing over streams

Method: Architect and optimize a streaming spatial query system

Result: CheetahGIS built on Apache Flink StateFun, modular architecture, optimizations (global grid index, metadata sync, load balancing), supports object, range count, k-NN queries; evaluated on real/synthetic datasets

Conclusion: CheetahGIS通过模块化设计与多项优化在大规模移动对象流与实时空间查询场景下实现了高扩展性与高性能，并适用于多类流式空间查询。

Abstract: Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.

</details>
