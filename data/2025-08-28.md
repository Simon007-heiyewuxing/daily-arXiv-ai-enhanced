<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 融合结构性与语义性意图的实时协作式生成绘图系统，结合轮廓保留与风格/语义驱动的图像合成，实现低延迟、多用户共创


<details>
  <summary>Details</summary>
Motivation: Enable low-latency, co-authored sketch transformation that respects both geometric sketch features and high-level semantics for inclusive human-AI creative tools

Method: Multi-stage conditional generation combining structural (formal) and semantic (contextual) intents

Result: A touchscreen, distributed, two-stage system that preserves contours and applies style/content-aware synthesis, supporting multi-user collaborative low-latency editing

Conclusion: 通过同时建模形式意图与语义意图，并在多阶段流水线中联合条件化，系统实现了实时、交互性强且易于参与的协同绘画平台。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF是一个训练免费、基于像素差+注意力相关性的选择性时序token融合方法，能稳健利用帧间连贯性提升VLA模型成功率并具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action模型逐帧独立处理视觉输入，忽视了操作序列中的时间连贯性，且对视觉噪声脆弱；因此需要一种能利用历史信息提升稳健性且无需重新训练的方法。

Method: 提出Temporal Token Fusion (TTF)：基于双维检测（高效的灰度像素差用于低成本运动变化检测 + 基于注意力的语义相关性评估）决定是否进行硬融合；采用关键帧锚定避免误差累积。方法不改动模型训练，适用于OpenVLA与VLA-Cache等架构，并探索在注意力中选择性重复Query矩阵以兼顾性能与加速潜力。

Result: 在多数据集与真实机器人评测上均有稳定提升：LIBERO平均提升4.0个百分点（72.4% vs 68.4%基线）；SimplerEnv跨环境验证相对提升4.8%；真实机器人任务相对提升8.7%。方法在OpenVLA与VLA-Cache上均有效，并揭示了选择性复用注意力中Query矩阵能提升效果、提示KQV复用用于加速的可行性。

Conclusion: TTF通过有选择地融合历史与当前视觉token，显著提升VLA模型在机器人操作任务中的推理质量，是一种无需额外训练、可移植到不同架构的有效时序增强方法。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [3] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出一种无监督、可解释的幻灯片质量评估管线：把7个设计指标与CLIP-ViT嵌入相结合，用Isolation Forest进行异常性评分，训练于12k专业幻灯片，在115张评估样本上与人工视觉评分相关性高达0.83，显著优于主流视觉-语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前自动化幻灯片质量评估缺乏可解释的视觉设计依据且现有视觉-语言模型在此任务上表现有限，作者旨在结合可解释的设计指标与强大的多模态表征以更贴近观众对幻灯片质量的感知并实现可扩展的实时反馈。

Method: 构建七个专家启发的低层视觉设计特征（空白、色彩丰富度、边缘密度、亮度对比、文本密度、色彩和谐、布局平衡）并提取CLIP-ViT嵌入，将这些特征输入Isolation Forest进行异常性评分，基于无监督学习输出幻灯片质量分数；训练集为12k专业幻灯片，评估集为6场学术报告（115张幻灯片），与人工评分比较并计算皮尔逊相关。

Result: 在115张评估幻灯片上，与人工视觉质量评分的皮尔逊相关最高达0.83，较主流视觉-语言模型的评分相关性提高1.79至3.23倍；验证了与视觉评分的聚合效度、与演讲者表现分数的区分效度，并探索性地与总体印象保持一致。

Conclusion: 该论文提出了一种结合专家启发的视觉设计度量和CLIP-ViT嵌入的无监督幻灯片质量评估方法，并用Isolation Forest异常评分进行综合评估，训练数据为1.2万张专业讲座幻灯片，实验证明与人工视觉评分高度相关，优于主流视觉-语言模型。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [4] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 提出一种数学驱动且可解释的轻量级净化网络，用于2D range-view LiDAR分割的对抗防御，在基准和实车演示中均表现优秀。


<details>
  <summary>Details</summary>
Motivation: 当前多数对抗防御针对3D点云且依赖大型生成模型，而许多高效的LiDAR分割系统使用2D范围视图表示，缺乏轻量级且专门针对范围视图的对抗防御方法，因此作者希望填补这一空白，提升实际自动驾驶系统的安全性。

Method: 作者在range-view域提出直接的对抗攻击建模，随后将防御设为一个有数学理论支撑的优化问题，并设计了可解释的轻量级净化网络以求解该优化问题，从而对输入的对抗扰动进行修复。该净化模块插入到现有的2D范围视图分割流水线中，具有低计算开销，便于实车部署。

Result: 在公开基准上，所提方法在对抗鲁棒性上优于生成式和对抗训练基线，并在实际车辆部署演示中保持了高分割精度和实时性，证明了方法在实用场景中的有效性。

Conclusion: 该论文提出了一种针对2D范围视图（range-view）LiDAR分割的轻量级基于模型的净化（purification）防御框架，通过在范围视图域内直接构建攻击，并基于数学优化问题设计可解释的净化网络，实现对对抗样本的强鲁棒性，同时保持较低计算开销。实验在公开基准上显示出优于生成式和对抗训练基线的性能，并在实际演示车辆上验证了实用性。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [5] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: LVLMs通过联合NLP与CV提升目标检测能力，未来有望超越传统方法，但需解决计算成本、标注需求和鲁棒性问题


<details>
  <summary>Details</summary>
Motivation: 融合语言与视觉提高目标检测的泛化与上下文理解

Method: 分析方法

Result: LVLMs在定位、分割、多模态理解上取得显著进展，但在实时性、数据效率与鲁棒性上仍有瓶颈

Conclusion: LVLMs已并将继续显著推动目标检测与机器人应用，需在模型效率、训练范式与评估基准上进一步优化

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [6] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 作者展示了两级微调LVLM在生成专业、风格化的体育图像描述上的优势：F1提升8-10%，BERT分数提升2-10%，并在超碗LIX中实现实时部署，处理1000+图像，速度为3-5秒生成6张图像的标题


<details>
  <summary>Details</summary>
Motivation: 现有LLM/LVLM缺乏领域专有的体育术语和风格化表达，导致无法生成生产级、自然的人类化体育赛事图像描述，需提出针对性微调以满足实时体育新闻需求

Method: 采用两级（两阶段）微调流水线：第一阶段可能进行通用体育术语与视觉对齐微调，第二阶段在目标风格（新闻/专业体育用语）和低延迟部署上进行细化；评估指标包括F1和BERT分数，并进行了实际赛事（Super Bowl LIX）在线部署验证

Result: 提出了一种两级微调的视觉语言模型(LVLM)流水线，用于从图像生成风格化的体育赛况标题，弥补现有LLM/LVLM在体育术语与生产级描述方面的不足

Conclusion: 通过领域微调和两阶段训练策略，可以显著提升LVLM在体育图像字幕生成的准确性与风格化程度；该方法具有低内存占用和快速推理，适合实时体育新闻应用

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [7] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: 本文通过对LLaVA、BLIP-2和PaliGemma在人口统计均衡数据集上的微调与评估，采用group-specific BERTScore与Fairness Discrepancy Rate等指标，实证揭示了LVLMs在人脸识别文本生成任务中的人口统计偏差，并指出不同模型对各族群公平性存在差异。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLMs在多项下游任务中表现优异，但其在生物识别人脸识别任务中可能对不同人口统计群体存在不公平性。研究旨在定量评估这些模型在带描述的人脸识别任务中的群体间性能差异，以揭示和缓解潜在的偏见问题。

Method: 在构建了人口统计学均衡的数据集后，作者对三种预训练LVLM（LLaVA、BLIP-2、PaliGemma）进行了微调并在文本生成型FR任务上评估。使用group-specific BERTScore和Fairness Discrepancy Rate等多种指标量化性能差异，比较不同群体间的表现并分析偏差来源。

Result: 实验表明，LVLMs在带文本生成的人脸识别任务中存在人口统计学偏差，PaliGemma和LLaVA在某些族群（例如Hispanic/Latino、Caucasian和South Asian）上的公平性问题更明显，而BLIP-2表现出较小的群体间差异。

Conclusion: 本研究通过DemoBias实证评估了大型视觉-语言模型（LVLMs）在带文本生成的人脸识别（FR）任务上的人口统计学偏差，发现不同模型在种族/族裔、性别和年龄等群体上的表现存在显著差异，尤其是PaliGemma和LLaVA在Hispanic/Latino、Caucasian和South Asian组上偏差更大，而BLIP-2相对更稳定。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [8] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec用SDF与自适应采样在原始空间直接编码地理实体，结合旋转不变位置编码，提供统一且高效的几何表征，优于现有方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么只能处理单一实体类型，要么通过将复杂实体分解并进行傅里叶变换（如Poly2Vec）来表示形状，导致计算开销高且变换空间缺乏几何对齐，依赖非自适应采样而模糊边界细节。

Method: 提出基于SDF的自适应采样策略和用神经网络拟合SDF；引入旋转不变的位置编码以建模高频空间变化；在原始几何空间操作，避免分解和傅里叶变换带来的计算负担和对齐问题。

Result: 在多项GeoAI真实任务中，Geo2Vec在形状/位置表示、拓扑与距离关系捕捉以及效率上均优于现有方法。

Conclusion: Geo2Vec通过直接基于Signed Distance Field(SDF)在原始空间自适应采样并编码符号距离，成功实现对点、折线、面等多种地理实体的统一、紧凑和几何感知表征。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [9] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 利用75,000张图像训练的深度学习模型可高效区分稻米品种并诊断叶片病害，且通过SHAP/LIME提升了可解释性，具备应用前景。


<details>
  <summary>Details</summary>
Motivation: 人工检验耗时、费力且易出错，为提升产量与质量控制需求，需开发自动化、可靠且可解释的稻米质量与病害诊断系统。

Method: 使用公开数据集（75000张图像）训练与测试深度学习模型进行五类稻米粒分类；使用多种网络（CNN、VGG16、ResNet50、MobileNetV2）对叶片病害进行识别；采用准确率、召回率、精确率、F1分数、ROC曲线与混淆矩阵等指标评估；并用SHAP和LIME解释模型预测依据。

Result: 模型在分类任务上达到了高准确率且误判率低；可解释性分析揭示了影响预测的关键谷粒和叶片特征，增强了模型可接受性与可用性。

Conclusion: 研究表明，基于深度学习的模型（CNN、VGG16、ResNet50、MobileNetV2）能够高效区分五种稻米品种并诊断多种稻叶病害；结合SHAP和LIME等可解释性方法可增强模型透明性与可靠性，具有实际应用潜力。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [10] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: 在联邦学习架构下将OpenMax整合入人脸识别，通过交换平均激活向量与局部距离信息（而非原始数据）来实现开放集下的已知/未知区分。实验表明该方法在分布式环境中可行，兼顾隐私与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Facial recognition models achieve high accuracy but face privacy and identity-management challenges, especially when the system encounters unknown individuals in open-set, distributed scenarios. Federated learning promises privacy benefits but lacks robust open-set handling.

Method: Design and implementation of a federated learning system augmented with the OpenMax algorithm. Clients compute local mean activation vectors and local distance measures, share them (rather than raw images or embeddings), and use OpenMax-based scoring to detect unknown identities across the federation.

Result: Experimental results show that the proposed federated OpenMax approach reliably identifies known vs unknown subjects, demonstrating improved robustness in open-set contexts while preserving privacy through limited information exchange. The implementation validates feasibility in distributed settings.

Conclusion: The paper concludes that integrating OpenMax into a federated learning framework, using exchanged mean activation vectors and local distance measures, can effectively distinguish known from unknown subjects in open-set facial recognition, yielding a privacy-aware and robust solution for distributed environments.

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [11] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 研究用地面照片和DeepLabV3-ResNet101对18类栖息地进行分类，平均F1=0.61，个别类别>0.9，展示了用于生态监测的潜力，并提供了一个网页应用。


<details>
  <summary>Details</summary>
Motivation: Develop a ground-level imagery-based habitat classification method to improve validation and scalability over satellite-based approaches, enabling use of citizen-science photos.

Method: 图片预处理（调整大小、归一化、增强）、重采样平衡训练集、微调DeepLabV3-ResNet101，五折交叉验证评估，部署网页应用供实践者使用。

Result: A DeepLabV3-ResNet101 model trained on preprocessed and class-balanced ground photos achieved mean F1-score 0.61 across 18 Living England habitat classes; some classes (BSSP, BS) >0.90.

Conclusion: 基于地面影像的深度学习方法能有效分类多类栖息地，适合大规模生态监测；但对混合或模糊类别表现较差，需要更多数据或改进方法。

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [12] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 通过将少量改动的LLM与扩散头和高压缩自编码器结合，本文实现了可实时交互的多模态自回归人类视频生成，兼顾可控性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有交互式数字人视频生成方法常受高延迟、计算开销大和可控性差的限制，难以满足实时、多模态交互需求。

Method: 在标准大语言模型上做最小修改，使其接受音频、姿态、文本等多模态条件编码，输出用于引导扩散头去噪的空间语义表示；构建约2万小时的多源对话数据集用于训练；并引入最大64×压缩比的深度压缩自编码器以降低自回归模型的长时序推理负担。

Result: 在双向对话（duplex conversation）、多语种人类合成和交互式世界模型任务上，方法在低延迟、高效率和细粒度多模态可控性方面表现优越。

Conclusion: 本文提出的自回归视频生成框架能实现互动式多模态控制与低延迟流式外推，兼具空间与语义一致性，适用于实时交互场景。

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [13] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本文系统评估了数字水印与隐写术在保护ICAO合规人像中的作用，指出它们可提供长期篡改可检测性，但需在鲁棒性、可见性与隐私之间权衡，并建议与其他安全机制结合以实现可靠部署。


<details>
  <summary>Details</summary>
Motivation: ICAO人像标准在全球身份验证中被广泛采用，但标准化使图像易受合成、morphing和deepfake等滥用，且传统PAD仅限实时检测，缺乏对已捕获图像的持久防篡改能力。

Method: 通过收集和分类近年来针对数字图像水印、鲁棒性评估、抗篡改检测与隐写术方法的文献，分析其在ICAO标准限制下（尺寸、质量、可见性、压缩）的适用性，并通过对比实验结果与安全威胁建模来评估实用性和风险。

Result: 综述总结了不同水印/隐写方案的能力和局限，提出了适配ICAO约束的设计原则（如不可见性、鲁棒性、可验证性、隐私保护），并给出部署建议与未来研究方向，例如联合活体检测、可追溯签名、对抗训练提升鲁棒性、以及标准化评估协议。

Conclusion: 该综述认为，数字水印与隐写术可作为对ICAO合规人像的持久性保护补充，但无法单独解决所有攻击场景，需与其他措施（如PAD、加密、可信签名及后端验证）结合使用。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [14] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM: self-supervised framework fusing temporally-aligned cine CMR features with EHR via medical prompts, yields superior MACE prediction and discovers imaging/clinical risk markers.


<details>
  <summary>Details</summary>
Motivation: Improve MACE risk prediction by integrating cine CMR imaging and structured EHR via self-supervision and medical prompts to obtain fine-grained, temporally synchronized features.

Method: Self-supervised multimodal survival modelling with prompt-guided image-text fusion

Result: Outperforms classical and SOTA deep learning survival models across four cohorts; uncovers three imaging risk signatures; identifies hypertension, diabetes, smoking as primary clinical contributors.

Conclusion: Prompt-guided integration of cine imaging and EHR in a self-supervised framework improves survival prediction and reveals interpretable imaging and clinical risk signatures.

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [15] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: 作者提出用EfficientNet+ViT并结合LoRA在全量ADNI T1 MRI上进行端到端三类分类，报告92.5%准确率与92.8% F1，优势为捕获局部和全局特征与高效域适配，但需更多细节与外部验证以评估泛化与临床可用性。


<details>
  <summary>Details</summary>
Motivation: AD不可逆，早期（MCI）诊断困难且临床意义重大；过去研究往往使用ADNI的有限子集，可能存在偏差，且直接微调大型预训练模型在域差异显著时效果欠佳。

Method: 构建一个将EfficientNet风格的CNN与Vision Transformer融合的端到端网络（EffNetViT），在全量ADNI T1加权MRI上训练；使用Low-Rank Adaptation（LoRA）技术对预训练ViT进行参数高效微调以降低过拟合并提升迁移效果。

Result: 在AD/MCI/CN三分类任务上报告整体准确率92.52%和F1-score 92.76%，并声称在全ADNI数据集上训练提高了模型的稳健性与临床可靠性。

Conclusion: 本文提出的EffNetViTLoRA在全量ADNI T1 MRI数据上对AD/MCI/CN三类进行端到端分类，得到了较高的总体性能（准确率92.52%、F1=92.76%），表明将CNN与ViT结合并用LoRA微调预训练ViT可有效提取局部与全局表征并在目标域上进行高效适配。

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [16] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 该研究评估了三家基于计算机视觉与AI的商业球员跟踪软件，使用2022世界杯一场比赛的广播画面与TRACAB Gen 5进行比对。结果显示位置误差RMSE为1.68–16.39 m、速度RMSE为0.34–2.38 m/s，总距离偏差在-21.8%至+24.3%之间。战术视角（tactical feed）能提升检测率与精度，720p与1080p均可接受。


<details>
  <summary>Details</summary>
Motivation: 评估能否通过常见广播视频与商用CV/AI软件获取可靠的球员位置与运动指标，并量化不同摄像头视角与分辨率对准确性的影响。

Method: 从2022卡塔尔世界杯一场比赛中提取战术、节目与摄像头1三种广播画面，让三家商用计算机视觉/AI跟踪供应商提取x,y位置与瞬时速度，比较高分辨率多摄像头系统TRACAB Gen 5作为金标准，计算RMSE与平均偏差。

Result: 位置RMSE 1.68–16.39 m；速度RMSE 0.34–2.38 m/s；总赛程距离偏差在-1745 m(-21.8%)到+1945 m(24.3%)。建议使用战术画面以最大化球员检测；720p/1080p在合适模型下可用于跟踪。

Conclusion: 商业计算机视觉与AI跟踪在球员被检测到时能达到“中等”（fair）精度，但存在较大个体与供应商间差异；建议使用战术摄像头以提高检测率与精度，分辨率720p或1080p均可。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [17] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 将视觉与语言模态联合用于气体泄漏分割，并加入后处理以降低误报，JVLGS在多场景、有监督与少样本评估中均超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 气体云在红外/可见光视频中通常表现为模糊且非刚性形态，传统仅基于视觉的方法对少量或间歇性泄漏存在漏报和误报问题；通过引入语言模态可提供语义约束以增强表示。

Method: 提出Joint Vision-Language Gas leak Segmentation (JVLGS)框架，将视觉信息与语言提示/文本特征融合以提升对模糊、非刚性气云的识别，同时加入后处理步骤以抑制噪声与非目标误报。

Result: 在多场景实验中，JVLGS在分割质量上显著优于现有最先进方法，且在有监督和少样本设置下均保持强鲁棒性。文中并报告了后处理对降低误报的积极影响。

Conclusion: JVLGS通过将视觉和文本模态联合，改进了气体泄漏的表征与分割，显著优于现有方法，在有监督与少样本学习下均表现稳健。

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [18] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM通过在logit与特征层分别采用投票式共识融合，解决了跨架构与跨数据集的教师模型整合问题，实现可扩展的知识迁移并显著提升无监督识别性能。


<details>
  <summary>Details</summary>
Motivation: 随着大量结构与训练集不同的预训练模型在线可得，如何在不受训练数据分布与模型架构限制的情况下有效整合这些异构模型的集体知识，以提升下游（尤其是无监督）任务表现，是一个未被充分解决的基础问题。

Method: 提出一种专门的投票机制：在logit层对能预测目标类的教师模型进行投票融合，在特征层对在任意标签空间上学习到的视觉表征进行投票融合，基于两层共识来指导学生模型训练，以处理教师模型的异构性。

Result: 实验表明UNIFORM在无监督目标识别上优于强基线方法，并展示出良好的可扩展性：随着教师数量增加（超过一百），性能持续提升，而现有方法在远小于此规模时便趋于饱和。

Conclusion: 本文提出的UNIFORM框架能够在无须对教师模型的训练数据分布或网络架构作强假设的前提下，将多个异构预训练模型的知识汇聚到单一学生模型，从而提升无监督目标识别性能，且在教师数量扩展到百余时仍能获益。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [19] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: 本文提出Sat2Flow，一种仅使用卫星影像输入的流量生成框架，解决现有方法依赖辅助特征和对区域索引重排列敏感的问题。通过多核编码器捕捉区域交互、置换感知扩散过程对齐潜在表示，并结合对比学习与等变扩散训练，保证结构一致性与拓扑鲁棒性。实验证明在数值精度和结构保持方面优于基线，适用于数据稀缺城市环境的全球扩展。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖难以普遍获取的辅助数据且对区域索引重排敏感，造成在全球或数据稀缺地区应用受限，故提出仅基于卫星影像并具备拓扑鲁棒性的OD生成方法。

Method: 提出多核编码器提取不同感受野的区域特征，采用置换感知的扩散模型在潜在空间生成OD流矩阵，并通过对比学习将卫星特征与OD模式对齐。训练时结合对比损失与等变扩散训练以保持拓扑一致性。

Result: 在真实城市数据集上，Sat2Flow在数值误差上优于物理与数据驱动基线，同时在索引重排实验中保持了经验分布和空间结构，展示了良好的拓扑不变性和可扩展性。

Conclusion: Sat2Flow能仅凭卫星影像生成具有结构一致性的OD流矩阵，摆脱对辅助数据的依赖，并在区域重编号等变性方面表现稳健，实验显示它在精度和结构保真度上超过现有基线方法。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [20] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 提出一种诊断驱动的半监督框架，通过伪标签和未标注数据缓解阴影偏差并提升召回，在低数据场景和公开基准上验证有效。


<details>
  <summary>Details</summary>
Motivation: Environmental challenges and high annotation costs degrade deep learning performance in real fields; need robust, low-data solutions.

Method: Diagnostic-driven semi-supervised pipeline for weed detection

Result: Established strong supervised baselines (ResNet, YOLO, RF-DETR) and revealed shadow bias; semi-supervised pseudo-labeling improves recall and mitigates bias, validated on public benchmark.

Conclusion: 该研究为精确农业中鲁棒视觉系统开发提供了可实践的流程：先诊断偏差再用半监督学习扩展数据多样性，从而减少漏检并改进模型泛化。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [21] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: 提出 TAPO 强化语义对齐与 MotionFLUX 实时流匹配生成，两者合力在质量、语义一致性和速度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动动作生成在语言与动作语义对齐上存在不足，且扩散类方法推理慢、需要多步去噪，无法满足实时应用需求。

Method: 提出 TAPO（Aligned Preference Optimization）通过对文本修饰词的偏好优化与迭代调整加强语言-动作语义对齐；提出 MotionFLUX，一种基于确定性整流流匹配（rectified flow matching）的快速生成框架，通过构建噪声分布与动作空间间的最优运输路径，线性化概率路径以减少多步采样需求，实现实时合成。

Result: 实验表明二者结合在语义一致性与动作质量上超越现有方法，并显著加速生成速度；作者计划开源代码和预训练模型。

Conclusion: TAPO 与 MotionFLUX 合并为统一系统，可在保持动作质量的同时提高语义一致性并显著加速生成。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [22] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: 提出首个跨视频关系推理基准CVBench（1000 QA，三层级），揭示MLLM在跨视频因果与复杂推理上的明显不足，并指出架构性瓶颈供未来优化参考。


<details>
  <summary>Details</summary>
Motivation: 尽管在单视频任务上表现强劲，现有MLLM在需要跨视频综合信息的实际应用（如多摄像头监控、跨视频学习）能力未被充分评估，因而需要专门基准来衡量并推动该能力发展。

Method: 构建了包含1000个问答的基准，分三级（对象关联、事件关联、复杂推理），涵盖五类视频集群；对10+主流MLLM（如GPT-4o、Gemini-2.0-flash等）在零-shot与链式思考提示下进行广泛评测。

Result: 实验显示显著差距：顶级模型在因果推理类任务上仅约60%准确率，而人类达到91%；分析指出模型在跨视频上下文保持与重叠实体消歧上存在根本性瓶颈。CVBench的代码与数据已开源。

Conclusion: CVBench揭示当前多模态大模型在跨视频关系推理上的显著短板，尤其是在因果/复杂推理与实体消歧上，并为后续改进提供了诊断框架。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [23] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: 在浏览器端实现可实时运行、少样本自适应的轻量化注视估计框架 WebEyeTrack，兼顾精度（2.32 cm）、延迟（2.4 ms）与隐私。


<details>
  <summary>Details</summary>
Motivation: 现有学术界的注视估计方法在基准上表现优异，但在真实应用场景中受到模型大小、推理延迟与隐私限制的制约；基于网页/网络摄像头的方法尤其在头部移动下精度不足。作者希望弥合学术成果与商用眼动追踪之间的差距，提供低延迟、隐私友好且易部署的解决方案。

Method: 在浏览器端运行轻量化 SOTA 注视估计网络，加入基于模型的头部姿态估计模块，并在设备端实施少样本（k < 9）微调/自适应策略以校准新用户；同时优化推理速度以支持移动端实时运行。

Result: 在 GazeCapture 数据集上达到 2.32 cm 的误差（宣称 SOTA），并在 iPhone 14 上实现约 2.4 ms 的实时单次推理时间；使用最多 9 个校准样本即可显著提升对新用户的适配效果。代码开源。

Conclusion: WebEyeTrack 将轻量级的最先进注视估计模型嵌入浏览器，结合基于模型的头部姿态估计与设备端少样本自适应，使得基于摄像头的注视追踪在实用性（延迟、隐私、计算量）和精度上更接近商用眼动仪。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [24] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: 用15k文本生成的伪真实图像+800张真实多视图样本及深度/法线融合伪标签与渐进训练，MonoRelief V2显著提升单图2.5D浮雕恢复的真实场景表现并达SOTA。


<details>
  <summary>Details</summary>
Motivation: MonoRelief V1仅在合成数据上训练，真实世界中材质和光照复杂导致泛化能力不足。为提高现实场景下的鲁棒性与精度，引入伪真实数据与小规模真实数据以缩小域差。

Method: 端到端网络直接从单张图像恢复2.5D浮雕；利用文本到图像生成器合成约15k伪真实图像，并通过深度与法线预测融合生成深度伪标签；构建800张真实样本集（多视角重建+细节修正）；采用渐进训练（先伪真实再真实）提升泛化与精细度。

Result: 在多个基准上显示深度和法线预测的显著提升，模型在效率和下游应用（如浮雕生成、重建等）方面具有较强潜力，且代码已开源以便复现。

Conclusion: MonoRelief V2通过引入约15k伪真实数据（text-to-image生成）和800张真实多视图重建样本，结合深度与法线融合伪标签及渐进训练策略，相比MonoRelief V1在复杂材质与光照下显著提升了2.5D浮雕恢复的鲁棒性、精度和效率，实验显示在深度与法线预测上达到或接近SOTA。

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [25] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet 对 DETR 进行解耦编码器优化，并用 GDU 与 SAA 增强场景感知与尺度适应性，实现在交通路口场景上的加速与精度双赢；并提供了新的评测集 Intersection-Flow-5k。


<details>
  <summary>Details</summary>
Motivation: 端到端（NMS-free）检测器虽有实时潜力，但计算成本高且在复杂路况（遮挡、高密度、尺度变化大）下性能受限，需要轻量且场景感知的架构改进。

Method: 基于 DETR 设计一个“解耦编码器优化”策略，并引入 Geometric Deformable Unit(GDU) 做交通感知几何建模，以及 Scale-Aware Attention(SAA) 保持极端尺度变换下的表示能力；并构建 Intersection-Flow-5k 数据集用于评测。

Result: 在新建的 Intersection-Flow-5k 上，较强基线 RT-DETR 提升 AP(test)+1.5%、AP50(test)+1.6%，同时 GFLOPs 减少63.2%、推理速度提高16.2%。数据集已开源。

Conclusion: FlowDet 在保持或提升检测精度的同时，大幅降低计算量并提高推理速度，适用于复杂路口高密度、强遮挡场景的实时检测。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [26] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 可训练编码器+动量分支稳定域适应；轻量原型提取器指导解码器精确重构；多样性感知对齐损失防止原型坍塌，提升异常定位和表示质量。


<details>
  <summary>Details</summary>
Motivation: 动机是医学图像异常检测受限于标注稀少且与自然图像存在域差异，现有依赖冻结预训练编码器的重构方法难以适应域特征，而原型方法虽可解释但易发生原型坍塌，需同时提升域适应性和原型多样性以改进定位和泛化。

Method: 方法包括：1) 可训练编码器与动量分支用于稳定学习域自适应特征；2) 轻量原型提取器挖掘规范样本原型并通过注意力引导解码器进行重构；3) 提出多样性感知对齐损失，包含多样性约束和逐原型归一化，以防止原型坍塌；整体端到端训练。

Result: 该论文提出了一种用于医学图像异常检测的统一框架，结合可训练编码器、原型引导重构和多样性感知对齐损失，解决了预训练编码器不可微调导致的域适应问题以及原型坍塌问题，实验显示在多个医学影像基准上取得了显著性能提升。

Conclusion: 通过将可训练编码器与原型引导重构和多样性约束结合，作者有效提升了医学图像异常检测的定位精度和鲁棒性，避免了原型支配性使用带来的泛化问题。

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [27] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch uses multimodal prototype-guided dual contrastive learning and a pathology-pretrained backbone to enhance semi-supervised pathological image segmentation, improving boundary modeling and overall performance.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised segmentation methods rely on perturbation-based consistency and image-only modalities, struggling to capture high-level semantic priors and handle ambiguous boundaries in pathology images; annotations are costly.

Method: Dual contrastive learning between image prototypes and pixel labels, and between text prototypes and pixel labels; replace ViT backbone in TransUNet with pathology-pretrained Uni foundation model; coarse-to-fine supervision; pixel-level contrastive learning.

Result: MPAMatch achieves superior performance on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI datasets compared to state-of-the-art methods.

Conclusion: This paper introduces MPAMatch, a semi-supervised multimodal prototype-guided contrastive learning framework for pathological image segmentation that outperforms prior methods by improving structural and semantic modeling.

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [28] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 提出针对人-物交互的定制化图像生成任务CHOI，构建相应数据并提出两阶段Interact-Custom：先生成交互掩码再在掩码引导下进行身份保持的交互图像生成，支持用户背景/位置控制，实验表明效果良好。


<details>
  <summary>Details</summary>
Motivation: 现有组合定制图像生成方法主要关注目标实体外观保持，忽视细粒度的目标间交互控制。为做到同时保持身份与控制交互，模型需将人/物分解为身份特征和受姿态驱动的交互特征，并保证合理的空间布局；而现有HOI数据集难以为此提供合适的训练样本。

Method: 1) 数据处理：构建/处理大规模数据集，使每个样本包含相同的人-物对但具有不同交互姿态，便于学会将身份与交互姿态分离。2) 两阶段模型Interact-Custom：阶段一生成前景交互掩码以显式建模人-物空间配置；阶段二在该掩码引导下生成交互图像，同时通过特征分解保持目标的身份信息。3) 可选模块：允许用户输入背景图与目标联合出现位置，增强内容可控性。

Result: 通过作者设计的CHOI专用评估指标与大量实验，Interact-Custom在身份保持、交互语义表达和空间配置控制上均表现优越；并且在可选的背景/位置控制条件下，模型能生成高可控性的交互图像。作者还可能提供消融与定量对比来验证各模块的作用。

Conclusion: 本文提出了定制化人-物交互图像生成（CHOI）任务，并设计了一个两阶段方法Interact-Custom与配套的大规模数据处理，以同时实现目标人/物的身份保持与交互语义控制。方法在空间配置建模（前景交互掩码）和在掩码引导下的身份保持生成之间拆解问题，并支持用户可选的背景与目标出现位置控制。实验证明该方法在作者定制的CHOI评估指标上有效。

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [29] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出频谱引导的深度分割复用与轻量时空网络结合的方法，解决高帧率全彩CGH的颜色串扰与计算瓶颈，实现高质量、高速全息视频。


<details>
  <summary>Details</summary>
Motivation: 解决高帧率全彩CGH中颜色串扰与计算效率的矛盾：学习模型相位过平滑导致窄带角谱和颜色串扰；逐帧优化忽略时空相关性，效率低下。

Method: Spectrum-Guided Depth Division Multiplexing (SGDDM) + HoloMamba

Result: SGDDM通过频谱引导的相位频率调制，实现高帧率下的高保真全彩显示；HoloMamba为轻量不对称Mamba-Unet，显式建模时空相关性，提升重建质量并显著加速。模拟与实测表明：SGDDM在不牺牲帧率下实现高保真全彩，HoloMamba能以>260 FPS生成FHD全彩全息视频，比此前方法快>2.6×。

Conclusion: SGDDM与HoloMamba联合可同时实现高帧率与高色彩保真及高效计算，为实用化全彩高帧率CGH提供有力方案。

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [30] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: 该论文提出Score-based Discriminator Correction (SBDC)，通过训练判别器并将其作为生成引导来纠正带噪条件扩散模型，从而提升可控性和生成质量。方法在生成早期阶段应用引导效果最好，无需重训且计算开销小。实验证明在多种噪声设置下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有大规模数据集中存在人工标注错误，然而这些噪声对条件扩散模型的生成质量和可控性影响未被充分研究。需要一种无须重训即可纠正或对齐噪声标签条件扩散模型的高效方法。

Method: 论文先训练一个判别器来区分真实/错误标签生成的样本，判别器基于对抗损失并借鉴先前噪声检测技术评估样本真实性。生成时，将判别器的评分转换为对扩散模型的score guidance，仅在生成的早期步骤应用该引导以避免过度干预。该方法与预训练扩散模型兼容，推理开销低。

Result: 在不同噪声设置（标签错误率等）下，SBDC在生成质量与条件一致性指标上均优于先前最先进方法，并显示仅在生成早期使用引导能获得更好效果，且计算开销仅有小幅增加。

Conclusion: SBDC能有效利用判别器引导来对齐有噪标签的条件扩散模型，提高生成样本的真实性和条件一致性，且只在生成早期使用引导可获得最佳效果；方法无需重新训练扩散模型，推理时仅小幅增加计算量，并在多种噪声场景下胜过现有方法。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [31] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: 通过可微NMS、深度等变主干、BEV分割与理论相机高度分析，论文系统性提升了Mono3D在遮挡、跨域、大目标和相机参数变动下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 单目3D检测在现实场景（遮挡、多数据分布、目标尺寸差异、不同相机参数）中容易失效，现有方法在这些泛化问题上表现不足，因此需要在算法设计和理论上同时改进以提升实际应用可靠性。

Method: 1) GrooMeD-NMS：将非极大值抑制设计为可微形式，以便端到端训练中优化遮挡场景下的抑制策略。2) DEVIANT：构造或训练使主干对深度变化等变的网络，以提升跨数据集和不同相机参数下的稳定性。3) SeaBird：在鸟瞰图空间进行语义/实例分割并使用dice损失，减少大目标定位受噪声影响的问题。4) 数学分析：从理论角度研究相机高度变化对Mono3D外推性的影响，并据此提出改进。

Result: 作者声称：GrooMeD-NMS在遮挡场景下提高了判别与回归一致性；DEVIANT在跨数据集迁移测试中提升精度与稳定性；SeaBird降低了大目标检测误差并提升召回；相机高度外推改进提高了模型在未见摄像机高度上的泛化性。量化细节（如数值提升、比较基线、统计显著性）需论文主体验证。

Conclusion: 本文提出了一套针对单目3D目标检测不同泛化挑战的解决方案：可微NMS（GrooMeD-NMS）提升遮挡稳健性；深度等变主干网络（DEVIANT）增强跨数据集泛化；面向大目标的BEV分割与dice损失（SeaBird）降低噪声敏感性；并对模型在未见相机高度下的外推性给出数学分析与改进方法。总体结论是多方面方法能显著改善Mono3D在复杂、未见分布下的表现。

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [32] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 对YOLO不同量化精度和退化感知校准在真实退化下的鲁棒性进行了评估，发现退化感知量化在大多数情况下未显著优于标准校准，但在大模型和特定噪声情形下有益。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署目标检测器需依赖后训练量化，但需评估量化后模型在噪声、模糊和压缩等真实退化下的鲁棒性与校准策略效果。

Method: 在COCO上对YOLO从nano到XL不同规模模型做FP32/FP16/Dynamic UINT8/Static INT8多精度评测，并设计混合退化的校准集用于TensorRT静态INT8校准，比较七种退化及混合情形下的mAP性能与推理速度。

Result: 详尽的实证研究

Conclusion: 静态INT8在提高速度上有明显优势，但通常会在清洁数据上带来中度精度损失；退化感知校准并未普遍提高鲁棒性，效果依赖模型规模与退化类型。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [33] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 论文通过引入拉普拉斯先验驱动的逆演化层到扩散生成与分割解码器，并辅以频域的多尺度融合，提升了合成数据质量并抑制伪影传播，从而显著改善了跨域语义分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的合成训练数据常含结构或语义缺陷，直接使用会导致分割模型性能下降与错误累积。需要一种能够识别并抑制这些缺陷的机制，同时防止伪影在分割网络内传播，从而提高在未见域上的泛化能力。

Method: 1) 在扩散模型的生成流程中嵌入逆演化层（IELs），利用拉普拉斯基准突出空间不连续性与语义不一致，以过滤不良生成模式，形成IELDM数据增强管线；2) 将同样的IEL机制嵌入语义分割模型的解码器以抑制伪影传播，提出IELFormer；3) 在IELFormer中加入多尺度频率融合（MFF）模块，通过频域分析有序整合多分辨率特征以增强跨尺度语义一致性。

Result: 提出的IELDM能够生成更高质量的增强图像，IELFormer在多项跨域分割基准上取得了优于现有方法的泛化性能。实验显示，通过抑制生成与网络内部的伪影，整体分割精度与跨域稳健性均得到显著提升。

Conclusion: 通过在生成与分割网络中引入基于拉普拉斯先验的逆演化层（IELs），论文提出的IELDM（用于高质量扩增的扩散框架）和IELFormer（在解码器嵌入IEL并加入多尺度频率融合模块）能够抑制扩散生成图像中的结构/语义缺陷、降低伪影传播并提升跨域语义分割的泛化能力。大量基准实验证明该方法在通用性上优于现有方法。

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [34] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: 提出LF-VAR，结合量化病灶测量与类型条件，通过VQVAE+VAR实现可控高质量皮肤图像合成，FID领先SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实临床皮肤图像有限，现有合成方法质量低且难以控制病灶位置与类型，需可控且临床相关的合成数据。

Method: 训练多尺度病灶聚焦VQVAE将图像编码为离散潜在表示，然后用VAR Transformer在token上自回归生成图像，病灶测量和类型作为条件嵌入加入。

Result: 在七种病灶类型上的平均FID为0.74，比先前SOTA提升6.3%；模型能根据语言提示生成指定病灶特征的高保真图像。代码开源。

Conclusion: LF-VAR通过将量化的病灶测量分数和病灶类型标签作为条件，引导多尺度病灶聚焦VQVAE与可视自回归（VAR）Transformer联合训练，实现可控高保真皮肤图像合成。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [35] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: 提出一种结合难度感知损失和专家间去中心化路由的长尾识别框架，能显著改善稀有与难学类别的识别效果。


<details>
  <summary>Details</summary>
Motivation: 传统按频率重加权只考虑类别样本不平衡，但忽略了类别本身的学习难度。为同时应对类别不平衡与类别间难度差异，需引入难度建模并结合灵活的专家分配机制。

Method: 基于预测不确定性与历史性能估计类难度，采用自适应损失加权；架构上采用多个专家（Mixture-of-Experts），每个专家专注于类别分布的不同区域；推断时利用专家特有的OOD检测器生成置信度对专家预测加权，实现无需集中路由器的输入自适应路由；端到端联合训练所有组件。

Result: 在标准长尾基准上，DQRoute显著提升整体性能，尤其在稀有类和难学类上有明显改进，证明了难度建模与去中心化专家路由的协同优势。

Conclusion: DQRoute通过将难度感知的优化与基于专家的动态协作相结合，有效提升了长尾视觉识别性能，尤其是在稀有和难学类别上。

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [36] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT用点级可重排token和频谱增强的序列模型，结合多代理精细对齐，实现更准确且高效的协作点云感知。


<details>
  <summary>Details</summary>
Motivation: 现有基于BEV的中间表示丢失重要的3D细粒度结构信息；点云无序、大量且对位置敏感，需要一种紧凑且对齐的点级表示来提升识别与定位精度。

Method: 引入点级优化tokens，构建点云原生处理流水线：1) 语义感知的1D token重排模块，根据场景与token语义生成自适应序列；2) 频谱增强的状态空间模型捕捉空间和谱域的长程依赖；3) 邻居到自车的闭环对齐模块结合全局代理级校正与本地token级精调以降低定位噪声。

Result: 在仿真与真实数据集上，CoPLOT在精度、通信与计算成本上均超越最先进方法；论文承诺开源代码。

Conclusion: CoPLOT提出使用点级(token)作为协作感知的中间表示，通过语义感知的重排、频谱增强的状态空间序列建模和邻居到自车的对齐模块，有效保留3D结构信息并减小通信开销，实验表明优于现有方法。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [37] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 无监督骨架+ASTGCN预训练+基于嵌入的ADM曲率拐点检测，实现了轻量且实时的细粒度运动边界定位，性能接近监督SOTA且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 细粒度动作定位在未修剪的体育视频中难度高，传统监督/弱监督方法依赖大量标注和大模型，计算耗费高且适应性差；因此需要一种轻量、实时且不依赖手工标签的解决方案。

Method: 在无标签的姿态序列上进行块式去噪预训练以学习动作动力学；使用注意力增强的时空图卷积网络（ASTGCN）提取低维时空嵌入；在推理时直接从这些嵌入计算ADM，检测曲率曲线的拐点作为动作起止点。

Result: 在DSV Diving数据集上实现mAP 82.66%与平均定位延迟29.09 ms，与最先进的监督方法性能相当，并能在未见的实景跳水视频上无需重训即可稳健泛化。

Conclusion: 提出了一种轻量级无监督骨架动作定位流水线：通过在块状分割的骨架序列上以去噪任务预训练基于注意力的时空图卷积网络（ASTGCN），并在推理阶段基于低维ASTGCN嵌入计算动作动力学度量（ADM），通过曲率拐点检测动作边界，达到了与监督方法相当的性能。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [38] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 通过局部/全局统计引导的像素级可变核与迭代滤波，本文提出的超小模型在未见噪声上也能实现高质量去噪，降低过拟合并提高实用性。


<details>
  <summary>Details</summary>
Motivation: 现有深度去噪方法对特定噪声分布依赖强、易过拟合且对未见噪声泛化能力差；用大量数据和高算力缓解仍不足，因此希望设计能在低参数、低计算开销下提升对未见噪声的鲁棒性。

Method: 提出一个紧凑网络结构：先用Feature Extraction Module提取噪声不变特征，再通过Global Statistics Module与Local Correlation Module分别捕捉全局噪声统计与局部结构相关性，接着Kernel Prediction Module基于这些信息为每像素生成可变卷积核，最后以迭代方式应用这些核进行去噪，整个过程采用高效运算以保持模型小且快速。

Result: 尽管只在单级高斯噪声上训练，该紧凑模型在多种噪声类型和噪声强度下表现优越，显示出迭代动态滤波在实际去噪中的潜力，兼顾效率与恢复效果。

Conclusion: 采用动态生成的像素可变卷积核并通过迭代滤波，可以在参数极少（约0.04M）的情况下实现对未见噪声类型和不同噪声强度的鲁棒图像去噪，减少过拟合并兼顾效率与恢复质量。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [39] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: 提出Video-LevelGauge用于检测LVLM的位置信息偏置，构建了严格的数据与分析流程，发现开源模型多有明显偏置而商用模型更稳健，并给出缓解路径。


<details>
  <summary>Details</summary>
Motivation: 现有评测多关注视频整体理解能力，忽视了‘上下文位置信息偏置’这一关键但被低估的问题——模型可能基于位置而非真实语义做出判断，需一个专门基准来系统检测与量化该偏置。

Method: 作者设计了标准化探针和可定制的上下文构造，灵活控制上下文长度、探针位置和上下文类型来模拟多样化场景；同时提出了将统计测量与形态模式识别相结合的综合分析方法。数据集由438段人工筛选的视频组成，包含1177道多选题和120道开放式问题，用以揭示位置信息偏置，并在27个主流LVLM（商用与开源）上进行评估。

Result: 基准成功暴露出显著的位置信息偏置：多数开源模型表现出头部或邻近内容偏好；商用模型整体表现更均衡。进一步关于上下文长度、上下文变化和模型规模的消融分析给出可操作的缓解建议与模型改进方向。

Conclusion: 本文提出了Video-LevelGauge基准，用于系统评估大型视频语言模型（LVLM）的位置信息偏置，结论是许多开源LVLM在视频序列中存在明显的位置信息偏好（如偏向序列头部或邻近内容），而部分商用模型（例如Gemini2.5-Pro）能在整个序列上保持更稳定的表现。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [40] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: 通过云车协同的ODAL框架并对轻量模型微调，作者在车内物体检测与定位上实现了高效且更可靠的性能，指标优于GPT-4o，但需补充评测细节与实际部署分析。


<details>
  <summary>Details</summary>
Motivation: 车内个人助手和安全应用需要可靠的物体识别与定位，但车载系统的计算资源有限，无法直接部署大型视觉基础模型，因此需要一种在算力受限环境下仍能利用强大视觉模型能力的方案。

Method: 提出分布式架构：将视觉基础模型的重计算任务放在云端，车载端负责轻量前处理与后处理；引入ODALbench作为检测与定位的综合评估指标；对比GPT-4o与LLaVA 1.5 7B，并对LLaVA进行微调以适应车内物体检测/定位任务。

Result: 微调后的ODAL-LLaVA取得ODAL_score 89%，较基线提升71%，并较GPT-4o高约20%；同时检测精度保持较高且显著降低幻觉，ODAL_SNR约为GPT-4o的三倍。

Conclusion: 提出的ODAL框架在车内场景理解上具有潜力，通过云端-车载分布式架构规避了车载算力限制，并通过微调轻量模型（LLaVA 1.5 7B）获得了显著性能提升。作者给出结果表明微调后的ODAL-LLaVA在ODAL_score和SNR上均优于未微调模型及GPT-4o。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [41] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1用模型自身生成的可独立回答问题的视觉感知作为奖励信号，结合最终输出监督，通过强化学习增强视觉理解，降低幻觉与依赖文本先验。


<details>
  <summary>Details</summary>
Motivation: 现有VLM后训练只监督最终输出且多依赖文本匹配，导致视觉信号稀疏使模型依赖语言先验，外部视觉监督昂贵或引入分布偏差，需一种无需外部标注的自我监督改进策略。

Method: 方法上先让VLM生成自包含的视觉感知描述，然后用同一模型仅基于该描述进行语言推理并计算自奖励，与最终答案监督结合进行训练。

Result: Vision-SR1提出一种自我奖励的增强训练方法，通过将视觉推理分解为视觉感知和语言推理两阶段，利用模型自身生成的视觉描述来作为再输入重测以计算奖励，从而无需外部视觉监督改善视觉推理，减少视觉幻觉和语言捷径问题。

Conclusion: Vision-SR1在多种视觉-语言任务上有效提升视觉推理能力、抑制视觉幻觉并减少语言捷径，且不依赖人工标注或外部模型标签。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [42] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: 在卫星位置回归任务中，SNN能达到与CNN相当的精度，但其能效优势仅在特定硬件与高稀疏输入下成立，评估时需透明披露硬件假设与数据特性。


<details>
  <summary>Details</summary>
Motivation: 当前文献往往宣称SNN在能效上优于ANN，但新研究表明这一结论依赖于实现细节，作者欲在实际回归任务与更现实的能耗模型下重新审视SNN的能效声誉。

Method: 构建基于LIF神经元的SNN用于多输出回归（单目图像估计卫星三维位置），最后一层使用膜电位训练；与参考CNN在同一光真实卫星数据集上比较；采用硬件不可知与硬件感知两种能耗估计方法，并分析暗像素比例与输入稀疏性对能耗的影响。

Result: SNN在MSE上与CNN可比；硬件不可知模型预测SNN节能50–60%；但硬件感知分析显示，仅在神经形态硬件且输入高度稀疏时才能获得显著节能；暗像素比例显著影响能耗估计。

Conclusion: SNN的能效取决于实现平台和输入稀疏性，不能笼统认为在所有数字平台上均优于ANN；必须在硬件感知的前提下评估并公开假设。

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [43] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 针对UWF图像的模糊与光照问题，本文用频率解耦去模糊+Retinex光照补偿的自监督框架，保留细节并提高诊断效果。


<details>
  <summary>Details</summary>
Motivation: UWF图像常受模糊和不均匀照明影响，现有增强方法难以兼顾UWF的病变细节保存需求。

Method: 提出频率解耦去模糊模块（含不对称通道融合以整合高低频信息）和基于Retinex的光照补偿模块（含色彩保持单元与多尺度时空频率信息），采用自监督训练策略。

Result: 实验表明方法在恢复细节、纠正光照不均方面优于现有方法，并能提升下游疾病诊断性能。

Conclusion: 本文提出的频率感知自监督UWF图像增强方法有效提升视网膜图像可视化质量并有助于疾病诊断。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [44] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT通过监督特征正则化融合多种几何先验，利用在线动画增强扩充数据，从单张图像生成更高质量且视角一致的纹理化3D人体模型。


<details>
  <summary>Details</summary>
Motivation: 单视图存在几何歧义且3D训练数据稀缺，现有方法难以有效融合多模态先验导致视角不一致与面部畸变，需统一融合先验并扩充数据以提高重建质量。

Method: 两阶段框架：1) 统一学习先验几何并用监督特征正则化（多视图网络提供中间特征作为监督）促进融合；2) 在线动画增强模块通过轻量级动画网络在线生成大量带姿态变化的训练样本；最终输出纹理化3D人体。

Result: 提出SAT框架，通过统一学习多种先验几何并引入监督特征正则化与在线动画增强，提升单目纹理化3D人体重建的一致性与质量。

Conclusion: SAT能更好融合SMPL、法线图等多模态几何先验，并通过多视图中间特征监督与在线动画数据扩增提高重建精度和视角一致性，优于现有方法。

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [45] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: Map deep features to nodes of a QC-LDPC graph, set edge couplings via pairwise similarities at Nishimori temperature to form RBIM; real images show spectral gaps in Bethe-Hessian indicating community structure, synthetics do not — enabling unsupervised detection.


<details>
  <summary>Details</summary>
Motivation: Supervised detectors fail to generalize to unseen generators and are brittle; need unsupervised, model-agnostic method leveraging intrinsic statistical differences between real and synthetic images.

Method: Spectral community detection on QC-LDPC graph using RBIM at Nishimori temperature

Result: Unsupervised detector using Bethe-Hessian spectrum of RBIM on LDPC graph achieves >94% accuracy on cat/dog and male/female tasks across real and GAN/diffusion images without labeled synthetic data.

Conclusion: Physics-inspired, unsupervised, model-agnostic detector effectively discriminates real vs synthetic images via spectral gaps; contributions include LDPC embedding, analytical Nishimori-RBIM to Bethe-Hessian link, and robust detector; aim to extend to video and multi-class anomalies.

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [46] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS adds label-awareness to 3D Gaussian Splatting via occlusion-aware lifting of 2D semantics to 3D Gaussians, conflict filtering, and random region sampling, achieving better segmentation and much faster training.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting provides fast, high-quality rendering but lacks semantic segmentation ability, constraining scene understanding and downstream tasks that require object-level isolation.

Method: Augment each 3D Gaussian with semantic labels and enforce cross-view consistent semantic masks; introduce an Occlusion Analysis Model to prevent overfitting occluded regions; use a Main Gaussian Labeling model to lift 2D semantic priors to 3D Gaussians; apply a Gaussian Projection Filter to resolve label conflicts; and use random region sampling to accelerate optimization.

Result: LabelGS outperforms previous methods (e.g., Feature-3DGS) on 3D scene segmentation benchmarks, achieves significant efficiency gains (reported 22× training speedup at 1440×1080), and provides effective decoupling of Gaussian representations for improved optimization.

Conclusion: LabelGS successfully extends 3D Gaussian Splatting with label-aware capabilities, enabling accurate and efficient 3D scene segmentation while maintaining high-fidelity rendering.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [47] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 将VPS改为track-by-detect并用两种无训练模块修正SAM2的空间误差与时间误差积累，从而实现稳定且具域泛化性的长时视频息肉分割与追踪。


<details>
  <summary>Details</summary>
Motivation: 现有VPS方法在时空建模和域泛化之间难以兼顾，且SAM2直接用于长序列追踪时会出现误差累积（雪球效应），影响分割稳定性和临床可用性。

Method: 将SAM2改造为无需训练的视频息肉分割器，设计两种训练自由模块：1) Intra-association filtering（帧内关联过滤）用于去除检测阶段引入的空间误差并减少假阳性；2) Inter-association refinement（帧间关联精化）用于自适应更新记忆库以防止随时间的错误积累，从而增强时间一致性。整体为track-by-detect流程，利用IPS进行检测，SAM2进行分割并通过两模块稳定结果。

Result: 引入的两模块协同工作后，稳定了SAM2在长时视频分割中的表现，在域内和域外测试中均达到了领先的性能，并展示了在长未剪辑结肠镜视频中鲁棒的追踪能力，具备临床分析潜力。

Conclusion: 本文提出将VPS重塑为track-by-detect范式，通过结合IPS的空间上下文和SAM2的时间建模能力来提升视频息肉分割的时空性能与域泛化能力。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [48] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 利用面部自监督基础模型（FSFM）并结合多数据集微调、三元组损失和归因监督，可显著提升视频deepfake检测的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决deepfake在真实世界数据中泛化差的问题，通过利用面部基础模型学习到的丰富人脸表征来提高视频deepfake检测的鲁棒性和泛化能力。

Method: 基于FSFM进行微调，训练集由多种face-swap和face-reenactment数据集组成；在训练中加入三元组损失变体以增强嵌入区分性；探索将deepfake按操控类型或数据来源进行分类的归因式监督，并对比多数据集组合的效果和泛化性。

Result: 在基于FSFM（自监督训练的面部模型）并使用来自多种deepfake数据集的集合微调后，结合三元组损失变体和基于归因的监督方案，模型在多个评测基准上显示出更好的泛化性，尤其在真实场景下表现显著提升。

Conclusion: 基于面部基础模型并结合三元组损失和操作类型/来源的归因监督可以提高deepfake检测模型的判别能力和泛化性能，适用于现实世界场景。

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [49] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: 本文提出POEv2，通过改进像素方向估计从边缘强度图提取线段，形成可与任意边缘检测器配合的通用框架，并在三项公开数据集上达到了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法分为两类：通用线段检测器与专注几何且大空间支持的线框检测器。两类设计目标不同，互相迁移效果不佳，需一个能兼顾二者的鲁棒框架。

Method: 在原有像素方向估计（POE）方法上做改进，提出POEv2。该方法从边缘强度图(edge strength maps)估计像素方向并将像素聚合为线段，可与任意边缘检测器结合以生成输入。

Result: 将POEv2与高效边缘检测器结合后，在三个公开数据集上实现了最新性能（state-of-the-art）。

Conclusion: POEv2是一个统一且鲁棒的线段检测框架，能够同时适用于通用线段检测（generic）与线框线段检测（wireframe）。

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [50] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: 该论文提出了面向光场显著性目标检测的自提示分割模型SPLF-SAM，包含统一多尺度特征嵌入块(UMFEB)用于捕捉不同尺寸目标和多尺度自适应滤波适配器(MAFA)用于频域去噪，提升小目标检测性能；在多个基准上优于十个SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有LF SOD方法往往忽视提示信息的提取及频域分析，导致小目标被噪声淹没，故提出结合自提示和频域滤波的改进方法。

Method: 构建自提示框架在SAM基础上：设计UMFEB进行多尺度特征嵌入以识别不同尺寸目标；设计MAFA在多尺度上学习频域特征以抑制噪声并保留小目标信息；将两者结合用于细化SAM输出并进行显著性分割。

Result: 在多个光场显著性检测基准上，SPLF-SAM在精度和鲁棒性上超过了十个现有SOTA方法（具体指标未在摘要给出）。

Conclusion: SPLF-SAM通过引入自提示机制、UMFEB和基于频域的MAFA，有效提高了光场显著性目标检测中对多尺度和微小目标的检测能力，实验结果优于10个SOTA方法。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [51] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar用一个大规模变换器和多粒度引导编码与增量高斯聚合，实现了面向多种日常录制场景的快速、可增量的3D Gaussian Splatting头像重建，在速度和质量间取得良好折衷。


<details>
  <summary>Details</summary>
Motivation: 现有3D头像重建方法要么优化慢、对输入质量敏感、要么不能充分利用零散或增量数据。需要一种快速、可扩展并能灵活利用不同记录类型（单图、多视角、视频）的统一解法。

Method: 核心为Large Gaussian Reconstruction Transformer，包含三大设计：1) VGGT风格的变体Transformer架构，通过注入初始3D提示（canonical 3DGS表示）聚合多帧线索；2) 多粒度引导编码（相机位姿、FLAME表情、头部姿态）用于缓解动画/表情引起的对齐问题，支持可变长度输入；3) 基于地标跟踪的增量高斯聚合与切片融合损失（sliced fusion losses），实现逐步融合与质量-速度可调。

Result: 作者声称FastAvatar在秒级重建高质量3DGS模型，较现有方法在质量上更好且速度具有竞争力，并支持随输入数量增加而逐步提升重建效果（增量重建）。在大量实验中展现了该方法的有效性。

Conclusion: FastAvatar提出一种统一的、前向推理的3D头像重建框架，能够在秒级内从单张图像、多视角或单目视频重建高质量的3D Gaussian Splatting模型，并支持增量式融合以随观测增加而提高质量。总体结论是该方法在质量和速度之间实现了良好权衡，提升了数据利用率并降低了对高质量输入的敏感性。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [52] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet 是一个面向传粉昆虫的小目标检测数据集，采用 256×256 切图与人工核验标注，RF-DETR 基线在两个主要类上表现优异，但总体 mAP 与未识别类暴露出标注与样本不平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 为实现可扩展、自动化的传粉昆虫监测，提供大规模、真实农业现场采集的标注图像基准，以推进小目标检测、噪声标签下的类别分离和生态计算机视觉研究。

Method: 使用外部数据训练的 YOLOv12 生成初始标注，随后用开源标注工具由人工核验并微调；图像裁切为 256×256 tiles 以增强小物体检测；基线采用 RF-DETR（transformer-based）目标检测器并报告 F1 与 mAP 指标。

Result: 数据集含 7856 张经人工核验的图像、8000+ 个注释实例，模型在蜜蜂/大黄蜂上取得 F1 分别为 0.94/0.92，mAP@0.50 最佳为 0.559；未识别类表现较差并带来标签歧义问题。

Conclusion: BuzzSet 提供了一个用于农业现场小目标（蜜蜂/大黄蜂/未识别昆虫）检测的大规模高分辨率图像数据集，并通过 transformer 基础的检测器给出了强基线结果，展示了在蜜蜂分类上的高 F1 分数，但在未识别类与总体 mAP 上存在挑战。

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [53] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出AIM，通过把主导模态的欠优化参数分离为辅助块并按深度自适应调制，实现不牺牲任一模态的均衡多模态学习，实验上优于现有方法并具广泛泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有不平衡多模态学习方法常通过削弱主导模态来提升弱模态，导致整体性能受损。作者发现这一问题源于网络内部存在的“优化偏置”（参数与深度间优化状态差异），这一因素通常被忽视，因此提出有针对性的网络内部调制策略。

Method: AIM将未充分优化的、属于主导模态的参数解耦为辅助块（Auxiliary Blocks），并在联合训练时鼓励模型更多依赖这些性能受损的块以与弱模态配合；同时，AIM在网络不同深度评估模态不平衡程度，并据此自适应调整每一深度的调制强度。

Result: 实验结果表明，AIM在多项基准上优于现有最先进的不平衡模态学习方法，且在不同骨干网络、融合策略和优化器上具有良好泛化性，能同时提升主导模态与弱模态的表现。

Conclusion: 本文提出的AIM（Adaptive Intra-Network Modulation）通过在网络内部针对不同参数与深度的优化状态进行自适应调节，解决了以往方法为提升弱势模态而抑制强势模态的问题，从而在不损害任一模态的情况下实现均衡的多模态学习。

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [54] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: 本文通过自动标注与模块化结构识别，实现在保证LaTeX生成性能的同时恢复笔迹与符号的显式对齐，提升可解释性并在CROHME-2023上有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管现代编码器-解码器架构和大模型在LaTeX生成上表现优秀，但缺乏显式的符号与笔迹对齐，限制了错误分析、可解释性和需要空间意识的交互式应用。

Method: 1) 自动标注系统：用神经网络将LaTeX方程映射到原始笔迹，自动生成符号分割、分类和空间关系的标注；2) 模块化结构识别：独立优化分割、分类、关系预测，结合图形化trace排序、卷积-递归混合网络和基于Transformer的纠正模块。

Result: 基于自动标注增强的数据集，系统在CROHME-2023基准上取得竞争性结果，并生成完整的trace—符号连接图以便透明的错误分析与可解释输出。

Conclusion: 提出的结构化识别方法能够生成将手写笔迹(trace)直接映射到预测符号的完整图结构，提升可解释性与错误分析能力，并在CROHME-2023上取得具有竞争力的性能。

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [55] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: 针对变形3D高斯体在高度动态区域表现欠佳的问题，MAPo按动态性递归分区并为高动态部分复制变形网络，配合跨帧一致性损失，显著提升渲染细节与连续性。


<details>
  <summary>Details</summary>
Motivation: 单一统一的变形场难以表示高度动态区域的多样运动模式，导致模糊和细节丢失，因而需要按动态性分化建模以提升精细运动表示。

Method: 提出基于动态分数的递归时间分区策略，将高动态3D高斯体按时间段分割并为每段复制变形网络；对低动态3DG视为静态以节约计算；并引入跨帧一致性损失以消除分区处的伪影。

Result: 在多组实验中，MAPo在复杂快速运动区域的渲染质量优于基线方法，同时保持相当的计算开销。

Conclusion: MAPo通过基于动态性分区和为高动态3D高斯体复制专用变形网络，有效提升动态场景重建的细节和渲染质量，同时通过跨帧一致性损失缓解分区边界处的视觉不连续性。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [56] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: 提出StableIntrinsic，一种一步扩散模型结合像素级损失与Detail Injection Network，用于多视角材质估计；比多步扩散方法更快、方差更小且细节更好，albedo PSNR提升9.9%，metallic/roughness MSE分别降44.4%/60.0%.


<details>
  <summary>Details</summary>
Motivation: Multi-step diffusion methods for material estimation are slow and stochastic, producing high-variance predictions that conflict with the deterministic nature of material estimation tasks. A faster, deterministic, low-variance approach is needed.

Method: Introduce a one-step diffusion framework trained with multiple pixel-space losses tailored to different material properties, and a Detail Injection Network (DIN) to recover high-frequency detail lost by VAE encoding and to sharpen predicted material maps.

Result: Achieves a 9.9% PSNR improvement on albedo and reduces MSE by 44.4% for metallic and 60.0% for roughness compared to state-of-the-art; produces sharper, lower-variance material parameter maps.

Conclusion: StableIntrinsic demonstrates that a one-step diffusion model can effectively perform multi-view material estimation with lower variance and better-detail preservation compared to multi-step diffusion baselines.

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [57] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 研究发现预训练扩散模型难以准确生成包含多种颜色属性的图像，现有推理时调整和编辑方法效果有限；作者提出一种专门的图像编辑技术，显著改善多颜色提示的语义对齐


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理包含多个对象和颜色属性的复杂提示时，常出现语义错位；颜色作为可量化且常见的属性，为研究和评估对齐问题提供良好场景。

Method: 通过对颜色提示进行定向图像编辑，调整生成图像中与文本颜色属性相关的注意力/像素分布（具体细节未在摘要给出），并在多个评估指标上验证改进

Result: 提出针对多对象多颜色语义错位的问题，设计了一种图像编辑方法以提升文本到图像生成中颜色属性的对齐性

Conclusion: 该方法在多种扩散文本到图像生成器上均显著提高了多颜色提示的对齐性能，优于现有推理调整与编辑方法

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [58] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 本文提出在编码器-解码器框架上加入综合注意力块、Mamba注意力和PCA数据融合，以处理多通道光谱图像，实现了对多种数据源（RGB/多/超光谱）更高精度的废弃物分类。


<details>
  <summary>Details</summary>
Motivation: 废弃物流复杂多变，传统视觉方法难以稳定区分非生物降解材料；利用多光谱或超光谱信息并在网络中设计注意力与融合模块可提升分类准确性与鲁棒性。

Method: 基于编码器-解码器主干；在解码器中加入综合注意力模块（卷积+上采样融合细粒度/语义特征）；并行集成Mamba注意力模块对特征重新加权；对超过三通道的图像先用PCA降维到3通道再通过Data Fusion Block融合到网络输入/中间层。

Result: 提出了一种改进的编码器-解码器神经网络用于固体非生物降解废弃物的自动分类，主要创新包括：解码器内的综合注意力模块（结合卷积与上采样以优化特征表示）、Mamba注意力架构的并行引入以提升性能、以及用于多通道（>3通道）图像的主成分分析（PCA）数据融合模块，将高维光谱降至三维用于后续处理。模型在RGB、超光谱、多光谱及RGB+超光谱组合数据上进行评估，结果显著优于现有方法。

Conclusion: 综合注意力块和Mamba注意力结合PCA数据融合可显著提升多源光谱图像下的废弃物分类性能，证明该结构在各种光谱数据（尤其是RGB+超光谱）上均能超越现有方法。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [59] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 通过在RTMDet-S上应用多域训练、均衡采样、精心增强和针对性负样本挖掘等训练技巧，实现了兼顾速度与准确性的实时有丝分裂检测，在多数据集验证和MIDOG2025测试中取得了良好F1表现。


<details>
  <summary>Details</summary>
Motivation: Improve robustness and speed of mitotic figure detection across scanners, stains, tissue types and artifacts to enable clinical deployment

Method: RTMDet-S single-stage detector with bag of tricks (multi-domain training, balanced sampling, augmentations, hard negative mining)

Result: Grouped 5-fold CV F1: 0.78–0.84; MIDOG 2025 preliminary test F1: 0.81, outperforming larger models

Conclusion: 提出的方法在速度和准确性间做出实用折中，适合临床部署，且在不熟悉领域具有良好适应性。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [60] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: CSSL通过基于输入分布的上下文感知阈值动态调控神经元激活，天然降低激活密度，无需显式稀疏损失，实现在活动极稀疏下的高性能事件视觉任务处理。


<details>
  <summary>Details</summary>
Motivation: 提升事件相机数据处理在资源受限边缘设备上的效率，利用事件数据稀疏性和神经形态计算节能优势，同时避免SNN在复杂视觉任务上性能不足以及手动调参稀疏约束的问题。

Method: Context-aware Sparse Spatiotemporal Learning (CSSL)

Result: 在事件驱动目标检测和光流估计任务上，CSSL在保持极高神经元稀疏性的同时，达到了与或优于现有最先进方法的性能。

Conclusion: CSSL为将事件相机与神经形态处理器结合用于高效边缘感知提供了可行路径，通过动态阈值机制自然产生高稀疏性并维持或提升任务性能。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [61] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS为无监督VIS提出质量感知自训练，通过伪标签和自动质量评估闭环缩小合成与真实域差，获得SOTA性能（52.6 AP50）。


<details>
  <summary>Details</summary>
Motivation: 解决视频实例分割在标注上的困难，减少对人工标注的依赖，通过质量引导的自训练从合成数据向真实视频自适应。

Method: 基于合成数据生成初始伪标签，设计自动质量评估器对伪标签打分，并将高质量伪标签用于自训练模型；迭代形成闭环，逐步适应真实视频分布。

Result: 提出AutoQ-VIS框架，通过伪标签生成与自动质量评估的闭环推动合成到真实的逐步适应。在YouTubeVIS-2019验证集上取得52.6 AP50，超越VideoCutLER 4.4%。

Conclusion: 质量感知的自训练可有效解决无监督VIS中的域差问题，使得无需人工标注也能达到甚至超过先前方法的性能。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [62] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: ERSR结合自适应伪标签过滤、椭圆形状先验与对称性一致性正则，能在少量标注下实现稳健且高精度的胎儿头部超声分割。


<details>
  <summary>Details</summary>
Motivation: 超声图像质量差且标注稀缺，现有半监督方法难以生成可靠伪标签并捕获头部的形状特征，导致分割性能受限。

Method: 提出三大模块：1) 双评分自适应过滤（基于边界一致性和轮廓规则性）评估并筛选教师网络输出；2) 椭圆约束伪标签精炼（最小二乘拟合椭圆，增强椭圆中心像素、抑制噪声）；3) 基于对称性的多重一致性正则（对扰动图像、对称区域及预测与伪标签间一致性约束）。

Result: 在HC18与PSFH两数据集上取得SOTA：HC18在10%/20%标注下Dice为92.05%/95.36%；PSFH为91.68%/93.70%。

Conclusion: ERSR通过自适应过滤、椭圆约束伪标签精炼与对称性一致性正则化，有效改善了半监督胎儿头部超声分割的伪标签质量与形状鲁棒性，显著提升了在少量标注下的分割性能。

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [63] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 提出一种无需目标域信息的校准框架：低频过滤+基于梯度的ID校正约束，从频域视角减少对高频域不变性的依赖，在分布迁移下显著提升置信度校准且保持ID性能。


<details>
  <summary>Details</summary>
Motivation: 深度网络在分布迁移下常因高频视觉线索被扰动而产生过度自信的错误预测；现有方法需目标域样本或其模拟，限制实际应用。作者希望设计无需目标域访问的校准策略，通过依赖更稳定的低频特征来提升迁移下可靠性。

Method: 从频域角度对输入图像应用低频滤波以去掉高频扰动，并在训练中引入一个基于梯度的修正项，作为硬约束保证模型在原训练分布上的校准误差不增加。训练目标结合分类损失、低频增强项和ID校准梯度约束。

Result: 在CIFAR-10/100-C与WILDS等数据集上的实验表明，该方法在迁移情况下显著降低ECE等校准指标，同时在ID数据上保持较好的准确率与校准表现。

Conclusion: 该方法在不访问目标域的前提下，通过低频滤波降低对易受扰动高频信息的依赖，并用梯度约束确保ID校准，从而在多种合成与真实迁移数据集上显著改善迁移下的校准表现，同时保持或轻微提升ID性能。

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [64] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出机器中心IQA范式并发布2.5M样本数据库与区域感知评估模型，显著优于HVS指标，但对背景退化与微小失真仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 机器视觉在不良视觉条件下性能下降严重，现有以人类视觉为中心的IQA无法反映对下游机器任务的影响，亟需一种能量化图像退化对机器性能影响的评估方法与数据集。

Method: 构建了大规模机器中心图像质量数据库(MIQD-2.5M)，覆盖75个视觉模型、250类退化、三类任务；提出区域感知的RA-MIQA模型，通过细粒度空间退化分析估计对一致性(consistency)和精度(accuracy)的影响，并与7种HVS指标及5种重新训练的经典骨干模型对比评测。

Result: RA-MIQA在多维度上优于HVS指标和重新训练骨干，示例性结果包括在图像分类任务上一致性SRCC提升13.56%，精度SRCC提升13.37%；同时揭示了任务特异性的退化敏感性。研究也指出HVS指标和现有MIQA模型在背景退化、精度导向估计及轻微失真方面表现不足。

Conclusion: 提出了面向机器视觉系统(MVS)的图像质量评估框架，强调传统以人类视觉系统(HVS)为中心的IQA方法不足以预测机器感知性能，需迁移到机器中心的评估范式。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [65] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出一个两阶段统一框架：先预测手部轨迹，再用因果交叉注意将动作信号引入LDM进行逐帧视频生成，实现动作与视觉后果联合预测，在Ego4D等数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Existing models either predict actions without modeling visual consequences or predict visuals without conditioning on specific actions, leading to implausible or inconsistent future frames; need a unified model for egocentric scenarios to support both understanding and robotic planning.

Method: Two-stage predictive framework with hand-conditioned LDM

Result: A unified two-stage model: (1) consecutive state modeling predicts future hand trajectories from heterogeneous inputs; (2) causal cross-attention fuses modalities to condition a Latent Diffusion Model for frame-by-frame video generation; outperforms SOTAs on Ego4D, BridgeData, RLBench for action prediction and video synthesis.

Conclusion: 该方法首次在同一模型中联合预测即将发生的动作及其视觉后果，支持人类活动理解与机器人操作，并在多数据集上验证了其有效性。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [66] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: 提出双分支架构：局部KNN卷积网络（KCN）保留精细几何，全球图卷积网络（GCN）捕捉长程结构，并用条件分支编码临床变量和时间间隔；构建了包含590条记录的长期胸主动脉瘤网格数据集TAAMesh；在几何精度和直径估计上超越基线。


<details>
  <summary>Details</summary>
Motivation: 需要同时建模复杂3D几何中的局部微小变形与全局解剖学变化，以实现个性化、时间可控的主动脉瘤进展预测。

Method: 方法包括双分支网络：局部KCN通过KNN构建局部邻域卷积以保留细节，全球GCN用于捕捉长程结构，条件分支整合年龄、性别和目标时间以实现时序控制；训练在新建TAAMesh数据集上，使用对抗损失和重建/直径相关损失进行优化。

Result: MCMeshGAN引入了多模态条件网格到网格生成对抗网络，用于3D主动脉瘤生长预测。

Conclusion: MCMeshGAN在个性化3D疾病轨迹建模方面是一项稳健进展，能生成时间可控且解剖学合理的主动脉瘤生长预测，具备临床部署潜力。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [67] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: 提出ProtoScale模块的自监督方法，通过保留场景上下文与多尺度层次化策略学习结构化视觉表示，在目标检测任务上优于SOTA，尤其在数据或训练资源受限时表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有SSL方法在全局图像理解上表现强劲，但在场景结构化表示（如对象级和多尺度特征）上存在不足，限制了在目标检测等密集预测任务中的表现。

Method: 引入ProtoScale模块以在多个空间尺度上捕获视觉元素；保留增强视图中的完整场景上下文，避免像DINO那样依赖随机裁剪和全局嵌入；结合语义分组、实例分离与层次化设计进行自监督训练。

Result: 在COCO和UA-DETRAC的组合子集上进行下游目标检测评估，实验表明该方法在有限标注数据和少量微调epoch条件下，仍能学到更具对象中心性的表示，优于当前最先进方法并提升了监督目标检测性能。

Conclusion: 该论文提出了一种基于自监督学习的多尺度结构化视觉表示方法（ProtoScale），通过语义分组、实例级分离和层次结构逐步构建对象为中心的表示，提高了密集预测任务的性能。

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [68] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet用轨迹+车速先验和视觉注意力的双分支Transformer，在准确率与实时性之间取得了良好平衡，达成SOTA并且延迟最低。


<details>
  <summary>Details</summary>
Motivation: 随着具备自动驾驶能力车辆上路，准确预测行人是否会过马路对于安全至关重要。将未来轨迹和车辆速度作为先验信息，结合视觉线索，有望提升过街意图预测的时效性与准确性，同时减少推理延迟。

Method: 模型由两个分支组成：序列注意力模块（SAM）处理观察到与预测的行人轨迹及车辆速度序列；视觉注意力模块（VAM）将预测的行人边界框叠加到场景图像上，学习视觉表示。两分支在Transformer框架下融合特征进行最终分类。

Result: 在三大常用数据集上取得了SOTA性能，并在模型运行时间与数据预处理时间总和上优于现有方法，展示出较好的实时部署潜力。

Conclusion: 本文提出TrajFusionNet，一种基于Transformer的行人过街意图预测方法，通过融合未来行人轨迹和车辆速度预测作为先验，提高预测准确性并保持低延迟。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [69] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出基于互信息的SMI模型，用整版光纤并通过增量互信息学习分别提取公共与个体天空成分，能在LAMOST数据上改进天空背景去除，蓝端效果明显。


<details>
  <summary>Details</summary>
Motivation: 传统基于空纤构建平均超天背景的方法忽略了目标周围环境差异，导致背景估计不精确，尤其在蓝端；因此利用整版光纤信息并通过互信息学习来建模局部天空背景。

Method: SMI包含两个主要网络：一是带波长校准模块的特征提取网络，以解决光谱特征位移；二是利用增量训练策略通过最大化不同光谱表示间互信息来学习公共成分，并最小化相邻光谱表示间互信息以提取个体成分，从而得到每个位置的天空背景。

Result: 在LAMOST光谱上进行实验，结果显示SMI在观测过程中能得到更好的目标局部天空背景估计，特别是在蓝端表现优于现有方法。

Conclusion: 提出一种基于互信息的天空背景估计模型SMI，通过两阶段网络和增量训练，从整版光纤光谱中分离公共（天空）与个体成分，实现每个目标位置的局部天空估计，实验证明在LAMOST数据上能改善蓝端的背景去除。

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [70] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: Using multispectral LiDAR and deep point-cloud models (especially SPT) yields accurate and efficient urban tree extraction, with pNDVI spectral input cutting errors by ~10.6 pp.


<details>
  <summary>Details</summary>
Motivation: Urban tree monitoring is crucial for greening policies and protecting electrical infrastructure, but traditional airborne laser scanning struggles in complex urban environments and with tree variability; multispectral LiDAR provides both 3D geometry and spectral information to potentially improve mapping.

Method: Compare three state-of-the-art point-cloud DL models (SPT, PTv3, PTv1) on multispectral LiDAR data for tree point extraction; include spatial-only and spatial+spectral (pNDVI) input configurations; evaluate using mean Intersection over Union (mIoU) and error-rate metrics, and measure runtime/time efficiency.

Result: SPT achieved the best trade-off with mIoU = 85.28% and notable time efficiency. Adding pNDVI to spatial features reduced error rate by 10.61 percentage points compared to spatial-only inputs. Overall findings indicate MS-LiDAR + DL improves tree extraction and supports better tree inventories.

Conclusion: Multispectral LiDAR combined with deep-learning point-cloud segmentation can substantially improve urban tree extraction; Superpoint Transformer (SPT) offers a good balance of speed and accuracy, and inclusion of a pseudo-NDVI spectral feature reduces detection error considerably.

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [71] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: Introduces Video-to-Video Motion Personalization; PersonaAnimator framework; PersonaVid dataset; physics-aware regularization; outperforms prior methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations in motion generation: lack of style learning, reliance on motion capture, and physical implausibility.

Method: Paper analysis

Result: PersonaAnimator learns personalized motion patterns from unconstrained videos and outperforms SOTA; PersonaVid dataset introduced; physics-aware regularization improves plausibility.

Conclusion: PersonaAnimator enables personalized, physically plausible motion transfer from videos and sets new benchmark with PersonaVid.

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [72] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 这篇综述评估了高光谱成像(HSI)在ADAS/自动驾驶中的潜力与现状。分析216款商业HSI相机，在帧率、空间分辨率、光谱维度和AEC-Q100温度耐受性上进行基准测试。结果显示商业设备与汽车需求存在明显差距，仅有4款相机满足性能门槛，但没有一款符合AEC-Q100。现有数据集规模、光谱一致性、波段数量和环境多样性均有限，阻碍了算法发展与验证。文章总结了研究方向以促进HSI实用化。


<details>
  <summary>Details</summary>
Motivation: 尽管高光谱成像在材料识别及复杂场景理解方面具备优势，但其在ADAS/自动驾驶领域的可行性、商业设备适配性及数据支持程度尚未被系统评估。本文旨在填补这一空白，评估技术与市场差距并指明研究路线。

Method: 本文采用文献综述与市场调研相结合的方法：系统回顾HSI在汽车感知的研究进展与应用，收集并分析216款市售高光谱/多光谱相机的关键规格（帧率、空间分辨率、光谱维度、温度合规性），并评估现有数据集及其限制，最后提出面向实际集成的研究方向与建议。

Result: 分析显示：在216款相机中仅4款满足定义的性能阈值，零款满足AEC-Q100车规温度标准；现有HSI数据集在尺度、光谱一致性、通道数量和环境多样性上存在显著不足；研究应用展示了HSI在路面分类、行人可分离性和恶劣天气感知的潜力，但缺乏充分的实车验证与长期多场景数据。

Conclusion: 当前高光谱成像在研究中显示出对道路表面分类、行人分离及恶劣天气感知的潜力，但商业相机在帧率、分辨率与车规温度标准方面尚不符合汽车场景要求，数据集也不足以支撑大规模算法评估。因此HSI距离在ADAS/自动驾驶中广泛部署仍有显著差距，需要在传感器硬件、数据采集、标准化和算法适应性上开展进一步工作。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [73] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 提出了OSS（object-based set similarity）指标，用对象级特征衡量训练集与目标域相似性，无需训练检测器即可评估主动学习方法并筛除低效方法；还能用于选择代表性验证集以提高评估鲁棒性。OSS对检测器无关，仅需标注的目标裁剪，适用于自动驾驶数据集并与现有AL流程兼容，节省大量计算开销并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 主动学习在目标检测上的应用面临高昂计算成本（需多次训练检测器）和评估不可靠（方法排名随验证集变化大），限制了在自动驾驶等安全关键场景的部署。

Method: 基于对象级特征提取（目标裁剪）计算训练集与目标域之间的集相似度（OSS），用于两方面：在主动学习迭代前筛除低效方法；基于相似度选择代表性验证集以稳定方法排名。验证在KITTI、BDD100K、CODA上，使用不确定性驱动的AL方法和两种检测器（EfficientDet、YOLOv3）。

Result: 在三个自动驾驶数据集与两种检测器上实验证明OSS能有效预测AL方法表现并稳定排名，减少不必要的训练从而显著降低计算开销，同时OSS仅需标注目标裁剪，兼容现有AL管线。

Conclusion: OSS能在不训练检测器的情况下预测主动学习方法在目标域的有效性，并帮助选取稳健的验证集，从而降低计算成本并提高评估可靠性，适用于多种数据集和检测器架构，便于实际部署。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [74] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 用2D Foundation Model生成掩码并投影到3D，结合基于置信度/不确定性的一致性正则化筛选伪标签，从而用少量3D标注实现更好的3D弱监督分割。


<details>
  <summary>Details</summary>
Motivation: 标注大规模、无序的3D点云昂贵且困难；而2D基础模型在语义分割上已有强大能力。论文旨在利用2D与3D的互补性，把2D模型的能力转化为对稀疏3D标注的有效增强，并同时处理伪标签噪声问题。

Method: 首先用强大的2D基础分割模型在多视角图像上生成2D分割掩码；通过建立2D视图与3D点云的几何对应关系，将2D掩码传播到3D场景，生成3D掩码并扩展原始稀疏3D标注；对3D点云的增强版本施加置信度与不确定性驱动的一致性正则化以筛选可靠伪标签；将可靠的伪标签在3D掩码范围内扩散以产生更多训练标签，并以此训练3D分割模型。

Result: 方法通过将2D掩码投射并结合伪标签筛选，显著扩大可用3D标签数量，降低伪标签噪声对训练的负面影响，从而在弱监督3D语义分割任务上取得性能提升（文中宣称为“明显改善”，但摘要未给出具体数值）。

Conclusion: 该论文提出将2D基础模型产生的分割掩码投射并扩展到3D，从而显著扩大稀疏3D标注的覆盖范围，并结合基于置信度与不确定性的多视角一致性正则化与伪标签选择机制，最终提升3D弱监督语义分割性能。

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [75] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: 结合自适应分层窗口和小波子带分解的WaveHiT-SR，在保持或提升SR性能的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于Transformer的超分方法受限于窗内自注意力的二次复杂度，只能使用小且固定窗口，从而限制了感受野和远程依赖建模能力。作者希望通过层次化窗口与小波分解解决这一问题。

Method: 提出WaveHiT-SR：在层次化Transformer框架中引入自适应分层窗口以扩展感受野，并在网络中嵌入小波变换进行多频率子带分解，逐层重建高分辨率图像以降低计算复杂度。

Result: 在实验证明中，WaveHiT-SR及其基于SwinIR-Light、SwinIR-NG、SRFormer-Light的改进版在参数更少、FLOPs更低且速度更快的情况下取得了领先的超分辨率结果。

Conclusion: WaveHiT-SR通过将自适应分层窗口与小波变换结合，在高效建模远程依赖和频率子带分解方面取得了改进，能在减少计算复杂度的同时提升超分辨率性能。

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [76] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是首个针对韩语文本丰富VQA的多领域基准及半自动化数据生成流程，促进低资源语言VLM评估与多语种扩展。


<details>
  <summary>Details</summary>
Motivation: 当前高资源语言已有文本丰富VQA基准，但低资源语言如韩语缺乏全面评价体系，阻碍对VLM在视觉文本理解与推理上的公平评估与比较。

Method: 提出半自动化VQA生成流程：细化的逐步图像分解以识别文本场景元素，结合七项评价指标筛选与优化问题-答案对，面向文本丰富场景优化生成策略。

Result: 提出KRETA，一个面向韩语的文本丰富VQA基准，覆盖15个领域和26种图像类型，聚焦视觉文本理解与推理能力评估；提供半自动化生成管道，采用逐步图像分解和七指标质量评估以保证数据质量，并开源代码数据集。

Conclusion: KRETA填补了韩语文本丰富VQA基准空白，提供高质量、多样化的数据与可复用生成管道，有助于推进多语种视觉语言模型研究。

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [77] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: 将Chan–Vese水平集离散化并模块化为可用于深度学习的损失函数，提供理论与实现并进行实验比较，代码开源，但摘要未报告详细定量结果。


<details>
  <summary>Details</summary>
Motivation: 把经典的主动轮廓/Chan–Vese几何先验引入现代神经网络训练，通过构造基于能量泛函的损失以改善分割边界与形状一致性，弥补纯像素级损失（如交叉熵、Dice）在几何约束方面的不足。

Method: 从经验研究出发对Chan–Vese模型的泛函能量和水平集方程进行离散化推导并证明结果，给出MATLAB实现；在深度学习框架中以水平集/Chan–Vese为基础设计函数式分割损失（PyTorch nn.Module形式），并在标准分割数据集上与传统损失函数进行对比评估。

Result: 论文实现并开源了离散化方案和PyTorch损失模块；摘要未给出具体数值或统计指标，仅声称完成了数据集上的比较评估，因此需要查看正文或代码以获取定量结果。

Conclusion: 本文通过离散化Chan–Vese能量与相应的水平集PDE，给出理论分析与实现（MATLAB），并将该水平集形式的主动轮廓损失封装为PyTorch的损失模块，用于与经典分割损失在常见数据集上比较，相关代码已开源。

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [78] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 對25個VLM在四個基準集上的系統評估顯示：社交媒體風格圖像易被精確定位（61%），街景圖像表現差，提示重大隱私與倫理風險。


<details>
  <summary>Details</summary>
Motivation: 鑑於VLM地理定位能力快速提升，迫切需要量化其精度、理解推理機制並評估對個人隱私的潛在威脅。

Method: 系統性評估了25個最先進的VLMs，使用四個具有不同場景（街景、社交媒體風格等）的基準圖像數據集，分析模型輸出與內部推理并比較性能差異。

Result: 總體上對通用街景圖像表現較差，但在類似社交媒體的圖像上精度高（報告61%），同時揭示模型在線索使用、地理偏倚與錯誤類型方面的模式。

Conclusion: 研究表明現有生成型視覺語言模型在地理定位任務上具有可觀能力：在社交媒體類圖像上達到高精度（約61%），但對於一般街景圖像表現較差，暴露出顯著的隱私風險與推理可解釋性問題。

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [79] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim is a training-free method that fuses global and local image-text embedding similarities to detect object hallucination in vision-language models, achieving superior benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Existing object hallucination detectors use either global or local views in isolation, limiting reliability. Combining both perspectives can capture complementary cues and improve detection performance.

Method: A training-free framework that computes complementary global and local embedding similarity signals between image and text modalities; fuses these signals to produce an object-level hallucination score (GLSim) for detection.

Result: GLSim outperforms competitive baselines in comprehensive benchmarks, showing a significant margin improvement in detection performance across tested datasets and scenarios.

Conclusion: GLSim effectively reduces object hallucination detection errors by combining global and local image-text embedding similarity, providing more robust and reliable detection across diverse scenarios without requiring additional training.

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [80] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 该文创新性地把分割建模为条件标签扩散任务，直接生成分割掩码，支持端到端训练并在PNG基准上取得最先进结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多以图像为中心，将扩散模型用作视觉特征提取、合成训练数据或从图像扩散模型反演注意力，未把分割本身作为生成目标，限制了对空间与语义一致性建模的能力。

Method: 提出GS（Generative Segmentation）框架，核心是标签扩散(label diffusion)：在条件（输入图像和语言描述）下反向扩散过程直接生成分割掩码，而不是基于图像生成或从图像扩散模型中提取特征/注意力。

Result: 在Panoptic Narrative Grounding（PNG）基准上，GS显著优于现有判别式与扩散型方法，刷新语言驱动分割任务的最新性能。

Conclusion: 该论文提出了将分割任务从判别式转为生成式的观点，通过对标签进行扩散建模，直接从噪声生成语义分割掩码，从而实现端到端训练和对空间语义的一致控制。

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [81] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: 提出面向VLM的增量测试时自适应（ITTA）基准与SegAssist模块——一种无训练的分割辅助主动标注方法，用以在测试时识别并标注潜在新类样本，从而增强VLM在持续出现未知类与域变动下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统的测试时自适应通常假设测试流仅来自预定义类别集合，难以应对现实场景中持续出现的未知类别与域转移。为此提出允许同时处理协变量与标签漂移的增量TTA框架，并通过主动标注扩展类别集合。

Method: 建立ITTA基准，将单张图像的TTA方法与主动标注（查询oracle）结合，提出SegAssist模块：无需训练，复用VLM分割功能来细化主动样本选择，优先选取可能属于未知类别的样本进行标注与适配。

Result: 在多个基准数据集上的大量实验表明，SegAssist能够在现实世界需要持续适配新出现数据的场景下，提升VLM的表现。

Conclusion: 该工作提出了面向视觉语言模型（VLM）的增量测试时自适应（ITTA）情境与基准，允许模型在测试期持续应对未知类别与域变化，并主动获取标签进行扩展。提出了一个无训练的分割辅助主动标注模块SegAssist，用以利用VLM的分割能力改进样本选择，从而更好识别可能属于新类别的样本。实验显示SegAssist能提升VLM在连续出现新数据时的性能。

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [82] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D是一个无人工注释、基于多视角图像的单阶段开放词汇室内3D检测器。通过图嵌入生成高质量3D伪框并用多样化CLIP特征对齐体素语义，达到了比现有方法更高的准确率与更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前OV-3D主要依赖点云方法，而基于图像的探索不足。希望在缺乏人工3D框和类别标注的情况下，利用2D图像分割、预训练视觉-语言模型（CLIP）和图嵌入技术，实现开放词汇的高效室内3D目标检测。

Method: 采用单阶段检测器结构，输入为多视角RGB-D（训练时）或多视角图像（推理时）。核心组件包括：1) 利用ImGeoNet生成的2D诱导体素特征作为背骨；2) 3D伪框生成模块，使用图嵌入将2D分割片段合并为一致的3D结构以产出高质量伪框；3) 两类损失联合训练——类无关的3D定位损失（依赖伪框作为目标）和体素-语义对齐损失（将从与每个3D结构相关联的2D片段采样的多样化CLIP特征与体素特征对齐）。推理阶段仅需多视图图像即可高效运行。

Result: 提出的3D伪框生成方法在精确率与召回率上优于包括OV-3DET在内的其他方法。联合训练的两个损失使单阶段检测器在精度与速度上同时优于强基线（包括使用ViT-CLIP做分类的两阶段方案及加入多视图深度估计的基线），在ScanNet200和ARKitScenes上显示出显著优势（约0.3秒/场景）。

Conclusion: 本文提出OpenM3D——一个无人工标注、基于多视图图像的单阶段开放词汇（OV）室内3D目标检测器。通过从ImGeoNet获取2D诱导的体素特征、基于图嵌入的高质量3D伪框生成以及从多样化CLIP特征到体素的语义对齐联合训练，OpenM3D在ScanNet200和ARKitScenes上实现了比现有方法更高的精度和更快的速度（约0.3s/场景）。

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [83] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 参赛团队在MARIO挑战中：Task1用融合CNN+模型集成做连续两次OCT切片的病情演变分类；Task2提出Patch Progression Masked Autoencoder生成未来OCT，再基于Task1方法判定进展，两项均入围Top10。


<details>
  <summary>Details</summary>
Motivation: 及时诊断与连续监测能显著改善抗VEGF治疗效果，因而研发能追踪与预测exudative AMD在OCT上病灶进展的自动化方法具有重要临床价值。

Method: Task1采用融合卷积神经网络并通过模型集成提升泛化；Task2设计Patch Progression Masked Autoencoder先生成未来的OCT体积（或切片），再用Task1的分类器比较当前与生成的OCT以判定病情进展。

Result: The paper reports participation in the MARIO challenge, addressing detection and prediction of neovascular AMD progression in OCT scans. For Task 1 they used a fusion CNN with ensembling for pairwise slice evolution classification; for Task 2 they proposed a Patch Progression Masked Autoencoder that generates next-exam OCT and reuses Task 1 solution to classify evolution. They achieved Top 10 in both tasks but were ineligible for prize due to organizational overlap.

Conclusion: 提出的方法在短期AMD进展预测与断面演变分类上表现出色，尤其Patch Progression MAE能生成下次检查的OCT，结合已有分类器实现自动化进展判断，但因作者与主办方存在组织关联，不具备领奖资格。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [84] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 针对无人机—卫星配对中常见的配对误对齐噪声，PAUL通过基于不确定性的样本划分与局部增强并结合evidential co-training来抑制噪声影响，从而显著提高跨视角地理定位的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景（城市峡谷、电磁干扰、恶劣天气）会导致无人机与卫星图像之间的GPS漂移与系统性位移，使得训练时成对图像仅部分对应，现有方法普遍假设完美对齐，难以适应这种噪声对应情形。

Method: 提出了Partition and Augmentation by Uncertainty Learning (PAUL) 框架：① 用不确定性估计与损失差异将训练对划分为高/低可信组；② 在高可信区域执行不确定性感知的共同增强（uncertainty-aware co-augmentation）；③ 采用evidential co-training机制利用不确定性对特征学习进行校正，抑制错配对应带来的有害信号。

Result: 论文通过全面消融与对比实验证明了PAUL各个模块的有效性，且在多种噪声比下均优于其它面向噪声对应的竞争方法，展示了在更贴近真实场景的设置下的鲁棒性提升。

Conclusion: PAUL有针对性地解决了实际跨视角地理定位中的“有噪声对应”问题，通过不直接丢弃或修正标签，而是基于不确定性估计进行样本划分与局部增强，从而为噪声样本提供稳健的监督，显著提升了模型在不同噪声率下的表现。

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [85] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: 本文提出Discrete Diffusion VLA：在单一Transformer中对离散化动作用离散扩散解码，训练采用交叉熵，支持并行和自适应解码顺序，提升一致性与纠错能力，在多项机器人任务上优于自回归与连续扩散基线。


<details>
  <summary>Details</summary>
Motivation: 现有VLA解码器要么采用固定左到右的自回归生成，要么将连续扩散/流匹配头附加在骨干外，前者限制并行性且易受顺序偏差，后者需专门训练与迭代采样，导致架构不统一且难以扩展。作者寻求一种既能保留扩散式逐步精化优点，又能与预训练VLM离散接口无缝结合的统一可扩展解码器。

Method: 作者将动作序列离散化为token chunks，并在Transformer主体内实现离散扩散过程：训练时使用与VLM骨干相同的交叉熵目标而非连续扩散的专用损失；解码时采用多轮重掩码(remasking)与自适应解码顺序，先确定容易的动作元素再逐步细化困难部分，且在每轮中对未确定位置重新掩码以实现错误修正和一致性提升。该方法避免了自回归的从左到右限制，支持并行预测并减少函数评估次数。

Result: 在多项任务上超越自回归与连续扩散基线：LIBERO任务得到96.3%平均成功率(SR)；SimplerEnv Fractal视觉匹配71.2%；SimplerEnv Bridge整体49.3%，显示在精确动作建模、一致性与鲁棒纠错方面的优势。作者还报告了并行解码、减少评估次数以及保持VLM先验的优势。

Conclusion: 本论文提出了一种将离散扩散(decision diffusion)引入视觉-语言-动作(VLA)模型的解码器设计，名为Discrete Diffusion VLA，通过在单一Transformer策略中对离散化的动作chunk进行离散扩散建模，保留了扩散的逐步精化优势，同时与视觉语言模型(VLM)的离散token接口天然兼容，允许并行解码并保留预训练VLM先验。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [86] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 将可微分双鱼眼相机标定与3D Gaussian Splatting 联合优化，从有瑕疵的双鱼眼全景输入中恢复无缝360°渲染，且实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 消费级双鱼眼相机经常因镜头分离和角度畸变导致不完美拼接的全景图，这对虚拟现实、机器人与自动驾驶中的全景重建与新视角合成造成障碍，现有360°渲染模型难以处理此类真实拍摄缺陷。

Method: 构建可微分的双鱼眼相机模型以模拟真实的镜头分离与角度畸变，将该模型嵌入3D Gaussian Splatting 渲染管线中；在训练/优化过程中对高斯参数与标定变量（描述镜头间隙与角度失真）进行联合优化；最终通过渲染获得无缝360°图像并与基线方法比较评估。

Result: 在多个真实世界数据集上的广泛评估表明，方法能从有缺陷的双鱼眼输入生成无缝渲染，并在图像质量与无缝性上超越现有360°渲染模型。

Conclusion: 提出并验证了一种将双鱼眼相机模型整合进3D Gaussian Splatting 的标定与优化框架，能够同时优化场景的高斯表示与模拟镜头间隙与角度畸变的标定变量，从而把有瑕疵的全景输入合成为无缝的360°新视角渲染，且在真实数据集上优于现有方法。

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: 提出将 LLM 与 TTA 端到端结合的 AudioStory，通过桥接/残差双查询实现事件内外一致性，从而提升长篇叙事音频的连贯性与质量，并在新建的 10K 基准上取得领先。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频系统在短片段生成上表现良好，但对长篇叙事音频缺乏时间连贯性和组合推理能力，难以保证场景过渡与情感基调一致性。

Method: 利用 LLM 将复杂叙事请求分解为时间有序的子任务并生成上下文提示；通过‘桥接查询’实现事件内语义对齐，通过‘残差查询’保持事件间一致性；将 LLM 输出与扩散（diffuser）式音频生成器耦合并进行端到端训练；同时构建 AudioStory-10K 基准用于评估。

Result: 在单段与叙事音频生成任务上，AudioStory 在指令遵循能力与音频保真度方面均超出既有 TTA 基线；公开了代码和数据集（AudioStory-10K）。

Conclusion: AudioStory 将大语言模型与文本到音频生成系统结合，形成一个端到端框架以生成长篇叙事音频，解决了时间一致性与组合推理问题，并在定量与定性实验中优于现有 TTA 基线。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 本文提出用有限现场标注+从大模型（BioCLIP2）蒸馏到轻量模型（ConvNeXt-tiny）的方案，在101种丹麦飞蛾的相机数据上证明了能在保持高精度的同时大幅减少计算成本，适合高效昆虫监测。


<details>
  <summary>Details</summary>
Motivation: 自动相机采集的现场飞蛾图像噪声大且与人工挑选的图像存在域差异，准确物种识别对研究昆虫数量下降很重要，但需要在准确性与部署成本之间取得平衡。

Method: 使用有限的专家标注的现场图像作为监督，采用知识蒸馏（从BioCLIP2到ConvNeXt-tiny）来训练轻量级分类模型；在来自AMI相机系统的101个丹麦飞蛾物种数据集上进行对比实验，评估性能与计算开销。

Result: BioCLIP2在该任务上显著优于其他方法；通过蒸馏得到的ConvNeXt-tiny在精度上与BioCLIP2可比，但计算资源和推理成本显著降低。

Conclusion: 结合少量现场专家标注数据并将高性能BioCLIP2基础模型的知识蒸馏到ConvNeXt-tiny上，可以在大幅降低计算成本的同时，保留接近的分类精度，从而为资源受限的昆虫监测系统提供实用方案。

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA通过两阶段的可训练组合（专家化+泛化）把通用规划与专业执行结合，解决了长时规划与精确执行的权衡，在科学GUI任务上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 在科学计算类GUI任务中需要长时程规划与精确执行，但现有方法在泛化与执行间存在权衡，且组合框架通常不可训练、无法从稀缺数据中适配。

Method: 提出将Cerebrum（通用规划器）和Cerebellum（专业执行器）组合，并设计两阶段训练流程：第一阶段Specialization使用解耦的GRPO为每个应用训练专家规划器；第二阶段Generalization聚合所有成功轨迹并进行监督微调以得到最终规划器。

Result: 在ScienceBoard的四个复杂应用上，CODA显著超过基线方法，成为开源模型中的新最优。

Conclusion: CODA通过可训练的组合框架成功结合了通用规划器与专业执行器，既实现了精准执行又保留了跨领域泛化能力，显著优于现有开源模型并在ScienceBoard基准上取得了新SOTA。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [90] [Robust Recursive Query Parallelism in Graph Database Management Systems](https://arxiv.org/abs/2508.19379)
*Anurag Chakraborty,Semih Salihoğlu*

Main category: cs.DB

TL;DR: 构建并评估了基于morsel粒度的派发策略设计空间，提出混合派发与multi-source打包，在实现于Kuzu后表现为对递归查询的稳健多核并行化方案。


<details>
  <summary>Details</summary>
Motivation: 图数据库中递归连接查询（如多源BFS）需高效利用多核并行；现有并行策略在不同查询/数据下表现各异，缺乏统一且鲁棒的派发策略。

Method: 将现有的morsel-driven（源节点粒度）与前沿粒度并行视作同一设计空间的两端，定义基于粒度的派发策略；提出并实现混合策略（同时在源节点和前沿级别发放morsel），并把多源BFS建模为将多个源打包成multi-source morsel；在Kuzu GDBMS中实现这些策略并与其它系统对比评估。

Result: 混合策略能在两种极端策略均能并行时捕获其行为，并在这些策略受限时超越它们；multi-source打包能降低扫描量，但仅在查询有足够多源节点时明显有利；在Kuzu及跨系统评测中验证了这些结论。

Conclusion: 提出并验证了一种混合morsel派发策略，在多核并行递归联接查询（如图数据库中的BFS）上比只在源节点或前沿粒度并行更鲁棒；当查询包含足够多源节点时，将多源打包为morsel（multi-source morsel）可减少扫描开销。

Abstract: Efficient multi-core parallel processing of recursive join queries is
critical for achieving good performance in graph database management systems
(GDBMSs). Prior work adopts two broad approaches. First is the state of the art
morsel-driven parallelism, whose vanilla application in GDBMSs parallelizes
computations at the source node level. Second is to parallelize each iteration
of the computation at the frontier level. We show that these approaches can be
seen as part of a design space of morsel dispatching policies based on picking
different granularities of morsels. We then empirically study the question of
which policies parallelize better in practice under a variety of datasets and
query workloads that contain one to many source nodes. We show that these two
policies can be combined in a hybrid policy that issues morsels both at the
source node and frontier levels. We then show that the multi-source
breadth-first search optimization from prior work can also be modeled as a
morsel dispatching policy that packs multiple source nodes into multi-source
morsels. We implement these policies inside a single system, the Kuzu GDBMS,
and evaluate them both within Kuzu and across other systems. We show that the
hybrid policy captures the behavior of both source morsel-only and frontier
morsel-only policies in cases when these approaches parallelize well, and
out-perform them on queries when they are limited, and propose it as a robust
approach to parallelizing recursive queries. We further show that assigning
multi-sources is beneficial, as it reduces the amount of scans, but only when
there is enough sources in the query.

</details>


### [91] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 用LLM风格的合成SQL生成技术构造训练集，可以用更少查询显著提升学习型成本模型的预测性能。


<details>
  <summary>Details</summary>
Motivation: 真实且多样的工作负载对于压力测试、漏洞测试以及优化数据库成本与性能极为关键，但获取大规模多样查询集困难，因此希望通过合成数据生成解决样本不足问题。

Method: 采用现代合成数据生成方法（借鉴生成式AI和大语言模型的技术），自动生成多样化的SQL查询和工作负载，用于训练基于学习的成本模型，并与现有的生成方法进行比较评估。

Result: 初步结果表明，使用该合成生成方法训练的成本模型能在比竞争性生成方法少45%查询的情况下达到或超过其预测精度。

Conclusion: 利用受生成式AI/LLM启发的合成数据生成技术，可以在用更少真实查询的情况下，构建高质量训练集，从而提高学到的查询执行成本模型的预测准确性。

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>
