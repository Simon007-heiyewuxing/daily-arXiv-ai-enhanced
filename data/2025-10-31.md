<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

TL;DR: 引入时序增强的T-YOLOv5能显著提升水下检测总体mAP，CBAM在复杂动态场景（遮挡、突变运动）进一步稳健，但在简单场景出现轻微精度下降。


<details>
  <summary>Details</summary>
Motivation: 提升水下视频中目标检测的稳定性与鲁棒性，尤其在海洋环境中目标出现突发运动、部分遮挡及缓慢移动时提高检测准确性。

Method: 阶段一：比较YOLOv5与T-YOLOv5（时序增强）在同一测试集上的mAP；阶段二：在T-YOLOv5中融入CBAM并对比三者在不同运动、遮挡和复杂度场景下的性能差异及泛化能力。

Result: T-YOLOv5与CBAM结合在时间建模下对水下目标检测具有总体提升，但存在情景依赖性。

Conclusion: T-YOLOv5相比标准YOLOv5在mAP@50-95上有明显提升；在加入CBAM后，对复杂动态场景的检测鲁棒性增强，但整体mAP微降，说明空间注意力模块带来的局部改进并非对所有场景都有益；需针对场景或引入自适应机制以兼顾简单与复杂场景表现。

Abstract: This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [2] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

TL;DR: MIRO trains with multiple reward models as conditioning to directly learn user preferences, improving quality, diversity and efficiency over post-hoc reward selection


<details>
  <summary>Details</summary>
Motivation: Post-hoc reward-based selection harms diversity, fidelity, efficiency; need to align generation with user preferences during training

Method: Proposed method

Result: MIRO conditions on multiple reward models during training, improving visual quality and training speed; achieves SOTA on GenEval and user-preference metrics

Conclusion: Conditioning on multiple reward models during training yields better alignment with user preferences, higher image quality, faster training, and SOTA results.

Abstract: Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [3] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

TL;DR: 论文提出了面向自行车的3D LiDAR分割方法与新数据集BikeScenes-lidarseg，通过在车载领域预训练模型上微调显著提升性能，强调域适应的重要性。


<details>
  <summary>Details</summary>
Motivation: 电动车和更快的e-bike增多使骑行者更脆弱，需将汽车感知技术适配至自行车以提升安全性。

Method: 基于多传感器SenseBike平台，收集3021帧连续LiDAR点云并手工语义标注29类；在SemanticKITTI预训练模型上进行微调评估mIoU改进。

Result: 提供对论文摘要的结构化分析

Conclusion: 自行车专用数据集与训练对提升LiDAR语义分割效果至关重要；在硬件受限的自行车平台上仍存在感知挑战。

Abstract: The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [4] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 用36张洁净Si(001):H实验图像，通过物理建模合成数据训练流匹配与扩散模型，实现STM图像修复与超分辨，能从稀疏采样中重建图像，提升2–4倍采集速率，降低换针频率


<details>
  <summary>Details</summary>
Motivation: reduce tip degradation and speed up STM by repairing images and super-resolution from sparse sampling

Method: flow-matching and diffusion models trained on physics-informed synthetic STM data

Result: models restore degraded images and reconstruct from sparsely sampled data, achieving 2–4× acquisition time reduction; quantitative improvement per CMMD and SSIM

Conclusion: 该ML框架可提升STM通量，减少针头调试频次并提高高速STM的帧率

Abstract: Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [5] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出 SplitFlow：通过语义分解子提示并对各自流进行投影和软聚合，在无反演的 ODE 编辑框架下同时提升多样性与语义一致性，优于现有零样本编辑方法。


<details>
  <summary>Details</summary>
Motivation: 解决 rectified flow 模型在图像编辑中反演不准确和编辑时梯度纠缠导致与目标提示不一致的问题。

Method: 基于无反演的 ODE 流模型，先将目标提示分解为多个子提示，分别计算子提示对应的速度场（flow）；引入投影（projection）与软聚合（soft-aggregation）机制，借鉴多任务学习中梯度冲突解决策略，按自适应权重合并子速度场以生成最终编辑轨迹。

Result: 提出了一种基于无反演（inversion-free）配合流分解与聚合的框架：将目标提示语义分解为多个子提示，为每个子提示计算独立的流并聚合，设计流的投影与软聚合机制以自适应加权子目标速度场，抑制语义冗余并强调区分方向。

Conclusion: 该方法在语义保真度与属性解耦上优于现有无反演图像编辑方法，能在保持多样性的同时实现对完整目标提示的一致引导。

Abstract: Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [6] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

TL;DR: Brain-IT通过功能聚类的Transformer在脑体素与局部图像特征间建立直接交互，预测语义与结构两类局部特征以引导扩散模型，从而在fMRI图像重建上提高忠实度并显著减少微调数据需求。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的fMRI图像重建方法虽进步明显，但往往缺乏对实际所见图像的忠实性。通过模拟脑中功能集群的交互并直接映射到局部图像特征，可提高重建的语义与结构一致性，同时通过跨被试共享和参数共享缓解数据稀缺问题。

Method: 1) 将脑体素按功能相似性聚类，形成跨被试共享的功能性聚类；2) 设计Brain Interaction Transformer（BIT），在聚类之间以及聚类与图像补丁特征之间建立交互，所有组件参数在聚类和被试间共享；3) BIT预测两类局部图像特征：高层语义特征和低层结构特征；4) 将高层语义与低层结构特征分别用于引导和初始化扩散模型以生成最终图像；5) 在少量新被试数据上进行微调，验证效率和效果。

Result: Brain-IT提出了一种基于脑启发的Transformer（BIT），通过在功能相似的脑体素聚类之间建立交互来提高fMRI到图像重建的真实性。共享的功能性聚类和共享模型参数使得跨被试、跨聚类训练更高效，适合数据受限场景。BIT预测两种局部图像特征：高层语义特征用于引导扩散模型生成正确语义内容；低层结构特征用于初始化扩散以获取正确的粗略布局。该设计实现了从脑体素聚类到局部图像特征的直接信息流，结果在视觉质量和客观指标上均超越现有SotA方法，并显著降低新被试微调所需的数据量（1小时数据可媲美传统方法的40小时）。

Conclusion: 引入共享的功能性脑聚类与BIT机制，可以在数据有限的情况下有效映射fMRI到细粒度局部图像特征，显著提升重建的真实性与效率。

Abstract: Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [7] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: 在数据稀缺和一秒运行限制下，论文选择了对SAM 2.1进行低速率微调并以首切片mask为提示的实时分割策略，在TrackRAD2025上达到了Dice≈0.88，证明了基础模型在MRI实时肿瘤追踪中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在TrackRAD2025挑战中，目标是实现对胸腹部cine-MRI序列的实时肿瘤追踪，但面临强烈的数据匮乏和运行时间（1秒）限制；因此需要在精度与效率间权衡，探索对数据友好的方法。

Method: 提出并比较了两种方法：1）基于IMPACT相似性度量的无监督图像配准；2）基于SAM 2.1的提示式分割，最终采用SAM2.1 b+模型，使用首个带注释切片的mask作为提示，微调prompt encoder、decoder和Hiera backbone，训练细节包括1024x1024 patch、batch=1、标准增强、Dice+IoU loss、LR=1e-4、300 epochs。

Result: 最终在隐藏测试集上取得Dice=0.8794，赛事排名第6；测试集中未采用测试时增强策略，因为提升有限。

Conclusion: 该论文通过在数据稀缺的情形下，将基于基础模型的分割方法（以SAM 2.1为核心）与无监督配准方法进行对比，最终选择轻量化、响应迅速的SAM微调方案，实现了实时MRI肿瘤追踪，达到了在TrackRAD2025隐测试集上Dice=0.8794并排名第6的结果。

Abstract: In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [8] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

TL;DR: 通过Hilbert Selective Scan提升扫描模式的Hausdorff维数，增强特征空间探索能力，改进低光图像增强的性能、视觉质量与效率。


<details>
  <summary>Details</summary>
Motivation: 提高Mamba框架的扫描模式复杂度，通过引入Hilbert Selective Scan以更好地探索特征空间，捕捉细尺度细节并改善覆盖性，从而解决信息不一致并强化空间局部性，同时保持长程依赖处理能力。

Method: 在Mamba框架中引入Hilbert Selective Scan机制以调整扫描路径，使得扫描模式具有更高的Hausdorff维数，从而更全面地覆盖特征空间并捕获细尺度信息，结合现有Mamba-based模型进行训练和评估。

Result: 改进后的Mamba-based低光图像增强方法在公开基准上的定量指标和视觉质量显著提升，同时降低了计算资源消耗并缩短了推理时间。

Conclusion: 该策略在提高低光图像增强效果的同时具备更低的计算开销与更快的推理速度，且可推广到其它使用Mamba技术的领域。

Abstract: We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [9] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

TL;DR: CAVE: first real-world visual anomaly benchmark with cognitive-science-inspired annotations for description, explanation, justification; VLMs struggle


<details>
  <summary>Details</summary>
Motivation: Introduce a realistic benchmark for real-world visual anomalies to address limitations of prior datasets focused on industrial or synthetic anomalies

Method: Construct dataset with fine-grained annotations and tasks; evaluate SOTA VLMs with advanced prompting to assess performance

Result: CAVE benchmark with annotations for description, explanation, justification, visual grounding, categorization by manifestation, complexity, severity, commonness; shows SOTA VLMs perform poorly on anomaly perception and commonsense reasoning

Conclusion: CAVE provides a realistic, cognitively grounded resource to improve VLMs' anomaly detection and commonsense reasoning capabilities

Abstract: Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [10] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

TL;DR: 提出一种轻量级CNN框架，基于视觉输入在阿布扎比和旧金山数据上实现泛化，能在平均上将洪水深度图预测的MAE降低约20%，适用于应对海平面上升的城市级规划。


<details>
  <summary>Details</summary>
Motivation: Provide a low-resource, generalizable DL method for city-scale coastal flood prediction under sea-level rise and shoreline adaptation scenarios, addressing computational limits of physics-based models and data scarcity in DL.

Method: CNN-based prediction of coastal flood depth maps

Result: A lightweight vision-based CNN trained on datasets from Abu Dhabi and San Francisco that reduces MAE by ~20% compared to state-of-the-art methods in flood depth map prediction.

Conclusion: 该方法为城市尺度沿海洪水管理提供了可扩展且实用的工具，支持决策者制定适应与缓解策略。

Abstract: Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [11] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

TL;DR: 通过在视觉编码器中嵌入堆叠时序注意力模块，STAVEQ2显著提升Video-LLM的时间推理与动作识别能力，在多项基准上取得最高约+5.5%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 现有的Multimodal/Video-LLM在捕捉复杂时间动态（如多步骤动作、事件顺序）时表现欠佳。本工作旨在通过在视觉侧显式建模时间信息来改善Video-LLM的时间理解能力。

Method: 在视觉编码器内部直接加入多层（stacked）时间注意力模块，使得视频帧在送入LLM之前就能通过时序注意力建模帧间关系和动作进展，从而生成更具时间结构的视觉特征。

Result: 在多个视频问答与动作识别基准（如VITATECS、MVBench、Video-MME）上，所提方法将性能提升最多达+5.5%，在时间推理与动作识别任务上显著优于现有模型。

Conclusion: 本文通过在视觉编码器中引入堆叠的时间注意力模块，有效提升了Video-LLM对动作序列和时间进程的理解能力，弥补了现有模型在时间推理方面的不足。

Abstract: Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [12] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

TL;DR: FlexICL: a flexible visual in-context learning framework with new image concatenation and augmentation methods that enables accurate bone segmentation in ultrasound from very few annotations


<details>
  <summary>Details</summary>
Motivation: Reduce need for dense expert annotations by leveraging few annotated frames within ultrasound sweeps via in-context learning; enable robust bony region segmentation with limited labels

Method: In-context visual segmentation with novel concatenation and augmentation strategies

Result: FlexICL attains robust segmentation across four wrist/elbow US datasets using only 5% labeled images, outperforming Painter, MAE-VQGAN, U-Net, TransUNet by 1–27% Dice on 1,252 sweeps

Conclusion: FlexICL is an efficient, scalable approach for US bone segmentation under label scarcity, improving over prior visual ICL and conventional segmentation models

Abstract: Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [13] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

TL;DR: 用VLM在反噪过程中动态生成负向提示，可更灵活抑制不想要内容，但需在强度与对齐度间权衡。


<details>
  <summary>Details</summary>
Motivation: 传统负向提示通常固定且静态，难以在反噪过程中根据中间预测调整以更有效地去除不期望内容；因此希望利用VLM的图文理解能力实现动态负向提示以提高控制精度。

Method: 在特定反噪步骤生成中间图像，使用视觉-语言模型（VLM）对该中间图像进行理解并生成上下文相关的负向提示，将其注入随后的反噪步骤中以引导生成过程。

Result: 在多个基准数据集上的实验表明该方法能在负向引导强度与文本-图像对齐度之间表现出可控的权衡，动态提示在抑制不想要元素时优于静态负向提示，但过强的负向引导会损害文本对齐。

Conclusion: 本文提出在扩散模型反噪过程动态生成负向提示，能更灵活抑制不想要的内容。

Abstract: We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [14] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

TL;DR: 发现多模态扩散模型的文本-图像对齐不足，提出仅修改输入图像的PReMA攻击，在不改变提示词下诱导生成不当/NSFW内容，在修图与风格迁移任务上有效。


<details>
  <summary>Details</summary>
Motivation: Investigate vulnerability of multi-modal diffusion models to adversarial inputs, focusing on misalignment between text and image modalities and risks of NSFW content generation.

Method: 构建对抗性图像（不改动提示词）以诱导模型在图像编辑（修补、风格迁移）任务中生成不当内容，评估多模型上的成功率与稳健性。

Result: Propose PReMA (Prompt-Restricted Multi-modal Attack), an attack that modifies only the input image to manipulate outputs under fixed prompts; demonstrate its effectiveness on inpainting and style transfer across models.

Conclusion: PReMA 展示了通过仅构造对抗图像即可在固定提示下强行改变生成结果的威胁，强调需加强多模态对齐与防护措施。

Abstract: Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [15] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

TL;DR: 提出EgoExo-Con基准和View-GRPO强化学习方法，揭示并改善Video-LLMs在跨视角时序理解上的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 研究Video-LLMs在多视角场景下的时序理解一致性问题，现有评测多关注单视角性能，而现实应用常有多视角输入，需保证跨视角语义与时序一致。

Method: 构建同步的第一视角（egocentric）与第三视角（exocentric）视频对，并设计人类修订的自然语言查询，提出两项任务：时序验证（Temporal Verification）和时序定位（Temporal Grounding）；基线包括单视图训练、双视图简单微调（SFT）及GRPO；提出View-GRPO，基于强化学习优化视角专属推理并引入一致性奖励。

Result: 实验证明：1) 现有Video-LLMs跨视角一致性远低于单视图性能；2) 直接用同步双视图微调虽提升一致性但性能常不如单视图训练；3) View-GRPO在提升跨视角一致性和时序任务表现上优于SFT和GRPO。

Conclusion: 本文提出了EgoExo-Con数据集，针对从不同视角拍摄的同一事件视频对，评估Video-LLMs在时间理解上的一致性，发现现有模型在跨视角一致性和时序推理上存在显著不足。通过View-GRPO方法在保持视角特异性推理的同时提升跨视角一致性，从而优于简单的有监督微调和GRPO。

Abstract: Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [16] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

TL;DR: 本文提出OracleAgent，一个面向甲骨文研究的代理系统，整合多模态工具与大模型，并构建了包含140万单字拓片图像和8万条释文字的领域知识库，实现高效结构化管理与检索，实验显示其在多模态推理与生成任务上优于主流模型并能显著降低专家研究时间成本。


<details>
  <summary>Details</summary>
Motivation: 甲骨文研究流程复杂且包含多并行子任务，信息组织与检索效率低，学者在检索、整理与管理资源上耗费大量时间，亟需系统化的工具来提升效率与准确性。

Method: 系统集成多种甲骨文分析工具并由大模型协调执行任务；构建多模态领域知识库（140万单字拓片、8万释文字）通过长期数据收集、清洗与专家标注；利用多模态工具支持字符、文献、释文与拓片图像的检索与分析；通过对比实验与案例研究验证性能。

Result: 构建了大型多模态知识库并开发OracleAgent，实验表明在多模态推理与生成任务上优于主流MLLM（例如GPT-4o），案例研究显示能显著降低专家的时间成本。

Conclusion: OracleAgent实现了甲骨文信息的结构化管理与高效检索，结合大模型与多模态知识库，提升了多模态推理与生成性能并辅助专家显著降低研究成本，推动了甲骨文辅助研究与自动化释读的实用化进程。

Abstract: As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [17] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

TL;DR: 提出无需COLMAP的联合优化框架，交替优化3D高斯点与相机位姿，通过可微渲染与定制3D光流减少投影误差，在多数据集上表现优于现有方法与COLMAP基线。


<details>
  <summary>Details</summary>
Motivation: 解决传统新视角合成依赖外部相机位姿估计工具（如COLMAP），这些工具带来计算瓶颈和误差传播，且在大视差和特征稀疏场景中表现不佳。

Method: 将联合优化解耦为两阶段交替流程：阶段一在固定位姿下用可微渲染优化3D高斯点参数；阶段二用含几何与光度约束的定制3D光流算法优化相机位姿，反复迭代直到投影误差收敛。

Result: 提出一个统一框架：联合优化3D高斯点和相机位姿，无需预标定输入。通过交替两阶段优化（固定位姿下用可微渲染更新3D高斯；然后用融合几何与光度约束的定制3D光流算法更新相机位姿），逐步减少投影误差，在多个数据集上优于现有无COLMAP方法并超越基于COLMAP的基线。

Conclusion: 交替优化3D高斯与相机位姿的方法可在无预标定条件下实现更高质量的场景重建与更准确的位姿估计，尤其在大视角变化和特征稀疏情况下显示出显著优势。

Abstract: Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [18] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

TL;DR: 提出WOD-E2E：一个专注<0.03%发生频率长尾场景的Waymo基准集（4021段，≈12小时），并引入RFS——基于人工评审偏好的开放环评估指标，已公开验证集标签并用于2025挑战赛。


<details>
  <summary>Details</summary>
Motivation: 现有E2E驾驶基准多为常规情形，难以测试模型在稀有复杂长尾场景下的能力，且传统开放环指标无法充分反映多模态驾驶决策与长尾性能，因而需要专门数据集和更接近人类偏好的评估指标。

Method: 基于Waymo Open Dataset精选并标注了长尾驾驶片段，数据以8路相机360视角、路由信息与ego状态构成；评估方法通过人工评审生成轨迹偏好标签，并用RFS衡量模型预测轨迹与标注偏好的一致性。

Result: 构建了一个关注长尾驾驶场景的基准数据集WOD-E2E，并提出了新的开放环评估指标Rater Feedback Score (RFS)。数据集包含4021段约12小时的驾驶片段，提供高层导航、ego状态和8路360度环视相机数据；验证集附带人工标注的轨迹偏好标签，测试集用于挑战赛。

Conclusion: WOD-E2E和RFS弥补了现有E2E基准在长尾场景覆盖和评估方法上的不足，有望推动更稳健、更通用的视觉端到端驾驶研究。

Abstract: Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [19] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

TL;DR: Use task-specific gradient-based attention fused with CNN features to enhance frame association in RGB-D indoor SLAM; yields better performance, notably in large scenes.


<details>
  <summary>Details</summary>
Motivation: Integrate gradient-based attention into CNN representations for semantic object understanding to improve visual tasks like RGB-D indoor SLAM.

Method: Compute layer-wise attention via network gradients, fuse attention maps with CNN feature representations, and use enriched features for frame association in RGB-D indoor SLAM.

Result: Layer-wise attention from network gradients combined with CNN features improves frame association performance in RGB-D indoor SLAM, outperforming baselines especially in large environments.

Conclusion: Integrating layer-wise gradient attention with CNN features enhances SLAM frame association, improving results over baselines for large environments.

Abstract: Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [20] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

TL;DR: FullPart结合隐式盒子token布局扩散与每零件独立全分辨率体素生成，配合中心点编码和大规模注释数据集PartVerse-XL，显著提升3D零件生成的细节与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么用隐式向量集合导致几何细节不足，要么在共享全局体素网格时小零件占用体素太少，影响质量；因此需要结合两者优点并保证小零件高分辨率表示和全局一致性。

Method: 采用两阶段方法：先用隐式box向量集扩散生成边界盒布局，再对每个零件在独立的固定全分辨率体素网格内生成细节；引入中心点编码解决不同尺寸零件间信息交换的错位问题；构建并训练在PartVerse-XL数据集上。

Result: 在自建的PartVerse-XL（40K对象，320K零件）和其他基准上，FullPart在零件细节、结构一致性与生成质量上优于现有隐式或全局体素方法，达到SOTA表现，并将公开代码与数据。

Conclusion: FullPart通过结合隐式盒子token扩散生成布局与为每个零件分配独立全分辨率体素网格，有效提升了零件细节与整体一致性，实验验证其在3D零件生成上达到了最先进水平。

Abstract: Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [21] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

TL;DR: BasicAVSR is a versatile strong baseline for arbitrary-scale video super-resolution combining Laplacian-based frequency priors, flow-guided aggregation, second-order motion compensation, and hyper-upsampling, with three RNN propagation modes for different latency constraints; outperforms prior methods in quality and speed.


<details>
  <summary>Details</summary>
Motivation: Address challenges of arbitrary-scale video SR: preserving spatial details across scales, ensuring temporal consistency across frames, and keeping inference efficient for various application constraints.

Method: Method combines (1) adaptive multi-scale frequency priors from Laplacian pyramids, (2) flow-guided propagation unit for spatiotemporal aggregation, (3) second-order motion compensation for accurate alignment, (4) hyper-upsampling unit producing scale-aware upsampling kernels; three RNN-based propagation variants implemented.

Result: Proposed BasicAVSR baseline integrates frequency priors, flow-guided propagation, second-order motion compensation, and hyper-upsampling for arbitrary-scale video SR. Three propagation variants (online, limited-lookahead, offline) target different latency scenarios. Empirical results claim superior quality, generalization, and speed.

Conclusion: BasicAVSR effectively balances spatial detail, temporal consistency, and computational efficiency, providing a flexible framework for online and offline AVSR tasks and improving over existing approaches.

Abstract: Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [22] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

TL;DR: 提出基于多视图乳腺影像与合成文本的跨模态自监督模型MV-MLM，实现更高的准确性与数据效率，三项分类任务上达SOTA，无需真实报告


<details>
  <summary>Details</summary>
Motivation: 解决获得带详细标注的大规模乳腺X线影像数据成本高、耗时长的问题，通过利用在大规模图文对上预训练的视觉-语言模型提高数据效率和鲁棒性

Method: 使用多视图监督与图文跨模态自监督：对每例多视图乳腺影像生成伪放射学报告并构造图文对；设计联合视觉-文本学习策略（可能包括对比学习、对齐和互信息最大化等）以学习富表示；在私有与公开数据集上进行微调与评估，比较有监督和VLM基线

Result: 提出MV-MLM（多视图乳腺X线与语言模型），在配对的乳腺影像与合成放射学报告上训练，采用跨模态自监督、多视图监督与联合视觉-文本学习策略，在恶性/良性分类、亚型分类和基于影像的癌症风险预测三项任务上达到了SOTA性能，并在数据效率上优于现有监督和VLM基线，且无需真实放射学报告

Conclusion: MV-MLM通过多视图和合成报告的联合训练有效提升乳腺影像诊断与风险预测性能，显著降低对人工标注和真实报告的依赖，具备良好的泛化和数据效率

Abstract: Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [23] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 作者用1730张标注图像训练YOLOv8实现自动三轮车实时检测，mAP50≈83.4%，精度/召回>78%，并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 解决在孟加拉等南亚城市中自动三轮车（auto-rickshaw）难以从监控视频中区分并受限于交通规则的监控问题；自动化检测以减轻人工视频分析负担。

Method: 收集并标注1730张交通图像（包含不同天气、密度场景），使用YOLOv8进行实时目标检测训练与评估，计算mAP50、二分类精度与召回率并测试在不同交通密度下表现。

Result: 基于YOLOv8的实时目标检测系统在含1730张多样化标注图像上训练，达到mAP50=83.447%，二分类精度和召回率均高于78%，能在稠密和稀疏交通场景中实时检测自动三轮车；同时数据集已公开。

Conclusion: YOLOv8在本数据集上能有效、实时检测自动三轮车，可用于交通监管场景，但仍需更大/多样化数据与跨地域验证以提高鲁棒性。

Abstract: Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [24] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

TL;DR: 提出CRAG-MM——用于可穿戴场景的多模态RAG综合基准，包含大量真实感图像和多轮问答，现有方法在该基准上表现较低，显示大量改进空间；已用于KDD Cup 2025并吸引广泛参与。


<details>
  <summary>Details</summary>
Motivation: 建立针对可穿戴设备场景的多模态检索增强生成（MM-RAG）评测基准，填补现有基准在多模态多轮对话和可穿戴场景上的空白。

Method: 构建大规模数据集（6.5K三元组、2K多轮会话、6.2K仿穿戴图像），刻意设计图像质量问题与多样问题类型，提供单源、多源与多轮三类任务，并配套图像-KG与网页检索API，进行基线与行业系统评估，并通过竞赛推动改进。

Result: 提出CRAG-MM基准：包含6.5K图-问-答三元组、2K多轮对话、13个领域、6.2K视角图像，并设计三项任务、检索语料库与检索API；评估显示当前RAG方法与业界最优解在真实度上分别仅约32%／43%（单轮/多轮），并通过KDD Cup 2025推动社区改进，胜出方案将基线提升28%。

Conclusion: CRAG-MM填补了可穿戴多模态RAG评测空白，揭示现有方法在真实穿戴场景下的局限性，为后续研究提供数据、任务与基准评估平台。

Abstract: Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [25] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出MoTDiff，用条件扩散模型从单幅运动模糊图像估计高分辨率运动轨迹，训练策略增强细粒度识别、整体形状与像素连通性，实验证明在盲去模糊与代码曝光摄影上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张运动模糊图像中恢复高质量的运动轨迹对计算成像与计算机视觉任务（如去模糊、光流估计、代码曝光摄影）至关重要。现有方法得到的运动表示往往粗糙且不准确，难以满足高分辨率精细轨迹重建需求。

Method: 设计一个以多尺度特征图作为条件的条件扩散框架，并提出新的训练损失或策略以强化细粒度轨迹识别、形状位置一致性与像素连通性。

Result: 提出MoTDiff：首个基于扩散模型的高分辨率运动轨迹估计框架。通过多尺度特征条件的条件扩散模型与新训练策略，能从单张模糊图像预测精细、连贯且位置形状一致的高分辨率运动轨迹。

Conclusion: MoTDiff有效提升了单张模糊图像的运动轨迹估计质量，能够生成更精细和一致的HR轨迹，进而提升盲图像去模糊与编码曝光摄影任务的性能。

Abstract: Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [26] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

TL;DR: ConceptScope通过在预训练视觉表征上训练稀疏自编码器自动发现可解释视觉概念，能分类这些概念并检测已知与新发现的数据偏差，为数据审计和模型诊断提供可扩展工具。


<details>
  <summary>Details</summary>
Motivation: 自动识别与量化数据集中的人类可解释视觉概念，以检测和分析数据偏差，避免昂贵的人工属性标注。

Method: 在视觉基础模型（如CLIP或其它）提取图像表示，训练稀疏自编码器以发现概念基；通过统计分析将概念按与目标类别的相关性和语义匹配分为目标、上下文与偏差；用概念激活进行空间归因并基于概念构建子群用于鲁棒性评估与偏差检测。

Result: 提出ConceptScope框架：使用稀疏自编码器在视觉基础模型的表征上发现并量化概念；按语义相关性与与类别的统计相关性将概念分为目标、上下文与偏差；基于概念分组评估鲁棒性并进行数据集审计。

Conclusion: ConceptScope能捕获对象、纹理、背景、面部属性与动作等多种视觉概念，生成有语义意义的空间归因，与注释数据集对齐，能可靠检测已知与未标注偏差，适用于数据集审计与模型诊断。

Abstract: Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [27] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

TL;DR: 论文通过扩散模型合成120k草图-3D姿态配对并训练端到端框架，实现了更快更准确的草图到3D人体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 草图具有抽象与不成比例的特性，缺乏大规模草图-3D姿态标注使得传统优化方法效率低且泛化差，因此需要通过合成数据与学习驱动方法提高准确性与效率。

Method: 先训练扩散模型从由3D姿态投影得到的2D关键点生成草图图像，构建包含120k草图-3D姿态配对的合成数据集SKEP-120K；在此基础上结合现有2D姿态检测器与生成式扩散先验提取草图特征，并用前馈神经网络进行高效2D关键点估计；引入多项启发式损失以保证3D-2D几何一致性和自接触准确性。

Result: 通过定性、定量与主观评估，模型在估计精度与速度方面均显著优于之前的方法。

Conclusion: 该工作通过合成训练数据和端到端数据驱动框架，显著提高了从草图到3D人体姿态估计的准确性和速度，突破了之前基于启发式优化方法的限制。

Abstract: 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [28] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

TL;DR: 提出一种结合Chebyshev图卷积与卷积网络的几何深度学习集成模型Ch-EGN，用于供应链的风险管理与可持续性相关分类任务，实验上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究供应链可持续性与风险管理，通过深度学习改进产品与边的分类，提高供应链性能与风险控制。

Method: 构建混合卷积与图神经网络的集成架构，利用Chebyshev多项式近似的图卷积处理供应链关系信息，并融合传统卷积网络特征；在SupplyGraph和DataCo数据集上分别进行产品分类、边分类与交付状态预测，评估准确率并与现有方法比较。

Result: 提出Chebyshev ensemble geometric network (Ch-EGN)，在两个数据集上取得高精度：DataCo交付状态预测98.95%，SupplyGraph的5类产品分类100%，4类产品关系98.07%，25公司关系92.37%。

Conclusion: Ch-EGN在供应链分类与风险预测任务上显著提升了性能，证明了几何深度学习在建模供应链关系与隐含状态推断方面的有效性。

Abstract: The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [29] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

TL;DR: 构建百万级多样文档布局数据集OmniLayout-1M，提出0.5B模型OmniLayout-LLM及两阶段粗到细学习并在多领域表现优异


<details>
  <summary>Details</summary>
Motivation: Lack of diverse, large-scale document layout datasets and poor generation on complex domains and long sequences

Method: Two-stage Coarse-to-Fine learning with OmniLayout-1M

Result: OmniLayout-LLM (0.5B) trained first on OmniLayout-1M then fine-tuned to domains, outperforms prior layout experts and general LLMs on M6Doc

Conclusion: 提供数据集和模型以提升开领域文档布局生成，显著优于现有方法并将公开发布代码与模型

Abstract: Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [30] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

TL;DR: AoT-PsyPhyBENCH用心理物理验证的视频基准评测VLM判断视频播放方向的能力，结果显示当前VLM普遍无法像人类那样快速准确识别时间箭头，暴露出对时间连续性与因果理解的根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多模态任务上表现优异，但在视频的时间信息理解上薄弱且评估不足。通过直接测试时间箭头判断，揭示模型在时间连续性与因果理解上的缺陷。

Method: 构建了以人为基准的短视频数据集和行为基线，覆盖物理不可逆过程（如自由落体、扩散/爆炸）和因果手工动作（如分割/合并）。对多种公开权重和专有、具备推理与非推理能力的VLM进行全面评估，比较模型与人类在相同刺激下的表现。

Result: 大多数模型在AoT任务上接近随机，最佳模型仍显著落后于人类，尤其在物理不可逆事件和因果手部动作上表现差距最大，表明缺乏捕捉时间连续性与因果关系的归纳偏置。

Conclusion: 本文提出了AoT-PsyPhyBENCH基准，通过心理物理学验证的自然视频刺激来评估视觉-语言模型（VLM）判断时间箭头（正放或倒放）的能力。评估显示大多数VLM接近随机猜测，即便是表现最好的模型在物理不可逆过程和因果手部动作上也远不及人类。

Abstract: Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [31] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

TL;DR: HCLFuse combines a variational bottleneck encoder and physically-guided diffusion to improve infrared-visible fusion, yielding better structure, detail, and segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing fusion methods struggle to balance modal information; generative models limited and lack interpretability, affecting reliability in complex scenarios. Inspiration from human cognition to improve generative fusion.

Method: Quantification of information mapping; multi-scale mask-regulated variational bottleneck encoder with posterior probability modeling and information decomposition; integrate diffusion generative model with time-varying physical guidance based on physical laws.

Result: Proposed HCLFuse: multi-scale mask-regulated variational bottleneck encoder for accurate low-level modal info; integrate diffusion model with physical laws for time-varying physical guidance; achieves SOTA fusion and better semantic segmentation.

Conclusion: HCLFuse, inspired by human cognition, enhances structural consistency and detail quality by quantifying information mapping and using probabilistic physical guidance, improving fusion and downstream tasks.

Abstract: Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [32] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

TL;DR: 训练并比较三种CNN（SqueezeNet、MobileNetv2、ResNet50）用于不同距离的眼周身份验证；ResNet50最强，但三网融合显著提升性能并创UBIPr新SOTA；LIME热图与JS散度显示网络关注不同区域，解释互补性。


<details>
  <summary>Details</summary>
Motivation: Investigate complementarity of different CNN architectures for periocular verification at varying distances using UBIPr database; determine whether fusing models improves performance over single CNNs and state-of-the-art.

Method: 在VGGFace2大规模眼部裁剪集上微调三种不同复杂度CNN（SqueezeNet、MobileNetv2、ResNet50）；使用余弦和chi2相似度评估，比较不同初始化，采用逻辑回归做分数级融合；用LIME生成关注热图并用Jensen-Shannon散度量化关注模式差异。

Result: ResNet50 best individually; score-level fusion of SqueezeNet, MobileNetv2, ResNet50 via logistic regression yields substantial gains, especially combining all three, achieving new state-of-the-art on UBIPr.

Conclusion: 模型互补性明显，融合多架构CNN并用得分级融合能显著提高远近距下的眼周验证性能；关注区域差异支持互补性机理；方法在UBIPr上达到新SOTA。

Abstract: We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>


### [33] [Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving](https://arxiv.org/abs/2510.26292)
*Lin Liu,Guanyi Yu,Ziying Song,Junqiao Li,Caiyan Jia,Feiyang Jia,Peiliang Wu,Yandan Luo*

Main category: cs.CV

TL;DR: CATG通过在流匹配生成过程中直接施加安全与动力学约束，结合可控的激进程度信号，提升轨迹生成的多样性与可控性，在NavSim v2上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决基于模仿学习的规划在模式坍缩和缺乏多样性的问题，并将安全与运动学约束直接融入生成过程，提高规划结果的可控性与安全性。

Method: 显式建模流匹配过程（Constrained Flow Matching），在匹配过程中直接施加安全与运动学约束；将驾驶激进程度参数化为生成时的条件信号；训练与评估在NavSim v2基准上。

Result: 提出CATG（Constrained Flow Matching）规划框架，通过显式建模流匹配并在生成过程中加入约束，避免模式坍缩、支持多样性与条件引导，并将驾驶激进程度作为控制信号。实验证明在NavSim v2挑战中取得第二名，EPDMS为51.31并获创新奖。

Conclusion: 在流匹配框架中引入显式约束与风格控制能有效缓解模式坍缩并保证轨迹满足安全与运动学要求，展示了在基准挑战中的实际效果。

Abstract: Planning is a critical component of end-to-end autonomous driving. However,
prevailing imitation learning methods often suffer from mode collapse, failing
to produce diverse trajectory hypotheses. Meanwhile, existing generative
approaches struggle to incorporate crucial safety and physical constraints
directly into the generative process, necessitating an additional optimization
stage to refine their outputs. To address these limitations, we propose CATG, a
novel planning framework that leverages Constrained Flow Matching. Concretely,
CATG explicitly models the flow matching process, which inherently mitigates
mode collapse and allows for flexible guidance from various conditioning
signals. Our primary contribution is the novel imposition of explicit
constraints directly within the flow matching process, ensuring that the
generated trajectories adhere to vital safety and kinematic rules. Secondly,
CATG parameterizes driving aggressiveness as a control signal during
generation, enabling precise manipulation of trajectory style. Notably, on the
NavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and
was honored with the Innovation Award.

</details>


### [34] [Leveraging Large-Scale Face Datasets for Deep Periocular Recognition via Ocular Cropping](https://arxiv.org/abs/2510.26294)
*Fernando Alonso-Fernandez,Kevin Hernandez-Diaz,Jose Maria Buades Rubio,Josef Bigun*

Main category: cs.CV

TL;DR: 用近两百万张眼周图像训练CNN，在高质量移动自拍数据上取得最优EER（1-2%），但在更不受控的大规模人脸图像上性能仍显著低于全脸识别（EER 9-15% vs 3-6%）。


<details>
  <summary>Details</summary>
Motivation: 利用眼周区域作为生物特征，因为该区域辨识度高且采集约束少，探究在大规模训练样本下，CNN能否提升periocular识别性能并在不同采集条件下的泛化能力。

Method: 对比三种不同深度与复杂度的卷积神经网络，在1,907,572张从VGGFace2提取的眼周裁剪图像上训练，测试集合包括VGGFace2-Pose的野外人脸图像子集与UFPR-Periocular移动自拍数据库，采用EER作为性能指标。

Result: 在VGGFace2-Pose（极不受控）上眼周裁剪图像的EER为9-15%，明显高于使用整张人脸时的3-6%；在UFPR-Periocular（高质量、受控采集）上达成1-2% EER，为该数据集目前最低报告值。

Conclusion: 该论文表明用大规模人脸数据库（VGGFace2）提取的眼周（periocular）图像训练CNN在移动与高质量自拍数据上表现良好，但在极不受控的大规模人脸图像（VGGFace2-Pose）上性能仍落后于完整人脸识别。

Abstract: We focus on ocular biometrics, specifically the periocular region (the area
around the eye), which offers high discrimination and minimal acquisition
constraints. We evaluate three Convolutional Neural Network architectures of
varying depth and complexity to assess their effectiveness for periocular
recognition. The networks are trained on 1,907,572 ocular crops extracted from
the large-scale VGGFace2 database. This significantly contrasts with existing
works, which typically rely on small-scale periocular datasets for training
having only a few thousand images. Experiments are conducted with ocular images
from VGGFace2-Pose, a subset of VGGFace2 containing in-the-wild face images,
and the UFPR-Periocular database, which consists of selfies captured via mobile
devices with user guidance on the screen. Due to the uncontrolled conditions of
VGGFace2, the Equal Error Rates (EERs) obtained with ocular crops range from
9-15%, noticeably higher than the 3-6% EERs achieved using full-face images. In
contrast, UFPR-Periocular yields significantly better performance (EERs of
1-2%), thanks to higher image quality and more consistent acquisition
protocols. To the best of our knowledge, these are the lowest reported EERs on
the UFPR dataset to date.

</details>


### [35] [Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology](https://arxiv.org/abs/2510.26297)
*Luting Wang,Yinghao Xiang,Hongliang Huang,Dongjun Li,Chen Gao,Si Liu*

Main category: cs.CV

TL;DR: 提出AEOS-Bench大规模真实调度基准与AEOS-Former约束感知Transformer模型，实验证明在任务完成与能效上效果显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模、动态和复杂约束下效果受限，缺乏大规模且真实的基准来评估AEOS星座调度方法，且模型难以兼顾多样场景与物理约束。

Method: 开发高保真仿真平台生成1到50颗卫星、50到300个成像任务的16410个场景并提供调度标注；设计AEOS-Former，采用Transformer架构和约束感知注意力机制，同时引入内部约束模块显式建模卫星物理与运控限制，通过基于仿真的迭代学习适配多样场景。

Result: 在AEOS-Bench上，AEOS-Former在任务完成率和能效上均优于基线模型；消融实验证明约束感知注意力与内部约束模块对性能提升有显著贡献。

Conclusion: 本文构建了首个面向真实航天器行为的大规模AEOS任务调度基准套件AEOS-Bench，并提出了基于Transformer的AEOS-Former调度模型，结合约束感知注意力与内部约束模块，通过仿真迭代学习在多场景下提高了任务完成率与能效。

Abstract: Agile Earth Observation Satellites (AEOSs) constellations offer unprecedented
flexibility for monitoring the Earth's surface, but their scheduling remains
challenging under large-scale scenarios, dynamic environments, and stringent
constraints. Existing methods often simplify these complexities, limiting their
real-world performance. We address this gap with a unified framework
integrating a standardized benchmark suite and a novel scheduling model. Our
benchmark suite, AEOS-Bench, contains $3,907$ finely tuned satellite assets and
$16,410$ scenarios. Each scenario features $1$ to $50$ satellites and $50$ to
$300$ imaging tasks. These scenarios are generated via a high-fidelity
simulation platform, ensuring realistic satellite behavior such as orbital
dynamics and resource constraints. Ground truth scheduling annotations are
provided for each scenario. To our knowledge, AEOS-Bench is the first
large-scale benchmark suite tailored for realistic constellation scheduling.
Building upon this benchmark, we introduce AEOS-Former, a Transformer-based
scheduling model that incorporates a constraint-aware attention mechanism. A
dedicated internal constraint module explicitly models the physical and
operational limits of each satellite. Through simulation-based iterative
learning, AEOS-Former adapts to diverse scenarios, offering a robust solution
for AEOS constellation scheduling. Experimental results demonstrate that
AEOS-Former outperforms baseline models in task completion and energy
efficiency, with ablation studies highlighting the contribution of each
component. Code and data are provided in
https://github.com/buaa-colalab/AEOSBench.

</details>


### [36] [Exploring the correlation between the type of music and the emotions evoked: A study using subjective questionnaires and EEG](https://arxiv.org/abs/2510.26304)
*Jelizaveta Jankowska,Bożena Kostek,Fernando Alonso-Fernandez,Prayag Tiwari*

Main category: cs.CV

TL;DR: 该研究通过问卷与EEG记录，考察不同音乐类型对情绪的影响，参与者多样化，分析表明情绪与脑电活动存在关联。


<details>
  <summary>Details</summary>
Motivation: 探究音乐类型如何影响人类情绪，并验证是否能通过EEG客观检测并与主观报告相对应。

Method: 在受试者听不同音乐时同步采集主观问卷（情绪评分）和EEG头盔数据，参与者包括不同性别和音乐偏好的人群；实验结束后进行问卷与EEG信号的关系分析，可能采用统计或信号处理方法来识别情绪相关的脑电特征。

Result: 分析发现情绪状态与EEG活动存在联系，不同音乐流派诱发的情绪差异可以在脑电信号中被检测到，问卷结果与EEG特征呈现相关性。

Conclusion: 实验表明不同音乐类型能引发可观测的情绪变化，并在EEG信号中体现出相关特征，问卷与脑电数据之间存在显著关联。

Abstract: The subject of this work is to check how different types of music affect
human emotions. While listening to music, a subjective survey and brain
activity measurements were carried out using an EEG helmet. The aim is to
demonstrate the impact of different music genres on emotions. The research
involved a diverse group of participants of different gender and musical
preferences. This had the effect of capturing a wide range of emotional
responses to music. After the experiment, a relationship analysis of the
respondents' questionnaires with EEG signals was performed. The analysis
revealed connections between emotions and observed brain activity.

</details>


### [37] [A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading](https://arxiv.org/abs/2510.26315)
*Junlai Qiu,Yunzhu Chen,Hao Zheng,Yawen Huang,Yuexiang Li*

Main category: cs.CV

TL;DR: 本文提出基于证据理论的混合骨干网络特征融合范式，将CNN与ViT提取的特征通过深度证据网络转化为支持证据，进而自适应地融合以提升糖尿病视网膜病变(DR)分级准确率并增强可解释性。在两个公开DR数据集上的实验显示了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是单一骨干（CNN或ViT）各有短板：CNN擅长局部特征但局部信息可能遗漏全局语义，ViT具备全局建模能力但对细节敏感性不足。通过融合二者可互补长处，提高DR分级的准确性与鲁棒性，同时通过证据理论获得可解释的融合过程。

Method: 方法包括：1) 使用CNN与ViT作为双重骨干分别提取局部与全局特征；2) 为每类骨干设计深度证据网络，将特征映射为支持某一分级的证据分布（置信度/不确定性）；3) 基于证据理论（例如Dempster-Shafer规则）聚合多源证据以形成总体“意见”，并据此自适应调整不同骨干间的融合权重；4) 在训练中联合优化分类损失与证据正则化以抑制过度自信并提升可靠性。

Result: 在两个公开DR分级数据集上的实验表明，所提出的混合模型在准确率等指标上超过若干最先进基线，同时融合过程与决策可解释性良好，展示了证据融合在医学图像诊断中的有效性。

Conclusion: 所提出的证据融合范式能够有效整合CNN与ViT的互补信息，不仅提升了DR分级性能，还提供了对特征融合与决策过程的可解释性，且在公开数据集上优于现有最先进方法。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged
and elderly people, which significantly impacts their daily lives and mental
health. To improve the efficiency of clinical screening and enable the early
detection of DR, a variety of automated DR diagnosis systems have been recently
established based on convolutional neural network (CNN) or vision Transformer
(ViT). However, due to the own shortages of CNN / ViT, the performance of
existing methods using single-type backbone has reached a bottleneck. One
potential way for the further improvements is integrating different kinds of
backbones, which can fully leverage the respective strengths of them
(\emph{i.e.,} the local feature extraction capability of CNN and the global
feature capturing ability of ViT). To this end, we propose a novel paradigm to
effectively fuse the features extracted by different backbones based on the
theory of evidence. Specifically, the proposed evidential fusion paradigm
transforms the features from different backbones into supporting evidences via
a set of deep evidential networks. With the supporting evidences, the
aggregated opinion can be accordingly formed, which can be used to adaptively
tune the fusion pattern between different backbones and accordingly boost the
performance of our hybrid model. We evaluated our method on two publicly
available DR grading datasets. The experimental results demonstrate that our
hybrid model not only improves the accuracy of DR grading, compared to the
state-of-the-art frameworks, but also provides the excellent interpretability
for feature fusion and decision-making.

</details>


### [38] [GLYPH-SR: Can We Achieve Both High-Quality Image Super-Resolution and High-Fidelity Text Recovery via VLM-guided Latent Diffusion Model?](https://arxiv.org/abs/2510.26339)
*Mingyu Sung,Seungjae Ham,Kangwoo Kim,Yeokyoung Yoon,Sangseok Yun,Il-Min Kim,Jae-Mo Kang*

Main category: cs.CV

TL;DR: GLYPH-SR通过OCR引导的ControlNet和交替调度，在复杂场景文本超分任务中同时提升识别率与感知质量。


<details>
  <summary>Details</summary>
Motivation: 动机是现有SR方法多优化PSNR/SSIM或通用感知指标，忽视字符级错误；以往文本超分工作多在简化基准（孤立字符）上，未考虑复杂自然场景中的文本，使得场景文本被当作普通纹理处理，导致OCR失败。

Method: 方法上引入了一个由OCR数据引导的Text-SR Fusion ControlNet（TS-ControlNet）和一个ping-pong调度器，在文本引导与场景引导之间交替，以在不改变主SR分支的情况下训练这些组件；训练使用合成语料以实现针对性文本恢复。

Result: 在SVT、SCUT-CTW1500、CUTE80等数据集的4x和8x超分上，GLYPH-SR在不牺牲MANIQA、CLIP-IQA、MUSIQ等感知指标的前提下，使OCR F1最多提升15.18个百分点（SVT x8，OpenOCR），实现可读性和视觉真实感的兼顾。

Conclusion: 本文提出了GLYPH-SR，一种面向场景文本超分辨率的视觉-语言引导扩散框架，旨在同时提升文本可读性和感知质量。

Abstract: Image super-resolution(SR) is fundamental to many vision system-from
surveillance and autonomy to document analysis and retail analytics-because
recovering high-frequency details, especially scene-text, enables reliable
downstream perception. Scene-text, i.e., text embedded in natural images such
as signs, product labels, and storefronts, often carries the most actionable
information; when characters are blurred or hallucinated, optical character
recognition(OCR) and subsequent decisions fail even if the rest of the image
appears sharp. Yet previous SR research has often been tuned to distortion
(PSNR/SSIM) or learned perceptual metrics (LIPIS, MANIQA, CLIP-IQA, MUSIQ) that
are largely insensitive to character-level errors. Furthermore, studies that do
address text SR often focus on simplified benchmarks with isolated characters,
overlooking the challenges of text within complex natural scenes. As a result,
scene-text is effectively treated as generic texture. For SR to be effective in
practical deployments, it is therefore essential to explicitly optimize for
both text legibility and perceptual quality. We present GLYPH-SR, a
vision-language-guided diffusion framework that aims to achieve both objectives
jointly. GLYPH-SR utilizes a Text-SR Fusion ControlNet(TS-ControlNet) guided by
OCR data, and a ping-pong scheduler that alternates between text- and
scene-centric guidance. To enable targeted text restoration, we train these
components on a synthetic corpus while keeping the main SR branch frozen.
Across SVT, SCUT-CTW1500, and CUTE80 at x4, and x8, GLYPH-SR improves OCR F1 by
up to +15.18 percentage points over diffusion/GAN baseline (SVT x8, OpenOCR)
while maintaining competitive MANIQA, CLIP-IQA, and MUSIQ. GLYPH-SR is designed
to satisfy both objectives simultaneously-high readability and high visual
realism-delivering SR that looks right and reds right.

</details>


### [39] [EEG-Driven Image Reconstruction with Saliency-Guided Diffusion Models](https://arxiv.org/abs/2510.26391)
*Igor Abramov,Ilya Makarov*

Main category: cs.CV

TL;DR: 本文通过将EEG嵌入与显著性地图联合用于Stable Diffusion（LoRA微调+ControlNet），显著提升EEG驱动图像重建的空间控制与语义一致性，在THINGS-EEG上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高基于EEG的图像重建在空间注意力和语义一致性方面的表现，解决现有方法忽视空间注意力导致的重建不准确和语义模糊问题。

Method: 1) 用Adaptive Thinking Mapper(ATM)提取EEG特征并生成嵌入；2) 基于LoRA对Stable Diffusion 2.1进行微调，使嵌入与视觉语义对齐；3) 使用ControlNet分支，将显著图作为空间条件输入以控制生成位置；4) 在THINGS-EEG数据集上评估低级（如颜色、纹理）与高级（语义一致性）特征的重建质量，并与人类注意力热图比较。

Result: 提出双条件框架：将EEG嵌入与空间显著图联合用于图像生成。使用Adaptive Thinking Mapper提取EEG特征，基于LoRA微调Stable Diffusion 2.1使神经信号与视觉语义对齐，并用ControlNet分支通过显著图提供空间控制。在THINGS-EEG数据集上实验，显著提升低级和高级视觉特征的重建质量，并与人类视觉注意力高度一致。

Conclusion: 引入注意力先验可缓解EEG信号的不确定性，提升重建图像的保真度与语义清晰度；采用预训练扩散模型的高效适配策略（ATM+LoRA+ControlNet）为医学诊断和神经自适应接口等应用提供可行路径。

Abstract: Existing EEG-driven image reconstruction methods often overlook spatial
attention mechanisms, limiting fidelity and semantic coherence. To address
this, we propose a dual-conditioning framework that combines EEG embeddings
with spatial saliency maps to enhance image generation. Our approach leverages
the Adaptive Thinking Mapper (ATM) for EEG feature extraction and fine-tunes
Stable Diffusion 2.1 via Low-Rank Adaptation (LoRA) to align neural signals
with visual semantics, while a ControlNet branch conditions generation on
saliency maps for spatial control. Evaluated on THINGS-EEG, our method achieves
a significant improvement in the quality of low- and high-level image features
over existing approaches. Simultaneously, strongly aligning with human visual
attention. The results demonstrate that attentional priors resolve EEG
ambiguities, enabling high-fidelity reconstructions with applications in
medical diagnostics and neuroadaptive interfaces, advancing neural decoding
through efficient adaptation of pre-trained diffusion models.

</details>


### [40] [LoCoT2V-Bench: A Benchmark for Long-Form and Complex Text-to-Video Generation](https://arxiv.org/abs/2510.26412)
*Xiangqing Zheng,Chengyue Wu,Kehai Chen,Min Zhang*

Main category: cs.CV

TL;DR: LoCoT2V-Bench为长视频生成提供复杂真实提示与多维评估，揭示当前模型在事件一致性与高层叙事维度的不足。


<details>
  <summary>Details</summary>
Motivation: 长期文本到视频（LVG）系统在复杂提示下的性能尚不清楚；现有评估基准过于简单，缺乏对事件级、一致性及抽象叙事特征的衡量。

Method: 基于真实长视频设计复杂提示集（含场景切换和事件动态），构建多维评价指标并对九个LVG模型进行量化与定性评估，分析弱点与改进方向。

Result: 提出LoCoT2V-Bench：包含真实视频衍生的复杂提示集与多维评估框架，引入事件级对齐、细粒度时间一致性、内容清晰度与HERD等新指标，并在九个代表性模型上做系统评估，发现模型在跨事件一致性、细粒度对齐和主题遵循等方面表现弱。

Conclusion: LoCoT2V-Bench是评估长视频生成复杂提示下表现的可靠平台，指出未来应在事件级建模、主题保持与人物发展等方向改进。

Abstract: Recently text-to-video generation has made impressive progress in producing
short, high-quality clips, but evaluating long-form outputs remains a major
challenge especially when processing complex prompts. Existing benchmarks
mostly rely on simplified prompts and focus on low-level metrics, overlooking
fine-grained alignment with prompts and abstract dimensions such as narrative
coherence and thematic expression. To address these gaps, we propose
LoCoT2V-Bench, a benchmark specifically designed for long video generation
(LVG) under complex input conditions. Based on various real-world videos,
LoCoT2V-Bench introduces a suite of realistic and complex prompts incorporating
elements like scene transitions and event dynamics. Moreover, it constructs a
multi-dimensional evaluation framework that includes our newly proposed metrics
such as event-level alignment, fine-grained temporal consistency, content
clarity, and the Human Expectation Realization Degree (HERD) that focuses on
more abstract attributes like narrative flow, emotional response, and character
development. Using this framework, we conduct a comprehensive evaluation of
nine representative LVG models, finding that while current methods perform well
on basic visual and temporal aspects, they struggle with inter-event
consistency, fine-grained alignment, and high-level thematic adherence, etc.
Overall, LoCoT2V-Bench provides a comprehensive and reliable platform for
evaluating long-form complex text-to-video generation and highlights critical
directions for future method improvement.

</details>


### [41] [A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models](https://arxiv.org/abs/2510.26441)
*Shihab Aaqil Ahamed,Udaya S. K. P. Miriya Thanthrige,Ranga Rodrigo,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: A-TPT通过最大化文本提示诱发的归一化特征在超球面上的最小角距离，引入角度多样性以改善VLM的测试时校准，实验显示比现有方法有更低的校准误差且精度相当，并具理论分析支持。


<details>
  <summary>Details</summary>
Motivation: 提高测试时提示调优（TPT）在无标注情况下适配大规模视觉-语言模型（VLMs）到未见任务时的校准性能，解决文本特征缺乏角度分散性导致的可靠性和安全性问题。

Method: 在TPT过程中对可学习文本提示产生的归一化文本特征施加最大化最小成对角距离的目标，即在单位超球面上优化角度间隔，从而达到均匀分布；通过理论分析和大规模实验比较证实其效果。

Result: 提出A-TPT框架，通过在单位超球面上最大化类间文本特征的最小成对角距离来引入角度多样性，从而实现归一化文本特征分布的均匀性。在多个骨干网络和数据集上实验表明，A-TPT在保持精度的同时显著降低了平均校准误差，并能在自然分布漂移和医疗数据集上表现出更好的零样本校准。

Conclusion: 促进角度多样性能实现更均匀分布的文本特征，显著提升测试时适配下VLM的校准性能；A-TPT在多种场景（包括分布漂移和医学任务）上验证了其有效性。

Abstract: Test-time prompt tuning (TPT) has emerged as a promising technique for
adapting large vision-language models (VLMs) to unseen tasks without relying on
labeled data. However, the lack of dispersion between textual features can hurt
calibration performance, which raises concerns about VLMs' reliability,
trustworthiness, and safety. Current TPT approaches primarily focus on
improving prompt calibration by either maximizing average textual feature
dispersion or enforcing orthogonality constraints to encourage angular
separation. However, these methods may not always have optimal angular
separation between class-wise textual features, which implies overlooking the
critical role of angular diversity. To address this, we propose A-TPT, a novel
TPT framework that introduces angular diversity to encourage uniformity in the
distribution of normalized textual features induced by corresponding learnable
prompts. This uniformity is achieved by maximizing the minimum pairwise angular
distance between features on the unit hypersphere. We show that our approach
consistently surpasses state-of-the-art TPT methods in reducing the aggregate
average calibration error while maintaining comparable accuracy through
extensive experiments with various backbones on different datasets. Notably,
our approach exhibits superior zero-shot calibration performance on natural
distribution shifts and generalizes well to medical datasets. We provide
extensive analyses, including theoretical aspects, to establish the grounding
of A-TPT. These results highlight the potency of promoting angular diversity to
achieve well-dispersed textual features, significantly improving VLM
calibration during test-time adaptation. Our code will be made publicly
available.

</details>


### [42] [PointSt3R: Point Tracking through 3D Grounded Correspondence](https://arxiv.org/abs/2510.26443)
*Rhodri Guerrier,Adam W. Harley,Dima Damen*

Main category: cs.CV

TL;DR: Fine-tune 3D reconstruction models for point tracking using dynamic/static correspondences and visibility head; achieves competitive or better performance across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Adapt foundational 3D reconstruction models (DUSt3R, MASt3R) for point tracking via 3D grounded correspondence, addressing dynamic correspondence and visibility.

Method: Fine-tune MASt3R with added dynamic correspondence loss and visibility head using synthetic training pairs (one frame contains query point), mixing static and dynamic correspondences, and evaluate on multiple benchmarks.

Result: Fine-tuned MASt3R (PointSt3R) with reconstruction + dynamic correspondence loss and visibility head, trained on synthetic data pairs, achieves competitive or superior tracking on multiple datasets (EgoPoints, TAP-Vid-DAVIS, RGB-S) and shows improvements over CoTracker2/3.

Conclusion: PointSt3R effectively repurposes 3D reconstruction models for point tracking without temporal context, improving performance on several datasets; dynamic correspondence training and visibility modeling are key.

Abstract: Recent advances in foundational 3D reconstruction models, such as DUSt3R and
MASt3R, have shown great potential in 2D and 3D correspondence in static
scenes. In this paper, we propose to adapt them for the task of point tracking
through 3D grounded correspondence. We first demonstrate that these models are
competitive point trackers when focusing on static points, present in current
point tracking benchmarks ($+33.5\%$ on EgoPoints vs. CoTracker2). We propose
to combine the reconstruction loss with training for dynamic correspondence
along with a visibility head, and fine-tuning MASt3R for point tracking using a
relatively small amount of synthetic data. Importantly, we only train and
evaluate on pairs of frames where one contains the query point, effectively
removing any temporal context. Using a mix of dynamic and static point
correspondences, we achieve competitive or superior point tracking results on
four datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\delta_{avg}$ / 85.8\%
occlusion acc. for PointSt3R compared to 75.7 / 88.3\% for CoTracker2; and
significantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs
82.8). We also present results on 3D point tracking along with several
ablations on training datasets and percentage of dynamic correspondences.

</details>


### [43] [Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection](https://arxiv.org/abs/2510.26464)
*Yuanting Fan,Jun Liu,Xiaochen Chen,Bin-Bin Gao,Jian Li,Yong Liu,Jinlong Peng,Chengjie Wang*

Main category: cs.CV

TL;DR: 通过自动构建多层次细粒度文本描述并设计多层可学习提示与语义对齐策略，FineGrainedAD改善了少样本异常检测的异常定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言预训练模型的FSAD方法由于文本描述粗粒度、与图像补丁语义不对齐，导致定位性能受限，需引入更细粒度的文本描述与多层次对齐机制。

Method: 自动构建多层次细粒度语义描述（MFSC），并基于此设计FineGrainedAD框架，包含多层可学习提示（MLLP）和多层语义对齐（MLSA）：MLLP通过自动替换与拼接机制将细粒度语义引入多层可学习提示；MLSA设计区域聚合策略与多层对齐训练，使提示更好地与相应视觉区域对齐。

Result: 在少样本设置下，FineGrainedAD在MVTec-AD与VisA数据集上实现了整体定位性能的提升，优于现有方法。

Conclusion: 本文提出通过构建多层次细粒度语义文本（MFSC）并设计FineGrainedAD框架，显著改善了少样本异常检测中的定位性能，尤其在MVTec-AD和VisA数据集上表现优越。

Abstract: Few-shot anomaly detection (FSAD) methods identify anomalous regions with few
known normal samples. Most existing methods rely on the generalization ability
of pre-trained vision-language models (VLMs) to recognize potentially anomalous
regions through feature similarity between text descriptions and images.
However, due to the lack of detailed textual descriptions, these methods can
only pre-define image-level descriptions to match each visual patch token to
identify potential anomalous regions, which leads to the semantic misalignment
between image descriptions and patch-level visual anomalies, achieving
sub-optimal localization performance. To address the above issues, we propose
the Multi-Level Fine-Grained Semantic Caption (MFSC) to provide multi-level and
fine-grained textual descriptions for existing anomaly detection datasets with
automatic construction pipeline. Based on the MFSC, we propose a novel
framework named FineGrainedAD to improve anomaly localization performance,
which consists of two components: Multi-Level Learnable Prompt (MLLP) and
Multi-Level Semantic Alignment (MLSA). MLLP introduces fine-grained semantics
into multi-level learnable prompts through automatic replacement and
concatenation mechanism, while MLSA designs region aggregation strategy and
multi-level alignment training to facilitate learnable prompts better align
with corresponding visual regions. Experiments demonstrate that the proposed
FineGrainedAD achieves superior overall performance in few-shot settings on
MVTec-AD and VisA datasets.

</details>


### [44] [Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition](https://arxiv.org/abs/2510.26466)
*Pei Peng,MingKun Xie,Hang Hao,Tong Jin,ShengJun Huang*

Main category: cs.CV

TL;DR: 通过在CLIP表示空间合成反事实（对象与替代背景重组）并估计总直接效应，论文在不训练模型的情况下，显著减少对象-背景快捷方式带来的错误，提升零样本情境鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型依赖训练中常见的对象-背景共现，导致在测试场景与训练共现不同的情况下表现不可靠。论文将此问题表述为因果推断问题，旨在评估并校正模型对背景提示的依赖，保证当对象处在不同环境时预测的稳定性。

Method: 在CLIP的表示空间中分别估计对象与背景的期望表示，通过从外部数据集、批次邻居或文本描述中采样多样化替代背景，将对象特征与这些背景重组以合成反事实嵌入；利用TDE估计和背景仅激活的减法来模拟干预，保留有利的对象-背景交互同时降低虚构分数；整个过程在推理时进行，无需额外训练或提示工程。

Result: 在若干情境敏感基准上，方法在最差组和平均准确率上均有显著提升，达成新的零样本最先进水平；此外提出了一种轻量级的表示层反事实方法，为多模态推理的因果去偏和可靠性提供了实用路径。

Conclusion: 该论文提出了一种基于因果推断的推理时对比事实生成方法，通过在CLIP表示空间中重组对象和背景特征并估计总直接效应（TDE），抑制对象-背景快捷方式的误导影响，从而提升零样本条件下对情境敏感任务的鲁棒性。

Abstract: Object-context shortcuts remain a persistent challenge in vision-language
models, undermining zero-shot reliability when test-time scenes differ from
familiar training co-occurrences. We recast this issue as a causal inference
problem and ask: Would the prediction remain if the object appeared in a
different environment? To answer this at inference time, we estimate object and
background expectations within CLIP's representation space, and synthesize
counterfactual embeddings by recombining object features with diverse
alternative contexts sampled from external datasets, batch neighbors, or
text-derived descriptions. By estimating the Total Direct Effect and simulating
intervention, we further subtract background-only activation, preserving
beneficial object-context interactions while mitigating hallucinated scores.
Without retraining or prompt design, our method substantially improves both
worst-group and average accuracy on context-sensitive benchmarks, establishing
a new zero-shot state of the art. Beyond performance, our framework provides a
lightweight representation-level counterfactual approach, offering a practical
causal avenue for debiased and reliable multimodal reasoning.

</details>


### [45] [Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing](https://arxiv.org/abs/2510.26474)
*Xin Guo,Zhiheng Xi,Yiwen Ding,Yitao Zhai,Xiaowei Shi,Xunliang Cai,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CV

TL;DR: 本工作指出自我提升过程中出现的“马太效应”：模型优先掌握简单样本的推理能力，忽视复杂样本，导致性能瓶颈。提出四种高效策略（分布重塑和轨迹重采样两类）对头尾数据进行再平衡，以提升探索-学习的自我提升效果。在Qwen2-VL-7B-Instruct和InternVL2.5-4B上的视觉推理任务上，平均优于原始自我提升3.86点。


<details>
  <summary>Details</summary>
Motivation: 观察到在自我探索与学习过程中，模型生成高质量轨迹集中在简单查询（头数据），难查询（尾数据）得到的成功轨迹稀少，导致训练偏向简单技能，阻碍长期提升。

Method: 提出四种策略，分为（1）分布重塑：通过修改采样分布，使模型更多接触难样本；（2）轨迹重采样：针对生成的成功轨迹按难度或稀有性重采样以增强学习信号。结合两类策略在自我提升循环中应用以实现头尾均衡。

Result: 在两款主流多模态模型上以及多项视觉推理任务中，提出的方法相比原始自我提升平均提升3.86点，显示出稳定的性能提升。

Conclusion: 通过对探索阶段的样本分布与成功轨迹采样进行重新设计，可以缓解自我提升中的马太效应，从而提升模型在复杂视觉推理任务上的表现，实验显示方法稳定有效。

Abstract: Self-improvement has emerged as a mainstream paradigm for advancing the
reasoning capabilities of large vision-language models (LVLMs), where models
explore and learn from successful trajectories iteratively. However, we
identify a critical issue during this process: the model excels at generating
high-quality trajectories for simple queries (i.e., head data) but struggles
with more complex ones (i.e., tail data). This leads to an imbalanced
optimization that drives the model to prioritize simple reasoning skills, while
hindering its ability to tackle more complex reasoning tasks. Over iterations,
this imbalance becomes increasingly pronounced--a dynamic we term the "Matthew
effect"--which ultimately hinders further model improvement and leads to
performance bottlenecks. To counteract this challenge, we introduce four
efficient strategies from two perspectives: distribution-reshaping and
trajectory-resampling, to achieve head-tail re-balancing during the
exploration-and-learning self-improvement process. Extensive experiments on
Qwen2-VL-7B-Instruct and InternVL2.5-4B models across visual reasoning tasks
demonstrate that our methods consistently improve visual reasoning
capabilities, outperforming vanilla self-improvement by 3.86 points on average.

</details>


### [46] [Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm](https://arxiv.org/abs/2510.26509)
*Vinícius Ferraria,Eurico Ruivo*

Main category: cs.CV

TL;DR: 提出一种基于二维元胞自动机并由元启发式优化与迁移学习辅助的可适应边缘检测器。扩展优化搜索空间无效，模型能自适应输入，但迁移学习未显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统边缘检测器在检测稀疏/松散边缘和在特定场景下缺乏上下文信息时表现欠佳，需一种能根据图像特性自适应调整的检测器。

Method: 构建二维元胞自动机边缘检测器，采用元启发式算法（未具体指明是哪种）对检测器参数进行优化，并尝试通过扩大优化搜索空间和应用迁移学习技术来提升性能；通过在自然图像集及其子集上进行多组实验与验证来评估效果。

Result: 实验结果表明：扩大优化搜索空间并未在该数据集上提高性能；模型在多次验证中表现出对输入的适应能力；迁移学习的应用未带来显著改进。

Conclusion: 该论文提出了基于二维元胞自动机的可适应边缘检测器，并结合元启发式优化和迁移学习进行参数调整，结论是：扩大优化搜索空间对所选图像集无明显效果；模型具有一定的输入适应性；所用迁移学习未带来显著提升。

Abstract: The edge detection task is essential in image processing aiming to extract
relevant information from an image. One recurring problem in this task is the
weaknesses found in some detectors, such as the difficulty in detecting loose
edges and the lack of context to extract relevant information from specific
problems. To address these weaknesses and adapt the detector to the properties
of an image, an adaptable detector described by two-dimensional cellular
automaton and optimized by meta-heuristic combined with transfer learning
techniques was developed. This study aims to analyze the impact of expanding
the search space of the optimization phase and the robustness of the
adaptability of the detector in identifying edges of a set of natural images
and specialized subsets extracted from the same image set. The results obtained
prove that expanding the search space of the optimization phase was not
effective for the chosen image set. The study also analyzed the adaptability of
the model through a series of experiments and validation techniques and found
that, regardless of the validation, the model was able to adapt to the input
and the transfer learning techniques applied to the model showed no significant
improvements.

</details>


### [47] [SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformation for Spine Segmentation from Ultrasound Volume Projection Imaging](https://arxiv.org/abs/2510.26568)
*Hao Xie,Zixun Huang,Yushen Zuo,Yakun Ju,Frank H. F. Leung,N. F. Law,Kin-Man Lam,Yong-Ping Zheng,Sai Ho Ling*

Main category: cs.CV

TL;DR: SA^2Net通过尺度自适应跨维度特征学习与基于类别亲和的结构变换结合Transformer解码，实现对脊柱VPI图像的结构感知分割，提升了分割精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决脊柱超声体积投影成像(VPI)中分割脊柱以辅助智能脊柱侧弯诊断时的挑战，尤其是跨维度长距离相关特征学习和脊柱结构知识的编码问题。

Method: 1) 尺度自适应互补策略：跨维度学习长距离相关特征以捕获骨骼全局上下文；2) 结构-亲和变换：基于类特定亲和矩阵将语义特征转换并与Transformer解码器结合，进行结构感知推理；3) 特征混合损失聚合：多尺度特征损失聚合以增强训练鲁棒性；4) 在多个主干网络上进行评估并与SOTA方法比较。

Result: 提出了一种新的尺度自适应结构感知网络(SA^2Net)，包含尺度自适应互补策略用于学习跨维度长距离相关特征、结构亲和变换结合Transformer解码器用于结构感知推理，以及特征混合损失聚合来增强训练。实验显示其在分割任务上优于现有方法，并能适配多种骨干网络。代码开源。

Conclusion: SA^2Net有效结合了跨维长距离相关特征与结构亲和信息，并通过损失聚合提高训练稳定性，实验证明其在脊柱分割任务中优于现有方法，适配性强，有助于智能脊柱侧弯诊断。

Abstract: Spine segmentation, based on ultrasound volume projection imaging (VPI),
plays a vital role for intelligent scoliosis diagnosis in clinical
applications. However, this task faces several significant challenges. Firstly,
the global contextual knowledge of spines may not be well-learned if we neglect
the high spatial correlation of different bone features. Secondly, the spine
bones contain rich structural knowledge regarding their shapes and positions,
which deserves to be encoded into the segmentation process. To address these
challenges, we propose a novel scale-adaptive structure-aware network
(SA$^{2}$Net) for effective spine segmentation. First, we propose a
scale-adaptive complementary strategy to learn the cross-dimensional
long-distance correlation features for spinal images. Second, motivated by the
consistency between multi-head self-attention in Transformers and semantic
level affinity, we propose structure-affinity transformation to transform
semantic features with class-specific affinity and combine it with a
Transformer decoder for structure-aware reasoning. In addition, we adopt a
feature mixing loss aggregation method to enhance model training. This method
improves the robustness and accuracy of the segmentation process. The
experimental results demonstrate that our SA$^{2}$Net achieves superior
segmentation performance compared to other state-of-the-art methods. Moreover,
the adaptability of SA$^{2}$Net to various backbones enhances its potential as
a promising tool for advanced scoliosis diagnosis using intelligent spinal
image analysis. The code and experimental demo are available at
https://github.com/taetiseo09/SA2Net.

</details>


### [48] [AdSum: Two-stream Audio-visual Summarization for Automated Video Advertisement Clipping](https://arxiv.org/abs/2510.26569)
*Wen Xie,Yanjun Zhu,Gijs Overgoor,Yakov Bart,Agata Lapedriza Garcia,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 本文提出用于广告视频自动裁剪的框架，把裁剪看作分镜选择问题，强调音频在广告中的重要性，提出两流音视频融合模型预测帧的重要性，并发布AdSum204数据集（102对30s和15s广告）。实验显示在AP、AUC、Spearman和Kendall等指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 广告主需要同一广告的不同时长版本，人工从长视频中手动挑选并重剪耗时费力；现有视频摘要方法侧重视觉内容，未充分考虑广告中音频（配乐、旁白、关键信息）的作用。故提出专门面向广告裁剪的方法并引入音频信息。

Method: 构建两流音视频融合网络，分别提取视觉和音频特征并融合，用于预测每帧被选入短广告的概率（重要性分数）。利用这些分数进行分镜选择以生成不同时长版本。并构建AdSum204数据集用于训练与评估，比较Average Precision、AUC、Spearman和Kendall等指标。

Result: 在AdSum204数据集上，所提两流音视频模型在平均精度（AP）、AUC、Spearman相关系数和Kendall秩相关等度量上超过现有最先进的视频摘要算法，表明方法更适合广告裁剪任务。

Conclusion: 将视频裁剪视为面向广告的分镜选择问题，并通过融入音频信息的两流模型给予显著性能提升；所提方法在AdSum204数据集上优于现有视频摘要方法，证明音频在广告裁剪中的重要性。

Abstract: Advertisers commonly need multiple versions of the same advertisement (ad) at
varying durations for a single campaign. The traditional approach involves
manually selecting and re-editing shots from longer video ads to create shorter
versions, which is labor-intensive and time-consuming. In this paper, we
introduce a framework for automated video ad clipping using video summarization
techniques. We are the first to frame video clipping as a shot selection
problem, tailored specifically for advertising. Unlike existing general video
summarization methods that primarily focus on visual content, our approach
emphasizes the critical role of audio in advertising. To achieve this, we
develop a two-stream audio-visual fusion model that predicts the importance of
video frames, where importance is defined as the likelihood of a frame being
selected in the firm-produced short ad. To address the lack of ad-specific
datasets, we present AdSum204, a novel dataset comprising 102 pairs of
30-second and 15-second ads from real advertising campaigns. Extensive
experiments demonstrate that our model outperforms state-of-the-art methods
across various metrics, including Average Precision, Area Under Curve,
Spearman, and Kendall.

</details>


### [49] [Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios](https://arxiv.org/abs/2510.26580)
*Manjunath Prasad Holenarasipura Rajiv,B. M. Vidyavathi*

Main category: cs.CV

TL;DR: 提出将预训练视觉Transformer与大语言模型对齐并通过动态推理模块融合全局与对象交互，实现零样本场景理解，基准测试显示显著性能提升（最高+18%）且具可解释性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 动机是现实世界中视觉系统常遇到未见情境且缺乏标注，传统模型泛化能力差，限制应用部署，故需一个可扩展且可解释的零样本场景理解方案。

Method: 方法上，利用预训练视觉Transformer与大规模语言模型对齐视觉语义与自然语言描述，构建动态推理模块，融合全局场景线索与对象级交互，并利用语言先验指导预测修正。

Result: 在COCO、Visual Genome和Open Images等零样本基准上，框架在复杂和未见环境中比基线模型最高提升约18%场景理解准确率，并在混乱或模糊场景中表现更鲁棒。

Conclusion: 该论文提出了一个结合视觉-语言对齐的动态上下文感知场景推理框架，旨在提升零样本（zero-shot）下的场景理解能力，能够在无标签、未见环境中推断并自适应。

Abstract: In real-world environments, AI systems often face unfamiliar scenarios
without labeled data, creating a major challenge for conventional scene
understanding models. The inability to generalize across unseen contexts limits
the deployment of vision-based applications in dynamic, unstructured settings.
This work introduces a Dynamic Context-Aware Scene Reasoning framework that
leverages Vision-Language Alignment to address zero-shot real-world scenarios.
The goal is to enable intelligent systems to infer and adapt to new
environments without prior task-specific training. The proposed approach
integrates pre-trained vision transformers and large language models to align
visual semantics with natural language descriptions, enhancing contextual
comprehension. A dynamic reasoning module refines predictions by combining
global scene cues and object-level interactions guided by linguistic priors.
Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and
Open Images demonstrate up to 18% improvement in scene understanding accuracy
over baseline models in complex and unseen environments. Results also show
robust performance in ambiguous or cluttered scenes due to the synergistic
fusion of vision and language. This framework offers a scalable and
interpretable approach for context-aware reasoning, advancing zero-shot
generalization in dynamic real-world settings.

</details>


### [50] [CATCH: A Modular Cross-domain Adaptive Template with Hook](https://arxiv.org/abs/2510.26582)
*Xinjin Li,Yulie Lu,Jinghan Cao,Yu Ma,Zhenglin Li,Yeyang Zhou*

Main category: cs.CV

TL;DR: CATCH通过域分类器与语言/视觉双适配器的动态注入，提供一种轻量可插拔的跨域VQA适配方案，在多领域基准上显著提升泛化性能且不需重训练主干。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的VQA在自然图像上表现优异，但在遥感、医学、数学图表等域外分布严重下降；传统的逐域微调或专门化管线代价高、扩展性差，需轻量、可插拔的跨域适配方案。

Method: 提出一个可插拔的跨域自适应框架，包括：1) 域分类器识别输入图像类型；2) 双适配器机制：Prompt Adapter用于语言提示调节，Visual Adapter用于视觉特征调整；3) 统一hook接口动态注入模块，无需重训练主干。

Result: 在四个领域特定VQA基准上取得稳定提升：MathVQA +2.3 BLEU，MedVQA-RAD +2.6 VQA（可能指准确率或分数），ChartQA +3.1 ROUGE，且无需重训练主干模型。

Conclusion: CATCH能在不改动主干模型的前提下，通过域分类器和双适配器（Prompt Adapter与Visual Adapter）分别对语言与视觉进行调节，从而提升VQA模型在跨域场景的泛化能力，适配性好且可插拔。

Abstract: Recent advances in Visual Question Answering (VQA) have demonstrated
impressive performance in natural image domains, with models like LLaVA
leveraging large language models (LLMs) for open-ended reasoning. However,
their generalization degrades significantly when transferred to out-of-domain
scenarios such as remote sensing, medical imaging, or math diagrams, due to
large distributional shifts and the lack of effective domain adaptation
mechanisms. Existing approaches typically rely on per-domain fine-tuning or
bespoke pipelines, which are costly, inflexible, and not scalable across
diverse tasks. In this paper, we propose CATCH, a plug-and-play framework for
cross-domain adaptation that improves the generalization of VQA models while
requiring minimal changes to their core architecture. Our key idea is to
decouple visual and linguistic adaptation by introducing two lightweight
modules: a domain classifier to identify the input image type, and a dual
adapter mechanism comprising a Prompt Adapter for language modulation and a
Visual Adapter for vision feature adjustment. Both modules are dynamically
injected via a unified hook interface, requiring no retraining of the backbone
model. Experimental results across four domain-specific VQA benchmarks
demonstrate that our framework achieves consistent performance gains without
retraining the backbone model, including +2.3 BLEU on MathVQA, +2.6 VQA on
MedVQA-RAD, and +3.1 ROUGE on ChartQA. These results highlight that CATCH
provides a scalable and extensible approach to multi-domain VQA, enabling
practical deployment across diverse application domains.

</details>


### [51] [Emu3.5: Native Multimodal Models are World Learners](https://arxiv.org/abs/2510.26583)
*Yufeng Cui,Honghao Chen,Haoge Deng,Xu Huang,Xinghang Li,Jirong Liu,Yang Liu,Zhuoyan Luo,Jinsheng Wang,Wenxuan Wang,Yueze Wang,Chengyuan Wang,Fan Zhang,Yingli Zhao,Ting Pan,Xianduo Li,Zecheng Hao,Wenxuan Ma,Zhuo Chen,Yulong Ao,Tiejun Huang,Zhongyuan Wang,Xinlong Wang*

Main category: cs.CV

TL;DR: Emu3.5是用10万亿token视频序列数据端到端训练的多模态世界模型，支持交错视-语生成，增强型RL优化，且通过DiDA将图像推理加速约20倍，在生成与交互任务上表现优异并已开源。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型多以文本为中心或采用模态分离的架构，难以原生处理交错视-语序列与长期时空一致性；为此提出一个能在视觉与语言间本地预测下一状态的统一世界模型并提升推理效率以适应实际应用。

Method: 1) 端到端统一下一个token预测目标的预训练，语料以互联网视频帧与转录为主，规模超过10万亿token；2) 后续大规模强化学习微调以提升多模态推理/生成；3) 提出离散扩散适配（DiDA），将逐步自回归解码改为双向并行的离散扩散式预测以提升推理效率；4) 实验包括图像生成编辑、交错视-语生成及开放世界具身任务对比基线（如Gemini）。

Result: 提出 Emu3.5，一个端到端预训练的大规模多模态世界模型，能在视觉与语言间本地预测下一状态，使用统一的下一个token预测目标，在含有超过10万亿token的视频帧与转录文本的语料上训练；支持交错的视-语输入与输出，经大规模强化学习后进一步提升推理与生成能力，并提出离散扩散适配（DiDA）将逐token解码转为双向并行预测，使单图推理加速约20倍；展示了长时程视-语生成、任意到图像生成（X2I）、复杂文本图像生成及在开放世界的时空一致探索与具身操作能力；在图像生成编辑上与Gemini 2.5 Flash Image相当，并在交错生成任务上优于其。

Conclusion: Emu3.5 展现了统一的视-语世界建模能力与高效推理方法（DiDA），能实现长时程一致性、多模态生成与开放世界操作，具备工业级生成与交互水平并开源促进研究。

Abstract: We introduce Emu3.5, a large-scale multimodal world model that natively
predicts the next state across vision and language. Emu3.5 is pre-trained
end-to-end with a unified next-token prediction objective on a corpus of
vision-language interleaved data containing over 10 trillion tokens, primarily
derived from sequential frames and transcripts of internet videos. The model
naturally accepts interleaved vision-language inputs and generates interleaved
vision-language outputs. Emu3.5 is further post-trained with large-scale
reinforcement learning to enhance multimodal reasoning and generation. To
improve inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),
which converts token-by-token decoding into bidirectional parallel prediction,
accelerating per-image inference by about 20x without sacrificing performance.
Emu3.5 exhibits strong native multimodal capabilities, including long-horizon
vision-language generation, any-to-image (X2I) generation, and complex
text-rich image generation. It also exhibits generalizable world-modeling
abilities, enabling spatiotemporally consistent world exploration and
open-world embodied manipulation across diverse scenarios and tasks. For
comparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image
(Nano Banana) on image generation and editing tasks and demonstrates superior
results on a suite of interleaved generation tasks. We open-source Emu3.5 at
https://github.com/baaivision/Emu3.5 to support community research.

</details>


### [52] [ResMatching: Noise-Resilient Computational Super-Resolution via Guided Conditional Flow Matching](https://arxiv.org/abs/2510.26601)
*Anirban Ray,Vera Galinova,Florian Jug*

Main category: cs.CV

TL;DR: ResMatching uses guided conditional flow matching to learn priors for microscopy CSR, yielding strong fidelity/realism trade-offs, calibrated posterior sampling, and uncertainty maps, outperforming baselines on BioSR tasks.


<details>
  <summary>Details</summary>
Motivation: Learn stronger data-driven priors to extrapolate unobserved frequencies for super-resolution in fluorescence microscopy, improving CSR especially under noisy LR images

Method: guided conditional flow matching with ResMatching

Result: ResMatching achieves competitive results on 4 structures from BioSR vs 7 baselines, best trade-off between fidelity and perceptual realism; effective when priors hard to learn; provides calibrated posterior samples and pixel-wise uncertainty

Conclusion: ResMatching provides improved data priors via conditional flow matching enabling better CSR, particularly for noisy LR data, and offers calibrated uncertainty for rejecting unreliable predictions.

Abstract: Computational Super-Resolution (CSR) in fluorescence microscopy has, despite
being an ill-posed problem, a long history. At its very core, CSR is about
finding a prior that can be used to extrapolate frequencies in a micrograph
that have never been imaged by the image-generating microscope. It stands to
reason that, with the advent of better data-driven machine learning techniques,
stronger prior can be learned and hence CSR can lead to better results. Here,
we present ResMatching, a novel CSR method that uses guided conditional flow
matching to learn such improved data-priors. We evaluate ResMatching on 4
diverse biological structures from the BioSR dataset and compare its results
against 7 baselines. ResMatching consistently achieves competitive results,
demonstrating in all cases the best trade-off between data fidelity and
perceptual realism. We observe that CSR using ResMatching is particularly
effective in cases where a strong prior is hard to learn, e.g. when the given
low-resolution images contain a lot of noise. Additionally, we show that
ResMatching can be used to sample from an implicitly learned posterior
distribution and that this distribution is calibrated for all tested use-cases,
enabling our method to deliver a pixel-wise data-uncertainty term that can
guide future users to reject uncertain predictions.

</details>


### [53] [CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing](https://arxiv.org/abs/2510.26609)
*Shayan Nejadshamsi,Yuanyuan Zhang,Shadi Zaki,Brock Porth,Lysa Porth,Vahab Khoshdel*

Main category: cs.CV

TL;DR: 提出CYPRESS，一种基于Prithvi大模型微调的深度回归框架，用于高分辨率田间油菜产量像素级预测，优于现有深度学习方法，可为精准农业提供可操作产量地图。


<details>
  <summary>Details</summary>
Motivation: 传统产量预测方法在可扩展性和田间级精度上不足，无法满足精准农业对高分辨率、连续产量信息的需求，因此需要将大规模地理空间基础模型应用并微调到具体回归任务以产生可操作的像素级产量地图。

Method: 采用预训练的Prithvi-EO-2.0-600M编码器提取多时相卫星影像特征，构建回归头生成像素级连续产量地图；在训练过程中对基础模型进行微调以适配回归任务并优化损失函数以捕捉产量空间变异。

Result: 在加拿大大草原的综合数据集上，CYPRESS在多个评估指标（如RMSE、R2或其他论文中报告的度量）上优于现有深度学习基线，生成更准确和更细粒度的产量地图，验证了方法有效性。

Conclusion: 微调大规模地理空间基础模型可显著提升田间级连续产量预测的精度与分辨率，CYPRESS在加拿大大草原数据集上优于已有模型，证明该方法可用于精细农业决策支持。

Abstract: Accurate and timely crop yield prediction is crucial for global food security
and modern agricultural management. Traditional methods often lack the
scalability and granularity required for precision farming. This paper
introduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoder
for Satellite Sensing), a deep learning model designed for high-resolution,
intra-field canola yield prediction. CYPRESS leverages a pre-trained,
large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it for
a continuous regression task, transforming multi-temporal satellite imagery
into dense, pixel-level yield maps. Evaluated on a comprehensive dataset from
the Canadian Prairies, CYPRESS demonstrates superior performance over existing
deep learning-based yield prediction models, highlighting the effectiveness of
fine-tuning foundation models for specialized agricultural applications. By
providing a continuous, high-resolution output, CYPRESS offers a more
actionable tool for precision agriculture than conventional classification or
county-level aggregation methods. This work validates a novel approach that
bridges the gap between large-scale Earth observation and on-farm
decision-making, offering a scalable solution for detailed agricultural
monitoring.

</details>


### [54] [Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras](https://arxiv.org/abs/2510.26614)
*Christoffer Koo Øhrstrøm,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 提出Spiking Patches，把事件流分成保留异步和稀疏性的tokens；在手势识别和目标检测上，较体素和帧表示推理速度快3.4x~10.4x，准确率相当或更好（手势提升最高3.8，检测提升最高1.4）。


<details>
  <summary>Details</summary>
Motivation: 现有将事件表示为帧或体素的方法破坏了事件数据的异步与稀疏特性，导致计算效率低下；因此需要一种能保留这些特性的高效表示。

Method: 设计一种基于事件时间和空间稀疏性的分词器Spiking Patches，将事件流切分为不同时空片段并生成稀疏tokens，随后在GNN（PCN）与Transformer上进行训练与评估，对比帧/体素表示的速度与准确率。

Result: 提出事件相似的分词方法“Spiking Patches”，为事件相机设计的Tokenizer，通过保留事件的异步性和空间稀疏性，在不降低准确率的前提下实现更快的推理。

Conclusion: Spiking Patches在保持事件相机固有属性同时实现更高效的计算，证明了针对事件数据的分词化是有前景的研究方向。

Abstract: We propose tokenization of events and present a tokenizer, Spiking Patches,
specifically designed for event cameras. Given a stream of asynchronous and
spatially sparse events, our goal is to discover an event representation that
preserves these properties. Prior works have represented events as frames or as
voxels. However, while these representations yield high accuracy, both frames
and voxels are synchronous and decrease the spatial sparsity. Spiking Patches
gives the means to preserve the unique properties of event cameras and we show
in our experiments that this comes without sacrificing accuracy. We evaluate
our tokenizer using a GNN, PCN, and a Transformer on gesture recognition and
object detection. Tokens from Spiking Patches yield inference times that are up
to 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. We
achieve this while matching their accuracy and even surpassing in some cases
with absolute improvements up to 3.8 for gesture recognition and up to 1.4 for
object detection. Thus, tokenization constitutes a novel direction in
event-based vision and marks a step towards methods that preserve the
properties of event cameras.

</details>


### [55] [PT-DETR: Small Target Detection Based on Partially-Aware Detail Focus](https://arxiv.org/abs/2510.26630)
*Bingcong Huo,Zhiming Wang*

Main category: cs.CV

TL;DR: 提出了PADF模块增强细节提取、MFFF模块融合中频特征以及Focaler-SIoU改进匹配策略，在VisDrone2019上较RT-DETR提升约1.6%–1.7% mAP，参数更少、复杂度更低，适用于无人机小目标检测。


<details>
  <summary>Details</summary>
Motivation: 无人机影像中小目标多、遮挡严重、背景复杂且光照变化大，传统检测器在特征表达和定位上不足，需提出针对小目标的轻量高效方法以提升检测精度与鲁棒性。

Method: 1) 在骨干引入PADF模块，局部增强小目标细节特征；2) 通过MFFF模块在多尺度或频域上融合中频信息以丰富上下文；3) 使用Focaler-SIoU改进边界框匹配与回归；基于RT-DETR进行轻量改造以降低复杂度和参数量。

Result: PT-DETR是一种基于RT-DETR、针对无人机小目标检测设计的算法，通过三大改进模块增强对小目标的表征与定位能力。

Conclusion: PT-DETR在保持轻量性的同时，通过特征细化和改进的匹配损失，提升了对密集、遮挡和光照变化下小目标的检测性能，实验证明其在VisDrone2019上具有稳健性和可行性。

Abstract: To address the challenges in UAV object detection, such as complex
backgrounds, severe occlusion, dense small objects, and varying lighting
conditions,this paper proposes PT-DETR based on RT-DETR, a novel detection
algorithm specifically designed for small objects in UAV imagery. In the
backbone network, we introduce the Partially-Aware Detail Focus (PADF) Module
to enhance feature extraction for small objects. Additionally,we design the
Median-Frequency Feature Fusion (MFFF) module,which effectively improves the
model's ability to capture small-object details and contextual information.
Furthermore,we incorporate Focaler-SIoU to strengthen the model's bounding box
matching capability and increase its sensitivity to small-object features,
thereby further enhancing detection accuracy and robustness. Compared with
RT-DETR, our PT-DETR achieves mAP improvements of 1.6% and 1.7% on the
VisDrone2019 dataset with lower computational complexity and fewer parameters,
demonstrating its robustness and feasibility for small-object detection tasks.

</details>


### [56] [All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles](https://arxiv.org/abs/2510.26641)
*Sayed Pedram Haeri Boroujeni,Niloufar Mehrabi,Hazim Alzorgan,Ahmad Sarlak,Mahlagha Fazeli,Abolfazl Razi*

Main category: cs.CV

TL;DR: 围绕传感器融合、数据集重构与VLM/LLM驱动的检测方法，提供未来自动驾驶目标检测的路线图与研究挑战。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶感知知识分散于多模态感知、上下文推理与协同智能领域，亟需整合最新的VLM/LLM/生成式AI进展，以形成面向未来的目标检测路线图。

Method: Survey + Vision-Language-Centric Analysis

Result: 系统回顾了传感器与其融合策略、提出了基于自车/基础设施/协同的数据集分类、并分析了2D/3D/混合与Transformer驱动的方法，指出VLM/LLM在跨模态理解与场景推理中的潜力。

Conclusion: 强调将VLM/LLM与多传感器融合、时空协同和可解释性结合是未来关键方向，同时呼吁构建更丰富的协同与跨模态数据集以推动实用化。

Abstract: Autonomous Vehicles (AVs) are transforming the future of transportation
through advances in intelligent perception, decision-making, and control
systems. However, their success is tied to one core capability, reliable object
detection in complex and multimodal environments. While recent breakthroughs in
Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable
progress, the field still faces a critical challenge as knowledge remains
fragmented across multimodal perception, contextual reasoning, and cooperative
intelligence. This survey bridges that gap by delivering a forward-looking
analysis of object detection in AVs, emphasizing emerging paradigms such as
Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI
rather than re-examining outdated techniques. We begin by systematically
reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,
and Radar) and their fusion strategies, highlighting not only their
capabilities and limitations in dynamic driving environments but also their
potential to integrate with recent advances in LLM/VLM-driven perception
frameworks. Next, we introduce a structured categorization of AV datasets that
moves beyond simple collections, positioning ego-vehicle, infrastructure-based,
and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a
cross-analysis of data structures and characteristics. Ultimately, we analyze
cutting-edge detection methodologies, ranging from 2D and 3D pipelines to
hybrid sensor fusion, with particular attention to emerging transformer-driven
approaches powered by Vision Transformers (ViTs), Large and Small Language
Models (SLMs), and VLMs. By synthesizing these perspectives, our survey
delivers a clear roadmap of current capabilities, open challenges, and future
opportunities.

</details>


### [57] [Towards Reliable Sea Ice Drift Estimation in the Arctic Deep Learning Optical Flow on RADARSAT-2](https://arxiv.org/abs/2510.26653)
*Daniela Martin,Joseph Gallego*

Main category: cs.CV

TL;DR: 对48个深度学习光流模型在RADARSAT-2 ScanSAR海冰影像上进行基准测试，多模型达亚公里精度，可用于像素级海冰漂移估计，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统光流方法对复杂运动假设过强，深度学习方法已在计算机视觉领域显著提升精度，促使其在海冰漂移估计中应用以提高准确性和空间连续性。

Method: 对48个深度学习光流模型在RADARSAT-2 ScanSAR影像上进行评估，使用GNSS浮标作为真值，通过EPE和Fl指标量化性能。

Result: Deep learning 光流方法在极地遥感 SAR 图像上可行，部分模型实现亚公里精度（EPE 6-8 像素，约300-400m），能生成像素级连续漂移场，对导航和气候建模有用。

Conclusion: 深度学习光流模型可有效迁移到极地SAR海冰漂移估计，能提供连续、高分辨率的漂移场，满足导航和气候研究需求。

Abstract: Accurate estimation of sea ice drift is critical for Arctic navigation,
climate research, and operational forecasting. While optical flow, a computer
vision technique for estimating pixel wise motion between consecutive images,
has advanced rapidly in computer vision, its applicability to geophysical
problems and to satellite SAR imagery remains underexplored. Classical optical
flow methods rely on mathematical models and strong assumptions about motion,
which limit their accuracy in complex scenarios. Recent deep learning based
approaches have substantially improved performance and are now the standard in
computer vision, motivating their application to sea ice drift estimation. We
present the first large scale benchmark of 48 deep learning optical flow models
on RADARSAT 2 ScanSAR sea ice imagery, evaluated with endpoint error (EPE) and
Fl all metrics against GNSS tracked buoys. Several models achieve sub kilometer
accuracy (EPE 6 to 8 pixels, 300 to 400 m), a small error relative to the
spatial scales of sea ice motion and typical navigation requirements in the
Arctic. Our results demonstrate that the models are capable of capturing
consistent regional drift patterns and that recent deep learning based optical
flow methods, which have substantially improved motion estimation accuracy
compared to classical methods, can be effectively transferred to polar remote
sensing. Optical flow produces spatially continuous drift fields, providing
motion estimates for every image pixel rather than at sparse buoy locations,
offering new opportunities for navigation and climate modeling.

</details>


### [58] [Improving Classification of Occluded Objects through Scene Context](https://arxiv.org/abs/2510.26681)
*Courtney M. King,Daniel D. Leeds,Damian Lyons,George Kalaitzis*

Main category: cs.CV

TL;DR: 本文提出两种基于场景信息的融合方法来增强RPN-DCNN目标检测在遮挡情况下的鲁棒性：其一在预测前根据背景场景选择定制的目标网络；其二在检测后将场景知识融入初始目标得分。实验在部分遮挡数据集上显示在召回率和精确率上均优于基线，并发现用遮挡和未遮挡图片混合训练能更好处理遮挡。方法具可解释性且易迁移。


<details>
  <summary>Details</summary>
Motivation: 遮挡导致传统强检测器性能下降，生物视觉中场景上下文有助识别。引入场景信息可补充被遮挡目标的可辨识信号，从而提高检测鲁棒性。

Method: 提出两类融合策略：1) 预测前(scene-conditioned model selection)：先识别背景场景，然后选择对应的定制目标检测网络（或分支）来处理图像；2) 检测后(score fusion)：在RPN-DCNN输出初始目标分数后，将场景知识作为额外信号与之融合以调整置信度；并在训练中对比只遮挡、只未遮挡与混合训练三种方案。

Result: 在含部分遮挡的挑战性数据集上，两种方法均在召回率和精确率上优于基线RPN-DCNN；混合遮挡与未遮挡训练策略表现最好。

Conclusion: 场景先验能显著提升RPN-DCNN在部分遮挡情形下的检测性能；采用预测前的分支网络选择和检测后得分融合两种策略均有效，且混合遮挡/未遮挡训练更优。方法可扩展并有实践应用潜力。

Abstract: The presence of occlusions has provided substantial challenges to
typically-powerful object recognition algorithms. Additional sources of
information can be extremely valuable to reduce errors caused by occlusions.
Scene context is known to aid in object recognition in biological vision. In
this work, we attempt to add robustness into existing Region Proposal
Network-Deep Convolutional Neural Network (RPN-DCNN) object detection networks
through two distinct scene-based information fusion techniques. We present one
algorithm under each methodology: the first operates prior to prediction,
selecting a custom object network to use based on the identified background
scene, and the second operates after detection, fusing scene knowledge into
initial object scores output by the RPN. We demonstrate our algorithms on
challenging datasets featuring partial occlusions, which show overall
improvement in both recall and precision against baseline methods. In addition,
our experiments contrast multiple training methodologies for occlusion
handling, finding that training on a combination of both occluded and
unoccluded images demonstrates an improvement over the others. Our method is
interpretable and can easily be adapted to other datasets, offering many future
directions for research and practical applications.

</details>


### [59] [Process Integrated Computer Vision for Real-Time Failure Prediction in Steel Rolling Mill](https://arxiv.org/abs/2510.26684)
*Vaibhav Kurrey,Sivakalyan Pujari,Gagan Raj Gupta*

Main category: cs.CV

TL;DR: A centralized video-server deep-learning system monitors rolling mill operations via cameras and sensors to predict failures early, localize issues, and guide proactive maintenance, lowering downtime and costs


<details>
  <summary>Details</summary>
Motivation: Reduce unplanned breakdowns by early detection of equipment failures using vision and sensor fusion without burdening PLCs

Method: Long-term deployment of machine vision for failure prediction in steel rolling mill

Result: Server-side deep-learning inference on multiple camera streams enabled early failure prediction, localization, and root-cause insights, improving reliability and productivity

Conclusion: Centralized ML-based vision with sensor fusion can scalably and effectively predict failures in industrial rolling mills, reducing unplanned downtime and resource load on PLCs.

Abstract: We present a long-term deployment study of a machine vision-based anomaly
detection system for failure prediction in a steel rolling mill. The system
integrates industrial cameras to monitor equipment operation, alignment, and
hot bar motion in real time along the process line. Live video streams are
processed on a centralized video server using deep learning models, enabling
early prediction of equipment failures and process interruptions, thereby
reducing unplanned breakdown costs. Server-based inference minimizes the
computational load on industrial process control systems (PLCs), supporting
scalable deployment across production lines with minimal additional resources.
By jointly analyzing sensor data from data acquisition systems and visual
inputs, the system identifies the location and probable root causes of
failures, providing actionable insights for proactive maintenance. This
integrated approach enhances operational reliability, productivity, and
profitability in industrial manufacturing environments.

</details>


### [60] [The Impact and Outlook of 3D Gaussian Splatting](https://arxiv.org/abs/2510.26694)
*Bernhard Kerbl*

Main category: cs.CV

TL;DR: 3DGS自提出以来催生大量后续工作，覆盖效率、动态表示、理论分析、移动/VR、大规模场景和即时重建等方向，使其从突破性表示演变为多场景适用的基础技术。


<details>
  <summary>Details</summary>
Motivation: 综述3D Gaussian Splatting（3DGS）相关研究方向与进展，强调其在效率、可扩展性、动态表示、数学基础以及移动/虚拟现实和大规模场景应用等方面的演变与影响。

Method: 通过梳理并分类近年相关工作与技术路线，归纳出主要研究方向与代表性进展，评估各方向的挑战与未来机会。

Result: 总结了若干关键方向：资源高效训练与渲染、四维动态表示（4DGS）、3DGS的数学建模与渲染分析、移动/VR部署、大规模环境扩展以及即时重建方法等，表明3DGS已成为3D视觉与图形的基础工具。

Conclusion: 3DGS的发展方向多样且相互促进：性能优化推动实时与移动端应用，理论研究增强可解释性与方法改进，动态与大尺度扩展满足更复杂场景需求，未来研究可聚焦于跨域通用性、自动化构建与与神经渲染融合。

Abstract: Since its introduction, 3D Gaussian Splatting (3DGS) has rapidly transformed
the landscape of 3D scene representations, inspiring an extensive body of
associated research. Follow-up work includes analyses and contributions that
enhance the efficiency, scalability, and real-world applicability of 3DGS. In
this summary, we present an overview of several key directions that have
emerged in the wake of 3DGS. We highlight advances enabling resource-efficient
training and rendering, the evolution toward dynamic (or four-dimensional,
4DGS) representations, and deeper exploration of the mathematical foundations
underlying its appearance modeling and rendering process. Furthermore, we
examine efforts to bring 3DGS to mobile and virtual reality platforms, its
extension to massive-scale environments, and recent progress toward
near-instant radiance field reconstruction via feed-forward or distributed
computation. Collectively, these developments illustrate how 3DGS has evolved
from a breakthrough representation into a versatile and foundational tool for
3D vision and graphics.

</details>


### [61] [SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models](https://arxiv.org/abs/2510.26769)
*Anushka Sivakumar,Andrew Zhang,Zaber Hakim,Chris Thomas*

Main category: cs.CV

TL;DR: SteerVLM is a lightweight activation-modulation steering module that uses latent prompt pairs to control VLM behavior at inference-time, improving steering and reducing hallucinations while keeping base model weights fixed; introduces VNIA dataset.


<details>
  <summary>Details</summary>
Motivation: Enable fine-grained, inference-time control of VLM outputs without changing base model weights; reduce hallucination and align narratives

Method: Activation modulation steering via latent prompt pairs

Result: SteerVLM (0.14% params) performs better than prior intervention methods on steering and hallucination benchmarks; VNIA dataset provided for development/eval

Conclusion: Activation engineering via a small steering module offers effective, low-cost, inference-time control of VLM outputs and mitigates hallucinations; VNIA helps benchmark such methods.

Abstract: This work introduces SteerVLM, a lightweight steering module designed to
guide Vision-Language Models (VLMs) towards outputs that better adhere to
desired instructions. Our approach learns from the latent embeddings of paired
prompts encoding target and converse behaviors to dynamically adjust
activations connecting the language modality with image context. This allows
for fine-grained, inference-time control over complex output semantics without
modifying model weights while preserving performance on off-target tasks. Our
steering module requires learning parameters equal to 0.14% of the original
VLM's size. Our steering module gains model control through dimension-wise
activation modulation and adaptive steering across layers without requiring
pre-extracted static vectors or manual tuning of intervention points.
Furthermore, we introduce VNIA (Visual Narrative Intent Alignment), a
multimodal dataset specifically created to facilitate the development and
evaluation of VLM steering techniques. Our method outperforms existing
intervention techniques on steering and hallucination mitigation benchmarks for
VLMs and proposes a robust solution for multimodal model control through
activation engineering.

</details>


### [62] [Surpassing state of the art on AMD area estimation from RGB fundus images through careful selection of U-Net architectures and loss functions for class imbalance](https://arxiv.org/abs/2510.26778)
*Valentyna Starodub,Mantas Lukoševičius*

Main category: cs.CV

TL;DR: 基于U-Net，通过改进预处理、编码器与损失函数，本文得到了在ADAM数据集上领先的AMD多类病变分割模型并开源代码。


<details>
  <summary>Details</summary>
Motivation: 解决非侵入性RGB视网膜图像中AMD病变检测的分割难题，提升现有在ADAM数据集上的多类病变分割性能，为临床辅助诊断提供更准确的自动化工具。

Method: 以U-Net连接结构为骨架，系统比较多种预处理方法、不同复杂度的编码器(backbone)深度网络以及针对类别不平衡的专用损失函数（像素级与图像级），并通过实验选择最终配置。

Result: 最终框架在ADAM挑战赛基准上取得优于以往所有提交的多类病变分割结果，证明所探讨的架构与训练策略改动能有效提升分割性能。研究同时公开了实验代码以利复现。

Conclusion: 本文提出基于U-Net的多策略改进框架，结合预处理、不同复杂度编码器和专用损失函数，显著提升了RGB视网膜图像中老年性黄斑变性（AMD）病变的多类语义分割性能，优于ADAM挑战赛先前提交结果，并开源了代码。

Abstract: Age-related macular degeneration (AMD) is one of the leading causes of
irreversible vision impairment in people over the age of 60. This research
focuses on semantic segmentation for AMD lesion detection in RGB fundus images,
a non-invasive and cost-effective imaging technique. The results of the ADAM
challenge - the most comprehensive AMD detection from RGB fundus images
research competition and open dataset to date - serve as a benchmark for our
evaluation. Taking the U-Net connectivity as a base of our framework, we
evaluate and compare several approaches to improve the segmentation model's
architecture and training pipeline, including pre-processing techniques,
encoder (backbone) deep network types of varying complexity, and specialized
loss functions to mitigate class imbalances on image and pixel levels. The main
outcome of this research is the final configuration of the AMD detection
framework, which outperforms all the prior ADAM challenge submissions on the
multi-class segmentation of different AMD lesion types in non-invasive RGB
fundus images. The source code used to conduct the experiments presented in
this paper is made freely available.

</details>


### [63] [ChartAB: A Benchmark for Chart Grounding & Dense Alignment](https://arxiv.org/abs/2510.26781)
*Aniruddh Bansal,Davit Soselia,Dang Nguyen,Tianyi Zhou*

Main category: cs.CV

TL;DR: 提出ChartAlign Benchmark(ChartAB)，通过JSON模板与两阶段推理评估VLM在图表数据提取、元素定位、属性识别及跨图表对齐比较的细粒度能力，揭示当前VLM在图表理解上的若干不足与偏差。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在图表的细节感知与结构提取方面存在不足，难以准确地从图形中获取精细信息并在多图比较中发生错误或出现幻觉，因而需要一个专门的基准来精细测评这些能力并揭示模型短板。

Method: 设计了专门的JSON模板用于度量各项对齐任务指标，并引入两阶段推理流程以评估模型在跨图表元素/属性对齐与比较上的能力；在多种类型与复杂度的图表上构建评测集合来测试VLM。

Result: 对若干近期VLM的评测显示了模型在图表理解中的偏差、薄弱环节、鲁棒性问题与幻觉现象，揭示了模型在细粒度任务上的性能差异并指出需要加强的具体技能。

Conclusion: 本论文提出ChartAB基准，用于全面评估视觉-语言模型（VLM）在图表理解中的细粒度对齐与推理能力，特别聚焦于从图表中提取表格数据、定位可视化元素及识别属性，并考察跨图表对齐比较能力。

Abstract: Charts play an important role in visualization, reasoning, data analysis, and
the exchange of ideas among humans. However, existing vision-language models
(VLMs) still lack accurate perception of details and struggle to extract
fine-grained structures from charts. Such limitations in chart grounding also
hinder their ability to compare multiple charts and reason over them. In this
paper, we introduce a novel "ChartAlign Benchmark (ChartAB)" to provide a
comprehensive evaluation of VLMs in chart grounding tasks, i.e., extracting
tabular data, localizing visualization elements, and recognizing various
attributes from charts of diverse types and complexities. We design a JSON
template to facilitate the calculation of evaluation metrics specifically
tailored for each grounding task. By incorporating a novel two-stage inference
workflow, the benchmark can further evaluate VLMs' capability to align and
compare elements/attributes across two charts. Our analysis of evaluations on
several recent VLMs reveals new insights into their perception biases,
weaknesses, robustness, and hallucinations in chart understanding. These
findings highlight the fine-grained discrepancies among VLMs in chart
understanding tasks and point to specific skills that need to be strengthened
in current models.

</details>


### [64] [HEIR: Learning Graph-Based Motion Hierarchies](https://arxiv.org/abs/2510.26786)
*Cheng Zheng,William Koch,Baiang Li,Felix Heide*

Main category: cs.CV

TL;DR: 提出一种可微图学习的分层运动建模方法，通过图神经网络学习父子依赖，并将全局运动分解为父继承模式与局部残差，能从数据中恢复运动层次并在多种任务上提升重建质量与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的复杂运动通常由更简单运动分量的协调交互产生，现有方法多依赖人工定义或启发式的层次结构及固定运动原语，限制了跨任务的泛化能力，因此需要一种能够从数据中自适应学习层次结构的方法。

Method: 将观测到的运动表示为基于图的层次结构，显式地将全局绝对运动分解为父级继承模式和局部运动残差。将层次推断表述为可微的图学习问题，顶点表示元素运动，定向边通过图神经网络学习父子依赖关系。并在1D平移、2D旋转和基于高斯点溅射的动态3D变形中进行评估。

Result: 在1D和2D实验中成功重建了内在运动层次，在动态3D高斯点溅射场景中相比基线方法生成了更逼真、更具可解释性的变形。总体上方法具有更强的适应性和通用性。

Conclusion: 该论文提出一种数据驱动的分层运动建模方法，能够从数据中学习结构化、可解释的运动关系，克服了基于手工或启发式层次和固定运动原语的局限性。该方法在多个任务上展示了更好的重建和更具解释性的变形结果。

Abstract: Hierarchical structures of motion exist across research fields, including
computer vision, graphics, and robotics, where complex dynamics typically arise
from coordinated interactions among simpler motion components. Existing methods
to model such dynamics typically rely on manually-defined or heuristic
hierarchies with fixed motion primitives, limiting their generalizability
across different tasks. In this work, we propose a general hierarchical motion
modeling method that learns structured, interpretable motion relationships
directly from data. Our method represents observed motions using graph-based
hierarchies, explicitly decomposing global absolute motions into
parent-inherited patterns and local motion residuals. We formulate hierarchy
inference as a differentiable graph learning problem, where vertices represent
elemental motions and directed edges capture learned parent-child dependencies
through graph neural networks. We evaluate our hierarchical reconstruction
approach on three examples: 1D translational motion, 2D rotational motion, and
dynamic 3D scene deformation via Gaussian splatting. Experimental results show
that our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,
and produces more realistic and interpretable deformations compared to the
baseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,
data-driven hierarchical modeling paradigm, our method offers a formulation
applicable to a broad range of motion-centric tasks. Project Page:
https://light.princeton.edu/HEIR/

</details>


### [65] [The Quest for Generalizable Motion Generation: Data, Model, and Evaluation](https://arxiv.org/abs/2510.26794)
*Jing Lin,Ruisi Wang,Junzhe Lu,Ziqi Huang,Guorui Song,Ailing Zeng,Xian Liu,Chen Wei,Wanqi Yin,Qingping Sun,Zhongang Cai,Lei Yang,Ziwei Liu*

Main category: cs.CV

TL;DR: 从数据、模型与评测三管齐下，将视频生成的先验迁移到三维动作生成，借助ViMoGen-228K、ViMoGen/ViMoGen-light与MBench显著提升MoGen的泛化与质量。


<details>
  <summary>Details</summary>
Motivation: 观察到视频生成领域在建模人类行为方面的强泛化能力，认为这些洞见可迁移以突破MoGen在泛化上的瓶颈。

Method: 构建大规模混合数据集ViMoGen-228K（含MoCap、视频标注与ViGen合成样本），提出基于流匹配的扩散Transformer模型ViMoGen及其蒸馏轻量版ViMoGen-light，通过门控多模态条件融合MoCap与ViGen先验；并设计分层评测基准MBench。

Result: 通过大规模数据、模型与评估三方面的系统迁移，实验证明在动作质量、提示一致性与泛化性上均有显著提升；并提供代码、数据与基准以促进行业复现。

Conclusion: 本论文提出通过从视频生成（ViGen）向三维人体动作生成（MoGen）迁移知识，显著提升MoGen的泛化能力，最终在自动与人工评测中均优于现有方法。

Abstract: Despite recent advances in 3D human motion generation (MoGen) on standard
benchmarks, existing models still face a fundamental bottleneck in their
generalization capability. In contrast, adjacent generative fields, most
notably video generation (ViGen), have demonstrated remarkable generalization
in modeling human behaviors, highlighting transferable insights that MoGen can
leverage. Motivated by this observation, we present a comprehensive framework
that systematically transfers knowledge from ViGen to MoGen across three key
pillars: data, modeling, and evaluation. First, we introduce ViMoGen-228K, a
large-scale dataset comprising 228,000 high-quality motion samples that
integrates high-fidelity optical MoCap data with semantically annotated motions
from web videos and synthesized samples generated by state-of-the-art ViGen
models. The dataset includes both text-motion pairs and text-video-motion
triplets, substantially expanding semantic diversity. Second, we propose
ViMoGen, a flow-matching-based diffusion transformer that unifies priors from
MoCap data and ViGen models through gated multimodal conditioning. To enhance
efficiency, we further develop ViMoGen-light, a distilled variant that
eliminates video generation dependencies while preserving strong
generalization. Finally, we present MBench, a hierarchical benchmark designed
for fine-grained evaluation across motion quality, prompt fidelity, and
generalization ability. Extensive experiments show that our framework
significantly outperforms existing approaches in both automatic and human
evaluations. The code, data, and benchmark will be made publicly available.

</details>


### [66] [Scaling Image Geo-Localization to Continent Level](https://arxiv.org/abs/2510.26795)
*Philipp Lindenberger,Paul-Edouard Sarlin,Jan Hosang,Matteo Balice,Marc Pollefeys,Simon Lynen,Eduard Trulls*

Main category: cs.CV

TL;DR: 提出一种混合方法，将代理分类学习到的位置原型与航空图像嵌入结合，以实现跨大范围（如整个大陆）精细图像地理定位；在覆盖大部分欧洲的数据集上，200m内定位成功率超过68%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在规模和精度间存在权衡：全局分类粗糙（10+ km），跨视角检索受域差影响且多在小区域研究；需要一种可扩展且在大陆级别实现细粒度定位的方案。

Method: 训练阶段使用代理分类任务学习富含位置信息的特征表征与位置原型；推断时将地面图像的嵌入与航拍图像嵌入和这些位置原型结合，进行直接检索以实现精细定位。

Result: 在覆盖欧洲大部分地区的数据集上，方法在200米范围内定位成功率超过68%，代码已公开。

Conclusion: 利用代理分类学习位置原型并与航拍图像嵌入结合，可在大陆级范围实现细粒度地理定位，显著提高了对地面图像稀疏性的鲁棒性和定位精度。

Abstract: Determining the precise geographic location of an image at a global scale
remains an unsolved challenge. Standard image retrieval techniques are
inefficient due to the sheer volume of images (>100M) and fail when coverage is
insufficient. Scalable solutions, however, involve a trade-off: global
classification typically yields coarse results (10+ kilometers), while
cross-view retrieval between ground and aerial imagery suffers from a domain
gap and has been primarily studied on smaller regions. This paper introduces a
hybrid approach that achieves fine-grained geo-localization across a large
geographic expanse the size of a continent. We leverage a proxy classification
task during training to learn rich feature representations that implicitly
encode precise location information. We combine these learned prototypes with
embeddings of aerial imagery to increase robustness to the sparsity of
ground-level data. This enables direct, fine-grained retrieval over areas
spanning multiple countries. Our extensive evaluation demonstrates that our
approach can localize within 200m more than 68\% of queries of a dataset
covering a large part of Europe. The code is publicly available at
https://scaling-geoloc.github.io.

</details>


### [67] [SEE4D: Pose-Free 4D Generation via Auto-Regressive Video Inpainting](https://arxiv.org/abs/2510.26796)
*Dongyue Lu,Ao Liang,Tianxin Huang,Xiao Fu,Yuyang Zhao,Baorui Ma,Liang Pan,Wei Yin,Lingdong Kong,Wei Tsang Ooi,Ziwei Liu*

Main category: cs.CV

TL;DR: SEE4D avoids explicit poses by rendering to fixed virtual cameras and using a view-conditional inpainting core with autoregressive traversal to synthesize coherent 4D views from casual videos


<details>
  <summary>Details</summary>
Motivation: remove dependence on manual camera poses and decouple camera control from scene dynamics to enable pose-free 4D synthesis from casual videos

Method: trajectory-to-camera pose-free video-to-4D via view-conditional inpainting and autoregressive traversal

Result: SEE4D trains a view-conditional video inpainting model on warped images from fixed virtual cameras and performs spatiotemporal autoregressive inference along virtual-camera splines, achieving better generalization and performance on cross-view generation and sparse reconstruction vs pose- or trajectory-conditioned baselines

Conclusion: pose-free trajectory-to-camera framework with inpainting and autoregressive inference yields improved generalization and practical 4D modeling without costly 3D supervision

Abstract: Immersive applications call for synthesizing spatiotemporal 4D content from
casual videos without costly 3D supervision. Existing video-to-4D methods
typically rely on manually annotated camera poses, which are labor-intensive
and brittle for in-the-wild footage. Recent warp-then-inpaint approaches
mitigate the need for pose labels by warping input frames along a novel camera
trajectory and using an inpainting model to fill missing regions, thereby
depicting the 4D scene from diverse viewpoints. However, this
trajectory-to-trajectory formulation often entangles camera motion with scene
dynamics and complicates both modeling and inference. We introduce SEE4D, a
pose-free, trajectory-to-camera framework that replaces explicit trajectory
prediction with rendering to a bank of fixed virtual cameras, thereby
separating camera control from scene modeling. A view-conditional video
inpainting model is trained to learn a robust geometry prior by denoising
realistically synthesized warped images and to inpaint occluded or missing
regions across virtual viewpoints, eliminating the need for explicit 3D
annotations. Building on this inpainting core, we design a spatiotemporal
autoregressive inference pipeline that traverses virtual-camera splines and
extends videos with overlapping windows, enabling coherent generation at
bounded per-step complexity. We validate See4D on cross-view video generation
and sparse reconstruction benchmarks. Across quantitative metrics and
qualitative assessments, our method achieves superior generalization and
improved performance relative to pose- or trajectory-conditioned baselines,
advancing practical 4D world modeling from casual videos.

</details>


### [68] [Masked Diffusion Captioning for Visual Feature Learning](https://arxiv.org/abs/2510.26799)
*Chao Feng,Zihao Wei,Andrew Owens*

Main category: cs.CV

TL;DR: 提出了一种称为 Masked Diffusion Captioning（MDC）的方法，通过对图像-文本对中随机遮掩文本令牌并用图像条件的解码器重建原文，从而学习视觉特征。该方法的视觉学习信号不依赖于序列位置，减少了对辅助目标的需求，并在多个模型与数据集上的线性探测实验中，与自回归和对比学习方法相当。


<details>
  <summary>Details</summary>
Motivation: 希望获得不依赖于序列位置强弱变化的均衡视觉学习信号，从而减少对辅助目标的需求，并探索基于扩散的掩码语言建模在视觉表征学习中的有效性。

Method: 训练时对每个图像-字幕对的文本令牌以随机比率遮掩，使用一个以视觉特征为条件的掩码扩散语言模型（解码器）去重建被遮掩的文本；训练完成后提取视觉特征用于下游任务。

Result: 在线性探测实验中，MDC 学到的视觉特征在多种模型规模与数据集上与自回归和对比学习方法表现相当。

Conclusion: MDC 能有效学习视觉表示，且在无需额外辅助目标的情况下，在线性探测基准上与自回归和对比方法竞争。

Abstract: We learn visual features by captioning images with an image-conditioned
masked diffusion language model, a formulation we call masked diffusion
captioning (MDC). During training, text tokens in each image-caption pair are
masked at a randomly chosen ratio, and a decoder conditioned on visual features
is trained to reconstruct the original text. After training, the learned visual
features can be applied to downstream vision tasks. Unlike autoregressive
captioning, the strength of the visual learning signal in MDC does not depend
on each token's position in the sequence, reducing the need for auxiliary
objectives. Linear probing experiments across a variety of academic-scale
models and datasets show that the learned visual features are competitive with
those produced by autoregressive and contrastive approaches.

</details>


### [69] [OmniX: From Unified Panoramic Generation and Perception to Graphics-Ready 3D Scenes](https://arxiv.org/abs/2510.26800)
*Yukun Huang,Jiwen Yu,Yanning Zhou,Jianan Wang,Xintao Wang,Pengfei Wan,Xihui Liu*

Main category: cs.CV

TL;DR: OmniX用跨模态适配器把2D生成模型变成全景感知器，结合大规模合成全景数据，生成可用于物理渲染与重光照的高质量图形级3D场景。


<details>
  <summary>Details</summary>
Motivation: 弥合现有3D场景构建方法（过程生成与2D提升）与图形级、可重光照3D内容需求之间的差距，希望利用强大的2D生成先验实现沉浸式且物理真实的虚拟世界生成。

Method: 提出轻量高效的跨模态适配器结构，复用2D生成模型对几何、纹理和PBR材质进行全景感知，并构建大规模合成全景数据集用于训练与评估。

Result: 在全景视觉感知、生成与补全任务上展示出有效性，并生成可用于PBR、重光照和模拟的高质量3D场景。

Conclusion: OmniX成功将2D生成先验扩展为全景感知，用于生成适用于PBR和重光照的图形级3D场景，克服了仅强调外观的2D提升方法的局限。

Abstract: There are two prevalent ways to constructing 3D scenes: procedural generation
and 2D lifting. Among them, panorama-based 2D lifting has emerged as a
promising technique, leveraging powerful 2D generative priors to produce
immersive, realistic, and diverse 3D environments. In this work, we advance
this technique to generate graphics-ready 3D scenes suitable for physically
based rendering (PBR), relighting, and simulation. Our key insight is to
repurpose 2D generative models for panoramic perception of geometry, textures,
and PBR materials. Unlike existing 2D lifting approaches that emphasize
appearance generation and ignore the perception of intrinsic properties, we
present OmniX, a versatile and unified framework. Based on a lightweight and
efficient cross-modal adapter structure, OmniX reuses 2D generative priors for
a broad range of panoramic vision tasks, including panoramic perception,
generation, and completion. Furthermore, we construct a large-scale synthetic
panorama dataset containing high-quality multimodal panoramas from diverse
indoor and outdoor scenes. Extensive experiments demonstrate the effectiveness
of our model in panoramic visual perception and graphics-ready 3D scene
generation, opening new possibilities for immersive and physically realistic
virtual world generation.

</details>


### [70] [Are Video Models Ready as Zero-Shot Reasoners? An Empirical Study with the MME-CoF Benchmark](https://arxiv.org/abs/2510.26802)
*Ziyu Guo,Xinyan Chen,Renrui Zhang,Ruichuan An,Yu Qi,Dongzhi Jiang,Xiangtai Li,Manyuan Zhang,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: Veo-3 exhibits promising local visual reasoning but fails on longer-horizon, geometric, and abstract logical tasks; not yet reliable as standalone zero-shot reasoner


<details>
  <summary>Details</summary>
Motivation: Assess if advanced video generation models like Veo-3 can act as zero-shot visual reasoners across diverse reasoning dimensions

Method: Empirical evaluation of Veo-3 on Chain-of-Frame reasoning

Result: Veo-3 shows strengths in short-horizon spatial coherence, fine-grained grounding, and locally consistent dynamics; weak in long-horizon causal reasoning, strict geometric constraints, and abstract logic

Conclusion: Video models are promising complementary visual engines but need integration with dedicated reasoning modules for reliable zero-shot reasoning

Abstract: Recent video generation models can produce high-fidelity, temporally coherent
videos, indicating that they may encode substantial world knowledge. Beyond
realistic synthesis, they also exhibit emerging behaviors indicative of visual
perception, modeling, and manipulation. Yet, an important question still
remains: Are video models ready to serve as zero-shot reasoners in challenging
visual reasoning scenarios? In this work, we conduct an empirical study to
comprehensively investigate this question, focusing on the leading and popular
Veo-3. We evaluate its reasoning behavior across 12 dimensions, including
spatial, geometric, physical, temporal, and embodied logic, systematically
characterizing both its strengths and failure modes. To standardize this study,
we curate the evaluation data into MME-CoF, a compact benchmark that enables
in-depth and thorough assessment of Chain-of-Frame (CoF) reasoning. Our
findings reveal that while current video models demonstrate promising reasoning
patterns on short-horizon spatial coherence, fine-grained grounding, and
locally consistent dynamics, they remain limited in long-horizon causal
reasoning, strict geometric constraints, and abstract logic. Overall, they are
not yet reliable as standalone zero-shot reasoners, but exhibit encouraging
signs as complementary visual engines alongside dedicated reasoning models.
Project page: https://video-cof.github.io

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [71] [Rethinking Text-to-SQL: Dynamic Multi-turn SQL Interaction for Real-world Database Exploration](https://arxiv.org/abs/2510.26495)
*Linzhuang Sun,Tianyu Guo,Hao Liang,Yuying Li,Qifeng Cai,Jingxuan Wei,Bihui Yu,Wentao Zhang,Bin Cui*

Main category: cs.DB

TL;DR: 该论文提出DySQL-Bench，一个针对多轮、交互式Text-to-SQL任务的基准，通过结构化树表示和两阶段自动化生成-验证流程构建数据集，并设计多轮评估框架。基准覆盖13个领域，总计1072个任务；实验显示即便GPT-4o表现也有限。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL研究集中在静态、单轮设置，但实际应用常为多轮交互、意图演化场景，需评估模型在连续交互中理解意图变化与逐步修正SQL的能力。

Method: 基于从原始数据库表提取的结构化树表示，使用LLM生成任务（两阶段：任务合成与交互过滤），随后专家验证并通过可执行数据库进行模拟评估；提出一个包含用户、被测模型与可执行数据库的多角色多轮评估框架。

Result: 构建了覆盖13领域、1072任务的DySQL-Bench，人工验证通过率100%，在该基准上GPT-4o总体准确率仅58.34%，Pass@5为23.81%，表明任务具有较高难度。

Conclusion: DySQL-Bench有效衡量模型在多轮交互场景下的SQL生成能力，数据合成方法可靠且经人工验证，实验结果表明现有大型模型在动态意图追踪和多轮SQL生成上仍存在显著不足，推动未来研究向交互适应能力发展。

Abstract: Recent advances in Text-to-SQL have achieved strong results in static,
single-turn tasks, where models generate SQL queries from natural language
questions. However, these systems fall short in real-world interactive
scenarios, where user intents evolve and queries must be refined over multiple
turns. In applications such as finance and business analytics, users
iteratively adjust query constraints or dimensions based on intermediate
results. To evaluate such dynamic capabilities, we introduce DySQL-Bench, a
benchmark assessing model performance under evolving user interactions. Unlike
previous manually curated datasets, DySQL-Bench is built through an automated
two-stage pipeline of task synthesis and verification. Structured tree
representations derived from raw database tables guide LLM-based task
generation, followed by interaction-oriented filtering and expert validation.
Human evaluation confirms 100% correctness of the synthesized data. We further
propose a multi-turn evaluation framework simulating realistic interactions
among an LLM-simulated user, the model under test, and an executable database.
The model must adapt its reasoning and SQL generation as user intents change.
DySQL-Bench covers 13 domains across BIRD and Spider 2 databases, totaling
1,072 tasks. Even GPT-4o attains only 58.34% overall accuracy and 23.81% on the
Pass@5 metric, underscoring the benchmark's difficulty. All code and data are
released at https://github.com/Aurora-slz/Real-World-SQL-Bench .

</details>
