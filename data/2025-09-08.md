<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: 动态驾驶与关键交互显著提高乘客感知风险，面部表情识别并不可靠，但车辆运动+皮电的模型可有效预测感知风险。


<details>
  <summary>Details</summary>
Motivation: 理解公众对自动驾驶车辆的信任与接受度，尤其是感知风险在关键交互情境下的动态变化及其可客观测量的方法。

Method: 使用驾驶模拟器在两种驾驶风格（平稳与动态）下进行实验，部分情境引入行人穿越。32名参与者采集持续主观舒适度评分、车辆运动数据、网络摄像头面部表情、皮肤电、心率与眼动。对面部表情进行分析，并用车辆运动与皮肤电训练神经网络以预测感知风险。

Result: 持续主观评分显示在转弯与制动时显著不适，随后恢复或变为舒适；动态驾驶风格引发更强不适；行人穿越在动态风格下将不适程度加倍，而在平稳风格下无显著影响。面部表情多数无反应，出现“快乐”或“惊讶”多于“恐惧”。基于车辆运动与皮肤电的神经网络与主观风险评分相关良好。

Conclusion: 面部表情识别对评估自动驾驶车辆中感知风险不可靠；动态驾驶风格和关键交互（如行人穿越）显著增加不适感；基于车辆运动与皮肤电的神经网络能较好预测感知风险。

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [2] [PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting](https://arxiv.org/abs/2509.04545)
*Linqing Wang,Ximing Xing,Yiji Cheng,Zhiyuan Zhao,Jiale Tao,Qixun Wang,Ruihuang Li,Xin Li,Mingrui Wu,Xinchi Deng,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: 提出PromptEnhancer：基于CoT的提示重写器+AlignEvaluator奖励，通过强化学习在不改变T2I模型权重的情况下显著改善图像-文本一致性，并提供新的人类偏好基准。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型在属性绑定、否定和组合关系等复杂提示解析上经常失败，导致生成图像与用户意图不匹配。需要一种无需改动生成模型权重即可提升对提示理解的方法。

Method: 提出一个通用的提示重写框架（PromptEnhancer），将重写器与生成器解耦。重写器采用Chain-of-Thought结构，通过强化学习进行训练，奖励由专门训练的AlignEvaluator给出，AlignEvaluator基于包含24项的系统性失败模式分类提供细粒度反馈。

Result: 在HunyuanImage 2.1模型上进行的大量实验表明，PromptEnhancer在各种语义与组合挑战上显著提高了图像—文本对齐性。同时构建了一个高质量的人类偏好基准用于后续研究。

Conclusion: PromptEnhancer能显著提升预训练T2I模型对复杂提示词的解析与对齐，避免改动生成模型权重，通过训练CoT重写器并以AlignEvaluator提供细粒度奖励进行强化学习优化。

Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated
remarkable capabilities in generating high-fidelity images. However, these
models often struggle to faithfully render complex user prompts, particularly
in aspects like attribute binding, negation, and compositional relationships.
This leads to a significant mismatch between user intent and the generated
output. To address this challenge, we introduce PromptEnhancer, a novel and
universal prompt rewriting framework that enhances any pretrained T2I model
without requiring modifications to its weights. Unlike prior methods that rely
on model-specific fine-tuning or implicit reward signals like image-reward
scores, our framework decouples the rewriter from the generator. We achieve
this by training a Chain-of-Thought (CoT) rewriter through reinforcement
learning, guided by a dedicated reward model we term the AlignEvaluator. The
AlignEvaluator is trained to provide explicit and fine-grained feedback based
on a systematic taxonomy of 24 key points, which are derived from a
comprehensive analysis of common T2I failure modes. By optimizing the CoT
rewriter to maximize the reward from our AlignEvaluator, our framework learns
to generate prompts that are more precisely interpreted by T2I models.
Extensive experiments on the HunyuanImage 2.1 model demonstrate that
PromptEnhancer significantly improves image-text alignment across a wide range
of semantic and compositional challenges. Furthermore, we introduce a new,
high-quality human preference benchmark to facilitate future research in this
direction.

</details>


### [3] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: 通过架构改进、大规模预训练与PDTR分阶段强化训练，论文实现了在2B参数规模下的顶级图像生成与编辑，并通过MetaQuery扩展为统一多模态模型。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型倾向于通过扩大参数量而非优化训练策略来提升性能，导致效率与性能不均衡；希望通过更好的训练策略与架构调整，在参数较小情况下达到或超越更大模型的效果。

Method: 基于SD3.5-Medium对架构进行改进，进行大规模高质量数据的预训练；提出Progressive Dual-Task Reinforcement (PDTR)分阶段强化文本生成与编辑任务；通过MetaQuery连接生成模型与大规模视觉语言模型实现统一多模态框架。

Result: 2B参数的UniPic2-SD3.5M-Kontext在图像生成与编辑上超过了参数更大的BAGEL(7B)和Flux-Kontext(12B)；通过与Qwen2.5-VL-7B连接并联合训练，生成的UniPic2-Metaquery在理解、生成和编辑任务上表现优异。

Conclusion: 该论文提出了一个高效的2B参数图像生成与编辑模型，并通过阶段性强化训练策略实现了在多模态任务上的优异表现。

Abstract: Recent advances in multimodal models have demonstrated impressive
capabilities in unified image generation and editing. However, many prominent
open-source models prioritize scaling model parameters over optimizing training
strategies, limiting their efficiency and performance. In this work, we present
UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which
achieves state-of-the-art image generation and editing while extending
seamlessly into a unified multimodal framework. Our approach begins with
architectural modifications to SD3.5-Medium and large-scale pre-training on
high-quality data, enabling joint text-to-image generation and editing
capabilities. To enhance instruction following and editing consistency, we
propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which
effectively strengthens both tasks in a staged manner. We empirically validate
that the reinforcement phases for different tasks are mutually beneficial and
do not induce negative interference. After pre-training and reinforcement
strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and
editing capabilities than models with significantly larger generation
parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following
the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a
connector and perform joint training to launch a unified multimodal model
UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and
editing, achieving top-tier performance across diverse tasks with a simple and
scalable training paradigm. This consistently validates the effectiveness and
generalizability of our proposed training paradigm, which we formalize as
Skywork UniPic 2.0.

</details>


### [4] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag把拖拽编辑拆成像素级弹性扭曲+通用修补，兼顾实时性、精度与模型通用性，显著优于基于潜在空间的方法。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式编辑主要依赖生成模型的潜在空间，导致精度受限、反馈延迟和模型专属性，需一种既精确又实时且兼容多修补模型的方案。

Method: 提出基于物理弹性形变的像素级双向扭曲实现即时预览，并将扭曲结果转换为普通的修补输入供任意修补模型处理，从而实现高效修补与实时交互（预览0.01s，修补0.3s @512x512）。

Result: 在512x512 分辨率下实现0.01s的实时扭曲预览和0.3s的高质量修补；实验显示在视觉质量与控制精度上均优于现有方法，同时交互延迟显著降低。

Conclusion: Inpaint4Drag通过将拖拽编辑分解为像素空间的双向扭曲和图像修补，实现了更精确、实时且通用的图像编辑；它避免了对生成模型潜在空间的依赖，能兼容任意修补模型并继承其改进。

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive
image manipulation. However, existing approaches predominantly rely on
manipulating the latent space of generative models, leading to limited
precision, delayed feedback, and model-specific constraints. Accordingly, we
present Inpaint4Drag, a novel framework that decomposes drag-based editing into
pixel-space bidirectional warping and image inpainting. Inspired by elastic
object deformation in the physical world, we treat image regions as deformable
materials that maintain natural shape under user manipulation. Our method
achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at
512x512 resolution, significantly improving the interaction experience compared
to existing methods that require minutes per edit. By transforming drag inputs
directly into standard inpainting formats, our approach serves as a universal
adapter for any inpainting model without architecture modification,
automatically inheriting all future improvements in inpainting technology.
Extensive experiments demonstrate that our method achieves superior visual
quality and precise control while maintaining real-time performance. Project
page: https://visual-ai.github.io/inpaint4drag/

</details>


### [5] [DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models](https://arxiv.org/abs/2509.04597)
*Jin Ma,Mohammed Aldeen,Christopher Salas,Feng Luo,Mashrur Chowdhury,Mert Pesé,Long Cheng*

Main category: cs.CV

TL;DR: 提出DISPATCH，一种基于扩散模型的目标检测对抗补丁防御方法，通过“重生并校正”策略重建图像并替换可疑区域，从而对多种隐藏与生成型补丁攻击表现出更好泛化性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测器易受对抗补丁攻击（能在现实世界贴片隐藏或伪造目标），且攻击种类多且可能未知，因此需要一种泛化性强、对自适应攻击鲁棒的防御方法。

Method: 使用扩散模型对整幅图像进行基于数据分布的重生成，然后通过校正模块检测并定位潜在对抗补丁，将对应区域替换为重生成的良性内容，最终送入目标检测器；该方法不需要事先知道补丁样式，属于攻击无关的防御策略。

Result: 在多种检测器和攻击上进行广泛实验，DISPATCH在隐藏攻击上取得89.3% mAP.5的最好总体表现，并在无目标创建攻击中将攻击成功率降至24.8%，且对自适应攻击表现稳健。

Conclusion: DISPATCH能在不依赖攻击先验的情况下，利用扩散模型将输入影像重生成并对可疑补丁区域进行替换，从而显著提升检测器在隐藏与伪造攻击下的mAP与降低创建攻击成功率，同时对自适应攻击具有较强鲁棒性。

Abstract: Object detection is fundamental to various real-world applications, such as
security monitoring and surveillance video analysis. Despite their
advancements, state-of-theart object detectors are still vulnerable to
adversarial patch attacks, which can be easily applied to real-world objects to
either conceal actual items or create non-existent ones, leading to severe
consequences. Given the current diversity of adversarial patch attacks and
potential unknown threats, an ideal defense method should be effective,
generalizable, and robust against adaptive attacks. In this work, we introduce
DISPATCH, the first diffusion-based defense framework for object detection.
Unlike previous works that aim to "detect and remove" adversarial patches,
DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative
models to disarm attack effects while preserving the integrity of the input
image. Specifically, we utilize the in-distribution generative power of
diffusion models to regenerate the entire image, aligning it with benign data.
A rectification process is then employed to identify and replace adversarial
regions with their regenerated benign counterparts. DISPATCH is attack-agnostic
and requires no prior knowledge of the existing patches. Extensive experiments
across multiple detectors and attacks demonstrate that DISPATCH consistently
outperforms state-of-the-art defenses on both hiding attacks and creating
attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and
lowering the attack success rate to 24.8% on untargeted creating attacks.
Moreover, it maintains strong robustness against adaptive attacks, making it a
practical and reliable defense for object detection systems.

</details>


### [6] [WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human](https://arxiv.org/abs/2509.04600)
*Qijun Ying,Zhongyuan Hu,Rui Zhang,Ronghui Li,Yu Lu,Zijiao Zeng*

Main category: cs.CV

TL;DR: WATCH is a unified framework that decouples heading angle analytically and integrates camera trajectory via a world-model-inspired module to better recover global human motion from monocular videos, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing human-motion-centric methods fail to exploit camera orientation and translation cues well, leading to inaccuracies in mapping poses from camera to world coordinates due to depth/motion ambiguity and camera-human entanglement.

Method: Analytical heading angle decomposition for separating orientation, and a world-model-inspired camera trajectory integration mechanism to incorporate translation cues beyond hard-decoding.

Result: State-of-the-art end-to-end trajectory reconstruction on in-the-wild benchmarks; code to be released.

Conclusion: WATCH improves global human motion reconstruction by jointly modeling camera and human motion, using heading angle decomposition and camera trajectory integration, achieving SOTA on benchmarks.

Abstract: Global human motion reconstruction from in-the-wild monocular videos is
increasingly demanded across VR, graphics, and robotics applications, yet
requires accurate mapping of human poses from camera to world coordinates-a
task challenged by depth ambiguity, motion ambiguity, and the entanglement
between camera and human movements. While human-motion-centric approaches excel
in preserving motion details and physical plausibility, they suffer from two
critical limitations: insufficient exploitation of camera orientation
information and ineffective integration of camera translation cues. We present
WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and
Human), a unified framework addressing both challenges. Our approach introduces
an analytical heading angle decomposition technique that offers superior
efficiency and extensibility compared to existing geometric methods.
Additionally, we design a camera trajectory integration mechanism inspired by
world models, providing an effective pathway for leveraging camera translation
information beyond naive hard-decoding approaches. Through experiments on
in-the-wild benchmarks, WATCH achieves state-of-the-art performance in
end-to-end trajectory reconstruction. Our work demonstrates the effectiveness
of jointly modeling camera-human motion relationships and offers new insights
for addressing the long-standing challenge of camera translation integration in
global human motion reconstruction. The code will be available publicly.

</details>


### [7] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: Sali4Vid通过帧重要性加权和语义自适应检索改进密集视频描述，显著提升YouCook2和ViTT的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端密集视频描述方法仅对文本使用时间戳监督且对所有帧一视同仁，同时从固定大小的视频块检索标题，忽略了帧重要性与场景切换，导致描述质量受限。

Method: 提出Saliency-aware Video Reweighting（将时间戳转为sigmoid型帧重要性权重）和Semantic-based Adaptive Caption Retrieval（基于帧相似度分割视频以捕捉场景切换并检索标题）。整体框架Sali4Vid将两者结合，用于强化帧级信息利用与改进检索策略。

Result: 在YouCook2和ViTT数据集上，Sali4Vid相比基线取得了SOTA性能提升，证明了同时改进视频加权与检索策略的有效性。

Conclusion: Sali4Vid通过引入基于sigmoid的帧重要性加权和基于语义的自适应检索模块，有效解决了时间戳监督稀薄与固定块检索忽略场景切换的问题，提升了密集视频描述的质量，在YouCook2和ViTT上实现了最先进的结果。

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [8] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: 提出一种基于UAV的端到端交通监测系统，通过模板匹配、卡尔曼滤波与单应性标定实现高空视频中的车辆检测、分类与跟踪，并能自动识别多种交通违规与生成OD、热力图等多尺度分析，实验表现优良，适用于智能城市管理。


<details>
  <summary>Details</summary>
Motivation: 传统固定摄像头与传感器监控覆盖面有限、适应性差与扩展性受限，难以满足城市复杂场景下的移动交通监测与执法需求，故提出无人机平台以实现高覆盖、灵活部署与多尺度交通分析。

Method: 系统结合多尺度多角度模板匹配用于检测，卡尔曼滤波用于目标跟踪，基于单应性的几何标定用于从航拍视频定位车辆与道路，融合围栏（geofencing）、运动滤波和轨迹偏差分析进行违规检测；配套分析模块实现流向/OD、车辆计数、类别相关性、密度与热力图等。

Result: 案例实验表明在真实城市航拍（约200m）场景下系统检测精度91.8%、F1=90.5%、MOTA=92.1%、MOTP=93.7%；系统可对5类车辆分类并自动检测危险变道、非法占道/违停与占用斑马线等违规，支持OD追踪、热力图与多尺度流量统计，验证了系统的可扩展性与实用性。

Conclusion: 该论文提出了一种基于UAV的城市交通监测系统，能在约200米高度通过多尺度多角度模板匹配、卡尔曼滤波和单应性标定实现车辆检测、分类、跟踪及行为分析，实验显示检测精度91.8%、F1=90.5%、MOTA=92.1%、MOTP=93.7%，并能识别多类违规（违停、占用斑马线、危险变道）及支持OD分析、热力图等多尺度交通分析，具有可扩展性和实际应用潜力。

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [9] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: 提出VCMamba：卷积与多向Mamba SSM的混合骨干，兼顾局部与全局建模，保持线性复杂度。在ImageNet和ADE20K上实现更优的准确率与参数效率。


<details>
  <summary>Details</summary>
Motivation: ViT和SSM（如Mamba）在捕捉全局上下文和长序列建模方面表现优异，但对局部细节敏感性不如CNN；而CNN在局部特征提取上有强先验但缺乏全局推理能力。目标是结合两者优点，既保留局部表征又具备高效的全局建模能力。

Method: 设计了一个混合骨干网络：卷积stem + 分级结构；前期使用卷积块提取细粒度局部特征；后期使用多向Mamba SSM块高效建模长程依赖与全局上下文；通过多阶段融合保持表示丰富性并维持相对于分辨率的线性计算复杂度。

Result: 在ImageNet-1K分类和ADE20K分割上表现优异：VCMamba-B在ImageNet-1K上达82.6% top-1，较PlainMamba-L3提升0.3%且参数少37%；较Vision GNN-B提升0.3%且参数少64%。在ADE20K上达到47.1 mIoU，较EfficientFormer-L7高2.0 mIoU且参数少62%。

Conclusion: VCMamba通过在网络前期引入卷积模块提取局部细节、后期采用多向Mamba SSM模块建模全局长程依赖，成功融合了CNN的局部表示能力与SSM/Transformer的全局建模优势，从而在保持线性复杂度的同时提升了图像分类与语义分割性能。

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [10] [Guideline-Consistent Segmentation via Multi-Agent Refinement](https://arxiv.org/abs/2509.04687)
*Vanshika Vats,Ashwani Rathee,James Davis*

Main category: cs.CV

TL;DR: 训练-free多智能体Worker-Supervisor循环：Worker做分割，Supervisor基于检索的段落级规范批评，辅以RL停止策略，能在复杂文本指导下显著提升分割结果的一致性与性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中分割任务需严格遵守复杂且不断演化的文本标注规范，现有方法需昂贵重训练或在长文本指导下表现不佳，故需训练无关、能处理长篇规范的解决方案。

Method: 提出Worker-Supervisor架构：Worker基于通用视觉-语言模型生成分割，Supervisor检索并依据段落级指导规范对分割结果进行批评，结合轻量级强化学习的停止策略决定是否继续迭代。整个过程不依赖任务特定重训练。

Result: 在Waymo和ReasonSeg数据集上显著优于最先进基线，表现出更强的泛化能力和对指令的一致性遵守（具体数值在摘要中未给出）。

Conclusion: 该论文提出了一种无需额外训练的多智能体迭代精修框架，用于在复杂文本标注规范下实现语义分割的一致性与准确性。

Abstract: Semantic segmentation in real-world applications often requires not only
accurate masks but also strict adherence to textual labeling guidelines. These
guidelines are typically complex and long, and both human and automated
labeling often fail to follow them faithfully. Traditional approaches depend on
expensive task-specific retraining that must be repeated as the guidelines
evolve. Although recent open-vocabulary segmentation methods excel with simple
prompts, they often fail when confronted with sets of paragraph-length
guidelines that specify intricate segmentation rules. To address this, we
introduce a multi-agent, training-free framework that coordinates
general-purpose vision-language models within an iterative Worker-Supervisor
refinement architecture. The Worker performs the segmentation, the Supervisor
critiques it against the retrieved guidelines, and a lightweight reinforcement
learning stop policy decides when to terminate the loop, ensuring
guideline-consistent masks while balancing resource use. Evaluated on the Waymo
and ReasonSeg datasets, our method notably outperforms state-of-the-art
baselines, demonstrating strong generalization and instruction adherence.

</details>


### [11] [Domain Adaptation for Different Sensor Configurations in 3D Object Detection](https://arxiv.org/abs/2509.04711)
*Satoshi Tanaka,Kok Seang Tan,Isamu Yamashita*

Main category: cs.CV

TL;DR: 提出在多传感器配置下通过下游数据集特定微调（Downstream Fine-tuning）和部分层微调（Partial Layer Fine-tuning）来提升3D目标检测的跨配置泛化性，相较于直接联合训练有稳定提升。


<details>
  <summary>Details</summary>
Motivation: 不同车辆平台使用不同LiDAR配置导致点云分布差异，现有方法多关注环境域差异或单一LiDAR的密度变化，未充分研究不同传感器配置间的域差，实际部署中需要一种简单可扩展的跨配置适配方法。

Method: 先进行多数据集（多配置）联合训练，再对目标配置的数据进行下游数据集特定微调；同时探索只更新模型的一部分层（比如头部或中间层）以提高跨配置的泛化能力并减少过拟合和训练成本。

Result: This paper studies domain adaptation across different LiDAR sensor configurations for 3D object detection in autonomous driving, proposing simple fine-tuning strategies to mitigate performance drops when transferring models across platforms.

Conclusion: 在配对数据集的验证中，联合训练后采用下游微调并只微调部分层，比对每种配置分别训练或直接联合训练有一致的提升，提供了实用可扩展的跨平台适配方案。

Abstract: Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.

</details>


### [12] [CD-Mamba: Cloud detection with long-range spatial dependency modeling](https://arxiv.org/abs/2509.04729)
*Tianxiang Xue,Jiayi Zhao,Jingsheng Li,Changlu Chen,Kun Zhan*

Main category: cs.CV

TL;DR: 提出CD-Mamba，一种将卷积与Mamba状态空间模型融合的混合云检测网络，同时捕获像素级细节与长程片段依赖，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Clouds in remote sensing images exhibit both short-range spatial redundancies and long-range atmospheric similarities; existing methods like CNNs capture local structure while Mamba handles long-range dependencies, motivating a hybrid approach to leverage both strengths.

Method: Design a hybrid network (CD-Mamba) integrating convolution modules for pixel-wise texture/detail capture and Mamba state-space blocks for modeling long-range patch-wise dependencies; train and evaluate on remote sensing cloud detection datasets, compare with existing methods.

Result: Extensive experiments show CD-Mamba outperforms existing methods in cloud detection accuracy, demonstrating better handling of multi-scale spatial relations and patchwise dependencies.

Conclusion: CD-Mamba successfully combines convolutional local feature extraction with Mamba state-space models for long-range dependencies, improving cloud detection accuracy across scales.

Abstract: Remote sensing images are frequently obscured by cloud cover, posing
significant challenges to data integrity and reliability. Effective cloud
detection requires addressing both short-range spatial redundancies and
long-range atmospheric similarities among cloud patches. Convolutional neural
networks are effective at capturing local spatial dependencies, while Mamba has
strong capabilities in modeling long-range dependencies. To fully leverage both
local spatial relations and long-range dependencies, we propose CD-Mamba, a
hybrid model that integrates convolution and Mamba's state-space modeling into
a unified cloud detection network. CD-Mamba is designed to comprehensively
capture pixelwise textural details and long term patchwise dependencies for
cloud detection. This design enables CD-Mamba to manage both pixel-wise
interactions and extensive patch-wise dependencies simultaneously, improving
detection accuracy across diverse spatial scales. Extensive experiments
validate the effectiveness of CD-Mamba and demonstrate its superior performance
over existing methods.

</details>


### [13] [Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation](https://arxiv.org/abs/2509.04732)
*Shengqian Zhu,Jiafei Wu,Xiaogang Xu,Chengrong Yu,Ying Song,Zhang Yi,Guangjun Li,Junjie Hu*

Main category: cs.CV

TL;DR: 提出TCT：通过MSH与ATH一致性、低一致性过滤与UAUWL，在不依赖额外模型下有效利用部分标注数据，缓解类别不平衡并提升多类医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 在多类医学图像分割中，获取所有类别的完全标注代价高昂，利用部分标注数据虽可行但会引入类别分布不均导致的类别不平衡问题。现有通过伪标签生成全标注的方法依赖额外模型且易受噪声影响，因此需一种无需额外模型且能利用未标注结构信息以缓解类别不平衡的新方法。

Method: 构建一个带有主分割头（MSH）与若干任务专用辅助头（ATHs）的骨干网络，通过一致性损失使MSH与相应ATH在未标注解剖结构上的预测保持一致；设计低一致性样本过滤策略以避免噪声传播；引入统一的辅助不确定性加权损失（UAUWL）来自动平衡不同任务的损失权重，防止特定任务主导训练。

Result: 在来自不同临床站点的八个腹部数据集上进行大量实验，结果表明TCT在处理部分标注数据的多类分割任务中优于现有方法，能有效利用未标注结构、缓解类别不平衡并提升分割质量。

Conclusion: 本文提出的任务一致性训练 (TCT) 框架，通过主分割头（MSH）与多个辅助任务头（ATH）之间的一致性约束，利用部分标注数据中未标注的解剖结构信息，避免引入额外模型并减轻标签噪声带来的性能下降。结合低一致性样本过滤策略与统一的辅助不确定性加权损失（UAUWL），能有效缓解类别不平衡和任务主导问题，提高分割性能。

Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of
multiple classes, while obtaining full annotations for all classes is often
impractical due to the time and labor required. Leveraging partially labeled
datasets (PLDs) presents a promising alternative; however, current VMIS
approaches face significant class imbalance due to the unequal category
distribution in PLDs. Existing methods attempt to address this by generating
pseudo-full labels. Nevertheless, these typically require additional models and
often result in potential performance degradation from label noise. In this
work, we introduce a Task Consistency Training (TCT) framework to address class
imbalance without requiring extra models. TCT includes a backbone network with
a main segmentation head (MSH) for multi-channel predictions and multiple
auxiliary task heads (ATHs) for task-specific predictions. By enforcing a
consistency constraint between the MSH and ATH predictions, TCT effectively
utilizes unlabeled anatomical structures. To avoid error propagation from
low-consistency, potentially noisy data, we propose a filtering strategy to
exclude such data. Additionally, we introduce a unified auxiliary
uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines
caused by the dominance of specific tasks. Extensive experiments on eight
abdominal datasets from diverse clinical sites demonstrate our approach's
effectiveness.

</details>


### [14] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: 在自动驾驶的恶劣天气下，显式建模不确定性能显著提高分割鲁棒性。通过对SAM2进行不确定性感知的多步微调和将UAT适配到SAM（UAT-SAM），在CamVid、BDD100K和GTA数据集上分别在极端天气及多样场景下取得更好表现。


<details>
  <summary>Details</summary>
Motivation: 在恶劣天气下视觉不确定性高，传统分割模型置信失真会导致自动驾驶系统风险；医学影像领域通过不确定性感知训练提高了模糊情形下的可靠性，类似方法可能能提升驾驶场景的安全性与鲁棒性。

Method: 方法一：对SAM2进行多步微调，训练目标中加入不确定性度量（如置信度、熵或MC-dropout估计）作为损失项或权重，以在模糊区域增强学习关注；方法二：将医学图像中使用的Uncertainty-Aware Adapter（UAT）移植到SAM结构，插入轻量适配器并用不确定性引导更新，保持基础模型参数大部分冻结，增强对不确定区域的判别能力。

Result: The paper proposes two methods to improve segmentation under adverse weather by incorporating uncertainty: (1) multi-step finetuning of SAM2 with uncertainty-aware loss, and (2) adapting Uncertainty-Aware Adapter (UAT) to SAM, called UAT-SAM. Evaluated on CamVid, BDD100K, GTA; UAT-SAM excels in extreme weather, SAM2 with uncertainty-aware loss improves across diverse scenes.

Conclusion: 显式的不确定性建模能提升视觉基础模型在安全关键的自动驾驶场景下的鲁棒性；UAT-SAM在极端天气下优于原始SAM，SAM2在引入不确定性感知损失后对多样场景性能提升明显。

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [15] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR is an on-smartwatch audio+inertial HAR system with an end-to-end trainable preprocessing+inference module achieving high accuracy (>90%) and low latency (≈9–12 ms), enabling privacy-preserving continuous activity tracking.


<details>
  <summary>Details</summary>
Motivation: Existing fine-grained multimodal HAR systems often rely on external servers or smartphones, raising privacy/latency concerns; goal is fully on-device HAR on resource-constrained smartwatches.

Method: Designs an end-to-end trainable architecture unifying sensor preprocessing and inference; optimizes pipeline components for computational efficiency; uses audio and inertial sensors; implements and measures on smartwatch hardware for event detection and classification.

Result: Achieves >90% accuracy across 25+ activity classes, 5x faster processing versus prior methods, 9.3 ms for event detection and 11.8 ms for multimodal classification, outperforming state-of-the-art baselines on detection and classification while running on device.

Conclusion: Paper demonstrates WatchHAR enables accurate, low-latency on-device HAR on smartwatches, showing feasibility of privacy-preserving continuous tracking without external processing.

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [16] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: MCANet通过多尺度Res2Net骨干与多头类别特定残差注意力，有效提升了灾后UAV图像的多标签损伤分类与可解释性，mAP最高达92.35%，未来可结合知识图谱与多模态大模型扩展适应性与语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的方法难以捕捉多尺度空间特征且难以区分视觉相似/共现的损伤类型，因此需要一种能学习多尺度表示并针对每类空间相关区域自适应关注的框架。

Method: 提出MCANet：使用Res2Net为骨干网络以增强多尺度空间上下文，并引入多头类别特定残差注意力模块；每个注意力分支关注不同空间粒度，从而平衡局部细节与全局语境。

Result: 在RescueNet数据集（4494张UAV图像）上，MCANet实现mAP 91.75%，显著优于多种主流骨干（ResNet、Res2Net、VGG、MobileNet、EfficientNet、ViT）；使用8个注意力头时mAP提升至92.35%，对诸如“Road Blocked”这类难分类别的AP提升超过6%；类激活图显示了良好的定位能力。

Conclusion: MCANet在UAV灾后损伤评估任务中有效提升了多标注分类性能，尤其在区分相似或共现的损伤类型方面表现优越，验证了多尺度表征与类别特定注意力机制的有效性。

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [17] [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](https://arxiv.org/abs/2509.04758)
*Kaname Yokoyama,Chihiro Nakatani,Norimichi Ukita*

Main category: cs.CV

TL;DR: 请给出简短的TLDR


<details>
  <summary>Details</summary>
Motivation: 请描述论文动机

Method: 请给出方法概述

Result: 请描述论文结果

Conclusion: 请给出结论

Abstract: This paper proposes dynamic human group detection in videos. For detecting
complex groups, not only the local appearance features of in-group members but
also the global context of the scene are important. Such local and global
appearance features in each frame are extracted using a Vision-Language Model
(VLM) augmented for group detection in our method. For further improvement, the
group structure should be consistent over time. While previous methods are
stabilized on the assumption that groups are not changed in a video, our method
detects dynamically changing groups by global optimization using a graph with
all frames' groupness probabilities estimated by our groupness-augmented CLIP
features. Our experimental results demonstrate that our method outperforms
state-of-the-art group detection methods on public datasets. Code:
https://github.com/irajisamurai/VLM-GroupDetection.git

</details>


### [18] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: FloodVision结合GPT-4o与知识图谱，使用真实物体高度作为参考实现零样本洪水深度估计，在众包图像上MAE=8.17 cm，比基线方法提升显著，具有良好泛化性和近实时性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法在洪水深度估计上受限于固定目标检测器和任务特定训练，导致精度不足且泛化性差；因此需要一种无需标注训练且能结合语义推理与物理尺寸知识以提高准确性与泛化能力的方法。

Method: 系统首先在RGB图像中动态识别可见的参考物体（车辆、行人、路边设施等）；然后从结构化知识图谱中检索这些对象的验证高度以防止模型幻觉；接着通过GPT-4o进行语义推理估计物体的浸没比例；最后对多个估计值应用统计异常值过滤并计算最终水深。系统为零样本，无需特定任务训练，可实时运行并集成于数字孪生或市民上报应用。

Result: 在110张来自MyCoast New York的众包图像上，FloodVision实现了平均绝对误差8.17 cm，相比GPT-4o基线（10.28 cm）提升约20.5%，并优于此前的CNN方法；系统在不同场景中表现稳定并能近实时运行。

Conclusion: 该论文提出了FloodVision，一种结合GPT-4o视觉-语言基础模型与领域知识图谱的零样本洪水深度估计框架，通过利用知识图谱中城市物体的真实尺寸作为参考，计算浸没率并进行统计滤除以输出深度估计。结果显示在MyCoast New York数据集上以8.17 cm的平均绝对误差优于GPT-4o基线和先前CNN方法。

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [19] [Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://arxiv.org/abs/2509.04773)
*Bangxiang Lan,Ruobing Xie,Ruixiang Zhao,Xingwu Sun,Zhanhui Kang,Gang Yang,Xirong Li*

Main category: cs.CV

TL;DR: 提出Hybrid-Tower与PIG，利用伪查询实现细粒度交互，兼顾效果与效率，R@1提升1.6%~3.9%，接近SOTA。


<details>
  <summary>Details</summary>
Motivation: Two-Tower方法高效但效果欠佳，Single-Tower效果好但效率低，研究旨在设计一种兼顾两者优点的新框架。

Method: 提出了伪查询生成器为每个视频生成伪文本查询，使得视频特征与伪查询文本特征在检索阶段能进行细粒度交互，从而获得接近Single-Tower的效果；在推理阶段不增加存储或计算开销，保持Two-Tower的效率。

Result: 在五个常用文本-视频检索基准上，PIG在R@1上比基线提升1.6%~3.9%，达到了接近SOTA的效果，同时保持Two-Tower的效率。

Conclusion: 本论文提出了Hybrid-Tower框架与PIG方法，成功结合了Two-Tower（高效）与Single-Tower（高效）优点，实现了在保持Two-Tower检索效率的同时显著提升检索效果。

Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by
textual queries with the same semantic meanings. Recent CLIP-based approaches
have explored two frameworks: Two-Tower versus Single-Tower framework, yet the
former suffers from low effectiveness, while the latter suffers from low
efficiency. In this study, we explore a new Hybrid-Tower framework that can
hybridize the advantages of the Two-Tower and Single-Tower framework, achieving
high effectiveness and efficiency simultaneously. We propose a novel hybrid
method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,
which includes a new pseudo-query generator designed to generate a pseudo-query
for each video. This enables the video feature and the textual features of
pseudo-query to interact in a fine-grained manner, similar to the Single-Tower
approaches to hold high effectiveness, even before the real textual query is
received. Simultaneously, our method introduces no additional storage or
computational overhead compared to the Two-Tower framework during the inference
stage, thus maintaining high efficiency. Extensive experiments on five commonly
used text-video retrieval benchmarks demonstrate that our method achieves a
significant improvement over the baseline, with an increase of $1.6\% \sim
3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower
models while achieving near state-of-the-art performance, highlighting the
advantages of the Hybrid-Tower framework.

</details>


### [20] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: 研究显示：结合充分的预处理，学习型匹配器（SuperGlue）优于经典特征方法，实现稳健的跨模态月球图像配准，特别是在极区条件下。


<details>
  <summary>Details</summary>
Motivation: 月球探测需精确图像配准以支持表面制图、资源定位与任务规划；不同传感器间存在分辨率、光照和畸变差异，导致跨模态配准困难。

Method: 构建预处理流水线：地理参考、分辨率对齐、强度归一化，并采用自适应直方图均衡、主成分分析和阴影校正等增强手段；评估五种特征匹配算法（SIFT、ASIFT、AKAZE、RIFT2、SuperGlue），在赤道与极区的跨模态图像对上进行配准并计算均方根误差与运行时间。

Result: SuperGlue在所有测试集中均实现最低RMSE和最短运行时间；SIFT和AKAZE在赤道区域接近光照一致时表现良好，但在极区强阴影与低太阳高度角下误匹配增多；预处理显著提升所有算法的稳健性，尤其对经典方法改进明显。

Conclusion: 基于实验，深度学习匹配器SuperGlue在不同传感器与极区条件下表现最稳健，具有最低配准误差和最快运行时间；经典特征如SIFT/AKAZE在赤道区效果良好，但在极区光照与阴影条件下性能明显下降。

Abstract: Accurate image registration is critical for lunar exploration, enabling
surface mapping, resource localization, and mission planning. Aligning data
from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,
Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),
and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya
mission) -- is challenging due to differences in resolution, illumination, and
sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,
AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using
cross-modality image pairs from equatorial and polar regions. A preprocessing
pipeline is proposed, including georeferencing, resolution alignment, intensity
normalization, and enhancements like adaptive histogram equalization, principal
component analysis, and shadow correction. SuperGlue consistently yields the
lowest root mean square error and fastest runtimes. Classical methods such as
SIFT and AKAZE perform well near the equator but degrade under polar lighting.
The results highlight the importance of preprocessing and learning-based
approaches for robust lunar image registration across diverse conditions.

</details>


### [21] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: 作者建立了一个含50+类的移动端皮肤病图像数据集，比较多种CNN与Transformer模型，发现Swin Transformer性能最佳，并通过Grad-CAM增强可解释性，显示出在资源受限环境中用于AI辅助皮肤病筛查的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于皮肤镜图像且疾病类别有限，难以代表真实世界的移动端图像与多样化疾病分类需求；目标是开发更具代表性的数据集并验证先进模型在移动采集场景下的效果，以支持在资源受限地区的可及性诊断。

Method: 构建包含50余类移动设备拍摄皮肤病图像的大规模数据集；对多种卷积神经网络（CNN）和Transformer架构（如Swin Transformer）进行训练与比较；使用Grad-CAM生成可视化热力图以解释模型决策。

Result: 在所构建的数据集上，Swin Transformer等Transformer模型在分类准确率和召回率方面优于所评估的CNN模型；Grad-CAM可视化结果显示模型关注的区域与临床相关部位一致，提升了预测透明性。

Conclusion: Transformer模型（尤其是Swin Transformer）在移动设备拍摄的皮肤病图像分类任务上表现优于传统卷积网络，能更好地捕捉全局上下文信息，并结合Grad-CAM提升可解释性，显示出在低资源环境下用于皮肤病筛查的潜力。

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [22] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 在不改动架构的情况下，Mixture-of-Experts能高效且更可靠地估计语义分割任务中的预测不确定性，简单门控和增加专家数均有助于更好校准。


<details>
  <summary>Details</summary>
Motivation: 提高计算机视觉模型在安全关键任务（如交通场景感知）中的可靠性，获得准确且校准良好的预测不确定性，同时寻找比模型集成更高效的方法。

Method: 作者比较了三种从MoE中提取不确定性的方法：预测熵（predictive entropy）、互信息（mutual information）和专家方差（expert variance）；并评估了路由不确定性（gate entropy）以及不同门控机制（simple gate vs classwise gate）；实验使用A2D2数据集（语义划分的两个专家）和Cityscapes（不同专家数目）进行定量对比。

Result: 实验显示：1) MoE在OOD数据上的条件正确性校准优于传统ensemble；2) 通过预测熵、互信息和专家方差均可从MoE提取有用不确定性估计；3) 简单的门控机制在路由不确定性校准上优于复杂的按类门控；4) 增加专家数量可进一步提升校准效果。

Conclusion: 该论文证明在不修改MoE架构的前提下，MoE能提供比传统模型集成更可靠的预测不确定性估计，尤其在OOD场景下对条件正确性指标更有利。

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [23] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 本文通过提出Sub-SS策略和双阶段建模，结合Mamba与Transformer，构建LFMT框架，高效提取空间-角度-视差信息，显著提升光场超分性能且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 提出Subspace Simple Scanning策略以解决当前多方向扫描在复杂光场数据上导致的低效和冗余特征提取问题。

Method: 提出Sub-SS策略并设计SSMB；双阶段：Stage I的SA-RSMB用于浅层空角特征提取，Stage II的并行EPMB与EPTB用于深层视差/极平面特征精炼；最终融合为混合Mamba-Transformer框架LFMT。

Result: 设计了Subspace Simple Mamba Block (SSMB)、双阶段建模策略（包括SA-RSMB、EPMB和EPTB）并构建混合Mamba-Transformer框架LFMT，在真实与合成数据上显著超越现有最先进方法，同时保持低计算复杂度。

Conclusion: Sub-SS和双阶段模型有效提升了LFSR中特征提取的效率与精度，LFMT在性能与复杂度之间取得良好平衡，适用于真实与合成光场数据。

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [24] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: PropVG是首个无额外检测器的端到端proposal-based visual grounding方法，通过CRS（句子/词级对比学习）和MTD（目标/语义融合）提升对目标的区分与缺席识别，在多个基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有端到端visual grounding方法只用被指代目标监督，忽视潜在显著候选目标；并且缺乏多粒度辨识能力，导致复杂场景下识别鲁棒性不足。

Method: 提出端到端的PropVG框架；引入Contrastive-based Refer Scoring (CRS)模块，在句子级与词级上进行对比学习；设计Multi-granularity Target Discrimination (MTD)模块，融合目标级与语义级信息用于缺席目标识别。

Result: 在gRefCOCO（GREC/GRES）、Ref-ZOM、R-RefCOCO和RefCOCO（REC/RES）等基准上取得了有效提升，代码与模型已开源。

Conclusion: PropVG通过将前景目标proposal生成与指代表达整合，实现了端到端proposal-based visual grounding，弥补了end-to-end方法忽视潜在候选目标和多粒度辨识的不足。

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [25] [TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution](https://arxiv.org/abs/2509.04834)
*Yifei Jia,Shiyu Cheng,Yu Dong,Guan Li,Dong Tian,Ruixiao Peng,Xuyi Lu,Yu Wang,Wei Yao,Guihua Shan*

Main category: cs.CV

TL;DR: TemporalFlowViz用ViT嵌入+降维聚类+专家标注+视觉-语言摘要，构建参数感知的时序流场可视分析流程，提升了冲压燃烧模拟数据的模式发现与可解释性。


<details>
  <summary>Details</summary>
Motivation: 大规模、高维的时序流场模拟数据阻碍可视解释、特征区分与跨案例比较，需一种可扩展且可解释的分析流程，帮助领域专家从海量模拟中发现模式并生成假设。

Method: 使用预训练的Vision Transformer从每帧流场图像提取高维嵌入，随后进行降维与基于密度的聚类以识别潜在燃烧模态，构建嵌入空间中的时间轨迹以追踪每个模拟随时间的演化；专家为簇质心标注语义标签，并利用视觉-语言模型基于这些标签生成帧级和案例级自然语言总结；系统提供参数过滤、相似检索与多视图协调交互。

Result: 通过两例专家指导的案例研究与专家反馈，系统展示了在模式发现、可解释性与假设生成方面的有效性，增强了在大规模冲压燃烧分析中的知识发现能力。

Conclusion: TemporalFlowViz有效提升了对大型冲压燃烧模拟时序流场数据的可视解析能力，支持通过预训练ViT嵌入、降维、密度聚类和语义化描述来发现潜在燃烧模式与演化轨迹，从而促进专家驱动的假设生成与知识发现。

Abstract: Understanding the complex combustion dynamics within scramjet engines is
critical for advancing high-speed propulsion technologies. However, the large
scale and high dimensionality of simulation-generated temporal flow field data
present significant challenges for visual interpretation, feature
differentiation, and cross-case comparison. In this paper, we present
TemporalFlowViz, a parameter-aware visual analytics workflow and system
designed to support expert-driven clustering, visualization, and interpretation
of temporal flow fields from scramjet combustion simulations. Our approach
leverages hundreds of simulated combustion cases with varying initial
conditions, each producing time-sequenced flow field images. We use pretrained
Vision Transformers to extract high-dimensional embeddings from these frames,
apply dimensionality reduction and density-based clustering to uncover latent
combustion modes, and construct temporal trajectories in the embedding space to
track the evolution of each simulation over time. To bridge the gap between
latent representations and expert reasoning, domain specialists annotate
representative cluster centroids with descriptive labels. These annotations are
used as contextual prompts for a vision-language model, which generates
natural-language summaries for individual frames and full simulation cases. The
system also supports parameter-based filtering, similarity-based case
retrieval, and coordinated multi-view exploration to facilitate in-depth
analysis. We demonstrate the effectiveness of TemporalFlowViz through two
expert-informed case studies and expert feedback, showing TemporalFlowViz
enhances hypothesis generation, supports interpretable pattern discovery, and
enhances knowledge discovery in large-scale scramjet combustion analysis.

</details>


### [26] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: OmniFHT通过傅里叶衍射定理与INR联合优化未知旋转和体积分布，实现在流式细胞中对任意形状多轴旋转细胞的无姿态高通量3D折射率重建，且对稀疏视角和受限角度鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有流式QPI方法依赖于单轴、已知姿态旋转，导致只能对近球形细胞和一部分细胞群体进行准确成像，限制统计分析能力。需要一种对任意形状和复杂旋转鲁棒的无姿态重建方法。

Method: 基于傅里叶衍射定理和隐式神经表示（INR），联合优化每个细胞的未知旋转轨迹和体积结构，在弱散射假设下从多角度投影重建RI分布；连续表示支持稀疏视角和受限角覆盖重建。

Result: OmniFHT可用少至10个视角或仅120°角范围得到高保真重建，实现了对整个流动细胞群体的就地高通量断层成像，支持无偏标签自由形态计量分析。

Conclusion: 提出了OmniFHT，一个无需已知姿态的3D折射率重建框架，能在流式细胞术中对任意形状和多轴旋转的细胞进行高通量断层成像。

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


### [27] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: 针对移动重建任务，CoRe-GS通过语义高斯点云生成粗分割并对PoI做颜色过滤细化，缩短训练时间并提升目标的3D重建与新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 移动无人机在实时或近实时任务中需要在保证目标对象重建质量的同时减少整体训练时间，且PoI通常在采集阶段已知，故专注对象可提升效率。

Method: 先用语义高斯点云生成粗略语义分割场景，然后对关注对象进行基于颜色的有效过滤与细化，从而只对PoI进行增量训练与优化。

Result: 在SCRREAM和NeRDS 360数据集上，CoRe-GS使训练时间约减少四分之一，同时在新视角合成质量上优于标准语义高斯点云方法。

Conclusion: CoRe-GS在半训练周期内即可实现语义目标的高质量重建，训练时间大幅减少且在新视角合成方面优于基线方法。

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.

</details>


### [28] [Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning](https://arxiv.org/abs/2509.04886)
*Trixia Simangan,Ahmed Nadeem Abbasi,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: Cryo-RL uses reinforcement learning to automate cryoablation planning, achieving better tumor coverage than automated baselines and matching experts while being faster.


<details>
  <summary>Details</summary>
Motivation: Develop an automated, efficient, and reproducible planning method for prostate cryoablation to overcome manual, expertise-dependent, and time-consuming current planning, improving tumor coverage and sparing healthy tissue.

Method: Model cryoablation planning as an MDP; train an RL agent in a simulated environment with clinical constraints and stochastic variability; agent sequentially selects probe positions and ice sphere diameters guided by a reward based on tumor coverage; evaluate on retrospective cases and compare to baselines and human experts.

Result: Cryo-RL, a reinforcement learning framework modeling cryoprobe placement as a Markov decision process, learns optimal probe positions and ice sphere diameters in a simulated environment, outperforming geometric optimization baselines by >8 percentage-point Dice and matching expert performance on 583 retrospective cases with much faster planning.

Conclusion: Reinforcement learning can produce clinically viable, reproducible, and efficient cryoablation plans, matching expert performance and improving over prior automated methods.

Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer
that destroys malignant tissue during de-freezing, while sparing surrounding
healthy structures. Its success depends on accurate preoperative planning of
cryoprobe placements to fully cover the tumour and avoid critical anatomy. This
planning is currently manual, expertise-dependent, and time-consuming, leading
to variability in treatment quality and limited scalability. In this work, we
introduce Cryo-RL, a reinforcement learning framework that models cryoablation
planning as a Markov decision process and learns an optimal policy for
cryoprobe placement. Within a simulated environment that models clinical
constraints and stochastic intraoperative variability, an agent sequentially
selects cryoprobe positions and ice sphere diameters. Guided by a reward
function based on tumour coverage, this agent learns a cryoablation strategy
that leads to optimal cryoprobe placements without the need for any
manually-designed plans. Evaluated on 583 retrospective prostate cancer cases,
Cryo-RL achieved over 8 percentage-point Dice improvements compared with the
best automated baselines, based on geometric optimisation, and matched human
expert performance while requiring substantially less planning time. These
results highlight the potential of reinforcement learning to deliver clinically
viable, reproducible, and efficient cryoablation plans.

</details>


### [29] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 预训练视觉模型能在可解释基础上以MAE≈10预测对蜘蛛的恐惧，但对数据规模敏感，且在特定视觉条件下误差较大，提示需要足够的数据和关注可解释性以用于临床自适应曝光疗法。


<details>
  <summary>Details</summary>
Motivation: 探索能否利用已有的计算机视觉预训练模型为自适应曝光疗法等临床应用提供实时、可解释的恐惧等级预测，从而推动情绪感知治疗技术的发展。

Method: 使用迁移学习调整三种不同的预训练计算机视觉模型，在313张标准化图像上预测0-100范围的人类恐惧评分，并用交叉验证评估性能；分析学习曲线以评估数据量影响；通过可解释性方法（如注意力/热图分析）检查模型关注的特征；进行类别误差分析以识别误差高的视觉条件。

Result: 三种模型的平均绝对误差(MAE)在10.1到11.0之间；减少训练数据显著降低性能，增加数据到当前规模上方收益有限；可解释性分析表明模型关注的确实是蜘蛛相关视觉特征；远距或人工/彩绘蜘蛛等图像类别导致更高的预测误差。

Conclusion: 该研究证明了预训练视觉模型可用于从蜘蛛相关图像预测人的恐惧评分，且模型可解释性验证了预测依赖蜘蛛相关特征，但对数据量敏感，且在某些视觉条件下误差较大。

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [30] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: 结合VLM与3D渲染生成可控合成生锈数据，训练的模型在真实图像上mAP50=0.87，展现出高效、可扩展的数据增强方案，用于工业磨损检测。


<details>
  <summary>Details</summary>
Motivation: 工业磨损（尤其生锈）数据难以获取且标注昂贵，导致难以训练鲁棒的计算机视觉模型。合成数据可低成本、高可控地覆盖多样化磨损场景，解决数据稀缺问题。

Method: 使用视觉语言模型（VLM）生成场景描述与参数，驱动3D仿真环境渲染各种生锈程度与位置的合成图像，并用这些图像标注训练检测模型（如目标检测网络）；最后在真实数据上验证性能。

Result: 使用该方法生成的合成数据训练的模型在真实重工业物体的生锈检测任务上达到mAP50=0.87，优于其他方法。方法具有可定制性，可扩展到其他磨损类型。

Conclusion: 该文提出了一种结合视觉语言模型与3D仿真渲染引擎生成合成数据的方法，用于工业磨损（以生锈为例）检测的模型训练，实验证明在真实图像上效果良好，具有可扩展性。

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [31] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 在sebocyte液滴计数任务中，简单的bag-level聚合MLP比未经优化的注意力MIL更稳健；MIL有潜力但需改进池化和正则化。


<details>
  <summary>Details</summary>
Motivation: 手工计数耗时且主观，需要自动化方法来量化sebocyte内脂滴以评估分化状态。

Method: 构建并比较两个模型：1) 基线MLP在聚合的patch级计数上训练；2) 基于注意力的MIL利用ResNet-50提取实例特征并对实例加权。使用Nile Red染色图像分为14类并通过数据增强扩充至约50,000个细胞，采用五折交叉验证评价。

Result: 五折交叉验证中，基线MLP表现更稳定（平均MAE=5.6），而注意力MIL平均MAE=10.7且波动较大，但在个别折次中表现更优。结果提示MIL需对齐的池化策略和正则化以发挥优势。

Conclusion: Attention-based MIL在本任务中未能稳定超越简单聚合基线；简单的bag-level聚合方法提供了更稳健的滑片级液滴计数基线。

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [32] [UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features](https://arxiv.org/abs/2509.04932)
*Haowang Cui,Rui Chen,Tao Luo,Rui Li,Jiaze Wang*

Main category: cs.CV

TL;DR: UniView 通过检索相似对象图像并用 MLLM 辅助选择参考，结合多层适配器与解耦三重注意力，有效减少单视图新视角合成的歪曲并提升性能。


<details>
  <summary>Details</summary>
Motivation: 单张图像合成新视角高度病态，基于输入视图插值或模糊先验常导致严重失真，故引入相似对象参考图像作为更强的先验以约束未观测区域的生成。

Method: 构建检索与增强系统并用 MLLM 协助筛选参考图像；设计可插拔适配器模块，含多层隔离层用于动态生成目标视角参考特征；提出解耦三重注意力机制以对齐并融合多分支特征到合成流程。

Result: UniView 提出了一种利用相似对象参考图像来改进单张图像新视角合成的方法，通过检索与增强系统结合多模态大语言模型（MLLM）选择参考图像，并引入可插拔适配器与多层隔离以及解耦三重注意力机制来生成并融合参考特征，从而在保持输入细节的同时提升合成质量。

Conclusion: 利用相似对象的参考图像作为强先验，配合动态生成的参考特征和解耦注意力融合机制，UniView 在多个挑战性数据集上明显优于现有方法，能更好保留原图细节并降低合成扭曲。

Abstract: The task of synthesizing novel views from a single image is highly ill-posed
due to multiple explanations for unobserved areas. Most current methods tend to
generate unseen regions from ambiguity priors and interpolation near input
views, which often lead to severe distortions. To address this limitation, we
propose a novel model dubbed as UniView, which can leverage reference images
from a similar object to provide strong prior information during view
synthesis. More specifically, we construct a retrieval and augmentation system
and employ a multimodal large language model (MLLM) to assist in selecting
reference images that meet our requirements. Additionally, a plug-and-play
adapter module with multi-level isolation layers is introduced to dynamically
generate reference features for the target views. Moreover, in order to
preserve the details of an original input image, we design a decoupled triple
attention mechanism, which can effectively align and integrate multi-branch
features into the synthesis process. Extensive experiments have demonstrated
that our UniView significantly improves novel view synthesis performance and
outperforms state-of-the-art methods on the challenging datasets.

</details>


### [33] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: MFM-Mapper融合双视觉编码器并用GPT-2替换线性mapper，实现更好且更高效的视频到音频生成条件对齐。


<details>
  <summary>Details</summary>
Motivation: 现有V2A方法从头训练代价高昂；使用基础模型（FMs）可实现更好知识迁移与泛化。已有工作通过轻量mapper连接视觉编码器与文本到音频生成模型，但存在语义/时序信息不足与线性映射能力受限的问题。

Method: 提出Multiple Foundation Model Mapper（MFM-Mapper）：1) 融合来自两个预训练视觉编码器的语义和时序特征；2) 用GPT-2作为自回归映射网络替代线性映射器实现跨模态特征对齐；仅训练轻量级mapper保持下游音频生成模型冻结。

Result: 在训练规模仅为之前mapper方法16%情况下，MFM-Mapper在语义与时序一致性上取得更好结果，并与大规模训练模型性能相当，体现出高效训练和优越的跨模态对齐能力。

Conclusion: MFM-Mapper通过融合双视觉编码器特征并用GPT-2替代线性映射器，在视频到音频生成任务上在语义和时序一致性上表现更好，同时显著降低训练资源需求。

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [34] [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/abs/2509.05000)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出GD^2Fusion：结合VLM进行退化感知，利用GFMSE（频域）与GSMAF（空域）双域协同优化，实现端到端鲁棒的红外-可见图像融合，显著优于传统预增强+融合的流水线方法。


<details>
  <summary>Details</summary>
Motivation: 现有IVIF方法假设输入为高质量图像，面对双源同时退化（如噪声、模糊、低光）通常需要手动多步预增强，导致错误累积和性能下降，因此需要一个端到端感知退化并联合优化的融合框架。

Method: 设计了两个引导模块：GFMSE在频域感知并抑制退化，选择性提取对融合有用的子带特征；GSMAF在空域执行跨模态退化过滤和自适应多源特征聚合，增强模态互补与结构一致性；整体利用VLM辅助退化感知并在双域联合优化融合网络。

Result: 在大量定性与定量实验中，GD^2Fusion在双源退化条件下明显优于现有方法和策略，展示了更好的细节保留、结构一致性及模态互补利用能力。

Conclusion: GD^2Fusion通过联合视觉语言模型感知退化并在频域与空域内协同优化，有效避免了预增强与融合解耦带来的误差累积，提升了双源退化场景下的红外-可见图像融合性能。

Abstract: Most existing infrared-visible image fusion (IVIF) methods assume
high-quality inputs, and therefore struggle to handle dual-source degraded
scenarios, typically requiring manual selection and sequential application of
multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion
pipeline inevitably leads to error accumulation and performance degradation. To
overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),
a novel framework that synergistically integrates vision-language models (VLMs)
for degradation perception with dual-domain (frequency/spatial) joint
optimization. Concretely, the designed Guided Frequency Modality-Specific
Extraction (GFMSE) module performs frequency-domain degradation perception and
suppression and discriminatively extracts fusion-relevant sub-band features.
Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries
out cross-modal degradation filtering and adaptive multi-source feature
aggregation in the spatial domain to enhance modality complementarity and
structural consistency. Extensive qualitative and quantitative experiments
demonstrate that GD^2Fusion achieves superior fusion performance compared with
existing algorithms and strategies in dual-source degraded scenarios. The code
will be publicly released after acceptance of this paper.

</details>


### [35] [Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study](https://arxiv.org/abs/2509.05004)
*Mohammad Abbadi,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 本文通过在多个超声数据集上比较SVM/KNN与多个CNN，展示了ResNet-18在乳腺超声良恶性分类上的极高性能（99.7%准确率）并用Grad-CAM增强可解释性，支持将AI诊断系统集成到临床。


<details>
  <summary>Details</summary>
Motivation: 提高超声影像在乳腺癌早期筛查中的诊断准确性和可解释性，尤其是在稠密乳腺患者中，推动AI工具在临床流程中的应用。

Method: 比较了经典机器学习（SVM、KNN）与深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet），使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集；对经典模型进行了深特征提取以提升其性能；使用Grad-CAM进行可视化解释。

Result: ResNet-18取得最高准确率99.7%并对恶性病变达到完美敏感性；CNN整体优于经典ML，但后者在结合深度特征后仍具有竞争力；Grad-CAM能突出诊断相关区域。

Conclusion: 该论文表明基于超声图像的深度学习方法，尤其是ResNet-18，在乳腺癌良恶性分类任务上可达到极高的性能，并且通过Grad-CAM提高了可解释性，具有临床应用潜力。

Abstract: Breast cancer remains a leading cause of cancer-related mortality among women
worldwide. Ultrasound imaging, widely used due to its safety and
cost-effectiveness, plays a key role in early detection, especially in patients
with dense breast tissue. This paper presents a comprehensive study on the
application of machine learning and deep learning techniques for breast cancer
classification using ultrasound images. Using datasets such as BUSI, BUS-BRA,
and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,
KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,
GoogLeNet). Experimental results show that ResNet-18 achieves the highest
accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML
models, though outperformed by CNNs, achieve competitive performance when
enhanced with deep feature extraction. Grad-CAM visualizations further improve
model transparency by highlighting diagnostically relevant image regions. These
findings support the integration of AI-based diagnostic tools into clinical
workflows and demonstrate the feasibility of deploying high-performing,
interpretable systems for ultrasound-based breast cancer detection.

</details>


### [36] [A biologically inspired separable learning vision model for real-time traffic object perception in Dark](https://arxiv.org/abs/2509.05012)
*Hulin Li,Qiliang Ren,Jun Li,Hanbing Wei,Zheng Liu,Linfang Fan*

Main category: cs.CV

TL;DR: 本文提出了面向真实低光交通场景的物理光照降解方法并构建了最大规模的低光交通数据集Dark-traffic，支持检测、实例分割与光流评估；同时提出生物启发的可分离学习视觉模型SLVM，包括光自适应瞳孔机制、特征可分离学习、任务解耦分支和空间对齐融合模块，在多项任务与基准上显著优于现有方法并降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有感知模型在真实低光交通场景中性能下降明显，且缺乏专门面向低光交通的大规模基准，导致方法难以快速适应与评估。

Method: 提出物理驱动的光照降解模拟生成真实低光效果；构建大规模标注数据集Dark-traffic；设计SLVM，包括：1) 光自适应瞳孔模块用于光照敏感特征提取；2) 特征级可分离学习策略以高效表征；3) 任务专用解耦分支实现多任务可分离学习；4) 空间错位感知融合模块用于精确多特征对齐融合。

Result: 在Dark-traffic上，SLVM分别在检测、实例分割和光流任务上显著超过RT-DETR、YOLOv12等基线，检测提升11.2个百分点，分割提升6.1个百分点，光流EPE下降12.37%；在LIS基准上，平均超越多种结合增强前处理的方法约11个百分点，并超过Mask RCNN+增强3.1个百分点。

Conclusion: SLVM在Dark-traffic和LIS基准上均实现了显著性能提升和更低计算成本，证明了可分离学习与生物启发机制在低光条件下的有效性；Dark-traffic为低光驾驶感知研究提供了重要资源。

Abstract: Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.

</details>


### [37] [Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition](https://arxiv.org/abs/2509.05019)
*Mohsine El Khayati,Ayyad Maafiri,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 将迁移学习与轻量级MbNets结合能在资源受限场景实现高精度AHCR，推荐使用全量微调与MobileNet/ShuffleNet等模型，并对数据集特性与数据增强进行进一步研究。


<details>
  <summary>Details</summary>
Motivation: 解决AHCR面临的计算开销大和标注数据匮乏问题，探索在移动/嵌入式场景下使用轻量级网络与迁移学习以实现高性能、低资源消耗的识别系统。

Method: 比较三种迁移学习策略（全量微调、部分微调、从头训练）在四种MbNets（MobileNet、SqueezeNet、MnasNet、ShuffleNet）上的表现，使用三个基准数据集（AHCD、HIJJA、IFHCDB）进行实验，评估指标包括准确率、收敛速度、鲁棒性与泛化能力。

Result: 主要结果为：MobileNet总体表现最好；ShuffleNet在泛化上表现优异，尤其在全量微调下；IFHCDB在MnasNet与全量微调下达到了99%准确率；AHCD在ShuffleNet上达到了97%；HIJJA最难，ShuffleNet最高92%。全量微调在准确率和收敛速度间取得最佳平衡，部分微调总体表现不佳。

Conclusion: 该研究表明，将迁移学习与轻量级移动端卷积神经网络(MbNets)结合，可在资源受限环境中实现高效的阿拉伯手写字符识别(AHCR)。其中全量微调表现最佳，MobileNet在准确性、鲁棒性与效率间取得均衡，ShuffleNet在泛化能力上突出。不同数据集表现差异显著，IFHCDB最易识别，HIJJA最具挑战性。

Abstract: The study explores the integration of transfer learning (TL) with
mobile-enabled convolutional neural networks (MbNets) to enhance Arabic
Handwritten Character Recognition (AHCR). Addressing challenges like extensive
computational requirements and dataset scarcity, this research evaluates three
TL strategies--full fine-tuning, partial fine-tuning, and training from
scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and
ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,
HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently
achieving superior accuracy, robustness, and efficiency, with ShuffleNet
excelling in generalization, particularly under full fine-tuning. The IFHCDB
dataset yielded the highest results, with 99% accuracy using MnasNet under full
fine-tuning, highlighting its suitability for robust character recognition. The
AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA
posed significant challenges due to its variability, achieving a peak accuracy
of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall
performance, balancing accuracy and convergence speed, while partial
fine-tuning underperformed across metrics. These findings underscore the
potential of combining TL and MbNets for resource-efficient AHCR, paving the
way for further optimizations and broader applications. Future work will
explore architectural modifications, in-depth dataset feature analysis, data
augmentation, and advanced sensitivity analysis to enhance model robustness and
generalizability.

</details>


### [38] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: LUIVITON实现无需人工的复杂3D服装拟合，支持多类角色与后续尺寸/材质调整


<details>
  <summary>Details</summary>
Motivation: 自动化虚拟试衣在多层复杂服装与任意姿态角色间对齐困难

Method: 几句概述方法

Result: 提出LUIVITON，用SMPL代理并分解为衣物-to-SMPL与身体-to-SMPL两对应任务，前者用几何学习，后者用扩散模型与2D基础模型多视图外观特征

Conclusion: 系统能高质量自动拟合复杂服装至多样化角色，兼具效率与可定制性

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on,
capable of draping complex, multi-layer clothing onto diverse and arbitrarily
posed humanoid characters. To address the challenge of aligning complex
garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy
representation and separate the clothing-to-body draping problem into two
correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,
where each has its unique challenges. While we address the clothing-to-SMPL
fitting problem using a geometric learning-based approach for
partial-to-complete shape correspondence prediction, we introduce a diffusion
model-based approach for body-to-SMPL correspondence using multi-view
consistent appearance features and a pre-trained 2D foundation model. Our
method can handle complex geometries, non-manifold meshes, and generalizes
effectively to a wide range of humanoid characters -- including humans, robots,
cartoon subjects, creatures, and aliens, while maintaining computational
efficiency for practical adoption. In addition to offering a fully automatic
fitting solution, LUIVITON supports fast customization of clothing size,
allowing users to adjust clothing sizes and material properties after they have
been draped. We show that our system can produce high-quality 3D clothing
fittings without any human labor, even when 2D clothing sewing patterns are not
available.

</details>


### [39] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: ADClick: click+text IIS for efficient pixel annotations; ADClick-Seg: prototype cross-modal AD achieving SOTA on MVTec AD


<details>
  <summary>Details</summary>
Motivation: Reduce need for pixel-level defect annotations by using minimal user input (few clicks + text) to generate pixel-wise labels, improving scalability and AD performance

Method: Interactive Image Segmentation for industrial Anomaly Detection

Result: ADClick produces high-quality pixel annotations with limited input improving AD models (AP 96.1% on MVTec AD); ADClick-Seg aligns visual features and text via prototype-based cross-modal approach, achieving SOTA on multi-class AD (AP 80.0%, PRO 97.5%, Pixel-AUROC 99.1%)

Conclusion: Combining minimal interactive annotations with language guidance yields scalable, high-performance anomaly detection and localization in industrial settings.

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [40] [Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction](https://arxiv.org/abs/2509.05071)
*Mojtaba Safari,Zach Eidex,Richard L. J. Qiu,Matthew Goette,Tonghe Wang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 深度学习生成模型能有效校正MRI运动伪影，但需公开标准数据集、统一报告伪影强度、并发展能在无配对数据下稳健工作的方法，以促进临床应用。


<details>
  <summary>Details</summary>
Motivation: 评价并汇总AI驱动方法在MRI运动伪影检测与校正领域的现状、有效性与局限，为未来研究与临床应用提供方向。

Method: 系统综述与荟萃分析，聚焦深度学习方法（特别是生成模型）用于MRI运动伪影检测与校正；提取定量数据包括数据集、网络架构、性能指标；对比分析模型效果与限制。

Result: 深度学习（尤其生成模型）在降低运动伪影和改善图像质量方面表现良好，但普遍问题包括泛化能力不足、对成对训练数据的依赖、视觉伪影风险及评估标准不统一。

Conclusion: AI方法，尤其是基于深度学习的生成模型，在减少MRI运动伪影和提升图像质量方面具有显著潜力，但存在泛化性差、依赖配对训练数据、可能引入视觉伪影等关键挑战。

Abstract: Background: To systematically review and perform a meta-analysis of
artificial intelligence (AI)-driven methods for detecting and correcting
magnetic resonance imaging (MRI) motion artifacts, assessing current
developments, effectiveness, challenges, and future research directions.
Methods: A comprehensive systematic review and meta-analysis were conducted,
focusing on deep learning (DL) approaches, particularly generative models, for
the detection and correction of MRI motion artifacts. Quantitative data were
extracted regarding utilized datasets, DL architectures, and performance
metrics. Results: DL, particularly generative models, show promise for reducing
motion artifacts and improving image quality; however, limited
generalizability, reliance on paired training data, and risk of visual
distortions remain key challenges that motivate standardized datasets and
reporting. Conclusions: AI-driven methods, particularly DL generative models,
show significant potential for improving MRI image quality by effectively
addressing motion artifacts. However, critical challenges must be addressed,
including the need for comprehensive public datasets, standardized reporting
protocols for artifact levels, and more advanced, adaptable DL techniques to
reduce reliance on extensive paired datasets. Addressing these aspects could
substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and
improve patient care outcomes.

</details>


### [41] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: GeoSplat通过结合一阶与二阶几何先验与稳健估计，改进Gaussian splatting的初始化与优化，从而提升新视点合成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用低阶几何先验且依赖对噪声敏感的估计，导致优化受限；引入高阶几何信息与鲁棒估计可提供更准确的几何约束，提升Gaussian splatting效果。

Method: 1) 在初始化阶段用主曲率设定3D高斯原语的尺度，改善表面覆盖；2) 在训练中将一阶和二阶几何量作为正则化项引导梯度更新；3) 在致密化阶段利用几何结构（如局部流形）与鲁棒估计动态生成几何先验以细化模型。

Result: GeoSplat提出一种几何约束优化框架，将一阶（法线）与二阶（主曲率）几何量融入Gaussian splatting训练流程，改进初始化、梯度更新与致密化过程；用基于主曲率的尺度初始化提高表面覆盖，并引入稳健估计方法提供动态几何先验；在多数据集的视点合成任务上显著优于先前基线。

Conclusion: 结合主曲率等二阶信息并采用噪声鲁棒的估计方法，可显著提升Gaussian splatting的训练质量与渲染性能，优于只用低阶几何先验的先前方法。

Abstract: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.

</details>


### [42] [Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction](https://arxiv.org/abs/2509.05078)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 将多尺度并行卷积产生的特征序列输入 Transformer，自注意力建模尺度间交互，显著提升面部美学回归性能（SCUT-FBP5500 PCC=0.9187）。


<details>
  <summary>Details</summary>
Motivation: 解决 CNN 在固定尺度下可能忽视不同粒度特征间交互的问题，提升面部美学预测的性能。

Method: 先用并行多尺度卷积模块提取不同感受野的特征，将这些多尺度表示拼接为序列，再通过 Transformer 编码器利用自注意力显式建模它们的交互与上下文关系，最后进行回归预测。

Result: 提出 Scale-Interaction Transformer (SIT)，结合多尺度并行卷积提取和 Transformer 编码器自注意力建模尺度间关系，在 SCUT-FBP5500 数据集上达成 Pearson 0.9187，刷新最优。

Conclusion: 显式建模多尺度视觉线索间的相互作用对高性能 FBP 至关重要，混合 CNN-Transformer 架构在复杂图像回归任务中具有潜力。

Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision
task due to the complex interplay of local and global facial features that
influence human perception. While Convolutional Neural Networks (CNNs) excel at
feature extraction, they often process information at a fixed scale,
potentially overlooking the critical inter-dependencies between features at
different levels of granularity. To address this limitation, we introduce the
Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture
that synergizes the feature extraction power of CNNs with the relational
modeling capabilities of Transformers. The SIT first employs a multi-scale
module with parallel convolutions to capture facial characteristics at varying
receptive fields. These multi-scale representations are then framed as a
sequence and processed by a Transformer encoder, which explicitly models their
interactions and contextual relationships via a self-attention mechanism. We
conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark
dataset, where the proposed SIT model establishes a new state-of-the-art. It
achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our
findings demonstrate that explicitly modeling the interplay between multi-scale
visual cues is crucial for high-performance FBP. The success of the SIT
architecture highlights the potential of hybrid CNN-Transformer models for
complex image regression tasks that demand a holistic, context-aware
understanding.

</details>


### [43] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 在ResNet上用稀疏MoE替换深层模块并配合对抗训练，可提升对抗鲁棒性；switch loss导致路由崩溃并促成鲁棒子路径，个别专家可超越整体MoE。


<details>
  <summary>Details</summary>
Motivation: 提高CNN对抗攻击的鲁棒性，同时避免额外推理开销，通过稀疏专家混合（MoE）层替换部分残差块或卷积层以增加模型容量。

Method: 在ResNet模型的深层阶段替换选定残差块或卷积层为稀疏MoE层，使用对抗训练（PGD/AutoPGD）评估鲁棒性；比较有无switch loss下的路由行为与各专家性能，分析专家专门化与鲁棒性关系。

Result: 在ResNet/CIFAR-100上，插入单个MoE层于深层阶段并结合对抗训练，可在PGD和AutoPGD攻击下稳定提升鲁棒性；使用switch loss会导致路由崩溃至少数专家，使这些路径通过对抗训练变得更加鲁棒；部分单独专家甚至比带门控的MoE整体更鲁棒，显示出专业化产生的鲁棒子路径。

Conclusion: 稀疏MoE层是提高对抗鲁棒性的有效手段，插在深层效果更好；需注意switch loss引发的路由失衡，可能产生被强化的子路径，提示未来设计应关注路由均衡与专家专门化之间的权衡。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [44] [Semi-supervised Deep Transfer for Regression without Domain Alignment](https://arxiv.org/abs/2509.05092)
*Mainak Biswas,Ambedkar Dukkipati,Devarajan Sridharan*

Main category: cs.CV

TL;DR: CRAFT是一种基于CUDA的无源、半监督回归迁移方法，通过对比辨别式正则化在无源条件下利用少量有标签目标和大量无标签目标数据，显著降低RMSE，适用于医学与生物学中的回归任务。


<details>
  <summary>Details</summary>
Motivation: 现实应用中源域数据常因隐私或体量大而不可用，同时目标域标注稀缺，但需将预训练模型迁移到域偏移的目标数据并处理连续值预测（回归），现有无源方法多为分类或需访问源数据，缺乏针对回归的有效策略。

Method: 基于Contradistinguisher(CUDA)扩展，提出CRAFT：在无源（source-free）条件下对预训练模型进行半监督迁移，采用对比辨别式正则化而非中间表示对齐，适配回归任务，结合有标签少量目标样本与大量无标签目标样本训练。

Result: 在EEG注视（gaze）和结构性MRI脑龄预测两类神经科学数据上，CRAFT在标注稀少时较微调模型最多降低9% RMSE，且较四种最先进的无源域适应模型提升超过3%。此外，在两项其他真实回归基准上也验证了其有效性。

Conclusion: CRAFT有效提高了在无源数据、标注稀缺的回归迁移场景下的预测性能，尤其在神经科学应用中表现突出。

Abstract: Deep learning models deployed in real-world applications (e.g., medicine)
face challenges because source models do not generalize well to domain-shifted
target data. Many successful domain adaptation (DA) approaches require full
access to source data. Yet, such requirements are unrealistic in scenarios
where source data cannot be shared either because of privacy concerns or
because it is too large and incurs prohibitive storage or computational costs.
Moreover, resource constraints may limit the availability of labeled targets.
We illustrate this challenge in a neuroscience setting where source data are
unavailable, labeled target data are meager, and predictions involve
continuous-valued outputs. We build upon Contradistinguisher (CUDA), an
efficient framework that learns a shared model across the labeled source and
unlabeled target samples, without intermediate representation alignment. Yet,
CUDA was designed for unsupervised DA, with full access to source data, and for
classification tasks. We develop CRAFT -- a Contradistinguisher-based
Regularization Approach for Flexible Training -- for source-free (SF),
semi-supervised transfer of pretrained models in regression tasks. We showcase
the efficacy of CRAFT in two neuroscience settings: gaze prediction with
electroencephalography (EEG) data and ``brain age'' prediction with structural
MRI data. For both datasets, CRAFT yielded up to 9% improvement in
root-mean-squared error (RMSE) over fine-tuned models when labeled training
examples were scarce. Moreover, CRAFT leveraged unlabeled target data and
outperformed four competing state-of-the-art source-free domain adaptation
models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two
other real-world regression benchmarks. We propose CRAFT as an efficient
approach for source-free, semi-supervised deep transfer for regression that is
ubiquitous in biology and medicine.

</details>


### [45] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 提出无需UV映射和可微渲染的Transformer-triplane方法，能在0.2s内从单张图和网格生成高质量3D纹理，效果优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有生成方法速度慢、依赖UV映射且难以保持对参考图像的忠实度，限制了大规模、高质量3D内容生成的可用性。

Method: 将triplane表示与基于深度的反向投影损失相结合，使用Transformer架构学习从图像和网格到3D纹理场的映射，训练后推理速度快（约0.2s/形状），无需UV或可微渲染。

Result: 在定性、定量及用户偏好实验中，该方法在单图纹理重建任务上优于最先进基线，既更忠实于输入图像，也在感知质量上更好。

Conclusion: 该论文提出了一种基于Transformer的框架，直接从单张图像和网格预测3D纹理场，省去UV映射与可微渲染，实现在单次前向传播中快速生成高保真纹理。

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [46] [SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing](https://arxiv.org/abs/2509.05144)
*Chaolei Wang,Yang Luo,Jing Du,Siyu Chen,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: 提出无训练的SGS-3D：先用几何原语过滤并拆分模糊掩码，再基于空间连续性与高层特征生长掩码，提升从2D到3D实例分割的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 2D-to-3D提升方法受到语义模糊和深度约束不足导致的掩码误差积累，亟需一种无需训练即可对提升掩码进行精化的方法，使得3D实例分割更精确。

Method: 先用基于几何原语的掩码过滤与分割净化模糊的提升掩码，然后通过空间连续性和高阶特征进行掩码生长，构建精细的3D实例，整个过程训练免费。

Result: 在ScanNet200、ScanNet++和KITTI-360上，SGS-3D在不依赖预训练模型重训练的情况下，显著提升了分割精度与对不准确掩码的鲁棒性，并在室内与室外场景均表现出良好泛化。

Conclusion: SGS-3D通过“先分割再生长”策略，有效利用几何与语义信息联合优化，被证明能显著提高2D-to-3D提升方法的实例分割精度和鲁棒性。

Abstract: Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.

</details>


### [47] [SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition](https://arxiv.org/abs/2509.05188)
*Ariel Basso Madjoukeng,Jérôme Fink,Pierre Poitier,Edith Belise Kenmogne,Benoit Frenay*

Main category: cs.CV

TL;DR: 引入自由负样本对和专用数据增强的自监督框架，针对手语识别中负样本相似与关键帧重要性问题，显著提升了表征判别性与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习在手语识别上受限于两个问题：视频中只有部分片段对识别有用，但方法却均等对待所有片段；不同手势共享动作使负样本之间高度相似，从而妨碍判别特征学习。为此需要一种能识别并侧重关键信息，同时缓解难负样本干扰的自监督方法。

Method: 设计了两个协同模块：一是自由负样本对(self-supervised approach with free-negative pairs)，允许在正负样本构造时避开或动态选择难负样本，减轻共享动作导致的负样本干扰；二是专门为手语视频开发的数据增强策略，强调保留重要手势信息同时制造有意义的正样本变体。训练采用对比学习框架，但对正负样本采样和增强进行了任务感知的调整。

Result: 在多个指标上都有明显提升：与若干主流对比与自监督方法相比，在线性评估、少量标签的半监督设置以及不同手语间的迁移学习任务上，均取得了可观的精度增益（文中宣称“considerable gain”）。

Conclusion: 提出的无监督框架通过引入自由负样本对和新的数据增强策略，针对手语识别中关键帧信息不均与不同手势共享动作导致负样本相似的问题，学习到更具判别性的特征，从而在线性评估、半监督学习和跨语言迁移任务上都显著提升了性能。

Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify
signs in videos. Due to the scarcity of annotated data, unsupervised methods
like contrastive learning have become promising in this field. They learn
meaningful representations by pulling positive pairs (two augmented versions of
the same instance) closer and pushing negative pairs (different from the
positive pairs) apart. In SLR, in a sign video, only certain parts provide
information that is truly useful for its recognition. Applying contrastive
methods to SLR raises two issues: (i) contrastive learning methods treat all
parts of a video in the same way, without taking into account the relevance of
certain parts over others; (ii) shared movements between different signs make
negative pairs highly similar, complicating sign discrimination. These issues
lead to learning non-discriminative features for sign recognition and poor
results in downstream tasks. In response, this paper proposes a self-supervised
learning framework designed to learn meaningful representations for SLR. This
framework consists of two key components designed to work together: (i) a new
self-supervised approach with free-negative pairs; (ii) a new data augmentation
technique. This approach shows a considerable gain in accuracy compared to
several contrastive and self-supervised methods, across linear evaluation,
semi-supervised learning, and transferability between sign languages.

</details>


### [48] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 清洗ModelNet40得到ModelNet-R，并提出轻量级Point-SkipNet，显示数据集质量和结构化轻量模型可同时提升3D点云分类效率与准确率。


<details>
  <summary>Details</summary>
Motivation: 现有ModelNet40数据集存在标签不一致、包含二维或尺寸不匹配样本以及类别区分不充分等问题，这些限制影响模型训练与评估的可靠性和性能，需要一个更干净且一致的基准数据集；同时希望设计更轻量且高效的3D点云分类模型。

Method: 本文构建了ModelNet-R数据集，清理不一致标签、剔除/替换2D或不完整样本并规范化尺寸与类划分；提出Point-SkipNet，一种基于图的网络，采用高效采样、邻域分组与跳跃连接以降低计算量并保持表达能力；在ModelNet-R上进行广泛实验并与现有方法比较。

Result: 在改进的数据集ModelNet-R上，多种模型性能显著提升；Point-SkipNet在ModelNet-R上取得了最先进的分类准确率，同时模型参数大幅减少，验证了数据质量与轻量模型设计的有效性。

Conclusion: 该论文通过修正ModelNet40的标签与数据问题并提出轻量级网络Point-SkipNet，证明了高质量数据集能显著提升3D点云分类性能，且能在参数更少的前提下达到或超越现有方法。

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [49] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: Introduce SGP-GenBench; propose RL with format-validity gate and cross-modal rewards to train LLMs to generate renderable, semantically aligned SVGs; achieves frontier-level performance and reveals improved object decomposition and scene coherence.


<details>
  <summary>Details</summary>
Motivation: Explore LLMs' ability to generate symbolic graphics programs (SVG) from text and use SGPs as lens into visual grounding; address gap between proprietary and open models in SGP generation.

Method: Construct SGP-GenBench; use format-validity gate and RL with cross-modal rewards from strong vision encoders (SigLIP, DINO) applied to Qwen-2.5-7B; evaluate improvements and analyze training dynamics.

Result: Frontier proprietary models outperform open-source baselines; RL with verifiable rewards on Qwen-2.5-7B substantially improves SVG quality and semantics to near frontier performance; training leads to finer object decomposition and better scene coherence.

Conclusion: Paper shows RL with verifiable rewards enables LLMs to generate high-quality SVG SGPs, closing gap with frontier proprietary models; symbolic graphics programming provides interpretable cross-modal grounding.

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [50] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: 这篇论文提出COGITAO，一个用于视觉领域系统研究“组合性”和“泛化”的模块化数据生成框架与基准。通过在网格环境上对对象应用可组合的规则变换（28种变换，可调组合深度），生成海量任务规则和无限样本。基线实验显示现有视觉模型在域内表现良好但无法推广到新组合。代码与数据均已开源。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型缺乏像人类那样将熟知元素组合并推广到新场景的能力。需要一个系统化、可控且大规模的基准来研究视觉领域的组合性与泛化问题。

Method: 设计一个类似ARC-AGI的问题生成机制：在网格环境中定义28种可互操作的规则变换，支持可调的组合深度和细粒度的网格与对象参数，按规则生成训练/测试数据，进行基线模型评估。

Result: 生成了数百万条独特任务规则并能对每条规则生成无限样本；基线视觉模型在域内任务上表现良好，但在遇到新组合时普遍失败，展示了组合性泛化的挑战。

Conclusion: COGITAO作为一个灵活、可扩展且大规模的基准，揭示了现代视觉模型在组合性泛化方面的显著不足，并为进一步研究提供了丰富的数据与工具。

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [51] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: WinT3R 通过滑动窗口和全局相机令牌池，在不牺牲效率的前提下实现了高精度实时相机位姿估计与高质量点云重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建质量与实时性能之间存在权衡，需在不显著增加计算的情况下提升在线重建质量和位姿精度。

Method: 提出滑动窗口机制以在窗口内帧间充分交换信息，并采用紧凑相机表示与全局相机令牌池以提高位姿估计可靠性，构建了前馈式重建模型进行在线预测。

Result: 在多个数据集上的广泛实验表明，WinT3R 在在线重建质量、相机位姿估计和重建速度上均取得了最先进的表现；代码与模型已开源。

Conclusion: WinT3R 在保持实时性的同时显著提升了在线重建和相机位姿估计质量，达到了领先性能。

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


### [52] [FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases](https://arxiv.org/abs/2509.05297)
*Matteo Poggi,Fabio Tosi*

Main category: cs.CV

TL;DR: FlowSeek通过融合深度基础模型与低维运动参数化，实现在单卡消费级GPU上训练并取得比SEA-RAFT更好的跨数据集光流性能。


<details>
  <summary>Details</summary>
Motivation: 降低训练硬件成本，同时保持或提升跨数据集的光流估计性能。

Method: 结合最新光流网络设计、单张图像深度基础模型和经典低维运动参数化，构建紧凑准确的架构，在单卡消费级GPU上训练。

Result: 在Sintel Final和KITTI数据集上相对于SEA-RAFT分别有约10%和15%的相对提升，并在Spring和LayeredFlow上表现良好；训练硬件预算约为主流方法的1/8。

Conclusion: FlowSeek提出了一个在训练时硬件资源需求极低但仍能获得优异性能的光流框架。

Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [53] [Schema Inference for Tabular Data Repositories Using Large Language Models](https://arxiv.org/abs/2509.04632)
*Zhenyu Wu,Jiaoyan Chen,Norman W. Paton*

Main category: cs.DB

TL;DR: 提出SI-LLM：用大语言模型从最少元数据（仅列头和单元格值）推断表格的分层概念模式，实验表明在端到端及子任务上表现有竞争力，并开源了资源。


<details>
  <summary>Details</summary>
Motivation: 现实世界的表格数据往往来自异构来源，元数据稀疏且表示不一致，给数据理解与集成带来困难。现有工作在模式推断上受限于元数据或规则，作者希望利用大语言模型的语义能力在最低元数据条件下推断结构化模式。

Method: SI-LLM利用大型语言模型对列头与单元格示例进行语义理解，通过逐步子任务（如实体类型识别、属性抽取、层次结构构建与关系识别）生成分层概念模式。作者提供了详细提示工程与端到端流程，并将方法在多数据集上与现有方法逐步比较。

Result: 在两个不同来源的数据集（网页表与开放数据）上的大量实验表明，SI-LLM在端到端效果上表现良好，并在每个子任务上与或优于最先进方法。代码、提示与数据集已开源。

Conclusion: 该论文提出了一种基于大语言模型的模式推断方法SI-LLM，可在仅有列头和单元格值的情况下，自动推断表格数据的概念模式，包括实体类型、属性及实体间关系。

Abstract: Minimally curated tabular data often contain representational inconsistencies
across heterogeneous sources, and are accompanied by sparse metadata. Working
with such data is intimidating. While prior work has advanced dataset discovery
and exploration, schema inference remains difficult when metadata are limited.
We present SI-LLM (Schema Inference using Large Language Models), which infers
a concise conceptual schema for tabular data using only column headers and cell
values. The inferred schema comprises hierarchical entity types, attributes,
and inter-type relationships. In extensive evaluation on two datasets from web
tables and open data, SI-LLM achieves promising end-to-end results, as well as
better or comparable results to state-of-the-art methods at each step. All
source code, full prompts, and datasets of SI-LLM are available at
https://github.com/PierreWoL/SILLM.

</details>


### [54] [Efficient Exact Resistance Distance Computation on Small-Treewidth Graphs: a Labelling Approach](https://arxiv.org/abs/2509.05129)
*Meihao Liao,Yueyang Pan,Rong-Hua Li,Guoren Wang*

Main category: cs.DB

TL;DR: 通过将阻抗距离的割性质与树分解结合，提出TreeIndex标签索引，实现了在小树宽大图上精确且高效的阻抗距离查询。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机游走的方法只给出近似结果且在小树宽图（如路网）上效率差，而最短路径在此类图上可以利用割性质和树分解高效处理；因此作者希望将类似思想应用到阻抗距离计算以获得精确且高效的方法。

Method: 分析阻抗距离的割性质并结合树分解，证明r(s,t)仅依赖于s和t到分解根路径上的标签；基于此构建标签结构，算法复杂度为O(n·h_G^2·d_max)，标签大小为O(n·h_G)，查询时间分别为O(h_G)（点对）和O(n·h_G)（单源）。

Result: 提出TreeIndex，在实测中显著优于现有方法。示例：在完整美国路网上，单线程7小时构建了405GB标签，点对查询耗时10^{-3}秒，单源查询耗时190秒，是首个扩展到此规模的大图的精确方法。

Conclusion: 该论文提出了一种基于树分解的阻抗距离索引方法TreeIndex，能在小树宽图上高效构建紧凑标签并支持精确查询，填补了随机游走方法在此类图上效率差的空白。

Abstract: Resistance distance computation is a fundamental problem in graph analysis,
yet existing random walk-based methods are limited to approximate solutions and
suffer from poor efficiency on small-treewidth graphs (e.g., road networks). In
contrast, shortest-path distance computation achieves remarkable efficiency on
such graphs by leveraging cut properties and tree decompositions. Motivated by
this disparity, we first analyze the cut property of resistance distance. While
a direct generalization proves impractical due to costly matrix operations, we
overcome this limitation by integrating tree decompositions, revealing that the
resistance distance $r(s,t)$ depends only on labels along the paths from $s$
and $t$ to the root of the decomposition. This insight enables compact
labelling structures. Based on this, we propose \treeindex, a novel index
method that constructs a resistance distance labelling of size $O(n \cdot
h_{\mathcal{G}})$ in $O(n \cdot h_{\mathcal{G}}^2 \cdot d_{\max})$ time, where
$h_{\mathcal{G}}$ (tree height) and $d_{\max}$ (maximum degree) behave as small
constants in many real-world small-treewidth graphs (e.g., road networks). Our
labelling supports exact single-pair queries in $O(h_{\mathcal{G}})$ time and
single-source queries in $O(n \cdot h_{\mathcal{G}})$ time. Extensive
experiments show that TreeIndex substantially outperforms state-of-the-art
approaches. For instance, on the full USA road network, it constructs a $405$
GB labelling in $7$ hours (single-threaded) and answers exact single-pair
queries in $10^{-3}$ seconds and single-source queries in $190$ seconds--the
first exact method scalable to such large graphs.

</details>
