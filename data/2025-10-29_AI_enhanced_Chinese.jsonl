{"id": "2510.24307", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2510.24307", "abs": "https://arxiv.org/abs/2510.24307", "authors": ["Shyam Jesalpura", "Shengda Zhu", "Amir Shaikhha", "Antonio Barbalace", "Boris Grot"], "title": "Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing", "comment": null, "summary": "Running data analytics queries on serverless (FaaS) workers has been shown to\nbe cost- and performance-efficient for a variety of real-world scenarios,\nincluding intermittent query arrival patterns, sudden load spikes and\nmanagement challenges that afflict managed VM clusters. Alas, existing\nserverless data analytics works focus primarily on the serverless execution\nengine and assume the existence of a \"good\" query execution plan or rely on\nuser guidance to construct such a plan. Meanwhile, even simple analytics\nqueries on serverless have a huge space of possible plans, with vast\ndifferences in both performance and cost among plans.\n  This paper introduces Odyssey, an end-to-end serverless-native data analytics\npipeline that integrates a query planner, cost model and execution engine.\nOdyssey automatically generates and evaluates serverless query plans, utilizing\nstate space pruning heuristics and a novel search algorithm to identify\nPareto-optimal plans that balance cost and performance with low latency even\nfor complex queries. Our evaluations demonstrate that Odyssey accurately\npredicts both monetary cost and latency, and consistently outperforms AWS\nAthena on cost and/or latency.", "AI": {"tldr": "Odyssey\u4e3a\u65e0\u670d\u52a1\u5668(FaaS)\u73af\u5883\u8bbe\u8ba1\u7aef\u5230\u7aef\u6570\u636e\u5206\u6790\u7ba1\u9053\uff0c\u7ed3\u5408\u67e5\u8be2\u89c4\u5212\u5668\u3001\u6210\u672c\u6a21\u578b\u4e0e\u6267\u884c\u5f15\u64ce\uff0c\u81ea\u52a8\u751f\u6210\u5e76\u641c\u7d22\u670d\u52a1\u5668less\u67e5\u8be2\u8ba1\u5212\uff0c\u5229\u7528\u526a\u679d\u542f\u53d1\u5f0f\u548c\u65b0\u641c\u7d22\u7b97\u6cd5\u5feb\u901f\u627e\u5230\u5728\u6210\u672c\u4e0e\u6027\u80fd\u4e0a\u5448\u5e15\u7d2f\u6258\u6700\u4f18\u7684\u8ba1\u5212\uff1b\u8bc4\u4f30\u663e\u793a\u53ef\u51c6\u786e\u9884\u6d4b\u6210\u672c\u4e0e\u5ef6\u8fdf\uff0c\u5e76\u5728\u6210\u672c\u6216\u5ef6\u8fdf\u4e0a\u4f18\u4e8eAWS Athena\u3002", "motivation": "\u73b0\u6709serverless\u6570\u636e\u5206\u6790\u5de5\u4f5c\u591a\u805a\u7126\u6267\u884c\u5f15\u64ce\uff0c\u5047\u8bbe\u5df2\u6709\u826f\u597d\u67e5\u8be2\u8ba1\u5212\u6216\u4f9d\u8d56\u7528\u6237\u624b\u5de5\u6307\u5b9a\u3002\u4f46\u5728serverless\u4e2d\uff0c\u67e5\u8be2\u8ba1\u5212\u7a7a\u95f4\u5de8\u5927\u4e14\u5f71\u54cd\u6210\u672c\u4e0e\u6027\u80fd\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u7684\u7aef\u5230\u7aef\u89c4\u5212\u4e0e\u9009\u62e9\u5de5\u5177\u3002", "method": "\u6784\u5efa\u7aef\u5230\u7aef\u7cfb\u7edf\uff1a\u67e5\u8be2\u751f\u6210\u5668\u3001\u57fa\u4e8eserverless\u7279\u6027\u7684\u6210\u672c\u6a21\u578b\u3001\u6267\u884c\u5f15\u64ce\uff1b\u5bf9\u8ba1\u5212\u7a7a\u95f4\u8fdb\u884c\u72b6\u6001\u526a\u679d\u4e0e\u63d0\u51fa\u65b0\u641c\u7d22\u7b97\u6cd5\uff0c\u7528\u4ee5\u9ad8\u6548\u53d1\u73b0\u5e15\u7d2f\u6258\u6700\u4f18\u8ba1\u5212\uff0c\u5e76\u5bf9\u8ba1\u5212\u8fdb\u884c\u8bc4\u4f30\u4ee5\u9009\u53d6\u6267\u884c\u8ba1\u5212\u3002", "result": "Odyssey\u80fd\u51c6\u786e\u9884\u6d4b\u91d1\u94b1\u6210\u672c\u4e0e\u5ef6\u8fdf\uff1b\u5728\u8bc4\u6d4b\u4e2d\u76f8\u8f83\u4e8eAWS Athena\u5728\u6210\u672c\u6216\u5ef6\u8fdf\u4e0a\u6301\u7eed\u66f4\u4f18\uff0c\u80fd\u4e3a\u590d\u6742\u67e5\u8be2\u5feb\u901f\u627e\u5230\u4f4e\u5ef6\u8fdf\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u8ba1\u5212\u3002", "conclusion": "Odyssey\u80fd\u5728serverless\u73af\u5883\u4e2d\u81ea\u52a8\u751f\u6210\u5e76\u6311\u9009\u6210\u672c-\u6027\u80fd\u5e73\u8861\u7684\u67e5\u8be2\u6267\u884c\u8ba1\u5212\uff0c\u51c6\u786e\u9884\u6d4b\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u5b9e\u6d4b\u4f18\u4e8eAWS Athena\uff0c\u9002\u7528\u4e8e\u590d\u6742\u67e5\u8be2\u5e76\u5728\u4f4e\u5ef6\u8fdf\u4e0b\u8fd0\u884c\u3002"}}
{"id": "2510.24599", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2510.24599", "abs": "https://arxiv.org/abs/2510.24599", "authors": ["Harsha Kokel", "Aamod Khatiwada", "Tejaswini Pedapati", "Haritha Ananthakrishnan", "Oktie Hassanzadeh", "Horst Samulowitz", "Kavitha Srinivas"], "title": "Evaluating Joinable Column Discovery Approaches for Context-Aware Search", "comment": "This is an Experiments and Analysis paper. The source code, data,\n  and/or other artifacts have been made available at\n  https://github.com/IBM/ContextAwareJoin", "summary": "Joinable Column Discovery is a critical challenge in automating enterprise\ndata analysis. While existing approaches focus on syntactic overlap and\nsemantic similarity, there remains limited understanding of which methods\nperform best for different data characteristics and how multiple criteria\ninfluence discovery effectiveness. We present a comprehensive experimental\nevaluation of joinable column discovery methods across diverse scenarios. Our\nstudy compares syntactic and semantic techniques on seven benchmarks covering\nrelational databases and data lakes. We analyze six key criteria -- unique\nvalues, intersection size, join size, reverse join size, value semantics, and\nmetadata semantics -- and examine how combining them through ensemble ranking\naffects performance. Our analysis reveals differences in method behavior across\ndata contexts and highlights the benefits of integrating multiple criteria for\nrobust join discovery. We provide empirical evidence on when each criterion\nmatters, compare pre-trained embedding models for semantic joins, and offer\npractical guidelines for selecting suitable methods based on dataset\ncharacteristics. Our findings show that metadata and value semantics are\ncrucial for data lakes, size-based criteria play a stronger role in relational\ndatabases, and ensemble approaches consistently outperform single-criterion\nmethods.", "AI": {"tldr": "\u5bf9\u6bd4\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u57fa\u4e8e\u8bed\u6cd5\u548c\u8bed\u4e49\u7684\u5217\u8fde\u63a5\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728\u4e03\u4e2a\u57fa\u51c6\u4e0a\u5206\u6790\u516d\u7c7b\u5224\u636e\u53ca\u5176\u7ec4\u5408\uff0c\u53d1\u73b0\u5143\u6570\u636e\u4e0e\u503c\u8bed\u4e49\u5bf9\u6570\u636e\u6e56\u91cd\u8981\uff0c\u57fa\u4e8e\u5927\u5c0f\u7684\u5224\u636e\u5728\u5173\u7cfb\u578b\u6570\u636e\u5e93\u66f4\u6709\u6548\uff0c\u96c6\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u591a\u5173\u6ce8\u8bed\u6cd5\u91cd\u53e0\u6216\u8bed\u4e49\u76f8\u4f3c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u6570\u636e\u7279\u6027\u4e0b\u5404\u65b9\u6cd5\u8868\u73b0\u7684\u7cfb\u7edf\u6bd4\u8f83\u4ee5\u53ca\u591a\u5224\u636e\u7ed3\u5408\u6548\u679c\u7684\u7406\u89e3\u3002", "method": "\u5728\u4e03\u4e2a\u8986\u76d6\u5173\u7cfb\u578b\u6570\u636e\u5e93\u548c\u6570\u636e\u6e56\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u6bd4\u8bed\u6cd5\uff08\u5982\u4ea4\u96c6\u3001\u552f\u4e00\u503c\u6bd4\uff09\u4e0e\u8bed\u4e49\uff08\u5982\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\uff09\u65b9\u6cd5\uff0c\u5206\u6790\u516d\u4e2a\u5224\u636e\uff08\u552f\u4e00\u503c\u3001\u4ea4\u96c6\u5927\u5c0f\u3001\u8fde\u63a5\u5927\u5c0f\u3001\u53cd\u5411\u8fde\u63a5\u5927\u5c0f\u3001\u503c\u8bed\u4e49\u3001\u5143\u6570\u636e\u8bed\u4e49\uff09\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u6392\u5e8f\u878d\u5408\u591a\u5224\u636e\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u5143\u6570\u636e\u4e0e\u503c\u8bed\u4e49\u5728\u6570\u636e\u6e56\u4e2d\u6700\u91cd\u8981\uff1b\u5927\u5c0f\u76f8\u5173\u5224\u636e\u5728\u5173\u7cfb\u578b\u6570\u636e\u5e93\u4e2d\u66f4\u6709\u533a\u5206\u529b\uff1b\u4e0d\u540c\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u5728\u8bed\u4e49\u8fde\u63a5\u4e0a\u8868\u73b0\u6709\u5dee\u5f02\uff1b\u96c6\u6210\u6392\u5e8f\u901a\u5e38\u4f18\u4e8e\u5355\u4e00\u5224\u636e\u3002", "conclusion": "\u4e0d\u540c\u6570\u636e\u60c5\u5883\u4e0b\uff0c\u5404\u5224\u636e\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1a\u6570\u636e\u6e56\u4f9d\u8d56\u5143\u6570\u636e\u4e0e\u8bed\u4e49\u4fe1\u606f\uff0c\u5173\u7cfb\u578b\u6570\u636e\u5e93\u66f4\u53d7\u5927\u5c0f\u7c7b\u5224\u636e\u5f71\u54cd\uff1b\u5c06\u591a\u79cd\u5224\u636e\u96c6\u6210\u53ef\u663e\u8457\u63d0\u5347\u53d1\u73b0\u6548\u679c\u3002\u7814\u7a76\u4e3a\u65b9\u6cd5\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2510.23775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2510.23775", "abs": "https://arxiv.org/abs/2510.23775", "authors": ["Aryan Mathur", "Asaduddin Ahmed", "Pushti Amit Vasoya", "Simeon Kandan Sonar", "Yasir Z", "Madesh Kuppusamy"], "title": "Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices", "comment": null, "summary": "The increasing realism of AI-generated imagery poses challenges for verifying\nvisual authenticity. We present an explainable image authenticity detection\nsystem that combines a lightweight convolutional classifier\n(\"Faster-Than-Lies\") with a Vision-Language Model (Qwen2-VL-7B) to classify,\nlocalize, and explain artifacts in 32x32 images. Our model achieves 96.5%\naccuracy on the extended CiFAKE dataset augmented with adversarial\nperturbations and maintains an inference time of 175ms on 8-core CPUs, enabling\ndeployment on local or edge devices. Using autoencoder-based reconstruction\nerror maps, we generate artifact localization heatmaps, which enhance\ninterpretability for both humans and the VLM. We further categorize 70 visual\nartifact types into eight semantic groups and demonstrate explainable text\ngeneration for each detected anomaly. This work highlights the feasibility of\ncombining visual and linguistic reasoning for interpretable authenticity\ndetection in low-resolution imagery and outlines potential cross-domain\napplications in forensics, industrial inspection, and social media moderation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u5c06\u8f7b\u91cf\u5377\u79ef\u7f51\u7edc\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u7684\u53ef\u89e3\u91ca\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u9488\u5bf932\u00d732\u56fe\u50cf\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0896.5%\uff09\u4e0e\u8fb9\u7f18\u8bbe\u5907\u5b9e\u65f6\u63a8\u7406\uff08175ms\uff09\uff0c\u5e76\u63d0\u4f9b\u4f2a\u5f71\u5b9a\u4f4d\u4e0e\u6587\u672c\u89e3\u91ca\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u56fe\u50cf\u7684\u771f\u5b9e\u6027\u9a8c\u8bc1\u96be\u9898\uff0c\u63d0\u4f9b\u65e2\u9ad8\u6548\u53c8\u53ef\u89e3\u91ca\u7684\u68c0\u6d4b\u7cfb\u7edf\uff0c\u9002\u7528\u4e8e\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u5377\u79ef\u5206\u7c7b\u5668\u201cFaster-Than-Lies\u201d\u8fdb\u884c\u5feb\u901f\u5206\u7c7b\uff1b\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u91cd\u5efa\u8bef\u5dee\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u56fe\uff1b\u8c03\u7528Qwen2-VL-7B\u5bf9\u70ed\u56fe\u4e0e\u56fe\u50cf\u8054\u5408\u8f93\u5165\u4ee5\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff1b\u5728\u6269\u5c55CiFAKE\u6570\u636e\u96c6\uff08\u542b\u5bf9\u6297\u6270\u52a8\uff09\u4e0a\u8bad\u7ec3\u4e0e\u8bc4\u4ef7\uff0c\u5e76\u57288\u6838CPU\u4e0a\u4f18\u5316\u63a8\u7406\u65f6\u95f4\u81f3175ms\u3002", "result": "\u63d0\u51fa\u7ed3\u5408\u8f7b\u91cf\u5377\u79ef\u5206\u7c7b\u5668\u201cFaster-Than-Lies\u201d\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578bQwen2-VL-7B\u7684\u53ef\u89e3\u91ca\u56fe\u50cf\u771f\u5b9e\u6027\u68c0\u6d4b\u7cfb\u7edf\uff0c\u572832\u00d732\u56fe\u50cf\u4e0a\u5b9e\u73b0\u5206\u7c7b\u3001\u5b9a\u4f4d\u4e0e\u89e3\u91ca\uff1b\u5728\u6269\u5c55\u7684CiFAKE\u6570\u636e\u96c6\uff08\u542b\u5bf9\u6297\u6270\u52a8\uff09\u4e0a\u8fbe\u621096.5%\u51c6\u786e\u7387\uff0cCPU\u63a8\u7406175ms\uff1b\u5229\u7528\u81ea\u7f16\u7801\u5668\u91cd\u5efa\u8bef\u5dee\u56fe\u751f\u6210\u4f2a\u5f71\u5b9a\u4f4d\u70ed\u56fe\uff0c\u5e76\u5c0670\u79cd\u89c6\u89c9\u4f2a\u5f71\u5f52\u7eb3\u4e3a8\u7c7b\u8bed\u4e49\u7ec4\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u6587\u672c\u8bf4\u660e\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u4e0e\u8bed\u8a00\u63a8\u7406\u53ef\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u771f\u5b9e\u6027\u68c0\u6d4b\uff0c\u5177\u6709\u53ef\u90e8\u7f72\u6027\u5e76\u80fd\u6269\u5c55\u5230\u53d6\u8bc1\u3001\u5de5\u4e1a\u68c0\u6d4b\u548c\u793e\u4ea4\u5e73\u53f0\u5ba1\u6838\u7b49\u9886\u57df\uff0c\u4f46\u9700\u8981\u6ce8\u610f\u5bf9\u6297\u9c81\u68d2\u6027\u3001\u6570\u636e\u504f\u5dee\u4e0e\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u7684\u5c40\u9650\u3002"}}
{"id": "2510.23785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23785", "abs": "https://arxiv.org/abs/2510.23785", "authors": ["Md Tanvir Hossain", "Akif Islam", "Mohd Ruhul Ameen"], "title": "CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting", "comment": "6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International\n  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE\n  2025)", "summary": "Humans can effortlessly count diverse objects by perceiving visual repetition\nand structural relationships rather than relying on class identity. However,\nmost existing counting models fail to replicate this ability; they often\nmiscount when objects exhibit complex shapes, internal symmetry, or overlapping\ncomponents. In this work, we introduce CountFormer, a transformer-based\nframework that learns to recognize repetition and structural coherence for\nclass-agnostic object counting. Built upon the CounTR architecture, our model\nreplaces its visual encoder with the self-supervised foundation model DINOv2,\nwhich produces richer and spatially consistent feature representations. We\nfurther incorporate positional embedding fusion to preserve geometric\nrelationships before decoding these features into density maps through a\nlightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model\nachieves performance comparable to current state-of-the-art methods while\ndemonstrating superior accuracy on structurally intricate or densely packed\nscenes. Our findings indicate that integrating foundation models such as DINOv2\nenables counting systems to approach human-like structural perception,\nadvancing toward a truly general and exemplar-free counting paradigm.", "AI": {"tldr": "CountFormer replaces CounTR encoder with DINOv2 and fuses positional embeddings, decoding to density maps, improving structural and dense-counting performance and approaching exemplar-free general counting", "motivation": "Enable class-agnostic counting by learning repetition and structural coherence; address failures on complex shapes, symmetry, overlap by richer features from foundation model", "method": "Transformer-based enhancement of CounTR using DINOv2 and positional fusion", "result": "Comparable to SOTA on FSC-147, superior on structurally intricate or densely packed scenes; density-map decoding via conv decoder; positional embedding preserves geometry", "conclusion": "Integrating self-supervised foundation models (DINOv2) and positional fusion in transformer-based counters enhances structural perception and counting accuracy, moving toward general class-agnostic counting"}}
{"id": "2510.23798", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23798", "abs": "https://arxiv.org/abs/2510.23798", "authors": ["Gauthier Grimmer", "Romain Wenger", "Cl\u00e9ment Flint", "Germain Forestier", "Gilles Rixhon", "Valentin Chardon"], "title": "A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras", "comment": null, "summary": "The proliferation of floating anthropogenic debris in rivers has emerged as a\npressing environmental concern, exerting a detrimental influence on\nbiodiversity, water quality, and human activities such as navigation and\nrecreation. The present study proposes a novel methodological framework for the\nmonitoring the aforementioned waste, utilising fixed, in-situ cameras. This\nstudy provides two key contributions: (i) the continuous quantification and\nmonitoring of floating debris using deep learning and (ii) the identification\nof the most suitable deep learning model in terms of accuracy and inference\nspeed under complex environmental conditions. These models are tested in a\nrange of environmental conditions and learning configurations, including\nexperiments on biases related to data leakage. Furthermore, a geometric model\nis implemented to estimate the actual size of detected objects from a 2D image.\nThis model takes advantage of both intrinsic and extrinsic characteristics of\nthe camera. The findings of this study underscore the significance of the\ndataset constitution protocol, particularly with respect to the integration of\nnegative images and the consideration of temporal leakage. In conclusion, the\nfeasibility of metric object estimation using projective geometry coupled with\nregression corrections is demonstrated. This approach paves the way for the\ndevelopment of robust, low-cost, automated monitoring systems for urban aquatic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fa\u5b9a\u6c34\u9762\u6444\u50cf\u5934\u7684\u81ea\u52a8\u76d1\u6d4b\u6846\u67b6\uff1a\u7528\u6df1\u5ea6\u5b66\u4e60\u8fde\u7eed\u68c0\u6d4b\u6f02\u6d6e\u5783\u573e\uff0c\u5e76\u901a\u8fc7\u76f8\u673a\u51e0\u4f55\u6a21\u578b\u4f30\u7b97\u7269\u4f53\u771f\u5b9e\u5c3a\u5bf8\uff1b\u5f3a\u8c03\u6570\u636e\u96c6\u6784\u5efa\u53ca\u907f\u514d\u65f6\u95f4/\u6cc4\u6f0f\u504f\u5dee\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6cb3\u6d41\u4e2d\u6f02\u6d6e\u7684\u4eba\u5de5\u5783\u573e\u5bf9\u751f\u6001\u3001\u822a\u884c\u4e0e\u6e38\u61a9\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u73b0\u6709\u76d1\u6d4b\u65b9\u6cd5\u6210\u672c\u9ad8\u6216\u8986\u76d6\u4e0d\u8db3\uff0c\u6545\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u81ea\u52a8\u5316\u4e14\u53ef\u8fde\u7eed\u91cf\u5316\u7684\u76d1\u6d4b\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u76ee\u6807\u68c0\u6d4b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bf9\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u6f02\u6d6e\u5783\u573e\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\uff0c\u68c0\u9a8c\u4e0d\u540c\u5b66\u4e60\u914d\u7f6e\u53ca\u65f6\u95f4\u6cc4\u6f0f\u7684\u5f71\u54cd\uff1b\u5e76\u57fa\u4e8e\u76f8\u673a\u5185\u53c2\u4e0e\u5916\u53c2\u5efa\u7acb\u6295\u5f71\u51e0\u4f55\u6a21\u578b\uff0c\u518d\u7528\u56de\u5f52\u4fee\u6b63\u63d0\u5347\u5c3a\u5bf8\u4f30\u8ba1\u7cbe\u5ea6\u3002", "result": "This paper presents a practical framework for monitoring floating anthropogenic debris in rivers using fixed in-situ cameras, combining deep learning detection with geometric size estimation.", "conclusion": "\u57fa\u4e8e\u56fa\u5b9a\u6444\u50cf\u5934\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u7ed3\u5408\u6295\u5f71\u51e0\u4f55\u548c\u56de\u5f52\u4fee\u6b63\u80fd\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u7a33\u5065\u7684\u6cb3\u9053\u6f02\u6d6e\u5783\u573e\u81ea\u52a8\u76d1\u6d4b\uff0c\u4f46\u6570\u636e\u96c6\u8d1f\u6837\u672c\u3001\u65f6\u95f4\u6cc4\u6f0f\u8981\u8c28\u614e\u5904\u7406\u3002"}}
{"id": "2510.23816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23816", "abs": "https://arxiv.org/abs/2510.23816", "authors": ["Forouzan Fallah", "Wenwen Li", "Chia-Yu Hsu", "Hyunho Lee", "Yezhou Yang"], "title": "RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features", "comment": null, "summary": "Super-resolution (SR) for remote sensing imagery often fails under\nout-of-distribution (OOD) conditions, such as rare geomorphic features captured\nby diverse sensors, producing visually plausible but physically inaccurate\nresults. We present RareFlow, a physics-aware SR framework designed for OOD\nrobustness. RareFlow's core is a dual-conditioning architecture. A Gated\nControlNet preserves fine-grained geometric fidelity from the low-resolution\ninput, while textual prompts provide semantic guidance for synthesizing complex\nfeatures. To ensure physically sound outputs, we introduce a multifaceted loss\nfunction that enforces both spectral and radiometric consistency with sensor\nproperties. Furthermore, the framework quantifies its own predictive\nuncertainty by employing a stochastic forward pass approach; the resulting\noutput variance directly identifies unfamiliar inputs, mitigating feature\nhallucination. We validate RareFlow on a new, curated benchmark of multi-sensor\nsatellite imagery. In blind evaluations, geophysical experts rated our model's\noutputs as approaching the fidelity of ground truth imagery, significantly\noutperforming state-of-the-art baselines. This qualitative superiority is\ncorroborated by quantitative gains in perceptual metrics, including a nearly\n40\\% reduction in FID. RareFlow provides a robust framework for high-fidelity\nsynthesis in data-scarce scientific domains and offers a new paradigm for\ncontrolled generation under severe domain shift.", "AI": {"tldr": "\u63d0\u51faRareFlow\uff1a\u7ed3\u5408Gated ControlNet\u3001\u6587\u672c\u63d0\u793a\u4e0e\u7269\u7406\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5e76\u7528\u968f\u673a\u524d\u5411\u4f20\u64ad\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u63d0\u9ad8\u9065\u611fSR\u5728\u5206\u5e03\u5916\u573a\u666f\u7684\u771f\u5b9e\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u9065\u611fSR\u65b9\u6cd5\u5728\u5206\u5e03\u5916\uff08\u5982\u7f55\u89c1\u5730\u8c8c\u548c\u591a\u4f20\u611f\u5668\u5dee\u5f02\uff09\u4e0b\u5e38\u4ea7\u751f\u89c6\u89c9\u4e0a\u5408\u7406\u4f46\u7269\u7406\u4e0a\u4e0d\u51c6\u786e\u7684\u7ed3\u679c\uff0c\u9700\u4e00\u79cd\u80fd\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u5e76\u80fd\u8bc6\u522b\u4e0d\u719f\u6089\u8f93\u5165\u7684\u9c81\u68d2SR\u65b9\u6cd5\u3002", "method": "\u53cc\u6761\u4ef6\u7f51\u7edc\u67b6\u6784\uff08Gated ControlNet + \u6587\u672c\u63d0\u793a\uff09\u7ed3\u5408\u591a\u5206\u91cf\u7269\u7406\u4e00\u81f4\u6027\u635f\u5931\uff08\u5149\u8c31\u3001\u8f90\u5c04\u4e0e\u4f20\u611f\u5668\u7279\u6027\u7ea6\u675f\uff09\uff0c\u5e76\u91c7\u7528\u968f\u673a\u524d\u5411\u4f20\u64ad\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u591a\u4f20\u611f\u5668\u536b\u661f\u5f71\u50cf\u57fa\u51c6\u4e0a\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002", "result": "RareFlow \u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u9065\u611f\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u7684\u7269\u7406\u611f\u77e5\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u6838\u5fc3\u4e3a\u53cc\u6761\u4ef6\u67b6\u6784\uff1aGated ControlNet \u4fdd\u6301\u4f4e\u5206\u8fa8\u7387\u8f93\u5165\u7684\u51e0\u4f55\u7ec6\u8282\uff0c\u6587\u672c\u63d0\u793a\u63d0\u4f9b\u8bed\u4e49\u6307\u5bfc\uff1b\u590d\u5408\u635f\u5931\u51fd\u6570\u7ea6\u675f\u5149\u8c31\u4e0e\u8f90\u5c04\u4e00\u81f4\u6027\u4ee5\u7b26\u5408\u4f20\u611f\u5668\u7279\u6027\uff1b\u4f7f\u7528\u968f\u673a\u524d\u5411\u4f20\u64ad\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4ee5\u68c0\u6d4b\u4e0d\u719f\u6089\u6837\u672c\u5e76\u51cf\u5c11\u865a\u5047\u7279\u5f81\u3002\u5b9e\u9a8c\u5728\u591a\u4f20\u611f\u5668\u536b\u661f\u5f71\u50cf\u57fa\u51c6\u4e0a\u663e\u793a\uff0c\u5728\u4e13\u5bb6\u76f2\u8bc4\u4e2d\u63a5\u8fd1\u771f\u5b9e\u5f71\u50cf\u7684\u4fdd\u771f\u5ea6\uff0cFID \u7b49\u611f\u77e5\u6307\u6807\u6709\u660e\u663e\u63d0\u5347\uff08FID \u7ea6\u4e0b\u964d40%\uff09\u3002", "conclusion": "RareFlow \u5728\u5904\u7406\u4f20\u611f\u5668\u591a\u6837\u6027\u548c\u7a00\u6709\u5730\u8c8c\u65f6\u80fd\u751f\u6210\u66f4\u7269\u7406\u4e00\u81f4\u4e14\u66f4\u5c11\u865a\u5047\u7684\u8d85\u5206\u7ed3\u679c\uff0c\u4e14\u53ef\u901a\u8fc7\u8f93\u51fa\u65b9\u5dee\u8bc6\u522b\u4e0d\u719f\u6089\u8f93\u5165\uff0c\u9002\u7528\u4e8e\u6570\u636e\u7a00\u7f3a\u7684\u79d1\u5b66\u573a\u666f\u3002"}}
{"id": "2510.23880", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.23880", "abs": "https://arxiv.org/abs/2510.23880", "authors": ["Hanke Chen", "Yuan Liu", "Minchen Li"], "title": "TRELLISWorld: Training-Free World Generation from Object Generators", "comment": null, "summary": "Text-driven 3D scene generation holds promise for a wide range of\napplications, from virtual prototyping to AR/VR and simulation. However,\nexisting methods are often constrained to single-object generation, require\ndomain-specific training, or lack support for full 360-degree viewability. In\nthis work, we present a training-free approach to 3D scene synthesis by\nrepurposing general-purpose text-to-3D object diffusion models as modular tile\ngenerators. We reformulate scene generation as a multi-tile denoising problem,\nwhere overlapping 3D regions are independently generated and seamlessly blended\nvia weighted averaging. This enables scalable synthesis of large, coherent\nscenes while preserving local semantic control. Our method eliminates the need\nfor scene-level datasets or retraining, relies on minimal heuristics, and\ninherits the generalization capabilities of object-level priors. We demonstrate\nthat our approach supports diverse scene layouts, efficient generation, and\nflexible editing, establishing a simple yet powerful foundation for\ngeneral-purpose, language-driven 3D scene construction.", "AI": {"tldr": "\u65e0\u9700\u91cd\u8bad\u7ec3\uff0c\u5229\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u74e6\u7247\u751f\u6210\u5668\uff0c\u91cd\u53e0\u533a\u57df\u72ec\u7acb\u751f\u6210\u5e76\u52a0\u6743\u5e73\u5747\u878d\u5408\uff0c\u652f\u6301\u5927\u5c3a\u5ea6\u3001\u53ef\u7f16\u8f91\u3001\u8bed\u4e49\u53ef\u63a7\u7684360\u00b0\u6587\u672c\u9a71\u52a83D\u573a\u666f\u5408\u6210\u3002", "motivation": "\u8bad\u7ec3-free\u3001\u591a\u7269\u4f53\u3001\u5168\u666f\u53ef\u89c6\u76843D\u573a\u666f\u751f\u6210\u9700\u6c42\uff1b\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u7269\u4f53\u3001\u9700\u7279\u5b9a\u8bad\u7ec3\u6216\u65e0\u6cd5360\u5ea6\u67e5\u770b\u3002", "method": "\u628a\u573a\u666f\u5206\u5272\u4e3a\u91cd\u53e0\u76843D\u74e6\u7247\uff0c\u4f7f\u7528\u5df2\u8bad\u7ec3\u7684\u6587\u672c\u52303D\u5bf9\u8c61\u6269\u6563\u6a21\u578b\u72ec\u7acb\u5bf9\u6bcf\u4e2a\u74e6\u7247\u8fdb\u884c\u91c7\u6837\u751f\u6210\uff0c\u6700\u540e\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u5bf9\u91cd\u53e0\u533a\u57df\u7684\u4f53\u7d20/\u7279\u5f81\u8fdb\u884c\u878d\u5408\uff0c\u91c7\u7528\u6700\u5c0f\u542f\u53d1\u5f0f\u7b56\u7565\u6765\u4fdd\u8bc1\u8bed\u4e49\u4e00\u81f4\u6027\u5e76\u652f\u6301\u5c40\u90e8\u7f16\u8f91\u4e0e\u53ef\u6269\u5c55\u5e03\u5c40\u3002", "result": "\u63d0\u51fa\u4e00\u79cd\u65e0\u8bad\u7ec3\u76843D\u573a\u666f\u5408\u6210\u65b9\u6cd5\uff1a\u628a\u901a\u7528\u6587\u672c\u52303D\u5bf9\u8c61\u7684\u6269\u6563\u6a21\u578b\u5f53\u4f5c\u6a21\u5757\u5316\u74e6\u7247\u751f\u6210\u5668\uff0c\u5c06\u573a\u666f\u751f\u6210\u6539\u5199\u4e3a\u591a\u74e6\u7247\u53bb\u566a\u95ee\u9898\uff0c\u72ec\u7acb\u751f\u6210\u91cd\u53e03D\u533a\u57df\u5e76\u901a\u8fc7\u52a0\u6743\u5e73\u5747\u65e0\u7f1d\u6df7\u5408\uff0c\u5b9e\u73b0\u5927\u5c3a\u5ea6\u4e00\u81f4\u573a\u666f\u7684\u53ef\u6269\u5c55\u5408\u6210\u3001\u4fdd\u7559\u5c40\u90e8\u8bed\u4e49\u63a7\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u57fa\u7840\uff0c\u7528\u4e8e\u901a\u7528\u8bed\u8a00\u9a71\u52a8\u76843D\u573a\u666f\u6784\u5efa\uff1a\u65e0\u9700\u573a\u666f\u7ea7\u6570\u636e\u6216\u91cd\u8bad\uff0c\u517c\u5177\u591a\u6837\u5e03\u5c40\u3001\u9ad8\u6548\u751f\u6210\u548c\u7075\u6d3b\u7f16\u8f91\u80fd\u529b\uff0c\u4f46\u53ef\u80fd\u53d7\u9650\u4e8e\u5bf9\u8c61\u7ea7\u5148\u9a8c\u7684\u8868\u5f81\u80fd\u529b\u548c\u74e6\u7247\u6df7\u5408\u7684\u7ec6\u8282\u4e00\u81f4\u6027\u3002"}}
{"id": "2510.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23894", "abs": "https://arxiv.org/abs/2510.23894", "authors": ["Jinxin Zhou", "Jiachen Jiang", "Zhihui Zhu"], "title": "Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation", "comment": "23 pages, 10 figures, 14 tables", "summary": "Extending CLIP models to semantic segmentation remains challenging due to the\nmisalignment between their image-level pre-training objectives and the\npixel-level visual understanding required for dense prediction. While prior\nefforts have achieved encouraging results by reorganizing the final layer and\nfeatures, they often inherit the global alignment bias of preceding layers,\nleading to suboptimal segmentation performance. In this work, we propose\nLHT-CLIP, a novel training-free framework that systematically exploits the\nvisual discriminability of CLIP across layer, head, and token levels. Through\ncomprehensive analysis, we reveal three key insights: (i) the final layers\nprimarily strengthen image-text alignment with sacrifice of visual\ndiscriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),\npartly due to the emergence of anomalous tokens; (ii) a subset of attention\nheads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual\ndiscriminability across datasets; (iii) abnormal tokens display sparse and\nconsistent activation pattern compared to normal tokens. Based on these\nfindings, we propose three complementary techniques: semantic-spatial\nreweighting, selective head enhancement, and abnormal token replacement to\neffectively restore visual discriminability and improve segmentation\nperformance without any additional training, auxiliary pre-trained networks, or\nextensive hyperparameter tuning. Extensive experiments on 8 common semantic\nsegmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art\nperformance across diverse scenarios, highlighting its effectiveness and\npracticality for real-world deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faLHT-CLIP\uff0c\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5c42\u3001\u5934\u3001\u6807\u8bb0\u4e09\u4e2a\u5c42\u9762\u6062\u590dCLIP\u7684\u89c6\u89c9\u53ef\u533a\u5206\u6027\u4ee5\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6027\u80fd\u3002\u4e3b\u8981\u53d1\u73b0\uff1a\u672b\u5c42\u5f3a\u5316\u56fe\u6587\u5bf9\u9f50\u4f46\u727a\u7272\u89c6\u89c9\u53ef\u533a\u5206\u6027\uff1b\u90e8\u5206\u6ce8\u610f\u529b\u5934\u957f\u671f\u4fdd\u7559\u53ef\u533a\u5206\u6027\uff1b\u5f02\u5e38\u6807\u8bb0\u6709\u7a00\u758f\u4e00\u81f4\u7684\u6fc0\u6d3b\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u8bed\u4e49-\u7a7a\u95f4\u91cd\u52a0\u6743\u3001\u9009\u62e9\u6027\u5934\u589e\u5f3a\u548c\u5f02\u5e38\u6807\u8bb0\u66ff\u6362\u4e09\u9879\u6280\u672f\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u6216\u989d\u5916\u7f51\u7edc\u5373\u53ef\u5728\u591a\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3CLIP\u4ece\u56fe\u50cf\u7ea7\u9884\u8bad\u7ec3\u5230\u50cf\u7d20\u7ea7\u5206\u5272\u7684\u9519\u914d\u95ee\u9898\uff1a\u6a21\u578b\u540e\u5c42\u504f\u5411\u56fe\u6587\u5168\u5c40\u5bf9\u9f50\uff0c\u964d\u4f4e\u5bf9\u5c40\u90e8\u50cf\u7d20\u5dee\u5f02\u7684\u654f\u611f\u6027\uff0c\u4ece\u800c\u5f71\u54cd\u5bc6\u96c6\u9884\u6d4b\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u6216\u5f15\u5165\u65b0\u6a21\u578b\u7684\u524d\u63d0\u4e0b\uff0c\u6316\u6398CLIP\u5185\u90e8\u4ecd\u5b58\u5728\u7684\u89c6\u89c9\u533a\u5206\u4fe1\u53f7\uff0c\u901a\u8fc7\u5c42\u3001\u5934\u3001\u6807\u8bb0\u7ea7\u522b\u7684\u5904\u7406\u6062\u590d\u50cf\u7d20\u7ea7\u5224\u522b\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u6b65\uff1a1) \u8bed\u4e49-\u7a7a\u95f4\u91cd\u52a0\u6743\uff1a\u9488\u5bf9\u4e0d\u540c\u5c42\u548c\u7a7a\u95f4\u4f4d\u7f6e\u8c03\u6574\u7279\u5f81\u6743\u91cd\u4ee5\u5f3a\u5316\u8bed\u4e49\u4e0e\u7a7a\u95f4\u4fe1\u606f\uff1b2) \u9009\u62e9\u6027\u5934\u589e\u5f3a\uff1a\u8bc6\u522b\u5e76\u653e\u5927\u90a3\u4e9b\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u89c6\u89c9\u53ef\u533a\u5206\u6027\u7684\u6ce8\u610f\u529b\u5934\uff1b3) \u5f02\u5e38\u6807\u8bb0\u66ff\u6362\uff1a\u68c0\u6d4b\u5e76\u7528\u6b63\u5e38\u6807\u8bb0\u6216\u63d2\u503c\u66ff\u6362\u6fc0\u6d3b\u7a00\u758f\u4e14\u4e00\u81f4\u7684\u5f02\u5e38\u6807\u8bb0\uff0c\u51cf\u5c11\u566a\u58f0\u5bf9\u5206\u5272\u7684\u5e72\u6270\u3002\u5168\u90e8\u65b9\u6cd5\u5747\u65e0\u8bad\u7ec3\uff0c\u76f4\u63a5\u57fa\u4e8eCLIP\u7279\u5f81\u91cd\u7ec4\u4e0e\u52a0\u6743\u3002", "result": "\u57288\u4e2a\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u4e0a\uff0cLHT-CLIP\u5728\u96f6\u6837\u672c\u3001\u5f31\u76d1\u7763\u548c\u901a\u7528\u5206\u5272\u8bbe\u7f6e\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8fbe\u5230\u6216\u8d85\u8d8a\u5f53\u524dSOTA\uff0c\u5c55\u793a\u51fa\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u3001\u573a\u666f\u4e0e\u7c7b\u96c6\u5408\u4e0b\u7684\u7a33\u5065\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "LHT-CLIP\u80fd\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u6216\u4f9d\u8d56\u8f85\u52a9\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5c42/\u5934/\u6807\u8bb0\u7ea7\u522b\u7684\u91cd\u6784\u6062\u590dCLIP\u7684\u50cf\u7d20\u7ea7\u89c6\u89c9\u5224\u522b\u80fd\u529b\uff0c\u5927\u5e45\u63d0\u5347\u96f6\u6837\u672c\u548c\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u8868\u73b0\uff0c\u5e76\u57288\u4e2a\u5e38\u7528\u57fa\u51c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.23907", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.23907", "abs": "https://arxiv.org/abs/2510.23907", "authors": ["Eddison Pham", "Prisha Priyadarshini", "Adrian Maliackel", "Kanishk Bandi", "Cristian Meo", "Kevin Zhu"], "title": "DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning", "comment": "16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026", "summary": "Scene-level captioning in instructional videos can enhance learning by\nrequiring an understanding of both visual cues and temporal structure. By\naligning visual cues with textual guidance, this understanding supports\nprocedural learning and multimodal reasoning, providing a richer context for\nskill acquisition. However, captions that fail to capture this structure may\nlack coherence and quality, which can create confusion and undermine the\nvideo's educational intent. To address this gap, we introduce DynaStride, a\npipeline to generate coherent, scene-level captions without requiring manual\nscene segmentation. Using the YouCookII dataset's scene annotations, DynaStride\nperforms adaptive frame sampling and multimodal windowing to capture key\ntransitions within each scene. It then employs a multimodal chain-of-thought\nprocess to produce multiple action-object pairs, which are refined and fused\nusing a dynamic stride window selection algorithm that adaptively balances\ntemporal context and redundancy. The final scene-level caption integrates\nvisual semantics and temporal reasoning in a single instructional caption.\nEmpirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,\ndemonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and\nsemantic similarity measures (BERTScore, CLIPScore). Qualitative analyses\nfurther show that DynaStride produces captions that are more temporally\ncoherent and informative, suggesting a promising direction for improving\nAI-powered instructional content generation.", "AI": {"tldr": "DynaStride automatically creates temporally coherent, informative scene-level captions for instructional videos using adaptive sampling and multimodal reasoning, outperforming strong baselines on standard metrics.", "motivation": "Scene-level captioning in instructional videos needs understanding of visual cues and temporal structure to support procedural learning; current captions often lack coherence and quality, harming educational value.", "method": "Adaptive frame sampling, multimodal windowing, multimodal chain-of-thought to produce multiple action-object pairs, refinement and fusion via dynamic stride window selection algorithm balancing temporal context and redundancy; uses YouCookII scene annotations for evaluation.", "result": "DynaStride pipeline generates coherent, scene-level captions without manual scene segmentation by using adaptive frame sampling, multimodal windowing, multimodal chain-of-thought to produce action-object pairs, and dynamic stride window selection to fuse them; improves metrics over VLLaMA3 and GPT-4o.", "conclusion": "DynaStride yields more temporally coherent and informative instructional captions by integrating visual semantics and temporal reasoning via adaptive sampling and dynamic stride fusion."}}
{"id": "2510.23929", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23929", "abs": "https://arxiv.org/abs/2510.23929", "authors": ["Emily Kim", "Julieta Martinez", "Timur Bagautdinov", "Jessica Hodgins"], "title": "TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis", "comment": null, "summary": "We introduce TurboPortrait3D: a method for low-latency novel-view synthesis\nof human portraits. Our approach builds on the observation that existing\nimage-to-3D models for portrait generation, while capable of producing\nrenderable 3D representations, are prone to visual artifacts, often lack of\ndetail, and tend to fail at fully preserving the identity of the subject. On\nthe other hand, image diffusion models excel at generating high-quality images,\nbut besides being computationally expensive, are not grounded in 3D and thus\nare not directly capable of producing multi-view consistent outputs. In this\nwork, we demonstrate that image-space diffusion models can be used to\nsignificantly enhance the quality of existing image-to-avatar methods, while\nmaintaining 3D-awareness and running with low-latency. Our method takes a\nsingle frontal image of a subject as input, and applies a feedforward\nimage-to-avatar generation pipeline to obtain an initial 3D representation and\ncorresponding noisy renders. These noisy renders are then fed to a single-step\ndiffusion model which is conditioned on input image(s), and is specifically\ntrained to refine the renders in a multi-view consistent way. Moreover, we\nintroduce a novel effective training strategy that includes pre-training on a\nlarge corpus of synthetic multi-view data, followed by fine-tuning on\nhigh-quality real images. We demonstrate that our approach both qualitatively\nand quantitatively outperforms current state-of-the-art for portrait novel-view\nsynthesis, while being efficient in time.", "AI": {"tldr": "TurboPortrait3D\uff1a\u57fa\u4e8e\u5355\u6b65\u56fe\u50cf\u6269\u6563\u5bf9\u56fe\u50cf\u52303D\u5934\u50cf\u7ed3\u679c\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7ec6\u5316\uff0c\u91c7\u7528\u5408\u6210\u9884\u8bad\u7ec3+\u771f\u5b9e\u5fae\u8c03\uff0c\u5b9e\u73b0\u5728\u5355\u5f20\u6b63\u9762\u56fe\u8f93\u5165\u4e0b\u7684\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u52303D\u6a21\u578b\u6613\u51fa\u73b0\u4f2a\u5f71\u3001\u7ec6\u8282\u4e0d\u8db3\u4e14\u96be\u4ee5\u4fdd\u6301\u4eba\u7269\u8eab\u4efd\uff1b\u56fe\u50cf\u6269\u6563\u6a21\u578b\u56fe\u50cf\u8d28\u91cf\u9ad8\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u7f3a\u4e4f3D\u4e00\u81f4\u6027\u3002\u7ed3\u5408\u4e24\u8005\u4ee5\u63d0\u5347\u8d28\u91cf\u3001\u4fdd\u75593D\u611f\u5e76\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3002", "method": "\u5148\u4f7f\u7528\u524d\u9988\u7684\u56fe\u50cf\u5230\u5934\u50cf\u65b9\u6cd5\u4ece\u5355\u4e2a\u6b63\u9762\u56fe\u50cf\u751f\u6210\u521d\u59cb3D\u8868\u793a\u53ca\u5bf9\u5e94\u566a\u6e32\u67d3\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u6709\u566a\u6e32\u67d3\u8f93\u5165\u4e00\u4e2a\u5355\u6b65\u6269\u6563\u6a21\u578b\uff08\u6761\u4ef6\u4e8e\u8f93\u5165\u56fe\u50cf\uff09\uff0c\u8be5\u6a21\u578b\u5728\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u7ea6\u675f\u4e0b\u8bad\u7ec3\u4ee5\u751f\u6210\u7cbe\u7ec6\u5316\u6e32\u67d3\u3002\u8bad\u7ec3\u7b56\u7565\u5305\u62ec\u5148\u5728\u5927\u91cf\u5408\u6210\u591a\u89c6\u56fe\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u9ad8\u8d28\u91cf\u771f\u5b9e\u56fe\u50cf\u4e0a\u5fae\u8c03\u3002", "result": "\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u8bc4\u4f30\u4e0a\u5747\u4f18\u4e8e\u5f53\u524d\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u6548\u7387\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51faTurboPortrait3D\uff0c\u901a\u8fc7\u5728\u56fe\u50cf\u52303D\u5934\u50cf\u6d41\u6c34\u7ebf\u540e\u5f15\u5165\u5355\u6b65\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5bf9\u566a\u6e32\u67d3\u8fdb\u884c\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7ec6\u5316\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u8d28\u91cf\u7684\u4eba\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u3002"}}
{"id": "2510.23930", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23930", "abs": "https://arxiv.org/abs/2510.23930", "authors": ["Xirui Jin", "Renbiao Jin", "Boying Li", "Danping Zou", "Wenxian Yu"], "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors", "comment": "Accepted by NeurIPS 2025. Project page: https://planargs.github.io", "summary": "Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an\nefficient representation for novel-view synthesis, achieving impressive visual\nquality. However, in scenes dominated by large and low-texture regions, common\nin indoor environments, the photometric loss used to optimize 3DGS yields\nambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome\nthis limitation, we introduce PlanarGS, a 3DGS-based framework tailored for\nindoor scene reconstruction. Specifically, we design a pipeline for\nLanguage-Prompted Planar Priors (LP3) that employs a pretrained vision-language\nsegmentation model and refines its region proposals via cross-view fusion and\ninspection with geometric priors. 3D Gaussians in our framework are optimized\nwith two additional terms: a planar prior supervision term that enforces planar\nconsistency, and a geometric prior supervision term that steers the Gaussians\ntoward the depth and normal cues. We have conducted extensive experiments on\nstandard indoor benchmarks. The results show that PlanarGS reconstructs\naccurate and detailed 3D surfaces, consistently outperforming state-of-the-art\nmethods by a large margin. Project page: https://planargs.github.io", "AI": {"tldr": "", "motivation": "", "method": "", "result": "", "conclusion": ""}}
{"id": "2510.23943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23943", "abs": "https://arxiv.org/abs/2510.23943", "authors": ["Diana Aldana", "Jo\u00e3o Paulo Lima", "Daniel Csillag", "Daniel Perazzo", "Haoan Feng", "Luiz Velho", "Tiago Novello"], "title": "Adaptive Training of INRs via Pruning and Densification", "comment": null, "summary": "Encoding input coordinates with sinusoidal functions into multilayer\nperceptrons (MLPs) has proven effective for implicit neural representations\n(INRs) of low-dimensional signals, enabling the modeling of high-frequency\ndetails. However, selecting appropriate input frequencies and architectures\nwhile managing parameter redundancy remains an open challenge, often addressed\nthrough heuristics and heavy hyperparameter optimization schemes. In this\npaper, we introduce AIRe ($\\textbf{A}$daptive $\\textbf{I}$mplicit neural\n$\\textbf{Re}$presentation), an adaptive training scheme that refines the INR\narchitecture over the course of optimization. Our method uses a neuron pruning\nmechanism to avoid redundancy and input frequency densification to improve\nrepresentation capacity, leading to an improved trade-off between network size\nand reconstruction quality. For pruning, we first identify less-contributory\nneurons and apply a targeted weight decay to transfer their information to the\nremaining neurons, followed by structured pruning. Next, the densification\nstage adds input frequencies to spectrum regions where the signal underfits,\nexpanding the representational basis. Through experiments on images and SDFs,\nwe show that AIRe reduces model size while preserving, or even improving,\nreconstruction quality. Code and pretrained models will be released for public\nuse.", "AI": {"tldr": "AIRe\u901a\u8fc7\u5faa\u73af\u6267\u884c\u795e\u7ecf\u5143\u526a\u679d\uff08\u4fe1\u606f\u8fc1\u79fb+\u7ed3\u6784\u5316\u526a\u679d\uff09\u548c\u8f93\u5165\u9891\u7387\u5bc6\u96c6\u5316\uff0c\u5b9e\u73b0\u66f4\u5c0f\u4e14\u6027\u80fd\u66f4\u597d\u7684INR\u6a21\u578b\uff0c\u7528\u4e8e\u56fe\u50cf\u4e0eSDF\u91cd\u5efa\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u4e3a\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u9009\u62e9\u5408\u9002\u7684\u8f93\u5165\u9891\u7387\u548c\u67b6\u6784\u65f6\u7684\u6311\u6218\uff0c\u907f\u514d\u4f9d\u8d56\u542f\u53d1\u5f0f\u548c\u5927\u91cf\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u540c\u65f6\u63a7\u5236\u53c2\u6570\u5197\u4f59\u4ee5\u63d0\u5347\u9ad8\u9891\u7ec6\u8282\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u4ea4\u66ff\u9636\u6bb5\uff1a1) \u526a\u679d\u9636\u6bb5\uff1a\u8bc6\u522b\u5bf9\u8f93\u51fa\u8d21\u732e\u5c0f\u7684\u795e\u7ecf\u5143\uff0c\u65bd\u52a0\u6709\u9488\u5bf9\u6027\u7684\u6743\u91cd\u8870\u51cf\u4ee5\u5c06\u5176\u4fe1\u606f\u8fc1\u79fb\u5230\u5269\u4f59\u795e\u7ecf\u5143\uff0c\u7136\u540e\u8fdb\u884c\u7ed3\u6784\u5316\u526a\u679d\uff1b2) \u5bc6\u96c6\u5316\u9636\u6bb5\uff1a\u5728\u4fe1\u53f7\u6b20\u62df\u5408\u7684\u9891\u8c31\u533a\u57df\u589e\u52a0\u8f93\u5165\u7684\u6b63\u5f26\u7f16\u7801\u9891\u7387\uff0c\u4ece\u800c\u6269\u5c55\u8868\u793a\u57fa\u51fd\u6570\u3002", "result": "\u5728\u56fe\u50cf\u548c\u9690\u5f0f\u8868\u9762\uff08SDFs\uff09\u5b9e\u9a8c\u4e2d\uff0cAIRe\u5728\u51cf\u5c0f\u6a21\u578b\u53c2\u6570\u91cf\u7684\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4e86\u91cd\u5efa\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86\u526a\u679d\u4e0e\u9891\u7387\u5bc6\u96c6\u5316\u7684\u4e92\u8865\u6027\u3002", "conclusion": "AIRe\u901a\u8fc7\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u5730\u88c1\u526a\u795e\u7ecf\u5143\u548c\u5728\u8f93\u5165\u9891\u8c31\u4e0a\u5bc6\u96c6\u5316\u7f16\u7801\u9891\u7387\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u6216\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\uff0c\u4ece\u800c\u6539\u5584\u7f51\u7edc\u5927\u5c0f\u4e0e\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2510.23956", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.23956", "abs": "https://arxiv.org/abs/2510.23956", "authors": ["Alejandro Escontrela", "Shrinu Kushagra", "Sjoerd van Steenkiste", "Yulia Rubanova", "Aleksander Holynski", "Kelsey Allen", "Kevin Murphy", "Thomas Kipf"], "title": "Neural USD: An object-centric framework for iterative editing and control", "comment": "22 pages, 16 figures, 1 table", "summary": "Amazing progress has been made in controllable generative modeling,\nespecially over the last few years. However, some challenges remain. One of\nthem is precise and iterative object editing. In many of the current methods,\ntrying to edit the generated image (for example, changing the color of a\nparticular object in the scene or changing the background while keeping other\nelements unchanged) by changing the conditioning signals often leads to\nunintended global changes in the scene. In this work, we take the first steps\nto address the above challenges. Taking inspiration from the Universal Scene\nDescriptor (USD) standard developed in the computer graphics community, we\nintroduce the \"Neural Universal Scene Descriptor\" or Neural USD. In this\nframework, we represent scenes and objects in a structured, hierarchical\nmanner. This accommodates diverse signals, minimizes model-specific\nconstraints, and enables per-object control over appearance, geometry, and\npose. We further apply a fine-tuning approach which ensures that the above\ncontrol signals are disentangled from one another. We evaluate several design\nconsiderations for our framework, demonstrating how Neural USD enables\niterative and incremental workflows. More information at:\nhttps://escontrela.me/neural_usd .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faNeural USD\uff08\u795e\u7ecf\u901a\u7528\u573a\u666f\u63cf\u8ff0\u7b26\uff09\uff0c\u5c06\u573a\u666f\u548c\u5bf9\u8c61\u4ee5\u7ed3\u6784\u5316\u3001\u5c42\u6b21\u5316\u65b9\u5f0f\u8868\u793a\uff0c\u652f\u6301\u5bf9\u5355\u4e2a\u5bf9\u8c61\u7684\u5916\u89c2\u3001\u51e0\u4f55\u4e0e\u59ff\u6001\u8fdb\u884c\u53ef\u63a7\u3001\u9010\u6b65\u7f16\u8f91\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u5b9e\u73b0\u63a7\u5236\u4fe1\u53f7\u7684\u76f8\u4e92\u89e3\u8026\uff0c\u51cf\u5c11\u5168\u5c40\u610f\u5916\u53d8\u5316\u3002", "motivation": "\u5f53\u524d\u53ef\u63a7\u751f\u6210\u6a21\u578b\u5728\u5bf9\u56fe\u50cf\u4e2d\u5355\u4e2a\u5bf9\u8c61\u8fdb\u884c\u7cbe\u786e\u7f16\u8f91\u65f6\u5e38\u5bfc\u81f4\u610f\u5916\u7684\u5168\u5c40\u53d8\u5316\uff0c\u9700\u4e00\u79cd\u652f\u6301\u5bf9\u8c61\u7ea7\u3001\u53ef\u8fed\u4ee3\u7f16\u8f91\u4e14\u4e0e\u6a21\u578b\u65e0\u5f3a\u4f9d\u8d56\u7684\u8868\u5f81\u65b9\u6cd5\u3002", "method": "\u501f\u9274\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u7684USD\u6807\u51c6\uff0c\u6784\u5efa\u5c42\u6b21\u5316\u573a\u666f\u4e0e\u5bf9\u8c61\u8868\u793a\uff1b\u5f15\u5165\u9488\u5bf9\u63a7\u5236\u4fe1\u53f7\u7684\u5fae\u8c03\u7b56\u7565\u4ee5\u5b9e\u73b0\u5916\u89c2\u3001\u51e0\u4f55\u4e0e\u59ff\u6001\u7684\u89e3\u8026\uff1b\u8bc4\u4f30\u591a\u79cd\u8bbe\u8ba1\u9009\u9879\u5e76\u5c55\u793a\u5176\u5728\u9010\u6b65\u7f16\u8f91\u4e0e\u589e\u91cf\u5de5\u4f5c\u6d41\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5728\u82e5\u5e72\u8bbe\u8ba1\u9009\u62e9\u7684\u8bc4\u4f30\u4e2d\uff0cNeural USD \u5c55\u793a\u4e86\u5bf9\u5355\u4e2a\u5bf9\u8c61\u7684\u7cbe\u786e\u7f16\u8f91\u80fd\u529b\u3001\u63a7\u5236\u4fe1\u53f7\u7684\u89e3\u8026\u6548\u679c\u4ee5\u53ca\u652f\u6301\u8fed\u4ee3\uff0f\u589e\u91cf\u7f16\u8f91\u7684\u5b9e\u7528\u6027\u3002\u66f4\u591a\u7ec6\u8282\u89c1\u9879\u76ee\u9875\u3002", "conclusion": "Neural USD \u80fd\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5b9e\u73b0\u7cbe\u786e\u3001\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\u7ea7\u7f16\u8f91\uff0c\u63d0\u4f9b\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\u4e0e\u89e3\u8026\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u4ece\u800c\u652f\u6301\u589e\u91cf\u5316\u5de5\u4f5c\u6d41\u5e76\u51cf\u5c11\u5bf9\u6a21\u578b\u7684\u7279\u6b8a\u9650\u5236\u3002"}}
{"id": "2510.23960", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2510.23960", "abs": "https://arxiv.org/abs/2510.23960", "authors": ["Peiyang Xu", "Minzhou Pan", "Zhaorun Chen", "Shuang Yang", "Chaowei Xiao", "Bo Li"], "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability", "comment": "42 pages, 9 figures", "summary": "With the rapid proliferation of digital media, the need for efficient and\ntransparent safeguards against unsafe content is more critical than ever.\nTraditional image guardrail models, constrained by predefined categories, often\nmisclassify content due to their pure feature-based learning without semantic\nreasoning. Moreover, these models struggle to adapt to emerging threats,\nrequiring costly retraining for new threats. To address these limitations, we\nintroduce SafeVision, a novel image guardrail that integrates human-like\nreasoning to enhance adaptability and transparency. Our approach incorporates\nan effective data collection and generation framework, a policy-following\ntraining pipeline, and a customized loss function. We also propose a diverse QA\ngeneration and training strategy to enhance learning effectiveness. SafeVision\ndynamically aligns with evolving safety policies at inference time, eliminating\nthe need for retraining while ensuring precise risk assessments and\nexplanations. Recognizing the limitations of existing unsafe image benchmarks,\nwhich either lack granularity or cover limited risks, we introduce VisionHarm,\na high-quality dataset comprising two subsets: VisionHarm Third-party\n(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse\nharmful categories. Through extensive experiments, we show that SafeVision\nachieves state-of-the-art performance on different benchmarks. SafeVision\noutperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while\nbeing over 16x faster. SafeVision sets a comprehensive, policy-following, and\nexplainable image guardrail with dynamic adaptation to emerging threats.", "AI": {"tldr": "\u63d0\u51faSafeVision\uff0c\u4e00\u79cd\u7ed3\u5408\u7c7b\u4eba\u63a8\u7406\u7684\u56fe\u50cf\u5b89\u5168\u9632\u62a4\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u6536\u96c6/\u751f\u6210\u3001\u7b56\u7565\u9075\u5faa\u8bad\u7ec3\u7ba1\u9053\u548c\u5b9a\u5236\u635f\u5931\uff0c\u652f\u6301\u63a8\u7406\u65f6\u52a8\u6001\u5bf9\u9f50\u7b56\u7565\uff0c\u65e0\u9700\u91cd\u8bad\u7ec3\u3002\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6VisionHarm\uff08T\u548cC\u5b50\u96c6\uff09\u3002\u5b9e\u9a8c\u663e\u793a\u5728VisionHarm\u4e0a\u8d85\u8d8aGPT-4o\u4e14\u901f\u5ea6\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf\u9632\u62a4\u6a21\u578b\u4f9d\u8d56\u9884\u5b9a\u4e49\u7c7b\u522b\u4e0e\u7eaf\u7279\u5f81\u5b66\u4e60\uff0c\u7f3a\u4e4f\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e14\u96be\u4ee5\u9002\u5e94\u65b0\u5174\u5a01\u80c1\uff0c\u9700\u8981\u6602\u8d35\u7684\u91cd\u8bad\u7ec3\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u53ef\u89e3\u91ca\u3001\u53ef\u52a8\u6001\u66f4\u65b0\u4e14\u5177\u7c7b\u4eba\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u6570\u636e\u6536\u96c6\u4e0e\u751f\u6210\u6846\u67b6\u3001\u7b56\u7565\u9075\u5faa\u8bad\u7ec3\u7ba1\u9053\u3001\u5b9a\u5236\u635f\u5931\u51fd\u6570\u4ee5\u53ca\u591a\u6837\u5316\u95ee\u7b54\u751f\u6210\u4e0e\u8bad\u7ec3\u7b56\u7565\uff1b\u6a21\u578b\u5728\u63a8\u7406\u9636\u6bb5\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u673a\u5236\u9002\u914d\u4e0d\u65ad\u6f14\u8fdb\u7684\u5b89\u5168\u7b56\u7565\uff0c\u514d\u53bb\u91cd\u8bad\u7ec3\u3002", "result": "\u5728VisionHarm-T\u4e0a\u6bd4GPT-4o\u9ad8\u51fa8.6%\uff0c\u5728VisionHarm-C\u4e0a\u9ad8\u51fa15.5%\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u8d85\u8fc7\u5bf9\u624b16\u500d\u4ee5\u4e0a\uff1b\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u8868\u73b0\u3002", "conclusion": "SafeVision\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u52a8\u6001\u9002\u914d\u7684\u56fe\u50cf\u5b89\u5168\u9632\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u9886\u5148\u6027\u80fd\u5e76\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u56fe\u50cf\u9632\u62a4\u6a21\u578b\u7684\u6cdb\u5316\u548c\u7b56\u7565\u66f4\u65b0\u95ee\u9898\u3002"}}
{"id": "2510.23968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23968", "abs": "https://arxiv.org/abs/2510.23968", "authors": ["Andriy Myronenko", "Dong Yang", "Baris Turkbey", "Mariam Aboian", "Sena Azamat", "Esra Akcicek", "Hongxu Yin", "Pavlo Molchanov", "Marc Edgar", "Yufan He", "Pengfei Guo", "Yucheng Tang", "Daguang Xu"], "title": "Reasoning Visual Language Model for Chest X-Ray Analysis", "comment": "NV-Reason-CXR-3B", "summary": "Vision-language models (VLMs) have shown strong promise for medical image\nanalysis, but most remain opaque, offering predictions without the transparent,\nstepwise reasoning clinicians rely on. We present a framework that brings\nchain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by\nreasoning-first training paradigms, our approach is designed to learn how\nexperts reason, not just what they conclude, by aligning intermediate steps\nwith observable image evidence and radiology workflow. Beyond accuracy, the\nexplicit reasoning traces support clinical auditability: they reveal why a\nconclusion was reached, which alternatives were considered, and where\nuncertainty remains, enabling quality assurance, error analysis, and safer\nhuman-AI collaboration.\n  Our model couples high-fidelity visual encoding with a two-stage training\nrecipe: a reasoning-style supervised fine-tuning (SFT) followed by\nreinforcement learning (RL) that uses verifiable rewards over a list of X-ray\nabnormalities. The model outputs reasoning that mirrors radiologists systematic\nthought process, uncertainty, and differential diagnosis. In\nout-of-distribution evaluation, the approach achieves competitive multi-label\nclassification while improving interpretability. In a reader study with expert\nradiologists, full reasoning traces increased confidence, supported error\nauditing, and reduced time to finalize reports. We release code and the model\nNV-Reason-CXR-3B to support community progress toward trustworthy, explainable\nAI in chest radiography and other medical imaging tasks where reasoning quality\nis as critical as prediction quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06Chain-of-Thought\u63a8\u7406\u5f15\u5165\u80f8\u7247\u89e3\u8bfb\u7684VLM\u6846\u67b6\uff0c\u901a\u8fc7SFT+RL\u8bad\u7ec3\u4f7f\u6a21\u578b\u751f\u6210\u53ef\u9a8c\u8bc1\u3001\u7b26\u5408\u653e\u5c04\u79d1\u5de5\u4f5c\u6d41\u7a0b\u7684\u5206\u6b65\u63a8\u7406\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u5e76\u4fdd\u6301\u5206\u7c7b\u6027\u80fd\uff1b\u53d1\u5e03\u6a21\u578bNV-Reason-CXR-3B\u3002", "motivation": "\u5f53\u524dVLM\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u591a\u4e3a\u201c\u9ed1\u76d2\u201d\u8f93\u51fa\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4e34\u5e8a\u5bf9\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u5206\u6b65\u63a8\u7406\u7684\u9700\u6c42\uff1b\u5e0c\u671b\u5b66\u4e60\u653e\u5c04\u79d1\u533b\u751f\u7684\u601d\u7ef4\u8fc7\u7a0b\u4ee5\u652f\u6301\u8d28\u91cf\u4fdd\u8bc1\u548c\u66f4\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\u3002", "method": "\u9ad8\u4fdd\u771f\u89c6\u89c9\u7f16\u7801\u5668 + \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u8fdb\u884c\u7b26\u5408\u63a8\u7406\u98ce\u683c\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u518d\u7528\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5f02\u5e38\u5217\u8868\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\uff1b\u8f93\u51fa\u5305\u542b\u6b65\u9aa4\u5316\u63a8\u7406\u3001\u4e0d\u786e\u5b9a\u6027\u548c\u9274\u522b\u8bca\u65ad\u3002", "result": "The paper introduces a VLM framework that generates chain-of-thought (CoT) style reasoning for chest X-ray interpretation, improving interpretability and maintaining competitive classification performance. It presents a two-stage training: reasoning-style supervised fine-tuning (SFT) and RL with verifiable rewards tied to X-ray abnormalities. The model outputs systematic radiologist-like reasoning, uncertainty, and differential diagnoses, and shows benefits in out-of-distribution tests and a radiologist reader study. NV-Reason-CXR-3B is released.", "conclusion": "\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u3001\u5ba1\u8ba1\u6027\u548c\u4e34\u5e8a\u534f\u4f5c\u4e0a\u5e26\u6765\u660e\u663e\u6539\u8fdb\uff0c\u540c\u65f6\u5728\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8bfb\u8005\u7814\u7a76\u8868\u660e\u5b8c\u6574\u63a8\u7406\u8f68\u8ff9\u63d0\u9ad8\u4e86\u653e\u5c04\u79d1\u533b\u751f\u4fe1\u5fc3\u5e76\u7f29\u77ed\u62a5\u544a\u65f6\u95f4\u3002"}}
{"id": "2510.23978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23978", "abs": "https://arxiv.org/abs/2510.23978", "authors": ["Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints", "comment": "9 pages", "summary": "Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is\ncrucial. Existing methods predict Fourier components one by one using a\nrecurrent neural network. However, this approach leads to performance\ndegradation and inefficiency due to independent prediction. This paper proposes\npredicting multiple components jointly to improve both quality and efficiency.", "AI": {"tldr": "\u9488\u5bf9\u9010\u4e00\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\u7684\u4f4e\u6548\u53ca\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u6765\u63d0\u5347\u8d85\u5206\u8fa8\u7387\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u9010\u4e00\u9884\u6d4b\u9891\u57df\u5206\u91cf\u7684\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u9884\u6d4b\u65f6\u4f1a\u4ea7\u751f\u8bef\u5dee\u7d2f\u79ef\u4e14\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9650\u5236\u4e86\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u7684\u5b9e\u7528\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u80fd\u4fdd\u6301\u6216\u63d0\u5347\u91cd\u5efa\u8d28\u91cf\u7684\u9884\u6d4b\u7b56\u7565\u3002", "method": "\u7528\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u8f93\u51fa\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u7684\u7f51\u7edc\u7ed3\u6784\u66ff\u4ee3\u4f20\u7edf\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u9010\u6b65\u9884\u6d4b\u673a\u5236\uff0c\u53ef\u80fd\u901a\u8fc7\u5e76\u884c\u5206\u652f\u6216\u591a\u5934\u8f93\u51fa\u5b9e\u73b0\u8054\u5408\u9884\u6d4b\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u5f15\u5165\u9002\u5f53\u7684\u635f\u5931\u51fd\u6570\u6765\u7ea6\u675f\u9891\u57df\u91cd\u5efa\u8d28\u91cf\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4efb\u610f\u653e\u5927\u500d\u6570\u8d85\u5206\u8fa8\u7387\u4efb\u52a1\u4e2d\u540c\u65f6\u63a7\u5236\u6210\u672c\u548c\u8d28\u91cf\uff08CQ\uff09\u7684\u6539\u8fdb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u9488\u5bf9\u73b0\u6709\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u9010\u4e00\u9884\u6d4b\u5085\u91cc\u53f6\u5206\u91cf\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u5206\u91cf\u8054\u5408\u9884\u6d4b\u7b56\u7565\uff0c\u4ece\u800c\u5728\u63d0\u9ad8\u91cd\u5efa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8054\u5408\u9884\u6d4b\u591a\u4e2a\u5085\u91cc\u53f6\u5206\u91cf\u80fd\u591f\u7f13\u89e3\u72ec\u7acb\u9884\u6d4b\u5e26\u6765\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u63a8\u7406\u74f6\u9888\uff0c\u4ece\u800c\u5728\u4efb\u610f\u5c3a\u5ea6\u8d85\u5206\u8fa8\u7387\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u8d28\u91cf-\u6210\u672c\u6743\u8861\u3002"}}
{"id": "2510.23981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.23981", "abs": "https://arxiv.org/abs/2510.23981", "authors": ["Jiaqi Yan", "Ruilong Ren", "Jingren Liu", "Shuning Xu", "Ling Wang", "Yiheng Wang", "Yun Wang", "Long Zhang", "Xiangyu Chen", "Changzhi Sun", "Jixiang Luo", "Dell Zhang", "Hao Sun", "Chi Zhang", "Xuelong Li"], "title": "TeleEgo: Benchmarking Egocentric AI Assistants in the Wild", "comment": null, "summary": "Egocentric AI assistants in real-world settings must process multi-modal\ninputs (video, audio, text), respond in real time, and retain evolving\nlong-term memory. However, existing benchmarks typically evaluate these\nabilities in isolation, lack realistic streaming scenarios, or support only\nshort-term tasks. We introduce \\textbf{TeleEgo}, a long-duration, streaming,\nomni-modal benchmark for evaluating egocentric AI assistants in realistic daily\ncontexts. The dataset features over 14 hours per participant of synchronized\negocentric video, audio, and text across four domains: work \\& study, lifestyle\n\\& routines, social activities, and outings \\& culture. All data is aligned on\na unified global timeline and includes high-quality visual narrations and\nspeech transcripts, curated through human refinement.TeleEgo defines 12\ndiagnostic subtasks across three core capabilities: Memory (recalling past\nevents), Understanding (interpreting the current moment), and Cross-Memory\nReasoning (linking distant events). It contains 3,291 human-verified QA items\nspanning multiple question formats (single-choice, binary, multi-choice, and\nopen-ended), evaluated strictly in a streaming setting. We propose two key\nmetrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess\ncorrectness, temporal responsiveness, and long-term retention. TeleEgo provides\na realistic and comprehensive evaluation to advance the development of\npractical AI assistants.", "AI": {"tldr": "TeleEgo \u662f\u4e00\u4e2a\u9762\u5411\u771f\u5b9e\u65e5\u5e38\u573a\u666f\u7684\u957f\u65f6\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5305\u542b\u5927\u91cf\u540c\u6b65\u7b2c\u4e00\u89c6\u89d2\u6570\u636e\u3001\u4eba\u5de5\u7cbe\u70bc\u7684\u6807\u6ce8\u300112\u4e2a\u5b50\u4efb\u52a1\u4e0e3,291\u6761QA\uff0c\u5e76\u63d0\u51fa\u5b9e\u65f6\u51c6\u786e\u7387\u548c\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\u4e24\u9879\u6307\u6807\u6765\u8bc4\u4f30AI\u52a9\u624b\u7684\u5b9e\u65f6\u6027\u4e0e\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u901a\u5e38\u5355\u9879\u8bc4\u4f30\u80fd\u529b\u3001\u7f3a\u4e4f\u771f\u5b9e\u6d41\u5f0f\u573a\u666f\u6216\u53ea\u652f\u6301\u77ed\u671f\u4efb\u52a1\uff0c\u65e0\u6cd5\u5145\u5206\u8861\u91cf\u9762\u5411\u771f\u5b9e\u5e94\u7528\u7684\u81ea\u6211\u4e2d\u5fc3\uff08egocentric\uff09AI\u52a9\u624b\u5728\u591a\u6a21\u6001\u3001\u5b9e\u65f6\u54cd\u5e94\u548c\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\u65b9\u9762\u7684\u7efc\u5408\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u73b0\u5b9e\u3001\u66f4\u5168\u9762\u7684\u8bc4\u6d4b\u5957\u4ef6\u3002", "method": "\u6784\u5efa\u5305\u542b\u6bcf\u4f4d\u53c2\u4e0e\u8005\u8d85\u8fc714\u5c0f\u65f6\u540c\u6b65\u7684\u7b2c\u4e00\u89c6\u89d2\u89c6\u9891\u3001\u97f3\u9891\u4e0e\u6587\u672c\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u7cbe\u70bc\u5f97\u5230\u9ad8\u8d28\u91cf\u89c6\u89c9\u53d9\u8ff0\u548c\u8bed\u97f3\u8f6c\u5f55\uff1b\u5728\u7edf\u4e00\u5168\u5c40\u65f6\u95f4\u7ebf\u4e0a\u5bf9\u9f50\u6570\u636e\uff1b\u8bbe\u8ba112\u4e2a\u8bca\u65ad\u5b50\u4efb\u52a1\u5e76\u6536\u96c63,291\u6761\u4eba\u5de5\u9a8c\u8bc1\u95ee\u7b54\uff0c\u6240\u6709\u4efb\u52a1\u5728\u4e25\u683c\u6d41\u5f0f\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u3002", "result": "\u63d0\u4f9b\u5305\u542b\u56db\u5927\u9886\u57df\uff08\u5de5\u4f5c\u4e0e\u5b66\u4e60\u3001\u751f\u6d3b\u4e0e\u65e5\u5e38\u3001\u793e\u4ea4\u6d3b\u52a8\u3001\u5916\u51fa\u4e0e\u6587\u5316\uff09\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u63d0\u51fa\u4e24\u9879\u5173\u952e\u6307\u6807\uff08\u5b9e\u65f6\u51c6\u786e\u7387\u4e0e\u8bb0\u5fc6\u6301\u4e45\u65f6\u95f4\uff09\u4ee5\u8054\u5408\u8861\u91cf\u6b63\u786e\u6027\u3001\u65f6\u95f4\u54cd\u5e94\u6027\u4e0e\u957f\u671f\u8bb0\u5fc6\u4fdd\u6301\uff1b\u5e76\u901a\u8fc73,291\u6761QA\u5728\u6d41\u5f0f\u8bbe\u7f6e\u4e0b\u5bf9\u6a21\u578b\u8fdb\u884c\u4e25\u82db\u8bc4\u4f30\uff0c\u4fc3\u8fdb\u5b9e\u7528\u52a9\u624b\u7684\u53d1\u5c55\u3002", "conclusion": "TeleEgo \u63d0\u4f9b\u4e86\u4e00\u4e2a\u957f\u65f6\u3001\u6d41\u5f0f\u3001\u5168\u6a21\u6001\u7684\u57fa\u51c6\uff0c\u80fd\u540c\u65f6\u8003\u5bdf\u8bb0\u5fc6\u3001\u7406\u89e3\u548c\u8de8\u8bb0\u5fc6\u63a8\u7406\u4e09\u9879\u80fd\u529b\uff0c\u5f25\u8865\u73b0\u6709\u57fa\u51c6\u5728\u771f\u5b9e\u6d41\u5f0f\u573a\u666f\u4e0e\u957f\u671f\u8bb0\u5fc6\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\u3002"}}
{"id": "2510.24000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24000", "abs": "https://arxiv.org/abs/2510.24000", "authors": ["Heethanjan Kanagalingam", "Thenukan Pathmanathan", "Mokeeshan Vathanakumar", "Tharmakulasingam Mukunthan"], "title": "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet\nearly and accurate detection can significantly improve treatment outcomes.\nWhile numerous Deep learning (DL) models have been developed to predict DR from\nfundus images, many face challenges in maintaining robustness due to\ndistributional variations caused by differences in acquisition devices,\ndemographic disparities, and imaging conditions. This paper addresses this\ncritical limitation by proposing a novel DR classification approach, a method\ncalled AdvBlur. Our method integrates adversarial blurred images into the\ndataset and employs a dual-loss function framework to address domain\ngeneralization. This approach effectively mitigates the impact of unseen\ndistributional variations, as evidenced by comprehensive evaluations across\nmultiple datasets. Additionally, we conduct extensive experiments to explore\nthe effects of factors such as camera type, low-quality images, and dataset\nsize. Furthermore, we perform ablation studies on blurred images and the loss\nfunction to ensure the validity of our choices. The experimental results\ndemonstrate the effectiveness of our proposed method, achieving competitive\nperformance compared to state-of-the-art domain generalization DR models on\nunseen external datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3aAdvBlur\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u8bad\u7ec3\u96c6\u4e2d\u52a0\u5165\u5bf9\u6297\u751f\u6210\u7684\u6a21\u7cca\u56fe\u50cf\u5e76\u4f7f\u7528\u53cc\u91cd\u635f\u5931\u51fd\u6570\u7ba1\u7406\u57df\u6cdb\u5316\uff0c\u63d0\u5347\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u5206\u7c7b\u6a21\u578b\u5bf9\u672a\u77e5\u5206\u5e03\u53d8\u5316\uff08\u5982\u8bbe\u5907\u3001\u62cd\u6444\u6761\u4ef6\u548c\u4f4e\u8d28\u91cf\u56fe\u50cf\uff09\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u3001\u4e0d\u540c\u76f8\u673a\u7c7b\u578b\u3001\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u6570\u636e\u89c4\u6a21\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u786e\u8ba4\u6a21\u7cca\u56fe\u50cf\u548c\u635f\u5931\u51fd\u6570\u7684\u8d21\u732e\uff0c\u53d6\u5f97\u63a5\u8fd1\u6216\u8d85\u8d8a\u73b0\u6709\u57df\u6cdb\u5316DR\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709DL\u6a21\u578b\u5728\u4e0d\u540c\u91c7\u96c6\u8bbe\u5907\u3001\u62cd\u6444\u6761\u4ef6\u548c\u4eba\u7fa4\u5206\u5e03\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u672a\u77e5\u5916\u90e8\u6570\u636e\u8868\u73b0\u4e0b\u964d\u3002\u901a\u8fc7\u5408\u6210\u6a21\u7cca\u6837\u672c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u4f18\u5316\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u53ef\u63d0\u9ad8\u6a21\u578b\u5bf9\u5206\u5e03\u53d8\u5316\u7684\u9c81\u68d2\u6027\uff0c\u4ece\u800c\u63d0\u5347\u4e34\u5e8a\u63a8\u5e7f\u4ef7\u503c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528\u5bf9\u6297\u7b56\u7565\u751f\u6210\u6a21\u7cca\u5316\u7684\u57fa\u91d1\u5f71\u50cf\uff08adversarial blur\uff09\u4ee5\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u62cd\u6444\u9000\u5316\uff1b2) \u5c06\u8fd9\u4e9b\u6a21\u7cca\u56fe\u50cf\u6269\u5145\u5230\u8bad\u7ec3\u96c6\u4ee5\u8986\u76d6\u66f4\u591a\u5206\u5e03\uff1b3) \u8bbe\u8ba1\u53cc\u91cd\u635f\u5931\u51fd\u6570\uff08\u53ef\u80fd\u5305\u542b\u5206\u7c7b\u635f\u5931\u4e0e\u57df\u5bf9\u6297/\u5bf9\u6bd4\u635f\u5931\uff09\u4ee5\u540c\u65f6\u4f18\u5316\u5206\u7c7b\u6027\u80fd\u548c\u57df\u4e0d\u53d8\u7279\u5f81\u5b66\u4e60\uff1b4) \u5728\u591a\u4e2a\u516c\u5f00\u773c\u5e95\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\uff0c\u5e76\u52a0\u5165\u5173\u4e8e\u76f8\u673a\u7c7b\u578b\u3001\u4f4e\u8d28\u91cf\u56fe\u50cf\u548c\u6570\u636e\u89c4\u6a21\u7684\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u5728\u591a\u6570\u636e\u96c6\u548c\u672a\u89c1\u5916\u90e8\u6d4b\u8bd5\u96c6\u4e0a\uff0cAdvBlur\u76f8\u6bd4\u57fa\u7ebf\u53ca\u90e8\u5206\u6700\u5148\u8fdb\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u6216\u76f8\u5f53\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u5bf9\u6297\u6a21\u7cca\u6837\u672c\u548c\u53cc\u91cd\u635f\u5931\u5404\u81ea\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "AdvBlur\u901a\u8fc7\u878d\u5165\u5bf9\u6297\u6a21\u7cca\u56fe\u50cf\u548c\u53cc\u91cd\u635f\u5931\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u56e0\u91c7\u96c6\u8bbe\u5907\u3001\u6210\u50cf\u6761\u4ef6\u548c\u4eba\u7fa4\u5dee\u5f02\u5bfc\u81f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u672a\u89c1\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u4f18\u4e8e\u6216\u63a5\u8fd1\u73b0\u6709\u6700\u5148\u8fdb\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\u3002"}}
{"id": "2510.24009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24009", "abs": "https://arxiv.org/abs/2510.24009", "authors": ["Yuan Jin", "Antonio Pepe", "Gian Marco Melito", "Yuxuan Chen", "Yunsu Byeon", "Hyeseong Kim", "Kyungwon Kim", "Doohyun Park", "Euijoon Choi", "Dosik Hwang", "Andriy Myronenko", "Dong Yang", "Yufan He", "Daguang Xu", "Ayman El-Ghotni", "Mohamed Nabil", "Hossam El-Kady", "Ahmed Ayyad", "Amr Nasr", "Marek Wodzinski", "Henning M\u00fcller", "Hyeongyu Kim", "Yejee Shin", "Abbas Khan", "Muhammad Asad", "Alexander Zolotarev", "Caroline Roney", "Anthony Mathur", "Martin Benning", "Gregory Slabaugh", "Theodoros Panagiotis Vagenas", "Konstantinos Georgas", "George K. Matsopoulos", "Jihan Zhang", "Zhen Zhang", "Liqin Huang", "Christian Mayer", "Heinrich M\u00e4chler", "Jan Egger"], "title": "Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge", "comment": null, "summary": "The automated analysis of the aortic vessel tree (AVT) from computed\ntomography angiography (CTA) holds immense clinical potential, but its\ndevelopment has been impeded by a lack of shared, high-quality data. We\nlaunched the SEG.A. challenge to catalyze progress in this field by introducing\na large, publicly available, multi-institutional dataset for AVT segmentation.\nThe challenge benchmarked automated algorithms on a hidden test set, with\nsubsequent optional tasks in surface meshing for computational simulations. Our\nfindings reveal a clear convergence on deep learning methodologies, with 3D\nU-Net architectures dominating the top submissions. A key result was that an\nensemble of the highest-ranking algorithms significantly outperformed\nindividual models, highlighting the benefits of model fusion. Performance was\nstrongly linked to algorithmic design, particularly the use of customized\npost-processing steps, and the characteristics of the training data. This\ninitiative not only establishes a new performance benchmark but also provides a\nlasting resource to drive future innovation toward robust, clinically\ntranslatable tools.", "AI": {"tldr": "\u53d1\u5e03\u4e86\u5927\u578b\u591a\u673a\u6784CTA AVT\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u4e3e\u529e\u7ade\u8d5b\uff0c3D U-Net\u4e3b\u5bfc\uff0c\u6a21\u578b\u878d\u5408\u548c\u5b9a\u5236\u540e\u5904\u7406\u663e\u8457\u63d0\u5347\u6548\u679c\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u57fa\u7ebf\u3002", "motivation": "\u7f3a\u4e4f\u5171\u4eab\u9ad8\u8d28\u91cfAVT\u5206\u5272\u6570\u636e\u963b\u788d\u9886\u57df\u53d1\u5c55\uff0c\u53d1\u5e03\u5927\u89c4\u6a21\u516c\u5f00\u6570\u636e\u96c6\u5e76\u901a\u8fc7\u7ade\u8d5b\u8bc4\u4ef7\u7b97\u6cd5\u4ee5\u63a8\u52a8\u7814\u7a76\u4e0e\u4e34\u5e8a\u5e94\u7528\u843d\u5730\u3002", "method": "\u53c2\u8d5b\u65b9\u6cd5\u4ee5\u6df1\u5ea6\u5b66\u4e60\u4e3a\u4e3b\uff0c\u5c24\u51763D U-Net\u53ca\u5176\u53d8\u4f53\uff1b\u5173\u952e\u5305\u62ec\u6570\u636e\u589e\u5f3a\u3001\u7c7b\u522b\u5e73\u8861\u635f\u5931\u3001\u7cbe\u7ec6\u5316\u7684\u540e\u5904\u7406\uff08\u8fde\u901a\u7ec4\u4ef6\u8fc7\u6ee4\u3001\u5f62\u6001\u5b66\u64cd\u4f5c\u3001\u6295\u5f71\u4fee\u6b63\u7b49\uff09\uff1b\u4e00\u4e9b\u56e2\u961f\u8fd8\u57fa\u4e8e\u8868\u9762\u91cd\u5efa\u8fdb\u884c\u53ef\u9009\u7f51\u683c\u751f\u6210\u3002", "result": "SEG.A challenge built a large multi-institutional CTA dataset for aortic vessel tree segmentation and benchmarked algorithms; top methods were deep learning 3D U-Nets; ensembles improved performance; customized post-processing and training data crucial; provided lasting resource and benchmark.", "conclusion": "\u7ade\u8d5b\u4fc3\u6210\u4e86\u7b97\u6cd5\u8fdb\u6b65\uff0c3D U-Net\u67b6\u6784\u4e3a\u4e3b\u6d41\uff0c\u96c6\u6210\u65b9\u6cd5\u4f18\u4e8e\u5355\u6a21\u578b\uff0c\u6570\u636e\u8d28\u91cf\u548c\u540e\u5904\u7406\u5bf9\u6027\u80fd\u5f71\u54cd\u5927\uff0c\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u672a\u6765\u4e34\u5e8a\u53ef\u8f6c\u5316\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.24010", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24010", "abs": "https://arxiv.org/abs/2510.24010", "authors": ["Mirali Purohit", "Bimal Gajera", "Vatsal Malaviya", "Irish Mehta", "Kunal Kasodekar", "Jacob Adler", "Steven Lu", "Umaa Rebbapragada", "Hannah Kerner"], "title": "Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks", "comment": "Accepted at NeurIPS 2025", "summary": "Foundation models have enabled rapid progress across many specialized domains\nby leveraging large-scale pre-training on unlabeled data, demonstrating strong\ngeneralization to a variety of downstream tasks. While such models have gained\nsignificant attention in fields like Earth Observation, their application to\nMars science remains limited. A key enabler of progress in other domains has\nbeen the availability of standardized benchmarks that support systematic\nevaluation. In contrast, Mars science lacks such benchmarks and standardized\nevaluation frameworks, which have limited progress toward developing foundation\nmodels for Martian tasks. To address this gap, we introduce Mars-Bench, the\nfirst benchmark designed to systematically evaluate models across a broad range\nof Mars-related tasks using both orbital and surface imagery. Mars-Bench\ncomprises 20 datasets spanning classification, segmentation, and object\ndetection, focused on key geologic features such as craters, cones, boulders,\nand frost. We provide standardized, ready-to-use datasets and baseline\nevaluations using models pre-trained on natural images, Earth satellite data,\nand state-of-the-art vision-language models. Results from all analyses suggest\nthat Mars-specific foundation models may offer advantages over general-domain\ncounterparts, motivating further exploration of domain-adapted pre-training.\nMars-Bench aims to establish a standardized foundation for developing and\ncomparing machine learning models for Mars science. Our data, models, and code\nare available at: https://mars-bench.github.io/.", "AI": {"tldr": "Mars-Bench\u662f\u7b2c\u4e00\u4e2a\u9762\u5411\u706b\u661f\u5f71\u50cf\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u542b20\u4e2a\u6570\u636e\u96c6\u4e0e\u591a\u79cd\u4efb\u52a1\uff0c\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\u706b\u661f\u4e13\u7528\u7684\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u6bd4\u901a\u7528\u6a21\u578b\u66f4\u4f18\u3002", "motivation": "\u706b\u661f\u79d1\u5b66\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u4e0e\u8bc4\u6d4b\u6846\u67b6\uff0c\u9650\u5236\u4e86\u57fa\u7840\u6a21\u578b\u5728\u706b\u661f\u4efb\u52a1\u4e0a\u7684\u53d1\u5c55\uff1b\u901a\u8fc7\u6784\u5efaMars-Bench\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4fc3\u8fdb\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0e\u6a21\u578b\u6bd4\u8f83\u3002", "method": "\u6536\u96c6\u5e76\u6807\u51c6\u5316\u6765\u81ea\u8f68\u9053\u548c\u5730\u9762\u5f71\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6574\u7406\u4e3a20\u4e2a\u4efb\u52a1\uff0c\u5b9e\u65bd\u57fa\u7ebf\u5b9e\u9a8c\u4ee5\u6bd4\u8f83\u5728\u4e0d\u540c\u9884\u8bad\u7ec3\u6e90\uff08\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u9065\u611f\u3001\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff09\u4e0a\u5fae\u8c03\u7684\u6a21\u578b\u6027\u80fd\uff0c\u516c\u5f00\u6570\u636e\u4e0e\u4ee3\u7801\u3002", "result": "Mars-Bench\u63d0\u51fa\u4e86\u7528\u4e8e\u706b\u661f\u79d1\u5b66\u7684\u9996\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u8986\u76d6\u8f68\u9053\u548c\u5730\u9762\u5f71\u50cf\uff0c\u5171\u5305\u542b20\u4e2a\u6570\u636e\u96c6\uff0c\u4efb\u52a1\u5305\u62ec\u5206\u7c7b\u3001\u5206\u5272\u3001\u76ee\u6807\u68c0\u6d4b\uff0c\u805a\u7126\u73af\u5f62\u5751\u3001\u5706\u9525\u3001\u5de8\u77f3\u548c\u971c\u7b49\u5730\u8d28\u7279\u5f81\u3002\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u5728\u81ea\u7136\u56fe\u50cf\u3001\u5730\u7403\u536b\u661f\u6570\u636e\u548c\u5148\u8fdb\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u5206\u6790\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u706b\u661f\u57df\u9002\u914d\u7684\u57fa\u7840\u6a21\u578b\u53ef\u80fd\u4f18\u4e8e\u901a\u7528\u57df\u6a21\u578b\uff0c\u63a8\u52a8\u9886\u57df\u9002\u914d\u9884\u8bad\u7ec3\u7684\u7814\u7a76\u3002", "conclusion": "\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u5bf9\u63a8\u52a8\u706b\u661f\u673a\u5668\u5b66\u4e60\u7814\u7a76\u81f3\u5173\u91cd\u8981\uff1b\u521d\u6b65\u7ed3\u679c\u652f\u6301\u5f00\u53d1\u706b\u661f\u57df\u9002\u914d\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u6570\u636e\u4e0e\u57fa\u7ebf\u4ee3\u7801\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2510.24034", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24034", "abs": "https://arxiv.org/abs/2510.24034", "authors": ["Yufan Liu", "Wanqian Zhang", "Huashan Chen", "Lin Wang", "Xiaojun Jia", "Zheng Lin", "Weiping Wang"], "title": "AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts", "comment": "Accepted by ICCV 2025", "summary": "Despite rapid advancements in text-to-image (T2I) models, their safety\nmechanisms are vulnerable to adversarial prompts, which maliciously generate\nunsafe images. Current red-teaming methods for proactively assessing such\nvulnerabilities usually require white-box access to T2I models, and rely on\ninefficient per-prompt optimization, as well as inevitably generate\nsemantically meaningless prompts easily blocked by filters. In this paper, we\npropose APT (AutoPrompT), a black-box framework that leverages large language\nmodels (LLMs) to automatically generate human-readable adversarial suffixes for\nbenign prompts. We first introduce an alternating optimization-finetuning\npipeline between adversarial suffix optimization and fine-tuning the LLM\nutilizing the optimized suffix. Furthermore, we integrates a dual-evasion\nstrategy in optimization phase, enabling the bypass of both perplexity-based\nfilter and blacklist word filter: (1) we constrain the LLM generating\nhuman-readable prompts through an auxiliary LLM perplexity scoring, which\nstarkly contrasts with prior token-level gibberish, and (2) we also introduce\nbanned-token penalties to suppress the explicit generation of banned-tokens in\nblacklist. Extensive experiments demonstrate the excellent red-teaming\nperformance of our human-readable, filter-resistant adversarial prompts, as\nwell as superior zero-shot transferability which enables instant adaptation to\nunseen prompts and exposes critical vulnerabilities even in commercial APIs\n(e.g., Leonardo.Ai.).", "AI": {"tldr": "APT uses LLMs to craft readable adversarial suffixes through iterative optimization and fine-tuning, adding perplexity scoring and banned-token penalties to evade filters, yielding effective and transferable red-teaming prompts.", "motivation": "The paper seeks to evaluate and exploit vulnerabilities in text-to-image models by creating adversarial prompts that can bypass safety filters while remaining human-readable, addressing limitations of white-box and semantically meaningless prior methods.", "method": "Alternating optimization and LLM fine-tuning loop for suffix generation; auxiliary LLM perplexity scoring to enforce human-readability; banned-token penalties to avoid blacklist words; evaluation across models and APIs demonstrating transferability and effectiveness.", "result": "Developed APT, a black-box framework using LLMs to generate adversarial suffixes via an alternating optimization-finetuning pipeline and dual-evasion strategies, achieving effective, human-readable, filter-resistant prompts with strong zero-shot transferability, exposing vulnerabilities including in commercial APIs.", "conclusion": "APT provides a practical black-box technique to generate human-readable, filter-resistant adversarial prompts that reveal safety flaws in T2I models and can adapt to unseen prompts and commercial services."}}
{"id": "2510.24036", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24036", "abs": "https://arxiv.org/abs/2510.24036", "authors": ["Xingyu Liu", "Kun Ming Goh"], "title": "ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning", "comment": "3 pages, 5 figures, 1 table", "summary": "Convolutional Neural Networks (CNNs) has revolutionized computer vision, but\ntraining very deep networks has been challenging due to the vanishing gradient\nproblem. This paper explores Residual Networks (ResNet), introduced by He et\nal. (2015), which overcomes this limitation by using skip connections. ResNet\nenables the training of networks with hundreds of layers by allowing gradients\nto flow directly through shortcut connections that bypass intermediate layers.\nIn our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%\naccuracy compared to 84.1% for a traditional deep CNN of similar depth, while\nalso converging faster and training more stably.", "AI": {"tldr": "ResNet\u901a\u8fc7\u6b8b\u5dee\u8fde\u63a5\u89e3\u51b3\u68af\u5ea6\u6d88\u5931\uff0c\u4f7f\u5f97\u8bad\u7ec3\u66f4\u6df1\u7f51\u7edc\u6210\u4e3a\u53ef\u80fd\uff1b\u5728CIFAR-10\u4e0aResNet-18\u51c6\u786e\u738789.9%\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6CNN\u768484.1%\u3002", "motivation": "\u89e3\u51b3\u6df1\u5c42\u5377\u79ef\u7f51\u7edc\u8bad\u7ec3\u56f0\u96be\uff08\u68af\u5ea6\u6d88\u5931\uff09\uff0c\u5b9e\u73b0\u66f4\u6df1\u7f51\u7edc\u4ee5\u63d0\u5347\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6b8b\u5dee\u5757\uff08skip connection\uff09\uff0c\u5728\u7f51\u7edc\u4e2d\u52a0\u5165\u6052\u7b49\u6620\u5c04\u7684\u5feb\u6377\u8fde\u63a5\u4ee5\u8ba9\u68af\u5ea6\u76f4\u63a5\u4f20\u9012\uff1b\u5728CIFAR-10\u4e0a\u5b9e\u73b0ResNet-18\u5e76\u4e0e\u7b49\u6df1\u4f20\u7edfCNN\u5bf9\u6bd4\u3002", "result": "ResNet\u5728CIFAR-10\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6CNN\uff0c\u63d0\u5347\u7cbe\u5ea6\u5e76\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u6b8b\u5dee\u8fde\u63a5\u663e\u8457\u6539\u5584\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u4f7f\u66f4\u6df1\u7f51\u7edc\u53ef\u884c\u4e14\u6027\u80fd\u66f4\u597d\u3002"}}
{"id": "2510.24037", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24037", "abs": "https://arxiv.org/abs/2510.24037", "authors": ["Shufan Shen", "Junshu Sun", "Shuhui Wang", "Qingming Huang"], "title": "Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision\nmodels to downstream tasks. Among PEFT paradigms, sparse tuning achieves\nremarkable performance by adjusting only the weights most relevant to\ndownstream tasks, rather than densely tuning the entire weight matrix. Current\nmethods follow a two-stage paradigm. First, it locates task-relevant weights by\ngradient information, which overlooks the parameter adjustments during\nfine-tuning and limits the performance. Second, it updates only the located\nweights by applying a sparse mask to the gradient of the weight matrix, which\nresults in high memory usage due to the storage of all weight matrices in the\noptimizer. In this paper, we propose a one-stage method named SNELLA to\novercome the above limitations. For memory usage, SNELLA selectively updates\nthe weight matrix by adding it to another sparse matrix that is merged by two\nlow-rank learnable matrices. We extend the low-rank decomposition by\nintroducing nonlinear kernel functions, thereby increasing the rank of the\nresulting merged matrix to prevent the interdependency among weight updates,\nenabling better adaptation to downstream tasks. For locating task-relevant\nweights, we propose an adaptive bi-level sparsity allocation mechanism that\nencourages weights to compete across and inside layers based on their\nimportance scores in an end-to-end manner. Extensive experiments are conducted\non classification, segmentation, and generation tasks using different\npre-trained vision models. The results show that SNELLA achieves SOTA\nperformance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.\n90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.\nCompared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%\nacross models with parameter scales from 86M to 632M. Our source codes are\navailable at https://github.com/ssfgunner/SNELL.", "AI": {"tldr": "SNELLA\u662f\u4e00\u79cd\u4e00\u9636\u6bb5\u3001\u7aef\u5230\u7aef\u7684\u7a00\u758f\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff1a\u901a\u8fc7\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u5408\u5e76\uff08\u5f15\u5165\u975e\u7ebf\u6027\u6838\uff09\u5b9e\u73b0\u9009\u62e9\u6027\u66f4\u65b0\uff0c\u5e76\u7528\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u5b9a\u4f4d\u91cd\u8981\u53c2\u6570\uff0c\u8fbe\u5230\u66f4\u9ad8\u7cbe\u5ea6\u548c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u5fae\u8c03\u65b9\u6cd5\u4e3a\u4e24\u9636\u6bb5\uff1a\u5148\u7528\u68af\u5ea6\u5b9a\u4f4d\u76f8\u5173\u53c2\u6570\u518d\u7a00\u758f\u66f4\u65b0\uff0c\u8fd9\u5ffd\u7565\u4e86\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u53c2\u6570\u8c03\u6574\u540c\u65f6\u5bfc\u81f4\u4f18\u5316\u5668\u9700\u4fdd\u5b58\u6240\u6709\u6743\u91cd\u81f4\u5185\u5b58\u5f00\u9500\u5927\uff1b\u9700\u8981\u4e00\u79cd\u80fd\u5728\u66f4\u65b0\u9636\u6bb5\u540c\u65f6\u5b9a\u4f4d\u548c\u7a00\u758f\u66f4\u65b0\u3001\u5e76\u964d\u4f4e\u5185\u5b58\u5360\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u9636\u6bb5\u65b9\u6cd5SNELLA\uff1a\u7528\u4e24\u4e2a\u53ef\u5b66\u4e60\u7684\u4f4e\u79e9\u77e9\u9635\u5408\u5e76\u6210\u7a00\u758f\u77e9\u9635\u5e76\u52a0\u56de\u6743\u91cd\u5b9e\u73b0\u9009\u62e9\u6027\u66f4\u65b0\uff0c\u6269\u5c55\u4f4e\u79e9\u5206\u89e3\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u4ee5\u63d0\u9ad8\u5408\u5e76\u77e9\u9635\u79e9\u5e76\u9632\u6b62\u66f4\u65b0\u95f4\u4f9d\u8d56\uff1b\u540c\u65f6\u8bbe\u8ba1\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\u673a\u5236\uff0c\u7aef\u5230\u7aef\u6839\u636e\u91cd\u8981\u6027\u5206\u6570\u5728\u5c42\u95f4\u5c42\u5185\u5206\u914d\u7a00\u758f\u9884\u7b97\u3002", "result": "\u5728\u5206\u7c7b\u3001\u5206\u5272\u3001\u751f\u6210\u4efb\u52a1\u53ca\u4e0d\u540c\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cSNELLA\u5728FGVC\u4e0aTop-1\u51c6\u786e\u7387\u8f83SPT-LoRA\u63d0\u53471.8%\uff0891.9% vs 90.1%\uff09\uff0c\u5e76\u572886M\u5230632M\u53c2\u6570\u89c4\u6a21\u7684\u6a21\u578b\u4e0a\u5b9e\u73b031.1%~39.9%\u7684\u5185\u5b58\u524a\u51cf\uff0c\u6574\u4f53\u8fbe\u5230SOTA\u6027\u80fd\u4e0e\u4f4e\u5185\u5b58\u5360\u7528\u7684\u6298\u4e2d\u3002", "conclusion": "SNELLA\u901a\u8fc7\u5c06\u7a00\u758f\u66f4\u65b0\u4e0e\u4f4e\u79e9\u53ef\u5b66\u4e60\u77e9\u9635\u5408\u5e76\u5e76\u5f15\u5165\u975e\u7ebf\u6027\u6838\u51fd\u6570\u3001\u4ee5\u53ca\u81ea\u9002\u5e94\u53cc\u5c42\u7a00\u758f\u5206\u914d\uff0c\u5728\u5185\u5b58\u53d7\u9650\u4e0b\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u4e00\u9636\u6bb5\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u80fd\u66f4\u6709\u6548\u5b9a\u4f4d\u5e76\u66f4\u65b0\u4efb\u52a1\u76f8\u5173\u53c2\u6570\uff0c\u83b7\u5f97SOTA\u6027\u80fd\u4e14\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\u3002"}}
{"id": "2510.24038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24038", "abs": "https://arxiv.org/abs/2510.24038", "authors": ["Xingyu Zhu", "Beier Zhu", "Shuo Wang", "Kesen Zhao", "Hanwang Zhang"], "title": "Enhancing CLIP Robustness via Cross-Modality Alignment", "comment": "NeurIPS 2025 Spotlight", "summary": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization\nin zero-shot classification but remain highly vulnerable to adversarial\nperturbations. Existing methods primarily focus on adversarial fine-tuning or\nprompt optimization; they often overlook the gaps in CLIP's encoded features,\nwhich is shown as the text and image features lie far apart from each other.\nThis misalignment is significantly amplified under adversarial perturbations,\nleading to severe degradation in classification performance. To address this\nproblem, we propose Cross-modality Alignment, dubbed COLA, an optimal\ntransport-based framework that explicitly addresses adversarial misalignment by\nrestoring both global image-text alignment and local structural consistency in\nthe feature space. (1) COLA first projects adversarial image embeddings onto a\nsubspace spanned by class text features, effectively filtering out non-semantic\ndistortions while preserving discriminative information. (2) It then models\nimages and texts as discrete distributions over multiple augmented views and\nrefines their alignment via OT, with the subspace projection seamlessly\nintegrated into the cost computation. This design ensures stable cross-modal\nalignment even under adversarial conditions. COLA is training-free and\ncompatible with existing fine-tuned models. Extensive evaluations across 14\nzero-shot classification benchmarks demonstrate the effectiveness of COLA,\nespecially with an average improvement of 6.7% on ImageNet and its variants\nunder PGD adversarial attacks, while maintaining high accuracy on clean\nsamples.", "AI": {"tldr": "COLA\u4e3a\u65e0\u8bad\u7ec3\u7684\u6700\u4f18\u4f20\u8f93\u6846\u67b6\uff0c\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u548c\u591a\u89c6\u89d2OT\u5bf9\u9f50\u4fee\u590d\u5bf9\u6297\u4e0b\u7684\u56fe\u6587\u7279\u5f81\u4e0d\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347CLIP\u7b49VLM\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "motivation": "\u89c2\u5bdf\u5230CLIP\u7b49VLM\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u56fe\u50cf\u4e0e\u6587\u672c\u7279\u5f81\u5b58\u5728\u8f83\u5927\u8ddd\u79bb\uff0c\u5bf9\u6297\u6270\u52a8\u4f1a\u653e\u5927\u8fd9\u79cd\u4e0d\u5bf9\u9f50\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u6b64\u9700\u8981\u5728\u7279\u5f81\u7a7a\u95f4\u663e\u5f0f\u6062\u590d\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u5148\u5c06\u5bf9\u6297\u56fe\u50cf\u5d4c\u5165\u6295\u5f71\u5230\u7531\u7c7b\u522b\u6587\u672c\u7279\u5f81\u5f20\u6210\u7684\u5b50\u7a7a\u95f4\uff0c\u4ee5\u6ee4\u9664\u975e\u8bed\u4e49\u6027\u566a\u58f0\uff1b\u518d\u5c06\u56fe\u50cf\u4e0e\u6587\u672c\u89c6\u4e3a\u591a\u4e2a\u589e\u5f3a\u89c6\u89d2\u7ec4\u6210\u7684\u79bb\u6563\u5206\u5e03\uff0c\u7ed3\u5408\u5b50\u7a7a\u95f4\u6295\u5f71\u7684\u6210\u672c\u51fd\u6570\uff0c\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7ec6\u5316\u5176\u5c40\u90e8\u7ed3\u6784\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u65e0\u8bad\u7ec3\u4e14\u53ef\u517c\u5bb9\u5df2\u5fae\u8c03\u7684\u6a21\u578b\u3002", "result": "\u572814\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728PGD\u5bf9\u6297\u653b\u51fb\u4e0b\uff0cCOLA\u5728ImageNet\u53ca\u5176\u53d8\u4f53\u4e0a\u5e73\u5747\u63d0\u5347\u7ea66.7%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5728\u5e72\u51c0\u6837\u672c\u4e0a\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "COLA\u901a\u8fc7\u5b50\u7a7a\u95f4\u6295\u5f71\u548c\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\u7684\u5c40\u90e8\u7ed3\u6784\u5bf9\u9f50\uff0c\u6709\u6548\u7f13\u89e3\u4e86VLM\u5728\u5bf9\u6297\u6270\u52a8\u4e0b\u7684\u6a21\u6001\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u96f6\u6837\u672c\u5206\u7c7b\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.24078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24078", "abs": "https://arxiv.org/abs/2510.24078", "authors": ["William Yang", "Xindi Wu", "Zhiwei Deng", "Esin Tureci", "Olga Russakovsky"], "title": "Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification", "comment": null, "summary": "Text-to-image (T2I) models are increasingly used for synthetic dataset\ngeneration, but generating effective synthetic training data for classification\nremains challenging. Fine-tuning a T2I model with a few real examples can help\nimprove the quality of synthetic training data; however, it may also cause\noverfitting and reduce diversity in the generated samples. We propose a\nfine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for\nfine-grained classification. Given a small set of real examples, we first\nextract class-agnostic attributes such as scene background and object pose. We\nthen explicitly condition on these attributes during fine-tuning of the T2I\nmodel and marginalize them out during generation. This design mitigates\noverfitting, preserves the T2I model's generative prior, reduces estimation\nerrors, and further minimizes unintended inter-class associations. Extensive\nexperiments across multiple T2I models, backbones, and datasets show that our\nmethod achieves state-of-the-art performance in low-shot fine-grained\nclassification when augmented with synthetic data. Concretely, BOB outperforms\nDataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning\na CLIP classifier with five real images augmented with 100 synthetic images).\nIn three of the four benchmarks, fine-tuning downstream models with 5 real\nimages augmented with BOB achieves better performance than fine-tuning with 10\nreal images. Collectively, BOB outperforms prior art in 18 of 24 experimental\nsettings, with 2+% accuracy improvements in 14 of these settings.", "AI": {"tldr": "BOB\u901a\u8fc7\u663e\u5f0f\u6761\u4ef6\u5316\u4e0e\u8fb9\u7f18\u5316\u7c7b\u65e0\u5173\u5c5e\u6027\uff0c\u51cf\u8f7b\u5fae\u8c03\u5f15\u8d77\u7684\u8fc7\u62df\u5408\u548c\u591a\u6837\u6027\u4e27\u5931\uff0c\u4ece\u800c\u4e3a\u4f4e\u6837\u672c\u7cbe\u7ec6\u5206\u7c7b\u751f\u6210\u66f4\u6709\u6548\u7684\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "motivation": "\u76f4\u63a5\u7528\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u5fae\u8c03T2I\u6a21\u578b\u4f1a\u9020\u6210\u8fc7\u62df\u5408\u4e0e\u6837\u672c\u591a\u6837\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u7528\u4e8e\u7cbe\u7ec6\u5206\u7c7b\u7684\u5408\u6210\u8bad\u7ec3\u96c6\u7684\u6709\u6548\u6027\u3002BOB\u65e8\u5728\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u907f\u514d\u8fd9\u4e9b\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u4ece\u5c11\u91cf\u771f\u5b9e\u6837\u672c\u4e2d\u63d0\u53d6\u573a\u666f\u80cc\u666f\u3001\u7269\u4f53\u59ff\u6001\u7b49\u7c7b\u65e0\u5173\u5c5e\u6027\uff1b\u5728T2I\u6a21\u578b\u5fae\u8c03\u9636\u6bb5\u5c06\u8fd9\u4e9b\u5c5e\u6027\u4f5c\u4e3a\u6761\u4ef6\u8f93\u5165\uff1b\u5728\u5408\u6210\u9636\u6bb5\u5bf9\u8fd9\u4e9b\u5c5e\u6027\u8fdb\u884c\u8fb9\u7f18\u5316\u4ee5\u6062\u590d\u591a\u6837\u6027\uff0c\u51cf\u5c11\u5bf9\u7c7b\u522b\u76f8\u5173\u7279\u5f81\u7684\u9519\u8bef\u5173\u8054\u3002", "result": "\u5728\u591a\u79cdT2I\u6a21\u578b\u3001\u9aa8\u5e72\u7f51\u7edc\u4e0e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u4e2d\uff0cBOB\u5728\u4f4e\u6837\u672c\u7cbe\u7ec6\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1b\u4f8b\u5982\u5728Aircraft\u6570\u636e\u96c6\u4e0a\u5c06CLIP\u5206\u7c7b\u5668\u57285\u5f20\u771f\u5b9e\u56fe+100\u5f20\u5408\u6210\u56fe\u4e0b\u7684\u51c6\u786e\u7387\u4ece50.0%\u63d0\u9ad8\u523057.4%\uff0c\u572818/24\u4e2a\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002", "conclusion": "BOB\u901a\u8fc7\u5728\u5fae\u8c03\u65f6\u663e\u5f0f\u6761\u4ef6\u5316\u7c7b\u65e0\u5173\u5c5e\u6027\u5e76\u5728\u751f\u6210\u65f6\u5bf9\u5176\u8fb9\u7f18\u5316\uff0c\u6210\u529f\u51cf\u8f7b\u4e86\u8fc7\u62df\u5408\u5e76\u4fdd\u7559\u4e86T2I\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\uff0c\u4ece\u800c\u5728\u4f4e\u6837\u672c\u7cbe\u7ec6\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u6570\u636e\u589e\u5f3a\u7684\u6548\u679c\u3002"}}
{"id": "2510.24093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24093", "abs": "https://arxiv.org/abs/2510.24093", "authors": ["Agus Gunawan", "Samuel Teodoro", "Yun Chen", "Soo Ye Kim", "Jihyong Oh", "Munchurl Kim"], "title": "OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation", "comment": "The first two authors contributed equally to this work. The last two\n  authors are co-corresponding authors", "summary": "Recent advancements in diffusion-based text synthesis have demonstrated\nsignificant performance in inserting and editing text within images via\ninpainting. However, despite the potential of text inpainting methods, three\nkey limitations hinder their applicability to broader Text Image Manipulation\n(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over\nthe style of rendered text, and (iii) a tendency to generate duplicated\nletters. To address these challenges, we propose OmniText, a training-free\ngeneralist capable of performing a wide range of TIM tasks. Specifically, we\ninvestigate two key properties of cross- and self-attention mechanisms to\nenable text removal and to provide control over both text styles and content.\nOur findings reveal that text removal can be achieved by applying\nself-attention inversion, which mitigates the model's tendency to focus on\nsurrounding text, thus reducing text hallucinations. Additionally, we\nredistribute cross-attention, as increasing the probability of certain text\ntokens reduces text hallucination. For controllable inpainting, we introduce\nnovel loss functions in a latent optimization framework: a cross-attention\ncontent loss to improve text rendering accuracy and a self-attention style loss\nto facilitate style customization. Furthermore, we present OmniText-Bench, a\nbenchmark dataset for evaluating diverse TIM tasks. It includes input images,\ntarget text with masks, and style references, covering diverse applications\nsuch as text removal, rescaling, repositioning, and insertion and editing with\nvarious styles. Our OmniText framework is the first generalist method capable\nof performing diverse TIM tasks. It achieves state-of-the-art performance\nacross multiple tasks and metrics compared to other text inpainting methods and\nis comparable with specialist methods.", "AI": {"tldr": "\u63d0\u51faOmniText\u2014\u2014\u4e00\u4e2a\u8bad\u7ec3\u514d\u8d39\u3001\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u64cd\u4f5c\u7684\u901a\u7528\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u9006\u8fd0\u7b97\u53bb\u9664\u6587\u672c\u3001\u91cd\u5206\u914d\u4ea4\u53c9\u6ce8\u610f\u529b\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e76\u5728\u6f5c\u7a7a\u95f4\u4f18\u5316\u4e2d\u5f15\u5165\u6ce8\u610f\u529b\u76f8\u5173\u635f\u5931\u5b9e\u73b0\u5185\u5bb9\u4e0e\u98ce\u683c\u53ef\u63a7\uff1b\u5e76\u6784\u5efaOmniText-Bench\u7528\u4e8e\u8bc4\u4f30\uff0c\u591a\u4efb\u52a1\u4e0a\u8fbeSOTA\u6216\u53ef\u6bd4\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u6587\u672c\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u6587\u672c\u53bb\u9664\u3001\u6837\u5f0f\u53ef\u63a7\u6027\u548c\u91cd\u590d\u5b57\u6bcd\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9650\u5236\u4e86\u5176\u5728\u5e7f\u6cdbTIM\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u6587\u672c\u79fb\u9664\u3001\u6837\u5f0f\u53ef\u63a7\u7684\u63d2\u5165\u4e0e\u7f16\u8f91\u7b49\u591a\u6837\u4efb\u52a1\u3002", "method": "\u5206\u6790\u5e76\u5229\u7528\u81ea\u6ce8\u610f\u529b\u4e0e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u6027\u8d28\uff1a1) \u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u9006\u8fd0\u7b97\u6291\u5236\u6a21\u578b\u5bf9\u5468\u56f4\u6587\u672c\u7684\u5173\u6ce8\u4ee5\u5b9e\u73b0\u6587\u672c\u53bb\u9664\uff1b2) \u901a\u8fc7\u91cd\u5206\u914d\u4ea4\u53c9\u6ce8\u610f\u529b\u6765\u51cf\u5c11\u6587\u672c\u91cd\u590d\u4e0e\u5e7b\u89c9\uff1b3) \u5728\u6f5c\u5728\u4f18\u5316\u6846\u67b6\u4e2d\u8bbe\u8ba1\u4ea4\u53c9\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\uff08\u63d0\u5347\u6587\u672c\u5185\u5bb9\u51c6\u786e\u6027\uff09\u4e0e\u81ea\u6ce8\u610f\u529b\u98ce\u683c\u635f\u5931\uff08\u5b9e\u73b0\u6837\u5f0f\u63a7\u5236\uff09\uff1b\u6b64\u5916\u6784\u5efa\u4e86OmniText-Bench\u8bc4\u6d4b\u6570\u636e\u96c6\u7528\u4e8e\u591a\u6837TIM\u4efb\u52a1\u8bc4\u4f30\u3002", "result": "OmniText\u5728\u5305\u542b\u79fb\u9664\u3001\u91cd\u7f29\u653e\u3001\u91cd\u5b9a\u4f4d\u3001\u63d2\u5165\u4e0e\u7f16\u8f91\u7b49\u591a\u79cd\u4efb\u52a1\u7684OmniText-Bench\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u6587\u672c\u4fee\u590d/\u63d2\u753b\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6216\u63a5\u8fd1\u7684\u6307\u6807\u4e0e\u89c6\u89c9\u6548\u679c\uff0c\u6210\u4e3a\u9996\u4e2a\u80fd\u8986\u76d6\u591a\u79cdTIM\u4efb\u52a1\u7684\u901a\u7528\u65b9\u6cd5\u3002", "conclusion": "OmniText\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u6587\u672c\u56fe\u50cf\u64cd\u4f5c\uff08TIM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u81ea\u6ce8\u610f\u529b\u9006\u8fd0\u7b97\u5b9e\u73b0\u6587\u672c\u53bb\u9664\u3001\u901a\u8fc7\u91cd\u5206\u914d\u4ea4\u53c9\u6ce8\u610f\u529b\u964d\u4f4e\u6587\u672c\u5e7b\u89c9\uff0c\u5e76\u5728\u6f5c\u7a7a\u95f4\u4f18\u5316\u4e2d\u5f15\u5165\u4ea4\u53c9\u6ce8\u610f\u529b\u5185\u5bb9\u635f\u5931\u548c\u81ea\u6ce8\u610f\u529b\u98ce\u683c\u635f\u5931\u4ee5\u5b9e\u73b0\u53ef\u63a7\u6587\u672c\u6e32\u67d3\u4e0e\u6837\u5f0f\u5b9a\u5236\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u548c\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6216\u53ef\u6bd4\u62df\u7684\u6027\u80fd\u3002"}}
{"id": "2510.24105", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24105", "abs": "https://arxiv.org/abs/2510.24105", "authors": ["Shufan Shen", "Zhaobo Qi", "Junshu Sun", "Qingming Huang", "Qi Tian", "Shuhui Wang"], "title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "comment": "ICLR 2025 (Spotlight)", "summary": "The visual representation of a pre-trained model prioritizes the\nclassifiability on downstream tasks, while the widespread applications for\npre-trained visual models have posed new requirements for representation\ninterpretability. However, it remains unclear whether the pre-trained\nrepresentations can achieve high interpretability and classifiability\nsimultaneously. To answer this question, we quantify the representation\ninterpretability by leveraging its correlation with the ratio of interpretable\nsemantics within the representations. Given the pre-trained representations,\nonly the interpretable semantics can be captured by interpretations, whereas\nthe uninterpretable part leads to information loss. Based on this fact, we\npropose the Inherent Interpretability Score (IIS) that evaluates the\ninformation loss, measures the ratio of interpretable semantics, and quantifies\nthe representation interpretability. In the evaluation of the representation\ninterpretability with different classifiability, we surprisingly discover that\nthe interpretability and classifiability are positively correlated, i.e.,\nrepresentations with higher classifiability provide more interpretable\nsemantics that can be captured in the interpretations. This observation further\nsupports two benefits to the pre-trained representations. First, the\nclassifiability of representations can be further improved by fine-tuning with\ninterpretability maximization. Second, with the classifiability improvement for\nthe representations, we obtain predictions based on their interpretations with\nless accuracy degradation. The discovered positive correlation and\ncorresponding applications show that practitioners can unify the improvements\nin interpretability and classifiability for pre-trained vision models. Codes\nare available at https://github.com/ssfgunner/IIS.", "AI": {"tldr": "They introduce IIS to measure how much of a pre-trained visual representation is interpretable (vs. lost); discover interpretability and classifiability are positively correlated, enabling joint improvement via interpretability-focused fine-tuning.", "motivation": "Pre-trained visual models prioritize classifiability but emerging applications require interpretability; unclear whether high interpretability and classifiability can co-exist, so they seek to quantify interpretability and explore its relation with classifiability.", "method": "They define interpretability as ratio of interpretable semantics captured by interpretations and measure information loss when mapping representations to these semantics; propose IIS metric; evaluate across representations with varying classifiability; fine-tune representations to maximize interpretability and test classification accuracy from interpretations.", "result": "IIS effectively quantifies interpretability; surprising positive correlation found: higher classifiability implies more interpretable semantics. Fine-tuning to maximize interpretability improves classifiability, and classifiers built from interpretations suffer less accuracy loss after such improvements.", "conclusion": "The paper proposes Inherent Interpretability Score (IIS) to quantify interpretability of pre-trained visual representations via measuring information loss when mapping to interpretable semantics, and finds a positive correlation between interpretability and classifiability, enabling joint improvements."}}
{"id": "2510.24116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24116", "abs": "https://arxiv.org/abs/2510.24116", "authors": ["Fengming Yu", "Haiwei Pan", "Kejia Zhang", "Jian Guan", "Haiying Jiang"], "title": "UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations", "comment": "14 pages, 4 figures", "summary": "Knowledge distillation (KD) is an effective model compression technique that\ntransfers knowledge from a high-performance teacher to a lightweight student,\nreducing cost while maintaining accuracy. In visual applications, where\nlarge-scale image models are widely used, KD enables efficient deployment.\nHowever, architectural diversity introduces semantic discrepancies that hinder\nthe use of intermediate representations. Most existing KD methods are designed\nfor homogeneous models and degrade in heterogeneous scenarios, especially when\nintermediate features are involved. Prior studies mainly focus on the logits\nspace, making limited use of the semantic information in intermediate layers.\nTo address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)\nis proposed as a framework that leverages intermediate features in the\nfrequency domain for cross-architecture transfer. Fourier transform is applied\nto capture global feature information, alleviating representational\ndiscrepancies between heterogeneous teacher-student pairs. A Feature\nTransformation Module (FTM) produces compact frequency-domain representations\nof teacher features, while a learnable Feature Alignment Module (FAM) projects\nstudent features and aligns them via multi-level matching. Training is guided\nby a joint objective combining mean squared error on intermediate features with\nKullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K\ndemonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD\nas an effective approach for unifying heterogeneous representations and\nenabling efficient utilization of visual knowledge", "AI": {"tldr": "UHKD \u5229\u7528\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u6559\u5e08\u4e2d\u95f4\u7279\u5f81\u8f6c\u4e3a\u9891\u57df\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u53ef\u5b66\u4e60\u5bf9\u9f50\u6a21\u5757\u4e0e\u5b66\u751f\u7279\u5f81\u8fdb\u884c\u591a\u5c42\u5339\u914d\uff0c\u7ed3\u5408 MSE \u4e0e KL \u635f\u5931\uff0c\u5728\u5f02\u6784\u6a21\u578b\u84b8\u998f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709 KD \u591a\u9488\u5bf9\u540c\u6784\u6a21\u578b\uff0c\u5f02\u6784\u6a21\u578b\u95f4\u56e0\u7ed3\u6784\u5dee\u5f02\u5bfc\u81f4\u4e2d\u95f4\u8868\u5f81\u8bed\u4e49\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u4e2d\u95f4\u5c42\u77e5\u8bc6\u7684\u5229\u7528\uff1b\u5e0c\u671b\u901a\u8fc7\u9891\u57df\u8868\u793a\u7edf\u4e00\u8bed\u4e49\u5dee\u5f02\u4ee5\u4fbf\u8de8\u67b6\u6784\u8fc1\u79fb\u3002", "method": "\u5bf9\u6559\u5e08\u4e2d\u95f4\u7279\u5f81\u8fdb\u884c\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4f7f\u7528 Feature Transformation Module \u63d0\u53d6\u7d27\u51d1\u7684\u9891\u57df\u8868\u793a\uff1b\u5b66\u751f\u7aef\u7528\u53ef\u5b66\u4e60\u7684 Feature Alignment Module \u5c06\u5176\u7279\u5f81\u6295\u5f71\u5e76\u8fdb\u884c\u591a\u5c42\u5bf9\u9f50\uff1b\u8bad\u7ec3\u76ee\u6807\u7ed3\u5408\u4e2d\u95f4\u7279\u5f81\u7684\u5747\u65b9\u8bef\u5dee\u548c logits \u7684 KL \u6563\u5ea6\u3002", "result": "\u5728 CIFAR-100 \u548c ImageNet-1K \u4e0a\u76f8\u8f83\u6700\u65b0\u65b9\u6cd5\u5206\u522b\u63d0\u5347\u7ea6 5.59% \u548c 0.83%\uff0c\u8bc1\u660e\u5728\u5f02\u6784\u84b8\u998f\u573a\u666f\u4e0b\u6709\u6548\u3002", "conclusion": "UHKD \u63d0\u51fa\u901a\u8fc7\u9891\u57df\u7279\u5f81\u8de8\u67b6\u6784\u84b8\u998f\uff0c\u80fd\u591f\u7f13\u89e3\u5f02\u6784\u6a21\u578b\u95f4\u7684\u8868\u5f81\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u5347\u5b66\u751f\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2510.24117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24117", "abs": "https://arxiv.org/abs/2510.24117", "authors": ["Zan Wang", "Siyu Chen", "Luya Mo", "Xinfeng Gao", "Yuxin Shen", "Lebin Ding", "Wei Liang"], "title": "DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery", "comment": "19 pages", "summary": "We present DogMo, a large-scale multi-view RGB-D video dataset capturing\ndiverse canine movements for the task of motion recovery from images. DogMo\ncomprises 1.2k motion sequences collected from 10 unique dogs, offering rich\nvariation in both motion and breed. It addresses key limitations of existing\ndog motion datasets, including the lack of multi-view and real 3D data, as well\nas limited scale and diversity. Leveraging DogMo, we establish four motion\nrecovery benchmark settings that support systematic evaluation across monocular\nand multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,\nwe further introduce a three-stage, instance-specific optimization pipeline\nthat fits the SMAL model to the motion sequences. Our method progressively\nrefines body shape and pose through coarse alignment, dense correspondence\nsupervision, and temporal regularization. Our dataset and method provide a\nprincipled foundation for advancing research in dog motion recovery and open up\nnew directions at the intersection of computer vision, computer graphics, and\nanimal behavior modeling.", "AI": {"tldr": "DogMo: 1.2k multi-view RGB-D canine motion sequences (10 dogs). Benchmarks for RGB/RGB-D and mono/multi-view. Method: three-stage instance-specific SMAL optimization (coarse alignment, dense correspondence supervision, temporal regularization) to recover body shape and pose.", "motivation": "Existing dog motion datasets lack multi-view, real 3D data, scale, and breed/motion diversity. DogMo aims to provide large-scale, diverse multi-view RGB-D data and benchmarks to enable systematic evaluation and improved motion recovery methods for dogs.", "method": "Three-stage optimization: 1) Coarse alignment of SMAL to sequences; 2) Dense correspondence supervision using RGB-D and multi-view cues to refine pose and shape; 3) Temporal regularization to ensure smooth, consistent motion across frames.", "result": "Provides DogMo dataset and three-stage SMAL fitting pipeline for dog motion recovery; establishes four benchmark settings for evaluation across monocular/multi-view and RGB/RGB-D.", "conclusion": "DogMo fills gaps in scale, diversity, and true 3D multi-view data for dog motion recovery; combined with the proposed optimization pipeline, it enables accurate instance-specific SMAL fits and standardized benchmarks to advance research."}}
{"id": "2510.24129", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24129", "abs": "https://arxiv.org/abs/2510.24129", "authors": ["Jiajian Xie", "Hubery Yin", "Chen Li", "Zhou Zhao", "Shengyu Zhang"], "title": "ETC: training-free diffusion models acceleration with Error-aware Trend Consistency", "comment": "17 pages, 10 figures", "summary": "Diffusion models have achieved remarkable generative quality but remain\nbottlenecked by costly iterative sampling. Recent training-free methods\naccelerate diffusion process by reusing model outputs. However, these methods\nignore denoising trends and lack error control for model-specific tolerance,\nleading to trajectory deviations under multi-step reuse and exacerbating\ninconsistencies in the generated results. To address these issues, we introduce\nError-aware Trend Consistency (ETC), a framework that (1) introduces a\nconsistent trend predictor that leverages the smooth continuity of diffusion\ntrajectories, projecting historical denoising patterns into stable future\ndirections and progressively distributing them across multiple approximation\nsteps to achieve acceleration without deviating; (2) proposes a model-specific\nerror tolerance search mechanism that derives corrective thresholds by\nidentifying transition points from volatile semantic planning to stable quality\nrefinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX\nwith negligible (-0.074 SSIM score) degradation of consistency.", "AI": {"tldr": "ETC predicts stable denoising trends from past steps and applies model-specific error tolerance thresholds to safely reuse outputs, speeding sampling with minimal consistency loss.", "motivation": "Existing training-free acceleration methods for diffusion reuse model outputs but ignore denoising trends and lack error control, causing deviations and inconsistent generations; ETC addresses these gaps to enable faster, controlled sampling.", "method": "1) Consistent trend predictor: projects historical denoising patterns into future directions and distributes them across steps. 2) Model-specific error tolerance search: finds corrective thresholds separating volatile semantic planning from stable quality refinement to control reuse errors.", "result": "The paper proposes a framework named Error-aware Trend Consistency (ETC) to accelerate diffusion model sampling by reusing model outputs while controlling error and preserving consistency.", "conclusion": "ETC enables multi-step reuse of model outputs without significant trajectory deviation by combining trend prediction and error-tolerance thresholds, achieving substantial acceleration with little SSIM degradation."}}
{"id": "2510.24133", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24133", "abs": "https://arxiv.org/abs/2510.24133", "authors": ["Minsuk Ji", "Sanghyeok Lee", "Namhyuk Ahn"], "title": "Compositional Image Synthesis with Inference-Time Scaling", "comment": "projcet page: https://github.com/gcl-inha/ReFocus", "summary": "Despite their impressive realism, modern text-to-image models still struggle\nwith compositionality, often failing to render accurate object counts,\nattributes, and spatial relations. To address this challenge, we present a\ntraining-free framework that combines an object-centric approach with\nself-refinement to improve layout faithfulness while preserving aesthetic\nquality. Specifically, we leverage large language models (LLMs) to synthesize\nexplicit layouts from input prompts, and we inject these layouts into the image\ngeneration process, where a object-centric vision-language model (VLM) judge\nreranks multiple candidates to select the most prompt-aligned outcome\niteratively. By unifying explicit layout-grounding with self-refine-based\ninference-time scaling, our framework achieves stronger scene alignment with\nprompts compared to recent text-to-image models. The code are available at\nhttps://github.com/gcl-inha/ReFocus.", "AI": {"tldr": "\u8bad\u7ec3\u514d\u8d39\uff0cLLM\u751f\u6210\u5e03\u5c40+\u5bf9\u8c61\u4e2d\u5fc3VLM\u8fed\u4ee3\u91cd\u6392\u884c\uff0c\u63d0\u5347\u5e03\u5c40\u548c\u573a\u666f\u5bf9\u9f50\u3002", "motivation": "\u73b0\u4ee3\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u903c\u771f\u4f46\u6784\u56fe\u80fd\u529b\u5dee\uff0c\u5e38\u9519\u4f4d\u3001\u8ba1\u6570\u9519\u8bef\u6216\u5c5e\u6027/\u7a7a\u95f4\u5173\u7cfb\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u5e03\u5c40\u5fe0\u5b9e\u5ea6\u7684\u65b9\u6848\u3002", "method": "LLM\u4ece\u6587\u672c\u63d0\u793a\u751f\u6210\u663e\u5f0f\u5e03\u5c40\uff08\u5bf9\u8c61\u4f4d\u7f6e/\u5c5e\u6027\uff09\uff1b\u5728\u56fe\u50cf\u751f\u6210\u9636\u6bb5\u6ce8\u5165\u8be5\u5e03\u5c40\uff1b\u751f\u6210\u591a\u5019\u9009\u56fe\u50cf\uff1b\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3VLM\u5bf9\u5019\u9009\u8fdb\u884c\u8bc4\u5206\u5e76\u91cd\u6392\uff0c\u8fed\u4ee3\u9009\u62e9\u6700\u7b26\u5408\u63d0\u793a\u7684\u7ed3\u679c\uff08self-refinement\uff09\u3002", "result": "Improves compositionality in text-to-image generation by using LLMs to create layouts and a VLM-based reranking self-refinement loop; training-free; preserves aesthetics.", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5e03\u5c40\u5f15\u5bfc\u548c\u8fed\u4ee3\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u4e0d\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u589e\u5f3a\u6587\u672c-\u56fe\u50cf\u6a21\u578b\u7684\u6784\u56fe\u4e00\u81f4\u6027\u4e0e\u63d0\u793a\u5bf9\u9f50\u3002"}}
{"id": "2510.24134", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24134", "abs": "https://arxiv.org/abs/2510.24134", "authors": ["Yang Du", "Zhuoran Lin", "Kaiqiang Song", "Biao Wang", "Zhicheng Zheng", "Tiezheng Ge", "Bo Zheng", "Qin Jin"], "title": "VC4VG: Optimizing Video Captions for Text-to-Video Generation", "comment": "Accepted by EMNLP 2025", "summary": "Recent advances in text-to-video (T2V) generation highlight the critical role\nof high-quality video-text pairs in training models capable of producing\ncoherent and instruction-aligned videos. However, strategies for optimizing\nvideo captions specifically for T2V training remain underexplored. In this\npaper, we introduce VC4VG (Video Captioning for Video Generation), a\ncomprehensive caption optimization framework tailored to the needs of T2V\nmodels.We begin by analyzing caption content from a T2V perspective,\ndecomposing the essential elements required for video reconstruction into\nmultiple dimensions, and proposing a principled caption design methodology. To\nsupport evaluation, we construct VC4VG-Bench, a new benchmark featuring\nfine-grained, multi-dimensional, and necessity-graded metrics aligned with\nT2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a\nstrong correlation between improved caption quality and video generation\nperformance, validating the effectiveness of our approach. We release all\nbenchmark tools and code at https://github.com/qyr0403/VC4VG to support further\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411T2V\u7684\u5b57\u5e55\u4f18\u5316\u6846\u67b6VC4VG\uff0c\u8bbe\u8ba1\u591a\u7ef4\u5fc5\u8981\u6027\u523b\u753b\u5e76\u6784\u5efaVC4VG-Bench\u6307\u6807\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6539\u8fdb\u5b57\u5e55\u80fd\u63d0\u5347\u89c6\u9891\u751f\u6210\u3002", "motivation": "Improve training captions for text-to-video generation by designing captions tailored for T2V needs and providing evaluation benchmarks.", "method": "\u4eceT2V\u89d2\u5ea6\u5206\u89e3\u5b57\u5e55\u8981\u7d20\u4e3a\u591a\u4e2a\u7ef4\u5ea6\uff0c\u63d0\u51fa\u5b57\u5e55\u8bbe\u8ba1\u65b9\u6cd5\u5e76\u6784\u5efa\u591a\u7ef4\u5fc5\u8981\u6027\u5206\u7ea7\u8bc4\u6d4b\u57fa\u51c6\uff0c\u57fa\u4e8e\u6b64\u5bf9T2V\u6a21\u578b\u8fdb\u884c\u7ec6\u5316\u8bad\u7ec3\u9a8c\u8bc1\u76f8\u5173\u6027\u3002", "result": "VC4VG framework for caption optimization, VC4VG-Bench benchmark, experiments show better captions improve T2V fine-tuning performance; code released.", "conclusion": "\u9488\u5bf9T2V\u9700\u6c42\u4f18\u5316\u5b57\u5e55\u8d28\u91cf\u80fd\u663e\u8457\u63d0\u9ad8\u751f\u6210\u6548\u679c\uff0cVC4VG\u53ca\u5176\u57fa\u51c6\u5de5\u5177\u53ef\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2510.24136", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24136", "abs": "https://arxiv.org/abs/2510.24136", "authors": ["Ovi Sarkar", "Md Shafiuzzaman", "Md. Faysal Ahamed", "Golam Mahmud", "Muhammad E. H. Chowdhury"], "title": "MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images", "comment": null, "summary": "Colorectal cancer (CRC) is a leading worldwide cause of cancer-related\nmortality, and the role of prompt precise detection is of paramount interest in\nimproving patient outcomes. Conventional diagnostic methods such as colonoscopy\nand histological examination routinely exhibit subjectivity, are extremely\ntime-consuming, and are susceptible to variation. Through the development of\ndigital pathology, deep learning algorithms have become a powerful approach in\nenhancing diagnostic precision and efficiency. In our work, we proposed a\nconvolutional neural network architecture named MSRANetV2, specially optimized\nfor the classification of colorectal tissue images. The model employs a\nResNet50V2 backbone, extended with residual attention mechanisms and\nsqueeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained\nspatial features. With channel alignment and upsampling operations, MSRANetV2\neffectively fuses multi-scale representations, thereby enhancing the robustness\nof the classification. We evaluated our model on a five-fold stratified\ncross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and\nNCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,\nrecall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900\nplus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and\n0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were\n0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,\n0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM\nvisualizations were incorporated to enhance model interpretability by\nhighlighting tissue areas that are medically relevant. These findings validate\nthat MSRANetV2 is a reliable, interpretable, and high-performing architectural\nmodel for classifying CRC tissues.", "AI": {"tldr": "\u63d0\u51faMSRANetV2\u7528\u4e8e\u7ed3\u76f4\u80a0\u7ec4\u7ec7\u5206\u7c7b\uff0c\u7ed3\u5408\u6b8b\u5dee\u6ce8\u610f\u529b\u548cSE\u3001\u591a\u5c3a\u5ea6\u878d\u5408\uff0c5\u6298\u4ea4\u53c9\u9a8c\u8bc1\u5728CRC-VAL-HE-7K\u4e0eNCT-CRC-HE-100K\u4e0a\u5747\u5b9e\u73b0\u224899%\u5206\u7c7b\u6027\u80fd\u5e76\u7528Grad-CAM\u89e3\u91ca\u9884\u6d4b\u3002", "motivation": "\u4e3a\u63d0\u9ad8\u7ed3\u76f4\u80a0\u764c\u75c5\u7406\u56fe\u50cf\u5206\u7c7b\u7684\u7cbe\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u7684\u4e3b\u89c2\u6027\u4e0e\u8017\u65f6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6784\u5efa\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u5316\u5206\u7c7b\u6a21\u578b\u3002", "method": "\u57fa\u4e8eResNet50V2\u9aa8\u5e72\uff0c\u52a0\u5165\u6b8b\u5dee\u6ce8\u610f\u529b\u6a21\u5757\u4e0eSE\u5757\u4ee5\u589e\u5f3a\u901a\u9053\u548c\u7a7a\u95f4\u7279\u5f81\u8868\u793a\uff0c\u901a\u8fc7\u901a\u9053\u5bf9\u9f50\u4e0e\u4e0a\u91c7\u6837\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u4f7f\u75285\u6298\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u7528Grad-CAM\u53ef\u89c6\u5316\u5173\u6ce8\u533a\u57df\u3002", "result": "MSRANetV2\u5728\u4e24\u4efd\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u591a\u9879\u6307\u6807\u5747\u8fbe\u5230\u6216\u63a5\u8fd10.99\uff0c\u8868\u73b0\u975e\u5e38\u4f18\u79c0\uff1b\u65b9\u6cd5\u4e0a\u7ed3\u5408ResNet50V2\u4e3b\u5e72\u3001\u6b8b\u5dee\u6ce8\u610f\u529b\u548cSE\u6a21\u5757\uff0c\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u4e0e\u4e0a\u91c7\u6837\u7528\u4e8e\u589e\u5f3a\u9c81\u68d2\u6027\uff1b\u4f7f\u7528Grad-CAM\u63d0\u5347\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "MSRANetV2\u5728\u516c\u5f00CRC\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u9ad8\u51c6\u786e\u7387\u4e0e\u7a33\u5b9a\u6027\uff0c\u4e14\u901a\u8fc7\u6ce8\u610f\u529b\u548cSE\u6a21\u5757\u63d0\u5347\u7279\u5f81\u8868\u8fbe\u3001\u901a\u8fc7Grad-CAM\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff0c\u5177\u6709\u4e34\u5e8a\u8f85\u52a9\u8bca\u65ad\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u6cdb\u5316\u6027\u4e0e\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u9a8c\u8bc1\u3002"}}
{"id": "2510.24152", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u7684\u7cfb\u7edf\u5316VLM\u63d0\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u5305\u542b\u95ee\u9898\u8def\u7531\u3001\u4efb\u52a1\u4e13\u5c5e\u63d0\u793a\u3001\u53ef\u89c6\u7ec4\u88c5\u548c\u4efb\u52a1\u7ea7\u63a8\u7406\u914d\u7f6e\uff0c\u5728Qwen2.5-VL-72B\u4e0a\u5728\u5e72\u51c0\u548c\u8150\u8d25\u6570\u636e\u4e0a\u5747\u53d6\u5f97\u7ea671-73%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "RoboSense Challenge\u8981\u6c42VLM\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u8de8\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u4e0e\u8150\u8d25\u68c0\u6d4b\u4efb\u52a1\u8868\u73b0\u53ef\u9760\uff0c\u6545\u9700\u8bbe\u8ba1\u80fd\u5904\u7406\u591a\u6837\u95ee\u9898\u7c7b\u578b\u3001\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u5e76\u6297\u8150\u8d25\u7684\u7ed3\u6784\u5316\u63d0\u793a\u4e0e\u89c6\u89c9\u9884\u5904\u7406\u65b9\u6848\u3002", "method": "\u56db\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) Mixture-of-Prompts\u8def\u7531\u5668\u5bf9\u95ee\u9898\u5206\u7c7b\u5e76\u5206\u53d1\u81f3\u4efb\u52a1\u4e13\u5bb6\u63d0\u793a\uff1b2) \u4efb\u52a1\u4e13\u5c5e\u63d0\u793a\u5d4c\u5165\u5750\u6807\u7cfb\u3001\u7a7a\u95f4\u63a8\u7406\u89c4\u5219\u3001\u89d2\u8272\u626e\u6f14\u3001CoT/ToT\u4e0e\u5c11\u6837\u672c\u793a\u4f8b\uff1b3) \u53ef\u89c6\u7ec4\u88c5\u6a21\u5757\u6839\u636e\u95ee\u9898\u9700\u6c42\u5408\u6210\u591a\u89c6\u56fe\u56fe\u50cf\u3001\u76ee\u6807\u88c1\u526a\u3001\u6807\u8bb0\u4e0e\u81ea\u9002\u5e94\u5386\u53f2\u5e27\uff1b4) \u4e3a\u6bcf\u4e2a\u4efb\u52a1\u8c03\u4f18\u63a8\u7406\u53c2\u6570\uff08\u6e29\u5ea6\u3001top-p\u3001\u6d88\u606f\u89d2\u8272\uff09\u3002\u5728Qwen2.5-VL-72B\u4e0a\u5b9e\u73b0\u5e76\u8bc4\u4f30\u3002", "result": "\u5728Qwen2.5-VL-72B\u4e0a\uff0cPhase-1\uff08\u5e72\u51c0\u6570\u636e\uff09\u5e73\u5747\u51c6\u786e\u738770.87%\uff0cPhase-2\uff08\u8150\u8d25\u6570\u636e\uff09\u5e73\u5747\u51c6\u786e\u738772.85%\uff0c\u8bc1\u660e\u65b9\u6cd5\u5728\u566a\u58f0/\u8150\u8d25\u6761\u4ef6\u4e0b\u4ecd\u7a33\u5065\uff1b\u4ee3\u7801\u4e0e\u63d0\u793a\u5df2\u5f00\u6e90\u3002", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u4e0e\u7a7a\u95f4\u5236\u56fe\u663e\u8457\u63d0\u5347\u4e86VLM\u5728\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u4e0e\u8150\u8d25\u68c0\u6d4b\u7b49\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1b\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e24\u4e2a\u9636\u6bb5\u5747\u8868\u73b0\u7a33\u5065\uff0c\u8bc1\u660e\u4e86\u63d0\u793a\u8def\u7531\u3001\u4efb\u52a1\u4e13\u5c5e\u63d0\u793a\u548c\u89c6\u89c9\u7ec4\u88c5\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2510.24195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24195", "abs": "https://arxiv.org/abs/2510.24195", "authors": ["Ziqi Zhou", "Yifan Hu", "Yufei Song", "Zijing Li", "Shengshan Hu", "Leo Yu Zhang", "Dezhong Yao", "Long Zheng", "Hai Jin"], "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2", "comment": "Accepted by NeurIPS 2025", "summary": "Recent studies reveal the vulnerability of the image segmentation foundation\nmodel SAM to adversarial examples. Its successor, SAM2, has attracted\nsignificant attention due to its strong generalization capability in video\nsegmentation. However, its robustness remains unexplored, and it is unclear\nwhether existing attacks on SAM can be directly transferred to SAM2. In this\npaper, we first analyze the performance gap of existing attacks between SAM and\nSAM2 and highlight two key challenges arising from their architectural\ndifferences: directional guidance from the prompt and semantic entanglement\nacross consecutive frames. To address these issues, we propose UAP-SAM2, the\nfirst cross-prompt universal adversarial attack against SAM2 driven by dual\nsemantic deviation. For cross-prompt transferability, we begin by designing a\ntarget-scanning strategy that divides each frame into k regions, each randomly\nassigned a prompt, to reduce prompt dependency during optimization. For\neffectiveness, we design a dual semantic deviation framework that optimizes a\nUAP by distorting the semantics within the current frame and disrupting the\nsemantic consistency across consecutive frames. Extensive experiments on six\ndatasets across two segmentation tasks demonstrate the effectiveness of the\nproposed method for SAM2. The comparative results show that UAP-SAM2\nsignificantly outperforms state-of-the-art (SOTA) attacks by a large margin.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56fe\u50cf\u5206\u5272\u57fa\u7840\u6a21\u578bSAM2\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411SAM2\u7684\u8de8\u63d0\u793a\uff08cross-prompt\uff09\u901a\u7528\u5bf9\u6297\u6270\u52a8UAP-SAM2\u3002\u65b9\u6cd5\u901a\u8fc7\u76ee\u6807\u626b\u63cf\u7b56\u7565\u51cf\u5c11\u63d0\u793a\u4f9d\u8d56\uff0c\u5e76\u6784\u5efa\u53cc\u91cd\u8bed\u4e49\u504f\u79bb\u76ee\u6807\u51fd\u6570\uff1a\u5728\u5f53\u524d\u5e27\u6270\u4e71\u8bed\u4e49\u5e76\u7834\u574f\u76f8\u90bb\u5e27\u4e4b\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u63d0\u5347\u8de8\u63d0\u793a\u548c\u8de8\u5e27\u7684\u653b\u51fb\u8f6c\u79fb\u6027\u3002\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u7c7b\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u653b\u51fb\u3002", "motivation": "\u5c3d\u7ba1SAM2\u5728\u89c6\u9891\u5206\u5272\u4e0a\u6cdb\u5316\u80fd\u529b\u5f3a\uff0c\u4f46\u5176\u9c81\u68d2\u6027\u672a\u77e5\uff0c\u4e14\u73b0\u6709\u9488\u5bf9SAM\u7684\u653b\u51fb\u80fd\u5426\u8fc1\u79fb\u5230SAM2\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3b\u8981\u53d7\u63d0\u793a\u65b9\u5411\u6027\u548c\u5e27\u95f4\u8bed\u4e49\u7ea0\u7f20\u5f71\u54cd\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u80fd\u5904\u7406\u63d0\u793a\u4e0e\u65f6\u5e8f\u8bed\u4e49\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u76ee\u6807\u626b\u63cf\u7b56\u7565\u5c06\u5e27\u5212\u5206\u4e3ak\u4e2a\u533a\u57df\u5e76\u968f\u673a\u5206\u914d\u63d0\u793a\u4ee5\u964d\u4f4e\u63d0\u793a\u4f9d\u8d56\uff1b\u6784\u5efa\u53cc\u91cd\u8bed\u4e49\u504f\u79bb\uff08dual semantic deviation\uff09\u635f\u5931\uff0c\u5305\u62ec\u5728\u5f53\u524d\u5e27\u4e0a\u626d\u66f2\u8bed\u4e49\u548c\u7834\u574f\u76f8\u90bb\u5e27\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027\uff1b\u57fa\u4e8e\u6b64\u4f18\u5316\u901a\u7528\u5bf9\u6297\u6270\u52a8\uff08UAP\uff09\u3002", "result": "\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u4e24\u79cd\u5206\u5272\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUAP-SAM2\u5728\u6210\u529f\u7387\u548c\u7834\u574f\u6548\u679c\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709SOTA\u653b\u51fb\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "UAP-SAM2\u80fd\u6709\u6548\u653b\u51fbSAM2\uff0c\u5728\u8de8\u63d0\u793a\u548c\u8de8\u5e27\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u8f6c\u79fb\u6027\u548c\u7834\u574f\u6027\uff0c\u660e\u663e\u8d85\u8fc7\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u8868\u660eSAM2\u4e5f\u5b58\u5728\u663e\u8457\u7684\u5bf9\u6297\u8106\u5f31\u6027\u3002"}}
{"id": "2510.24202", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24202", "abs": "https://arxiv.org/abs/2510.24202", "authors": ["Anshul Kaushal", "Kunal Jangid", "Vinod K. Kurmi"], "title": "CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation", "comment": "The 36th British Machine Vision Conference (BMVC) 2025", "summary": "Accurate polyp and cardiac segmentation for early detection and treatment is\nessential for the diagnosis and treatment planning of cancer-like diseases.\nTraditional convolutional neural network (CNN) based models have represented\nlimited generalizability, robustness, and inability to handle uncertainty,\nwhich affects the segmentation performance. To solve these problems, this paper\nintroduces CLFSeg, an encoder-decoder based framework that aggregates the\nFuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy\nlogic. This module enhances the segmentation performance by identifying local\nand global features while minimizing the uncertainty, noise, and ambiguity in\nboundary regions, ensuring computing efficiency. In order to handle class\nimbalance problem while focusing on the areas of interest with tiny and\nboundary regions, binary cross-entropy (BCE) with dice loss is incorporated.\nOur proposed model exhibits exceptional performance on four publicly available\ndatasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.\nExtensive experiments and visual studies show CLFSeg surpasses the existing\nSOTA performance and focuses on relevant regions of interest in anatomical\nstructures. The proposed CLFSeg improves performance while ensuring computing\nefficiency, which makes it a potential solution for real-world medical\ndiagnostic scenarios. Project page is available at\nhttps://visdomlab.github.io/CLFSeg/", "AI": {"tldr": "\u63d0\u51faCLFSeg\uff1a\u5728\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4e2d\u96c6\u6210\u6a21\u7cca-\u5377\u79ef\u6a21\u5757\u4ee5\u63d0\u5347\u8fb9\u754c\u5904\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\uff0c\u7ed3\u5408BCE+Dice\u635f\u5931\uff0c\u63d0\u5347\u606f\u8089\u4e0e\u5fc3\u810f\u5206\u5272\u6027\u80fd\u5e76\u4fdd\u8bc1\u8ba1\u7b97\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709SOTA\u3002", "motivation": "\u4f20\u7edfCNN\u5728\u6cdb\u5316\u3001\u9c81\u68d2\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u4e0a\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u8fb9\u754c\u548c\u5c0f\u76ee\u6807\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u9700\u63d0\u9ad8\u7cbe\u786e\u5ea6\u4e0e\u7a33\u5b9a\u6027\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u8bca\u65ad\u9700\u6c42\u3002", "method": "\u63d0\u51faFuzzy-Convolutional\u6a21\u5757\uff0c\u5c06\u5377\u79ef\u7279\u5f81\u4e0e\u6a21\u7cca\u903b\u8f91\u8fd0\u7b97\u7ed3\u5408\u4ee5\u51cf\u5c11\u8fb9\u754c\u566a\u58f0\u4e0e\u4e0d\u786e\u5b9a\u6027\uff0c\u96c6\u6210\u4e8e\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\u5e76\u4ee5BCE+Dice\u8054\u5408\u635f\u5931\u8bad\u7ec3\u3002\u8fdb\u884c\u4e86\u5927\u91cf\u6d88\u878d\u4e0e\u53ef\u89c6\u5316\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "The paper proposes CLFSeg, an encoder-decoder segmentation framework that integrates a Fuzzy-Convolutional (FC) module combining convolutional layers and fuzzy logic to enhance local/global feature extraction and reduce uncertainty at boundaries, improving robustness and efficiency. It uses a combined BCE with Dice loss to address class imbalance and small/boundary regions. The model outperforms SOTA on four datasets (three polyp datasets and ACDC cardiac dataset) and is efficient for real-world use.", "conclusion": "CLFSeg\u901a\u8fc7\u878d\u5408\u6a21\u7cca\u903b\u8f91\u4e0e\u5377\u79ef\uff0c\u6539\u5584\u4e86\u5206\u5272\u6a21\u578b\u5728\u4e0d\u786e\u5b9a\u8fb9\u754c\u548c\u5c0f\u76ee\u6807\u5904\u7684\u8868\u73b0\uff0c\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u5177\u5907\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.24211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24211", "abs": "https://arxiv.org/abs/2510.24211", "authors": ["Junhyuk So", "Hyunho Kook", "Chaeyeon Jang", "Eunhyeok Park"], "title": "MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration", "comment": null, "summary": "While autoregressive (AR) modeling has recently emerged as a new paradigm in\nvisual generation, its practical adoption is severely constrained by the slow\ninference speed of per-token generation, which often requires thousands of\nsteps to produce a single sample. To address this challenge, we propose MC-SJD,\na training-free, lossless parallel decoding framework designed to accelerate AR\nvisual generation by extending the recently introduced Speculative Jacobi\nDecoding (SJD). Although SJD shows strong potential for accelerating AR\ngeneration, we demonstrate that token instability across iterations\nsignificantly reduces the acceptance rate, a limitation that primarily arises\nfrom the independent sampling process used during draft token generation. To\novercome this, we introduce MC-SJD, an information-theoretic approach based on\ncoupling, which substantially accelerates standard SJD by maximizing the\nprobability of sampling identical draft tokens across consecutive iterations,\nall while preserving its lossless property. Remarkably, this method requires\nonly a single-line modification to the existing algorithm, yet achieves\nsubstantial performance gains, delivering up to a ~4.2x acceleration in image\ngeneration and ~13.3x acceleration in video generation compared to standard AR\ndecoding, without any degradation in output quality.", "AI": {"tldr": "MC-SJD: single-line, training-free coupling change to SJD that stabilizes draft tokens and yields large, lossless parallel decoding speedups for images (~4.2x) and videos (~13.3x).", "motivation": "AR visual generation is slow due to per-token sequential sampling; prior SJD accelerates but suffers low acceptance due to token instability from independent draft sampling.", "method": "Speculative Jacobi Decoding (SJD) extended by Monte Carlo coupling", "result": "Introduce MC-SJD, a coupling-based information-theoretic modification that maximizes probability of identical draft tokens across iterations, preserving lossless decoding and requires one-line change; achieves up to ~4.2x image and ~13.3x video speedups over AR decoding without quality loss.", "conclusion": "Coupling draft token sampling across iterations greatly raises acceptance rates and parallelism of SJD, enabling substantial, quality-preserving acceleration of AR visual generation with minimal implementation changes."}}
{"id": "2510.24213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24213", "abs": "https://arxiv.org/abs/2510.24213", "authors": ["Haoxin Yang", "Yihong Lin", "Jingdan Kang", "Xuemiao Xu", "Yue Li", "Cheng Xu", "Shengfeng He"], "title": "Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization", "comment": null, "summary": "Face anonymization aims to conceal identity information while preserving\nnon-identity attributes. Mainstream diffusion models rely on inference-time\ninterventions such as negative guidance or energy-based optimization, which are\napplied post-training to suppress identity features. These interventions often\nintroduce distribution shifts and entangle identity with non-identity\nattributes, degrading visual fidelity and data utility. To address this, we\npropose \\textbf{ID\\textsuperscript{2}Face}, a training-centric anonymization\nframework that removes the need for inference-time optimization. The rationale\nof our method is to learn a structured latent space where identity and\nnon-identity information are explicitly disentangled, enabling direct and\ncontrollable anonymization at inference. To this end, we design a conditional\ndiffusion model with an identity-masked learning scheme. An Identity-Decoupled\nLatent Recomposer uses an Identity Variational Autoencoder to model identity\nfeatures, while non-identity attributes are extracted from same-identity pairs\nand aligned through bidirectional latent alignment. An Identity-Guided Latent\nHarmonizer then fuses these representations via soft-gating conditioned on\nnoisy feature prediction. The model is trained with a recomposition-based\nreconstruction loss to enforce disentanglement. At inference, anonymization is\nachieved by sampling a random identity vector from the learned identity space.\nTo further suppress identity leakage, we introduce an Orthogonal Identity\nMapping strategy that enforces orthogonality between sampled and source\nidentity vectors. Experiments demonstrate that ID\\textsuperscript{2}Face\noutperforms existing methods in visual quality, identity suppression, and\nutility preservation.", "AI": {"tldr": "Training-time disentangled latent diffusion for face anonymization enabling direct anonymization by sampling identity vectors, avoiding inference-time optimization and improving quality and utility.", "motivation": "The paper aims to avoid inference-time interventions for face anonymization which cause distribution shifts and entangle attributes; instead they propose a training-centric approach to disentangle identity and non-identity attributes in latent space.", "method": "analysis of method", "result": "ID2Face (ID^2Face) designs a conditional diffusion model with identity-masked learning, an Identity-Decoupled Latent Recomposer using an Identity VAE, bidirectional latent alignment, an Identity-Guided Latent Harmonizer with soft-gating, recomposition-based reconstruction loss, and Orthogonal Identity Mapping; at inference they sample random identity vectors for anonymization; experiments show improvements in visual quality, identity suppression, and utility preservation.", "conclusion": "ID^2Face provides a structured latent space separating identity and non-identity, enabling controllable anonymization and outperforming prior methods."}}
{"id": "2510.24214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24214", "abs": "https://arxiv.org/abs/2510.24214", "authors": ["Jinhong Deng", "Wen Li", "Joey Tianyi Zhou", "Yang He"], "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs", "comment": "NeurIPS 2025", "summary": "Multimodal Large Language Models (MLLMs) typically process a large number of\nvisual tokens, leading to considerable computational overhead, even though many\nof these tokens are redundant. Existing visual token pruning methods primarily\nfocus on selecting the most salient tokens based on attention scores, resulting\nin the semantic incompleteness of the selected tokens. In this paper, we\npropose a novel visual token pruning strategy, called\n\\textbf{S}aliency-\\textbf{C}overage \\textbf{O}riented token \\textbf{P}runing\nfor \\textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and\ncoverage of the selected visual tokens to better preserve semantic\ncompleteness. Specifically, we introduce a set-coverage for a given set of\nselected tokens, computed based on the token relationships. We then define a\ntoken-coverage gain for each unselected token, quantifying how much additional\ncoverage would be obtained by including it. By integrating the saliency score\ninto the token-coverage gain, we propose our SCOPE score and iteratively select\nthe token with the highest SCOPE score. We conduct extensive experiments on\nmultiple vision-language understanding benchmarks using the LLaVA-1.5 and\nLLaVA-Next models. Experimental results demonstrate that our method\nconsistently outperforms prior approaches. Our code is available at\n\\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.", "AI": {"tldr": "SCOPE prunes visual tokens by combining saliency and coverage: compute coverage via token relationships, define coverage gain for unselected tokens, integrate saliency into gain to score tokens, and iteratively pick highest-scoring tokens; outperforms prior methods on LLaVA variants.", "motivation": "Many visual tokens in MLLMs are redundant, and selecting only high-attention tokens leads to semantic gaps; need to consider both importance (saliency) and semantic completeness (coverage) when pruning tokens.", "method": "Compute set-coverage for selected tokens using token relationships; for each unselected token compute token-coverage gain (how much new coverage it adds); integrate saliency score into coverage gain to form SCOPE score; iteratively select highest SCOPE-scored token until budget.", "result": "The paper introduces SCOPE, a visual token pruning strategy for MLLMs that jointly models saliency and coverage to preserve semantic completeness, using a set-coverage measure and token-coverage gain integrated into a SCOPE score for iterative token selection.", "conclusion": "SCOPE achieves better performance than previous visual token pruning methods by preserving semantic completeness through joint saliency-coverage modeling and iterative selection based on SCOPE score."}}
{"id": "2510.24231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24231", "abs": "https://arxiv.org/abs/2510.24231", "authors": ["Waseem Shariff", "Timothy Hanley", "Maciej Stec", "Hossein Javidnia", "Peter Corcoran"], "title": "Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation", "comment": "Accepted in British Machine Vision Conference (BMVC) 2025, Main\n  Conference", "summary": "Microsaccades are small, involuntary eye movements vital for visual\nperception and neural processing. Traditional microsaccade studies typically\nuse eye trackers or frame-based analysis, which, while precise, are costly and\nlimited in scalability and temporal resolution. Event-based sensing offers a\nhigh-speed, low-latency alternative by capturing fine-grained spatiotemporal\nchanges efficiently. This work introduces a pioneering event-based microsaccade\ndataset to support research on small eye movement dynamics in cognitive\ncomputing. Using Blender, we render high-fidelity eye movement scenarios and\nsimulate microsaccades with angular displacements from 0.5 to 2.0 degrees,\ndivided into seven distinct classes. These are converted to event streams using\nv2e, preserving the natural temporal dynamics of microsaccades, with durations\nranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,\nSpiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an\noptical-flow-enhanced variant implemented in SpikingJelly. The models achieve\naround 90 percent average accuracy, successfully classifying microsaccades by\nangular displacement, independent of event count or duration. These results\ndemonstrate the potential of spiking neural networks for fine motion\nrecognition and establish a benchmark for event-based vision research. The\ndataset, code, and trained models will be publicly available at\nhttps://waseemshariff126.github.io/microsaccades/ .", "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u626b\u89c6\u6570\u636e\u96c6\uff0c\u901a\u8fc7Blender\u4eff\u771f\u773c\u7403\u8fd0\u52a8\u5e76\u7528v2e\u8f6c\u4e3a\u4e8b\u4ef6\u6d41\uff0c\u8986\u76d60.5\u20132.0\u5ea6\u30010.25\u20132.25ms\u7684\u4e03\u7c7b\u4f4d\u79fb\uff1b\u7528Spiking-VGG\u7cfb\u5217\u4e0e\u65b0\u589e\u5149\u6d41\u589e\u5f3a\u6a21\u578bSpiking-VGG16Flow\u5728SpikingJelly\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5747\u53d6\u5f97\u7ea690%\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u7ec6\u5fae\u8fd0\u52a8\u8bc6\u522b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5c06\u5f00\u6e90\u6570\u636e\u4e0e\u4ee3\u7801\u3002", "motivation": "\u73b0\u6709\u5fae\u626b\u89c6\u7814\u7a76\u591a\u4f9d\u8d56\u9ad8\u6210\u672c\u773c\u52a8\u4eea\u6216\u5e27\u7ea7\u5206\u6790\uff0c\u53d7\u91c7\u6837\u7387\u4e0e\u65f6\u5ef6\u9650\u5236\u3002\u4e8b\u4ef6\u76f8\u673a\u4ee5\u9ad8\u65f6\u95f4\u5206\u8fa8\u7387\u4e0e\u4f4e\u5ef6\u8fdf\u63d0\u4f9b\u6355\u83b7\u7ec6\u5fae\u3001\u5feb\u901f\u773c\u52a8\u7684\u65b0\u9014\u5f84\uff0c\u6709\u671b\u6269\u5927\u53ef\u6d4b\u89c4\u6a21\u5e76\u63d0\u5347\u5bf9\u5c0f\u5e45\u8fd0\u52a8\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u5728Blender\u4e2d\u6e32\u67d3\u9ad8\u4fdd\u771f\u773c\u7403\u573a\u666f\u5e76\u6309\u8bbe\u5b9a\u89d2\u4f4d\u79fb\u751f\u6210\u4e03\u7c7b\u5fae\u626b\u89c6\u5e8f\u5217\uff1b\u7528v2e\u5c06\u5e27\u7ea7\u6e32\u67d3\u8f6c\u6210\u4e8b\u4ef6\u6d41\uff08\u4fdd\u7559\u65f6\u95f4\u52a8\u6001\uff0c\u6301\u7eed0.25\u20132.25ms\uff09\uff1b\u57fa\u4e8eSpikingJelly\u5b9e\u73b0Spiking-VGG11/13/16\uff0c\u5e76\u63d0\u51fa\u5305\u542b\u5149\u6d41\u6a21\u5757\u7684Spiking-VGG16Flow\uff1b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u4f7f\u7528\u4e8b\u4ef6\u6570\u636e\u5206\u7c7b\u89d2\u4f4d\u79fb\u3002", "result": "\u6784\u5efa\u5e76\u516c\u5f00\u9996\u4e2a\u4e8b\u4ef6\u578b\u5fae\u626b\u89c6\u6570\u636e\u96c6\uff08\u4e03\u7c7b\uff0c0.5\u20132.0\u00b0\uff0c0.25\u20132.25ms\uff09\uff1bSpiking-VGG\u7cfb\u5217\u4e0eSpiking-VGG16Flow\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u7ea690%\uff0c\u5206\u7c7b\u4e0e\u4e8b\u4ef6\u6570\u91cf\u6216\u6301\u7eed\u65f6\u957f\u65e0\u663e\u8457\u76f8\u5173\u6027\u3002", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u5fae\u626b\u89c6\u6570\u636e\u96c6\u4e0eSpiking-VGG16Flow\u6a21\u578b\u80fd\u7a33\u5b9a\u5206\u7c7b\u5fae\u5c0f\u89d2\u4f4d\u79fb\uff080.5\u20132.0\u00b0\uff09\uff0c\u5b9e\u73b0\u7ea690%\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u4e8b\u4ef6\u611f\u77e5+\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u5728\u7cbe\u7ec6\u8fd0\u52a8\u8bc6\u522b\u7684\u6f5c\u529b\u5e76\u63d0\u4f9b\u57fa\u51c6\u4f9b\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.24232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24232", "abs": "https://arxiv.org/abs/2510.24232", "authors": ["Qing Zhao", "Weijian Deng", "Pengxu Wei", "ZiYi Dong", "Hannan Lu", "Xiangyang Ji", "Liang Lin"], "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy", "comment": "NeurIPS 2025", "summary": "To improve detection robustness in adverse conditions (e.g., haze and low\nlight), image restoration is commonly applied as a pre-processing step to\nenhance image quality for the detector. However, the functional mismatch\nbetween restoration and detection networks can introduce instability and hinder\neffective integration -- an issue that remains underexplored. We revisit this\nlimitation through the lens of Lipschitz continuity, analyzing the functional\ndifferences between restoration and detection networks in both the input space\nand the parameter space. Our analysis shows that restoration networks perform\nsmooth, continuous transformations, while object detectors operate with\ndiscontinuous decision boundaries, making them highly sensitive to minor\nperturbations. This mismatch introduces instability in traditional cascade\nframeworks, where even imperceptible noise from restoration is amplified during\ndetection, disrupting gradient flow and hindering optimization. To address\nthis, we propose Lipschitz-regularized object detection (LROD), a simple yet\neffective framework that integrates image restoration directly into the\ndetector's feature learning, harmonizing the Lipschitz continuity of both tasks\nduring training. We implement this framework as Lipschitz-regularized YOLO\n(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive\nexperiments on haze and low-light benchmarks demonstrate that LR-YOLO\nconsistently improves detection stability, optimization smoothness, and overall\naccuracy.", "AI": {"tldr": "Propose LROD that enforces Lipschitz continuity alignment between restoration and detection; implement as LR-YOLO, improving robustness in adverse conditions", "motivation": "Existing cascade of restoration then detection suffers from functional mismatch; restoration smooth, detector discontinuous => instability", "method": "Lipschitz-regularized YOLO for adverse conditions", "result": "LR-YOLO improves stability, optimization, and accuracy on haze and low-light benchmarks", "conclusion": "Integrating restoration into detector with Lipschitz regularization harmonizes transformations, reduces noise amplification and training instability, yielding better detection performance"}}
{"id": "2510.24260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24260", "abs": "https://arxiv.org/abs/2510.24260", "authors": ["Zhaotong Yang", "Yi Chen", "Yanying Li", "Shengfeng He", "Yangyang Xu", "Junyu Dong", "Jian Yang", "Yong Du"], "title": "DeshadowMamba: Deshadowing as 1D Sequential Similarity", "comment": null, "summary": "Recent deep models for image shadow removal often rely on attention-based\narchitectures to capture long-range dependencies. However, their fixed\nattention patterns tend to mix illumination cues from irrelevant regions,\nleading to distorted structures and inconsistent colors. In this work, we\nrevisit shadow removal from a sequence modeling perspective and explore the use\nof Mamba, a selective state space model that propagates global context through\ndirectional state transitions. These transitions yield an efficient global\nreceptive field while preserving positional continuity. Despite its potential,\ndirectly applying Mamba to image data is suboptimal, since it lacks awareness\nof shadow-non-shadow semantics and remains susceptible to color interference\nfrom nearby regions. To address these limitations, we propose CrossGate, a\ndirectional modulation mechanism that injects shadow-aware similarity into\nMamba's input gate, allowing selective integration of relevant context along\ntransition axes. To further ensure appearance fidelity, we introduce ColorShift\nregularization, a contrastive learning objective driven by global color\nstatistics. By synthesizing structured informative negatives, it guides the\nmodel to suppress color contamination and achieve robust color restoration.\nTogether, these components adapt sequence modeling to the structural integrity\nand chromatic consistency required for shadow removal. Extensive experiments on\npublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-art\nvisual quality and strong quantitative performance.", "AI": {"tldr": "\u5c06\u5e8f\u5217\u6a21\u578bMamba\u6539\u9020\u7528\u4e8e\u56fe\u50cf\u53bb\u9634\u5f71\uff0c\u901a\u8fc7CrossGate\u6ce8\u5165\u9634\u5f71\u611f\u77e5\u76f8\u4f3c\u6027\uff0c\u548cColorShift\u5bf9\u6bd4\u6b63\u5219\u5316\u6291\u5236\u989c\u8272\u6c61\u67d3\uff0c\u5b9e\u73b0\u7ed3\u6784\u4e0e\u8272\u5f69\u4fdd\u771f\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u6a21\u578b\u56fa\u5b9a\u6ce8\u610f\u6a21\u5f0f\u6613\u6df7\u5165\u65e0\u5173\u533a\u57df\u7684\u5149\u7167\u4fe1\u606f\uff0c\u5bfc\u81f4\u7ed3\u6784\u5931\u771f\u548c\u989c\u8272\u4e0d\u4e00\u81f4\uff0c\u56e0\u800c\u63a2\u7d22\u5e8f\u5217\u5efa\u6a21\u7684\u5b9a\u5411\u72b6\u6001\u7a7a\u95f4\u4ee5\u66f4\u597d\u63a7\u5236\u4e0a\u4e0b\u6587\u4f20\u64ad\u3002", "method": "\u7528Mamba\u7684\u65b9\u5411\u6027\u72b6\u6001\u8f6c\u79fb\u4f20\u64ad\u5168\u5c40\u4e0a\u4e0b\u6587\uff1bCrossGate\u5728\u8f93\u5165\u95e8\u4e2d\u6309\u65b9\u5411\u9009\u62e9\u6027\u878d\u5408\u4e0e\u9634\u5f71\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff1bColorShift\u901a\u8fc7\u6784\u9020\u6709\u4fe1\u606f\u7684\u8d1f\u6837\u672c\u53ca\u57fa\u4e8e\u5168\u5c40\u989c\u8272\u7edf\u8ba1\u7684\u5bf9\u6bd4\u635f\u5931\u7ea6\u675f\u8272\u5f69\u6062\u590d\u3002", "result": "Paper proposes DeshadowMamba: uses Mamba state space model with CrossGate and ColorShift regularization for shadow removal", "conclusion": "\u5f15\u5165CrossGate\u548cColorShift\u540e\uff0cMamba\u80fd\u66f4\u597d\u4fdd\u6301\u7ed3\u6784\u8fde\u7eed\u6027\u4e0e\u8272\u5f69\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u53bb\u9634\u5f71\u89c6\u89c9\u548c\u91cf\u5316\u6307\u6807\u3002"}}
{"id": "2510.24262", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24262", "abs": "https://arxiv.org/abs/2510.24262", "authors": ["Jiyu Guo", "Shuo Yang", "Yiming Huang", "Yancheng Long", "Xiaobo Xia", "Xiu Su", "Bo Zhao", "Zeke Xie", "Liqiang Nie"], "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025)", "summary": "Data augmentation using generative models has emerged as a powerful paradigm\nfor enhancing performance in computer vision tasks. However, most existing\naugmentation approaches primarily focus on optimizing intrinsic data attributes\n-- such as fidelity and diversity -- to generate visually high-quality\nsynthetic data, while often neglecting task-specific requirements. Yet, it is\nessential for data generators to account for the needs of downstream tasks, as\ntraining data requirements can vary significantly across different tasks and\nnetwork architectures. To address these limitations, we propose UtilGen, a\nnovel utility-centric data augmentation framework that adaptively optimizes the\ndata generation process to produce task-specific, high-utility training data\nvia downstream task feedback. Specifically, we first introduce a weight\nallocation network to evaluate the task-specific utility of each synthetic\nsample. Guided by these evaluations, UtilGen iteratively refines the data\ngeneration process using a dual-level optimization strategy to maximize the\nsynthetic data utility: (1) model-level optimization tailors the generative\nmodel to the downstream task, and (2) instance-level optimization adjusts\ngeneration policies -- such as prompt embeddings and initial noise -- at each\ngeneration round. Extensive experiments on eight benchmark datasets of varying\ncomplexity and granularity demonstrate that UtilGen consistently achieves\nsuperior performance, with an average accuracy improvement of 3.87% over\nprevious SOTA. Further analysis of data influence and distribution reveals that\nUtilGen produces more impactful and task-relevant synthetic data, validating\nthe effectiveness of the paradigm shift from visual characteristics-centric to\ntask utility-centric data augmentation.", "AI": {"tldr": "\u63d0\u51faUtilGen\uff1a\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\u53cd\u9988\u5b9e\u884c\u751f\u6210\u5668\u548c\u6837\u672c\u7ea7\u53cc\u91cd\u4f18\u5316\uff0c\u6309\u6837\u672c\u5206\u914d\u6743\u91cd\u4ee5\u751f\u6210\u5bf9\u4e0b\u6e38\u4efb\u52a1\u66f4\u6709\u7528\u7684\u5408\u6210\u6570\u636e\uff0c\u5e73\u5747\u63d0\u53473.87%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6570\u636e\u589e\u5f3a\u591a\u5173\u6ce8\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0e\u591a\u6837\u6027\uff0c\u5ffd\u89c6\u4e0b\u6e38\u4efb\u52a1\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u7279\u5b9a\u9700\u6c42\uff1b\u4e0d\u540c\u4efb\u52a1\u548c\u67b6\u6784\u5bf9\u6570\u636e\u7684\u9700\u6c42\u5dee\u5f02\u5927\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4ee5\u4efb\u52a1\u6548\u7528\u4e3a\u4e2d\u5fc3\u7684\u751f\u6210\u6570\u636e\u4f18\u5316\u6846\u67b6\u3002", "method": "\u5f15\u5165\u6743\u91cd\u5206\u914d\u7f51\u7edc\u8bc4\u4f30\u6bcf\u4e2a\u5408\u6210\u6837\u672c\u7684\u4e0b\u6e38\u4efb\u52a1\u6548\u7528\uff1b\u5728\u6a21\u578b\u7ea7\u901a\u8fc7\u5bf9\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u4ee5\u53ca\u5728\u5b9e\u4f8b\u7ea7\u901a\u8fc7\u8c03\u6574prompt\u5d4c\u5165\u548c\u521d\u59cb\u566a\u58f0\u7b49\u751f\u6210\u7b56\u7565\uff0c\u8fdb\u884c\u53cc\u5c42\u8fed\u4ee3\u4f18\u5316\u6765\u6700\u5927\u5316\u6837\u672c\u6548\u7528\u3002", "result": "UtilGen proposes a utility-driven data augmentation framework using generative models to produce task-specific synthetic data, guided by downstream task feedback. It introduces a weight allocation network for per-sample utility estimation and uses dual-level optimization: model-level (fine-tune generator) and instance-level (adjust prompts/noise). Experiments on 8 benchmarks show average +3.87% accuracy over SOTA, with analysis showing generated data is more task-relevant and influential.", "conclusion": "\u4ece\u89c6\u89c9\u8d28\u91cf\u8f6c\u5411\u4efb\u52a1\u6548\u7528\u7684\u8303\u5f0f\u80fd\u663e\u8457\u63d0\u5347\u6570\u636e\u589e\u5f3a\u6548\u679c\uff1bUtilGen\u901a\u8fc7\u8bc4\u4f30\u4e0e\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u4ea7\u751f\u66f4\u5177\u4efb\u52a1\u76f8\u5173\u6027\u548c\u5f71\u54cd\u529b\u7684\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2510.24278", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24278", "abs": "https://arxiv.org/abs/2510.24278", "authors": ["Pietro Bongini", "Valentina Molinari", "Andrea Costanzo", "Benedetta Tondi", "Mauro Barni"], "title": "Training-free Source Attribution of AI-generated Images via Resynthesis", "comment": "14 pages, 4 figures, 1 table, accepted at \"The 17th IEEE\n  INTERNATIONAL WORKSHOP ON INFORMATION FORENSICS AND SECURITY (WIFS2025)\",\n  Perth, Australia", "summary": "Synthetic image source attribution is a challenging task, especially in data\nscarcity conditions requiring few-shot or zero-shot classification\ncapabilities. We present a new training-free one-shot attribution method based\non image resynthesis. A prompt describing the image under analysis is\ngenerated, then it is used to resynthesize the image with all the candidate\nsources. The image is attributed to the model which produced the resynthesis\nclosest to the original image in a proper feature space. We also introduce a\nnew dataset for synthetic image attribution consisting of face images from\ncommercial and open-source text-to-image generators. The dataset provides a\nchallenging attribution framework, useful for developing new attribution models\nand testing their capabilities on different generative architectures. The\ndataset structure allows to test approaches based on resynthesis and to compare\nthem to few-shot methods. Results from state-of-the-art few-shot approaches and\nother baselines show that the proposed resynthesis method outperforms existing\ntechniques when only a few samples are available for training or fine-tuning.\nThe experiments also demonstrate that the new dataset is a challenging one and\nrepresents a valuable benchmark for developing and evaluating future few-shot\nand zero-shot methods.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u91cd\u5408\u6210\u4e00\u62cd\u5f52\u5c5e\u65b9\u6cd5\u5e76\u6784\u5efa\u4eba\u8138\u5408\u6210\u5f52\u5c5e\u6570\u636e\u96c6\uff1b\u5728\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u6709\u4ef7\u503c\u57fa\u51c6\u3002", "motivation": "\u5408\u6210\u56fe\u50cf\u6765\u6e90\u5f52\u5c5e\u5728\u5c0f\u6837\u672c\u751a\u81f3\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u4ecd\u96be\u4ee5\u89e3\u51b3\uff0c\u73b0\u6709\u5c11\u6837\u672c\u65b9\u6cd5\u6027\u80fd\u53d7\u9650\uff1b\u56e0\u6b64\u63d0\u51fa\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u91cd\u5408\u6210\u7b56\u7565\u5e76\u6784\u5efa\u65b0\u57fa\u51c6\u6570\u636e\u96c6\u6765\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u3002", "method": "\u9996\u5148\u4e3a\u5f85\u6d4b\u56fe\u50cf\u751f\u6210\u63cf\u8ff0\u6027\u63d0\u793a(prompt)\uff0c\u7136\u540e\u7528\u5019\u9009\u751f\u6210\u6a21\u578b\u5206\u522b\u57fa\u4e8e\u8be5\u63d0\u793a\u91cd\u5408\u6210\u56fe\u50cf\uff1b\u5728\u5408\u9002\u7684\u7279\u5f81\u7a7a\u95f4\u4e2d\u6bd4\u8f83\u539f\u56fe\u4e0e\u5404\u91cd\u5408\u6210\u56fe\u50cf\u7684\u76f8\u4f3c\u5ea6\uff0c\u5c06\u56fe\u50cf\u5f52\u5c5e\u5230\u4e0e\u539f\u56fe\u6700\u76f8\u8fd1\u7684\u6a21\u578b\u3002", "result": "\u5728\u5305\u542b\u5546\u7528\u4e0e\u5f00\u6e90\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u5668\u4eba\u8138\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\u4e0a\uff0c\u91cd\u5408\u6210\u65b9\u6cd5\u5728\u5c11\u91cf\u6837\u672c\u60c5\u5f62\u4e0b\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u5c11\u6837\u672c\u65b9\u6cd5\u548c\u5176\u4ed6\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u8be5\u6570\u636e\u96c6\u5177\u6709\u6311\u6218\u6027\u4e14\u9002\u5408\u4f5c\u4e3a\u672a\u6765\u65b9\u6cd5\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u5408\u6210\u7684\u65e0\u8bad\u7ec3\u4e00\u62cd(one-shot)\u5408\u6210\u56fe\u50cf\u6765\u6e90\u5f52\u5c5e\u65b9\u6cd5\uff0c\u9488\u5bf9\u6837\u672c\u7a00\u7f3a\u4e0b\u7684\u5f52\u5c5e\u95ee\u9898\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2510.24285", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24285", "abs": "https://arxiv.org/abs/2510.24285", "authors": ["Juntian Zhang", "Song Jin", "Chuanqi Cheng", "Yuhan Liu", "Yankai Lin", "Xun Zhang", "Yufei Zhang", "Fei Jiang", "Guojun Yin", "Wei Lin", "Rui Yan"], "title": "ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model", "comment": null, "summary": "The limited capacity for fine-grained visual perception presents a critical\nbottleneck for Vision-Language Models (VLMs) in real-world applications.\nAddressing this is challenging due to the scarcity of high-quality data and the\nlimitations of existing methods: supervised fine-tuning (SFT) often compromises\ngeneral capabilities, while reinforcement fine-tuning (RFT) prioritizes textual\nreasoning over visual perception. To bridge this gap, we propose a novel\ntwo-stage task that structures visual perception learning as a coarse-to-fine\nprogressive process. Based on this task formulation, we develop ViPER, a\nself-bootstrapping framework specifically designed to enable iterative\nevolution through self-critiquing and self-prediction. By synergistically\nintegrating image-level and instance-level reconstruction with a two-stage\nreinforcement learning strategy, ViPER establishes a closed-loop training\nparadigm, where internally synthesized data directly fuel the enhancement of\nperceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the\nQwen-Viper series. With an average gain of 1.7% on seven comprehensive\nbenchmarks spanning various tasks and up to 6.0% on fine-grained perception,\nQwen-Viper consistently demonstrates superior performance across different\nvision-language scenarios while maintaining generalizability. Beyond enabling\nself-improvement in perceptual capabilities, ViPER provides concrete evidence\nfor the reciprocal relationship between generation and understanding, a\nbreakthrough to developing more autonomous and capable VLMs.", "AI": {"tldr": "\u63d0\u51faViPER\uff1a\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u6211\u5f15\u5bfc\uff08\u81ea\u6211\u6279\u8bc4+\u81ea\u6211\u9884\u6d4b\uff09\u548c\u56fe\u50cf/\u5b9e\u4f8b\u91cd\u5efa\u7684\u95ed\u73af\u8bad\u7ec3\uff0c\u81ea\u751f\u6210\u6570\u636e\u63d0\u5347VLM\u7ec6\u7c92\u5ea6\u611f\u77e5\uff0cQwen-Viper\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u7a33\u5b9a\u63d0\u5347", "motivation": "Improve fine-grained visual perception of VLMs despite scarce labeled data and limitations of SFT/RFT by structuring perception as progressive coarse-to-fine task and enabling self-generated data loop", "method": "Two-stage coarse-to-fine self-bootstrapping via self-critiquing and self-prediction", "result": "ViPER uses image- and instance-level reconstruction with two-stage RL to iteratively self-improve, producing Qwen-Viper models with avg +1.7% on seven benchmarks and up to +6.0% on fine-grained tasks", "conclusion": "ViPER\u8bc1\u660e\u4e86\u751f\u6210\u4e0e\u7406\u89e3\u7684\u4e92\u60e0\u5173\u7cfb\uff0c\u4e3a\u66f4\u81ea\u4e3b\u5f3a\u5927\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u53ef\u884c\u8def\u5f84"}}
{"id": "2510.24321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.24321", "abs": "https://arxiv.org/abs/2510.24321", "authors": ["Ivica Dimitrovski", "Vlatko Spasev", "Ivan Kitanovski"], "title": "Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning", "comment": null, "summary": "Remote sensing applications increasingly rely on deep learning for scene\nclassification. However, their performance is often constrained by the scarcity\nof labeled data and the high cost of annotation across diverse geographic and\nsensor domains. While recent vision-language models like CLIP have shown\npromise by learning transferable representations at scale by aligning visual\nand textual modalities, their direct application to remote sensing remains\nsuboptimal due to significant domain gaps and the need for task-specific\nsemantic adaptation. To address this critical challenge, we systematically\nexplore prompt learning as a lightweight and efficient adaptation strategy for\nfew-shot remote sensing image scene classification. We evaluate several\nrepresentative methods, including Context Optimization, Conditional Context\nOptimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating\nConstraints. These approaches reflect complementary design philosophies: from\nstatic context optimization to conditional prompts for enhanced generalization,\nmulti-modal prompts for joint vision-language adaptation, and semantically\nregularized prompts for stable learning without forgetting. We benchmark these\nprompt-learning methods against two standard baselines: zero-shot CLIP with\nhand-crafted prompts and a linear probe trained on frozen CLIP features.\nThrough extensive experiments on multiple benchmark remote sensing datasets,\nincluding cross-dataset generalization tests, we demonstrate that prompt\nlearning consistently outperforms both baselines in few-shot scenarios.\nNotably, Prompting with Self-Regulating Constraints achieves the most robust\ncross-domain performance. Our findings underscore prompt learning as a scalable\nand efficient solution for bridging the domain gap in satellite and aerial\nimagery, providing a strong foundation for future research in this field.", "AI": {"tldr": "Evaluate multiple prompt-learning strategies for few-shot remote sensing; prompt tuning improves accuracy and generalization over CLIP baselines, especially with semantic regularization", "motivation": "CLIP-like models underperform on remote sensing due to domain gaps and lack of task-specific semantics; need lightweight adaptation for few-shot scenarios", "method": "Prompt learning for few-shot remote sensing scene classification", "result": "Prompt learning methods consistently outperform zero-shot hand-crafted CLIP and linear probe on frozen CLIP; Self-Regulating Constraints prompt yields best cross-domain robustness", "conclusion": "Prompt learning is an efficient, scalable way to adapt vision-language models to remote sensing, reducing domain gap and enabling robust few-shot scene classification"}}
{"id": "2510.24366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24366", "abs": "https://arxiv.org/abs/2510.24366", "authors": ["Thanh-Huy Nguyen", "Hoang-Thien Nguyen", "Ba-Thinh Lam", "Vi Vu", "Bach X. Nguyen", "Jianhua Xing", "Tianyang Wang", "Xingjian Li", "Min Xu"], "title": "Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation", "comment": "The paper is under review at Pattern Recognition Journal", "summary": "Teacher-student frameworks have emerged as a leading approach in\nsemi-supervised medical image segmentation, demonstrating strong performance\nacross various tasks. However, the learning effects are still limited by the\nstrong correlation and unreliable knowledge transfer process between teacher\nand student networks. To overcome this limitation, we introduce a novel\nswitching Dual-Student architecture that strategically selects the most\nreliable student at each iteration to enhance dual-student collaboration and\nprevent error reinforcement. We also introduce a strategy of Loss-Aware\nExponential Moving Average to dynamically ensure that the teacher absorbs\nmeaningful information from students, improving the quality of pseudo-labels.\nOur plug-and-play framework is extensively evaluated on 3D medical image\nsegmentation datasets, where it outperforms state-of-the-art semi-supervised\nmethods, demonstrating its effectiveness in improving segmentation accuracy\nunder limited supervision.", "AI": {"tldr": "\u63d0\u51fa\u5207\u6362\u53cc\u5b66\u751f\u548c\u635f\u5931\u611f\u77e5EMA\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u534f\u4f5c\u548c\u52a8\u6001\u6559\u5e08\u66f4\u65b0\u63d0\u5347\u4f2a\u6807\u7b7e\u8d28\u91cf\uff0c\u663e\u8457\u6539\u8fdb\u534a\u76d1\u77633D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u5728\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u53d7\u5230\u6559\u5e08\u4e0e\u5b66\u751f\u4e4b\u95f4\u5f3a\u76f8\u5173\u6027\u548c\u4e0d\u53ef\u9760\u77e5\u8bc6\u4f20\u9012\u7684\u9650\u5236\uff0c\u5bfc\u81f4\u4f2a\u6807\u7b7e\u566a\u58f0\u548c\u9519\u8bef\u5f3a\u5316\uff0c\u5f71\u54cd\u5b66\u4e60\u6548\u679c\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u67b6\u6784\u4e0e\u66f4\u65b0\u7b56\u7565\u8bbe\u8ba1\u964d\u4f4e\u8fd9\u79cd\u4e0d\u826f\u76f8\u5173\u6027\u3001\u63d0\u9ad8\u4f2a\u6807\u7b7e\u53ef\u4fe1\u5ea6\u5e76\u63d0\u5347\u5206\u5272\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u5b66\u751f\u7f51\u7edc\u7684\u5207\u6362\u673a\u5236\uff0c\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u57fa\u4e8e\u53ef\u9760\u6027\u9009\u62e9\u8868\u73b0\u66f4\u53ef\u9760\u7684\u5b66\u751f\u4e0e\u53e6\u4e00\u5b66\u751f\u534f\u4f5c\uff0c\u907f\u514d\u9519\u8bef\u4fe1\u606f\u76f8\u4e92\u653e\u5927\uff1b\u540c\u65f6\u5f15\u5165\u635f\u5931\u611f\u77e5\u7684EMA\u66f4\u65b0\u7b56\u7565\uff0c\u6839\u636e\u5b66\u751f\u7684\u635f\u5931\u52a8\u6001\u8c03\u6574\u6559\u5e08\u53c2\u6570\u7684\u66f4\u65b0\u6743\u91cd\uff0c\u4ece\u800c\u4f7f\u6559\u5e08\u66f4\u53ef\u9760\u5730\u5438\u6536\u6709\u7528\u4fe1\u606f\u5e76\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u4f2a\u6807\u7b7e\u3002", "result": "\u5728\u591a\u4e2a3D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5728\u6709\u9650\u6807\u6ce8\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u5206\u5272\u51c6\u786e\u7387\uff08\u6587\u4e2d\u7ed9\u51fa\u5177\u4f53\u5b9a\u91cf\u5bf9\u6bd4\uff0c\u8868\u660e\u7a33\u5065\u7684\u6539\u8fdb\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u5207\u6362\u53cc\u5b66\u751f\uff08switching Dual-Student\uff09\u67b6\u6784\u548c\u635f\u5931\u611f\u77e5\u6307\u6570\u79fb\u52a8\u5e73\u5747\uff08Loss-Aware EMA\uff09\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u4e2d\u77e5\u8bc6\u4f20\u9012\u7684\u76f8\u5173\u6027\u95ee\u9898\u4e0e\u8bef\u5dee\u5f3a\u5316\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4f2a\u6807\u7b7e\u8d28\u91cf\u548c\u534a\u76d1\u77633D\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6027\u80fd\u3002"}}
{"id": "2510.24374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24374", "abs": "https://arxiv.org/abs/2510.24374", "authors": ["Yuda Zou", "Zijian Zhang", "Yongchao Xu"], "title": "Decoupling What to Count and Where to See for Referring Expression Counting", "comment": null, "summary": "Referring Expression Counting (REC) extends class-level object counting to\nthe fine-grained subclass-level, aiming to enumerate objects matching a textual\nexpression that specifies both the class and distinguishing attribute. A\nfundamental challenge, however, has been overlooked: annotation points are\ntypically placed on class-representative locations (e.g., heads), forcing\nmodels to focus on class-level features while neglecting attribute information\nfrom other visual regions (e.g., legs for \"walking\"). To address this, we\npropose W2-Net, a novel framework that explicitly decouples the problem into\n\"what to count\" and \"where to see\" via a dual-query mechanism. Specifically,\nalongside the standard what-to-count (w2c) queries that localize the object, we\nintroduce dedicated where-to-see (w2s) queries. The w2s queries are guided to\nseek and extract features from attribute-specific visual regions, enabling\nprecise subclass discrimination. Furthermore, we introduce Subclass Separable\nMatching (SSM), a novel matching strategy that incorporates a repulsive force\nto enhance inter-subclass separability during label assignment. W2-Net\nsignificantly outperforms the state-of-the-art on the REC-8K dataset, reducing\ncounting error by 22.5% (validation) and 18.0% (test), and improving\nlocalization F1 by 7% and 8%, respectively. Code will be available.", "AI": {"tldr": "W2-Net\u7528\u53cc\u67e5\u8be2\u673a\u5236\uff08w2c + w2s\uff09\u548cSSM\u5339\u914d\u7b56\u7565\uff0c\u6539\u5584\u4e86\u7ec6\u7c92\u5ea6\u5c5e\u6027\u8bc6\u522b\u4e0e\u8ba1\u6570\uff0c\u663e\u8457\u964d\u4f4eREC\u4efb\u52a1\u7684\u8ba1\u6570\u8bef\u5dee\u5e76\u63d0\u5347\u5b9a\u4f4dF1\u3002", "motivation": "\u73b0\u6709REC\u6570\u636e\u96c6\u7684\u6807\u6ce8\u70b9\u901a\u5e38\u4f4d\u4e8e\u7c7b\u522b\u4ee3\u8868\u6027\u533a\u57df\uff0c\u5bfc\u81f4\u6a21\u578b\u66f4\u591a\u5b66\u4e60\u5230\u7c7b\u7ea7\u7279\u5f81\u800c\u5ffd\u7565\u5c5e\u6027\u76f8\u5173\u533a\u57df\uff0c\u5f71\u54cd\u5b50\u7c7b\u7ea7\u8ba1\u6570\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u660e\u786e\u5b66\u4e60\u201c\u5728\u54ea\u770b\u201d\u4ee5\u8865\u5145\u201c\u770b\u4ec0\u4e48\u201d\u3002", "method": "\u63d0\u51fa\u53cc\u67e5\u8be2\u67b6\u6784\uff1a\u6807\u51c6\u7684what-to-count (w2c)\u67e5\u8be2\u8d1f\u8d23\u76ee\u6807\u5b9a\u4f4d\uff1b\u65b0\u589ewhere-to-see (w2s)\u67e5\u8be2\u5f15\u5bfc\u6a21\u578b\u63d0\u53d6\u5c5e\u6027\u76f8\u5173\u533a\u57df\u7279\u5f81\uff1b\u5e76\u5f15\u5165Subclass Separable Matching(SSM)\u5728\u6807\u7b7e\u5206\u914d\u9636\u6bb5\u52a0\u5165\u6392\u65a5\u529b\u4ee5\u589e\u5f3a\u5b50\u7c7b\u53ef\u5206\u6027\u3002", "result": "W2-Net\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u201cwhat to count\u201d\u548c\u201cwhere to see\u201d\u89e3\u8026\u7684\u53cc\u67e5\u8be2\u6846\u67b6\uff0c\u5f25\u8865\u4e86\u4ee5\u5f80\u6807\u6ce8\u70b9\u504f\u5411\u7c7b\u522b\u4ee3\u8868\u6027\u533a\u57df\u5bfc\u81f4\u5c5e\u6027\u5224\u522b\u5f31\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5f15\u5165w2s\u67e5\u8be2\uff0c\u6a21\u578b\u80fd\u5173\u6ce8\u5c5e\u6027\u76f8\u5173\u7684\u89c6\u89c9\u533a\u57df\uff1b\u5e76\u8bbe\u8ba1\u4e86Subclass Separable Matching(SSM)\u6765\u589e\u5f3a\u5b50\u7c7b\u95f4\u7684\u53ef\u5206\u6027\u3002\u5b9e\u9a8c\u5728REC-8K\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "W2-Net\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u76ee\u6807\u5b9a\u4f4d\u4e0e\u5c5e\u6027\u611f\u77e5\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u5b50\u7c7b\u8ba1\u6570\u4e0e\u5b9a\u4f4d\uff0c\u9a8c\u8bc1\u4e86w2s\u67e5\u8be2\u548cSSM\u5728REC\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2510.24378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24378", "abs": "https://arxiv.org/abs/2510.24378", "authors": ["Yann Kerverdo", "Florent Leray", "Youwan Mah\u00e9", "St\u00e9phanie Leplaideur", "Francesca Galassi"], "title": "Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool", "comment": null, "summary": "Deep learning frameworks such as nnU-Net achieve state-of-the-art performance\nin brain lesion segmentation but remain difficult to deploy clinically due to\nheavy dependencies and monolithic design. We introduce \\textit{StrokeSeg}, a\nmodular and lightweight framework that translates research-grade stroke lesion\nsegmentation models into deployable applications. Preprocessing, inference, and\npostprocessing are decoupled: preprocessing relies on the Anima toolbox with\nBIDS-compliant outputs, and inference uses ONNX Runtime with \\texttt{Float16}\nquantisation, reducing model size by about 50\\%. \\textit{StrokeSeg} provides\nboth graphical and command-line interfaces and is distributed as Python scripts\nand as a standalone Windows executable. On a held-out set of 300 sub-acute and\nchronic stroke subjects, segmentation performance was equivalent to the\noriginal PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that\nhigh-performing research pipelines can be transformed into portable, clinically\nusable tools.", "AI": {"tldr": "StrokeSeg decouples preprocessing (Anima/BIDS), Float16 ONNX inference, and postprocessing into a lightweight, cross-platform tool matching original accuracy while halving model size", "motivation": "Make high-performing stroke lesion segmentation models usable in clinic by removing heavy dependencies and monolithic design", "method": "Modular deployment + quantised ONNX inference", "result": "Equivalent segmentation performance to original PyTorch (Dice diff < 1e-3); model size reduced ~50% with Float16 ONNX", "conclusion": "Research-grade segmentation pipelines can be converted into portable, clinician-friendly applications without loss of performance using modular design and model quantisation."}}
{"id": "2510.24379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24379", "abs": "https://arxiv.org/abs/2510.24379", "authors": ["Zhuangfan Huang", "Xiaosong Li", "Gao Wang", "Tao Ye", "Haishu Tan", "Huafeng Li"], "title": "A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset", "comment": null, "summary": "Polarization image fusion combines S0 and DOLP images to reveal surface\nroughness and material properties through complementary texture features, which\nhas important applications in camouflage recognition, tissue pathology\nanalysis, surface defect detection and other fields. To intergrate\ncoL-Splementary information from different polarized images in complex\nluminance environment, we propose a luminance-aware multi-scale network (MLSN).\nIn the encoder stage, we propose a multi-scale spatial weight matrix through a\nbrightness-branch , which dynamically weighted inject the luminance into the\nfeature maps, solving the problem of inherent contrast difference in polarized\nimages. The global-local feature fusion mechanism is designed at the bottleneck\nlayer to perform windowed self-attention computation, to balance the global\ncontext and local details through residual linking in the feature dimension\nrestructuring stage. In the decoder stage, to further improve the adaptability\nto complex lighting, we propose a Brightness-Enhancement module, establishing\nthe mapping relationship between luminance distribution and texture features,\nrealizing the nonlinear luminance correction of the fusion result. We also\npresent MSP, an 1000 pairs of polarized images that covers 17 types of indoor\nand outdoor complex lighting scenes. MSP provides four-direction polarization\nraw maps, solving the scarcity of high-quality datasets in polarization image\nfusion. Extensive experiment on MSP, PIF and GAND datasets verify that the\nproposed MLSN outperms the state-of-the-art methods in subjective and objective\nevaluations, and the MS-SSIM and SD metircs are higher than the average values\nof other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,\nrespectively. The source code and dataset is avalable at\nhttps://github.com/1hzf/MLS-UNet.", "AI": {"tldr": "\u63d0\u51faMLSN\uff0c\u901a\u8fc7\u4eae\u5ea6\u5206\u652f\u7684\u591a\u5c3a\u5ea6\u6743\u91cd\u6ce8\u5165\u4e0e\u4eae\u5ea6\u589e\u5f3a\u6a21\u5757\uff0c\u7ed3\u5408\u5168\u5c40-\u5c40\u90e8\u81ea\u6ce8\u610f\u529b\u8fdb\u884c\u6781\u5316\u56fe\u50cf\u878d\u5408\uff0c\u5e76\u63d0\u4f9b1000\u5bf9MSP\u6570\u636e\u96c6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "Integrate complementary information from different polarized images under complex luminance by injecting luminance into features and correcting fused output.", "method": "Luminance-aware multi-scale network for polarization image fusion", "result": "Proposed MLSN with brightness-branch spatial weight, global-local windowed self-attention, Brightness-Enhancement module; introduced MSP dataset of 1000 image pairs; outperforms SOTA on MSP, PIF, GAND with notable metric improvements.", "conclusion": "MLSN\u6709\u6548\u89e3\u51b3\u6781\u5316\u56fe\u50cf\u4eae\u5ea6\u5dee\u5f02\uff0c\u63d0\u5347\u878d\u5408\u8d28\u91cf\u4e0e\u7eb9\u7406/\u7ed3\u6784\u6062\u590d\uff0c\u4e14\u65b0\u6570\u636e\u96c6\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2510.24385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24385", "abs": "https://arxiv.org/abs/2510.24385", "authors": ["Herman Bergstr\u00f6m", "Zhongqi Yue", "Fredrik D. Johansson"], "title": "When are radiology reports useful for training medical image classifiers?", "comment": null, "summary": "Medical images used to train machine learning models are often accompanied by\nradiology reports containing rich expert annotations. However, relying on these\nreports as inputs for clinical prediction requires the timely manual work of a\ntrained radiologist. This raises a natural question: when can radiology reports\nbe leveraged during training to improve image-only classification? Prior works\nare limited to evaluating pre-trained image representations by fine-tuning them\nto predict diagnostic labels, often extracted from reports, ignoring tasks with\nlabels that are weakly associated with the text. To address this gap, we\nconduct a systematic study of how radiology reports can be used during both\npre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,\n12-month readmission), and under varying training set sizes. Our findings\nreveal that: (1) Leveraging reports during pre-training is beneficial for\ndownstream classification tasks where the label is well-represented in the\ntext; however, pre-training through explicit image-text alignment can be\ndetrimental in settings where it's not; (2) Fine-tuning with reports can lead\nto significant improvements and even have a larger impact than the pre-training\nmethod in certain settings. These results provide actionable insights into when\nand how to leverage privileged text data to train medical image classifiers\nwhile highlighting gaps in current research.", "AI": {"tldr": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u653e\u5c04\u79d1\u62a5\u544a\u5728\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u9636\u6bb5\u5bf9\u533b\u5b66\u5f71\u50cf\u5206\u7c7b\u7684\u5f71\u54cd\uff1a\u5f53\u6807\u7b7e\u5728\u6587\u672c\u4e2d\u88ab\u5145\u5206\u8868\u5f81\u65f6\uff0c\u62a5\u544a\u6709\u52a9\u4e8e\u9884\u8bad\u7ec3\uff1b\u82e5\u5173\u8054\u5f31\uff0c\u663e\u5f0f\u5bf9\u9f50\u9884\u8bad\u7ec3\u53ef\u80fd\u6709\u5bb3\uff1b\u5fae\u8c03\u4f7f\u7528\u62a5\u544a\u5728\u82e5\u5e72\u573a\u666f\u4e2d\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u5e38\u4f34\u968f\u653e\u5c04\u79d1\u62a5\u544a\uff0c\u542b\u4e30\u5bcc\u4e13\u5bb6\u6ce8\u91ca\uff0c\u4f46\u4e34\u5e8a\u4f7f\u7528\u56fe\u50cf\u65f6\u62a5\u544a\u5e76\u4e0d\u603b\u53ef\u7528\u3002\u7814\u7a76\u65e8\u5728\u5f04\u6e05\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u201c\u7279\u6743\u201d\u6587\u672c\u4ee5\u63d0\u5347\u4ec5\u57fa\u4e8e\u56fe\u50cf\u7684\u5206\u7c7b\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u586b\u8865\u5148\u524d\u7814\u7a76\u5728\u4efb\u52a1\u8303\u56f4\u548c\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u7684\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e86\u7cfb\u7edf\u5316\u5b9e\u9a8c\uff1a\u5728\u4e0d\u540c\u4efb\u52a1\uff08\u8bca\u65ad vs \u9884\u540e\uff09\u3001\u4e0d\u540c\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u4e0b\u6bd4\u8f83\u591a\u79cd\u5229\u7528\u62a5\u544a\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u4ec5\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u62a5\u544a\uff08\u5982\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\uff09\u3001\u5728\u5fae\u8c03\u9636\u6bb5\u63d0\u4f9b\u62a5\u544a\uff08\u5982\u8054\u5408\u8f93\u5165\u6216\u8f85\u52a9\u4efb\u52a1\uff09\uff0c\u5e76\u4ee5\u82e5\u5e72\u57fa\u51c6\u6570\u636e\u96c6\u4e0e\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\uff0c\u5982\u4f55\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u653e\u5c04\u79d1\u62a5\u544a\uff08\u6587\u672c\uff09\u6765\u63d0\u5347\u4ec5\u57fa\u4e8e\u56fe\u50cf\u7684\u5206\u7c7b\u6027\u80fd\u3002\u4f5c\u8005\u7cfb\u7edf\u6027\u5730\u6bd4\u8f83\u4e86\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u4e2a\u9636\u6bb5\u5229\u7528\u62a5\u544a\u7684\u4e0d\u540c\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u8bca\u65ad\u7c7b\u4e0e\u9884\u540e\u7c7b\u4efb\u52a1\uff08\u4f8b\u598212\u4e2a\u6708\u518d\u5165\u9662\uff09\u4ee5\u53ca\u4e0d\u540c\u8bad\u7ec3\u96c6\u89c4\u6a21\u4e0b\u7684\u6548\u679c\u3002\u4e3b\u8981\u53d1\u73b0\u5305\u62ec\uff1a\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u9ad8\u5ea6\u76f8\u5173\u7684\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u62a5\u544a\u8fdb\u884c\u9884\u8bad\u7ec3\u80fd\u5e26\u6765\u6536\u76ca\uff0c\u4f46\u5728\u6807\u7b7e\u4e0e\u6587\u672c\u5173\u8054\u8f83\u5f31\u7684\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u7684\u9884\u8bad\u7ec3\u53cd\u800c\u53ef\u80fd\u6709\u5bb3\uff1b\u5728\u4e00\u4e9b\u8bbe\u7f6e\u4e2d\uff0c\u5fae\u8c03\u9636\u6bb5\u5f15\u5165\u62a5\u544a\u80fd\u5e26\u6765\u663e\u8457\u63d0\u5347\uff0c\u751a\u81f3\u8d85\u8fc7\u9884\u8bad\u7ec3\u65b9\u6cd5\u7684\u5f71\u54cd\u3002\u8bba\u6587\u7ed9\u51fa\u4e86\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u5229\u7528\u201c\u7279\u6743\u201d\u6587\u672c\u6570\u636e\u4ee5\u8bad\u7ec3\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "conclusion": "\u62a5\u544a\u53ef\u5728\u8bad\u7ec3\u4e0d\u540c\u9636\u6bb5\u63d0\u4f9b\u6536\u76ca\uff0c\u4f46\u5176\u6548\u7528\u53d6\u51b3\u4e8e\u6807\u7b7e\u4e0e\u6587\u672c\u7684\u5173\u8054\u6027\uff1b\u5fae\u8c03\u9636\u6bb5\u4f7f\u7528\u62a5\u544a\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6bd4\u9884\u8bad\u7ec3\u66f4\u91cd\u8981\uff1b\u9700\u8c28\u614e\u4f7f\u7528\u663e\u5f0f\u56fe\u50cf-\u6587\u672c\u5bf9\u9f50\u9884\u8bad\u7ec3\u4ee5\u514d\u5728\u5f31\u76f8\u5173\u4efb\u52a1\u4e2d\u635f\u5bb3\u6027\u80fd\u3002"}}
{"id": "2510.24398", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24398", "abs": "https://arxiv.org/abs/2510.24398", "authors": ["Youwan Mah\u00e9", "Elise Bannier", "St\u00e9phanie Leplaideur", "Elisa Fromont", "Francesca Galassi"], "title": "Unsupervised Detection of Post-Stroke Brain Abnormalities", "comment": null, "summary": "Post-stroke MRI not only delineates focal lesions but also reveals secondary\nstructural changes, such as atrophy and ventricular enlargement. These\nabnormalities, increasingly recognised as imaging biomarkers of recovery and\noutcome, remain poorly captured by supervised segmentation methods. We evaluate\nREFLECT, a flow-based generative model, for unsupervised detection of both\nfocal and non-lesional abnormalities in post-stroke patients. Using dual-expert\ncentral-slice annotations on ATLAS data, performance was assessed at the object\nlevel with Free-Response ROC analysis for anomaly maps. Two models were trained\non lesion-free slices from stroke patients (ATLAS) and on healthy controls\n(IXI) to test the effect of training data. On ATLAS test subjects, the\nIXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and\nimproved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).\nTraining on fully healthy anatomy improves the modelling of normal variability,\nenabling broader and more reliable detection of structural abnormalities.", "AI": {"tldr": "Flow-based unsupervised model REFLECT detects both lesions and non-lesional post-stroke abnormalities; training on healthy controls improves detection and modelling of normal variability.", "motivation": "Detect post-stroke MRI abnormalities including focal lesions and secondary non-lesional changes (atrophy, ventricular enlargement) which supervised methods poorly capture.", "method": "Trained REFLECT on lesion-free slices from stroke patients (ATLAS) and on healthy controls (IXI); evaluated using dual-expert central-slice annotations and FROC for anomaly maps, assessed object-level performance and Dice for lesion segmentation.", "result": "REFLECT, a flow-based generative model, was evaluated for unsupervised detection; models trained on healthy controls (IXI) outperformed those trained on stroke lesion-free slices (ATLAS) in lesion segmentation (Dice 0.37 vs 0.27) and sensitivity to non-lesional abnormalities (FROC 0.62 vs 0.43).", "conclusion": "Training on fully healthy anatomy yields better modelling of normal variability and more reliable detection of structural abnormalities in post-stroke MRI than training on lesion-free patient slices."}}
{"id": "2510.24399", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24399", "abs": "https://arxiv.org/abs/2510.24399", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "GenTrack: A New Generation of Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper introduces a novel multi-object tracking (MOT) method, dubbed\nGenTrack, whose main contributions include: a hybrid tracking approach\nemploying both stochastic and deterministic manners to robustly handle unknown\nand time-varying numbers of targets, particularly in maintaining target\nidentity (ID) consistency and managing nonlinear dynamics, leveraging particle\nswarm optimization (PSO) with some proposed fitness measures to guide\nstochastic particles toward their target distribution modes, enabling effective\ntracking even with weak and noisy object detectors, integration of social\ninteractions among targets to enhance PSO-guided particles as well as improve\ncontinuous updates of both strong (matched) and weak (unmatched) tracks,\nthereby reducing ID switches and track loss, especially during occlusions, a\nGenTrack-based redefined visual MOT baseline incorporating a comprehensive\nstate and observation model based on space consistency, appearance, detection\nconfidence, track penalties, and social scores for systematic and efficient\ntarget updates, and the first-ever publicly available source-code reference\nimplementation with minimal dependencies, featuring three variants, including\nGenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.\nExperimental results have shown that GenTrack provides superior performance on\nstandard benchmarks and real-world scenarios compared to state-of-the-art\ntrackers, with integrated implementations of baselines for fair comparison.\nPotential directions for future work are also discussed. The source-code\nreference implementations of both the proposed method and compared-trackers are\nprovided on GitHub: https://github.com/SDU-VelKoTek/GenTrack", "AI": {"tldr": "\u63d0\u51fa\u540d\u4e3aGenTrack\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7ed3\u5408\u968f\u673a\uff08PSO\uff09\u4e0e\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u5229\u7528\u793e\u4ea4\u4ea4\u4e92\u4e0e\u7efc\u5408\u89c2\u6d4b\u6a21\u578b\u6765\u63d0\u5347ID\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63d0\u4f9b\u4e09\u79cd\u53d8\u4f53\u4e0e\u5f00\u6e90\u5b9e\u73b0\uff0c\u5e76\u5728\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u672a\u77e5\u4e14\u968f\u65f6\u95f4\u53d8\u5316\u76ee\u6807\u6570\u3001\u5f31/\u566a\u58f0\u68c0\u6d4b\u5668\u3001\u975e\u7ebf\u6027\u8fd0\u52a8\u53ca\u906e\u6321\u5bfc\u81f4\u7684ID\u5207\u6362\u548c\u8f68\u8ff9\u4e22\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u6df7\u5408\u968f\u673a\u4e0e\u786e\u5b9a\u6027\u65b9\u6cd5\u53ca\u793e\u4ea4\u7ea6\u675f\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u4e00\u81f4\u6027\u3002", "method": "\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u5f15\u5bfc\u968f\u673a\u7c92\u5b50\u5411\u76ee\u6807\u5206\u5e03\u6a21\u6001\u6536\u655b\uff0c\u5e76\u4e0e\u786e\u5b9a\u6027\u8ddf\u8e2a\u673a\u5236\u6df7\u5408\uff1b\u63d0\u51fa\u82e5\u5e72\u9002\u5e94\u5ea6\u51fd\u6570\u3001\u5f15\u5165\u793e\u4ea4\u4ea4\u4e92\u5f97\u5206\u7528\u4e8e\u7c92\u5b50\u589e\u5f3a\u4e0e\u8f68\u8ff9\u66f4\u65b0\uff1b\u6784\u5efa\u5305\u62ec\u4f4d\u7f6e\u3001\u5916\u89c2\u3001\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u3001\u8f68\u8ff9\u60e9\u7f5a\u548c\u793e\u4ea4\u5206\u6570\u7684\u72b6\u6001-\u89c2\u6d4b\u6a21\u578b\uff1b\u5b9e\u73b0\u4e09\u79cd\u53d8\u4f53\uff1aBasic\u3001PSO\u3001PSO-Social\u3002", "result": "\u5728\u6807\u51c6MOT\u57fa\u51c6\u548c\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\u4e2d\uff0cGenTrack\u53ca\u5176\u53d8\u4f53\u5728ID\u5207\u6362\u3001\u4e22\u5931\u7387\u548c\u6574\u4f53\u6027\u80fd\u4e0a\u4f18\u4e8e\u591a\u9879\u73b0\u6709\u6700\u5148\u8fdb\u8ddf\u8e2a\u5668\uff0c\u4e14\u5f00\u6e90\u4ee3\u7801\u4fdd\u8bc1\u4e86\u516c\u5e73\u5bf9\u6bd4\u4e0e\u53ef\u590d\u73b0\u6027\u3002", "conclusion": "GenTrack\u901a\u8fc7\u878d\u5408PSO\u548c\u793e\u4ea4\u4fe1\u606f\uff0c\u6539\u8fdb\u4e86\u5f31\u68c0\u6d4b\u4e0e\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u76ee\u6807\u7ef4\u62a4\u4e0eID\u4e00\u81f4\u6027\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6807\u51c6\u57fa\u51c6\u548c\u5b9e\u9645\u573a\u666f\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u63d0\u4f9b\u5f00\u6e90\u5b9e\u73b0\u4fbf\u4e8e\u590d\u73b0\u3002"}}
{"id": "2510.24410", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2510.24410", "abs": "https://arxiv.org/abs/2510.24410", "authors": ["Toan Van Nguyen", "Rasmus G. K. Christiansen", "Dirk Kraft", "Leon Bodenhagen"], "title": "A Hybrid Approach for Visual Multi-Object Tracking", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This paper proposes a visual multi-object tracking method that jointly\nemploys stochastic and deterministic mechanisms to ensure identifier\nconsistency for unknown and time-varying target numbers under nonlinear\ndynamics. A stochastic particle filter addresses nonlinear dynamics and\nnon-Gaussian noise, with support from particle swarm optimization (PSO) to\nguide particles toward state distribution modes and mitigate divergence through\nproposed fitness measures incorporating motion consistency, appearance\nsimilarity, and social-interaction cues with neighboring targets. Deterministic\nassociation further enforces identifier consistency via a proposed cost matrix\nincorporating spatial consistency between particles and current detections,\ndetection confidences, and track penalties. Subsequently, a novel scheme is\nproposed for the smooth updating of target states while preserving their\nidentities, particularly for weak tracks during interactions with other targets\nand prolonged occlusions. Moreover, velocity regression over past states\nprovides trend-seed velocities, enhancing particle sampling and state updates.\nThe proposed tracker is designed to operate flexibly for both pre-recorded\nvideos and camera live streams, where future frames are unavailable.\nExperimental results confirm superior performance compared to state-of-the-art\ntrackers. The source-code reference implementations of both the proposed method\nand compared-trackers are provided on GitHub:\nhttps://github.com/SDU-VelKoTek/GenTrack2", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u968f\u673a\u7c92\u5b50\u6ee4\u6ce2+PSO\u548c\u786e\u5b9a\u6027\u5173\u8054\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u5668\uff0c\u901a\u8fc7\u8fd0\u52a8\u3001\u5916\u89c2\u548c\u4ea4\u4e92\u9002\u5e94\u7c92\u5b50\u91c7\u6837\u4e0e\u76ee\u6807\u4fdd\u6301\uff0c\u5e76\u5f15\u5165\u8eab\u4efd\u4fdd\u62a4\u7684\u5e73\u6ed1\u66f4\u65b0\u4e0e\u901f\u5ea6\u56de\u5f52\uff0c\u9002\u7528\u4e8e\u79bb\u7ebf\u4e0e\u5b9e\u65f6\u89c6\u9891\uff1b\u5b9e\u9a8c\u4f18\u4e8eSOTA\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "motivation": "Ensure robust multi-object tracking under nonlinear dynamics, non-Gaussian noise, unknown and varying target numbers, and appearance changes/occlusions.", "method": "Combine particle filter with PSO using fitness that blends motion consistency, appearance similarity, and social cues; deterministic association with cost matrix (spatial consistency, detection confidence, track penalties); smooth identity-preserving state update for occlusions and interactions; velocity regression from past states to seed particles.", "result": "A hybrid tracker combining particle filter with PSO-guided particles, deterministic data association using a novel cost matrix, smooth identity-preserving state updates, and velocity regression for trend-seed velocities, achieving superior performance vs SOTA; code on GitHub.", "conclusion": "Hybrid stochastic-deterministic framework improves identity consistency for challenging MOT scenarios with nonlinear dynamics and occlusions; velocity regression and PSO-enhanced particles are key, and deterministic association plus smooth update protects weak tracks."}}
{"id": "2510.24413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24413", "abs": "https://arxiv.org/abs/2510.24413", "authors": ["Ali Ahmad Faour", "Nabil Amacha", "Ali J. Ghandour"], "title": "50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon", "comment": null, "summary": "The sustainable management of the Qaraaoun Reservoir, the largest surface\nwater body in Lebanon located in the Bekaa Plain, depends on reliable\nmonitoring of its storage volume despite frequent sensor malfunctions and\nlimited maintenance capacity. This study introduces a sensor-free approach that\nintegrates open-source satellite imagery, advanced water-extent segmentation,\nand machine learning to estimate the reservoir surface area and volume in near\nreal time. Sentinel-2 and Landsat images are processed, where surface water is\ndelineated using a newly proposed water segmentation index. A machine learning\nmodel based on Support Vector Regression (SVR) is trained on a curated dataset\nthat includes water surface area, water level, and water volume calculations\nusing a reservoir bathymetry survey. The model is then able to estimate\nreservoir volume relying solely on surface area extracted from satellite\nimagery, without the need for ground measurements. Water segmentation using the\nproposed index aligns with ground truth for more than 95 percent of the\nshoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR\nperformance with error under 1.5 percent of full reservoir capacity and\ncoefficients of determination exceeding 0.98. These results demonstrate the\nrobustness and cost-effectiveness of the method, offering a practical solution\nfor continuous, sensor-independent monitoring of reservoir storage. The\nproposed methodology can be replicated for other water bodies, and the\nresulting 50 years of time-series data is valuable for research on climate\nchange and environmental patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65e0\u4f20\u611f\u5668\u65b9\u6cd5\uff0c\u7ed3\u5408Sentinel-2/Landsat\u9065\u611f\u5f71\u50cf\u3001\u65b0\u7684\u6c34\u4f53\u5206\u5272\u6307\u6570\u548c\u652f\u6301\u5411\u91cf\u56de\u5f52\uff08SVR\uff09\uff0c\u5b9e\u73b0Qaraaoun\u6c34\u5e93\u8868\u9762\u79ef\u548c\u84c4\u6c34\u91cf\u7684\u8fd1\u5b9e\u65f6\u4f30\u7b97\u3002\u6c34\u4f53\u5206\u5272\u4e0e\u5b9e\u5730\u5cb8\u7ebf\u543b\u5408\u5ea6\u8d85\u8fc795%\uff0cSVR\u7ecf\u7f51\u683c\u641c\u7d22\u4f18\u5316\u540e\u5bb9\u91cf\u8bef\u5dee\u4f4e\u4e8e1.5%\uff0cR^2>0.98\uff0c\u65b9\u6cd5\u5177\u6210\u672c\u6548\u76ca\u4e14\u53ef\u63a8\u5e7f\u3002", "motivation": "\u7531\u4e8eQaraaoun\u6c34\u5e93\u4f20\u611f\u5668\u5e38\u6545\u969c\u4e14\u7ef4\u62a4\u80fd\u529b\u6709\u9650\uff0c\u9700\u4e00\u79cd\u65e0\u9700\u5730\u9762\u4f20\u611f\u5668\u3001\u6210\u672c\u4f4e\u4e14\u53ef\u6301\u7eed\u7684\u84c4\u6c34\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528Sentinel-2\u548cLandsat\u5f71\u50cf\uff0c\u63d0\u51fa\u65b0\u7684\u6c34\u4f53\u5206\u5272\u6307\u6570\u8fdb\u884c\u6c34\u9762\u63d0\u53d6\uff1b\u57fa\u4e8e\u6c34\u9762\u4e0e\u6c34\u6df1-\u4f53\u79ef\u6d4b\u6df1\u6570\u636e\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5e76\u901a\u8fc7GridSearchCV\u8c03\u53c2\u4f18\u5316\u7684\u652f\u6301\u5411\u91cf\u56de\u5f52\u6a21\u578b\uff0c\u4ec5\u7528\u9065\u611f\u63d0\u53d6\u7684\u6c34\u9762\u9762\u79ef\u9884\u6d4b\u6c34\u5e93\u84c4\u6c34\u91cf\u3002", "result": "\u6c34\u4f53\u5206\u5272\u4e0e\u5cb8\u7ebf\u5b9e\u6d4b\u543b\u5408\u5ea6>95%\uff1b\u4f18\u5316\u540e\u7684SVR\u5728\u5168\u5e93\u5bb9\u8bef\u5dee<1.5%\uff0cR^2>0.98\uff1b\u65b9\u6cd5\u53ef\u590d\u5236\u5e76\u4ea7\u751f50\u5e74\u65f6\u5e8f\u6570\u636e\uff0c\u9002\u7528\u4e8e\u6c14\u5019\u4e0e\u73af\u5883\u7814\u7a76\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5728\u65e0\u5730\u9762\u4f20\u611f\u5668\u60c5\u51b5\u4e0b\uff0c\u80fd\u9ad8\u7cbe\u5ea6\u3001\u5b9e\u65f6\u4f30\u7b97Qaraaoun\u6c34\u5e93\u84c4\u6c34\u91cf\uff0c\u5177\u6709\u9c81\u68d2\u6027\u3001\u7ecf\u6d4e\u6027\u548c\u53ef\u590d\u5236\u6027\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u957f\u65f6\u95f4\u5e8f\u5217\u4ee5\u652f\u6301\u6c14\u5019\u548c\u73af\u5883\u7814\u7a76\u3002"}}
{"id": "2510.24414", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24414", "abs": "https://arxiv.org/abs/2510.24414", "authors": ["Reem Hammoud", "Abdul karim Gizzini", "Ali J. Ghandour"], "title": "XAI Evaluation Framework for Semantic Segmentation", "comment": null, "summary": "Ensuring transparency and trust in artificial intelligence (AI) models is\nessential, particularly as they are increasingly applied in safety-critical and\nhigh-stakes domains. Explainable AI (XAI) has emerged as a promising approach\nto address this challenge, yet the rigorous evaluation of XAI methods remains\ncrucial for optimizing the trade-offs between model complexity, predictive\nperformance, and interpretability. While extensive progress has been achieved\nin evaluating XAI techniques for classification tasks, evaluation strategies\ntailored to semantic segmentation remain relatively underexplored. This work\nintroduces a comprehensive and systematic evaluation framework specifically\ndesigned for assessing XAI in semantic segmentation, explicitly accounting for\nboth spatial and contextual task complexities. The framework employs\npixel-level evaluation strategies and carefully designed metrics to provide\nfine-grained interpretability insights. Simulation results using recently\nadapted class activation mapping (CAM)-based XAI schemes demonstrate the\nefficiency, robustness, and reliability of the proposed methodology. These\nfindings contribute to advancing transparent, trustworthy, and accountable\nsemantic segmentation models.", "AI": {"tldr": "\u4e3a\u8bed\u4e49\u5206\u5272\u8bbe\u8ba1\u7684\u7cfb\u7edf\u6027XAI\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\u50cf\u7d20\u7ea7\u7b56\u7565\u548c\u4e13\u95e8\u5ea6\u91cf\uff0c\u5229\u7528CAM\u7c7b\u65b9\u6cd5\u4eff\u771f\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709XAI\u8bc4\u4f30\u591a\u96c6\u4e2d\u4e8e\u5206\u7c7b\u4efb\u52a1\uff0c\u7f3a\u5c11\u9488\u5bf9\u8bed\u4e49\u5206\u5272\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff1b\u8bed\u4e49\u5206\u5272\u5728\u7a7a\u95f4\u5e03\u5c40\u4e0e\u4e0a\u4e0b\u6587\u4f9d\u8d56\u4e0a\u66f4\u590d\u6742\uff0c\u9700\u8981\u50cf\u7d20\u7ea7\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8bc4\u4f30\u624b\u6bb5\u4ee5\u4fdd\u8bc1\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u57fa\u4e8e\u50cf\u7d20\u7ea7\u8bc4\u4f30\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e13\u95e8\u7684\u8bc4\u4ef7\u6307\u6807\u6765\u540c\u65f6\u6355\u6349\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u4e0e\u7c7b\u522b\u76f8\u5173\u6027\uff1b\u5c06\u591a\u79cd\u6539\u7f16\u7684CAM\u65b9\u6cd5\u5e94\u7528\u4e8e\u8bed\u4e49\u5206\u5272\u5e76\u5728\u4eff\u771f\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u5176\u8f93\u51fa\u4e0e\u771f\u5b9e\u50cf\u7d20\u6807\u7b7e\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u8fdb\u800c\u91cf\u5316\u89e3\u91ca\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8bed\u4e49\u5206\u5272\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u50cf\u7d20\u7ea7\u8bc4\u4f30\u4e0e\u5ea6\u91cf\uff0c\u8003\u8651\u7a7a\u95f4\u4e0e\u4e0a\u4e0b\u6587\u590d\u6742\u6027\uff1b\u4f7f\u7528\u57fa\u4e8eCAM\u7684XAI\u65b9\u6cd5\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u65b9\u6cd5\u9ad8\u6548\u3001\u9c81\u68d2\u3001\u53ef\u9760\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u8bed\u4e49\u5206\u5272\u6a21\u578b\u7684\u900f\u660e\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u7ec6\u7c92\u5ea6\u5730\u8861\u91cf\u89e3\u91ca\u6027\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u517c\u987e\u7a7a\u95f4\u4e0e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u5176\u5728\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u900f\u660e\u4e0e\u8d1f\u8d23\u4efb\u7684\u5206\u5272\u6a21\u578b\u3002"}}
{"id": "2510.24437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24437", "abs": "https://arxiv.org/abs/2510.24437", "authors": ["Zhineng Zhao", "Zhihai He", "Zikun Zhou", "Siwei Ma", "Yaowei Wang"], "title": "Deeply-Conditioned Image Compression via Self-Generated Priors", "comment": null, "summary": "Learned image compression (LIC) has shown great promise for achieving high\nrate-distortion performance. However, current LIC methods are often limited in\ntheir capability to model the complex correlation structures inherent in\nnatural images, particularly the entanglement of invariant global structures\nwith transient local textures within a single monolithic representation. This\nlimitation precipitates severe geometric deformation at low bitrates. To\naddress this, we introduce a framework predicated on functional decomposition,\nwhich we term Deeply-Conditioned Image Compression via self-generated priors\n(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior\nto encapsulate the image's structural backbone. This prior is subsequently\nutilized not as mere side-information, but to holistically modulate the entire\ncompression pipeline. This deep conditioning, most critically of the analysis\ntransform, liberates it to dedicate its representational capacity to the\nresidual, high-entropy details. This hierarchical, dependency-driven approach\nachieves an effective disentanglement of information streams. Our extensive\nexperiments validate this assertion; visual analysis demonstrates that our\nmethod substantially mitigates the geometric deformation artifacts that plague\nconventional codecs at low bitrates. Quantitatively, our framework establishes\nhighly competitive performance, achieving significant BD-rate reductions of\n14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,\nand Tecnick datasets.", "AI": {"tldr": "Introduce DCIC-sgp: self-generated structural priors deeply condition compression pipeline to separate structure and texture, reducing geometric artifacts and improving RD by ~15% over VTM-12.1.", "motivation": "Current learned image compression methods struggle to model complex correlations in natural images, mixing global structure and local texture in a single representation, causing geometric deformation at low bitrates.", "method": "Functional decomposition: first encode a self-generated prior capturing structural backbone; use prior to modulate whole compression pipeline (especially analysis transform) so main representation focuses on residual high-entropy details; hierarchical dependency-driven design.", "result": "Proposes DCIC-sgp: encode a self-generated prior representing structural backbone, use it to deeply condition the entire pipeline, especially analysis transform, enabling representation of residual details; achieves disentanglement and reduces geometric deformation; reports BD-rate reductions of ~14\u201315% vs VTM-12.1 on Kodak, CLIC, Tecnick.", "conclusion": "Deep conditioning via self-generated priors effectively disentangles global structure from local details, mitigating deformation at low bitrates and improving rate-distortion performance significantly."}}
{"id": "2510.24448", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68T20", "I.2.10; I.4.8; I.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2510.24448", "abs": "https://arxiv.org/abs/2510.24448", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "Rethinking Visual Intelligence: Insights from Video Pretraining", "comment": "Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on\n  visual intelligence. This work can be considered as v2", "summary": "Large language models (LLMs) have demonstrated that large-scale pretraining\nenables systems to adapt rapidly to new problems with little supervision in the\nlanguage domain. This success, however, has not translated as effectively to\nthe visual domain, where models, including LLMs, continue to struggle with\ncompositional understanding, sample efficiency, and general-purpose\nproblem-solving. We investigate Video Diffusion Models (VDMs) as a promising\ndirection for bridging this gap. Pretraining on spatiotemporal data endows\nthese models with strong inductive biases for structure and dynamics, which we\nhypothesize can support broad task adaptability. To test this, we design a\ncontrolled evaluation in which both a pretrained LLM and a pretrained VDM are\nequipped with lightweight adapters and presented with tasks in their natural\nmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,\nroute planning, and cellular automata, VDMs demonstrate higher data efficiency\nthan their language counterparts. Taken together, our results indicate that\nvideo pretraining offers inductive biases that support progress toward visual\nfoundation models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u66ff\u4ee3\u6216\u8865\u5145\uff0c\u8ba4\u4e3a\u89c6\u9891\u9884\u8bad\u7ec3\u4e3a\u7ed3\u6784\u4e0e\u52a8\u6001\u5efa\u6a21\u63d0\u4f9b\u6709\u5229\u5f52\u7eb3\u504f\u7f6e\uff0c\u4ece\u800c\u5728\u5c11\u6837\u672c\u3001\u591a\u4efb\u52a1\u7684\u89c6\u89c9\u7406\u89e3\u548c\u63a8\u7406\u4e0a\u6bd4\u4ec5\u57fa\u4e8e\u8bed\u8a00\u7684\u6a21\u578b\u66f4\u9ad8\u6548\u3002\u4f5c\u8005\u901a\u8fc7\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7684LLM\u4e0eVDM\uff08\u914d\u5907\u8f7b\u91cf\u9002\u914d\u5668\uff09\uff0c\u53d1\u73b0VDM\u5728\u6570\u636e\u6548\u7387\u548c\u82e5\u5e72\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u5c3d\u7ba1\u5927\u89c4\u6a21\u8bed\u8a00\u9884\u8bad\u7ec3\u5e26\u6765\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u89c6\u89c9\u9886\u57df\u7684\u7ec4\u5408\u6027\u7406\u89e3\u3001\u6837\u672c\u6548\u7387\u548c\u901a\u7528\u95ee\u9898\u89e3\u51b3\u4e0a\u4ecd\u5b58\u5728\u4e0d\u8db3\uff1b\u89c6\u9891\u9884\u8bad\u7ec3\u53ef\u80fd\u901a\u8fc7\u5b66\u4e60\u7ed3\u6784\u548c\u52a8\u6001\u4fe1\u606f\u63d0\u4f9b\u66f4\u597d\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u5728\u53d7\u63a7\u8bc4\u4ef7\u6846\u67b6\u4e0b\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u5206\u522b\u914d\u5907\u8f7b\u91cf\u9002\u914d\u5668\uff08adapters\uff09\uff0c\u5e76\u5728\u5176\u5404\u81ea\u7684\u81ea\u7136\u6a21\u6001\u4e0a\u8fdb\u884c\u5fae\u8c03\u4e0e\u5c11\u6837\u672c\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5728ARC-AGI\u3001ConceptARC\u3001\u89c6\u89c9\u6e38\u620f\u3001\u8def\u5f84\u89c4\u5212\u548c\u7ec6\u80de\u81ea\u52a8\u673a\u7b49\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cVDM\u5728\u591a\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e0a\u6bd4\u7b49\u4ef7\u7684LLM\u5c55\u793a\u51fa\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u652f\u6301\u5c06\u89c6\u9891\u9884\u8bad\u7ec3\u4f5c\u4e3a\u53d1\u5c55\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6709\u524d\u666f\u65b9\u5411\u3002", "conclusion": "\u89c6\u9891\u9884\u8bad\u7ec3\u7684VDM\u5728\u591a\u79cd\u89c6\u89c9\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u6bd4\u8bed\u8a00\u6a21\u578b\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\uff0c\u8bf4\u660e\u89c6\u9891\u6570\u636e\u6240\u5e26\u6765\u7684\u65f6\u7a7a\u5f52\u7eb3\u504f\u7f6e\u6709\u52a9\u4e8e\u6784\u5efa\u901a\u7528\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2510.24456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24456", "abs": "https://arxiv.org/abs/2510.24456", "authors": ["Vivek Chetia", "Abdul Taher Khan", "Rahish Gogoi", "David Kapsian Khual", "Purnendu Bikash", "Sajal Saha"], "title": "A Critical Study towards the Detection of Parkinsons Disease using ML Technologies", "comment": null, "summary": "The proposed solution is Deep Learning Technique that will be able classify\nthree types of tea leaves diseases from which two diseases are caused by the\npests and one due to pathogens (infectious organisms) and environmental\nconditions and also show the area damaged by a disease in leaves. Namely Red\nRust, Helopeltis and Red spider mite respectively. In this paper we have\nevaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for\nthe object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU\nrange of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.\nWhile Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95\nand recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than\nSSD. Also used Mask R-CNN for Object Instance Segmentation where we have\nimplemented our custom method to calculate the damaged diseased portion of\nleaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red\nSpider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.", "AI": {"tldr": "\u8bba\u6587\u7528SSD\u548cFaster R-CNN\u505a\u8336\u53f6\u75c5\u866b\u5bb3\u68c0\u6d4b\uff0c\u5e76\u7528Mask R-CNN\u5206\u5272\u8ba1\u7b97\u53d7\u635f\u9762\u79ef\uff1bFaster R-CNN\u6548\u679c\u7a0d\u597d\uff0c\u4f46\u603b\u4f53mAP\u8f83\u4f4e\uff0c\u5206\u5272\u8bc4\u4f30\u6307\u6807\u7f3a\u5931\u3002", "motivation": "\u81ea\u52a8\u5316\u8bc6\u522b\u8336\u53f6\u4e09\u7c7b\u5e38\u89c1\u75c5\u866b\u5bb3\uff08\u7ea2\u9508\u75c5\u3001Helopeltis\u548c\u7ea2\u8718\u86db\u87a8\uff09\u5e76\u91cf\u5316\u53f6\u7247\u53d7\u635f\u9762\u79ef\uff0c\u4ee5\u8f85\u52a9\u519c\u4e1a\u76d1\u6d4b\u548c\u9632\u6cbb\u51b3\u7b56\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08SSD MobileNet V2\u3001Faster R-CNN ResNet50 V1\uff09\u8fdb\u884c\u75c5\u866b\u5bb3\u7c7b\u522b\u8bc6\u522b\uff0c\u5e76\u4f7f\u7528Mask R-CNN\u8fdb\u884c\u5b9e\u4f8b\u5206\u5272\u4ee5\u4f30\u7b97\u53f6\u7247\u53d7\u635f\u9762\u79ef\uff1b\u5bf9\u6a21\u578b\u5728IOU 0.50:0.95\u8303\u56f4\u5185\u7684\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u548cmAP\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "Faster R-CNN ResNet50 V1\u5728\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8eSSD MobileNet V2\uff08mAP\u5206\u522b\u7ea625% vs 20.9%\uff0cPrecision\u5206\u522b0.252 vs 0.209\uff0cRecall\u5206\u522b0.044 vs 0.02\uff09\uff1b\u540c\u65f6\u5b9e\u73b0\u4e86Mask R-CNN\u7684\u75c5\u53d8\u533a\u57df\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f46\u672a\u7ed9\u51fa\u8be5\u90e8\u5206\u7684\u91cf\u5316\u6307\u6807\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8336\u53f6\u75c5\u866b\u5bb3\u68c0\u6d4b\u4e0e\u75c5\u53d8\u533a\u57df\u5206\u5272\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86SSD MobileNet V2\u3001Faster R-CNN ResNet50 V1\u7528\u4e8e\u76ee\u6807\u68c0\u6d4b\uff0c\u5e76\u57fa\u4e8eMask R-CNN\u5b9e\u73b0\u75c5\u53d8\u9762\u79ef\u8ba1\u7b97\u3002"}}
{"id": "2510.24464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24464", "abs": "https://arxiv.org/abs/2510.24464", "authors": ["Charles Javerliat", "Pierre Raimbaud", "Guillaume Lavou\u00e9"], "title": "Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras", "comment": null, "summary": "Markerless multiview motion capture is often constrained by the need for\nprecise camera calibration, limiting accessibility for non-experts and\nin-the-wild captures. Existing calibration-free approaches mitigate this\nrequirement but suffer from high computational cost and reduced reconstruction\naccuracy.\n  We present Kineo, a fully automatic, calibration-free pipeline for markerless\nmotion capture from videos captured by unsynchronized, uncalibrated,\nconsumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf\ndetectors to simultaneously calibrate cameras, including Brown-Conrady\ndistortion coefficients, and reconstruct 3D keypoints and dense scene point\nmaps at metric scale. A confidence-driven spatio-temporal keypoint sampling\nstrategy, combined with graph-based global optimization, ensures robust\ncalibration at a fixed computational cost independent of sequence length. We\nfurther introduce a pairwise reprojection consensus score to quantify 3D\nreconstruction reliability for downstream tasks.\n  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements\nover prior calibration-free methods. Compared to previous state-of-the-art\napproaches, Kineo reduces camera translation error by approximately 83-85%,\ncamera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by\n83-91%.\n  Kineo is also efficient in real-world scenarios, processing multi-view\nsequences faster than their duration in specific configuration (e.g., 36min to\nprocess 1h20min of footage). The full pipeline and evaluation code are openly\nreleased to promote reproducibility and practical adoption at\nhttps://liris-xr.github.io/kineo/.", "AI": {"tldr": "Kineo \u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u81ea\u52a8\u3001\u514d\u6807\u5b9a\u7684\u591a\u89c6\u56fe\u4eba\u4f53\u52a8\u4f5c\u6355\u6349\u7ba1\u7ebf\uff0c\u4f7f\u7528\u65e0\u540c\u6b65\u3001\u65e0\u6807\u5b9a\u7684\u6d88\u8d39\u7ea7 RGB \u6444\u50cf\u673a\u8f93\u5165\uff0c\u4ec5\u4f9d\u8d56 2D \u5173\u952e\u70b9\u68c0\u6d4b\uff0c\u540c\u65f6\u8054\u5408\u4f30\u8ba1\u76f8\u673a\u5185\u5916\u53c2\u6570\uff08\u542b\u7578\u53d8\uff09\u30013D \u5173\u952e\u70b9\u4e0e\u7a20\u5bc6\u573a\u666f\u70b9\u3002\u5173\u952e\u8d21\u732e\u5305\u62ec\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u65f6\u7a7a\u5173\u952e\u70b9\u91c7\u6837\u3001\u56fe\u5168\u5c40\u4f18\u5316\u4ee5\u4fdd\u6301\u56fa\u5b9a\u8ba1\u7b97\u6210\u672c\uff0c\u4ee5\u53ca\u7528\u4e8e\u8bc4\u4f30\u91cd\u5efa\u53ef\u9760\u6027\u7684\u914d\u5bf9\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u5f97\u5206\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728 EgoHumans \u548c Human3.6M \u4e0a\u76f8\u6bd4\u5148\u524d\u65e0\u6807\u5b9a\u65b9\u6cd5\u5728\u76f8\u673a\u5e73\u79fb\u3001\u89d2\u5ea6\u4e0e W-MPJPE \u4e0a\u5747\u6709\u5927\u5e45\u63d0\u5347\uff0c\u4e14\u5728\u5b9e\u9645\u62cd\u6444\u573a\u666f\u4e2d\u6548\u7387\u9ad8\u3001\u4ee3\u7801\u5f00\u6e90\u3002", "motivation": "\u5f53\u524d\u514d\u6807\u5b9a\u591a\u89c6\u56fe\u52a8\u4f5c\u6355\u6349\u65b9\u6cd5\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\u4e14\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5728\u975e\u4e13\u4e1a\u4e0e\u91ce\u5916\u573a\u666f\u7684\u5e94\u7528\u3002\u7814\u7a76\u52a8\u673a\u662f\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3001\u63d0\u9ad8\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u4f7f\u666e\u901a\u6d88\u8d39\u7ea7\u6444\u50cf\u673a\u4e5f\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u89c6\u89d2\u4eba\u4f53\u52a8\u4f5c\u6355\u6349\u3002", "method": "\u4f7f\u7528\u73b0\u6210\u7684 2D \u5173\u952e\u70b9\u68c0\u6d4b\u5668\u63d0\u53d6\u5173\u952e\u70b9\uff1b\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u65f6\u7a7a\u91c7\u6837\u7b56\u7565\u9009\u53d6\u7528\u4e8e\u914d\u51c6\u7684\u5173\u952e\u70b9\uff1b\u6784\u5efa\u56fe\u5f62\u5f0f\u7684\u5168\u5c40\u4f18\u5316\u540c\u65f6\u4f30\u8ba1\u76f8\u673a\u5185\u53c2\uff08\u542b Brown-Conrady \u7578\u53d8\uff09\u3001\u76f8\u673a\u5916\u53c2\u4e0e 3D \u5173\u952e\u70b9\u4e0e\u7a20\u5bc6\u573a\u666f\u70b9\uff1b\u5f15\u5165\u914d\u5bf9\u91cd\u6295\u5f71\u4e00\u81f4\u6027\u5206\u6570\u8bc4\u4f30 3D \u91cd\u5efa\u53ef\u9760\u6027\uff1b\u8bbe\u8ba1\u4f7f\u8ba1\u7b97\u6210\u672c\u56fa\u5b9a\u4e14\u72ec\u7acb\u4e8e\u5e8f\u5217\u957f\u5ea6\u3002", "result": "\u5728 EgoHumans \u4e0e Human3.6M \u4e0a\u4e0e\u5148\u524d\u65e0\u6807\u5b9a\u65b9\u6cd5\u76f8\u6bd4\uff0cKineo \u5728\u76f8\u673a\u5e73\u79fb\u8bef\u5dee\u964d\u4f4e\u7ea6 83-85%\uff0c\u89d2\u5ea6\u8bef\u5dee\u964d\u4f4e 86-92%\uff0cW-MPJPE \u964d\u4f4e 83-91%\u3002\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8fd0\u884c\u9ad8\u6548\uff08\u5982\u5728\u67d0\u914d\u7f6e\u4e0b\u7528 36 \u5206\u949f\u5904\u7406 1 \u5c0f\u65f6 20 \u5206\u949f\u89c6\u9891\uff09\uff0c\u5e76\u5f00\u6e90\u5b8c\u6574\u7ba1\u7ebf\u4e0e\u8bc4\u6d4b\u4ee3\u7801\u3002", "conclusion": "Kineo \u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u7387\u5e76\u5b58\u7684\u514d\u6807\u5b9a\u591a\u89c6\u56fe\u4eba\u4f53\u52a8\u4f5c\u6355\u6349\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u514d\u6807\u5b9a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u975e\u4e13\u4e1a\u4e0e\u91ce\u5916\u91c7\u96c6\u573a\u666f\uff0c\u5e76\u63d0\u4f9b\u53ef\u9760\u6027\u5ea6\u91cf\u4ee5\u652f\u6301\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2510.24474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24474", "abs": "https://arxiv.org/abs/2510.24474", "authors": ["Kyungmin Lee", "Sihyun Yu", "Jinwoo Shin"], "title": "Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling", "comment": null, "summary": "Denoising generative models, such as diffusion and flow-based models, produce\nhigh-quality samples but require many denoising steps due to discretization\nerror. Flow maps, which estimate the average velocity between timesteps,\nmitigate this error and enable faster sampling. However, their training\ntypically demands architectural changes that limit compatibility with\npretrained flow models. We introduce Decoupled MeanFlow, a simple decoding\nstrategy that converts flow models into flow map models without architectural\nmodifications. Our method conditions the final blocks of diffusion transformers\non the subsequent timestep, allowing pretrained flow models to be directly\nrepurposed as flow maps. Combined with enhanced training techniques, this\ndesign enables high-quality generation in as few as 1 to 4 steps. Notably, we\nfind that training flow models and subsequently converting them is more\nefficient and effective than training flow maps from scratch. On ImageNet\n256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,\nrespectively, surpassing prior art by a large margin. Furthermore, we achieve\nFID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the\nperformance of flow models while delivering over 100x faster inference.", "AI": {"tldr": "\u63d0\u51faDecoupled MeanFlow\u65b9\u6cd5\uff0c\u5c06\u9884\u8bad\u7ec3\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u56fe\u6a21\u578b\uff0c\u65e0\u9700\u6539\u52a8\u67b6\u6784\uff1b\u901a\u8fc7\u5728\u6269\u6563\u53d8\u6362\u5668\u6700\u540e\u5757\u4e0a\u6761\u4ef6\u5316\u540e\u7eed\u65f6\u95f4\u6b65\uff0c\u7ed3\u5408\u8bad\u7ec3\u6280\u5de7\uff0c\u5b9e\u73b01\u20134\u6b65\u9ad8\u8d28\u91cf\u751f\u6210\u3002\u5728ImageNet\u4e0a1\u6b65FID\u22482.16/2.12\uff08256/512\uff09\uff0c4\u6b65FID\u22481.51/1.68\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347100x\u4ee5\u4e0a\u3002", "motivation": "\u51cf\u5c11\u6269\u6563/\u6d41\u5f0f\u53bb\u566a\u6a21\u578b\u7684\u79bb\u6563\u5316\u8bef\u5dee\uff0c\u51cf\u5c11\u53bb\u566a\u6b65\u6570\u4ee5\u52a0\u901f\u91c7\u6837\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6837\u672c\u5e76\u517c\u5bb9\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "method": "\u5728\u6269\u6563\u53d8\u6362\u5668\u7684\u6700\u540e\u82e5\u5e72\u5757\u4e0a\u5bf9\u540e\u7eed\u65f6\u95f4\u6b65\u8fdb\u884c\u6761\u4ef6\u5316\uff08\u5373\u5bf9\u89e3\u7801\u5668\u8fdb\u884c\u65f6\u95f4\u6b65\u4f9d\u8d56\u7684\u8c03\u5236\uff09\uff0c\u4f5c\u4e3a\u4e00\u79cd\u89e3\u7801\u7b56\u7565\u628a\u6d41\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u56fe\u6a21\u578b\uff1b\u7ed3\u5408\u82e5\u5e72\u589e\u5f3a\u8bad\u7ec3\u6280\u5de7\uff08\u8bba\u6587\u672a\u5177\u4f53\u5217\u51fa\u7ec6\u8282\uff09\uff0c\u4ee5\u652f\u63011\u20134\u6b65\u91c7\u6837\u3002", "result": "\u5728ImageNet 256x256\u4e0e512x512\u4e0a\uff0c1\u6b65FID\u5206\u522b\u4e3a2.16\u4e0e2.12\uff1b4\u6b65FID\u5206\u522b\u4e3a1.51\u4e0e1.68\uff0c\u51e0\u4e4e\u5339\u914d\u6d41\u6a21\u578b\u6027\u80fd\u4e14\u63a8\u7406\u901f\u5ea6\u63d0\u5347\u8d85\u8fc7100x\u3002", "conclusion": "Decoupled MeanFlow\u80fd\u5728\u4e0d\u6539\u52a8\u67b6\u6784\u4e0b\u628a\u6d41\u6a21\u578b\u8f6c\u4e3a\u6d41\u56fe\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5c11\u6b65\u91c7\u6837\u8d28\u91cf\u4e0e\u901f\u5ea6\uff1b\u8bad\u7ec3\u5148\u505a\u6d41\u6a21\u578b\u7136\u540e\u8f6c\u6362\u6bd4\u4ece\u5934\u8bad\u7ec3\u6d41\u56fe\u66f4\u9ad8\u6548\u66f4\u6709\u6548\u3002"}}
{"id": "2510.24486", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.24486", "abs": "https://arxiv.org/abs/2510.24486", "authors": ["Tinsae G. Dulecha", "Leonardo Righetto", "Ruggero Pintus", "Enrico Gobbetti", "Andrea Giachetti"], "title": "Fast and accurate neural reflectance transformation imaging through knowledge distillation", "comment": "18 pages", "summary": "Reflectance Transformation Imaging (RTI) is very popular for its ability to\nvisually analyze surfaces by enhancing surface details through interactive\nrelighting, starting from only a few tens of photographs taken with a fixed\ncamera and variable illumination. Traditional methods like Polynomial Texture\nMaps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle\nto accurately capture complex reflectance fields using few per-pixel\ncoefficients and fixed bases, leading to artifacts, especially in highly\nreflective or shadowed areas. The NeuralRTI approach, which exploits a neural\nautoencoder to learn a compact function that better approximates the local\nreflectance as a function of light directions, has been shown to produce\nsuperior quality at comparable storage cost. However, as it performs\ninteractive relighting with custom decoder networks with many parameters, the\nrendering step is computationally expensive and not feasible at full resolution\nfor large images on limited hardware. Earlier attempts to reduce costs by\ndirectly training smaller networks have failed to produce valid results. For\nthis reason, we propose to reduce its computational cost through a novel\nsolution based on Knowledge Distillation (DisK-NeuralRTI). ...", "AI": {"tldr": "Use knowledge distillation to compress NeuralRTI into a small student network that runs much faster and uses less memory while keeping visual quality nearly unchanged.", "motivation": "Reduce computational cost of NeuralRTI for interactive relighting while keeping quality, enabling full-resolution rendering on limited hardware.", "method": "Train a compact per-pixel MLP student using distillation losses (MSE on predicted coefficients and perceptual/image-space losses) with data augmentation and careful architecture choices; compare against baseline PTM/HSH/NeuralRTI on reconstruction and relighting metrics; measure inference latency and memory.", "result": "Proposed DisK-NeuralRTI: knowledge distillation to train a compact 'student' network guided by a large pre-trained 'teacher' NeuralRTI model, achieving similar perceptual quality with much lower latency and memory; evaluated on two datasets with significant speedups and comparable PSNR/SSIM.", "conclusion": "DisK-NeuralRTI effectively transfers the teacher's expressive reflectance modeling to a lightweight student, enabling real-time, full-resolution RTI relighting on limited hardware with minor perceptual differences."}}
{"id": "2510.24514", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.24514", "abs": "https://arxiv.org/abs/2510.24514", "authors": ["Huanyu Zhang", "Wenshan Wu", "Chengzu Li", "Ning Shang", "Yan Xia", "Yangyu Huang", "Yifan Zhang", "Li Dong", "Zhang Zhang", "Liang Wang", "Tieniu Tan", "Furu Wei"], "title": "Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding,\nthey often struggle in complex scenarios that require visual planning and\nimagination. Inspired by how humans use sketching as a form of visual thinking\nto develop and communicate ideas, we introduce Latent Sketchpad, a framework\nthat equips MLLMs with an internal visual scratchpad. The internal visual\nrepresentations of MLLMs have traditionally been confined to perceptual\nunderstanding. We repurpose them to support generative visual thought without\ncompromising reasoning ability. Building on frontier MLLMs, our approach\nintegrates visual generation directly into their native autoregressive\nreasoning process. It allows the model to interleave textual reasoning with the\ngeneration of visual latents. These latents guide the internal thought process\nand can be translated into sketch images for interpretability. To realize this,\nwe introduce two components: a Context-Aware Vision Head autoregressively\nproduces visual representations, and a pretrained Sketch Decoder renders these\ninto human-interpretable images. We evaluate the framework on our new dataset\nMazePlanning. Experiments across various MLLMs show that Latent Sketchpad\ndelivers comparable or even superior reasoning performance to their backbone.\nIt further generalizes across distinct frontier MLLMs, including Gemma3 and\nQwen2.5-VL. By extending model's textual reasoning to visual thinking, our\nframework opens new opportunities for richer human-computer interaction and\nbroader applications. More details and resources are available on our project\npage: https://latent-sketchpad.github.io/.", "AI": {"tldr": "Introduce Latent Sketchpad: adds autoregressive visual latent generation to MLLMs via a Context-Aware Vision Head and Sketch Decoder, enabling internal visual thinking and improved or comparable planning performance on MazePlanning", "motivation": "Humans use sketching as visual thinking; MLLMs lack internal visual planning and imagination, so provide an internal visual scratchpad to support generative visual thought", "method": "Latent Sketchpad integrates visual generation into autoregressive reasoning by adding a Context-Aware Vision Head and a pretrained Sketch Decoder", "result": "Latent Sketchpad matches or exceeds backbone MLLMs on MazePlanning; generalizes across Gemma3 and Qwen2.5-VL; allows interleaving text reasoning with visual latent generation and renders sketches for interpretability", "conclusion": "Extending MLLMs with internal visual scratchpad enables richer reasoning, interpretability, and cross-model generalization"}}
{"id": "2510.24563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24563", "abs": "https://arxiv.org/abs/2510.24563", "authors": ["Hongrui Jia", "Jitong Liao", "Xi Zhang", "Haiyang Xu", "Tianbao Xie", "Chaoya Jiang", "Ming Yan", "Si Liu", "Wei Ye", "Fei Huang"], "title": "OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents", "comment": null, "summary": "With advances in decision-making and reasoning capabilities, multimodal\nagents show strong potential in computer application scenarios. Past\nevaluations have mainly assessed GUI interaction skills, while tool invocation\nabilities, such as those enabled by the Model Context Protocol (MCP), have been\nlargely overlooked. Comparing agents with integrated tool invocation to those\nevaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,\nthe first comprehensive and fair benchmark for assessing computer-use agents'\ntool invocation, GUI operation, and decision-making abilities in a real-world\nenvironment. We design a novel automated code-generation pipeline to create\ntools and combine them with a curated selection from existing tools. Rigorous\nmanual validation yields 158 high-quality tools (covering 7 common\napplications), each verified for correct functionality, practical\napplicability, and versatility. Extensive evaluations of state-of-the-art\nmultimodal agents on OSWorld-MCP show that MCP tools generally improve task\nsuccess rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%\nto 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of\nassessing tool invocation capabilities. However, even the strongest models have\nrelatively low tool invocation rates, Only 36.3%, indicating room for\nimprovement and highlighting the benchmark's challenge. By explicitly measuring\nMCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents\nand sets a new standard for evaluating performance in complex, tool-assisted\nenvironments. Our code, environment, and data are publicly available at\nhttps://osworld-mcp.github.io.", "AI": {"tldr": "\u63d0\u51fa OSWorld-MCP\uff1a\u7b2c\u4e00\u4e2a\u516c\u5e73\u8bc4\u4f30\u591a\u6a21\u6001\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u901a\u8fc7 MCP \u5de5\u5177\u8c03\u7528\u4e0e GUI \u64cd\u4f5c\u80fd\u529b\u7684\u57fa\u51c6\u3002\u6784\u5efa\u4e86 158 \u4e2a\u7ecf\u4eba\u5de5\u9a8c\u8bc1\u7684\u5de5\u5177\u5e76\u53d1\u73b0\u5de5\u5177\u4e00\u822c\u80fd\u63d0\u5347\u6210\u529f\u7387\uff0c\u4f46\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u7387\u4ecd\u4f4e\uff0c\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4fa7\u91cd GUI \u4ea4\u4e92\uff0c\u5ffd\u89c6\u4e86\u501f\u52a9\u5de5\u5177\uff08\u5982 MCP \u534f\u8bae\uff09\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\uff1b\u5c06\u4e24\u7c7b\u4ee3\u7406\u76f4\u63a5\u6bd4\u8f83\u4e0d\u516c\u5e73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u516c\u5e73\u3001\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u4e0e GUI \u64cd\u4f5c\u5e76\u91cd\u7684\u771f\u5b9e\u4f7f\u7528\u80fd\u529b\u3002", "method": "\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u6d41\u6c34\u7ebf\u4ee5\u521b\u5efa\u5de5\u5177\uff0c\u5e76\u7ed3\u5408\u7cbe\u9009\u73b0\u6709\u5de5\u5177\uff0c\u968f\u540e\u5bf9\u751f\u6210\u5de5\u5177\u8fdb\u884c\u4e25\u683c\u4eba\u5de5\u9a8c\u8bc1\uff0c\u6700\u7ec8\u5f62\u6210 158 \u4e2a\u5de5\u5177\u3002\u57fa\u4e8e\u771f\u5b9e\u73af\u5883\u7684 OSWorld-MCP benchmark\uff0c\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u7edf\u8ba1\u4efb\u52a1\u6210\u529f\u7387\u3001\u5de5\u5177\u8c03\u7528\u7387\u7b49\u6307\u6807\u3002", "result": "OSWorld-MCP \u63d0\u51fa\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9488\u5bf9\u591a\u6a21\u6001\u4ee3\u7406\u5728\u771f\u5b9e\u8ba1\u7b97\u73af\u5883\u4e2d\u4f7f\u7528\u5de5\u5177\u8c03\u7528\uff08\u901a\u8fc7 MCP \u534f\u8bae\uff09\u3001GUI \u64cd\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\u7684\u516c\u5e73\u57fa\u51c6\u3002\u4f5c\u8005\u901a\u8fc7\u81ea\u52a8\u5316\u4ee3\u7801\u751f\u6210\u4e0e\u4eba\u5de5\u7b5b\u9009\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u5e76\u9a8c\u8bc1\u4e86 158 \u4e2a\u9ad8\u8d28\u91cf\u5de5\u5177\uff0c\u8986\u76d6 7 \u7c7b\u5e38\u7528\u5e94\u7528\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5f15\u5165 MCP \u5de5\u5177\u666e\u904d\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff08\u4f8b\u5982 OpenAI o3 \u5728 15 \u6b65\u65f6\u4ece 8.3% \u63d0\u5347\u5230 20.4%\uff1bClaude 4 Sonnet \u5728 50 \u6b65\u65f6\u4ece 40.1% \u63d0\u5347\u5230 43.3%\uff09\uff0c\u4f46\u9876\u5c16\u6a21\u578b\u7684\u5de5\u5177\u8c03\u7528\u7387\u4ecd\u4f4e\uff08\u4ec5 36.3%\uff09\uff0c\u8868\u660e\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u4ecd\u6709\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002\u57fa\u51c6\u73af\u5883\u3001\u4ee3\u7801\u548c\u6570\u636e\u5df2\u516c\u5f00\u3002", "conclusion": "OSWorld-MCP \u6709\u6548\u586b\u8865\u4e86\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u80fd\u529b\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u548c\u771f\u5b9e\u73af\u5883\u6d4b\u8bd5\uff0c\u8bc1\u660e MCP \u5de5\u5177\u80fd\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f46\u5f53\u524d\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u4e0a\u7684\u8868\u73b0\u4e0d\u8db3\uff0c\u672a\u6765\u7814\u7a76\u9700\u805a\u7126\u63d0\u9ad8\u5de5\u5177\u4f7f\u7528\u5224\u65ad\u4e0e\u8c03\u7528\u7b56\u7565\u3002"}}
{"id": "2510.24579", "categories": ["cs.CV", "I.4.5; I.5"], "pdf": "https://arxiv.org/pdf/2510.24579", "abs": "https://arxiv.org/abs/2510.24579", "authors": ["Xu Jiang", "Huiying Pan", "Ligen Shi", "Jianing Sun", "Wenfeng Xu", "Xing Zhao"], "title": "Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT", "comment": "8 pages, 6 figures", "summary": "Cone-beam CT (CBCT) employs a flat-panel detector to achieve\nthree-dimensional imaging with high spatial resolution. However, CBCT is\nsusceptible to scatter during data acquisition, which introduces CT value bias\nand reduced tissue contrast in the reconstructed images, ultimately degrading\ndiagnostic accuracy. To address this issue, we propose a deep learning-based\nscatter artifact correction method inspired by physical prior knowledge.\nLeveraging the fact that the observed point scatter probability density\ndistribution exhibits rotational symmetry in the projection domain. The method\nuses Gaussian Radial Basis Functions (RBF) to model the point scatter function\nand embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides\nefficient nonlinear mapping capabilities for learning high-dimensional scatter\nfeatures. By incorporating the physical characteristics of the scattered photon\ndistribution together with the complex function mapping capacity of KAN, the\nmodel improves its ability to accurately represent scatter. The effectiveness\nof the method is validated through both synthetic and real-scan experiments.\nExperimental results show that the model can effectively correct the scatter\nartifacts in the reconstructed images and is superior to the current methods in\nterms of quantitative metrics.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u5148\u9a8c\uff08\u6295\u5f71\u57df\u6563\u5c04\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff09\u4e0eKAN\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u6563\u5c04\u6821\u6b63\u65b9\u6cd5\uff0c\u4f7f\u7528\u9ad8\u65afRBF\u5efa\u6a21\u70b9\u6563\u5c04\u5e76\u5d4c\u5165\u7f51\u7edc\uff0c\u5b9e\u73b0\u5bf9CBCT\u6563\u5c04\u4f2a\u5f71\u7684\u6709\u6548\u6821\u6b63\uff0c\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "CBCT\u6613\u53d7\u6563\u5c04\u5f71\u54cd\u5bfc\u81f4CT\u503c\u504f\u7f6e\u548c\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u5f71\u54cd\u8bca\u65ad\u3002\u4f20\u7edf\u6563\u5c04\u6821\u6b63\u65b9\u6cd5\u6709\u9650\uff0c\u6545\u5f15\u5165\u8003\u8651\u7269\u7406\u5148\u9a8c\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6563\u5c04\u5efa\u6a21\u548c\u6821\u6b63\u7cbe\u5ea6\u3002", "method": "\u57fa\u4e8e\u7269\u7406\u5148\u9a8c\uff0c\u9996\u5148\u5728\u6295\u5f71\u57df\u5047\u8bbe\u70b9\u6563\u5c04\u6982\u7387\u5bc6\u5ea6\u5448\u65cb\u8f6c\u5bf9\u79f0\uff0c\u7528\u9ad8\u65afRBF\u5bf9\u70b9\u6563\u5c04\u51fd\u6570\u5efa\u6a21\uff1b\u7136\u540e\u5c06\u8be5RBF\u8868\u793a\u5d4c\u5165KAN\u5c42\u4ee5\u63d0\u4f9b\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u9ad8\u7ef4\u6620\u5c04\u80fd\u529b\uff0c\u7f51\u7edc\u5b66\u4e60\u6563\u5c04\u7279\u5f81\u5e76\u9884\u6d4b/\u6821\u6b63\u6295\u5f71\u6570\u636e\u7684\u6563\u5c04\u6210\u5206\uff1b\u6700\u540e\u91cd\u5efa\u7ecf\u6821\u6b63\u6295\u5f71\u4ee5\u5f97\u5230\u53bb\u6563\u5c04\u7684\u4e09\u7ef4\u56fe\u50cf\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u626b\u63cf\u6570\u636e\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u53bb\u9664\u6563\u5c04\u4f2a\u5f71\uff0c\u6062\u590dCT\u503c\u4e0e\u7ec4\u7ec7\u5bf9\u6bd4\uff0c\u4e14\u5728\u591a\u4e2a\u5b9a\u91cf\u6307\u6807\u4e0a\uff08\u4f8b\u5982\u8bef\u5dee\u3001\u5bf9\u6bd4\u5ea6\u6062\u590d\u6216\u5cf0\u503c\u4fe1\u566a\u6bd4\u7b49\uff09\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7269\u7406\u5148\u9a8c\uff08\u6563\u5c04\u70b9\u6982\u7387\u5bc6\u5ea6\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff09\u4e0e\u6df1\u5ea6\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u9ad8\u65af\u5f84\u5411\u57fa\u51fd\u6570\u5efa\u6a21\u70b9\u6563\u5c04\u51fd\u6570\u5e76\u5d4c\u5165Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u5c42\uff0c\u5b9e\u73b0\u5bf9\u6563\u5c04\u5206\u5e03\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u4ece\u800c\u6709\u6548\u7ea0\u6b63CBCT\u6563\u5c04\u4f2a\u5f71\u3002\u7efc\u5408\u5b9e\u9a8c\uff08\u5408\u6210\u4e0e\u771f\u5b9e\u626b\u63cf\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u5b9a\u91cf\u6307\u6807\u548c\u91cd\u5efa\u56fe\u50cf\u8d28\u91cf\u4e0a\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2510.24640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24640", "abs": "https://arxiv.org/abs/2510.24640", "authors": ["Xin Zhang", "Yuqi Song", "Fei Zuo"], "title": "A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries", "comment": null, "summary": "The rapid advancement of generative AI has enabled the creation of highly\nrealistic forged facial images, posing significant threats to AI security,\ndigital media integrity, and public trust. Face forgery techniques, ranging\nfrom face swapping and attribute editing to powerful diffusion-based image\nsynthesis, are increasingly being used for malicious purposes such as\nmisinformation, identity fraud, and defamation. This growing challenge\nunderscores the urgent need for robust and generalizable face forgery detection\nmethods as a critical component of AI security infrastructure. In this work, we\npropose a novel dual-branch convolutional neural network for face forgery\ndetection that leverages complementary cues from both spatial and frequency\ndomains. The RGB branch captures semantic information, while the frequency\nbranch focuses on high-frequency artifacts that are difficult for generative\nmodels to suppress. A channel attention module is introduced to adaptively fuse\nthese heterogeneous features, highlighting the most informative channels for\nforgery discrimination. To guide the network's learning process, we design a\nunified loss function, FSC Loss, that combines focal loss, supervised\ncontrastive loss, and a frequency center margin loss to enhance class\nseparability and robustness. We evaluate our model on the DiFF benchmark, which\nincludes forged images generated from four representative methods:\ntext-to-image, image-to-image, face swap, and face edit. Our method achieves\nstrong performance across all categories and outperforms average human\naccuracy. These results demonstrate the model's effectiveness and its potential\ncontribution to safeguarding AI ecosystems against visual forgery attacks.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u5206\u652f\uff08RGB+\u9891\u57df\uff09+\u901a\u9053\u6ce8\u610f\u529b\u7684\u5377\u79ef\u7f51\u7edc\u4e0eFSC\u7edf\u4e00\u635f\u5931\uff0c\u5728DiFF\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5bf9\u591a\u7c7b\u4eba\u8138\u4f2a\u9020\u7684\u5f3a\u68c0\u6d4b\u6027\u80fd\uff0c\u8d85\u8fc7\u4eba\u7c7b\u5e73\u5747\u6c34\u5e73\u3002", "motivation": "\u751f\u6210\u5f0fAI\u80fd\u4ea7\u751f\u9ad8\u5ea6\u903c\u771f\u7684\u4eba\u8138\u4f2a\u9020\u56fe\u50cf\uff0c\u5a01\u80c1\u5b89\u5168\u4e0e\u4fe1\u4efb\uff0c\u9700\u6784\u5efa\u901a\u7528\u4e14\u7a33\u5065\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7279\u522b\u9700\u540c\u65f6\u5229\u7528\u7a7a\u95f4\u8bed\u4e49\u4e0e\u9891\u8c31\u9ad8\u9891\u4f2a\u8ff9\u3002", "method": "\u8bbe\u8ba1\u4e86RGB\u5206\u652f\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\u3001\u9891\u57df\u5206\u652f\u6355\u6349\u9ad8\u9891\u4f2a\u9020\u75d5\u8ff9\uff0c\u4f7f\u7528\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u81ea\u9002\u5e94\u878d\u5408\u5f02\u6784\u7279\u5f81\uff1b\u63d0\u51faFSC Loss\uff08focal loss + \u76d1\u7763\u5bf9\u6bd4\u635f\u5931 + \u9891\u7387\u4e2d\u5fc3\u95f4\u9694\u635f\u5931\uff09\u4ee5\u589e\u5f3a\u7c7b\u522b\u53ef\u5206\u6027\u4e0e\u9c81\u68d2\u6027\uff1b\u5728DiFF\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5305\u542b\u6587\u672c\u5230\u56fe\u50cf\u3001\u56fe\u50cf\u5230\u56fe\u50cf\u3001\u6362\u8138\u4e0e\u4eba\u8138\u7f16\u8f91\u56db\u7c7b\u4f2a\u9020\u65b9\u6cd5\u7684DiFF\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u7c7b\u522b\u5747\u8868\u73b0\u51fa\u5f3a\u6027\u80fd\uff0c\u4f18\u4e8e\u5e73\u5747\u4eba\u7c7b\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5206\u652f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\uff08RGB\uff09\u548c\u9891\u57df\u7279\u5f81\u5e76\u5f15\u5165\u901a\u9053\u6ce8\u610f\u529b\u4e0e\u7edf\u4e00\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u5bf9\u4eba\u8138\u4f2a\u9020\u7684\u9c81\u68d2\u68c0\u6d4b\u3002"}}
{"id": "2510.24653", "categories": ["cs.CV", "cs.HC", "J.3"], "pdf": "https://arxiv.org/pdf/2510.24653", "abs": "https://arxiv.org/abs/2510.24653", "authors": ["Veronica Thai", "Rui Li", "Meng Ling", "Shuning Jiang", "Jeremy Wolfe", "Raghu Machiraju", "Yan Hu", "Zaibo Li", "Anil Parwani", "Jian Chen"], "title": "Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology", "comment": "16 pages, 9 figures, submitted to Nature Scientific Data", "summary": "Interpretation of giga-pixel whole-slide images (WSIs) is an important but\ndifficult task for pathologists. Their diagnostic accuracy is estimated to\naverage around 70%. Adding a second pathologist does not substantially improve\ndecision consistency. The field lacks adequate behavioral data to explain\ndiagnostic errors and inconsistencies. To fill in this gap, we present\nPathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual\nsearch and decision-making processes of the full diagnostic workflow during\ncancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse\ninteraction, stimulus tracking, viewport navigation, and diagnostic decision\ndata (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data\ncollection process emphasizes ecological validity through an\napplication-grounded testbed, called PTAH. In total, we recorded 171,909\nfixations, 263,320 saccades, and 1,867,362 mouse interaction events. In\naddition, such data could also be used to improve the training of both\npathologists and AI systems that might support human experts. All experiments\nwere preregistered at https://osf.io/hj9a7, and the complete dataset along with\nanalysis code is available at https://go.osu.edu/pathogaze.", "AI": {"tldr": "PathoGaze1.0\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u751f\u6001\u6548\u5ea6\u9ad8\u7684\u75c5\u7406\u8bca\u65ad\u884c\u4e3a\u6570\u636e\uff08\u773c\u52a8+\u4ea4\u4e92+\u51b3\u7b56\uff09\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u8bca\u65ad\u9519\u8bef\u3001\u57f9\u8bad\u4e0eAI\u8f85\u52a9\u7cfb\u7edf\uff1b\u6570\u636e\u516c\u5f00\u53ef\u590d\u73b0\u3002", "motivation": "\u586b\u8865\u75c5\u7406\u5b66\u9886\u57df\u5173\u4e8e\u8bca\u65ad\u8fc7\u7a0b\u884c\u4e3a\u6570\u636e\u7684\u7a7a\u767d\uff0c\u89e3\u91ca\u75c5\u7406\u8bca\u65ad\u4e2d\u7684\u9519\u8bef\u4e0e\u4e0d\u4e00\u81f4\u6027\uff0c\u63d0\u9ad8\u57f9\u8bad\u4e0eAI\u652f\u6301\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u7ea7\u6d4b\u8bd5\u5e73\u53f0PTAH\uff0c\u5728\u63a5\u8fd1\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e0b\u8bb0\u5f5519\u540d\u75c5\u7406\u5b66\u5bb6\u9605\u8bfb397\u5f20WSI\u65f6\u7684\u773c\u52a8\u3001\u9f20\u6807\u3001\u89c6\u7a97\u548c\u51b3\u7b56\u6570\u636e\uff1b\u6240\u6709\u5b9e\u9a8c\u9884\u6ce8\u518c\u5e76\u516c\u5f00\u6570\u636e\u4e0e\u5206\u6790\u4ee3\u7801\u3002", "result": "\u6784\u5efa\u4e86PathoGaze1.0\u6570\u636e\u96c6\uff0c\u5305\u542b19\u540d\u75c5\u7406\u5b66\u5bb6\u89e3\u91ca397\u5f20WSI\u65f6\u768418.69\u5c0f\u65f6\u773c\u52a8\u3001\u9f20\u6807\u4ea4\u4e92\u3001\u89c6\u7a97\u5bfc\u822a\u4e0e\u8bca\u65ad\u51b3\u7b56\u6570\u636e\uff08EMSVD\uff09\uff1b\u8bb0\u5f55\u4e86171,909\u6b21\u6ce8\u89c6\u3001263,320\u6b21\u626b\u89c6\u548c1,867,362\u6b21\u9f20\u6807\u4e8b\u4ef6\uff1b\u6570\u636e\u4e0e\u4ee3\u7801\u516c\u5f00\u5e76\u4e14\u5b9e\u9a8c\u9884\u6ce8\u518c\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u75c5\u7406\u8bca\u65ad\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u80fd\u7528\u4e8e\u5206\u6790\u89c6\u89c9\u641c\u7d22\u7b56\u7565\u3001\u51b3\u7b56\u8fc7\u7a0b\u5e76\u63a8\u52a8\u6559\u80b2\u4e0eAI\u8f85\u52a9\u5de5\u5177\u53d1\u5c55\u3002"}}
{"id": "2510.24657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24657", "abs": "https://arxiv.org/abs/2510.24657", "authors": ["Xuanpu Zhang", "Xuesong Niu", "Ruidong Chen", "Dan Song", "Jianhao Zeng", "Penghui Du", "Haoxiang Cao", "Kai Wu", "An-an Liu"], "title": "Group Relative Attention Guidance for Image Editing", "comment": null, "summary": "Recently, image editing based on Diffusion-in-Transformer models has\nundergone rapid development. However, existing editing methods often lack\neffective control over the degree of editing, limiting their ability to achieve\nmore customized results. To address this limitation, we investigate the\nMM-Attention mechanism within the DiT model and observe that the Query and Key\ntokens share a bias vector that is only layer-dependent. We interpret this bias\nas representing the model's inherent editing behavior, while the delta between\neach token and its corresponding bias encodes the content-specific editing\nsignals. Based on this insight, we propose Group Relative Attention Guidance, a\nsimple yet effective method that reweights the delta values of different tokens\nto modulate the focus of the model on the input image relative to the editing\ninstruction, enabling continuous and fine-grained control over editing\nintensity without any tuning. Extensive experiments conducted on existing image\nediting frameworks demonstrate that GRAG can be integrated with as few as four\nlines of code, consistently enhancing editing quality. Moreover, compared to\nthe commonly used Classifier-Free Guidance, GRAG achieves smoother and more\nprecise control over the degree of editing. Our code will be released at\nhttps://github.com/little-misfit/GRAG-Image-Editing.", "AI": {"tldr": "GRAG adjusts token-wise 'delta' (token minus layer bias) in MM-Attention to continuously control editing strength in DiT-based image editing; simple to integrate and outperforms CFG in smoothness and precision.", "motivation": "Investigate MM-Attention in DiT and control editing degree in diffusion-in-Transformer image editing; propose mechanism to modulate editing intensity without tuning.", "method": "Paper analysis", "result": "Propose Group Relative Attention Guidance (GRAG) that reweights token deltas to control editing intensity; integrates with minimal code and offers smoother control than Classifier-Free Guidance; validated with experiments.", "conclusion": "GRAG provides fine-grained, continuous control of editing intensity by reweighting content-specific deltas in attention; easy to implement and improves editing quality across frameworks."}}
{"id": "2510.24667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24667", "abs": "https://arxiv.org/abs/2510.24667", "authors": ["Mia Kan", "Yilin Liu", "Niloy Mitra"], "title": "SAGE: Structure-Aware Generative Video Transitions between Diverse Clips", "comment": "Website: https://kan32501.github.io/sage.github.io/", "summary": "Video transitions aim to synthesize intermediate frames between two clips,\nbut naive approaches such as linear blending introduce artifacts that limit\nprofessional use or break temporal coherence. Traditional techniques\n(cross-fades, morphing, frame interpolation) and recent generative inbetweening\nmethods can produce high-quality plausible intermediates, but they struggle\nwith bridging diverse clips involving large temporal gaps or significant\nsemantic differences, leaving a gap for content-aware and visually coherent\ntransitions. We address this challenge by drawing on artistic workflows,\ndistilling strategies such as aligning silhouettes and interpolating salient\nfeatures to preserve structure and perceptual continuity. Building on this, we\npropose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot\napproach that combines structural guidance, provided via line maps and motion\nflow, with generative synthesis, enabling smooth, semantically consistent\ntransitions without fine-tuning. Extensive experiments and comparison with\ncurrent alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate\nthat SAGE outperforms both classical and generative baselines on quantitative\nmetrics and user studies for producing transitions between diverse clips. Code\nto be released on acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAGE\uff0c\u4e00\u79cd\u96f6\u6837\u672c\u7ed3\u6784\u611f\u77e5\u89c6\u9891\u8fc7\u6e21\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ebf\u63cf\u548c\u5149\u6d41\u5f15\u5bfc\u4e0e\u751f\u6210\u6a21\u578b\u5408\u6210\u4e2d\u95f4\u5e27\uff0c\u80fd\u5728\u8bed\u4e49\u5dee\u5f02\u5927\u548c\u65f6\u95f4\u95f4\u9694\u957f\u7684\u7247\u6bb5\u95f4\u751f\u6210\u8fde\u8d2f\u8fc7\u6e21\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u4e49\u5dee\u5f02\u5927\u6216\u65f6\u95f4\u5dee\u8ddd\u957f\u7684\u7247\u6bb5\u65f6\u5e38\u4ea7\u751f\u4f2a\u5f71\u6216\u7ed3\u6784\u6df7\u4e71\uff0c\u9700\u8981\u4e00\u79cd\u5185\u5bb9\u611f\u77e5\u4e14\u4fdd\u6301\u611f\u77e5\u8fde\u8d2f\u6027\u7684\u8fc7\u6e21\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u53d7\u827a\u672f\u5bb6\u5de5\u4f5c\u6d41\u542f\u53d1\uff0c\u63d0\u53d6\u5e76\u5bf9\u9f50\u8f6e\u5ed3\u7ebf\u56fe\u548c\u8fd0\u52a8\u5149\u6d41\u4f5c\u4e3a\u7ed3\u6784\u4fe1\u606f\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5f15\u5bfc\u5408\u6210\uff0c\u91c7\u7528\u663e\u8457\u7279\u5f81\u63d2\u503c\u7b56\u7565\u6765\u4fdd\u6301\u7ed3\u6784\u8fde\u7eed\u6027\uff0c\u5e76\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u8fd0\u884c\u3002", "result": "\u5728\u4e0eFILM\u3001TVG\u3001DiffMorpher\u3001VACE\u3001GI\u7b49\u65b9\u6cd5\u7684\u6bd4\u8f83\u4e2d\uff0cSAGE\u5728\u91cf\u5316\u6307\u6807\u548c\u7528\u6237\u8bc4\u4ef7\u4e2d\u5747\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u66f4\u7a33\u5065\u5730\u751f\u6210\u591a\u6837\u7247\u6bb5\u95f4\u7684\u89c6\u89c9\u8fde\u8d2f\u8fc7\u6e21\u3002", "conclusion": "SAGE\u5728\u5b9a\u91cf\u6307\u6807\u548c\u7528\u6237\u7814\u7a76\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u4f20\u7edf\u548c\u751f\u6210\u57fa\u7ebf\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u7ed3\u6784\u548c\u8bed\u4e49\u4e00\u81f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002"}}
{"id": "2510.24688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24688", "abs": "https://arxiv.org/abs/2510.24688", "authors": ["Yun Zhang", "Zhaoliang Zheng", "Johnson Liu", "Zhiyu Huang", "Zewei Zhou", "Zonglin Meng", "Tianhui Cai", "Jiaqi Ma"], "title": "MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection", "comment": null, "summary": "Infrastructure-based perception plays a crucial role in intelligent\ntransportation systems, offering global situational awareness and enabling\ncooperative autonomy. However, existing camera-based detection models often\nunderperform in such scenarios due to challenges such as multi-view\ninfrastructure setup, diverse camera configurations, degraded visual inputs,\nand various road layouts. We introduce MIC-BEV, a Transformer-based\nbird's-eye-view (BEV) perception framework for infrastructure-based\nmulti-camera 3D object detection. MIC-BEV flexibly supports a variable number\nof cameras with heterogeneous intrinsic and extrinsic parameters and\ndemonstrates strong robustness under sensor degradation. The proposed\ngraph-enhanced fusion module in MIC-BEV integrates multi-view image features\ninto the BEV space by exploiting geometric relationships between cameras and\nBEV cells alongside latent visual cues. To support training and evaluation, we\nintroduce M2I, a synthetic dataset for infrastructure-based object detection,\nfeaturing diverse camera configurations, road layouts, and environmental\nconditions. Extensive experiments on both M2I and the real-world dataset\nRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D\nobject detection. It also remains robust under challenging conditions,\nincluding extreme weather and sensor degradation. These results highlight the\npotential of MIC-BEV for real-world deployment. The dataset and source code are\navailable at: https://github.com/HandsomeYun/MIC-BEV.", "AI": {"tldr": "\u63d0\u51faMIC-BEV\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u548c\u56fe\u589e\u5f3a\u7684BEV\u6846\u67b6\uff0c\u652f\u6301\u5f02\u6784\u591a\u6444\u50cf\u5934\u5e76\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u4e14\u9c81\u68d2\u6027\u5f3a", "motivation": "Existing camera-based models struggle in infrastructure scenarios due to multi-view setups, heterogeneous camera parameters, degraded inputs, and varied road layouts", "method": "Transformer-based BEV for infrastructure multi-camera 3D detection", "result": "MIC-BEV supports variable camera numbers with heterogeneous intrinsics/extrinsics, robust to sensor degradation; introduces graph-enhanced fusion to integrate multi-view features into BEV; M2I synthetic dataset; state-of-the-art results on M2I and RoScenes", "conclusion": "MIC-BEV\u6709\u6548\u5e94\u5bf9\u57fa\u7840\u8bbe\u65bd\u611f\u77e5\u6311\u6218\uff0c\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u6f5c\u529b\uff0c\u4f34\u968fM2I\u6570\u636e\u96c6\u548c\u4ee3\u7801\u516c\u5f00"}}
{"id": "2510.24709", "categories": ["cs.CV", "cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2510.24709", "abs": "https://arxiv.org/abs/2510.24709", "authors": ["Yihao Li", "Saeed Salehi", "Lyle Ungar", "Konrad P. Kording"], "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?", "comment": "Accepted as a Spotlight at NeurIPS 2025", "summary": "Object binding, the brain's ability to bind the many features that\ncollectively represent an object into a coherent whole, is central to human\ncognition. It groups low-level perceptual features into high-level object\nrepresentations, stores those objects efficiently and compositionally in\nmemory, and supports human reasoning about individual object instances. While\nprior work often imposes object-centric attention (e.g., Slot Attention)\nexplicitly to probe these benefits, it remains unclear whether this ability\nnaturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they\ncould: recognizing which patches belong to the same object should be useful for\ndownstream prediction and thus guide attention. Motivated by the quadratic\nnature of self-attention, we hypothesize that ViTs represent whether two\npatches belong to the same object, a property we term IsSameObject. We decode\nIsSameObject from patch embeddings across ViT layers using a similarity probe,\nwhich reaches over 90% accuracy. Crucially, this object-binding capability\nemerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker\nin ImageNet-supervised models, suggesting that binding is not a trivial\narchitectural artifact, but an ability acquired through specific pretraining\nobjectives. We further discover that IsSameObject is encoded in a\nlow-dimensional subspace on top of object features, and that this signal\nactively guides attention. Ablating IsSameObject from model activations\ndegrades downstream performance and works against the learning objective,\nimplying that emergent object binding naturally serves the pretraining\nobjective. Our findings challenge the view that ViTs lack object binding and\nhighlight how symbolic knowledge of \"which parts belong together\" emerges\nnaturally in a connectionist system.", "AI": {"tldr": "\u81ea\u76d1\u7763ViT\u81ea\u53d1\u5b66\u5230\u2018\u54ea\u4e9bpatch\u5c5e\u4e8e\u540c\u4e00\u5bf9\u8c61\u2019\uff08IsSameObject\uff09\uff0c\u53ef\u9ad8\u7cbe\u5ea6\u89e3\u7801\u5e76\u5f71\u54cd\u6ce8\u610f\u529b\uff1b\u76d1\u7763\u8bad\u7ec3\u5219\u5f31\uff1b\u8be5\u4fe1\u53f7\u4e3a\u4f4e\u7ef4\u53ef\u6d88\u878d\u5b50\u7a7a\u95f4\uff0c\u5173\u4e4e\u4e0b\u6e38\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76ViT\u662f\u5426\u80fd\u81ea\u7136\u51fa\u73b0\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\uff08\u5c06patchs\u5f52\u5e76\u4e3a\u5bf9\u8c61\uff09\uff0c\u4ee5\u53ca\u8fd9\u79cd\u80fd\u529b\u662f\u5426\u4f9d\u8d56\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u6216\u4ec5\u7531\u67b6\u6784\u5f15\u8d77\u3002", "method": "\u4f7f\u7528\u76f8\u4f3c\u6027\u63a2\u9488\u5728\u5404\u5c42patch\u5d4c\u5165\u4e0a\u89e3\u7801IsSameObject\uff0c\u6bd4\u8f83\u591a\u79cd\u9884\u8bad\u7ec3\uff08DINO\u3001MAE\u3001CLIP\u4e0eImageNet\u76d1\u7763\uff09\uff0c\u5206\u6790\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u5e76\u6d4b\u91cf\u5bf9\u6ce8\u610f\u529b\u4e0e\u4e0b\u6e38\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "ViTs\u5728\u65e0\u76d1\u7763\u9884\u8bad\u7ec3\u4e0b\u80fd\u81ea\u53d1\u7f16\u7801\u5bf9\u8c61\u7ed1\u5b9a\uff08IsSameObject\uff09\uff0c\u5728patch\u5d4c\u5165\u4e2d\u53ef\u7528\u76f8\u4f3c\u6027\u63a2\u9488\u89e3\u7801\uff0c\u51c6\u786e\u7387>90%\u3002\u8fd9\u79cd\u80fd\u529b\u5728DINO\u3001MAE\u3001CLIP\u7b49\u81ea\u76d1\u7763\u6a21\u578b\u4e2d\u7a33\u5b9a\u51fa\u73b0\uff0c\u800c\u5728ImageNet\u76d1\u7763\u6a21\u578b\u4e2d\u5f31\uff0c\u8868\u660e\u662f\u9884\u8bad\u7ec3\u76ee\u6807\u9a71\u52a8\u800c\u975e\u67b6\u6784\u672c\u8eab\u3002IsSameObject\u4f4d\u4e8e\u5bf9\u8c61\u7279\u5f81\u4e4b\u4e0a\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\uff0c\u5e76\u80fd\u5f15\u5bfc\u6ce8\u610f\u529b\u3002\u6d88\u878d\u8be5\u4fe1\u53f7\u4f1a\u964d\u4f4e\u4e0b\u6e38\u6027\u80fd\u5e76\u4e0e\u5b66\u4e60\u76ee\u6807\u51b2\u7a81\uff0c\u8bf4\u660e\u8be5\u80fd\u529b\u670d\u52a1\u4e8e\u9884\u8bad\u7ec3\u76ee\u6807\u3002", "conclusion": "ViTs\u5e76\u975e\u7f3a\u4e4f\u5bf9\u8c61\u7ed1\u5b9a\u80fd\u529b\uff1b\u7279\u5b9a\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u4f1a\u4fc3\u4f7f\u6a21\u578b\u5728\u5d4c\u5165\u4e2d\u5f62\u6210\u6e05\u6670\u7684IsSameObject\u4fe1\u53f7\uff0c\u8fd9\u4e00\u7b26\u53f7\u5f0f\u77e5\u8bc6\u81ea\u7136\u51fa\u73b0\u5728\u8fde\u63a5\u4e3b\u4e49\u7cfb\u7edf\u4e2d\u5e76\u652f\u6301\u6ce8\u610f\u529b\u4e0e\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2510.24711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24711", "abs": "https://arxiv.org/abs/2510.24711", "authors": ["Yujie Wei", "Shiwei Zhang", "Hangjie Yuan", "Yujin Han", "Zhekai Chen", "Jiayu Wang", "Difan Zou", "Xihui Liu", "Yingya Zhang", "Yu Liu", "Hongming Shan"], "title": "Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model\ncapacity while preserving computational efficiency. Despite its notable success\nin large language models (LLMs), existing attempts to apply MoE to Diffusion\nTransformers (DiTs) have yielded limited gains. We attribute this gap to\nfundamental differences between language and visual tokens. Language tokens are\nsemantically dense with pronounced inter-token variation, while visual tokens\nexhibit spatial redundancy and functional heterogeneity, hindering expert\nspecialization in vision MoE. To this end, we present ProMoE, an MoE framework\nfeaturing a two-step router with explicit routing guidance that promotes expert\nspecialization. Specifically, this guidance encourages the router to partition\nimage tokens into conditional and unconditional sets via conditional routing\naccording to their functional roles, and refine the assignments of conditional\nimage tokens through prototypical routing with learnable prototypes based on\nsemantic content. Moreover, the similarity-based expert allocation in latent\nspace enabled by prototypical routing offers a natural mechanism for\nincorporating explicit semantic guidance, and we validate that such guidance is\ncrucial for vision MoE. Building on this, we propose a routing contrastive loss\nthat explicitly enhances the prototypical routing process, promoting\nintra-expert coherence and inter-expert diversity. Extensive experiments on\nImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods\nunder both Rectified Flow and DDPM training objectives. Code and models will be\nmade publicly available.", "AI": {"tldr": "ProMoE\u901a\u8fc7\u6761\u4ef6+\u539f\u578b\u4e24\u6b65\u663e\u5f0f\u8def\u7531\u548c\u8def\u7531\u5bf9\u6bd4\u635f\u5931\uff0c\u4fc3\u8fdb\u89c6\u89c9MoE\u4e13\u5bb6\u4e13\u95e8\u5316\uff0c\u5728ImageNet\u4e0a\u8d85\u8d8a\u73b0\u6709DiT MoE\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9token\u4e0e\u8bed\u8a00token\u5728\u7279\u6027\u4e0a\u5dee\u5f02\u663e\u8457\uff0c\u89c6\u89c9token\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\u4e0e\u529f\u80fd\u5f02\u8d28\u6027\uff0c\u5bfc\u81f4\u4f20\u7edfMoE\u96be\u4ee5\u5728\u89c6\u89c9\u6269\u6563Transformer\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u4e13\u95e8\u5316\uff0c\u9700\u8bbe\u8ba1\u663e\u5f0f\u8def\u7531\u5f15\u5bfc\u3002", "method": "\u8bbe\u8ba1\u4e86\u6761\u4ef6\u8def\u7531\u5c06\u56fe\u50cftoken\u6309\u529f\u80fd\u5206\u4e3a\u6761\u4ef6/\u65e0\u6761\u4ef6\u96c6\u5408\uff0c\u968f\u540e\u7528\u5e26\u53ef\u5b66\u4e60\u539f\u578b\u7684\u539f\u578b\u8def\u7531\u57fa\u4e8e\u8bed\u4e49\u5185\u5bb9\u7ec6\u5316\u6761\u4ef6token\u7684\u5206\u914d\uff1b\u5f15\u5165\u76f8\u4f3c\u6027\u57fa\u7840\u7684\u4e13\u5bb6\u5206\u914d\u4e0e\u663e\u5f0f\u8bed\u4e49\u5f15\u5bfc\uff0c\u5e76\u63d0\u51fa\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u4ee5\u589e\u5f3a\u4e13\u5bb6\u5185\u90e8\u4e00\u81f4\u6027\u4e0e\u4e13\u5bb6\u95f4\u591a\u6837\u6027\u3002", "result": "\u5728ImageNet\u4e0a\u3001\u5728Rectified Flow\u4e0eDDPM\u8bad\u7ec3\u76ee\u6807\u4e0b\uff0cProMoE\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff1b\u5e76\u9a8c\u8bc1\u4e86\u663e\u5f0f\u8bed\u4e49\u5f15\u5bfc\u4e0e\u8def\u7531\u5bf9\u6bd4\u635f\u5931\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51faProMoE\uff0c\u901a\u8fc7\u4e24\u6b65\u663e\u5f0f\u8def\u7531\u5f15\u5bfc\u4fc3\u8fdb\u89c6\u89c9MoE\u4e13\u5bb6\u4e13\u95e8\u5316\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u5c06MoE\u4ece\u8bed\u8a00\u5e94\u7528\u5230\u89c6\u89c9\u6269\u6563\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2510.24717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.24717", "abs": "https://arxiv.org/abs/2510.24717", "authors": ["Haoge Deng", "Ting Pan", "Fan Zhang", "Yang Liu", "Zhuoyan Luo", "Yufeng Cui", "Wenxuan Wang", "Chunhua Shen", "Shiguang Shan", "Zhaoxiang Zhang", "Xinlong Wang"], "title": "Uniform Discrete Diffusion with Metric Path for Video Generation", "comment": "19 pages, 10 figures", "summary": "Continuous-space video generation has advanced rapidly, while discrete\napproaches lag behind due to error accumulation and long-context inconsistency.\nIn this work, we revisit discrete generative modeling and present Uniform\ndiscRete diffuSion with metric pAth (URSA), a simple yet powerful framework\nthat bridges the gap with continuous approaches for the scalable video\ngeneration. At its core, URSA formulates the video generation task as an\niterative global refinement of discrete spatiotemporal tokens. It integrates\ntwo key designs: a Linearized Metric Path and a Resolution-dependent Timestep\nShifting mechanism. These designs enable URSA to scale efficiently to\nhigh-resolution image synthesis and long-duration video generation, while\nrequiring significantly fewer inference steps. Additionally, we introduce an\nasynchronous temporal fine-tuning strategy that unifies versatile tasks within\na single model, including interpolation and image-to-video generation.\nExtensive experiments on challenging video and image generation benchmarks\ndemonstrate that URSA consistently outperforms existing discrete methods and\nachieves performance comparable to state-of-the-art continuous diffusion\nmethods. Code and models are available at https://github.com/baaivision/URSA", "AI": {"tldr": "URSA \u662f\u4e00\u79cd\u6539\u8fdb\u7684\u79bb\u6563\u6269\u6563\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u5316\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u4f9d\u8d56\u65f6\u95f4\u6b65\u8fc1\u79fb\uff0c\u5b9e\u73b0\u5bf9\u79bb\u6563\u65f6\u7a7a token \u7684\u9ad8\u6548\u8fed\u4ee3\u7cbe\u4fee\uff0c\u663e\u8457\u7f29\u5c0f\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7684\u5dee\u8ddd\uff0c\u652f\u6301\u9ad8\u5206\u8fa8\u7387\u4e0e\u957f\u65f6\u957f\u89c6\u9891\u751f\u6210\u5e76\u7edf\u4e00\u63d2\u5e27/\u56fe\u50cf\u5230\u89c6\u9891\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u8fde\u7eed\u7a7a\u95f4\u89c6\u9891\u751f\u6210\u5feb\u901f\u8fdb\u5c55\uff0c\u800c\u79bb\u6563\u65b9\u6cd5\u56e0\u8bef\u5dee\u7d2f\u79ef\u548c\u957f\u65f6\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u843d\u540e\u3002\u7814\u7a76\u52a8\u673a\u662f\u8ba9\u79bb\u6563\u751f\u6210\u6a21\u578b\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u4e0e\u957f\u65f6\u957f\u573a\u666f\u4e0b\u4e0e\u8fde\u7eed\u65b9\u6cd5\u7ade\u4e89\uff0c\u540c\u65f6\u4fdd\u6301\u79bb\u6563\u8868\u793a\u7684\u4f18\u70b9\uff08\u5982\u53ef\u63a7\u6027\u3001\u7d27\u51d1\u6027\uff09\u3002", "method": "\u5c06\u89c6\u9891\u751f\u6210\u89c6\u4e3a\u5bf9\u79bb\u6563\u65f6\u7a7a token \u7684\u8fed\u4ee3\u5168\u5c40\u7cbe\u4fee\uff1b\u63d0\u51fa Linearized Metric Path\uff08\u7ebf\u6027\u5316\u5ea6\u91cf\u8def\u5f84\uff09\u4ee5\u6539\u8fdb\u76f8\u4f3c\u6027\u5ea6\u91cf\u4e0e\u566a\u58f0\u8c03\u5ea6\uff1b\u5f15\u5165 Resolution-dependent Timestep Shifting\uff08\u5206\u8fa8\u7387\u4f9d\u8d56\u7684\u65f6\u95f4\u6b65\u8fc1\u79fb\uff09\u4ee5\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0a\u9ad8\u6548\u6269\u5c55\u5e76\u51cf\u5c11\u63a8\u7406\u6b65\u6570\uff1b\u5e76\u91c7\u7528\u5f02\u6b65\u65f6\u95f4\u5411\u7cbe\u8c03\u7b56\u7565\u7edf\u4e00\u63d2\u5e27\u4e0e\u56fe\u50cf\u5230\u89c6\u9891\u7b49\u4efb\u52a1\u3002", "result": "\u5728\u591a\u9879\u6311\u6218\u6027\u89c6\u9891\u4e0e\u56fe\u50cf\u751f\u6210\u57fa\u51c6\u4e0a\uff0cURSA \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u79bb\u6563\u65b9\u6cd5\uff0c\u5e76\u5728\u751f\u6210\u8d28\u91cf\u4e0e\u4e00\u81f4\u6027\u4e0a\u63a5\u8fd1\u751a\u81f3\u5339\u914d\u6700\u5148\u8fdb\u7684\u8fde\u7eed\u6269\u6563\u65b9\u6cd5\uff1b\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u4e0e\u957f\u65f6\u957f\u751f\u6210\u65f6\u63a8\u7406\u6b65\u6570\u66f4\u5c11\u4e14\u66f4\u9ad8\u6548\uff1b\u4ee3\u7801\u4e0e\u6a21\u578b\u5f00\u6e90\u3002", "conclusion": "URSA \u5c06\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e0e\u65b0\u7684\u5ea6\u91cf\u8def\u5f84\u548c\u5206\u8fa8\u7387\u4f9d\u8d56\u65f6\u95f4\u6b65\u8fc1\u79fb\u673a\u5236\u7ed3\u5408\uff0c\u901a\u8fc7\u8fed\u4ee3\u5168\u5c40\u7cbe\u4fee\u79bb\u6563\u65f6\u7a7a token\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u6563\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u4f7f\u5176\u5728\u9ad8\u5206\u8fa8\u7387\u4e0e\u957f\u65f6\u957f\u89c6\u9891\u751f\u6210\u4e0a\u63a5\u8fd1\u8fde\u7eed\u6269\u6563\u65b9\u6cd5\u8868\u73b0\u3002"}}
{"id": "2510.24718", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.24718", "abs": "https://arxiv.org/abs/2510.24718", "authors": ["Chonghyuk Song", "Michal Stary", "Boyuan Chen", "George Kopanas", "Vincent Sitzmann"], "title": "Generative View Stitching", "comment": "Project website: https://andrewsonga.github.io/gvs", "summary": "Autoregressive video diffusion models are capable of long rollouts that are\nstable and consistent with history, but they are unable to guide the current\ngeneration with conditioning from the future. In camera-guided video generation\nwith a predefined camera trajectory, this limitation leads to collisions with\nthe generated scene, after which autoregression quickly collapses. To address\nthis, we propose Generative View Stitching (GVS), which samples the entire\nsequence in parallel such that the generated scene is faithful to every part of\nthe predefined camera trajectory. Our main contribution is a sampling algorithm\nthat extends prior work on diffusion stitching for robot planning to video\ngeneration. While such stitching methods usually require a specially trained\nmodel, GVS is compatible with any off-the-shelf video model trained with\nDiffusion Forcing, a prevalent sequence diffusion framework that we show\nalready provides the affordances necessary for stitching. We then introduce\nOmni Guidance, a technique that enhances the temporal consistency in stitching\nby conditioning on both the past and future, and that enables our proposed\nloop-closing mechanism for delivering long-range coherence. Overall, GVS\nachieves camera-guided video generation that is stable, collision-free,\nframe-to-frame consistent, and closes loops for a variety of predefined camera\npaths, including Oscar Reutersv\\\"ard's Impossible Staircase. Results are best\nviewed as videos at https://andrewsonga.github.io/gvs.", "AI": {"tldr": "GVS enables parallel sampling for camera-guided video using Diffusion Forcing models and Omni Guidance to ensure collision-free, temporally consistent, loop-closing generations.", "motivation": "Autoregressive video diffusion models can't use future conditioning, causing collisions with scenes in camera-guided generation with predefined trajectories.", "method": "Extend diffusion stitching methods from robot planning to video; sample entire sequence in parallel; use models trained with Diffusion Forcing; introduce Omni Guidance conditioning on both past and future and loop-closing mechanism.", "result": "Proposed Generative View Stitching (GVS) samples entire sequence in parallel using diffusion stitching methods adapted for video; compatible with off-the-shelf models trained with Diffusion Forcing; introduces Omni Guidance for conditioning on past and future and loop-closing; yields collision-free, consistent, loop-closing camera-guided videos.", "conclusion": "GVS provides a sampling algorithm and Omni Guidance that together allow off-the-shelf diffusion video models to generate long, consistent, collision-free, loop-closing camera-guided videos."}}
