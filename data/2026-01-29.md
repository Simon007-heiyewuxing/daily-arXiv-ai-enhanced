<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Size Matters: Reconstructing Real-Scale 3D Models from Monocular Images for Food Portion Estimation](https://arxiv.org/abs/2601.20051)
*Gautham Vinod,Bruce Coburn,Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 这篇文章提出从单目图像恢复真实尺度的3D重建以用于精确营养估算，通过利用在大规模数据上训练的视觉特征来预测尺度，从而将单视图重建转换为物理真实模型，显著降低体积估计误差。


<details>
  <summary>Details</summary>
Motivation: 当前单目图像的食物体积估计受限于无法恢复真实尺度，影响精确营养评估；已有3D重建方法几乎恢复几何但缺乏真实世界尺度信息。作者目的是弥合3D视觉与数字健康，提升单视图体积估计准确性。

Method: 基于单目3D重建，使用从大规模数据集训练的模型提取的丰富视觉特征来预测重建对象的真实尺度（scale）。将该学习到的尺度应用到单视图3D重建结果，从而输出真实尺寸的三维模型。进行了广泛实验与消融研究验证方法有效性。

Result: 在两个公开数据集上的实验表明，该方法比现有技术稳定更优，实现了平均绝对体积估计误差约30%的降低。

Conclusion: 提出的方法有效恢复单目图像重建的真实尺度，能将3D视觉技术应用于精确营养领域，显著改善食物体积估计，有助于饮食相关慢性病管理。

Abstract: The rise of chronic diseases related to diet, such as obesity and diabetes, emphasizes the need for accurate monitoring of food intake. While AI-driven dietary assessment has made strides in recent years, the ill-posed nature of recovering size (portion) information from monocular images for accurate estimation of ``how much did you eat?'' is a pressing challenge. Some 3D reconstruction methods have achieved impressive geometric reconstruction but fail to recover the crucial real-world scale of the reconstructed object, limiting its usage in precision nutrition. In this paper, we bridge the gap between 3D computer vision and digital health by proposing a method that recovers a true-to-scale 3D reconstructed object from a monocular image. Our approach leverages rich visual features extracted from models trained on large-scale datasets to estimate the scale of the reconstructed object. This learned scale enables us to convert single-view 3D reconstructions into true-to-life, physically meaningful models. Extensive experiments and ablation studies on two publicly available datasets show that our method consistently outperforms existing techniques, achieving nearly a 30% reduction in mean absolute volume-estimation error, showcasing its potential to enhance the domain of precision nutrition. Code: https://gitlab.com/viper-purdue/size-matters

</details>


### [2] [DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2601.20064)
*Zhen Yao,Xin Li,Taotao Jing,Shuai Zhang,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: DiSa利用显著性引导的前/背景解耦与层次化精炼模块，解决VLM在开放词汇语义分割中的前景偏差与定位模糊问题，并在六个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VLM（如CLIP）在图文预训练中对显著、以物体为中心区域有偏好，导致在分割任务中忽视背景（前景偏差）且空间定位不精确；需要一个专门抑制前景偏差并增强空间细节的方法。

Method: 提出Saliency-aware Disentanglement Module (SDM)用于基于显著性提示将特征分为前景和背景两个ensemble并分别建模；提出Hierarchical Refinement Module (HRM)通过像素级空间上下文和通道级多层次更新来精炼特征，从而提升边界定位与背景识别。两者结合形成DiSa框架，面向基于VLM的开放词汇分割任务。

Result: 在六个基准数据集上的大量实验表明，DiSa在常见评价指标上稳定领先于最先进方法，显著提高了背景识别能力与边界精确度。

Conclusion: DiSa通过显式建模显著性（saliency）信息，将前景和背景特征解耦，配合层次化的空间-通道精炼模块，有效缓解了VLM在开放词汇语义分割中的前景偏差与定位限制问题，最终在六个基准上超过了现有方法。

Abstract: Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.

</details>


### [3] [Semi-Supervised Masked Autoencoders: Unlocking Vision Transformer Potential with Limited Data](https://arxiv.org/abs/2601.20072)
*Atik Faysal,Mohammad Rostami,Reihaneh Gh. Roshan,Nikhil Muralidhar,Huaxia Wang*

Main category: cs.CV

TL;DR: 提出SSMAE，在带少量标注和大量未标注数据下训练ViT，结合掩码重建和伪标签分类，使用验证驱动网关在高置信且跨增强一致时开启伪标签，从而减少确认偏差，在CIFAR-10/100小标签率下显著优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 解决在标注稀缺但未标注数据充足情况下，如何高效训练Vision Transformer，尤其关注伪标签引入时机和一致性以减少确认偏差。

Method: 提出Semi-Supervised Masked Autoencoder (SSMAE)：同时优化掩码重建和分类任务，使用动态选择的伪标签；引入验证驱动的门控机制（validation-driven gating），仅在模型对弱/强增强视图均给出高置信且一致预测时启用伪标签。

Result: 在CIFAR-10和CIFAR-100上，SSMAE优于监督ViT和微调的MAE，标签稀少时提升最大（例如CIFAR-10 10%标签下比ViT提高9.24%）。

Conclusion: 伪标签何时被引入与如何生成同等重要；通过慎重控制引入时机和一致性检测，可显著提升ViT在低标注资源下的性能。

Abstract: We address the challenge of training Vision Transformers (ViTs) when labeled data is scarce but unlabeled data is abundant. We propose Semi-Supervised Masked Autoencoder (SSMAE), a framework that jointly optimizes masked image reconstruction and classification using both unlabeled and labeled samples with dynamically selected pseudo-labels. SSMAE introduces a validation-driven gating mechanism that activates pseudo-labeling only after the model achieves reliable, high-confidence predictions that are consistent across both weakly and strongly augmented views of the same image, reducing confirmation bias. On CIFAR-10 and CIFAR-100, SSMAE consistently outperforms supervised ViT and fine-tuned MAE, with the largest gains in low-label regimes (+9.24% over ViT on CIFAR-10 with 10% labels). Our results demonstrate that when pseudo-labels are introduced is as important as how they are generated for data-efficient transformer training. Codes are available at https://github.com/atik666/ssmae.

</details>


### [4] [Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning](https://arxiv.org/abs/2601.20075)
*Chuan Qin,Constantin Venhoff,Sonia Joseph,Fanyi Xiao,Stefan Scherer*

Main category: cs.CV

TL;DR: 本文提出在CLIP训练中直接引入稀疏性（Sparse CLIP），得到既可解释又高效的多模态表示；与后验稀疏自编码器相比，保留了下游性能、多模态能力并提升可解释性，且支持语义概念对齐与可解释的视觉驱动模型。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP表示不透明且难以解释，且普遍认为可解释性与性能冲突；后验稀疏方法（如SAE）虽提升可解释性但损失下游表现和多模态性。作者希望探索在训练阶段引入稀疏性以同时保持性能与可解释性。

Method: 在CLIP训练过程中直接施加稀疏性约束（具体细节未在摘要中给出），以学习稀疏的多模态特征。与后验稀疏自编码器对比，评估下游任务性能、可解释性和模态间对齐能力，并分析训练动态。还训练了一个使用稀疏CLIP表示的视觉-语言模型以验证可解释的视觉控制能力。

Result: Sparse CLIP在保持或接近原始CLIP下游任务性能的同时，显著提升了表示的可解释性；学到的特征多为多模态，可用于直接语义对齐；训练过程揭示了跨模态知识如何出现。作为示例，基于稀疏表示训练的视觉-语言模型能够实现可解释的视觉引导。

Conclusion: 挑战了“可解释性与准确性冲突”的传统观念，证明在训练中共同优化稀疏性与性能是可行的，为未来模型设计提供了可解释-性能并重的路线。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP's dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP's inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.

</details>


### [5] [NucFuseRank: Dataset Fusion and Performance Ranking for Nuclei Instance Segmentation](https://arxiv.org/abs/2601.20104)
*Nima Torbati,Anastasia Meshcheryakova,Ramona Woitek,Sepideh Hatamikia,Diana Mechtcheriakova,Amirreza Mahbod*

Main category: cs.CV

TL;DR: 标准化并融合多个公开H&E细胞核标注数据集，使用两种SOTA模型评估并排序数据集，提出统一测试集NucFuse-test和训练集NucFuse-train，发布实现以建立更公平的细胞核实例分割基准。


<details>
  <summary>Details</summary>
Motivation: Nuclei instance segmentation in H&E images is crucial for automated histological analysis, but existing research emphasizes model development and evaluates on a few arbitrary datasets, limiting generalizability.

Method: Collected and standardized publicly available manually annotated H&E nuclei segmentation datasets; evaluated and ranked them using two SOTA models (CNN and CNN+ViT); proposed unified test set (NucFuse-test) and unified training set (NucFuse-train) by merging datasets; performed external validation and made code public.

Result: Ranked datasets by segmentation performance, demonstrated that fused training set improves generalization, provided comprehensive analyses and external validation results, and released implementations and unified datasets.

Conclusion: Providing standardized datasets, benchmarks, and fused training/testing sets offers a new, fair benchmark that can improve and harmonize evaluation of nuclei instance segmentation methods on H&E images.

Abstract: Nuclei instance segmentation in hematoxylin and eosin (H&E)-stained images plays an important role in automated histological image analysis, with various applications in downstream tasks. While several machine learning and deep learning approaches have been proposed for nuclei instance segmentation, most research in this field focuses on developing new segmentation algorithms and benchmarking them on a limited number of arbitrarily selected public datasets.
  In this work, rather than focusing on model development, we focused on the datasets used for this task. Based on an extensive literature review, we identified manually annotated, publicly available datasets of H&E-stained images for nuclei instance segmentation and standardized them into a unified input and annotation format. Using two state-of-the-art segmentation models, one based on convolutional neural networks (CNNs) and one based on a hybrid CNN and vision transformer architecture, we systematically evaluated and ranked these datasets based on their nuclei instance segmentation performance. Furthermore, we proposed a unified test set (NucFuse-test) for fair cross-dataset evaluation and a unified training set (NucFuse-train) for improved segmentation performance by merging images from multiple datasets.
  By evaluating and ranking the datasets, performing comprehensive analyses, generating fused datasets, conducting external validation, and making our implementation publicly available, we provided a new benchmark for training, testing, and evaluating nuclei instance segmentation models on H&E-stained histological images.

</details>


### [6] [Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing](https://arxiv.org/abs/2601.20107)
*Zhuchenyang Liu,Ziyu Hu,Yao Zhang,Yu Xiao*

Main category: cs.CV

TL;DR: Training-free Structural Anchor Pruning (SAP) finds middle-layer semantic patches to compress visual tokens >90% while keeping retrieval accuracy; OSR protocol shows middle layers retain useful structure.


<details>
  <summary>Details</summary>
Motivation: Reduce index size overheads of vision-language models for Visual Document Retrieval without training.

Method: Identify key visual patches from middle layers (SAP) and evaluate layer-wise information via Oracle Score Retention (OSR).

Result: SAP achieves >90% index vector reduction with strong retrieval performance on ViDoRe.

Conclusion: Middle-layer structural anchors enable effective training-free pruning; final-layer pruning loses structural signals.

Abstract: Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (> 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.

</details>


### [7] [Efficient Token Pruning for LLaDA-V](https://arxiv.org/abs/2601.20168)
*Zhewen Wan,Tianchen Song,Chen Lin,Zhiyong Zhao,Xianpeng Lang*

Main category: cs.CV

TL;DR: 通过对注意力的分析发现LLaDA-V在中后层聚合跨模态信息，提出在第一去噪步骤的中后层进行结构化视觉token剪枝，可将计算量最多减少65%且平均保留95%性能。


<details>
  <summary>Details</summary>
Motivation: Reduce computation of diffusion-based large multimodal models (e.g., LLaDA-V) caused by bidirectional attention and iterative denoising by identifying when cross-modal aggregation occurs and pruning visual tokens accordingly.

Method: Perform attention analysis to find that cross-modal aggregation happens in middle-to-late layers. Propose structured token pruning inspired by FastV: selectively remove visual tokens at designated middle-to-late layers of the first denoising step to cut FLOPs while preserving semantics; this first-step pruning reduces work across subsequent steps.

Result: Best configuration achieves up to 65% reduction in computational cost while preserving on average 95% of task performance across multiple benchmarks.

Conclusion: Targeted structured token pruning in middle-to-late layers of the first denoising step is an effective method to make diffusion-based multimodal models like LLaDA-V more efficient, providing empirical basis for vision-aware pruning and maintaining output quality.

Abstract: Diffusion-based large multimodal models, such as LLaDA-V, have demonstrated impressive capabilities in vision-language understanding and generation. However, their bidirectional attention mechanism and diffusion-style iterative denoising paradigm introduce significant computational overhead, as visual tokens are repeatedly processed across all layers and denoising steps. In this work, we conduct an in-depth attention analysis and reveal that, unlike autoregressive decoders, LLaDA-V aggregates cross-modal information predominantly in middle-to-late layers, leading to delayed semantic alignment. Motivated by this observation, we propose a structured token pruning strategy inspired by FastV, selectively removing a proportion of visual tokens at designated layers to reduce FLOPs while preserving critical semantic information. To the best of our knowledge, this is the first work to investigate structured token pruning in diffusion-based large multimodal models. Unlike FastV, which focuses on shallow-layer pruning, our method targets the middle-to-late layers of the first denoising step to align with LLaDA-V's delayed attention aggregation to maintain output quality, and the first-step pruning strategy reduces the computation across all subsequent steps. Our framework provides an empirical basis for efficient LLaDA-V inference and highlights the potential of vision-aware pruning in diffusion-based multimodal models. Across multiple benchmarks, our best configuration reduces computational cost by up to 65% while preserving an average of 95% task performance.

</details>


### [8] [TeleStyle: Content-Preserving Style Transfer in Images and Videos](https://arxiv.org/abs/2601.20175)
*Shiwen Zhang,Xiaoyan Yang,Bojia Zi,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出TeleStyle：在Qwen-Image-Edit上通过课程化持续学习和视频一致性模块，实现对图像与视频的内容保留风格迁移，并在三项关键指标上获得领先结果。


<details>
  <summary>Details</summary>
Motivation: 解决Diffusion Transformers内部内容与风格表征纠缠，提升内容保留同时实现广泛风格泛化的图像与视频风格迁移能力。

Method: 基于Qwen-Image-Edit构建轻量化模型TeleStyle；构建高质量特定风格数据集并合成大量嘈杂风格三元组；提出课程化持续学习框架，用于在干净与嘈杂数据上训练；新增视频到视频风格迁移模块以增强时序一致性。

Result: 在风格相似性、内容一致性与美学质量三项核心评估指标上达到或超越现有方法，代码与预训练模型已发布。

Conclusion: TeleStyle通过在Qwen-Image-Edit基础上引入课程化持续学习和视频一致性模块，实现了在风格多样性与内容保真之间的有效折衷，能对图像与视频进行高质量、风格保留的迁移。

Abstract: Content-preserving style transfer, generating stylized outputs based on content and style references, remains a significant challenge for Diffusion Transformers (DiTs) due to the inherent entanglement of content and style features in their internal representations. In this technical report, we present TeleStyle, a lightweight yet effective model for both image and video stylization. Built upon Qwen-Image-Edit, TeleStyle leverages the base model's robust capabilities in content preservation and style customization. To facilitate effective training, we curated a high-quality dataset of distinct specific styles and further synthesized triplets using thousands of diverse, in-the-wild style categories. We introduce a Curriculum Continual Learning framework to train TeleStyle on this hybrid dataset of clean (curated) and noisy (synthetic) triplets. This approach enables the model to generalize to unseen styles without compromising precise content fidelity. Additionally, we introduce a video-to-video stylization module to enhance temporal consistency and visual quality. TeleStyle achieves state-of-the-art performance across three core evaluation metrics: style similarity, content consistency, and aesthetic quality. Code and pre-trained models are available at https://github.com/Tele-AI/TeleStyle

</details>


### [9] [Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale](https://arxiv.org/abs/2601.20196)
*Brayden Hamilton,Tim Cashmore,Peter Driscoll,Trevor Gee,Henry Williams*

Main category: cs.CV

TL;DR: CV模型擅长极端类别，LLM在无训练下也能给出可解释判断；混合分割+LLM方法最有前景。


<details>
  <summary>Details</summary>
Motivation: 为了降低潜水检查的危险性并扩大巡检规模，探索自动化、可解释的船体生物污损分级方法，以支持生态与生物安全管理。

Method: 评估了卷积神经网络、基于Transformer的分割模型与零样本LLM（通过结构化提示与检索）的表现，基于新西兰初级产业部的专家标注数据集比较准确率、易解释性与在不同LoF等级上的效果。

Result: Computer vision models（CNNs与Transformer分割模型）在区分极端LoF类别上表现优异，但对中间等级的判别受到数据不均衡与图像取景问题影响。LLMs通过结构化提示与检索在无训练条件下取得了可竞争的性能，并提供可解释的输出。两种方法互补，整合分割覆盖率与LLM推理的混合方法被认为是实现可扩展且可解释生物污损评估的有前景路径。

Conclusion: 建议采用混合方法：使用分割模型估计覆盖率并将其结果与图像及背景信息一起输入LLM，以提升中间LoF等级判别能力并保持可解释性。同时需要更多均衡的数据与标准化取景，以改善模型泛化。

Abstract: Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.

</details>


### [10] [DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment](https://arxiv.org/abs/2601.20218)
*Haoyou Deng,Keyu Yan,Chaojie Mao,Xiang Wang,Yu Liu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DenseGRPO为流匹配模型引入逐步奖励预测与奖励感知采样，通过细粒度反馈和自适应探索显著改善了文本到图像生成的人类偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO-based流匹配模型在文本到图像生成对齐人类偏好时的稀疏奖励问题，使反馈信号与中间去噪步骤的贡献对齐。

Method: 提出DenseGRPO：1) 通过基于ODE的方法在中间清晰图像上应用奖励模型，预测每个去噪步骤的逐步奖励增益作为密集奖励；2) 基于估计的密集奖励，设计奖励感知方案，按时间步自适应调整SDE采样器中的随机注入，从而校准探索空间。

Result: 在多个标准基准上实验表明，DenseGRPO在性能上优于现有GRPO-based方法，密集奖励和奖励感知探索显著提升了人类偏好对齐效果。

Conclusion: 通过引入有效的密集奖励并配合奖励感知的探索校准，DenseGRPO解决了稀疏奖励带来的中间步骤反馈错配问题，为流匹配模型的偏好对齐提供了更精细和有效的训练信号。

Abstract: Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.

</details>


### [11] [Feature Projection Learning for Better Vision-Language Reasoning](https://arxiv.org/abs/2601.20224)
*Yi Zhang,Weicheng Lin,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: 提出FPL：将类别原型投影到查询图像特征空间并重建特征图，以负均方重建误差作为类分数，结合原始CLIP预测，达到更高精度与更低成本。


<details>
  <summary>Details</summary>
Motivation: 现有将CLIP迁移到下游任务的方法在性能、可训练参数或训练时间上存在权衡，需要一种既高效又有效的适配方法。

Method: 设计一个投影模型，将类别原型特征投影到查询图像特征空间并重建查询特征图；以负平均平方重建误差作为类得分；最终结果与原始CLIP输出结合。

Result: 在多项实验证明FPL在精度上大幅超越现有SOTA方法，同时保持参数与计算开销较低。

Conclusion: FPL通过将分类转换为特征投影与重建问题，实现了在参数和计算效率上对CLIP的高效适配，显著优于现有方法。

Abstract: Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \textit{\textbf{F}eature \textbf{P}rojection \textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.

</details>


### [12] [Visual Prompt-Agnostic Evolution](https://arxiv.org/abs/2601.20232)
*Junze Wang,Lei Fan,Dezheng Zhang,Weipeng Jing,Donglin Di,Yang Song,Sidong Liu,Cong Cong*

Main category: cs.CV

TL;DR: PAE通过频域初始化、共享Koopman线性演化与Lyapunov正则化来建模提示动态，解决VPT层间不一致和训练震荡，达到更快更稳的训练及1–3%精度提升。


<details>
  <summary>Details</summary>
Motivation: 观察到现有VPT在训练中表现出梯度震荡——浅层提示早期停滞而深层提示方差大，导致跨层不匹配、收敛慢和性能下降，因而需要显式建模提示动态以稳定训练并提升表现。

Method: PAE包含三部分：1) 频域初始化：通过发现并传播骨干网络利用的频率捷径模式，给提示提供任务感知初始方向；2) 共享Koopman算子：对所有层统一施加全局线性变换以保证层间一致的提示演化，而非各层独立更新；3) Lyapunov风格正则：引入正则项约束演化过程中的误差放大，稳定训练。

Result: 在25个数据集和多种下游任务上，PAE平均加速收敛1.41×，并将精度提升1–3%。此外方法与多种VPT变体兼容、轻量且在推理时无修改。

Conclusion: 本文通过建模提示(prompt)在ViT不同层的动态演化，提出Prompt-Agnostic Evolution (PAE)，有效缓解了VPT训练不稳定、跨层不匹配的问题，从而加速收敛并提升精度。

Abstract: Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\mathtt{PAE}$ accelerates convergence with an average $1.41\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.

</details>


### [13] [BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning](https://arxiv.org/abs/2601.20246)
*Jan Niklas Kolf,Ozan Tezcan,Justin Theiss,Hyung Jun Kim,Wentao Bao,Bhargav Bhushanam,Khushi Gupta,Arun Kejariwal,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 该文提出BLenDeR，一种基于扩散模型的可控采样方法，通过对去噪残差应用集合论启发的并/交运算来增加类内多样性，从而提升深度度量学习表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型用于DML能补充类内样本，但难以在可控且多样的属性组合上进行合成。作者希望通过可控合成提高类内多样性，进而提升DML下游任务性能。

Method: 提出BLenDeR：在扩散模型的采样过程中，对不同文本提示或样本生成的去噪残差进行集合论启发的并操作（保留任一提示的属性）和交操作（通过主成分方向提取共性），以可控地组合属性并生成具有更高类内多样性的样本用于数据增强。

Result: 在标准DML基准（如CUB-200、Cars-196）和多种backbone上进行实验，BLenDeR优于现有最先进方法。具体表现为：在CUB-200上Recall@1提升3.7%，在Cars-196上提升1.8%。

Conclusion: BLenDeR通过对去噪残差的并/交操作，提供了一种可控且有效的生成式样本增强策略，能显著提升DML性能，展示了在类内多样性控制方面的潜力。

Abstract: The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.

</details>


### [14] [Reversible Efficient Diffusion for Image Fusion](https://arxiv.org/abs/2601.20260)
*Xingxin Xu,Bing Cao,DongDong Li,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 提出RED：可逆高效扩散框架，用显式监督替代分布估计，解决扩散融合中的细节损失和效率问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成上能力强，但直接应用于多模态图像融合时由于马尔可夫过程中的噪声累积导致细节丢失，且端到端显式监督训练计算代价高。需要一种既保留扩散生成能力又避免噪声累积与高计算成本的方法。

Method: 设计可逆高效扩散（RED），通过可逆结构和显式监督训练机制继承扩散模型的生成能力，同时跳过分布估计步骤，减少噪声累积并提高计算效率。

Result: They propose RED, a Reversible Efficient Diffusion model for multi-modal image fusion, providing explicit supervision without diffusion's distribution estimation, aiming to preserve details and efficiency.

Conclusion: RED leverages reversible architecture to avoid Markov noise accumulation and enables end-to-end supervised training, improving detail preservation and computational efficiency compared to traditional diffusion-based fusion.

Abstract: Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.

</details>


### [15] [Hallucination Begins Where Saliency Drops](https://arxiv.org/abs/2601.20279)
*Xiaofeng Zhang,Yuanchao Zhu,Chaochen Gu,Xiaosong Yuan,Qiyan Zhao,Jiawei Cao,Feilong Tang,Sinan Fan,Yaomin Shen,Chen Shen,Hao Tang*

Main category: cs.CV

TL;DR: 将注意力与输入梯度融合以度量token级视觉落地性，基于此提出显著性引导的候选拒绝与局部连贯性增强两种推理时机制，有效减少LVLMs幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的幻觉检测方法仅依赖前向注意力模式，忽视梯度信号对token影响路径的揭示，导致难以可靠区分幻觉与事实性输出。作者希望通过引入梯度信息提升诊断与干预的准确性与可解释性。

Method: 方法包括两部分：一是将前向注意力与对应输入梯度相结合计算token级显著性，揭示上下文记忆丧失与幻觉的关联；二是在推理时采用双机制：① Saliency-Guided Rejection Sampling (SGRS)，基于自适应阈值在采样中动态拒绝显著性过低的候选token；② Local Coherence Reinforcement (LocoRE)，轻量插件模块增强当前token对最近前驱token的注意力以抗衡上下文遗忘。

Result: 在多种LVLMs与任务上大规模实验表明，所提方法显著降低了幻觉率，同时保持了生成流畅性与任务性能，提供了一种可解释且实用的模型可靠性增强方案。代码已开源。

Conclusion: 本文提出了LVLMs-Saliency，一种将注意力权重与输入梯度融合的梯度感知诊断框架，用于量化输出每个token的视觉落地（grounding）强度，并据此检测和缓解大视觉-语言模型中的幻觉问题。

Abstract: Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency

</details>


### [16] [A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency](https://arxiv.org/abs/2601.20284)
*Debopom Sutradhar,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出一种无源域适应方法，通过在目标域上对样本进行多视图增强并在潜在空间中强制一致性来学习域不变特征；使用ConvNeXt编码器与分类+一致性损失，免去源域数据、对抗训练或伪标签精修，在三个Office数据集上分别得到90.72%、84%和97.12%的平均分类精度，较现有方法分别提升约+1.23%、+7.26%和+1.77%。


<details>
  <summary>Details</summary>
Motivation: 现有域适应方法通常依赖源域数据访问、对抗训练或复杂伪标签策略，计算成本高且实现复杂。希望设计一种仅利用目标域数据、无需源数据或伪标签的高效适配方法。

Method: 提出首个结合多视图增强与潜在空间一致性（latent consistency）的无源域适应方法：对目标域样本生成多种增强视图，使用ConvNeXt为编码器提取特征；在潜在空间中最小化不同视图特征间距离以强制一致性，同时结合分类损失以保持判别性；整个训练过程中不进行源-目标对齐或伪标签精修。

Result: 在Office-31、Office-Home、Office-Caltech三数据集上分别达到平均分类精度90.72%、84%和97.12%；相对于比较方法，平均提升约+1.23%、+7.26%和+1.77%。

Conclusion: 通过在目标域内利用多视图增强与潜在一致性即可学到可迁移的域不变表征，无需访问源数据或复杂伪标签策略，方法简单且在标准数据集上表现优于现有多数方法，尤其在Office-Home上提升显著。

Abstract: Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\%, 84\%, and 97. 12\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\%, +7.26\%, and +1.77\% on the respective datasets.

</details>


### [17] [Artifact-Aware Evaluation for High-Quality Video Generation](https://arxiv.org/abs/2601.20297)
*Chen Zhu,Jiashu Zhu,Yanxun Li,Meiqi Wu,Bingze Song,Chubin Chen,Jiahong Wu,Xiangxiang Chu,Yangang Wang*

Main category: cs.CV

TL;DR: 本文提出了面向生成视频的细粒度评估协议，通过外观、运动、相机三轴及10类常见伪影分类，建立了大规模数据集GenVID（8万条生成视频）并训练DVAR框架用于密集伪影识别，显著提升了伪影检测与低质量视频筛选能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估方法只给出粗糙质量分数，缺乏定位与分类具体生成缺陷的能力，难以满足对生成视频的可解释审计与内容筛查需求。

Method: 提出三轴（Appearance, Motion, Camera）+10类伪影的分类体系；构建GenVID数据集（80k条由多种SOTA生成模型产出的视频并按伪影类别人工注释）；设计DVAR（Dense Video Artifact Recognition）框架，利用GenVID训练用于逐帧/视频片段的伪影检测与分类。

Result: 实验表明DVAR在伪影检测与分类任务上较基线有明显提升，支持更精细的质量分析与低质量内容过滤，且泛化能力在多模型生成视频上表现良好。

Conclusion: 该工作通过体系化伪影分类、数据集与识别模型，为生成视频的细粒度评估与审计提供了实用工具，有助于提升生成内容质量控制与合规过滤。

Abstract: With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.

</details>


### [18] [Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization](https://arxiv.org/abs/2601.20301)
*Jialuo He,Huangxun Chen*

Main category: cs.CV

TL;DR: Shift sharpness perturbations from weights to pruning masks to get compact robust models


<details>
  <summary>Details</summary>
Motivation: Standard SAM doesn't handle discrete pruning changes; need robustness-aware pruning patterns

Method: analyze abstract

Result: C-SAM trains with mask perturbations to improve robustness under pruning; experiments show up to 42% certified robustness gains while keeping accuracy

Conclusion: C-SAM is effective for finding prunable architectures that retain robustness; outperforms baselines

Abstract: Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.

</details>


### [19] [Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy](https://arxiv.org/abs/2601.20302)
*Suresh Das,Siladittya Manna,Sayantari Ghosh*

Main category: cs.CV

TL;DR: 将少量WA数据（10–30%）加入大量NA训练集，可在导管导致的分布偏移下恢复膀胱分割性能，达到接近纯WA训练的效果，Dice最高达0.94。


<details>
  <summary>Details</summary>
Motivation: 在放射性近距离治疗（gynecological brachytherapy）中，插入导管导致器官形变和成像伪影，使得已大量可用的不含导管CT（NA）无法直接用于训练；研究是否少量目标域（WA）样本能与大量源域（NA）样本联合提升分割鲁棒性。

Method: 构建一个混合数据集并在轴状、冠状和矢状平面上使用多种深度学习架构进行实验；比较仅NA、仅WA以及不同NA/WA比例混合训练的分割性能，评估Dice和IoU等指标来量化域适应效果。

Result: 在混合训练策略下，模型可获得高达Dice=0.94和IoU=0.92的分割性能；10–30% WA样本混入即可显著提升模型对WA图像的泛化能力。

Conclusion: 在膀胱分割任务中，结合无导管（NA）和有导管（WA）CT数据的双域学习策略能在分布偏移下显著提升性能；即便仅加入10–30% WA样本，也能达到接近纯WA训练的效果，从而缓解数据稀缺问题。

Abstract: Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.
  We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.
  The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.

</details>


### [20] [Physically Guided Visual Mass Estimation from a Single RGB Image](https://arxiv.org/abs/2601.20303)
*Sungjae Lee,Junhan Jeong,Yeonjoo Hong,Kwang In Kim*

Main category: cs.CV

TL;DR: 提出将单目几何与材料语义结合并通过自适应门控融合、分支预测体积与密度的单图质量估计方法，在公开数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 质量由体积和材料密度共同决定，而两者均难以直接从RGB外观确定；因此需要引入物理可解释的中间表征来约束解空间，提高从像素预测质量的可靠性与泛化性。

Method: 方法首先从单张图像通过单目深度估计恢复面向物体的三维几何以估计体积；同时利用视觉-语言模型提取粗略的材料语义以辅助密度推断，并编码外观特征。几何、语义和外观表示通过一个实例自适应门控机制融合，然后通过两个独立的回归头在只有质量监督的条件下分别预测与体积和密度相关的潜在因子，最终得到质量估计。

Result: 在image2mass和ABO-500数据集上的实验表明，该方法在质量估计任务上持续优于最先进方法，验证了利用几何和材料语义作为物理引导表示的有效性。

Conclusion: 本文提出了一种从单张RGB图像估计物体质量的物理结构化框架，通过将视觉线索与质量的物理因素（体积与密度）对齐以缓解单目外观不可观测性带来的病态性问题。

Abstract: Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.

</details>


### [21] [Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction](https://arxiv.org/abs/2601.20304)
*Genyuan Zhang,Zihao Wang,Zhifan Gao,Lei Xu,Zhen Zhou,Haijun Yu,Jianjia Zhang,Xiujian Liu,Weiwei Zhang,Shaoyu Wang,Huazhu Fu,Fenglin Liu,Weiwen Wu*

Main category: cs.CV

TL;DR: 提出了SLDM模型，通过结构先验、语义空间监督和减影增强模块，将低剂量碘对比剂CT图像增强为常规剂量效果，提高结构一致性和对造影剂区域的对比度，从而在CT血管造影重建中取得定性与定量改进。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在用低剂量ICM合成常规剂量图像时，难以在不完全配对的数据上实现准确增强，主要因为模型难以识别和保持特定的解剖结构，导致结构不一致或伪影，影响诊断可靠性。

Method: 提出Structure-constrained Language-informed Diffusion Model（SLDM）：1）提取图像结构先验以约束扩散模型的推断，保证结构一致性；2）引入带有空间智能的语义监督策略，将视觉感知与空间推理融合以实现精确增强；3）采用减影血管造影增强模块提高含ICM区域对比度至便于观察的区间。

Result: 在低剂量CT血管造影增强任务上，SLDM在主观视觉对比和若干定量指标（未列明具体指标）上均优于对比方法，效果显著。

Conclusion: SLDM通过结构约束和语言/空间智能的语义监督，能在不完全配对的数据背景下有效将低剂量ICM CT图像增强为接近常规剂量的成像效果，改善了结构一致性与造影剂区域对比，有利于临床诊断。

Abstract: The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.

</details>


### [22] [TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration](https://arxiv.org/abs/2601.20306)
*Yanjie Tu,Qingsen Yan,Axi Niu,Jiacong Tang*

Main category: cs.CV

TL;DR: TPGDiff fuses structural, semantic, and degradation priors at different diffusion depths to enhance unified image restoration, achieving strong results on varied degradations.


<details>
  <summary>Details</summary>
Motivation: Existing unified restoration models fail in heavily degraded regions and naive semantic integration into shallow diffusion layers harms spatial structure; need hierarchical prior integration to guide reconstruction effectively.

Method: Introduce Triple-Prior Guided Diffusion: (1) multi-source structural priors for shallow layers capturing fine details; (2) distillation-driven semantic extractor for robust high-level priors applied to deep layers; (3) degradation extractor providing stage-adaptive guidance across diffusion timesteps; train/evaluate on single- and multi-degradation benchmarks.

Result: TPGDiff proposes integrating three priors—structural, semantic, and degradation—into diffusion-based all-in-one image restoration, assigning structural priors to shallow layers, semantic priors to deep layers, and degradation priors across timesteps. Uses multi-source structural cues, distillation-driven semantic extractor, and degradation extractor. Claims superior performance on single- and multi-degradation benchmarks.

Conclusion: Hierarchical, complementary prior guidance within diffusion models improves reconstruction, especially in severely degraded regions, by preserving spatial details and providing robust high-level semantics, leading to better generalization across restoration scenarios.

Abstract: All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.

</details>


### [23] [OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion](https://arxiv.org/abs/2601.20308)
*Shuoyan Wei,Feng Li,Chen Zhou,Runmin Cong,Yao Zhao,Huihui Bai*

Main category: cs.CV

TL;DR: 提出首个基于扩散的一步STVSR方法OSDEnhancer，结合预插值、混合专家与双向可变形VAE，在复杂真实降解下实现高质量时空重建


<details>
  <summary>Details</summary>
Motivation: 现有VSR/ STVSR方法多基于简化降解假设且难以同时兼顾空间细节与时间一致性，扩散模型虽能生成细节但未充分用于时空联合超分辨

Method: 线性预插值初始化时空结构；TR-SE MoE分别学习时序一致性与空间增强的专家路径并在推理时协同；双向可变形VAE解码器进行递归时空聚合与传播；整个流程通过一步扩散实现高效推理。

Result: 提出了OSDEnhancer，一种用于真实世界时空视频超分辨率（STVSR）的单步扩散框架

Conclusion: 通过线性预插值初始化、TR-SE MoE（时序精化与空间增强混合专家）和双向可变形VAE解码器的协同设计，模型在重建保真度与时间一致性上取得了SOTA并具备较好泛化能力

Abstract: Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.

</details>


### [24] [CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting](https://arxiv.org/abs/2601.20318)
*Jiyuan Xu,Wenyu Zhang,Xin Jing,Shuai Chen,Shuai Zhang,Jiahao Nie*

Main category: cs.CV

TL;DR: 提出CPiRi，通过时空解耦与通道置换不变正则，使模型从数据中学习跨通道关系，具备置换不变性并在多项基准上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决通道依赖模型对通道顺序过拟合和通道独立模型忽略跨通道依赖的问题，提升对通道增减或重排时的适应性

Method: Spatio-temporal decoupling + permutation-invariant regularization

Result: State-of-the-art forecasting performance; robust to channel reorderings; generalizes to unseen channels; efficient on large datasets

Conclusion: CPiRi achieves permutation invariance, learns cross-channel relations without memorizing order, and enables deployment under structural/distributional co-drift

Abstract: Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \textbf{CPiRi}, a \textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \textbf{spatio-temporal decoupling architecture} with \textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \textbf{inductive generalization} to unseen channels even when trained on \textbf{only half} of the channels, while maintaining \textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.

</details>


### [25] [GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction](https://arxiv.org/abs/2601.20331)
*Mai Su,Qihan Yu,Zhongtao Wang,Yilong Li,Chengwei Pan,Yisong Chen,Guoping Wang*

Main category: cs.CV

TL;DR: 通过可见性加权的多视角几何一致性和渐进四叉树块级单目深度校准，该工作显著改善了3D高斯点重建的表面几何准确性，实验证明在DTU与TNT数据集上优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯渲染的表面重建在深度估计精度上受限：多视角几何一致性在大几何差异下失效，单目深度先验存在尺度不确定性与局部不一致，导致高斯深度监督不准确。论文旨在通过结合可见性加权的多视角约束与分层校准的单目深度先验来解决这些问题。

Method: 提出两大核心方法：1）Gaussian visibility-aware multi-view geometric consistency：通过聚合不同视图中共享高斯原语的可见性信息，对几何监督进行加权，提升了几何约束的准确性和稳定性；2）Progressive quadtree-calibrated Monocular depth constraint：在从粗到细的块级（四叉树）尺度上执行仿射标定，缓解单目深度的尺度歧义并保留细节。

Result: 在DTU和TNT数据集上进行了大量实验证明：相较于先前高斯基和隐式表面重建方法，该方法在几何精度上具有稳定且一致的提升。作者并公开了代码仓库以便复现。

Conclusion: 该论文通过引入可见性感知的多视角几何一致性约束和渐进四叉树校准的单目深度约束，有效提升了基于3D高斯点渲染（Gaussian Splatting）方法的表面重建精度，克服了多视角约束在几何差异大时不稳定以及单目深度先验尺度歧义与局部不一致的问题。

Abstract: 3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.

</details>


### [26] [Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining](https://arxiv.org/abs/2601.20333)
*Ali Zia,Usman Ali,Umer Ramzan,Abdul Rehman,Abdelwahed Khamis,Wei Xiang*

Main category: cs.CV

TL;DR: TopoOT利用多滤波PD与最优传输链对齐生成稳定性伪标签，结合TTA和对比学习实现对分布漂移的稳健异常分割，显著提升了2D/3D基准性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于阈值的二值化在分布漂移下易碎，而拓扑数据分析能刻画跨尺度的全局结构不变量，适合用于异常分割任务。

Method: 核心方法为Optimal Transport Chaining：对不同阈值和滤波器的PD进行序列化对齐，计算沿路径的测地稳定性分数，并用这些分数生成伪标签指导轻量级头部在线训练，损失包含OT一致性和对比目标。

Result: 在标准2D和3D异常检测基准上，TopoOT达到最先进表现，2D数据集平均F1提升最多+24.1%，3D提升最多+10.2%。

Conclusion: 本文提出了TopoOT，通过将多重滤波 persistence diagrams 与最优传输（OT）和测试时自适应（TTA）结合，实现了鲁棒的异常分割。

Abstract: Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.

</details>


### [27] [MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis](https://arxiv.org/abs/2601.20347)
*Chengying She,Chengwei Chen,Xinran Zhang,Ben Wang,Lizhuang Liu,Chengwei Shao,Yun Bian*

Main category: cs.CV

TL;DR: 提出了MMSF，一种多任务多模态监督框架，基于线性复杂度MIL骨干，分解并融合WSI和临床数据的共享/特有表示；包含图特征提取、临床嵌入、特征融合和Mamba MIL编码器；在CAMELYON16、TCGA-NSCLC及五个TCGA生存队列上均优于对比方法。


<details>
  <summary>Details</summary>
Motivation: 病理学中全片图(WSI)与患者临床特征互为补充，但两类模态特征分布和尺度差异大，使得融合困难，且需有效利用切片内空间拓扑信息与患者级属性进行预测与生存分析。

Method: 提出MMSF框架：1) 图特征提取模块在patch级别编码组织拓扑；2) 临床数据嵌入模块对患者属性标准化并编码；3) 特征融合模块显式分解为模态共享与模态特异表示并对齐；4) 基于Mamba的线性复杂度MIL编码器与多任务预测头联合训练，实现分类与生存预后任务。

Result: 在CAMELYON16和TCGA-NSCLC分类任务上，MMSF分别比强基线提升2.1–6.6%的准确率与2.2–6.9%的AUC；在五个TCGA生存队列上，C-index较单模态方法提升7.1–9.8%，较其他多模态方法提升5.6–7.1%。

Conclusion: MMSF通过显式分解并对齐跨模态共享与特异表示，同时利用组织拓扑与临床嵌入，有效提升了病理图像与临床信息融合的分类与生存预测性能，证明了该方法在多模态病理学任务上的优势。

Abstract: Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\% accuracy and 2.2--6.9\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\% C-index improvements compared with unimodal methods and 5.6--7.1\% over multimodal alternatives.

</details>


### [28] [PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification](https://arxiv.org/abs/2601.20351)
*Chenke Zhang,Ziyuan Yang,Licheng Yan,Shuyi Li,Andrew Beng Jin Teoh,Bob Zhang,Yi Zhang*

Main category: cs.CV

TL;DR: 提出PalmBridge：基于向量量化的特征空间对齐框架，通过学习代表向量并在特征上进行映射与融合，抑制域间无关变异以提升掌纹开放集验证的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有掌纹模型在部署时受异构条件导致的特征分布漂移影响，容易过拟合数据集特有纹理，数据增强不足以覆盖严重域差异，需在特征层面实现领域不变表示。

Method: PalmBridge学习一组紧凑的代表向量（码本），在注册与验证时将每个特征映射到最近代表向量并与原始特征按权重融合；代表向量与主干网络共同优化，辅以任务监督、特征一致性损失和正交正则化以构建稳定共享嵌入空间，同时通过映射一致性和碰撞率分析模型对融合权重的敏感性。

Result: 在多数据集与不同主干网络上进行实验，PalmBridge在数据集内开放集评测中持续降低EER，并在跨数据集泛化上提升性能，且运行开销从可忽略到适度。

Conclusion: PalmBridge作为可插拔模块，通过特征空间量化与融合，有效抑制域间噪声而保留判别信息，提升掌纹验证在异构部署下的鲁棒性与泛化能力。

Abstract: Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.

</details>


### [29] [Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models](https://arxiv.org/abs/2601.20354)
*Zengbin Wang,Xuecai Hu,Yong Wang,Feng Xiong,Man Zhang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 本文提出SpatialGenEval基准与SpatialT2I数据集，通过信息密集的长提示系统评估并改进T2I模型的空间智能，重大评测显示高阶空间推理仍是瓶颈，微调带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 分析当前Text-to-image模型在处理复杂空间关系（如位置、布局、遮挡与因果）方面的不足，提出一个更严格的信息密集型基准以评估模型的空间智能。

Method: 构建SpatialGenEval基准：1,230条长且信息密集的提示，覆盖25个真实场景与10个空间子域，每条提示附带10个多项选择问答；并基于信息密集提示重写生成SpatialT2I数据集，包含15,400条文本-图像对，用于微调并评估模型。评测了21个先进模型，并在Stable Diffusion-XL、Uniworld-V1、OmniGen2上进行微调实验。

Result: 在21个模型的广泛评估中，发现高阶空间推理依然是主要瓶颈。使用SpatialT2I微调三大模型分别带来+4.2%、+5.7%、+4.4%的平均性能提升，并在空间关系表现上更逼真。

Conclusion: 信息密集、结构化的提示和数据能有效提升T2I模型的空间智能，提出了以数据为中心的路线来弥补当前模型在空间推理与关系表达方面的不足。

Abstract: Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.

</details>


### [30] [CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization](https://arxiv.org/abs/2601.20355)
*Yue Liang,Jiatong Du,Ziyi Yang,Yanjun Huang,Hong Chen*

Main category: cs.CV

TL;DR: CURVE通过变分不确定性和原型条件去偏实现结构稀疏化与域稳定，从而提高场景图对分布偏移的鲁棒性并支持风险估计。


<details>
  <summary>Details</summary>
Motivation: 现有场景图方法易陷入数据中虚假的共现偏差，导致在分布外测试时性能显著下降；需要一种可提供可靠不确定性估计且能学习域不变结构的框架。

Method: 结合变分不确定性建模与基于不确定性的结构正则化；利用原型条件去偏，分离不变交互动力学与环境相关变异；通过稀疏化拓扑实现域稳定性。

Result: 在零样本转移与低样本sim-to-real自适应任务上，CURVE学到稀疏、域稳定的拓扑并提供可信的不确定性估计，从而在分布转移下改善风险预测和性能表现。

Conclusion: CURVE能有效抑制与环境相关的高方差关系，提升场景图在分布外场景的泛化能力。

Abstract: Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.

</details>


### [31] [RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching](https://arxiv.org/abs/2601.20364)
*Zhen Liu,Diedong Feng,Hai Jiang,Liaoyuan Zeng,Hao Wang,Chaoyu Feng,Lei Lei,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 作者将RGB-to-RAW重建重塑为潜在空间的确定性流匹配问题，提出RAW-Flow（包含跨尺度上下文引导和双域潜在自编码器）以提升细节与色彩恢复，实验验证优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法将任务视为直接回归，受逆ISP的病态性和量化RGB信息缺失影响，在细节一致性和颜色保真上受限，因此采用生成式潜在输运观点以更好恢复高保真RAW。

Method: 提出RAW-Flow框架：在潜在空间使用流匹配（flow matching）学习从RGB潜在表示到RAW潜在表示的确定性向量场；设计跨尺度上下文引导模块，将层次化RGB特征注入流估计；构建双域潜在自编码器（同时编码RGB与RAW）并加入特征对齐约束以促进稳定训练与高保真重建。

Result: 大量实验表明RAW-Flow在定量指标与视觉效果上均优于现有最先进方法，能更准确地恢复结构细节与颜色信息。

Conclusion: 本文提出将RGB-to-RAW重建视为确定性潜在输运问题，通过流匹配学习潜在空间确定性向量场，能更好恢复细节与色彩，并通过跨尺度上下文引导与双域潜在自编码器及特征对齐稳定训练与提升重建质量。

Abstract: RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.

</details>


### [32] [Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics](https://arxiv.org/abs/2601.20366)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib,Nihal Das Ankur,Anish Giri*

Main category: cs.CV

TL;DR: 本文提出并实现了一个低成本的双子系统IoT平台：子系统1为RFID认证与伺服门控并同步记录至Google Sheets，子系统2为火焰、水流、人员识别等安全监测；45天实测表明系统在准确性和可靠性上表现优异，且实现成本显著低于商业方案。


<details>
  <summary>Details</summary>
Motivation: 当前物理安防与环境安全系统通常各自为政，造成应急响应延迟与管理复杂度高。为提高效率与降低成本，提出一个统一的云架构以实现两类系统的协同工作。

Method: 设计两套协调子系统并采用ESP32进行边缘处理：子系统1实现RFID认证、伺服控制与Google Sheets实时记录；子系统2集成火焰传感、水流测量、LCD显示与人员识别。引入本地缓存以在网络中断时保证数据完整性，所有数据统一上报到云端进行监控与日志记录。

Result: 45天实验结果：RFID认证准确率99.2%，平均响应时间0.82秒；火焰检测在5米范围内可靠性98.5%；云端数据记录成功率99.8%；系统在网络断连时通过本地缓存维持完整性；总成本约5,400 BDT（约48美元），较商业集成解决方案节省82%。

Conclusion: 该论文展示了一个将物理安防与环境安全监测融合的双模态物联网框架，通过ESP32边缘处理、RFID门禁控制和多传感器安全监测实现协同运行，证明在低成本下仍能达到接近商用系统的性能。

Abstract: The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\% RFID authentication accuracy with 0.82-second average response time, 98.5\% flame detection reliability within 5-meter range, and 99.8\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \$48), representing an 82\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.

</details>


### [33] [RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting](https://arxiv.org/abs/2601.20369)
*Mas Nurul Achmadiah,Chi-Chia Sun,Wen-Kai Kuo,Jun-Wei Hsieh*

Main category: cs.CV

TL;DR: RepSFNet：轻量级无注意力群计数模型，RepLK-ViT大核提取+ASPP+CAN融合，MSE+OT损失，推理延迟降低最多34%。


<details>
  <summary>Details</summary>
Motivation: 解决群体计数中尺度变化、遮挡与模型复杂度高的问题，目标是实现准确且适合实时/低功耗边缘计算的网络。

Method: 采用RepLK-ViT大卷积核骨干进行高效多尺度特征提取；构建ASPP+CAN的Feature Fusion以实现密度自适应上下文建模；使用Concatenate Fusion保持空间分辨率生成高质量密度图；训练时结合MSE与Optimal Transport损失以提升计数精度与空间分布对齐。

Result: RepSFNet提出了一种轻量化、实时的群体计数网络，通过大卷积核的RepLK-ViT骨干网络和融合ASPP与CAN的特征融合模块实现多尺度与密度自适应语境建模，同时使用Concat融合保留空间分辨率；训练时结合MSE与Optimal Transport损失；在多个数据集上达到与SOTA相近精度并显著降低推理延迟。

Conclusion: RepSFNet在参数与计算复杂度上显著优化，适合实时与边缘设备部署，同时在精度上保持竞争力，证明大核重参数化+简洁融合模块是解决尺度与遮挡问题的有效途径。

Abstract: Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.

</details>


### [34] [HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation](https://arxiv.org/abs/2601.20383)
*Mengge Liu,Yan Di,Gu Wang,Yun Qu,Dekai Zhu,Yanyan Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: HINT: first autoregressive diffusion framework with hierarchical interaction modeling (disentangled latent + sliding-window with local/global conditioning) enabling variable agents and long texts; achieves strong SOTA results (InterHuman FID 3.100).


<details>
  <summary>Details</summary>
Motivation: Existing offline fixed-length, fixed-agent methods cannot handle long/variable texts or changing agent counts; need autoregressive models that model interactions over long horizons while handling variable participants; HINT addresses this with hierarchical interaction modeling in diffusion.

Method: Autoregressive hierarchical diffusion for multi-human motion

Result: HINT uses a disentangled canonical latent motion representation separating local per-person motions and inter-person interactions; employs sliding-window autoregressive generation aggregating within-window local conditions and cross-window global conditions; diffusion model samples future window conditioned on past and text; adapts to variable agent counts and long texts; achieves SOTA FID 3.100 on InterHuman beating prior 5.154.

Conclusion: HINT provides scalable, online-capable multi-human motion generation with superior fidelity and interaction modeling, matching offline models and outperforming autoregressive baselines.

Abstract: Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.

</details>


### [35] [Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models](https://arxiv.org/abs/2601.20419)
*Yuhao Sun,Chengyi Cai,Jiacheng Zhang,Zesheng Ye,Xingliang Yuan,Feng Liu*

Main category: cs.CV

TL;DR: BiFTA prunes redundant image patches and text descriptions to enhance fine-grained alignment, yielding better zero-shot CLIP results.


<details>
  <summary>Details</summary>
Motivation: Fine-grained alignment helps CLIP but redundant patches/descriptions reduce effectiveness; need to refine views and descriptions to increase distinctiveness and diversity.

Method: Summarize method, results, conclusion, tldr, motivation

Result: BiFTA removes redundant image patches (by IoU) and redundant text descriptions (by cosine similarity) to improve fine-grained text-visual alignment. They call these View refinement and Description refinement; applied to CLIP (ViT and ResNet) improves zero-shot performance on 6 benchmarks.

Conclusion: Removing redundant visual and textual information improves zero-shot alignment and performance; BiFTA is effective across architectures.

Abstract: Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.

</details>


### [36] [Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance](https://arxiv.org/abs/2601.20425)
*Chenliang Zhou,Fangcheng Zhong,Weihao Xia,Albert Miao,Canberk Baykal,Cengiz Oztireli*

Main category: cs.CV

TL;DR: 提出四重扩散（Global latent、Symmetry、Parts、Assembly）用于结构感知点云生成，实现对称性保证、分部组合、细粒度可控，并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么整体生成缺乏可控性与结构保障，要么仅支持部件组合但不保证对称性。本工作旨在将对称性和部件先验集成进生成流程，提升生成质量与可控性。

Method: 设计四个协同的扩散模型分别学习全局形状潜变量、对称性分布、语义部件以及部件空间组装，利用全局潜变量强化组装一致性，并通过显式对称性模块保证对称性，支持对单独部件进行操控。

Result: Quartet of Diffusions introduces a structure-aware point cloud generation framework using four coordinated diffusion models for global latents, symmetries, semantic parts, and spatial assembly.

Conclusion: The method enforces symmetry and part composition during generation, yielding coherent, controllable, diverse, high-quality 3D point clouds and achieves state-of-the-art performance.

Abstract: We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.

</details>


### [37] [Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding](https://arxiv.org/abs/2601.20430)
*Kun Yin,Yunfei Wu,Bing Liu,Zhongpeng Cai,Xiaotian Li,Huang Chen,Xin Li,Haoyu Cao,Yinsong Liu,Deqiang Jiang,Xing Sun,Yunsheng Wu,Qianyu Li,Antai Guo,Yanzhen Liao,Yanqiu Qu,Haodong Lin,Chengxu He,Shuangyin Liu*

Main category: cs.CV

TL;DR: 本文提出了 Youtu-Parsing：使用动态分辨率 ViT 提取共享视觉特征，配合提示引导的 Youtu-LLM-2B 进行布局分析和区域提示解码；引入 token parallelism（最多并行生成64个候选token）和 query parallelism（最多并行处理5个框）两种高并行解码策略，在保持质量的前提下分别带来约5–11x和额外约2x的加速，在表格等结构化场景中效果显著，且在多基准上达到 SOTA。


<details>
  <summary>Details</summary>
Motivation: 降低文档解析中自回归解码的瓶颈，提高大规模文档智能应用的推理效率与适应性，同时支持多种元素（文本、公式、表格、图表、印章、层级结构）并增强对稀有字符、多语种和手写体的鲁棒性。

Method: 1) 采用原生 ViT 的动态分辨率视觉编码器以提取可复用的共享特征；2) 使用提示引导的 Youtu-LLM-2B 进行布局分析与区域提示解码；3) 提出 token parallelism：每步并行生成多达64个 token 候选并通过验证机制筛选，减少自回归步数；4) 提出 query parallelism：在同一前向过程中并行对最多5个边界框进行内容预测，进一步提速；5) 结合两种并行策略，适配多种文档元素并保持输出质量。

Result: 在 OmniDocBench 与 olmOCR-bench 基准上达到 SOTA 性能；在高度结构化任务（如表格识别）上 token parallelism 带来约5–11倍加速，query parallelism 再提供约2倍额外加速；模型对稀有字符、多语种与手写文本展现出较强鲁棒性，具备实际大规模部署价值。

Conclusion: Youtu-Parsing 是一个高效且多功能的文档解析模型，通过将视觉编码与语言模块解耦并复用特征，实现了显著的解码加速与强鲁棒性。

Abstract: This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.

</details>


### [38] [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/abs/2601.20433)
*Wenbo Xu,Wei Lu,Xiangyang Luo,Jiantao Zhou*

Main category: cs.CV

TL;DR: 提出MARE：结合视觉-语言模型、多模态对齐、RLHF奖惩函数和伪造解缠模块，提高Deepfake检测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成模型快速发展导致更复杂的伪造手段，传统仅分类或空间定位的方法难以满足可解释性和可靠性需求，需利用VLM进行文本-空间对齐的推理并结合人类反馈提升模型行为。

Method: MARE设计了综合奖励函数并引入RLHF，促使模型生成符合人类偏好的文本-空间对齐推理内容；同时增加伪造解缠模块以从高层面人脸语义提取伪造痕迹并与VLM对齐。

Result: 在定量与定性评估中，MARE在检测准确率和生成推理的可靠性上超过现有方法，展示了更好的可解释性与稳健性。

Conclusion: 本文提出的MARE方法通过多模态对齐、RLHF和伪造解缠模块提升了VLM在Deepfake检测与可解释性推理上的性能，实验证明在准确性和可靠性上达到SOTA水平。

Abstract: Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.

</details>


### [39] [Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection](https://arxiv.org/abs/2601.20461)
*Yanzhu Liu,Xiao Liu,Yuexuan Wang,Mondal Soumik*

Main category: cs.CV

TL;DR: Use generators' final components to perturb real images, train detector on these, achieves 98.83% avg accuracy on unseen generators with few samples


<details>
  <summary>Details</summary>
Motivation: Many modern image generators, despite different paradigms, share common final components that map representations to images; leveraging them helps build generalizable detectors

Method: contaminate real images with generator final components and train detector

Result: detector fine-tuned on DINOv3 backbone using 100 samples per category, evaluated on 22 unseen generators, 98.83% average accuracy

Conclusion: contaminating real images with final components of generators yields detectors that generalize well to unseen generators

Abstract: With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to "contaminate" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.

</details>


### [40] [Efficient Autoregressive Video Diffusion with Dummy Head](https://arxiv.org/abs/2601.20499)
*Hang Guo,Zhaoyang Jia,Jiahao Li,Bin Li,Yuanhao Cai,Jiangshan Wang,Yawei Li,Yan Lu*

Main category: cs.CV

TL;DR: 提出 Dummy Forcing，用于在自回归视频扩散模型中减少多头自注意力对历史帧的冗余利用，通过异构内存分配、动态头编程和上下文打包，降低 KV 缓存占用并加速生成，在不额外训练下取得最高 2.0x 加速且质量下降 <0.5%。


<details>
  <summary>Details</summary>
Motivation: 观察到多头自注意力中约 25% 的头几乎只关注当前帧，丢弃这些头的 KV 缓存对性能影响小，表明存在头间上下文利用冗余与缓存浪费，动机是减少不必要的历史上下文访问以提升解码速度与内存效率。

Method: 提出 Dummy Forcing，包括：1) 异构内存分配：为不同头分配不同的上下文访问权限以减少冗余；2) 动态头编程：在推理时自适应地分类头的类型（如只看当前帧或跨帧），以决定是否使用缓存；3) 上下文打包：对可控的历史上下文进行紧凑压缩以更激进地减少缓存大小。该方法无需额外训练即可应用。

Result: 在不额外训练的条件下，Dummy Forcing 在基线模型上实现最多 2.0x 的推理加速，支持 24.3 FPS 视频生成，且质量下降小于 0.5%。此外，约 25% 的头被识别为仅关注当前帧，并可安全丢弃它们的 KV 缓存。

Conclusion: 通过对多头注意力中各头的上下文访问进行显式控制和压缩，可以显著减少推理时的缓存负担并加速自回归视频扩散生成，同时基本保持生成质量。

Abstract: The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.

</details>


### [41] [Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI](https://arxiv.org/abs/2601.20503)
*Jesse Phitidis,Alison Q. Smithard,William N. Whiteley,Joanna M. Wardlaw,Miguel O. Bernabeu,Maria Valdés Hernández*

Main category: cs.CV

TL;DR: 提出并比较了六种训练策略，用于在部分标注的多源MRI数据上联合分割WMH和ISL，伪标签策略表现最好。


<details>
  <summary>Details</summary>
Motivation: WMH与ISL在FLAIR序列上视觉相似且常共存，导致联合分割困难；然而现实中大量数据为部分标注，研究如何利用部分标注数据提高联合分割性能具有重要价值。

Method: 在总计2052个MRI体积的数据集上，结合私有完全/部分标注数据和公开部分标注数据，评估六种训练策略，包括直接训练、联合训练、冻结策略、多任务学习、伪标签生成等，比较它们在WMH和ISL分割上的效果。

Result: 该论文构建并验证了用于分割白质高信号（WMH）和缺血性脑梗死病灶（ISL）的深度学习模型，目标是处理部分标注数据的挑战。

Conclusion: 在包含2052例MRI数据（其中1341例带有WMH标注，1152例带有ISL标注）的数据集中，多个方法能有效利用部分标注数据提升模型性能；生成伪标签的方法效果最佳。

Abstract: White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.

</details>


### [42] [Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V](https://arxiv.org/abs/2601.20504)
*Meiqi Wu,Bingze Song,Ruimin Lin,Chen Zhu,Xiaokun Feng,Jiahong Wu,Xiangxiang Chu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 提出了Latent Temporal Discrepancy (LTD)，一种在隐空间中度量帧间变化并用于加权训练损失的运动先验，从而提升视频生成模型对动态区域的建模能力与训练稳定性，在VBench和VMBench上分别提升了3.31%与3.58%。


<details>
  <summary>Details</summary>
Motivation: 现有扩散视频生成模型使用对静态场景统一的静态损失，无法有效应对剧烈动态变化；噪声会破坏时间一致性并增加动态区域学习难度，因此需要一种能够区分并强调动态区域的损失策略。

Method: 在隐空间计算相邻帧之间的差异作为Latent Temporal Discrepancy (LTD)，基于LTD对训练损失进行加权：高差异区域赋予更大惩罚以增强对高频动态的重建，低差异区域保持常规优化以保证稳定性。该策略作为运动先验融入扩散模型训练。

Result: 在通用基准VBench和注重运动的VMBench上进行大量实验，方法分别带来3.31%和3.58%的性能提升，显著改善运动质量，相较强基线有一致性增益。

Conclusion: LTD作为运动感知的损失加权方法，可稳定训练并提升模型对快速动态的重建能力，是解决视频生成中时间一致性和动态建模问题的一种有效途径。

Abstract: Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.

</details>


### [43] [Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits](https://arxiv.org/abs/2601.20511)
*Zelong Sun,Jiahui Wu,Ying Ba,Dong Jing,Zhiwu Lu*

Main category: cs.CV

TL;DR: 提出Portrait Collection Generation(PCG)任务和CHEESE数据集及SCheese模型，解决多属性编辑与细节保持问题，数据集含24K集合、573K样本，方法结合自适应特征融合与ConsistencyNet实现身份和细节一致性。


<details>
  <summary>Details</summary>
Motivation: 随着社交媒体上对多样化高质量人像集合的需求增长，需要能通过自然语言指令基于参考人像生成连贯人像集合的方法，同时要应对复杂多属性编辑与高保真细节保持的挑战。

Method: 构建CHEESE：使用大型视觉语言模型和基于反演的验证构建包含24K集合和573K样本的数据集；提出SCheese框架：结合文本驱动生成、分层身份与细节保持，采用自适应特征融合保持身份一致性，使用ConsistencyNet注入细粒度特征保持细节一致。

Result: 基于CHEESE的数据和SCheese模型在全面实验中展示了显著优势，SCheese在PCG任务上达到了最先进性能，验证了数据集和方法的有效性。

Conclusion: 提出了PCG任务、首个大规模数据集CHEESE和专门模型SCheese，为通过自然语言在参考人像基础上生成高质量、一致的人像集合提供了可行路径，促进该领域研究。

Abstract: As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.

</details>


### [44] [Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective](https://arxiv.org/abs/2601.20520)
*Qiyan Zhao,Xiaofeng Zhang,Shuochen Chang,Qianyu Chen,Xiaosong Yuan,Xuhang Chen,Luoqi Liu,Jiajun Zhang,Xu-Yao Zhang,Da-Han Wang*

Main category: cs.CV

TL;DR: Paper analyzes why cached dMLLMs repeat text (information flow disruption), shows context token entropy convergence is key, and proposes CoTA to fix repetition by boosting context attention and penalizing uncertain context tokens during decoding.


<details>
  <summary>Details</summary>
Motivation: To address high inference latency in diffusion-based MLLMs and the repetition artifacts introduced by caching techniques (Repeat Curse), by understanding the underlying mechanism through information flow and proposing a mitigation method.

Method: Analyze information flow and token entropy across layers; design CoTA: increase attention to context tokens to preserve entropy convergence and add a decoding penalty term based on context token confidence to avoid uncertain-driven repetitions; implement as plug-and-play with experiments.

Result: This paper identifies and addresses repetition problems (Repeat Curse) in diffusion-based multimodal LLMs using an information flow perspective; proposes CoTA method to enhance context token attention and penalize uncertain context during decoding; shows empirical improvements.

Conclusion: CoTA reduces repetition by preserving information flow and adjusting decoding confidence, yielding consistent gains on multiple tasks.

Abstract: Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA

</details>


### [45] [AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors](https://arxiv.org/abs/2601.20524)
*Matic Fučka,Vitjan Zavrtanik,Danijel Skočaj*

Main category: cs.CV

TL;DR: 提出AnomalyVFM：三阶段合成异常数据生成 + 低秩特征适配器与置信加权像素损失，使VFM在零样本异常检测上超越基于VLM的方法，RADIO骨干在9个数据集上图像级AUROC达94.1%。


<details>
  <summary>Details</summary>
Motivation: 现有基于VFM的方法落后于VLM方法，原因在于辅助异常数据多样性不足和VFM仅做浅层适配。作者希望通过改进数据合成与更深层次的参数高效适配来弥补差距。

Method: AnomalyVFM包括：1）三阶段合成数据生成方案，用以提升异常样本多样性；2）基于低秩特征适配器（LoRA-like）对VFM进行参数高效适配；3）引入置信加权像素损失对像素级预测进行优化。

Result: 在RADIO骨干上，AnomalyVFM在9个数据集上的平均图像级AUROC为94.1%，比之前方法提升约3.3个百分点。

Conclusion: AnomalyVFM提出了一种将预训练视觉基础模型（VFM）转为强零样本异常检测器的通用框架，通过合成数据生成与参数高效适配显著提升性能。

Abstract: Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/

</details>


### [46] [IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework](https://arxiv.org/abs/2601.20526)
*Shaokun Wang,Yifan Yu,Yuhang He,Weili Guan,Yihong Gong*

Main category: cs.CV

TL;DR: 提出IOTA框架，将数据驱动的Black Box模块与知识驱动的White Box模块结合，通过对比错误与正确预测生成可解释的纠正性知识（以提示形式），并利用知识引导的提示选择来改善下游任务微调，在12个图像分类基准的少样本与逐步困难适配设置上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有PET方法把预训练模型视为黑箱，仅依赖数据驱动优化，未充分利用模型的先验和可解释知识，导致下游适配效果受限。作者希望将模型内生知识显式化并与数据驱动方法结合以提升适配效果。

Method: 提出黑白盒提示学习框架IOTA：包含Black Box模块（数据驱动）和White Box模块（知识驱动）。White Box通过对比错误预测与正确认知提取纠正性知识，并将其口语化为人类可解释的提示；设计纠正性知识引导的提示选择策略，将生成的提示用于指导Black Box模块，结合两种信号共同训练以改进预测。

Result: 在12个图像分类基准上（在少样本与从易到难的适配设置中），IOTA展示了纠正性知识的有效性，并且性能优于现有最先进的方法。

Conclusion: 将可解释的纠正性知识与数据驱动微调结合，可显著提高参数高效调优的下游适配效果，表明显式利用模型先验与人工可理解提示是改进PET的有效方向。

Abstract: Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.

</details>


### [47] [Advancing Open-source World Models](https://arxiv.org/abs/2601.20540)
*Robbyant Team,Zelin Gao,Qiuyu Wang,Yanhong Zeng,Jiapeng Zhu,Ka Leong Cheng,Yixuan Li,Hanlin Wang,Yinghao Xu,Shuailei Ma,Yihang Chen,Jie Liu,Yansong Cheng,Yao Yao,Jiayi Zhu,Yihao Meng,Kecheng Zheng,Qingyan Bai,Jingye Chen,Zehong Shen,Yue Yu,Xing Zhu,Yujun Shen,Hao Ouyang*

Main category: cs.CV

TL;DR: LingBot-World是一个开源、支持长时记忆与实时交互的视频驱动世界模拟器，具有高保真、多环境适配和低延迟特性，面向内容创作、游戏与机器人学习。


<details>
  <summary>Details</summary>
Motivation: 开发一个开源、高保真且可交互的世界模拟器，弥合开源与闭源技术差距，推动内容创作、游戏和机器人学习等应用。

Method: 提出LingBot-World视频生成驱动的世界模拟器，强调支持多样环境风格、高保真动态、分钟级长视野记忆与实时交互（16fps下延迟<1s），并开源代码与模型。

Result: 模型在多种环境（真实、科学、卡通等）保持高保真和稳健动力学，实现了分钟级长期一致性和实时交互能力；公开发布代码与模型。

Conclusion: LingBot-World作为顶级世界模型，能为内容创作、游戏和机器人学习提供实用工具，促进社区研究与应用发展。

Abstract: We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as "long-term memory". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.

</details>


### [48] [DeepSeek-OCR 2: Visual Causal Flow](https://arxiv.org/abs/2601.20552)
*Haoran Wei,Yaofeng Sun,Yukun Li*

Main category: cs.CV

TL;DR: 提出DeepSeek-OCR 2与DeepEncoder V2，通过在送入LLM前动态重排序视觉token以模拟人类语义驱动的扫描路径，探索用两级一维因果推理实现二维图像理解的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统VLM将视觉token以固定光栅顺序和位置编码送入LLM，违背人类基于语义与逻辑结构的灵活视觉扫描方式。复杂布局场景下，模拟人类的因果序列处理可能提升理解效果。

Method: 设计DeepEncoder V2，使编码器具备因果推理能力：基于图像语义动态重排序视觉token（打破固定光栅顺序），形成更符合逻辑的序列后交由LLM解释。提出将二维理解拆解为两级串联的一维因果推理结构的架构探索，并公开代码与模型权重。

Result: 论文声称实现了基于重排序的编码器框架（DeepEncoder V2），验证了两级一维因果推理在若干任务上对复杂布局图像理解的可行性，且提供了开源实现。

Conclusion: 提出一种新范式：通过在编码阶段引入语义驱动的序列重排与因果推理，有望弥合传统光栅编码与人类视觉理解的差距，为二维推理提供新的架构路径。

Abstract: We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.

</details>


### [49] [DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression](https://arxiv.org/abs/2601.20564)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 提出DiffVC-RT，一种实时扩散模型视频压缩框架，通过高效网络结构、显式与隐式一致性建模以及异步并行解码管线，实现了在HEVC数据集上显著的LPIPS码率节省与实时编码解码速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式神经视频压缩在信息损失、推理延迟和时序一致性上存在严重问题，阻碍了实用化和实时应用场景。作者旨在构建一个在保真感知指标、速度和一致性之间取得平衡的实时扩散式NVC系统。

Method: 1) 高效信息化模型架构：通过模块替换与剪枝降低计算复杂度并减少结构信息损失；2) 显式与隐式一致性建模：在U-Net中引入零成本的在线时序移位模块(Online Temporal Shift)并辅以混合隐式一致性约束以抑制生成闪烁；3) 异步并行解码管线：采用混合半精度、异步潜码解码和批次维度时序移位设计实现并行帧重建。

Result: 在HEVC数据集上，DiffVC-RT相对于VTM-17.0在LPIPS上实现80.1%的码率节省；在NVIDIA H800 GPU上对720p视频的实时编码/解码速度分别达到206/30 fps。

Conclusion: DiffVC-RT实现了首个实用级别的实时扩散式感知视频压缩，显著提升了压缩效率、时间一致性并满足实时编码解码需求，推动了扩散模型在视频编码领域的实际部署。

Abstract: The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.

</details>


### [50] [StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval](https://arxiv.org/abs/2601.20597)
*Shaokun Wang,Weili Guan,Jizhou Han,Jianlong Wu,Yupeng Hu,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出StructAlign：结合ETF原型的交叉模态对齐损失与跨模态关系保留损失，缓解两类特征漂移，提升持续文本-视频检索性能。


<details>
  <summary>Details</summary>
Motivation: 在CTVR中，模型需要在增量学习新语义类别的同时保持已有类别的文本-视频对齐，但会遭遇跨模态失配和模态内部特征漂移，导致灾难性遗忘。利用结构化几何先验和关系保留，可以稳定表示并减少遗忘。

Method: 1) 采用simplex ETF几何作为类别原型的几何先验；2) 设计交叉模态ETF对齐损失，将文本和视频特征对齐到类别级ETF原型；3) 设计跨模态关系保持损失，利用互补模态的相似度关系作为稳定监督；4) 联合优化以同时抑制非协作跨模态漂移和模态内漂移。

Result: StructAlign introduces structured cross-modal alignment for Continual Text-to-Video Retrieval (CTVR).

Conclusion: StructAlign mitigates catastrophic forgetting by using simplex ETF geometry for cross-modal alignment and a Cross-modal Relation Preserving loss to maintain intra-modal relations, improving retrieval performance over baselines.

Abstract: Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.

</details>


### [51] [Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?](https://arxiv.org/abs/2601.20598)
*Lakshman Balasubramanian*

Main category: cs.CV

TL;DR: 在11个模型和9个数据集的对比中，监督模型擅长训练域但跨域脆弱，语言对齐模型（如SigLIP2）未显式训练ReID却具备较好跨域稳健性。


<details>
  <summary>Details</summary>
Motivation: 评估不同训练范式（监督、自监督、语言对齐）在行人重识别（ReID）任务中的跨域泛化能力，并探讨基础模型（foundation models）是否通过更丰富、可迁移的视觉表征提升泛化性。

Method: 对11个模型（覆盖监督、自监督与语言对齐范式）在9个ReID数据集上进行系统评估，比较在训练域与跨域设置下的性能差异，分析失败案例并探究模型脆弱点。

Result: 实验显示监督模型在训练域指标领先但跨域性能大幅下降；语言对齐模型在多种数据集上保持较稳定的性能，证明其表征更具可迁移性；论文识别出当前模型在遮挡、视角变化和细粒度外观差异上的不足。

Conclusion: 语言对齐（foundation）模型在跨域ReID任务中表现出较强的泛化能力，而传统监督模型在训练域内表现优秀但跨域表现显著下降。

Abstract: Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.

</details>


### [52] [CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification](https://arxiv.org/abs/2601.20601)
*Zhuonan Wang,Wenjie Yan,Wenqiao Zhang,Xiaohui Song,Jian Ma,Ke Yao,Yibo Yu,Beng Chin Ooi*

Main category: cs.CV

TL;DR: 提出了CLEAR-Mamba：结合超网络自适应条件层（HaC）与证据不确定性驱动的可靠性感知预测（RaP），在FFA/ICGA多模态眼底造影数据上提升了泛化性与可靠性。


<details>
  <summary>Details</summary>
Motivation: FFA和ICGA提供了常规眼底照相无法获得的血流动力学与病变结构信息，但单模态、病变细微、设备差异大导致现有模型在泛化与高置信预测上受限，需提高跨设备/模态鲁棒性与对低置信样本的处理能力。

Method: 在MedMamba基础上，提出HaC（超网络生成输入特征分布相关的层参数）以增强跨域适应；提出基于evidential uncertainty的RaP策略，通过鼓励模型关注低置信样本来提升稳定性与可靠性；构建包含FFA与ICGA的多疾病大型数据集进行训练与评测。

Result: 在构建的数据集和多项评测指标上，CLEAR-Mamba优于原始MedMamba与其他基线模型，尤其在多疾病分类准确性与可靠性相关指标（例如低置信样本的校准和不确定性度量）上表现突出。

Conclusion: CLEAR-Mamba通过在网络架构中加入HaC自适应调节层与基于证据不确定性的RaP可靠性感知预测策略，有效提升了FFA/ICGA眼底血管造影图像的跨域泛化能力与预测可靠性。实验表明，在多疾病分类和低置信样本处理上优于原始MedMamba及其他基线方法。

Abstract: Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.

</details>


### [53] [GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2601.20618)
*Shuguang Zhang,Junhong Lian,Guoxin Yu,Baoxun Xu,Xiang Ao*

Main category: cs.CV

TL;DR: 用多模态大模型生成的客观图像描述作为语义锚点，计算与原文本的语义/情感差异和视文保真度，结合门控融合得到更鲁棒的讽刺检测模型（GDCNet）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像与文本松散关联或语义间接时难以检测不一致性；而用LLM生成的讽刺线索多样且主观，噪声多。为此，作者用客观、可证实的图像描述作为语义基准，减少生成噪声并更可靠地捕捉跨模态冲突。

Method: GDCNet首先用MLLM生成事实性图像描述作为稳定锚点，计算该描述与原文本之间的语义差异、情感差异，以及描述与图片之间的视觉-文本一致性（保真度）。然后将这些差异性特征与视觉、文本表征通过门控模态融合模块进行融合，以自适应平衡各模态贡献，最后用于分类。

Result: 在多个多模态讽刺检测基准上进行了大量实验，GDCNet在准确率和鲁棒性上表现出显著提升，在MMSD2.0基准上达成新的最先进水平。

Conclusion: 该文提出引入多模态大模型生成的客观图像描述作为语义锚点，通过比较描述与原文本之间的语义与情感差异并结合视文一致性来建模讽刺不一致性，从而提升多模态讽刺检测准确性与鲁棒性。

Abstract: Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.

</details>


### [54] [OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks](https://arxiv.org/abs/2601.20650)
*Jing Wu,Daphne Barretto,Yiye Chen,Nicholas Gydé,Yanan Jian,Yuhang He,Vibhav Vineet*

Main category: cs.CV

TL;DR: 本文构建了包含242个长期重复任务的基准OS-Marathon，并提出用少量示例压缩构建演示以指导代理学习工作流逻辑，从而在更大数据集上泛化执行。


<details>
  <summary>Details</summary>
Motivation: 长时程、重复性的办公类任务（如处理票据、录入成绩）对人类枯燥且耗时，但适合由可学习结构化子工作流逻辑的计算机代理来完成。目前缺乏针对此类任务的评估基准，阻碍了方法的比较和进步。

Method: 作者收集并构建了242个任务（覆盖两个领域），建立基准OS-Marathon；提出一种成本敏感的演示构建方法，通过少量示例压缩（condensed demonstration）来捕捉并传授工作流的核心逻辑，从而使代理能在更大规模的未见数据上复现相同流程。

Result: 在大量实验中，作者展示了OS-Marathon任务对SOTA代理的挑战性，并证明了所提的示例压缩方法能显著提升代理在长时程重复工作流上的表现。

Conclusion: 该论文提出并发布了一个评估长期重复性工作流的基准（OS-Marathon），并设计了一种低成本的示例压缩方法，通过少量示例教会代理工作流逻辑，从而能在更大、未见过的数据上有效执行类似任务。实验表明任务具有挑战性且所提方法能显著提升代理表现。

Abstract: Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.

</details>


### [55] [FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection](https://arxiv.org/abs/2601.20656)
*Diogo J. Paulo,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: 利用去噪谱残差与区域化的频域证据融合（MRF），提出的轻量S-MAD方法在跨数据集/跨伪造场景下实现了显著泛化性能，部分评测集上优于或接近深度方法。


<details>
  <summary>Details</summary>
Motivation: 当前S-MAD在跨数据集泛化性较差，深度方法往往对训练集外的伪造方式鲁棒性不足。作者观察到不同人脸局部在频域上对真伪的可分性，故提出频域残差与区域融合以提升泛化能力与轻量性。

Method: 方法基于两点：一是引入频域残差概念，通过去除自然谱衰减（spectral decay）来放大真实与伪造样本在频谱上的差异；二是按人脸局部区域提取谱特征，并用马尔可夫随机场（MRF）在局部证据间进行联合推理，实现全局一致判决。仅用频谱特征训练轻量模型，并在SMDD上训练，在FRLL-Morph与MAD22上进行跨数据集/跨伪造方式评测。

Result: 在仅用SMDD训练的前提下，方法在FRLL-Morph上平均EER为1.85%，在MAD22上排名第二平均EER为6.12%；同时在低APCER下取得较好的BPCER，显示出仅频谱特征即可达成强鲁棒性。

Conclusion: 本文提出的区域感知频域残差方法在跨数据集与跨伪造方式的S-MAD任务中表现优异，证明了频域残差特征结合局部-全局融合是一种高效且轻量的替代深度模型的方案。

Abstract: Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.

</details>


### [56] [ProSkill: Segment-Level Skill Assessment in Procedural Videos](https://arxiv.org/abs/2601.20661)
*Michele Mazzamuto,Daniele Di Mauro,Gianpiero Francesca,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: ProSkill：首次面向程序化复杂任务的动作级技能评估数据集，采用瑞士轮+ELO将成对比较转为连续绝对分数，现有方法表现不佳，数据集推动该领域研究。


<details>
  <summary>Details</summary>
Motivation: 现有技能评估研究多聚焦于体育，缺乏面向复杂程序化活动的大规模动作级数据集；常用标注为成对或二分类，不能提供绝对、细粒度的技能量化，限制方法发展。

Method: 设计可扩展的标注协议：使用瑞士轮（Swiss Tournament）高效选择需比较的视频对，通过人工完成成对比较；利用ELO评分系统将这些成对比较结果聚合成一致的连续全局技能分数，同时提供成对与绝对两类标签。用ProSkill构建基准并评测当前主流基于排序与基于成对的方法。

Result: 发布了ProSkill数据集与标注协议，并对现有算法进行基准测试，结果显示当前最先进的方法在该数据集上效果不佳，体现动作级程序化技能评估的难点与研究价值。

Conclusion: ProSkill提出了用于程序化任务动作级技能评估的首个大规模基准数据集，并设计了从成对比较到绝对分数的可扩展标注协议，结合瑞士轮比赛方案与ELO评分聚合。基线实验证明现有方法在该任务上表现欠佳，凸显挑战性和数据集价值。

Abstract: Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/

</details>


### [57] [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/abs/2601.20675)
*Pankhi Kashyap,Mainak Singha,Biplab Banerjee*

Main category: cs.CV

TL;DR: 使用图像字幕与视觉特征融合，通过交叉注意力生成上下文化提示，提升CLIP在遥感领域的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法在遥感影像上表现欠佳，原因包括多标签场景、高类内变异和多分辨率导致难以捕捉主导语义线索并泛化到新类。需要一种能结合文本语义摘要与视觉特征的轻量级方法，在不改动预训练VLM骨干的前提下提升迁移能力。

Method: 1) 用冻结的图像字幕模型（BLIP-2）对遥感图像生成文本摘要；2) 使用BERT分词器对字幕进行编码；3) 从CLIP视觉编码器提取高层视觉特征；4) 将文本嵌入与视觉特征融合；5) 通过轻量级交叉注意力模块，用融合表示条件化一个可学习的查询提示，生成上下文化提示；6) 冻结CLIP骨干，仅学习提示和交叉注意力模块参数；7) 在四个遥感数据集的三个领域泛化任务上评估并与基线比较。

Result: BiMoRS提出了一种轻量级双模态提示学习框架，利用冻结的图像标题生成模型（如BLIP-2）从遥感图像提取文本语义摘要，并将这些字幕用BERT分词器编码，与CLIP编码器的高级视觉特征融合，通过轻量级交叉注意力模块使可学习的查询提示依赖于融合后的文本-视觉表示，从而在不改变CLIP主干的情况下生成上下文化提示。

Conclusion: 在四个遥感数据集的三个领域泛化任务上，BiMoRS consistently提高了性能，平均超越强基线约2%，证明通过跨模态语义摘要可以有效提升提示学习在遥感场景的迁移泛化能力。

Abstract: Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.

</details>


### [58] [Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework](https://arxiv.org/abs/2601.20689)
*Xinyue Li,Zhichao Zhang,Zhiming Xu,Shubo Xu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: LEAF通过用MLLM教师提供密集点/对监督并蒸馏给轻量回归器，再用少量MOS校准，从而在保持MOS相关性下大幅减少人工标注需求，使轻量IQA在低标注预算下可行。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型(MLLMs)在图像质量评估(IQA)上对大量MOS标注依赖和适配成本高的问题，提出低标注需求的框架。

Method: 提出LEAF框架：用MLLM作为教师生成密集监督信号（逐点判断与成对偏好及决策置信度），蒸馏到轻量回归学生网络，学生通过联合蒸馏学习感知模式并在小规模MOS子集上进行校准以对齐人类评价。

Result: 在用户生成与AI生成图像的IQA基准上，LEAF在显著降低人工标注量的同时，仍保持与MOS高度相关的评价性能，证明了方法在有限标注预算下的实用性。

Conclusion: MOS标度校准而非感知能力是MLLMs在IQA中的主要瓶颈。通过从MLLM蒸馏感知先验并用少量MOS进行校准，可以实现标注高效且性能良好的轻量IQA模型。

Abstract: Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.

</details>


### [59] [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/abs/2601.20705)
*Zhuang Yu,Lei Shen,Jing Zhao,Shiliang Sun*

Main category: cs.CV

TL;DR: LEMON是专为长时程STEM讲座视频设计的多模态理解基准，包含2277段视频片段和4181个高质量QA，覆盖6大任务12子任务。评测结果显示当前MLLMs在时序推理和教学预测上仍有显著不足，呼唤更强的跨模态长时程理解能力。


<details>
  <summary>Details</summary>
Motivation: 提出一个专门用于评估多模态大语言模型在长时程、知识密集型、时间结构化教学内容上的理解能力的基准，弥补现有基准在讲座视频场景下的空白。

Method: 构建LEMON数据集：收集2277段STEM课程视频片段，覆盖5个学科29门课程，平均时长196.1秒；人工标注4181个高质量QA对（3413选择题，768开放问答），并划分为6大任务12子任务，强调语义丰富性、多模态紧耦合、显式时间与教学结构及上下文多轮提问。对多种先进MLLMs（包括GPT-4o）在这些任务上的表现进行了全面评测。

Result: 实验显示在多项任务上存在显著性能差距，尤其是时间推理与教学预测任务中，即便是最先进的模型也表现不足，表明现有模型在处理长时程教学视频的时序理解和推理方面存在挑战。

Conclusion: LEMON作为一个具有挑战性且可扩展的基准，有望推动多模态感知、推理与生成在长时程教学内容上的研究与发展。

Abstract: Recent multimodal large language models (MLLMs) have shown remarkable progress across vision, audio, and language tasks, yet their performance on long-form, knowledge-intensive, and temporally structured educational content remains largely unexplored. To bridge this gap, we introduce LEMON, a Lecture-based Evaluation benchmark for MultimOdal uNderstanding, focusing on STEM lecture videos that require long-horizon reasoning and cross-modal integration. LEMON comprises 2,277 video segments spanning 5 disciplines and 29 courses, with an average duration of 196.1 seconds, yielding 4,181 high-quality QA pairs, including 3,413 multiple-choice and 768 open-ended questions. Distinct from existing video benchmarks, LEMON features: (1) semantic richness and disciplinary density, (2) tightly coupled video-audio-text modalities, (3) explicit temporal and pedagogical structure, and (4) contextually linked multi-turn questioning. It further encompasses six major tasks and twelve subtasks, covering the full cognitive spectrum from perception to reasoning and then to generation. Comprehensive experiments reveal substantial performance gaps across tasks, highlighting that even state-of-the-art MLLMs like GPT-4o struggle with temporal reasoning and instructional prediction. We expect LEMON to serve as an extensible and challenging benchmark for advancing multimodal perception, reasoning, and generation in long-form instructional contents.

</details>


### [60] [Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction](https://arxiv.org/abs/2601.20720)
*Matej Halinkovic,Nina Masarykova,Alexey Vinel,Marek Galinski*

Main category: cs.CV

TL;DR: Li-ViP3D++ uses Query-Gated Deformable Fusion to fuse multi-view images and LiDAR in query space, boosting detection, tracking, and forecasting on nuScenes with better metrics and speed.


<details>
  <summary>Details</summary>
Motivation: Modular pipelines limit information flow and amplify errors; fully differentiable, query-based multimodal fusion can better leverage complementary camera and LiDAR cues for end-to-end perception and trajectory prediction.

Method: Query-Gated Deformable Fusion (QGDF) in Li-ViP3D++: combines masked attention over images, differentiable BEV LiDAR sampling with learned per-query offsets, and query-conditioned gating to weight modalities.

Result: On nuScenes, Li-ViP3D++ improves EPA to 0.335, mAP to 0.502, reduces FP ratio to 0.147, and runs faster (139.82 ms) than prior variant (145.91 ms).

Conclusion: Query-space, fully differentiable fusion of camera and LiDAR via QGDF enhances end-to-end perception and prediction, improving accuracy, reducing false positives, and increasing efficiency.

Abstract: End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.

</details>


### [61] [Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification](https://arxiv.org/abs/2601.20742)
*Xin Jin,Jinming Liu,Yuntao Wei,Junyan Lin,Zhicheng Wang,Jianguo Huang,Xudong Yang,Yanxiao Liu,Wenjun Zeng*

Main category: cs.CV

TL;DR: 将传统视觉编码与生成式视觉token在优化目标上统一，提出任务导向token有望成为高效通用的下一代视觉表示与标准。


<details>
  <summary>Details</summary>
Motivation: 观察到压缩效率与智能能力相关，即更高的语义压缩率通常带来更好模型表现；同时视觉token在生成式多模态模型中的兴起与传统视觉编码在工业界的成熟标准化存在方法与目标上的相似性，促使将二者统一分析以推进下一代高效通用表示标准。

Method: 论文首先回顾视觉编码和视觉token两大技术族群的历史与代表方法，随后构建统一的优化表述，将压缩效率与模型性能的权衡形式化，最后通过任务导向token实验评估在多模态LLM、AIGC与具身AI等实际应用中的潜力与效果。

Result: 通过理论统一与实验验证，论文展示任务导向的视觉token在下游多模态任务中能显著提升效率并保持或改善性能，支持将视觉token走向标准化以服务广泛智能应用的可行性。

Conclusion: 本文提出将传统视觉编码与视觉token技术统一为同一优化框架，指出二者在追求语义保真和计算成本最小化上的共性，并预测面向任务的token标准化可能成为下一代视觉编解码方向。

Abstract: "Compression Tells Intelligence", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.

</details>


### [62] [FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models](https://arxiv.org/abs/2601.20791)
*Haonan Zhong,Wei Song,Tingxu Han,Maurice Pagnucco,Jingling Xue,Yang Song*

Main category: cs.CV

TL;DR: FairT2V：训练免费且针对文本编码器诱发的性别偏见，使用锚点基球面测地变换中和prompt嵌入，并在早期去噪步骤动态应用，以在不影响质量的前提下降低职业类视频的性别偏差。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频扩散模型在社会属性（尤其性别）上的偏见尚未充分研究，且主要来源于预训练文本编码器在中性提示中也携带隐含性别关联。需要一种无需微调模型即可减轻偏见的方法，以免高成本训练并保持模型能力。

Method: 1) 通过性别倾向得分量化文本编码器对中性提示的性别倾向；2) 使用锚点基的球面测地变换将prompt嵌入中和至性别中立方向，同时保持语义；3) 仅在生成早期（身份形成阶段）应用变换，通过动态去噪时间表保持时间一致性；4) 提出结合VideoLLM推理与人工验证的视频级公平性评估协议。

Result: FairT2V提出了一个无需训练的消偏框架，通过在生成早期步骤对prompt嵌入进行锚点基球面测地变换来中和编码器引入的性别偏见，同时保持语义一致并使用动态去噪计划以维持时间连贯性。实验在Open-Sora模型上表明该方法在职业分类上显著降低了人口统计学偏差，对视频质量影响小。

Conclusion: 编码器（预训练文本编码器）是T2V模型性别偏差的主要来源；通过对prompt嵌入进行无训练的几何中和并限制在早期去噪步骤应用，可以有效减少生成视频中的性别偏向，同时保持视频质量和时间连贯性。

Abstract: Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.
  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.

</details>


### [63] [Open-Vocabulary Functional 3D Human-Scene Interaction Generation](https://arxiv.org/abs/2601.20835)
*Jie Liu,Yu Sun,Alpar Cseke,Yao Feng,Nicolas Heron,Michael J. Black,Yan Zhang*

Main category: cs.CV

TL;DR: 提出FunHSI：一个训练免费、以功能为导向的框架，通过功能感知接触推理、场景重建、接触图建模、视觉-语言驱动的初步姿态估计与阶段式优化，实现开放词汇任务提示下的功能正确且物理合理的三维人-场景交互生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常缺乏对场景对象功能性及相应人-场景接触的显式推理，导致生成的交互在功能性或物理合理性上失败。为解决这个问题，作者提出从功能视角出发，显式识别功能元素并以接触推理约束人体姿态，提升交互的功能正确性与物理合理性。

Method: FunHSI首先基于任务提示进行功能感知的接触推理以定位功能性场景元素并重建其三维几何，构建描述高层交互的接触图；随后利用视觉-语言模型从图像生成执行任务的人并估计初步的三维身体与手部姿态；最后通过分阶段优化精化所提议的三维姿态以保证物理可行性与功能性。该方法为训练免费，并支持开放词汇的任务提示。

Result: 在室内与室外多样场景上的大量实验证明，FunHSI在生成功能正确且物理合理的人-场景交互方面持续优于现有方法，既能处理一般交互（如"坐在沙发上"），也支持细粒度功能性交互（如"增大室内温度"）。

Conclusion: FunHSI提出了一个无须训练、以功能为驱动的三维人-场景交互生成框架，通过功能感知的接触推理、场景元素三维重建、接触图建模、视觉-语言模型生成图像中属于任务的人的三维姿态估计，并通过阶段式优化实现物理可行且功能正确的人体配置，从而比现有方法更好地生成可用且物理合理的交互。

Abstract: Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as "sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., "increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.

</details>


### [64] [A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion](https://arxiv.org/abs/2601.20847)
*Willams de Lima Costa,Thifany Ketuli Silva de Souza,Jonas Ferreira Silva,Carlos Gabriel Bezerra Pereira,Bruno Reis Vila Nova,Leonardo Silvino Brito,Rafael Raider Leoni,Juliano Silva,Valter Ferreira,Sibele Miguel Soares Neto,Samantha Uehara,Daniel Giacomo,João Marcelo Teixeira,Veronica Teichrieb,Cristiano Coelho de Araújo*

Main category: cs.CV

TL;DR: 提出图像+IMU的轻量级双向交叉注意力+自适应门控的路面分类方法，并发布多子集ROAD数据集，在多场景和恶劣条件下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RSC方法泛化性差，受限于单一感知模态与缺乏环境多样性的数据集；需要低成本多模态方案与更丰富基准以提升稳健性。

Method: 设计轻量级双向交叉注意力模块融合RGB与IMU特征，后接自适应门控层根据域偏移调整各模态权重；构建ROAD数据集三子集以覆盖实景、多摄像头与合成OOD场景。

Result: 在PVS基准上提升+1.4个百分点，在ROAD多模态子集上提升+11.6个百分点；在少数类F1分数和夜间、暴雨、混合路面等挑战性条件下表现稳健，证明廉价相机+IMU结合多模态注意力的可行性。

Conclusion: 本文提出了一种融合图像与IMU的多模态道路路面分类框架，采用轻量级双向交叉注意力模块和自适应门控层以应对域偏移，并引入了ROAD数据集（包含真实多模态、视觉大规模和合成子集）。实验表明方法在PVS基准上和ROAD数据集上均优于先前方法，尤其在少数类和恶劣视觉条件下表现更稳健。

Abstract: Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.

</details>


### [65] [FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models](https://arxiv.org/abs/2601.20857)
*Hongyu Zhou,Zisen Shao,Sheng Miao,Pan Wang,Dongfeng Bai,Bingbing Liu,Yiyi Liao*

Main category: cs.CV

TL;DR: FreeFix通过交替的2D-3D无微调精化，并用像素置信掩码定位不确定区域，实现在外推视角上以高一致性和高保真度改善渲染，兼顾泛化与质量。


<details>
  <summary>Details</summary>
Motivation: 现有用扩散模型增强新视角合成的方法在泛化与保真度之间存在权衡：对扩散模型微调能提高保真度但易过拟合，免微调方法泛化好但保真度不足。目标是在不微调扩散模型的前提下提升外推视角的渲染质量和多视图一致性。

Method: 提出交错的2D-3D精化策略：在渲染的2D视图上使用预训练图像扩散模型进行修复，然后将修复信息反馈至3D表示（如NeRF或3D Gaussian Splatting），交替迭代以提升多帧一致性；引入基于像素的不确定性置信掩码以定位需重点修复的区域，从而避免对高置信区域的过度修改。整个流程不需要对扩散模型进行微调。

Result: 在多个数据集上的实验表明，FreeFix能显著提升外推视角的渲染一致性，定量和定性指标可与微调基线持平或更优，同时保持了强泛化能力。

Conclusion: FreeFix在不微调预训练扩散模型的前提下，利用图像扩散模型对外推视角进行一致性修复，达到了与微调方法相当甚至更好的效果，同时保持了强泛化能力。

Abstract: Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [66] [DBTuneSuite: An Extendible Experimental Suite to Test the Time Performance of Multi-layer Tuning Options on Database Management Systems](https://arxiv.org/abs/2601.20015)
*Amani Agrawal,Tianxin Wang,Dennis Shasha*

Main category: cs.DB

TL;DR: DBTuneSuite is a reproducible, extensible benchmark suite for four free DBMSs that evaluates performance under diverse workloads and tunings, finding that system choice and tuning effects are workload-dependent and non-uniform across systems.


<details>
  <summary>Details</summary>
Motivation: Provide practical, reproducible benchmarks and tools to help engineers, advanced users, and students evaluate which DBMS and tuning choices suit different query/update workloads.

Method: Designed an experiment suite with scripts to generate data, run configurable query/upsert workloads across four free DBMSs, and vary tuning options; collected quantitative performance metrics to compare systems and tunings.

Result: Provided comparative performance results, system-specific recommendations for query types, and evidence that tuning options widely used in practice can behave very differently across systems; also released expandable scripts.

Conclusion: DBTuneSuite shows that performance and tuning effects vary significantly across four popular free DBMSs; no single system is best for all workloads, and common tuning knobs can have different impacts.

Abstract: DBTuneSuite is a suite of experiments on four widely deployed free database systems to test their performance under various query/upsert loads and under various tuning options. The suite provides: (i) scripts to generate data and to install and run tests, making it expandable to other tests and systems; (ii) suggestions of which systems work best for which query types; and (iii) quantitative evidence that tuning options widely used in practice can behave very differently across systems. This paper is most useful for database system engineers, advanced database users and troubleshooters, and students.

</details>


### [67] [Delta Fair Sharing: Performance Isolation for Multi-Tenant Storage Systems](https://arxiv.org/abs/2601.20030)
*Tyler Griggs,Soujanya Ponnapalli,Dev Bali,Wenjie Ma,James DeLoye,Audrey Cheng,Jaewan Hong,Natacha Crooks,Scott Shenker,Ion Stoica,Matei Zaharia*

Main category: cs.DB

TL;DR: 提出Delta Fair Sharing，给出δ界定的公平与效率性质并在RocksDB扩展FAIRDB中实现，能在高抢占延迟资源下提供有界尾延迟和高利用率。


<details>
  <summary>Details</summary>
Motivation: 传统的公平共享在存在高可抢占延迟（如写缓冲、读缓存）时无法保证性能隔离，导致尾延迟峰值不可接受，需新算法界定资源获取延迟并保证利用率。

Method: 定义δ-公平性和δ-帕累托效率两个性质，设计满足这两者的资源分配算法，并将其集成到RocksDB扩展FAIRDB中进行实现与评测。

Result: 理论上证明算法满足δ-公平性与δ-帕累托效率；在FAIRDB实现中实验表明能更好地隔离守规客户端，减少尾延迟峰值且保持高资源利用率，优于现有替代方案。

Conclusion: 本文提出了Delta Fair Sharing算法族，通过对高可抢占延迟资源的延迟界定来实现存储系统中的性能隔离；实现FAIRDB并在RocksDB上验证，优于现有方法。

Abstract: Modern storage systems, often deployed to support multiple tenants in the cloud, must provide performance isolation. Unfortunately, traditional approaches such as fair sharing do not provide performance isolation for storage systems, because their resources (e.g., write buffers and read caches) exhibit high preemption delays. These delays lead to unacceptable spikes in client tail latencies, as clients may be forced to wait arbitrarily long to receive their fair share of resources.
  We introduce Delta Fair Sharing, a family of algorithms for sharing resources with high preemption delays. These algorithms satisfy two key properties: $δ$-fairness, which bounds a client's delay in receiving its fair share of resources to $δ$ time units, and $δ$-Pareto-efficiency, which allocates unused resources to clients with unmet demand. Together, these properties capture resource-acquisition delays end-to-end, bound well-behaved clients' tail-latency spikes to $δ$ time units, and ensure high utilization. We implement such algorithms in FAIRDB, an extension of RocksDB. Our evaluation shows that FAIRDB isolates well-behaved clients from high-demand workloads better than state-of-the-art alternatives.

</details>


### [68] [ConStruM: A Structure-Guided LLM Framework for Context-Aware Schema Matching](https://arxiv.org/abs/2601.20482)
*Houming Chen,Zhe Zhang,H. V. Jagadish*

Main category: cs.DB

TL;DR: ConStruM通过构建context tree与相似性超图，在预算内选取具区分力的上下文包，作为对上游匹配器的提示增强，显著改进了列匹配准确率。


<details>
  <summary>Details</summary>
Motivation: 列名和描述虽重要，但常需额外上下文证据才能正确匹配；直接提供全部元数据不可行，需在预算约束下选择并组织最有区分力的上下文信息。

Method: 构建轻量可复用的结构（context tree 和全局相似性超图），在查询时从中组装小规模上下文包，结合分组感知差异提示（在线或离线计算），并作为附加模块增强上游匹配器的最终LLM提示。

Result: 在真实数据集上的实验证明，ConStruM能在有限预算下提供和组织正确的上下文证据，从而提升匹配性能。

Conclusion: ConStruM通过结构化、预算感知的证据压缩与组织方法，有效提升了LLM在列匹配任务中的最终选择准确性。

Abstract: Column matching is a central task in reconciling schemas for data integration. Column names and descriptions are valuable for this task. LLMs can leverage such natural-language schema metadata. However, in many datasets, correct matching requires additional evidence beyond the column itself. Because it is impractical to provide an LLM with the entire schema metadata needed to capture this evidence, the core challenge becomes to select and organize the most useful contextual information.
  We present ConStruM, a structure-guided framework for budgeted evidence packing in schema matching. ConStruM constructs a lightweight, reusable structure in which, at query time, it assembles a small context pack emphasizing the most discriminative evidence. ConStruM is designed as an add-on: given a shortlist of candidate targets produced by an upstream matcher, it augments the matcher's final LLM prompt with structured, query-specific evidence so that the final selection is better grounded. For this purpose, we develop a context tree for budgeted multi-level context retrieval and a global similarity hypergraph that surfaces groups of highly similar columns (on both the source and target sides), summarized via group-aware differentiation cues computed online or precomputed offline. Experiments on real datasets show that ConStruM improves matching by providing and organizing the right contextual evidence.

</details>


### [69] [ALER: An Active Learning Hybrid System for Efficient Entity Resolution](https://arxiv.org/abs/2601.20664)
*Dimitrios Karapiperis,Leonidas Akritidis,Panayiotis Bozanis,Vassilios Verykios*

Main category: cs.DB

TL;DR: ALER 通过冻结 bi-encoder 生成静态嵌入、分块候选池和混合查询策略，在不显著牺牲精度的情况下，大幅提高 ER 主动学习的可扩展性与运行效率。


<details>
  <summary>Details</summary>
Motivation: 解决监督深度学习 ER 模型对大量标注数据的依赖与现有主动学习方法在每轮重新训练或求解复杂选择问题时的计算瓶颈，使 AL 在实际大规模数据库上可用。

Method: 采用冻结双编码器（bi-encoder）一次性生成静态嵌入，并在其上迭代训练轻量分类器；通过抽样+K-Means对候选池分块以降低内存与搜索成本；使用混合查询策略（困惑样本+高置信错误）来高效改进决策边界并修正高置信误判。

Result: 在多组评测中，ALER 在效率和延迟上优于现有基线，尤其在大型 DBLP 数据集上：训练循环加速约 1.3 倍，解析延迟降低约 3.8 倍。

Conclusion: ALER 在保持语义精度的同时显著提升了主动学习在实体解析任务上的计算可扩展性，适合大规模场景。

Abstract: Entity Resolution (ER) is a critical task for data integration, yet state-of-the-art supervised deep learning models remain impractical for many real-world applications due to their need for massive, expensive-to-obtain labeled datasets. While Active Learning (AL) offers a potential solution to this "label scarcity" problem, existing approaches introduce severe scalability bottlenecks. Specifically, they achieve high accuracy but incur prohibitive computational costs by re-training complex models from scratch or solving NP-hard selection problems in every iteration. In this paper, we propose ALER, a novel, semi-supervised pipeline designed to bridge the gap between semantic accuracy and computational scalability. ALER eliminates the training bottleneck by using a frozen bi-encoder architecture to generate static embeddings once and then iteratively training a lightweight classifier on top. To address the memory bottleneck associated with large-scale candidate pools, we first select a representative sample of the data and then use K-Means to partition this sample into semantically coherent chunks, enabling an efficient AL loop. We further propose a hybrid query strategy that combines "confused" and "confident" pairs to efficiently refine the decision boundary while correcting high-confidence errors.Extensive evaluation demonstrates ALER's superior efficiency, particularly on the large-scale DBLP dataset: it accelerates the training loop by 1.3x while drastically reducing resolution latency by a factor of 3.8 compared to the fastest baseline.

</details>


### [70] [The Monotone Priority System: Foundations of Contract-Specific Sequencing](https://arxiv.org/abs/2601.20783)
*Naveen Durvasula*

Main category: cs.DB

TL;DR: 为智能合约函数调用引入整数优先级约束，区块构建者按优先级从高到低排序，满足五条公理且唯一


<details>
  <summary>Details</summary>
Motivation: 现有区块链应用需要在交易之间指定序列约束，但过于复杂的约束会影响区块生产的可行性；因此需要一种既有表达力又便于排序和出块的形式化系统

Method: 为每个调用允许设定全局整数优先级，要求调用的优先级不高于其引用调用的优先级；区块构建者按优先级从高到低排序并在同优先级内自由决定顺序；通过公理化证明该机制的唯一性

Result: 提出一种基于优先级的智能合约调用排序机制

Conclusion: 系统通过允许开发者为调用设置全局整数优先级并要求对引用的调用优先级不低于当前调用，实现了表达力与出块可行性间的平衡；该系统满足五个公理并且在满足这些公理的机制中是唯一的

Abstract: Modern blockchain applications benefit from the ability to specify sequencing constraints on the transactions that interact with them. This paper proposes a principled and axiomatically justified way of adding sequencing constraints on smart contract function calls that balances expressivity with the tractability of block production. Specifically, we propose a system in which contract developers are allowed to set an integer global priority for each of their calls, so long as that the call's chosen priority is no higher than the priority of any of its referenced calls. Block builders must then simply sequence transactions in priority order (from high to low priority), breaking ties however they would like. We show that this system is the unique system that satisfies five independent axioms.

</details>
