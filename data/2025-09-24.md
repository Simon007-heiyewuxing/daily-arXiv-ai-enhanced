<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出将U-Net与Grad-CAM结合的可解释息肉分割框架，在Kvasir-SEG上取得高IoU（0.9257）和高Dice（>0.96），并通过Grad-CAM提升模型可解释性，有助于临床应用。


<details>
  <summary>Details</summary>
Motivation: 早期准确分割胃肠道息肉可减少结直肠癌进展，但手动标注耗时且主观性强；深度学习虽能自动化分割，但可解释性不足阻碍临床采纳，因此提出可解释的分割框架。

Method: 将U-Net用于像素级分割，并在输出层或中间特征图上结合Grad-CAM生成关注图，训练和评估在Kvasir-SEG（1000张标注图像）上，通过常规分割损失优化模型。

Result: 在测试集上平均IoU为0.9257，训练/验证集Dice系数均>0.96；Grad-CAM可视化显示模型关注临床相关区域，增强可信度。

Conclusion: PolypSeg-GradCAM在Kvasir-SEG数据集上表现出高精度分割并提供可视化解释，使其在临床辅助息肉检测中具有潜力。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: PerceptronCARE使用轻量卷积神经网络实现85.4%准确率的糖网病变自动检测，并通过云端架构支持远程筛查和数据管理，适合资源有限地区推广。


<details>
  <summary>Details</summary>
Motivation: 解决偏远和资源受限地区糖尿病视网膜病变筛查资源不足，提高早期诊断覆盖率并降低医疗成本。

Method: 采用比较不同卷积神经网络（ResNet-18、EfficientNet-B0、SqueezeNet），在权衡准确率与计算效率后选定最终模型，并构建云端多用户、数据安全的远程诊疗系统。

Result: 最终模型在测试集上分类准确率为85.4%，实现了实时筛查能力；系统支持云扩展、安全数据管理和多用户协作。

Conclusion: 本文提出的PerceptronCARE展示了AI在糖尿病视网膜病变筛查中的可行性，但结论需谨慎对待。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 作者提出一种基于逆映射的通用正则化 SIM（及高效变体 ρSIM），通过从输出重建输入来减少信息损失，提升表示学习。该方法计算高效、可插拔，并在视觉和非视觉多种任务上稳定提高性能。


<details>
  <summary>Details</summary>
Motivation: 常规模型正则化依赖经验启发式方法，在不同设置下效果不稳定。为获得更可靠且普适的正则化手段，作者提出利用数据内在信息和逆映射来减少前向传播中的信息损失并促进梯度流动，从而改善泛化。

Method: 提出 Self Identity Mapping (SIM)，使用逆映射机制从模型输出重建输入以约束中间特征；为降低计算开销，设计了 ρSIM，结合了patch级别特征采样和基于投影的潜在特征重建方法，使其高效且可插拔地应用于不同架构与任务。

Result: 在多项任务上（图像分类、few-shot prompt learning、域泛化、语义分割、图像翻译、音频分类、时序异常检测）ρSIM 均超越基线，且能与其他正则化方法叠加带来额外提升；同时在稠密预测任务中更好地保持语义信息。

Conclusion: SIM 引入自反向映射的正则化思想，通过从变换后输出重建输入来减少信息损失并促进更平滑的梯度流，从而提升表示学习能力。实验证明 ρSIM 在图像分类、少样本提示学习、域泛化，以及语义分割、图像翻译、音频分类和时间序列异常检测等任务上均能稳定提升性能，并且能与现有正则化方法协同增益。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA通过闭式重缩放和动量混合整批/子集损失，在单轮平均梯度下实现了无标签、高效且高保真的多图像重建，性能优于先前方法且成本相当。


<details>
  <summary>Details</summary>
Motivation: 在单轮平均梯度（SAG）设置下，单个批次的均值梯度将多个样本的线索混合，使得现有梯度反演方法难以分离并重建出单张图像的细节。希望提出一种无需标签且在大批量下仍能有效恢复多张图像的方法。

Method: 提出了基于动量的自适应校正框架：1) 引入一个闭式组合重缩放（combinatorial rescaling）以得到更紧的优化界；2) 将整批与子集损失通过动量混合来增强重建稳健性。通过随机子集探查来感知潜在的每张图像信号，且计算开销与标准求解器相当。

Result: 实验表明，MAGIA在大批量场景中显著优于现有先进方法，能实现高保真多图像重建，并且不依赖任何辅助信息，计算资源需求与常规求解器相近。

Conclusion: 本文提出的MAGIA在单轮平均梯度（SAG）情形下，通过动量自适应校正实现了对批量内多张图像的高保真重建，成功解决了以往方法在大批量和无标签信息情况下重建失败的问题。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: 提出针对阿拉伯文档OCR微调的MLLM（Baseer）与高质量评测集（Misraj-DocOCR），decoder-only微调结合大规模合成与真实数据，获得WER 0.25的新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在高资源语言上的文档理解进展显著，但对阿拉伯语这种书写连写、字体多样、有元音符号且从右向左书写的语言表现有限，需专门适配以提升OCR准确率。

Method: 采用decoder-only微调策略，在不破坏预训练通用视觉特征的前提下，使用混合合成与真实的文档数据进行训练；构建并使用expert-verified的Misraj-DocOCR基准进行评估。

Result: Baseer在Misraj-DocOCR上取得WER 0.25，显著优于现有开源与商业方案，确立了阿拉伯文档OCR的新基线。

Conclusion: Baseer通过对预训练多模态大模型（MLLM）进行解码器层的微调，结合大规模合成与真实阿拉伯文档数据集，实现了针对阿拉伯文文档OCR的显著性能提升，达到了新的SOTA水平。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 将稀疏InSAR点数据构造成密集时空张量，结合CNN-LSTM可实现更准确且空间一致的地表形变预测，优于LightGBM与LASSO，并通过可解释性分析证明必要性。


<details>
  <summary>Details</summary>
Motivation: 稀疏的InSAR点时间序列难以直接应用计算机视觉模型，限制了高分辨率、空间一致的形变预测能力；通过把稀疏测量变为密集时空表示，可以利用先进时空深度模型提升预测性能。

Method: 将稀疏InSAR时间序列点测量插值或映射为规则网格的时空张量；设计混合CNN提取空间特征与LSTM建模时间依赖的端到端深度学习框架（CNN-LSTM）；在爱尔兰东部的Sentinel-1数据集上训练与评估，并与LightGBM和LASSO比较，加入可解释性分析以揭示基线模型的持久性偏好。

Result: 在Sentinel-1实测数据上，所提模型在误差和空间连贯性上显著优于LightGBM与LASSO；可解释性分析显示基线模型倾向于简单的持久性预报，而CNN-LSTM能捕捉更复杂的时空动态。

Conclusion: 本文提出的将稀疏InSAR点数据转换为密集时空张量并用CNN-LSTM建模的方法，可显著提升地表形变预报的精度与空间连贯性，优于LightGBM与LASSO基线。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: 提出Scrapbook框架自动生成面向基础概念的大规模多样化问题集，用于系统评估视觉语言模型；结果显示模型在位置和复杂约束理解上仍需改进。


<details>
  <summary>Details</summary>
Motivation: 在处理复杂任务前，先验证模型对基本概念（如位置、属性和对象识别）的掌握程度，以便定位弱点并推动改进。

Method: 通过自动生成大量围绕单一概念（物体识别、绝对与相对位置、属性识别）的问答样本，包含广泛的语言变体，构建数据集并在多个视觉语言模型上评估。

Result: 模型在对象识别和计数上表现较好，但在位置理解、几何形状相关问题和含附加约束的问题上表现差，MobileVLM-V2存在答案不一致和可行误答，其他模型有肯定答复偏向。

Conclusion: Scrapbook框架能生成多样化的问题数据集，适合检测模型对基础概念的理解，但实验显示模型在位置理解和复杂约束下表现不足。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 作者通过150对图像和LPIPS/SSIM/颜色距离评估，实证表明将视觉信息经文本中介再生成会造成普遍且显著的感知和结构损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在创意工作流程中的普及，量化视觉信息通过文本中介时的降解对于评估系统局限性变得重要，但该问题尚未被充分量化。

Method: 作者构建了describe-then-generate流程，生成150对图像（原始图像与通过文本描述再生成的图像），并使用LPIPS、SSIM和颜色距离等现有指标评估感知、结构和色彩维度的信息保留。

Result: 实验结果显示99.3%的样本在感知层面出现显著降解，91.5%的样本表现出明显的结构信息丢失，表明describe-then-generate瓶颈是现代多模态系统的一致性限制。

Conclusion: 本文实证证明在视觉-语言-视觉管道中，通过自然语言作为中介会导致显著的信息损失。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: 提出一种面向SIDS的AI工作流，从高分辨率卫星影像自动推断屋顶属性，最佳模型在坡度和材料分类上分别达F1=0.88和0.83，有助于填补建筑数据缺口，支持减灾与城市治理。


<details>
  <summary>Details</summary>
Motivation: 许多小岛发展中国家缺乏详细建筑结构信息，限制了基于建筑细节的灾害损害评估与减灾规划。通过自动化从卫星影像提取屋顶属性，可弥补这一数据缺口。

Method: 对比使用地理空间基础模型（geospatial foundation models）加浅层分类器与微调深度学习模型对屋顶分类的效用，并评估从邻近小岛引入额外训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度（roof pitch）分类上取得F1=0.88，在屋顶材料（roof material）分类上取得F1=0.83；表明方法具有较高准确性，且利用邻近SIDS的数据可提升性能。

Conclusion: 本研究展示了在数据匮乏的加勒比小岛国家，通过高分辨率卫星影像结合AI方法自动推断屋顶属性的可行性，为灾害风险评估与城市韧性规划提供实用数据。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 提出轻量VLA-LPAF，通过潜空间多视图融合在仅用2D数据下显著提升VLA模型的视角适应性，RoboFlamingo-LPAF在多个基准上实现了明显的成功率提升。


<details>
  <summary>Details</summary>
Motivation: 第三人视全局和手腕内局部相机在不同环境中视角和数量不一致，导致视觉特征差异大，限制VLA模型的泛化能力，需提高视角自适应性。

Method: 设计轻量级模块VLA-LPAF，只用单视图图像微调并在潜在表征层融合其他多视图观测，集成到RoboFlamingo构建RoboFlamingo-LPAF。

Result: 在CALVIN上平均提升约8%任务成功率，LIBERO提升约15%，自定义仿真基准提升约30%；并在真实任务中展示了视角自适应特性。

Conclusion: 提出VLA-LPAF能在仅用2D数据下提升VLA模型对视角差异的适应性，通过在潜空间融合多视图信息，缓解视角不一致问题。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 提出URNet：采用局部-全局细化与KL散度不确定性建模的事件相机双目深度估计方法，在DSEC数据集上超越SOTA，提升精度与可靠性。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围、低延迟，传统方法难以充分利用其优势或处理不确定性；因此需要专门的网络结构来提升事件驱动的双目深度估计精度与可靠性。

Method: 提出不确定性感知的细化网络URNet；核心包括局部-全局细化模块用于捕捉细粒度局部信息与长程全局上下文，以及基于KL散度的不确定性建模用于增强预测可靠性。可能结合初始深度估计器与逐级细化结构。

Result: 在DSEC数据集上，URNet在定性和定量指标上均优于SOTA方法，表明细化模块与不确定性建模有效提升了性能与预测可信度。

Conclusion: 本文提出的URNet在事件相机双目深度估计上通过局部-全局细化模块和KL散度不确定性建模提高了精度与可靠性，实验在DSEC上优于现有SOTA方法。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves 通过自动分割+符号化空间推理替代手动 ROI 的传统束流追踪，实现对外周神经（腰骶神经丛）的更准确、可重复识别，具有临床潜力用于非侵入性神经病变评估。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常伴慢性盆腔痛及神经受累，但外周神经成像和识别困难，传统束流追踪依赖经验性 ROI 且易受伪影影响，故提出结合先验解剖知识的自动化方法。

Method: 两阶段：A) 使用深度学习自动分割解剖结构（多模态 MRI 输入）；B) 基于符号化的模糊空间关系进行束流追踪后的神经识别与筛选，避免手动 ROI 选择。

Result: 在 10 名确诊或疑似子宫内膜异位症女性的腰骶神经丛实验中，相较于标准束流追踪，Dice 分数最高提升约 25%，空间误差降至 <5 mm，显示显著改进。

Conclusion: Visionerves 是一套结合深度学习分割与符号空间推理的混合 AI 框架，用于从多梯度扩散加权成像（DWI）和形态学 MRI 中识别外周神经，尤其针对腰骶神经丛在子宫内膜异位症患者中的应用，能自动、可重复地提取神经走行并提升重建精度。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive在巴基斯坦采集隐私保护的多模态驾驶数据（加速度计、陀螺仪、GPS与路面视频），时间同步、分层存储，填补了发达国家数据集无法代表新兴经济体驾驶行为的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多来自发达国家，缺乏对新兴经济体（如巴基斯坦）驾驶行为多样性的代表性，且面部视频存在隐私问题，因此需要一个本地化且隐私保护的多模态数据集。

Method: 使用自定义Android应用同步记录高频加速度计、陀螺仪、GPS与车外路面视频，采集多种道路类型（市区干道、次要道路、高速公路）上的正常、激进和危险三类驾驶行为，数据按原始、处理和语义层组织。

Result: 成功构建并发布了V-SenseDrive数据集，包含时间对齐的传感器与路面视频数据，覆盖多路况与驾驶行为，数据格式便于未来行为分类、交通安全分析与ADAS开发。

Conclusion: V-SenseDrive是首个在巴基斯坦收集的隐私保护多模态驾驶行为数据集，为研究新兴经济体的驾驶行为差异提供了关键资源，并为ADAS和交通安全分析奠定了基础。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: Qianfan-VL通过多阶段训练和高精度数据合成实现领域增强，在OCR、文档理解和复杂推理等任务上达到或超越SOTA，并证明了在大规模国产算力上高效训练的可行性。


<details>
  <summary>Details</summary>
Motivation: 提高多模态大模型在特定行业/领域（如OCR、文档理解、数学推理）上的能力，同时保持或不牺牲通用性能，以便应用于企业级场景，并验证大规模国产算力训练能力。

Method: 采用多阶段渐进训练策略与高精度数据合成管线，结合领域增强策略，使模型在保持通用能力的同时显著提升领域能力；部分模型引入长链路思维（long chain-of-thought）以增强复杂推理能力；全部在百度昆仑P800芯片上训练，展示了在5000芯片上超90%扩展效率。

Result: 在通用基准上与开源最佳模型相当，并在CCBench、SEEDBench IMG、ScienceQA、MMStar等基准上达到SOTA；OCRBench 873和DocVQA分别在OCR和文档理解上取得优异成绩（DocVQA 94.75%）；MathVista数学推理达到78.6%；在昆仑P800上实现超过90%扩展效率。

Conclusion: Qianfan-VL通过多阶段训练和高精度数据合成，在通用和领域任务上都取得了显著提升，尤其在OCR、文档理解、数学推理和逻辑推断等领域表现突出；同时展示了在大规模AI基础设施上训练多模态大模型的可行性与高效性。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 把ASM变成ODE，用Rectified Flow学轨迹+单步推理；用MCBM生成非齐次雾训练数据；在真实数据上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于ASM的物理模型无法充分应对真实世界中复杂、多样的雾霾分布，且缺乏成对真实数据导致深度模型泛化能力不足，因此需要一种既有物理约束又能适应复杂真实雾的学习框架。

Method: 将ASM形式化为ODE框架，使用受Rectified Flow启发的学习策略训练模型以获得从有雾图像到清晰图像的最优ODE轨迹，支持单步推理；并设计MCBM方法生成逼真的非齐次雾图像用于训练。

Result: 在多种真实世界去雾基准数据集上，HazeFlow在主观和客观指标上达到了最新的最优或竞争性表现，证明其在真实场景中的适应性和有效性。

Conclusion: HazeFlow通过将大气散射模型（ASM）重构为常微分方程（ODE），并结合基于Rectified Flow的最优ODE轨迹学习与单步推理，显著提升了真实场景去雾性能；同时引入基于马尔可夫链布朗运动（MCBM）的非齐次雾生成方法以缓解配对真实数据稀缺问题。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 通过剪枝+QAT+TensorRT优化，EcoWeedNet在Jetson Orin Nano上实现了显著压缩与加速，同时保持甚至提升了检测精度，适用于精准农业的实时部署。


<details>
  <summary>Details</summary>
Motivation: 农业边缘设备算力和存储受限，需将准确的杂草检测模型压缩并加速以实现实时部署和精细农业应用。

Method: 对EcoWeedNet进行结构化通道剪枝以减少参数和计算量，结合量化感知训练（QAT）保持精度，并在NVIDIA TensorRT上进行FP16加速，同时解决了残差、注意力、拼接和CSP块等复杂结构剪枝的挑战。

Result: 模型体积最多减小68.5%，计算量减少约3.2 GFLOPs，FP16下推理速度达184 FPS，比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝后的模型优于YOLO11n和20%剪枝的YOLO12n，达到83.7%精度、77.5%召回和85.9% mAP50。

Conclusion: 该论文通过结构化通道剪枝、量化感知训练和在Jetson Orin Nano上使用TensorRT加速，成功显著压缩并加速了EcoWeedNet，使其更适合资源受限的农业边缘设备部署。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出一种结合可学习模态token、增强模态丢弃和对比学习的多模态框架，提升缺失鲁棒性与单模态下性能，并在临床视觉+表格任务上达成SOTA，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现实临床数据常存在模态不均衡与部分模态缺失，现有多模态方法在缺失或仅单模态可用时性能下降，且需要可扩展、低成本且易于与基础模型集成的解决方案。

Method: 引入可学习的模态token以处理缺失感知融合；在训练中使用增强的模态dropout策略模拟真实缺失；将单模态对比目标扩展为包含融合后多模态表示的对比损失，以增强跨模态对齐与鲁棒性。

Result: 在大规模临床视觉+表格数据集的疾病检测与预测任务上取得SOTA结果，尤其在仅有单一模态输入的困难场景中表现显著；同时展示了与CT基础模型集成的可适配性。

Conclusion: 提出的框架通过可学习的模态token、增强的模态丢弃和对比学习，提升了在模态不平衡与缺失场景下的多模态融合性能，特别是在仅有单模态可用时仍能保持SOTA表现，并能与大型CT基础模型结合，具备实际临床应用潜力。

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 系统比较了九种分割架构在490例密集标注CTPA上的表现，发现3D U-Net+ResNet和3D模型总体最佳，CNN优于ViT，分类预训练可能不利于分割，远端栓子依然难以分割；最佳模型Dice≈0.713并通过公有数据验证泛化性。


<details>
  <summary>Details</summary>
Motivation: 肺动脉栓塞（PE）在CTPA影像中呈现形态多变且常为小体积病灶，现有模型和预训练策略对PE分割的适用性和差异尚未充分比较；需要明确哪类架构和训练策略更适合提高临床可用的PE分割性能。

Method: 构建490例密集标注的CTPA内部数据集；在统一测试框架下评估九种分割架构（包含CNN与ViT家族），比较随机初始化与预训练权重两种方案；通过定量指标（Dice、误检/漏检计数等）和栓子级别检测统计分析性能；对中心/大体积与远端小体积极别进行性能分析，并在公有数据集上做泛化验证。

Result: 主要观测包括：1) 3D U-Net+ResNet为最有效架构；2) 3D模型适配PE形态学特点；3) CNN优于ViT；4) 分类预训练（即便基于大规模PE数据）可能降低分割性能；5) 不同架构在相同训练数据上的表现模式高度一致；6) 远端栓子分割仍困难且数据匮乏。最佳模型在60例内部测试扫描中以Dice=0.7131分割出181个栓子，伴随49个假阳性与28个假阴性，并在公开数据集上验证了泛化能力。

Conclusion: 本研究通过密集标注的内部CTPA数据集和统一评测框架，对九种流行的分割架构进行了系统对比，得出3D U-Net+ResNet、3D模型整体优势、CNN优于ViT、分类预训练可能损害分割性能、不同架构在相同数据下表现一致性高、远端栓子分割仍具挑战性等结论。最佳模型在内部测试上达到了平均Dice 0.7131，并在公有数据集上验证了泛化性。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出结合解剖学图结构与对比学习的GNN，分离时序与静态手形，首创序列手形识别基准，准确率从25%提升到46%。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有重要的音系意义，但现有计算方法很少显式建模手形，导致识别精度和语言学分析受限。

Method: 采用了解剖学驱动的手部拓扑图与对比学习策略，模型将静态手形配置与时间维度分离，利用图神经网络学习关节间关系并通过对比损失增强类内一致性与类间区分。

Result: 在37类手形的识别基准上取得46%的准确率，显著优于基线方法25%的成绩。

Conclusion: 本文提出了一种将时间动态与静态手形结构分离的图神经网络，用于提高手形识别性能，并建立了首个面向序列签名中结构化手形识别的基准。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 研究表明分类任务本身对胎儿超声的OOD检测有巨大影响；选择任务应基于ID-OOD定义，并与不确定性方法和下游应用目标对齐。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注不确定性量化方法，但很少研究分类任务本身对OOD检测的影响；在胎儿超声等异质性大的医疗图像场景，理解任务影响对可靠部署至关重要。

Method: 在胎儿超声数据上，针对四个分类任务（涉及不同ID-OOD定义）比较八种不确定性量化方法，评估每种组合的OOD检测与放弃预测性能。

Result: 实验证明：1）OOD检测表现随分类任务显著不同；2）最佳任务取决于OOD来源（图像特征变化或解剖特征变化）；3）OOD检测优越并不必然导致放弃预测最优，需针对具体应用选择任务和不确定性策略。

Conclusion: 任务选择显著影响超出分布（OOD）检测效果，最佳任务依赖于ID与OOD定义（图像特征变化或解剖特征变化）；更好的OOD检测不一定带来更优的放弃预测，需要根据下游应用对齐任务与不确定性策略。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出利用正射地理数据的轻量级定位范式，发布OrthoLoC大规模多模态数据集，并提出AdHoP匹配精化方法，显著提升跨域无人机—正射图像定位性能。


<details>
  <summary>Details</summary>
Motivation: 在资源受限（无网络、无GNSS）场景下，传统依赖大型图像库或重3D模型的高精度视觉定位不可行，然而政府释放的正射地理数据轻量且可用，尚未被充分利用。

Method: 构建16,425张来自德国和美国的UAV影像与配对正射图像数据集；设计可分离评价框架，独立评估检索与特征匹配；提出通用的匹配精化方法AdHoP，可与任意特征匹配器结合提升匹配率与减少平移误差。

Result: 发布OrthoLoC数据集（16,425张UAV图像，德国与美国）、系统评估域间迁移、分辨率与共视性对定位的影响；AdHoP将匹配率提升最多95%，平移误差最多降低63%。

Conclusion: OrthoLoC提出了利用正射地理数据进行高精度无人机视觉定位的新范式，构建了包含多模态、跨域两国的大规模数据集，并提出了能显著提升匹配与定位的后处理方法AdHoP。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: SSDnet通过在单张图像上进行patch级自我重建并引入掩码/打乱/噪声与内积感知损失，实现无需外部数据的高效异常定位，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景中缺乏训练集或参考样本，需在仅给定单张测试图像的情况下完成异常定位，因此利用CNN的归纳偏置学习图像先验以实现零样本检测。

Method: 基于Deep Image Prior的思想，设计了一个patch级别的自我重建网络：直接输入图像进行自监督训练，同时使用掩码、patch打乱与小高斯噪声防止恒等映射，并采用基于内积相似度的感知损失捕捉结构信息。

Result: 在MVTec-AD上达成0.99 AUROC和0.60 AUPRC，在fabric数据集上达成0.98 AUROC和0.67 AUPRC，优于现有方法。

Conclusion: 该论文提出了一种名为SSDnet的单图像异常定位方法，能在无训练数据的零样本情况下实现异常检测与定位。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 针对孟加拉语，提出基于MaxViT+mBART-50并结合PAL+InfoNCE+Sinkhorn OT的三重损失训练，用LaBSE验证的双语对和11万合成图像训练，显著提高了图文对齐与定位性能，实验证明在Flickr30k和MSCOCO数据集上超越CE基线并缩小真实-合成差距。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在低资源语言上容易生成语义流畅但对错的描述，原因包括配对数据稀缺、基于翻译的对齐断裂以及以英语为中心的预训练忽视目标语言语义。需要一种计算成本可控且能改善定位与对齐的方案。

Method: 构建一个计算友好的孟加拉语图片描述流水线：使用冻结的MaxViT提取稳定视觉patch，孟加拉语原生mBART-50作为解码器，轻量级桥接模块连接视觉与语言。训练数据包括LaBSE验证的EN-BN对和11万条双语提示生成的合成图像。核心训练目标为三重损失：Patch-Alignment Loss通过解码器的交叉注意力对齐真实与合成patch描述子；InfoNCE约束全局真实-合成分离；Sinkhorn最优传输确保细粒度patch一一对应和平衡。

Result: 在Flickr30k-1k上获得BLEU-4 12.29、METEOR 27.98、BERTScore-F1 71.20；在MSCOCO-1k上获得BLEU-4 12.00、METEOR 28.14、BERTScore-F1 75.40；相较强CE基线有显著提升，并将真实-合成质心差距缩小41%。

Conclusion: 通过三重损失（PAL+InfoNCE+OT）在低资源语言（孟加拉语）上显著改善视觉-语言对齐与定位能力，实验证明在Flickr30k-1k和MSCOCO-1k上超越了交叉熵基线并缩小了真实-合成中心差距。

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV: distills UniAD into a 28M camera-only BEV model via multi-stage distillation, achieving near-teacher full-stack autonomy (detection, map segmentation, forecasting, occupancy, planning) at real-time speed with much fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Enable deployment-ready, real-time full-stack autonomous driving by compressing large multi-modal perception-planning models into efficient camera-only models suitable for resource-constrained settings.

Method: Proposes a model-agnostic, multi-stage distillation combining feature-level, output-level, and adaptive region-aware supervision to transfer knowledge from a planning-oriented teacher (UniAD) to a 28M-parameter student backbone.

Result: On nuScenes, TinyBEV (camera-only, 28M params) achieves 39.0 mAP detection, 1.08 minADE forecasting, 0.32 collision rate, runs at 11 FPS (5x faster) and reduces parameters by 78% compared to UniAD.

Conclusion: TinyBEV demonstrates that a compact, camera-only BEV model can retain comprehensive autonomy-stack capabilities via multi-stage distillation from a large teacher, achieving strong performance with much lower parameters and real-time speed.

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 把球标在模糊条纹中心并标注模糊属性，配合多帧注意力网络（BlurBall），能更好地检测高速运动球并提高轨迹预测，已发布相关乒乓球数据集。


<details>
  <summary>Details</summary>
Motivation: 传统标注将球置于模糊前沿，造成标签不对称并丢失与速度相关的运动信息；通过更合理的中心化标注与模糊属性标注，可以利用模糊条纹提供的速度线索，改善检测与预测。

Method: 提出将标注点从模糊前沿改为模糊中心，同时添加模糊属性标签；构建并发布一个新的乒乓球数据集；设计BlurBall模型，采用多帧输入并结合注意力机制（如Squeeze-and-Excitation）联合估计球位置与模糊属性。

Result: 在多种模型上均观察到检测性能提升；BlurBall在球检测上达到SOTA结果，并通过利用模糊信息提升轨迹预测的可靠性，利于实时体育分析。

Conclusion: 将球标注在模糊条纹中心并显式标注模糊属性，有助于提高快速运动物体（如乒乓球）的检测与轨迹预测性能。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 用压缩域运动向量在关键帧检测结果上做传播，可在不训练且无标签的前提下降低开域检测器调用次数，并在视频中保持较好的零样本检测性能。


<details>
  <summary>Details</summary>
Motivation: 在视频中对每帧运行开域检测器成本高昂，作者希望通过低成本的传播策略减少检测器调用频次，同时保留开域（open-vocab）检测的零样本能力和较高精度。

Method: 方法在固定间隔关键帧上运行OWLv2，仅在关键帧上获得检测框，再用压缩域的运动向量（3x3网格聚合）对框进行平移和均匀尺度更新，辅以面积增长检测和可选的单类切换机制；不需要任何训练或标注，使用统一的提示词列表。

Result: 在ILSVRC2015-VID验证集上，MVP在mAP@0.5上达到0.609、mAP@[0.5:0.95]=0.316。在较宽松IoU阈值（0.2/0.3）下性能接近逐帧OWLv2-Large；优于在相同关键帧计划下的跟踪器（MOSSE、KCF、CSRT）。相比监督的YOLOv12x（mAP@0.5=0.631），MVP保持无标注和开放词汇优势。

Conclusion: 该论文提出一种训练免费的视频目标检测传播方法（MVP），通过在关键帧上运行开域检测器（OWLv2）并使用视频压缩域运动向量将检测结果传播到中间帧，从而显著减少检测器调用，同时保持较强的零样本覆盖能力。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 通过在输入端加白平衡预处理，可以在不重训光照估计模型的情况下，显著提升单张图像HDR光照颜色估计的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 单张图像HDR光照估计在AR等场景中广受关注，但颜色鲁棒性常被忽视。现有评估通常把颜色与强度、方向等混为一谈；作者希望将颜色作为独立变量，验证是否能通过简单适配提升颜色准确度，从而改善合成结果的真实感。

Method: 不提出新模型，而是在输入端使用已有的适应策略，主要比较几种预处理和适配方法；提出并测试使用预训练白平衡网络对输入图像进行校正，再送入既有光照估计网络。通过包含多种光色的HDR数据集进行系统化评估，并在三种最新方法上复现验证。

Result: 在新构建的多样灯色HDR数据集上，预训练白平衡网络的预处理策略在所有测试场景中均超越其它适配方法，提升了颜色估计精度且不需对下游光照估计模型重训练；在三个最先进方法上均获得一致改进，显示该策略的普适性。

Conclusion: 简单的白平衡预处理能显著提高现有单张图像HDR光照估计模型的颜色鲁棒性，无需重新训练模型即可在多种场景中改善颜色准确性。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出一个训练零样本的支票字段检测框架，结合VLM与MLLM实现关键字段的自动定位，能在受限标注数据下有效工作并用于生成标注数据以训练专用模型。


<details>
  <summary>Details</summary>
Motivation: 传统支票字段检测依赖大量多样且精细标注的数据，但因隐私和专有限制难以获得；因此提出一个无需训练、能在现实金融场景快速部署的解决方案。

Method: 使用VLM对支票图像进行视觉特征理解，并通过MLLM用自然语言提示（prompt）推断并定位关键字段；系统不需额外训练，直接进行零样本检测；同时将模型输出转换为可用的标注（如边界框）以构建训练集。

Result: 在110张手工整理的多格式支票数据集上进行定量评估，模型显示出较强的性能和泛化能力；此外，框架可以作为生成高质量标注数据的引导，以支持后续专用实时目标检测模型的训练。

Conclusion: 该论文提出了一个无需训练的支票字段检测框架，结合视觉-语言模型（VLM）与多模态大语言模型（MLLM），实现对签名、MICR、票面金额等关键字段的零样本检测，并可用于生成标注数据以训练专用检测模型。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: TL;DR：研究表明主流VLM在有噪声或遮挡的图表上容易产生幻觉和自相矛盾回答。作者发布了CHART NOISe数据集并提出基线缓解方法，为提升图表理解鲁棒性提供了测试平台。


<details>
  <summary>Details</summary>
Motivation: 动机：现有图表理解基准假设图表干净且问题基于事实匹配，但现实世界图表常有畸变和遮挡，且需要超出简单匹配的推理，现有VLM在此类场景下易出错且给出误导性解释，亟需严格测试集与评估方法。

Method: 方法：对比评估三种大模型（ChatGPT 4o、Claude Sonnet 4、Gemini 2.5 Pro）在含有腐损和遮挡的图表上的表现，构建CHART NOISe数据集（包含图表腐损、遮挡和基于韩国CSAT英语题型的多项选择问题），并引入“prompt reverse inconsistency”测试模型自相矛盾的倾向。同时提出质量过滤与遮挡检测等缓解策略作为基线。

Result: 结果：在存在腐损或遮挡时，三种模型性能显著下降，幻觉现象（如数值捏造、趋势误判、实体混淆）与自相矛盾回答频率上升；模型对退化输入依然过度自信。CHART NOISe有效暴露这些弱点，初步缓解策略（质量过滤、遮挡检测）能带来一定改进但不足以完全消除问题。

Conclusion: 本文结论：当前最先进的视觉语言模型在图表理解上对噪声和遮挡高度脆弱，容易产生幻觉（数值捏造、趋势误判、实体混淆）并在退化输入下保持过度自信。引入CHART NOISe数据集并提出基础缓解策略可作为提高鲁棒性和可靠性的基准。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出一种模板/相位自由的神经表示4D-MRI方法，使用SAN和基于Transformer的TMN通过1D呼吸替代信号生成连续形变，显著提升重建效率并保持高解剖精度，适用于实时自适应放疗。


<details>
  <summary>Details</summary>
Motivation: 传统4D重建依赖相位分箱或模板扫描，难以处理时间变异、增加工作流程复杂性并且计算量大，因此需要一种能连续表示呼吸运动、提高效率且保持解剖精度的方法。

Method: 框架由两个协同网络构成：Spatial Anatomy Network (SAN)用于编码连续的3D解剖表示；Temporal Motion Network (TMN)基于Transformer提取的呼吸信号生成时间一致的变形场，直接将运动建模与图像重建融合。

Result: 在19名志愿者的自由呼吸数据集上验证，方法能准确捕获规则及不规则呼吸模式，保持血管和支气管连续性，解剖保真度高；总处理时间从传统方法约5小时降至15分钟训练，单个3D体积推断<1秒；在重建精度上优于传统方法。

Conclusion: 提出了一种基于神经表示的4D-MRI重建框架，将呼吸运动建模为由1D替代信号控制的连续平滑形变，从而取代传统的离散分箱/模板方法。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 对1万帧壁球数据的实测表明：现有卡尔曼滤波跟踪器在快速小目标上误差大且漂移严重，需开发专门方法以满足体育机器人等应用需求。


<details>
  <summary>Details</summary>
Motivation: 快速移动且不规则弹跳的小目标跟踪对体育机器人等场景至关重要，但现有通用跟踪器在此类极端条件下性能未知或不足，故通过实证评估寻找性能瓶颈并为专用方法提供依据。

Method: 基于自建1万帧720p-1280p标注壁球数据集，对五种最新的卡尔曼滤波跟踪器（OCSORT、DeepOCSORT、ByteTrack、BoTSORT、StrongSORT）在四种场景下进行对比实验，关注推理速度和每帧更新频率，并采用平均位移误差（ADE）评估跟踪精度。

Result: DeepOCSORT在ADE上最好（31.15像素），ByteTrack推理最快（26.6ms）；总体5种方法ADE范围31–114像素（对应空间误差约3–11cm），比标准基准误差高3–4倍，显示显著漂移和鲁棒性不足。

Conclusion: 当前基于卡尔曼滤波的跟踪方法在应对快速、小目标（如壁球）时存在明显局限，表现为较大的空间漂移和误差，难以满足运动机器人等应用的高精度需求。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 使用压缩域的运动矢量做无参自适应裁剪，简单高效地提升动作识别准确率或降低计算量，易于集成并适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 减少视频动作识别的计算开销，同时利用压缩域中已有的运动信息找到“运动密集”区域以保留有判别力的视觉内容，从而提高精度或降低FLOPs以满足实时和移动部署需求。

Method: 在H.264压缩域提取运动矢量，进行去噪与合并(DM)、蒙特卡洛采样(MCS)以及运动密度子矩阵搜索的自适应裁剪(AC)，生成单个剪裁区域并将其应用到所有I帧，模块无训练、无新增参数，可插拔到不同骨干网络。

Result: 在UCF101数据集上对多种骨干网络均有一致提升。以ResNet-50为例：在相同FLOPs下Top-1提升3.5%，或在效率设置下以26.5%更少FLOPs提升2.4%；在CoViAR上保持原成本下达到89.2% Top-1，或降低计算至8.5 GFLOPs时仍达到88.5%。对MobileNet-V3、EfficientNet-B1、Swin-B亦有稳定增益。

Conclusion: MoCrop通过利用压缩视频中的运动矢量进行自适应裁剪，在不增加训练或参数负担的前提下显著提升视频动作识别的效率与精度，适用于多种主干网络并可用于实时部署。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出基于码本的自适应特征压缩+语义增强（CAFC-SE），用VQ在边缘把特征映射为离散索引选择性传输，能在低码率下更好保留关键信息，实验表明在率-准方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对图像或中间特征的压缩方法在低码率下表现差，原因在于保留冗余细节或学到过度集中的符号分布，亟需一种在低码率下仍能保持分析性能的压缩方案。

Method: 在边缘端用向量量化(VQ)将连续特征映射为离散索引并通过码本选择性传输到云端；引入语义增强模块以提升所传特征的信息量；利用VQ将特征投影到最近的视觉原语以在低码率下保持重要模式。

Result: 通过大量实验验证，CAFC-SE在相同或更低比特率下，比基线方法在任务精度上有明显提升，证明了该方法在低码率条件下的鲁棒性与效率。

Conclusion: 提出的CAFC-SE在低码率下能更好保留有信息的视觉模式，从而提升边缘—云图像分析性能，实验显示在率-准确度上优于对比方法。

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet利用多核深度卷积块与多种注意力，得到0.316M参数的超轻量U-Net变体，在多个医学分割任务上以极低资源消耗显著超越多种SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备（如即时检测设备）上实现高精度医学图像分割，需要在极低计算成本下保持或超越SOTA性能。

Method: 设计MKDC以并行多核处理并结合深度可分离卷积，采用通道注意力、空间注意力和分组门控注意力以增强特征表示；构建U形编码-解码网络保持轻量化（0.316M参数，0.314G FLOPs）。

Result: 在六个二分类医学图像数据集上，MK-UNet在DICE等指标上显著优于包括TransUNet、UNeXt、MedT、CMUNeXt、EGE-UNet、Rolling-UNet等方法，同时参数和FLOPs大幅降低（如较TransUNet分别低约333×和123×）。

Conclusion: MK-UNet提出了一个超轻量的多核U形网络架构，通过多核深度可分离卷积块（MKDC）和多种注意力机制实现了资源极低但性能优异的医学图像分割。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat 通过将基于 CT 的网格与可渲染的 3D 高斯结合并在光度监督下联合优化，实现了在单目手术视频指导下对术前 CT 的可逆形变，从而改进了变形手术导航。


<details>
  <summary>Details</summary>
Motivation: 弥合外科视频（单目 RGB）与体积化的患者术前影像（CT）之间的信息鸿沟，以便在存在器官形变时仍能利用术前 CT 进行导航和可视化。

Method: 将 3D 高斯（Gaussians）绑到 CT 网格上，并通过将每个高斯参数化为其父网格三角形的相对坐标来强制高斯与网格对齐；使用光度监督联合优化高斯参数和网格形变，从而将形变反向传播更新 CT。

Result: 在猪腹部实际手术视频和人体肝脏仿真数据上验证，显示方法能在单目 RGB 数据下得到对术前 CT 的合理变形。

Conclusion: BridgeSplat 提出了一种将术中三维重建与术前 CT 数据耦合的变形手术导航方法，能够基于单目 RGB 视频对术前 CT 进行合理变形以适配术中视图。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 本文在SFDA语义分割上提出DGLE：先用置信度过滤+超分辨率获得高质量种子伪标签，再用扩散模型扩展为完整伪标签集，有效降低噪声并提升目标域性能。


<details>
  <summary>Details</summary>
Motivation: 针对SFDA中伪标签噪声多、直接优化全部伪标签效果差的难题，提出从少量可靠标签出发再扩散生成高质量伪标签的策略，以避免噪声累积并提升自训练效果。

Method: 首先通过置信度过滤与超分辨率增强的伪标签融合策略，获取少量高质量伪标签作为种子；然后利用扩散模型的去噪与建模能力，将不规则分布的种子标签传播为完整伪标签集，保证新生成标签的质量。

Result: 方法有效提高了伪标签质量，避免直接优化完整伪标签集的困难，从而在目标域上提升了模型性能（具体数值和对比结果需参见论文正文）。

Conclusion: 提出了基于扩散模型的伪标签扩展框架DGLE，通过从少量高质量伪标签出发并利用扩散模型传播，生成完整且高质量的伪标签，从而提升源-free领域自适应语义分割性能。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: 把特征嵌入Poincaré球，设计双曲对比损失与双曲分类层，并在双曲空间用最大熵分布生成增强样本，提升C2FSCIL的粗细类性能。


<details>
  <summary>Details</summary>
Motivation: 针对C2FSCIL中粗到细标签层次结构的本质，作者认为超曲率（双曲）空间更适合表示层次数据，从而能更好地保持类间层次关系并缓解增量学习中的表征冲突与过拟合。

Method: 将特征映射到Poincaré球模型，设计了超曲率对比损失(hyperbolic contrastive loss)、超曲率全连接层用于分类，并在超曲率空间上用最大熵分布估计细类特征分布以生成增强样本。

Result: 在C2FSCIL基准测试上，方法在粗类与细类准确率上均有提升，表明将嵌入空间由欧氏切换到双曲空间并结合相应损失与数据生成策略是有效的。

Conclusion: 该论文提出在Poincaré球面上构建特征提取器与对比学习、全连接分类层及最大熵概率分布，以提升C2FSCIL任务中粗细类别的表示与分类性能。

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出将对象移除解耦为几何移除与外观渲染的两阶段方法，通过几何级别的严格掩码监督和偏好驱动目标，有效删除目标及其因果视觉伪影，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于外观的方法要么严格按掩码训练但无法移除未显式掩码的因果效应，要么放松掩码对齐但可控性差且可能过度擦除。作者认为原因在于忽视了对象几何存在与视觉效应之间的因果关系。

Method: 两阶段：1) 使用严格掩码对齐的监督在几何域（深度）上进行对象移除，利用偏好驱动目标（正负样本对）约束，既移除对象及其因果伪影又避免新增结构；2) 基于修改后的几何渲染光栅级别的 RGB 图像，使视觉效果作为几何变化的隐式结果产生。

Result: 在两个流行基准上，方法在同时移除对象及其关联伪影（阴影/反射）方面达到最先进性能。代码已公开。

Conclusion: 本文提出了一种几何感知的两阶段图像编辑框架，通过先从几何（如深度）上删除目标对象，再基于更新的几何信息渲染外观，从而更好地移除目标及其因果视觉伪影（如阴影、反射）。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: Proposes SEGA: ensemble of Gaussian-smoothed source gradients plus perturbation filtering to craft transferable, imperceptible black-box attacks against NR-IQA models; validated on CLIVE.


<details>
  <summary>Details</summary>
Motivation: Existing white-box attacks on NR-IQA models lack transferability to black-box settings; need methods that can craft transferable adversarial examples without access to the target model.

Method: SEGA approximates target gradients by Gaussian smoothing applied to multiple source models and ensembles their signed gradients; it also applies a perturbation filter mask to remove perceptible perturbations.

Result: On the CLIVE dataset, SEGA achieves superior transferability compared to baselines, enabling successful transfer-based black-box attacks while keeping perturbations imperceptible.

Conclusion: This paper proposes SEGA, a transferable black-box attack for NR-IQA models, demonstrating improved transferability and imperceptibility of adversarial examples.

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet用Hadamard乘法在transformer与D-Marker间实现参数无关的直接融合，既提升识别性能又降低参数和训练复杂度，适合实时情感计算部署。


<details>
  <summary>Details</summary>
Motivation: 现有结合深度学习与手工D-Marker的多任务方法在辅助任务监督和损失平衡上效率低下，需更高效直接的特征融合方法以便实用部署。

Method: 提出15种特征融合策略并系统评估，最终采用Hadamard乘法融合transformer特征与D-Marker，无需额外任务监督或复杂损失平衡，实现参数减少与计算效率提升。

Result: 在UvA-NEMO、MMI、SPOS、BBC四个基准集上分别达到88.7%、99.7%、98.5%、100%的准确率，较多任务方法在若干数据集有显著提升，同时参数减少26%并简化训练流程。

Conclusion: HadaSmileNet通过参数无关的Hadamard乘法融合transformer表示与生理学D-Marker特征，有效区分真挚与伪装笑容，达成多数据集新SOTA并减少参数与训练复杂度。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 利用事件相机和3D Gaussian Splatting，提出事件引导损失和统一语义高斯体素表示，实现无需外部人体掩码的高质量快速运动下人-场景联合重建，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目RGB视频在快速运动下存在运动模糊，导致动态人体与场景联合重建困难；事件相机具有微秒级时间分辨率，可提供对快速运动更鲁棒的信号。

Method: 构建统一的3D高斯体素集合，附带可学习的语义属性；仅对被分类为人体的高斯体素进行形变以实现动画，场景高斯体素保持静态。提出事件引导损失，通过匹配连续渲染亮度变化与事件流来强化快速运动区域的局部细节。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur两个基准数据集上，该方法在PSNR/SSIM上显著优于强基线，并在LPIPS上表现更低，尤其对高速运动主体提升明显。

Conclusion: 该工作提出了一种基于事件相机与3D Gaussian Splatting的统一框架，实现了从单目事件流同时重建动态人体与静态场景。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: 提出Live-E2T：用语义元组＋在线去重＋CoT微调的LLM，实现实时且可解释的威胁监测，实验证明优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时兼顾实时性能与决策可解释性，监督或生成模型在信息压缩或推理透明度上存在权衡，故提出统一框架以解决该矛盾。

Method: 1) 将视频帧解析为Human-Object-Interaction-Place语义元组，形成紧凑语义表示；2) 引入高效的在线事件去重与更新机制，去除时空冗余以保持实时性；3) 对大模型进行Chain-of-Thought风格微调，使其在事件序列上进行透明的推理并生成威胁评估报告。

Result: 在XD-Violence与UCF-Crime等基准数据集上，Live-E2T在威胁检测准确率、实时响应效率及可解释性方面均明显优于现有最先进方法。

Conclusion: Live-E2T通过结构化语义元组、在线去重与更新机制以及基于Chain-of-Thought微调的LLM，实现了实时威胁监测与可解释评估的统一，显著提升了检测精度、效率与可解释性。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 作者通过数据集、模型和基准三方面推动MLLM在摄影美学理解的能力，实验表明所提方法显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM在美学视觉理解上存在差距，主要被训练用于一般视觉任务（识别、定位），难以处理需要摄影专业知识的美学评估与描述。

Method: 构建来自专业摄影师讨论的大规模、多样化、高专业性数据集PhotoCritique；提出语言引导的多视角视觉融合模型PhotoEye；设计综合性基准PhotoBench用于评估美学视觉理解。

Result: 在现有基准和PhotoBench上，PhotoEye相较于现有模型显示出明显优势，能更好地完成专业化的摄影美学分析和描述。

Conclusion: 本文提出了PhotoCritique数据集、PhotoEye模型和PhotoBench基准，有助于提升MLLM在摄影美学理解方面的能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 基于XMem的记忆增强实时肿瘤分割框架，针对长序列与标注稀缺，作者报告了定性良好且满足实时性的初步结果，但缺乏可复现的量化实验数据。


<details>
  <summary>Details</summary>
Motivation: 提升MRI引导放疗中肿瘤实时跟踪与分割的精度与可靠性，尤其在长序列cine-MRI与标注稀缺场景下保持实时性能。

Method: 采用XMem记忆增强网络，将关键帧记忆与当前帧特征结合以实现长时序肿瘤跟踪和分割，适配实时处理需求并在有限标注下利用记忆机制缓解标注稀缺问题。

Result: 作者丢失了详细实验记录，无法给出精确定量结果；但开发期初步印象显示方法在挑战条件下仍能提供合理分割并满足临床实时性要求。

Conclusion: 该工作提出了一种基于XMem的肿瘤分割框架，面向实时MRI引导放疗，强调了长序列记忆增强跟踪的思路，认为可满足实时性和合理的分割性能。

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM，通过动态对齐、语义级聚合和时空-频率融合三模块，有效提升MC-MRI超分辨的对齐与细节恢复，参数更少且性能领先。


<details>
  <summary>Details</summary>
Motivation: 动机是利用高分辨率参考对比度改善低分辨率目标对比度的细节和采集效率，但传统方法在空间语义对齐和频域信息利用上不足，导致细节恢复和解剖一致性差。

Method: 方法包括三个关键模块：1) 动态空间扭曲模块用于跨对比度的像素级/区域级配准以解决结构和运动差异；2) 语义感知token聚合块用于建模长距离语义一致性，增强对参考图像的语义引导；3) 空间-频率融合块将空间域特征与频域高频信息融合，用以恢复细微结构和纹理。整体网络轻量化且参数较少。

Result: 在公开及私有数据库上的实验证明SSCM以更少参数实现了更好的图像质量、细节保留和空间-语义一致性，达到或超过现有最先进方法的性能。

Conclusion: 本文提出的SSCM在保持空间语义一致性方面有效，通过动态空间扭曲、语义感知token聚合和时空-频率融合模块，提升了MC-MRI超分辨重建的细节恢复能力，并在公开及私有数据集上取得了领先性能。

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO（单阶段RL+oracle偏好监督）和FactS（事实级句子奖励），在低资源下用小模型实现CheXpert Plus SOTA，显著提高学习效率与临床一致性。


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告生成方法依赖多阶段训练、大规模配对语料和超大模型，数据与计算消耗高。目标是在有限数据与算力下提高临床相关病例的学习效率和报告质量。

Method: 提出单阶段、仅基于强化学习的训练流程（OraPO），通过在探索失败的罕见或困难病例上加入轻量级的oracle步骤，将失败探索转化为直接的偏好监督；同时设计FactScore（FactS）奖励，通过抽取原子临床事实并与真实标签进行蕴含检查，为句子级别提供稠密且可解释的奖励信号。

Result: 在受限训练数据和小型视觉语言模型下，结合OraPO和FactS在CheXpert Plus上达到0.341的F1，训练数据需求比现有方法减少2--3个数量级，且在困难病例上表现显著提升。

Conclusion: 本文提出了在受限资源下通过Oracle-educated GRPO（OraPO）和基于事实的奖励（FactS）在放射学报告生成任务中取得高效训练的方法，展示了在CheXpert Plus数据集上用小规模模型和极少训练数据实现SOTA性能的可能性。

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个无需训练的框架，通过语义token分解与相似性重加权注入冻结扩散模型，实现可控且可扩展的多参考风格融合，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有参考式方法通常只能接受单一风格参考且缺乏在多个风格之间进行原则性平衡的机制，限制了混合美学与扩展性。

Method: 将所有风格图像和文本提示编码为语义token，并在每层cross-attention中注入这些token的分解表示；设计相似性感知重加权模块，在每个去噪步骤重新分配每个风格组件的注意力权重，实现无微调的动态融合。

Result: 在定性与定量评估中，AMSF在多风格融合效果上优于现有最先进方法，并能无缝扩展到两个或更多风格，显示出更好的平衡性和可控性。

Conclusion: AMSF提出了一种训练-free的参考风格多融合框架，通过在冻结的扩散模型的交叉注意力层中注入自适应的语义token分解模块，并结合相似性感知重加权模块，在去噪每一步动态平衡多风格影响，从而实现可控且可扩展的多风格混合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: 提出MLF-4DRCNet，通过点/场景/提案三级多模态融合（ERPE/HSFP/PLFE）增强4D雷达—相机检测，显著提高性能并在公开数据集上达到或接近LiDAR水平。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达点云稀疏且噪声多，传统借鉴自激光雷达的BEV级融合方法忽视雷达几何不完整性和细粒度信息，限制感知性能，因此需要针对雷达特点设计多层次融合策略。

Method: 提出三大模块：ERPE在点级利用2D图像实例增密雷达点并用Triple-Attention VFE编码为体素；HSFP在场景级采用可变形注意力多尺度融合体素与图像特征并进行池化；PLFE在候选框级进一步融合图像特征与HSFP池化特征以精炼提案。整体为两阶段检测框架，支持点-场景-提案三级信息交互。

Result: 在VoD与TJ4DRadSet上取得SOTA成绩；在VoD上性能可与基于LiDAR的模型相媲美，证明多层次融合能弥补雷达点云固有缺陷。

Conclusion: 本文提出的MLF-4DRCNet通过多层次融合4D雷达与相机信息，显著提升了稀疏噪声雷达点云的检测性能，并在VoD与TJ4DRadSet数据集上达到了最先进水平，尤其在VoD上接近激光雷达模型表现。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS用提示引导的双潜在流和LQR控制，在不训练的前提下通过动态步级控制避免语义漂移，能更好地同时保留图像细节和语义一致性，实验证明优于单一潜变量方法。


<details>
  <summary>Details</summary>
Motivation: 单一潜变量编码难以在结构保真与语义准确之间取得平衡，常导致重建出现模糊或属性错误等语义漂移问题。

Method: 基于Rectified Flow稳定反演路径，构建结构路径保持源图像细节，语义路径由文本提示引导；将双重引导建模为最优控制问题，推导出线性二次调节器(LQR)的闭式解，用以在每一步动态引导生成轨迹，无需逐图优化。

Result: 在FFHQ-1K和ImageNet-1K上，对高斯去模糊、运动去模糊、超分辨率和任意形状修补等多种反演任务，PDLS在结构保真度和语义对齐上均优于单潜变量基线方法。

Conclusion: PDLS通过双流（结构路径+语义路径）和LQR控制器在扩散模型潜变量反演中有效防止语义漂移，实现了在多种损坏图像重建任务中兼顾结构保真与语义准确性的改进。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: Prima, a VLM trained on 220K+ MRI studies and tested on 30K in a year-long study, achieves 92% mean AUROC across 52 neuro diagnoses, offers explainable outputs and workflow tools, outperforms existing models, and shows fairness and potential to reduce healthcare disparities.


<details>
  <summary>Details</summary>
Motivation: Rising MRI demand strains healthcare systems, increases turnaround time and physician burnout, disproportionately affecting low-resource and rural populations; need for a scalable AI foundation for neuroimaging that can handle real-world clinical MRI studies and reduce disparities.

Method: Trained Prima on over 220,000 MRI studies using a hierarchical vision architecture to learn transferable MRI features; evaluated in a prospective 1-year health system-wide study with 30K MRI studies, comparing performance across 52 diagnoses and against state-of-the-art models; provided explainable outputs, worklist prioritization, and referral recommendations.

Result: Prima achieved mean diagnostic AUROC of 92.0 across 52 radiologic diagnoses, outperformed other AI models, provided explainable differential diagnoses and operational tools, demonstrated algorithmic fairness, and helped mitigate system biases like longer turnaround times for underserved groups.

Conclusion: Prima is a scalable, health-system-trained vision-language model that significantly improves MRI diagnostic performance and workflow, showing high accuracy, explainability, and fairness across demographics and MRI systems, with potential to reduce disparities and radiologist workload.

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: UiG通过在生成过程中循环利用统一模型的理解能力并以图像编辑指导生成，从而显著提升文本到图像生成性能（TIIF长提示提升3.92%）。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought方法将理解与生成分离，导致生成端难以受益于强大的理解能力；因此希望在推理时将理解能力主动引导到生成环节以弥补生成缺陷。

Method: 提出以“图像编辑”作为桥梁的推理流程：先生成图像并由统一模型进行理解与验证，将理解结果转化为编辑指令，逐步对图像进行增强以将理解信息注入生成过程。

Result: 在文本到图像生成任务上，UiG显著优于现有推理方法；在TIIF基准的长提示设置上提升约3.92%。

Conclusion: 本文提出Understanding-in-Generation (UiG)框架，通过将统一模型的理解能力融入生成过程以提升图像生成质量。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 作者发布了EndoSynth合成数据集并建立真实内镜图像上的深度估计基准，表明用该合成数据微调可显著提升模型在真实内镜场景的表现。


<details>
  <summary>Details</summary>
Motivation: 目前内镜图像领域缺乏高质量数据集与稳健的基准，阻碍了基于大型基础模型的深度估计在临床场景中的推广与验证。

Method: 收集并评估现有基于transformer的度量和相对深度估计模型，在真实未见内镜图像上进行基准测试；构建包含手术器械、真实感渲染的合成数据集（带度量深度与分割标签）；使用该合成数据对深度基础模型进行微调，并在真实数据上验证性能提升。

Result: 公开了EndoSynth合成数据集与训练权重，基于合成数据的微调在多数未见真实内镜数据上显著提升了深度估计精度；同时提供了对多种最先进模型的综合比较与泛化分析。

Conclusion: 本文通过建立基准测试和发布合成数据集EndoSynth，显著改善了内镜图像深度估计的泛化性与性能，为临床应用提供了实用资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba：结合局部强调SSM和SSM自适应融合，用线性复杂度建模长距依赖并强化局部语义与跨模态融合，显著提升RGB-D/RGB-T显著性检测的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 动机是现有基于CNN的方法受限于局部感受野，Transformer方法则面临二次复杂度成本，难以在性能与计算效率之间取得平衡。近期SSM（Mamba）展现了线性复杂度下建模长距离依赖的潜力，但直接应用于RGB-D SOD会导致局部语义缺失和跨模态融合不足，因此需要设计增强局部语义和自适应融合的机制。

Method: 方法包括两个关键模块：1) 局部强调状态空间模块（LE-SSM），用于同时在RGB和深度模态上捕捉多尺度局部依赖；2) 基于SSM的自适应融合模块（AFM），用于实现互补的跨模态交互和可靠的融合。整体框架基于Mamba SSM，保持线性复杂度。

Result: 实验表明，LEAF-Mamba在多个RGB-D SOD基准上优于16个最先进方法，在效率和效果上均有提升；此外在RGB-T SOD任务上也表现优异，显示出良好的泛化能力。

Conclusion: 该论文提出了基于状态空间模型（SSM）的RGB-D显著性目标检测方法LEAF-Mamba，通过引入本地强调模块和自适应融合模块，克服了SSM在局部语义建模和跨模态融合上的不足，从而在性能和效率上均优于现有方法。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出WMHAM+SAM的轻量级Transformer模型，在两个食物分类数据集上取得高精度并大幅降低参数和计算开销，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 为了解决Vision Transformer在食物图像分类任务中参数多、计算复杂度高，不利于在生产线或嵌入式设备部署的问题，提出轻量化注意力机制以在效率与性能之间取得平衡。

Method: 在模型结构上，使用WMHAM对图像进行窗口划分以捕获局部和全局上下文，降低计算复杂度；同时引入SAM在空间维度自适应地强调关键区域以增强判别特征。

Result: 在Food-101和Vireo Food-172数据集上分别达到了95.24%和94.33%的准确率，与基线方法相比在参数量和FLOPs上显著减少，证明了方法的有效性。

Conclusion: 该论文提出了一种轻量级的食物图像分类算法，通过引入窗口多头注意力机制（WMHAM）和空间注意力机制（SAM），在保持较高分类准确率的同时显著降低参数量和计算量，适合资源受限场景部署。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA：一个无标注、三阶段的开放集遥感土地覆盖发现与描述框架，结合SAM微调分割、MLLM语义描述与LLM评判，实现可扩展且可解释的地物自动发现与语义标注。


<details>
  <summary>Details</summary>
Motivation: 解决遥感开放集分析中既要进行细粒度空间定位又要实现语义开放标注的挑战，避免依赖人工标注并提高解释性与可扩展性。

Method: 三阶段流水线：(1) 基于微调的可提示分割模型(SAM)进行精确发现与掩码提取；(2) 通过两阶段微调的多模态大模型(MLLM)进行语义归属与上下文描述；(3) 使用LLM作为评判并结合人工评分对MLLM输出进行评估。

Result: 提出的OSDA框架在多种卫星影像上无需标注即可实现目标发现、分割与语义描述，支持自动制图更新和大规模地观测分析，展示出较强的实用潜力（文中宣称的性能提升与案例演示）。

Conclusion: OSDA提出了一种无监督、三阶段的开放集土地覆盖发现、分割及描述框架，可在无需人工标注下实现像素级定位与高层语义解释，具备可扩展性和跨影像的鲁棒性。

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF2021透過數十萬標本與少量照片的跨域訓練，證實herbarium數據能幫助提升對熱帶地區植物的自動辨識，且結合metadata與形態性狀能進一步改善表現。


<details>
  <summary>Details</summary>
Motivation: 現有深度學習植物辨識資料多集中在北美與歐洲，熱帶地區物種多樣性高卻資料缺乏；herbaria保存大量標本資料，若能善用可改善資料貧乏區域的自動辨識效能。

Method: 將挑戰設計為跨域分類：訓練集由數十萬張herbarium掃描圖與數千張實地照片組成，並輔以位置、日期、分類等metadata及每種5項形態/功能性狀；測試集僅含實地照片。參賽隊伍多採用深度學習視覺模型、域適應/對齊策略、metadata融合與特徵工程。

Result: 基於約1000物種的Guiana Shield資料集，實驗顯示利用herbarium資料能提高對熱帶物種在實地照片上的辨識率；最佳方法結合視覺模型與metadata/形態性狀，可進一步提升準確性。

Conclusion: 作者證明使用herbarium（標本）資料可在跨域情境下提升熱帶植物自動辨識，特別是當訓練資料包含大量標本且少數照片時，能學到標本與實地照片之間的對應。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 提出AGSwap：通过组级嵌入交换与自适应组更新实现更连贯的跨类别对象融合，并构建大规模COF基准，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像的跨类别对象融合方法易产生重叠伪影、视觉混乱与语义不一致，且缺乏大规模基准数据集限制了该领域进展。

Method: 提出两部分：1) 组级嵌入交换（Group-wise Embedding Swapping）：在特征空间对不同概念的语义属性按组进行交换与融合；2) 自适应组更新（Adaptive Group Updating）：基于平衡评估分数动态优化组的更新以确保合成结果的连贯性。

Result: 在作者构建的大规模COF基准（基于ImageNet-1K/WordNet，95个超类、每超类10个子类、451,250个融合对）上，AGSwap在各种简单与复杂提示词下均优于包括GPT-Image-1在内的最先进组合T2I方法。

Conclusion: AGSwap通过组级嵌入交换和自适应组更新，有效改善了跨类别对象融合的连贯性与语义一致性，实验表明在COF数据集上优于现有方法。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2019构建了一个聚焦圭亚那盾与北亚马逊的10K物种热带植物数据集，评估了深度学习系统在数据不足地区的识别能力，汇总参赛方法并与专家对比，揭示了当前方法的局限与未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前自动植物识别主要覆盖数万种，但远低于地球约369K种的总量；因此需在物种数据稀缺的热带地区评估识别系统的通用性与实际效果。

Method: 组织了包含约10K热带物种的数据集，采用深度学习方法为主的参赛系统进行识别，评估与热带植物专家的对比分析，并汇总参赛团队的技术路线与实现细节。

Result: 展示了多种基于深度学习的方案在热带植物识别上的表现，提供系统间的比较，并指出与人类专家仍存在差距，同时提出数据质量、长尾分布和现场采集条件等挑战。

Conclusion: 本文总结了PlantCLEF 2019挑战的资源、评估方法、参赛系统与关键结果，强调在数据不足地区进行植物识别的重要性与困难。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: 提出训练免费且基于冻结基础模型的RSVG-ZeroOV，通过VLM注意力、扩散模型细化和注意力演化得到纯化掩码，在零样本开放词汇遥感视觉定位任务中表现优于现有弱监督与零-shot方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于封闭词汇和昂贵训练/微调；已有尝试依赖高质量数据和耗时微调，难以推广到开放世界与零样本场景。作者希望探索冻结的通用基础模型在零样本开放词汇遥感视觉定位中的潜力。

Method: 方法包含三阶段：Overview阶段利用VLM生成跨模态注意力图；Focus阶段借助扩散模型补全结构与形状信息；Evolve阶段通过注意力演化模块净化注意力图生成分割掩码。整个框架不进行任务特定训练。

Result: 在大量实验中，RSVG-ZeroOV优于现有弱监督和零样本方法，展示出在开放词汇遥感视觉定位上的竞争力和实用性。

Conclusion: 该文提出了一种无需训练的框架RSVG-ZeroOV，利用冻结的通用基础模型实现零样本开放词汇遥感视觉定位，具有高效可扩展性。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出APC：用过完备属性字典+属性提示生成器生成属性感知特征，并用快慢训练平衡判别与泛化，显著提升ReID的识别与跨域性能。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型在单域或跨域场景存在局限：单域模型过拟合域特征，跨域模型通过归一化策略可能抑制身份判别性特征；利用文本语义与VLM的泛化能力来同时提升判别性与泛化性。

Method: 设计了属性提示生成器（APG），包含过完备的语义属性字典（SAD）和提示组合模块（PCM），通过从SAD自适应组合属性生成属性感知特征；提出快慢训练策略（FSTS），包含快更新流（FUS）用于快速学习ReID判别信息，慢更新流（SUS）用于保留来自预训练视觉语言模型的通用特征，两者互相作用以抑制过拟合。

Result: 在常规与域泛化ReID基准上，APC在判别性和泛化性指标上均优于最新方法，证明了属性提示与快慢训练策略的有效性。

Conclusion: 该论文提出了基于文本语义的属性提示组合（APC）框架，通过属性提示生成器和快慢训练策略，在保持可识别性的同时增强了域泛化能力，实验证明在常规和跨域ReID数据集上均优于现有方法。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出OTCCLIP：基于最优传输的细粒度图像-文本重构方法，能更好抵御CLIP的投毒/后门攻击并提升受污染数据下的下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于全局表示的匹配方法忽略了视觉与文本的细粒度特征，容易引入错误配对，从而损害CLIP预训练并无法有效防御投毒攻击。

Method: 提出了一种将图像与文本的细粒度特征集合之间的最优传输距离用于重新匹配caption的框架，并通过最优传输目标函数促进跨模态及模态内的细粒度对齐。

Result: 在多种投毒攻击场景下，OTCCLIP显著降低了攻击成功率，同时相比先前方法提升了CLIP在受污染数据上的零样本和线性探测表现。

Conclusion: OTCCLIP通过对图像-文本对进行基于最优传输的重构，有效缓解了针对CLIP模型的数据投毒和后门攻击，并在受污染数据上提升了零次和线性探测性能。

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出基于交互建模的LFI框架，通过Interaction Queries和交互监督将VLM的动态交互模式迁移到VFM，带来更好泛化、收敛和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有VFM多采用面向结果的范式，忽视交互过程，与擅长跨模态交互的VLM在表征上存在差异，导致知识迁移困难和泛化能力受限。

Method: 提出Learning from Interactions (LFI) 框架，核心包括Interaction Queries（跨层持久的关系结构）和基于交互的监督（来源于VLM的跨模态注意力），用于将VLM的交互模式迁移到VFM。

Result: 在多个基准上均有一致提升：TinyImageNet分类提升3.3mAP，COCO检测/分割提升1.6mAP/2.4AP；跨域任务PACS和VLCS分别提升2.4和9.3的零-shot性能；人类评估在语义一致性上超过面向结果方法2.7倍。

Conclusion: 该论文提出通过模拟交互过程来实现从VLM到VFM的知识迁移，从而弥补现有表征差异，提升视觉基础模型的泛化能力。

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出HyPSAM：用DFNet生成高质量视觉提示，再用P2RNet构建混合提示引导SAM细化，解决RGB-T显著性检测的边界与完整性问题，并在多个数据集上达SOTA且具备良好通用性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T SOD在特征融合与数据匮乏下难以同时获得完整目标与精确边界；而SAM具备强大的零样本分割能力，但需有效提示信息。因而通过设计高质量的视觉提示和混合提示策略，结合动态融合网络与精炼网络，提高SAM在RGB-T任务上的适配性和性能。

Method: 方法包括两部分：1) 动态融合网络（DFNet）：引入动态卷积与多分支解码器，基于RGB与热成像自适应交互生成高质量初始显著图，作为视觉提示；2) 插件式精炼网络（P2RNet）：构造混合提示（文本提示确保模态输入可靠，mask与box提示用于精确定位）来引导SAM对初始显著图进行细化。整体框架称为HyPSAM，将DFNet输出作为提示并借助SAM的分割能力完成精细化。

Result: 在三个公开数据集上，HyPSAM取得了最先进的性能；同时该方法具有良好通用性，可作为插件与不同RGB-T SOD方法结合以显著提升表现；作者已开源代码与结果。

Conclusion: 本文提出的HyPSAM通过结合DFNet和P2RNet，有效利用SAM的零样本泛化能力，改善了RGB-T显著性目标检测中边界精度和完整性问题，实验证明在三个公开数据集上达到或超越现有方法，并能与其它方法兼容提升性能。

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE通过多模态（文本+图像深度+LiDAR）交叉注意力融合，提高了点云自编码器在极端噪声和对抗攻击下的重建鲁棒性，适用于低数据真实场景。


<details>
  <summary>Details</summary>
Motivation: 原始点云对噪声、遮挡与对抗扰动高度敏感；自编码器虽适合去噪重建，但在真实复杂条件下性能下降，故引入图像深度与文本语义作为先验以提升鲁棒性。

Method: 提出TriFusion-AE：一个多模态交叉注意力自编码器，融合文本语义、来自多视图图像的单目深度特征与LiDAR点云，通过跨模态注意力对齐语义、几何与空间信息，作为可插拔模块与任意CNN点云自编码器联合训练以增强鲁棒性。

Result: 在nuScenes-mini低数据场景下评估，TriFusion-AE在强对抗攻击和重噪声条件下显著优于传统CNN自编码器，且设计为模型无关可与任意CNN自编码器集成；但在轻微扰动下效果提升有限。

Conclusion: TriFusion-AE通过结合文本先验、单目深度和LiDAR点云的多模态交叉注意力自编码器，显著提升了在强噪声和对抗攻击下的重建稳健性，但在轻微扰动下增益有限。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 提出COLT：用可学习工具代码本实现视频LLM的连续工具使用，配合VideoToolBench数据集，在避免灾难性遗忘的同时提升工具调用性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM方法依赖固定工具库或对闭源LLM进行提示/微调，无法适应工具持续流入和更新的现实环境；因此需要一种能在连续流中学习新工具且不遗忘旧工具的方案。

Method: 引入可学习的“工具代码本”（tool codebook）作为工具特定的记忆系统；通过计算用户指令与代码本中工具特征的相似度，动态选择相关工具；并构建视频中心的指令调优数据集VideoToolBench用于训练和评估。

Result: 在既有视频LLM基准和新构建的VideoToolBench上，COLT展示了最先进的性能，证明其在连续工具流场景下能有效学习与调用工具。

Conclusion: 该论文提出了COLT，一个用于视频大模型的连续工具使用机制，目标是在工具流不断更新的真实环境中，避免灾难性遗忘并保持对新旧工具的有效调用。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS用扩散模型蒸馏和自适应渐进增强，无训练即可为稀疏视角的3D Gaussian Splatting重建提供跨视图一致的去伪影与修补，显著提升视觉与重建质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS在稀疏视角下重建会因信息不足产生明显伪影和缺失，现有利用生成先验的方法难以保证多视图一致性，导致结构模糊和细节不合理，故需一种能兼顾去伪影、修补和多视图一致性的方案。

Method: 通过蒸馏（distillation）方式从现有扩散模型提取更准确且跨视图一致的先验，用于去伪影与图像修补；并引入自适应的渐进增强策略，在欠约束区域逐步细化重建。方法为训练免费。

Result: 在大量实验中，FixingGS在视觉质量和重建性能上优于现有最先进方法。代码将公开。

Conclusion: 提出了FixingGS，一种无需训练的基于扩散模型的蒟蒻点云（3D Gaussian Splatting）稀疏视角重建增强方法，能消除伪影并完成缺失区域，同时保持多视图一致性。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM通过基于高斯分位数的显著性分组与混合量化，在≤2位量化下实现明显性能与效率提升，并结合图像token剪枝进一步减少计算。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型计算和存储成本高，难以在受限硬件上部署；需要在极低位宽（≤2位）下保留性能以实现高效推理。

Method: 按权重分布的高斯分位数把权重分为若干子集（显著的outlier和多个非显著的inlier），对不同子集施加不同的量化约束（缩放因子和二值矩阵）并结合显著性度量，提出显著性感知混合量化算法；同时在量化后进行图像token剪枝以进一步提高效率。

Result: 在语言模型部分的视觉问答任务上，比最新方法领先3%-47%（在4个基准、3个模型上）；在整体VLM上提升4%-45%；并发现量化模型中图像token存在90%-99%的冗余，可通过剪枝进一步提升效率。

Conclusion: 提出的Bi-VLM通过基于高斯分位数的非均匀权重划分和显著性感知的混合量化，在超低比特（≤2位）下保持并提升VLM性能，填补了高效部署中的关键空白。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: 提出DiSSECT：在SSL中加入多尺度向量量化形成离散瓶颈，抑制捷径学习并学习结构化特征，从而提升医学影像的迁移性能与标签效率，在多任务多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像SSL方法依赖复杂架构、解剖先验或高度调参的增强，且容易产生shortcut learning；特别是在胸片等解剖相似性高、病变细微的模态下更为严重。

Method: 在SSL流水线中集成多尺度向量量化模块，形成离散表示瓶颈，约束模型学习可重复且结构相关的特征；可能结合对比学习或重建等常见SSL目标，但核心是通过量化减少视图特异性和噪声信息。

Result: DiSSECT在多个公开医学影像数据集上，在分类与分割任务上均取得强性能，特别是在低标签场景下表现出高标签效率，且在迁移性和鲁棒性上优于现有SOTA方法。

Conclusion: DiSSECT通过在自监督学习中引入多尺度向量量化的离散瓶颈，抑制视角/低价值模式并强化结构感知、可重复的特征，从而提升了在有限标签下的迁移性能和标签效率，且在多个医学影像数据集上对分类/分割任务均表现优越。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 利用热成像+深度学习+CV2X的车载实时鹿检测与预警系统在自建数据集和实地测试中表现优异，但需关注数据集偏倚、跨域泛化、误报/漏报成本和部署可行性。


<details>
  <summary>Details</summary>
Motivation: 美国鹿车碰撞频发，造成人员伤亡和巨额经济损失，且传统可见光摄像头在低光或恶劣天气下效果差，热成像与车联网可提供更稳定的早期警示。

Method: 结合热成像相机、深度学习目标检测模型和CV2X通信，实现车载实时检测、报警和消息广播；训练数据为在北卡罗来纳Mars Hill采集的1.2万张热像鹿图像；系统端到端延迟<100ms。

Result: 在自建数据集上平均精度(mAP)98.84%，精确率95.44%，召回95.96%；实地测试在不同天气下热像检测准确率88–92%，优于可见光摄像头(<60%)；实现了高概率阈值下的CV2X消息广播与<100ms端到端延迟。

Conclusion: 该系统在减少鹿车碰撞方面具有可行性和实际价值，但需注意数据集局限、泛化和部署问题。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: 提出任务感知的扩散模型微调方法，通过偏好对齐生成样本，提高外科视觉任务在数据稀缺情形下的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成逼真图像，但会出现记忆化、样本多样性不足或与下游任务不一致，导致合成数据无法改善或甚至损害下游任务性能。

Method: 构建“偏好/非偏好”合成图像对，并对扩散模型进行轻量微调，使生成过程显式对齐到下游模型的目标；并采用迭代精炼合成样本以进一步提升性能。

Result: 在三个外科数据集上，SAADi在分类任务上提升7–9%、分割任务上提升2–10%，对弱势类别有明显改进；迭代精炼额外提升4–10%。

Conclusion: 该论文提出了一种名为SAADi的框架，通过使扩散模型生成更符合下游任务偏好的合成样本，缓解了外科图像标注数据稀缺问题。

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出了一种将KMDS模型与可学习参数结合的端到端神经网络（neural KMDS-Net），用于动态PET短帧去噪，实验显示其优于基线方法并提高时空分辨率。


<details>
  <summary>Details</summary>
Motivation: 短帧动态PET统计量有限，导致图像质量差；深度学习在医学图像去噪中表现良好，期望结合模型约束与神经网络自适应能力提升去噪效果。

Method: 基于内核空间的多维稀疏（KMDS）模型，利用动态PET的帧间空间相关性和帧内结构一致性；将参数估计的固有形式替换为可学习的神经网络，实现自适应参数优化，构建端到端的神经KMDS-Net。

Result: 在模拟和真实数据上的大量实验表明neural KMDS-Net在峰值信噪比、结构相似度等指标以及视觉上均优于先前方法，能在保持结构细节的同时显著抑制噪声。

Conclusion: 本文提出的neural KMDS-Net能有效提升动态PET短帧图像的质量，实验（模拟与真实数据）显示其在去噪性能上优于基线方法，能实现更高时空分辨率。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 用光流把关键帧分割标签传播到相邻帧，结合多任务学习来解决标注稀疏与时空不平衡，从而提升机器人辅助手术的视频场景理解的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 实际外科场景存在复杂时序动态和多样器械交互，且像素级分割注释昂贵且稀疏，导致单任务方法难以充分利用数据。需要一种方法在时空上平衡标签稀疏性并提升多任务学习效果。

Method: 基于光流估计从带注释的关键帧向相邻无标注帧传播分割标签，扩大像素级监督；与多任务学习框架融合，同时训练长时序任务（阶段/步骤识别）和短时序任务（器械分割、动作检测），以利用丰富的长时序注释和平衡时空信息。

Result: 通过光流插值增强空间监督后，模型在器械分割与动作检测等短时任务上获得更高的准确率，同时多任务训练利用长时注释提高整体场景理解性能和训练效率。

Conclusion: 该论文提出将光流插值与多任务学习相结合，通过将关键帧的语义分割标签向相邻帧传播来缓解标注稀疏、时空不平衡问题，从而提升机器人辅助手术场景理解的精度与效率。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel提出对自回归与扩散分别用speculative decoding和多阶段蒸馏，得到6-NFE无损及1-NFE近实时模型，显著加速多模态理解与生成（理解>2x，生成最高达22x），并保持质量。


<details>
  <summary>Details</summary>
Motivation: 随着多模态输入与生成的令牌数量增长，扩散去噪和自回归解码的迭代计算负担显著，需统一框架同时加速理解与生成。

Method: 采用分而治之：对自回归下一个token预测使用speculative decoding；对扩散模型噪声去除采用多阶段（包括对抗）蒸馏，训练出6-NFE与1-NFE加速模型，并结合人类反馈微调。

Result: 报告理解任务>2x加速；生成方面6-NFE无损模型文本到图像16.67x、图像编辑22x加速；1-NFE实现近实时交互；保持原模型输出质量，且通过对抗蒸馏+人类反馈提升成本效益与响应性。

Conclusion: Hyper-Bagel通过推测性解码和多阶段蒸馏同时加速理解与生成任务，宣称在理解任务上超2倍加速；在生成任务有无损6-NFE模型分别实现文本到图像16.67x、图像编辑22x加速，并有1-NFE近实时模型辅以对抗蒸馏与人类反馈学习提高成本效益与响应性。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 通用多模态LLM（如GPT-4o、Gemini-2.5）在复杂的圣像分类任务中表现良好，优于微调ResNet50；提示工程（尤其加入类描述）有助于零-shot，少样本效果有限，未来应研究提示优化和扩展分类策略。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态LLM/VLM在文化遗产领域（基督教圣像识别）中替代或补充传统监督分类器的能力，探索是否能用于元数据策划。

Method: 采用基于Iconclass的三个数据集（ArtDL、ICONCLASS、Wikidata），筛选出现频率前十的类别；比较CLIP、SigLIP、GPT-4o、Gemini-2.5 Pro在三种条件下（仅标签、标签+Iconclass描述、五示例少样本）与在相同数据上微调的ResNet50的表现。

Result: Gemini-2.5 Pro和GPT-4o总体上超过ResNet50；Wikidata数据集准确率显著下降，SigLIP在该集上表现最好；加入类描述能提升零-shot表现，少样本学习总体降低效果，只有少量情况下略有改进。

Conclusion: 本研究表明通用多模态LLM在基督教圣像单标签分类任务上具有可行性，并在多数情况下优于基准的ResNet50；但性能受数据集特性（如图像尺寸、元数据一致性）影响较大。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 提出LRGC：用key-query注意力计算节点相似度，结合可学习的soft-threshold重参数化进行边选择，实现每层无超参的可学习图构建，在ImageNet-1k上优于同规模ViG模型。


<details>
  <summary>Details</summary>
Motivation: 传统ViG使用非参数化的统计方法（如k-NN、超图或相似度阈值）构建图，这些方法需要手工设定或无法学习，可能无法为每个节点选择最佳邻居，限制表示能力。作者希望提出一个可学习且无超参的图构建方案，提高图的适应性和表达能力。

Method: 在每对节点间计算key-query注意力得相似度分数，然后通过可学习的soft-threshold重参数化来生成边权，阈值在训练中被学习而非作为超参数；将该图构建模块集成到Vision GNN并在ImageNet-1k上训练评估。

Result: 在ImageNet-1k上，与相似规模的现有ViG模型相比，ViG-LRGC在分类性能上有所提升，证明可学习的图构建能带来性能增益。

Conclusion: 提出了一种可学习、无超参的图构建方法（LRGC），通过key-query注意力和soft-threshold重参数化实现边选择，能够在每层自适应学习邻居关系，从而减少传统统计方法的偏差并提升ViG性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 通过将反思结构化为可训练的诊断+修正调用并用专门奖励与评估基准训练，显著提高了多轮工具调用的成功与错误恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的LLM训练常用监督模仿或粗粒度的RL，且自我反思依赖启发式prompt或单向推理，导致多轮交互中重复错误、脆弱性高。需要将错误诊断和修复作为可训练的显式动作。

Method: 方法包括：1）定义结构化反思格式：短而精确的诊断基于上一步证据并生成可执行的后续调用；2）训练目标结合DAPO和GSPO并设计针对工具使用的奖励，优化按步策略：Reflect->Call->Final；3）构建Tool-Reflection-Bench用于程序化评估结构化有效性、可执行性、参数正确性和结果一致性。

Result: 在BFCL v3和所提Tool-Reflection-Bench上的实验显示：多轮工具调用成功率和错误恢复能力显著提升，冗余调用减少，证明显式反思并直接优化能提高工具交互可靠性并为代理从失败中学习提供可复现路径。

Conclusion: 该论文提出了结构化反思（structured reflection），使模型能在工具调用错误后以可控、可训练的方式生成诊断与可执行的修正调用，从而提高多轮工具交互的鲁棒性。

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出基于对抗强化学习的Point Prompt Defender，通过在图结构的点提示环境中用DQN训练攻击者与防御者，推理仅用防御者对粗提示集进行优化，从而提高SAM的分割鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有提示多依赖启发式或人工设计，缺乏可扩展性与泛化能力；因此希望通过自动化、对抗式的优化方法提升提示质量与SAM的稳健性。

Method: 构建由图表示的任务无关点提示环境（图节点为图像补丁，边同时编码物理与语义距离），使用DQN训练对抗设置下的攻击者与防御者——攻击者激活会破坏分割的点提示，防御者学会抑制这些点提示以恢复性能，推理时只部署防御者对任意粗提示集进行细化。

Result: 大量实验证明Point Prompt Defender在提升SAM鲁棒性和泛化能力方面有效，能无需重新训练模型下改善不同任务的分割效果。

Conclusion: 该文提出了利用对抗强化学习自动优化点提示以提升SAM鲁棒性和泛化性的框架，结论是Point Prompt Defender在多任务上能有效提升SAM的分割性能并具有可插拔性和可解释性。

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds首次提供航拍、相机陷阱与生物声学三模态同步野外监测数据，证明模态互补性并发布可复现协议与开放数据，助力保护领域的多模态AI研究。


<details>
  <summary>Details</summary>
Motivation: 为保护生物多样性与濒危物种研究提供多模态数据支持，弥补单模态监测在物种检测、行为解析与栖息地评估方面的局限，促进可复现的守护生态学与计算机视觉研究。

Method: 在俄亥俄The Wilds保护区进行为期四天的试点部署，同步采集三种传感器模态的数据，覆盖220英亩牧场；对传感器模态的检测性能、土地利用模式、行为分析与栖息地监测能力进行了比较分析。

Result: 发布了开放数据集，展示各模态在物种检测与生态监测中的互补优势；设定了数据采集与同步协议；并计划后续版本加入带GPS的个体追踪、志愿者科学数据与更长时间覆盖。

Conclusion: 该论文发布了首个SmartWilds多模态野生动物监测数据集，提供航拍影像、相机陷阱照片/视频与生物声学记录的同步数据，并建立了可复现的多模态监测协议。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 作者发布了包含54,951对像素级对齐遥感影像与深度图的RS3DBench数据集及文本描述，并提出基于稳定扩散的深度估计模型，推动遥感领域3D视觉模型研究。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏精确对齐的深度信息，制约大规模通用3D视觉模型在遥感领域的发展，需构建高质量、多场景、带文本描述的对齐数据集并提供强基线模型。

Method: 构建包含54,951对遥感影像与像素级对齐深度图的数据集，并配套文本描述；提出基于Stable Diffusion的多模态融合深度估计模型用于训练与评测。

Result: 提供了大规模对齐数据集RS3DBench和相应模型，模型在该数据集上达到了先进性能；数据集、模型与代码将公开（网站提供）。

Conclusion: 提出了RS3DBench数据集与基于稳定扩散的遥感深度估计模型，旨在推动遥感图像的3D视觉理解与通用大模型发展。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出DeblurSplat：首个无SfM、融合集成事件相机的去模糊3D高斯点方法，通过DUSt3R直接从模糊图像构建点云并用事件流解码清晰图像进行监督，提升重建质量与渲染效率。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统基于SfM的去模糊3D重建中相机位姿估计误差导致的点云位置不准确问题，并利用事件相机对动态变化的高敏感性提高去模糊和重建质量与效率。

Method: 方法包括两部分：1) 利用预训练的稠密立体模块DUSt3R直接从模糊图像恢复精确初始点云，跳过相机位姿估计，避免位姿误差传播；2) 将事件相机数据引入去模糊管线，通过从事件流与模糊图像解码潜在的清晰图像，为场景重建优化提供细粒度监督，并用于3D高斯点渲染优化。

Result: 在多种场景上实验表明，DeblurSplat在生成高保真新视角图像方面优于现有去模糊3D高斯点方法，并在渲染效率方面取得显著提升。

Conclusion: 该论文提出了首个基于事件相机、无需Structure-from-Motion的去模糊3D高斯点渲染方法DeblurSplat，能生成高质量的无模糊初始点云并利用事件流辅助重建，从而提高新视角渲染质量与效率。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: 提出MoiréNet：用方向性频空编码器和频空自适应选择器的U-Net变体，高效抑制各向异性多尺度摩尔纹，效果领先且参数更少。


<details>
  <summary>Details</summary>
Motivation: 摩尔纹由显示器像素格与相机传感器网格的频谱混叠引起，表现为各向异性、多尺度的强结构性伪影，现有方法在去除这些复杂伪影和模型轻量化之间存在权衡，因而需要一种既有效又参数高效的解决方案。

Method: 基于U-Net的卷积框架，提出DFSE模块利用方向差分卷积捕获摩尔纹方向性并提取频域信息；提出FSAS模块在频域和空域特征间进行自适应选择与抑制；整体架构在编码器-解码器路径中融合多尺度频空特征来还原清晰图像。

Result: 在公开与实际使用的数据集上，MoiréNet在恢复质量上达到或优于现有最先进方法，同时参数量仅为5.513M，比ESDNet-L减少约48%，在性能与效率间取得平衡。

Conclusion: MoiréNet在去摩尔纹任务上通过同时融合频域与空域特征、引入方向性频空编码器和频空自适应选择器，实现了更优的恢复效果与显著的参数效率提升，适合资源受限场景应用。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出基于频域分解与MoE路由的FAVS框架，有效解决音频与视觉高频信息矛盾，提升AVS任务表现并实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 观察到音频高频含有大量噪声而视觉高频含有结构性细节，两者在频域存在内在矛盾，直接融合会导致性能下降，因此重新将AVS表述为频域分解-重构问题。

Method: 提出两个模块：FDED模块通过残差迭代的频域分解区分模态特有语义与结构特征；SCMC模块采用专家混合（MoE）动态路由，增强跨模态语义一致性并保留模态特异性。

Result: 在三个基准数据集上达到SOTA性能，并通过大量可视化验证所提模块的有效性。

Conclusion: 本文提出FAVS框架，通过频域分解与重构解决音频与视觉模态在高频信息上的矛盾，提高了AVS性能。

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 综述了显著性图、概念瓶颈、原型及混合四类视觉可解释方法，比较机制与评价，指出各自局限并建议结合方法与改进评估以促进实际应用。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在视觉任务上表现优秀但缺乏可解释性，影响关键场景下的可靠性与可接受性，因而需要系统性梳理现有xAI方法以为研究与应用提供指导。

Method: 通过文献综述，分别介绍并比较了显著性图、概念瓶颈模型、基于原型的方法及混合方法的技术细节、实现方式和适用场景，同时总结了现有评估指标并讨论其不足。

Result: 归纳出各类方法的优缺点：显著性图易实现但局限于局部解释；CBM具备语义性但依赖概念标注且对性能有折衷；原型方法直观但扩展性和泛化性不足；混合方法尝试综合优点但复杂度高。提出未来方向包括统一评估框架、概念自动发现、面向实际场景的鲁棒性研究和人机交互评估。

Conclusion: 该论文对视觉感知任务中四类可解释性方法进行了综述，指出每类方法在机制、优劣和评估指标上的差异，认为没有单一方法能全面解决可解释性问题，提倡结合多种方法并改进评估标准以推进实际应用。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 改进的DDPM结合新的噪声调度与时间嵌入，在点云投影上生成高质量合成LiDAR数据，显著提升自动驾驶感知任务的性能，并优于多数SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界LiDAR采集成本高且受噪声、稀疏性影响，数据不足与质量问题限制了自动驾驶系统的泛化与鲁棒性。通过合成高质量且多样的点云用于数据增强，可改善检测与感知性能。

Method: 基于DDPM框架，作者设计了特定的噪声调度策略（提高在不同噪声水平下的恢复能力）以及改进的时间步嵌入（增强模型对不同去噪阶段的辨识），并在点云投影空间上进行建模以生成3D数据。实验在IAMCV和KITTI-360数据集上比较了四项指标，并与多种SOTA方法对比，展示其在生成质量与下游任务提升方面的优势。

Result: 在IAMCV与KITTI-360上的实验证明，该方法在四项性能指标上优于大多数现有基线，能生成具有丰富空间关系和结构细节的多样化点云，并在减弱噪声与稀疏数据影响方面表现有效。

Conclusion: 该论文提出将改进的去噪扩散概率模型（DDPM）用于生成高质量合成LiDAR点云以增强数据，从而提升自动驾驶感知任务的性能。通过引入新的噪声调度和时间步嵌入策略，模型在去噪过程和时间感知上得到增强，能生成更真实、稠密且结构丰富的点云，缓解真实数据中噪声与稀疏性问题。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: 提出AGSSP：基于异常先验的两阶段自监督预训练（异常图蒸馏+伪框检测预训练），能有效捕获缺陷特征，显著提升金属表面缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: ImageNet预训练存在领域差距，而现有自监督目标无法区分细微缺陷与复杂背景，导致在金属表面缺陷检测中自监督预训练效果不佳。需要一种显式利用异常先验引导表征学习的方法。

Method: 两阶段框架：1) 基于异常图的蒸馏对backbone进行预训练，使网络学习缺陷显著特征；2) 使用从异常图生成的伪缺陷框对检测器进行预训练，增强定位能力；并提出知识增强的异常图生成方法，同时构建大规模工业数据集。

Result: 在多种设置下均显著优于ImageNet预训练模型，mAP@0.5提升最多约10%，mAP@0.5:0.95提升11.4%。同时公开了代码、预训练模型和数据集。

Conclusion: AGSSP通过引入异常先验并采用两阶段自监督预训练，有效缩小了领域差距并提升了金属表面缺陷检测的表现。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出UHAP与音频到表达空间映射，首次实现可泛化且同时建模几何与外观的音频驱动写实人头头像，效果在唇型同步和视觉真实感上领先。


<details>
  <summary>Details</summary>
Motivation: 现有工作多只关注将音频映射为几何形变，忽略了音频相关的外观（如口腔内部、光照与皮肤细节）变化；缺乏可泛化到新主体并同时建模外观与几何的音频到头像的端到端方法。

Method: 结合人无关的语音模型与新提出的通用人头先验（UHAP），UHAP在交叉身份多视角视频上训练，并以中性扫描数据监督以捕获身份特异细节；语音模型直接将原始音频映射到UHAP的表达潜空间，该空间同时编码几何和外观变化；对新主体使用单目编码器进行轻量个性化，回归每帧动态表达，使微调阶段仅需学习全局外观与几何；通过UHAP解码音频驱动的表达码生成最终影像。

Result: 方法在唇动同步、图像质量及感知真实度等指标上均优于以几何为主的竞争方法，能生成带有眉毛、目光、真实口腔内部等细腻表达的高真实感头像。

Conclusion: 本论文提出了首个音频驱动的通用写实人头化身合成方法，能够在跨身份、多视角数据上生成高保真、口型同步和细致表情的虚拟头像。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 提出了SynapFlow，一个结合transformer检测、深度与时间跟踪及特征提取的端到端自动化管线，用于双光子显微镜3D+time数据中树突棘的可扩展检测与追踪，并开源数据和代码。


<details>
  <summary>Details</summary>
Motivation: 树突棘大小可作为突触效能的代理，长期、三维级别追踪棘的动态对于理解学习与记忆的神经基础至关重要，但现有大规模3D+time数据的手工标注非常耗时且挑战巨大，因而需要自动化、可扩展的分析管线。

Method: 方法包括：1) 基于transformer的检测模块，对三维体积中的树突棘进行定位；2) 深度（z向）跟踪组件，整合空间特征以解决不同层面棘的识别；3) 时间跟踪模块，通过空间一致性关联不同时间点的3D棘；4) 特征提取单元，量化生物学相关的棘属性。使用公开标注数据及作者发布的两个互补数据集（检测/深度跟踪与时间跟踪）进行验证，并开源代码与预训练权重。

Result: 在公开与作者新发布的标注数据集上验证，显示检测与跟踪性能良好，数据、代码和预训练模型已开源，提供了可复现的基线供后续研究使用。文中还首次发布了用于时间跟踪的标注数据集。

Conclusion: 本文提出了一个模块化的机器学习流水线（SynapFlow），用于自动检测、时间跟踪并提取用双光子显微镜在长期记录的三维+时间体积数据中树突棘的特征。该方法在检测与跟踪任务上表现稳健，可作为大规模自动分析树突棘结构动力学的基线工具。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出通过VLM筛选高置信样本并用预训练视觉模型增强特征，在测试数据上自我训练轻量分类器的零样本图像分类方法，无需微调VLM或用大模型，在十个数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 动机是解决深度学习模型在缺乏标注数据场景下的应用问题，利用VLM提供语义信息与预训练视觉模型的视觉表征能力，通过自学习在目标数据上动态适配分类器以提升零样本分类性能。

Method: 方法包括使用VLM对输入样本进行置信度评估以选取高置信度样本，利用预训练视觉模型提取并增强这些样本的视觉特征，随后在这些增强特征上迭代训练一个轻量级分类器（无监督伪标签自学习循环），无需对VLM进行微调或使用大型语言模型。

Result: 在十个多样化数据集上的实验证明，所提方法在零样本条件下优于基线零-shot方法，证明了结合VLM与预训练视觉模型并通过伪标签自学习能有效捕捉互补的语义与视觉信息。

Conclusion: 本论文提出了一种结合视觉-语言模型（VLM）与预训练视觉模型的无监督自学习循环框架，通过置信度伪标签策略在测试集上训练轻量分类器，实现零样本图像分类，并在十个数据集上优于基线方法。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 作者发布MirrorScene3D并提出ReflectiveGS：将镜面反射作为额外视角信息融入3D Gaussian Splatting，显著提升含镜面场景的重建质量与速度。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法在含镜面场景中性能显著下降，且现有方法多把镜面视为对称映射或噪声，忽略了反射中包含的有价值互补视角信息。作者希望利用这些反射信息填补场景缺失细节并提升重建质量。

Method: 构建包含1256张高质量图像并标注镜面掩码的MirrorScene3D数据集；在3D Gaussian Splatting基础上扩展为ReflectiveGS，通过识别镜面和利用反射建立额外的视角约束，将反射视为补偿信息而非对称映射；在训练中融合反射视图以改善几何重建与细节恢复。

Result: 在MirrorScene3D数据集上，ReflectiveGS在SSIM、PSNR、LPIPS等指标以及训练速度上均优于现有方法，表明其在镜面丰富场景中能更好地恢复几何和细节。

Conclusion: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，表明利用镜面反射作为补充视角能显著提升含镜面场景的三维重建与新视图合成性能，优于现有NeRF和3DGS方法。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 基于标注白光术中图像、YOLO检测及GAN数据合成，本文旨在提高手术中胆道可视化，降低胆管损伤风险，并讨论了实验与伦理问题。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术尽管恢复快、创伤小，但胆管损伤风险更高，需提高手术中胆道可视化以减少并发症和改善生存及生活质量。

Method: 构建并标注了术中白光图像数据库，使用YOLO检测算法进行胆道定位，并在传统数据增强基础上引入GAN生成合成训练样本以扩充数据集。

Result: 实验展示了YOLO在含GAN合成数据训练下的定位性能提升（文中讨论了具体实验结果，但摘要未给出数值），并对方法的伦理问题进行了讨论。

Conclusion: 本文提出基于深度学习的术中白光图像胆道定位方法，以降低腹腔镜胆囊切除术中胆管损伤风险。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS：一个可用任意点提示的多任务域适应分割框架，通过中心点检测和提示引导对比学习，能实现无监督/弱监督域适配并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有SAM需要对每个实例提供提示，且在大规模EM器官分割任务上的样本效率有限。希望构建一个能利用少量标注并适应域差异的灵活框架，从而实现注释高效的器官亚型分割。

Method: 提出Prompt-DAS：1) 在训练和测试阶段均支持任意数量的点提示；2) 增加辅助中心点检测任务以支持从全点、稀疏点乃至无点的多种监督形式；3) 引入提示引导的对比学习以增强判别性特征学习。

Result: 在多项具有挑战性的基准数据集上，Prompt-DAS在UDA、WDA及基于SAM的方法上均取得了优于现有方法的性能，验证了中心点辅助和提示对比学习的有效性。

Conclusion: Prompt-DAS是一种基于提示的多任务域自适应分割框架，能在不同提示配置下实现无监督、弱监督域适配和交互式分割，并通过中心点检测和提示引导的对比学习提高性能。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 提出VIR-Bench——200段长距离旅行视频基准，用行程重构评测MLLM的地理-时间推理；实验显示现有模型普遍弱，并通过一个原型代理证明基准可提升实际旅行规划效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准偏向室内或短程活动，未覆盖长距离旅行的时空推理需求，而这类能力对导航、具身AI规划等实际应用至关重要。

Method: 构建并发布包含200段旅行视频的新基准，将行程重构定义为核心评价任务；对比评估多种现有SOTA MLLMs，并开发一个基于基准的旅行规划代理进行案例研究。

Result: 多数SOTA及专有MLLM在VIR-Bench上表现不佳；基于基准设计的旅行规划代理在行程推荐上显著提升，表明基准能推动实用性能改进。

Conclusion: VIR-Bench揭示了现有多模态大模型在长距离地理-时间理解上的显著不足，说明该领域仍需发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 提出一个透明且有效的step-level链式推理框架：包含step-level数据、过程奖励模型（PRM）和基于PRM的强化学习训练，从而改善视觉-语言推理性能并便于评估和推理扩展。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言链式推理多为粗粒度，难以进行细粒度结构化推理且无法有效评价中间步骤的质量，限制了强化学习和推理规模扩展的效果。

Method: 构建step-level推理数据集，设计过程奖励模型（PRM）对中间推理步骤打分，并结合强化学习训练（基于PRM的奖励信号）及推理时的尺度扩展技术进行优化。

Result: 在若干挑战性视觉-语言基准上取得一致改进，设置了强基线；实验和消融显示各组件的贡献，并揭示推理时尺度扩展的一些有趣性质。

Conclusion: 该论文提出了细粒度的“step-level”链式推理方法用于视觉-语言模型，通过对每一步推理进行评估并用过程奖励模型（PRM）在强化学习中优化，能够提升模型在复杂视觉-语言基准上的表现，并提供用于推理时缩放的见解。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 利用Swin Transformer生成的CAM提示SAM进行弱监督食物图像分割，单/多掩码与预处理可提升结果；在FoodSeg103上多掩码mIoU=0.54，平均2.4个掩码/图。


<details>
  <summary>Details</summary>
Motivation: 减少像素级标注成本，利用SAM的零样本和prompt能力与ViT的注意力机制结合，自动或半自动生成食物分割掩码以加速标注或嵌入营养追踪应用。

Method: 使用Swin Transformer在只有图像级注释下训练以生成CAM；将CAM转换为SAM的点/框等提示，调用SAM生成单掩码或多掩码；对输入图像做预处理（未详述具体方法）以改善掩码；在FoodSeg103数据集上评估，统计平均每图生成掩码数和mIoU。

Result: 在FoodSeg103上多掩码方案平均每图生成2.4个掩码（不含背景），mIoU为0.54；说明方法可产出合理质量掩码，适合辅助标注或下游应用。

Conclusion: 该论文提出了将ViT（Swin Transformer）生成的CAM用于为SAM生成提示，从而实现食物图像的弱监督语义分割；训练仅依赖图像级标签，避免像素级标注，结合图像预处理和单/多掩码策略提升SAM掩码质量。

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet通过EDG和CPDA结合Swin-Transformer分支，实现了在回声心动图上兼顾精度与时间稳定性的分割方法。


<details>
  <summary>Details</summary>
Motivation: 回声心动图易受形变和散斑噪声影响，导致帧间分割抖动，进而削弱功能估计和临床可解释性，因此需要在保持高单帧精度的同时提高时间稳定性。

Method: 方法包含构建Echo-Dynamics Graph(EDG)以编码视频动态信息；采用多个基于Swin-Transformer的编码器-解码器分支处理单帧；在跳跃连接上使用CPDA，将EDG特征与心脏相位信息融合以约束时间一致性。

Result: 在CAMUS与EchoNet-Dynamic数据集上，DyL-UNet在保持分割精度与现有方法相当的同时，展现出更优的时间一致性，提升了自动化临床回声分析的可靠性。

Conclusion: 本文提出DyL-UNet，通过动态图学习构建Echo-Dynamics Graph并在U-Net跳跃连接处引入Cardiac Phase-Dynamics Attention，实现了在回声心动图序列上既保持单帧分割精度又显著提升时间一致性的目标。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: 提出ColorBlindnessEval基准（500张Ishihara风格图），评估9个VLM的数字识别鲁棒性，发现普遍错误与高幻觉率，建议改进模型鲁棒性与训练数据多样性。


<details>
  <summary>Details</summary>
Motivation: 受Ishihara色盲测试启发，考察VLM在颜色对抗/复杂视觉图案中识别关键信息（如数字）的能力，以评估其在现实应用中的可靠性与安全性。

Method: 构建500张Ishihara样式图像（数字0-99、不同配色），设计Yes/No与开放式提示，对9个VLM进行评估，并与人类参与者表现对比，统计准确率和错误类型，分析幻觉与误识别情况。

Result: 实验显示多数VLM在该数据集上性能显著下降，开放式提示下幻觉率更高；部分模型在Yes/No提示下表现略优但仍不足以匹配人类。结果强调当前VLM对颜色/图案对抗的脆弱性。

Conclusion: 本文提出的ColorBlindnessEval基准能揭示视觉-语言模型在受色彩对抗影响场景中的脆弱性，显示模型在Ishihara样式图像中识别数字时普遍出错并存在幻觉问题，表明需提升模型在复杂视觉环境下的鲁棒性。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: 将扩散修复移至小波低频子带并辅以轻量高频修复与在线随机掩码，显著加速稀疏视角3D Gaussian重建，同时保持渲染性能。


<details>
  <summary>Details</summary>
Motivation: 在稀疏视角条件下，3D Gaussian Splatting性能下降显著；现有用扩散模型修复渲染再优化的方法有效但计算昂贵，需更高效的修复与训练策略。

Method: 将扩散模型从像素域转移到小波域：仅对低分辨率LL子带进行扩散微调与修复，高频子带由轻量级网络提升；引入在线随机掩码策略生成训练对，替代leave-one-out策略，从而减少计算开销；将修复后的图像作为伪真值用于后续3D Gaussian Splatting优化。

Result: 在Mip-NeRF 360和OmniObject3D两个基准数据集上，WaveletGaussian在渲染质量上与现有方法具有可比性，但训练时间显著减少。

Conclusion: WaveletGaussian通过在小波域仅对低频LL子带应用扩散修复，并用轻量网络修复高频子带，结合在线随机掩码训练对比传统方法显著降低训练时间，同时在稀疏视角下仍能保持有竞争力的渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: 通过修复训练/推理不一致的实现细节，Sa2VA-i显著提升了Sa2VA在指代视频目标分割任务上的表现，达成多项新SOTA并开源代码。


<details>
  <summary>Details</summary>
Motivation: 观察到广泛使用且已获SOTA的Sa2VA在指代视频目标分割任务上并未发挥其全部潜力，怀疑是实现层面的训练/推理不一致导致性能受限，因而提出改进以释放模型潜能。

Method: 分析并修复了Sa2VA在训练与推理阶段的不一致实现细节，提出一致性的改进（具体实现细节未在摘要中列出），在不改动原始训练检查点的情况下调整推理/训练流程生成Sa2VA-i版本。

Result: Sa2VA-i在多个视频分割基准上取得显著提升：MeViS +11.6 J&F，Ref-YT-VOS +1.4，Ref-DAVIS +3.3，ReVOS +4.1。使用相同检查点时，Sa2VA-i-1B在MeViS上与原Sa2VA-26B相当。代码与模型已开源。

Conclusion: Sa2VA-i通过修正训练与推理不一致的问题，显著提升了原Sa2VA在指代视频目标分割任务上的表现，成为多项基准的新SOTA，并在某些情况下用小模型达到原大模型的性能。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出一种无需训练的零样本策略，通过输入适配与指令注入，使仅用RGB训练的通用多模态模型（如Gemini2.5）能够利用多光谱遥感数据，显著提升土地覆盖/利用分类的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 多光谱遥感数据包含与地面物质高度相关的光谱信息，但主流强大多模态模型只在RGB图像上训练，无法直接利用多光谱信号。训练专用多光谱模型成本高且不利于通用模型生态，因此需要一种能让通用多模态模型零样本理解多光谱输入的可行方案。

Method: 方法基于训练-free的输入适配和指令注入：将多光谱数据转换或映射到通用视觉空间（使其与RGB输入兼容），并通过将域特定信息以自然语言指令的形式喂入（instruction injection）来引导模型理解多光谱通道的含义。作者在Gemini2.5上实现并评估该流程，未进行模型权重更新。

Result: 在多个遥感公开基准（土地覆盖/土地利用分类任务）上，方法在零样本设置下显著超过了直接将多光谱作为RGB输入的基线，展示了易适配性与实用价值。

Conclusion: 该论文提出了一种无需训练的零样本方法，将多光谱遥感数据作为输入，注入到仅用RGB训练的通用多模态模型（以Gemini2.5为例），从而实现对多光谱信号的理解和利用。实验在土地覆盖与土地利用分类基准上显示了显著的零样本性能提升，证明该方法能让地理空间专业人士在不额外训练模型的情况下利用强大的多模态模型。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个集检测、分割与多模态链式推理于一体的开源医学基础模型，通过新型训练方法和数据集，实现像素级定位与医生式诊断推理，在多项基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型通常专注于单一任务或需要多个专门化网络，泛化能力有限；临床应用需要精确的视觉定位、多模态融合和连贯的推理能力，因此需要一个统一的多模态医学模型。

Method: 作者结合检测、分割与多模态链式思维推理，提出了一种新的多模态训练方法，并发布了涵盖推理、检测、分割与文档理解任务的开源数据集。模型在单一框架内实现像素级定位、结构化报告和诊断推理。

Result: 实验显示Citrus-V在多个评测基准上优于现有开源医学模型及部分专家级影像系统，提供从视觉定位到临床推理的统一流程，并支持精确病灶量化、自动化报告与可靠的第二意见。

Conclusion: Citrus-V提出了一个统一的多模态医学基础模型，旨在实现从像素级病灶定位到结构化报告生成与链式推理的一体化解决方案，声称在多个基准上优于现有开源模型和专家系统。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 研究在CARLA模拟数据上评估MLLM零样本交通事故检测，发现Pixtral召回与F1最好，Gemini精度经提示提升但牺牲召回，Gemma 3最稳健；结合YOLO/Deep SORT/SAM的视觉分析能提升模型可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 现实中基础设施摄像头事故数据稀缺且标注昂贵，研究零样本多模态模型能否在无需大量标注的情况下实现可靠的自动化事故检测与描述，对提升道路安全与应急响应具有重要意义。

Method: 采用CARLA生成的DeepAccident模拟数据评估MLLM零样本能力；比较Gemini 1.5/2.0、Gemma 3与Pixtral四种模型的检测与描述表现；将YOLO（目标检测）、Deep SORT（多目标跟踪）、SAM（实例分割）等视觉分析结果作为增强提示输入MLLM，提高识别准确性和可解释性。

Result: Pixtral在实验中表现最佳（F1=0.71，召回=83%）；通过增强提示Gemini模型精度提高（如Gemini 1.5精度升至90%），但其F1与召回显著下降；Gemma 3在各项指标上最为均衡且稳健，增强提示对其影响最小。

Conclusion: 本文展示了在基础无微调条件下，多模态大模型（MLLMs）可用于基础设施摄像头图像的事故检测与描述，结合视觉分析模块能显著提升可解释性与性能，但不同模型在精度/召回/F1上存在权衡。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2通过因果Transformer与内存机制，在纯合成训练下实现高效、鲁棒的在线长时点跟踪，实验证明其在多项基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在线实时场景中长时点追踪的挑战：外观显著变化、快速运动与遮挡导致的漂移，以及离线方法依赖未来帧限制实时应用。

Method: 在因果在线框架下，Track-On2使用轻量化Transformer对帧间特征进行补丁级粗分类并通过后续细化步骤精确定位，采用显式记忆模块保持时间一致性以应对遮挡和漂移；训练阶段系统性采用多种合成场景和记忆控制策略来提升长期鲁棒性。

Result: 在五个合成与真实世界基准上，Track-On2超过了此前的在线跟踪器，并优于一些使用双向上下文的强离线方法，证明了因果+记忆架构结合纯合成训练的可行性与扩展性。

Conclusion: Track-On2是一种基于Transformer的在线长时点跟踪模型，通过架构改进、内存机制优化和合成数据训练策略提升了性能与效率，能够在无未来帧信息的因果设置下处理漂移与遮挡，并在多项基准上达到或超越现有方法。

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是开源的多摄多光谱空中调查系统，通过标定、同步和实时检测，显著缩短处理时间并实现准确的动物检测与面积映射。


<details>
  <summary>Details</summary>
Motivation: 为提高空中冰栖动物（如海豹与北极熊）调查的效率与准确性，减少数据处理时间并利用多光谱信息提升检测能力。

Method: 结合多摄像头与多光谱成像，通过严格的相机标定与硬件同步、实时检测算法、元数据注释和将影像与检测结果映射到地面平面来实现。

Result: 与以往方法相比，KAMERA在数据集处理时间上最多可减少80%，并实现了多光谱联合检测、元数据注释及世界平面映射以便面积估算与快速评估。

Conclusion: KAMERA是一个面向多摄、多光谱同步和实时检测海豹与北极熊的综合系统，显著提高了空中冰栖动物调查的数据处理效率，并通过严格标定、硬件同步和空间映射实现准确的检测与面积估算。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: 提出NeuCODEX，通过脉冲压缩与动态早停在边缘-云SNN协同推理中同步减少时空冗余，显著降低传输与能耗并加速推理，精度仅微降。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上完整执行SNN推理受限于高时间步开销导致的延迟与能耗；直接边缘-云协同又面临高延迟和特征传输成本，需在保证精度的前提下减少时空冗余。

Method: 提出了一个包含学习型脉冲驱动压缩模块和动态早停机制的神经形态协同推理架构；压缩模块在时间维度上减少脉冲数据传输，早停机制根据置信度自适应终止边缘推理。并在多种静态与事件流数据集上以及基于ResNet-18和VGG-16的真实边缘-云测试平台上评估。

Result: 在CIFAR10/Caltech和CIFAR10-DVS/N-Caltech数据集上，NeuCODEX实现了最高2048x的数据传输减少、边缘能耗超过90%降低和端到端延迟最多3x加速，且精度损失低于2%。在实际原型上验证了方法的有效性。

Conclusion: NeuCODEX通过联合优化时空冗余，在边缘-云协同推理中显著降低传输数据量、边缘能耗和端到端延迟，同时仅带来极小的精度损失，具备实际可行性。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 通过将视觉基础模型的鲁棒先验注入特征提取器并基于清晰-恶劣图像对构建场景对应监督，提出了两阶段的鲁棒自监督训练策略，从而提高了恶劣天气下的立体匹配精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在夜间、雨雾等恶劣天气下，图像噪声与能见度下降导致CNN特征提取器失效，且光度一致性假设不再可靠，影响自监督立体匹配方法的准确性。引入更稳健的先验和基于场景对应的监督可以缓解这些问题。

Method: 方法包括两部分：1) 将视觉基础模型的鲁棒先验注入到CNN特征提取器，以提升在反光与无纹理等退化区域的特征表示；2) 构建含清晰-恶劣图像对的合成立体数据集，利用场景对应先验替代单纯光度一致性监督，提出鲁棒自监督训练范式（场景对应学习与恶劣天气蒸馏），通过对齐清晰与恶劣图像对的场景结果来改进视差估计。

Result: 在合成带有真实感恶劣天气退化的数据集和多项实验证明，该方法在恶劣天气下显著优于现有自监督立体匹配方法，具有良好泛化性与适用性。代码已开源。

Conclusion: 该论文提出通过注入视觉基础模型的鲁棒先验并利用场景对应先验构建稳健的自监督信号，实现对恶劣天气下立体匹配性能的显著提升。

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: YOLO-LAN结合M2IoU损失、数据增强与负样本训练，在两个公开息肉数据集上显著提升检测精度和鲁棒性，具备临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌早期主要由息肉演变而来，常规内镜人工检测存在漏诊和一致性差的问题，引入基于深度学习的实时息肉检测以提高筛查准确性和效率。

Method: 使用YOLOv12和YOLOv8为检测骨干，训练时采用M2IoU损失函数、各种数据增强策略和负样本（非息肉图像）以提高鲁棒性，评估指标为mAP@50和mAP@50:95。

Result: 在Kvasir-seg上，YOLOv12达到mAP@50=0.9619、mAP@50:95=0.8599；YOLOv8达到mAP@50=0.9540、mAP@50:95=0.8487，均优于现有方法，特别是在mAP@50:95上提升明显，并在不同息肉大小和位置上表现稳健。

Conclusion: 本文提出了YOLO-LAN——基于YOLO的息肉检测方法，通过引入M2IoU损失、丰富的数据增强和负样本训练以模拟临床场景，显著提升了在Kvasir-seg和BKAI-IGH Neopolyp数据集上的检测性能。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 通过在SeC/SAM-2基础上引入并验证长时记忆与概念感知记忆，提升了对遮挡与语义干扰的鲁棒性，在MOSEv2上取得第一名，JF=39.89%。


<details>
  <summary>Details</summary>
Motivation: MOSEv2挑战包含长时遮挡、目标重现与语义干扰等复杂情形，现有方法在保持长时一致性与抑制语义干扰方面存在不足，需利用记忆机制结合强语义先验提升分割性能。

Method: 在SeC（增强的SAM-2）框架上进行适配与实验，重点设计并评估长时记忆模块（用于在遮挡和重现情形下维持时间连续性）与概念感知记忆模块（提供语义先验以抑制干扰），通过消融实验和定性分析验证各模块贡献。

Result: 在MOSEv2测试集上取得JF=39.89%，在该轨道取得排名第一，表明所提出的记忆模块改进在挑战性场景下有效。

Conclusion: 该方法通过分析并改进SeC（基于SAM-2）在MOSEv2轨道中的表现，证明了长时记忆与概念感知记忆的互补性，从而提升复杂半监督视频目标分割的鲁棒性，最终在MOSEv2任务上取得了第一名。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 将VLM视觉处理拆分为物体识别与空间感知，揭示语义展开与位置几何结构，并提出令牌压缩与RoPE缩放以提升效率与空间推理。


<details>
  <summary>Details</summary>
Motivation: 现有VLM以串行方式处理视觉信息，不符合人类视觉的并行性且机制不透明，阻碍理解与架构创新；因此借鉴人类双流视觉假说，将视觉处理拆分为“what”和“where”以便分别研究。

Method: 作者将图像转换为文本令牌映射来研究物体识别，并在理论推导与实证验证的基础上分析位置表示的几何结构；在此基础上设计了一个可插拔的视觉解码器用于指令不可知的令牌压缩，以及RoPE缩放用于增强空间推理。

Result: 实验证明了模型在物体识别上呈现属性到语义的两阶段感知过程，以及位置表示存在可解释的几何结构；所提的令牌压缩与RoPE缩放在解码效率与空间推理性能上带来改进。

Conclusion: 本文通过将视觉处理分解为物体识别与空间感知两部分，证明了VLM内部存在由浅入深的语义展开过程与位置表示的几何结构，并提出了令牌压缩算法与RoPE缩放技术以提升效率与空间推理能力。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 用VLLM生成图像文本描述，转为文本-文本检索，摆脱视觉编码器，缩小模态差、提高组合理解并保护隐私，少量算力下实现SOTA零样本检索。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习的视觉-语言模型（如CLIP）存在浅层语言理解、词袋化行为和模态裂隙问题，且依赖海量网络数据训练带来计算与隐私成本。作者质疑视觉编码器对检索任务的必要性，尝试用文本描述替代图像以改善理解与隐私。

Method: 利用VLLM生成结构化图像描述，将图像库转为文本库；训练/校准单一文本编码器（0.3B参数级别）用于文本-文本检索；设计并发布基于Flickr30k和COCO的两个新的组合性短文本查询基准（subFlickr、subCOCO）；少量GPU和数小时校准即可实现性能提升。

Result: 在多个检索与组合性基准上实现了领先的零样本性能，小至0.3B参数的模型也能匹配或优于传统多模态方法；新的subFlickr与subCOCO基准用于评估短文本组合查询；方法只需两张GPU、数小时校准即可获得上述效果。

Conclusion: 本文提出了一种无需视觉编码器的单编码器检索流水线，通过将图像替换为由大型视觉语言模型生成的结构化文本描述，实现从文本到文本的检索范式，从而缩小模态间差距、提升组合理解并增强隐私性，且在小模型和少量校准资源下即能达到或超越传统多模态模型的零样本检索与组合性基准性能。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 可组合性与长句理解相互促进，但依赖高质量长句数据和合适训练设计，差的数据或冻结参数会损害泛化。


<details>
  <summary>Details</summary>
Motivation: 当前对VLMs理解长且稠密的图文描述的能力不足；作者提出假设：可组合性（对对象-属性绑定和对象间关系的推理能力）是理解长描述的关键，进而研究两者之间的相互影响。

Method: 作者通过构建并训练多种模型，分别针对可组合性和长句理解进行训练与评估，比较不同数据质量（高质量密集描述与结构差的长句）、不同训练策略（如冻结位置信息、有限参数更新）对两项能力的影响，使用长句检索任务和可组合性基准进行定量评估。

Result: 实验显示双向促进关系：可组合训练提升长句检索，长句训练提升可组合性，但前提是使用高质量、定位明确的长句并进行充分参数更新；使用低质量数据或保守训练策略未能带来泛化改善。

Conclusion: 该论文结论为：可组合性（compositionality）与长描述理解之间存在双向促进关系：针对可组合性训练能提升长句检索表现，反之在长句数据上训练也能促进可组合性。但这些提升对数据质量与模型设计敏感，劣质长句或有限参数更新无法泛化；保留通用对齐（如冻结位置信息）并不能提升可组合理解。总体而言，在高质量、具定位信息的长句数据上训练可同时学得两种能力。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 通过合成RGB、少量真实样本与CycleGAN-turbo跨域翻译，能在热成像植物分割任务中显著提升性能，杂草与植物类分别最多提升22%与17%。


<details>
  <summary>Details</summary>
Motivation: 室外高通量田间表型分析中热成像的植物分割面临低对比度与遮挡问题，真实标注昂贵且难以获取，因而探索合成数据与生成模型跨域对齐以提升泛化性能。

Method: 使用1128张含作物与杂草复杂混合的合成图像生成分割掩码，利用CycleGAN-turbo将RGB图像翻译为热成像以实现跨模态对齐，并在训练中加入少量（最低5张）真实手工分割图像进行微调，比较不同采样策略的效果。

Result: 将所有合成图像与少量真实标注结合，较完全基于真实数据的基线模型在杂草类上最大相对提升22%，植物类最大相对提升17%。使用CycleGAN-turbo实现无校准的模板匹配与跨模态翻译，显著提升复杂田间环境下的多模态图像分割性能。

Conclusion: 本文提出了一种结合合成RGB图像、少量真实标注和GAN跨模态对齐的框架，用于提升热成像中植物语义分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: HyKid：包含48例儿科脑积水1mm重建3D MRI及专家分割（含脉络丛）的公开数据集，脉络丛体积与总CSF高度相关，基于此的预测模型AUC=0.87，可作为脑积水研究与算法开发的基准。


<details>
  <summary>Details</summary>
Motivation: 儿科脑积水评估困难，现有研究受限于缺乏公开且包含脉络丛分割的专家注释数据集，阻碍算法开发与可比较性研究。

Method: 将常规低分辨率MRI通过切片到体积(slice-to-volume)重建为1mm各向同性3D图像；由有经验的神经科医生手工修正分割白质、灰质、侧脑室、外周脑脊液和脉络丛；使用基于检索增强生成(RAG)的框架从放射科临床报告中提取结构化数据；基于脉络丛体积和总CSF体积构建预测模型评估脑积水。

Result: 公开了48例儿科脑积水患者的高分辨率重建MRI与手工修正分割标签，提取了结构化临床报告信息；发现脉络丛体积与总CSF体积强相关；基于该特征的预测模型AUC达0.87；数据集作为高质量基准有助于神经影像算法开发。

Conclusion: 该论文提出并公开了HyKid数据集，填补了儿科脑积水影像学中高质量、专家标注（包含脉络丛分割）公开数据集的空白，并展示脉络丛体积与总脑脊液体积高度相关，脉络丛相关特征可作为脑积水评估的潜在生物标志物。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出MsFIN，通过短/中/长时尺度特征提取、Transformer交互与因果时间处理融合场景与对象信息，实现更早更准确的事故预判，实验证明优于单尺度基线。


<details>
  <summary>Details</summary>
Motivation: 行车纪录仪视角下，参与交通主体常被遮挡且行为存在异步多时序性，单尺度特征难以建模复杂交互与长期/短期行为线索，影响事故提前预判的正确性与早期性。

Method: 设计了三层结构：多尺度特征聚合模块提取短/中/长时尺度的场景表示；基于Transformer的交互机制实现对象与场景间的特征交互；时间处理模块在因果约束下捕捉序列演化；多尺度后融合模块跨时尺度融合场景与对象特征生成风险表示。

Result: 在DAD和DADA数据集上，MsFIN在预测准确率与提前量（earliness）上显著优于现有单尺度方法；消融实验显示各模块均对性能提升有贡献，验证了多尺度融合和上下文交互的重要性。

Conclusion: 本文提出的MsFIN通过多尺度特征聚合、Transformer交互和因果时间处理，有效提升了行车纪录仪视频中的事故提前预判性能，实验在DAD和DADA数据集上优于单尺度方法。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 把人脸伪造检测做成增量学习，Real-/Fake-LoRA专家分组＋正交约束／正交梯度防遗忘，有效应对新伪造类型。


<details>
  <summary>Details</summary>
Motivation: 现实中伪造人脸生成方法快速迭代，难以穷尽所有伪造变体；真实人脸样本丰富且采集相对稳定，故采用增量学习使检测器能在数据和计算受限情况下持续学习新伪造类型而不遗忘旧有类型。

Method: 提出Developmental Mixture of Experts架构：使用多位LoRA专家分为Real-LoRA（学习真实人脸）和多个Fake-LoRA（分别捕捉不同伪造类型）；对Fake-LoRA施加子空间正交约束，并在训练中整合正交梯度以避免梯度干扰；训练时按任务增量更新Fake-LoRA以适应新伪造类型。

Result: 在数据集增量和伪造类型增量两种协议下，实验表明该方法在保持旧任务性能同时有效适应新伪造类型，验证了正交约束与开发性MoE结构的有效性。

Conclusion: 本文将人脸伪造检测问题视为增量学习问题，通过Developmental MoE架构及LoRA专家组分离真实与伪造知识，利用正交约束和正交梯度防止灾难性遗忘，从而在新伪造类型出现时实现快速适应且保持对已有伪造类型的检测能力。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O：一个统一的多模态掩蔽扩散模型，通过新架构与采样技术，实现物体定位、图像编辑与1024px高分辨率图像合成，并用理解能力指导生成，达到多项基准领先且推理更快。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型在理解能力（多为图像级别）和生成能力（分辨率与编辑功能）上存在局限，难以同时实现高质量的图像理解与高分辨率、可控的图像生成与编辑。Lavida-O 的目标是构建一个统一框架，填补理解与生成之间的鸿沟并提升效率与性能。

Method: 引入了若干新技术：Elastic Mixture-of-Transformer 架构以支持多任务与高效计算；通用文本条件化（universal text conditioning）用于统一不同文本指令的表示；分层采样（stratified sampling）则提升训练与采样效率。模型通过掩蔽扩散机制同时处理理解（如物体定位）与生成（如高分辨率图像合成、编辑）任务，并采用规划与迭代自反（self-reflection）策略将理解结果反馈用于生成优化。

Result: 在多个基准（如 RefCOCO 的物体定位、GenEval 的文本到图像生成、ImgEdit 的图像编辑）上取得了最先进表现，超越 Qwen2.5-VL、FluxKontext-dev 等自回归与连续扩散模型，同时在推理速度上有可观加速。模型展示了物体定位（object grounding）、图像编辑、高分辨率合成（1024px）等新能力。

Conclusion: Lavida-O 是一个统一的多模态掩蔽扩散模型（MDM），在图像理解与生成两方面实现了更广泛与更先进的能力。该模型不仅支持物体定位、图像编辑和高分辨率（1024px）图像合成，还首次将理解能力用于引导生成与编辑，通过规划与迭代自反提升结果质量。实验显示在多项基准上优于现有自回归与连续扩散模型，并在推理速度上具有显著优势。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: ConViS：按预定义概念计算视频对相似性，提供可解释分数与文本说明，并通过ConViS-Bench展示模型在概念化相似性任务上的差异与挑战。


<details>
  <summary>Details</summary>
Motivation: 现有视频相似性通常依赖全局相似度评分，无法反映人类按不同方面（如动作、地点等）进行比较的能力。大型多模态模型带来用自然语言引导比较任务的新契机，因此需要一个可解释、概念化的相似性评估框架与基准。

Method: 作者构建了ConViS任务定义与ConViS-Bench基准数据集，数据集中每对视频包含针对多个概念的相似性分数以及描述相同点和差异的文本。并在若干最先进的视频-语言模型上进行基准测试以评估与人类判断的一致性。

Result: 在ConViS评测中，不同模型在概念层面表现差异显著，某些概念在估计视频相似性时更具挑战性；整体表明当前模型在与人类判断对齐方面仍有改进空间。

Conclusion: 本文提出了ConViS，一种基于概念的视频相似性估计任务，通过在预定义语义概念集合上计算可解释的相似性分数，实现了对视频相似性的细粒度、多维度比较。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 引入对抗细化的VQ-GAN与密集运动令牌化，有效压缩时空热图并消除重建伪影，在CMU Panoptic上显著优于dVAE，且揭示了2D/3D运动表示的词汇规模差异。


<details>
  <summary>Details</summary>
Motivation: 连续人体运动高维且冗余，需高效压缩与表达以便分析复杂运动动态，同时保持细粒度运动痕迹以支持下游任务。

Method: 使用密集运动令牌化对时空热图进行矢量量化，并在VQ-GAN框架中加入对抗训练来细化重建，消除运动模糊和时间不对齐等伪影；比较了dVAE基线并在CMU Panoptic数据集上评估性能指标如SSIM和时间稳定性。

Result: 在CMU Panoptic上相比dVAE基线SSIM提升9.31%，时间不稳定性降低37.1%；还发现2D运动可用128令牌词汇紧凑表示，而3D运动需1024令牌以达到忠实重建。

Conclusion: 本文提出了一种基于VQ-GAN并结合对抗细化和密集运动令牌化的方法，用于高效压缩和重建时空热图，从而更好地保留细粒度人体运动轨迹。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 提出一种基于体素级聚类+图论特征的GrRAiL描述子，有效表征病灶内异质性，在多中心MRI数据上对三类临床任务均显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 常规放射组学多为区域汇总特征，忽略病灶内部复杂的空间关系，导致对混淆病理学（如放射性损伤 vs 复发）鉴别能力不足。GrRAiL旨在捕获高阶空间关联来改进诊断准确性。

Method: 先在每个体素提取放射特征并进行聚类以识别亚区域；然后构建加权图（节点为簇、边反映空间邻近与强度关系），计算图论度量作为描述符输入分类器进行区分。

Result: 在947例多中心病例中，GrRAiL在三种临床任务上均显著优于GNN、纹理放射组学和强度图分析。GBM复发判别CV/Test准确率89%/78%（测试提升>10%）；脑转移复发判别84%/74%（>13%提升）；IPMN高危分层84%/75%（>10%提升）。

Conclusion: GrRAiL提出了一种基于图的放射组学描述子，通过在像素/体素级别进行簇分割并计算图论特征，能更好地表征病灶内部异质性，从而在多中心临床MRI数据上提高肿瘤复发与放射性损伤以及胰腺IPMN风险分层的分类性能。

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS通过先验动作模型与基于第一视角视觉的高层Q-learning控制相结合，实现仅用egocentric视觉感知的类人化身导航与运动生成，能在视觉障碍下产生人类般的规避行走。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视感知与动作的联动，使用与人类不同的任务专用“感知”；作者主张通过第一视角视觉模拟人类感知以产生更人类化的运动。

Method: 方法包括两步：先在大规模动作捕捉数据上训练动作先验模型（低级运动技能），再用Q-learning训练策略将第一视角视觉输入映射为动作先验的高级控制命令。

Result: 实验证明基于第一视角视觉可以产生具有人类运动特征的化身行为，例如在视觉视野内规避障碍物，表明类人传感器有助于训练类人化化身。

Conclusion: 作者认为生成类人化动作需要类人化的感知，提出CLOPS系统仅用第一视角视觉驱动化身导航与运动生成。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 识别布局重叠带来的两大挑战，提出OverLayScore评估重叠复杂度，发布重叠难度平衡的OverLayBench，并通过amodal mask微调得到CreatiLayout-AM以提升复杂重叠场景的生成效果。


<details>
  <summary>Details</summary>
Motivation: 当前基准偏向低重叠样本，不能反映现实场景中大量重叠实例导致生成质量下降的问题，需要新的度量和数据来评估与改进模型。

Method: 提出OverLayScore量化重叠复杂度；构建OverLayBench平衡不同重叠难度的数据集；基于amodal mask的精心策划数据对CreatiLayout模型进行微调得到CreatiLayout-AM。

Result: 定义了OverLayScore并证明现有数据集偏向简单情况；提供了高质量、重叠难度平衡的OverLayBench；微调后的CreatiLayout-AM在复杂重叠场景上取得初步改进（具体数值摘要未给出）。

Conclusion: 本文指出现有布局到图像生成方法在处理重叠bounding box时表现不足，提出评估指标和数据集并给出初步改进方法，推动更鲁棒的生成研究。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 通过自蒸馏将视频扩散模型的2D想象能力转化为显式3DGS表示，支持从文本、单图或单目视频生成可实时渲染的静态与动态3D场景，免去真实多视角训练数据。


<details>
  <summary>Details</summary>
Motivation: 克服现有基于学习的3D重建方法对真实多视角数据的强依赖，以及利用视频扩散模型已具备的想象力但缺乏3D可用性的短板。

Method: 在传统2D RGB解码器的基础上并行加入3DGS解码器，利用RGB解码器的输出作为监督信号，使用视频扩散模型合成的合成数据训练3DGS解码器；推理时可基于文本或单张图生成场景，并支持从单目视频生成动态3D场景。

Result: 实验显示在静态与动态3D场景生成任务上均达到或优于现有最先进方法，并实现了实时渲染能力。

Conclusion: 该方法提出了将视频扩散模型中的隐式3D知识蒸馏到显式3D高斯点云表示（3DGS）的自蒸馏框架，实现无需实景多视角数据即可生成可实时渲染的静态与动态3D场景。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: 用体素对齐高斯替代像素对齐，提升了多视图一致性、几何精度与渲染效果，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统像素对齐的3DGS对输入视图数量敏感，且容易在遮挡或低纹理区域产生对齐错误与视图偏倚密度，限制了重建质量与一致性。

Method: 提出从预测的3D体素网格直接生成高斯（voxel-aligned Gaussians）的前馈多视图网络，避免像素级特征匹配；允许基于3D场景复杂度自适应控制高斯密度。

Result: 在RealEstate10K和ScanNet基准上，VolSplat在新视图合成质量和高斯点云的视图一致性上取得SOTA，同时生成更稠密稳健的表示。

Conclusion: VolSplat通过用体素对齐的高斯取代像素对齐范式，解决了视图数量依赖、视图偏置密度分布和对齐误差等问题，从而提高了几何一致性与新视图渲染质量。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow通过对源/目标分布做可学习的条件化平移，简化流匹配学习任务，带来训练加速与生成质量提升，且参数开销极小。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散/流方法需要模型同时学习将无条件的初始噪声输运到带条件的数据分布，以及在此过程中注入条件信息，任务复杂。通过在源/目标分布上施加条件感知的可学习平移，可以降低模型需学习的变换量，从而加速与提高性能。

Method: 在流匹配框架中引入Condition-Aware Reparameterization（CAR），对初始标准高斯噪声或目标分布进行可学习的条件化位移，使模型在输运过程中接收已部分注入的条件信息。方法在低维合成数据上可视化分析，并在ImageNet-256上将SiT-XL/2与CAR-Flow结合以评估效果。

Result: 在合成数据上定性和定量展示了CAR对概率路径和训练效率的影响；在ImageNet-256上，将SiT-XL/2与CAR-Flow结合后FID从2.07降至1.68，且仅增加不到0.6%的参数量。

Conclusion: 提出的CAR-Flow通过对源分布/目标分布进行条件感知的轻量级可学习平移，使流匹配模型只需学习更短的概率路径，从而减轻模型在质量运输与条件注入方面的负担。实验表明，这种重参数化能加速训练并提升生成质量。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [124] [ExtGraph: A Fast Extraction Method of User-intended Graphs from a Relational Database](https://arxiv.org/abs/2509.18534)
*Jeongho Park,Geonho Lee,Min-Soo Kim*

Main category: cs.DB

TL;DR: 提出ExtGraph，通过外连接与物化视图混合处理高效抽取用户意图图，针对TPC-DS/DBLP/IMDB数据集实验最多加速2.78×。


<details>
  <summary>Details</summary>
Motivation: 关系数据库是很多公司重要数据来源，但直接从关系模式通过复杂联接构造图通常效率低下且难以得到用户期望的图结构，需一种高效且可控的图抽取方法。

Method: 提出混合查询处理策略，结合外连接（outer join）和物化视图（materialized view）来避免全量复杂联接计算，按需构建图的节点与边。

Result: 在TPC-DS、DBLP、IMDB数据集上的实验显示，ExtGraph 在图抽取时间上相较于最先进方法最多可加速2.78倍。

Conclusion: ExtGraph 能有效从关系型数据库中提取用户期望的图数据，通过外连接与物化视图的混合查询处理减少复杂连接带来的开销。

Abstract: Graph analytics is widely used in many fields to analyze various complex
patterns. However, in most cases, important data in companies is stored in
RDBMS's, and so, it is necessary to extract graphs from relational databases to
perform graph analysis. Most of the existing methods do not extract a
user-intended graph since it typically requires complex join query processing.
We propose an efficient graph extraction method, \textit{ExtGraph}, which can
extract user-intended graphs efficiently by hybrid query processing of outer
join and materialized view. Through experiments using the TPC-DS, DBLP, and
IMDB datasets, we have shown that \textit{ExtGraph} outperforms the
state-of-the-art methods up to by 2.78x in terms of graph extraction time.

</details>


### [125] [CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases](https://arxiv.org/abs/2509.18670)
*Yeonwoo Jeong,Hyunji Cho,Kyuri Park,Youngjae Kim,Sungyong Park*

Main category: cs.DB

TL;DR: CALL机制通过基于集群访问模式的上下文感知查询分组和智能预取，显著减少缓存未命中，降低向量数据库的搜索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有向量数据库中，嵌入模型导致查询在向量空间中集群访问模式不均匀，现有方法忽视了查询的集群访问模式对缓存命中的影响，导致缓存未命中惩罚较高。

Method: 提出CALL机制，根据共享的集群访问模式对查询进行上下文感知分组，并引入组感知预取策略和延迟感知的集群加载方法，减少缓存未命中。

Result: 实验表明CALL将99百分位尾延迟降低了最多33%，并始终保持更高的缓存命中率，大幅降低了搜索延迟。

Conclusion: CALL通过优化查询的集群访问模式分组和预取策略，有效减少了缓存未命中，提高了向量数据库的查询效率。

Abstract: Embedding models capture both semantic and syntactic structures of queries,
often mapping different queries to similar regions in vector space. This
results in non-uniform cluster access patterns in modern disk-based vector
databases. While existing approaches optimize individual queries, they overlook
the impact of cluster access patterns, failing to account for the locality
effects of queries that access similar clusters. This oversight increases cache
miss penalty. To minimize the cache miss penalty, we propose CALL, a
context-aware query grouping mechanism that organizes queries based on shared
cluster access patterns. Additionally, CALL incorporates a group-aware
prefetching method to minimize cache misses during transitions between query
groups and latency-aware cluster loading. Experimental results show that CALL
reduces the 99th percentile tail latency by up to 33% while consistently
maintaining a higher cache hit ratio, substantially reducing search latency.

</details>


### [126] [Teaching RDM in a smart advanced inorganic lab course and its provision in the DALIA platform](https://arxiv.org/abs/2509.18902)
*Alexander Hoffmann,Jochen Ortmeyer,Fabian Fink,Charles Tapley Hoyt,Jonathan D. Geiger,Paul Kehrein,Torsten Schrade,Sonja Herres-Pawlis*

Main category: cs.DB

TL;DR: 在本科化学实验课程中引入开源ELN（Chemotion）并结合教学资源与DALIA发现平台，可有效教授FAIR原则与RDM实践，提升数据可重用性与可发现性。


<details>
  <summary>Details</summary>
Motivation: 化学研究数据传统上以手写或不可访问的格式（如PDF）存储，限制了数据重用与机器学习应用；有必要在本科阶段教授FAIR原则并提供可操作的RDM工具。

Method: 在五学期的实验课程中采用开源ELN Chemotion，结合研讨会、在线视频与Moodle互动练习，并利用DALIA平台作为发现工具，记录学生实验的计划、元数据、分析与评估，从而实现数字化数据管理。

Result: 学生能够使用Chemotion以结构化、带元数据的方式记录实验，推动可持续共享与长期可重用；配套教学资源（研讨会、视频、Moodle）增强了学习效果，DALIA平台支持数据发现。

Conclusion: 引入电子实验记录和以FAIR为导向的RDM教学能显著提高化学本科生的数据素养和数据可重用性。

Abstract: Research data management (RDM) is a key data literacy skill that chemistry
students must acquire. Concepts such as the FAIR data principles (Findable,
Accessible, Interoperable, Reusable) should be taught and applied in
undergraduate studies already. Traditionally, research data from labs, theses,
and internships were handwritten and stored in inaccessible formats such as
PDFs, limiting reuse and machine learning applications. At RWTH Aachen
University, a fifth-semester lab course introduces students to the electronic
laboratory notebook (ELN) Chemotion, an open-source DFG-funded tool linked to
the national NFDI4Chem initiative. Students plan, document, and evaluate
experiments digitally, ensuring metadata and analysis are captured for
long-term reuse. Chemotion's intuitive interface and repository enable
sustainable data sharing. To reinforce RDM, students receive a seminar and
access to online training videos with interactive Moodle elements. Herein we
highlight the use of the DALIA platform as a discovery tool for the students.

</details>


### [127] [A decentralized future for the open-science databases](https://arxiv.org/abs/2509.19206)
*Gaurav Sharma,Viorel Munteanu,Nika Mansouri Ghiasi,Jineta Banerjee,Susheel Varma,Luca Foschini,Kyle Ellrott,Onur Mutlu,Dumitru Ciorbă,Roel A. Ophoff,Viorel Bostan,Christopher E Mason,Jason H. Moore,Despoina Sousoni,Arunkumar Krishnan,Christopher E. Mason,Mihai Dimian,Gustavo Stolovitzky,Fabio G. Liberante,Taras K. Oleksyk,Serghei Mangul*

Main category: cs.DB

TL;DR: 集中式科研数据库易受单点故障威胁。本文评估联邦与去中心化替代方案并提出混合框架，提高可用性、FAIR性与长期可持续性。


<details>
  <summary>Details</summary>
Motivation: 数据生成快速增长与地缘政治、资助不稳定性增加，使集中化模型易受中断，威胁科研连续性与数据完整性，故需寻找更韧性的治理与技术方案。

Method: 本文通过结构性论证和对比分析，评估集中式、联邦和去中心化模型的优缺点，提出混合框架设计原则以兼顾可用性、FAIR原则和可持续性。

Result: 提出一种混合式框架，结合集中化的标准化与去中心化的冗余分发，能显著降低治理和基础设施脆弱性，增强全球可访问性与公平性。

Conclusion: 集中式生物数据库存在单点故障风险，需向联邦/去中心化架构转型以提高韧性并保障数据长期可用性。

Abstract: Continuous and reliable access to curated biological data repositories is
indispensable for accelerating rigorous scientific inquiry and fostering
reproducible research. Centralized repositories, though widely used, are
vulnerable to single points of failure arising from cyberattacks, technical
faults, natural disasters, or funding and political uncertainties. This can
lead to widespread data unavailability, data loss, integrity compromises, and
substantial delays in critical research, ultimately impeding scientific
progress. Centralizing essential scientific resources in a single geopolitical
or institutional hub is inherently dangerous, as any disruption can paralyze
diverse ongoing research. The rapid acceleration of data generation, combined
with an increasingly volatile global landscape, necessitates a critical
re-evaluation of the sustainability of centralized models. Implementing
federated and decentralized architectures presents a compelling and
future-oriented pathway to substantially strengthen the resilience of
scientific data infrastructures, thereby mitigating vulnerabilities and
ensuring the long-term integrity of data. Here, we examine the structural
limitations of centralized repositories, evaluate federated and decentralized
models, and propose a hybrid framework for resilient, FAIR, and sustainable
scientific data stewardship. Such an approach offers a significant reduction in
exposure to governance instability, infrastructural fragility, and funding
volatility, and also fosters fairness and global accessibility. The future of
open science depends on integrating these complementary approaches to establish
a globally distributed, economically sustainable, and institutionally robust
infrastructure that safeguards scientific data as a public good, further
ensuring continued accessibility, interoperability, and preservation for
generations to come.

</details>


### [128] [Gate-Based and Annealing-Based Quantum Algorithms for the Maximum K-Plex Problem](https://arxiv.org/abs/2509.19214)
*Xiaofan Li,Gao Cong,Rui Zhou*

Main category: cs.DB

TL;DR: 本文把MKP问题分别用门型量子算法与量子退火框架处理，提出qTKP/qMKP(理论上达到O^*(1.42^n))和退火近似算法qaMKP，并在IBM模拟器与D-Wave上做了初步实验验证。


<details>
  <summary>Details</summary>
Motivation: 传统的团模型对噪声敏感，不适合实际图数据。k-plex作为松弛模型更适合真实网络，但求解MKP为NP难问题，且经典算法复杂度高，故尝试用量子计算加速MKP求解。

Method: 基于门的qTKP通过量子搜索结合图的二进制编码、度数计数与比较以及规模判定，在给定规模下寻找k-plex；qMKP对qTKP外包以二分搜索用于逐步确定最大k-plex。基于退火的qaMKP将MKP重写为QUBO问题，以更高效地利用量子退火器的比特资源进行近似求解。

Result: 理论上门模型算法将时间复杂度降至O^*(1.42^n)，优于现有c_k^n (c_k>1.94)的经典结果；退火算法在比特资源利用上更省；并通过IBM模拟器与D-Wave实验验证了方案的可行性（proof-of-principle）。

Conclusion: 本文提出了针对最大k-plex问题(MKP)的两类量子算法：两种基于门模型的算法(qTKP、qMKP)和一种基于退火的近似算法(qaMKP)，并在模拟器与退火机上进行了验证，显示在理论复杂度与资源利用上具有优势。

Abstract: The $ k $-plex model, which allows each vertex to miss connections with up to
$ k $ neighbors, serves as a relaxation of the clique. Its adaptability makes
it more suitable for analyzing real-world graphs where noise and imperfect data
are common and the ideal clique model is often impractical. The problem of
identifying the maximum $ k $-plex (MKP, which is NP-hard) is gaining attention
in fields such as social network analysis, community detection, terrorist
network identification, and graph clustering. Recent works have focused on
optimizing the time complexity of MKP algorithms. The state-of-the-art has
reduced the complexity from a trivial $ O^*(2^n) $ to $ O^*(c_k^n) $, with $
c_k > 1.94 $ for $ k \geq 3 $, where $ n $ denotes the vertex number. This
paper investigates the MKP using two quantum models: gate-based model and
annealing-based model. Two gate-based algorithms, qTKP and qMKP, are proposed
to achieve $ O^*(1.42^n) $ time complexity. qTKP integrates quantum search with
graph encoding, degree counting, degree comparison, and size determination to
find a $ k $-plex of a given size; qMKP uses binary search to progressively
identify the maximum solution. Furthermore, by reformulating MKP as a quadratic
unconstrained binary optimization problem, we propose qaMKP, the first
annealing-based approximation algorithm, which utilizes qubit resources more
efficiently than gate-based algorithms. To validate the practical performance,
proof-of-principle experiments were conducted using the latest IBM gate-based
quantum simulator and D-Wave adiabatic quantum computer. This work holds
potential to be applied to a wide range of clique relaxations, e.g., $ n $-clan
and $ n $-club.

</details>
