<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 171]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes](https://arxiv.org/abs/2512.17939)
*Yuncheng Lu,Yucen Shi,Aobo Li,Zehao Li,Junying Li,Bo Wang,Tony Tae-Hyoung Kim*

Main category: cs.CV

TL;DR: 提出帧-事件混合的低能耗反无人机芯片，利用事件帧重建、区提议、自适应模式切换和零跳过神经处理器，实现高识别率（98.2%）与极低能耗（96 pJ/frame·pixel，61 pJ/event）。


<details>
  <summary>Details</summary>
Motivation: 应对小型、快速移动无人机的检测难题：传统帧式摄像在高速度与高帧率下功耗高，事件相机虽低功耗但信息稀疏且难以直接用于识别；因此需要结合两者优势、在硬件层面优化以实现高效、实时和鲁棒的反无人机解决方案。

Method: 系统重建二值事件帧（使用运行长度编码），生成区域候选并根据目标尺寸与速度在帧模式与事件模式间自适应切换；Fast Object Tracking Unit通过自适应阈值和基于轨迹的分类提高对高速目标的鲁棒性；神经处理单元同时支持灰度补丁与轨迹推理，采用定制指令集和零跳过MAC架构以避开冗余神经计算（减少>97%）。

Result: 在40 nm工艺下实现2 mm²芯片，工作电压0.8 V，能耗为96 pJ/帧·像素和61 pJ/事件，公开无人机数据集上跨50–400 m距离和5–80 px/s速度达到98.2%识别准确率，展示了端到端能效的先进性。

Conclusion: 该论文提出了一种能效极高的反无人机系统，通过帧式与事件驱动混合跟踪、运行长度编码的事件帧重建、区域建议与自适应模式切换，实现对小型高速无人机的可靠检测；引入快速目标跟踪单元和零跳过MAC神经处理器，显著降低冗余计算；在40 nm工艺上的芯片实现验证了其在能耗与识别性能上的领先地位。

Abstract: We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.

</details>


### [2] [NystagmusNet: Explainable Deep Learning for Photosensitivity Risk Prediction](https://arxiv.org/abs/2512.17943)
*Karthik Prabhakar*

Main category: cs.CV

TL;DR: 提出基于双支路CNN的NystagmusNet，输入亮度与眼动方差，输出光敏感风险评分（合成数据75%验证准确率），并结合SHAP/GradCAM解释与规则推荐，未来计划智能眼镜部署与强化学习个性化。


<details>
  <summary>Details</summary>
Motivation: 缓解光敏感性眼震患者因环境亮度引发的不自主眼动，通过提前预测高风险视觉环境并给出实时适配建议来减少症状和日常困扰。

Method: 使用双支路卷积神经网络，一支处理环境亮度图像或特征，一支处理眼动时间序列或方差特征；训练数据来自合成与增强的眼动-环境配对；输出光敏感风险分数；集成SHAP和GradCAM用于可解释性；基于阈值的规则引擎给出滤光器等实时建议。

Result: NystagmusNet predicts photosensitivity risk from environmental brightness and eye movement variance using dual-branch CNN trained on synthetic/augmented data.

Conclusion: Model reaches 75% validation accuracy on synthetic data; explainability (SHAP, GradCAM) and rule-based recommendations improve trust; real-world deployment and personalization remain future work.

Abstract: Nystagmus patients with photosensitivity face significant daily challenges due to involuntary eye movements exacerbated by environmental brightness conditions. Current assistive solutions are limited to symptomatic treatments without predictive personalization. This paper proposes NystagmusNet, an AI-driven system that predicts high-risk visual environments and recommends real-time visual adaptations. Using a dual-branch convolutional neural network trained on synthetic and augmented datasets, the system estimates a photosensitivity risk score based on environmental brightness and eye movement variance. The model achieves 75% validation accuracy on synthetic data. Explainability techniques including SHAP and GradCAM are integrated to highlight environmental risk zones, improving clinical trust and model interpretability. The system includes a rule-based recommendation engine for adaptive filter suggestions. Future directions include deployment via smart glasses and reinforcement learning for personalized recommendations.

</details>


### [3] [SuperFlow: Training Flow Matching Models with RL on the Fly](https://arxiv.org/abs/2512.17951)
*Kaijie Chen,Zhiyang Xu,Ying Shen,Zihao Lin,Yuguang Yao,Lifu Huang*

Main category: cs.CV

TL;DR: SuperFlow adjusts per-prompt group sizes based on variance and computes step-level advantages consistent with continuous-time flows, yielding large training speedups and quality gains over baselines.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies of fixed group sizes and biased credit assignment in RL training for flow-based generative models to improve sample efficiency and alignment.

Method: Variance-aware group sizing; continuous-time consistent step-level advantage estimation

Result: Empirical improvements: reaches similar/better performance using only 5.4%-56.3% of original training steps; reduces training time by 5.2%-16.7%; improves over SD3.5-M by 4.6%-47.2% and over Flow-GRPO by 1.7%-16.0% on T2I tasks

Conclusion: SuperFlow improves training efficiency and credit assignment for flow-based RL training without architecture changes via variance-aware sampling and continuous-time consistent advantages.

Abstract: Recent progress in flow-based generative models and reinforcement learning (RL) has improved text-image alignment and visual quality. However, current RL training for flow models still has two main problems: (i) GRPO-style fixed per-prompt group sizes ignore variation in sampling importance across prompts, which leads to inefficient sampling and slower training; and (ii) trajectory-level advantages are reused as per-step estimates, which biases credit assignment along the flow. We propose SuperFlow, an RL training framework for flow-based models that adjusts group sizes with variance-aware sampling and computes step-level advantages in a way that is consistent with continuous-time flow dynamics. Empirically, SuperFlow reaches promising performance while using only 5.4% to 56.3% of the original training steps and reduces training time by 5.2% to 16.7% without any architectural changes. On standard text-to-image (T2I) tasks, including text rendering, compositional image generation, and human preference alignment, SuperFlow improves over SD3.5-M by 4.6% to 47.2%, and over Flow-GRPO by 1.7% to 16.0%.

</details>


### [4] [Seeing Beyond the Scene: Analyzing and Mitigating Background Bias in Action Recognition](https://arxiv.org/abs/2512.17953)
*Ellie Zhou,Jihoon Chung,Olga Russakovsky*

Main category: cs.CV

TL;DR: 研究发现各种动作识别模型普遍存在背景偏差；对分类模型使用人体分割输入能减少3.78%偏差；对VLLM的提示调优能使模型更多依据人体动作，改进约9.85%。


<details>
  <summary>Details</summary>
Motivation: 动作识别模型被应有依赖人体动作和姿态，但在真实数据中模型往往借助背景或场景线索来做判断，导致泛化性差和对抗性脆弱性。系统性量化这种背景偏差并提出可行的缓解策略，有助于提高模型的可靠性与公平性。

Method: 1) 系统性评估：在分类模型、对比式文图预训练模型和VLLM上设计实验以测量模型对背景与人体信息的依赖。2) 缓解分类模型：引入人体分割作为输入（单独或与原图结合）并对比性能与背景依赖指标。3) VLLM提示调优：设计手工提示与自动化提示搜索策略，引导模型将注意力从背景转向人体动作。

Result: 实验证明三类模型均表现出强背景推理倾向；对分类模型采用人体分割输入可将背景偏差降低约3.78%；对VLLM通过提示设计可实现约9.85%的向人体推理迁移，显示提示对减少背景依赖有效。

Conclusion: 本文展示背景偏差在动作识别模型（分类模型、对比文图预训练模型和视频大语言模型）中普遍存在，且所有模型倾向依赖背景而非人体动作。通过为分类模型引入分割到的人体输入，可以将背景偏差降低约3.78%。同时，通过手工与自动提示调优可将VLLM的预测更向“以人为核心”推理方向引导，提升约9.85%。

Abstract: Human action recognition models often rely on background cues rather than human movement and pose to make predictions, a behavior known as background bias. We present a systematic analysis of background bias across classification models, contrastive text-image pretrained models, and Video Large Language Models (VLLM) and find that all exhibit a strong tendency to default to background reasoning. Next, we propose mitigation strategies for classification models and show that incorporating segmented human input effectively decreases background bias by 3.78%. Finally, we explore manual and automated prompt tuning for VLLMs, demonstrating that prompt design can steer predictions towards human-focused reasoning by 9.85%.

</details>


### [5] [SCS-SupCon: Sigmoid-based Common and Style Supervised Contrastive Learning with Adaptive Decision Boundaries](https://arxiv.org/abs/2512.17954)
*Bin Wang,Fadi Dornaika*

Main category: cs.CV

TL;DR: SCS-SupCon用可学习的sigmoid对比损失和风格-内容解耦约束，强调难负样本，缓解负样本稀释，在多个数据集和骨干网络上显著优于SupCon/CS-SupCon，结果稳健。


<details>
  <summary>Details</summary>
Motivation: 现有监督对比学习（如SupCon）在细粒度识别中受限于类间差异小、类内变化大，同时InfoNCE基的损失存在负样本稀释和固定决策边界问题，导致判别力不足。

Method: 提出基于sigmoid的成对对比损失：使用可学习的温度和偏置以构建自适应决策边界，增强对难负样本的权重；同时引入显式的风格距离约束以在特征空间中解耦风格与内容，提升鲁棒性。

Result: 在六个基准数据集（含CUB200-2011、Stanford Dogs）和CNN/Transformer骨干上都达到SOTA。在CIFAR-100(ResNet-50)上相比SupCon提升约3.9个百分点，相比CS-SupCon提升约1.7；在细粒度集上超越CS-SupCon 0.4--3.0点。消融与统计检验（Friedman与Nemenyi）验证改进的稳健性。

Conclusion: 提出SCS-SupCon，通过sigmoid基的成对对比损失（可学习温度和偏置）和显式风格距离约束，解决了负样本稀释和决策边界不自适应的问题，从而在细粒度分类中提升判别能力。

Abstract: Image classification is hindered by subtle inter-class differences and substantial intra-class variations, which limit the effectiveness of existing contrastive learning methods. Supervised contrastive approaches based on the InfoNCE loss suffer from negative-sample dilution and lack adaptive decision boundaries, thereby reducing discriminative power in fine-grained recognition tasks. To address these limitations, we propose Sigmoid-based Common and Style Supervised Contrastive Learning (SCS-SupCon). Our framework introduces a sigmoid-based pairwise contrastive loss with learnable temperature and bias parameters to enable adaptive decision boundaries. This formulation emphasizes hard negatives, mitigates negative-sample dilution, and more effectively exploits supervision. In addition, an explicit style-distance constraint further disentangles style and content representations, leading to more robust feature learning. Comprehensive experiments on six benchmark datasets, including CUB200-2011 and Stanford Dogs, demonstrate that SCS-SupCon achieves state-of-the-art performance across both CNN and Transformer backbones. On CIFAR-100 with ResNet-50, SCS-SupCon improves top-1 accuracy over SupCon by approximately 3.9 percentage points and over CS-SupCon by approximately 1.7 points under five-fold cross-validation. On fine-grained datasets, it outperforms CS-SupCon by 0.4--3.0 points. Extensive ablation studies and statistical analyses further confirm the robustness and generalization of the proposed framework, with Friedman tests and Nemenyi post-hoc evaluations validating the stability of the observed improvements.

</details>


### [6] [A Modular Framework for Single-View 3D Reconstruction of Indoor Environments](https://arxiv.org/abs/2512.17955)
*Yuxiao Li*

Main category: cs.CV

TL;DR: 用扩散技术做视图级的遮挡补全与布局修复，再转为3D，模块化设计包括amodal补全、专门的房间inpaint、混合深度估计与2D/3D视图对齐，在3D-Front上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 直接从不完整2D图像回归3D形状效果差，特别是复杂实例和遮挡。通过在视图层面先恢复完整像素信息，再投影生成3D，可以减轻不确定性与错误传递。

Method: 框架包含四个核心模块：amodal实例补全（基于扩散模型恢复被遮挡对象完整视图）；针对房间布局训练的inpainting模型（恢复背景与墙地面等）；混合深度估计（组合全局几何与细节敏感估计器）；以及视图空间对齐（结合2D语义与3D几何线索精确放置实例）。最终将补全视图和深度/对齐信息融合生成3D模型，评估于3D-Front。

Result: 提出了一个模块化框架，利用扩散模型分步重建单视图室内场景的背景与前景实例，实验在3D-Front数据集上优于SOTA。

Conclusion: 通过先进行视图补全（背景与被遮挡实例）再转为3D，并结合混合深度估计与视图对齐，能更好地恢复细节与几何精度，适用于室内设计、房地产与AR。

Abstract: We propose a modular framework for single-view indoor scene 3D reconstruction, where several core modules are powered by diffusion techniques. Traditional approaches for this task often struggle with the complex instance shapes and occlusions inherent in indoor environments. They frequently overshoot by attempting to predict 3D shapes directly from incomplete 2D images, which results in limited reconstruction quality. We aim to overcome this limitation by splitting the process into two steps: first, we employ diffusion-based techniques to predict the complete views of the room background and occluded indoor instances, then transform them into 3D. Our modular framework makes contributions to this field through the following components: an amodal completion module for restoring the full view of occluded instances, an inpainting model specifically trained to predict room layouts, a hybrid depth estimation technique that balances overall geometric accuracy with fine detail expressiveness, and a view-space alignment method that exploits both 2D and 3D cues to ensure precise placement of instances within the scene. This approach effectively reconstructs both foreground instances and the room background from a single image. Extensive experiments on the 3D-Front dataset demonstrate that our method outperforms current state-of-the-art (SOTA) approaches in terms of both visual quality and reconstruction accuracy. The framework holds promising potential for applications in interior design, real estate, and augmented reality.

</details>


### [7] [Enhancing Tea Leaf Disease Recognition with Attention Mechanisms and Grad-CAM Visualization](https://arxiv.org/abs/2512.17987)
*Omar Faruq Shikdar,Fahad Ahammed,B. M. Shahria Alam,Golam Kibria,Tawhidur Rahman,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 构建了5278张、7类的茶叶病害新数据集，使用DenseNet/Inception/（以及在集成中使用EfficientNet）与两种注意力机制并结合集成学习，达到85.68%准确率并引入可解释AI。


<details>
  <summary>Details</summary>
Motivation: 茶叶病害会对产量和经济造成重大损失，人工检测效率低且不可靠；因此需要自动化、准确且可解释的病害识别系统以帮助农户及时采取防治措施。

Method: 收集并标注茶叶病害图像形成新数据集；进行图像预处理；迁移学习使用预训练模型DenseNet和Inception（EfficientNet仅用于集成）；在模型中加入两种注意力模块以提升特征提取能力；构建模型集成（融合多个模型输出）；训练并评估模型性能；采用可解释AI方法（如可视化注意力图或热力图）解释模型决策。

Result: 数据集包含5278张图像、7个类别；通过预处理和训练，单模型表现良好，最终集成模型达到85.68%准确率；引入的注意力模块和集成策略提高了性能；可解释AI工具帮助理解模型关注区域。

Conclusion: 该论文提出了一个基于深度学习的自动化茶叶病害分类系统，构建了包含5278张图像的七类新数据集，采用预训练模型（DenseNet、Inception、EfficientNet）并引入两种注意力模块与集成学习，最终集成模型在数据集上取得85.68%的最高准确率，并使用可解释AI提高模型可解释性。

Abstract: Tea is among the most widely consumed drinks globally. Tea production is a key industry for many countries. One of the main challenges in tea harvesting is tea leaf diseases. If the spread of tea leaf diseases is not stopped in time, it can lead to massive economic losses for farmers. Therefore, it is crucial to identify tea leaf diseases as soon as possible. Manually identifying tea leaf disease is an ineffective and time-consuming method, without any guarantee of success. Automating this process will improve both the efficiency and the success rate of identifying tea leaf diseases. The purpose of this study is to create an automated system that can classify different kinds of tea leaf diseases, allowing farmers to take action to minimize the damage. A novel dataset was developed specifically for this study. The dataset contains 5278 images across seven classes. The dataset was pre-processed prior to training the model. We deployed three pretrained models: DenseNet, Inception, and EfficientNet. EfficientNet was used only in the ensemble model. We utilized two different attention modules to improve model performance. The ensemble model achieved the highest accuracy of 85.68%. Explainable AI was introduced for better model interpretability.

</details>


### [8] [Name That Part: 3D Part Segmentation and Naming](https://arxiv.org/abs/2512.18003)
*Soumava Paul,Prakhar Kaushik,Ankit Vaidya,Anand Bhattad,Alan Yuille*

Main category: cs.CV

TL;DR: 提出ALIGN-Parts：通过将隐式partlet与文本嵌入对齐并用二分匹配命名3D部件，实现一次性、开放词汇的跨数据集3D部件分割与命名，并构建统一本体和新数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集部件定义不一致，无法进行鲁棒有标签训练；需要一种可扩展、开放词汇且可统一不同数据集部件标签的方法。

Method: Direct set alignment of implicit partlets to text via bipartite matching; combines 3D part fields, multiview vision features, and LLM-generated affordance descriptions; trains with text-alignment loss enabling open-vocabulary matching.

Result: One-shot, efficient named 3D part segmentation and naming; supports zero-shot matching and confidence-calibrated predictions; produced unified ontology (1,794 parts) aligning multiple datasets and created Tex-Parts dataset; introduced two new evaluation metrics.

Conclusion: ALIGN-Parts enables scalable, cross-dataset, open-vocabulary 3D part naming via aligning implicit partlets with text embeddings, facilitating dataset unification and downstream annotation tasks.

Abstract: We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task.

</details>


### [9] [Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models](https://arxiv.org/abs/2512.18004)
*Shubham Kumar Nigam,Parjanya Aditya Shukla,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CV

TL;DR: 比较两种方法：传统OCR先识别再翻译 vs Vision LLMs直接翻译手写马拉地法律文本；端到端方法更简洁、易部署，但在精度和边缘情况处理上存在不足，建议混合或领域微调。


<details>
  <summary>Details</summary>
Motivation: 需要可扩展、准确的系统来数字化印度法院的法律手写记录（如FIR、笔录），以便非母语者和法律专业人员访问和处理这些低资源语言文档。

Method: 构建并评估：1) 传统OCR（训练/微调OCR模型）+ MT（神经机器翻译）流水线；2) Vision LLMs（端到端图像到目标语言）直接翻译。使用人工策划的马拉地语法律手写数据集进行实验，比较准确性、错误类型、鲁棒性与部署复杂度。

Result: 本研究比较了传统OCR+MT流水线与Vision LLMs（端到端直接从手写图像翻译）的性能，针对马拉地语法律手写文本进行评估。

Conclusion: 在低资源马拉地语法律文档上，端到端Vision LLMs在简化流程与部署上具有潜力，但在准确性和鲁棒性方面仍需与专门OCR+MT组合竞争或结合使用；具体效果取决于模型规模、训练数据和领域适配。

Abstract: Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.

</details>


### [10] [NodMAISI: Nodule-Oriented Medical AI for Synthetic Imaging](https://arxiv.org/abs/2512.18038)
*Fakrul Islam Tushar,Ehsan Samei,Cynthia Rudin,Joseph Y. Lo*

Main category: cs.CV

TL;DR: NodMAISI: a ControlNet-rectified-flow CT synthesis framework trained on 7k patients improves fidelity and lesion realism vs MAISI-v2, boosting nodule detection sensitivity and malignancy classification AUC under limited clinical data.


<details>
  <summary>Details</summary>
Motivation: Public datasets lack consistent, well-annotated small pulmonary nodules; need anatomically consistent synthetic data to augment training.

Method: Unified multi-source curation linking CTs with organ masks and nodule annotations; ControlNet-conditioned rectified-flow generator based on MAISI-v2; lesion-aware augmentation via controlled nodule mask perturbation to produce paired variants.

Result: NodMAISI

Conclusion: Anatomically constrained, lesion-aware synthesis improves realism and downstream detection/classification, especially for small nodules and low-data regimes.

Abstract: Objective: Although medical imaging datasets are increasingly available, abnormal and annotation-intensive findings critical to lung cancer screening, particularly small pulmonary nodules, remain underrepresented and inconsistently curated. Methods: We introduce NodMAISI, an anatomically constrained, nodule-oriented CT synthesis and augmentation framework trained on a unified multi-source cohort (7,042 patients, 8,841 CTs, 14,444 nodules). The framework integrates: (i) a standardized curation and annotation pipeline linking each CT with organ masks and nodule-level annotations, (ii) a ControlNet-conditioned rectified-flow generator built on MAISI-v2's foundational blocks to enforce anatomy- and lesion-consistent synthesis, and (iii) lesion-aware augmentation that perturbs nodule masks (controlled shrinkage) while preserving surrounding anatomy to generate paired CT variants. Results: Across six public test datasets, NodMAISI improved distributional fidelity relative to MAISI-v2 (real-to-synthetic FID range 1.18 to 2.99 vs 1.69 to 5.21). In lesion detectability analysis using a MONAI nodule detector, NodMAISI substantially increased average sensitivity and more closely matched clinical scans (IMD-CT: 0.69 vs 0.39; DLCS24: 0.63 vs 0.20), with the largest gains for sub-centimeter nodules where MAISI-v2 frequently failed to reproduce the conditioned lesion. In downstream nodule-level malignancy classification trained on LUNA25 and externally evaluated on LUNA16, LNDbv4, and DLCS24, NodMAISI augmentation improved AUC by 0.07 to 0.11 at <=20% clinical data and by 0.12 to 0.21 at 10%, consistently narrowing the performance gap under data scarcity.

</details>


### [11] [YolovN-CBi: A Lightweight and Efficient Architecture for Real-Time Detection of Small UAVs](https://arxiv.org/abs/2512.18046)
*Ami Pandat,Punna Rajasekhar,Gopika Vinod,Rohit Shukla*

Main category: cs.CV

TL;DR: 提出基于CBAM与BiFPN的Yolov5-CBi并用知识蒸馏得到轻量模型，提升小无人机检测精度与速度，蒸馏模型在mA@P0.5:0.9上比教师提高6.51%，速度提升82.9%。


<details>
  <summary>Details</summary>
Motivation: 提升无人机（UAV）在实际场景中对小目标的检测精度和实时性，针对此前模型在小尺寸、快速移动和低对比度目标检测上的不足，提出改进的检测架构和轻量化方案以便边缘部署。

Method: 提出Yolov5-CBi架构：在YolovN基础上引入CBAM注意力模块和BiFPN双向特征金字塔融合；构建包含28K张训练图像的数据集并收集本地2500张小尺寸无人机测试集；与多个基准数据集和模型（Yolov5基线、Yolov8、Yolov12）进行对比；设计4种CBi变体并用知识蒸馏（Yolov5m-CBi教师 -> Yolov5n-CBi学生）实现模型轻量化用于边缘实时部署。

Result: 在多个基准数据集和本地测试集上，Yolov5及Yolov5-CBi在小目标速度-精度权衡上优于Yolov8和Yolov12。蒸馏后的轻量模型mA@P0.5:0.9达到0.6573，比教师模型0.6171提高6.51%；且蒸馏模型比基线快82.9%，适合实时检测。

Conclusion: 将CBAM与BiFPN融合到Yolov5中，并通过知识蒸馏得到的轻量化模型，能显著提升小型无人机检测的准确性和推理速度，适合边缘实时部署。

Abstract: Unmanned Aerial Vehicles, commonly known as, drones pose increasing risks in civilian and defense settings, demanding accurate and real-time drone detection systems. However, detecting drones is challenging because of their small size, rapid movement, and low visual contrast. A modified architecture of YolovN called the YolovN-CBi is proposed that incorporates the Convolutional Block Attention Module (CBAM) and the Bidirectional Feature Pyramid Network (BiFPN) to improve sensitivity to small object detections. A curated training dataset consisting of 28K images is created with various flying objects and a local test dataset is collected with 2500 images consisting of very small drone objects. The proposed architecture is evaluated on four benchmark datasets, along with the local test dataset. The baseline Yolov5 and the proposed Yolov5-CBi architecture outperform newer Yolo versions, including Yolov8 and Yolov12, in the speed-accuracy trade-off for small object detection. Four other variants of the proposed CBi architecture are also proposed and evaluated, which vary in the placement and usage of CBAM and BiFPN. These variants are further distilled using knowledge distillation techniques for edge deployment, using a Yolov5m-CBi teacher and a Yolov5n-CBi student. The distilled model achieved a mA@P0.5:0.9 of 0.6573, representing a 6.51% improvement over the teacher's score of 0.6171, highlighting the effectiveness of the distillation process. The distilled model is 82.9% faster than the baseline model, making it more suitable for real-time drone detection. These findings highlight the effectiveness of the proposed CBi architecture, together with the distilled lightweight models in advancing efficient and accurate real-time detection of small UAVs.

</details>


### [12] [FOODER: Real-time Facial Authentication and Expression Recognition](https://arxiv.org/abs/2512.18057)
*Sabri Mustafa Kahya,Muhammet Sami Yavuz,Boran Hamdi Sivrikaya,Eckehard Steinbach*

Main category: cs.CV

TL;DR: Proposes FOODER: radar-only, hierarchical system combining OOD face authentication and expression recognition with strong performance (AUROC 94.13%, expr acc 94.7%)


<details>
  <summary>Details</summary>
Motivation: Enable privacy-preserving facial authentication and expression recognition using low-cost FMCW radar while detecting OOD faces to ensure safety

Method: multi-encoder multi-decoder with BP and ILED; ResNet + two MobileViT branches for dynamic/static; uses FMCW radar range-Doppler and micro range-Doppler

Result: AUROC 94.13%, FPR95 18.12% for authentication; expression accuracy 94.70%; outperforms SOTA OOD and transformer-based models; real-time operation

Conclusion: FOODER provides privacy-preserving, real-time radar-based facial authentication with effective OOD detection and high expression recognition accuracy, suitable for safe deployment

Abstract: Out-of-distribution (OOD) detection is essential for the safe deployment of neural networks, as it enables the identification of samples outside the training domain. We present FOODER, a real-time, privacy-preserving radar-based framework that integrates OOD-based facial authentication with facial expression recognition. FOODER operates using low-cost frequency-modulated continuous-wave (FMCW) radar and exploits both range-Doppler and micro range-Doppler representations. The authentication module employs a multi-encoder multi-decoder architecture with Body Part (BP) and Intermediate Linear Encoder-Decoder (ILED) components to classify a single enrolled individual as in-distribution while detecting all other faces as OOD. Upon successful authentication, an expression recognition module is activated. Concatenated radar representations are processed by a ResNet block to distinguish between dynamic and static facial expressions. Based on this categorization, two specialized MobileViT networks are used to classify dynamic expressions (smile, shock) and static expressions (neutral, anger). This hierarchical design enables robust facial authentication and fine-grained expression recognition while preserving user privacy by relying exclusively on radar data. Experiments conducted on a dataset collected with a 60 GHz short-range FMCW radar demonstrate that FOODER achieves an AUROC of 94.13% and an FPR95 of 18.12% for authentication, along with an average expression recognition accuracy of 94.70%. FOODER outperforms state-of-the-art OOD detection methods and several transformer-based architectures while operating efficiently in real time.

</details>


### [13] [FPBench: A Comprehensive Benchmark of Multimodal Large Language Models for Fingerprint Analysis](https://arxiv.org/abs/2512.18073)
*Ekta Balkrishna Gavas,Sudipta Banerjee,Chinmay Hegde,Nasir Memon*

Main category: cs.CV

TL;DR: 该论文提出FPBench基准，评估20个多模态大模型在指纹理解上的性能。通过7个真实和合成数据集、8项任务以及零样本和链式思维提示，分析性能、可解释性和局限性，为指纹领域的基础模型奠定基准。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs已在虹膜和人脸生物特征分析中被探索，但指纹理解能力尚未研究。作者构建系统性评估以填补这一空白，推动指纹领域的基础模型发展。

Method: 构建FPBench基准：选择20个开源和专有MLLMs；挑选7个真实与合成指纹数据集；定义8项生物识别/法医任务（如指纹识别、副特征描述、质量评估等）；采用零样本和链式思维提示策略进行评测；同时评估可解释性输出并记录失败模式与限制。

Result: 在多模型、多任务评估中发现模型在某些高层语义任务（如描述性解释、质量评估）表现较好，但在精确识别与细粒度比对任务上普遍欠缺；链式思维提示在可解释性方面有帮助但并不总能提升准确性；合成数据与真实数据表现转移存在差异。

Conclusion: FPBench首次系统评估MLLMs在指纹理解的能力，揭示当前模型在细粒度比对和可靠识别方面的不足，强调数据集质量、领域适配与可解释性需求，呼吁未来研究专注于指纹特定预训练、跨域鲁棒性和法律/伦理考量。

Abstract: Multimodal LLMs (MLLMs) have gained significant traction in complex data analysis, visual question answering, generation, and reasoning. Recently, they have been used for analyzing the biometric utility of iris and face images. However, their capabilities in fingerprint understanding are yet unexplored. In this work, we design a comprehensive benchmark, \textsc{FPBench} that evaluates the performance of 20 MLLMs (open-source and proprietary) across 7 real and synthetic datasets on 8 biometric and forensic tasks using zero-shot and chain-of-thought prompting strategies. We discuss our findings in terms of performance, explainability and share our insights into the challenges and limitations. We establish \textsc{FPBench} as the first comprehensive benchmark for fingerprint domain understanding with MLLMs paving the path for foundation models for fingerprints.

</details>


### [14] [Uncertainty-Gated Region-Level Retrieval for Robust Semantic Segmentation](https://arxiv.org/abs/2512.18082)
*Shreshth Rajan,Raymond Liu*

Main category: cs.CV

TL;DR: 提出一种基于区域的不确定性门控检索机制，在域移位下提高语义分割的准确性和校准性，mIoU提升11.3%，仅对12.5%区域进行检索，检索成本下降87.5%。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等场景中，需在多变环境和噪声下实时准确区分道路、行人等关键对象。通过引入检索增强信息可以提升域外（domain shift）下的分割性能，但需控制在线检索成本与延迟。

Method: 设计区域级不确定性估计器决定是否触发外部检索（记忆库或检索器）以获取补充特征/标签信息；只有高不确定性区域才检索，从而显著降低检索频率。结合检索到的上下文信息与原始分割模型进行融合，提升预测和校准。

Result: 在目标域上实现mIoU提升11.3%，同时检索次数仅为总区域的12.5%，相比始终检索（100%区域）降低检索成本87.5%，并改善校准指标。

Conclusion: The paper presents a region-level uncertainty-gated retrieval mechanism that improves semantic segmentation accuracy and calibration under domain shift, achieving significant IoU gains and major retrieval cost reduction.

Abstract: Semantic segmentation of outdoor street scenes plays a key role in applications such as autonomous driving, mobile robotics, and assistive technology for visually-impaired pedestrians. For these applications, accurately distinguishing between key surfaces and objects such as roads, sidewalks, vehicles, and pedestrians is essential for maintaining safety and minimizing risks. Semantic segmentation must be robust to different environments, lighting and weather conditions, and sensor noise, while being performed in real-time. We propose a region-level, uncertainty-gated retrieval mechanism that improves segmentation accuracy and calibration under domain shift. Our best method achieves an 11.3% increase in mean intersection-over-union while reducing retrieval cost by 87.5%, retrieving for only 12.5% of regions compared to 100% for always-on baseline.

</details>


### [15] [SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping](https://arxiv.org/abs/2512.18128)
*Thomas Boudras,Martin Schwartz,Rasmus Fensholt,Martin Brandt,Ibrahim Fayad,Jean-Pierre Wigneron,Gabriel Belouze,Fajwel Fogel,Philippe Ciais*

Main category: cs.CV

TL;DR: 提出SERA-H：结合EDSR超分辨率和UTAE时间注意力，用Sentinel-1/2时序并以ALS高密度LiDAR监督，生成2.5 m林冠高度图，在法国基准上MAE=2.6m，R^2=0.82，优于常规模型并可比商业超高分辨率影像方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于卫星影像的林高预测在数据可得性与空间分辨率间存在权衡；研究旨在用可免费获取的Sentinel时序数据与高分辨率LiDAR监督，突破输入影像原生分辨率限制，实现高精度、高分辨率林冠高度映射。

Method: 提出端到端模型SERA-H：组合EDSR超分辨率模块与UTAE时间注意力编码；以Sentinel-1（雷达）和Sentinel-2（光学，10m）时序为输入，采用ALS（高密度机载LiDAR）点云作为高分辨率监督，训练模型直接输出2.5m分辨率高度图。

Result: 在法国开源基准数据集上，SERA-H达成MAE=2.6m，R^2=0.82，优于基于Sentinel-1/2的标准基线，且与依赖SPOT-6/7、PlanetScope、Maxar等商业超高分辨率影像的方法表现相当或更优，表明结合高分辨率监督与时序信息能重建细节，支持以免费数据实现高精度林冠高度映射。

Conclusion: SERA-H通过将超分辨率模块(EDSR)与时间注意力编码(UTAE)结合，在仅使用Sentinel-1/2时序数据并以高密度ALS点云为监督的情况下，成功生成2.5 m分辨率的林冠高度图，达到了与使用商业超高分辨率影像的方法相当或更优的性能，证明了时空信息与高分辨率监督相结合可以重建超出输入传感器原生分辨率的细节。

Abstract: High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors' native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery. The source code is available at https://github.com/ThomasBoudras/SERA-H#

</details>


### [16] [EndoStreamDepth: Temporally Consistent Monocular Depth Estimation for Endoscopic Video Streams](https://arxiv.org/abs/2512.18159)
*Hao Li,Daiwei Lu,Jiacheng Wang,Robert J. Webster,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出EndoStreamDepth：单帧网络+多层Mamba时间模块+多尺度分层监督，显著提升结肠镜深度估计的边界清晰度与时序稳定性，实时可用，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法多用批处理输入或仅关注单帧，导致深度边界钝化或帧间抖动，难以满足机器人手术等需精确、连续几何信息的下游应用。作者希望在保持实时性的同时提升边界精细度和时间一致性。

Method: 1) 设计针对内镜影像的单帧深度网络和特定变换以提升单帧精度；2) 引入多层级（multi-level）Mamba时间模块，在不同特征尺度上传播帧间信息以稳定预测并利用上下文；3) 分层多尺度监督与互补损失（兼顾局部边界与全局几何一致性）联合训练以提升边界清晰度和整体几何准确性。系统对单帧逐帧处理并在时间模块中融合历史信息，避免了批处理的局限，支持实时推理。

Result: 在两个公开结肠镜深度数据集上，EndoStreamDepth较现有单目深度方法显著提升了性能，尤其是在边界对齐与时序一致性方面表现突出，并能实时运行，代码已开源。

Conclusion: EndoStreamDepth在内窥镜单目深度估计上实现了帧内精确边界与帧间一致性的平衡，兼顾实时性，适合手术自动化下游任务。

Abstract: This work presents EndoStreamDepth, a monocular depth estimation framework for endoscopic video streams. It provides accurate depth maps with sharp anatomical boundaries for each frame, temporally consistent predictions across frames, and real-time throughput. Unlike prior work that uses batched inputs, EndoStreamDepth processes individual frames with a temporal module to propagate inter-frame information. The framework contains three main components: (1) a single-frame depth network with endoscopy-specific transformation to produce accurate depth maps, (2) multi-level Mamba temporal modules that leverage inter-frame information to improve accuracy and stabilize predictions, and (3) a hierarchical design with comprehensive multi-scale supervision, where complementary loss terms jointly improve local boundary sharpness and global geometric consistency. We conduct comprehensive evaluations on two publicly available colonoscopy depth estimation datasets. Compared to state-of-the-art monocular depth estimation methods, EndoStreamDepth substantially improves performance, and it produces depth maps with sharp, anatomically aligned boundaries, which are essential to support downstream tasks such as automation for robotic surgery. The code is publicly available at https://github.com/MedICL-VU/EndoStreamDepth

</details>


### [17] [Local Patches Meet Global Context: Scalable 3D Diffusion Priors for Computed Tomography Reconstruction](https://arxiv.org/abs/2512.18161)
*Taewon Yang,Jason Hu,Jeffrey A. Fessler,Liyue Shen*

Main category: cs.CV

TL;DR: 提出位置感知的3D patch-based扩散先验，耦合局部patch与下采样全局体积，实现高效高分辨率3D CT生成与重建，优于SOTA且能重建512x512x256体积（约20min）。


<details>
  <summary>Details</summary>
Motivation: 直接在3D数据上训练扩散模型代价高昂，现有方法多复用2D先验无法充分发挥扩散模型对高维数据的生成能力。需要一种在有限数据与计算资源下学习完整3D先验的方案，用于高分辨率3D图像生成与逆问题。

Method: 提出3D patch-based扩散模型：以带位置信息的3D局部patch作为建模单位，利用下采样的全局3D体积作为全局上下文，学习两者的联合分布，从而耦合局部与全局信息，实现可扩展的高分辨率3D生成与快速逆问题求解。

Result: 提出了一种3D基于patch的扩散模型，能在有限数据与计算下学习完整的3D先验并生成高分辨率3D图像。核心在于建模带位置信息的3D局部patch与下采样全局体积的联合分布，从而结合局部与全局信息。应用于3D CT重建，实验在多个数据集上优于现有方法，能高效重建512x512x256体积（约20分钟）。

Conclusion: 通过学习位置感知的3D局部patch与下采样全局上下文的联合先验，能在资源受限下实现高质量、高分辨率的3D生成与逆问题求解，显著提升性能与效率。

Abstract: Diffusion models learn strong image priors that can be leveraged to solve inverse problems like medical image reconstruction. However, for real-world applications such as 3D Computed Tomography (CT) imaging, directly training diffusion models on 3D data presents significant challenges due to the high computational demands of extensive GPU resources and large-scale datasets. Existing works mostly reuse 2D diffusion priors to address 3D inverse problems, but fail to fully realize and leverage the generative capacity of diffusion models for high-dimensional data. In this study, we propose a novel 3D patch-based diffusion model that can learn a fully 3D diffusion prior from limited data, enabling scalable generation of high-resolution 3D images. Our core idea is to learn the prior of 3D patches to achieve scalable efficiency, while coupling local and global information to guarantee high-quality 3D image generation, by modeling the joint distribution of position-aware 3D local patches and downsampled 3D volume as global context. Our approach not only enables high-quality 3D generation, but also offers an unprecedentedly efficient and accurate solution to high-resolution 3D inverse problems. Experiments on 3D CT reconstruction across multiple datasets show that our method outperforms state-of-the-art methods in both performance and efficiency, notably achieving high-resolution 3D reconstruction of $512 \times 512 \times 256$ ($\sim$20 mins).

</details>


### [18] [Atlas is Your Perfect Context: One-Shot Customization for Generalizable Foundational Medical Image Segmentation](https://arxiv.org/abs/2512.18176)
*Ziyu Zhang,Yi Yu,Simeng Zhu,Ahmed Aly,Yunhe Gao,Ning Gu,Yuan Xue*

Main category: cs.CV

TL;DR: AtlasSegFM 用一个带注释的atlas通过配准生成提示，并在测试时融合配准与基础模型预测，实现一次标注定制，显著提升医学图像分割（对小结构效果尤佳）。


<details>
  <summary>Details</summary>
Motivation: 尽管交互式基础模型通过大规模多模态预训练提升了泛化能力，但在训练数据欠代表的临床情境下仍依赖精确提示且性能不足；因此需要一种能用少量标注（一次示例）便将基础模型定制到特定临床上下文的方案。

Method: 1) 通过将一个上下文atlas与查询图像进行配准，生成针对该查询的上下文感知提示供基础模型使用；2) 在测试时使用适配器融合来自atlas配准的分割与基础模型的预测，以弥补各自弱点并提升最终分割质量。

Result: AtlasSegFM 提出了一种基于atlas引导的一次标注定制框架，用于改进交互式基础模型在医学图像分割任务中的表现。通过将一个上下文atlas与待测图像配准，生成上下文感知的提示并在测试时融合atlas配准与基础模型的预测，从而在一次标注的设置下提升分割准确性。

Conclusion: AtlasSegFM 在多模态、多器官的公开与内部数据集上显著提高了分割性能，尤其在小而精细结构上效果明显，为在临床工作流中轻量级部署基础模型的一次定制提供了可行方案。

Abstract: Accurate medical image segmentation is essential for clinical diagnosis and treatment planning. While recent interactive foundation models (e.g., nnInteractive) enhance generalization through large-scale multimodal pretraining, they still depend on precise prompts and often perform below expectations in contexts that are underrepresented in their training data. We present AtlasSegFM, an atlas-guided framework that customizes available foundation models to clinical contexts with a single annotated example. The core innovations are: 1) a pipeline that provides context-aware prompts for foundation models via registration between a context atlas and query images, and 2) a test-time adapter to fuse predictions from both atlas registration and the foundation model. Extensive experiments across public and in-house datasets spanning multiple modalities and organs demonstrate that AtlasSegFM consistently improves segmentation, particularly for small, delicate structures. AtlasSegFM provides a lightweight, deployable solution one-shot customization of foundation models in real-world clinical workflows. The code will be made publicly available.

</details>


### [19] [MACE-Dance: Motion-Appearance Cascaded Experts for Music-Driven Dance Video Generation](https://arxiv.org/abs/2512.18181)
*Kaixing Yang,Jiashu Zhu,Xulong Tang,Ziqiao Peng,Xiangyue Zhang,Puwei Wang,Jiahong Wu,Xiangxiang Chu,Hongyan Liu,Jun He*

Main category: cs.CV

TL;DR: 提出MACE-Dance，一种级联Mixture-of-Experts框架，实现音乐驱动的舞蹈视频生成。Motion Expert负责从音乐生成3D动作（使用扩散模型、BiMamba-Transformer和无引导训练策略）以保证运动合理性和艺术表现力；Appearance Expert负责基于动作和参考图像合成视频（采用解耦运动-美学微调）以保留视觉身份和时空一致性。作者构建了大规模数据集和评估协议，在多个子任务上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 在线舞蹈视频平台兴起与AIGC发展推动音乐驱动舞蹈生成需求，但现有相关领域方法无法直接迁移，且现有研究难以同时兼顾高质量视觉外观与真实人类运动。

Method: MACE-Dance由两级专家组成：Motion Expert使用扩散模型结合BiMamba-Transformer混合架构及Guidance-Free Training（GFT）进行音乐到3D动作生成，强调运动学合理性与艺术性；Appearance Expert进行运动与参考条件的视频合成，采用解耦的运动-美学微调策略以维持身份与时空一致性。整体采用级联MoE设计，分别优化动作与外观模块。作者还构建大规模数据集并设计动作-外观评估协议。

Result: 在构建的数据集与评估协议上，MACE-Dance在3D舞蹈生成、基于姿态的图像动画和整体音乐驱动舞蹈视频生成三项任务均达到或超过现有SOTA性能。

Conclusion: 提出的级联MoE框架有效解耦并优化动作生成与视觉合成两大子任务，显著提升音乐驱动舞蹈视频生成的动作合理性与视觉质量，为该领域提供数据集和评估基准。

Abstract: With the rise of online dance-video platforms and rapid advances in AI-generated content (AIGC), music-driven dance generation has emerged as a compelling research direction. Despite substantial progress in related domains such as music-driven 3D dance generation, pose-driven image animation, and audio-driven talking-head synthesis, existing methods cannot be directly adapted to this task. Moreover, the limited studies in this area still struggle to jointly achieve high-quality visual appearance and realistic human motion. Accordingly, we present MACE-Dance, a music-driven dance video generation framework with cascaded Mixture-of-Experts (MoE). The Motion Expert performs music-to-3D motion generation while enforcing kinematic plausibility and artistic expressiveness, whereas the Appearance Expert carries out motion- and reference-conditioned video synthesis, preserving visual identity with spatiotemporal coherence. Specifically, the Motion Expert adopts a diffusion model with a BiMamba-Transformer hybrid architecture and a Guidance-Free Training (GFT) strategy, achieving state-of-the-art (SOTA) performance in 3D dance generation. The Appearance Expert employs a decoupled kinematic-aesthetic fine-tuning strategy, achieving state-of-the-art (SOTA) performance in pose-driven image animation. To better benchmark this task, we curate a large-scale and diverse dataset and design a motion-appearance evaluation protocol. Based on this protocol, MACE-Dance also achieves state-of-the-art performance. Project page: https://macedance.github.io/

</details>


### [20] [Is There a Better Source Distribution than Gaussian? Exploring Source Distributions for Image Flow Matching](https://arxiv.org/abs/2512.18184)
*Junho Lee,Kwanseok Kim,Joonseok Lee*

Main category: cs.CV

TL;DR: They analyze flow matching in a 2D simulation uncovering issues with density approximation, directional entanglement, and norm misalignment; propose norm-aligned training and directionally-pruned sampling that boosts performance and efficiency and can be applied post hoc to Gaussian-trained models


<details>
  <summary>Details</summary>
Motivation: Explore alternative source distributions beyond Gaussian for flow matching and understand learning dynamics in high-dimensional settings via an interpretable 2D simulation

Method: Construct 2D simulation capturing high-dimensional geometry; analyze training dynamics; derive insights; propose norm-aligned training and directionally-pruned sampling applicable post-training; evaluate empirically

Result: Paper proposes analysis and practical improvements for flow matching source distributions

Conclusion: Norm-aligned training + directional pruning improves generation quality and sampling efficiency without retraining; Gaussian source remains robust; pruning applies to Gaussian-trained models

Abstract: Flow matching has emerged as a powerful generative modeling approach with flexible choices of source distribution. While Gaussian distributions are commonly used, the potential for better alternatives in high-dimensional data generation remains largely unexplored. In this paper, we propose a novel 2D simulation that captures high-dimensional geometric properties in an interpretable 2D setting, enabling us to analyze the learning dynamics of flow matching during training. Based on this analysis, we derive several key insights about flow matching behavior: (1) density approximation can paradoxically degrade performance due to mode discrepancy, (2) directional alignment suffers from path entanglement when overly concentrated, (3) Gaussian's omnidirectional coverage ensures robust learning, and (4) norm misalignment incurs substantial learning costs. Building on these insights, we propose a practical framework that combines norm-aligned training with directionally-pruned sampling. This approach maintains the robust omnidirectional supervision essential for stable flow learning, while eliminating initializations in data-sparse regions during inference. Importantly, our pruning strategy can be applied to any flow matching model trained with a Gaussian source, providing immediate performance gains without the need for retraining. Empirical evaluations demonstrate consistent improvements in both generation quality and sampling efficiency. Our findings provide practical insights and guidelines for source distribution design and introduce a readily applicable technique for improving existing flow matching models. Our code is available at https://github.com/kwanseokk/SourceFM.

</details>


### [21] [ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection](https://arxiv.org/abs/2512.18187)
*Janghyun Baek,Mincheol Chang,Seokha Moon,Seung Joon Lee,Jinkyu Kim*

Main category: cs.CV

TL;DR: ALIGN: object-aware query initialization using LiDAR+image (OCE, ANS, DQB) improves nuScenes mAP by up to +0.9 and NDS by +1.2, especially for occluded/dense scenes.


<details>
  <summary>Details</summary>
Motivation: Existing query-based 3D detectors initialize queries via random sampling or BEV heatmap-based sampling, leading to inefficient use and poor detection for occluded/crowded objects. Need object-aware, occlusion-robust initialization to improve query utilization and accuracy.

Method: Propose ALIGN with three components: (i) Occlusion-aware Center Estimation (OCE) combining LiDAR geometry and image semantics to estimate centers; (ii) Adaptive Neighbor Sampling (ANS) generating object candidates from LiDAR clustering and sampling spatially/semantically aligned points around each; (iii) Dynamic Query Balancing (DQB) adaptively balances queries between foreground and background. Integrate into existing query-based detectors.

Result: On nuScenes ALIGN yields consistent improvements across multiple SOTA detectors, up to +0.9 mAP and +1.2 NDS, with larger gains in occluded/crowded scenes.

Conclusion: ALIGN provides an effective, plug-and-play query initialization that enhances occlusion robustness and detection accuracy; code to be released.

Abstract: Recent query-based 3D object detection methods using camera and LiDAR inputs have shown strong performance, but existing query initialization strategies,such as random sampling or BEV heatmap-based sampling, often result in inefficient query usage and reduced accuracy, particularly for occluded or crowded objects. To address this limitation, we propose ALIGN (Advanced query initialization with LiDAR and Image GuidaNce), a novel approach for occlusion-robust, object-aware query initialization. Our model consists of three key components: (i) Occlusion-aware Center Estimation (OCE), which integrates LiDAR geometry and image semantics to estimate object centers accurately (ii) Adaptive Neighbor Sampling (ANS), which generates object candidates from LiDAR clustering and supplements each object by sampling spatially and semantically aligned points around it and (iii) Dynamic Query Balancing (DQB), which adaptively balances queries between foreground and background regions. Our extensive experiments on the nuScenes benchmark demonstrate that ALIGN consistently improves performance across multiple state-of-the-art detectors, achieving gains of up to +0.9 mAP and +1.2 NDS, particularly in challenging scenes with occlusions or dense crowds. Our code will be publicly available upon publication.

</details>


### [22] [Multi-Part Object Representations via Graph Structures and Co-Part Discovery](https://arxiv.org/abs/2512.18192)
*Alex Foo,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: 提出用部件图的共部件对象发现算法，在三个新基准上相比SOTA表现更好，尤其在遮挡和OOD场景。


<details>
  <summary>Details</summary>
Motivation: 现有隐式对象表示方法未能可靠编码部件与整体的关系，导致在遮挡或分布外场景中识别失败；因此需要显式的部件图来学习稳健的对象中心表征。

Method: 构建显式图表示表示部件节点与其关系，提出co-part算法对图结构进行学习与聚类以发现对象，并在训练/评估中引入有针对性的遮挡与OOD基准。

Result: 该论文提出了基于显式部件图的对象发现方法，针对图像中多部件对象的鲁棒性和泛化性问题，提出了共部件（co-part）对象发现算法，并设计了三个基准用于评估在遮挡和分布外情景下的识别能力。实验在模拟、真实感和真实世界图像上显示相比最先进方法有明显提升，可更准确地预测下游任务的关键物体属性。

Conclusion: 显式建模部件间的图结构能更好地捕捉部件—整体关系，从而提升在遮挡与分布外场景下的对象发现与识别性能，且学到的表征在下游任务上更有用。

Abstract: Discovering object-centric representations from images can significantly enhance the robustness, sample efficiency and generalizability of vision models. Works on images with multi-part objects typically follow an implicit object representation approach, which fail to recognize these learned objects in occluded or out-of-distribution contexts. This is due to the assumption that object part-whole relations are implicitly encoded into the representations through indirect training objectives. We address this limitation by proposing a novel method that leverages on explicit graph representations for parts and present a co-part object discovery algorithm. We then introduce three benchmarks to evaluate the robustness of object-centric methods in recognizing multi-part objects within occluded and out-of-distribution settings. Experimental results on simulated, realistic, and real-world images show marked improvements in the quality of discovered objects compared to state-of-the-art methods, as well as the accurate recognition of multi-part objects in occluded and out-of-distribution contexts. We also show that the discovered object-centric representations can more accurately predict key object properties in a downstream task, highlighting the potential of our method to advance the field of object-centric representations.

</details>


### [23] [Unsupervised Anomaly Detection with an Enhanced Teacher for Student-Teacher Feature Pyramid Matching](https://arxiv.org/abs/2512.18219)
*Mohammad Zolfaghari,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 论文通过对教师网络（ResNet-18）进行ImageNet预训练后在目标数据集微调，构建ET-STPM模型，在MVTech-AD上实现了图像级0.971和像素级0.977的高性能异常检测。


<details>
  <summary>Details</summary>
Motivation: 提升学生-教师异常检测框架中教师特征表达能力，以提高整个系统在图像级和像素级异常检测的准确性。

Method: 使用ResNet-18作为教师网络，先在ImageNet上预训练，再在MVTech-AD数据集上微调；学生网络与教师构成学生-教师特征金字塔（STPM），提出增强教师（Enhanced Teacher）版本ET-STPM用于异常评分与定位。

Result: 在MVTech-AD数据集上，ET-STPM在图像级Achieved mean accuracy 0.971，在像素级Achieved mean accuracy 0.977；相较于先前方法表现更好。

Conclusion: 该论文提出通过增强教师网络的学生-教师框架提升无监督异常检测性能，利用在ImageNet预训练并在MVTech-AD上微调的ResNet-18作为教师，实验在图像级和像素级上分别取得0.971和0.977的平均精度。

Abstract: Anomaly detection or outlier is one of the challenging subjects in unsupervised learning . This paper is introduced a student-teacher framework for anomaly detection that its teacher network is enhanced for achieving high-performance metrics . For this purpose , we first pre-train the ResNet-18 network on the ImageNet and then fine-tune it on the MVTech-AD dataset . Experiment results on the image-level and pixel-level demonstrate that this idea has achieved better metrics than the previous methods . Our model , Enhanced Teacher for Student-Teacher Feature Pyramid (ET-STPM), achieved 0.971 mean accuracy on the image-level and 0.977 mean accuracy on the pixel-level for anomaly detection.

</details>


### [24] [Multifaceted Exploration of Spatial Openness in Rental Housing: A Big Data Analysis in Tokyo's 23 Wards](https://arxiv.org/abs/2512.18226)
*Takuya OKi,Yuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一个基于2D平面能见度(VGA)与3D室内语义分割(Mask2Former)的多维度量化框架，用于评估住宅空间的开放性，并基于东京23区4004套出租房数据分析其时空变化及与租金和房屋属性的关系。结果显示：客厅能见度上升、整体开放性在1990年代达到峰值；2D与3D指标相关性弱但高开放性通常对应更高租金；印象分与开放性关系较弱，受室内设计与家具影响更大。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将影响空间开放性的因素单独处理，缺乏将平面与立体视角结合的量化框架，难以全面把握住宅开放性如何随时间、空间与市场属性变化。作者旨在构建可量化、可扩展的多维评估方法，并将其与租金和建筑特征等实际变量关联。

Method: 样本：东京23区共4004套出租房的平面图與室内照片；2D指标：使用可视图分析(VGA)在平面图上计算客厅与整体能见度；3D指标：用Mask2Former对室内图像做语义分割，识别墙、天花板、地板、窗等并据此估算立体开放度；时空分析：按建成年代与地理位置统计开放性变化；相关分析：检验开放性与租金、楼龄、楼高等建筑属性的偏相关并作空间可视化；印象分比较：将既有模型预测的主观印象分与开放性指标对比。

Result: 客厅能见度有上升趋势，整体开放性在1990年代达到峰值；空间分布表明开放性与租金和建筑特征存在部分相关，反映城市重建与市场变化；2D与3D开放性指标间相关性低，但高开放性样本往往租金更高；既有印象分模型与开放性指标仅弱相关，说明家具与室内设计对感知空间影响更大。

Conclusion: 该研究提供了一个结合2D/3D数据驱动的住宅开放性量化框架，能揭示时空演变与市场关联，具有为城市设计、住房政策与房地产评估提供证据支持的潜力，同时提示未来研究需进一步整合家具、布局与主观评价。

Abstract: Understanding spatial openness is vital for improving residential quality and design; however, studies often treat its influencing factors separately. This study developed a quantitative framework to evaluate the spatial openness in housing from two- (2D) and three- (3D) dimensional perspectives. Using data from 4,004 rental units in Tokyo's 23 wards, we examined the temporal and spatial variations in openness and its relationship with rent and housing attributes. 2D openness was computed via planar visibility using visibility graph analysis (VGA) from floor plans, whereas 3D openness was derived from interior images analysed using Mask2Former, a semantic segmentation model that identifies walls, ceilings, floors, and windows. The results showed an increase in living room visibility and a 1990s peak in overall openness. Spatial analyses revealed partial correlations among openness, rent, and building characteristics, reflecting urban redevelopment trends. Although the 2D and 3D openness indicators were not directly correlated, higher openness tended to correspond to higher rent. The impression scores predicted by the existing models were only weakly related to openness, suggesting that the interior design and furniture more strongly shape perceived space. This study offers a new multidimensional data-driven framework for quantifying residential spatial openness and linking it with urban and market dynamics.

</details>


### [25] [Investigating Spatial Attention Bias in Vision-Language Models](https://arxiv.org/abs/2512.18231)
*Aryan Chaudhary,Sanchit Goyal,Pratik Narang,Dhruv Kumar*

Main category: cs.CV

TL;DR: VLMs consistently describe left-positioned content before right-positioned content (~97% under neutral prompts); bias persists across models and even with Arabic-finetuned models; dataset guidelines lack explicit left-first instruction—suggests architectural origin.


<details>
  <summary>Details</summary>
Motivation: To uncover whether VLMs have systematic spatial biases in processing horizontally arranged visual content and to determine causes (language reading direction, dataset instructions, or architecture).

Method: Controlled experiments using horizontally concatenated image pairs across multiple open-source and closed-source VLMs; neutral prompting; testing Arabic-finetuned model; reviewing annotation guidelines of PixMo and Visual Genome.

Result: Models describe left-positioned content first in ~97% of cases; bias remains in Arabic-finetuned model; dataset guidelines show no explicit left-first ordering; indicates bias likely from model architecture or training processes.

Conclusion: VLMs exhibit a robust left-first spatial attention bias when describing horizontally concatenated images, persisting across architectures and unaffected by language reading direction or explicit dataset ordering, indicating architectural or training-process sources.

Abstract: Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.

</details>


### [26] [Joint Learning of Depth, Pose, and Local Radiance Field for Large Scale Monocular 3D Reconstruction](https://arxiv.org/abs/2512.18237)
*Shahram Najam Syed,Yitian Hu,Yuchao Yao*

Main category: cs.CV

TL;DR: Use metric ViT depth, feature BA, and incremental hash-grid NeRFs to solve depth scale, pose drift, and scalability for large-scale monocular reconstruction


<details>
  <summary>Details</summary>
Motivation: Monocular NeRF reconstructions fail at scale due to depth scale ambiguity, pose drift, and single global NeRF limits; joint learning of depth, pose, radiance addresses these issues.

Method: Joint framework: metric-scale ViT depth + multi-scale feature BA + incremental local hash-grid NeRFs

Result: Globally consistent metric depths; drift-free poses with feature BA; scalable city-block radiance fields enabling single-GPU large scenes

Conclusion: Coupling metric depth, feature-space BA, and local radiance hierarchies yields accurate, drift-free, large-scale photorealistic 3D reconstructions from monocular video

Abstract: Photorealistic 3-D reconstruction from monocular video collapses in large-scale scenes when depth, pose, and radiance are solved in isolation: scale-ambiguous depth yields ghost geometry, long-horizon pose drift corrupts alignment, and a single global NeRF cannot model hundreds of metres of content. We introduce a joint learning framework that couples all three factors and demonstrably overcomes each failure case. Our system begins with a Vision-Transformer (ViT) depth network trained with metric-scale supervision, giving globally consistent depths despite wide field-of-view variations. A multi-scale feature bundle-adjustment (BA) layer refines camera poses directly in feature space--leveraging learned pyramidal descriptors instead of brittle keypoints--to suppress drift on unconstrained trajectories. For scene representation, we deploy an incremental local-radiance-field hierarchy: new hash-grid NeRFs are allocated and frozen on-the-fly when view overlap falls below a threshold, enabling city-block-scale coverage on a single GPU. Evaluated on the Tanks and Temples benchmark, our method reduces Absolute Trajectory Error to 0.001-0.021 m across eight indoor-outdoor sequences--up to 18x lower than BARF and 2x lower than NoPe-NeRF--while maintaining sub-pixel Relative Pose Error. These results demonstrate that metric-scale, drift-free 3-D reconstruction and high-fidelity novel-view synthesis are achievable from a single uncalibrated RGB camera.

</details>


### [27] [SG-RIFE: Semantic-Guided Real-Time Intermediate Flow Estimation with Diffusion-Competitive Perceptual Quality](https://arxiv.org/abs/2512.18241)
*Pan Ben Wong,Chengli Wu,Hanyue Lu*

Main category: cs.CV

TL;DR: 在预训练RIFE上注入DINOv3语义，使用Split-FAPM和DSF模块进行特征压缩与对齐，显著提升感知质量并在保持低延迟下超越或接近扩散方法。


<details>
  <summary>Details</summary>
Motivation: 解决实时视频帧插值(VFI)中流式方法在复杂场景（大位移、遮挡）下感知质量不佳，而扩散方法感知质量虽高但延迟过大无法实时应用的矛盾。

Method: 提出SG-RIFE：基于预训练RIFE主干进行参数高效微调，注入来自冻结的DINOv3 ViT语义先验。设计了Split-Fidelity Aware Projection Module(Split-FAPM)用于压缩并优化高维特征，以及Deformable Semantic Fusion(DSF)模块用于将语义先验与像素级运动场对齐。

Result: 在SNU-FILM上实验表明语义注入显著提升感知保真度。SG-RIFE在FID/LPIPS上优于扩散基LDMVFI，并在复杂基准上达到与Consec. BB可比的质量，同时运行速度显著更快。

Conclusion: 通过语义一致性引入，基于光流的方法可以在近实时情况下达到与扩散方法相当的感知质量，弥合了速度与感知质量之间的差距。

Abstract: Real-time Video Frame Interpolation (VFI) has long been dominated by flow-based methods like RIFE, which offer high throughput but often fail in complicated scenarios involving large motion and occlusion. Conversely, recent diffusion-based approaches (e.g., Consec. BB) achieve state-of-the-art perceptual quality but suffer from prohibitive latency, rendering them impractical for real-time applications. To bridge this gap, we propose Semantic-Guided RIFE (SG-RIFE). Instead of training from scratch, we introduce a parameter-efficient fine-tuning strategy that augments a pre-trained RIFE backbone with semantic priors from a frozen DINOv3 Vision Transformer. We propose a Split-Fidelity Aware Projection Module (Split-FAPM) to compress and refine high-dimensional features, and a Deformable Semantic Fusion (DSF) module to align these semantic priors with pixel-level motion fields. Experiments on SNU-FILM demonstrate that semantic injection provides a decisive boost in perceptual fidelity. SG-RIFE outperforms diffusion-based LDMVFI in FID/LPIPS and achieves quality comparable to Consec. BB on complex benchmarks while running significantly faster, proving that semantic consistency enables flow-based methods to achieve diffusion-competitive perceptual quality in near real-time.

</details>


### [28] [Spectral Discrepancy and Cross-modal Semantic Consistency Learning for Object Detection in Hyperspectral Image](https://arxiv.org/abs/2512.18245)
*Xiao He,Chang Tang,Xinwang Liu,Wei Zhang,Zhimin Gao,Chuankun Li,Shaohua Qiu,Jiangfeng Xu*

Main category: cs.CV

TL;DR: 提出SDCM模型，通过SCL、SGG和SDA模块减少光谱通道不一致与冗余，提高高光谱图像目标检测性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像虽能区分相似物质，但受不同波段的空间差异、传感器噪声和照明等干扰，导致类内/类间相似性高，影响目标检测效果，需要减少通道不一致和冗余并挖掘有区分性的光谱信息。

Method: 引入SCL模块利用跨波段上下文一致化信息，SGG根据波段重要性门控去除冗余，SDA提取像素级光谱特征以增强高层语义表示，整体形成SDCM网络同时利用光谱维度定位感兴趣区域。

Result: 在两个高光谱数据集上进行的大量实验表明，SDCM在目标检测任务上优于其他对比方法，取得了最先进的性能。

Conclusion: The paper proposes SDCM, combining semantic consistency learning, spectral gated generator, and spectral discrepancy aware module to handle inter-band inconsistencies and redundancy in hyperspectral object detection, achieving SOTA on two datasets.

Abstract: Hyperspectral images with high spectral resolution provide new insights into recognizing subtle differences in similar substances. However, object detection in hyperspectral images faces significant challenges in intra- and inter-class similarity due to the spatial differences in hyperspectral inter-bands and unavoidable interferences, e.g., sensor noises and illumination. To alleviate the hyperspectral inter-bands inconsistencies and redundancy, we propose a novel network termed \textbf{S}pectral \textbf{D}iscrepancy and \textbf{C}ross-\textbf{M}odal semantic consistency learning (SDCM), which facilitates the extraction of consistent information across a wide range of hyperspectral bands while utilizing the spectral dimension to pinpoint regions of interest. Specifically, we leverage a semantic consistency learning (SCL) module that utilizes inter-band contextual cues to diminish the heterogeneity of information among bands, yielding highly coherent spectral dimension representations. On the other hand, we incorporate a spectral gated generator (SGG) into the framework that filters out the redundant data inherent in hyperspectral information based on the importance of the bands. Then, we design the spectral discrepancy aware (SDA) module to enrich the semantic representation of high-level information by extracting pixel-level spectral features. Extensive experiments on two hyperspectral datasets demonstrate that our proposed method achieves state-of-the-art performance when compared with other ones.

</details>


### [29] [Towards Ancient Plant Seed Classification: A Benchmark Dataset and Baseline Model](https://arxiv.org/abs/2512.18247)
*Rui Xing,Runmin Cong,Yingying Wu,Can Wang,Zhongming Tang,Fen Wang,Hao Wu,Sam Kwong*

Main category: cs.CV

TL;DR: 构建APS数据集（8340张、17类），提出APSNet（SPE用于尺度信息，ADD用于异步解码），在古植物种子细粒度分类上达90.5%。


<details>
  <summary>Details</summary>
Motivation: 传统考古植物学依赖专家知识，耗时低效；现有智能方法在考古学有进展，但在考古植物学的种子分类上缺乏数据集与专门方法，因此需要构建数据集并设计针对性的网络以提高分类效率和准确性。

Method: 1) 构建APS数据集，包含17个属/种级类别的种子图像；2) 设计APSNet：在编码器中加入SPE模块以显式获取种子尺度信息并嵌入特征；3) 提出ADD解码架构，基于渐进学习设计异步解耦的通道和空间解码路径，以更有效地学习判别性细粒度特征；4) 在数据集上进行训练与对比评测。

Result: 模型在APS数据集上取得90.5%准确率，定量和定性分析均显示优于现有主流图像分类方法，表明方法能有效为大型系统化考古研究提供工具。

Conclusion: 本文构建了首个古植物种子图像分类数据集APS，包含8340张来自中国18处遗址的17类种子图像，并提出APSNet框架。通过在编码器中加入Size Perception and Embedding (SPE)模块显式提取种子尺度信息，及基于渐进学习的Asynchronous Decoupled Decoding (ADD)解码结构从通道和空间两方面解码特征，模型在细粒度分类任务中取得了90.5%的准确率，优于现有方法。

Abstract: Understanding the dietary preferences of ancient societies and their evolution across periods and regions is crucial for revealing human-environment interactions. Seeds, as important archaeological artifacts, represent a fundamental subject of archaeobotanical research. However, traditional studies rely heavily on expert knowledge, which is often time-consuming and inefficient. Intelligent analysis methods have made progress in various fields of archaeology, but there remains a research gap in data and methods in archaeobotany, especially in the classification task of ancient plant seeds. To address this, we construct the first Ancient Plant Seed Image Classification (APS) dataset. It contains 8,340 images from 17 genus- or species-level seed categories excavated from 18 archaeological sites across China. In addition, we design a framework specifically for the ancient plant seed classification task (APSNet), which introduces the scale feature (size) of seeds based on learning fine-grained information to guide the network in discovering key "evidence" for sufficient classification. Specifically, we design a Size Perception and Embedding (SPE) module in the encoder part to explicitly extract size information for the purpose of complementing fine-grained information. We propose an Asynchronous Decoupled Decoding (ADD) architecture based on traditional progressive learning to decode features from both channel and spatial perspectives, enabling efficient learning of discriminative features. In both quantitative and qualitative analyses, our approach surpasses existing state-of-the-art image classification methods, achieving an accuracy of 90.5%. This demonstrates that our work provides an effective tool for large-scale, systematic archaeological research.

</details>


### [30] [Loom: Diffusion-Transformer for Interleaved Generation](https://arxiv.org/abs/2512.18254)
*Mingcheng Ye,Jiaming Liu,Yiren Song*

Main category: cs.CV

TL;DR: Loom是一种交错文本-图像生成模型，交替编码文本与图像并使用语言分步规划和有限历史条件，能有效提升组合性、时序连贯性与文本对齐，优于Anole


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在长时序、多条件推理及文本-图像对齐上的局限，支持在单序列中生成互相关联的多帧图像与描述以满足风格迁移、组合合成与教程生成等应用

Method: 基于Bagel统一模型进行全参微调，设计交错架构替换简单拼接历史：先由语言规划器将用户指令分解为步骤式提示与帧嵌入，然后逐帧生成，条件仅使用小批量采样先前帧加上全局文本上下文，从而实现可控且高效的长程生成

Result: 提出Loom模型，一种统一扩散-Transformer框架用于交错文本-图像生成，通过交替文本与视觉嵌入并结合语言规划实现长时序一致与语义对齐

Conclusion: Loom在风格迁移、组合生成与教程类序列任务上在时间一致性与文本-图像对齐方面优于开源基线Anole，并在构建的50K教程数据集上显著提升性能

Abstract: Interleaved text-image generation aims to jointly produce coherent visual frames and aligned textual descriptions within a single sequence, enabling tasks such as style transfer, compositional synthesis, and procedural tutorials. We present Loom, a unified diffusion-transformer framework for interleaved text-image generation. Loom extends the Bagel unified model via full-parameter fine-tuning and an interleaved architecture that alternates textual and visual embeddings for multi-condition reasoning and sequential planning. A language planning strategy first decomposes a user instruction into stepwise prompts and frame embeddings, which guide temporally consistent synthesis. For each frame, Loom conditions on a small set of sampled prior frames together with the global textual context, rather than concatenating all history, yielding controllable and efficient long-horizon generation. Across style transfer, compositional generation, and tutorial-like procedures, Loom delivers superior compositionality, temporal coherence, and text-image alignment. Experiments demonstrate that Loom substantially outperforms the open-source baseline Anole, achieving an average gain of 2.6 points (on a 5-point scale) across temporal and semantic metrics in text-to-interleaved tasks. We also curate a 50K interleaved tutorial dataset and demonstrate strong improvements over unified and diffusion editing baselines.

</details>


### [31] [Who Can See Through You? Adversarial Shielding Against VLM-Based Attribute Inference Attacks](https://arxiv.org/abs/2512.18264)
*Yucheng Fan,Jiawei Chen,Yu Tian,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 提出视觉一致性约束的隐私保护方法与VPI-COCO基准，实验证明在保护隐私同时保留效用方面表现优秀


<details>
  <summary>Details</summary>
Motivation: VLM-based 属性推断攻击对社媒图片隐私构成威胁，现有方法往往损害图像质量或干扰视觉功能，需在保护隐私和用户体验间取得平衡

Method: 在视觉一致性约束下联合优化隐私抑制与效用保持；构建522张图片的VPI-COCO基准，设计层级隐私问题与非隐私对照，进行细粒度评估

Result: 提出了一种在视觉一致性约束下联合优化隐私抑制与效用保持的保护方法，并构建了公开基准数据集VPI-COCO用于评估

Conclusion: 方法在多种VLM上能将PAR降至25%以下，NPAR维持在88%以上，视觉一致性高，且对未见及改写问题具有良好泛化

Abstract: As vision-language models (VLMs) become widely adopted, VLM-based attribute inference attacks have emerged as a serious privacy concern, enabling adversaries to infer private attributes from images shared on social media. This escalating threat calls for dedicated protection methods to safeguard user privacy. However, existing methods often degrade the visual quality of images or interfere with vision-based functions on social media, thereby failing to achieve a desirable balance between privacy protection and user experience. To address this challenge, we propose a novel protection method that jointly optimizes privacy suppression and utility preservation under a visual consistency constraint. While our method is conceptually effective, fair comparisons between methods remain challenging due to the lack of publicly available evaluation datasets. To fill this gap, we introduce VPI-COCO, a publicly available benchmark comprising 522 images with hierarchically structured privacy questions and corresponding non-private counterparts, enabling fine-grained and joint evaluation of protection methods in terms of privacy preservation and user experience. Building upon this benchmark, experiments on multiple VLMs demonstrate that our method effectively reduces PAR below 25%, keeps NPAR above 88%, maintains high visual consistency, and generalizes well to unseen and paraphrased privacy questions, demonstrating its strong practical applicability for real-world VLM deployments.

</details>


### [32] [Building UI/UX Dataset for Dark Pattern Detection and YOLOv12x-based Real-Time Object Recognition Detection System](https://arxiv.org/abs/2512.18269)
*Se-Young Jang,Su-Yeon Yoon,Jae-Woong Jung,Dong-Hun Lee,Seong-Hun Choi,Soo-Kyung Jun,Yu-Bin Kim,Young-Seon Ju,Kyounggon Kim*

Main category: cs.CV

TL;DR: 构建了包含4,066张标注UI截图的数据集，使用YOLOv12x迁移学习实现了92.8% mAP@50和40.5 FPS的实时可视化暗模式检测


<details>
  <summary>Details</summary>
Motivation: 应对日益增多的线上平台暗模式，需开发主动、实时的检测技术

Method: Collected a dataset of 4,066 annotated UI screenshots and trained YOLOv12x with transfer learning for real-time detection

Result: Achieved mAP@50 92.8% and 40.5 FPS inference speed

Conclusion: Framework is effective for practical deployment; dataset publicly released

Abstract: With the accelerating pace of digital transformation and the widespread adoption of online platforms, both social and technical concerns regarding dark patterns-user interface designs that undermine users' ability to make informed and rational choices-have become increasingly prominent. As corporate online platforms grow more sophisticated in their design strategies, there is a pressing need for proactive and real-time detection technologies that go beyond the predominantly reactive approaches employed by regulatory authorities. In this paper, we propose a visual dark pattern detection framework that improves both detection accuracy and real-time performance. To this end, we constructed a proprietary visual object detection dataset by manually collecting 4,066 UI/UX screenshots containing dark patterns from 194 websites across six major industrial sectors in South Korea and abroad. The collected images were annotated with five representative UI components commonly associated with dark patterns: Button, Checkbox, Input Field, Pop-up, and QR Code. This dataset has been publicly released to support further research and development in the field. To enable real-time detection, this study adopted the YOLOv12x object detection model and applied transfer learning to optimize its performance for visual dark pattern recognition. Experimental results demonstrate that the proposed approach achieves a high detection accuracy of 92.8% in terms of mAP@50, while maintaining a real-time inference speed of 40.5 frames per second (FPS), confirming its effectiveness for practical deployment in online environments. Furthermore, to facilitate future research and contribute to technological advancements, the dataset constructed in this study has been made publicly available at https://github.com/B4E2/B4E2-DarkPattern-YOLO-DataSet.

</details>


### [33] [UniMPR: A Unified Framework for Multimodal Place Recognition with Arbitrary Sensor Configurations](https://arxiv.org/abs/2512.18279)
*Zhangshuo Qi,Jingyi Xu,Luqi Cheng,Shichen Wen,Yiming Ma,Guangming Xiong*

Main category: cs.CV

TL;DR: UniMPR通过将不同传感器数据统一到极坐标BEV特征空间，使用多分支网络并结合大规模混合数据预训练和自适应标签分配，实现单模型处理任意模态组合，提升缺失/退化模态下的稳定性和跨配置泛化；在七个数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决MPR中模型对任意模态输入的适配、缺失/退化模态下鲁棒性、以及不同传感器配置间的泛化问题。

Method: 1) 将相机、LiDAR、雷达等传感器数据统一投影为极坐标BEV特征；2) 设计多分支网络以提取模态内与模态间判别特征，支持任意模态组合输入；3) 构建多数据集的大规模训练集，并提出自适应标签分配策略进行广泛预训练以提升泛化和鲁棒性。

Result: 提出了UniMPR统一多模态地点识别框架

Conclusion: 单模型适配任意模态组合，在多数据集上表现领先，具备鲁棒性与泛化能力

Abstract: Place recognition is a critical component of autonomous vehicles and robotics, enabling global localization in GPS-denied environments. Recent advances have spurred significant interest in multimodal place recognition (MPR), which leverages complementary strengths of multiple modalities. Despite its potential, most existing MPR methods still face three key challenges: (1) dynamically adapting to arbitrary modality inputs within a unified framework, (2) maintaining robustness with missing or degraded modalities, and (3) generalizing across diverse sensor configurations and setups. In this paper, we propose UniMPR, a unified framework for multimodal place recognition. Using only one trained model, it can seamlessly adapt to any combination of common perceptual modalities (e.g., camera, LiDAR, radar). To tackle the data heterogeneity, we unify all inputs within a polar BEV feature space. Subsequently, the polar BEVs are fed into a multi-branch network to exploit discriminative intra-model and inter-modal features from any modality combinations. To fully exploit the network's generalization capability and robustness, we construct a large-scale training set from multiple datasets and introduce an adaptive label assignment strategy for extensive pre-training. Experiments on seven datasets demonstrate that UniMPR achieves state-of-the-art performance under varying sensor configurations, modality combinations, and environmental conditions. Our code will be released at https://github.com/QiZS-BIT/UniMPR.

</details>


### [34] [Pyramidal Adaptive Cross-Gating for Multimodal Detection](https://arxiv.org/abs/2512.18291)
*Zidong Gu,Shoufu Tian*

Main category: cs.CV

TL;DR: A deep fusion backbone (PACGNet) with horizontal symmetric gating (SCG) and pyramidal hierarchical gating (PFMG) reduces cross-modal noise and preserves feature hierarchy, leading to SOTA mAP50 on DroneVehicle and VEDAI.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal fusion for aerial object detection uses simple aggregation that introduces cross-modal noise and damages the feature pyramid, harming small-object detection; need deep, structured fusion to selectively integrate modalities while preserving pyramid details.

Method: Proposed methods: PACGNet with Symmetrical Cross-Gating (SCG) and Pyramidal Feature-aware Multimodal Gating (PFMG). SCG: bidirectional horizontal gating to selectively absorb complementary info and suppress noise; PFMG: progressive hierarchical gating using higher-resolution features to guide lower-resolution fusion within backbone.

Result: Evaluated on DroneVehicle and VEDAI, achieving state-of-the-art mAP50 of 81.7% and 82.1%.

Conclusion: PACGNet's deep, gated fusion within backbone preserves modality semantics and feature pyramid hierarchy, improving small object detection in aerial images and reducing cross-modal noise.

Abstract: Object detection in aerial imagery is a critical task in applications such as UAV reconnaissance. Although existing methods have extensively explored feature interaction between different modalities, they commonly rely on simple fusion strategies for feature aggregation. This introduces two critical flaws: it is prone to cross-modal noise and disrupts the hierarchical structure of the feature pyramid, thereby impairing the fine-grained detection of small objects. To address this challenge, we propose the Pyramidal Adaptive Cross-Gating Network (PACGNet), an architecture designed to perform deep fusion within the backbone. To this end, we design two core components: the Symmetrical Cross-Gating (SCG) module and the Pyramidal Feature-aware Multimodal Gating (PFMG) module. The SCG module employs a bidirectional, symmetrical "horizontal" gating mechanism to selectively absorb complementary information, suppress noise, and preserve the semantic integrity of each modality. The PFMG module reconstructs the feature hierarchy via a progressive hierarchical gating mechanism. This leverages the detailed features from a preceding, higher-resolution level to guide the fusion at the current, lower-resolution level, effectively preserving fine-grained details as features propagate. Through evaluations conducted on the DroneVehicle and VEDAI datasets, our PACGNet sets a new state-of-the-art benchmark, with mAP50 scores reaching 81.7% and 82.1% respectively.

</details>


### [35] [MatE: Material Extraction from Single-Image via Geometric Prior](https://arxiv.org/abs/2512.18312)
*Zeyu Zhang,Wei Zhai,Jian Yang,Yang Cao*

Main category: cs.CV

TL;DR: MatE：单张随拍→深度粗校正→双分支（旋转/尺度一致性）扩散→输出 albedo/normal/roughness/height 的可平铺 PBR 材质。


<details>
  <summary>Details</summary>
Motivation: 制作高保真 PBR 材质通常需要专用设备和专家后处理，阻碍了广泛使用；目标是从随手拍摄图像中自动生成可用于渲染的材质贴图，降低门槛。

Method: 1. 输入图像与掩码；2. 用估计深度进行粗几何校正以减少透视畸变；3. 采用双分支扩散模型，分别处理旋转对齐与尺度对齐数据以学习一致性，修正残余畸变并翻译为完整的材质图（albedo, normal, roughness, height）；4. 输出对未知照明和透视不变的材质贴图，支持平铺。

Result: MatE 能从单张非受控场景图像生成可平铺的 PBR 材质贴图，并包含校正、双分支扩散模型和最终的材质图输出。

Conclusion: 该方法通过深度引导的粗校正与旋转/尺度对齐的双分支扩散模型，有效去除视角和光照变形，能从随手拍摄恢复内在材质属性，适用于生成真实感材质。

Abstract: The creation of high-fidelity, physically-based rendering (PBR) materials remains a bottleneck in many graphics pipelines, typically requiring specialized equipment and expert-driven post-processing. To democratize this process, we present MatE, a novel method for generating tileable PBR materials from a single image taken under unconstrained, real-world conditions. Given an image and a user-provided mask, MatE first performs coarse rectification using an estimated depth map as a geometric prior, and then employs a dual-branch diffusion model. Leveraging a learned consistency from rotation-aligned and scale-aligned training data, this model further rectify residual distortions from the coarse result and translate it into a complete set of material maps, including albedo, normal, roughness and height. Our framework achieves invariance to the unknown illumination and perspective of the input image, allowing for the recovery of intrinsic material properties from casual captures. Through comprehensive experiments on both synthetic and real-world data, we demonstrate the efficacy and robustness of our approach, enabling users to create realistic materials from real-world image.

</details>


### [36] [MatSpray: Fusing 2D Material World Knowledge on 3D Geometry](https://arxiv.org/abs/2512.18314)
*Philipp Langsteiner,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: See fields


<details>
  <summary>Details</summary>
Motivation: See fields

Method: Extract methods, results, conclusions, TL;DR, motivation from the abstract

Result: See fields

Conclusion: See fields

Abstract: Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines.

</details>


### [37] [A two-stream network with global-local feature fusion for bone age assessment](https://arxiv.org/abs/2512.18331)
*Qiong Lou,Han Yang,Fang Lu*

Main category: cs.CV

TL;DR: BoNet+ uses Transformer-based global stream and RFAConv-based local stream, fuses features and refines with Inception-V3, achieving MAE 3.81 (RSNA) and 5.65 (RHPE) months


<details>
  <summary>Details</summary>
Motivation: Existing deep learning BAA methods struggle to balance global context and local skeletal details; need automated, accurate, objective assessment

Method: Two-stream deep learning (BoNet+): global and local channels; Transformer for global, RFAConv for local, concat and Inception-V3 optimization

Result: Validated on RSNA and RHPE datasets; MAE 3.81 months (RSNA), 5.65 months (RHPE); comparable to state-of-the-art

Conclusion: BoNet+ effectively balances global and local feature extraction, achieving high-precision automated BAA and reducing clinical workload

Abstract: Bone Age Assessment (BAA) is a widely used clinical technique that can accurately reflect an individual's growth and development level, as well as maturity. In recent years, although deep learning has advanced the field of bone age assessment, existing methods face challenges in efficiently balancing global features and local skeletal details. This study aims to develop an automated bone age assessment system based on a two-stream deep learning architecture to achieve higher accuracy in bone age assessment. We propose the BoNet+ model incorporating global and local feature extraction channels. A Transformer module is introduced into the global feature extraction channel to enhance the ability in extracting global features through multi-head self-attention mechanism. A RFAConv module is incorporated into the local feature extraction channel to generate adaptive attention maps within multiscale receptive fields, enhancing local feature extraction capabilities. Global and local features are concatenated along the channel dimension and optimized by an Inception-V3 network. The proposed method has been validated on the Radiological Society of North America (RSNA) and Radiological Hand Pose Estimation (RHPE) test datasets, achieving mean absolute errors (MAEs) of 3.81 and 5.65 months, respectively. These results are comparable to the state-of-the-art. The BoNet+ model reduces the clinical workload and achieves automatic, high-precision, and more objective bone age assessment.

</details>


### [38] [MCVI-SANet: A lightweight semi-supervised model for LAI and SPAD estimation of winter wheat under vegetation index saturation](https://arxiv.org/abs/2512.18344)
*Zhiheng Zhang,Jiajun Yang,Hong Sun,Dong Wang,Honghua Jiang,Yaru Chen,Tangyuan Ning*

Main category: cs.CV

TL;DR: 提出轻量半监督视觉模型MCVI-SANet，含VI-SABlock和VICReg半监督训练，按植被高度划分数据，十次重复实验显示在LAI与SPAD估测上分别达R2 0.8123/0.6846，优于基线且仅0.10M参数。


<details>
  <summary>Details</summary>
Motivation: 传统基于VI或纹理的机器学习方法在稠密冠层阶段存在植被指数(VI)饱和问题且标注稀缺，深度学习方法受域差与数据需求限制，亟需一种既能缓解VI饱和、又能在小样本下泛化的轻量化方法。

Method: 设计VI-SABlock用于自适应的通道-空间特征增强以缓解VI饱和，并结合VICReg损失的半监督学习策略扩充无标注样本的表征学习；数据按植被高度进行分层划分以保证各生长期的代表性；模型参数量仅0.10M。

Result: 十次重复实验平均：LAI R2=0.8123，RMSE=0.4796；SPAD R2=0.6846，RMSE=2.4222。相较最佳基线，LAI R2提升8.95%，SPAD R2提升8.17%，并保持高推理速度与轻量化。

Conclusion: MCVI-SANet通过引入植被指数饱和感知模块与基于VICReg的半监督策略，有效提升了在冬小麦LAI与SPAD估测任务上的泛化与精度，同时保持模型轻量化与高推理速度。

Abstract: Vegetation index (VI) saturation during the dense canopy stage and limited ground-truth annotations of winter wheat constrain accurate estimation of LAI and SPAD. Existing VI-based and texture-driven machine learning methods exhibit limited feature expressiveness. In addition, deep learning baselines suffer from domain gaps and high data demands, which restrict their generalization. Therefore, this study proposes the Multi-Channel Vegetation Indices Saturation Aware Net (MCVI-SANet), a lightweight semi-supervised vision model. The model incorporates a newly designed Vegetation Index Saturation-Aware Block (VI-SABlock) for adaptive channel-spatial feature enhancement. It also integrates a VICReg-based semi-supervised strategy to further improve generalization. Datasets were partitioned using a vegetation height-informed strategy to maintain representativeness across growth stages. Experiments over 10 repeated runs demonstrate that MCVI-SANet achieves state-of-the-art accuracy. The model attains an average R2 of 0.8123 and RMSE of 0.4796 for LAI, and an average R2 of 0.6846 and RMSE of 2.4222 for SPAD. This performance surpasses the best-performing baselines, with improvements of 8.95% in average LAI R2 and 8.17% in average SPAD R2. Moreover, MCVI-SANet maintains high inference speed with only 0.10M parameters. Overall, the integration of semi-supervised learning with agronomic priors provides a promising approach for enhancing remote sensing-based precision agriculture.

</details>


### [39] [Enhancing 3D Semantic Scene Completion with a Refinement Module](https://arxiv.org/abs/2512.18363)
*Dunxing Zhang,Jiachen Lu,Han Yang,Lei Bao,Bo Song*

Main category: cs.CV

TL;DR: Proposes ESSC-RM, a plug-and-play two-phase refinement framework (PNAM + VLGM) that refines coarse SSC outputs and yields small but consistent mIoU gains on SemanticKITTI.


<details>
  <summary>Details</summary>
Motivation: To improve coarse voxel-based semantic scene completion outputs by addressing prediction noise and local geometry errors using a refinement module that can be attached to existing SSC models.

Method: Describe proposed method, two-phase with PNAM and VLGM; plug-and-play refinement; 3D U-Net

Result: Improves mean IoU modestly on SemanticKITTI when plugged into CGFormer and MonoScene; general applicability

Conclusion: ESSC-RM is a lightweight, general refinement module that consistently boosts SSC performance via noise-aware and local-geometry refinement under multiscale supervision

Abstract: We propose ESSC-RM, a plug-and-play Enhancing framework for Semantic Scene Completion with a Refinement Module, which can be seamlessly integrated into existing SSC models. ESSC-RM operates in two phases: a baseline SSC network first produces a coarse voxel prediction, which is subsequently refined by a 3D U-Net-based Prediction Noise-Aware Module (PNAM) and Voxel-level Local Geometry Module (VLGM) under multiscale supervision. Experiments on SemanticKITTI show that ESSC-RM consistently improves semantic prediction performance. When integrated into CGFormer and MonoScene, the mean IoU increases from 16.87% to 17.27% and from 11.08% to 11.51%, respectively. These results demonstrate that ESSC-RM serves as a general refinement framework applicable to a wide range of SSC models.

</details>


### [40] [Efficient Zero-Shot Inpainting with Decoupled Diffusion Guidance](https://arxiv.org/abs/2512.18365)
*Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 提出一种无需反向传播通过去噪器的新型似然代理，使采样过程为高斯后验转移，从而显著降低内存与运行时开销，同时保持与微调方法相当的重建质量与观测一致性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本扩散方法依赖一系列替代似然函数，并在每个逆向步需要通过去噪网络做向量-雅可比积，导致昂贵的内存与运行时成本。作者希望设计一种不需反向传播的新型似然代理，以提高推理效率。

Method: 构建一种新的似然代理，使得逆向采样步骤中的后验转移变为易于采样的高斯分布，从而避免在每一步对去噪器进行向量-雅可比积计算，显著降低推理开销，同时保持观测一致性与重建质量。

Result: The paper introduces a new likelihood surrogate for zero-shot diffusion-based image editing that avoids costly vector-Jacobian products through the denoiser, enabling efficient Gaussian posterior transitions during sampling.

Conclusion: Their method achieves strong observation consistency and high-quality reconstructions comparable to fine-tuned baselines while reducing inference cost significantly.

Abstract: Diffusion models have emerged as powerful priors for image editing tasks such as inpainting and local modification, where the objective is to generate realistic content that remains consistent with observed regions. In particular, zero-shot approaches that leverage a pretrained diffusion model, without any retraining, have been shown to achieve highly effective reconstructions. However, state-of-the-art zero-shot methods typically rely on a sequence of surrogate likelihood functions, whose scores are used as proxies for the ideal score. This procedure however requires vector-Jacobian products through the denoiser at every reverse step, introducing significant memory and runtime overhead. To address this issue, we propose a new likelihood surrogate that yields simple and efficient to sample Gaussian posterior transitions, sidestepping the backpropagation through the denoiser network. Our extensive experiments show that our method achieves strong observation consistency compared with fine-tuned baselines and produces coherent, high-quality reconstructions, all while significantly reducing inference cost. Code is available at https://github.com/YazidJanati/ding.

</details>


### [41] [RecurGS: Interactive Scene Modeling via Discrete-State Recurrent Gaussian Fusion](https://arxiv.org/abs/2512.18386)
*Wenhao Hu,Haonan Zhou,Zesheng Li,Liu Liu,Jiacheng Dong,Zhizhong Su,Gaoang Wang*

Main category: cs.CV

TL;DR: RecurGS通过递归对象级对齐与可见性感知体素融合，将离散场景状态合成为一个可交互的演化3D表示，实现高质量、效率和长时序更新。


<details>
  <summary>Details</summary>
Motivation: 帮助应对离散场景变化和构建可交互3D环境的需求，同时克服现有方法不能跨多观察融合信息和不能合成新状态的缺点。

Method: 提出RecurGS：一种递归融合框架，逐步将离散高斯场景状态整合到单一演化表示。关键模块包括对象级变化检测、基于语义对应与李代数SE(3)细化的几何对齐、重放监督保存历史结构，以及体素化可见性感知融合模块选择性融合新观测区域并固定稳定区域。

Result: 在合成与真实数据集上进行大量实验，结果显示重建质量高、更新效率显著提升，支持物体级操控且能在不额外扫描的情况下合成新场景状态，保持写实保真。

Conclusion: RecurGS有效缓解灾难性遗忘，能进行长时序高效更新，迈出构建持续可交互高斯世界的可扩展一步。

Abstract: Recent advances in 3D scene representations have enabled high-fidelity novel view synthesis, yet adapting to discrete scene changes and constructing interactive 3D environments remain open challenges in vision and robotics. Existing approaches focus solely on updating a single scene without supporting novel-state synthesis. Others rely on diffusion-based object-background decoupling that works on one state at a time and cannot fuse information across multiple observations. To address these limitations, we introduce RecurGS, a recurrent fusion framework that incrementally integrates discrete Gaussian scene states into a single evolving representation capable of interaction. RecurGS detects object-level changes across consecutive states, aligns their geometric motion using semantic correspondence and Lie-algebra based SE(3) refinement, and performs recurrent updates that preserve historical structures through replay supervision. A voxelized, visibility-aware fusion module selectively incorporates newly observed regions while keeping stable areas fixed, mitigating catastrophic forgetting and enabling efficient long-horizon updates. RecurGS supports object-level manipulation, synthesizes novel scene states without requiring additional scans, and maintains photorealistic fidelity across evolving environments. Extensive experiments across synthetic and real-world datasets demonstrate that our framework delivers high-quality reconstructions with substantially improved update efficiency, providing a scalable step toward continuously interactive Gaussian worlds.

</details>


### [42] [Automated Mosaic Tesserae Segmentation via Deep Learning Techniques](https://arxiv.org/abs/2512.18406)
*Charilaos Kapelonis,Marios Antonakakis,Konstantinos Politof,Aristomenis Antoniadis,Michalis Zervakis*

Main category: cs.CV

TL;DR: 本文提出基于Meta AI的Segment Anything Model 2 (SAM 2) 对马赛克（tesserae）进行自动分割，并构建了带注释的数据集。微调后的模型在IOU、召回率及F-measure上均优于基线，且显著降低了预测颗粒数的绝对误差，推动实时分割应用。


<details>
  <summary>Details</summary>
Motivation: 马赛克作为重要文化遗产因年久脆弱需数字化保护，关键步骤是将小片（tesserae）从背景中分割出来，但公开数据集稀缺且传统模型表现有限。

Method: 使用SAM 2作为基础模型并在新注释的马赛克图像数据集上进行微调与评估。对比基线SAM 2及先前方法，使用IOU、召回率、F-measure及颗粒数绝对误差等指标评估性能提升。

Result: 微调后IOU从89.00%提升到91.02%，召回率从92.12%提升到95.89%；在既有基准上F-measure提高约3%，颗粒数绝对误差从0.20降至0.02。

Conclusion: 微调的SAM 2与新注释数据集显著提升了马赛克分割效果，有望用于实时分割与文化遗产数字化保护的实际应用。

Abstract: Art is widely recognized as a reflection of civilization and mosaics represent an important part of cultural heritage. Mosaics are an ancient art form created by arranging small pieces, called tesserae, on a surface using adhesive. Due to their age and fragility, they are prone to damage, highlighting the need for digital preservation. This paper addresses the problem of digitizing mosaics by segmenting the tesserae to separate them from the background within the broader field of Image Segmentation in Computer Vision. We propose a method leveraging Segment Anything Model 2 (SAM 2) by Meta AI, a foundation model that outperforms most conventional segmentation models, to automatically segment mosaics. Due to the limited open datasets in the field, we also create an annotated dataset of mosaic images to fine-tune and evaluate the model. Quantitative evaluation on our testing dataset shows notable improvements compared to the baseline SAM 2 model, with Intersection over Union increasing from 89.00% to 91.02% and Recall from 92.12% to 95.89%. Additionally, on a benchmark proposed by a prior approach, our model achieves an F-measure 3% higher than previous methods and reduces the error in the absolute difference between predicted and actual tesserae from 0.20 to just 0.02. The notable performance of the fine-tuned SAM 2 model together with the newly annotated dataset can pave the way for real-time segmentation of mosaic images.

</details>


### [43] [Through the PRISm: Importance-Aware Scene Graphs for Image Retrieval](https://arxiv.org/abs/2512.18407)
*Dimitrios Georgoulopoulos,Nikolaos Chaidos,Angeliki Dimitriou,Giorgos Stamou*

Main category: cs.CV

TL;DR: PRISm prunes irrelevant objects via importance prediction and encodes relations with an edge-aware GNN to produce semantically rich image embeddings, yielding improved and interpretable image retrieval.


<details>
  <summary>Details</summary>
Motivation: Traditional retrieval misses relational/contextual nuances; need to model object importance and interactions explicitly to align retrieval with human perception.

Method: From abstract: propose PRISm with Importance Prediction Module and Edge-Aware GNN to prune irrelevant objects and encode relations; multimodal framework for image-to-image retrieval.

Result: PRISm achieves superior top-ranked retrieval on benchmarks and real-world datasets; qualitative shows interpretable results capturing key objects/interactions.

Conclusion: PRISm improves semantically grounded image retrieval by pruning unimportant elements and modeling relational structure, leading to better alignment with human perception.

Abstract: Accurately retrieving images that are semantically similar remains a fundamental challenge in computer vision, as traditional methods often fail to capture the relational and contextual nuances of a scene. We introduce PRISm (Pruning-based Image Retrieval via Importance Prediction on Semantic Graphs), a multimodal framework that advances image-to-image retrieval through two novel components. First, the Importance Prediction Module identifies and retains the most critical objects and relational triplets within an image while pruning irrelevant elements. Second, the Edge-Aware Graph Neural Network explicitly encodes relational structure and integrates global visual features to produce semantically informed image embeddings. PRISm achieves image retrieval that closely aligns with human perception by explicitly modeling the semantic importance of objects and their interactions, capabilities largely absent in prior approaches. Its architecture effectively combines relational reasoning with visual representation, enabling semantically grounded retrieval. Extensive experiments on benchmark and real-world datasets demonstrate consistently superior top-ranked performance, while qualitative analyses show that PRISm accurately captures key objects and interactions, producing interpretable and semantically meaningful results.

</details>


### [44] [AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning](https://arxiv.org/abs/2512.18411)
*Fei Song,Yi Li,Jiangmeng Li,Rui Wang,Changwen Zheng,Fanjiang Xu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出AmPLe：对不同视觉-语言模型的多提示输出进行加权集成，并通过信息论引导从样本中提取提示相关语义以自适应去偏，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有多提示学习忽视了两个关键偏差：模型-提示匹配偏差（同一提示在不同V-L模型上语义不一致）和样本-提示匹配偏差（样本中包含提示无关语义影响权重计算），这限制了多提示方法的效果。

Method: 提出基于集成学习的AmPLe：对来自不同V-L模型与提示的多样预测进行加权聚合；用信息论分析指导从输入样本中抽取提示相关语义，以计算去偏的自适应集成权重；并结合因果视角进行理论验证。

Result: 在新类泛化、新目标数据集与未见域移三类任务上，AmPLe在多基准实验中显著优于现有多提示方法，理论与实证结果均支持其有效性。

Conclusion: AmPLe通过适配并去偏的集成多提示学习，有效缓解了模型-提示匹配偏差与样本-提示匹配偏差，从而在泛化、新数据集和域移任务上优于现有方法。

Abstract: Multi-prompt learning methods have emerged as an effective approach for facilitating the rapid adaptation of vision-language models to downstream tasks with limited resources. Existing multi-prompt learning methods primarily focus on utilizing various meticulously designed prompts within a single foundation vision-language model to achieve superior performance. However, the overlooked model-prompt matching bias hinders the development of multi-prompt learning, i.e., the same prompt can convey different semantics across distinct vision-language models, such as CLIP-ViT-B/16 and CLIP-ViT-B/32, resulting in inconsistent predictions of identical prompt. To mitigate the impact of this bias on downstream tasks, we explore an ensemble learning approach to sufficiently aggregate the benefits of diverse predictions. Additionally, we further disclose the presence of sample-prompt matching bias, which originates from the prompt-irrelevant semantics encapsulated in the input samples. Thus, directly utilizing all information from the input samples for generating weights of ensemble learning can lead to suboptimal performance. In response, we extract prompt-relevant semantics from input samples by leveraging the guidance of the information theory-based analysis, adaptively calculating debiased ensemble weights. Overall, we propose Adaptive-Debiased Ensemble MultiPrompt Learning, abbreviated as AmPLe, to mitigate the two types of bias simultaneously. Extensive experiments on three representative tasks, i.e., generalization to novel classes, new target datasets, and unseen domain shifts, show that AmPLe can widely outperform existing methods. Theoretical validation from a causal perspective further supports the effectiveness of AmPLe.

</details>


### [45] [E-RGB-D: Real-Time Event-Based Perception with Structured Light](https://arxiv.org/abs/2512.18429)
*Seyed Ehsan Marjani Bajestani,Giovanni Beltrame*

Main category: cs.CV

TL;DR: 结合DLP投影的主动结构光与事件相机，实时无帧采集彩色深度信息，达成1400fps颜色检测等效与4kHz像素深度检测，生成彩色点云并公开代码。


<details>
  <summary>Details</summary>
Motivation: 分析事件相机在静态/慢速目标检测和颜色信息缺失方面的不足，并通过主动结构光结合投影解决这些问题，实现RGB-D感知。

Method: 将DLP投影仪（TI LightCrafter 4500）与单目单色事件相机结合，采用动态投影来选择性获取颜色数据并通过时序事件编码像素深度，生成彩色点云。实现了帧率等效1400fps的颜色检测和4kHz的像素深度检测。

Result: 成功实现了无帧RGB-D 感知，生成高分辨率彩色点云；实验表明在速度和深度检测频率上显著提升。代码公开在GitHub。

Conclusion: 通过ASL（Active Structured Light）将事件相机与投影结合，可弥补事件相机在颜色与静态检测方面的不足，推动机器人和3D重建等领域的应用。

Abstract: Event-based cameras (ECs) have emerged as bio-inspired sensors that report pixel brightness changes asynchronously, offering unmatched speed and efficiency in vision sensing. Despite their high dynamic range, temporal resolution, low power consumption, and computational simplicity, traditional monochrome ECs face limitations in detecting static or slowly moving objects and lack color information essential for certain applications. To address these challenges, we present a novel approach that integrates a Digital Light Processing (DLP) projector, forming Active Structured Light (ASL) for RGB-D sensing. By combining the benefits of ECs and projection-based techniques, our method enables the detection of color and the depth of each pixel separately. Dynamic projection adjustments optimize bandwidth, ensuring selective color data acquisition and yielding colorful point clouds without sacrificing spatial resolution. This integration, facilitated by a commercial TI LightCrafter 4500 projector and a monocular monochrome EC, not only enables frameless RGB-D sensing applications but also achieves remarkable performance milestones. With our approach, we achieved a color detection speed equivalent to 1400 fps and 4 kHz of pixel depth detection, significantly advancing the realm of computer vision across diverse fields from robotics to 3D reconstruction methods. Our code is publicly available: https://github.com/MISTLab/event_based_rgbd_ros

</details>


### [46] [MeniMV: A Multi-view Benchmark for Meniscus Injury Severity Grading](https://arxiv.org/abs/2512.18437)
*Shurui Xu,Siqi Yang,Jiapin Ren,Zhong Cao,Hongwei Yang,Mengzhen Fan,Yuyu Sun,Shuyan Li*

Main category: cs.CV

TL;DR: MeniMV是一个为半月板前后角四级分级设计的多视角MRI基准，含750名患者的3000次检查和6000张配准影像，由专家标注并用于评测多种深度模型，扩展了数据规模并强调了分级任务的挑战。


<details>
  <summary>Details</summary>
Motivation: 研发针对半月板角（horn）撕裂的精确分级方法，以提高膝关节损伤诊断的自动化能力，因为现有方法标签粗糙或仅做二分类，缺乏定位和严重度信息。

Method: 构建多视角数据集MeniMV，包含750名患者的3000个MRI检查（每例矢状面和冠状面配对，共6000张配准图像），由主治骨科医生注释前后半月板角的四级（0-3）严重度标签；在该数据集上对多种CNN和Transformer模型进行基线评测。

Result: MeniMV提供了比现有数据集多一倍以上的病理标注数据，并保留了临床所需的双视角信息；通过对比实验，建立了强基线并揭示了在细粒度严重度分级任务中的挑战。

Conclusion: MeniMV是一个独特且规模更大的多视角半月板角伤病分级基准，将促进自动化肌肉骨骼影像学研究，尤其是提高半月板角撕裂的定位与分级性能。

Abstract: Precise grading of meniscal horn tears is critical in knee injury diagnosis but remains underexplored in automated MRI analysis. Existing methods often rely on coarse study-level labels or binary classification, lacking localization and severity information. In this paper, we introduce MeniMV, a multi-view benchmark dataset specifically designed for horn-specific meniscus injury grading. MeniMV comprises 3,000 annotated knee MRI exams from 750 patients across three medical centers, providing 6,000 co-registered sagittal and coronal images. Each exam is meticulously annotated with four-tier (grade 0-3) severity labels for both anterior and posterior meniscal horns, verified by chief orthopedic physicians. Notably, MeniMV offers more than double the pathology-labeled data volume of prior datasets while uniquely capturing the dual-view diagnostic context essential in clinical practice. To demonstrate the utility of MeniMV, we benchmark multiple state-of-the-art CNN and Transformer-based models. Our extensive experiments establish strong baselines and highlight challenges in severity grading, providing a valuable foundation for future research in automated musculoskeletal imaging.

</details>


### [47] [Object-Centric Framework for Video Moment Retrieval](https://arxiv.org/abs/2512.18448)
*Zongyao Li,Yongkang Wong,Satoshi Yamazaki,Jianquan Liu,Mohan Kankanhalli*

Main category: cs.CV

TL;DR: 本文用场景图解析和关系跟踪Transformer，将视频转为对象级时序表示，从而更好地处理涉及特定实体与交互的查询，三项基准上超过SOTA


<details>
  <summary>Details</summary>
Motivation: 现有方法主要用帧/片段级全局特征，缺乏细粒度对象语义和外观信息，不能很好处理以对象为中心的查询和对象级时间动态

Method: 先用场景图解析器提取与查询相关的对象并从每帧生成场景图，构建对象级特征序列；然后用关系跟踪Transformer建模对象间时空关联和状态变化，以此进行时刻定位

Result: 提出了一种面向对象的时刻检索框架

Conclusion: 通过构建基于场景图的对象级特征序列并用关系跟踪转换器建模对象时空动态，显著提升了面向对象查询的定位精度

Abstract: Most existing video moment retrieval methods rely on temporal sequences of frame- or clip-level features that primarily encode global visual and semantic information. However, such representations often fail to capture fine-grained object semantics and appearance, which are crucial for localizing moments described by object-oriented queries involving specific entities and their interactions. In particular, temporal dynamics at the object level have been largely overlooked, limiting the effectiveness of existing approaches in scenarios requiring detailed object-level reasoning. To address this limitation, we propose a novel object-centric framework for moment retrieval. Our method first extracts query-relevant objects using a scene graph parser and then generates scene graphs from video frames to represent these objects and their relationships. Based on the scene graphs, we construct object-level feature sequences that encode rich visual and semantic information. These sequences are processed by a relational tracklet transformer, which models spatio-temporal correlations among objects over time. By explicitly capturing object-level state changes, our framework enables more accurate localization of moments aligned with object-oriented queries. We evaluated our method on three benchmarks: Charades-STA, QVHighlights, and TACoS. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across all benchmarks.

</details>


### [48] [Plasticine: A Traceable Diffusion Model for Medical Image Translation](https://arxiv.org/abs/2512.18455)
*Tianyang Zhanng,Xinxing Cheng,Jun Cheng,Shaoming Zheng,He Zhao,Huazhu Fu,Alejandro F Frangi,Jiang Liu,Jinming Duan*

Main category: cs.CV

TL;DR: 提出Plasticine，通过在扩散模型中联合建模强度与空间变换，实现具像素级可追溯性的医学图像翻译，增强解释性与临床可用性。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中由成像设备和人群分布变化引起的域差异，提升模型在不同域间的泛化与可追溯性。

Method: 提出Plasticine，一种将强度翻译与空间变换结合进去噪扩散框架的端到端图像到图像翻译方法；通过在扩散过程中同时建模像素强度变化和可逆的空间变形，实现翻译过程中的像素级对应。

Result: 生成的合成图像同时具备可解释的强度过渡和空间一致的变形；在保留解剖结构与像素级可追溯性方面优于传统只关注风格或强度的翻译方法。

Conclusion: Plasticine首次将可追溯性作为核心目标引入图像到图像翻译，证明在医学影像合成中可实现强度与空间双重可解释性，从而提高临床可用性与可信度。

Abstract: Domain gaps arising from variations in imaging devices and population distributions pose significant challenges for machine learning in medical image analysis. Existing image-to-image translation methods primarily aim to learn mappings between domains, often generating diverse synthetic data with variations in anatomical scale and shape, but they usually overlook spatial correspondence during the translation process. For clinical applications, traceability, defined as the ability to provide pixel-level correspondences between original and translated images, is equally important. This property enhances clinical interpretability but has been largely overlooked in previous approaches. To address this gap, we propose Plasticine, which is, to the best of our knowledge, the first end-to-end image-to-image translation framework explicitly designed with traceability as a core objective. Our method combines intensity translation and spatial transformation within a denoising diffusion framework. This design enables the generation of synthetic images with interpretable intensity transitions and spatially coherent deformations, supporting pixel-wise traceability throughout the translation process.

</details>


### [49] [Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.18496)
*Xiaoyang Guo,Keze Wang*

Main category: cs.CV

TL;DR: 提出Adaptive-VoCo，在VoCo-LLaMA基础上加入轻量预测器，根据图像的视觉复杂度（如patch token熵、注意力方差）自适应选择压缩率，并通过包含率正则与复杂度对齐的联合损失在效率与表征能力间权衡。实验在多模态任务上优于固定压缩比基线。


<details>
  <summary>Details</summary>
Motivation: 固定压缩率无法兼顾不同图像的复杂度，导致在复杂场景下信息丢失或在简单场景下计算浪费，需自适应压缩以平衡效率与性能。

Method: 在VoCo-LLaMA上增加一个轻量预测器，利用来自视觉编码器的统计特征（patch token熵、注意力图方差等）预测最佳压缩率。引入联合损失：率正则化（鼓励低压缩率以节省资源）与复杂度对齐（在高复杂度图像上保留更多信息）。训练时该预测器与主模型联合优化。

Result: 在多个多模态理解与推理任务上，对比固定压缩率基线，Adaptive-VoCo在准确性和资源使用（计算、内存）间取得更好权衡，尤其在视觉复杂场景提升明显。

Conclusion: 自适应视觉压缩能显著提升大规模视觉语言模型的效率与鲁棒性。Adaptive-VoCo通过轻量预测器和联合损失机制，在保持跨模态对齐的同时，实现按需压缩，推荐用于资源受限或场景多样的应用。

Abstract: In recent years, large-scale vision-language models (VLMs) have demonstrated remarkable performance on multimodal understanding and reasoning tasks. However, handling high-dimensional visual features often incurs substantial computational and memory costs. VoCo-LLaMA alleviates this issue by compressing visual patch tokens into a few VoCo tokens, reducing computational overhead while preserving strong cross-modal alignment. Nevertheless, such approaches typically adopt a fixed compression rate, limiting their ability to adapt to varying levels of visual complexity. To address this limitation, we propose Adaptive-VoCo, a framework that augments VoCo-LLaMA with a lightweight predictor for adaptive compression. This predictor dynamically selects an optimal compression rate by quantifying an image's visual complexity using statistical cues from the vision encoder, such as patch token entropy and attention map variance. Furthermore, we introduce a joint loss function that integrates rate regularization with complexity alignment. This enables the model to balance inference efficiency with representational capacity, particularly in challenging scenarios. Experimental results show that our method consistently outperforms fixed-rate baselines across multiple multimodal tasks, highlighting the potential of adaptive visual compression for creating more efficient and robust VLMs.

</details>


### [50] [PlantDiseaseNet-RT50: A Fine-tuned ResNet50 Architecture for High-Accuracy Plant Disease Detection Beyond Standard CNNs](https://arxiv.org/abs/2512.18500)
*Santwana Sagnika,Manav Malhotra,Ishtaj Kaur Deol,Soumyajit Roy,Swarnav Kumar*

Main category: cs.CV

TL;DR: 基于ResNet50的PlantDiseaseNet-RT50通过系统解冻末端层、加入批归一化与dropout、使用cosine decay学习率等技术，在多类植物病害数据集上实现约98%的准确率、精确率与召回率，证明针对性微调能显著提升病害检测效果并具有实用部署潜力。


<details>
  <summary>Details</summary>
Motivation: 传统人工目测检测耗时、费力且难以大规模推广，影响及时防治和产量保障。将深度学习模型应用于自动化、高效的病害诊断可以显著降低损失并支持农业决策。

Method: 以ResNet50为骨干，采用预训练权重并对末端若干层进行系统性解冻；替换或扩展分类头，引入批归一化与dropout作为正则化；使用cosine decay的动态学习率调度和其他训练优化技巧进行微调。

Result: 在包含多作物、多病害类别的综合数据集上，模型在测试集上取得约98%的准确率、精确率与召回率，表明改进的微调策略能将标准预训练模型转化为高效的农业诊断工具。

Conclusion: 本文提出的PlantDiseaseNet-RT50通过目标化微调和正则化策略有效提升了ResNet50在多作物病害分类任务上的性能，展示了将预训练通用模型转为农业诊断工具的可行性和高效性。

Abstract: Plant diseases pose a significant threat to agricultural productivity and global food security, accounting for 70-80% of crop losses worldwide. Traditional detection methods rely heavily on expert visual inspection, which is time-consuming, labour-intensive, and often impractical for large-scale farming operations. In this paper, we present PlantDiseaseNet-RT50, a novel fine-tuned deep learning architecture based on ResNet50 for automated plant disease detection. Our model features strategically unfrozen layers, a custom classification head with regularization mechanisms, and dynamic learning rate scheduling through cosine decay. Using a comprehensive dataset of distinct plant disease categories across multiple crop species, PlantDiseaseNet-RT50 achieves exceptional performance with approximately 98% accuracy, precision, and recall. Our architectural modifications and optimization protocol demonstrate how targeted fine-tuning can transform a standard pretrained model into a specialized agricultural diagnostic tool. We provide a detailed account of our methodology, including the systematic unfreezing of terminal layers, implementation of batch normalization and dropout regularization and application of advanced training techniques. PlantDiseaseNet-RT50 represents a significant advancement in AI-driven agricultural tools, offering a computationally efficient solution for rapid and accurate plant disease diagnosis that can be readily implemented in practical farming contexts to support timely interventions and reduce crop losses.

</details>


### [51] [NASTaR: NovaSAR Automated Ship Target Recognition Dataset](https://arxiv.org/abs/2512.18503)
*Benyamin Hosseiny,Kamirul Kamirul,Odysseas Pappas,Alin Achim*

Main category: cs.CV

TL;DR: NASTaR: 3415 NovaSAR S-band ship patches, 23 classes, AIS labels, inshore/offshore and wake info; achieves 60-87% accuracy on several classification tasks; data and code publicly available.


<details>
  <summary>Details</summary>
Motivation: Lack of large, annotated SAR ship datasets across frequencies/resolutions limits deep learning-based ship-type classification; growing SAR platforms increase need for more labeled data to improve model generalization.

Method: Collected NovaSAR S-band imagery, matched ship detections to AIS labels to create 3415 ship patches; annotated 23 classes, separated inshore/offshore and marked wakes; evaluated dataset using benchmark deep learning classifiers and reported classification accuracies for several tasks.

Result: Created NASTaR dataset of 3415 ship patches from NovaSAR S-band imagery, labeled with AIS, 23 classes, inshore/offshore separation, and wake auxiliary labels; benchmarked with deep learning achieving varying accuracies.

Conclusion: NASTaR is a useful, publicly available SAR ship dataset for diverse classification tasks, enabling reasonable benchmark performance and aiding research across different ship-type scenarios.

Abstract: Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://10.5523/bris, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.

</details>


### [52] [GTMA: Dynamic Representation Optimization for OOD Vision-Language Models](https://arxiv.org/abs/2512.18504)
*Jensen Zhang,Ningyuan Liu,Keze Wang*

Main category: cs.CV

TL;DR: 提出GTMA：在推理时为每张OOD图片构建并优化一个连续伪词嵌入，与图像特征对齐，带来15-20%零/少样本精度提升且不损害ID性能。


<details>
  <summary>Details</summary>
Motivation: VLM在开放世界场景中因文本编码器受限于离散词汇无法表达新语义锚点，导致与视觉编码器出现模态不对称，进而引发跨模态对齐崩溃和零样本性能下降。

Method: 在推理阶段为每张测试图像构造一个可微的连续伪词嵌入，通过自适应的基于梯度的表示策略优化（包含语义正则化以保持与模型先验的一致性）使该嵌入与图像视觉锚点对齐，从而绕过固定词汇表的限制。

Result: 在ImageNet-R和VISTA-Beyond上，GTMA相较基线VLM在OOD零样本和少样本设置下提升约15-20%，且在ID概念上性能保持稳定；消融实验表明伪词优化是性能提升的关键。

Conclusion: GTMA通过在推理时优化连续伪词嵌入，有效缓解了视觉-文本模型在开放世界下的模态不对称问题，从而显著提升了对OOD概念的零样本与少样本识别性能，同时保持在内分布概念上的表现。

Abstract: Vision-language models (VLMs) struggle in open-world applications, where out-of-distribution (OOD) concepts often trigger cross-modal alignment collapse and severely degrade zero-shot performance. We identify the root cause as modal asymmetry: while the visual encoder can extract discriminative features from unseen images, the text encoder is constrained by a fixed discrete vocabulary and cannot synthesize new semantic anchors. Existing approaches such as CoOp or LoRA provide only partial remedies, as they remain confined to the pre-trained semantic space.
  To overcome this bottleneck, we propose dynamic representation optimization, realized through the Guided Target-Matching Adaptation (GTMA) framework. At inference time, GTMA constructs a continuous pseudo-word embedding that best aligns with an OOD image's visual anchor, effectively bypassing vocabulary limitations. The optimization is driven by an adaptive gradient-based representation policy optimization algorithm, which incorporates semantic regularization to preserve plausibility and compatibility with the model's prior knowledge.
  Experiments on ImageNet-R and the VISTA-Beyond benchmark demonstrate that GTMA improves zero-shot and few-shot OOD accuracy by up to 15-20 percent over the base VLM while maintaining performance on in-distribution concepts. Ablation studies further confirm the necessity of pseudo-word optimization.

</details>


### [53] [Detection of AI Generated Images Using Combined Uncertainty Measures and Particle Swarm Optimised Rejection Mechanism](https://arxiv.org/abs/2512.18527)
*Rahul Yumlembam,Biju Issac,Nauman Aslam,Eaby Kollonoor Babu,Josh Collyer,Fraser Kennedy*

Main category: cs.CV

TL;DR: 通过融合Fisher信息、MC Dropout熵和GP预测方差，并用粒子群优化权重与阈值，提出一个能在分布转移与对抗攻击下稳健识别AI生成图像的拒绝机制。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像愈发逼真，单一模型预测不可靠，需基于不确定性决定何时拒绝并回收样本以提升长期鲁棒性。

Method: 提取三类不确定性信号：参数敏感性的Fisher信息、基于MC Dropout的预测熵、以及Deep Kernel Learning中GP的预测方差；对这些信号进行归一化并用粒子群优化学习加权融合与拒绝阈值；训练在Stable Diffusion，测试跨GLIDE、VQDM、Midjourney、BigGAN、StyleGAN3，并评估在FGSM/PGD对抗攻击下的拒绝性能。

Result: 提出了一种基于多重不确定性度量的AI生成图像检测框架，结合Fisher信息、MC Dropout熵和基于高斯过程（Deep Kernel Learning）的预测方差，并用粒子群优化学习权重与自适应拒绝阈值。在分布外测试（多种生成器）中，组合不确定性在过滤错误分类的AI样本上表现稳健；在对抗攻击下也能拒绝部分成功攻击。

Conclusion: 融合多源不确定性能增强检测器在分布转移和对抗场景下的稳健性；系统采取保守拒绝策略以便用于后续重训练。

Abstract: As AI-generated images become increasingly photorealistic, distinguishing them from natural images poses a growing challenge. This paper presents a robust detection framework that leverages multiple uncertainty measures to decide whether to trust or reject a model's predictions. We focus on three complementary techniques: Fisher Information, which captures the sensitivity of model parameters to input variations; entropy-based uncertainty from Monte Carlo Dropout, which reflects predictive variability; and predictive variance from a Deep Kernel Learning framework using a Gaussian Process classifier. To integrate these diverse uncertainty signals, Particle Swarm Optimisation is used to learn optimal weightings and determine an adaptive rejection threshold. The model is trained on Stable Diffusion-generated images and evaluated on GLIDE, VQDM, Midjourney, BigGAN, and StyleGAN3, each introducing significant distribution shifts. While standard metrics such as prediction probability and Fisher-based measures perform well in distribution, their effectiveness degrades under shift. In contrast, the Combined Uncertainty measure consistently achieves an incorrect rejection rate of approximately 70 percent on unseen generators, successfully filtering most misclassified AI samples. Although the system occasionally rejects correct predictions from newer generators, this conservative behaviour is acceptable, as rejected samples can support retraining. The framework maintains high acceptance of accurate predictions for natural images and in-domain AI data. Under adversarial attacks using FGSM and PGD, the Combined Uncertainty method rejects around 61 percent of successful attacks, while GP-based uncertainty alone achieves up to 80 percent. Overall, the results demonstrate that multi-source uncertainty fusion provides a resilient and adaptive solution for AI-generated image detection.

</details>


### [54] [WoundNet-Ensemble: A Novel IoMT System Integrating Self-Supervised Deep Learning and Multi-Model Fusion for Automated, High-Accuracy Wound Classification and Healing Progression Monitoring](https://arxiv.org/abs/2512.18528)
*Moses Kiprono*

Main category: cs.CV

TL;DR: 提出WoundNet-Ensemble，结合ResNet-50、DINOv2与Swin Transformer的IoMT系统，在5175张伤口图像上实现99.90%准确率，并提供愈合率与预警功能。


<details>
  <summary>Details</summary>
Motivation: 当前伤口评估主观且不一致，需自动化、远程化的客观分类与监测工具以改善临床决策和资源分配。

Method: Ensemble of ResNet-50, DINOv2, Swin; IoMT system and longitudinal tracker

Result: 99.90% ensemble accuracy on 5,175 images; 3.7% improvement over SOTA; implements healing rates, severity scores, alerts

Conclusion: WoundNet-Ensemble is a robust, accurate, clinically deployable AI tool for wound classification and monitoring, suitable for telemedicine.

Abstract: Chronic wounds, including diabetic foot ulcers which affect up to one-third of people with diabetes, impose a substantial clinical and economic burden, with U.S. healthcare costs exceeding 25 billion dollars annually. Current wound assessment remains predominantly subjective, leading to inconsistent classification and delayed interventions. We present WoundNet-Ensemble, an Internet of Medical Things system leveraging a novel ensemble of three complementary deep learning architectures: ResNet-50, the self-supervised Vision Transformer DINOv2, and Swin Transformer, for automated classification of six clinically distinct wound types. Our system achieves 99.90 percent ensemble accuracy on a comprehensive dataset of 5,175 wound images spanning diabetic foot ulcers, pressure ulcers, venous ulcers, thermal burns, pilonidal sinus wounds, and fungating malignant tumors. The weighted fusion strategy demonstrates a 3.7 percent improvement over previous state-of-the-art methods. Furthermore, we implement a longitudinal wound healing tracker that computes healing rates, severity scores, and generates clinical alerts. This work demonstrates a robust, accurate, and clinically deployable tool for modernizing wound care through artificial intelligence, addressing critical needs in telemedicine and remote patient monitoring. The implementation and trained models will be made publicly available to support reproducibility.

</details>


### [55] [Hierarchical Bayesian Framework for Multisource Domain Adaptation](https://arxiv.org/abs/2512.18553)
*Alexander M. Glandon,Khan M. Iftekharuddin*

Main category: cs.CV

TL;DR: 提出分层贝叶斯预训练框架，通过在源域间共享先验捕捉相似性，改进多源域迁移的预训练，实验在Daily-DA上比SOTA提升17.29%。


<details>
  <summary>Details</summary>
Motivation: 现有多源域迁移预训练要么共享权重要么独立训练，忽略源域间相似性；用贝叶斯分层先验可更好地整合多源信息以提升目标域性能。

Method: Hierarchical Bayesian pretraining for MDA: model sources with a shared hierarchical prior to capture inter-source similarity; use MAP/variational inference to obtain pretrained weights for target adaptation.

Result: Improved recognition accuracies on benchmark datasets; on Daily-DA RGB video human action recognition achieved +17.29% over prior SOTA.

Conclusion: Modeling source similarities via a hierarchical Bayesian prior yields better pretrained models for multisource domain adaptation, boosting downstream target performance.

Abstract: Multisource domain adaptation (MDA) aims to use multiple source datasets with available labels to infer labels on a target dataset without available labels for target supervision. Prior works on MDA in the literature is ad-hoc as the pretraining of source models is either based on weight sharing or uses independently trained models. This work proposes a Bayesian framework for pretraining in MDA by considering that the distributions of different source domains are typically similar. The Hierarchical Bayesian Framework uses similarity between the different source data distributions to optimize the pretraining for MDA. Experiments using the proposed Bayesian framework for MDA show that our framework improves accuracy on recognition tasks for a large benchmark dataset. Performance comparison with state-of-the-art MDA methods on the challenging problem of human action recognition in multi-domain benchmark Daily-DA RGB video shows the proposed Bayesian Framework offers a 17.29% improvement in accuracy when compared to the state-of-the-art methods in the literature.

</details>


### [56] [Enhancing Medical Large Vision-Language Models via Alignment Distillation](https://arxiv.org/abs/2512.18554)
*Aofei Chang,Ting Wang,Fenglong Ma*

Main category: cs.CV

TL;DR: 提出MEDALIGN：两个蒸馏损失（基于视觉token相似结构的空间感知视觉对齐损失与引导关注诊断相关区域的注意力感知蒸馏损失），从CLIP向Med-LVLM传递视觉对齐知识，提升报告生成与VQA性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有Med-LVLM在临床任务中易产生幻觉（与图像不一致的输出），主要归因于视觉表征学习不足与视觉注意力对齐差。需要一种轻量的对齐方法将领域视觉理解能力传递给Med-LVLM。

Method: 设计轻量对齐蒸馏框架MEDALIGN：1）基于视觉token层面的相似性结构构造空间感知视觉对齐损失，以对齐图像局部表征；2）构造注意力感知蒸馏损失，引导模型注意力集中于诊断相关区域。两者从域特定CLIP向Med-LVLM迁移对齐知识，训练时可作为附加损失加入。

Result: 在多个医疗报告生成与医疗VQA基准上，MEDALIGN持续提升性能与可解释性，生成更具视觉依据的输出，减少幻觉现象。

Conclusion: MEDALIGN通过从域特定的CLIP模型蒸馏对齐知识到医疗大视觉-语言模型，显著缓解了幻觉输出问题，提升了视觉对齐性与可解释性。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown promising results in clinical applications, but often suffer from hallucinated outputs due to misaligned visual understanding. In this work, we identify two fundamental limitations contributing to this issue: insufficient visual representation learning and poor visual attention alignment. To address these problems, we propose MEDALIGN, a simple, lightweight alignment distillation framework that transfers visual alignment knowledge from a domain-specific Contrastive Language-Image Pre-training (CLIP) model to Med-LVLMs. MEDALIGN introduces two distillation losses: a spatial-aware visual alignment loss based on visual token-level similarity structures, and an attention-aware distillation loss that guides attention toward diagnostically relevant regions. Extensive experiments on medical report generation and medical visual question answering (VQA) benchmarks show that MEDALIGN consistently improves both performance and interpretability, yielding more visually grounded outputs.

</details>


### [57] [OpenView: Empowering MLLMs with Out-of-view VQA](https://arxiv.org/abs/2512.18563)
*Qixiang Chen,Cheng Zhang,Chi-Wing Fu,Jingwen Ye,Jianfei Cai*

Main category: cs.CV

TL;DR: 提出用于生成视野外问答的管线OpenView，构建合成数据集和评测基准，微调后能将模型平均准确率从48.6%提升到64.1%


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs主要擅长图像视野内（in-view）推理，缺乏对视野外情境推断的能力，需要数据与评测促进此类能力发展

Method: 设计四阶段OpenView数据生成管线、构建OpenView-Dataset用于监督微调，并建立OpenView-Bench用于同时评估选择和推理准确性。

Result: 提出OpenView框架及数据集与基准，解决视野外（OOV）理解问题

Conclusion: OpenView能显著提升MLLMs在OOV VQA任务上的性能，但仍与人类有较大差距

Abstract: Recent multimodal large language models (MLLMs) show great potential in natural image understanding. Yet, they perform well, mainly on reasoning in-view contents within the image frame. This paper presents the first study on out-of-view (OOV) understanding, i.e., the ability to reason objects, activities, and scenes beyond the visible frame of a perspective view. Our technical contributions are threefold. First, we design OpenView, a four-stage pipeline to massively generate multi-choice VQA by leveraging panoramic imagery to enable context-rich and spatial-grounded VQA synthesis with free-view framing. Second, we curate OpenView-Dataset, a high-quality synthetic dataset from diverse real-world panoramas to empower MLLMs upon supervised fine-tuning. Third, we build OpenView-Bench, a benchmark that jointly measures choice and rationale accuracy for interpretable and diagnosable evaluation. Experimental results show that despite having a large gap from human performance in OOV VQA answer selection, upon empowered by OpenView, multiple MLLMs can consistently boost their performance, uplifted from 48.6% to 64.1% on average. Code, benchmark, and data will be available at https://github.com/q1xiangchen/OpenView.

</details>


### [58] [Placenta Accreta Spectrum Detection Using an MRI-based Hybrid CNN-Transformer Model](https://arxiv.org/abs/2512.18573)
*Sumaiya Ali,Areej Alhothali,Ohoud Alzamzami,Sameera Albasri,Ahmed Abduljabbar,Muhammad Alwazzan*

Main category: cs.CV

TL;DR: 提出并评估了一种结合3D DenseNet121与3D ViT的混合模型，在1133例MRI体积数据上测试，独立测试集平均准确率84.3%，优于其他3D模型，显示出作为辅助诊断工具的潜力。


<details>
  <summary>Details</summary>
Motivation: PAS在MRI上的诊断存在较大主观性和可变性，需自动化方法提高一致性与准确性。

Method: 构建混合模型：3D DenseNet121负责局部特征提取，3D Vision Transformer建模全局空间上下文；在回顾性1133例MRI体积数据集上训练和评估，比较多种3D深度学习结构，使用五次运行取平均评价性能。

Result: Hybrid 3D DenseNet121-ViT model achieved best results for PAS detection from MRI volumes.

Conclusion: Hybrid CNN-Transformer outperforms other 3D architectures; potential to improve radiologist consistency and diagnostic accuracy for PAS.

Abstract: Placenta Accreta Spectrum (PAS) is a serious obstetric condition that can be challenging to diagnose with Magnetic Resonance Imaging (MRI) due to variability in radiologists' interpretations. To overcome this challenge, a hybrid 3D deep learning model for automated PAS detection from volumetric MRI scans is proposed in this study. The model integrates a 3D DenseNet121 to capture local features and a 3D Vision Transformer (ViT) to model global spatial context. It was developed and evaluated on a retrospective dataset of 1,133 MRI volumes. Multiple 3D deep learning architectures were also evaluated for comparison. On an independent test set, the DenseNet121-ViT model achieved the highest performance with a five-run average accuracy of 84.3%. These results highlight the strength of hybrid CNN-Transformer models as a computer-aided diagnosis tool. The model's performance demonstrates a clear potential to assist radiologists by providing a robust decision support to improve diagnostic consistency across interpretations, and ultimately enhance the accuracy and timeliness of PAS diagnosis.

</details>


### [59] [Commercial Vehicle Braking Optimization: A Robust SIFT-Trajectory Approach](https://arxiv.org/abs/2512.18597)
*Zhe Li,Kun Cheng,Hanyue Mo,Jintao Lu,Ziwen Kuang,Jianwen Ye,Lixu Xu,Xinya Meng,Jiahui Zhao,Shengda Ji,Shuyuan Liu,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出一种基于盲区摄像头的视频轨迹分析算法，结合自适应CLAHE增强SIFT特征、KNN-RANSAC匹配、5帧滑窗位移统计、双阈值决策和OBD动态ROI，实时高精度判定车辆低速运动状态，实测显著减少误刹


<details>
  <summary>Details</summary>
Motivation: 商业车AEB在低速时因CAN信号抖动误判为移动导致误刹或无法制动；需采用视觉备份提高运动状态识别可靠性

Method: 在Jetson AGX Xavier上处理连续视频帧，先用自适应CLAHE增强图像，再用SIFT提取特征，采用KNN-RANSAC进行匹配；基于5帧滑窗计算位移统计，并结合双阈值矩阵与OBD-II提供的动态ROI调整，实现动静分类

Result: 视觉轨迹分析用于解决低速运行时CAN信号不准导致的“零速制动”问题

Conclusion: 基于Jetson AGX Xavier的SIFT+KNN-RANSAC匹配与多帧位移统计和双阈值决策矩阵，可在复杂环境下高精度识别静止/振动/移动三类状态，显著降低误刹和故障率

Abstract: A vision-based trajectory analysis solution is proposed to address the "zero-speed braking" issue caused by inaccurate Controller Area Network (CAN) signals in commercial vehicle Automatic Emergency Braking (AEB) systems during low-speed operation. The algorithm utilizes the NVIDIA Jetson AGX Xavier platform to process sequential video frames from a blind spot camera, employing self-adaptive Contrast Limited Adaptive Histogram Equalization (CLAHE)-enhanced Scale-Invariant Feature Transform (SIFT) feature extraction and K-Nearest Neighbors (KNN)-Random Sample Consensus (RANSAC) matching. This allows for precise classification of the vehicle's motion state (static, vibration, moving). Key innovations include 1) multiframe trajectory displacement statistics (5-frame sliding window), 2) a dual-threshold state decision matrix, and 3) OBD-II driven dynamic Region of Interest (ROI) configuration. The system effectively suppresses environmental interference and false detection of dynamic objects, directly addressing the challenge of low-speed false activation in commercial vehicle safety systems. Evaluation in a real-world dataset (32,454 video segments from 1,852 vehicles) demonstrates an F1-score of 99.96% for static detection, 97.78% for moving state recognition, and a processing delay of 14.2 milliseconds (resolution 704x576). The deployment on-site shows an 89% reduction in false braking events, a 100% success rate in emergency braking, and a fault rate below 5%.

</details>


### [60] [SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback](https://arxiv.org/abs/2512.18599)
*Jianglin Lu,Yuanwei Wu,Ziyi Zhao,Hongcheng Wang,Felix Jimenez,Abrar Majeedi,Yun Fu*

Main category: cs.CV

TL;DR: 提出一种基于策略优化的无标签图像恢复框架，训练轻量智能体决定工具调用序列，使用多模态大模型提供奖励信号。推理时确定性执行，显著提速，在无监督条件下在全参考指标匹配SOTA、无参考指标更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的恢复代理效率低下（反思、回滚、迭代搜索）且依赖大量标注的降解识别模型，限制无标签场景应用。需设计高效、无监督的恢复策略学习方法。

Method: 将恢复视为序贯决策问题，训练轻量策略代理通过策略优化选择每步的恢复操作。引入多模态大模型作为人类对齐的评估器，提供感知奖励以在无标签环境中训练。训练后代理输出确定性工具调用序列以避免冗余。

Result: 在多种复合降解场景下，方法在无监督条件下在全参考指标达到SOTA、在无参考指标超越现有方法，同时显著减少推理时间和工具调用次数。

Conclusion: The paper presents a policy optimization-based, label-free framework that trains a lightweight agent to plan deterministic restoration tool sequences using a multimodal LLM-driven reward; it improves efficiency and matches or exceeds SOTA on various metrics.

Abstract: Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.

</details>


### [61] [Text2Graph VPR: A Text-to-Graph Expert System for Explainable Place Recognition in Changing Environments](https://arxiv.org/abs/2512.18613)
*Saeideh Yousefzadeh,Hamidreza Pourreza*

Main category: cs.CV

TL;DR: 将图像→文本→场景图→图匹配，融合GAT与SP核，实现可解释且鲁棒的长期视觉定位。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素或深度的VPR在光照、天气、季节变化下鲁棒性差且不可解释，需引入语义与拓扑信息以增强长期部署可靠性并提高可解释性。

Method: 1) 图像序列转文本场景描述；2) 文本解析为场景图（对象、属性、关系）；3) 聚合帧级场景图为地点图谱；4) 使用双相似度检索：GAT学得的嵌入与基于最短路径的结构核结合；5) 支持零样本文本查询与可视化中间步骤。

Result: Text2Graph VPR 提出了一种基于语义场景图的可解释视觉定位方法，将图像序列转为文本描述，解析为场景图，聚合为地点表示，并结合GAT嵌入与最短路径核进行检索。

Conclusion: 混合的学习与结构匹配策略在大幅外观变化下表现稳健，且中间语义表示支持可解释性和零样本文本查询。

Abstract: Visual Place Recognition (VPR) in long-term deployment requires reasoning beyond pixel similarity: systems must make transparent, interpretable decisions that remain robust under lighting, weather and seasonal change. We present Text2Graph VPR, an explainable semantic localization system that converts image sequences into textual scene descriptions, parses those descriptions into structured scene graphs, and reasons over the resulting graphs to identify places. Scene graphs capture objects, attributes and pairwise relations; we aggregate per-frame graphs into a compact place representation and perform retrieval with a dual-similarity mechanism that fuses learned Graph Attention Network (GAT) embeddings and a Shortest-Path (SP) kernel for structural matching. This hybrid design enables both learned semantic matching and topology-aware comparison, and -- critically -- produces human-readable intermediate representations that support diagnostic analysis and improve transparency in the decision process. We validate the system on Oxford RobotCar and MSLS (Amman/San Francisco) benchmarks and demonstrate robust retrieval under severe appearance shifts, along with zero-shot operation using human textual queries. The results illustrate that semantic, graph-based reasoning is a viable and interpretable alternative for place recognition, particularly suited to safety-sensitive and resource-constrained settings.

</details>


### [62] [PTTA: A Pure Text-to-Animation Framework for High-Quality Creation](https://arxiv.org/abs/2512.18614)
*Ruiqi Chen,Kaitong Cai,Yijia Fan,Keze Wang*

Main category: cs.CV

TL;DR: PTTA: fine-tune HunyuanVideo on small high-quality animation-text dataset to achieve better text-to-animation results.


<details>
  <summary>Details</summary>
Motivation: paper addresses gap: existing video generation models work poorly for animation; text-to-video for animation underexplored

Method: construct small high-quality animation-text paired dataset; fine-tune pretrained text-to-video HunyuanVideo to animation style

Result: visual evaluations show PTTA outperforms baselines across multiple dimensions

Conclusion: PTTA is effective pure text-to-animation framework enabling high-quality animation generation via fine-tuning with small dataset

Abstract: Traditional animation production involves complex pipelines and significant manual labor cost. While recent video generation models such as Sora, Kling, and CogVideoX achieve impressive results on natural video synthesis, they exhibit notable limitations when applied to animation generation. Recent efforts, such as AniSora, demonstrate promising performance by fine-tuning image-to-video models for animation styles, yet analogous exploration in the text-to-video setting remains limited.
  In this work, we present PTTA, a pure text-to-animation framework for high-quality animation creation. We first construct a small-scale but high-quality paired dataset of animation videos and textual descriptions. Building upon the pretrained text-to-video model HunyuanVideo, we perform fine-tuning to adapt it to animation-style generation. Extensive visual evaluations across multiple dimensions show that the proposed approach consistently outperforms comparable baselines in animation video synthesis.

</details>


### [63] [Uni-Neur2Img: Unified Neural Signal-Guided Image Generation, Editing, and Stylization via Diffusion Transformers](https://arxiv.org/abs/2512.18635)
*Xiyue Bai,Ronghao Yu,Jia Xiu,Pengfei Zhou,Jie Xia,Peng Ji*

Main category: cs.CV

TL;DR: 提出Uni-Neur2Img，通过LoRA式可插拔神经信号注入与因果注意力实现多模态、参数高效的EEG驱动图像生成/编辑，实验在多个数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 将神经信号直接用于图像生成与编辑，弥合神经科学与视觉生成之间的空白，提供统一高效的多模态条件控制方法。

Method: 提出Uni-Neur2Img框架：使用LoRA风格的参数高效神经信号注入模块作为可插拔条件器，独立处理每种条件信号；引入因果注意力以应对长序列建模；构建并使用EEG-Style数据集以支持视觉模态作为直接条件。

Result: 在公共基准与自采数据上进行大规模评估，包括CVPR40（EEG驱动生成）、Loongx（神经信号引导的图像编辑）和EEG-Style（EEG驱动风格迁移）。实验表明在生成保真度、编辑一致性和风格迁移质量上有显著提升，同时保持低计算开销和良好扩展性。

Conclusion: Uni-Neur2Img提供了一个统一、可扩展且参数高效的解决方案，将神经信号与视觉内容生成连接起来，支持多模态条件输入且不需修改基础模型参数。

Abstract: Generating or editing images directly from Neural signals has immense potential at the intersection of neuroscience, vision, and Brain-computer interaction. In this paper, We present Uni-Neur2Img, a unified framework for neural signal-driven image generation and editing. The framework introduces a parameter-efficient LoRA-based neural signal injection module that independently processes each conditioning signal as a pluggable component, facilitating flexible multi-modal conditioning without altering base model parameters. Additionally, we employ a causal attention mechanism accommodate the long-sequence modeling demands of conditional generation tasks. Existing neural-driven generation research predominantly focuses on textual modalities as conditions or intermediate representations, resulting in limited exploration of visual modalities as direct conditioning signals. To bridge this research gap, we introduce the EEG-Style dataset. We conduct comprehensive evaluations across public benchmarks and self-collected neural signal datasets: (1) EEG-driven image generation on the public CVPR40 dataset; (2) neural signal-guided image editing on the public Loongx dataset for semantic-aware local modifications; and (3) EEG-driven style transfer on our self-collected EEG-Style dataset. Extensive experimental results demonstrate significant improvements in generation fidelity, editing consistency, and style transfer quality while maintaining low computational overhead and strong scalability to additional modalities. Thus, Uni-Neur2Img offers a unified, efficient, and extensible solution for bridging neural signals and visual content generation.

</details>


### [64] [Geometric-Photometric Event-based 3D Gaussian Ray Tracing](https://arxiv.org/abs/2512.18640)
*Kai Kohyama,Yoshimitsu Aoki,Guillermo Gallego,Shintaro Shiba*

Main category: cs.CV

TL;DR: 提出将事件流分为按事件计算深度和基于事件图像计算颜色的双分支3DGS方法，无需预训练模型或COLMAP初始化，在真实数据集上达SOTA并能快速训练、在边缘处重建更清晰。


<details>
  <summary>Details</summary>
Motivation: 事件相机能提供比传统帧相机更高的时间分辨率，但稀疏事件如何被用于基于3D高斯点的场景重建仍不清楚。现有方法在精度与时间分辨率之间存在权衡，且常依赖预训练图像重建或COLMAP初始化。

Method: 核心是将渲染分为两个独立分支：1) 事件逐条进行的几何渲染，通过光线追踪利用稀疏事件直接估计深度；2) 基于事件对齐（warp）生成的快照图像进行的辐射/强度渲染以估计颜色。两分支分别优化并在最终合成时融合，从而利用事件的细粒度时间信息同时保持色彩一致性。

Result: 在多个真实场景数据集上实现了最先进的性能，在合成数据集上表现竞争性。方法不依赖预训练图像重建模型或COLMAP初始化，对事件数选择更灵活，训练速度快，并在场景边缘处得到更锐利的重建。代码将开源。

Conclusion: 本文提出了一种用于事件相机的3D高斯点渲染（3DGS）框架，通过将渲染解耦为逐事件的几何（深度）渲染与快照式的辐射（强度）渲染，兼顾高时间分辨率与重建精度。

Abstract: Event cameras offer a high temporal resolution over traditional frame-based cameras, which makes them suitable for motion and structure estimation. However, it has been unclear how event-based 3D Gaussian Splatting (3DGS) approaches could leverage fine-grained temporal information of sparse events. This work proposes a framework to address the trade-off between accuracy and temporal resolution in event-based 3DGS. Our key idea is to decouple the rendering into two branches: event-by-event geometry (depth) rendering and snapshot-based radiance (intensity) rendering, by using ray-tracing and the image of warped events. The extensive evaluation shows that our method achieves state-of-the-art performance on the real-world datasets and competitive performance on the synthetic dataset. Also, the proposed method works without prior information (e.g., pretrained image reconstruction models) or COLMAP-based initialization, is more flexible in the event selection number, and achieves sharp reconstruction on scene edges with fast training time. We hope that this work deepens our understanding of the sparse nature of events for 3D reconstruction. The code will be released.

</details>


### [65] [Adversarial Robustness in Zero-Shot Learning:An Empirical Study on Class and Concept-Level Vulnerabilities](https://arxiv.org/abs/2512.18651)
*Zhiyuan Peng,Zihan Ye,Shreyank N Gowda,Yuping Yan,Haotian Xu,Ling Shao*

Main category: cs.CV

TL;DR: ZSL models can be broken by class-level and concept-level attacks; CBEA fully breaks GZSL; concept attacks manipulate concepts.


<details>
  <summary>Details</summary>
Motivation: Assess robustness of ZSL models to systematic input perturbations and concept manipulation.

Method: Empirical evaluation of clsA, CBEA, CPconA, NCPconA across three ZSL models and various architectures; analyze calibration effects and propose class-bias enhancement.

Result: Paper analyzes ZSL robustness under attacks.

Conclusion: ZSL models vulnerable; propose CBEA and concept attacks CPconA/NCPconA; need robustness improvements.

Abstract: Zero-shot Learning (ZSL) aims to enable image classifiers to recognize images from unseen classes that were not included during training. Unlike traditional supervised classification, ZSL typically relies on learning a mapping from visual features to predefined, human-understandable class concepts. While ZSL models promise to improve generalization and interpretability, their robustness under systematic input perturbations remain unclear. In this study, we present an empirical analysis about the robustness of existing ZSL methods at both classlevel and concept-level. Specifically, we successfully disrupted their class prediction by the well-known non-target class attack (clsA). However, in the Generalized Zero-shot Learning (GZSL) setting, we observe that the success of clsA is only at the original best-calibrated point. After the attack, the optimal bestcalibration point shifts, and ZSL models maintain relatively strong performance at other calibration points, indicating that clsA results in a spurious attack success in the GZSL. To address this, we propose the Class-Bias Enhanced Attack (CBEA), which completely eliminates GZSL accuracy across all calibrated points by enhancing the gap between seen and unseen class probabilities.Next, at concept-level attack, we introduce two novel attack modes: Class-Preserving Concept Attack (CPconA) and NonClass-Preserving Concept Attack (NCPconA). Our extensive experiments evaluate three typical ZSL models across various architectures from the past three years and reveal that ZSL models are vulnerable not only to the traditional class attack but also to concept-based attacks. These attacks allow malicious actors to easily manipulate class predictions by erasing or introducing concepts. Our findings highlight a significant performance gap between existing approaches, emphasizing the need for improved adversarial robustness in current ZSL models.

</details>


### [66] [SplatBright: Generalizable Low-Light Scene Reconstruction from Sparse Views via Physically-Guided Gaussian Enhancement](https://arxiv.org/abs/2512.18655)
*Yue Wen,Liang Song,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出SplatBright，一种通用的3D高斯表示方法，用于从稀疏sRGB低光输入实现低光增强与三维重建，结合物理引导的照明建模和几何-外观解耦，通过双分支预测器、频率先验的照明一致性与外观细化模块来恢复一致且细节丰富的视图；使用基于相机物理的暗视图合成进行训练，在公开和自采数据上优于现有2D/3D方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低光稀疏视角下常出现曝光不一致、颜色退化和视图不一致性，且多数需要对每个场景单独训练，缺乏泛化能力。研究目标是实现泛化的、跨视图一致的低光增强与3D重建。

Method: 提出SplatBright：
- 双分支预测器：一支提供稳定的几何初始化用于3D高斯参数；另一支负责外观估计。
- 物理引导的照明建模与频率先验：在外观分支引入频率先验以实现可控且跨视图一致的照明表现。
- 外观细化模块：将照明、材质和视角相关项进一步解耦以恢复细节纹理。
- 数据合成：采用基于相机物理模型的暗视图合成来弥补缺乏大规模几何一致配对数据的问题。
训练目标是联合优化几何和外观以提高新视图合成质量和一致性。

Result: 在公开数据集和自采低光场景上，SplatBright在新视图合成质量、跨视图一致性和泛化能力上均优于现有的2D低光增强方法和基于NeRF/3D的重建方法。实验展示了更好的颜色恢复、细节保留及视图间照明一致性。

Conclusion: SplatBright通过物理感知的照明建模与几何-外观解耦，实现了首个泛化的3D高斯框架用于稀疏sRGB低光增强与重建。该方法在一致性、细节恢复和泛化性方面带来明显提升，并可通过合成暗视图的训练策略扩展到更多场景。

Abstract: Low-light 3D reconstruction from sparse views remains challenging due to exposure imbalance and degraded color fidelity. While existing methods struggle with view inconsistency and require per-scene training, we propose SplatBright, which is, to our knowledge, the first generalizable 3D Gaussian framework for joint low-light enhancement and reconstruction from sparse sRGB inputs. Our key idea is to integrate physically guided illumination modeling with geometry-appearance decoupling for consistent low-light reconstruction. Specifically, we adopt a dual-branch predictor that provides stable geometric initialization of 3D Gaussian parameters. On the appearance side, illumination consistency leverages frequency priors to enable controllable and cross-view coherent lighting, while an appearance refinement module further separates illumination, material, and view-dependent cues to recover fine texture. To tackle the lack of large-scale geometrically consistent paired data, we synthesize dark views via a physics-based camera model for training. Extensive experiments on public and self-collected datasets demonstrate that SplatBright achieves superior novel view synthesis, cross-view consistency, and better generalization to unseen low-light scenes compared with both 2D and 3D methods.

</details>


### [67] [PMPGuard: Catching Pseudo-Matched Pairs in Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2512.18660)
*Pengxiang Ouyang,Qing Ma,Zheng Wang,Cong Bai*

Main category: cs.CV

TL;DR: 提出带门控的跨模态注意力和正负意识注意力，用于识别并抑制伪匹配或误导信息，在RSICD、RSITMD、RS5M上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 真实遥感图文数据集中存在大量伪匹配对（语义不匹配或弱对齐），这些噪声配对会误导模型学习错误的跨模态对齐，从而降低检索性能。需设计机制区分有用信息与误导信息，减少噪声影响。

Method: 引入跨模态门控注意力模块以动态调节来自另一模态的信息流，结合正负意识注意力机制显式区分并强调正样本中的有信息区域，抑制负样本或误导性特征。在训练阶段通过注意力加权策略对相似性损失进行调节，从而弱化伪匹配对的贡献。

Result: 在RSICD、RSITMD和大规模RS5M基准上进行广泛实验，所提方法在多项检索指标上均超过现有方法，证明了其在处理真实世界不匹配与PMPs方面的有效性。

Conclusion: 本文提出的跨模态门控注意力与正负意识注意力机制，有效抑制伪匹配对(PMPs)对遥感图文检索的负面影响，从而提升模型在真实世界弱对齐数据集上的鲁棒性与准确性。

Abstract: Remote sensing (RS) image-text retrieval faces significant challenges in real-world datasets due to the presence of Pseudo-Matched Pairs (PMPs), semantically mismatched or weakly aligned image-text pairs, which hinder the learning of reliable cross-modal alignments. To address this issue, we propose a novel retrieval framework that leverages Cross-Modal Gated Attention and a Positive-Negative Awareness Attention mechanism to mitigate the impact of such noisy associations. The gated module dynamically regulates cross-modal information flow, while the awareness mechanism explicitly distinguishes informative (positive) cues from misleading (negative) ones during alignment learning. Extensive experiments on three benchmark RS datasets, i.e., RSICD, RSITMD, and RS5M, demonstrate that our method consistently achieves state-of-the-art performance, highlighting its robustness and effectiveness in handling real-world mismatches and PMPs in RS image-text retrieval tasks.

</details>


### [68] [SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse](https://arxiv.org/abs/2512.18671)
*Yiming Sun,Mi Zhang,Feifei Li,Geng Hong,Min Yang*

Main category: cs.CV

TL;DR: SmartSight通过多候选生成和基于注意力的幻觉评分（Temporal Attention Collapse与Visual Attention Vanishing）在无需训练的情况下显著降低Video-LLMs的感知幻觉并提升视频理解与推理性能，同时节省解码成本。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在感知幻觉（perceptual hallucinations）问题上存在安全隐患，限制了其实际应用。现有抑制方法往往损害模型的视频理解与推理能力。作者提出无需训练的新方法，利用模型自我内省能力来降低幻觉同时保留理解能力。

Method: 提出SmartSight方法：通过生成多个候选响应以发现低幻觉输出；使用Temporal Attention Collapse（时间注意力坍塌）评分来评估每个响应的幻觉程度，衡量模型是否过度关注视频中的琐碎时间区域；引入Visual Attention Vanishing点以提高估计精度并实现对幻觉响应的早期终止，从而降低解码成本。

Result: 在实验中，SmartSight显著降低了Qwen2.5-VL-7B在VRIPT-HAL数据集上的幻觉率，降低幅度为10.59%；同时在VideoMMMU任务上提升视频理解与推理性能，上升最多8.86%。方法还能减少解码开销。

Conclusion: SmartSight是一种训练-free、基于模型内省的有效策略，能在降低感知幻觉的同时提升或保持视频理解与推理能力，增强开源Video-LLMs的可靠性和实用性。

Abstract: Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.

</details>


### [69] [AsyncDiff: Asynchronous Timestep Conditioning for Enhanced Text-to-Image Diffusion Inference](https://arxiv.org/abs/2512.18675)
*Longhuan Xu,Feng Yin,Cunjian Chen*

Main category: cs.CV

TL;DR: 本文提出了一种在文本到图像扩散推断中解耦数值积分步与去噪器条件步的异步推断机制，通过轻量级的时间步预测模块（TPM）和组相对策略优化（GRPO）学习在不改变图像更新计划前提下选择更合适的条件时间步，从而控制图像细节与纹理丰富度。只在低步数（SD3.5 15步，Flux 10步）下测试，基于复合奖励在MS-COCO和T2I-CompBench上展示一致提升。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散推断通常同步推进积分器与去噪器条件时间步，限制了模型在低步数推断时对噪声水平与图像细节的灵活控制。作者希望通过解耦这两者来改善低步数下的图像质量与描述一致性。

Method: 引入一个轻量的时间步预测模块（TPM），用组相对策略优化（GRPO）训练，该模块基于当前潜在状态预测去噪器应使用的条件时间步，与积分器实际推进的更新时间步不同。训练目标为最大化复合奖励（Image Reward, HPSv2, CLIP Score, Pick Score平均）。部署时可用一个缩放超参数在原始时间步与预测时间步之间插值以调整保守或激进策略。实验在Stable Diffusion 3.5 Medium和Flux.1-dev上，推断步数分别上限为15和10步。

Result: 在MS-COCO 2014和T2I-CompBench上，使用复合奖励优化后，方法在多项评测指标上较同步基线表现出一致提升，表明异步条件时间步能够在同样的图像更新计划下改善低步数推断结果。

Conclusion: 在低步数推断场景，通过让去噪器在与积分器不同的、更合适的条件时间步上运行，可以有效提升图像质量和文本-图像一致性。TPM+GRPO提供了一种可控且轻量的手段，部署时通过缩放超参数可在保守与激进策略间平衡。

Abstract: Text-to-image diffusion inference typically follows synchronized schedules, where the numerical integrator advances the latent state to the same timestep at which the denoiser is conditioned. We propose an asynchronous inference mechanism that decouples these two, allowing the denoiser to be conditioned at a different, learned timestep while keeping image update schedule unchanged. A lightweight timestep prediction module (TPM), trained with Group Relative Policy Optimization (GRPO), selects a more feasible conditioning timestep based on the current state, effectively choosing a desired noise level to control image detail and textural richness. At deployment, a scaling hyper-parameter can be used to interpolate between the original and de-synchronized timesteps, enabling conservative or aggressive adjustments. To keep the study computationally affordable, we cap the inference at 15 steps for SD3.5 and 10 steps for Flux. Evaluated on Stable Diffusion 3.5 Medium and Flux.1-dev across MS-COCO 2014 and T2I-CompBench datasets, our method optimizes a composite reward that averages Image Reward, HPSv2, CLIP Score and Pick Score, and shows consistent improvement.

</details>


### [70] [brat: Aligned Multi-View Embeddings for Brain MRI Analysis](https://arxiv.org/abs/2512.18679)
*Maxime Kayser,Maksim Gridnev,Wanting Wang,Max Bain,Aneesh Rangnekar,Avijit Chatterjee,Aleksandr Petrov,Harini Veeraraghavan,Nathaniel C. Swinburne*

Main category: cs.CV

TL;DR: 提出BRAT：在约80k含报告的脑MRI上进行多视图预训练，通过隐式查询-特征匹配与质量-多样性概念将MRI多视图嵌入对齐到报告句子，显著提升视—语言与视觉任务表现，并发布模型与数据集。


<details>
  <summary>Details</summary>
Motivation: 脑MRI含大量细微、局部异常，单一全卷积或单视图表示难以捕捉。临床报告提供丰富、局部化的语义提示，利用报告句子进行多视图对齐可强化模型对局部异常的识别与表达能力。

Method: 构建约80k 3D脑MRI-报告配对数据集；设计多视图预训练框架，使用隐式查询（从报告句子生成或学习的查询表示）与图像特征进行匹配；引入质量-多样性（quality-diversity）机制以生成多个互补的视图嵌入并避免模式坍缩；在多个视觉—语言与纯视觉下游任务上进行评估并公开模型。

Result: The paper introduces BRAT, a multi-view representation learning framework for brain MRI using paired radiology reports. They build a large dataset (~80k 3D scans with reports) and propose a multi-view pretraining inspired by document retrieval, using implicit query-feature matching and quality-diversity concepts to produce multi-view embeddings aligned with report sentence features. They show significant gains on vision-language and vision tasks and release foundation models.

Conclusion: Multi-view alignment of MRI slices to report sentences via implicit query matching and quality-diversity yields stronger representations, improving downstream performance across tasks; dataset and models are released publicly.

Abstract: We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.

</details>


### [71] [A Study of Finetuning Video Transformers for Multi-view Geometry Tasks](https://arxiv.org/abs/2512.18684)
*Huimin Wu,Kwang-Ting Cheng,Stephen Lin,Zhirong Wu*

Main category: cs.CV

TL;DR: Video-pretrained vision transformers, with a linear decoder and iterative refinement, achieve SOTA optical flow and strong multi-view geometry results, showing excellent cross-dataset generalization.


<details>
  <summary>Details</summary>
Motivation: Investigate whether general video foundation models can be directly adapted to multi-view geometry tasks without custom architectures or task-specific pretraining.

Method: Fine-tune video-pretrained ViT with linear decoder and iterative refinement for optical flow and multi-view tasks

Result: State-of-the-art optical flow EPE: Sintel clean 0.69, Sintel final 1.78, KITTI 3.15; online test EPE: 0.79,1.88; F1 3.79. Strong performance on depth and stereo.

Conclusion: General-purpose video-pretrained transformers transfer effectively to geometric vision tasks with minimal adaptation; simple linear decoder plus refinement suffices.

Abstract: This paper presents an investigation of vision transformer learning for multi-view geometry tasks, such as optical flow estimation, by fine-tuning video foundation models. Unlike previous methods that involve custom architectural designs and task-specific pretraining, our research finds that general-purpose models pretrained on videos can be readily transferred to multi-view problems with minimal adaptation. The core insight is that general-purpose attention between patches learns temporal and spatial information for geometric reasoning. We demonstrate that appending a linear decoder to the Transformer backbone produces satisfactory results, and iterative refinement can further elevate performance to stateof-the-art levels. This conceptually simple approach achieves top cross-dataset generalization results for optical flow estimation with end-point error (EPE) of 0.69, 1.78, and 3.15 on the Sintel clean, Sintel final, and KITTI datasets, respectively. Our method additionally establishes a new record on the online test benchmark with EPE values of 0.79, 1.88, and F1 value of 3.79. Applications to 3D depth estimation and stereo matching also show strong performance, illustrating the versatility of video-pretrained models in addressing geometric vision tasks.

</details>


### [72] [EcoSplat: Efficiency-controllable Feed-forward 3D Gaussian Splatting from Multi-view Images](https://arxiv.org/abs/2512.18692)
*Jongmin Park,Minh-Quan Viet Bui,Juan Luis Gonzalez Bello,Jaeho Moon,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: EcoSplat通过两阶段训练（像素对齐高斯训练+重要性感知微调）实现推理时按目标原语数量自适应生成高斯表示，从而在严格原语数约束下提升效率与效果。


<details>
  <summary>Details</summary>
Motivation: 现有前馈3DGS在密集视角下会生成过多像素对齐原语，缺乏对原语数量的显式控制，导致效率低下和资源浪费。

Method: 采用两阶段优化：1) 像素对齐高斯训练(PGT)学习初始预测；2) 重要性感知高斯微调(IGF)学习原语排序并根据目标原语数自适应调整参数，从而在推理时按需裁剪或重调高斯集合。

Result: 提出EcoSplat，一种可控效率的前馈3D Gaussian Splatting框架，能在推理时针对任意目标原语数量自适应预测3D表示。

Conclusion: EcoSplat在多视角密集场景下对受限高斯原语数量表现更稳健，优于现有方法，适合灵活的下游渲染任务。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) enables efficient one-pass scene reconstruction, providing 3D representations for novel view synthesis without per-scene optimization. However, existing methods typically predict pixel-aligned primitives per-view, producing an excessive number of primitives in dense-view settings and offering no explicit control over the number of predicted Gaussians. To address this, we propose EcoSplat, the first efficiency-controllable feed-forward 3DGS framework that adaptively predicts the 3D representation for any given target primitive count at inference time. EcoSplat adopts a two-stage optimization process. The first stage is Pixel-aligned Gaussian Training (PGT) where our model learns initial primitive prediction. The second stage is Importance-aware Gaussian Finetuning (IGF) stage where our model learns rank primitives and adaptively adjust their parameters based on the target primitive count. Extensive experiments across multiple dense-view settings show that EcoSplat is robust and outperforms state-of-the-art methods under strict primitive-count constraints, making it well-suited for flexible downstream rendering tasks.

</details>


### [73] [Rectification Reimagined: A Unified Mamba Model for Image Correction and Rectangling with Prompts](https://arxiv.org/abs/2512.18718)
*Linwei Qiu,Gongzhe Li,Xiaozhe Zhang,Qinlin Sun,Fengying Xie*

Main category: cs.CV

TL;DR: 提出 UniRect，一个统一的图像校正框架，通过模拟多种镜头畸变，把多任务转为一致的失真校正问题；框架由变形模块（RP-TPS）和恢复模块（RMBs）组成，并引入SMoEs以改善多任务训练，实验显示达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为任务专用，泛化能力差，难以在不同校正任务间共享并处理多种失真类型，实际应用（如手机摄影）需要更通用且高效的方案。

Method: 将各种任务相关的逆问题统一建模为通用的畸变模型，通过镜头模拟生成不同失真。采用任务无关的双组件结构：Deformation Module使用残差渐进薄板样条（RP-TPS）解决复杂几何变形；Restoration Module用残差Mamba块（RMBs）恢复因变形引入的劣化并提升细节。为缓解多任务学习中的任务竞争，设计了稀疏专家混合（SMoEs）结构进行路由和参数分配。

Result: 大量实验表明UniRect在多个图像校正/矩形化任务上优于现有最新方法，取得了SOTA性能，并能处理多种类型的失真同时保持较好视觉质量和细节恢复。

Conclusion: 通过统一的畸变视角和模块化设计，UniRect实现了跨任务的强泛化能力与高质量校正效果，SMoEs进一步提升了多任务协同训练的稳定性和性能，具备良好的实际应用前景。

Abstract: Image correction and rectangling are valuable tasks in practical photography systems such as smartphones. Recent remarkable advancements in deep learning have undeniably brought about substantial performance improvements in these fields. Nevertheless, existing methods mainly rely on task-specific architectures. This significantly restricts their generalization ability and effective application across a wide range of different tasks. In this paper, we introduce the Unified Rectification Framework (UniRect), a comprehensive approach that addresses these practical tasks from a consistent distortion rectification perspective. Our approach incorporates various task-specific inverse problems into a general distortion model by simulating different types of lenses. To handle diverse distortions, UniRect adopts one task-agnostic rectification framework with a dual-component structure: a {Deformation Module}, which utilizes a novel Residual Progressive Thin-Plate Spline (RP-TPS) model to address complex geometric deformations, and a subsequent Restoration Module, which employs Residual Mamba Blocks (RMBs) to counteract the degradation caused by the deformation process and enhance the fidelity of the output image. Moreover, a Sparse Mixture-of-Experts (SMoEs) structure is designed to circumvent heavy task competition in multi-task learning due to varying distortions. Extensive experiments demonstrate that our models have achieved state-of-the-art performance compared with other up-to-date methods.

</details>


### [74] [Breast Cancer Recurrence Risk Prediction Based on Multiple Instance Learning](https://arxiv.org/abs/2512.18734)
*Jinqiu Chen,Huyan Xu*

Main category: cs.CV

TL;DR: 使用CLAM-SB、ABMIL和ConvNeXt-MIL-XGBoost三种MIL框架在210例病例的H&E全片图上预测乳腺癌5年复发风险，CLAM-SB表现最佳（平均AUC 0.836，准确率76.2%）。


<details>
  <summary>Details</summary>
Motivation: 21基因复发评分虽具预测价值但成本高且耗时，研究旨在探索能否通过常规组织病理切片和深度学习提供快速、低成本的复发风险分层。

Method: 在210例病例上构建三种基于多实例学习的模型（CLAM-SB、ABMIL、ConvNeXt-MIL-XGBoost），使用UNI和CONCH预训练模型提取切片特征，基于21基因RS将5年复发风险分为低/中/高三类，采用5折交叉验证评估模型性能并比较AUC与准确率。

Result: 在5折交叉验证中，改进后的CLAM-SB在三分类任务上表现最佳，平均AUC为0.836，分类准确率为76.2%，表明MIL方法能从常规H&E切片中提取与基因组学相关的风险信号。

Conclusion: 该研究证明利用常规H&E切片结合深度学习的MIL框架可有效预测乳腺癌5年复发风险，提供与21基因复发评分相关的自动分层方法，具有潜在临床应用价值。

Abstract: Predicting breast cancer recurrence risk is a critical clinical challenge. This study investigates the potential of computational pathology to stratify patients using deep learning on routine Hematoxylin and Eosin (H&E) stained whole-slide images (WSIs). We developed and compared three Multiple Instance Learning (MIL) frameworks -- CLAM-SB, ABMIL, and ConvNeXt-MIL-XGBoost -- on an in-house dataset of 210 patient cases. The models were trained to predict 5-year recurrence risk, categorized into three tiers (low, medium, high), with ground truth labels established by the 21-gene Recurrence Score. Features were extracted using the UNI and CONCH pre-trained models. In a 5-fold cross-validation, the modified CLAM-SB model demonstrated the strongest performance, achieving a mean Area Under the Curve (AUC) of 0.836 and a classification accuracy of 76.2%. Our findings demonstrate the feasibility of using deep learning on standard histology slides for automated, genomics-correlated risk stratification, highlighting a promising pathway toward rapid and cost-effective clinical decision support.

</details>


### [75] [$M^3-Verse$: A "Spot the Difference" Challenge for Large Multimodal Models](https://arxiv.org/abs/2512.18735)
*Kewei Wei,Bocheng Hu,Jie Cao,Xiaohan Chen,Zhengxi Lu,Wubing Xia,Weili Xu,Jiaao Wu,Junchen He,Mingyu Jia,Ciyun Zhao,Ye Sun,Yizhi Li,Zhonghan Zhao,Jian Zhang,Gaoang Wang*

Main category: cs.CV

TL;DR: 提出M^3-Verse基准来评估并改进LMMs在多状态、多视角视频中对物体状态转换的理解，评测16款模型并给出有效基线，显著提高相关任务性能。


<details>
  <summary>Details</summary>
Motivation: 评估大规模多模态模型(LMMs)在同一空间语境下理解两个视频观察之间物体动态变化（状态转换）能力的不足，促进空间智能研究。

Method: 构建M^3-Verse基准——包含270个场景、2932个问题、50+子任务，基于成对视频（前后状态、多视角）设计问题以考查4项核心能力；评测16个现有LMMs并提出一个简单有效的基线以提升多状态感知性能。

Result: 发现多数现有LMM在追踪状态转换方面存在明显局限；所提基线在多状态感知任务上显著提升了性能。

Conclusion: M^3-Verse提供了一个具有挑战性的测试平台，推动下一代模型在动态视觉世界中实现更全面的理解，并公开数据和构建流水线以便社区研究。

Abstract: Modern Large Multimodal Models (LMMs) have demonstrated extraordinary ability in static image and single-state spatial-temporal understanding. However, their capacity to comprehend the dynamic changes of objects within a shared spatial context between two distinct video observations, remains largely unexplored. This ability to reason about transformations within a consistent environment is particularly crucial for advancements in the field of spatial intelligence. In this paper, we introduce $M^3-Verse$, a Multi-Modal, Multi-State, Multi-Dimensional benchmark, to formally evaluate this capability. It is built upon paired videos that provide multi-perspective observations of an indoor scene before and after a state change. The benchmark contains a total of 270 scenes and 2,932 questions, which are categorized into over 50 subtasks that probe 4 core capabilities. We evaluate 16 state-of-the-art LMMs and observe their limitations in tracking state transitions. To address these challenges, we further propose a simple yet effective baseline that achieves significant performance improvements in multi-state perception. $M^3-Verse$ thus provides a challenging new testbed to catalyze the development of next-generation models with a more holistic understanding of our dynamic visual world. You can get the construction pipeline from https://github.com/Wal-K-aWay/M3-Verse_pipeline and full benchmark data from https://www.modelscope.cn/datasets/WalKaWay/M3-Verse.

</details>


### [76] [AMLID: An Adaptive Multispectral Landmine Identification Dataset for Drone-Based Detection](https://arxiv.org/abs/2512.18738)
*James E. Gallagher,Edward J. Oughton*

Main category: cs.CV

TL;DR: AMLID: 12k RGB+LWIR labeled images of 21 landmine types across varied fusion levels, altitudes, seasons, and illumination—enables accessible research for automated landmine detection.


<details>
  <summary>Details</summary>
Motivation: Humanitarian need to improve safe, efficient, and accessible landmine detection methods without using live ordnance; current methods hazardous and costly.

Method: Collected multispectral RGB and LWIR imagery via UAS across controlled deployments varying sensor altitude, seasonal ground covers, illumination, and sensor fusion levels; annotated images with mine type labels; organized into open-source dataset for benchmarking.

Result: Created AMLID dataset: 12,078 labeled RGB-LWIR images covering 21 landmine types, 11 fusion levels, 4 altitudes, 2 seasons, 3 illumination conditions.

Conclusion: AMLID fills a gap by providing the first open-source multispectral UAS dataset for landmine detection, enabling safe, low-cost research and benchmarking across diverse conditions.

Abstract: Landmines remain a persistent humanitarian threat, with an estimated 110 million mines deployed across 60 countries, claiming approximately 26,000 casualties annually. Current detection methods are hazardous, inefficient, and prohibitively expensive. We present the Adaptive Multispectral Landmine Identification Dataset (AMLID), the first open-source dataset combining Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) imagery for Unmanned Aerial Systems (UAS)-based landmine detection. AMLID comprises of 12,078 labeled images featuring 21 globally deployed landmine types across anti-personnel and anti-tank categories in both metal and plastic compositions. The dataset spans 11 RGB-LWIR fusion levels, four sensor altitudes, two seasonal periods, and three daily illumination conditions. By providing comprehensive multispectral coverage across diverse environmental variables, AMLID enables researchers to develop and benchmark adaptive detection algorithms without requiring access to live ordnance or expensive data collection infrastructure, thereby democratizing humanitarian demining research.

</details>


### [77] [Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation](https://arxiv.org/abs/2512.18741)
*Tianrui Zhu,Shiyi Zhang,Zhirui Sun,Jingqi Tian,Yansong Tang*

Main category: cs.CV

TL;DR: MAG将记忆模型和生成器分离，压缩历史为KV缓存，提升长视频历史一致性，提出MAG-Bench评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有帧级自回归长视频生成方法在使用窗口注意力时会丢失窗口外历史信息，导致灾难性遗忘和场景不一致；而保留全部历史又会产生高内存成本。为此需要一种在保留历史信息与控制内存之间的折中方法。

Method: 方法包括两阶段：先训练记忆模型，将历史帧编码并压缩成KV缓存；再训练生成器模型，在生成过程中查询该紧凑缓存以生成后续帧；此外设计MAG-Bench用于严格测试历史记忆保留能力。

Result: MAG提出了在长视频生成中通过将记忆压缩和帧生成解耦来解决窗口注意力导致的历史遗忘问题。其主要贡献包括：1) 训练独立的记忆模型，将历史信息压缩为紧凑的KV缓存；2) 训练独立的生成器模型，基于该缓存生成后续帧；3) 提出MAG-Bench基准用于严格评估历史记忆保留。实验显示MAG在历史场景一致性上优于现有方法，并在标准视频生成基准上维持竞争性能。

Conclusion: 通过将记忆压缩与生成分离，MAG在保留长期历史信息的同时避免了全量历史存储的高内存开销，显著改善了长视频的历史一致性问题。

Abstract: Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose \textbf{Memorize-and-Generate (MAG)}, a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce \textbf{MAG-Bench} to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.

</details>


### [78] [InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search](https://arxiv.org/abs/2512.18745)
*Kaican Li,Lewei Yao,Jiannan Wu,Tiezheng Yu,Jierun Chen,Haoli Bai,Lu Hou,Lanqing Hong,Wei Zhang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: 提出一个测试细粒度多步视觉推理的新基准O3-Bench，OpenAI o3表现40.8%。提出InSight-o3框架，包含vReasoner和经过强化学习训练的vSearcher，提高前沿模型表现。代码公开。


<details>
  <summary>Details</summary>
Motivation: 弥补开放多模态系统在复杂视觉推理和通用视觉检索上的不足，评估并提升图像中跨区域细节推理能力

Method: Paper analysis of O3-Bench and InSight-o3

Result: O3-Bench accuracy and InSight-o3 improvements

Conclusion: InSight-o3's vSearcher improves multimodal agents' visual reasoning; benchmark remains challenging

Abstract: The ability for AI agents to "think with images" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .

</details>


### [79] [IPCV: Information-Preserving Compression for MLLM Visual Encoders](https://arxiv.org/abs/2512.18747)
*Yuan Chen,Zichen Wen,Yuzhou Wu,Xuyang Liu,Shuang Chen,Junpeng Ma,Weijia Li,Conghui He,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出IPCV：在ViT内部进行激进视觉token剪枝，使用邻域重构(NGR)临时恢复被剪掉的token参与自注意力，并通过注意力稳定化(AS)近似被剪token的K/V以减轻特征失真，训练-free且显著降低计算。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM计算开销大，主要来自ViT处理大量视觉token。现有两类剪枝方法要么在LLM端忽视ViT成本，要么在ViT端无语言引导导致丢弃文本相关视觉信息并放大双向注意力带来的失真。需要一个无需训练且能在ViT内部安全激进剪枝的方案。

Method: IPCV包含两部分：1) Neighbor-Guided Reconstruction (NGR)：对被剪的token利用邻域重构临时重建，使其以低开销参与注意力计算，随后在传给LLM前完全恢复原始token；2) Attention Stabilization (AS)：为被剪token近似构建K/V以稳定注意力分布，可单独应用于LLM端剪枝方法以改进性能。整个框架无需额外训练。

Result: 在多种图像与视频基准上，IPCV在大幅减少端到端计算的同时，优于现有最先进的无训练token压缩方法。代码已开源。

Conclusion: IPCV是一个训练-free的视觉编码器压缩框架，通过在ViT内部激进剪枝并使用邻域重构与注意力稳定化来保留关键信息，从而显著降低端到端计算开销并在多项基准上优于其他无训练压缩方法。

Abstract: Multimodal Large Language Models (MLLMs) deliver strong vision-language performance but at high computational cost, driven by numerous visual tokens processed by the Vision Transformer (ViT) encoder. Existing token pruning strategies are inadequate: LLM-stage token pruning overlooks the ViT's overhead, while conventional ViT token pruning, without language guidance, risks discarding textually critical visual cues and introduces feature distortions amplified by the ViT's bidirectional attention. To meet these challenges, we propose IPCV, a training-free, information-preserving compression framework for MLLM visual encoders. IPCV enables aggressive token pruning inside the ViT via Neighbor-Guided Reconstruction (NGR) that temporarily reconstructs pruned tokens to participate in attention with minimal overhead, then fully restores them before passing to the LLM. Besides, we introduce Attention Stabilization (AS) to further alleviate the negative influence from token pruning by approximating the K/V of pruned tokens. It can be directly applied to previous LLM-side token pruning methods to enhance their performance. Extensive experiments show that IPCV substantially reduces end-to-end computation and outperforms state-of-the-art training-free token compression methods across diverse image and video benchmarks. Our code is available at https://github.com/Perkzi/IPCV.

</details>


### [80] [Context-Aware Network Based on Multi-scale Spatio-temporal Attention for Action Recognition in Videos](https://arxiv.org/abs/2512.18750)
*Xiaoyang Li,Wenzhu Yang,Kanglin Wang,Tiebiao Wang,Qingsong Fei*

Main category: cs.CV

TL;DR: 提出Context-Aware Network (CAN)，包含多尺度时间线索模块（MTCM）与分组空间线索模块（GSCM），在五个数据集上取得有竞争力的结果，强调多尺度时空特征重要性。


<details>
  <summary>Details</summary>
Motivation: 现有动作识别方法忽视动作的多粒度特性，需在时域与空域同时捕捉多尺度线索以提升识别鲁棒性。

Method: Analysis of proposed CAN paper

Result: Introduces CAN with MTCM and GSCM, extracts multi-scale temporal and grouped spatial cues, achieves competitive accuracies on five datasets.

Conclusion: CAN effectively captures multi-scale spatio-temporal cues, improving action recognition and outperforming many baselines; multi-granularity modeling is beneficial.

Abstract: Action recognition is a critical task in video understanding, requiring the comprehensive capture of spatio-temporal cues across various scales. However, existing methods often overlook the multi-granularity nature of actions. To address this limitation, we introduce the Context-Aware Network (CAN). CAN consists of two core modules: the Multi-scale Temporal Cue Module (MTCM) and the Group Spatial Cue Module (GSCM). MTCM effectively extracts temporal cues at multiple scales, capturing both fast-changing motion details and overall action flow. GSCM, on the other hand, extracts spatial cues at different scales by grouping feature maps and applying specialized extraction methods to each group. Experiments conducted on five benchmark datasets (Something-Something V1 and V2, Diving48, Kinetics-400, and UCF101) demonstrate the effectiveness of CAN. Our approach achieves competitive performance, outperforming most mainstream methods, with accuracies of 50.4% on Something-Something V1, 63.9% on Something-Something V2, 88.4% on Diving48, 74.9% on Kinetics-400, and 86.9% on UCF101. These results highlight the importance of capturing multi-scale spatio-temporal cues for robust action recognition.

</details>


### [81] [MaskFocus: Focusing Policy Optimization on Critical Steps for Masked Image Generation](https://arxiv.org/abs/2512.18766)
*Guohui Zhang,Hu Yu,Xiaoxiao Ma,Yaning Pan,Hang Xu,Feng Zhao*

Main category: cs.CV

TL;DR: MaskFocus针对masked生成模型，用中间图像与最终生成图像相似度衡量步骤信息增益，选取关键步骤做策略优化；并引入基于熵的动态路由采样鼓励低熵样本探索，有效提升Text-to-Image任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在后训练语言模型和自回归视觉生成模型上有效，但对masked生成模型困难：策略优化需考虑多步迭代中每步的采样概率，直接使用完整轨迹代价大，随机选步往往效果差。需要一种既高效又能有效优化策略的方法。

Method: 1) 用每步中间图像与最终生成图像的相似度来衡量该步对最终结果的信息增益；2) 基于该信息增益选择对最终结果影响最大的若干“关键步骤”，仅在这些步骤上执行策略优化；3) 设计基于熵的动态路由采样机制，使低熵样本能探索更多有价值的mask策略，从而获得更丰富的训练信号。

Result: 在多个Text-to-Image基准上进行大量实验，结果显示MaskFocus在生成质量或下游指标上显著优于在masked生成模型上直接应用常规RL或随机步优化的基线，验证了关键步骤聚焦与动态路由采样的有效性。

Conclusion: 提出MaskFocus，通过关注生成过程中对最终图像影响最大的关键步骤，降低了蒙版生成模型上强化学习的计算复杂度并提升策略优化效果。

Abstract: Reinforcement learning (RL) has demonstrated significant potential for post-training language models and autoregressive visual generative models, but adapting RL to masked generative models remains challenging. The core factor is that policy optimization requires accounting for the probability likelihood of each step due to its multi-step and iterative refinement process. This reliance on entire sampling trajectories introduces high computational cost, whereas natively optimizing random steps often yields suboptimal results. In this paper, we present MaskFocus, a novel RL framework that achieves effective policy optimization for masked generative models by focusing on critical steps. Specifically, we determine the step-level information gain by measuring the similarity between the intermediate images at each sampling step and the final generated image. Crucially, we leverage this to identify the most critical and valuable steps and execute focused policy optimization on them. Furthermore, we design a dynamic routing sampling mechanism based on entropy to encourage the model to explore more valuable masking strategies for samples with low entropy. Extensive experiments on multiple Text-to-Image benchmarks validate the effectiveness of our method.

</details>


### [82] [In-Context Audio Control of Video Diffusion Transformers](https://arxiv.org/abs/2512.18772)
*Wenze Liu,Weicai Ye,Minghong Cai,Quande Liu,Xintao Wang,Xiangyu Yue*

Main category: cs.CV

TL;DR: This paper adds audio conditioning to full-attention video diffusion transformers, compares attention injection methods, and proposes Masked 3D Attention to enforce temporal alignment, yielding improved lip sync and video quality.


<details>
  <summary>Details</summary>
Motivation: Investigate integration of audio signals into unified transformer-based video diffusion models for speech-driven video generation, addressing underexplored time-synchronous modalities.

Method: Introduce ICAC framework; explore three audio-conditioning mechanisms: cross-attention, 2D self-attention, unified 3D self-attention; propose Masked 3D Attention to enforce temporal alignment and stabilize training.

Result: Found 3D attention best for spatio-temporal audio-visual correlations but unstable; Masked 3D Attention stabilized training and achieved superior lip synchronization and video quality when conditioned on audio stream and reference images.

Conclusion: Masked 3D Attention enables effective and stable audio conditioning in full-attention video diffusion transformers, improving speech-driven video generation quality and synchronization.

Abstract: Recent advancements in video generation have seen a shift towards unified, transformer-based foundation models that can handle multiple conditional inputs in-context. However, these models have primarily focused on modalities like text, images, and depth maps, while strictly time-synchronous signals like audio have been underexplored. This paper introduces In-Context Audio Control of video diffusion transformers (ICAC), a framework that investigates the integration of audio signals for speech-driven video generation within a unified full-attention architecture, akin to FullDiT. We systematically explore three distinct mechanisms for injecting audio conditions: standard cross-attention, 2D self-attention, and unified 3D self-attention. Our findings reveal that while 3D attention offers the highest potential for capturing spatio-temporal audio-visual correlations, it presents significant training challenges. To overcome this, we propose a Masked 3D Attention mechanism that constrains the attention pattern to enforce temporal alignment, enabling stable training and superior performance. Our experiments demonstrate that this approach achieves strong lip synchronization and video quality, conditioned on an audio stream and reference images.

</details>


### [83] [Eff-GRot: Efficient and Generalizable Rotation Estimation with Transformers](https://arxiv.org/abs/2512.18784)
*Fanis Mathioulakis,Gorjan Radevski,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: Eff-GRot：用 Transformer 在潜在空间比较查询与多参考图像的旋转表征，单次前向传播直接估计旋转，通用、高效、端到端。


<details>
  <summary>Details</summary>
Motivation: 现有旋转估计方法通常需要类别/物体特定训练或多步优化，导致推理耗时或泛化差。作者希望设计一种无需物体/类别特定训练、在延迟敏感场景中高效且能推广的旋转估计方法。

Method: 提出基于 Transformer 的比较模块，在潜在空间联合处理来自多个参考图像（带已知朝向）的旋转感知表征与查询图像表征，直接回归目标旋转。该框架简洁、可扩展、端到端训练，兼顾准确性与推理效率。

Result: 实验表明 Eff-GRot 在保持较低延迟的同时提供有竞争力的旋转估计精度，证明其在延迟敏感应用中的潜力。

Conclusion: Eff-GRot 提出了一种基于 Transformer 的高效通用旋转估计方法，通过在潜在空间中对比查询图像与多参考图像的旋转感知表征，实现单次前向传播直接预测目标旋转，无需类别特定训练。该方法在准确性与计算效率之间取得良好平衡，端到端可扩展，适用于低延迟场景。

Abstract: We introduce Eff-GRot, an approach for efficient and generalizable rotation estimation from RGB images. Given a query image and a set of reference images with known orientations, our method directly predicts the object's rotation in a single forward pass, without requiring object- or category-specific training. At the core of our framework is a transformer that performs a comparison in the latent space, jointly processing rotation-aware representations from multiple references alongside a query. This design enables a favorable balance between accuracy and computational efficiency while remaining simple, scalable, and fully end-to-end. Experimental results show that Eff-GRot offers a promising direction toward more efficient rotation estimation, particularly in latency-sensitive applications.

</details>


### [84] [Tempo as the Stable Cue: Hierarchical Mixture of Tempo and Beat Experts for Music to 3D Dance Generation](https://arxiv.org/abs/2512.18804)
*Guangtao Lyu,Chenghao Xu,Qi Liu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出TempoMoE，通过层次化节拍感知Mixture-of-Experts模块增强扩散模型，使音乐到3D舞蹈生成在节奏对齐与舞蹈质量上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖音乐风格标签，但这些标签往往嘈杂、粗糙或不可用，不能充分描述真实音乐多样性，导致节奏错位或风格漂移。作者发现节拍（tempo）更稳定且跨数据集一致，因而用节拍作为主要条件。

Method: 提出TempoMoE：将动作专家按节拍范围分组（60-200 BPM），在多尺度上设置节拍专家以捕捉短时与长时节奏特征；设计层次化节奏自适应路由（Hierarchical Rhythm-Adaptive Routing），根据音乐特征动态选择并融合专家，无需手工风格标签。将TempoMoE嵌入扩散模型以增强节奏感知与舞蹈生成。

Result: 大量实验表明TempoMoE在舞蹈质量和节奏对齐上优于现有方法，达成SOTA效果。

Conclusion: 基于稳定的节拍信息组织专家并动态路由，可以在无风格标签情况下有效提升音乐驱动舞蹈生成的节奏一致性与质量。

Abstract: Music to 3D dance generation aims to synthesize realistic and rhythmically synchronized human dance from music. While existing methods often rely on additional genre labels to further improve dance generation, such labels are typically noisy, coarse, unavailable, or insufficient to capture the diversity of real-world music, which can result in rhythm misalignment or stylistic drift. In contrast, we observe that tempo, a core property reflecting musical rhythm and pace, remains relatively consistent across datasets and genres, typically ranging from 60 to 200 BPM. Based on this finding, we propose TempoMoE, a hierarchical tempo-aware Mixture-of-Experts module that enhances the diffusion model and its rhythm perception. TempoMoE organizes motion experts into tempo-structured groups for different tempo ranges, with multi-scale beat experts capturing fine- and long-range rhythmic dynamics. A Hierarchical Rhythm-Adaptive Routing dynamically selects and fuses experts from music features, enabling flexible, rhythm-aligned generation without manual genre labels. Extensive experiments demonstrate that TempoMoE achieves state-of-the-art results in dance quality and rhythm alignment.

</details>


### [85] [FedVideoMAE: Efficient Privacy-Preserving Federated Video Moderation](https://arxiv.org/abs/2512.18809)
*Ziyuan Tao,Chuanzhi Xu,Sandaru Jayawardana,Wei Bao,Kanchana Thilakarathna,Teng Joon Lim*

Main category: cs.CV

TL;DR: On-device federated VideoMAE + LoRA + DP gives 77.25% acc no-DP and ~65% with strong DP, 28x less communication.


<details>
  <summary>Details</summary>
Motivation: Avoid sending raw short videos to cloud; reduce bandwidth, latency, privacy risks while enabling model updates on-device.

Method: Use self-supervised VideoMAE backbone; fine-tune via LoRA (5.5M params), apply DP-SGD and secure aggregation, evaluate on RWF-2000 with 40 clients.

Result: This paper presents an on-device federated learning framework for video violence detection combining VideoMAE, LoRA, and differential privacy.

Conclusion: The framework attains competitive accuracy with large communication savings; privacy reduces accuracy but remains usable.

Abstract: The rapid growth of short-form video platforms increases the need for privacy-preserving moderation, as cloud-based pipelines expose raw videos to privacy risks, high bandwidth costs, and inference latency. To address these challenges, we propose an on-device federated learning framework for video violence detection that integrates self-supervised VideoMAE representations, LoRA-based parameter-efficient adaptation, and defense-in-depth privacy protection. Our approach reduces the trainable parameter count to 5.5M (~3.5% of a 156M backbone) and incorporates DP-SGD with configurable privacy budgets and secure aggregation. Experiments on RWF-2000 with 40 clients achieve 77.25% accuracy without privacy protection and 65-66% under strong differential privacy, while reducing communication cost by $28.3\times$ compared to full-model federated learning. The code is available at: {https://github.com/zyt-599/FedVideoMAE}

</details>


### [86] [Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction](https://arxiv.org/abs/2512.18813)
*Guangtao Lyu,Xinyi Cheng,Chenghao Xu,Qi Liu,Muli Yang,Fen Fang,Huilin Chen,Jiexi Yan,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 本文通过分析大视觉语言模型（LVLM）内部的视觉感知与生成过程，发现感知遵循GATE（三阶段：Global-Approach-Tighten-Explore）模式，生成遵循SAD（从次dominant累积到dominant）模式。基于此提出VDC（验证性主导纠正）策略，检测并替换缺乏支持的假象token，显著减少幻觉。


<details>
  <summary>Details</summary>
Motivation: LVLM虽能力强，但输出幻觉问题严重。作者希望从模型内部演化路径入手，系统性揭示视觉感知与生成阶段性行为，从而找到可行的纠错策略以降低幻觉。

Method: 对多款LVLM在不同层级的视觉注意力及token生成过程进行层析分析，归纳出GATE感知过程与SAD生成模式。基于两者，提出VDC策略：实时检测不被视觉注意力或FFN支持的subdominant token，并用通过验证（如更强注意力或置信度）的主导token替换之，以保证输出一致性。

Result: 在多个模型和基准数据集上进行广泛实验，结果显示VDC能显著减少幻觉发生率，提高答案可靠性和可解释性。具体数值未在摘要中给出，但描述为“实质性缓解”。

Conclusion: 通过揭示LVLM感知与生成的内部演化规律并据此设计VDC纠错机制，能有效抑制幻觉，提升输出可信度，为进一步研究视觉-语言模型的可解释性与可靠性提供了新方向。

Abstract: Large Vision-Language Models (LVLMs) have shown remarkable capabilities, yet hallucinations remain a persistent challenge. This work presents a systematic analysis of the internal evolution of visual perception and token generation in LVLMs, revealing two key patterns. First, perception follows a three-stage GATE process: early layers perform a Global scan, intermediate layers Approach and Tighten on core content, and later layers Explore supplementary regions. Second, generation exhibits an SAD (Subdominant Accumulation to Dominant) pattern, where hallucinated tokens arise from the repeated accumulation of subdominant tokens lacking support from attention (visual perception) or feed-forward network (internal knowledge). Guided by these findings, we devise the VDC (Validated Dominance Correction) strategy, which detects unsupported tokens and replaces them with validated dominant ones to improve output reliability. Extensive experiments across multiple models and benchmarks confirm that VDC substantially mitigates hallucinations.

</details>


### [87] [EchoMotion: Unified Human Video and Motion Generation via Dual-Modality Diffusion Transformer](https://arxiv.org/abs/2512.18814)
*Yuxiao Yang,Hualian Sheng,Sijia Cai,Jing Lin,Jiahao Wang,Bing Deng,Junzhe Lu,Haoqian Wang,Jieping Ye*

Main category: cs.CV

TL;DR: 提出EchoMotion，通过双分支DiT扩展与MVS-RoPE统一3D位置编码、两阶段训练和大规模HuMoVe数据集，将人体运动显式建模与外观联合，提高复杂人体动作视频生成的连贯性与合理性。


<details>
  <summary>Details</summary>
Motivation: 像素级训练目标偏重外观、难以学习运动学约束，导致模型在复杂人体动作合成上失败。需要将人体运动显式表示并与外观联合建模以提供运动归纳偏置。

Method: EchoMotion在DiT基础上采用双分支架构，处理来自视频与运动模态拼接的tokens；提出MVS-RoPE为视频与运动token提供统一的3D位置编码以实现时序对齐；采用Motion-Video两阶段训练策略以支持联合生成与跨模态条件生成。为训练构建了约8万条视频-运动配对数据集HuMoVe。

Result: 在人中心视频生成任务中，引入显式运动表示后，模型在动作连贯性、运动合理性和跨模态生成（如根据动作生成视频/反之）方面有显著提升（摘要声称但未给具体指标）。

Conclusion: 显式建模人体运动与外观联合训练可作为有效的归纳偏置，显著改善复杂人体动作视频生成，且MVS-RoPE与两阶段训练策略是实现该目标的关键组成部分。

Abstract: Video generation models have advanced significantly, yet they still struggle to synthesize complex human movements due to the high degrees of freedom in human articulation. This limitation stems from the intrinsic constraints of pixel-only training objectives, which inherently bias models toward appearance fidelity at the expense of learning underlying kinematic principles. To address this, we introduce EchoMotion, a framework designed to model the joint distribution of appearance and human motion, thereby improving the quality of complex human action video generation. EchoMotion extends the DiT (Diffusion Transformer) framework with a dual-branch architecture that jointly processes tokens concatenated from different modalities. Furthermore, we propose MVS-RoPE (Motion-Video Syncronized RoPE), which offers unified 3D positional encoding for both video and motion tokens. By providing a synchronized coordinate system for the dual-modal latent sequence, MVS-RoPE establishes an inductive bias that fosters temporal alignment between the two modalities. We also propose a Motion-Video Two-Stage Training Strategy. This strategy enables the model to perform both the joint generation of complex human action videos and their corresponding motion sequences, as well as versatile cross-modal conditional generation tasks. To facilitate the training of a model with these capabilities, we construct HuMoVe, a large-scale dataset of approximately 80,000 high-quality, human-centric video-motion pairs. Our findings reveal that explicitly representing human motion is complementary to appearance, significantly boosting the coherence and plausibility of human-centric video generation.

</details>


### [88] [Brain-Gen: Towards Interpreting Neural Signals for Stimulus Reconstruction Using Transformers and Latent Diffusion Models](https://arxiv.org/abs/2512.18843)
*Hasib Aslam,Muhammad Talal Faiz,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 提出基于transformers提取EEG时空表征并融合进LDM注意力以重建视觉刺激，提升语义聚类与零样本泛化


<details>
  <summary>Details</summary>
Motivation: 现有EEG解码受噪声、空间扩散与时间可变性限制，需更可解释和泛化的表征方法

Method: transformers-based framework + LDM attention

Result: improved latent clustering accuracy and zero-shot generalization; comparable IS and FID

Conclusion: method advances semantic interpretation of EEG, offering better generalizability

Abstract: Advances in neuroscience and artificial intelligence have enabled preliminary decoding of brain activity. However, despite the progress, the interpretability of neural representations remains limited. A significant challenge arises from the intrinsic properties of electroencephalography (EEG) signals, including high noise levels, spatial diffusion, and pronounced temporal variability. To interpret the neural mechanism underlying thoughts, we propose a transformers-based framework to extract spatial-temporal representations associated with observed visual stimuli from EEG recordings. These features are subsequently incorporated into the attention mechanisms of Latent Diffusion Models (LDMs) to facilitate the reconstruction of visual stimuli from brain activity. The quantitative evaluations on publicly available benchmark datasets demonstrate that the proposed method excels at modeling the semantic structures from EEG signals; achieving up to 6.5% increase in latent space clustering accuracy and 11.8% increase in zero shot generalization across unseen classes while having comparable Inception Score and Fréchet Inception Distance with existing baselines. Our work marks a significant step towards generalizable semantic interpretation of the EEG signals.

</details>


### [89] [VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference](https://arxiv.org/abs/2512.18853)
*Sicheng Song,Yanjie Zhang,Zixin Chen,Huamin Qu,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出VizDefender：半脆弱水印+多模态大模型，实现可视化篡改的定位检测与意图分析，评估与用户研究表明有效且不显著损害图像质量。


<details>
  <summary>Details</summary>
Motivation: 数据可视化易被图像编辑技术篡改，导致误导性信息传播，需要检测和分析篡改以维护可视化完整性。

Method: 提出VizDefender框架，包括半脆弱水印模块（嵌入位置映射用于精确定位篡改区域且保持视觉质量）和意图分析模块（利用多模态大模型解读篡改，推断攻击者意图及误导效果）。

Result: 通过广泛评估和用户研究，方法在检测篡改、定位被篡改区域以及推断攻击意图方面表现有效，且水印对视觉质量影响小。

Conclusion: VizDefender能有效检测和分析可视化图像的篡改，结合半脆弱水印与MLLM实现精确定位与语义层面的意图推断，为维护数据可视化诚信提供实用工具。

Abstract: The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.

</details>


### [90] [Cross-modal Counterfactual Explanations: Uncovering Decision Factors and Dataset Biases in Subjective Classification](https://arxiv.org/abs/2512.18864)
*Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: DeX利用图像特定概念和跨模态分解生成自然语言反事实，提供稀疏、可量化的解释，无需训练，对图像隐私决策的解释与偏差发现表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有反事实与概念解释方法在保持语义可解释性、图像特异性与最小扰动之间存在权衡；图像隐私决策具有高度上下文性与主观性，亟需能量化各场景元素差异贡献且不依赖额外训练的可解释方法。

Method: DeX首先通过跨模态分解将图像与语义概念对齐，利用图像特定概念库并结合多准则选择机制（兼顾图像相似性以保证最小扰动与决策置信度以优先重要变化）来生成语义反事实；然后以自然语言表述并评估各概念的贡献与相互影响，提供稀疏、图像定制的解释。

Result: 在图像隐私决策任务上，DeX在解释稀疏性、定量贡献识别与发现数据偏差方面显著优于现有方法，并能在不训练新模型的情况下辅助制定偏差缓解策略以提升公平性。

Conclusion: DeX提出了一种基于跨模态分解与图像特定概念的训练免费可解释框架，通过生成以自然语言表达的概念驱动反事实来解释分类器决策，尤其适用于上下文主观的图像隐私决策任务，能够量化场景关键元素的差异性贡献、发现数据偏差并提升解释稀疏性与效果。

Abstract: Concept-driven counterfactuals explain decisions of classifiers by altering the model predictions through semantic changes. In this paper, we present a novel approach that leverages cross-modal decompositionality and image-specific concepts to create counterfactual scenarios expressed in natural language. We apply the proposed interpretability framework, termed Decompose and Explain (DeX), to the challenging domain of image privacy decisions, which are contextual and subjective. This application enables the quantification of the differential contributions of key scene elements to the model prediction. We identify relevant decision factors via a multi-criterion selection mechanism that considers both image similarity for minimal perturbations and decision confidence to prioritize impactful changes. This approach evaluates and compares diverse explanations, and assesses the interdependency and mutual influence among explanatory properties. By leveraging image-specific concepts, DeX generates image-grounded, sparse explanations, yielding significant improvements over the state of the art. Importantly, DeX operates as a training-free framework, offering high flexibility. Results show that DeX not only uncovers the principal contributing factors influencing subjective decisions, but also identifies underlying dataset biases allowing for targeted mitigation strategies to improve fairness.

</details>


### [91] [Application of deep learning approaches for medieval historical documents transcription](https://arxiv.org/abs/2512.18865)
*Maksym Voloshchuk,Bohdana Zarembovska,Mykola Kozlenko*

Main category: cs.CV

TL;DR: 作者为中世纪拉丁手稿提出了一套从数据准备到检测与识别的深度学习流水线，并提供了详尽的评估指标与实现代码。


<details>
  <summary>Details</summary>
Motivation: 现有OCR/HTR对现代文本效果好，但对中世纪拉丁文风格、字体变体和纸张退化等特性适应差，亟需针对该类型文献的专门方法和数据处理流程。

Method: 构建并标注专用数据集，进行解释性数据分析；采用物体检测模型定位文字（行/词/字符），再用分类模型及词图像嵌入方法进行词识别；通过混合损失与后处理（如编辑距离校正）提升识别精度；最后以多种指标和可视化曲线评估性能。

Result: 提取并识别中世纪拉丁手写文本的深度学习流水线实现与评估

Conclusion: 本文构建了针对9至11世纪拉丁手写文献的端到端深度学习处理流程，包括数据集制作、目标检测、词级识别与词图像嵌入，并通过多项指标（召回、精确率、F1、IoU、混淆矩阵、平均字符串距离）对模型性能进行评估，代码已在GitHub开源。

Abstract: Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.

</details>


### [92] [CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://arxiv.org/abs/2512.18878)
*Kaidi Liang,Ke Li,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: CrashChat: an MLLM for multitask crash video analysis using instruction fine-tuning and task decoupling/grouping, achieving SOTA results and practical readiness.


<details>
  <summary>Details</summary>
Motivation: Automating crash video analysis is essential to leverage growing driving video data for traffic safety research and accountability for autonomous driving; existing models cannot perform all required tasks unifiedly and training strategies are underexplored.

Method: Propose CrashChat, an MLLM built on VideoLLaMA3, using instruction fine-tuning for domain knowledge and a novel multitask learning strategy of task decoupling and grouping to maximize joint learning and mitigate negative transfer.

Result: CrashChat outperforms existing MLLMs and vision-based methods, achieves near-perfect crash recognition, 176% improvement in crash localization, 40% improvement in pre-crash localization, and boosts BLEU and ROUGE scores by 0.18-0.41 and 0.18-0.42 respectively for descriptions and reasoning.

Conclusion: CrashChat is a strong end-to-end analytical tool for crash video analysis with available dataset and code.

Abstract: Automating crash video analysis is essential to leverage the growing availability of driving video data for traffic safety research and accountability attribution in autonomous driving. Crash video analysis is a challenging multitask problem due to the complex spatiotemporal dynamics of crash events in video data and the diverse analytical requirements involved. It requires capabilities spanning crash recognition, temporal grounding, and high-level video understanding. Existing models, however, cannot perform all these tasks within a unified framework, and effective training strategies for such models remain underexplored. To fill these gaps, this paper proposes CrashChat, a multimodal large language model (MLLM) for multitask traffic crash analysis, built upon VideoLLaMA3. CrashChat acquires domain-specific knowledge through instruction fine-tuning and employs a novel multitask learning strategy based on task decoupling and grouping, which maximizes the benefit of joint learning within and across task groups while mitigating negative transfer. Numerical experiments on consolidated public datasets demonstrate that CrashChat consistently outperforms existing MLLMs across model scales and traditional vision-based methods, achieving state-of-the-art performance. It reaches near-perfect accuracy in crash recognition, a 176\% improvement in crash localization, and a 40\% improvement in the more challenging pre-crash localization. Compared to general MLLMs, it substantially enhances textual accuracy and content coverage in crash description and reasoning tasks, with 0.18-0.41 increases in BLEU scores and 0.18-0.42 increases in ROUGE scores. Beyond its strong performance, CrashChat is a convenient, end-to-end analytical tool ready for practical implementation. The dataset and implementation code for CrashChat are available at https://github.com/Liangkd/CrashChat.

</details>


### [93] [Localising Shortcut Learning in Pixel Space via Ordinal Scoring Correlations for Attribution Representations (OSCAR)](https://arxiv.org/abs/2512.18888)
*Akshit Achara,Peter Triantafillou,Esther Puyol-Antón,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: OSCAR analyses attribution maps across baseline, test, and sensitive-attribute models to quantify and localise shortcuts using rank-profile correlations; validated on CelebA, CheXpert, ADNI; stable and useful for mitigation


<details>
  <summary>Details</summary>
Motivation: Need for quantitative, dataset-level, model-agnostic methods to detect and localise shortcut learning, especially when cues are not human-visible (e.g., medical imaging)

Method: OSCAR: convert attribution maps to rank profiles; compute pairwise/partial/deviation correlations across BA, TS, SA

Result: Quantitative metrics showing degree of shortcut reliance; ranking of image regions contributing most

Conclusion: OSCAR provides stable, sensitive, and localising metrics for shortcut detection and enables simple mitigation via test-time attenuation

Abstract: Deep neural networks often exploit shortcuts. These are spurious cues which are associated with output labels in the training data but are unrelated to task semantics. When the shortcut features are associated with sensitive attributes, shortcut learning can lead to biased model performance. Existing methods for localising and understanding shortcut learning are mostly based upon qualitative, image-level inspection and assume cues are human-visible, limiting their use in domains such as medical imaging. We introduce OSCAR (Ordinal Scoring Correlations for Attribution Representations), a model-agnostic framework for quantifying shortcut learning and localising shortcut features. OSCAR converts image-level task attribution maps into dataset-level rank profiles of image regions and compares them across three models: a balanced baseline model (BA), a test model (TS), and a sensitive attribute predictor (SA). By computing pairwise, partial, and deviation-based correlations on these rank profiles, we produce a set of quantitative metrics that characterise the degree of shortcut reliance for TS, together with a ranking of image-level regions that contribute most to it. Experiments on CelebA, CheXpert, and ADNI show that our correlations are (i) stable across seeds and partitions, (ii) sensitive to the level of association between shortcut features and output labels in the training data, and (iii) able to distinguish localised from diffuse shortcut features. As an illustration of the utility of our method, we show how worst-group performance disparities can be reduced using a simple test-time attenuation approach based on the identified shortcut regions. OSCAR provides a lightweight, pixel-space audit that yields statistical decision rules and spatial maps, enabling users to test, localise, and mitigate shortcut reliance. The code is available at https://github.com/acharaakshit/oscar

</details>


### [94] [Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs](https://arxiv.org/abs/2512.18897)
*Dmitry Demidov,Zaigham Zaheer,Zongyan Han,Omkar Thawakar,Rao Anwer*

Main category: cs.CV

TL;DR: FiNDR利用推理增强的LMM自动为图像生成候选名称、通过视-语模型过滤并训练轻量分类器，实现无词表细粒度识别，取得显著领先且可由开源模型复制。


<details>
  <summary>Details</summary>
Motivation: 提出一种无需预定义词表的细粒度图像识别方法，解决现有方法依赖大型固定词表或复杂管线带来的局限，利用大型多模态模型（LMMs）的推理、分解问题、检索知识和自我修正能力实现自动化命名与分类。

Method: 提出FiNDR框架，三步自动化流程：(i) 启用推理的LMM为每张图像生成描述性候选标签；(ii) 视-语模型过滤并排序这些候选项，形成一致的类别集合；(iii) 用验证过的名称构建轻量级多模态分类器用于推理。

Result: 在多个细粒度分类基准上，FiNDR在无词表设定下取得最先进性能，相较先前方法最高提升18.8%相对幅度。方法甚至超过使用预定义真实名称的零样本基线，并证明通过精心设计的提示，开源LMM可匹敌专有模型。

Conclusion: 推理增强的大型多模态模型为可扩展、全自动、开放世界的细粒度视觉识别提供了有效基础，挑战了人类策划词表作为上限的假设，代码已开源。

Abstract: Vocabulary-free fine-grained image recognition aims to distinguish visually similar categories within a meta-class without a fixed, human-defined label set. Existing solutions for this problem are limited by either the usage of a large and rigid list of vocabularies or by the dependency on complex pipelines with fragile heuristics where errors propagate across stages. Meanwhile, the ability of recent large multi-modal models (LMMs) equipped with explicit or implicit reasoning to comprehend visual-language data, decompose problems, retrieve latent knowledge, and self-correct suggests a more principled and effective alternative. Building on these capabilities, we propose FiNDR (Fine-grained Name Discovery via Reasoning), the first reasoning-augmented LMM-based framework for vocabulary-free fine-grained recognition. The system operates in three automated steps: (i) a reasoning-enabled LMM generates descriptive candidate labels for each image; (ii) a vision-language model filters and ranks these candidates to form a coherent class set; and (iii) the verified names instantiate a lightweight multi-modal classifier used at inference time. Extensive experiments on popular fine-grained classification benchmarks demonstrate state-of-the-art performance under the vocabulary-free setting, with a significant relative margin of up to 18.8% over previous approaches. Remarkably, the proposed method surpasses zero-shot baselines that exploit pre-defined ground-truth names, challenging the assumption that human-curated vocabularies define an upper bound. Additionally, we show that carefully curated prompts enable open-source LMMs to match proprietary counterparts. These findings establish reasoning-augmented LMMs as an effective foundation for scalable, fully automated, open-world fine-grained visual recognition. The source code is available on github.com/demidovd98/FiNDR.

</details>


### [95] [Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models](https://arxiv.org/abs/2512.18910)
*Mohamad Zamini,Diksha Shukla*

Main category: cs.CV

TL;DR: Delta-LLaVA: low-rank projection + lightweight Transformers compress visual tokens to 144, boosting inference and training efficiency while improving performance.


<details>
  <summary>Details</summary>
Motivation: Reduce computational redundancy of dense visual tokens bridging vision encoder and LLM, improving efficiency and scalability.

Method: From abstract: propose Delta-LLaVA, a token-efficient visual projector using low-rank DeltaProjection then lightweight Transformer specialization layers; aligns multi-level vision features into compact subspace; 144 tokens.

Result: Improved throughput up to 55% inference, training 4-5x faster in pretraining, 1.5x in finetuning; consistent gains across benchmarks.

Conclusion: Base-then-specialize token formation is effective; reduces redundancy and scales better than MLP projectors.

Abstract: Multimodal Large Language Models (MLLMs) combine visual and textual representations to enable rich reasoning capabilities. However, the high computational cost of processing dense visual tokens remains a major bottleneck. A critical component in this pipeline is the visual projector, which bridges the vision encoder and the language model. Standard designs often employ a simple multi-layer perceptron for direct token mapping, but this approach scales poorly with high-resolution inputs, introducing significant redundancy. We present Delta-LLaVA, a token-efficient projector that employs a low-rank DeltaProjection to align multi-level vision features into a compact subspace before further interaction. On top of this base alignment, lightweight Transformer blocks act as specialization layers, capturing both global and local structure under constrained token budgets. Extensive experiments and ablations demonstrate that this base-then-specialize design yields consistent gains across multiple benchmarks with only 144 tokens, highlighting the importance of token formation prior to scaling interaction capacity. With Delta-LLaVA, inference throughput improves by up to 55%, while end-to-end training accelerates by nearly 4-5x in pretraining and over 1.5x in finetuning, highlighting the dual benefits of our design in both efficiency and scalability.

</details>


### [96] [LouvreSAE: Sparse Autoencoders for Interpretable and Controllable Style Transfer](https://arxiv.org/abs/2512.18930)
*Raina Panda,Daniel Fein,Arpita Singhal,Mark Fiore,Maneesh Agrawala,Matyas Bohacek*

Main category: cs.CV

TL;DR: 提出LouvreSAE：在生成模型潜在空间上训练稀疏自编码器（SAE），学得可解释、部分解耦的风格与构图概念，构建可分解的风格剖面向量，实现无微调、无额外推理步骤的风格迁移。比现有方法速度快1.7–20×，在ArtBench10上风格指标不输且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有艺术风格迁移多依赖模型微调、适配器或提示工程，计算昂贵且风格与主题常被纠缠。需一种训练与推理开销小、可解释且能从少量参考图像直接迁移风格的方法。

Method: 在生成模型的潜在嵌入之上训练一个面向艺术的稀疏自编码器（SAE）。SAE在艺术数据上训练后会自发学习到大量部分解耦的概念，涵盖笔触、纹理、调色板等风格要素及语义结构要素。基于这些概念构建“风格剖面”（可分解的引导向量），并在生成时直接用该向量进行风格引导，无需微调、LoRA或额外推理优化。

Result: 在ArtBench10基准上，使用VGG Style Loss和CLIP Score Style等指标评估，LouvreSAE在风格保真度或相似性上达到或超过现有方法，同时速度提升1.7–20倍。方法还能以少量参考图像实现风格迁移并提供可解释性。

Conclusion: 提出一种轻量、可解释且无需模型改动的艺术风格表示与迁移方法，兼顾效果与效率，为可控生成艺术风格提供实用路径。

Abstract: Artistic style transfer in generative models remains a significant challenge, as existing methods often introduce style only via model fine-tuning, additional adapters, or prompt engineering, all of which can be computationally expensive and may still entangle style with subject matter. In this paper, we introduce a training- and inference-light, interpretable method for representing and transferring artistic style. Our approach leverages an art-specific Sparse Autoencoder (SAE) on top of latent embeddings of generative image models. Trained on artistic data, our SAE learns an emergent, largely disentangled set of stylistic and compositional concepts, corresponding to style-related elements pertaining brushwork, texture, and color palette, as well as semantic and structural concepts. We call it LouvreSAE and use it to construct style profiles: compact, decomposable steering vectors that enable style transfer without any model updates or optimization. Unlike prior concept-based style transfer methods, our method requires no fine-tuning, no LoRA training, and no additional inference passes, enabling direct steering of artistic styles from only a few reference images. We validate our method on ArtBench10, achieving or surpassing existing methods on style evaluations (VGG Style Loss and CLIP Score Style) while being 1.7-20x faster and, critically, interpretable.

</details>


### [97] [Point What You Mean: Visually Grounded Instruction Policy](https://arxiv.org/abs/2512.18933)
*Hang Yu,Juntu Zhao,Yufeng Liu,Kaiyu Li,Cheng Ma,Di Zhang,Yingdong Hu,Guang Chen,Junyuan Xie,Junliang Guo,Junqiao Zhao,Yang Gao*

Main category: cs.CV

TL;DR: Introduce Point-VLA: visually grounded cues + auto annotation improves VLA object referring, robust in cluttered and OOD scenarios


<details>
  <summary>Details</summary>
Motivation: Resolve referential ambiguity in VLA models by adding explicit pixel-level visual grounding and scalable annotations

Method: Summarize methods, dataset, evaluation

Result: Point-VLA policy augmenting language with bounding boxes; automatic annotation pipeline for scaling; evaluation on real-world referring tasks comparing vs text-only VLAs

Conclusion: Point-VLA improves object referring and grounding in cluttered/OOD scenes, yielding better embodied control and generalization

Abstract: Vision-Language-Action (VLA) models align vision and language with embodied control, but their object referring ability remains limited when relying solely on text prompt, especially in cluttered or out-of-distribution (OOD) scenes. In this study, we introduce the Point-VLA, a plug-and-play policy that augments language instructions with explicit visual cues (e.g., bounding boxes) to resolve referential ambiguity and enable precise object-level grounding. To efficiently scale visually grounded datasets, we further develop an automatic data annotation pipeline requiring minimal human effort. We evaluate Point-VLA on diverse real-world referring tasks and observe consistently stronger performance than text-only instruction VLAs, particularly in cluttered or unseen-object scenarios, with robust generalization. These results demonstrate that Point-VLA effectively resolves object referring ambiguity through pixel-level visual grounding, achieving more generalizable embodied control.

</details>


### [98] [Symmetrization of 3D Generative Models](https://arxiv.org/abs/2512.18953)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: Train on half-objects (mirror one side along x=0) so generator learns partial geometry; reflect at generation to get symmetric, plausible shapes. Improves symmetry versus original training.


<details>
  <summary>Details</summary>
Motivation: Current 3D generative models often produce asymmetric shapes; changing architecture is costly. A data-centric solution—training on half-objects—may induce symmetry without modifying models.

Method: Create half-object dataset by cutting shapes at x=0 and reflecting one half to create complete half-objects; train existing generative models (two variants) on this dataset; at generation, reflect outputs to produce full symmetric shapes; evaluate symmetry and consistency against models trained on original data.

Result: Generative model trained on half-objects produces symmetric outputs.

Conclusion: Data-centric training on reflected half-objects improves reflectional symmetry in 3D generative outputs across tested ShapeNet classes.

Abstract: We propose a novel data-centric approach to promote symmetry in 3D generative models by modifying the training data rather than the model architecture. Our method begins with an analysis of reflectional symmetry in both real-world 3D shapes and samples generated by state-of-the-art models. We hypothesize that training a generative model exclusively on half-objects, obtained by reflecting one half of the shapes along the x=0 plane, enables the model to learn a rich distribution of partial geometries which, when reflected during generation, yield complete shapes that are both visually plausible and geometrically symmetric. To test this, we construct a new dataset of half-objects from three ShapeNet classes (Airplane, Car, and Chair) and train two generative models. Experiments demonstrate that the generated shapes are symmetrical and consistent, compared with the generated objects from the original model and the original dataset objects.

</details>


### [99] [VOIC: Visible-Occluded Decoupling for Monocular 3D Semantic Scene Completion](https://arxiv.org/abs/2512.18954)
*Zaidao Han,Risa Higashita,Jiang Liu*

Main category: cs.CV

TL;DR: 提出VRLE离线可见区域标签提取策略与双解码器网络VOIC，将SSC分解为可见区域感知与遮挡区域推理，先构建基底体素，再由可见解码器生成高保真先验，遮挡解码器结合跨模态交互完成全局推理。在SemanticKITTI和SSCBench-KITTI360上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有单目到3DSSC方法在可见区域高置信感知与遮挡区域低置信推理间存在干扰，导致特征稀释与错误传播，需要显式分离监督以净化子任务训练空间。

Method: 1) VRLE：从密集3D真值中离线提取可见区域体素标签，分离可见与遮挡监督。2) 构建基于图像特征与深度占据的基底体素表示。3) VOIC双解码器：可见解码器生成几何与语义先验；遮挡解码器利用先验与跨模态交互（如图像-体素交互）进行一致的全局场景补全。

Result: 在SemanticKITTI和SSCBench-KITTI360基准上，VOIC在几何补全与语义分割精度上均优于现有单目SSC方法，达到或接近SOTA。

Conclusion: 通过离线可见区域标签分离与显式解耦的双解码器设计，可缓解单目SSC中可见/遮挡信息干扰，提升补全与语义性能，具有良好泛化性与实用价值。

Abstract: Camera-based 3D Semantic Scene Completion (SSC) is a critical task for autonomous driving and robotic scene understanding. It aims to infer a complete 3D volumetric representation of both semantics and geometry from a single image. Existing methods typically focus on end-to-end 2D-to-3D feature lifting and voxel completion. However, they often overlook the interference between high-confidence visible-region perception and low-confidence occluded-region reasoning caused by single-image input, which can lead to feature dilution and error propagation.
  To address these challenges, we introduce an offline Visible Region Label Extraction (VRLE) strategy that explicitly separates and extracts voxel-level supervision for visible regions from dense 3D ground truth. This strategy purifies the supervisory space for two complementary sub-tasks: visible-region perception and occluded-region reasoning. Building on this idea, we propose the Visible-Occluded Interactive Completion Network (VOIC), a novel dual-decoder framework that explicitly decouples SSC into visible-region semantic perception and occluded-region scene completion. VOIC first constructs a base 3D voxel representation by fusing image features with depth-derived occupancy. The visible decoder focuses on generating high-fidelity geometric and semantic priors, while the occlusion decoder leverages these priors together with cross-modal interaction to perform coherent global scene reasoning.
  Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that VOIC outperforms existing monocular SSC methods in both geometric completion and semantic segmentation accuracy, achieving state-of-the-art performance.

</details>


### [100] [DVI: Disentangling Semantic and Visual Identity for Training-Free Personalized Generation](https://arxiv.org/abs/2512.18964)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: DVI通过语义-视觉正交解耦，利用VAE均值/方差注入视觉氛围并按扩散阶段动态调度，实现零样本高保真身份定制，减少“贴纸感”。


<details>
  <summary>Details</summary>
Motivation: 现有免微调的身份定制方法常忽略输入图像的光照、皮肤质感和环境色调等视觉上下文，导致语义正确但视觉不协调的“贴纸化”效果，亟需结合视觉氛围以提升自然性。

Method: DVI包含：1) 将身份分为细粒度语义流和粗粒度视觉流；2) 利用VAE潜变量的均值和方差作为轻量级视觉描述；3) 参数无关的特征调制机制，用视觉统计量自适应调整语义嵌入；4) 动态时间颗粒度调度器，在扩散去噪早期优先注入视觉氛围，后期精细语义细节。

Result: DVI提出了一种在无需微调的情况下，同时保留语义身份和视觉氛围的定制方法。

Conclusion: 通过在VAE潜变量空间利用均值/方差作为视觉描述并结合参数无关的特征调制与动态时间调度，DVI在不训练参数的前提下显著提升了视觉一致性与大气保真度，同时保持身份保留性能。

Abstract: Recent tuning-free identity customization methods achieve high facial fidelity but often overlook visual context, such as lighting, skin texture, and environmental tone. This limitation leads to ``Semantic-Visual Dissonance,'' where accurate facial geometry clashes with the input's unique atmosphere, causing an unnatural ``sticker-like'' effect. We propose **DVI (Disentangled Visual-Identity)**, a zero-shot framework that orthogonally disentangles identity into fine-grained semantic and coarse-grained visual streams. Unlike methods relying solely on semantic vectors, DVI exploits the inherent statistical properties of the VAE latent space, utilizing mean and variance as lightweight descriptors for global visual atmosphere. We introduce a **Parameter-Free Feature Modulation** mechanism that adaptively modulates semantic embeddings with these visual statistics, effectively injecting the reference's ``visual soul'' without training. Furthermore, a **Dynamic Temporal Granularity Scheduler** aligns with the diffusion process, prioritizing visual atmosphere in early denoising stages while refining semantic details later. Extensive experiments demonstrate that DVI significantly enhances visual consistency and atmospheric fidelity without parameter fine-tuning, maintaining robust identity preservation and outperforming state-of-the-art methods in IBench evaluations.

</details>


### [101] [Total Curvature Regularization and its_Minimization for Surface and Image Smoothing](https://arxiv.org/abs/2512.18968)
*Tianle Lu,Ke Chen,Yuping Duan*

Main category: cs.CV

TL;DR: 通过惩罚多方向法向曲率并借助算子分裂求解时间相关PDE，该方法高效稳健地实现了保边缘且各向同性的曲面/图像平滑。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的曲率正则化形式，通过惩罚来自多个方向的法向曲率，以在平滑同时保持锐边并实现各向同性性

Method: 将高阶非线性优化问题重写为求时间相关PDE系统的稳态解，采用算子分裂进行时间离散化，分步子问题具有闭式解或可用高效算法求解

Result: 方法在曲面和平滑图像任务中在保边缘和各向同性方面表现良好，算法对参数不敏感且高效

Conclusion: 该总法向曲率正则化为曲面/图像平滑提供了鲁棒且高效的工具，避免了复杂参数调参，并能得到具有锐边和精确各向同性特性的结果

Abstract: We introduce a novel formulation for curvature regularization by penalizing normal curvatures from multiple directions. This total normal curvature regularization is capable of producing solutions with sharp edges and precise isotropic properties. To tackle the resulting high-order nonlinear optimization problem, we reformulate it as the task of finding the steady-state solution of a time-dependent partial differential equation (PDE) system. Time discretization is achieved through operator splitting, where each subproblem at the fractional steps either has a closed-form solution or can be efficiently solved using advanced algorithms. Our method circumvents the need for complex parameter tuning and demonstrates robustness to parameter choices. The efficiency and effectiveness of our approach have been rigorously validated in the context of surface and image smoothing problems.

</details>


### [102] [Self-Attention with State-Object Weighted Combination for Compositional Zero Shot Learning](https://arxiv.org/abs/2512.18969)
*Cheng-Hong Chang,Pei-Hsuan Tsai*

Main category: cs.CV

TL;DR: Propose SASOW: add self-attention to state/object classifiers and weight them during composition, achieving modest gains over prior methods on benchmarks


<details>
  <summary>Details</summary>
Motivation: CZSL needed to generalize to unseen state-object pairs; KG-SP good but lacks attention and composition weighting, so improvements targeted these aspects

Method: Introduce self-attention in state and object classifiers; weight states and objects during composition; build on KG-SP framework

Result: Improved recognition accuracy for states and objects and more reasonable composed predictions

Conclusion: SASOW outperforms KG-SP and OW-CZSL on unseen composition accuracy by 2.1%, 1.7%, 0.4% on MIT-States, UT Zappos, C-GQA respectively

Abstract: Object recognition has become prevalent across various industries. However, most existing applications are limited to identifying objects alone, without considering their associated states. The ability to recognize both the state and object simultaneously remains less common. One approach to address this is by treating state and object as a single category during training. However, this approach poses challenges in data collection and training since it requires comprehensive data for all possible combinations. Compositional Zero-shot Learning (CZSL) emerges as a viable solution by treating the state and object as distinct categories during training. CZSL facilitates the identification of novel compositions even in the absence of data for every conceivable combination. The current state-of-the-art method, KG-SP, addresses this issue by training distinct classifiers for states and objects, while leveraging a semantic model to evaluate the plausibility of composed compositions. However, KG-SP's accuracy in state and object recognition can be further improved, and it fails to consider the weighting of states and objects during composition. In this study, we propose SASOW, an enhancement of KG-SP that considers the weighting of states and objects while improving composition recognition accuracy. First, we introduce self-attention mechanisms into the classifiers for states and objects, leading to enhanced accuracy in recognizing both. Additionally, we incorporate the weighting of states and objects during composition to generate more reasonable and accurate compositions. Our validation process involves testing SASOW on three established benchmark datasets. Experimental outcomes affirm when compared against OW-CZSL approach, KG-SP, SASOW showcases improvements of 2.1%, 1.7%, and 0.4% in terms of accuracy for unseen compositions across the MIT-States, UT Zappos, and C-GQA datasets, respectively.

</details>


### [103] [ICP-4D: Bridging Iterative Closest Point and LiDAR Panoptic Segmentation](https://arxiv.org/abs/2512.18991)
*Gyeongrok Oh,Youngdong Jang,Jonghyun Choi,Suk-Ju Kang,Guang Lin,Sangpil Kim*

Main category: cs.CV

TL;DR: 提出无需训练的ICP-4D，用ICP对实例点集对齐并结合Sinkhorn软匹配及三类实例策略，实现高效鲁棒的4D LiDAR时序实例关联，在SemanticKITTI和panoptic nuScenes上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有4D LiDAR全景分割方法依赖大规模叠加点云训练或设计实例关联模块，导致重复点处理、计算开销大且忽视原始点云的几何先验。本论文旨在提出一个无需训练、利用几何关系进行实例级时空关联的方法，降低计算复杂度并提高关联鲁棒性。

Method: 提出ICP-4D框架：使用Iterative Closest Point (ICP)在实例级点集之间直接对齐源与目标点集以完成时序实例关联；为应对噪声实例预测，加入基于Sinkhorn的软匹配以利用实例分布获得准确点级对应；设计考虑三类实例（静止、动态、缺失）以实现高效且遮挡感知的匹配流程。该方法无需额外训练或额外点云输入。

Result: 在SemanticKITTI和panoptic nuScenes两个数据集上进行大量实验，结果显示ICP-4D在没有额外训练或输入的情况下，持续超越现有最先进方法，在关联精度和效率上均有提升。

Conclusion: ICP-4D证明通过直接利用点云几何先验和简单的ICP+Sinkhorn联合策略，可以实现训练免费但性能优越的4D LiDAR全景分割时序实例关联，兼顾鲁棒性、遮挡处理与计算效率。

Abstract: Dominant paradigms for 4D LiDAR panoptic segmentation are usually required to train deep neural networks with large superimposed point clouds or design dedicated modules for instance association. However, these approaches perform redundant point processing and consequently become computationally expensive, yet still overlook the rich geometric priors inherently provided by raw point clouds. To this end, we introduce ICP-4D, a simple yet effective training-free framework that unifies spatial and temporal reasoning through geometric relations among instance-level point sets. Specifically, we apply the Iterative Closest Point (ICP) algorithm to directly associate temporally consistent instances by aligning the source and target point sets through the estimated transformation. To stabilize association under noisy instance predictions, we introduce a Sinkhorn-based soft matching. This exploits the underlying instance distribution to obtain accurate point-wise correspondences, resulting in robust geometric alignment. Furthermore, our carefully designed pipeline, which considers three instance types-static, dynamic, and missing-offers computational efficiency and occlusion-aware matching. Our extensive experiments across both SemanticKITTI and panoptic nuScenes demonstrate that our method consistently outperforms state-of-the-art approaches, even without additional training or extra point cloud inputs.

</details>


### [104] [Towards AI-Guided Open-World Ecological Taxonomic Classification](https://arxiv.org/abs/2512.18994)
*Cheng Yaw Low,Heejoon Koo,Jaewoo Park,Kaleb Mesfin Asfaw,Meeyoung Cha*

Main category: cs.CV

TL;DR: 提出TaxoNet，通过嵌入编码器和双边距惩罚损失改善长尾和细粒度植物分类，在多个植物数据集上优于基线，尤其提升稀有类性能


<details>
  <summary>Details</summary>
Motivation: 应对生态分类中长尾分布、细粒度差异、时空域外迁移和开放集识别限制，提高植物类群监测能力与可持续性工作支持

Method: Embedding-based encoder with dual-margin penalization loss; TaxoNet strengthens signals for rare taxa and mitigates dominance of common taxa

Result: TaxoNet outperforms baselines on Google Auto-Arborist, iNat-Plantae, NAFlora-Mini; better on rare taxa; foundation multimodal models limited in plant domain

Conclusion: TaxoNet provides strong foundation for open-world plant taxonomic monitoring and addresses long-tail, fine-grained, domain shift, and open-set challenges

Abstract: AI-guided classification of ecological families, genera, and species underpins global sustainability efforts such as biodiversity monitoring, conservation planning, and policy-making. Progress toward this goal is hindered by long-tailed taxonomic distributions from class imbalance, along with fine-grained taxonomic variations, test-time spatiotemporal domain shifts, and closed-set assumptions that can only recognize previously seen taxa. We introduce the Open-World Ecological Taxonomy Classification, a unified framework that captures the co-occurrence of these challenges in realistic ecological settings. To address them, we propose TaxoNet, an embedding-based encoder with a dual-margin penalization loss that strengthens learning signals from rare underrepresented taxa while mitigating the dominance of overrepresented ones, directly confronting interrelated challenges. We evaluate our method on diverse ecological domains: Google Auto-Arborist (urban trees), iNat-Plantae (Plantae observations from various ecosystems in iNaturalist-2019), and NAFlora-Mini (a curated herbarium collection). Our model consistently outperforms baselines, particularly for rare taxa, establishing a strong foundation for open-world plant taxonomic monitoring. Our findings further show that general-purpose multimodal foundation models remain constrained in plant-domain applications.

</details>


### [105] [CETCAM: Camera-Controllable Video Generation via Consistent and Extensible Tokenization](https://arxiv.org/abs/2512.19020)
*Zelin Zhao,Xinyu Gong,Bangya Liu,Ziyang Song,Jun Zhang,Suhui Wu,Yongxin Chen,Hao Zhang*

Main category: cs.CV

TL;DR: CETCAM 提出一种无需相机注释的可控视频生成框架，通过几何感知令牌（tokens）将深度和相机参数统一编码并注入预训练视频扩散模型，实现高几何一致性和时间稳定性的相机控制，同时兼容额外控制模态。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖难以扩展且与深度估计不一致的相机位姿注释，造成训练与测试不匹配，限制了可控视频生成的可扩展性与几何一致性。

Method: 使用几何基础模型（如VGGT）估计深度与相机参数，将其转为统一的几何感知令牌，并通过轻量的上下文模块注入到预训练视频扩散骨干中。训练分为两阶段：第一阶段在多样化原始视频上学习鲁棒的相机可控性；第二阶段在高质量数据上微调以提升细节与视觉质量。

Result: 在多项基准上，CETCAM 在几何一致性、时间稳定性和视觉真实感方面达到最先进水平，并展示了对补洞（inpainting）与布局控制等额外控制模态的良好适应性。

Conclusion: CETCAM 提供了一种无需相机注释的可扩展、几何一致且灵活的相机可控视频生成方案，兼具高质量与多模态控制扩展能力。

Abstract: Achieving precise camera control in video generation remains challenging, as existing methods often rely on camera pose annotations that are difficult to scale to large and dynamic datasets and are frequently inconsistent with depth estimation, leading to train-test discrepancies. We introduce CETCAM, a camera-controllable video generation framework that eliminates the need for camera annotations through a consistent and extensible tokenization scheme. CETCAM leverages recent advances in geometry foundation models, such as VGGT, to estimate depth and camera parameters and converts them into unified, geometry-aware tokens. These tokens are seamlessly integrated into a pretrained video diffusion backbone via lightweight context blocks. Trained in two progressive stages, CETCAM first learns robust camera controllability from diverse raw video data and then refines fine-grained visual quality using curated high-fidelity datasets. Extensive experiments across multiple benchmarks demonstrate state-of-the-art geometric consistency, temporal stability, and visual realism. Moreover, CETCAM exhibits strong adaptability to additional control modalities, including inpainting and layout control, highlighting its flexibility beyond camera control. The project page is available at https://sjtuytc.github.io/CETCam_project_page.github.io/.

</details>


### [106] [VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation](https://arxiv.org/abs/2512.19021)
*Sihao Lin,Zerui Li,Xunyi Zhao,Gengze Zhou,Liuyi Wang,Rong Wei,Rui Tang,Juncheng Li,Hanqing Wang,Jiangmiao Pang,Anton van den Hengel,Jiajun Liu,Qi Wu*

Main category: cs.CV

TL;DR: 提出VLNVerse：一个大规模、可扩展的VLN基准，结合多任务统一框架、真实物理仿真和完整运动学代理，弥合仿真到现实的差距并支持大规模数据与MLLM预训练。


<details>
  <summary>Details</summary>
Motivation: 现有VLN基准数据集规模小、仿真简单（如瞬移/无运动学）、任务碎片化，且数据规模不足以支持现代大模型预训练，限制了对仿真到现实泛化的研究与统一进展。

Method: 构建VLNVerse：1）统一多任务框架与可扩展工具包，整合先前分散任务；2）引入具完整运动学的实体代理并采用稳健物理引擎实现真实仿真，替代瞬移“幽灵”代理；3）大规模、多样化数据收集与标注，并评估从经典模型到多模态大模型（MLLM）性能；4）提出一个统一的多任务模型来处理所有任务。

Result: 使用VLNVerse对现有方法进行了全面评估，展示在真实仿真与多任务设置下的性能差异，并验证了所提统一多任务模型在基准上能同时处理多任务。大量规模与真实物理仿真显著影响模型泛化与评估结果。

Conclusion: VLNVerse作为一个可扩展、现实感强的VLN基准，促进仿真与现实间的闭合，支持大规模预训练与统一多任务研究，有望推动通用实体移动代理研究的发展。

Abstract: Despite remarkable progress in Vision-Language Navigation (VLN), existing benchmarks remain confined to fixed, small-scale datasets with naive physical simulation. These shortcomings limit the insight that the benchmarks provide into sim-to-real generalization, and create a significant research gap. Furthermore, task fragmentation prevents unified/shared progress in the area, while limited data scales fail to meet the demands of modern LLM-based pretraining. To overcome these limitations, we introduce VLNVerse: a new large-scale, extensible benchmark designed for Versatile, Embodied, Realistic Simulation, and Evaluation. VLNVerse redefines VLN as a scalable, full-stack embodied AI problem. Its Versatile nature unifies previously fragmented tasks into a single framework and provides an extensible toolkit for researchers. Its Embodied design moves beyond intangible and teleporting "ghost" agents that support full-kinematics in a Realistic Simulation powered by a robust physics engine. We leverage the scale and diversity of VLNVerse to conduct a comprehensive Evaluation of existing methods, from classic models to MLLM-based agents. We also propose a novel unified multi-task model capable of addressing all tasks within the benchmark. VLNVerse aims to narrow the gap between simulated navigation and real-world generalization, providing the community with a vital tool to boost research towards scalable, general-purpose embodied locomotion agents.

</details>


### [107] [Steering Vision-Language Pre-trained Models for Incremental Face Presentation Attack Detection](https://arxiv.org/abs/2512.19022)
*Haoze Li,Jie Zhang,Guoying Zhao,Stephen Lin,Shiguang Shan*

Main category: cs.CV

TL;DR: SVLP-IL: VLP-based rehearsal-free incremental learning for face PAD using Multi-Aspect Prompting and Selective EWC to reduce forgetting and boost cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Need for rehearsal-free incremental learning for face PAD due to evolving spoofing tactics and privacy rules; VLP models' promptability offers efficient adaptation without retaining past data.

Method: Proposed method: SVLP-IL uses vision-language pre-trained models with Multi-Aspect Prompting (MAP) and Selective Elastic Weight Consolidation (SEWC) to enable rehearsal-free incremental learning for face PAD.

Result: Experiments across multiple PAD benchmarks show reduced catastrophic forgetting and improved unseen domain performance compared to baselines.

Conclusion: SVLP-IL provides a privacy-compliant, practical approach for lifelong PAD under rehearsal-free constraints by balancing stability and plasticity through MAP and SEWC.

Abstract: Face Presentation Attack Detection (PAD) demands incremental learning (IL) to combat evolving spoofing tactics and domains. Privacy regulations, however, forbid retaining past data, necessitating rehearsal-free IL (RF-IL). Vision-Language Pre-trained (VLP) models, with their prompt-tunable cross-modal representations, enable efficient adaptation to new spoofing styles and domains. Capitalizing on this strength, we propose \textbf{SVLP-IL}, a VLP-based RF-IL framework that balances stability and plasticity via \textit{Multi-Aspect Prompting} (MAP) and \textit{Selective Elastic Weight Consolidation} (SEWC). MAP isolates domain dependencies, enhances distribution-shift sensitivity, and mitigates forgetting by jointly exploiting universal and domain-specific cues. SEWC selectively preserves critical weights from previous tasks, retaining essential knowledge while allowing flexibility for new adaptations. Comprehensive experiments across multiple PAD benchmarks show that SVLP-IL significantly reduces catastrophic forgetting and enhances performance on unseen domains. SVLP-IL offers a privacy-compliant, practical solution for robust lifelong PAD deployment in RF-IL settings.

</details>


### [108] [Finer-Personalization Rank: Fine-Grained Retrieval Examines Identity Preservation for Personalized Generation](https://arxiv.org/abs/2512.19026)
*Connor Kilrain,David Carlyn,Julia Chae,Sara Beery,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 提出基于检索排名的Finer-Personalization Rank用于衡量生成图像的身份保真，比传统语义相似度更能检测细粒度身份漂移。


<details>
  <summary>Details</summary>
Motivation: 评估个性化生成模型在保持主体身份细节上的能力，目前度量偏重语义相似性而忽略细粒度辨识特征。

Method: 提出Finer-Personalization Rank，将生成图像视作对带身份标签的相似真实图像库的检索查询，采用检索度量（如mAP）在多粒度身份级别上评估保真度。

Result: 在CUB、Stanford Cars和动物Re-ID基准上，Finer-Personalization Rank比语义度量更能反映身份保持状况，并发现在若干流行个性化方法中存在显著身份漂移。

Conclusion: 基于图库的排名评估协议为个性化生成的身份保持提供了更有原则且实用的评估方法。

Abstract: The rise of personalized generative models raises a central question: how should we evaluate identity preservation? Given a reference image (e.g., one's pet), we expect the generated image to retain precise details attached to the subject's identity. However, current generative evaluation metrics emphasize the overall semantic similarity between the reference and the output, and overlook these fine-grained discriminative details. We introduce Finer-Personalization Rank, an evaluation protocol tailored to identity preservation. Instead of pairwise similarity, Finer-Personalization Rank adopts a ranking view: it treats each generated image as a query against an identity-labeled gallery consisting of visually similar real images. Retrieval metrics (e.g., mean average precision) measure performance, where higher scores indicate that identity-specific details (e.g., a distinctive head spot) are preserved. We assess identity at multiple granularities -- from fine-grained categories (e.g., bird species, car models) to individual instances (e.g., re-identification). Across CUB, Stanford Cars, and animal Re-ID benchmarks, Finer-Personalization Rank more faithfully reflects identity retention than semantic-only metrics and reveals substantial identity drift in several popular personalization methods. These results position the gallery-based protocol as a principled and practical evaluation for personalized generation.

</details>


### [109] [Automatic Neuronal Activity Segmentation in Fast Four Dimensional Spatio-Temporal Fluorescence Imaging using Bayesian Approach](https://arxiv.org/abs/2512.19032)
*Ran Li,Pan Xiao,Kaushik Dutta,Youdong Guo*

Main category: cs.CV

TL;DR: 提出贝叶斯深度学习方法，利用时间相关性和空间均值图对4D钙成像数据进行神经元活动检测，产生概率分割与不确定性评估，Dice约0.8且测试可重复性良好。


<details>
  <summary>Details</summary>
Motivation: 手工分割耗时且泛化差，需自动、精确地从大规模体内钙成像（4D）数据中检测与行为相关的神经元活动。

Method: 计算像素级时间相关性（时间信息）与均值摘要图（空间信息）作为输入，构建贝叶斯深度学习网络以输出概率分割图并估计不确定性；并以再现性测试评估网络泛化。

Result: 相对于由Otsu方法生成的合成真值，平均Dice得分0.81；在可重复性测试中两次运行间平均Dice为0.79，表明检测准确且稳定。

Conclusion: 该论文提出了一个基于贝叶斯深度学习的框架，用于在光片显微镜采集的4D时空钙成像数据中检测活跃神经元，结合像素级时间相关性图和空间均值摘要图，输出概率分割和不确定性估计，并通过可重复性测试验证了泛化能力。

Abstract: Fluorescence Microcopy Calcium Imaging is a fundamental tool to in-vivo record and analyze large scale neuronal activities simultaneously at a single cell resolution. Automatic and precise detection of behaviorally relevant neuron activity from the recordings is critical to study the mapping of brain activity in organisms. However a perpetual bottleneck to this problem is the manual segmentation which is time and labor intensive and lacks generalizability. To this end, we present a Bayesian Deep Learning Framework to detect neuronal activities in 4D spatio-temporal data obtained by light sheet microscopy. Our approach accounts for the use of temporal information by calculating pixel wise correlation maps and combines it with spatial information given by the mean summary image. The Bayesian framework not only produces probability segmentation maps but also models the uncertainty pertaining to active neuron detection. To evaluate the accuracy of our framework we implemented the test of reproducibility to assert the generalization of the network to detect neuron activity. The network achieved a mean Dice Score of 0.81 relative to the synthetic Ground Truth obtained by Otsu's method and a mean Dice Score of 0.79 between the first and second run for test of reproducibility. Our method successfully deployed can be used for rapid detection of active neuronal activities for behavioural studies.

</details>


### [110] [Distinguishing Visually Similar Actions: Prompt-Guided Semantic Prototype Modulation for Few-Shot Action Recognition](https://arxiv.org/abs/2512.19036)
*Xiaoyang Li,Mingming Lu,Ruiqi Wang,Hao Li,Zewei Le*

Main category: cs.CV

TL;DR: 提出CLIP-SPM：HSMR提纯运动特征抑制静态干扰，SPM生成查询相关文本提示弥合模态差距，PADM对原型与查询做双重调制；在Kinetics、SSv2、UCF101、HMDB51上在1/3/5-shot均表现良好。


<details>
  <summary>Details</summary>
Motivation: 少样本动作识别面临三大问题：时序信息易被静态背景干扰、不同类别间视觉相似导致难以区分、以及支持集文本/视觉原型与仅视觉查询间的模态不匹配。

Method: CLIP-SPM包含：1) HSMR：对浅层与深层运动特征进行层次化对齐与协同抑制背景噪声；2) SPM：基于查询生成语义相关的文本提示并与视觉特征融合以提升判别性；3) PADM：对支持原型进行精炼并将查询对齐到全局语义锚点以增强支持-查询一致性。

Result: 在Kinetics、SSv2-Full/Smal、UCF101、HMDB51等基准上，在1/3/5-shot设置下取得竞争性性能；消融与可视化分析证明各模块有效性。

Conclusion: 本文提出CLIP-SPM框架，通过三大模块协同改善少样本动作识别中的时序建模、视觉相似性区分及视觉-文本模态差异问题，实验表明在多个基准上取得有竞争力的结果。

Abstract: Few-shot action recognition aims to enable models to quickly learn new action categories from limited labeled samples, addressing the challenge of data scarcity in real-world applications. Current research primarily addresses three core challenges: (1) temporal modeling, where models are prone to interference from irrelevant static background information and struggle to capture the essence of dynamic action features; (2) visual similarity, where categories with subtle visual differences are difficult to distinguish; and (3) the modality gap between visual-textual support prototypes and visual-only queries, which complicates alignment within a shared embedding space. To address these challenges, this paper proposes a CLIP-SPM framework, which includes three components: (1) the Hierarchical Synergistic Motion Refinement (HSMR) module, which aligns deep and shallow motion features to improve temporal modeling by reducing static background interference; (2) the Semantic Prototype Modulation (SPM) strategy, which generates query-relevant text prompts to bridge the modality gap and integrates them with visual features, enhancing the discriminability between similar actions; and (3) the Prototype-Anchor Dual Modulation (PADM) method, which refines support prototypes and aligns query features with a global semantic anchor, improving consistency across support and query samples. Comprehensive experiments across standard benchmarks, including Kinetics, SSv2-Full, SSv2-Small, UCF101, and HMDB51, demonstrate that our CLIP-SPM achieves competitive performance under 1-shot, 3-shot, and 5-shot settings. Extensive ablation studies and visual analyses further validate the effectiveness of each component and its contributions to addressing the core challenges. The source code and models are publicly available at GitHub.

</details>


### [111] [WaTeRFlow: Watermark Temporal Robustness via Flow Consistency](https://arxiv.org/abs/2512.19048)
*Utae Jeong,Sumin In,Hyunju Ryu,Jaewan Choi,Feng Yang,Jongheon Jeong,Seungryong Kim,Sangpil Kim*

Main category: cs.CV

TL;DR: WaTeRFlow trains watermark decoder with flow-guided edits and consistency/semantic losses to recover watermarks from videos generated by I2V, yielding better per-frame and first-frame accuracy under distortions.


<details>
  <summary>Details</summary>
Motivation: I2V conversion weakens per-frame watermark detection; need cross-modal watermark recovery robust to diffusion edits and temporal inconsistencies.

Method: Paper proposes WaTeRFlow for robust watermark recovery under image-to-video (I2V). It combines FUSE (instruction-driven edits + video diffusion proxy), optical-flow warping with Temporal Consistency Loss, and semantic preservation loss.

Result: Higher first-frame and per-frame bit accuracy across I2V models; resilience to distortions pre/post video generation.

Conclusion: WaTeRFlow effectively stabilizes and preserves watermark detection in I2V scenarios, improving robustness and temporal consistency.

Abstract: Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.

</details>


### [112] [Decoupled Generative Modeling for Human-Object Interaction Synthesis](https://arxiv.org/abs/2512.19049)
*Hwanhee Jung,Seunggwan Lee,Jeongyoon Yoon,SeungHyeon Kim,Giljoo Nam,Qixing Huang,Sangpil Kim*

Main category: cs.CV

TL;DR: DecHOI通过将轨迹规划和动作生成解耦并使用针对远端关节的对抗判别器，在动态场景中生成更真实、连贯且无穿透的人-物体交互，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成真实的人-物体交互(HOI)对3D视觉和机器人学很重要，现有方法依赖手动中间路径点且将优化目标放在单一网络中，导致复杂性高、灵活性差以及不同步或穿透等错误。

Method: 提出DecHOI：将路径规划和动作合成解耦。先用轨迹生成器生成无人为指定路径点的人和物体轨迹，再用动作生成器在这些路径条件下合成细节动作；使用聚焦远端关节动力学的判别器进行对抗训练以改进接触真实感；同时建模移动对方并支持动态场景下的响应性长序列规划并保持计划一致性。

Result: 在FullBodyManipulation和3D-FUTURE两个基准上，DecHOI在多数定量指标和定性评估中超越先前方法，且感知研究也偏好本方法生成结果。

Conclusion: 通过解耦路径规划与动作合成并引入针对远端关节的判别器与动态场景建模，DecHOI提升了HOI合成的自然性和稳定性，为长序列与响应性场景提供更一致和真实的交互生成。

Abstract: Synthesizing realistic human-object interaction (HOI) is essential for 3D computer vision and robotics, underpinning animation and embodied control. Existing approaches often require manually specified intermediate waypoints and place all optimization objectives on a single network, which increases complexity, reduces flexibility, and leads to errors such as unsynchronized human and object motion or penetration. To address these issues, we propose Decoupled Generative Modeling for Human-Object Interaction Synthesis (DecHOI), which separates path planning and action synthesis. A trajectory generator first produces human and object trajectories without prescribed waypoints, and an action generator conditions on these paths to synthesize detailed motions. To further improve contact realism, we employ adversarial training with a discriminator that focuses on the dynamics of distal joints. The framework also models a moving counterpart and supports responsive, long-sequence planning in dynamic scenes, while preserving plan consistency. Across two benchmarks, FullBodyManipulation and 3D-FUTURE, DecHOI surpasses prior methods on most quantitative metrics and qualitative evaluations, and perceptual studies likewise prefer our results.

</details>


### [113] [6DAttack: Backdoor Attacks in the 6DoF Pose Estimation](https://arxiv.org/abs/2512.19058)
*Jihui Guo,Zongmin Zhang,Zhen Sun,Yuhao Yang,Jinlin Wu,Fu Zhang,Xinlei He*

Main category: cs.CV

TL;DR: 本文提出6DAttack：一种对6DoF目标位姿估计模型的后门攻击框架，利用3D触发器在不影响正常性能的情况下诱导模型输出受控错误位姿。对PVNet、DenseFusion、PoseDiffusion和LINEMOD、YCB-Video、CO3D数据集的实验表明高攻击成功率（ASR），触发样本可达97.70% ADD-P，且现有防御无效。


<details>
  <summary>Details</summary>
Motivation: 现有后门研究主要集中在2D分类/检测任务，6DoF位姿估计涉及连续的平移与旋转参数，传统2D后门方法难以直接迁移，存在被忽视的安全威胁。作者旨在构建能精确控制连续位姿参数的后门攻击，验证其有效性与防御难度。

Method: 提出6DAttack：使用3D物体触发器（非仅2D图案）施加到输入场景中，训练阶段将带触发器的样本标注为目标错误位姿，优化使模型在触发下输出预设的错误平移/旋转，而在无触发时保持正常性能。实现细节涵盖触发器设计、注入策略以及损失构造以同时保持清洁精度与攻击效果。

Result: 在多个主流位姿估计方法（PVNet、DenseFusion、PoseDiffusion）与数据集（LINEMOD、YCB-Video、CO3D）上均实现高ASR和高干净精度。报告指标包括干净样本的ADD准确率可达100%，ASR达100%，触发样本的ADD-P最高97.70%。同时评估一种代表性防御方法，仍无法有效缓解攻击。

Conclusion: 6DoF位姿估计存在严重且未被充分关注的后门风险。通过3D触发器可实现对连续位姿参数的精确控制，现有防御方法难以检测或阻止，呼吁更多研究投入防御策略与安全评估。

Abstract: Deep learning advances have enabled accurate six-degree-of-freedom (6DoF) object pose estimation, widely used in robotics, AR/VR, and autonomous systems. However, backdoor attacks pose significant security risks. While most research focuses on 2D vision, 6DoF pose estimation remains largely unexplored. Unlike traditional backdoors that only change classes, 6DoF attacks must control continuous parameters like translation and rotation, rendering 2D methods inapplicable. We propose 6DAttack, a framework using 3D object triggers to induce controlled erroneous poses while maintaining normal behavior. Evaluations on PVNet, DenseFusion, and PoseDiffusion across LINEMOD, YCB-Video, and CO3D show high attack success rates (ASRs) without compromising clean performance. Backdoored models achieve up to 100% clean ADD accuracy and 100% ASR, with triggered samples reaching 97.70% ADD-P. Furthermore, a representative defense remains ineffective. Our findings reveal a serious, underexplored threat to 6DoF pose estimation.

</details>


### [114] [Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding](https://arxiv.org/abs/2512.19070)
*Ruiqi Ma,Yu Yan,Chunhong Zhang,Minghao Yin,XinChao Liu,Zhihong Jin,Zheng Hu*

Main category: cs.CV

TL;DR: HDD是一种无需训练的解码策略：用分割增强图+空白图减少LVLM的语言与视觉幻觉，从而提高物体识别准确性。


<details>
  <summary>Details</summary>
Motivation: 提出HDD以同时降低视觉与语言模态中的幻觉问题，解决LVLM在物体识别时生成与图像不符的流畅但错误文本，避免真实应用中严重后果。

Method: 无训练方法：对原始图片进行分割并选择作为增强的图像，同时使用空白图像来消除语言先验引起的幻觉，结合原图与分割图与空白图的解码以减少模型对语言先验的依赖并增强视觉信息利用。

Result: 通过在原图、分割图与空白图上的推理，HDD在减少语言幻觉与视觉幻觉方面均有改进；在若干基准/实验中表现出提升（论文附有代码实现）。

Conclusion: HDD是一个无需额外训练的简单有效策略，通过图像分割增强与空白图像反事实设计，能同时缓解LVLM的语言与视觉幻觉，提升物体识别的可靠性。

Abstract: Large Vision-Language Models (LVLMs) bridge the gap between visual and linguistic modalities, demonstrating strong potential across a variety of domains. However, despite significant progress, LVLMs still suffer from severe hallucination issues in object recognition tasks. These models often fail to accurately identify certain objects, leading to text generation that appears fluent but does not correspond to the visual content, which can have serious consequences in real-world applications. Recently, several methods have been proposed to alleviate LVLM hallucinations, but most focus solely on reducing hallucinations in the language modality. To mitigate hallucinations in both the language and visual modalities, we introduce Hallucination Disentangled Decoding (HDD) method that requires no training. HDD enhances the original image by segmenting it and selecting images that augment the original, while also utilizing a blank image to eliminate language prior hallucinations in both the original and segmented images. This design not only reduces the model's dependence on language priors but also enhances its visual performance. (Code: https://github.com/rickeyhhh/Hallucination-Disentangled-Decoding)

</details>


### [115] [Retrieving Objects from 3D Scenes with Box-Guided Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2512.19088)
*Khanh Nguyen,Dasith de Silva Edirimuni,Ghulam Mubashar Hassan,Ajmal Mian*

Main category: cs.CV

TL;DR: 通过用2D开放词汇检测器引导从RGB生成3D实例掩码，本文实现了实时且对稀有类别健壮的开域3D实例检索，避免了重度依赖SAM/CLIP的昂贵流程。


<details>
  <summary>Details</summary>
Motivation: 现有开域3D实例分割方法依赖SAM/CLIP导致计算开销大且推理慢；Open-YOLO 3D尽管高效，但对训练数据中少见类别泛化差。因此需要一种兼具实时性与对新颖/长尾类别识别能力的方法。

Method: 利用2D open-vocabulary检测器在图像中定位并提供类别提示，按提示生成对应的2D掩码并将其投影/对齐到场景点云以产生3D实例掩码；结合预训练3D分割器和几何一致性校正以提高掩码质量，并用轻量分类器进行高效检索。

Result: 提出了一种结合2D open-vocabulary检测器与RGB引导的3D实例掩码生成方法，用于在场景点云中快速检索稀有或未见类别的物体。

Conclusion: 方法继承2D检测器对新颖类别的识别能力，同时保持高效分类，克服了仅依赖点云预训练分割器在长尾类别上的泛化不足，显著加快推理并提高稀有实例检索准确性。

Abstract: Locating and retrieving objects from scene-level point clouds is a challenging problem with broad applications in robotics and augmented reality. This task is commonly formulated as open-vocabulary 3D instance segmentation. Although recent methods demonstrate strong performance, they depend heavily on SAM and CLIP to generate and classify 3D instance masks from images accompanying the point cloud, leading to substantial computational overhead and slow processing that limit their deployment in real-world settings. Open-YOLO 3D alleviates this issue by using a real-time 2D detector to classify class-agnostic masks produced directly from the point cloud by a pretrained 3D segmenter, eliminating the need for SAM and CLIP and significantly reducing inference time. However, Open-YOLO 3D often fails to generalize to object categories that appear infrequently in the 3D training data. In this paper, we propose a method that generates 3D instance masks for novel objects from RGB images guided by a 2D open-vocabulary detector. Our approach inherits the 2D detector's ability to recognize novel objects while maintaining efficient classification, enabling fast and accurate retrieval of rare instances from open-ended text queries. Our code will be made available at https://github.com/ndkhanh360/BoxOVIS.

</details>


### [116] [Auditing Significance, Metric Choice, and Demographic Fairness in Medical AI Challenges](https://arxiv.org/abs/2512.19091)
*Ariel Lubonja,Pedro R. A. S. Bassi,Wenxuan Li,Hualin Qiao,Randal Burns,Alan L. Yuille,Zongwei Zhou*

Main category: cs.CV

TL;DR: RankInsight provides statistical pair-wise significance maps, organ-appropriate metric re-evaluation, and intersectional fairness audits to make medical AI leaderboards more reliable and fair.


<details>
  <summary>Details</summary>
Motivation: Current medical AI leaderboards give potentially misleading rankings due to untested score significance, unsuitable single metrics across organs, and lack of intersectional fairness reporting.

Method: RankInsight computes pair-wise statistical significance maps, supports organ-specific metrics (e.g., NSD for tubular structures), and audits performance across intersecting demographic groups; applied to a dataset including Johns Hopkins Hospital data and existing challenge submissions (nnU-Net, Vision-Language, MONAI).

Result: RankInsight toolkit addresses three limitations in medical AI leaderboards: lack of statistical significance testing, inappropriate averaged metrics for organs, and absence of intersectional fairness audits.

Conclusion: Using RankInsight changes leaderboard conclusions: nnU-Net outperforms others with statistical certainty; organ-appropriate metrics (e.g., NSD) can reverse model rankings; intersectional audits reveal significant gender-race disparities in MONAI submissions.

Abstract: Open challenges have become the de facto standard for comparative ranking of medical AI methods. Despite their importance, medical AI leaderboards exhibit three persistent limitations: (1) score gaps are rarely tested for statistical significance, so rank stability is unknown; (2) single averaged metrics are applied to every organ, hiding clinically important boundary errors; (3) performance across intersecting demographics is seldom reported, masking fairness and equity gaps. We introduce RankInsight, an open-source toolkit that seeks to address these limitations. RankInsight (1) computes pair-wise significance maps that show the nnU-Net family outperforms Vision-Language and MONAI submissions with high statistical certainty; (2) recomputes leaderboards with organ-appropriate metrics, reversing the order of the top four models when Dice is replaced by NSD for tubular structures; and (3) audits intersectional fairness, revealing that more than half of the MONAI-based entries have the largest gender-race discrepancy on our proprietary Johns Hopkins Hospital dataset. The RankInsight toolkit is publicly released and can be directly applied to past, ongoing, and future challenges. It enables organizers and participants to publish rankings that are statistically sound, clinically meaningful, and demographically fair.

</details>


### [117] [Mamba-Based Modality Disentanglement Network for Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2512.19095)
*Weiyi Lyu,Xinming Fang,Jun Wang,Jun Shi,Guixu Zhang,Juncheng Li*

Main category: cs.CV

TL;DR: 提出MambaMDN，一种双域多对比MRI重建框架：先用全采样参考K空间补全欠采样目标K空间，再用Mamba解缠网络去除参考特异性混合特征，并通过迭代精炼提高重建质量，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 加速MRI受限于别名伪影和多对比融合中参考信息污染；需要同时利用K空间先验并避免参考特异性信息干扰。

Method: 在K域用参考全采样数据完成目标欠采样K空间，得到结构对齐但模态混合的输入；设计Mamba-based模态解缠网络提取并去除参考特异性特征；引入迭代精炼机制对特征进行多轮净化与重建。

Result: 大量实验表明MambaMDN在重建质量上明显超过现有多对比重建方法，减少别名伪影并提高结构保真度。

Conclusion: 通过双域策略结合K空间先验与模态解缠、迭代净化，MambaMDN有效提升多对比MRI重建性能，具有潜在临床应用价值。

Abstract: Magnetic resonance imaging (MRI) is a cornerstone of modern clinical diagnosis, offering unparalleled soft-tissue contrast without ionizing radiation. However, prolonged scan times remain a major barrier to patient throughput and comfort. Existing accelerated MRI techniques often struggle with two key challenges: (1) failure to effectively utilize inherent K-space prior information, leading to persistent aliasing artifacts from zero-filled inputs; and (2) contamination of target reconstruction quality by irrelevant information when employing multi-contrast fusion strategies. To overcome these challenges, we present MambaMDN, a dual-domain framework for multi-contrast MRI reconstruction. Our approach first employs fully-sampled reference K-space data to complete the undersampled target data, generating structurally aligned but modality-mixed inputs. Subsequently, we develop a Mamba-based modality disentanglement network to extract and remove reference-specific features from the mixed representation. Furthermore, we introduce an iterative refinement mechanism to progressively enhance reconstruction accuracy through repeated feature purification. Extensive experiments demonstrate that MambaMDN can significantly outperform existing multi-contrast reconstruction methods.

</details>


### [118] [GaussianImage++: Boosted Image Representation and Compression with 2D Gaussian Splatting](https://arxiv.org/abs/2512.19108)
*Tiantian Li,Xinjie Zhang,Xingtong Ge,Tongda Xu,Dailan He,Jun Zhang,Yan Wang*

Main category: cs.CV

TL;DR: GaussianImage++ uses distortion-driven densification, context-aware Gaussian filters, and learnable quantizers to achieve better representation/compression than GaussianImage and INR COIN with fewer primitives and real-time decoding.


<details>
  <summary>Details</summary>
Motivation: Reduce training time/memory and number of Gaussian primitives needed for high-fidelity image representation and compression compared to INRs and prior Gaussian Splatting methods.

Method: analysis of methods

Result: Proposed GaussianImage++ with three components: distortion-driven densification allocating Gaussians by signal intensity; context-aware Gaussian filters per primitive to assist densification and optimize primitives; attribute-separated learnable scalar quantizers and quantization-aware training for compression. Implements rendering via 2D Gaussian splatting with limited primitives. Experiments compare to GaussianImage and COIN showing improved representation and compression, real-time decoding and low memory.

Conclusion: GaussianImage++ effectively reduces required primitives while improving image representation and compression; combines adaptive allocation, content-aware filtering, and quantization-aware compression to outperform prior GS and INR-based methods.

Abstract: Implicit neural representations (INRs) have achieved remarkable success in image representation and compression, but they require substantial training time and memory. Meanwhile, recent 2D Gaussian Splatting (GS) methods (\textit{e.g.}, GaussianImage) offer promising alternatives through efficient primitive-based rendering. However, these methods require excessive Gaussian primitives to maintain high visual fidelity. To exploit the potential of GS-based approaches, we present GaussianImage++, which utilizes limited Gaussian primitives to achieve impressive representation and compression performance. Firstly, we introduce a distortion-driven densification mechanism. It progressively allocates Gaussian primitives according to signal intensity. Secondly, we employ context-aware Gaussian filters for each primitive, which assist in the densification to optimize Gaussian primitives based on varying image content. Thirdly, we integrate attribute-separated learnable scalar quantizers and quantization-aware training, enabling efficient compression of primitive attributes. Experimental results demonstrate the effectiveness of our method. In particular, GaussianImage++ outperforms GaussianImage and INRs-based COIN in representation and compression performance while maintaining real-time decoding and low memory usage.

</details>


### [119] [Trifocal Tensor and Relative Pose Estimation with Known Vertical Direction](https://arxiv.org/abs/2512.19110)
*Tao Li,Zhenbao Yu,Banglei Guan,Jianli Han,Weimin Lv,Friedrich Fraundorfer*

Main category: cs.CV

TL;DR: 利用IMU提供的竖直方向，作者提出基于四点（线性闭式）和三点（Gröbner基最小解）的三视图相对位姿估计算法，在合成与KITTI数据上表现优越，适合RANSAC框架。


<details>
  <summary>Details</summary>
Motivation: 在已知摄像机视图竖直方向的情况下，简化相对位姿估计问题，提高RANSAC下的效率和鲁棒性，适用于装有IMU的设备（自动驾驶、手机、无人机）。

Method: 提出两种求解器：1）线性闭式解：在三视图中只需四个点匹配，解两旋转角和两平移向量；2）最小解：使用Gröbner基求解器，仅需三个点匹配。

Result: 在合成数据和KITTI真实场景上测试，所提方法在位姿估计精度上优于其他替代方法，且因所需点更少在RANSAC中更高效。

Conclusion: 利用已知竖直方向能显著减少自由度，从而设计出高效、鲁棒的多视图位姿估计算法，适合实时视觉里程计与大规模场景。

Abstract: This work presents two novel solvers for estimating the relative poses among views with known vertical directions. The vertical directions of camera views can be easily obtained using inertial measurement units (IMUs) which have been widely used in autonomous vehicles, mobile phones, and unmanned aerial vehicles (UAVs). Given the known vertical directions, our lgorithms only need to solve for two rotation angles and two translation vectors. In this paper, a linear closed-form solution has been described, requiring only four point correspondences in three views. We also propose a minimal solution with three point correspondences using the latest Gröbner basis solver. Since the proposed methods require fewer point correspondences, they can be efficiently applied within the RANSAC framework for outliers removal and pose estimation in visual odometry. The proposed method has been tested on both synthetic data and real-world scenes from KITTI. The experimental results show that the accuracy of the estimated poses is superior to other alternative methods.

</details>


### [120] [Generative Giants, Retrieval Weaklings: Why do Multimodal Large Language Models Fail at Multimodal Retrieval?](https://arxiv.org/abs/2512.19115)
*Hengyi Feng,Zeang Sheng,Meiyi Qiang,Wentao Zhang*

Main category: cs.CV

TL;DR: MLLM嵌入被文本语义主导且为生成优化导致同质化，一些相似度相关成分是有害干扰，需增强视觉成分与抑制干扰以提升零样本检索性能。


<details>
  <summary>Details</summary>
Motivation: 分析为何多模态大语言模型(MLLMs)在零样本多模态检索任务中表现差，找出影响检索能力的内部机制。

Method: 使用稀疏自编码器(SAEs)将MLLM的输出表示分解为可解释的语义成分，并通过这些成分分析表示空间的构成及它们对相似度计算的贡献。

Result: 发现表示空间被文本语义主导，视觉信息占比很小；模型为生成任务而优化的跨模态桥接使嵌入趋同，降低判别性；部分用于相似度计算的特征成分实际上是干扰项，降低检索性能。

Conclusion: 首次对MLLM表示在多模态检索场景下进行深入可解释性分析，指出文本主导与嵌入同质化为主要瓶颈，并为提升MLLM检索能力提出改进方向。

Abstract: Despite the remarkable success of multimodal large language models (MLLMs) in generative tasks, we observe that they exhibit a counterintuitive deficiency in the zero-shot multimodal retrieval task. In this work, we investigate the underlying mechanisms that hinder MLLMs from serving as effective retrievers. With the help of sparse autoencoders (SAEs), we decompose MLLM output representations into interpretable semantic concepts to probe their intrinsic behavior. Our analysis reveals that the representation space of MLLMs is overwhelmingly dominated by textual semantics; the visual information essential for multimodal retrieval only constitutes a small portion. This imbalance is compounded by the heavy focus of MLLMs on bridging image-text modalities, which facilitates generation but homogenizes embeddings and finally diminishes the discriminative power required for multimodal retrieval. We further discover that the specific feature components that contribute most to the similarity computations for MLLMs are in fact distractors that actively degrade retrieval performance. Overall, our work provides the first in-depth interpretability analysis of MLLM representations in the context of multimodal retrieval and offers possible directions for enhancing the multimodal retrieval capabilities of MLLMs.

</details>


### [121] [AMap: Distilling Future Priors for Ahead-Aware Online HD Map Construction](https://arxiv.org/abs/2512.19150)
*Ruikai Li,Xinrun Li,Mengwei Xie,Hao Shan,Shoumeng Qiu,Xinyuan Chang,Yizhe Fan,Feng Xiong,Han Jiang,Yilong Ren,Haiyang Yu,Mu Xu,Yang Long,Varun Ojha,Zhiyong Cui*

Main category: cs.CV

TL;DR: 提出一种从有未来信息的教师到仅用当前帧学生的蒸馏方法（AMap），通过多层BEV蒸馏与非对称查询自适应，使学生获得零推理代价的前瞻能力，在nuScenes和Argoverse2上对前方区域感知有明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前在线高精地图（HD map）构建依赖历史时序融合，但该范式存在安全缺陷：主要改善已行驶区域，却对前方不可见区域提升有限；而前方感知错误会直接导致危险驾驶。为填补此安全空白，提出能“前瞻感知”的方法。

Method: 提出AMap框架：采用“从未来蒸馏”（distill-from-future）范式，让拥有未来时序信息的教师模型指导仅用当前帧的轻量学生模型；引入多层BEV蒸馏（Multi-Level BEV Distillation）并配合空间掩模，以及非对称查询自适应（Asymmetric Query Adaptation）模块，将未来感知能力压缩到当前帧的静态查询中。

Result: 在nuScenes和Argoverse 2上大量实验，AMap显著提升当前帧感知性能；尤其在关键的前方区域超过了最先进的时序模型，同时保持了仅用当前帧推理的效率。

Conclusion: AMap通过从未来蒸馏的设计，在不增加在线推理成本的情况下增强了前瞻感知能力，填补了时序融合方法的“空间向后”的安全漏洞，对自动驾驶HD地图构建具有重要实用价值。

Abstract: Online High-Definition (HD) map construction is pivotal for autonomous driving. While recent approaches leverage historical temporal fusion to improve performance, we identify a critical safety flaw in this paradigm: it is inherently ``spatially backward-looking." These methods predominantly enhance map reconstruction in traversed areas, offering minimal improvement for the unseen road ahead. Crucially, our analysis of downstream planning tasks reveals a severe asymmetry: while rearward perception errors are often tolerable, inaccuracies in the forward region directly precipitate hazardous driving maneuvers. To bridge this safety gap, we propose AMap, a novel framework for Ahead-aware online HD Mapping. We pioneer a ``distill-from-future" paradigm, where a teacher model with privileged access to future temporal contexts guides a lightweight student model restricted to the current frame. This process implicitly compresses prospective knowledge into the student model, endowing it with ``look-ahead" capabilities at zero inference-time cost. Technically, we introduce a Multi-Level BEV Distillation strategy with spatial masking and an Asymmetric Query Adaptation module to effectively transfer future-aware representations to the student's static queries. Extensive experiments on the nuScenes and Argoverse 2 benchmark demonstrate that AMap significantly enhances current-frame perception. Most notably, it outperforms state-of-the-art temporal models in critical forward regions while maintaining the efficiency of single current frame inference.

</details>


### [122] [OmniMoGen: Unifying Human Motion Generation via Learning from Interleaved Text-Motion Instructions](https://arxiv.org/abs/2512.19159)
*Wendong Bu,Kaihang Pan,Yuze Lin,Jiacheng Li,Kai Shen,Wenqiao Zhang,Juncheng Li,Jun Xiao,Siliang Tang*

Main category: cs.CV

TL;DR: OmniMoGen unifies motion tasks via an RVQ-VAE and transformer to process interleaved text-motion instructions; trained on X2Mo dataset, it attains SOTA on multiple tasks and shows emergent compositional and knowledge-aware capabilities.


<details>
  <summary>Details</summary>
Motivation: Unify diverse human motion generation tasks under a single instruction-driven framework, inspired by LLMs' unified approach to linguistic tasks.

Method: Propose OmniMoGen: RVQ-VAE for motion discretization and a transformer-based model accepting interleaved text-motion instructions. Build X2Mo dataset (137K interleaved instructions) and AnyContext benchmark; train end-to-end instruction-driven generation supporting multiple tasks.

Result: OmniMoGen achieves state-of-the-art on text-to-motion and motion editing and performs well on AnyContext. Shows emergent capabilities: compositional editing, self-reflective generation, knowledge-informed generation.

Conclusion: OmniMoGen demonstrates feasibility of a unified, instruction-driven paradigm for versatile motion generation, advancing toward more intelligent, flexible motion synthesis.

Abstract: Large language models (LLMs) have unified diverse linguistic tasks within a single framework, yet such unification remains unexplored in human motion generation. Existing methods are confined to isolated tasks, limiting flexibility for free-form and omni-objective generation. To address this, we propose OmniMoGen, a unified framework that enables versatile motion generation through interleaved text-motion instructions. Built upon a concise RVQ-VAE and transformer architecture, OmniMoGen supports end-to-end instruction-driven motion generation. We construct X2Mo, a large-scale dataset of over 137K interleaved text-motion instructions, and introduce AnyContext, a benchmark for evaluating interleaved motion generation. Experiments show that OmniMoGen achieves state-of-the-art performance on text-to-motion, motion editing, and AnyContext, exhibiting emerging capabilities such as compositional editing, self-reflective generation, and knowledge-informed generation. These results mark a step toward the next intelligent motion generation. Project Page: https://OmniMoGen.github.io/.

</details>


### [123] [PEDESTRIAN: An Egocentric Vision Dataset for Obstacle Detection on Pavements](https://arxiv.org/abs/2512.19190)
*Marios Thoma,Zenonas Theodosiou,Harris Partaourides,Vassilis Vassiliades,Loizos Michael,Andreas Lanitis*

Main category: cs.CV

TL;DR: PEDESTRIAN: 340 egocentric videos, 29 sidewalk obstacle classes, baseline deep-learning benchmarks for obstacle detection and recognition


<details>
  <summary>Details</summary>
Motivation: Provide a comprehensive, balanced egocentric dataset to develop real-time obstacle detection systems for pedestrian safety

Method: Dataset creation and benchmarking

Result: Created PEDESTRIAN dataset: 340 egocentric videos of 29 obstacle classes recorded with mobile phones; trained several state-of-the-art deep learning models and reported benchmark results (detection/recognition accuracy, etc.)

Conclusion: PEDESTRIAN dataset is suitable for training and benchmarking pavement obstacle detection systems to improve urban pedestrian safety

Abstract: Walking has always been a primary mode of transportation and is recognized as an essential activity for maintaining good health. Despite the need for safe walking conditions in urban environments, sidewalks are frequently obstructed by various obstacles that hinder free pedestrian movement. Any object obstructing a pedestrian's path can pose a safety hazard. The advancement of pervasive computing and egocentric vision techniques offers the potential to design systems that can automatically detect such obstacles in real time, thereby enhancing pedestrian safety. The development of effective and efficient identification algorithms relies on the availability of comprehensive and well-balanced datasets of egocentric data. In this work, we introduce the PEDESTRIAN dataset, comprising egocentric data for 29 different obstacles commonly found on urban sidewalks. A total of 340 videos were collected using mobile phone cameras, capturing a pedestrian's point of view. Additionally, we present the results of a series of experiments that involved training several state-of-the-art deep learning algorithms using the proposed dataset, which can be used as a benchmark for obstacle detection and recognition tasks. The dataset can be used for training pavement obstacle detectors to enhance the safety of pedestrians in urban areas.

</details>


### [124] [InvCoSS: Inversion-driven Continual Self-supervised Learning in Medical Multi-modal Image Pre-training](https://arxiv.org/abs/2512.19213)
*Zihao Luo,Shaohao Rui,Zhenyu Tang,Guotai Wang,Xiaosong Wang*

Main category: cs.CV

TL;DR: InvCoSS prevents catastrophic forgetting in medical CSSL by inverting pretrained models to generate synthetic prior-data with an InvUNet and repulsive representation loss, enabling joint training without real data replay and achieving competitive downstream performance.


<details>
  <summary>Details</summary>
Motivation: Avoid data replay in CSSL for medical imaging to preserve privacy and enable multi-site continual pre-training without transferring real patient data; generate synthetic prior-task images via model inversion to maintain performance.

Method: Inversion-driven continual self-supervised learning (InvCoSS)

Result: InvCoSS inverts a pretrained self-supervised model to synthesize images approximating prior task data, uses InvUNet for multi-scale image reconstruction, and employs a repulsive representation-learning mechanism to enhance diversity; synthetic images are mixed with new task data for joint optimization, preventing forgetting without real data replay.

Conclusion: InvCoSS matches or exceeds replay-based methods on nine downstream tasks while removing privacy risks and storage costs by using inverted synthetic images and repulsive representation learning.

Abstract: Continual self-supervised learning (CSSL) in medical imaging trains a foundation model sequentially, alleviating the need for collecting multi-modal images for joint training and offering promising improvements in downstream performance while preserving data privacy. However, most existing methods still rely on replaying data from previous stages to prevent catastrophic forgetting, which compromises privacy and limits their applicability in real-world scenarios where data transfer across sites is often restricted. In this work, we propose InvCoSS, an inversion-driven continual self-supervised learning framework for medical multi-modal image pre-training. Specifically, after training on a previous task, InvCoSS inverts the pre-trained self-supervised model to generate synthetic images that approximate the original training distribution. These synthetic images are then combined with data from the new task for joint optimization, which effectively mitigates catastrophic forgetting while strictly adhering to the constraint of no access to previous real data. Furthermore, to improve the fidelity of synthetic images, we introduce a novel InvUNet with a multi-scale fusion architecture to restore both high- and low-frequency components of the inverted images. To enhance diversity and prevent mode collapse, we design a repulsive representation-learning mechanism that encourages a diverse feature space for synthetic images without class guidance. Extensive experiments across nine downstream tasks validate the effectiveness of InvCoSS, achieving performance comparable to or even superior to prior data-replay methods while significantly reducing storage requirements and eliminating data privacy constraints.

</details>


### [125] [HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry](https://arxiv.org/abs/2512.19214)
*Na Gao,Chenfei Ye,Yanwu Yang,Anqi Li,Zhengbo He,Li Liang,Zhiyuan Liu,Xingyu Hao,Ting Ma,Tengfei Guo*

Main category: cs.CV

TL;DR: 提出HippMetric：基于s-rep的可变形骨架坐标系，尊重海马纵向层片结构，结合几何约束的spoke精化，以实现稳定准确的点对应和形态测量，优于现有模型


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏稳定的内在坐标系，难以处理海马高度变异和复杂折叠，限制跨个体和纵向一致性分析，因此需要生物学上有意义且数学有效的参照系以实现可靠对应

Method: s-rep based skeletal coordinate system aligned with hippocampal anatomy; two modules: skeletal-based coordinate system and individualized s-reps via surface reconstruction/deformation/spoke refinement

Result: higher accuracy, reliability, and correspondence stability on two international cohorts compared to existing shape models

Conclusion: HippMetric provides anatomically grounded, stable inter- and intra-subject point-wise correspondence for hippocampal substructural morphometry, outperforming existing methods

Abstract: Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.

</details>


### [126] [Towards Minimal Fine-Tuning of VLMs](https://arxiv.org/abs/2512.19219)
*Tiange Luo,Lajanugen Logeswaran,Jaekyeom Kim,Justin Johnson,Honglak Lee*

Main category: cs.CV

TL;DR: Image-LoRA只对视觉token的attention value路径和部分head做低秩适配，节省参数和计算，性能接近LoRA且不损害文本推理。


<details>
  <summary>Details</summary>
Motivation: 现有对VLM的微调（如LoRA）在参数和计算开销上仍较高，且视觉token占比在输入中波动大，需更高效的PEFT方法以减少adapter-only训练FLOPs并保持纯文本能力。

Method: （1）仅对attention中value矩阵应用低秩适配，且仅在视觉token span内生效；（2）通过先用rank-1 Image-LoRA估计head影响力分数，从而选择性地只适配部分最重要的attention heads；（3）对每层采用选择规模归一化以稳定更新并保持跨层一致性。

Result: 在多种screen-centric的grounding和referring基准上，Image-LoRA以更少的可训练参数和更低的adapter-only FLOPs，达到或接近标准LoRA的精度，同时在GSM8K等纯文本推理任务上不降低性能。

Conclusion: Image-LoRA在视觉-语言模型上通过只对视觉token部分的attention的value路径以及仅一部分attention head应用低秩适配，在显著降低训练FLOPs和可训练参数的同时，基本保持了与标准LoRA相当的下游定位和指代任务性能，并保留了模型的纯文本推理能力。

Abstract: We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.

</details>


### [127] [From Pixels to Predicates Structuring urban perception with scene graphs](https://arxiv.org/abs/2512.19221)
*Yunlong Liu,Shuyang Li,Pengyuan Liu,Yu Zhang,Rudi Stouffs*

Main category: cs.CV

TL;DR: Transforming street images into scene graphs and learning graph embeddings yields much better and more generalizable perception predictions, and reveals interpretable harmful relations.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore explicit relational structures in urban scenes; modelling object-object relations can better capture cues humans use for perception and improve prediction and interpretability.

Method: Three-stage pipeline: OpenPSG -> GraphMAE -> NN for perception prediction

Result: Improved prediction accuracy by ~26% over image-only baselines; strong cross-city generalization; interpretable relational patterns contributing to low perception scores (e.g., graffiti on wall, car parked on sidewalk).

Conclusion: Graph-based structured representations from SVI enhance accuracy, generalization, and interpretability for urban perception prediction versus pixel/object-feature baselines.

Abstract: Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.

</details>


### [128] [VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis](https://arxiv.org/abs/2512.19243)
*Meng Chu,Senqiao Yang,Haoxuan Che,Suiyun Zhang,Xichen Zhang,Shaozuo Yu,Haokun Gui,Zhefan Rao,Dandan Tu,Rui Liu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出LGBench（2,000个长指令任务）揭示生成模型对多目标指令的薄弱；提出VisionDirector（训练免费监督器）加上G-RPO微调，显著提升多目标图像生成与编辑的准确性和编辑效率。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在长、紧耦合、多目标的专业设计指令下表现不佳，尤其容易漏掉局部编辑和精细要求，需要真实场景基准与更稳健的控制策略来推动进步。

Method: 1) 构建LGBench，包含T2I与I2I各1000个平均含18-22个紧耦合目标的任务；2) VisionDirector：从长指令中提取结构化目标，动态选择一次生成或分阶段编辑，进行微网格采样并在每次编辑后用语义校验与回滚机制保证目标完成，同时记录目标级奖励；3) 用Group Relative Policy Optimization对规划器微调以减少编辑步骤并提高对齐度。

Result: 在GenEval上提升约7%总体得分，在ImgEdit上提升0.07绝对值；微调后平均编辑步数从4.2降至3.1；在排版、多物体场景与姿态编辑上显示定性改进，且多数模型在LGBench上目标完成率低于72%。

Conclusion: 本文提出了长目标基准LGBench以评估生成模型在真实设计场景下对多目标指令的处理能力，并引入VisionDirector，一个无需训练的视觉-语言监督框架，通过结构化目标提取、动态决策（一次生成或分阶段编辑）、微网格采样与语义验证回滚及目标级奖励日志化来提升编辑质量；进一步用Group Relative Policy Optimization微调规划器以缩短编辑轨迹并增强对齐，最终在若干评测上领先现有方法。

Abstract: Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.

</details>


### [129] [3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory](https://arxiv.org/abs/2512.19271)
*Xinyang Song,Libin Wang,Weining Wang,Zhiwei Li,Jianxin Sun,Dandan Zheng,Jingdong Chen,Qi Li,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出统一模型3SGen，通过ATM模块解耦存储任务特定先验，结合MLLM与VAE实现主/风格/结构三模态条件生成并提升性能。


<details>
  <summary>Details</summary>
Motivation: Current methods handle subject, style, structure separately causing entanglement and poor transfer; need unified scalable approach.

Method: analysis of method

Result: They propose 3SGen: MLLM with learnable semantic queries + VAE branch; Adaptive Task-specific Memory (ATM) module with gating and memory items; unified model for subject/style/structure conditioning.

Conclusion: 3SGen reduces task interference, supports compositional inputs, outperforms baselines on 3SGen-Bench and public benchmarks.

Abstract: Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.

</details>


### [130] [Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation](https://arxiv.org/abs/2512.19275)
*Ivan DeAndres-Tame,Chengwei Ye,Ruben Tolosana,Ruben Vera-Rodriguez,Shiqi Yu*

Main category: cs.CV

TL;DR: GenAI能生成高质量视觉动画，但无法可靠保留用于步态识别的细微时空运动特征；模型更依赖外观纹理而非真实运动。


<details>
  <summary>Details</summary>
Motivation: 评估当下最先进的GenAI人体动画模型是否能在生成高质量视觉效果的同时保留步态等细微时空特征，从而支持基于步态的身份识别，尤其在行为生物识别和安全相关应用中具有实际意义。

Method: 选取四种最先进的GenAI人体动画模型，在两个评估任务上测试：1) 在不同复杂度条件下从参考视频恢复步态模式；2) 将这些步态模式迁移到不同的视觉身份上。使用步态识别指标和视觉质量指标评估生物特征保真度与外观保真度，并设计身份迁移实验检验纹理与运动解耦对识别性能的影响。

Result: 视觉质量总体较高，但在身份识别相关任务中生物特征保真度低。步态恢复任务中，模型难以精确再现区分个体的时序细节；身份迁移任务显示，当纹理与运动解耦时，识别性能显著下降，说明当前模型更依赖静态外观而非动态运动线索。

Conclusion: 本研究表明，尽管先进的生成式AI（GenAI）人体动画模型在视觉质量上表现优异，但在保持步态（作为行为生物特征）以支持个体识别方面存在显著不足。模型倾向于依赖外观纹理而非时序运动细节，从而导致生物特征保真度低，身份迁移实验进一步证明当纹理与运动解耦时，基于外观的步态识别会失败。

Abstract: Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.

</details>


### [131] [Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context](https://arxiv.org/abs/2512.19283)
*Kyungwon Cho,Hanbyul Joo*

Main category: cs.CV

TL;DR: HaMoS: hand-aware, sequence-level diffusion using head trajectory + intermittent hand cues, novel augmentation and local attention; yields SOTA egocentric full-body motion reconstruction


<details>
  <summary>Details</summary>
Motivation: Estimate full-body motion from egocentric videos despite occlusions and limited FOV by leveraging intermittently visible hands and sequence context, avoiding reliance on continuous hand tracking or only head trajectories

Method: Sequence-level diffusion conditioned on head trajectory and intermittent hand cues; novel augmentation modeling diverse camera views; local attention for long sequences

Result: State-of-the-art accuracy and temporal smoothness on public benchmarks; practical for in-the-wild egocentric 3D motion understanding

Conclusion: Incorporating intermittent hand cues and sequence-level context with diffusion and augmentation yields more accurate and smooth full-body motion from egocentric video than prior methods

Abstract: Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.

</details>


### [132] [RMLer: Synthesizing Novel Objects across Diverse Categories via Reinforcement Mixing Learning](https://arxiv.org/abs/2512.19300)
*Jun Li,Zikun Chen,Haibo Chen,Shuo Chen,Jian Yang*

Main category: cs.CV

TL;DR: 将跨类别文本概念混合建模为强化学习问题，学习动态混合系数并用视觉相似度与组合平衡作为奖励，PPO训练并在推理中选择最优样本


<details>
  <summary>Details</summary>
Motivation: 现有方法混合不足、评估不严谨、输出常表现为概念失衡或简单并置，需更有效的跨类别概念融合方法

Method: 设计MLP策略网络预测文本嵌入的动态混合系数；定义两类视觉奖励（语义相似性与组成平衡）；采用PPO优化策略；推理时基于奖励选择高质量融合对象。

Result: 提出RMLer（一种基于强化学习的文本到图像中跨类别概念融合框架）

Conclusion: RMLer通过策略网络预测动态混合系数并以语义相似度与组合平衡为视觉奖励，使用PPO优化，推理时基于奖励选择最优融合，实验表明在生成连贯高保真混合对象上优于现有方法

Abstract: Novel object synthesis by integrating distinct textual concepts from diverse categories remains a significant challenge in Text-to-Image (T2I) generation. Existing methods often suffer from insufficient concept mixing, lack of rigorous evaluation, and suboptimal outputs-manifesting as conceptual imbalance, superficial combinations, or mere juxtapositions. To address these limitations, we propose Reinforcement Mixing Learning (RMLer), a framework that formulates cross-category concept fusion as a reinforcement learning problem: mixed features serve as states, mixing strategies as actions, and visual outcomes as rewards. Specifically, we design an MLP-policy network to predict dynamic coefficients for blending cross-category text embeddings. We further introduce visual rewards based on (1) semantic similarity and (2) compositional balance between the fused object and its constituent concepts, optimizing the policy via proximal policy optimization. At inference, a selection strategy leverages these rewards to curate the highest-quality fused objects. Extensive experiments demonstrate RMLer's superiority in synthesizing coherent, high-fidelity objects from diverse categories, outperforming existing methods. Our work provides a robust framework for generating novel visual concepts, with promising applications in film, gaming, and design.

</details>


### [133] [Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing](https://arxiv.org/abs/2512.19302)
*Xu Zhang,Junyao Ge,Yang Zheng,Kaitai Guo,Jimin Liang*

Main category: cs.CV

TL;DR: 本文提出Think2Seg-RS：将大视觉-语言模型（LVLM）与冻结的Segment Anything Model（SAM）解耦，通过结构化几何提示和仅掩码的强化学习目标训练LVLM生成空间化操作，从而实现语义推理到像素级分割的转换。在EarthReason数据集上达到SOTA，并具备零样本泛化能力；发现紧凑分割器在语义监督下优于大模型，负提示在异质航拍背景中无效。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割框架将语言推理与像素预测耦合进行端到端微调，导致几何定位能力弱、任务间泛化差。需要一种能把抽象语义推理映射到稳健空间动作的方法。

Method: 提出Think2Seg-RS：冻结SAM作为分割器，训练LVLM生成结构化几何提示（例如点、框、掩码指令），使用仅掩码的强化学习（mask-only RL）作为目标，鼓励LVLM把语义推理转化为空间化提示，训练过程不微调分割器。

Result: 在EarthReason数据集上取得SOTA表现；学到的提示策略能零样本泛化到多个指称分割基准，表明语义级别与实例级别定位存在明显差异。实验还显示小型分割器在语义监督下表现优于大型分割器，且负提示在异构航拍背景中效果差。

Conclusion: 提出语义级推理分割作为地理空间理解的新范式，表明通过解耦提示学习可实现可解释且统一的LVLM驱动地球观测分析，并公开代码与模型。

Abstract: Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.

</details>


### [134] [MixFlow Training: Alleviating Exposure Bias with Slowed Interpolation Mixture](https://arxiv.org/abs/2512.19311)
*Hui Li,Jiayue Lyu,Fu-Yun Wang,Kaihui Cheng,Siyu Zhu,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出 MixFlow 方法，通过在训练后为每个训练时间步使用“放慢时间步（slowed timestep）”的插值混合来缓解扩散模型的训练-测试差异（暴露偏差）。基于观察到的“慢流（Slow Flow）”现象，即与生成噪声最接近的真实插值对应更高噪声的时间步，从而改善图像生成质量。实验在分类条件图像生成与文本到图像任务上显示效果显著，在 ImageNet 上的 RAE 模型获得了极佳的 FID。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型训练与采样时输入分布不一致的问题：训练时网络看到的是与真实噪声-数据插值，而测试时看到的是模型生成的噪声，导致性能下降。

Method: 观察并利用 Slow Flow 现象：对于采样时间步 t，找到与生成噪声最接近的真实插值对应的更高噪声时间步 s（slowed timestep），构建该时刻的插值混合（slowed interpolation mixture），并将其用于对每个训练时间步的预测网络进行后训练（post-training）。

Result: 在分类条件图像生成（SiT、REPA、RAE）和文本到图像任务上均验证了 MixFlow 的有效性。特别是 RAE 模型在 ImageNet 上达到了 256x256 条件下 FID 1.43（无引导）和 1.10（有引导），512x512 下 FID 1.55（无引导）和 1.10（有引导）。

Conclusion: MixFlow 通过利用 slowed timestep 的插值混合，有效缓解了训练-测试分布不匹配，提高了扩散模型生成质量，能作为现有模型的轻量级后训练改进策略。

Abstract: This paper studies the training-testing discrepancy (a.k.a. exposure bias) problem for improving the diffusion models. During training, the input of a prediction network at one training timestep is the corresponding ground-truth noisy data that is an interpolation of the noise and the data, and during testing, the input is the generated noisy data. We present a novel training approach, named MixFlow, for improving the performance. Our approach is motivated by the Slow Flow phenomenon: the ground-truth interpolation that is the nearest to the generated noisy data at a given sampling timestep is observed to correspond to a higher-noise timestep (termed slowed timestep), i.e., the corresponding ground-truth timestep is slower than the sampling timestep. MixFlow leverages the interpolations at the slowed timesteps, named slowed interpolation mixture, for post-training the prediction network for each training timestep. Experiments over class-conditional image generation (including SiT, REPA, and RAE) and text-to-image generation validate the effectiveness of our approach. Our approach MixFlow over the RAE models achieve strong generation results on ImageNet: 1.43 FID (without guidance) and 1.10 (with guidance) at 256 x 256, and 1.55 FID (without guidance) and 1.10 (with guidance) at 512 x 512.

</details>


### [135] [Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations](https://arxiv.org/abs/2512.19316)
*Marica Muffoletto,Uxio Hermida,Charlène Mauger,Avan Suinesiaputra,Yiyang Xu,Richard Burns,Lisa Pankewitz,Andrew D McCulloch,Steffen E Petersen,Daniel Rueckert,Alistair A Young*

Main category: cs.CV

TL;DR: Introduces NIHCs, an implicit coordinate system predicting dense 3D heart anatomy from sparse 2D segmentations with ~2.3–2.5 mm error and faster inference (5–15s).


<details>
  <summary>Details</summary>
Motivation: To enable accurate, anatomically consistent patient-specific 3D cardiac reconstructions from limited clinical imaging data by leveraging a standardized implicit coordinate system based on universal ventricular coordinates.

Method: Neural Implicit Heart Coordinates (NIHCs): predict NIHCs from sparse 2D segmentations, decode to dense 3D segmentations and high-res meshes; trained on 5000 meshes; uses universal ventricular coordinates as basis.

Result: Mean Euclidean surface errors: 2.51±0.33 mm (diseased n=4549), 2.3±0.36 mm (healthy n=5576); robust to slice sparsity and segmentation noise; recovers valve planes; inference 5-15s vs >60s for traditional pipelines.

Conclusion: NIHCs provide a robust, efficient anatomical representation enabling accurate patient-specific 3D cardiac reconstruction from sparse clinical images, improving speed and anatomical coherence.

Abstract: Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.

</details>


### [136] [Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome](https://arxiv.org/abs/2512.19327)
*Moamal Fadhil Abdul,Jonas Bruun Hubrechts,Thomas Martini Jørgensen,Emil Hovad*

Main category: cs.CV

TL;DR: 扩展OpenTTGames，添加详细的击球类型（正反手及子类型）、玩家姿态与回合结果注释，提供编码与标注工具，促进公开、可复现的细粒度乒乓球视频分析研究。


<details>
  <summary>Details</summary>
Motivation: 自动检测与分类乒乓球视频中的击球可提升训练、转播和性能分析。现有公开数据不足以支持细粒度战术与击球类型分析，且许多资源受限或未公开，故需扩充带有详细注释的数据集。

Method: 在OpenTTGames上为原有事件（bounce、net、empty_event）增加每帧的击球类型标签与每位选手的姿态分类，并设计紧凑编码方案和代码辅助标注流程以保证可复现性与标注效率。

Result: 发布了含详尽注释的扩展数据集，支持从事件检测向战术理解过渡，并提供基线与工具，注释在CC BY-NC-SA 4.0许可下公开。

Conclusion: 本文扩展了公开的OpenTTGames数据集，增加了帧级精确的击球类型注释、运动员姿态标签和回合结果标记，旨在支持细粒度的击球理解与战术分析。作者提供了紧凑的编码方案和代码辅助标注流程，并在相同CC BY-NC-SA 4.0许可下发布注释。

Abstract: Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either "bounce", "net", or "empty_event" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.

</details>


### [137] [DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis](https://arxiv.org/abs/2512.19331)
*Yueting Zhu,Yuehao Song,Shuai Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 本文提出DeltaMIL，一种针对全切片图像的多实例学习框架，通过“delta”记忆机制与门控快速遗忘机制选择并整合判别区域，同时结合局部模式混合保留细粒度病理信息，在生存预测与病理切片分类上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: WSI规模大且内部异质性强，导致信息冗余与分散，现有MIL方法难以有效剔除无关信息或整合多补丁的判别特征，限制了在大规模异质WSI上的表现。

Method: 提出DeltaMIL：引入带有遗忘与记忆的块状‘delta规则’（动态更新记忆，按与当前补丁的相关性替换旧值），并辅以门控机制快速遗忘无关信号；同时加入局部模式混合模块以保留细粒度病理局部性。

Result: 实验显示在生存预测任务上，DeltaMIL较基线提升：ResNet-50特征下+3.69%，UNI特征下+2.36%；在切片分类任务上，ResNet-50下+3.09%，UNI下+3.75%，总体表现稳健优异。

Conclusion: DeltaMIL通过有选择的信息过滤与记忆更新机制，有效抑制冗余噪声并强化判别信号，提升了WSI多实例学习的鲁棒性与判别能力，适用于多种WSI任务。

Abstract: Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\% using ResNet-50 features and 2.36\% using UNI features. For slide-level classification, it increases accuracy by 3.09\% with ResNet-50 features and 3.75\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.

</details>


### [138] [GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis](https://arxiv.org/abs/2512.19336)
*Siyuan Mei,Yan Xia,Fuxin Fan*

Main category: cs.CV

TL;DR: 提出GANeXt，一种基于3D ConvNeXt的Patch-GAN用于从MRI/CBCT合成CT，采用U形生成器、条件PatchGAN判别器，多损失函数（MAE、感知损失、分割掩码MAE、对抗损失及分割损失），训练细节与数据预处理/增强和推理滑窗策略详尽。


<details>
  <summary>Details</summary>
Motivation: 在自适应放疗中需要从MRI或CBCT合成高质量CT以支持剂量计算与解剖表征；现有方法在跨模态与跨解剖区域的泛化性与细节保真方面仍有提升空间。

Method: 提出GANeXt：3D patch-based U形生成器由堆叠的3D ConvNeXt模块构成，判别器为条件PatchGAN；损失集合包括MAE、感知损失、基于分割的掩码MAE、对抗损失，及用于多头分割判别器的Dice+交叉熵。训练使用AdamW（两优化器）、warmup+余弦退火学习率，数据通过变形配准、前景裁剪、分位数归一化和线性CT归一化处理；增强含随机缩放、固定大小裁剪与翻转；推理用滑窗0.8重叠与平均折叠。

Result: 在所有解剖区域联合训练（无微调）后，分别在MRI→CT训练3000 epoch与CBCT→CT训练1000 epoch，得到最终模型（使用完整训练集）。文中宣称合成CT质量与细节有所提升并在跨区域任务中表现稳健（具体数值在摘要未给出）。

Conclusion: GANeXt通过结合ConvNeXt架构、条件PatchGAN与多样化损失与训练策略，实现了统一且泛化的CT合成方案，适用于多模态与多解剖区域的sCT生成。

Abstract: The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\times10^{-4}$ and $1\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\times160\times192$ for MRI-to-CT and $32\times128\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.

</details>


### [139] [ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining](https://arxiv.org/abs/2512.19354)
*Zhenyang Huang,Xiao Yu,Yi Zhang,Decheng Wang,Hang Ruan*

Main category: cs.CV

TL;DR: 提出ReasonCD：利用大语言模型挖掘用户隐式意图，用于遥感图像变化检测，BCDD上F1=92.1%，并能解释推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在遥感变化检测中依赖显式文本描述的变化区域（CRoI），面对隐式描述时性能会显著下降，因此需要能理解用户隐含意图的模型来生成相应的检测结果并提供解释。

Method: 构建多模态推理变化检测模型ReasonCD：用预训练大语言模型推理用户隐式意图，结合遥感影像特征进行变化检测，能够根据不同意图输出不同的检测结果并生成推理解释。

Result: 在公开数据集上取得优秀变化检测效果：BCDD数据集F1=92.1%；在SECOND子集的推理标注上展示了在推理型任务与可解释性方面的优势。

Conclusion: 本文提出的ReasonCD模型通过大语言模型推理用户隐式任务意图，以生成基于意图的变化检测结果，并在公开数据集上取得了优秀性能（BCDD上F1=92.1%）；此外对SECOND子集进行推理标注，展示了模型在推理型变化检测与可解释性上的能力。

Abstract: Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.

</details>


### [140] [Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization](https://arxiv.org/abs/2512.19365)
*Zhongwei Chen,Hai-Jun Rong,Zhao-Xu Yang,Guoqi Li*

Main category: cs.CV

TL;DR: 提出SpikeViMFormer，用轻量脉冲Transformer提取特征，设计脉冲选择性注意（SSA）补偿关键信息丢失，脉冲混合状态空间（SHS）学习长程依赖，并通过层次重排序对齐学习（HRAL）直接优化骨干，推理仅用骨干以降低计算，实验显示优于现有SNN并与高级ANN竞争。


<details>
  <summary>Details</summary>
Motivation: 现有基于ANN的无人机视角地理定位性能佳但计算密集、功耗高；SNN具低功耗潜力但在稀疏脉冲表示下易丢失关键信息且难以捕捉长程依赖，尚未在DVGL领域被深入研究。

Method: 设计轻量级脉冲驱动Transformer骨干；引入脉冲选择性注意（SSA）块，利用脉冲门控机制对特征进行选择性增强以突出判别性区域；引入脉冲混合状态空间（SHS）块，结合混合状态空间机制以捕捉长程依赖；训练阶段采用层次重排序对齐学习（HRAL），通过邻域重排序和跨批次一致性约束优化骨干；推理阶段仅保留骨干以节省计算。

Result: 在实验中，SpikeViMFormer在多个评估指标上优于现有SNN方法，并与若干先进ANN方法性能相当，同时具备更低的功耗和计算成本；代码已开源。

Conclusion: 本文提出了首个用于无人机视角地理定位的脉冲神经网络框架SpikeViMFormer，通过脉冲驱动的Transformer骨干、选择性注意力与混合状态空间模块，解决脉冲网络在特征稀疏与长程依赖学习上的不足，在保持低功耗的同时取得了优于先前SNN且与先进ANN可比的性能。

Abstract: Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer

</details>


### [141] [DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition](https://arxiv.org/abs/2512.19387)
*Yueyao Chen,Kai-Ni Wang,Dario Tayupo,Arnaud Huaulm'e,Krystel Nyangoh Timoh,Pierre Jannin,Qi Dou*

Main category: cs.CV

TL;DR: 提出DSTED：RMP保持历史一致性，UPR用原型强化难判样本，置信门融合，提升了准确率和F1并减少帧间抖动。


<details>
  <summary>Details</summary>
Motivation: 当前方法在连续帧预测时存在抖动且对模糊阶段区分差，需一个能选择性利用历史信息并显式建模不确定性的稳定框架。

Method: 构建双通路模型：RMP通过多准则评估筛选高信度历史特征并融合以维持时间连贯性；UPR从高不确定性样本学习可训练的类别原型并进行自适应匹配以增强模糊帧的表示；用基于置信度的门控在两路径间动态加权。

Result: 该论文提出了一个双通路框架（DSTED），通过可靠记忆传播（RMP）和不确定性感知原型检索（UPR）解决手术流程识别中的抖动和模糊阶段问题。RMP筛选并融合高置信度历史特征以维持时间一致性，UPR从高不确定性样本学习类特异性原型以提升模糊帧的判别，二者由置信门动态平衡。结果显示在AutoLaparo-hysterectomy数据集上实现了SOTA性能，显著降低时间抖动并改善阶段转换处的识别。

Conclusion: 双通路设计通过分离时间一致性建模与模糊阶段强化，显著提升了手术流程识别的稳定性与性能，具有潜在临床应用价值。

Abstract: Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.
  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.
  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.
  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.

</details>


### [142] [Non-Contrast CT Esophageal Varices Grading through Clinical Prior-Enhanced Multi-Organ Analysis](https://arxiv.org/abs/2512.19415)
*Xiaoming Zhang,Chunli Li,Jiacheng Hao,Yuan Gao,Danyang Tu,Jianyi Qiao,Xiaoli Yin,Le Lu,Ling Zhang,Ke Yan,Yang Hou,Yu Shi*

Main category: cs.CV

TL;DR: 本研究提出MOON++，基于非对比CT的多器官多模态学习框架，用食管、肝脏、脾脏体积与影像特征联合评估食管静脉曲张（EV）等级。基于1631例数据训练，239例验证、289例独立测试，性能优于单器官方法；G3 vs <G3 AUC 0.894 vs 0.803，>=G2 vs <G2 AUC 0.921 vs 0.793。与放射科医生读片对比研究进一步验证效果。


<details>
  <summary>Details</summary>
Motivation: 目前EV诊断主要依赖侵入性内镜，迫切需要非侵入性替代技术。临床已有证据表明器官体积关系（肝、脾、食管）与肝病严重度相关，因此利用NCCT的多器官信息可能提高EV评估准确性。

Method: 提出MOON++框架：基于NCCT进行食管、肝脏、脾脏的自动分割与特征提取，结合器官体积等临床先验，通过多模态融合网络进行联合学习和分级预测。使用1631例患者（内镜确诊EV并分为四级）进行训练，设置239例验证集与289例独立测试集；并设计与常规模型（单器官方法）和放射科医生的对比实验。

Result: MOON++在独立测试集上显著优于单器官方法：严重级别（G3 vs <G3）AUC 0.894 vs 0.803；中重度（>=G2 vs <G2）AUC 0.921 vs 0.793。读者研究显示MOON++在敏感性/特异性或诊断一致性上对比经验放射科医师有优势（具体指标未在摘要中详列）。

Conclusion: MOON++是首个将临床知识先验与多器官NCCT联合分析用于EV评估的框架，能作为一种有潜力的非侵入性辅助诊断工具，未来需在多中心和前瞻性研究中进一步验证与推广。

Abstract: Esophageal varices (EV) represent a critical complication of portal hypertension, affecting approximately 60% of cirrhosis patients with a significant bleeding risk of ~30%. While traditionally diagnosed through invasive endoscopy, non-contrast computed tomography (NCCT) presents a potential non-invasive alternative that has yet to be fully utilized in clinical practice. We present Multi-Organ-COhesion Network++ (MOON++), a novel multimodal framework that enhances EV assessment through comprehensive analysis of NCCT scans. Inspired by clinical evidence correlating organ volumetric relationships with liver disease severity, MOON++ synthesizes imaging characteristics of the esophagus, liver, and spleen through multimodal learning. We evaluated our approach using 1,631 patients, those with endoscopically confirmed EV were classified into four severity grades. Validation in 239 patient cases and independent testing in 289 cases demonstrate superior performance compared to conventional single organ methods, achieving an AUC of 0.894 versus 0.803 for the severe grade EV classification (G3 versus <G3) and 0.921 versus 0.793 for the differentiation of moderate to severe grades (>=G2 versus <G2). We conducted a reader study involving experienced radiologists to further validate the performance of MOON++. To our knowledge, MOON++ represents the first comprehensive multi-organ NCCT analysis framework incorporating clinical knowledge priors for EV assessment, potentially offering a promising non-invasive diagnostic alternative.

</details>


### [143] [dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://arxiv.org/abs/2512.19433)
*Yi Xin,Siqi Luo,Qi Qin,Haoxing Chen,Kaiwen Zhu,Zhiwei Zhang,Yangfan He,Rongchao Zhang,Jinbin Bai,Shuo Cao,Bin Fu,Junjun He,Yihao Liu,Yuewen Cao,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出dMLLM-TTS，一种针对扩散多模态大模型的测试时扩展框架，沿“轨迹探索”和“迭代精炼”两轴扩展生成能力；通过层次化搜索将复杂度从O(NT)降到O(N+T)，并引入模型自验证的文本-图像对齐评估，消除外部验证器。实验证明在GenEval上对三种dMLLM显著提升生成质量并最高6倍效率提升。


<details>
  <summary>Details</summary>
Motivation: 当前dMLLM在生成潜力上受限，常用的测试时扩展（TTS）通过在轨迹数N和迭代步数T上线性搜索来提升质量，但代价高（O(NT)）且需外部验证器进行best-of-N选择。需要更高效且自洽的TTS方法。

Method: 提出dMLLM-TTS：两条互补扩展轴——轨迹探索（提高假设多样性）与迭代精炼（稳定生成）。设计了（1）层次化搜索算法以自适应展开与剪枝采样轨迹，使复杂度降为O(N+T)；（2）自验证反馈机制，利用dMLLM的图像理解能力评估文本-图像对齐，无需外部验证器。

Result: 在GenEval基准上，针对Lumina-DiMOO、MMaDA、Muddit等三种代表性dMLLM进行大量实验，结果显示dMLLM-TTS在生成质量上有明显提升，同时在效率上相比传统线性搜索最高可达6倍加速。

Conclusion: dMLLM-TTS通过层次化搜索与自验证机制，实现在不依赖外部验证器的前提下高效提升扩散多模态LLM的生成质量，为TTS设计提供了新的思路与实用工具。

Abstract: Diffusion Multi-modal Large Language Models (dMLLMs) have recently emerged as a novel architecture unifying image generation and understanding. However, developing effective and efficient Test-Time Scaling (TTS) methods to unlock their full generative potential remains an underexplored challenge. To address this, we propose dMLLM-TTS, a novel framework operating on two complementary scaling axes: (1) trajectory exploration scaling to enhance the diversity of generated hypotheses, and (2) iterative refinement scaling for stable generation. Conventional TTS approaches typically perform linear search across these two dimensions, incurring substantial computational costs of O(NT) and requiring an external verifier for best-of-N selection. To overcome these limitations, we propose two innovations. First, we design an efficient hierarchical search algorithm with O(N+T) complexity that adaptively expands and prunes sampling trajectories. Second, we introduce a self-verified feedback mechanism that leverages the dMLLMs' intrinsic image understanding capabilities to assess text-image alignment, eliminating the need for external verifier. Extensive experiments on the GenEval benchmark across three representative dMLLMs (e.g., Lumina-DiMOO, MMaDA, Muddit) show that our framework substantially improves generation quality while achieving up to 6x greater efficiency than linear search. Project page: https://github.com/Alpha-VLLM/Lumina-DiMOO.

</details>


### [144] [MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation](https://arxiv.org/abs/2512.19438)
*Fei Ge,Ying Huang,Jie Liu,Guixuan Zhang,Zhi Zeng,Shuwu Zhang,Hu Guan*

Main category: cs.CV

TL;DR: 将嵌入与提取重构为显式协作模块，配合AFMM进行内容感知的特征调制，实现互教训练与闭环优化，从而提升提取准确率和鲁棒性，同时保持高视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法嵌入器与提取器仅通过最终损失弱耦合，缺乏结构化协作机制，无法在训练中相互引导，从而限制鲁棒性与泛化。

Method: 引入CIM建立嵌入器与提取器之间的直接双向通信，采用互教师训练范式。设计AFMM解耦调制结构与强度，实现内容敏感的特征调节；在CIM下双侧AFMM形成闭环，协调嵌入行为与提取目标，改变鲁棒性学习途径，从单纯畸变仿真转向协同表示学习。

Result: 提出一种协作式深度图像水印框架，包含协作交互机制(CIM)和自适应特征调制模块(AFMM)，实现嵌入器与提取器之间的双向通信与互教训练，从而提升鲁棒性与提取准确率，同时保持高感知质量。

Conclusion: 通过显式协作与闭环特征调制，系统在真实与AI生成数据集上优于现有方法，在提取准确率、鲁棒性与泛化能力上有明显提升，且不依赖大量畸变仿真。

Abstract: Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.

</details>


### [145] [D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning](https://arxiv.org/abs/2512.19443)
*Evelyn Zhang,Fufu Yu,Aoqi Wu,Zichen Wen,Ke Yan,Shouhong Ding,Biqing Qi,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出D2Pruner，一种结合去偏重要性评分与结构化剪枝（基于最大独立集）的视觉token剪枝框架，解决位置偏差与结构盲点问题，在保持性能下大幅降低计算。


<details>
  <summary>Details</summary>
Motivation: 现有视觉token剪枝要么基于重要性但受位置偏差影响、要么基于多样性但忽略提示与空间冗余，导致在精细定位任务上失败；因此需要同时兼顾去偏的重要性与结构化多样性以保持定位能力与理解能力。

Method: 先用去偏的注意力分数选择一组核心“枢纽”token；对剩余token构建混合图（边表示空间接近性与语义相似性），并在其上执行最大独立集（MIS）选择：迭代保留最重要且可用的token并移除其邻居，从而在重要性和多样性间取得平衡。

Result: 在LLaVA-1.5-7B上的通用理解任务中，FLOPs下降74.2%，性能保留99.2%；在InternVL-2.5-8B的定位基准上，90% token减少时保持85.7%性能，较现有方法最高提升63.53%。

Conclusion: D2Pruner通过去偏重要性与结构化最大独立集选择，有效解决位置偏差与结构盲点，显著提升视觉token剪枝在精细定位与通用理解任务上的效率与可靠性。

Abstract: Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\% while retaining 99.2\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\% performance at a 90\% token reduction rate, marking a significant advancement with up to 63. 53\% improvement over existing methods.

</details>


### [146] [Sign Language Recognition using Parallel Bidirectional Reservoir Computing](https://arxiv.org/abs/2512.19451)
*Nitin Kumar Singh,Arie Rachmad Syulistyo,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CV

TL;DR: 通过MediaPipe提取手部关键点并用并行双向ESN的水库计算架构进行分类，实现了在WLASL数据集上高效且快速的轻量级手语识别，适合部署于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 介绍一种适用于边缘设备的轻量级手语识别系统，解决深度学习模型在资源受限环境中计算开销大、训练时间长的问题。

Method: 结合MediaPipe进行手部关键点跟踪与坐标提取，并使用并行双向回声态网络（PBRC）作为时序建模结构。PBRC由两个基于ESN的双向水库计算模块并行构成，用以捕捉时间依赖并生成分类用特征。

Result: 在WLASL数据集上取得top-1 60.85%、top-5 85.86%、top-10 91.74%的识别准确率；训练时间显著缩短到18.67秒，相较Bi-GRU等深度学习方法（>55分钟）大幅降低。

Conclusion: 该方法在准确率与计算效率间取得平衡，适合在边缘设备上实现实时、低成本的手语识别。

Abstract: Sign language recognition (SLR) facilitates communication between deaf and hearing communities. Deep learning based SLR models are commonly used but require extensive computational resources, making them unsuitable for deployment on edge devices. To address these limitations, we propose a lightweight SLR system that combines parallel bidirectional reservoir computing (PBRC) with MediaPipe. MediaPipe enables real-time hand tracking and precise extraction of hand joint coordinates, which serve as input features for the PBRC architecture. The proposed PBRC architecture consists of two echo state network (ESN) based bidirectional reservoir computing (BRC) modules arranged in parallel to capture temporal dependencies, thereby creating a rich feature representation for classification. We trained our PBRC-based SLR system on the Word-Level American Sign Language (WLASL) video dataset, achieving top-1, top-5, and top-10 accuracies of 60.85%, 85.86%, and 91.74%, respectively. Training time was significantly reduced to 18.67 seconds due to the intrinsic properties of reservoir computing, compared to over 55 minutes for deep learning based methods such as Bi-GRU. This approach offers a lightweight, cost-effective solution for real-time SLR on edge devices.

</details>


### [147] [Emotion-Director: Bridging Affective Shortcut in Emotion-Oriented Image Generation](https://arxiv.org/abs/2512.19479)
*Guoli Jia,Junyao Hu,Xinwei Long,Kai Tian,Kaiyan Zhang,KaiKai Zhao,Ning Ding,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出Emotion-Director：MC-Diffusion结合视觉/文本提示并用负视觉提示改进DPO，MC-Agent用多智能体链式概念改写文本以表达目标情感，实验表明效果优越。


<details>
  <summary>Details</summary>
Motivation: 当前情感导向图像生成方法将情感近似为语义，忽视情感与语义的差异，导致模型难以在相同语义下表达不同情感。作者旨在突破这一“情感捷径”。

Method: 两个模块：1) MC-Diffusion：在扩散模型中融合视觉提示与文本提示，引入负视觉提示改进DPO优化以增强情感敏感性；2) MC-Agent：多智能体系统重写文本提示以体现目标情感，采用链式概念工作流提升视觉表达力。

Result: Emotion-Director提出一种跨模态协作框架用于情感导向图像生成。

Conclusion: 该方法通过视觉提示与文本提示融合（MC-Diffusion）以及多智能体文本改写（MC-Agent），有效减轻了将情感简化为语义的偏差，提高了在相同语义下生成不同情感图像的能力。

Abstract: Image generation based on diffusion models has demonstrated impressive capability, motivating exploration into diverse and specialized applications. Owing to the importance of emotion in advertising, emotion-oriented image generation has attracted increasing attention. However, current emotion-oriented methods suffer from an affective shortcut, where emotions are approximated to semantics. As evidenced by two decades of research, emotion is not equivalent to semantics. To this end, we propose Emotion-Director, a cross-modal collaboration framework consisting of two modules. First, we propose a cross-Modal Collaborative diffusion model, abbreviated as MC-Diffusion. MC-Diffusion integrates visual prompts with textual prompts for guidance, enabling the generation of emotion-oriented images beyond semantics. Further, we improve the DPO optimization by a negative visual prompt, enhancing the model's sensitivity to different emotions under the same semantics. Second, we propose MC-Agent, a cross-Modal Collaborative Agent system that rewrites textual prompts to express the intended emotions. To avoid template-like rewrites, MC-Agent employs multi-agents to simulate human subjectivity toward emotions, and adopts a chain-of-concept workflow that improves the visual expressiveness of the rewritten prompts. Extensive qualitative and quantitative experiments demonstrate the superiority of Emotion-Director in emotion-oriented image generation.

</details>


### [148] [Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration](https://arxiv.org/abs/2512.19486)
*Shaochen Bi,Yuting He,Weiming Wang,Hao Chen*

Main category: cs.CV

TL;DR: 提出DySNet，含AdSB和DySA，通过动态调整感受野与权重抑制组合爆炸，提升DMIR效果。


<details>
  <summary>Details</summary>
Motivation: DMIR faces combinatorial explosion due to dual-input feature combinations causing interference during feature modeling.

Method: Propose DySNet with Adaptive Stream Basin (AdSB) to dynamically adjust receptive field shape and Dynamic Stream Attention (DySA) to generate dynamic weights for selecting valuable feature relationships.

Result: DySNet outperforms state-of-the-art DMIR methods in extensive experiments and shows strong generalization.

Conclusion: Dynamic adjustment of receptive fields and weights effectively reduces interfering feature combinations and improves DMIR performance; code to be released on GitHub.

Abstract: Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.

</details>


### [149] [FusionNet: Physics-Aware Representation Learning for Multi-Spectral and Thermal Data via Trainable Signal-Processing Priors](https://arxiv.org/abs/2512.19504)
*Georgios Voulgaris*

Main category: cs.CV

TL;DR: Introduce FusionNet that fuses SWIR ratio and TIR with physics-aware priors; achieves state-of-the-art accuracy (90.6%) and better transfer across spectra; ImageNet pretraining can harm TIR performance.


<details>
  <summary>Details</summary>
Motivation: Standard deep models rely on inductive biases misaligned with physical signal formation, failing under cross-spectral/real-world conditions; need to model stable signatures from long-term physical processes (e.g., soil changes due to heat).

Method: Compute a geological SWIR ratio sensitive to soil properties; intermediate fusion of SWIR ratio with TIR in FusionNet backbone that includes trainable differential filters inside conv layers, mixed pooling, and larger receptive fields. Perform ablations, compare to baselines (including DGCNN), and test transfer learning effects.

Result: The paper proposes a physics-aware multi-spectral representation learning framework combining SWIR ratio and TIR via FusionNet, embedding differential signal-processing priors, mixed pooling, wider receptive fields, and showing improved robustness.

Conclusion: Integrating physics-derived features (SWIR ratio) with modality-aware architectures (FusionNet) yields more robust, generalizable multi-spectral representations than standard pretrained or single-modality approaches.

Abstract: Modern deep learning models operating on multi-modal visual signals often rely on inductive biases that are poorly aligned with the physical processes governing signal formation, leading to brittle performance under cross-spectral and real-world conditions. In particular, approaches that prioritise direct thermal cues struggle to capture indirect yet persistent environmental alterations induced by sustained heat emissions.
  This work introduces a physics-aware representation learning framework that leverages multi-spectral information to model stable signatures of long-term physical processes. Specifically, a geological Short Wave Infrared (SWIR) ratio sensitive to soil property changes is integrated with Thermal Infrared (TIR) data through an intermediate fusion architecture, instantiated as FusionNet. The proposed backbone embeds trainable differential signal-processing priors within convolutional layers, combines mixed pooling strategies, and employs wider receptive fields to enhance robustness across spectral modalities.
  Systematic ablations show that each architectural component contributes to performance gains, with DGCNN achieving 88.7% accuracy on the SWIR ratio and FusionNet reaching 90.6%, outperforming state-of-the-art baselines across five spectral configurations. Transfer learning experiments further show that ImageNet pretraining degrades TIR performance, highlighting the importance of modality-aware training for cross-spectral learning.
  Evaluated on real-world data, the results demonstrate that combining physics-aware feature selection with principled deep learning architectures yields robust and generalisable representations, illustrating how first-principles signal modelling can improve multi-spectral learning under challenging conditions.

</details>


### [150] [Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://arxiv.org/abs/2512.19512)
*Ziyang Song,Zelin Zang,Zuyao Chen,Xusheng Liang,Dong Yi,Jinlin Wu,Hongbin Liu,Jiebo Luo*

Main category: cs.CV

TL;DR: 针对GRPO在解剖识别上知识共享不足和策略单一的问题，本文提出通过按答案相似度划分难度的课程学习和通过问题扩增增加搜索多样性，从而提升医疗多模态大模型的解剖推理能力，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 医学影像尤其是临床外科解剖图像推理要求高精度与临床一致性，但缺乏高质量标注和数据规模，传统SFT效果有限。GRPO虽可在少量数据下提升推理，但在解剖识别上存在知识共享和策略多样性不足的问题，影响收敛与性能。

Method: 1) Anatomical Similarity Curriculum Learning：通过基于答案选项相似度控制问题难度，按难度逐步训练，使模型先掌握相对简单的区分再学习更细致的解剖差异，从而促进知识迁移与稳定收敛。2) Group Diversity Question Augmentation：对困难问题进行组内问题扩增，扩大模型在难题上的搜索空间，防止快速陷入单一路径，增强回应多样性和探索能力。两方法结合于GRPO训练流程中。

Result: 在SGG-VQA与OmniMedVQA两个基准上进行综合实验，方法在多个评估指标上显著优于基线，显示在解剖结构识别和问答准确性上均有提升，代码已开源。

Conclusion: 该论文提出在GRPO基础上引入两种方法以提升解剖结构识别的推理能力：基于答案相似度的渐进学习（Anatomical Similarity Curriculum Learning）和基于问题扩增的组多样性增强（Group Diversity Question Augmentation）。实验在SGG-VQA和OmniMedVQA上取得显著提升，证明方法有效。

Abstract: Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1

</details>


### [151] [A Convolutional Neural Deferred Shader for Physics Based Rendering](https://arxiv.org/abs/2512.19522)
*Zhuo He,Yingdong Ru,Qianying Liu,Paul Henderson,Nicolas Pugeault*

Main category: cs.CV

TL;DR: pbnds+: CNN-based neural deferred shading with energy regularization for efficient, robust photorealistic relighting, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: MLP-based neural renderers are heavy (many parameters), require lots of data, and can be biased by unbalanced illumination; need a lighter, more robust model for real-world relighting without material ground truth.

Method: Replace MLPs with convolutional neural networks in a physics-based deferred shading pipeline; introduce energy regularization to penalize erroneous reflections under dark illumination; train on real-world datasets and compare to classical baselines, a SOTA neural shading model, and a diffusion-based method.

Result: This paper proposes pbnds+, a physics-based neural deferred shading pipeline using convolutional neural networks to reduce parameters and improve shading/relighting performance; introduces energy regularization to handle dark illumination; demonstrates better results than baselines.

Conclusion: pbnds+ achieves more efficient and robust neural shading/relighting with fewer parameters and improved handling of unbalanced/dark lighting, outperforming classical and recent neural methods.

Abstract: Recent advances in neural rendering have achieved impressive results on photorealistic shading and relighting, by using a multilayer perceptron (MLP) as a regression model to learn the rendering equation from a real-world dataset. Such methods show promise for photorealistically relighting real-world objects, which is difficult to classical rendering, as there is no easy-obtained material ground truth. However, significant challenges still remain the dense connections in MLPs result in a large number of parameters, which requires high computation resources, complicating the training, and reducing performance during rendering. Data driven approaches require large amounts of training data for generalization; unbalanced data might bias the model to ignore the unusual illumination conditions, e.g. dark scenes. This paper introduces pbnds+: a novel physics-based neural deferred shading pipeline utilizing convolution neural networks to decrease the parameters and improve the performance in shading and relighting tasks; Energy regularization is also proposed to restrict the model reflection during dark illumination. Extensive experiments demonstrate that our approach outperforms classical baselines, a state-of-the-art neural shading model, and a diffusion-based method.

</details>


### [152] [Multi-Modal Soccer Scene Analysis with Masked Pre-Training](https://arxiv.org/abs/2512.19528)
*Marc Peral,Guillem Capellera,Luis Ferraz,Antonio Rubio,Antonio Agudo*

Main category: cs.CV

TL;DR: 提出一个融合结构化轨迹与视觉裁剪的多模态变换器，并用CropDrop预训练避免对图像过度依赖，实现无需球追踪的球轨迹推断及稳健的球状态和持球人识别，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实比赛中球经常遮挡、轨迹难以可靠追踪，且仅依靠手工规则或精确球追踪不够稳健或泛化性差。作者希望构建一个能在缺失或噪声球位置信息下，利用球员行为和视觉线索推断球状态与持球关系的系统。

Method: 提出一个统一多模态架构，输入为球员轨迹序列、球员类型（类别/角色）和每名球员的图像裁剪；使用级联的社会-时序（sociotemporal）Transformer块捕捉个体间交互与时间依赖；设计CropDrop——对图像裁剪进行模态特定掩码预训练，强制模型学会跨模态依赖而非仅依赖视觉；输出为球轨迹估计、球状态分类（例如脚下、空中、失控等）及持球人识别。

Result: 在大规模真实顶级联赛数据集上，所提方法在三项任务上均显著优于最先进基线，证明结合结构化轨迹与视觉特征的变换器以及现实感掩码策略能提升鲁棒性与性能。

Conclusion: 该论文提出了一个多模态变换器框架用于从战术摄像头视频中分析足球场景，主要解决球轨迹推断、球状态分类和球持球人识别三项任务。通过融合球员轨迹、球员类别以及球员图像裁剪等三种输入模态，并采用社会-时序变换器块处理时空动态，同时引入CropDrop掩码预训练策略，模型在不直接观察球位置信息的情况下仍能鲁棒推断并在真实联赛数据上超过现有基线。

Abstract: In this work we propose a multi-modal architecture for analyzing soccer scenes from tactical camera footage, with a focus on three core tasks: ball trajectory inference, ball state classification, and ball possessor identification. To this end, our solution integrates three distinct input modalities (player trajectories, player types and image crops of individual players) into a unified framework that processes spatial and temporal dynamics using a cascade of sociotemporal transformer blocks. Unlike prior methods, which rely heavily on accurate ball tracking or handcrafted heuristics, our approach infers the ball trajectory without direct access to its past or future positions, and robustly identifies the ball state and ball possessor under noisy or occluded conditions from real top league matches. We also introduce CropDrop, a modality-specific masking pre-training strategy that prevents over-reliance on image features and encourages the model to rely on cross-modal patterns during pre-training. We show the effectiveness of our approach on a large-scale dataset providing substantial improvements over state-of-the-art baselines in all tasks. Our results highlight the benefits of combining structured and visual cues in a transformer-based architecture, and the importance of realistic masking strategies in multi-modal learning.

</details>


### [153] [SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates](https://arxiv.org/abs/2512.19534)
*Chi Zhang,Braedon Gunn,Andrew M. Read-Fuller*

Main category: cs.CV

TL;DR: 开源3D Slicer扩展：虚拟配准与评估预制眶板，提供可重复的板-眶距离量化指标与可视化，支持病人特异与群体分析，改善术前决策并促进研究教学。


<details>
  <summary>Details</summary>
Motivation: 缺乏公开工具与标准化量化指标来比较不同厂商/尺寸/患者解剖下的预制眶板贴合性，导致术后并发症与返修手术。

Method: 在3D Slicer上开发扩展，实现交互式虚拟配准、多板比较、生成板到眶的距离度量与可视化，并提供试点研究、示例数据与教程以支持可重复性。

Result: Developed SlicerOrbitSurgerySim, an open-source 3D Slicer extension for virtual registration, evaluation, and comparison of preformed orbital plates.

Conclusion: The tool enables reproducible quantitative metrics and visualization for patient-specific and population-level analysis, aiming to improve preoperative planning, reduce intraoperative modification, and support research and education.

Abstract: Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.

</details>


### [154] [CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion](https://arxiv.org/abs/2512.19535)
*Moritz Böhle,Amélie Royer,Juliette Marrie,Edouard Grave,Patrick Pérez*

Main category: cs.CV

TL;DR: CASA：在跨注意力层加入自注意力以恢复文本之间的局部交互，兼顾效果与可扩展性，提升细粒度视觉理解任务表现并适用于长上下文场景。


<details>
  <summary>Details</summary>
Motivation: 传统将图像 token 插入语言模型的做法在融合上效果好但计算、内存代价高，对于高分辨率图像、长会话或流视频不适用；而仅用跨注意力的方法效率高但在细粒度视觉任务上性能较差，作者因此寻求在保持效率的同时缩小性能差距。

Method: 提出 CASA（Cross-Attention via Self-Attention）：在跨注意力层中额外引入局部的自注意力机制以实现文本-文本交互，使文本能够在跨注意力步骤中相互影响，从而提高对细节的捕捉能力，同时保持与传统跨注意力模型相当的计算与内存开销。

Result: CASA 提出了一种在跨注意力层中通过自注意力实现文本-文本交互的方法，以提高跨注意力 VLM 在细粒度视觉任务上的性能，同时保持高效可扩展性。

Conclusion: 通过在跨注意力模块中引入局部的文本-文本交互（即将自注意力机制整合到跨注意力路径），CASA 在常见图像理解基准上显著缩小了与全 token 插入方法的性能差距，并在长上下文多模态任务（如流式视频描述）中保持与传统跨注意力方法相同的可扩展性和效率。

Abstract: Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .

</details>


### [155] [StoryMem: Multi-shot Long Video Storytelling with Memory](https://arxiv.org/abs/2512.19539)
*Kaiwen Zhang,Liming Jiang,Angtian Wang,Jacob Zhiyuan Fang,Tiancheng Zhi,Qing Yan,Hao Kang,Xin Lu,Xingang Pan*

Main category: cs.CV

TL;DR: 提出StoryMem：用动态关键帧记忆库和Memory-to-Video注入策略，把单镜头视频扩散模型改为多镜头叙事；配合语义关键帧选择与美学过滤，并构建ST-Bench基准，提升跨镜头一致性与美观度。


<details>
  <summary>Details</summary>
Motivation: 长篇视频叙事要求镜头间的风格与语义一致性，现有单镜头生成方法难以保持长程一致性。受人类记忆启发，作者希望用显式可更新的视觉记忆来维护历史信息，指导后续镜头生成。

Method: 设计Memory-to-Video (M2V)：维护紧凑的关键帧记忆库，使用潜在拼接(latent concatenation)与负RoPE位移(negative RoPE shifts)将记忆注入单镜头扩散模型；仅对LoRA参数进行微调。引入语义关键帧选择和美学偏好过滤，保证记忆信息有效且稳定；框架支持平滑镜头过渡与定制化生成。

Result: 在作者构建的ST-Bench上，StoryMem在跨镜头一致性上优于既有方法，同时保持高美学质量与对提示语的遵从，能生成连贯的分钟级视频叙事。

Conclusion: StoryMem通过引入显式视觉记忆并将其注入单镜头视频扩散模型，实现了迭代镜头合成，从而把预训练的单镜头模型扩展为多镜头讲故事系统。

Abstract: Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.

</details>


### [156] [ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars](https://arxiv.org/abs/2512.19546)
*Ziqiao Peng,Yi Chen,Yifeng Ma,Guozhen Zhang,Zhiyao Sun,Zixiang Zhou,Youliang Zhang,Zhengguang Zhou,Zhaoxin Fan,Hongyan Liu,Yuan Zhou,Qinglin Lu,Jun He*

Main category: cs.CV

TL;DR: 提出ActAvatar，通过PACA、分层音视对齐和两阶段训练，实现基于文本的精确时段级动作控制，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Existing talking avatar methods lack precise text-following for diverse actions, temporal alignment with audio, and rely on extra control signals like pose skeletons; need a framework to provide phase-level action control using text while preserving audio-visual coherence.

Method: Introduce ActAvatar with: (1) Phase-Aware Cross-Attention (PACA) splitting prompts into a global base block plus temporally-anchored phase blocks to focus on phase-relevant tokens; (2) Progressive Audio-Visual Alignment where early layers favor text for action structure and deeper layers favor audio for lip refinement; (3) Two-stage training: first learn audio-visual correspondence on diverse data, then fine-tune with structured action annotations to inject action control.

Result: Extensive experiments show ActAvatar outperforms state-of-the-art in action control accuracy and visual quality, demonstrating better text-following, temporal-semantic alignment, and preserved audio-visual synchronization.

Conclusion: ActAvatar enables precise, phase-level textual control of avatar actions without extra control signals by combining PACA, progressive modality alignment, and staged training, improving both control and visual quality.

Abstract: Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.

</details>


### [157] [BabyFlow: 3D modeling of realistic and expressive infant faces](https://arxiv.org/abs/2512.19560)
*Antonia Alomar,Mireia Masias,Marius George Linguraru,Federico M. Sukno,Gemma Piella*

Main category: cs.CV

TL;DR: BabyFlow uses normalizing flows plus adult-to-infant expression transfer to learn disentangled, probabilistic infant face models, improving 3D reconstruction (mouth/eyes/nose), enabling expression control and generating consistent 2D images for augmentation


<details>
  <summary>Details</summary>
Motivation: Early detection of developmental disorders requires accurate infant facial morphology analysis, but limited and uncontrolled infant expression data make modeling difficult; BabyFlow aims to overcome data scarcity and provide disentangled, expressive, controllable infant face models

Method: Generative AI model (BabyFlow) using normalizing flows and cross-age expression transfer

Result: Improved 3D reconstruction accuracy in expressive regions; enables synthesis/modification of infant expressions; generates 2D images with consistent 3D geometry via diffusion integration

Conclusion: BabyFlow provides a flexible probabilistic representation disentangling identity and expression, addresses scarce infant expressive data via cross-age transfer, and yields better reconstruction and data augmentation capabilities

Abstract: Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.

</details>


### [158] [No Data? No Problem: Robust Vision-Tabular Learning with Missing Values](https://arxiv.org/abs/2512.19602)
*Marta Hasny,Laura Daza,Keno Bressem,Maxime Di Folco,Julia Schnabel*

Main category: cs.CV

TL;DR: 提出RoVTL：在对比预训练中模拟表格缺失、下游用门控交叉注意力融合并引入Tabular More vs. Fewer损失与解耦梯度学习，实现对任意比例表格缺失的稳健多模态学习，在医学影像与自然图像上均优于基线。


<details>
  <summary>Details</summary>
Motivation: 大规模生物库含丰富影像与表格信息，但实际应用中常只有部分表格属性可用，需方法在训练时利用全部信息、推理时应对缺失，提高模型泛化与实用性。

Method: RoVTL包含两阶段：1) 对比预训练：在构建正负样本与表示学习时有意随机掩蔽表格属性作为增强，提升对缺失的鲁棒性；2) 下游微调：用门控交叉注意力模块进行视觉与表格特征融合；并引入Tabular More vs. Fewer排序损失，按表格可用量对性能进行排序训练，结合解耦梯度学习以避免冲突梯度，保证不同缺失级别下的一致表现。

Result: 在UK Biobank心脏MRI上，RoVTL在不同表格缺失比例下均表现优异，较先前方法更稳健；在外部心脏MRI数据集进行多模态疾病分类任务上表现良好，显示出迁移能力；在汽车广告自然图像数据集上也验证了对表格缺失的稳健性。

Conclusion: 本文提出RoVTL框架，旨在在训练时利用完整表格数据、推理时面对任意缺失比例的表格属性仍保持稳健。通过在对比预训练阶段引入表格属性缺失作为数据增强，以及在下游微调阶段使用门控交叉注意力进行多模态融合，并设计Tabular More vs. Fewer损失结合解耦梯度学习，模型在UK Biobank心脏MRI及外部数据集和自然图像数据集上展示出对表格缺失的鲁棒性，优于已有方法。

Abstract: Large-scale medical biobanks provide imaging data complemented by extensive tabular information, such as demographics or clinical measurements. However, this abundance of tabular attributes does not reflect real-world datasets, where only a subset of attributes may be available. This discrepancy calls for methods that can leverage all the tabular data during training while remaining robust to missing values at inference. To address this challenge, we propose RoVTL (Robust Vision-Tabular Learning), a framework designed to handle any level of tabular data availability, from 0% to 100%. RoVTL comprises two key stages: contrastive pretraining, where we introduce tabular attribute missingness as data augmentation to promote robustness, and downstream task tuning using a gated cross-attention module for multimodal fusion. During fine-tuning, we employ a novel Tabular More vs. Fewer loss that ranks performance based on the amount of available tabular data. Combined with disentangled gradient learning, this enables consistent performance across all tabular data completeness scenarios. We evaluate RoVTL on cardiac MRI scans from the UK Biobank, demonstrating superior robustness to missing tabular data compared to prior methods. Furthermore, RoVTL successfully generalizes to an external cardiac MRI dataset for multimodal disease classification, and extends to the natural images domain, achieving robust performance on a car advertisements dataset. The code is available at https://github.com/marteczkah/RoVTL.

</details>


### [159] [MapTrace: Scalable Data Generation for Route Tracing on Maps](https://arxiv.org/abs/2512.19609)
*Artemis Panagopoulou,Aveek Purohit,Achin Kulshrestha,Soroosh Yazdani,Mohit Goyal*

Main category: cs.CV

TL;DR: 通过合成地图与像素级标注生成23k路径数据，对多模态大模型微调后，在MapBench上显著提升路径追踪性能，证明细粒度空间推理可通过合成监督学习得。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型在视觉与文本推理上表现优异，但在细粒度空间理解（如在地图上追踪路径）存在不足，部分原因是获取像素级路径标注成本高且困难。作者旨在通过合成数据解决标注缺乏问题，从而提高模型的空间推理能力。

Method: 提出一个可扩展的合成数据生成管线，利用合成地图图像与像素级解析自动生成精确标注；基于该管线构建包含23k路径样本、覆盖4k地图的微调数据集，并对开源与专有多模态大模型进行微调。

Result: 在MapBench基准上，微调显著提升了模型的鲁棒性，成功率最多提高6.4个百分点，并降低了路径追踪误差（如NDTW指标），表明模型能学到更接近人类的空间能力。

Conclusion: 细粒度空间推理能力并非先验预训练模型固有，通过合成像素级监督可以显式教会模型这些技能；合成数据管线为大规模标注提供了可行路径。

Abstract: While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.

</details>


### [160] [Generative diffusion models for agricultural AI: plant image generation, indoor-to-outdoor translation, and expert preference alignment](https://arxiv.org/abs/2512.19632)
*Da Tan,Michael Beck,Christopher P. Bidinosti,Robert H. Gulden,Christopher J. Henry*

Main category: cs.CV

TL;DR: 通过微调Stable Diffusion、室内-室外图像翻译及专家偏好微调，论文展示了用合成植物图像扩充数据集并提升检测/分类任务性能的可行路径。


<details>
  <summary>Details</summary>
Motivation: 解决农业AI中真实田间作物图像采集成本高、劳动力需求大、季节限制严重的问题，通过生成模型合成高质量植物图像以扩充数据集。

Method: 1) 对Stable Diffusion进行微调，使用带描述的室内与室外作物图像生成大豆与油菜的文本条件图像；2) 使用DreamBooth文本倒置和图像引导扩散实现室内到室外的翻译，生成可用于YOLOv8的杂草检测与分类训练数据；3) 提出基于专家偏好的微调框架，训练奖励模型并进行奖励加权更新以使生成结果与专家偏好一致。

Result: 合成图像在Inception Score、Frechet Inception Distance及表型分类下表现良好；作为增强数据能提升分类准确率；翻译图像改善了YOLOv8的杂草检测与分类性能；偏好引导微调提高了生成稳定性和与专家偏好的一致性。

Conclusion: 提出的扩散模型微调与偏好对齐流程，为农业AI提供了一个高效、实用的合成数据管线，能够减少对昂贵田间数据的依赖并提升下游任务性能。

Abstract: The success of agricultural artificial intelligence depends heavily on large, diverse, and high-quality plant image datasets, yet collecting such data in real field conditions is costly, labor intensive, and seasonally constrained. This paper investigates diffusion-based generative modeling to address these challenges through plant image synthesis, indoor-to-outdoor translation, and expert preference aligned fine tuning. First, a Stable Diffusion model is fine tuned on captioned indoor and outdoor plant imagery to generate realistic, text conditioned images of canola and soybean. Evaluation using Inception Score, Frechet Inception Distance, and downstream phenotype classification shows that synthetic images effectively augment training data and improve accuracy. Second, we bridge the gap between high resolution indoor datasets and limited outdoor imagery using DreamBooth-based text inversion and image guided diffusion, generating translated images that enhance weed detection and classification with YOLOv8. Finally, a preference guided fine tuning framework trains a reward model on expert scores and applies reward weighted updates to produce more stable and expert aligned outputs. Together, these components demonstrate a practical pathway toward data efficient generative pipelines for agricultural AI.

</details>


### [161] [4D Gaussian Splatting as a Learned Dynamical System](https://arxiv.org/abs/2512.19648)
*Arnold Caleb Asiimwe,Carl Vondrick*

Main category: cs.CV

TL;DR: 本文提出EvoGS：将高斯splat表示视为受学习动力学法则驱动的连续演化系统，带来样本效率高、可外推时间、局部可控等优势，实验显示在动态场景基准上优于变形场方法，同时维持实时渲染。


<details>
  <summary>Details</summary>
Motivation: 现有4D Gaussian Splatting多以逐帧或基于变形场的方法处理场景运动，导致对稀疏时序数据学习效率差、难以外推时间、以及对局部可控性不足。将表示改为连续时间动力学系统可直接学习运动法则，改善这些局限性。

Method: 提出EvoGS：把高斯splat集合的狀态视为连续时间状态向量，由一个神经网络表示的动力学场（状态的时间导数）驱动，通过数值积分得到任意时间点的高斯参数（位置、形状、颜色等）。训练时对稀疏时间观测做监督，动力学场参数化支持局部控制（注入局部动力学模块），并可用于前向/后向时间积分实现外推。与变形场方法相比无需每帧学习变形场，而是学习通用运动法则。

Result: 在多个动态场景基准上，EvoGS在运动连贯性和时间一致性指标上优于基线变形场方法，同时在稀疏时序监督下表现更稳健，并保持实时级渲染速度，且支持时间外推与局部动力学编辑。

Conclusion: EvoGS将4D Gaussian Splatting重新解释为连续时间的神经动力学系统，通过学习的动力学场驱动高斯场随时间演化，从而实现比基于变形场的方法更好的运动一致性与时间连贯性，并支持稀疏时序监督的高效学习、时间外推与局部可组合动力学注入，且保持实时渲染能力。

Abstract: We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering

</details>


### [162] [Over++: Generative Video Compositing for Layer Interaction Effects](https://arxiv.org/abs/2512.19661)
*Luchao Qi,Jiaye Wu,Jun Myeong Choi,Cary Phillips,Roni Sengupta,Dan B Goldman*

Main category: cs.CV

TL;DR: 提出augmented compositing任务和Over++框架，在不依赖相机位姿或深度监督的情况下，基于文本与输入视频层生成真实、可编辑的半透明环境效果，同时保持原始场景。


<details>
  <summary>Details</summary>
Motivation: 专业视频合成中，艺术家需手工添加与环境交互的视觉效果，现有模型难以在保持原视频的同时生成真实环境效果，且视频修复方法需要昂贵的逐帧掩码或产生不合理结果。

Method: We introduce a new task named augmented compositing and propose Over++, a framework for text-conditioned video effect generation that preserves input scenes without assuming camera pose or needing depth supervision.

Result: Over++ generates diverse, realistic semi-transparent environmental effects (shadows, reflections, dust, splashes) conditioned on text prompts and input layers, with optional mask and keyframe controls, outperforming baselines while trained on limited data.

Conclusion: Over++ enables practical, editable video compositing by synthesizing realistic environmental interactions while maintaining scene fidelity, reducing manual effort in professional workflows.

Abstract: In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.

</details>


### [163] [Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2512.19663)
*Argha Kamal Samanta,Harshika Goyal,Vasudha Joshi,Tushar Mungle,Pabitra Mitra*

Main category: cs.CV

TL;DR: 本文通过ViT、Bio-ClinicalBERT与MLP分别编码图像、文本与结构化数据，利用联合Transformer和多重损失（对比、重建、分类）实现卓越的跨模态检索（BRSET Recall@1 99.94%）与强泛化能力（DeepEyeNet 零-shot Recall@1 93.95%）。


<details>
  <summary>Details</summary>
Motivation: 通用领域的视觉—语言模型（如CLIP）在医学子领域表现不佳，尤其在眼科影像—文本检索与跨模态对齐方面，需设计专门结合医学知识与多模态信息的模型来弥补这一空白。

Method: 使用ViT-B/16编码视网膜图像、Bio-ClinicalBERT编码临床叙述、MLP编码结构化人口学和临床特征，三者通过带模态标识的联合Transformer融合。训练目标包含多对比损失（模态间对齐）、图像/文本重建损失以及针对ICDR与SDRG分级任务的分类损失。

Result: 在BRSET数据集上，文本—图像检索Recall@1达99.94%（明显优于微调CLIP的1.29%）；SDRG与ICDR分级准确率分别为97.05%与97.97%。在未见的DeepEyeNet零-shot测试中，Recall@1达到93.95%（CLIP仅0.22%），显示出很强的泛化能力与跨模态对齐效果。

Conclusion: 该研究提出了一种知识增强的联合嵌入框架，通过多模态Transformer融合视网膜图像、临床文本与结构化患者数据，显著提升了医学影像—文本对齐与检索性能，同时保持或超越DR分级的分类精度。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.

</details>


### [164] [Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning](https://arxiv.org/abs/2512.19676)
*Mojtaba Safari,Shansong Wang,Vanessa L Wildman,Mingzhe Hu,Zach Eidex,Chih-Wei Chang,Erik H Middlebrooks,Richard L. J Qiu,Pretesh Patel,Ashesh B. Jania,Hui Mao,Zhen Tian,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出结合MHSSM与轻量级通道MLP的高效MRI超分辨框架，在脑与前列腺数据上以极低参数与计算量显著超越多种基线方法，具备临床可行性。


<details>
  <summary>Details</summary>
Motivation: 缩短高分辨率MRI的采集限制，通过高效深度学习超分辨保持解剖细节以利临床应用。

Method: Multi-head selective state-space models with channel MLP and hybrid 2D patch scanning; MambaFormer block combining MHSSM, depthwise conv, gated channel mixing

Result: Superior SR performance on 7T brain and 1.5T prostate datasets with SSIM, PSNR, LPIPS, GMSD metrics; outperforms baselines; uses 0.9M params and 57 GFLOPs

Conclusion: Efficient and accurate MRI SR framework with strong potential for clinical translation due to low compute and high fidelity

Abstract: Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.

</details>


### [165] [WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion](https://arxiv.org/abs/2512.19678)
*Hanyang Kong,Xingyi Yang,Xiaoxu Zheng,Xinchao Wang*

Main category: cs.CV

TL;DR: WorldWarp用3D几何缓存做结构骨架，显式将历史视图warp到新视角；对warp留下的空洞用带时空可变噪声方案的扩散模型fill-and-revise，动态更新缓存以维持跨帧一致性


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在相机条件的潜空间表现好但像素级3D一致性差，导致遮挡和复杂相机轨迹下生成质量低；需要把3D几何约束和2D生成能力结合起来

Method: 维持一个基于Gaussian Splatting的在线3D几何缓存，显式将历史内容warp到新视角作为结构支架；使用Spatio-Temporal Diffusion模型进行fill-and-revise，设计时空可变噪声调度对warp区域做部分噪声（精修）对空白区域做全噪声（生成），并在每步动态更新3D缓存以保持一致性

Result: WorldWarp提出结合3D几何锚点和2D扩散细化器以生成长时序、一致的视频

Conclusion: 通过在线3D几何缓存（基于Gaussian Splatting）与时空变噪声调度的扩散模型联合，WorldWarp在保结构一致性的同时提升纹理质量，显著改善遮挡与复杂相机运动下的视频生成

Abstract: Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a "fill-and-revise" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.

</details>


### [166] [VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation](https://arxiv.org/abs/2512.19680)
*Xinyao Liao,Qiyuan He,Kai Xu,Xiaoye Qu,Yicong Li,Wei Wei,Angela Yao*

Main category: cs.CV

TL;DR: VA-π aligns autoregressive image generators with tokenizers by optimizing a pixel-space ELBO and using a reinforcement-style pixel reconstruction reward; it's a fast, tokenizer-free post-training method that substantially improves image quality metrics.


<details>
  <summary>Details</summary>
Motivation: Autoregressive visual generation models use tokenizers mapping images to discrete tokens, but tokenizers are trained to reconstruct clean images while AR generators optimize token likelihood only, causing misalignment: generated tokens may decode to low-quality images. The paper aims to align generator and tokenizer via pixel-space objective.

Method: Proposes VA-π, a lightweight post-training framework formulating generator-tokenizer alignment as variational optimization. Derives ELBO combining pixel reconstruction and autoregressive modeling. Introduces a reinforcement-based alignment treating AR generator as policy, using pixel-space reconstruction under teacher forcing as intrinsic reward, plus ELBO regularization to maintain token distribution consistency. No tokenizer retraining or external reward models needed.

Result: With 1% ImageNet-1K and 25 minutes tuning, VA-π reduces FID from 14.36 to 7.65 and increases IS from 86.55 to 116.70 on LlamaGen-XXL. Also improves text-to-image metrics on GenEval: LlamaGen from 0.306 to 0.339 and Janus-Pro from 0.725 to 0.744.

Conclusion: VA-π provides an efficient post-training method to align AR generators with tokenizers via pixel-space ELBO and reinforcement-based rewards, yielding significant quality improvements with minimal data and compute, without retraining tokenizers.

Abstract: Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.

</details>


### [167] [From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs](https://arxiv.org/abs/2512.19683)
*Mingrui Wu,Zhaozhi Wang,Fangjinhua Wang,Jiaolong Yang,Marc Pollefeys,Tong Zhang*

Main category: cs.CV

TL;DR: 构建基于行人视角带LiDAR/IMU/GPS的户外大规模数据集并自动生成层级化空间问题，发现当前MLLM在开放世界场景缺乏真实的度量化空间推理能力，倾向依赖语言先验。


<details>
  <summary>Details</summary>
Motivation: 评估和提升多模态大语言模型（MLLMs）在空间推理与物理感知方面的能力，填补现有基准在户外真实场景中缺乏可验证度量真值的数据空白。

Method: 构建大规模基准数据集：使用行人视角的视频，配同步立体相机、LiDAR与IMU/GPS，生成具备精确3D度量信息的数据集；基于此自动生成层级化空间推理问题，覆盖定性关系、定量度量与运动学理解；并在公开场景上评估现有MLLMs，同时用合成异常场景与盲测分析模型是否依赖语言先验。

Result: 在开放世界（户外）场景中，模型在结构化室内基准上观测到的性能提升消失；合成异常场景与盲测显示当前MLLMs过度依赖语言先验，而非真正的视觉-物理推理。

Conclusion: 所提基准提供了一个有原则的平台，用以诊断并推动MLLMs在物理与度量化空间智能方面的进步，揭示了现有系统在真实世界情境下的显著局限性。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.

</details>


### [168] [Zero-shot Reconstruction of In-Scene Object Manipulation from Video](https://arxiv.org/abs/2512.19684)
*Dixuan Lin,Tianyou Wang,Zhuoyang Pan,Yufu Wang,Lingjie Liu,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 提出一个从单目RGB视频恢复场景内手-物体交互的系统：先用基础模型初始化物体网格与位姿、场景点云和手位姿，再通过两阶段优化恢复物体与手相对于场景的物理一致运动。


<details>
  <summary>Details</summary>
Motivation: 单目视频重建场景内物体操作具有病态的场景重建、手物体深度模糊及物理合理性要求高等挑战。现有方法多在手中心坐标系下工作并忽略场景信息，导致尺度与度量不准且难以实际应用。作者希望克服这些限制，得到对场景一致且度量准确的手-物体重建结果。

Method: 方法先用数据驱动的基础模型初始化三大组件：物体网格与位姿、场景点云和手位姿。随后进行两阶段优化：第一阶段估计抓取与初始对齐，第二阶段在场景约束下优化手物体全程运动以保证与视频观测及场景一致并满足物理合理性（如接触与碰撞约束）。

Result: 系统能从单目RGB视频重建从抓取到交互的完整手-物体运动，并与场景信息一致，提高了度量精度和实际可用性。论文证明了方法在重建准确性和物理一致性方面优于忽略场景的基线方法。

Conclusion: 该论文提出了首个从单目RGB视频重建场景内物体操作的系统，通过结合基于数据的基础模型初始化和两阶段优化，实现了从抓取到交互的手-物体运动恢复，且与场景信息一致。

Abstract: We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.

</details>


### [169] [Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models](https://arxiv.org/abs/2512.19686)
*Zixuan Ye,Quande Liu,Cong Wei,Yuanxing Zhang,Xintao Wang,Pengfei Wan,Kun Gai,Wenhan Luo*

Main category: cs.CV

TL;DR: 在CoT基础上加入视觉一致性：先生成视觉检查清单（Adaptive Visual Planning），再基于清单迭代自我反思与修正（Iterative Visual Correction），用监督微调+flow-GRPO的视觉检查奖励训练，提升多模态生成的视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT侧重文本与提示一致性，忽视生成内容与参考图像之间的视觉一致性，导致无法保持关键视觉属性（身份、物体属性、风格）在多参考图像生成任务中的一致性。

Method: 1) Adaptive Visual Planning：模型生成结构化的视觉检查清单以列出需保持的一致性视觉元素；2) Iterative Visual Correction：依据清单进行自我反思与多轮修正生成结果；训练上用监督微调教授规划与自我修正流程，并引入flow-GRPO自定义视觉检查奖励以强化视觉一致性。

Result: 在多模态生成任务中，相较于零-shot统一模型以及仅使用文本CoT的模型，方法在保持视觉语境一致性方面取得更好表现，能更稳定地维护人像身份、物体属性与风格等关键视觉特征。

Conclusion: 该论文提出在生成推理过程中引入视觉语境一致性，通过规划视觉检查清单和迭代自我修正来保持关键视觉特征，从而提升多模态、多参考图像生成的视觉一致性。实验表明优于只使用文本CoT或零-shot模型。

Abstract: Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.

</details>


### [170] [Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models](https://arxiv.org/abs/2512.19692)
*Pablo Ruiz-Ponce,Sergio Escalera,José García-Rodríguez,Jiankang Deng,Rolandos Alexandros Potamias*

Main category: cs.CV

TL;DR: Interact2Ar：首个文本驱动的自回归扩散生成全身多人交互动作，包含手部细节、记忆机制与大上下文窗口，提升适应性并支持实时与多人应用，实验显示性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有生成交互动作的方法要么忽略手部细节导致真实感不足，要么一次生成整段序列无法体现互动中的反应性与适应性；此外数据与学习复杂度限制了表达能力。为此需要同时处理手部与身体细节并支持分段/自回归生成以更好建模互动过程。

Method: 提出Interact2Ar：1) 文本条件自回归扩散框架，按时间步生成动作序列而非一次生成全序列；2) 并行手部分支专门建模手部运动以保留细节；3) 引入记忆技术与大上下文窗口提升对历史交互的适应能力；4) 支持从双人扩展到多人、实时扰动适应与时序拼接。

Result: 通过定量与定性实验，结合新设计的交互评估器和扩展度量，Interact2Ar在动作真实度、协调性与手部细节保真度上均优于现有基线，并展示了实时适应与多人扩展能力。

Conclusion: 本文提出了Interact2Ar，一种首创的端到端文本条件自回归扩散模型，用于生成高保真全身人际交互动作，特别强调手部运动的细节与交互协调。模型通过并行分支捕捉手部运动并引入自回归流水线与记忆机制以扩展上下文窗口，从而支持时序组合、实时扰动适应及多人场景扩展。实验与新设计的评估指标显示其在生成真实交互动作方面优于现有方法。

Abstract: Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.

</details>


### [171] [The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding](https://arxiv.org/abs/2512.19693)
*Weichen Fan,Haiwen Diao,Quan Wang,Dahua Lin,Ziwei Liu*

Main category: cs.CV

TL;DR: 从频谱角度，语义编码器偏低频，像素编码器含高频；Prism假设把模态视为共享特征频谱的投影；提出UAE用频带调制器统一两者，ImageNet与MS-COCO上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有多模态表示在语义抽象与像素细节之间存在矛盾，缺乏统一解释与方法，作者希望通过频谱分析揭示本质并提出统一模型。

Method: 系统分析编码器频谱特性，提出Prism Hypothesis；设计Unified Autoencoding(UAE)，使用频带调制器在潜空间中协调低频语义与高频像素信息；在ImageNet和MS-COCO上进行大量评价。

Result: 论文通过频谱视角分析不同模态编码器的表征特性，提出了Prism Hypothesis并设计了UAE模型，实现语义与像素信息的统一编码。

Conclusion: 语义编码器主要保留低频表示抽象语义，像素编码器额外保留高频细节；基于此提出的UAE能在单一潜空间兼顾语义抽象与像素保真，实验证明效果优越。

Abstract: Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [172] [Memelang: An Axial Grammar for LLM-Generated Vector-Relational Queries](https://arxiv.org/abs/2512.17967)
*Bri Holt*

Main category: cs.DB

TL;DR: 提出轴向文法（axial grammar），在一维标记序列中通过秩别分隔符恢复多维结构；实现为Memelang，一种面向LLM的紧凑查询IR，可直接解析并编译为Postgres SQL


<details>
  <summary>Details</summary>
Motivation: 减少LLM生成工具调用中冗长语法和歧义，通过固定坐标角色的紧凑中间表示实现可确定解析与流式编译

Method: Introduce axial grammar and Memelang; describe parser and SQL compiler

Result: Axial grammar encodes n-dimensional structure via rank-specific separators; Memelang is an instantiation mapping coordinates to table/column/value slots and supports relative references, variable binding, implicit context, and inline tags for grouping/agg/ordering; provides lexer/parser and compiler to parameterized PostgreSQL (with pgvector)

Conclusion: Axial grammar + Memelang enable deterministic, compact LLM-emittable IRs that parse in one left-to-right pass and compile to SQL for efficient execution, reducing verbosity and parsing ambiguity

Abstract: Structured generation for LLM tool use highlights the value of compact DSL intermediate representations (IRs) that can be emitted directly and parsed deterministically. This paper introduces axial grammar: linear token sequences that recover multi-dimensional structure from the placement of rank-specific separator tokens. A single left-to-right pass assigns each token a coordinate in an n-dimensional grid, enabling deterministic parsing without parentheses or clause-heavy surface syntax. This grammar is instantiated in Memelang, a compact query language intended as an LLM-emittable IR whose fixed coordinate roles map directly to table/column/value slots. Memelang supports coordinate-stable relative references, parse-time variable binding, and implicit context carry-forward to reduce repetition in LLM-produced queries. It also encodes grouping, aggregation, and ordering via inline tags on value terms, allowing grouped execution plans to be derived in one streaming pass over the coordinate-indexed representation. Provided are a reference lexer/parser and a compiler that emits parameterized PostgreSQL SQL (optionally using pgvector operators).

</details>


### [173] [Sync Without Guesswork: Incomplete Time Series Alignment](https://arxiv.org/abs/2512.18238)
*Ding Jia,Jingyu Zhu,Yu Sun,Aoqian Zhang,Shaoxu Song,Haiwei Zhang,Xiaojie Yuan*

Main category: cs.DB

TL;DR: 提出了一种针对不完整多变量时间序列的基于约束的对齐框架，无需先行插补，设计了三个近似算法在准确性与效率间权衡，并在真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多变量时间序列对齐在分析中很重要，但缺失值与时间戳不一致使得对齐困难。现有方法依赖插补会引入误差，导致对齐效果欠佳。作者希望避免插补，保证时间与结构一致性。

Method: 构建一个基于约束的对齐框架，直接在含缺失的数据上定义对齐约束，兼顾时间与结构一致性；提出三种近似算法以平衡准确性与可扩展性（论文可能描述了不同的贪心、动态规划或启发式剪枝策略）。

Result: 在多个真实世界数据集上进行了实验，在不同缺失率下，与现有方法比较，该方法在对齐质量（对齐准确率和对齐元组数量）上均表现更好。

Conclusion: 该工作形式化定义了不完整多元时间数据对齐问题，提出无需插补的约束型框架和三种近似算法，并通过实证验证了其在对齐准确性与产出数量上的改进。

Abstract: Multivariate time series alignment is critical for ensuring coherent analysis across variables, but missing values and timestamp inconsistencies make this task highly challenging. Existing approaches often rely on prior imputation, which can introduce errors and lead to suboptimal alignments. To address these limitations, we propose a constraint-based alignment framework for incomplete multivariate time series that avoids imputation and ensures temporal and structural consistency. We further design efficient approximation algorithms to balance accuracy and scalability. Experiments on multiple real-world datasets demonstrate that our approach achieves superior alignment quality compared to existing methods under varying missing rates. Our contributions include: (1) formally defining incomplete multiple temporal data alignment problem; (2) proposing three approximation algorithms balancing accuracy and efficiency; and (3) validating our approach on diverse real-world datasets, where it consistently outperforms existing methods in alignment accuracy and the number of aligned tuples.

</details>


### [174] [Towards Scalable Visual Data Wrangling via Direct Manipulation](https://arxiv.org/abs/2512.18405)
*El Kindi Rezig,Mir Mahathir Mohammad,Nicolas Baret,Ricardo Mayerhofer,Andrew McNutt,Paul Rosen*

Main category: cs.DB

TL;DR: Buckaroo 是一个可扩展的可视化数据清洗系统，允许用户通过交互式可视化检测并修复缺失值、异常值和类型不匹配，支持用户自定义检测器/修复器、变更可追溯和可复现脚本，并通过差分存储与索引优化性能，集成 Hopara 实现大数据集可交互导航。


<details>
  <summary>Details</summary>
Motivation: 现有数据清洗工具要么依赖易错且难调试的手写脚本，要么使用不透明的黑箱自动化流水线，二者在可控性与可解释性之间存在折中；因此需要一个既可交互又可扩展、可重现且具备可伸缩性的可视化数据整理方案。

Method: 将数据清洗重构为基于可视化的直接操作任务：用户通过协调可视化（可交互图表）探索并标注异常，系统支持用户自定义的错误检测器与修复器；维护变更的溯源以支持撤销/重做并生成可重复运行的脚本；用索引与差分存储局部化异常检测以减少重新计算；并与 Hopara pan-and-zoom 引擎集成提供多层次大数据导航。

Result: 通过经验评估和专家评审，作者展示了 Buckaroo 在可扩展性和交互性方面缩小了人工可视检查与可编程修复之间的差距，验证了系统在处理大数据集时仍能保持交互性和可重现性。

Conclusion: Buckaroo 提出了一种基于可视化直接操作的数据清洗范式，兼顾可扩展性与可重现性，通过交互式可视化检测并修复异常，结合索引与差分存储以减少重算，并与 Hopara 引擎集成实现大数据集的多层缩放导航。

Abstract: Data wrangling - the process of cleaning, transforming, and preparing data for analysis - is a well-known bottleneck in data science workflows. Existing tools either rely on manual scripting, which is error-prone and hard to debug, or automate cleaning through opaque black-box pipelines that offer limited control. We present Buckaroo, a scalable visual data wrangling system that restructures data preparation as a direct manipulation task over visualizations. Buckaroo enables users to explore and repair data anomalies - such as missing values, outliers, and type mismatches - by interacting directly with coordinated data visualizations. The system extensibly supports user-defined error detectors and wranglers, tracks provenance for undo/redo, and generates reproducible scripts for downstream tasks. Buckaroo maintains efficient indexing data structures and differential storage to localize anomaly detection and minimize recomputation. To demonstrate the applicability of our model, Buckaroo is integrated with the \textit{Hopara} pan-and-zoom engine, which enables multi-layered navigation over large datasets without sacrificing interactivity. Through empirical evaluation and an expert review, we show that Buckaroo makes visual data wrangling scalable - bridging the gap between visual inspection and programmable repairs.

</details>


### [175] [A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback](https://arxiv.org/abs/2512.18622)
*Thanh Dat Hoang,Thanh Trung Huynh,Matthias Weidlich,Thanh Tam Nguyen,Tong Chen,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 本文提出MATS，一种为小型LLM设计的Text2SQL框架，通过多代理分工与基于执行反馈的强化学习对齐，能在单GPU上用少量参数取得与大模型相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 企业因隐私与成本倾向内部部署小型LLM，但小模型泛化能力不足，难以胜任复杂Text2SQL任务。需要一种能在有限模型容量下仍保持高性能的方法。

Method: MATS采用多代理机制，为辅助代理分配专门角色以分担任务并相互交互；引入基于执行反馈的强化学习训练方案，用运行时得到的反馈对代理进行对齐和优化，从而在小模型上提升Text2SQL生成质量。

Result: 在基准数据集上评估表明，MATS在单GPU服务器上运行，使用明显更少的参数仍能达到与大规模LLM相当的准确率。

Conclusion: 通过多代理协作与基于执行反馈的强化学习，MATS有效弥补了小型LLM在Text2SQL任务上的能力差距，为隐私/成本受限场景提供可行方案，并已开源代码与数据。

Abstract: Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.

</details>
