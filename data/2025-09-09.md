<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 162]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

TL;DR: Label Smoothing++对目标类保留固定标签，允许网络学习非目标类概率，从而保留类间关系并提高泛化和减少过confidence。


<details>
  <summary>Details</summary>
Motivation: 传统label smoothing通过向one-hot标签加入均匀概率来正则化，但这会平等对待所有非目标类，破坏类间关系；提出方法以保持类间语义/相似性信息。

Method: 对训练目标进行部分参数化：目标类使用固定值，非目标类标签不再均匀分配而由网络学习；在损失中结合这种可学习的非目标类标签以引导训练，从实验上与标准label smoothing和不平滑标签比较。

Result: 在多数据集的大量实验中，Label Smoothing++减少了模型的过度自信，提升了泛化性能，并能反映更合理的类间概率分布，优于传统label smoothing和基线。

Conclusion: Label Smoothing++通过保留目标类固定标签并让网络学习非目标类标签，有效缓解过度自信并保留类间关系，提高泛化性。

Abstract: Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [2] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

TL;DR: VILOD通过可视分析将人类与主动学习结合，用交互式选择替代或补充自动采样，提升目标检测标注的可解释性与效果，实验显示其策略能达到与不确定性采样类似的性能。


<details>
  <summary>Details</summary>
Motivation: 目标检测深度学习发展受限于高质量标注数据的获取成本。现有主动学习方法虽能减少标注量，但往往缺乏透明性与可控性，且可能错过与策略不一致的有价值样本。将人类直觉和可视分析纳入标注流程可提高效果与信任。

Method: 设计并实现VILOD系统：包括图像特征的t-SNE投影、基于模型不确定度的热图、模型训练状态视图与交互式样本选择机制。通过若干比较实验与用例分析，评估不同交互式标注策略相较于自动化不确定性采样基线的效果。

Result: 交互式可视化工具使用户更好地理解模型状态与数据分布（RQ1），并能采用多样的视觉引导策略，在若干任务上取得与自动不确定性采样基线相当甚至更优的性能轨迹（RQ2）。研究证明VILOD在提升流程透明性、可管理性与标注效率方面具有潜力。

Conclusion: 本论文提出并实现了VILOD，一个面向目标检测标注的交互式可视化标注工具，通过将t-SNE图、置信度热图和模型状态视图等组件整合入人机闭环标注流程，提升了标注过程的可解释性与策略灵活性。

Abstract: The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [3] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

TL;DR: 提出AKD：动态学习/计算alpha并结合CAM（MLP+注意力）重权教师类输出，能提高KD效果和稳定性（在CIFAR-10上优于固定alpha基线）。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统KD使用固定权重alpha在训练中难以兼顾不同阶段对hard/soft监督的需求，固定alpha可能导致训练次优或不稳定，因此需要一个随训练进程自适应调整权重的方法。

Method: 方法包括两部分：1) 将alpha设计为可学习参数并基于学生-教师差异引入动态计算公式，以在训练过程中自适应调整；2) 设计CAM（MLP+注意力机制）对教师对各类别的soft输出进行类级别重加权，从而提供更有针对性的软监督。

Result: 在CIFAR-10数据集上，采用ResNet-50作为教师、ResNet-18作为学生的实验表明，AKD在准确率上优于固定权重的KD基线方法，并且训练收敛更稳定。

Conclusion: 本文提出的自适应知识蒸馏（AKD）框架通过动态调整蒸馏权重alpha并引入上下文感知模块（CAM），可以更好地平衡hard-label与soft-label监督，从而提升学生模型的性能与训练稳定性。

Abstract: Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [4] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

TL;DR: 提出基于SPGN+扩散模型的生成框架与对齐流水线，公开1000+视频-EEG合成配对数据，旨在解决多模态模型缺乏视频-EEG训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型缺乏大规模视频-EEG配对数据，限制了EEG对齐能力与情绪/脑机接口研究；因此通过合成高质量EEG数据扩充数据集并提供对齐流水线，促进研究与工程应用。

Method: 构建视频与EEG对齐工程流水线；使用自对弈图网络（SPGN）结合扩散模型生成个性化62通道、200Hz EEG信号；在SEED-VD上生成并发布1000+视频-EEG配对样本并附带情绪标签。

Result: 公开了Video2EEG-SPGN-Diffusion开源框架、对齐流水线和一个包含1000+样本的合成62通道200Hz EEG与SEED-VD视频及情绪标签的数据集；展示了该方法在情绪分析、数据增强和脑机接口任务中的潜在应用价值（论文宣称具有实用性与工程意义）。

Conclusion: 本文提出了Video2EEG-SPGN-Diffusion框架，能够基于SEED-VD视频刺激生成个性化多通道EEG并公开配对数据集，推动视频-EEG对齐与多模态研究。

Abstract: This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [5] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

TL;DR: FRC作为一种更高效的边中心性度量，可在RWNN剪枝中提供接近ORC的性能，显著减少参数并带来理论加速，同时引发模块化增强与效率下降的结构性变化。


<details>
  <summary>Details</summary>
Motivation: 探究网络拓扑对深度学习影响及利用边中心性度量作为剪枝依据，以减小RWNN复杂度并保持性能，同时寻找计算开销更小的有效度量（尤其关注FRC与ORC的折衷）。

Method: 在三个随机网络生成器（ER、WS、BA）上构建RWNN，用于新冠胸片分类任务；计算每条边的FRC、ORC和边介数中心性（EBC），按度量排序并按阈值或压缩率剪枝；比较压缩比、理论加速比和分类性能；并分析剪枝后网络的模块度和全局效率。

Result: 实验证明FRC剪枝在多种生成模型下均能显著压缩网络且保持分类性能接近未剪枝或ORC剪枝的水平，同时FRC计算更快，EBC表现次之；剪枝后网络表现出模块度上升与全局效率下降的权衡。

Conclusion: 作者验证了边中心性度量可用于RWNN剪枝，且Forman-Ricci曲率（FRC）在计算效率上优于Ollivier-Ricci曲率（ORC），同时在保持性能（准确率、特异性、敏感性）方面与ORC相当，因此FRC是一个有效且更高效的剪枝指标。

Abstract: Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [6] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

TL;DR: 提供首个大规模手写爵士lead sheet数据集（293份、2021谱线）与对应OMR模型，利用合成数据与预训练提升识别，并公开全部资源。


<details>
  <summary>Details</summary>
Motivation: 现有OMR系统缺乏对和弦这一谱面要素的处理，且手写爵士lead sheet具有高度变异与质量问题，亟需专门的数据集和模型来提高识别性能。

Method: 构建包含293份手写lead sheet（2021个五线谱）并对齐Humdrum **kern与MusicXML作为标注；生成合成谱面图像用于数据增强；设计针对lead sheet的tokenization方案；基于预训练模型进行训练与微调，比较合成数据与真实手稿的效果。

Result: 发布了一个包含手写与合成数据的公开数据集与相应预训练模型与代码。实验证明使用合成数据与预训练权重能提升识别准确率，且所提tokenization适配lead sheet的特点。

Conclusion: 本文提出了面向手写爵士lead sheet的OMR数据集与模型，填补了现有系统对和弦识别的空白，并公开代码与资源，具备实用价值。

Abstract: In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [7] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

TL;DR: 用合成“四线索”数据微调视觉指令模型，并通过生成-反思两阶段推理显著改善域偏移下的目标识别鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实部署中目标识别模型遭遇域偏移（低层图像统计变化、视角与遮挡、类别间混淆）导致性能下降，需提升模型在多种变化下的可迁移与稳健理解能力。

Method: 构建合成数据管线生成包含四类注释的训练样本；对预训练视觉指令模型进行参数高效监督微调；推理时先输出四线索，再以这些输出作为证据进行自检与迭代校正（Re-Thinking）。

Result: 在多个分离单一域偏移的鲁棒性基准上，RT-VLM稳健优于强基线，表明结构化多模态证据结合显式自我批判循环能提升视觉理解的可靠性与可迁移性。

Conclusion: 本文提出RT-VLM框架，通过合成数据生成带“四线索”注释（精确框、类名、对象级详细描述、场景级上下文描述），并对Llama 3.2 11B Vision Instruct进行参数高效微调，在推理时采用先生成线索再反思迭代修正的两阶段策略，显著提升在各类域偏移场景下的鲁棒性。

Abstract: Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [8] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

TL;DR: 本文利用YOLOv5+卡尔曼滤波和时空标定的视频运动学方法，在智能手机上实现了低成本、用户友好的羽毛球扣杀速度测量系统。


<details>
  <summary>Details</summary>
Motivation: 现有运动性能测量设备昂贵且复杂，业余和休闲玩家难以获得；羽毛球作为受欢迎的运动，需要可及性高的速度测量工具以促进训练与表现提升。

Method: 使用自定义训练的YOLOv5进行羽毛球（羽毛球）检测，结合卡尔曼滤波进行轨迹跟踪；通过时空尺度变换的视频运动学方法估算速度，整合进移动应用。

Result: 实现了一个可在普通智能手机摄像下自动检测并估算羽毛球速度的系统，提供实时/离线的速度统计并封装为易用的移动应用，降低了入门门槛。

Conclusion: 该论文提出了一个基于智能手机视频的羽毛球扣杀速度测量系统，主张用低成本、易用的方式让大众获得专业级表现分析。

Abstract: Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [9] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

TL;DR: 作者建立了含42名被试、每人1200字的大规模汉字手写数据库，升级了手写工具箱并用多元回归发现正字法和语音因素在字符、偏旁、笔画层面均影响书写，且效应随层级减弱。


<details>
  <summary>Details</summary>
Motivation: 探究哪些语言学成分调节汉字在字符、偏旁和笔画层面的手写过程，并提供可复用的精细手写数据采集与批处理工具。

Method: 招募42名被试进行听写任务，每人书写1200字；改进并发布工具箱以捕捉笔画级轨迹与批量处理测量（潜伏期、时长、笔压）；使用多元回归分析评估正字法与语音预测变量对不同层级书写的影响。

Result: 多元回归显示正字法预测变量影响书写的准备与执行在字符、偏旁与笔画层面均存在；语音因素也影响三个层面执行；整体效应呈层级递减（字符>偏旁>笔画）。构建的数据库与工具箱可支持未来心理语言学与神经语言学研究。

Conclusion: 该研究构建了大规模汉字手写数据库并扩展了OpenHandWrite_Toolbox，证实语言成分（正字法与语音）在字符、偏旁和笔画层面均影响书写准备与执行，且效应随层级递减。

Abstract: Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [10] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

TL;DR: TL;DR：本文提出DGNN+LSTM混合模型，解耦步态分类与运动预测，实现提前跌倒预测和过渡态监测，在两个数据集上取得优于单一DGNN和现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 动机：现有研究在跌倒检测上已有进展，但对跌倒发生前的预测以及稳定与跌倒之间的过渡态分析较少，提前预测对助行机器人与安全监护有重要意义。

Method: 方法：使用实时视频骨架特征，构建两支路模型：一支为DGNN分类器，对步态分为稳定、过渡、跌倒三个状态；另一支为LSTM时序预测器，预测未来若干时刻的人体运动。解耦任务（分类与预测）并联合使用两者输出以实现提前检测。

Result: 结果：在OUMVLP-Pose和URFD数据集上训练验证，所提混合模型在预测误差和识别准确率方面优于仅用DGNN及相关文献模型，证明解耦预测与分类能提升性能，并能有效识别并监测过渡态。

Conclusion: 论文结论：将动态图神经网络(DGNN)与LSTM解耦地结合用于步态分类与运动预测，可实现提前且高精度的跌倒预测。方法在OUMVLP-Pose和URFD数据集上优于仅用DGNN或相关文献方法，且能监测过渡态，为辅助系统提供有价值的信息。

Abstract: Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [11] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

TL;DR: K-Means更快但不够精准，FCM更准但更慢，存在速度与精度的权衡。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割对临床决策、放疗计划和疾病监测至关重要，肿瘤形态和强度分布异质性使得自动分割具有挑战性，因此比较不同聚类策略的适用性和性能。

Method: 在BraTS2020数据集上进行实验，预处理包括高斯滤波和CLAHE，对比K-Means（硬聚类）与FCM（软聚类），使用Dice系数和处理时间作为评价指标。

Result: 实验结果显示K-Means平均运行时间约0.3s/张但平均DSC仅0.43；FCM平均运行时间约1.3s/张但平均DSC为0.67，表明FCM在边界精度上优于K-Means，但代价是更高的计算时间。

Conclusion: 本文比较了K-Means与FCM在MRI脑肿瘤分割中的表现，结论是K-Means速度更快但精度较低，FCM精度更高但计算开销大，体现了效率与边界精确度的权衡。

Abstract: Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [12] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

TL;DR: 通过在预训练CNN中加入注意力模块并结合数据增强，论文实现了洋葱病虫多分类任务的高精度（96.90%准确率，0.96 F1）。


<details>
  <summary>Details</summary>
Motivation: 现有方法多集中于二分类，无法满足实际农业中需区分具体病虫种类的需求，提升多分类性能有助于精准防治和决策支持。

Method: 基于预训练卷积神经网络，集成注意力模块（例如通道注意或空间注意），并采用全面的数据增强策略（裁剪、旋转、色彩变换等）以扩充少数类样本。训练时采用多类交叉熵损失，评估指标包括准确率和F1分数。

Result: 在真实田间图像数据集上，模型取得了96.90%整体准确率和0.96的F1分数，优于使用相同数据集的其他方法。

Conclusion: 该论文提出了一个用于洋葱病虫多分类的深度学习模型，通过在预训练CNN中加入注意力模块并结合数据增强来缓解类别不平衡，实现了高准确率。

Abstract: Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [13] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

TL;DR: DVRF通过建模速度场差值并加入时变位移，在rectified flow中实现无反演、路径感知的高质量文本图像编辑，理论上连接并泛化若干既有方法，实验表现优越。


<details>
  <summary>Details</summary>
Motivation: 解决先前蒸馏采样方法中常见的过平滑(oversmoothing)伪影与目标轨迹对齐不足问题，提升编辑结果的质量与可控性，同时保持高效与易适配性。

Method: 基于蒸馏的Delta Velocity建模：显式估计源/目标速度场差值；引入时间依赖位移项将噪声潜变量推向目标轨迹；理论上当位移为零时退化为Delta Denoising Score，若位移为线性并在rectified-flow下运作则泛化FlowEdit。无需改动模型结构。

Result: 实验表明DVRF在编辑质量、保真度和可控性上优于现有方法，并且不需架构修改，实用性强。

Conclusion: DVRF提出了一种在rectified flow框架下用于文本到图像编辑的无反演、路径感知的蒸馏方法，通过建模源与目标速度场差异并引入时变位移项，缓解了过平滑伪影，兼顾质量、保真度与可控性。

Abstract: We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [14] [Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis](https://arxiv.org/abs/2509.05343)
*Zahid Ullah,Minki Hong,Tahir Mahmood,Jihie Kim*

Main category: cs.CV

TL;DR: 将SE或CBAM注意力模块系统性地嵌入五种常用CNN，可稳定提升医学影像分类与定位性能，EfficientNetB5+CBAM效果最佳。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在医学影像中难以捕捉细粒度和复杂的诊断特征，注意力机制有助于模型聚焦于病变或关键组织区域，从而提升判别性能与可解释性。

Method: 在VGG16、ResNet18、InceptionV3、DenseNet121、EfficientNetB5五种基础CNN中分别嵌入Squeeze-and-Excitation（SE）模块或混合的Convolutional Block Attention Module（CBAM），在通道和空间维度上对特征进行自适应重校准，并在脑肿瘤MRI与胎盘/组织病理（Products of Conception）数据集上进行比较实验。

Result: 所有嵌入注意力的模型在准确率、召回率、F1等指标上均优于对应基线模型；其中EfficientNetB5+混合注意力在两个数据集上取得最高综合性能，且在特征热图/定位任务上表现更好，显示更强的跨模态泛化能力。

Conclusion: 引入注意力机制可以显著提升主流CNN在医学影像分类任务中的表现，尤其是在EfficientNetB5+混合注意力组合上取得最佳效果。注意力模块不仅提高了分类准确率，还改善了特征定位与泛化能力。

Abstract: Deep learning has become a powerful tool for medical image analysis; however,
conventional Convolutional Neural Networks (CNNs) often fail to capture the
fine-grained and complex features critical for accurate diagnosis. To address
this limitation, we systematically integrate attention mechanisms into five
widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,
DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient
regions and improve discriminative performance. Specifically, each baseline
model is augmented with either a Squeeze and Excitation block or a hybrid
Convolutional Block Attention Module, allowing adaptive recalibration of
channel and spatial feature representations. The proposed models are evaluated
on two distinct medical imaging datasets, a brain tumor MRI dataset comprising
multiple tumor subtypes, and a Products of Conception histopathological dataset
containing four tissue categories. Experimental results demonstrate that
attention augmented CNNs consistently outperform baseline architectures across
all metrics. In particular, EfficientNetB5 with hybrid attention achieves the
highest overall performance, delivering substantial gains on both datasets.
Beyond improved classification accuracy, attention mechanisms enhance feature
localization, leading to better generalization across heterogeneous imaging
modalities. This work contributes a systematic comparative framework for
embedding attention modules in diverse CNN architectures and rigorously
assesses their impact across multiple medical imaging tasks. The findings
provide practical insights for the development of robust, interpretable, and
clinically applicable deep learning based decision support systems.

</details>


### [15] [Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset](https://arxiv.org/abs/2509.05348)
*Ashen Rodrigo,Isuru Munasinghe,Asanka Perera*

Main category: cs.CV

TL;DR: 论文通过自建COCO格式数据集比较了五种目标检测模型在太阳能电池板缺陷与污染检测上的精度与速度，揭示了精度与效率的权衡，为实际部署提供指导，并公开了数据集链接。


<details>
  <summary>Details</summary>
Motivation: 及时准确地检测电池板的物理与电气缺陷及表面污染（如灰尘、鸟粪）对维持光伏系统效率和可靠性至关重要，因此需要评估不同检测模型在该任务中的适用性。

Method: 构建了符合COCO格式的定制太阳能电池板缺陷与污染数据集并开发了训练/评估界面，训练并比较了YOLOv3、Faster R-CNN、RetinaNet、EfficientDet和Swin Transformer，使用mAP、precision、recall和推理速度等指标评估性能。

Result: 实验展示了各模型在检测准确率与计算开销上的不同表现，揭示了某些模型在高精度检测上的优势（可能是Faster R-CNN或Swin Transformer），而轻量或实时模型（如YOLOv3、EfficientDet）则在推理速度上更优，论文给出了各模型的相对优劣与适用场景建议。

Conclusion: 该论文比较评估了五种主流目标检测模型在太阳能电池板缺陷与污染检测任务上的表现，指出在精度与计算效率间存在权衡，为实际监测方案选择提供参考。

Abstract: Timely and accurate detection of defects and contaminants in solar panels is
critical for maintaining the efficiency and reliability of photovoltaic
systems. This study presents a comprehensive evaluation of five
state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,
EfficientDet, and Swin Transformer, for identifying physical and electrical
defects as well as surface contaminants such as dust, dirt, and bird droppings
on solar panels. A custom dataset, annotated in the COCO format and
specifically designed for solar panel defect and contamination detection, was
developed alongside a user interface to train and evaluate the models. The
performance of each model is assessed and compared based on mean Average
Precision (mAP), precision, recall, and inference speed. The results
demonstrate the trade-offs between detection accuracy and computational
efficiency, highlighting the relative strengths and limitations of each model.
These findings provide valuable guidance for selecting appropriate detection
approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at
https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.

</details>


### [16] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

TL;DR: 提出无人工标注的实例分割框架：MultiCut生成粗掩码→掩码过滤→超像素引导的hard/soft掩码损失训练→自训练与自适应损失提升，实验优于前史方法。


<details>
  <summary>Details</summary>
Motivation: 减少对人工标注的依赖，在无需人工标注情况下仍能实现高质量的实例分割，降低标注成本并提高方法泛化性。

Method: 1) 对自监督特征应用MultiCut算法得到粗分割掩码；2) 使用掩码过滤模块筛选高质量粗掩码；3) 结合超像素（低级图像特征）设计超像素引导的掩码损失，包括hard loss和soft loss，用于训练分割网络；4) 采用自训练策略并引入自适应损失进一步提升预测掩码质量。

Result: 在公开实例分割和目标检测数据集上的实验结果表明，该框架优于先前无监督/弱监督的方法，提升了掩码质量和检测性能。

Conclusion: 该论文提出了一个无需人工标注的实例分割框架，通过自监督特征的MultiCut生成粗掩码，进一步用掩码过滤和超像素引导损失优化训练，并结合自训练与自适应损失提升预测掩码质量。实验显示在实例分割和目标检测上优于之前方法。

Abstract: Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [17] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

TL;DR: 提出将结构保持神经网络与传统神经网络结合的混合模型，用于基于力学与视觉提取环境特征的细胞迁移与分裂预测，在模拟与真实数据上均表现良好。


<details>
  <summary>Details</summary>
Motivation: 细胞力学生物学过程复杂、多因素相互作用，现有方法难以同时兼顾机械驱动与外部环境影响，因而需一个能整合二者的预测模型以更准确推断细胞集体行为与分裂事件。

Method: 采用Structure Preserving Neural Networks（SPNN）对细胞作为纯机械系统的运动进行建模，同时用人工神经网络（ANN）处理来自计算机视觉提取的环境因素，两者融合进行轨迹roll-out预测；另行训练ANN用于基于相同观测特征的有丝分裂事件预测。

Result: 在模拟与真实细胞迁移数据上，模型能高精度回滚预测完整轨迹，并能合理预测有丝分裂事件，表明融合力学结构保持模型与数据驱动网络有助于捕捉复杂动力学。

Conclusion: 本论文提出了一种将结构保持神经网络与传统神经网络相结合的混合框架，用于从力学和环境特征共同建模细胞迁移与分裂事件，验证显示在模拟与真实数据上均能准确预测细胞轨迹与有较好分裂预测能力。

Abstract: Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>


### [18] [Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding](https://arxiv.org/abs/2509.05431)
*GodsGift Uzor,Tania-Amanda Nkoyo Fredrick Eneye,Chukwuebuka Ijezue*

Main category: cs.CV

TL;DR: 论文提出EMCAD解码器以降低计算开销并保持合理分割性能，在BraTs2020上取得中等Dice得分（约0.29），展示了稳定训练但性能仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有解码器在提高分割精度的同时通常带来高计算开销，实际部署受限设备或实时场景中难以应用，因此需要一种在性能与效率之间取得更好折衷的解码器设计。

Method: 作者设计了多尺度卷积和注意力机制结合的解码器模块，用于从编码器特征中重建分割图，重点优化计算资源受限场景下的推理成本。模型在BraTs2020数据集（369例患者MRI）上进行了训练与验证。

Result: 在BraTs2020上的初步结果为最佳Dice 0.31，训练过程中的平均Dice为0.285±0.015，模型在验证集上未出现明显过拟合迹象，整体性能属于中等水平。

Conclusion: 该论文提出了一种名为EMCAD的高效多尺度卷积注意力解码器，旨在平衡脑肿瘤分割任务中的性能与计算效率。

Abstract: Brain tumor segmentation is a critical pre-processing step in the medical
image analysis pipeline that involves precise delineation of tumor regions from
healthy brain tissue in medical imaging data, particularly MRI scans. An
efficient and effective decoding mechanism is crucial in brain tumor
segmentation especially in scenarios with limited computational resources.
However these decoding mechanisms usually come with high computational costs.
To address this concern EMCAD a new efficient multi-scale convolutional
attention decoder designed was utilized to optimize both performance and
computational efficiency for brain tumor segmentation on the BraTs2020 dataset
consisting of MRI scans from 369 brain tumor patients. The preliminary result
obtained by the model achieved a best Dice score of 0.31 and maintained a
stable mean Dice score of 0.285 plus/minus 0.015 throughout the training
process which is moderate. The initial model maintained consistent performance
across the validation set without showing signs of over-fitting.

</details>


### [19] [FAVAE-Effective Frequency Aware Latent Tokenizer](https://arxiv.org/abs/2509.05441)
*Tejaswini Medi,Hsien-Yi Wang,Arianna Rampini,Margret Keuper*

Main category: cs.CV

TL;DR: 本文指出现有潜在tokenizer偏向低频导致过度平滑，并提出基于小波的频率感知VAE，通过低高频解耦优化提升高频细节重建，显著改善生成图像的感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有潜在生成模型的tokenizer在联合优化时偏向低频信息，导致高频细节（纹理、边缘）丢失和图像过平滑，影响感知现实感，需引入频率感知的优化机制以改善高频重建。

Method: 在变分自编码器框架中引入小波变换，将图像分解为多尺度频带；对低频和高频分量采用解耦的编码器、解码器和损失项（例如重建损失、感知损失与频带特定正则化），并在训练中对不同频带加权以平衡频率重建；最终在潜在token生成器上输出频率增强的潜在表示供二阶段生成器使用。

Result: 通过频率分解分析验证了SOTA tokenizer的低频偏向；在多个数据集和基准上，FA-VAE在PSNR、LPIPS及主观视觉效果上均优于基线，尤其在高频纹理恢复和边缘清晰度方面有明显提升；并展示在内容创作、神经渲染和医学成像中的潜在应用价值。

Conclusion: 提出的FA-VAE通过在小波域分离低高频分量并分别优化，能显著提高高频细节重建，缓解现有潜在tokenizer偏向低频导致的过度平滑问题，从而提升重建感知质量。

Abstract: Latent generative models have shown remarkable progress in high-fidelity
image synthesis, typically using a two-stage training process that involves
compressing images into latent embeddings via learned tokenizers in the first
stage. The quality of generation strongly depends on how expressive and
well-optimized these latent embeddings are. While various methods have been
proposed to learn effective latent representations, the reconstructed images
often lack realism, particularly in textured regions with sharp transitions,
due to loss of fine details governed by high frequencies. We conduct a detailed
frequency decomposition of existing state-of-the-art (SOTA) latent tokenizers
and show that conventional objectives inherently prioritize low-frequency
reconstruction, often at the expense of high-frequency fidelity. Our analysis
reveals these latent tokenizers exhibit a bias toward low-frequency
information, when jointly optimized, leading to over-smoothed outputs and
visual artifacts that diminish perceptual quality. To address this, we propose
a wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework
that explicitly decouples the optimization of low- and high-frequency
components. This decoupling enables improved reconstruction of fine textures
while preserving global structure. Our approach bridges the fidelity gap in
current latent tokenizers and emphasizes the importance of frequency-aware
optimization for realistic image representation, with broader implications for
applications in content creation, neural rendering, and medical imaging.

</details>


### [20] [Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's](https://arxiv.org/abs/2509.05446)
*Iftekhar Haider Chowdhury,Zaed Ikbal Syed,Ahmed Faizul Haque Dhrubo,Mohammad Abdul Qayum*

Main category: cs.CV

TL;DR: 提出一种通过融合梯度、泰勒近似与激活KL散度不一致性的单次滤波器剪枝方法，计算高效（单次前反传），在大幅减少FLOPs同时保持近原始精度。


<details>
  <summary>Details</summary>
Motivation: 现有卷积网络剪枝方法多为迭代或复杂搜索，计算开销大且不稳定；单一指标可能无法可靠反映滤波器重要性。通过融合多种敏感度指标并量化其不一致性，可更鲁棒地识别结构不稳定或不重要的滤波器，实现一次性高效剪枝。

Method: DSFP对每个滤波器计算差分敏感度分数：融合基于梯度的敏感度、基于一阶泰勒展开的近似以及激活分布的KL散度三种指标的差异；应用指数缩放放大不同指标间不一致较大的滤波器分数；仅需一次前向-反向传播即可得到评分并执行剪枝，无需迭代或RL策略。

Result: 在50%~70%剪枝率范围内，DSFP能显著降低模型复杂度，最多达到80%以上的FLOPs减少；在70%剪枝时仍可保留高达98.23%的基线精度，优于传统启发式方法，在压缩率和泛化性上有提升。

Conclusion: 该论文提出了一种新的单次滤波器剪枝方法——差分敏感度融合剪枝（Differential Sensitivity Fusion Pruning，DSFP），通过融合多种重要性度量的不一致性来判断滤波器冗余，从而高效确定剪枝候选。

Abstract: Deep Convolutional Neural Networks have achieved state of the art performance
across various computer vision tasks, however their practical deployment is
limited by computational and memory overhead. This paper introduces
Differential Sensitivity Fusion Pruning, a novel single shot filter pruning
framework that focuses on evaluating the stability and redundancy of filter
importance scores across multiple criteria. Differential Sensitivity Fusion
Pruning computes a differential sensitivity score for each filter by fusing the
discrepancies among gradient based sensitivity, first order Taylor expansion,
and KL divergence of activation distributions. An exponential scaling mechanism
is applied to emphasize filters with inconsistent importance across metrics,
identifying candidates that are structurally unstable or less critical to the
model performance. Unlike iterative or reinforcement learning based pruning
strategies, Differential Sensitivity Fusion Pruning is efficient and
deterministic, requiring only a single forward-backward pass for scoring and
pruning. Extensive experiments across varying pruning rates between 50 to 70
percent demonstrate that Differential Sensitivity Fusion Pruning significantly
reduces model complexity, achieving over 80 percent Floating point Operations
Per Seconds reduction while maintaining high accuracy. For instance, at 70
percent pruning, our approach retains up to 98.23 percent of baseline accuracy,
surpassing traditional heuristics in both compression and generalization. The
proposed method presents an effective solution for scalable and adaptive Deep
Convolutional Neural Networks compression, paving the way for efficient
deployment on edge and mobile platforms.

</details>


### [21] [Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging](https://arxiv.org/abs/2509.05483)
*Jinhao Wang,Florian Vogl,Pascal Schütz,Saša Ćuković,William R. Taylor*

Main category: cs.CV

TL;DR: Veriserum：约11万张带自动/人工配准的双平面膝关节X光图像数据集，含标定工具和200张人工基准，公开可用，促进医疗图像配准与重建研究。


<details>
  <summary>Details</summary>
Motivation: 弥补外科相关双平面荧光透视分析中标注数据匮乏的问题，推动2D/3D配准、分割、畸变校正与三维重建等算法的发展与可重复评估。

Method: 收集约110,000张双平面膝关节植入物X光图像，覆盖10种植入物组合，在1600次试验中记录包含日常活动姿态；使用自动配准生成每张图像的姿态标注，并对200张图像提供人工配准作为基准；提供标定工具与数据存储和访问途径。

Result: 发布了名为Veriserum的开源数据集，包含大规模标注图像、双平面成像与标定工具，并提供在线永久存储与DOI，已经可用于训练深度模型与基准测试。

Conclusion: 该数据集为双平面X射线注册和相关计算机视觉任务提供了丰富、公开且带标注的实验素材，适合用于训练和评估深度学习与经典算法。

Abstract: Veriserum is an open-source dataset designed to support the training of deep
learning registration for dual-plane fluoroscopic analysis. It comprises
approximately 110,000 X-ray images of 10 knee implant pair combinations (2
femur and 5 tibia implants) captured during 1,600 trials, incorporating poses
associated with daily activities such as level gait and ramp descent. Each
image is annotated with an automatically registered ground-truth pose, while
200 images include manually registered poses for benchmarking.
  Key features of Veriserum include dual-plane images and calibration tools.
The dataset aims to support the development of applications such as 2D/3D image
registration, image segmentation, X-ray distortion correction, and 3D
reconstruction. Freely accessible, Veriserum aims to advance computer vision
and medical imaging research by providing a reproducible benchmark for
algorithm development and evaluation. The Veriserum dataset used in this study
is publicly available via
https://movement.ethz.ch/data-repository/veriserum.html, with the data stored
at ETH Z\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.

</details>


### [22] [An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures](https://arxiv.org/abs/2509.05490)
*Andrzej D. Dobrzycki,Ana M. Bernardos,José R. Casar*

Main category: cs.CV

TL;DR: 针对YOLOv8/v10的冻结策略无万能解，需依据数据特性选择：冻结主干适合保留通用特征，浅层冻结更能应对类别不平衡；合理冻结可减少显存并在部分情况下提升或不损失mAP@50；结合梯度与Grad-CAM分析可更好理解训练动态并指导配置选择。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备（如无人机）上部署YOLO需要高效迁移学习，但现有工作缺乏对不同冻结配置在现代YOLOv8/v10上的系统性研究，尤其缺少与数据特性和训练动态的关联分析。

Method: 系统地在YOLOv8与YOLOv10多个变体上测试多种冻结配置，使用四个关键基础设施监测数据集；结合L2范数梯度分析与Grad-CAM可视化解释训练动态；并记录显存使用、训练曲线与mAP@50等指标。

Result: 结果显示：冻结主干可保存通用特征、浅层冻结更适合极端类别不平衡；在特定配置下显存减少最多达28%，部分情况下mAP@50优于完全微调；梯度L2范数和收敛行为在中度冻结模型中表现出不同模式，支持这些发现；研究提供了针对不同数据特性的实用冻结策略建议。

Conclusion: 本文得出结论：没有单一的最优冻结策略，最佳冻结方式依赖于数据集特性；在某些场景（如保存通用特征或处理极端类别不平衡）冻结主干或浅层冻结能在降低显存占用的同时维持或提升mAP@50。

Abstract: The You Only Look Once (YOLO) architecture is crucial for real-time object
detection. However, deploying it in resource-constrained environments such as
unmanned aerial vehicles (UAVs) requires efficient transfer learning. Although
layer freezing is a common technique, the specific impact of various freezing
configurations on contemporary YOLOv8 and YOLOv10 architectures remains
unexplored, particularly with regard to the interplay between freezing depth,
dataset characteristics, and training dynamics. This research addresses this
gap by presenting a detailed analysis of layer-freezing strategies. We
systematically investigate multiple freezing configurations across YOLOv8 and
YOLOv10 variants using four challenging datasets that represent critical
infrastructure monitoring. Our methodology integrates a gradient behavior
analysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper
insights into training dynamics under different freezing strategies. Our
results reveal that there is no universal optimal freezing strategy but,
rather, one that depends on the properties of the data. For example, freezing
the backbone is effective for preserving general-purpose features, while a
shallower freeze is better suited to handling extreme class imbalance. These
configurations reduce graphics processing unit (GPU) memory consumption by up
to 28% compared to full fine-tuning and, in some cases, achieve mean average
precision (mAP@50) scores that surpass those of full fine-tuning. Gradient
analysis corroborates these findings, showing distinct convergence patterns for
moderately frozen models. Ultimately, this work provides empirical findings and
practical guidelines for selecting freezing strategies. It offers a practical,
evidence-based approach to balanced transfer learning for object detection in
scenarios with limited resources.

</details>


### [23] [Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection](https://arxiv.org/abs/2509.05512)
*Bryce Grant,Peng Wang*

Main category: cs.CV

TL;DR: 提出近似四元数网络（QUAN），用实值运算逼近四元数卷积并引入IQBN与空间注意力扩展，取得参数高效且具有旋转感知能力的分类与检测性能提升。


<details>
  <summary>Details</summary>
Motivation: 利用四元数固有的旋转表达能力，在保持几何不变性的同时降低实现复杂度和计算成本，以适配资源受限且需要旋转感知的视觉系统。

Method: 使用Hamilton乘积分解将四元数卷积近似为实值运算并以自定义CUDA内核实现；提出独立四元数批量归一化（IQBN）；将四元数扩展到空间注意力机制。

Result: 在CIFAR-10/100、ImageNet分类和COCO、DOTA检测任务上，QUAN以更少参数和更快收敛达到或超越常规模型与现有四元数模型，并在四元数CNN检测任务上达成SOTA。

Conclusion: QUAN通过近似四元数卷积并结合实值操作实现旋转等变性，在分类和目标检测任务上展示了参数效率和更好的旋转处理能力，适合资源受限的机器人视觉系统。

Abstract: This paper introduces Quaternion Approximate Networks (QUAN), a novel deep
learning framework that leverages quaternion algebra for rotation equivariant
image classification and object detection. Unlike conventional quaternion
neural networks attempting to operate entirely in the quaternion domain, QUAN
approximates quaternion convolution through Hamilton product decomposition
using real-valued operations. This approach preserves geometric properties
while enabling efficient implementation with custom CUDA kernels. We introduce
Independent Quaternion Batch Normalization (IQBN) for training stability and
extend quaternion operations to spatial attention mechanisms. QUAN is evaluated
on image classification (CIFAR-10/100, ImageNet), object detection (COCO,
DOTA), and robotic perception tasks. In classification tasks, QUAN achieves
higher accuracy with fewer parameters and faster convergence compared to
existing convolution and quaternion-based models. For objection detection, QUAN
demonstrates improved parameter efficiency and rotation handling over standard
Convolutional Neural Networks (CNNs) while establishing the SOTA for quaternion
CNNs in this downstream task. These results highlight its potential for
deployment in resource-constrained robotic systems requiring rotation-aware
perception and application in other domains.

</details>


### [24] [OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation](https://arxiv.org/abs/2509.05513)
*Ahad Jawaid,Yu Xiang*

Main category: cs.CV

TL;DR: OpenEgo：1107小时、多数据源、标准化手姿与意图对齐动作原语，支持语言条件的灵巧手模仿学习，推动egocentric操控研究。


<details>
  <summary>Details</summary>
Motivation: 现有第一视角视频数据集要么缺少细粒度、时间定位的动作描述，要么缺乏灵巧手部（dexterous hand）标注，限制了从egocentric视频学习复杂操控的进展。OpenEgo旨在填补这一缺口，降低学习灵巧操控的门槛并促进可复现研究。

Method: 作者将六个公开数据集统一收集并标准化手部姿态布局，构建时间戳化、描述性强的动作原语标签，合计1107小时、覆盖290个操作任务与600+环境，训练语言条件下的模仿学习策略来预测灵巧手轨迹以验证数据集效用。

Result: 数据集规模1107小时、290个任务、600+环境；提供标准化手部姿态与时间戳动作原语；通过训练语言条件的模仿学习策略成功预测灵巧手轨迹，展示了数据集在学习操控任务上的实用性。

Conclusion: OpenEgo 是一个规模大、标注全面的第一视角操作数据集，标准化了手部姿态并对动作进行了意图对齐的原子化标注，能推动基于视觉-语言-动作的模仿学习与可复现研究。

Abstract: Egocentric human videos provide scalable demonstrations for imitation
learning, but existing corpora often lack either fine-grained, temporally
localized action descriptions or dexterous hand annotations. We introduce
OpenEgo, a multimodal egocentric manipulation dataset with standardized
hand-pose annotations and intention-aligned action primitives. OpenEgo totals
1107 hours across six public datasets, covering 290 manipulation tasks in 600+
environments. We unify hand-pose layouts and provide descriptive, timestamped
action primitives. To validate its utility, we train language-conditioned
imitation-learning policies to predict dexterous hand trajectories. OpenEgo is
designed to lower the barrier to learning dexterous manipulation from
egocentric video and to support reproducible research in vision-language-action
learning. All resources and instructions will be released at
www.openegocentric.com.

</details>


### [25] [Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2509.05515)
*Sen Wang,Kunyi Li,Siyun Liang,Elena Alegret,Jing Ma,Nassir Navab,Stefano Gasperini*

Main category: cs.CV

TL;DR: VALA通过可见性筛选与流式几何中位数融合多视图语言特征，解决背景干扰与视图不一致性，提升3D高斯的语言驱动定位与分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有将2D开放词汇语言特征蒸馏到3D高斯的方法存在两大问题：背景高斯与前景高斯特征相同导致噪声，以及多视图语言嵌入含视角特定噪声导致不一致性。

Method: 计算每条光线中每个高斯的边际贡献并用可见性感知门控只保留可见高斯；在余弦空间使用流式加权几何中位数来融合多视图噪声嵌入。

Result: 方法能快速且内存高效地生成鲁棒、视图一致的语言特征嵌入，在开放词汇局部化与分割任务上优于现有方法。

Conclusion: 提出了Visibility-Aware Language Aggregation (VALA)，通过基于可见性过滤和流式加权几何中位数融合多视图语言特征，提升了3D高斯表示的视图一致性与局部化/分割性能。

Abstract: Recently, distilling open-vocabulary language features from 2D images into 3D
Gaussians has attracted significant attention. Although existing methods
achieve impressive language-based interactions of 3D scenes, we observe two
fundamental issues: background Gaussians contributing negligibly to a rendered
pixel get the same feature as the dominant foreground ones, and multi-view
inconsistencies due to view-specific noise in language embeddings. We introduce
Visibility-Aware Language Aggregation (VALA), a lightweight yet effective
method that computes marginal contributions for each ray and applies a
visibility-aware gate to retain only visible Gaussians. Moreover, we propose a
streaming weighted geometric median in cosine space to merge noisy multi-view
features. Our method yields a robust, view-consistent language feature
embedding in a fast and memory-efficient manner. VALA improves open-vocabulary
localization and segmentation across reference datasets, consistently
surpassing existing works.

</details>


### [26] [DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation](https://arxiv.org/abs/2509.05543)
*Haitao Tian,Pierre Payeur*

Main category: cs.CV

TL;DR: 提出DuoCLR：通过Shuffle and Warp生成跨序列排列，结合CPC与ROR两项替代任务在多尺度上进行对比学习预训练，从而增强骨架动作分割性能，在未裁剪数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的表征学习多针对动作识别并以序列级别孤立表示为主，无法充分利用跨序列与多尺度信息以提升动作分割性能，论文旨在通过预训练学习更适合分割任务的表征。

Method: 提出一种基于对比学习的预训练框架DuoCLR，核心包括多尺度表示与跨序列变体的利用；设计了新型数据增强‘Shuffle and Warp’用于生成多动作排列；引入两个替代任务：Cross Permutation Contrasting (CPC)用于对比相同动作类别在不同排列间的表示，和Relative Order Reasoning (ROR)用于预测两个排列之间的相对映射；总体在多尺度特征上联合优化。

Result: 在预训练于裁剪骨架数据并在未裁剪数据上评估时，DuoCLR相较于最新方法在多类与多标签动作分割任务上均表现出显著提升；论文还通过消融实验验证了各组件（Shuffle and Warp、CPC、ROR、多尺度表示等）的有效性。

Conclusion: 该论文通过在裁剪的单动作骨架序列上进行对比学习预训练，提出了DuoCLR框架，从而显著提升了在人类动作分割（尤其是未裁剪序列上的多类与多标签分割）任务上的表现。

Abstract: In this paper, a contrastive representation learning framework is proposed to
enhance human action segmentation via pre-training using trimmed (single
action) skeleton sequences. Unlike previous representation learning works that
are tailored for action recognition and that build upon isolated sequence-wise
representations, the proposed framework focuses on exploiting multi-scale
representations in conjunction with cross-sequence variations. More
specifically, it proposes a novel data augmentation strategy, 'Shuffle and
Warp', which exploits diverse multi-action permutations. The latter effectively
assists two surrogate tasks that are introduced in contrastive learning: Cross
Permutation Contrasting (CPC) and Relative Order Reasoning (ROR). In
optimization, CPC learns intra-class similarities by contrasting
representations of the same action class across different permutations, while
ROR reasons about inter-class contexts by predicting relative mapping between
two permutations. Together, these tasks enable a Dual-Surrogate Contrastive
Learning (DuoCLR) network to learn multi-scale feature representations
optimized for action segmentation. In experiments, DuoCLR is pre-trained on a
trimmed skeleton dataset and evaluated on an untrimmed dataset where it
demonstrates a significant boost over state-the-art comparatives in both
multi-class and multi-label action segmentation tasks. Lastly, ablation studies
are conducted to evaluate the effectiveness of each component of the proposed
approach.

</details>


### [27] [RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation](https://arxiv.org/abs/2509.05554)
*Yihong Leng,Siming Zheng,Jinwei Chen,Bo Li,Jiaojiao Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: Proposes RED: uses random masking on events and disentangled cross-modal attention to handle incomplete event data, leading to superior deblurring performance.


<details>
  <summary>Details</summary>
Motivation: Event streams are inherently incomplete due to DVS thresholding (sensitivity vs noise), degrading motion priors and limiting event-guided deblurring; need robustness to incomplete events.

Method: Introduce Robustness-Oriented Perturbation Strategy (random masking on events), disentangled OmniAttention modeling intra/inter-motion and cross-modality correlations, plus two interactive modules to enhance motion regions and inject semantics.

Result: RED outperforms prior methods on synthetic and real datasets in both accuracy and robustness.

Conclusion: The paper presents RED, a method improving event-guided deblurring by addressing event incompleteness via robustness training and disentangled attention, achieving state-of-the-art results.

Abstract: Event cameras provide sparse yet temporally high-temporal-resolution motion
information, demonstrating great potential for motion deblurring. Existing
methods focus on cross-modal interaction, overlooking the inherent
incompleteness of event streams, which arises from the trade-off between
sensitivity and noise introduced by the thresholding mechanism of Dynamic
Vision Sensors (DVS). Such degradation compromises the integrity of motion
priors and limits the effectiveness of event-guided deblurring. To tackle these
challenges, we propose a Robust Event-guided Deblurring (RED) network with
modality-specific disentangled representation. First, we introduce a
Robustness-Oriented Perturbation Strategy (RPS) that applies random masking to
events, which exposes RED to incomplete patterns and then foster robustness
against various unknown scenario conditions.Next, a disentangled OmniAttention
is presented to explicitly model intra-motion, inter-motion, and cross-modality
correlations from two inherently distinct but complementary sources: blurry
images and partially disrupted events. Building on these reliable features, two
interactive modules are designed to enhance motion-sensitive areas in blurry
images and inject semantic context into incomplete event representations.
Extensive experiments on synthetic and real-world datasets demonstrate RED
consistently achieves state-of-the-art performance in both accuracy and
robustness.

</details>


### [28] [Sensitivity-Aware Post-Training Quantization for Deep Neural Networks](https://arxiv.org/abs/2509.05576)
*Zekang Zheng,Haokun Li,Yaofo Chen,Mingkui Tan,Qing Du*

Main category: cs.CV

TL;DR: A sensitivity-guided, column-clustered PTQ method uses selective quantization and a shared inverse-Hessian update to achieve order-of-magnitude speedups while keeping accuracy loss under 0.3%.


<details>
  <summary>Details</summary>
Motivation: Reduce the computation and resource overhead of iterative PTQ methods so quantization becomes practical for edge and real-time applications without sacrificing accuracy.

Method: Perform parameter sensitivity analysis, prioritize quantizing high-sensitivity parameters while leaving low-sensitivity ones unquantized to compensate errors; exploit column-wise clustering to enable row-parallel quantization and use a globally shared inverse Hessian matrix update to cut computational cost by ~10x.

Result: On ResNet-50 and YOLOv5s, method yields 20-200x faster quantization than Optimal Brain Quantization, with mean accuracy loss under 0.3%.

Conclusion: The paper concludes that sensitivity-guided PTQ with column-wise clustering and a shared inverse Hessian update can dramatically reduce computation while preserving accuracy, achieving large speedups with minimal accuracy loss.

Abstract: Model quantization reduces neural network parameter precision to achieve
compression, but often compromises accuracy. Existing post-training
quantization (PTQ) methods employ iterative parameter updates to preserve
accuracy under high compression ratios, incurring significant computational
complexity and resource overhead, which limits applicability in
resource-constrained edge computing and real-time inference scenarios. This
paper proposes an efficient PTQ method guided by parameter sensitivity
analysis. The approach prioritizes quantization of high-sensitivity parameters,
leveraging unquantized low-sensitivity parameters to compensate for
quantization errors, thereby mitigating accuracy degradation. Furthermore, by
exploiting column-wise clustering of parameter sensitivity, the method
introduces a row-parallel quantization framework with a globally shared inverse
Hessian matrix update mechanism, reducing computational complexity by an order
of magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a
20-200-fold quantization speedup over the Optimal Brain Quantization baseline,
with mean accuracy loss below 0.3%, confirming the method's efficacy in
balancing efficiency and accuracy.

</details>


### [29] [Reconstruction and Reenactment Separated Method for Realistic Gaussian Head](https://arxiv.org/abs/2509.05582)
*Zhiling Ye,Cong Zhou,Xiubao Zhang,Haifeng Shen,Weihong Deng,Quan Lu*

Main category: cs.CV

TL;DR: 提出基于WebSSL的one-shot 3D Gaussian头生成与重建-驱动分离框架，采用两阶段训练，实现高保真高帧率可控头像生成，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升单张人像生成可控3D头像的通用性与细节还原能力，同时保证实时高帧率驱动渲染的效率。

Method: 基于WebSSL构建大规模one-shot Gaussian head生成器，采用两阶段训练（可能为预训练+微调）提升泛化与高频细节；分离重建模块和驱动模块，推理时只用轻量驱动器进行控制。

Result: 实现512x512分辨率下90 FPS实时渲染；扩展参数规模的重建模块能提升性能（符合缩放律）；在多项定量与定性实验中超越现有SOTA方法。

Conclusion: 该论文提出了一个“重建-驱动分离”的单张图像可控3D Gaussian 头像生成框架，通过大规模one-shot生成器和两阶段训练，实现了良好的泛化和高频纹理重建能力，并在推理时使用超轻量高帧率avatar实现实时渲染。

Abstract: In this paper, we explore a reconstruction and reenactment separated
framework for 3D Gaussians head, which requires only a single portrait image as
input to generate controllable avatar. Specifically, we developed a large-scale
one-shot gaussian head generator built upon WebSSL and employed a two-stage
training approach that significantly enhances the capabilities of
generalization and high-frequency texture reconstruction. During inference, an
ultra-lightweight gaussian avatar driven by control signals enables high
frame-rate rendering, achieving 90 FPS at a resolution of 512x512. We further
demonstrate that the proposed framework follows the scaling law, whereby
increasing the parameter scale of the reconstruction module leads to improved
performance. Moreover, thanks to the separation design, driving efficiency
remains unaffected. Finally, extensive quantitative and qualitative experiments
validate that our approach outperforms current state-of-the-art methods.

</details>


### [30] [MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios](https://arxiv.org/abs/2509.05592)
*Changtao Miao,Yi Zhang,Man Luo,Weiwei Feng,Kaiyuan Zheng,Qi Chu,Tao Gong,Jianshu Li,Yunfeng Diao,Wei Zhou,Joey Tianyi Zhou,Xiaoshuai Hao*

Main category: cs.CV

TL;DR: 作者构建了包含50种伪造技术和1024K样本的MFFI数据集，通过多维策略增强真实感并在基准上显著提升了跨域泛化与场景复杂性，推动Deepfake检测面向现实场景的研究。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake数据集在真实世界多样性和复杂性方面不足，限制检测模型的泛化和稳健性，因此需要一个更接近真实传播条件的多维数据集来促进检测技术发展。

Method: 通过构建包含50种不同伪造方法、涵盖多种面部场景、整合多源真实数据并设计多级退化操作的策略来生成1024K图像样本的数据集；并在基准检测任务上评估数据集的场景复杂性、跨域泛化性和难度梯度。

Result: MFFI拥有1024K图像、50种伪造方法，并在基准评测中展现出比现有公开数据集更强的场景复杂度、跨域泛化能力和检测难度梯度。

Conclusion: 该论文提出了一个面向真实场景的多维人脸伪造图像数据集MFFI，旨在弥补现有Deepfake数据集在伪造方法、场景变化、真实数据多样性和传播退化方面的不足，提升检测方法的泛化能力和难度等级测试能力。

Abstract: Rapid advances in Artificial Intelligence Generated Content (AIGC) have
enabled increasingly sophisticated face forgeries, posing a significant threat
to social security. However, current Deepfake detection methods are limited by
constraints in existing datasets, which lack the diversity necessary in
real-world scenarios. Specifically, these data sets fall short in four key
areas: unknown of advanced forgery techniques, variability of facial scenes,
richness of real data, and degradation of real-world propagation. To address
these challenges, we propose the Multi-dimensional Face Forgery Image
(\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances
realism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied
Facial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation
Operations. MFFI integrates $50$ different forgery methods and contains $1024K$
image samples. Benchmark evaluations show that MFFI outperforms existing public
datasets in terms of scene complexity, cross-domain generalization capability,
and detection difficulty gradients. These results validate the technical
advance and practical utility of MFFI in simulating real-world conditions. The
dataset and additional details are publicly available at
{https://github.com/inclusionConf/MFFI}.

</details>


### [31] [Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization](https://arxiv.org/abs/2509.05604)
*Jungin Park,Jiyoung Lee,Kwanghoon Sohn*

Main category: cs.CV

TL;DR: 将对象与帧构造成时空图并注入语言查询，通过递归图神经网络细化表示，实现了语义驱动的视频关键帧摘要，取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于帧间的时间建模忽略了细粒度视觉实体（如对象）的语义关系，且语言引导的摘要需要更全面的语言理解，因此希望通过时空图与语言融合解决语义关联建模问题。

Method: 构建两类图：空间图（对象节点）和时间图（帧节点），节点间通过边传递语义关系；将从视频生成的语言查询编码并注入到节点表示以避免仅靠视觉相似性建立边；采用递归策略反复细化初始图结构与节点表示，最后对帧节点进行关键帧分类；支持监督与无监督训练。

Result: 在多项公共基准（通用与查询驱动的视频摘要）上，VideoGraph在有监督和无监督设置下均达到或超越现有最先进方法的性能，代码开源。

Conclusion: 该文提出VideoGraph，通过递归时空图建模，将对象与帧分别作为空间与时间图的节点，并引入语言查询融入节点表示以传递语义信息，从而改善视频摘要性能，最终在多项基准上取得了最先进的结果。

Abstract: Video summarization aims to select keyframes that are visually diverse and
can represent the whole story of a given video. Previous approaches have
focused on global interlinkability between frames in a video by temporal
modeling. However, fine-grained visual entities, such as objects, are also
highly related to the main content of the video. Moreover, language-guided
video summarization, which has recently been studied, requires a comprehensive
linguistic understanding of complex real-world videos. To consider how all the
objects are semantically related to each other, this paper regards video
summarization as a language-guided spatiotemporal graph modeling problem. We
present recursive spatiotemporal graph networks, called VideoGraph, which
formulate the objects and frames as nodes of the spatial and temporal graphs,
respectively. The nodes in each graph are connected and aggregated with graph
edges, representing the semantic relationships between the nodes. To prevent
the edges from being configured with visual similarity, we incorporate language
queries derived from the video into the graph node representations, enabling
them to contain semantic knowledge. In addition, we adopt a recursive strategy
to refine initial graphs and correctly classify each frame node as a keyframe.
In our experiments, VideoGraph achieves state-of-the-art performance on several
benchmarks for generic and query-focused video summarization in both supervised
and unsupervised manners. The code is available at
https://github.com/park-jungin/videograph.

</details>


### [32] [Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning](https://arxiv.org/abs/2509.05606)
*Juan Yeo,Ijun Jang,Taesup Kim*

Main category: cs.CV

TL;DR: 提出PaKA，通过核对齐教师-学生的稠密patch关系并配合稠密特征专用增强，成功将全局语义迁移到局部特征，提升稠密视觉任务性能并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法侧重全局表征，缺乏对局部语义和空间精度的建模，导致在密集预测任务上效果不佳，需要一种能将语义知识注入稠密特征的办法。

Method: 提出Patch-level Kernel Alignment（PaKA）损失，通过对教师与学生模型的稠密patch特征分布进行对齐，匹配patch间的结构关系；并设计了针对稠密表征学习的数据增强策略。

Result: 在多种稠密视觉基准上取得了SOTA级别的结果，验证了PaKA和特殊增强策略在提升局部表征与下游任务性能上的有效性。

Conclusion: 该论文提出了在预训练表征基础上，通过额外的自监督学习将语义知识迁移到稠密特征空间，从而提升稠密视觉任务的表现。

Abstract: Dense representations are essential for vision tasks that require spatial
precision and fine-grained detail. While most self-supervised representation
learning methods focus on global representations that summarize the image as a
whole, such approaches often fall short in capturing the localized semantics
necessary for dense prediction tasks. To overcome these limitations, we propose
a framework that builds on pretrained representations through additional
self-supervised learning, aiming to transfer existing semantic knowledge into
the dense feature space. Our method aligns the distributions of dense features
between a teacher and a student model. Specifically, we introduce Patch-level
Kernel Alignment (PaKA), a simple yet effective alignment objective that
captures statistical dependencies, thereby matching the structural
relationships of dense patches across the two models. In addition, we
investigate augmentation strategies specifically designed for dense
representation learning. Our framework achieves state-of-the-art results across
a variety of dense vision benchmarks, demonstrating the effectiveness of our
approach.

</details>


### [33] [SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning](https://arxiv.org/abs/2509.05614)
*Hanzhen Wang,Jiaming Xu,Jiayi Pan,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: 提出无训练的SpecPrune-VLA：结合动作级静态剪枝、层级动态剪枝与动作感知控制器，利用历史与当前信息实现对VLA模型的有效剪枝，带来约1.5×推理加速且几乎不降低成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA剪枝方法仅使用当前动作的局部信息，忽视了动作间高相似性和历史全局信息，导致成功率显著下降和速度提升受限。通过融合全局与局部信息可更智能地选择保留token，减少性能损失。

Method: 两级剪枝（动作级静态剪枝与层级动态剪枝）结合轻量级动作感知控制器。静态剪枝利用历史全局信息与当前局部信息减少每个动作的视觉token数；动态剪枝根据每层的重要性逐层剪枝；控制器基于动作的粗细粒度调节剪枝激进度。

Result: 在LIBERO数据集上，相比OpenVLA-OFT，SpecPrune-VLA在NVIDIA A800上实现1.46×加速，在RTX 3090上实现1.57×加速，同时成功率损失可以忽略。

Conclusion: SpecPrune-VLA在保持任务成功率几乎不变的情况下，实现了显著的推理加速，是针对VLA模型的有效无训练剪枝策略。

Abstract: Pruning accelerates compute-bound models by reducing computation. Recently
applied to Vision-Language-Action (VLA) models, existing methods prune tokens
using only local info from current action, ignoring global context from prior
actions, causing >20% success rate drop and limited speedup. We observe high
similarity across consecutive actions and propose leveraging both local
(current) and global (past) info for smarter token selection. We introduce
SpecPrune-VLA, a training-free method with two-level pruning and heuristic
control: (1) Static pruning at action level: uses global history and local
context to reduce visual tokens per action; (2) Dynamic pruning at layer level:
prunes tokens per layer based on layer-specific importance; (3) Lightweight
action-aware controller: classifies actions as coarse/fine-grained (by speed),
adjusting pruning aggressiveness since fine-grained actions are
pruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times
speedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.
OpenVLA-OFT, with negligible success rate loss.

</details>


### [34] [SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.05625)
*Kien Nguyen,Anh Tran,Cuong Pham*

Main category: cs.CV

TL;DR: SuMa maps target concept subspace to a reference subspace to robustly erase narrow concepts while preserving image quality, outperforming or matching SOTA on multiple erasure tasks.


<details>
  <summary>Details</summary>
Motivation: Existing concept erasure methods either sacrifice robustness or image quality and fail on narrow concepts like copyrighted characters; need finer-grained manipulation.

Method: Derive target subspace for the concept then map it to a reference subspace minimizing distance between them; neutralizes target concept while preserving image quality.

Result: Across subclass, celebrity, artistic style, and instance erasure tasks, SuMa matches image quality of effectiveness-focused methods and completeness of robustness-focused methods.

Conclusion: SuMa successfully erases narrow concepts by mapping the target subspace to a reference subspace, achieving both robustness and image quality.

Abstract: The rapid growth of text-to-image diffusion models has raised concerns about
their potential misuse in generating harmful or unauthorized contents. To
address these issues, several Concept Erasure methods have been proposed.
However, most of them fail to achieve both robustness, i.e., the ability to
robustly remove the target concept., and effectiveness, i.e., maintaining image
quality. While few recent techniques successfully achieve these goals for NSFW
concepts, none could handle narrow concepts such as copyrighted characters or
celebrities. Erasing these narrow concepts is critical in addressing copyright
and legal concerns. However, erasing them is challenging due to their close
distances to non-target neighboring concepts, requiring finer-grained
manipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel
method specifically designed to achieve both robustness and effectiveness in
easing these narrow concepts. SuMa first derives a target subspace representing
the concept to be erased and then neutralizes it by mapping it to a reference
subspace that minimizes the distance between the two. This mapping ensures the
target concept is robustly erased while preserving image quality. We conduct
extensive experiments with SuMa across four tasks: subclass erasure, celebrity
erasure, artistic style erasure, and instance erasure and compare the results
with current state-of-the-art methods. Our method achieves image quality
comparable to approaches focused on effectiveness, while also yielding results
that are on par with methods targeting completeness.

</details>


### [35] [Self-supervised Learning for Hyperspectral Images of Trees](https://arxiv.org/abs/2509.05630)
*Moqsadur Rahman,Saurav Kumar,Santosh S. Palmate,M. Shahriar Hossain*

Main category: cs.CV

TL;DR: 用自监督学习从无人机高光谱图像学习植被相关嵌入，构建的树木表示在下游任务上优于直接使用高光谱植被特征。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感在精准农业中很有价值，但标注稀缺或缺失时，如何从高光谱数据中提取有意义的树木/植被表征是一个挑战，促使研究自监督表征学习方法。

Method: 作者采用自监督学习框架，对高光谱图像进行无标签表征学习，生成与植被属性相关的嵌入空间，并以该嵌入为树木表征用于下游机器学习任务评价。

Result: 实验表明，基于所构建的植被属性相关嵌入空间的树木表示，在下游任务（例如分类或回归）中表现优于直接使用传统高光谱植被属性作为表征。

Conclusion: 本文提出的自监督学习方法能从无人机高光谱图像中构建反映植被特性（树冠级别）的嵌入表示，并在下游任务中优于直接使用高光谱植被特征。

Abstract: Aerial remote sensing using multispectral and RGB imagers has provided a
critical impetus to precision agriculture. Analysis of the hyperspectral images
with limited or no labels is challenging. This paper focuses on self-supervised
learning to create neural network embeddings reflecting vegetation properties
of trees from aerial hyperspectral images of crop fields. Experimental results
demonstrate that a constructed tree representation, using a vegetation
property-related embedding space, performs better in downstream machine
learning tasks compared to the direct use of hyperspectral vegetation
properties as tree representations.

</details>


### [36] [Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh](https://arxiv.org/abs/2509.05652)
*Ha Meem Hossain,Pritam Nath,Mahitun Nesa Mahi,Imtiaz Uddin,Ishrat Jahan Eiste,Syed Nasibur Rahman Ratul,Md Naim Uddin Mozumdar,Asif Mohammed Saad*

Main category: cs.CV

TL;DR: 针对孟加拉国29类本地化车辆数据集评估六种YOLO模型，YOLOv11x精度最高但推理慢，中等模型（YOLOv8m/YOLOv11m）在精度与速度间折中最佳；数据不均衡导致少数类检测极差，类别相似性引发混淆。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有在非孟加拉训练的数据集对孟加拉特有车辆与道路场景识别不足，影响自动驾驶与智能交通在发展中国家应用的可靠性，需要构建本地化数据集并评估主流检测模型的适用性。

Method: 本文构建了一个包含29类车辆的本地化高分辨率（1920x1080）数据集，使用LabelImg按YOLO格式手工标注边界框。比较了六个YOLO变体的性能，指标包括mAP@0.5、mAP@0.5:0.95、召回率、F1-score及每张图像推理时间。还通过混淆矩阵分析类别混淆和错误来源。

Result: 主要结果：YOLOv11x取得最高mAP@0.5=63.7%、mAP@0.5:0.95=43.8%、召回61.4%、F1=61.6%，推理时间45.8ms；YOLOv8m与YOLOv11m在约14-15ms推理时间下，mAP@0.5分别为62.5%和61.8%，为性能与速度的折中选择。少数类如Construction Vehicles与Desi Nosimon几乎无法识别。混淆矩阵显示Mini Truck与Mini Covered Van等视觉相似类别易混淆。

Conclusion: 本文结论是：在本地化数据集上训练的YOLO系列模型能够显著提升孟加拉国道路环境下的车辆检测性能，其中YOLOv11x表现最优，但计算开销较大；中等模型（YOLOv8m、YOLOv11m）在精度与推理时间间权衡较好；少数类（如Construction Vehicles、Desi Nosimon）检测性能极差，主要受限于数据不均衡与样本不足。

Abstract: Vehicle detection systems trained on Non-Bangladeshi datasets struggle to
accurately identify local vehicle types in Bangladesh's unique road
environments, creating critical gaps in autonomous driving technology for
developing regions. This study evaluates six YOLO model variants on a custom
dataset featuring 29 distinct vehicle classes, including region-specific
vehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and
``CNG''. The dataset comprises high-resolution images (1920x1080) captured
across various Bangladeshi roads using mobile phone cameras and manually
annotated using LabelImg with YOLO format bounding boxes. Performance
evaluation revealed YOLOv11x as the top performer, achieving 63.7\% mAP@0.5,
43.8\% mAP@0.5:0.95, 61.4\% recall, and 61.6\% F1-score, though requiring 45.8
milliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)
struck an optimal balance, delivering robust detection performance with mAP@0.5
values of 62.5\% and 61.8\% respectively, while maintaining moderate inference
times around 14-15 milliseconds. The study identified significant detection
challenges for rare vehicle classes, with Construction Vehicles and Desi
Nosimons showing near-zero accuracy due to dataset imbalances and insufficient
training samples. Confusion matrices revealed frequent misclassifications
between visually similar vehicles, particularly Mini Trucks versus Mini Covered
Vans. This research provides a foundation for developing robust object
detection systems specifically adapted to Bangladesh traffic conditions,
addressing critical needs in autonomous vehicle technology advancement for
developing regions where conventional generic-trained models fail to perform
adequately.

</details>


### [37] [EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation](https://arxiv.org/abs/2509.05659)
*Guandong Li,Zhaobin Chu*

Main category: cs.CV

TL;DR: EditIDv2通过改进ID特征整合与训练策略，在无需微调且只用少量数据润滑的情况下，提升了复杂叙事长文本场景中人物编辑的可编辑性与身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有人物编辑方法在简单提示下表现良好，但在包含多语义层、时序逻辑和复杂上下文关系的长文本叙事场景中，会出现编辑能力下降、语义理解偏差和身份一致性破坏。为了解决这些问题，提出EditIDv2以增强长文本高复杂度场景下的可编辑性与身份保持。

Method: 对PerceiverAttention进行精巧分解；引入ID损失并与扩散模型进行联合动态训练；设计离线融合的ID特征整合模块；使用少量数据润滑（data lubrication）来注入可编辑性。

Result: 在复杂叙事和长提示场景下，EditIDv2实现了深度、多层次的语义编辑并维持身份一致性，满足长提示和高质量图像生成需求，在IBench评测中取得优秀结果。

Conclusion: EditIDv2在复杂叙事和长文本场景下提供了无需微调的图像人物编辑方案，通过改进ID特征整合模块、引入ID损失、PerceiverAttention分解、与扩散模型联合动态训练以及离线融合策略，实现了深层多级语义编辑并保持身份一致性，适用于长提示和高质量生成，在IBench上表现优异。

Abstract: We propose EditIDv2, a tuning-free solution specifically designed for
high-complexity narrative scenes and long text inputs. Existing character
editing methods perform well under simple prompts, but often suffer from
degraded editing capabilities, semantic understanding biases, and identity
consistency breakdowns when faced with long text narratives containing multiple
semantic layers, temporal logic, and complex contextual relationships. In
EditID, we analyzed the impact of the ID integration module on editability. In
EditIDv2, we further explore and address the influence of the ID feature
integration module. The core of EditIDv2 is to discuss the issue of editability
injection under minimal data lubrication. Through a sophisticated decomposition
of PerceiverAttention, the introduction of ID loss and joint dynamic training
with the diffusion model, as well as an offline fusion strategy for the
integration module, we achieve deep, multi-level semantic editing while
maintaining identity consistency in complex narrative environments using only a
small amount of data lubrication. This meets the demands of long prompts and
high-quality image generation, and achieves excellent results in the IBench
evaluation.

</details>


### [38] [OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation](https://arxiv.org/abs/2509.05661)
*Xiaomeng Zhu,Changwei Wang,Haozhe Wang,Xinyu Liu,Fangzhen Lin*

Main category: cs.CV

TL;DR: 将场景图预测拆分为视觉捕获和语言预测两步，提出面对对象的两阶段LLM方法（先预测物体出入，再生成关系），显著提升长期预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景图预测主要依赖视觉线索，难以融入常识知识，导致长期预测不稳定。通过将预测转为纯文本并利用LLM的常识推理能力，可以增强长期预测鲁棒性。

Method: 提出OOTSM：先由场景图捕获模型将视频转换为场景图序列；然后用微调的开源大语言模型进行两阶段文本预测——阶段一预测物体出现/消失，阶段二生成人-物关系；在SGA上与STTran++结合进行端到端评估。

Result: 在Action Genome构造的基准上，微调的开源LLM在LSGA对比零-shot API（GPT-4o等）表现良好；与STTran++结合的SGA任务上，短期mean-Recall(@10)提升3.4%，长期mean-Recall(@50)显著提升21.9%。

Conclusion: 本文提出了一种将场景图预测任务解耦为场景图捕获与纯文本预测两步的新思路，重点研究第二步的语言化场景图预测（LSGA），通过面向对象的两阶段方法（OOTSM）先预测物体的出现/消失再生成详细的人-物关系，从而更好地利用常识知识，提高长期预测表现。

Abstract: A scene graph is a structured represention of objects and their relationships
in a scene. Scene Graph Anticipation (SGA) involves predicting future scene
graphs from video clips, enabling applications as intelligent surveillance and
human-machine collaboration. Existing SGA approaches primarily leverage visual
cues, often struggling to integrate valuable commonsense knowledge, thereby
limiting long-term prediction robustness. To explicitly leverage such
commonsense knowledge, we propose a new approach to better understand the
objects, concepts, and relationships in a scene graph. Our approach decouples
the SGA task in two steps: first a scene graph capturing model is used to
convert a video clip into a sequence of scene graphs, then a pure text-based
model is used to predict scene graphs in future frames. Our focus in this work
is on the second step, and we call it Linguistic Scene Graph Anticipation
(LSGA) and believes it should have independent interest beyond the use in SGA
discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method
(OOTSM) where an Large Language Model (LLM) first forecasts object appearances
and disappearances before generating detailed human-object relations. We
conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we
evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,
GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome
annotations. For SGA, we combine our OOTSM with STTran++ from, and our
experiments demonstrate effective state-of-the-art performance: short-term
mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves
dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.

</details>


### [39] [WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising](https://arxiv.org/abs/2509.05662)
*Wasikul Islam*

Main category: cs.CV

TL;DR: Embed pileup-mitigation priors (conservation, locality, isolation) into CNN/UNet (WIPUNet) to gain denoising robustness under strong Gaussian noise; competitive at low noise and superior at high noise on CIFAR-10 and BSD500.


<details>
  <summary>Details</summary>
Motivation: Pileup in particle physics obscures signals; mitigation uses physical priors (conservation, locality, isolation). Authors ask whether embedding analogous priors into neural nets improves image denoising robustness under heavy corruption.

Method: Design hierarchy of denoisers: residual CNN with conservation constraints, Gaussian-noise variants, and WIPUNet (Weighted Inductive Pileup-physics-inspired U-Net) integrating conservation, locality, isolation into UNet architecture; evaluate on CIFAR-10 and BSD500 with varying Gaussian noise levels.

Result: PU-inspired CNNs competitive with baselines at moderate noise; WIPUNet outperforms increasingly as noise grows, showing stability where data-driven models degrade; demonstrated on CIFAR-10 and BSD500 across sigma={15,25,50,75,100}.

Conclusion: Physics-inspired priors (from pileup mitigation) can improve denoising robustness under strong corruption, especially with WIPUNet showing widening gains at high noise levels.

Abstract: In high-energy particle physics, collider measurements are contaminated by
"pileup", overlapping soft interactions that obscure the hard-scatter signal of
interest. Dedicated subtraction strategies exploit physical priors such as
conservation, locality, and isolation. Inspired by this analogy, we investigate
how such principles can inform image denoising by embedding physics-guided
inductive biases into neural architectures. This paper is a proof of concept:
rather than targeting state-of-the-art (SOTA) benchmarks, we ask whether
physics-inspired priors improve robustness under strong corruption.
  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with
conservation constraints, its Gaussian-noise variants, and the Weighted
Inductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which
integrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at
$\sigma\in\{15,25,50,75,100\}$, PU-inspired CNNs are competitive with standard
baselines, while WIPUNet shows a \emph{widening margin} at higher noise.
Complementary BSD500 experiments show the same trend, suggesting
physics-inspired priors provide stability where purely data-driven models
degrade. Our contributions are: (i) translating pileup-mitigation principles
into modular inductive biases; (ii) integrating them into UNet; and (iii)
demonstrating robustness gains at high noise without relying on heavy SOTA
machinery.

</details>


### [40] [Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance](https://arxiv.org/abs/2509.05669)
*Weijie Shen,Xinrui Wang,Yuanqi Nie,Apiradee Boonmee*

Main category: cs.CV

TL;DR: CAMVR通过VCMU记忆和AVFG视觉关注引导，提升LVLM多轮视觉-文本推理的连贯性与准确性，并在多项数据集上达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM/LVLM在单轮任务表现优异，但在需要深度上下文理解与复杂视觉推理的多轮场景中表现欠佳，出现上下文遗失与断裂式推理，导致回答不一致或产生幻觉，故需设计能维护历史视觉-文本上下文并引导视觉关注的机制。

Method: 提出VCMU（Visual-Textual Context Memory Unit）实现动态读写存储跨模态特征与对应关系，并通过AVFG（Adaptive Visual Focus Guidance）基于记忆动态调整视觉编码器的注意力，结合多层次推理融合策略在生成阶段使用累积上下文约束输出。

Result: 在VisDial、改编的A-OKVQA及作者构建的MTIF数据集上，CAMVR在多项评价指标上均超过现有最优方法，呈现更高的准确性、一致性与鲁棒性，验证了其在复杂多轮视觉-文本任务中的有效性。

Conclusion: CAMVR通过引入持久的视觉-文本上下文记忆单元和自适应视觉关注引导机制，有效解决了LVLM在多轮交互中出现的上下文丢失、碎片化推理和幻觉问题，从而显著提升多轮视觉-文本推理的一致性与准确性。

Abstract: Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)
excel in single-turn tasks but face significant challenges in multi-turn
interactions requiring deep contextual understanding and complex visual
reasoning, often leading to fragmented reasoning, context loss, and
hallucinations. To address these limitations, we propose Context-Aware
Multi-Turn Visual Reasoning (CAMVR), a novel framework designed to empower
LVLMs with robust and coherent multi-turn visual-textual inference
capabilities. CAMVR introduces two key innovations: a Visual-Textual Context
Memory Unit (VCMU), a dynamic read-write memory network that stores and manages
critical visual features, textual semantic representations, and their
cross-modal correspondences from each interaction turn; and an Adaptive Visual
Focus Guidance (AVFG) mechanism, which leverages the VCMU's context to
dynamically adjust the visual encoder's attention to contextually relevant
image regions. Our multi-level reasoning integration strategy ensures that
response generation is deeply coherent with both current inputs and accumulated
historical context. Extensive experiments on challenging datasets, including
VisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following
(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art
performance.

</details>


### [41] [MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics](https://arxiv.org/abs/2509.05670)
*Gašper Podobnik,Tomaž Vrtovec*

Main category: cs.CV

TL;DR: 针对距离度量实现陷阱，作者提出基于网格的MeshMetrics工具，提高评价的精度与可复现性，已开源。


<details>
  <summary>Details</summary>
Motivation: 当前图像分割评估中的距离类指标（如Hausdorff距离、归一化表面距离）在实现上存在陷阱，导致不同开源工具对同一分割结果报告出显著不同的数值，从而影响可复现性。

Method: 提出MeshMetrics，一个将分割体表面转换为三角网格并在网格上直接计算距离度量的方法；通过理论分析和实验证明其优于传统基于栅格的方法，减少量化误差对距离计算的影响。

Result: MeshMetrics在准确性和精确性上优于已有工具，显著降低了离散化伪影的影响；示例中不同工具的Hausdorff距离误差可超过100 mm，归一化表面距离可差30%pt，MeshMetrics减小了这些偏差。

Conclusion: 本文指出基于像素网格的距离度量在实现上存在严重可复现性问题，并提出基于网格三角网格（mesh）的度量框架以提高精度和稳定性。

Abstract: The surge of research in image segmentation has yielded remarkable
performance gains but also exposed a reproducibility crisis. A major
contributor is performance evaluation, where both selection and implementation
of metrics play critical roles. While recent efforts have improved the former,
the reliability of metric implementation has received far less attention.
Pitfalls in distance-based metric implementation can lead to considerable
discrepancies between common open-source tools, for instance, exceeding 100 mm
for the Hausdorff distance and 30%pt for the normalized surface distance for
the same pair of segmentations. To address these pitfalls, we introduce
MeshMetrics, a mesh-based framework that provides a more precise computation of
distance-based metrics than conventional grid-based approaches. Through
theoretical analysis and empirical validation, we demonstrate that MeshMetrics
achieves higher accuracy and precision than established tools, and is
substantially less affected by discretization artifacts, such as distance
quantization. We release MeshMetrics as an open-source Python package,
available at https://github.com/gasperpodobnik/MeshMetrics.

</details>


### [42] [Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization](https://arxiv.org/abs/2509.05695)
*Jingwei Peng,Zhixuan Qiu,Boyu Jin,Surasakdi Siripong*

Main category: cs.CV

TL;DR: 将视频转换为语义动作token并输入微调的视觉语言大模型，实现高精度与可解释的人体动作识别。


<details>
  <summary>Details</summary>
Motivation: Address shortcomings of traditional action recognition (limited semantic understanding, complex context handling, fine-grained distinctions) by using LVLMs' strong multimodal reasoning and language alignment.

Method: Introduce a Video-to-Semantic-Tokens (VST) module to convert video sequences into discrete semantic action tokens; feed these tokens along with natural language prompts into a LoRA-fine-tuned LVLM (e.g., LLaVA-13B) for classification and reasoning.

Result: Reported strong performance on NTU RGB+D and NTU RGB+D 120 (e.g., 94.1% on NTU RGB+D X-Sub, 90.0% on NTU RGB+D 120 X-Set) and improved interpretability via generated natural-language explanations.

Conclusion: LVLM-VAR successfully leverages pretrained LVLMs for video action recognition, producing state-of-the-art accuracy while improving interpretability through natural-language explanations.

Abstract: Human action recognition often struggles with deep semantic understanding,
complex contextual information, and fine-grained distinction, limitations that
traditional methods frequently encounter when dealing with diverse video data.
Inspired by the remarkable capabilities of large language models, this paper
introduces LVLM-VAR, a novel framework that pioneers the application of
pre-trained Vision-Language Large Models (LVLMs) to video action recognition,
emphasizing enhanced accuracy and interpretability. Our method features a
Video-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video
sequences into discrete, semantically and temporally consistent "semantic
action tokens," effectively crafting an "action narrative" that is
comprehensible to an LVLM. These tokens, combined with natural language
instructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)
for robust action classification and semantic reasoning. LVLM-VAR not only
achieves state-of-the-art or highly competitive performance on challenging
benchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant
improvements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),
but also substantially boosts model interpretability by generating natural
language explanations for its predictions.

</details>


### [43] [JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization](https://arxiv.org/abs/2509.05696)
*Hongyu Zhou,Yunzhou Zhang,Tingsong Huang,Fawei Ge,Man Qi,Xichen Zhang,Yizhong Zhang*

Main category: cs.CV

TL;DR: 将法线与RGB联合用于跨视角地理定位，使用DAFM与JCIA进行深度融合并用3D地理增强提升视角不变性，在University-1652和SUES-200上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖RGB语义特征，忽略空间结构信息，导致在大视角差异和外观变化下定位性能下降，因此引入法线图以补充空间结构线索并提升视角不变性。

Method: 提出双分支特征提取框架，分别处理RGB和法线图，并引入Difference-Aware Fusion Module(DAFM)用于差异感知融合，以及Joint-Constrained Interaction Aggregation(JCIA)用于语义与结构信息的联合约束聚合；此外提出3D地理增强生成视角变换样本以提升视角不变性学习。

Result: 在University-1652和SUES-200数据集上进行的大量实验表明，所提方法在面对复杂视角变化时表现出更强的鲁棒性并实现了领先的性能指标。

Conclusion: 本论文通过将法线图与RGB图联合建模，提出JRN-Geo网络，有效增强了跨视角几何不变特征的提取能力，从而提高UAV定位的鲁棒性与精度。

Abstract: Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle
(UAV) localization and navigation. However, significant challenges arise from
the drastic viewpoint differences and appearance variations between images.
Existing methods predominantly rely on semantic features from RGB images, often
neglecting the importance of spatial structural information in capturing
viewpoint-invariant features. To address this issue, we incorporate geometric
structural information from normal images and introduce a Joint perception
network to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a
dual-branch feature extraction framework, leveraging a Difference-Aware Fusion
Module (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to
enable deep fusion and joint-constrained semantic and structural information
representation. Furthermore, we propose a 3D geographic augmentation technique
to generate potential viewpoint variation samples, enhancing the network's
ability to learn viewpoint-invariant features. Extensive experiments on the
University-1652 and SUES-200 datasets validate the robustness of our method
against complex viewpoint ariations, achieving state-of-the-art performance.

</details>


### [44] [Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis](https://arxiv.org/abs/2509.05703)
*Ragib Amin Nihal,Benjamin Yen,Takeshi Ashizawa,Kazuhiro Nakadai*

Main category: cs.CV

TL;DR: 将VLM的视觉描述与LLM的文本验证结合，可在无需标注或重训的情况下，初步解读海洋哺乳动物声谱图，但仍需专家校验以确保准确性。


<details>
  <summary>Details</summary>
Motivation: 探索现有通用VLM是否能直接用于分析特定领域的可视化数据（如生物声学声谱图），以减少对专门标注数据和模型重训的需求。

Method: 构建一个结合VLM可视理解与LLM文本验证的框架：先用VLM生成声谱图的视觉描述与标签，再用LLM基于领域知识对这些描述进行验证和补充，从而迭代建立领域知识库，无需手工标注或重新训练模型。

Result: 提出的框架在无监督条件下能生成有用的声谱图解释与高层次模式识别结果，但存在误判与细节错误，需通过LLM校正并结合专家知识以提高可靠性。

Conclusion: VLMs能在一定程度上从海洋哺乳动物的声谱图中提取有意义的视觉模式，但准确性和专业性受限，需加以验证与领域知识结合。

Abstract: Marine mammal vocalization analysis depends on interpreting bioacoustic
spectrograms. Vision Language Models (VLMs) are not trained on these
domain-specific visualizations. We investigate whether VLMs can extract
meaningful patterns from spectrograms visually. Our framework integrates VLM
interpretation with LLM-based validation to build domain knowledge. This
enables adaptation to acoustic data without manual annotation or model
retraining.

</details>


### [45] [LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction](https://arxiv.org/abs/2509.05728)
*Niels Balemans,Ali Anwar,Jan Steckel,Siegfried Mercelis*

Main category: cs.CV

TL;DR: LiDAR-BIND-T通过三个显式时间一致性机制和改进的网络结构，在保持模块化多模态插拔性的同时显著提升了时序稳定性与SLAM下游性能。


<details>
  <summary>Details</summary>
Motivation: 原始LiDAR-BIND缺乏显式时间一致性约束，导致多时刻输出在连续性与空间一致性上表现欠佳，影响下游SLAM鲁棒性与精度，因此需要加入时间层面的约束与融合机制。

Method: 在原LiDAR-BIND框架上增加三种时间一致性机制：1) 时序嵌入相似度损失以对齐相邻潜在表示；2) 运动对齐变换损失以匹配预测与真实LiDAR的位移；3) 窗口时序融合模块用于在时间窗口内融合信息；同时更新网络结构以更好保留空间结构。

Result: 在雷达/声纳到LiDAR翻译任务上，LiDAR-BIND-T在时间与空间连贯性上表现更好，带来更低的绝对轨迹误差（ATE）和更准确的Cartographer占据图；并提出基于FVMD和相关峰距的度量用于评估时序质量。

Conclusion: LiDAR-BIND-T通过在LiDAR定义的潜在空间中引入显式的时间一致性机制，有效提升了异构传感器（雷达、声纳）到LiDAR翻译的时序稳定性与空间连贯性，从而改善基于SLAM的轨迹与占据图表现。

Abstract: This paper extends LiDAR-BIND, a modular multi-modal fusion framework that
binds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,
with mechanisms that explicitly enforce temporal consistency. We introduce
three contributions: (i) temporal embedding similarity that aligns consecutive
latents, (ii) a motion-aligned transformation loss that matches displacement
between predictions and ground truth LiDAR, and (iii) windows temporal fusion
using a specialised temporal module. We further update the model architecture
to better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR
translation demonstrate improved temporal and spatial coherence, yielding lower
absolute trajectory error and better occupancy map accuracy in
Cartographer-based SLAM (Simultaneous Localisation and Mapping). We propose
different metrics based on the Fr\'echet Video Motion Distance (FVMD) and a
correlation-peak distance metric providing practical temporal quality
indicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or
LiDAR-BIND-T, maintains plug-and-play modality fusion while substantially
enhancing temporal stability, resulting in improved robustness and performance
for downstream SLAM.

</details>


### [46] [Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras](https://arxiv.org/abs/2509.05740)
*Xinyu Zhang,Kai Huang,Junqiao Zhao,Zihan Yuan,Tiantian Feng*

Main category: cs.CV

TL;DR: 提出将多鱼眼相机观测统一到全景视觉特征模型，并结合外参补偿及因子图的紧耦合LIDAR-视觉-惯性融合，显著提高多相机系统的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多摄像头融合可提升观测覆盖与鲁棒性，但处理多相机信息时常需重复单个相机的处理并面临视图不一致、三角化误差与外参不准导致的优化问题。作者希望通过统一的全景模型简化多摄像头处理并提高约束一致性与精度。

Method: 设计并引入全景视觉特征模型，将多鱼眼相机观测映射到统一的全景表示；在因相机坐标与全景坐标不对齐导致三角化误差的问题上提出外参补偿策略；将该视觉模型与LiDAR、IMU信息通过因子图（紧耦合）进行融合，支持回环检测与全局位姿优化。

Result: 在公开数据集上广泛实验：全景视觉特征提升了多相机约束质量与一致性；外参补偿显著降低三角化与优化误差；与现有多相机LiDAR-视觉-惯性系统相比，在精度与鲁棒性上均有提升。

Conclusion: 该文提出了Multi-LVI-SAM，多相机（鱼眼）+LiDAR+IMU紧耦合里程计框架，通过全景视觉特征模型统一多摄像头观测，实现全局几何优化、回环与位姿优化，简化系统设计。提出外参补偿以缓解相机帧与全景模型帧不对齐导致的三角化不一致，提升特征跨视图一致性与位姿精度。实验表明优于现有多相机LVI系统。

Abstract: We propose a multi-camera LiDAR-visual-inertial odometry framework,
Multi-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and
inertial sensors for highly accurate and robust state estimation. To enable
efficient and consistent integration of visual information from multiple
fisheye cameras, we introduce a panoramic visual feature model that unifies
multi-camera observations into a single representation. The panoramic model
serves as a global geometric optimization framework that consolidates
multi-view constraints, enabling seamless loop closure and global pose
optimization, while simplifying system design by avoiding redundant handling of
individual cameras. To address the triangulation inconsistency caused by the
misalignment between each camera's frame and the panoramic model's frame, we
propose an extrinsic compensation method. This method improves feature
consistency across views and significantly reduces triangulation and
optimization errors, leading to more accurate pose estimation. We integrate the
panoramic visual feature model into a tightly coupled LiDAR-visual-inertial
system based on a factor graph. Extensive experiments on public datasets
demonstrate that the panoramic visual feature model enhances the quality and
consistency of multi-camera constraints, resulting in higher accuracy and
robustness than existing multi-camera LiDAR-visual-inertial systems.

</details>


### [47] [Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation](https://arxiv.org/abs/2509.05746)
*Tianhao Guo,Bingjie Lu,Feng Wang,Zhengyang Lu*

Main category: cs.CV

TL;DR: 提出第一个理论上有根基的距离自适应超分框架，通过距离依赖的拟微分退化模型与深度条件卷积核实现空间变分重建，在多数据集上验证了对深度变异场景的显著改进。


<details>
  <summary>Details</summary>
Motivation: 真实成像系统的退化通常随距离变化，传统假设空间不变导致模型不适应复杂的远近场差异，因而需要显式利用几何场景信息进行空间自适应重建。

Method: 将退化算子建模为具有距离依赖谱特性的拟微分算子，推导重建的能量泛函；在神经网络中通过级联残差块实现离散梯度流动力学，采用深度条件卷积核和自适应核生成网络，使滤波器随深度连续变化。网络中引入基于理论的频谱约束与学习的距离自适应正则化以防止远场噪声放大并保证收敛到能量泛函的驻点。

Result: 在五个基准数据集上进行全面评估，在KITTI户外场景上2倍和4倍尺度分别获得36.89/0.9516和30.54/0.8721的PSNR/SSIM，分别优于现有方法0.44dB和0.36dB；在距离变化情形下显著提升，并在传统基准上保持竞争力。

Conclusion: 该论文提出了一个基于变分理论的距离自适应单幅图像超分辨框架，克服了传统空间不变退化假设的局限，在处理由景深、大气散射和透视等引起的距离相关退化时更有效。

Abstract: Single image super-resolution traditionally assumes spatially-invariant
degradation models, yet real-world imaging systems exhibit complex
distance-dependent effects including atmospheric scattering, depth-of-field
variations, and perspective distortions. This fundamental limitation
necessitates spatially-adaptive reconstruction strategies that explicitly
incorporate geometric scene understanding for optimal performance. We propose a
rigorous variational framework that characterizes super-resolution as a
spatially-varying inverse problem, formulating the degradation operator as a
pseudodifferential operator with distance-dependent spectral characteristics
that enable theoretical analysis of reconstruction limits across depth ranges.
Our neural architecture implements discrete gradient flow dynamics through
cascaded residual blocks with depth-conditional convolution kernels, ensuring
convergence to stationary points of the theoretical energy functional while
incorporating learned distance-adaptive regularization terms that dynamically
adjust smoothness constraints based on local geometric structure. Spectral
constraints derived from atmospheric scattering theory prevent bandwidth
violations and noise amplification in far-field regions, while adaptive kernel
generation networks learn continuous mappings from depth to reconstruction
filters. Comprehensive evaluation across five benchmark datasets demonstrates
state-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM
at 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by
0.44dB and 0.36dB respectively. This work establishes the first
theoretically-grounded distance-adaptive super-resolution framework and
demonstrates significant improvements on depth-variant scenarios while
maintaining competitive performance across traditional benchmarks.

</details>


### [48] [InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios](https://arxiv.org/abs/2509.05747)
*Leo Ho,Yinghao Huang,Dafei Qin,Mingyi Shi,Wangpok Tse,Wei Liu,Junichi Yamagishi,Taku Komura*

Main category: cs.CV

TL;DR: 发布了首个包含长时目标驱动双人交互的多模态数据集InterAct，并提出层次化扩散生成方法及嘴唇微调策略，从语音生成语义一致的双人表情与动作。


<details>
  <summary>Details</summary>
Motivation: 现有工作多只关注单人或短时对话手势，忽视动态位置与朝向变化及更长时、目标驱动的互动场景，因而需要新的数据与方法来建模更复杂的双人交互。

Method: 采集241段包含双人音频、身体动作与面部表情的多模态长序列数据；提出层次回归的扩散模型用于从语音生成双人肢体动作，并设计微调机制提升嘴唇同步精度。

Result: 构建了包含多样复杂动作与长时交互模式的InterAct数据集；展示了所提扩散式生成方法能从语音估计双人表情与肢体动作，并通过微调提升嘴唇准确性；数据与代码已公开。

Conclusion: 提出了一个用于捕捉人与人之间长时、目标驱动且语义一致的交互动作的数据集InterAct，并提出基于扩散模型的双人表情与肢体动作生成方法。

Abstract: We address the problem of accurate capture of interactive behaviors between
two people in daily scenarios. Most previous works either only consider one
person or solely focus on conversational gestures of two people, assuming the
body orientation and/or position of each actor are constant or barely change
over each interaction. In contrast, we propose to simultaneously model two
people's activities, and target objective-driven, dynamic, and semantically
consistent interactions which often span longer duration and cover bigger
space. To this end, we capture a new multi-modal dataset dubbed InterAct, which
is composed of 241 motion sequences where two people perform a realistic and
coherent scenario for one minute or longer over a complete interaction. For
each sequence, two actors are assigned different roles and emotion labels, and
collaborate to finish one task or conduct a common interaction activity. The
audios, body motions, and facial expressions of both persons are captured.
InterAct contains diverse and complex motions of individuals and interesting
and relatively long-term interaction patterns barely seen before. We also
demonstrate a simple yet effective diffusion-based method that estimates
interactive face expressions and body motions of two people from speech inputs.
Our method regresses the body motions in a hierarchical manner, and we also
propose a novel fine-tuning mechanism to improve the lip accuracy of facial
expressions. To facilitate further research, the data and code is made
available at https://hku-cg.github.io/interact/ .

</details>


### [49] [Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation](https://arxiv.org/abs/2509.05751)
*Bingrui Zhao,Lin Yuanbo Wu,Xiangtian Fan,Deyin Liu,Lu Zhang,Ruyi He,Jialie Shen,Ximing Li*

Main category: cs.CV

TL;DR: PARSE-VOS用LLM将文本解析为语义命令，生成时空轨迹候选，再通过粗粒度运动推理和条件性姿态验证两阶段筛选，训练自由且在三大基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将静态文本与动态视觉对齐时困难，尤其目标外观相似但运动/姿态不一致或描述为复杂组合时，整体视觉-语言融合容易失败，需更细粒度的推理与分层验证策略。

Method: 先将自然语言查询解析为结构化语义命令；再用时空定位模块在视频中生成所有潜在目标的轨迹候选；最后用分层识别模块通过两阶段推理筛选目标——先用LLM做粗粒度运动推理排除大部分候选，若仍有歧义则触发细粒度姿态验证来最终确认。整个流程为训练自由。

Result: 在Ref-YouTube-VOS、Ref-DAVIS17和MeViS三大基准上取得了最先进性能，证明了分层、由LLM驱动的推理与时空候选生成策略在复杂语言条件下的有效性。

Conclusion: 提出了一种无训练、大语言模型驱动的分层粗到细推理框架PARSE-VOS，用于在视频中基于自然语言定位并分割目标对象，能够在复杂复合描述下有效消歧并得到精确分割。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment an object of
interest throughout a video based on a language description. The prominent
challenge lies in aligning static text with dynamic visual content,
particularly when objects exhibiting similar appearances with inconsistent
motion and poses. However, current methods often rely on a holistic
visual-language fusion that struggles with complex, compositional descriptions.
In this paper, we propose \textbf{PARSE-VOS}, a novel, training-free framework
powered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine
reasoning across text and video domains. Our approach begins by parsing the
natural language query into structured semantic commands. Next, we introduce a
spatio-temporal grounding module that generates all candidate trajectories for
all potential target objects, guided by the parsed semantics. Finally, a
hierarchical identification module select the correct target through a
two-stage reasoning process: it first performs coarse-grained motion reasoning
with an LLM to narrow down candidates; if ambiguity remains, a fine-grained
pose verification stage is conditionally triggered to disambiguate. The final
output is an accurate segmentation mask for the target object.
\textbf{PARSE-VOS} achieved state-of-the-art performance on three major
benchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.

</details>


### [50] [PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters](https://arxiv.org/abs/2509.05773)
*Zijian Chen,Wenjie Hua,Jinhao Li,Lirong Deng,Fan Du,Tingzhu Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: PictOBI-20k为评估LMM视觉破译甲骨文字象形能力的数据集，基线实验显示LMM初具能力但多受语言偏置限制，需增强视觉利用。


<details>
  <summary>Details</summary>
Motivation: 甲骨文为最早的汉字形式，对理解早期生产方式至关重要，但考古与铭文语料稀少，现有破译方法受限，希望利用LMM的视觉感知能力推动象形甲骨文的自动破译与评估。

Method: 收集并整理2万张甲骨文字与真实物体图像，构建约1.5万道多选题；进行主观注释以评估人类与LMM在视觉推理中的参照点一致性；在若干通用LMM上进行性能测试和分析。

Result: 构建了PictOBI-20k数据集并提供基线评估，实验证明通用LMM能在某些情况下进行视觉破译但多数时候依赖语言先验而非视觉线索，提示需改进视觉注意力机制。

Conclusion: 提出了PictOBI-20k数据集以评估大多模态模型对甲骨文象形字符的视觉破译能力，发现通用LMM具有初步视觉破译技能但受语言先验限制，未能有效利用视觉信息。

Abstract: Deciphering oracle bone characters (OBCs), the oldest attested form of
written Chinese, has remained the ultimate, unwavering goal of scholars,
offering an irreplaceable key to understanding humanity's early modes of
production. Current decipherment methodologies of OBC are primarily constrained
by the sporadic nature of archaeological excavations and the limited corpus of
inscriptions. With the powerful visual perception capability of large
multimodal models (LMMs), the potential of using LMMs for visually deciphering
OBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed
to evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It
includes 20k meticulously collected OBC and real object images, forming over
15k multi-choice questions. We also conduct subjective annotations to
investigate the consistency of the reference point between humans and LMMs in
visual reasoning. Experiments indicate that general LMMs possess preliminary
visual decipherment skills, and LMMs are not effectively using visual
information, while most of the time they are limited by language priors. We
hope that our dataset can facilitate the evaluation and optimization of visual
attention in future OBC-oriented LMMs. The code and dataset will be available
at https://github.com/OBI-Future/PictOBI-20k.

</details>


### [51] [Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models](https://arxiv.org/abs/2509.05776)
*Jonathan Aellen,Florian Burkhardt,Thomas Vetter,Marcel Lüthi*

Main category: cs.CV

TL;DR: Align statistical shape models to the target pose before reconstructing partial shapes: a lightweight preprocessing step (no training data needed) yields large gains—exact for translations, effective for small rotations, and improves accuracy and uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: Partial shape reconstruction depends not only on model expressiveness but also on relative pose of training data; unaligned poses cause biased reconstructions. There is a need for efficient ways to adapt prebuilt models to the pose of specific targets, especially in medical imaging where only partial observations are available.

Method: A preprocessing adjustment to existing linear statistical shape models that modifies the model to align with the target pose. The method preserves linear-model computational efficiency, exactly compensates for translations and approximates small rotations, and operates without accessing original training data, enabling plug-and-play adaptation.

Result: Significant improvement in reconstruction accuracy and predicted variance when applying the pose-adaptive preprocessing. Exact alignment recovery for translations and good approximations for small rotations shown. Compatible with existing pipelines and computationally efficient.

Conclusion: Pose alignment between training shapes and target partial shapes is critical; misalignment biases reconstructions especially for small observed parts. The proposed method efficiently adapts existing linear point distribution models to target pose without original training data, yielding exact recovery for translations and good approximation for small rotations, improving reconstruction accuracy and variance predictions.

Abstract: In medical imaging, point distribution models are often used to reconstruct
and complete partial shapes using a statistical model of the full shape. A
commonly overlooked, but crucial factor in this reconstruction process, is the
pose of the training data relative to the partial target shape. A difference in
pose alignment of the training and target shape leads to biased solutions,
particularly when observing small parts of a shape. In this paper, we
demonstrate the importance of pose alignment for partial shape reconstructions
and propose an efficient method to adjust an existing model to a specific
target. Our method preserves the computational efficiency of linear models
while significantly improving reconstruction accuracy and predicted variance.
It exactly recovers the intended aligned model for translations, and provides a
good approximation for small rotations, all without access to the original
training data. Hence, existing shape models in reconstruction pipelines can be
adapted by a simple preprocessing step, making our approach widely applicable
in plug-and-play scenarios.

</details>


### [52] [3DPillars: Pillar-based two-stage 3D object detection](https://arxiv.org/abs/2509.05780)
*Jongyoun Noh,Junghyup Lee,Hyekang Park,Bumsub Ham*

Main category: cs.CV

TL;DR: 本文通过3DPillars和带稀疏场景上下文的RoI head，实现了基于伪图像表示的首个高效两阶段3D检测，兼顾精度与速度。


<details>
  <summary>Details</summary>
Motivation: PointPillars虽然速度快但存在伪图像表示不能精确保留3D结构、且难以采用通常性能更好的两阶段检测流水线的局限。

Method: 提出3DPillars网络，通过将体素的3D特征视作一组伪图像，用可分离体素特征模块在不使用3D卷积的情况下用2D卷积高效提取体素特征；以及设计带稀疏场景上下文特征模块的RoI head，从3DPillars的多尺度特征聚合稀疏场景特征以在两阶段流水线中用于候选框精修。

Result: 在KITTI和Waymo Open数据集上显示，该方法在速度与精度之间实现良好折中，提升了性能同时保持高效性（具体数值未在摘要给出）。

Conclusion: 本文提出了首个基于伪图像表示的两阶段3D检测框架，在保持PointPillars高效性的同时，显著缩小了与最先进方法的性能差距。

Abstract: PointPillars is the fastest 3D object detector that exploits pseudo image
representations to encode features for 3D objects in a scene. Albeit efficient,
PointPillars is typically outperformed by state-of-the-art 3D detection methods
due to the following limitations: 1) The pseudo image representations fail to
preserve precise 3D structures, and 2) they make it difficult to adopt a
two-stage detection pipeline using 3D object proposals that typically shows
better performance than a single-stage approach. We introduce in this paper the
first two-stage 3D detection framework exploiting pseudo image representations,
narrowing the performance gaps between PointPillars and state-of-the-art
methods, while retaining its efficiency. Our framework consists of two novel
components that overcome the aforementioned limitations of PointPillars: First,
we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D
voxel-based features from the pseudo image representation efficiently using 2D
convolutions. The basic idea behind 3DPillars is that 3D features from voxels
can be viewed as a stack of pseudo images. To implement this idea, we propose a
separable voxel feature module that extracts voxel-based features without using
3D convolutions. Second, we introduce an RoI head with a sparse scene context
feature module that aggregates multi-scale features from 3DPillars to obtain a
sparse scene feature. This enables adopting a two-stage pipeline effectively,
and fully leveraging contextual information of a scene to refine 3D object
proposals. Experimental results on the KITTI and Waymo Open datasets
demonstrate the effectiveness and efficiency of our approach, achieving a good
compromise in terms of speed and accuracy.

</details>


### [53] [CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation](https://arxiv.org/abs/2509.05785)
*In-Jae Lee,Sihwan Hwang,Youngseok Kim,Wonjune Kim,Sanmin Kim,Dongsuk Kum*

Main category: cs.CV

TL;DR: CRAB在后向投影的相机-雷达BEV变换中用雷达占据信息减轻图像深度歧义并加入雷达上下文的空间交叉注意力，显著提升了nuScenes上的3D检测性能（NDS 62.4%、mAP 54.0%）。


<details>
  <summary>Details</summary>
Motivation: 现有相机-雷达融合的BEV检测方法存在两类问题：前向投影生成稀疏BEV特征，后向投影则会因图像深度歧义产生误检。利用雷达的精确深度信息可以在后向投影中减少深度模糊带来的虚警，从而兼顾稠密视觉信息与雷达深度精度。

Method: CRAB使用后向投影将图像透视特征聚合到BEV查询上；将来自图像的稠密但不可靠的深度分布与雷达的稀疏精确深度（占据信息）结合，以增强沿同一光线的查询间深度区分；并加入基于含雷达上下文特征的空间交叉注意力模块来提升3D场景理解与特征融合。

Result: 在nuScenes数据集上，CRAB在后向投影的相机-雷达融合方法中取得了最先进的性能：NDS 62.4%、mAP 54.0%，证明了利用雷达占据信息与空间交叉注意力能有效提升检测与分割性能。

Conclusion: 本文提出CRAB，通过在向BEV变换时融合雷达占据信息缓解深度歧义，并引入含雷达语境的空间交叉注意力，提高了后向投影方法的深度区分能力和场景理解。实验在nuScenes上达到了62.4% NDS和54.0% mAP，优于同类后向投影的相机-雷达融合方法。

Abstract: Recently, camera-radar fusion-based 3D object detection methods in bird's eye
view (BEV) have gained attention due to the complementary characteristics and
cost-effectiveness of these sensors. Previous approaches using forward
projection struggle with sparse BEV feature generation, while those employing
backward projection overlook depth ambiguity, leading to false positives. In
this paper, to address the aforementioned limitations, we propose a novel
camera-radar fusion-based 3D object detection and segmentation model named CRAB
(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based
view transformation), using a backward projection that leverages radar to
mitigate depth ambiguity. During the view transformation, CRAB aggregates
perspective view image context features into BEV queries. It improves depth
distinction among queries along the same ray by combining the dense but
unreliable depth distribution from images with the sparse yet precise depth
information from radar occupancy. We further introduce spatial cross-attention
with a feature map containing radar context information to enhance the
comprehension of the 3D scene. When evaluated on the nuScenes open dataset, our
proposed approach achieves a state-of-the-art performance among backward
projection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in
3D object detection.

</details>


### [54] [Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance](https://arxiv.org/abs/2509.05796)
*Julio Zanon Diaz,Georgios Siogkas,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出两种注意力自编码器：基于4-MS-SSIM的轻量实时检测（ACC最高0.931）与基于马氏距离的后置监管监测（ACC 0.722），兼顾效率、灵敏度与法规合规。


<details>
  <summary>Details</summary>
Motivation: 医疗器械制造的视觉检验存在样本不平衡、图像分辨率高及法规合规需求，需轻量且可解释的异常检测方案以实现在线检测与事后监管。

Method: 第一种模型基于结构相似性（4-MS-SSIM）计算重建与原图间的差异作为异常得分；第二种在编码器低维特征上使用马氏距离评估样本与正常分布的距离，二者均引入注意力机制提升检测聚焦能力。

Result: 4-MS-SSIM方法在Surface Seal Image测试集（仅10%缺陷样本）达到ACC 0.903（无监督阈值）和0.931（有监督阈值）；马氏距离方法在有监督阈值下ACC 0.722。两方法均优于重实现的基线。

Conclusion: 本文提出两种注意力引导的自编码器用于视觉异常检测，分别针对实时在线检测与后端监管监测，均在小样本、高分辨率及监管要求下表现优于基线方法。

Abstract: Automating visual inspection in medical device manufacturing remains
challenging due to small and imbalanced datasets, high-resolution imagery, and
stringent regulatory requirements. This work proposes two attention-guided
autoencoder architectures for deep anomaly detection designed to address these
constraints. The first employs a structural similarity-based anomaly score
(4-MS-SSIM), offering lightweight and accurate real-time defect detection,
yielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised
thresholding) on the - Surface Seal Image - Test split with only 10% of
defective samples. The second applies a feature-distance approach using
Mahalanobis scoring on reduced latent features, providing high sensitivity to
distributional shifts for supervisory monitoring, achieving ACC 0.722 with
supervised thresholding. Together, these methods deliver complementary
capabilities: the first supports reliable inline inspection, while the second
enables scalable post-production surveillance and regulatory compliance
monitoring. Experimental results demonstrate that both approaches surpass
re-implemented baselines and provide a practical pathway for deploying deep
anomaly detection in regulated manufacturing environments, aligning accuracy,
efficiency, and the regulatory obligations defined for high-risk AI systems
under the EU AI Act.

</details>


### [55] [A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation](https://arxiv.org/abs/2509.05809)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 将SAM概率化，利用变分潜变量建模分割分布，能高效采样多样化掩码，并在医疗数据上展现优越的不确定性刻画能力。


<details>
  <summary>Details</summary>
Motivation: 现实任务（尤其是医疗影像）中同一目标常存在多种合理分割，专家间存在差异；确定性分割模型无法刻画这种不确定性，影响决策与下游任务。

Method: 在SAM框架中引入潜变量空间，构建先验网络和后验网络，并采用变分目标训练；潜代码调制提示嵌入以生成多样化掩码，推理时通过采样高效生成多重输出。

Result: 在LIDC-IDRI肺结节数据集上，Probabilistic SAM生成的多样化分割与专家分歧一致，在不确定性相关指标上优于已有概率基线。

Conclusion: 本文提出将SAM扩展为概率化模型，从而能对同一图像与提示生成一组可能的分割，解决了确定性SAM无法表达标注不确定性的问题。

Abstract: Recent advances in promptable segmentation, such as the Segment Anything
Model (SAM), have enabled flexible, high-quality mask generation across a wide
range of visual domains. However, SAM and similar models remain fundamentally
deterministic, producing a single segmentation per object per prompt, and fail
to capture the inherent ambiguity present in many real-world tasks. This
limitation is particularly troublesome in medical imaging, where multiple
plausible segmentations may exist due to annotation uncertainty or inter-expert
variability. In this paper, we introduce Probabilistic SAM, a probabilistic
extension of SAM that models a distribution over segmentations conditioned on
both the input image and prompt. By incorporating a latent variable space and
training with a variational objective, our model learns to generate diverse and
plausible segmentation masks reflecting the variability in human annotations.
The architecture integrates a prior and posterior network into the SAM
framework, allowing latent codes to modulate the prompt embeddings during
inference. The latent space allows for efficient sampling during inference,
enabling uncertainty-aware outputs with minimal overhead. We evaluate
Probabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate
its ability to produce diverse outputs that align with expert disagreement,
outperforming existing probabilistic baselines on uncertainty-aware metrics.
Our code is available at: https://github.com/tbwa233/Probabilistic-SAM/.

</details>


### [56] [Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data](https://arxiv.org/abs/2509.05887)
*Caleb Gates,Patrick Moorhead,Jayden Ferguson,Omar Darwish,Conner Stallman,Pablo Rivas,Paapa Quansah*

Main category: cs.CV

TL;DR: 论文提出基于3D卷积的多光谱（36波段+热分裂）近实时像素级沙尘检测系统，训练效率改进后可快速处理整幅场景，在17幅独立MODIS场景上达约0.92准确率，边缘检测仍可通过更大窗口或注意力机制改进。


<details>
  <summary>Details</summary>
Motivation: 沙尘暴对健康与能见度有重大影响，需从卫星影像中快速且准确地检测并预警沙尘。

Method: 使用3D卷积网络同时学习光谱和空间特征，输入包括36个波段及热红外分裂带；通过简单归一化和局部填补处理缺失数据；改进版本提高训练速度并可快速处理整幅场景。

Result: 在17幅独立MODIS场景上总体准确率约0.92，均方误差0.014；在羽状核心区域与人工或基准地图高度一致，误判多出现在羽缘。

Conclusion: 该系统能基于MODIS多光谱影像实现近实时的像素级沙尘检测，主干有效，尤其在羽状核心区域性能优良。

Abstract: Dust storms harm health and reduce visibility; quick detection from
satellites is needed. We present a near real-time system that flags dust at the
pixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D
convolutional network learns patterns across all 36 bands, plus split thermal
bands, to separate dust from clouds and surface features. Simple normalization
and local filling handle missing data. An improved version raises training
speed by 21x and supports fast processing of full scenes. On 17 independent
MODIS scenes, the model reaches about 0.92 accuracy with a mean squared error
of 0.014. Maps show strong agreement in plume cores, with most misses along
edges. These results show that joint band-and-space learning can provide timely
dust alerts at global scale; using wider input windows or attention-based
models may further sharpen edges.

</details>


### [57] [Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets](https://arxiv.org/abs/2509.05892)
*Phongsakon Mark Konrad,Andrei-Alexandru Popa,Yaser Sabzehmeidani,Liang Zhong,Elisa A. Liehn,Serkan Ayvaz*

Main category: cs.CV

TL;DR: 在心血管组织学小样本数据上，模型性能受数据划分影响很大，常规基准评测不可靠，应谨慎解读模型排名与临床适用性。


<details>
  <summary>Details</summary>
Motivation: 心血管组织学图像上的标注稀缺限制了深度学习模型的开发与推广。研究旨在明确在低样本量情形下，不同先进分割模型的表现是否稳健，以及常规基准评测能否反映实际临床价值。

Method: 系统性评估包括对比U-Net、DeepLabV3+（卷积网络）、SegFormer（视觉Transformer），以及SAM、MedSAM与MedSAM+UNet等基础模型。采用贝叶斯超参数搜索进行广泛优化，并在多个数据拆分上反复训练与验证，以量化性能随数据划分的波动。

Result: 尽管进行了充分的超参数优化，不同模型在少量数据上的表现差异并不稳定，且受数据拆分影响显著。部分模型偶发领先但无法保证跨拆分一致性，表明排名受统计噪声主导。基准比较在这种低数据情境下可能导致误导性结论。

Conclusion: 在有限的心血管组织学数据上，深度学习分割模型的比较结果高度不稳定，性能差异主要受数据划分带来的统计波动影响，而非算法本身的明显优越性。因此，现有基准评估在低样本临床场景下可信度受限，不能可靠指示临床实用性。

Abstract: Accurate segmentation of carotid artery structures in histopathological
images is vital for advancing cardiovascular disease research and diagnosis.
However, deep learning model development in this domain is constrained by the
scarcity of annotated cardiovascular histopathological data. This study
investigates a systematic evaluation of state-of-the-art deep learning
segmentation models, including convolutional neural networks (U-Net,
DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models
(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology
images. Despite employing an extensive hyperparameter optimization strategy
with Bayesian search, our findings reveal that model performance is highly
sensitive to data splits, with minor differences driven more by statistical
noise than by true algorithmic superiority. This instability exposes the
limitations of standard benchmarking practices in low-data clinical settings
and challenges the assumption that performance rankings reflect meaningful
clinical utility.

</details>


### [58] [BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model](https://arxiv.org/abs/2509.05895)
*Yujie Li,Wenjia Xu,Yuanben Zhang,Zhiwei Wei,Mugen Peng*

Main category: cs.CV

TL;DR: BTCChat是一种面向双时相卫星影像的多模态大模型，包含Change Extraction和Prompt Augmentation两项关键设计，显著提升了时间变化建模与空间语义对齐，进而在变化描述和VQA任务上取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有将双时相影像简单拼接的做法无法充分建模时间关联与空间语义变化，导致视觉-语义对齐不足，限制了变化理解效果，因此需要专门的机制提升双时相影像的变化感知与生成能力。

Method: 提出多时相MLLM框架BTCChat；设计Change Extraction模块以专门提取双时相影像间的时序与语义变化特征；引入Prompt Augmentation机制，将上下文线索融入提示以增强对空间细节的关注；保留单帧解析能力以兼顾通用视觉理解。

Result: 在变化描述（change captioning）与视觉问答（VQA）基准上，BTCChat表现超越现有方法，达到了SOTA水平，验证了Change Extraction与Prompt Augmentation的有效性。

Conclusion: BTCChat通过引入Change Extraction模块和Prompt Augmentation机制，显著提高了多时相影像的时序特征建模与空间语义变化捕捉能力，从而在变化描述与视觉问答任务上取得了领先表现。

Abstract: Bi-temporal satellite imagery supports critical applications such as urban
development monitoring and disaster assessment. Although powerful multimodal
large language models (MLLMs) have been applied in bi-temporal change analysis,
previous methods process image pairs through direct concatenation, inadequately
modeling temporal correlations and spatial semantic changes. This deficiency
hampers visual-semantic alignment in change understanding, thereby constraining
the overall effectiveness of current approaches. To address this gap, we
propose BTCChat, a multi-temporal MLLM with advanced bi-temporal change
understanding capability. BTCChat supports bi-temporal change captioning and
retains single-image interpretation capability. To better capture temporal
features and spatial semantic changes in image pairs, we design a Change
Extraction module. Moreover, to enhance the model's attention to spatial
details, we introduce a Prompt Augmentation mechanism, which incorporates
contextual clues into the prompt to enhance model performance. Experimental
results demonstrate that BTCChat achieves state-of-the-art performance on
change captioning and visual question answering tasks.

</details>


### [59] [A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features](https://arxiv.org/abs/2509.05913)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Tamanna Shermin,Md Rafiqul Islam,Mukhtar Hussain,Sami Azam*

Main category: cs.CV

TL;DR: 作者提出ViSK-GAT，通过细粒度跨模态注意力与几何对齐模块在视觉与骨骼坐标间建立更有效的融合，在自建多模态数据集上显著提升肌肉骨骼风险分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖单一数据模态，在复杂、非受控环境下表现不稳，早期准确评估运动员的肌肉骨骼风险具有重要预防意义，因此需要能处理复杂环境的多模态方法。

Method: 构建了包含图像与骨骼坐标的多模态数据集；设计ViSK-GAT架构，融合残差块和轻量级Transformer块；引入FGAM（细粒度跨模态注意力）和MGCM（多模态几何对应模块）以提升特征融合与对齐；训练并评估模型，与九种迁移学习骨干网络比较。

Result: 在自建数据集上，验证集与测试集准确率分别为93.55%、93.89%；精确率93.86%、F1为93.85%；Cohen's Kappa与MCC均为93%；预测概率分布的RMSE为0.1205、MAE为0.0156；在与九种迁移学习骨干比较中表现优越。

Conclusion: 该研究提出了ViSK-GAT，一种结合视觉与骨骼坐标的多模态深度学习框架，用于肌肉骨骼风险分类，在公开数据上取得高性能，展示了跨模态注意力与几何对齐的优势。

Abstract: Musculoskeletal disorders pose significant risks to athletes, and assessing
risk early is important for prevention. However, most existing methods are
designed for controlled settings and fail to reliably assess risk in complex
environments due to their reliance on a single type of data. This research
proposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel
multimodal deep learning framework designed to classify musculoskeletal risk
using visual and skeletal coordinate-based features. In addition, a custom
multimodal dataset is constructed by combining visual data and skeletal
coordinates for risk assessment. Each sample is labeled into eight risk
categories based on the Rapid Entire Body Assessment system. ViSK-GAT combines
a Residual Block with a Lightweight Transformer Block to learn spatial and
temporal dependencies jointly. It incorporates two novel modules: the
Fine-Grained Attention Module (FGAM), which enables precise inter-modal feature
refinement through cross-attention between visual and skeletal inputs, and the
Multimodal Geometric Correspondence Module (MGCM), which enhances cross-modal
coherence by aligning image features with coordinate-based representations.
ViSK-GAT achieved strong performance with validation and test accuracies of
93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of
93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. The
regression results also indicated a low Root Mean Square Error of the predicted
probability distribution of 0.1205 and a corresponding Mean Absolute Error of
0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT
consistently outperformed previous methods. The ViSK-GAT model advances
artificial intelligence implementation and application, transforming
musculoskeletal risk classification and enabling impactful early interventions
in sports.

</details>


### [60] [Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models](https://arxiv.org/abs/2509.05925)
*Ruiqi Shen,Haotian Wu,Wenjing Zhang,Jiangjing Hu,Deniz Gunduz*

Main category: cs.CV

TL;DR: 利用CLIP特征进行极低比特率的语义压缩，舍弃像素重建以换取跨任务与跨域的语义保真与极高压缩率。


<details>
  <summary>Details</summary>
Motivation: 随着应用从像素级重建转向语义感知任务，传统以重建为目标的压缩方法在语义保持和跨域鲁棒性上受限。基于CLIP的多模态表征具备零样本泛化与强语义表达能力，因而适合作为语义压缩的目标表示，以满足低比特传输且保留下游任务性能的需求。

Method: 作者不直接压缩图像像素，而是对CLIP提取的图像-文本对齐特征进行量化/编码，目标是以最小比特表示保留对多任务友好的语义嵌入。具体做法包括（1）提取CLIP特征；（2）对特征进行极低比特率编码；（3）在下游任务或检索中直接使用解码后的特征进行推断，省去像素重构步骤。

Result: 实验表明，在基准数据集上该方法能将比特率降低到约2-3×10^{-3} bits/pixel，约为主流图像压缩方法所需比特率的<5%，同时在多种下游任务和不同数据分布间表现出零样本鲁棒性，语义性能基本保持。

Conclusion: 该论文提出了一种基于CLIP特征的语义压缩范式，通过压缩多模态基础模型（CLIP）生成的特征嵌入以极低比特率保存语义信息，从而替代像素重建为目标的传统有损图像压缩。论文展示在多个基准数据集和下游任务上，即使在极端压缩率下也能保持语义完整性并具备零样本鲁棒性。

Abstract: Recent deep learning-based methods for lossy image compression achieve
competitive rate-distortion performance through extensive end-to-end training
and advanced architectures. However, emerging applications increasingly
prioritize semantic preservation over pixel-level reconstruction and demand
robust performance across diverse data distributions and downstream tasks.
These challenges call for advanced semantic compression paradigms. Motivated by
the zero-shot and representational capabilities of multimodal foundation
models, we propose a novel semantic compression method based on the contrastive
language-image pretraining (CLIP) model. Rather than compressing images for
reconstruction, we propose compressing the CLIP feature embeddings into minimal
bits while preserving semantic information across different tasks. Experiments
show that our method maintains semantic integrity across benchmark datasets,
achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This
is less than 5% of the bitrate required by mainstream image compression
approaches for comparable performance. Remarkably, even under extreme
compression, the proposed approach exhibits zero-shot robustness across diverse
data distributions and downstream tasks.

</details>


### [61] [AttriPrompt: Dynamic Prompt Composition Learning for CLIP](https://arxiv.org/abs/2509.05949)
*Qiqi Zhan,Shiwei Li,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: AttriPrompt使用视觉中间层聚类检索提示并在文本编码器层间注入，结合双流对比学习和自正则化，实现内容感知且细粒度的语义对齐，显著提升小样本与跨域性能。


<details>
  <summary>Details</summary>
Motivation: 现有深层文本提示方法过度依赖对比学习忽视细粒度特征、提示静态无法针对不同输入自适应，导致性能受限。

Method: 设计Attribute Retrieval模块对每层视觉特征聚类并检索相似提示，拼接到文本编码器每层输入；引入Dual-stream Contrastive Learning实现细粒度对齐；通过Self-Regularization在提示/非提示文本特征间施加正则化以防过拟合。

Result: 在三个基准上优于SOTA，base-to-novel设置下最高提升7.37%，在跨域知识迁移表现优异。

Conclusion: AttriPrompt通过利用CLIP视觉编码器的中间层特征与层级提示池实现细粒度的语义对齐和内容自适应，从而在基准测试上显著优于现有方法。

Abstract: The evolution of prompt learning methodologies has driven exploration of
deeper prompt designs to enhance model performance. However, current deep text
prompting approaches suffer from two critical limitations: Over-reliance on
constrastive learning objectives that prioritize high-level semantic alignment,
neglecting fine-grained feature optimization; Static prompts across all input
categories, preventing content-aware adaptation. To address these limitations,
we propose AttriPrompt-a novel framework that enhances and refines textual
semantic representations by leveraging the intermediate-layer features of
CLIP's vision encoder. We designed an Attribute Retrieval module that first
clusters visual features from each layer. The aggregated visual features
retrieve semantically similar prompts from a prompt pool, which are then
concatenated to the input of every layer in the text encoder. Leveraging
hierarchical visual information embedded in prompted text features, we
introduce Dual-stream Contrastive Learning to realize fine-grained alignment.
Furthermore, we introduce a Self-Regularization mechanism by applying explicit
regularization constraints between the prompted and non-prompted text features
to prevent overfitting on limited training data. Extensive experiments across
three benchmarks demonstrate AttriPrompt's superiority over state-of-the-art
methods, achieving up to 7.37\% improvement in the base-to-novel setting. The
observed strength of our method in cross-domain knowledge transfer positions
vision-language pre-trained models as more viable solutions for real-world
implementation.

</details>


### [62] [Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching](https://arxiv.org/abs/2509.05952)
*Feng Wang,Zihao Yu*

Main category: cs.CV

TL;DR: 提出CPS替代SDE采样以去除噪声伪影，改善流匹配模型中基于强化学习的奖励学习与收敛性。


<details>
  <summary>Details</summary>
Motivation: 当前在提升Flow Matching生成模型与提示对齐时，在线强化学习需要在本为确定性的Flow Matching框架中引入随机性。常用的SDE做法会在生成图像中引入明显噪声，损害奖励学习，从而降低RL效果。

Method: 通过理论分析指出SDE采样在推理阶段注入过量随机性导致噪声，并借鉴DDIM思想重构采样过程，设计CPS以保留系数而不引入额外噪声；在此基础上进行奖励建模和在线强化学习优化。

Result: CPS消除了SDE带来的噪声伪影，使奖励建模更准确，并加速且稳定地收敛于Flow-GRPO和Dance-GRPO等RL优化器。

Conclusion: 本文提出Coefficients-Preserving Sampling (CPS)方法，用于在Flow Matching模型中引入必要的随机性而避免SDE采样带来的噪声伪影，从而提升基于强化学习的优化器（如Flow-GRPO、Dance-GRPO）的收敛速度与稳定性。

Abstract: Reinforcement Learning (RL) has recently emerged as a powerful technique for
improving image and video generation in Diffusion and Flow Matching models,
specifically for enhancing output quality and alignment with prompts. A
critical step for applying online RL methods on Flow Matching is the
introduction of stochasticity into the deterministic framework, commonly
realized by Stochastic Differential Equation (SDE). Our investigation reveals a
significant drawback to this approach: SDE-based sampling introduces pronounced
noise artifacts in the generated images, which we found to be detrimental to
the reward learning process. A rigorous theoretical analysis traces the origin
of this noise to an excess of stochasticity injected during inference. To
address this, we draw inspiration from Denoising Diffusion Implicit Models
(DDIM) to reformulate the sampling process. Our proposed method,
Coefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This
leads to more accurate reward modeling, ultimately enabling faster and more
stable convergence for reinforcement learning-based optimizers like Flow-GRPO
and Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS

</details>


### [63] [Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation](https://arxiv.org/abs/2509.05953)
*Jeonghyun Noh,Wangsu Jeon,Jinsun Park*

Main category: cs.CV

TL;DR: 提出DIFM：利用双向交叉注意力在原始与增强图间交互并结合多尺度边界损失，有效提升医学图像分割边界与整体性能。


<details>
  <summary>Details</summary>
Motivation: 增强处理可提升图像可视性但可能破坏原始图像中用于诊断的重要细节；传统特征拼接等融合方法难以兼顾保留原图关键信息与利用增强图优点，故需一种能抑制增强副作用并充分挖掘互补性的融合机制。

Method: 提出DIFM模块：双向交叉注意力用于在原始与增强图像间相互关注对应空间信息，随后通过全局空间注意力细化互补特征；并引入基于梯度提取的多尺度边界损失以加强边界分割。实现上将增强图像与原始图像分别编码，再通过DIFM在不同层级交互融合特征，最后解码生成分割结果。

Result: 在ACDC与Synapse数据集上的定量与定性实验显示，本文方法在分割准确度及边界表现上优于对比方法，证明双向交互融合与多尺度边界损失的有效性。作者提供了代码实现链接。

Conclusion: 本文提出的双向交互融合模块（DIFM）有效整合了原始图像与增强图像的互补信息，提升了分割特征的空间表达，从而改善了医学图像分割性能。结合基于梯度提取的多尺度边界损失，模型在ACDC和Synapse数据集上表现优于基线。

Abstract: Medical image segmentation is a crucial method for assisting professionals in
diagnosing various diseases through medical imaging. However, various factors
such as noise, blurriness, and low contrast often hinder the accurate diagnosis
of diseases. While numerous image enhancement techniques can mitigate these
issues, they may also alter crucial information needed for accurate diagnosis
in the original image. Conventional image fusion strategies, such as feature
concatenation can address this challenge. However, they struggle to fully
leverage the advantages of both original and enhanced images while suppressing
the side effects of the enhancements. To overcome the problem, we propose a
dual interactive fusion module (DIFM) that effectively exploits mutual
complementary information from the original and enhanced images. DIFM employs
cross-attention bidirectionally to simultaneously attend to corresponding
spatial information across different images, subsequently refining the
complementary features via global spatial attention. This interaction leverages
low- to high-level features implicitly associated with diverse structural
attributes like edges, blobs, and object shapes, resulting in enhanced features
that embody important spatial characteristics. In addition, we introduce a
multi-scale boundary loss based on gradient extraction to improve segmentation
accuracy at object boundaries. Experimental results on the ACDC and Synapse
datasets demonstrate the superiority of the proposed method quantitatively and
qualitatively. Code available at: https://github.com/JJeong-Gari/DIN

</details>


### [64] [StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud](https://arxiv.org/abs/2509.05954)
*Weichao Wang,Wendong Mao,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 提出SAB与硬件友好分层骨干的轻量3D检测框架StripDet，实现低参数、高效率且在KITTI上达到接近或超过基线的检测精度。


<details>
  <summary>Details</summary>
Motivation: 在点云3D检测中，高精度模型计算量和内存开销大，难以部署到边缘设备。目标是设计一个参数少、推理快且仍保持高精度的模型以满足实际设备部署需求。

Method: 核心方法包括：1) 提出Strip Attention Block（SAB），将标准2D卷积分解为方向性不对称条带卷积以捕获长程空间依赖并把复杂度从平方降到线性；2) 设计基于SAB与深度可分离卷积的分层主干网络，结合简单多尺度特征融合，实现端到端高效推理。

Result: 在KITTI数据集上，StripDet仅用0.65M参数即可达到79.97% mAP（Car），相比PointPillars在参数量上减少约7倍，同时在精度-效率比上优于其他轻量或蒸馏方法。

Conclusion: StripDet提出了轻量级有效的3D目标检测框架，通过引入条带注意力块（SAB）和硬件友好的分层主干网络，实现了在点云检测任务上优秀的精确度-效率权衡。

Abstract: The deployment of high-accuracy 3D object detection models from point cloud
remains a significant challenge due to their substantial computational and
memory requirements. To address this, we introduce StripDet, a novel
lightweight framework designed for on-device efficiency. First, we propose the
novel Strip Attention Block (SAB), a highly efficient module designed to
capture long-range spatial dependencies. By decomposing standard 2D
convolutions into asymmetric strip convolutions, SAB efficiently extracts
directional features while reducing computational complexity from quadratic to
linear. Second, we design a hardware-friendly hierarchical backbone that
integrates SAB with depthwise separable convolutions and a simple multiscale
fusion strategy, achieving end-to-end efficiency. Extensive experiments on the
KITTI dataset validate StripDet's superiority. With only 0.65M parameters, our
model achieves a 79.97% mAP for car detection, surpassing the baseline
PointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms
recent lightweight and knowledge distillation-based methods, achieving a
superior accuracy-efficiency trade-off while establishing itself as a practical
solution for real-world 3D detection on edge devices.

</details>


### [65] [Neural Bloom: A Deep Learning Approach to Real-Time Lighting](https://arxiv.org/abs/2509.05963)
*Rafal Karp,Dawid Gruszka,Tomasz Trzcinski*

Main category: cs.CV

TL;DR: 提出用神经网络替代传统模糊管线生成bloom亮度掩码，FastNBL和NBL在质量相当或更好前提下分别实现了约28%和12%的加速，适合实时渲染优化。


<details>
  <summary>Details</summary>
Motivation: 传统bloom实现依赖多次模糊、纹理采样和条件分支，耗时较多，成为实时渲染的性能瓶颈；目标是在不牺牲视觉质量的前提下加速bloom效果生成，以节省计算资源并保持高帧率沉浸感。

Method: 通过训练神经网络直接从3D场景视图生成亮度掩码，替代传统多次模糊和纹理采样操作；提出两种变体：侧重性能的FastNBL和侧重质量的NBL；在多种3D场景上评估亮度掩码准确性与推理速度以比较基线实现。

Result: 实验表明在相同任务上两种方法均胜过现有标准实现，FastNBL平均加速约28%、NBL约12%，并能生成高质量的bloom效果，验证了方法在速度与视觉质量上的优势。

Conclusion: 该论文提出两种基于神经网络的实时bloom光照生成方法（NBL和FastNBL），在保持高质量的同时提升了性能，FastNBL比现有标准方法快28%，NBL快12%，能更高效地产生亮度掩码并降低实时渲染开销。

Abstract: We propose a novel method to generate bloom lighting effect in real time
using neural networks. Our solution generate brightness mask from given 3D
scene view up to 30% faster than state-of-the-art methods. The existing
traditional techniques rely on multiple blur appliances and texture sampling,
also very often have existing conditional branching in its implementation.
These operations occupy big portion of the execution time. We solve this
problem by proposing two neural network-based bloom lighting methods, Neural
Bloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on
their quality and performance. Both methods were tested on a variety of 3D
scenes, with evaluations conducted on brightness mask accuracy and inference
speed. The main contribution of this work is that both methods produce
high-quality bloom effects while outperforming the standard state-of-the-art
bloom implementation, with FastNBL being faster by 28% and NBL faster by 12%.
These findings highlight that we can achieve realistic bloom lighting phenomena
faster, moving us towards more realism in real-time environments in the future.
This improvement saves computational resources, which is a major bottleneck in
real-time rendering. Furthermore, it is crucial for sustaining immersion and
ensuring smooth experiences in high FPS environments, while maintaining
high-quality realism.

</details>


### [66] [Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks](https://arxiv.org/abs/2509.05967)
*Yiqin Zhang,Meiling Chen,Zhengjie Zhang*

Main category: cs.CV

TL;DR: 为提高医学3D影像自监督学习的可解释性，论文设计三子任务并进行多粒度空间关系建模，在保证训练稳定的同时实现与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督方法多借鉴2D视觉领域设计，难以直观展示模型对3D空间知识的学习过程，导致医学可解释性不足；因此需要专门针对3D医学影像的可解释自监督设计。

Method: 方法由三项子任务组成，遵循可观测原则以保证可解释性，并利用3D影像额外维度进行多粒度空间关系建模以保持训练的稳定性。

Result: 实验表明该方法在性能上与当前方法相当，同时提供了对自监督学习过程的直观理解。

Conclusion: 该论文提出了一种针对医学3D影像的自监督学习方法，设计了三个子任务以捕捉空间相关语义，强调可解释性并在性能上与现有方法相当。

Abstract: The application of self-supervised techniques has become increasingly
prevalent within medical visualization tasks, primarily due to its capacity to
mitigate the data scarcity prevalent in the healthcare sector. The majority of
current works are influenced by designs originating in the generic 2D visual
domain, which lack the intuitive demonstration of the model's learning process
regarding 3D spatial knowledge. Consequently, these methods often fall short in
terms of medical interpretability. We propose a method consisting of three
sub-tasks to capture the spatially relevant semantics in medical 3D imaging.
Their design adheres to observable principles to ensure interpretability, and
minimize the performance loss caused thereby as much as possible. By leveraging
the enhanced semantic depth offered by the extra dimension in 3D imaging, this
approach incorporates multi-granularity spatial relationship modeling to
maintain training stability. Experimental findings suggest that our approach is
capable of delivering performance that is on par with current methodologies,
while facilitating an intuitive understanding of the self-supervised learning
process.

</details>


### [67] [OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization](https://arxiv.org/abs/2509.05970)
*Ye Wang,Zili Yi,Yibo Zhang,Peng Zheng,Xuping Xie,Jiang Lin,Yilin Wang,Rui Ma*

Main category: cs.CV

TL;DR: 通过去风格化生成大规模成对数据（DST-100K），并用它训练简单模型OmniStyle2，作者在艺术风格迁移任务上取得了超过现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 艺术风格迁移缺乏真实成对的训练数据，限制了监督学习方法的效果。通过反向思路——去除艺术作品的风格恢复原始内容，可以自动生成高质量的训练配对，从而解决数据匮乏问题。

Method: 提出了DST（基于文本引导的去风格化模型）用于从艺术作品重建无风格的自然图像；提出DST-Filter，多阶段链式思维评估器用于自动过滤低质量配对；基于生成的数据集DST-100K训练简单的前馈模型OmniStyle2（基于FLUX.1-dev）。

Result: 构建了大规模DST-100K数据集，并训练出OmniStyle2，在定性和定量基准上均优于最先进方法，证明去风格化生成的数据在艺术风格迁移中是可靠的监督来源。

Conclusion: OmniStyle2通过将风格迁移问题重构为数据生成问题，利用去风格化生成大规模配对数据集DST-100K，从而在无真实成对数据的情形下实现了更可靠的监督，最终在多项评测中优于现有方法。

Abstract: OmniStyle2 introduces a novel approach to artistic style transfer by
reframing it as a data problem. Our key insight is destylization, reversing
style transfer by removing stylistic elements from artworks to recover natural,
style-free counterparts. This yields DST-100K, a large-scale dataset that
provides authentic supervision signals by aligning real artistic styles with
their underlying content. To build DST-100K, we develop (1) DST, a text-guided
destylization model that reconstructs stylefree content, and (2) DST-Filter, a
multi-stage evaluation model that employs Chain-of-Thought reasoning to
automatically discard low-quality pairs while ensuring content fidelity and
style accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward
model based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently
surpasses state-of-the-art methods across both qualitative and quantitative
benchmarks. Our results demonstrate that scalable data generation via
destylization provides a reliable supervision paradigm, overcoming the
fundamental challenge posed by the lack of ground-truth data in artistic style
transfer.

</details>


### [68] [ConstStyle: Robust Domain Generalization with Unified Style Transformation](https://arxiv.org/abs/2509.05975)
*Nam Duong Tran,Nam Nguyen Phuong,Hieu H. Pham,Phi Le Nguyen,My T. Thai*

Main category: cs.CV

TL;DR: ConstStyle通过学习一个统一域并将训练/测试样本映射至该域来减少域偏移，在有限训练域或大域差距场景下显著提升域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统域泛化方法在训练域有限或训练-测试域差距较大时表现欠佳。作者假设训练中若模型见到的域更接近未见测试域，泛化性能会更好，因此提出通过构建统一域来模拟或接近未知测试域。

Method: 提出了ConstStyle方法：构建并学习一个统一域（unified domain），在训练阶段将所有样本映射到该域并在上面优化；测试阶段将未见域样本同样投影到统一域再进行预测。该方法包含理论分析以证明其缩小域间差距的有效性。

Result: 在多种设置下，ConstStyle稳健优于现有方法；尤其在训练域极少时，相比次优方法最高可提升约19.82%的准确率。

Conclusion: ConstStyle通过将所有训练和测试样本投影到一个统一域，有效缓解域偏移问题，从而提升了域泛化性能。

Abstract: Deep neural networks often suffer performance drops when test data
distribution differs from training data. Domain Generalization (DG) aims to
address this by focusing on domain-invariant features or augmenting data for
greater diversity. However, these methods often struggle with limited training
domains or significant gaps between seen (training) and unseen (test) domains.
To enhance DG robustness, we hypothesize that it is essential for the model to
be trained on data from domains that closely resemble unseen test domains-an
inherently difficult task due to the absence of prior knowledge about the
unseen domains. Accordingly, we propose ConstStyle, a novel approach that
leverages a unified domain to capture domain-invariant features and bridge the
domain gap with theoretical analysis. During training, all samples are mapped
onto this unified domain, optimized for seen domains. During testing, unseen
domain samples are projected similarly before predictions. By aligning both
training and testing data within this unified domain, ConstStyle effectively
reduces the impact of domain shifts, even with large domain gaps or few seen
domains. Extensive experiments demonstrate that ConstStyle consistently
outperforms existing methods across diverse scenarios. Notably, when only a
limited number of seen domains are available, ConstStyle can boost accuracy up
to 19.82\% compared to the next best approach.

</details>


### [69] [Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction](https://arxiv.org/abs/2509.05992)
*Zekun Zhou,Yanru Gong,Liu Shi,Qiegen Liu*

Main category: cs.CV

TL;DR: STRIDE通过稀疏条件联合训练、时变重加权引导、线性回归分布校正和双网络子频域优化，显著提高稀疏视图CT重建质量。


<details>
  <summary>Details</summary>
Motivation: 针对稀疏视图CT重建中信息缺失、视图不足导致结构与细节恢复困难、以及引导过程中分布不一致等问题，提出一种能够逐步感知稀疏视图信息并修正分布偏移的扩散模型以提升重建质量。

Method: 方法包括：1) 基于稀疏条件概率的联合训练机制以完成缺失投影视图的补全与全局信息建模；2) 提出时变稀疏条件重加权的引导策略，在扩散去噪过程中动态调整条件权重；3) 使用线性回归修正已知与生成数据间的分布偏移；4) 设计双网络并行架构，在多子频率分量上进行全局校正与优化。

Result: 在公开与真实数据集上的实验显示，STRIDE较最佳基线方法最高提升PSNR 2.58 dB、SSIM提升2.37%，MSE降低0.236，重建图像在结构一致性、细节恢复和伪影抑制方面表现优异且泛化稳健。

Conclusion: 本文提出了一种名为STRIDE的引导扩散模型，用于稀疏视图CT重建，通过稀疏条件概率联合训练、时变重加权指导和线性回归校正实现高质量重建，同时采用双网络并行结构在子频域上进行全局修正。

Abstract: Diffusion models have demonstrated remarkable generative capabilities in
image processing tasks. We propose a Sparse condition Temporal Rewighted
Integrated Distribution Estimation guided diffusion model (STRIDE) for
sparse-view CT reconstruction. Specifically, we design a joint training
mechanism guided by sparse conditional probabilities to facilitate the model
effective learning of missing projection view completion and global information
modeling. Based on systematic theoretical analysis, we propose a temporally
varying sparse condition reweighting guidance strategy to dynamically adjusts
weights during the progressive denoising process from pure noise to the real
image, enabling the model to progressively perceive sparse-view information.
The linear regression is employed to correct distributional shifts between
known and generated data, mitigating inconsistencies arising during the
guidance process. Furthermore, we construct a dual-network parallel
architecture to perform global correction and optimization across multiple
sub-frequency components, thereby effectively improving the model capability in
both detail restoration and structural preservation, ultimately achieving
high-quality image reconstruction. Experimental results on both public and real
datasets demonstrate that the proposed method achieves the best improvement of
2.58 dB in PSNR, increase of 2.37\% in SSIM, and reduction of 0.236 in MSE
compared to the best-performing baseline methods. The reconstructed images
exhibit excellent generalization and robustness in terms of structural
consistency, detail restoration, and artifact suppression.

</details>


### [70] [S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion](https://arxiv.org/abs/2509.05999)
*Diana-Alexandra Sas,Florin Oniga*

Main category: cs.CV

TL;DR: 用外部分割先验去耦合地融合到单目3D检测特征中，可在KITTI上提升对小目标的检测效果，且无需增加检测模型复杂度或增加预测分支。


<details>
  <summary>Details</summary>
Motivation: 单目输入缺乏深度线索导致深度估计困难，作者旨在验证是否通过引入额外的分割先验（不增加检测器参数或联合训练）即可改善检测性能，尤其是对小目标的表现。

Method: 对现有基于CNN或Transformer的单目3D检测后台，采用去耦合策略：先用独立分割模型生成语义/实例分割信息，然后将这些分割先验直接嵌入并融合进检测模型的特征表示中，而不对检测网络进行扩展或联合训练分割分支。

Result: 在KITTI 3D数据集上，所提方法相比仅使用RGB特征的同类架构，在检测小目标（行人、自行车手）上有明显提升，证明理解输入（分割先验）能在一定程度上替代额外传感器或大规模训练数据的需求。

Conclusion: 在不增加检测模型复杂度或额外预测分支的情况下，将预先计算的分割信息作为先验注入并融合到特征空间，可有效提升单目3D目标检测对小目标（行人、自行车手）深度估计与检测性能。

Abstract: Monocular 3D Object Detection represents a challenging Computer Vision task
due to the nature of the input used, which is a single 2D image, lacking in any
depth cues and placing the depth estimation problem as an ill-posed one.
Existing solutions leverage the information extracted from the input by using
Convolutional Neural Networks or Transformer architectures as feature
extraction backbones, followed by specific detection heads for 3D parameters
prediction. In this paper, we introduce a decoupled strategy based on injecting
precomputed segmentation information priors and fusing them directly into the
feature space for guiding the detection, without expanding the detection model
or jointly learning the priors. The focus is on evaluating the impact of
additional segmentation information on existing detection pipelines without
adding additional prediction branches. The proposed method is evaluated on the
KITTI 3D Object Detection Benchmark, outperforming the equivalent architecture
that relies only on RGB image features for small objects in the scene:
pedestrians and cyclists, and proving that understanding the input data can
balance the need for additional sensors or training data.

</details>


### [71] [Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation](https://arxiv.org/abs/2509.06000)
*Jose Sosa,Dan Pineau,Arunkumar Rathinam,Abdelrahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 将含时序信息的运动感知heatmap与光流融入ViT特征，用于航天器2D关键点检测并通过PnP恢复6-DoF位姿，在公开数据集上优于单帧基线并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有单张图像静态关键点定位方法未能利用太空任务中丰富的时间动态信息；作者希望通过引入光流和时序热图来提升关键点定位与位姿估计准确性及鲁棒性。

Method: 基于ViT编码器提取图像特征，结合预训练的光流模型（提供运动信息）生成motion-aware heatmaps用于2D关键点定位；之后用PnP算法将估计的2D关键点与已知3D模型进行对应，恢复6-DoF位姿。训练评估使用SPADES-RGB数据集，并在SPARK-2024的真实与合成数据上测试泛化性能。

Result: 在SPADES-RGB上，相较于单张图像基线方法，提出方法在2D关键点定位和6-DoF位姿估计上均有提升；在SPARK-2024上显示出较好的跨数据集泛化能力。

Conclusion: 该论文提出将人体姿态估计中的时序信息利用方法迁移至航天器位姿估计领域，通过融合ViT图像特征与预训练光流模型生成的运动线索，使用运动感知heatmap提升2D关键点检测，并通过PnP求解6自由度位姿。

Abstract: Monocular 6-DoF pose estimation plays an important role in multiple
spacecraft missions. Most existing pose estimation approaches rely on single
images with static keypoint localisation, failing to exploit valuable temporal
information inherent to space operations. In this work, we adapt a deep
learning framework from human pose estimation to the spacecraft pose estimation
domain that integrates motion-aware heatmaps and optical flow to capture motion
dynamics. Our approach combines image features from a Vision Transformer (ViT)
encoder with motion cues from a pre-trained optical flow model to localise 2D
keypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers
6-DoF poses from known 2D-3D correspondences. We train and evaluate our method
on the SPADES-RGB dataset and further assess its generalisation on real and
synthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates
improved performance over single-image baselines in both 2D keypoint
localisation and 6-DoF pose estimation. Furthermore, it shows promising
generalisation capabilities when testing on different data distributions.

</details>


### [72] [Khana: A Comprehensive Indian Cuisine Dataset](https://arxiv.org/abs/2509.06006)
*Omkar Prabhu*

Main category: cs.CV

TL;DR: 提出Khana：面向印度菜系的食物图像基准数据集，80类、约131K张、提供分类/分割/检索基线，弥补印度美食在现有数据集中的覆盖不足。


<details>
  <summary>Details</summary>
Motivation: 现有食物图像数据集难以全面覆盖印度菜系的地域多样性与复杂烹饪形式，影响相关模型在印度食物识别与应用中的效果，因而需要一个专门且规模适中的基准数据集。

Method: 构建了印度菜谱分类体系并从网络/人工采集与清洗图像，统一裁剪为500x500像素，标注80个类别，总计约131K张图像；并使用若干现有最先进的分类、分割与检索模型进行基线评估。

Result: 发布了Khana数据集（80类、约131K张、500x500分辨率），并给出分类、分割与检索任务上的基线实验结果，证明数据集具有挑战性同时可用于推动相关研究与实际应用。

Conclusion: Khana填补了现有食物数据集在印度美食覆盖上的空白，提供了标准化的分类、分割与检索基准，对研究与应用均具有重要价值。

Abstract: As global interest in diverse culinary experiences grows, food image models
are essential for improving food-related applications by enabling accurate food
recognition, recipe suggestions, dietary tracking, and automated meal planning.
Despite the abundance of food datasets, a noticeable gap remains in capturing
the nuances of Indian cuisine due to its vast regional diversity, complex
preparations, and the lack of comprehensive labeled datasets that cover its
full breadth. Through this exploration, we uncover Khana, a new benchmark
dataset for food image classification, segmentation, and retrieval of dishes
from Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian
cuisine and offering around 131K images in the dataset spread across 80 labels,
each with a resolution of 500x500 pixels. This paper describes the dataset
creation process and evaluates state-of-the-art models on classification,
segmentation, and retrieval as baselines. Khana bridges the gap between
research and development by providing a comprehensive and challenging benchmark
for researchers while also serving as a valuable resource for developers
creating real-world applications that leverage the rich tapestry of Indian
cuisine. Webpage: https://khana.omkar.xyz

</details>


### [73] [BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users](https://arxiv.org/abs/2509.06010)
*Wanyin Cheng,Zanxi Ruan*

Main category: cs.CV

TL;DR: 为解决BLV用户提出的模糊视觉问题，该文提出BLaVe-CoT：生成多答案、逐一定位并用链式思考判断答案是否一致，从而更好应对多答案/多区域的不确定性，显著提升VQA在助残场景的表现。


<details>
  <summary>Details</summary>
Motivation: 动机是现有VQA系统假设单一答案和单一区域，但BLV用户因拍摄质量和表述困难导致问题模糊且存在多重合理答案，需设计能够处理人类不确定性和多区域对应的VQA系统以提升辅助可及性。

Method: 方法包括三步：1) 用LoRA微调的BLIP-2生成多样候选答案；2) 使用PolyFormer为每个答案做空间定位（grounding）；3) 引入chain-of-thought模块来推理这些答案是否指向相同或不同的图像区域，并据此判断答案一致性。

Result: 在VQA-AnswerTherapy基准上，BLaVe-CoT超过了前置方法，并在含有模糊和视觉噪声的助残场景中更鲁棒；同时作者公开了代码以促进后续研究。

Conclusion: 该论文提出了BLaVe-CoT框架，有效处理BLV用户在真实场景中因模糊图像和模糊问题带来的多答案、多区域问题，通过生成多样答案、逐一定位并用链式推理判断答案一致性，从而比以往方法在VQA-AnswerTherapy上表现更好。

Abstract: Visual Question Answering (VQA) holds great potential for assisting Blind and
Low Vision (BLV) users, yet real-world usage remains challenging. Due to visual
impairments, BLV users often take blurry or poorly framed photos and face
difficulty in articulating specific questions about what they cannot fully see.
As a result, their visual questions are frequently ambiguous, and different
users may interpret them in diverse ways. This leads to multiple valid answers,
each grounded in different image regions-posing a mismatch with conventional
VQA systems that assume a single answer and region. To bridge this gap, we
present BLaVe-CoT, a VQA framework designed to reason about answer consistency
in the face of ambiguity. Our method proposes diverse candidate answers using a
LoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,
and finally applies a chain-of-thought reasoning module to assess whether the
answers refer to the same or different regions. Evaluated on the
VQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves
more robust to the ambiguity and visual noise common in assistive settings.
This work highlights the need for VQA systems that can adapt to real human
uncertainty and provide inclusive support for BLV users. To foster further
research and accessibility applications, we have made the code publicly
available at https://github.com/Accecwan/BLaVe-CoT.

</details>


### [74] [Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection](https://arxiv.org/abs/2509.06011)
*Zhenhai Weng,Zhongliang Yu*

Main category: cs.CV

TL;DR: 本文通过构建大规模UAV专用数据集（UAVDE-2M、UAVCAP-15k）并在YOLO-World-v2中加入CAGE跨注意力门控融合模块，显著提升了开放词汇目标检测在无人机与遥感影像上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大规模OVD预训练数据主要为地面自然图像，存在显著域差，导致模型在无人机影像（俯视、小目标、密集场景）上的性能大幅下降，因此需要专门的UAV数据与模块来弥补这一差距。

Method: 一是设计并使用改进的UAV-Label标注引擎构建两个UAV数据集：UAVDE-2M（约200万实例、1800类）和UAVCAP-15k（约1.5万图像）；二是在YOLO-World-v2骨干上引入Cross-Attention Gated Enhancement Fusion (CAGE)模块，通过跨注意力与门控融合机制增强特征交互与语义对齐；最后在VisDrone和SIMD数据集上进行大量实验验证。

Result: 构建的数据集规模大、类别丰富，结合CAGE模块的YOLO-World-v2在VisDrone和SIMD上表现出明显性能提升（文中宣称在UAV影像与遥感场景下效果验证成功），表明数据+模型的策略有效缩小域差并提高OVD在UAV上的泛化能力。

Conclusion: 该工作针对UAV图像与主流地面图像在语义和视角上的域差提出了数据与模型双管齐下的解决方案，结合大规模UAV专属数据集和改进的检测模块，显著提升了在无人机影像上的开放词汇目标检测性能。

Abstract: Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology
for applications involving Unmanned Aerial Vehicles (UAVs). However, the
prevailing large-scale datasets for OVD pre-training are predominantly composed
of ground-level, natural images. This creates a significant domain gap, causing
models trained on them to exhibit a substantial drop in performance on UAV
imagery. To address this limitation, we first propose a refined UAV-Label
engine. Then we construct and introduce UAVDE-2M(contains over 2,000,000
instances and 1800 categories) and UAVCAP-15k(contains over 15,000 images).
Furthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE)
module and integrate it into the YOLO-World-v2 architecture. Finally, extensive
experiments on the VisDrone and SIMD datasets verify the effectiveness of our
proposed method for applications in UAV-based imagery and remote sensing.

</details>


### [75] [Micro-Expression Recognition via Fine-Grained Dynamic Perception](https://arxiv.org/abs/2509.06015)
*Zhiwen Shao,Yifan Cheng,Fan Zhang,Xuehuai Shi,Canlin Li,Lizhuang Ma,Dit-yan Yeung*

Main category: cs.CV

TL;DR: 提出通过帧级排序与动态图像重建的细粒度动态感知框架，显著提升微表情识别，缓解数据稀缺。


<details>
  <summary>Details</summary>
Motivation: 微表情短暂且细微，现有方法要么依赖手工特征和关键帧，要么受限于小规模低多样性数据的深网络训练，因而需要一种能更好编码细微时序动态且缓解数据不足的新框架。

Method: 设计了一个局部-全局特征感知Transformer用于帧表示学习；引入排序评分器为每帧分配时序秩分数，并在时间维上池化得到动态表示；共享该动态表示于MER分类模块和基于编码器-解码器的动态图像构建模块，利用重建任务增强对细微面部动作的捕捉并缓解数据稀缺。

Result: 在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3四个数据集上，FDP在F1-score上分别较此前最好结果提升了4.05%、2.50%、7.71%和2.11%，同时在动态图像构建上表现良好。

Conclusion: 本文提出的FDP框架通过对帧级特征进行时序排序并结合局部-全局特征感知的Transformer、排序评分器和动态图像构建任务，有效编码微表情的动态信息，从而显著提升了微表情识别性能。

Abstract: Facial micro-expression recognition (MER) is a challenging task, due to the
transience, subtlety, and dynamics of micro-expressions (MEs). Most existing
methods resort to hand-crafted features or deep networks, in which the former
often additionally requires key frames, and the latter suffers from small-scale
and low-diversity training data. In this paper, we develop a novel fine-grained
dynamic perception (FDP) framework for MER. We propose to rank frame-level
features of a sequence of raw frames in chronological order, in which the rank
process encodes the dynamic information of both ME appearances and motions.
Specifically, a novel local-global feature-aware transformer is proposed for
frame representation learning. A rank scorer is further adopted to calculate
rank scores of each frame-level feature. Afterwards, the rank features from
rank scorer are pooled in temporal dimension to capture dynamic representation.
Finally, the dynamic representation is shared by a MER module and a dynamic
image construction module, in which the former predicts the ME category, and
the latter uses an encoder-decoder structure to construct the dynamic image.
The design of dynamic image construction task is beneficial for capturing
facial subtle actions associated with MEs and alleviating the data scarcity
issue. Extensive experiments show that our method (i) significantly outperforms
the state-of-the-art MER methods, and (ii) works well for dynamic image
construction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%
over the previous best results in terms of F1-score on the CASME II, SAMM,
CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at
https://github.com/CYF-cuber/FDP.

</details>


### [76] [DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion](https://arxiv.org/abs/2509.06023)
*Mengmeng Liu,Michael Ying Yang,Jiuming Liu,Yunpeng Zhang,Jiangtao Li,Sander Oude Elberink,George Vosselman,Hao Cheng*

Main category: cs.CV

TL;DR: DVLO4D通过稀疏时空融合（Sparse Query Fusion、Temporal Interaction and Update、Temporal Clip Training+Collective Average Loss）提升视觉-激光雷达里程计的精度与鲁棒性，KITTI/Argoverse上取得SOTA，推理82 ms。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-激光雷达里程计方法在传感器未对齐、未充分利用时间信息以及需手工调参以适应不同配置时，精度与鲁棒性受限，故提出稀疏时空融合方法以提升性能与泛化能力。

Method: 提出三大模块：1) Sparse Query Fusion，用稀疏LiDAR查询实现高效多模态融合；2) Temporal Interaction and Update，将预测的时序位姿与当前帧信息融合，为位姿估计提供更好初始化并抑制累积误差；3) Temporal Clip Training结合Collective Average Loss，对多帧损失进行聚合以实现全局优化并减小尺度漂移。

Result: 在KITTI与Argoverse里程计数据集上，DVLO4D在位姿精度与鲁棒性上达到了领先水平，推理时间约82 ms，显示出实时部署潜力。

Conclusion: DVLO4D通过稀疏时空融合显著提升了视觉-激光雷达里程计的精度和鲁棒性，解决了传感器对齐、时间信息利用不足及长期尺度漂移问题，并在KITTI与Argoverse上取得了SOTA表现且具实时潜力。

Abstract: Visual-LiDAR odometry is a critical component for autonomous system
localization, yet achieving high accuracy and strong robustness remains a
challenge. Traditional approaches commonly struggle with sensor misalignment,
fail to fully leverage temporal information, and require extensive manual
tuning to handle diverse sensor configurations. To address these problems, we
introduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse
spatial-temporal fusion to enhance accuracy and robustness. Our approach
proposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse
LiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction
and Update module that integrates temporally-predicted positions with current
frame data, providing better initialization values for pose estimation and
enhancing model's robustness against accumulative errors; and (3) a Temporal
Clip Training strategy combined with a Collective Average Loss mechanism that
aggregates losses across multiple frames, enabling global optimization and
reducing the scale drift over long sequences. Extensive experiments on the
KITTI and Argoverse Odometry dataset demonstrate the superiority of our
proposed DVLO4D, which achieves state-of-the-art performance in terms of both
pose accuracy and robustness. Additionally, our method has high efficiency,
with an inference time of 82 ms, possessing the potential for the real-time
deployment.

</details>


### [77] [Analysis of Blood Report Images Using General Purpose Vision-Language Models](https://arxiv.org/abs/2509.06033)
*Nadia Bakhsheshi,Hamid Beigy*

Main category: cs.CV

TL;DR: General-purpose VLMs can help analyze blood report images and improve patient understanding, but findings are preliminary due to limited data and variable model performance.


<details>
  <summary>Details</summary>
Motivation: Patients struggle to interpret blood reports; automated VLM analysis could improve understanding, reduce anxiety, and make health information more accessible.

Method: Comparative evaluation of Qwen-VL-Max, Gemini 2.5 Pro, and Llama 4 Maverick on 100 blood report images; prompted each with clinically relevant questions per report; processed answers with Sentence-BERT for similarity-based evaluation.

Result: VLMs provided generally clear interpretations from images, indicating feasibility for patient-facing tools; however performance varies and conclusions limited by small dataset.

Conclusion: General-purpose VLMs show promise for preliminary blood report analysis but need cautious interpretation due to dataset limits and model variability.

Abstract: The reliable analysis of blood reports is important for health knowledge, but
individuals often struggle with interpretation, leading to anxiety and
overlooked issues. We explore the potential of general-purpose Vision-Language
Models (VLMs) to address this challenge by automatically analyzing blood report
images. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini
2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of
100 diverse blood report images. Each model was prompted with clinically
relevant questions adapted to each blood report. The answers were then
processed using Sentence-BERT to compare and evaluate how closely the models
responded. The findings suggest that general-purpose VLMs are a practical and
promising technology for developing patient-facing tools for preliminary blood
report analysis. Their ability to provide clear interpretations directly from
images can improve health literacy and reduce the limitations to understanding
complex medical information. This work establishes a foundation for the future
development of reliable and accessible AI-assisted healthcare applications.
While results are encouraging, they should be interpreted cautiously given the
limited dataset size.

</details>


### [78] [TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection](https://arxiv.org/abs/2509.06035)
*Jiaming Cui*

Main category: cs.CV

TL;DR: 针对无人机输电线路小缺陷检测，TinyDef-DETR通过保留细节的下采样、边缘感知卷积、双域多尺度注意力和难度自适应回归损失，显著提升小目标检测性能且计算开销有限。


<details>
  <summary>Details</summary>
Motivation: 无人机自动巡检面临小型、模糊缺陷在复杂背景下难以检测的问题；传统检测器因下采样丢失细节、轻量主干边界敏感性差、全局与局部信息整合不足而效果受限。

Method: 提出基于DETR的TinyDef-DETR，包括：1)无步幅的space-to-depth模块实现无损下采样；2)边缘增强卷积用于边界敏感特征提取；3)跨阶段双域多尺度注意力模块结合全局上下文与局部细节；4)Focaler-Wise-SIoU回归损失提升小目标定位。

Result: 在CSG-ADCD数据集上，TinyDef-DETR在精度与召回上显著优于竞争基线，尤其在小目标子集获得显著增益；计算开销仅有小幅增加。在VisDrone上的额外实验证明了方法的泛化能力。

Conclusion: 本文提出TinyDef-DETR，针对无人机输电线路小目标缺陷检测进行了有效改进，综合提升了精度和召回率，尤其对小目标子集提升明显，并在VisDrone上验证了泛化性。

Abstract: Automated inspection of transmission lines using UAVs is hindered by the
difficulty of detecting small and ambiguous defects against complex
backgrounds. Conventional detectors often suffer from detail loss due to
strided downsampling, weak boundary sensitivity in lightweight backbones, and
insufficient integration of global context with local cues. To address these
challenges, we propose TinyDef-DETR, a DETR-based framework designed for
small-defect detection. The method introduces a stride-free space-to-depth
module for lossless downsampling, an edge-enhanced convolution for
boundary-aware feature extraction, a cross-stage dual-domain multi-scale
attention module to jointly capture global and local information, and a
Focaler-Wise-SIoU regression loss to improve localization of small objects.
Experiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR
achieves substantial improvements in both precision and recall compared to
competitive baselines, with particularly notable gains on small-object subsets,
while incurring only modest computational overhead. Further validation on the
VisDrone benchmark confirms the generalization capability of the proposed
approach. Overall, the results indicate that integrating detail-preserving
downsampling, edge-sensitive representations, dual-domain attention, and
difficulty-adaptive regression provides a practical and efficient solution for
UAV-based small-defect inspection in power grids.

</details>


### [79] [BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models](https://arxiv.org/abs/2509.06040)
*Yuming Li,Yikai Wang,Yuying Zhu,Zhongyu Zhao,Ming Lu,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: BranchGRPO reduces rollout cost via branch sampling, stabilizes learning with tree-based advantage and dense rewards, and prunes redundant paths—achieving +16% alignment and -50% training time.


<details>
  <summary>Details</summary>
Motivation: High compute and instability in GRPO-based alignment due to on-policy rollouts, excessive SDE steps, and sparse rewards motivated a method to reduce cost and stabilize training while preserving exploration.

Method: Introduce branch sampling policy that updates SDE sampling with shared computation across common prefixes; use a tree-based advantage estimator with dense process-level rewards; apply path and depth pruning to eliminate low-reward and redundant branches.

Result: BranchGRPO improved alignment scores by 16% over strong baselines and reduced training time by 50% in experiments on image and video preference alignment.

Conclusion: BranchGRPO effectively reduces computational cost and improves alignment performance by introducing branch sampling, tree-based advantage estimation, and pruning strategies, demonstrating substantial gains on image and video alignment tasks.

Abstract: Recent advancements in aligning image and video generative models via GRPO
have achieved remarkable gains in enhancing human preference alignment.
However, these methods still face high computational costs from on-policy
rollouts and excessive SDE sampling steps, as well as training instability due
to sparse rewards. In this paper, we propose BranchGRPO, a novel method that
introduces a branch sampling policy updating the SDE sampling process. By
sharing computation across common prefixes and pruning low-reward paths and
redundant depths, BranchGRPO substantially lowers the per-update compute cost
while maintaining or improving exploration diversity. This work makes three
main contributions: (1) a branch sampling scheme that reduces rollout and
training cost; (2) a tree-based advantage estimator incorporating dense
process-level rewards; and (3) pruning strategies exploiting path and depth
redundancy to accelerate convergence and boost performance. Experiments on
image and video preference alignment show that BranchGRPO improves alignment
scores by 16% over strong baselines, while cutting training time by 50%.

</details>


### [80] [Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities](https://arxiv.org/abs/2509.06041)
*Mohammad Ahangarkiasari,Hassan Pouraria*

Main category: cs.CV

TL;DR: 针对GNN难以建模高分辨率图长程依赖的问题，提出一种层次化多阶段GNN，通过池化/反池化逐级建模全局到局部交互，在矩形腔体自然对流数据集上显著提升了预测准确性与长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统高保真CFD计算成本高、需要专家知识，不利于快速迭代；现有数据驱动GNN在不规则网格上的表现有优势，但难以捕捉高分辨率图结构中的长程依赖，于是提出多尺度方法弥补这一短板。

Method: 设计了一种多阶段GNN架构，利用逐级下采样（池化）与上采样（反池化）在不同空间尺度上建模长程依赖，并在不规则网格上直接操作图节点与边特征；在矩形腔体自然对流问题上构建新的CFD数据集进行训练与评估。

Result: 在所构建的矩形封闭腔体自然对流数据集上，提出模型在预测精度、训练效率以及长期误差累积方面均优于SOTA GNN基线，验证了多阶段层次化处理的有效性。

Conclusion: 提出的多阶段GNN通过层次化的池化与反池化操作，有效捕捉了全局到局部的多尺度交互，从而在封闭腔体自然对流模拟中优于现有GNN基线。

Abstract: Buoyancy-driven heat transfer in closed cavities serves as a canonical
testbed for thermal design High-fidelity CFD modelling yields accurate thermal
field solutions, yet its reliance on expert-crafted physics models, fine
meshes, and intensive computation limits rapid iteration. Recent developments
in data-driven modeling, especially Graph Neural Networks (GNNs), offer new
alternatives for learning thermal-fluid behavior directly from simulation data,
particularly on irregular mesh structures. However, conventional GNNs often
struggle to capture long-range dependencies in high-resolution graph
structures. To overcome this limitation, we propose a novel multi-stage GNN
architecture that leverages hierarchical pooling and unpooling operations to
progressively model global-to-local interactions across multiple spatial
scales. We evaluate the proposed model on our newly developed CFD dataset
simulating natural convection within a rectangular cavities with varying aspect
ratios where the bottom wall is isothermal hot, the top wall is isothermal
cold, and the two vertical walls are adiabatic. Experimental results
demonstrate that the proposed model achieves higher predictive accuracy,
improved training efficiency, and reduced long-term error accumulation compared
to state-of-the-art (SOTA) GNN baselines. These findings underscore the
potential of the proposed multi-stage GNN approach for modeling complex heat
transfer in mesh-based fluid dynamics simulations.

</details>


### [81] [Home-made Diffusion Model from Scratch to Hatch](https://arxiv.org/abs/2509.06068)
*Shih-Ying Yeh*

Main category: cs.CV

TL;DR: HDM通过XUT架构与一套高效训练策略，在低成本硬件上实现了高质量1024x1024文本到图像生成，表明小规模精心设计的模型能成为大模型的可行替代方案。


<details>
  <summary>Details</summary>
Motivation: 降低高质量文本到图像扩散模型的训练与推理门槛，使得使用消费级硬件的研究者与小团队也能训练出高分辨率生成模型，推动大模型依赖的替代方案。

Method: 提出Cross-U-Transformer (XUT)——在U形结构中用cross-attention替代传统跳连以增强特征融合；结合TREAD加速、shifted square crop任意长宽比训练策略及渐进分辨率扩展的训练配方；并使用343M参数的小模型进行训练以验证架构与训练策略的有效性。

Result: 在四块RTX5090上以$535-620的训练成本实现了1024x1024生成，展示了在计算资源大幅减少的情况下仍获得竞争性质量和组合一致性，并观察到模型具备诸如直观相机控制等涌现能力。

Conclusion: HDM表明通过精心设计的模型架构与训练策略，小规模模型也能在有限硬件上实现高质量1024x1024文本到图像生成，具备很高的计算效率和较低训练成本，适合个人或小型机构使用。

Abstract: We introduce Home-made Diffusion Model (HDM), an efficient yet powerful
text-to-image diffusion model optimized for training (and inferring) on
consumer-grade hardware. HDM achieves competitive 1024x1024 generation quality
while maintaining a remarkably low training cost of $535-620 using four RTX5090
GPUs, representing a significant reduction in computational requirements
compared to traditional approaches. Our key contributions include: (1)
Cross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer
(XUT), that employs cross-attention for skip connections, providing superior
feature integration that leads to remarkable compositional consistency; (2) a
comprehensive training recipe that incorporates TREAD acceleration, a novel
shifted square crop strategy for efficient arbitrary aspect-ratio training, and
progressive resolution scaling; and (3) an empirical demonstration that smaller
models (343M parameters) with carefully crafted architectures can achieve
high-quality results and emergent capabilities, such as intuitive camera
control. Our work provides an alternative paradigm of scaling, demonstrating a
viable path toward democratizing high-quality text-to-image generation for
individual researchers and smaller organizations with limited computational
resources.

</details>


### [82] [High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization](https://arxiv.org/abs/2509.06082)
*Anuraag Mishra,Andrea Gilch,Benjamin Apeleo Zubiri,Jan Rolfes,Frauke Liers*

Main category: cs.CV

TL;DR: 作者通过训练边缘检测网络并将其输出作为优化先验，成功提高了纳米/微层析的重建质量，尤其在保留尖锐界面与材料均质性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于投影的纳米/微层析重建在存在均质材料相和尖锐界面时常出现模糊与伪影，作者希望利用已学到的边缘信息提升界面清晰度与材料均匀性。

Method: 训练一个神经网络对小图像块中的边缘进行识别，然后将网络输出作为先验融入数学优化模型；优化在尊重预测边缘的同时仍依据原始投影数据决定最终解，从而平衡先验与数据拟合。

Result: 在实验数据上，该方法相比基线算法显著增强了界面锐度与材料相的均匀性，消除了模糊，生成更高质量的重建图像。

Conclusion: 该论文提出了一种结合深度学习边缘检测与数学优化的图像重建方法，针对由均质材料相和清晰边界构成的样本能显著提高重建质量，减少模糊与伪影。

Abstract: In this work, we develop a novel technique for reconstructing images from
projection-based nano- and microtomography. Our contribution focuses on
enhancing reconstruction quality, particularly for specimen composed of
homogeneous material phases connected by sharp edges. This is accomplished by
training a neural network to identify edges within subpictures. The trained
network is then integrated into a mathematical optimization model, to reduce
artifacts from previous reconstructions. To this end, the optimization approach
favors solutions according to the learned predictions, however may also
determine alternative solutions if these are strongly supported by the raw
data. Hence, our technique successfully incorporates knowledge about the
homogeneity and presence of sharp edges in the sample and thereby eliminates
blurriness. Our results on experimental datasets show significant enhancements
in interface sharpness and material homogeneity compared to benchmark
algorithms. Thus, our technique produces high-quality reconstructions,
showcasing its potential for advancing tomographic imaging techniques.

</details>


### [83] [MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.06096)
*Yiwen Ye,Yicheng Wu,Xiangde Luo,He Zhang,Ziyang Chen,Ting Dang,Yanning Zhang,Yong Xia*

Main category: cs.CV

TL;DR: MedSeqFT通过MDS样本选择与LoRA蒸馏的顺序微调，实现了在医疗3D分割任务上更好地性能与迁移性，同时保留预训练知识。


<details>
  <summary>Details</summary>
Motivation: 现有微调策略（并行微调和多任务微调）分别存在任务隔离或需同时访问所有数据且难以增量集成的缺陷，需一种能顺序适配新任务并保留共享知识的方法。

Method: 提出了两个核心模块：最大数据相似性（MDS）用于选择与预训练分布最相似的下游样本；知识与泛化保留微调（K&G RFT）基于LoRA的知识蒸馏策略，用以在任务特化与保持预训练知识之间取得平衡。

Result: 在两个多任务数据集上的十个3D分割任务中，MedSeqFT优于现有最优微调方法，平均Dice提升约3.0%；在两个未见任务（COVID-19-20和Kidney）上也展现出更好的迁移性，尤其对肿瘤分割效果显著；损失面与参数变化分析表明方法更稳健。

Conclusion: MedSeqFT通过顺序微调策略，在适应新任务的同时保留预训练知识，实现了对医疗图像分割任务的稳健提升。

Abstract: Foundation models have become a promising paradigm for advancing medical
image analysis, particularly for segmentation tasks where downstream
applications often emerge sequentially. Existing fine-tuning strategies,
however, remain limited: parallel fine-tuning isolates tasks and fails to
exploit shared knowledge, while multi-task fine-tuning requires simultaneous
access to all datasets and struggles with incremental task integration. To
address these challenges, we propose MedSeqFT, a sequential fine-tuning
framework that progressively adapts pre-trained models to new tasks while
refining their representational capacity. MedSeqFT introduces two core
components: (1) Maximum Data Similarity (MDS) selection, which identifies
downstream samples most representative of the original pre-training
distribution to preserve general knowledge, and (2) Knowledge and
Generalization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge
distillation scheme that balances task-specific adaptation with the retention
of pre-trained knowledge. Extensive experiments on two multi-task datasets
covering ten 3D segmentation tasks demonstrate that MedSeqFT consistently
outperforms state-of-the-art fine-tuning strategies, yielding substantial
performance gains (e.g., an average Dice improvement of 3.0%). Furthermore,
evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT
enhances transferability, particularly for tumor segmentation. Visual analyses
of loss landscapes and parameter variations further highlight the robustness of
MedSeqFT. These results establish sequential fine-tuning as an effective,
knowledge-retentive paradigm for adapting foundation models to evolving
clinical tasks. Code will be released.

</details>


### [84] [PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology](https://arxiv.org/abs/2509.06105)
*Yating Huang,Ziyan Huang,Lintao Xiang,Qijun Yang,Hujun Yin*

Main category: cs.CV

TL;DR: 提出用于病理领域的VL评测基准PathoHR-Bench与病理特定的多模态对比学习训练策略，显著提升细粒度病理语义理解与跨模态推理，达成SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 病理图像具有高度结构相似性和细微形态差异，现有VL模型难以处理病理报告中的复杂推理，影响自动化肿瘤诊断的准确性与临床应用价值。

Method: 构建PathoHR-Bench评测基准；提出病理专用VL训练策略，包含样本增强与扰动生成，结合多模态对比学习来强化细粒度病理表示学习。

Result: 在PathoHR-Bench上以及六个额外的病理数据集上实现领先性能，表明所提方法能更好地捕捉细粒度的病理特征并改善跨模态理解与推理能力。

Conclusion: 本文提出PathoHR-Bench基准，用以评估视觉-语言模型在病理领域的层次语义理解与组合推理能力，并指出现有VL模型在复杂跨模态关系建模上存在不足。为提升模型性能，作者设计了病理特定的视觉-语言训练方案，通过生成增强与扰动样本进行多模态对比学习，最终在PathoHR-Bench及另外六个病理数据集上取得SOTA表现。

Abstract: Accurate analysis of pathological images is essential for automated tumor
diagnosis but remains challenging due to high structural similarity and subtle
morphological variations in tissue images. Current vision-language (VL) models
often struggle to capture the complex reasoning required for interpreting
structured pathological reports. To address these limitations, we propose
PathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in
hierarchical semantic understanding and compositional reasoning within the
pathology domain. Results of this benchmark reveal that existing VL models fail
to effectively model intricate cross-modal relationships, hence limiting their
applicability in clinical setting. To overcome this, we further introduce a
pathology-specific VL training scheme that generates enhanced and perturbed
samples for multimodal contrastive learning. Experimental evaluations
demonstrate that our approach achieves state-of-the-art performance on
PathoHR-Bench and six additional pathology datasets, highlighting its
effectiveness in fine-grained pathology representation.

</details>


### [85] [CARDIE: clustering algorithm on relevant descriptors for image enhancement](https://arxiv.org/abs/2509.06116)
*Giulia Bonino,Luca Alberto Rizzo*

Main category: cs.CV

TL;DR: 提出CARDIE，一种基于亮度与色彩的无监督图像聚类方法，并通过量化亮度与局部方差变化验证其在图像增强任务上的有效性，进而通过簇驱动的数据重采样提升色调映射与去噪性能；代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统自动图像聚类虽在语义任务中有效，但其所生成的簇对图像增强任务意义有限，亟需一种与亮度/色彩相关的无监督聚类方法。

Method: 提出基于色彩与亮度信息的图像表征并进行聚类；引入衡量增强算法对亮度分布与局部方差影响的量化方法；使用这些簇进行增强数据集的重采样并在色调映射与去噪任务上验证性能提升。

Result: 实验表明：1) CARDIE生成的簇在图像增强相关性上优于基于语义属性的簇；2) 利用这些簇重采样数据集能提升色调映射与去噪算法的表现；3) 公布了代码以促进复现。

Conclusion: CARDIE在无监督条件下基于亮度与色彩内容构建了更适合图像增强任务的簇，从而提高了数据重采样后对色调映射与去噪算法的性能。

Abstract: Automatic image clustering is a cornerstone of computer vision, yet its
application to image enhancement remains limited, primarily due to the
difficulty of defining clusters that are meaningful for this specific task. To
address this issue, we introduce CARDIE, an unsupervised algorithm that
clusters images based on their color and luminosity content. In addition, we
introduce a method to quantify the impact of image enhancement algorithms on
luminance distribution and local variance. Using this method, we demonstrate
that CARDIE produces clusters more relevant to image enhancement than those
derived from semantic image attributes. Furthermore, we demonstrate that CARDIE
clusters can be leveraged to resample image enhancement datasets, leading to
improved performance for tone mapping and denoising algorithms. To encourage
adoption and ensure reproducibility, we publicly release CARDIE code on our
GitHub.

</details>


### [86] [SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks](https://arxiv.org/abs/2509.06122)
*Tang Sui,Songxi Yang,Qunying Huang*

Main category: cs.CV

TL;DR: 提出SpecSwin3D，用3D移窗Transformer+级联训练与优化波段序列，从5波段多光谱恢复224波段高光谱，显著提升重建和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 多光谱与高光谱图像在空间与光谱分辨率间存在权衡，现有方法难以同时兼顾空间细节与光谱保真，且远离输入波段的谱带重建误差较大。

Method: 采用基于3D shifted-window的Transformer架构（SpecSwin3D），输入重复/排序后的5个多光谱波段序列以构建3D体数据，结合级联训练策略（逐步扩展重建光谱范围）来稳定学习并减少远谱带误差。

Result: 在重建任务上达成PSNR 35.82 dB、SAM 2.40°、SSIM 0.96，相较基线MHF-Net提升约5.6 dB PSNR并使ERGAS减半。并在土地利用分类和火烧区分割两个下游任务中展示了实际效用。

Conclusion: 本文提出SpecSwin3D，通过3D移窗Transformer从5个多光谱波段重建224个高光谱波段，能同时保持空间细节与光谱保真。采用级联训练逐步扩展光谱范围并设计优化波段序列以提升谱间关系建模。

Abstract: Multispectral and hyperspectral imagery are widely used in agriculture,
environmental monitoring, and urban planning due to their complementary spatial
and spectral characteristics. A fundamental trade-off persists: multispectral
imagery offers high spatial but limited spectral resolution, while
hyperspectral imagery provides rich spectra at lower spatial resolution. Prior
hyperspectral generation approaches (e.g., pan-sharpening variants, matrix
factorization, CNNs) often struggle to jointly preserve spatial detail and
spectral fidelity. In response, we propose SpecSwin3D, a transformer-based
model that generates hyperspectral imagery from multispectral inputs while
preserving both spatial and spectral quality. Specifically, SpecSwin3D takes
five multispectral bands as input and reconstructs 224 hyperspectral bands at
the same spatial resolution. In addition, we observe that reconstruction errors
grow for hyperspectral bands spectrally distant from the input bands. To
address this, we introduce a cascade training strategy that progressively
expands the spectral range to stabilize learning and improve fidelity.
Moreover, we design an optimized band sequence that strategically repeats and
orders the five selected multispectral bands to better capture pairwise
relations within a 3D shifted-window transformer framework. Quantitatively, our
model achieves a PSNR of 35.82 dB, SAM of 2.40{\deg}, and SSIM of 0.96,
outperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by
more than half. Beyond reconstruction, we further demonstrate the practical
value of SpecSwin3D on two downstream tasks, including land use classification
and burnt area segmentation.

</details>


### [87] [RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving](https://arxiv.org/abs/2509.06142)
*Zhengquan Luo,Chi Liu,Dongfu Xiao,Zhen Yu,Yueye Wang,Tianqing Zhu*

Main category: cs.CV

TL;DR: 提出RetinaGuard：一种基于特征级对抗性掩蔽和多对一知识蒸馏的框架，有效隐藏眼底图像中的视网膜年龄，兼顾图像质量与诊断实用性，对抗黑盒年龄预测器并具可扩展性。


<details>
  <summary>Details</summary>
Motivation: Retinal age predicted from fundus images can reveal sensitive biometric information, posing privacy risks; need to prevent bioinformation leakage while retaining diagnostic value.

Method: Feature-level generative adversarial masking with multiple-to-one knowledge distillation using a retinal foundation model and diverse surrogate age encoders to defend against black-box age predictors.

Result: Comprehensive evaluations show effective obfuscation of retinal age predictions, minimal impact on image quality and pathological feature representation, and flexibility to extend to other biomarkers.

Conclusion: RetinaGuard successfully obscures retinal age from fundus images while preserving visual quality and diagnostic utility, and can generalize to other medical-image biomarkers.

Abstract: The integration of AI with medical images enables the extraction of implicit
image-derived biomarkers for a precise health assessment. Recently, retinal
age, a biomarker predicted from fundus images, is a proven predictor of
systemic disease risks, behavioral patterns, aging trajectory and even
mortality. However, the capability to infer such sensitive biometric data
raises significant privacy risks, where unauthorized use of fundus images could
lead to bioinformation leakage, breaching individual privacy. In response, we
formulate a new research problem of biometric privacy associated with medical
images and propose RetinaGuard, a novel privacy-enhancing framework that
employs a feature-level generative adversarial masking mechanism to obscure
retinal age while preserving image visual quality and disease diagnostic
utility. The framework further utilizes a novel multiple-to-one knowledge
distillation strategy incorporating a retinal foundation model and diverse
surrogate age encoders to enable a universal defense against black-box age
prediction models. Comprehensive evaluations confirm that RetinaGuard
successfully obfuscates retinal age prediction with minimal impact on image
quality and pathological feature representation. RetinaGuard is also flexible
for extension to other medical image derived biomarkers. RetinaGuard is also
flexible for extension to other medical image biomarkers.

</details>


### [88] [UniVerse-1: Unified Audio-Video Generation via Stitching of Experts](https://arxiv.org/abs/2509.06155)
*Duomin Wang,Wei Zuo,Aojie Li,Ling-Hao Chen,Xinyao Liao,Deyu Zhou,Zixin Yin,Xili Dai,Daxin Jiang,Gang Yu*

Main category: cs.CV

TL;DR: 提出 UniVerse-1，用 SoE 把视频与音乐专家深度拼接并结合在线注释流水线，绕过从头训练，在7600小时数据上微调后能生成协调的音视频，发布了 Verse-Bench 与开源实现。


<details>
  <summary>Details</summary>
Motivation: 希望在生成协调的音视频方面实现单一模型（类似 Veo-3）的能力，同时提高训练效率并避免因文本注释时间错位引起的性能下降。

Method: 采用 stitching of experts (SoE) 方法，把预训练视频专家与音乐专家对应模块深度拼接，并在训练时配合在线注释流水线实时生成时间对齐的标签，绕过从零训练。

Result: 在约7600小时数据微调后，模型在环境声生成上表现出音画协调良好、在语音生成上具有较强对齐能力；并发布了 Verse-Bench 基准数据集与开源代码以推动研究。

Conclusion: UniVerse-1 成功地将预训练的视频与音乐生成专家模型深度融合，通过 SoE 方法与在线注释流水线，提高训练效率并解决文本注释错位问题，从而在约7600小时音视频数据上微调后，实现了环境音与语音的音画协调生成。

Abstract: We introduce UniVerse-1, a unified, Veo-3-like model capable of
simultaneously generating coordinated audio and video. To enhance training
efficiency, we bypass training from scratch and instead employ a stitching of
experts (SoE) technique. This approach deeply fuses the corresponding blocks of
pre-trained video and music generation experts models, thereby fully leveraging
their foundational capabilities. To ensure accurate annotations and temporal
alignment for both ambient sounds and speech with video content, we developed
an online annotation pipeline that processes the required training data and
generates labels during training process. This strategy circumvents the
performance degradation often caused by misalignment text-based annotations.
Through the synergy of these techniques, our model, after being finetuned on
approximately 7,600 hours of audio-video data, produces results with
well-coordinated audio-visuals for ambient sounds generation and strong
alignment for speech generation. To systematically evaluate our proposed
method, we introduce Verse-Bench, a new benchmark dataset. In an effort to
advance research in audio-video generation and to close the performance gap
with state-of-the-art models such as Veo3, we make our model and code publicly
available. We hope this contribution will benefit the broader research
community. Project page: https://dorniwang.github.io/UniVerse-1/.

</details>


### [89] [UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning](https://arxiv.org/abs/2509.06165)
*Huy Le,Nhat Chung,Tung Kieu,Jingkang Yang,Ngan Le*

Main category: cs.CV

TL;DR: UNO是一个单阶段、统一的对象中心视频场景图生成框架，利用扩展slot attention、时间一致性学习和动态三元组预测，实现了跨粒度的端到端VidSGG。


<details>
  <summary>Details</summary>
Motivation: 现有VidSGG方法通常针对盒级或像素级单一粒度，依赖任务特异性架构和多阶段训练，缺乏统一且高效的端到端方案。

Method: 基于扩展的slot attention将视觉特征分解为对象slots和关系slots；引入对象时间一致性学习以在帧间保持对象表征一致；动态三元组预测模块将关系slots与对象对关联，捕捉随时间演化的交互。

Result: UNO在标准盒级与像素级基准上均取得有竞争力的性能，同时通过统一的对象中心设计提升了效率。

Conclusion: UNO提出了一个统一的单阶段面向对象的视频场景图生成框架，能够同时处理盒级和像素级任务，在参数共享和效率上优于任务专用方法。

Abstract: Video Scene Graph Generation (VidSGG) aims to represent dynamic visual
content by detecting objects and modeling their temporal interactions as
structured graphs. Prior studies typically target either coarse-grained
box-level or fine-grained panoptic pixel-level VidSGG, often requiring
task-specific architectures and multi-stage training pipelines. In this paper,
we present UNO (UNified Object-centric VidSGG), a single-stage, unified
framework that jointly addresses both tasks within an end-to-end architecture.
UNO is designed to minimize task-specific modifications and maximize parameter
sharing, enabling generalization across different levels of visual granularity.
The core of UNO is an extended slot attention mechanism that decomposes visual
features into object and relation slots. To ensure robust temporal modeling, we
introduce object temporal consistency learning, which enforces consistent
object representations across frames without relying on explicit tracking
modules. Additionally, a dynamic triplet prediction module links relation slots
to corresponding object pairs, capturing evolving interactions over time. We
evaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results
demonstrate that UNO not only achieves competitive performance across both
tasks but also offers improved efficiency through a unified, object-centric
design.

</details>


### [90] [AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models](https://arxiv.org/abs/2509.06228)
*Amna Hassan,Ilsa Afzaal,Nouman Muneeb,Aneeqa Batool,Hamail Noor*

Main category: cs.CV

TL;DR: 作者用FracAtlas数据集训练了一个轻量级自定义CNN进行骨折检测，达到了约96%准确率和0.91的F1，但迁移学习模型表现不佳，结果受数据不平衡和外部验证缺乏限制。


<details>
  <summary>Details</summary>
Motivation: 在低资源环境中，放射科专家稀缺且传统影像存在成本和辐射问题，开发自动化、轻量级的X光骨折检测工具以提高可及性和诊断效率。

Method: 使用FracAtlas数据集（4,083张肌肉骨骼X光片）训练一个自定义卷积神经网络，并与基于迁移学习的EfficientNetB0、MobileNetV2、ResNet50模型比较；报告指标包括准确率、精确率、召回率和F1分数。

Result: 自定义CNN在FracAtlas上表现最好：准确率95.96%、精确率0.94、召回率0.88、F1=0.91；迁移学习模型在此设置下表现较差，但可能受类不平衡和数据集限制影响。

Conclusion: 该研究展示了基于轻量级自定义CNN的骨折检测在FracAtlas数据集上的可行性，取得较高的准确率和F1分数，表明模型在受限资源环境中具有潜力，但受数据不平衡和外部验证缺乏限制，需谨慎解读。

Abstract: Bone fractures present a major global health challenge, often resulting in
pain, reduced mobility, and productivity loss, particularly in low-resource
settings where access to expert radiology services is limited. Conventional
imaging methods suffer from high costs, radiation exposure, and dependency on
specialized interpretation. To address this, we developed an AI-based solution
for automated fracture detection from X-ray images using a custom Convolutional
Neural Network (CNN) and benchmarked it against transfer learning models
including EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on
the publicly available FracAtlas dataset, comprising 4,083 anonymized
musculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94
precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.
Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)
performed poorly in this specific setup, these results should be interpreted in
light of class imbalance and data set limitations. This work highlights the
promise of lightweight CNNs for detecting fractures in X-rays and underscores
the importance of fair benchmarking, diverse datasets, and external validation
for clinical translation

</details>


### [91] [Exploring Light-Weight Object Recognition for Real-Time Document Detection](https://arxiv.org/abs/2509.06246)
*Lucas Wojcik,Luiz Coelho,Roger Granada,David Menotti*

Main category: cs.CV

TL;DR: 将IWPOD-Net迁移到证件检测，使用合成数据训练并以OCR质量（基于Levenshtein距离）为最终评估指标，获得更小更快且OCR性能接近SOTA的实时证件检测与矫正方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么追求更大模型以提高性能，要么追求更小模型以提高效率，但实时证件检测与矫正在实际信息检索管道中尚未充分研究；希望在保证OCR信息检索质量的前提下设计更高效的小模型方案。

Method: 将IWPOD-Net在合成ID卡数据集（NBID）上微调，并通过数据增强和在MIDV数据集上的跨验证选择最佳配置；与现有物体检测与倾斜估计方法比较，通过检测后对证件进行透视矫正并使用OCR读取，利用Levenshtein距离构建的OCR质量指标评估整体检索效果。

Result: 实验表明，在保持较小模型体积与更高效率的同时，该方法在OCR质量指标上可与现有最优方法竞争；且文档矫正无需完美即可达到良好OCR检索效果。源码已开源。

Conclusion: 本文提出将轻量级车牌检测网络IWPOD-Net迁移用于证件检测与矫正，结合合成数据集训练与跨数据集验证，提出基于Levenshtein距离的OCR质量评估，在保持较小模型与高效率的前提下，实现有竞争力的OCR检索性能。

Abstract: Object Recognition and Document Skew Estimation have come a long way in terms
of performance and efficiency. New models follow one of two directions:
improving performance using larger models, and improving efficiency using
smaller models. However, real-time document detection and rectification is a
niche that is largely unexplored by the literature, yet it remains a vital step
for automatic information retrieval from visual documents. In this work, we
strive towards an efficient document detection pipeline that is satisfactory in
terms of Optical Character Recognition (OCR) retrieval and faster than other
available solutions. We adapt IWPOD-Net, a license plate detection network, and
train it for detection on NBID, a synthetic ID card dataset. We experiment with
data augmentation and cross-dataset validation with MIDV (another synthetic ID
and passport document dataset) to find the optimal scenario for the model.
Other methods from both the Object Recognition and Skew Estimation
state-of-the-art are evaluated for comparison with our approach. We use each
method to detect and rectify the document, which is then read by an OCR system.
The OCR output is then evaluated using a novel OCR quality metric based on the
Levenshtein distance. Since the end goal is to improve automatic information
retrieval, we use the overall OCR quality as a performance metric. We observe
that with a promising model, document rectification does not have to be perfect
to attain state-of-the-art performance scores. We show that our model is
smaller and more efficient than current state-of-the-art solutions while
retaining a competitive OCR quality metric. All code is available at
https://github.com/BOVIFOCR/iwpod-doc-corners.git

</details>


### [92] [Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes](https://arxiv.org/abs/2509.06266)
*Mohsen Gholami,Ahmad Rezaei,Zhou Weimin,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: 提出面向自我视角多视图的Ego3D-Bench和可插拔的Ego3D-VLM后训练框架，通过生成基于全局3D坐标的认知地图，显著提升VLM在多视图户外空间推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在3D空间关系推理上受限，且现有数据集多为单视图或室内视频，无法反映机器人/自动驾驶等依赖自我视角多视图的真实场景需求；因此需新基准和方法推动多视图户外空间理解。

Method: 构建含8600+问答对的Ego3D-Bench（人工标注确保质量），基于自我视角多视图户外数据评估16个SOTA VLM；提出Ego3D-VLM模块化后训练方法，利用估计的全局3D坐标生成认知地图并与任意VLM集成以增强空间推理。

Result: 在Ego3D-Bench上16个SOTA模型远低于人类水平；应用Ego3D-VLM后，多选题平均提升约12%，绝对距离估计平均提升约56%，表明生成基于全局坐标的认知地图能显著改善空间推理能力。

Conclusion: 本文提出Ego3D-Bench评估基于自我视角、多视图的户外空间推理能力，并通过基准测试揭示当前VLM的显著差距；提出Ego3D-VLM后训练框架，基于估计的全局3D坐标生成认知地图，显著提升空间推理性能。

Abstract: Understanding 3D spatial relationships remains a major limitation of current
Vision-Language Models (VLMs). Prior work has addressed this issue by creating
spatial question-answering (QA) datasets based on single images or indoor
videos. However, real-world embodied AI agents such as robots and self-driving
cars typically rely on ego-centric, multi-view observations. To this end, we
introduce Ego3D-Bench, a new benchmark designed to evaluate the spatial
reasoning abilities of VLMs using ego-centric, multi-view outdoor data.
Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvement
from human annotators to ensure quality and diversity. We benchmark 16 SOTA
VLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results
reveal a notable performance gap between human level scores and VLM
performance, highlighting that current VLMs still fall short of human level
spatial understanding. To bridge this gap, we propose Ego3D-VLM, a
post-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM
generates cognitive map based on estimated global 3D coordinates, resulting in
12% average improvement on multi-choice QA and 56% average improvement on
absolute distance estimation. Ego3D-VLM is modular and can be integrated with
any existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for
advancing toward human level spatial understanding in real-world, multi-view
environments.

</details>


### [93] [AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution](https://arxiv.org/abs/2509.06282)
*Cecelia Soh,Rizhao Cai,Monalisha Paul,Dennis Sng,Alex Kot*

Main category: cs.CV

TL;DR: 首次提出基于自拍面部图像并使用Skin-Prior Adaptive ViT和对称对比正则化来无接触估计皮肤水分和TEWL，推动了AI驱动的可及皮肤评估。


<details>
  <summary>Details</summary>
Motivation: 传统SH和TEWL测量依赖专用仪器和临床环境，普通用户难以获得；因此希望通过智能手机自拍图像实现无接触、便捷的皮肤屏障指标估计。

Method: 建立SH/TEWL数据集并进行预处理，设计Skin-Prior Adaptive Vision Transformer（基于皮肤先验的自适应视觉Transformer）用于回归预测；针对标注不平衡问题引入对称性对比正则项以减轻模型偏置。

Result: 通过实验验证模型在从自拍图像估计SH/TEWL上的有效性，并证明对称性对比正则化可缓解数据标注不平衡带来的性能偏差。

Conclusion: 该文提出了首个从自拍面部图像远程估计皮肤水分(SH)和经表皮水分流失(TEWL)的系统化方案，为普及皮肤屏障功能评估提供可行路径。

Abstract: Skin health and disease resistance are closely linked to the skin barrier
function, which protects against environmental factors and water loss. Two key
physiological indicators can quantitatively represent this barrier function:
skin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH
and TEWL is valuable for the public to monitor skin conditions regularly,
diagnose dermatological issues, and personalize their skincare regimens.
However, these measurements are not easily accessible to general users unless
they visit a dermatology clinic with specialized instruments. To tackle this
problem, we propose a systematic solution to estimate SH and TEWL from selfie
facial images remotely with smartphones. Our solution encompasses multiple
stages, including SH/TEWL data collection, data preprocessing, and formulating
a novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.
Through experiments, we identified the annotation imbalance of the SH/TEWL data
and proposed a symmetric-based contrastive regularization to reduce the model
bias due to the imbalance effectively. This work is the first study to explore
skin assessment from selfie facial images without physical measurements. It
bridges the gap between computer vision and skin care research, enabling
AI-driven accessible skin analysis for broader real-world applications.

</details>


### [94] [Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding](https://arxiv.org/abs/2509.06291)
*Jiangnan Xie,Xiaolong Zheng,Liang Zheng*

Main category: cs.CV

TL;DR: PAML通过ALBEF对齐、视觉判别编码、语义原型发现与多阶段解码，有效解决开放词汇视觉定位中的对齐与泛化问题，实验证明其显著提升了新类别检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法在开放词汇视觉定位上受限，主要因视觉-语言对齐不充分、跨模态特征融合不足和语义原型利用不够，需一个系统性方法提升对新类别的识别能力。

Method: 框架包括：1) 使用ALBEF进行初始跨模态编码以改进对齐；2) 视觉判别特征编码器增强显著目标并抑制无关背景；3) 原型发现与继承机制提取并聚合多邻域语义原型以支持开放词汇识别；4) 多阶段解码器进行全面的多模态融合，最终回归边界框。

Result: 在五个基准数据集上进行的大量实验表明：在开放词汇场景上达到了SOTA结果，在标准场景上表现与现有方法相当或略优。

Conclusion: PAML提高了视觉与语言对齐、加强了跨模态融合并利用语义原型信息，从而在开放词汇场景下显著提升视觉定位性能；在标准场景仍保持竞争力。

Abstract: Visual Grounding (VG) aims to utilize given natural language queries to
locate specific target objects within images. While current transformer-based
approaches demonstrate strong localization performance in standard scene (i.e,
scenarios without any novel objects), they exhibit notable limitations in
open-vocabulary scene (i.e, both familiar and novel object categories during
testing). These limitations primarily stem from three key factors: (1)
imperfect alignment between visual and linguistic modalities, (2) insufficient
cross-modal feature fusion, and (3) ineffective utilization of semantic
prototype information. To overcome these challenges, we present Prototype-Aware
Multimodal Learning (PAML), an innovative framework that systematically
addresses these issues through several key components: First, we leverage ALBEF
to establish robust cross-modal alignment during initial feature encoding.
Subsequently, our Visual Discriminative Feature Encoder selectively enhances
salient object representations while suppressing irrelevant visual context. The
framework then incorporates a novel prototype discovering and inheriting
mechanism that extracts and aggregates multi-neighbor semantic prototypes to
facilitate open-vocabulary recognition. These enriched features undergo
comprehensive multimodal integration through our Multi-stage Decoder before
final bounding box regression. Extensive experiments across five benchmark
datasets validate our approach, showing competitive performance in standard
scene while achieving state-of-the-art results in open-vocabulary scene. Our
code is available at https://github.com/plankXie/PAML.

</details>


### [95] [Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning](https://arxiv.org/abs/2509.06306)
*Zhang Jing,Pu Nan,Xie Yu Xiang,Guo Yanming,Lu Qianqi,Zou Shiwei,Yan Jie,Chen Yan*

Main category: cs.CV

TL;DR: 将GCD推广到视频，提出MCCL（CACL+MGRE）通过一致性加权对比学习与双层记忆增强表示，构建Video-GCD基准，实验显示在视频上显著优于图像方法，强调时序信息价值。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法主要针对静态图像，缺乏利用视频的时序和多视角信息，单一静态视觉内容难以可靠发现新类别，故扩展到视频以利用时间信息提升GCD性能。

Method: 提出MCCL框架，包括两部分：Consistency-Aware Contrastive Learning (CACL)利用多视角时序特征估计未标注样本间一致性并据此加权对比损失；Memory-Guided Representation Enhancement (MGRE)引入特征级和logit级双层记忆缓冲区，提供全局上下文以增强类内紧致性和类间可分性，二者形成互相强化的循环。

Result: 在构建的Video-GCD基准（包含动作识别和鸟类分类视频数据集）上，方法显著优于从图像GCD改编的竞争方法，验证了时间信息在视频中发现新类别的重要性。

Conclusion: 该论文提出将通用类别发现（GCD）扩展到视频领域，称为Video-GCD，并提出了Memory-guided Consistency-aware Contrastive Learning（MCCL）框架，通过一致性引导投票和双层记忆库增强视频中时空特征表示，从而提升未知类别发现效果。

Abstract: Generalized Category Discovery (GCD) is an emerging and challenging
open-world problem that has garnered increasing attention in recent years. Most
existing GCD methods focus on discovering categories in static images. However,
relying solely on static visual content is often insufficient to reliably
discover novel categories. To bridge this gap, we extend the GCD problem to the
video domain and introduce a new setting, termed Video-GCD. Thus, effectively
integrating multi-perspective information across time is crucial for accurate
Video-GCD. To tackle this challenge, we propose a novel Memory-guided
Consistency-aware Contrastive Learning (MCCL) framework, which explicitly
captures temporal-spatial cues and incorporates them into contrastive learning
through a consistency-guided voting mechanism. MCCL consists of two core
components: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided
Representation Enhancement (MGRE). CACL exploits multiperspective temporal
features to estimate consistency scores between unlabeled instances, which are
then used to weight the contrastive loss accordingly. MGRE introduces a
dual-level memory buffer that maintains both feature-level and logit-level
representations, providing global context to enhance intra-class compactness
and inter-class separability. This in turn refines the consistency estimation
in CACL, forming a mutually reinforcing feedback loop between representation
learning and consistency modeling. To facilitate a comprehensive evaluation, we
construct a new and challenging Video-GCD benchmark, which includes action
recognition and bird classification video datasets. Extensive experiments
demonstrate that our method significantly outperforms competitive GCD
approaches adapted from image-based settings, highlighting the importance of
temporal information for discovering novel categories in videos. The code will
be publicly available.

</details>


### [96] [Text4Seg++: Advancing Image Segmentation via Generative Language Modeling](https://arxiv.org/abs/2509.06321)
*Mengcheng Lan,Chaofeng Chen,Jiaxing Xu,Zongrui Li,Yiping Ke,Xudong Jiang,Yingchen Yu,Yunqing Zhao,Song Bai*

Main category: cs.CV

TL;DR: 通过将分割任务重构为文本生成并引入语义描述符与R-RLE/语义砖机制，作者提出Text4Seg与Text4Seg++，实现了高效、可扩展且无需微调的MLLM文本驱动图像分割方案。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在视觉语言任务表现优秀，但难以有效集成图像分割；传统方法需额外解码器或复杂结构，限制与MLLM的融合。text-as-mask旨在消除额外解码器并利用语言建模能力直接生成分割掩码。

Method: 设计语义描述符（image-wise和box-wise），将segmentation表述为生成每个patch对应文本标签的任务；提出Row-wise Run-Length Encoding(R-RLE)以压缩语义描述符并加速推理；引入语义砖和next-brick预测将区域掩码建模为结构化序列生成。

Result: R-RLE将语义描述符长度缩短约74%，推理加速约3倍；Text4Seg和Text4Seg++在自然图像与遥感数据集上在多项基准上超越或匹配SOTA，且无需任务特定微调，兼容现有MLLM骨干。

Conclusion: 本文提出将分割任务转换为文本生成的text-as-mask范式，通过语义描述符将图像patch映射为文本标签，实现与语言建模流水线的无缝融合。引入R-RLE压缩冗余文本序列并提出box-wise语义描述符和语义砖结构以提升精度与紧凑性，最终Text4Seg++在多数据集上无需任务特定微调即超越SOTA。

Abstract: Multimodal Large Language Models (MLLMs) have shown exceptional capabilities
in vision-language tasks. However, effectively integrating image segmentation
into these models remains a significant challenge. In this work, we propose a
novel text-as-mask paradigm that casts image segmentation as a text generation
problem, eliminating the need for additional decoders and significantly
simplifying the segmentation process. Our key innovation is semantic
descriptors, a new textual representation of segmentation masks where each
image patch is mapped to its corresponding text label. We first introduce
image-wise semantic descriptors, a patch-aligned textual representation of
segmentation masks that integrates naturally into the language modeling
pipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding
(R-RLE), which compresses redundant text sequences, reducing the length of
semantic descriptors by 74% and accelerating inference by $3\times$, without
compromising performance. Building upon this, our initial framework Text4Seg
achieves strong segmentation performance across a wide range of vision tasks.
To further improve granularity and compactness, we propose box-wise semantic
descriptors, which localizes regions of interest using bounding boxes and
represents region masks via structured mask tokens called semantic bricks. This
leads to our refined model, Text4Seg++, which formulates segmentation as a
next-brick prediction task, combining precision, scalability, and generative
efficiency. Comprehensive experiments on natural and remote sensing datasets
show that Text4Seg++ consistently outperforms state-of-the-art models across
diverse benchmarks without any task-specific fine-tuning, while remaining
compatible with existing MLLM backbones. Our work highlights the effectiveness,
scalability, and generalizability of text-driven image segmentation within the
MLLM framework.

</details>


### [97] [Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap](https://arxiv.org/abs/2509.06329)
*Ruiming Du,Guangxun Zhai,Tian Qiu,Yu Jiang*

Main category: cs.CV

TL;DR: 该文通过数据集纵览、深度学习方法综述、开源基准PSS与大量实证评测，提出了面向3D植物表型的规范化研究路径，显示稀疏卷积与Transformer实例分割优越，合成数据对sim-to-real迁移益处明显，使3D植物分割更可行且数据高效。


<details>
  <summary>Details</summary>
Motivation: 精准表征植物形态对理解植物与环境/遗传关系至关重要，但3D点云分割在植物表型领域应用受限，主要因大规模标注数据缺乏、现有先进网络难以直接迁移到植物点云，以及缺乏针对植物科学的统一基准与评测协议。

Method: 综合性文献综述+方法分类总结（语义分割与实例分割的深度学习方法），提出PSS开源基准平台，并通过大量量化实验评估代表性网络与sim-to-real策略（包括稀疏卷积、Transformer实例分割、合成数据生成方法等）。

Result: 总结并比较了现有3D植物数据集与通用3D分割领域差异，证明稀疏卷积骨干与Transformer实例分割效果显著；合成数据生成（建模驱动与增强驱动）可显著降低标注需求，PSS平台实现了可复现的基准测试，公开数据与代码。

Conclusion: 本综述论文系统性梳理了3D点云在植物表型定量中的应用，识别并攻克了数据稀缺、模型移植难、评测不统一三大障碍，提出并实现了Plant Segmentation Studio(PSS)框架，实验证明稀疏卷积骨干和Transformer实例分割表现优良，合成数据（建模与增强）对sim-to-real迁移有互补价值，从而推动数据高效、可泛化的3D植物表型深度学习研究落地。

Abstract: The precise characterization of plant morphology provides valuable insights
into plant environment interactions and genetic evolution. A key technology for
extracting this information is 3D segmentation, which delineates individual
plant organs from complex point clouds. Despite significant progress in general
3D computer vision domains, the adoption of 3D segmentation for plant
phenotyping remains limited by three major challenges: i) the scarcity of
large-scale annotated datasets, ii) technical difficulties in adapting advanced
deep neural networks to plant point clouds, and iii) the lack of standardized
benchmarks and evaluation protocols tailored to plant science. This review
systematically addresses these barriers by: i) providing an overview of
existing 3D plant datasets in the context of general 3D segmentation domains,
ii) systematically summarizing deep learning-based methods for point cloud
semantic and instance segmentation, iii) introducing Plant Segmentation Studio
(PSS), an open-source framework for reproducible benchmarking, and iv)
conducting extensive quantitative experiments to evaluate representative
networks and sim-to-real learning strategies. Our findings highlight the
efficacy of sparse convolutional backbones and transformer-based instance
segmentation, while also emphasizing the complementary role of modeling-based
and augmentation-based synthetic data generation for sim-to-real learning in
reducing annotation demands. In general, this study bridges the gap between
algorithmic advances and practical deployment, providing immediate tools for
researchers and a roadmap for developing data-efficient and generalizable deep
learning solutions in 3D plant phenotyping. Data and code are available at
https://github.com/perrydoremi/PlantSegStudio.

</details>


### [98] [Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users](https://arxiv.org/abs/2509.06331)
*Md Sultanul Islam Ovi,Mainul Hossain,Md Badsha Biswas*

Main category: cs.CV

TL;DR: 论文提出了面额分类、UCDI损伤评估和基于模板的伪钞检测的统一框架，利用轻量级模型和8.2万图像数据，实现了可解释、实时、设备端的纸币评估。


<details>
  <summary>Details</summary>
Motivation: 现有纸币识别方法多聚焦于面额分类，忽视了物理磨损与伪造判别，难以应对视障用户与离线验证等低资源实际应用场景。论文旨在提出一个紧凑、可解释且适合设备端部署的统一评估系统。

Method: 采用轻量级CNN进行面额分类；设计了统一纸币损伤指数（UCDI），基于二值掩码损失、色差失真和结构特征损失计算连续可用性分数；使用基于特征的模板匹配方法进行伪钞检测。数据集包含8.2万余张带注释的真实、损坏和伪造纸币图像。

Result: Custom_CNN在保证参数量低的情况下实现了高分类性能；UCDI能持续反映纸币可用性；伪钞检测模块在多样化拍摄条件下表现可靠；整体系统支持实时、设备端推理并适用于受限环境。

Conclusion: 该论文提出了一个统一的纸币评估框架，结合面额分类、损伤量化（UCDI）和伪钞检测三部分，实现了在低资源和离线环境下的实时设备端推理，提升了系统的可用性与可解释性。

Abstract: Currency recognition systems often overlook usability and authenticity
assessment, especially in low-resource environments where visually impaired
users and offline validation are common. While existing methods focus on
denomination classification, they typically ignore physical degradation and
forgery, limiting their applicability in real-world conditions. This paper
presents a unified framework for currency evaluation that integrates three
modules: denomination classification using lightweight CNN models, damage
quantification through a novel Unified Currency Damage Index (UCDI), and
counterfeit detection using feature-based template matching. The dataset
consists of over 82,000 annotated images spanning clean, damaged, and
counterfeit notes. Our Custom_CNN model achieves high classification
performance with low parameter count. The UCDI metric provides a continuous
usability score based on binary mask loss, chromatic distortion, and structural
feature loss. The counterfeit detection module demonstrates reliable
identification of forged notes across varied imaging conditions. The framework
supports real-time, on-device inference and addresses key deployment challenges
in constrained environments. Results show that accurate, interpretable, and
compact solutions can support inclusive currency evaluation in practical
settings.

</details>


### [99] [Multi-Modal Camera-Based Detection of Vulnerable Road Users](https://arxiv.org/abs/2509.06333)
*Penelope Brown,Julie Stephany Berrio Perez,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 将RGB与热像融合并微调YOLOv8、结合类重加权和轻增强，可在复杂条件下提升对少数VRU类别的检测召回，热像提高精度，RGB→热增强进一步提高召回，640分辨率与部分主干冻结在效率—精度间最优。


<details>
  <summary>Details</summary>
Motivation: 动机在于改善行人、骑行者和摩托车手等脆弱道路使用者在低光、恶劣天气及类别不平衡数据条件下的检测性能，从而降低交通死亡率。

Method: 方法为将RGB与热红外图像融合，基于微调的YOLOv8进行训练；使用KITTI、BDD100K与Teledyne FLIR数据集；采用类别重加权、轻量级增强、部分主干冻结与不同分辨率实验来优化精度与效率；并探索RGB到热像增强以提升召回。

Result: 实验表明：热像模型在精度上表现最好；类权重损失可以提高少数类的召回；640像素分辨率与部分主干冻结在精度与效率间取得良好折衷；RGB到热像的增强提升了召回率。

Conclusion: 本文提出的多模态检测框架在检测交叉口的脆弱道路使用者（VRUs）方面显示出潜力，但仍有改进空间。

Abstract: Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists
represent more than half of global traffic deaths, yet their detection remains
challenging in poor lighting, adverse weather, and unbalanced data sets. This
paper presents a multimodal detection framework that integrates RGB and thermal
infrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,
BDD100K, and Teledyne FLIR datasets, with class re-weighting and light
augmentations to improve minority-class performance and robustness, experiments
show that 640-pixel resolution and partial backbone freezing optimise accuracy
and efficiency, while class-weighted losses enhance recall for rare VRUs.
Results highlight that thermal models achieve the highest precision, and
RGB-to-thermal augmentation boosts recall, demonstrating the potential of
multimodal detection to improve VRU safety at intersections.

</details>


### [100] [Harnessing Object Grounding for Time-Sensitive Video Understanding](https://arxiv.org/abs/2509.06335)
*Tz-Ying Wu,Sharath Nittur Sridhar,Subarna Tripathi*

Main category: cs.CV

TL;DR: 提出GO-Tokenizer，通过在线编码紧凑对象表示，解决在提示中加入对象文本引起的长度与噪声问题，从而提升Video-LLM在时间敏感视频理解任务上的性能并具备较好泛化性。


<details>
  <summary>Details</summary>
Motivation: 假设TSV任务可从帧内的对象级信息（grounded objects）中受益；直接把对象文本加入提示能提升性能但会增加token长度并易受噪声影响，因此需要更高效的编码方式。

Method: 提出GO-Tokenizer：利用现成目标检测器在推理时对帧内对象进行检测并编码为紧凑表示，作为Video-LLM的附加模块；在预训练阶段引入该模块，以替代在提示中加入对象文本描述的方法。

Result: 在多种模型、数据集和视频理解任务（如时序定位推理和密集字幕生成）上，带GO-Tokenizer的模型优于原始Video-LLM和将对象文本加入提示的对照方法，表明方法具有泛化能力。

Conclusion: GO-Tokenizer有效提升Video-LLM在时间敏感视频理解（TSV）任务上的表现，通过轻量级模块在推理时编码紧凑的对象信息，避免了文本描述带来的长序列和噪声问题。

Abstract: We propose to improve the time-sensitive video understanding (TSV) capability
of video large language models (Video-LLMs) with grounded objects (GO). We
hypothesize that TSV tasks can benefit from GO within frames, which is
supported by our preliminary experiments on LITA, a state-of-the-art Video-LLM
for reasoning temporal localization. While augmenting prompts with textual
description of these object annotations improves the performance of LITA, it
also introduces extra token length and susceptibility to the noise in object
level information. To address this, we propose GO-Tokenizer, a lightweight
add-on module for Video-LLMs leveraging off-the-shelf object detectors to
encode compact object information on the fly. Experimental results demonstrate
that pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its
counterpart utilizing textual description of objects in the prompt. The gain
generalizes across different models, datasets and video understanding tasks
such as reasoning temporal localization and dense captioning.

</details>


### [101] [Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing](https://arxiv.org/abs/2509.06336)
*Jeongmin Yu,Susang Kim,Kisu Lee,Taekyoung Kwon,Won-Yong Shin,Ha Young Kim*

Main category: cs.CV

TL;DR: MVP-FAS通过多视角槽注意力和多文本补丁对齐，利用多重文本提示与patch级信息增强CLIP的局部线索捕捉与语义鲁棒性，从而在跨域FAS任务上取得更好泛化与SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP-based FAS模型未充分利用patch tokens，且仅用单一文本提示限制了模型泛化能力，难以捕捉关键欺骗线索并受域偏差影响。

Method: 提出MVS模块从CLIP的patch embeddings提取局部细节与全局上下文，利用多视角（多文本提示）生成多槽位表示；提出MTPA模块将补丁与多种文本表征对齐，增强语义鲁棒性。两者结合构成MVP-FAS训练与推理流程。

Result: 在多项跨域人脸反欺骗数据集上，MVP-FAS显著优于先前方法，达到了新的SOTA表现，证明多文本与patch级对齐能提升泛化能力。

Conclusion: MVP-FAS通过多视角槽注意力和多文本补丁对齐模块，改进了CLIP在跨域人脸反欺骗任务中的局部线索捕捉和文本多样性，从而提升了泛化性能。

Abstract: Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain
performance by employing vision-language models like CLIP. However, existing
CLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,
failing to detect critical spoofing clues. Moreover, these models rely on a
single text prompt per class (e.g., 'live' or 'fake'), which limits
generalization. To address these issues, we propose MVP-FAS, a novel framework
incorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text
Patch Alignment (MTPA). Both modules utilize multiple paraphrased texts to
generate generalized features and reduce dependence on domain-specific text.
MVS extracts local detailed spatial features and global context from patch
embeddings by leveraging diverse texts with multiple perspectives. MTPA aligns
patches with multiple text representations to improve semantic robustness.
Extensive experiments demonstrate that MVP-FAS achieves superior generalization
performance, outperforming previous state-of-the-art methods on cross-domain
datasets. Code: https://github.com/Elune001/MVP-FAS.

</details>


### [102] [A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study](https://arxiv.org/abs/2509.06351)
*Krithik Ramesh,Ritvik Koneru*

Main category: cs.CV

TL;DR: 该研究利用ResNet-50构建了一个可解释且可复现的统一深度学习管道，融合了类平衡、鲁棒增强与校准技术，能够同时对结肠组织学图像与结肠镜帧进行分类，旨在简化并加速结直肠疾病的诊断流程。


<details>
  <summary>Details</summary>
Motivation: 传统结直肠疾病诊断依赖分别对组织病理学图像和内镜视频进行独立评估，流程繁琐且易受主观差异影响。作者希望通过一个统一的深度学习管道整合多种诊断模态，从而提升诊断效率、减少人为变异并实现可解释的自动化辅助诊断。

Method: 构建基于ResNet-50的卷积神经网络，输入来自PathMNIST的组织学静态图像和HyperKvasir的结肠镜视频帧。训练过程中采用类平衡策略（如重采样或加权损失）、数据增强（旋转、翻转、颜色扰动等）以及概率校准（如温度缩放）来提升泛化和可靠性；对模型可解释性进行了后验分析（如可视化类激活图）。

Result: 在PathMNIST和HyperKvasir数据集上的试点实验显示，统一模型在两类模态上均达到有竞争力的分类性能（具体指标论文摘要未给出），并通过可解释性分析和校准方法证明了预测的可靠性与可复现性。

Conclusion: 该论文提出并验证了一个统一的深度学习管道，使用ResNet-50同时对结直肠组织病理切片和结肠镜视频帧进行分类，旨在减少传统诊断流程中的分离评估和低效问题。实验表明，在PathMNIST和HyperKvasir两个公共数据集上，结合类平衡训练、鲁棒增强和校准方法后，模型能在多模态输入上取得可解释且可复现的诊断性能。

Abstract: Colorectal diseases, including inflammatory conditions and neoplasms, require
quick, accurate care to be effectively treated. Traditional diagnostic
pipelines require extensive preparation and rely on separate, individual
evaluations on histological images and colonoscopy footage, introducing
possible variability and inefficiencies. This pilot study proposes a unified
deep learning network that uses convolutional neural networks (CN N s) to
classify both histopathological slides and colonoscopy video frames in one
pipeline. The pipeline integrates class-balancing learning, robust
augmentation, and calibration methods to ensure accurate results. Static colon
histology images were taken from the PathMNIST dataset, and the lower
gastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.
The CNN architecture used was ResNet-50. This study demonstrates an
interpretable and reproducible diagnostic pipeline that unifies multiple
diagnostic modalities to advance and ease the detection of colorectal diseases.

</details>


### [103] [MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification](https://arxiv.org/abs/2509.06367)
*Aswini Kumar Patra,Lingaraj Sahoo*

Main category: cs.CV

TL;DR: 提出一个受ResNet/DenseNet/MobileNet启发的轻量级混合CNN并结合梯度范数影响函数实现机器遗忘，在马铃薯田航拍旱害识别任务中达到了参数大幅减少且精度仍有竞争力，适合资源受限场景应用。


<details>
  <summary>Details</summary>
Motivation: 传统方法时间消耗大、人工成本高；现有CNN和Transformer模型参数量大，不适合资源受限和实时农业监测场景，因此需要轻量、准确且可适应的模型。

Method: 结合ResNet、DenseNet和MobileNet的思想设计轻量级网络结构，使用参数共享和深度可分离卷积等技术减少参数量；引入基于梯度范数的影响函数以度量并移除样本对模型的影响，实现机器遗忘。

Result: 在带专家标注的马铃薯田空中影像数据集上，模型在保证高精度的同时将可训练参数显著降低，且机器遗忘机制能有效移除指定样本影响，降低计算成本并提高实用性。

Conclusion: 本文提出一个轻量级混合CNN框架，显著降低可训练参数（约15倍减少），并引入基于梯度范数的影响函数的机器“遗忘”机制，实现对特定训练样本影响的定向移除，从而提高模型适应性。

Abstract: Drought stress is a major threat to global crop productivity, making its
early and precise detection essential for sustainable agricultural management.
Traditional approaches, though useful, are often time-consuming and
labor-intensive, which has motivated the adoption of deep learning methods. In
recent years, Convolutional Neural Network (CNN) and Vision Transformer
architectures have been widely explored for drought stress identification;
however, these models generally rely on a large number of trainable parameters,
restricting their use in resource-limited and real-time agricultural settings.
To address this challenge, we propose a novel lightweight hybrid CNN framework
inspired by ResNet, DenseNet, and MobileNet architectures. The framework
achieves a remarkable 15-fold reduction in trainable parameters compared to
conventional CNN and Vision Transformer models, while maintaining competitive
accuracy. In addition, we introduce a machine unlearning mechanism based on a
gradient norm-based influence function, which enables targeted removal of
specific training data influence, thereby improving model adaptability. The
method was evaluated on an aerial image dataset of potato fields with
expert-annotated healthy and drought-stressed regions. Experimental results
show that our framework achieves high accuracy while substantially lowering
computational costs. These findings highlight its potential as a practical,
scalable, and adaptive solution for drought stress monitoring in precision
agriculture, particularly under resource-constrained conditions.

</details>


### [104] [Your Super Resolution Model is not Enough for Tackling Real-World Scenarios](https://arxiv.org/abs/2509.06387)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 提出SAAM，一种轻量可插拔的尺度感知注意力模块，用于将固定尺度SR模型扩展到任意尺度，结合SimAM与梯度方差损失，在多种骨干与放大倍数下实现高质量、低开销的多尺度超分。


<details>
  <summary>Details</summary>
Motivation: 传统单图像超分模型在不同放大倍数间泛化能力差，难以满足现实场景中多样的放大需求，需一种能在保持模型轻量和性能的前提下扩展固定尺度模型到任意尺度的方法。

Method: 提出一种可插拔的尺度感知注意力模块（SAAM），包含轻量的尺度自适应特征提取与上采样，并引入无参数的简单注意力模块（SimAM）作为引导以及梯度方差损失以增强细节锐利度；将该模块集成到现有固定尺度SR骨干中实现任意尺度超分。

Result: 在多种基准数据集与不同整数/非整数放大倍数下，SAAM在与多款SOTA骨干结合时表现出与或优于原模型的重建质量，且引入的计算与参数开销较低，证明了其实用性与鲁棒性。

Conclusion: SAAM能有效地使固定尺度超分模型支持任意放大倍数，且计算开销小，适配多种主流骨干网络，提升或保持在整数与非整数放大倍数下的性能。

Abstract: Despite remarkable progress in Single Image Super-Resolution (SISR),
traditional models often struggle to generalize across varying scale factors,
limiting their real-world applicability. To address this, we propose a plug-in
Scale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR
models with the ability to perform arbitrary-scale SR. SAAM employs
lightweight, scale-adaptive feature extraction and upsampling, incorporating
the Simple parameter-free Attention Module (SimAM) for efficient guidance and
gradient variance loss to enhance sharpness in image details. Our method
integrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,
HiT-SR, OverNet), delivering competitive or superior performance across a wide
range of integer and non-integer scale factors. Extensive experiments on
benchmark datasets demonstrate that our approach enables robust multi-scale
upscaling with minimal computational overhead, offering a practical solution
for real-world scenarios.

</details>


### [105] [AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery](https://arxiv.org/abs/2509.06396)
*Lorenz Achim Kuhn,Daniel Abler,Jonas Richiardi,Andreas F. Hottinger,Luis Schiappacasse,Vincent Dunet,Adrien Depeursinge,Vincent Andrearczyk*

Main category: cs.CV

TL;DR: 自动化整理纵向SRS-MRI数据并用聚类与机器学习（含图模型）预测BM 12个月反应，最大AUC约0.90，展示了可扩展的数据策划与临床决策支持潜力。


<details>
  <summary>Details</summary>
Motivation: 临床上纵向MRI标注工作量巨大，多数随访仅靠观察评估；研究纵向影像的生长轨迹和早期预测有助于提高治疗成功率与减少毒性，优化个体化决策。

Method: 构建自动化数据整理管线筛选出896个病灶（177名患者）>360天随访数据；使用数据驱动聚类识别5类增长轨迹；用梯度提升和图机器学习对12个月病灶级反应进行预测，输入为术前及首次随访或多时点配置。

Result: 识别出5个主要增长轨迹并对应不同最终反应；基于术前与首次随访影像的梯度提升模型可达AUC 0.90（95%CI 0.88-0.92）；图机器学习在多时点输入下也能稳定达到AUC 0.88（95%CI 0.86-0.90）。

Conclusion: 本文实现了一个自动化流程来整理大规模纵向SRS治疗数据，并通过数据驱动聚类和机器学习预测脑转移瘤（BM）对治疗的12个月反应，达到了高AUC性能，显示出自动化评估与临床决策支持的潜力。

Abstract: Brain Metastases (BM) are a large contributor to mortality of patients with
cancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored
with Magnetic Resonance Imaging (MRI) at regular follow-up intervals according
to treatment guidelines. Analyzing and quantifying this longitudinal imaging
represents an intractable workload for clinicians. As a result, follow-up
images are not annotated and merely assessed by observation. Response to
treatment in longitudinal imaging is being studied, to better understand growth
trajectories and ultimately predict treatment success or toxicity as early as
possible. In this study, we implement an automated pipeline to curate a large
longitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in
177 patients who were monitored for >360 days at approximately two-month
intervals at Lausanne University Hospital (CHUV). We use a data-driven
clustering to identify characteristic trajectories. In addition, we predict 12
months lesion-level response using classical as well as graph machine learning
Graph Machine Learning (GML). Clustering revealed 5 dominant growth
trajectories with distinct final response categories. Response prediction
reaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first
follow-up MRI with gradient boosting. Similarly, robust predictive performance
of up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more
flexibility with a single model for multiple input time-points configurations.
Our results suggest potential automation and increased precision for the
comprehensive assessment and prediction of BM response to SRS in longitudinal
MRI. The proposed pipeline facilitates scalable data curation for the
investigation of BM growth patterns, and lays the foundation for clinical
decision support systems aiming at optimizing personalized care.

</details>


### [106] [3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom](https://arxiv.org/abs/2509.06400)
*Matthieu Gendrin,Stéphane Pateux,Théo Ladune*

Main category: cs.CV

TL;DR: 针对3DoF+场景的坐标量化问题，基于投影误差与距离平方倒数关系提出球坐标量化方案，实验证明在Garden场景上能带来更好的率失真表现。


<details>
  <summary>Details</summary>
Motivation: 在大场景中输入视图仅采自有限空间区域时，常见的6DoF重建实际上只需支持相机在中心位置附近的小幅移动（3DoF+），因此坐标量化策略应考虑深度（距离）对投影误差的放大效应以提高压缩效率。

Method: 分析坐标量化对像素投影误差的影响，证明投影误差与被投影点距离的平方的倒数成正比；基于该关系提出在球坐标（半径、俯仰、方位）下进行量化的方案，分配比特更多给对投影敏感的径向分量。

Result: 理论推导表明投影误差与点距离的平方的倒数成正比；在Garden场景上进行的率失真实验显示，球坐标量化在相同期望比特率下能降低平均投影误差，并在某些比特率下提升视觉质量。

Conclusion: 提出的基于球坐标的量化方案在理论上能更好地控制由于坐标量化引起的投影误差，尤其对远距离点的误差抑制更有针对性；在Garden场景上的率失真实验展示了可观的性能提升。

Abstract: 3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene
reconstruction. With a number of views of a given object or scene, the
algorithm trains a model composed of 3D gaussians, which enables the production
of novel views from arbitrary points of view. This freedom of movement is
referred to as 6DoF for 6 degrees of freedom: a view is produced for any
position (3 degrees), orientation of camera (3 other degrees). On large scenes,
though, the input views are acquired from a limited zone in space, and the
reconstruction is valuable for novel views from the same zone, even if the
scene itself is almost unlimited in size. We refer to this particular case as
3DoF+, meaning that the 3 degrees of freedom of camera position are limited to
small offsets around the central position. Considering the problem of
coordinate quantization, the impact of position error on the projection error
in pixels is studied. It is shown that the projection error is proportional to
the squared inverse distance of the point being projected. Consequently, a new
quantization scheme based on spherical coordinates is proposed. Rate-distortion
performance of the proposed method are illustrated on the well-known Garden
scene.

</details>


### [107] [VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results](https://arxiv.org/abs/2509.06413)
*Yixiao Li,Xin Li,Chris Wei Zhou,Shuo Xing,Hadi Amirpour,Xiaoshuai Hao,Guanghui Yue,Baoquan Zhao,Weide Liu,Xiaoyuan Yang,Zhengzhong Tu,Xinyu Li,Chuanbiao Song,Chenqi Zhang,Jun Lan,Huijia Zhu,Weiqiang Wang,Xiaoyan Sun,Shishun Tian,Dongyang Yan,Weixia Zhang,Junlin Chen,Wei Sun,Zhihua Wang,Zhuohang Shi,Zhizun Luo,Hang Ouyang,Tianxin Xiao,Fan Yang,Zhaowang Wu,Kaixin Deng*

Main category: cs.CV

TL;DR: ISRGC-Q挑战基于ISRGen-QA数据集，专注于GAN和扩散模型生成的超分图像质量评估，吸引大量关注并产生4个SOTA提交，推动生成式超分图像质量评估研究。


<details>
  <summary>Details</summary>
Motivation: 现有SR-IQA数据集与评价方法主要针对传统插值或预测型超分方法，对生成式超分（如GAN、扩散模型）引入的特殊伪影和感知变化缺乏专门评估。因此需要一个以生成式超分为主的评测平台与数据集来分析并推动更有效的感知质量评估指标。

Method: 基于ISRGen-QA数据集发起公开挑战（ISRGC-Q），组织VQualA竞赛的一部分。收集最新生成式超分（GAN和扩散模型）结果，设置测试平台，吸引108名注册者，4支队伍提交最终有效方案并提供事实表（fact sheets），并对提交的方法在数据集上进行评价与排名。

Result: 挑战吸引108名注册者，4支队伍提交有效方案并取得SOTA水平的性能，说明参与方法在ISRGen-QA上表现优异。项目代码和数据集在GitHub公开。

Conclusion: 该挑战成功构建并推动了针对生成式超分图像感知质量评估的研究，参赛方法在ISRGen-QA数据集上达到了SOTA表现，表明现有指标和方法在评估生成式超分图像质量方面取得进展。

Abstract: This paper presents the ISRGC-Q Challenge, built upon the Image
Super-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and
organized as part of the Visual Quality Assessment (VQualA) Competition at the
ICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment
(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated
by the latest generative approaches, including Generative Adversarial Networks
(GANs) and diffusion models. The primary goal of this challenge is to analyze
the unique artifacts introduced by modern super-resolution techniques and to
evaluate their perceptual quality effectively. A total of 108 participants
registered for the challenge, with 4 teams submitting valid solutions and fact
sheets for the final testing phase. These submissions demonstrated
state-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is
publicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.

</details>


### [108] [Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models](https://arxiv.org/abs/2509.06415)
*Jaemin Son,Sujin Choi,Inyong Yun*

Main category: cs.CV

TL;DR: 提出了一种在VLM前进行图像patch级非文本过滤的轻量级token剪枝方法，通过二分类器和最大池化细化来降低计算负担并保持性能。


<details>
  <summary>Details</summary>
Motivation: VLM在文档理解任务中效果显著但计算资源消耗大，需在保持性能的同时降低计算成本。

Method: 使用一个二分类的patch级分类器来识别并去除非文本区域，随后通过最大池化的细化步骤恢复被切分的文本区域以增强空间连贯性。

Result: 在真实世界的文档数据集上，方法显著降低了计算成本，同时保持了可比的准确率。

Conclusion: 该论文提出了一种轻量级的token剪枝框架，通过在输入VLM前移除文档图像中的非信息背景区域来减少计算开销，同时保持准确率。

Abstract: Recent progress in vision-language models (VLMs) has led to impressive
results in document understanding tasks, but their high computational demands
remain a challenge. To mitigate the compute burdens, we propose a lightweight
token pruning framework that filters out non-informative background regions
from document images prior to VLM processing. A binary patch-level classifier
removes non-text areas, and a max-pooling refinement step recovers fragmented
text regions to enhance spatial coherence. Experiments on real-world document
datasets demonstrate that our approach substantially lowers computational
costs, while maintaining comparable accuracy.

</details>


### [109] [Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM](https://arxiv.org/abs/2509.06422)
*Hua Zhang,Changjiang Luo,Ruoyu Chen*

Main category: cs.CV

TL;DR: 提出 Phantom-Insight：通过 LLM 融合时空特征、动态视觉令牌评分与提示微调 SAM 以及前后景解耦训练，显著提升视频伪装物检测的边缘分离和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有 SAM 基础方法因模型冻结导致边缘分离困难，MLLM 基础方法因大模型倾向合并前景和背景导致可分性差，需要一种方法同时解决边缘细节和前后景分离问题。

Method: 方法包括：以时间和空间线索表示视频序列，通过 LLM 进行特征融合；引入动态前景视觉令牌评分模块和提示网络生成多重线索，引导并微调 SAM；提出前景-背景解耦学习策略，分别生成并训练前后景线索以增强可分性。

Result: 在 MoCA-Mask 数据集上取得了多项指标的 SOTA 表现，并在 CAD2016 上展示了对未见伪装物体的良好泛化能力。

Conclusion: Phantom-Insight 有效提升了视频伪装物体检测（VCOD）的性能，通过结合 SAM 和 MLLM 的优势并引入多重提示与前后景解耦学习策略，使模型在边缘细节分离和前后景可分性上均有显著改进。

Abstract: Video camouflaged object detection (VCOD) is challenging due to dynamic
environments. Existing methods face two main issues: (1) SAM-based methods
struggle to separate camouflaged object edges due to model freezing, and (2)
MLLM-based methods suffer from poor object separability as large language
models merge foreground and background. To address these issues, we propose a
novel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the
separability of object edge details, we represent video sequences with temporal
and spatial clues and perform feature fusion via LLM to increase information
density. Next, multiple cues are generated through the dynamic foreground
visual token scoring module and the prompt network to adaptively guide and
fine-tune the SAM model, enabling it to adapt to subtle textures. To enhance
the separability of objects and background, we propose a decoupled
foreground-background learning strategy. By generating foreground and
background cues separately and performing decoupled training, the visual token
can effectively integrate foreground and background information independently,
enabling SAM to more accurately segment camouflaged objects in the video.
Experiments on the MoCA-Mask dataset show that Phantom-Insight achieves
state-of-the-art performance across various metrics. Additionally, its ability
to detect unseen camouflaged objects on the CAD2016 dataset highlights its
strong generalization ability.

</details>


### [110] [When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection](https://arxiv.org/abs/2509.06427)
*Rabin Dulal,Lihong Zheng,Muhammad Ashad Kabir*

Main category: cs.CV

TL;DR: 提出一种基于Grounding DINO的零样本、无需标注的牛嘴部检测方法，通过自然语言提示实现检测，在不依赖标注数据的情况下达到76.8% mAP@0.5，适合行业部署。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量标注数据并对训练数据敏感，限制了对新牛个体或不同环境的泛化能力；为提高可扩展性和部署便捷性，提出无需标注数据的检测方法。

Method: 利用视觉-语言模型Grounding DINO，采用自然语言提示（prompt）引导检测过程，从而实现零样本检测；评估指标为mAP@0.5，实验得到76.8%的性能。

Result: 在真实世界数据上实现了mAP@0.5=76.8%，证明了在无监督/无标注条件下的可行性，提供了行业可用的、便于部署的方案。

Conclusion: 该论文提出了基于Grounding DINO的零样本牛嘴部（muzzle）检测框架，能够在无需任务特定训练或标注数据的情况下，实现对不同品种和环境下牛嘴部的定位。

Abstract: Muzzle patterns are among the most effective biometric traits for cattle
identification. Fast and accurate detection of the muzzle region as the region
of interest is critical to automatic visual cattle identification.. Earlier
approaches relied on manual detection, which is labor-intensive and
inconsistent. Recently, automated methods using supervised models like YOLO
have become popular for muzzle detection. Although effective, these methods
require extensive annotated datasets and tend to be trained data-dependent,
limiting their performance on new or unseen cattle. To address these
limitations, this study proposes a zero-shot muzzle detection framework based
on Grounding DINO, a vision-language model capable of detecting muzzles without
any task-specific training or annotated data. This approach leverages natural
language prompts to guide detection, enabling scalable and flexible muzzle
localization across diverse breeds and environments. Our model achieves a mean
Average Precision (mAP)@0.5 of 76.8\%, demonstrating promising performance
without requiring annotated data. To our knowledge, this is the first research
to provide a real-world, industry-oriented, and annotation-free solution for
cattle muzzle detection. The framework offers a practical alternative to
supervised methods, promising improved adaptability and ease of deployment in
livestock monitoring applications.

</details>


### [111] [Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment](https://arxiv.org/abs/2509.06442)
*Yixiao Li,Xiaoyuan Yang,Guanghui Yue,Jun Fu,Qiuping Jiang,Xu Jia,Paul L. Rosin,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: 提出PBAN：结合双向注意、分组多尺度可变形卷积和子信息激励的感知导向网络，用于超分图像的全参考质量评估，在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有用于比较和评估超分算法的全参考图像质量评估指标不足，且缺乏与生成和评估过程一致的注意建模。作者希望通过模拟人类视觉的注意机制和自适应感知失真来提升SR图像质量评价的准确性。

Method: 方法包括三大模块：图像编码器提取特征；感知导向双向注意（PBA）模块，采用双向注意机制构建对失真信息的双向视觉关注，并引入分组多尺度可变形卷积以自适应感知失真；子信息激励卷积用于引导对亚像素和子通道的注意；最后质量预测模块融合质量感知特征并回归质量得分。

Result: 在大量实验中，PBAN在SR-FR-IQA任务上优于当前最先进的方法，表明所设计的双向注意、分组多尺度可变形卷积和子信息激励模块有效提高了感知一致性和评估性能。

Conclusion: 该论文提出了面向感知的双向注意网络（PBAN）用于超分辨率图像的全参考图像质量评估，能够更好地模拟人类视觉系统对失真感知，从而提高评估性能。

Abstract: Many super-resolution (SR) algorithms have been proposed to increase image
resolution. However, full-reference (FR) image quality assessment (IQA) metrics
for comparing and evaluating different SR algorithms are limited. In this work,
we propose the Perception-oriented Bidirectional Attention Network (PBAN) for
image SR FR-IQA, which is composed of three modules: an image encoder module, a
perception-oriented bidirectional attention (PBA) module, and a quality
prediction module. First, we encode the input images for feature
representations. Inspired by the characteristics of the human visual system, we
then construct the perception-oriented PBA module. Specifically, different from
existing attention-based SR IQA methods, we conceive a Bidirectional Attention
to bidirectionally construct visual attention to distortion, which is
consistent with the generation and evaluation processes of SR images. To
further guide the quality assessment towards the perception of distorted
information, we propose Grouped Multi-scale Deformable Convolution, enabling
the proposed method to adaptively perceive distortion. Moreover, we design
Sub-information Excitation Convolution to direct visual perception to both
sub-pixel and sub-channel attention. Finally, the quality prediction module is
exploited to integrate quality-aware features and regress quality scores.
Extensive experiments demonstrate that our proposed PBAN outperforms
state-of-the-art quality assessment methods.

</details>


### [112] [Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark](https://arxiv.org/abs/2509.06456)
*Zongyi Xu,Zhongpeng Lang,Yilong Chen,Shanshan Zhao,Xiaoshui Huang,Yifan Zuo,Yan Zhang,Qianni Zhang,Xinbo Gao*

Main category: cs.CV

TL;DR: 构建了最大的真实跨源点云数据集Cross3DReg，并提出基于重叠预测与视觉-几何注意力匹配的配准框架，有效提升跨源配准精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 跨源点云配准受限于缺乏大规模真实训练数据以及不同传感器导致的点云差异，致使特征提取和匹配困难，进而降低配准精度。为推动该领域研究，需要真实大规模数据集和鲁棒的跨源特征对齐方法。

Method: 设计了基于重叠区域预测的跨源配准框架：首先利用未对齐图像预测点云重叠区域以去除非重叠噪声点；其次提出视觉-几何注意力引导的匹配模块，融合图像和几何特征以提高跨源特征一致性并建立可靠对应，从而进行精确配准。

Result: 在构建的Cross3DReg数据集上，所提方法将相对旋转误差(RRE)降低63.2%，相对平移误差(RTE)降低40.2%，配准召回(RR)提升5.4%，达到了当前最先进的配准性能。

Conclusion: 本文提出了一个用于跨源点云配准的实用框架，并构建了当前最大的真实多模态跨源点云配准数据集Cross3DReg，实验显示方法在RRE、RTE和RR指标上均有明显提升。

Abstract: Cross-source point cloud registration, which aims to align point cloud data
from different sensors, is a fundamental task in 3D vision. However, compared
to the same-source point cloud registration, cross-source registration faces
two core challenges: the lack of publicly available large-scale real-world
datasets for training the deep registration models, and the inherent
differences in point clouds captured by multiple sensors. The diverse patterns
induced by the sensors pose great challenges in robust and accurate point cloud
feature extraction and matching, which negatively influence the registration
accuracy. To advance research in this field, we construct Cross3DReg, the
currently largest and real-world multi-modal cross-source point cloud
registration dataset, which is collected by a rotating mechanical lidar and a
hybrid semi-solid-state lidar, respectively. Moreover, we design an
overlap-based cross-source registration framework, which utilizes unaligned
images to predict the overlapping region between source and target point
clouds, effectively filtering out redundant points in the irrelevant regions
and significantly mitigating the interference caused by noise in
non-overlapping areas. Then, a visual-geometric attention guided matching
module is proposed to enhance the consistency of cross-source point cloud
features by fusing image and geometric information to establish reliable
correspondences and ultimately achieve accurate and robust registration.
Extensive experiments show that our method achieves state-of-the-art
registration performance. Our framework reduces the relative rotation error
(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,
respectively, and improves the registration recall (RR) by $5.4\%$, which
validates its effectiveness in achieving accurate cross-source registration.

</details>


### [113] [IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks](https://arxiv.org/abs/2509.06459)
*Sebastian-Vasile Echim,Andrei-Alexandru Preda,Dumitru-Clementin Cercel,Florin Pop*

Main category: cs.CV

TL;DR: 提出基于仿射变换与遗传算法的两种黑盒迭代对抗攻击（ATA、AGA），在多模型多数据集上优于若干现有黑盒方法，最高提升8.82%的攻击效果，并分析参数与防御影响。


<details>
  <summary>Details</summary>
Motivation: 深度网络虽表现卓越，但可解释性差且存在脆弱性，尤其在黑盒场景下攻击困难。目标是探索更有效的黑盒对抗攻击策略，揭示模型弱点并研究鲁棒性与防御策略。

Method: 提出两种黑盒迭代算法：1) ATA：通过随机仿射变换迭代最大化自定义攻击得分函数；2) AGA：基于遗传算法，结合随机噪声与仿射变换进行进化搜索。对比基线包括Pixel和Square Attack，并在ResNet-18、DenseNet-121、SwinV2、ViT上，用Tiny ImageNet、Caltech-256、Food-101进行评估，考察参数变化、数据增强以及全局/定向攻击场景。

Result: 实验表明ATA和AGA在多数据集和多架构上均能有效攻击，部分配置下攻击成功率或降低的准确率优于Pixel和Square Attack，最高提升达8.82%。对抗效果受算法参数、数据增强和攻击目标（全局/定向）显著影响。

Conclusion: 论文结论指出提出的两种基于仿射变换和遗传算法的黑盒迭代对抗攻击（ATA与AGA）在多个网络架构和数据集上能有效降低模型准确率，并在某些实验设置下相较现有方法提升最多8.82%的攻击效果。同时分析了参数变化、数据增强与全局/定向攻击对鲁棒性的影响，给出对成功攻击与防御的见解。

Abstract: Deep neural networks currently dominate many fields of the artificial
intelligence landscape, achieving state-of-the-art results on numerous tasks
while remaining hard to understand and exhibiting surprising weaknesses. An
active area of research focuses on adversarial attacks, which aim to generate
inputs that uncover these weaknesses. However, this proves challenging,
especially in the black-box scenario where model details are inaccessible. This
paper explores in detail the impact of such adversarial algorithms on
ResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network
architectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101
datasets, we benchmark two novel black-box iterative adversarial algorithms
based on affine transformations and genetic algorithms: 1) Affine
Transformation Attack (ATA), an iterative algorithm maximizing our attack score
function using random affine transformations, and 2) Affine Genetic Attack
(AGA), a genetic algorithm that involves random noise and affine
transformations. We evaluate the performance of the models in the algorithm
parameter variation, data augmentation, and global and targeted attack
configurations. We also compare our algorithms with two black-box adversarial
algorithms, Pixle and Square Attack. Our experiments yield better results on
the image classification task than similar methods in the literature, achieving
an accuracy improvement of up to 8.82%. We provide noteworthy insights into
successful adversarial defenses and attacks at both global and targeted levels,
and demonstrate adversarial robustness through algorithm parameter variation.

</details>


### [114] [Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning](https://arxiv.org/abs/2509.06461)
*Yuyao Ge,Shenghua Liu,Yiwei Wang,Lingrui Mei,Baolong Bi,Xuanshan Zhou,Jiayu Yao,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CV

TL;DR: 作者发现视觉复杂度会提高注意力熵并损害推理，注意力层次上从浅层全局扫描到深层聚焦，提出训练-free的CARVE通过像素级注意力对比提取语义信号，显著提升VLM的视觉推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有提升方法要么需要额外训练、依赖外部分割工具或仅在粗粒度上工作，忽视了VLM自身的潜在能力。作者希望利用VLM内在的注意力机制，在不额外训练的情况下改善其在复杂视觉场景下的表现。

Method: 通过分析VLM的注意力模式，作者发现注意力熵与视觉复杂度相关，并理论证明通过对比一般查询与任务特定查询的注意力图可将视觉信号分解为语义信号和视觉噪声。基于此，提出CARVE：一种训练-free的方法，在像素级别进行注意力对比以提取任务相关信号。

Result: 实验表明，CARVE在多种场景下持续提升性能，最大在开源模型上达到了75%的提升，验证了注意力对比在去噪和增强视觉信号方面的有效性。

Conclusion: 本文探讨了视觉-语言模型（VLM）在复杂视觉环境中性能下降的原因，并提出了一种无须训练的对比注意力精炼方法CARVE，用于在像素级别提取任务相关的视觉信号，从而提升视觉推理能力。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable success across
diverse visual tasks, yet their performance degrades in complex visual
environments. While existing enhancement approaches require additional
training, rely on external segmentation tools, or operate at coarse-grained
levels, they overlook the innate ability within VLMs. To bridge this gap, we
investigate VLMs' attention patterns and discover that: (1) visual complexity
strongly correlates with attention entropy, negatively impacting reasoning
performance; (2) attention progressively refines from global scanning in
shallow layers to focused convergence in deeper layers, with convergence degree
determined by visual complexity. (3) Theoretically, we prove that the contrast
of attention maps between general queries and task-specific queries enables the
decomposition of visual signal into semantic signals and visual noise
components. Building on these insights, we propose Contrastive Attention
Refinement for Visual Enhancement (CARVE), a training-free method that extracts
task-relevant visual signals through attention contrasting at the pixel level.
Extensive experiments demonstrate that CARVE consistently enhances performance,
achieving up to 75% improvement on open-source models. Our work provides
critical insights into the interplay between visual complexity and attention
mechanisms, offering an efficient pathway for improving visual reasoning with
contrasting attention.

</details>


### [115] [A Statistical 3D Stomach Shape Model for Anatomical Analysis](https://arxiv.org/abs/2509.06464)
*Erez Posner,Ore Shtalrid,Oded Erell,Daniel Noy,Moshe Bouhnik*

Main category: cs.CV

TL;DR: 提出一种合成+半监督对齐的胃三维形状建模流程，得到首个胃的统计形状模型并在真实CT上验证，数据及模型开放。


<details>
  <summary>Details</summary>
Motivation: 内部器官（如胃）精细3D模型受限于数据不足与方法学挑战，需一种能生成多样解剖形态并可泛化到真实CT的建模方法以支持研究和臨床应用。

Method: 设计合成胃体生成管道，构建合成胃数据集；基于该数据集训练低维3D统计形状模型；利用公开CT网格通过半监督对齐细化模型以提高对真实解剖变异的拟合能力；在真实CT持出测试集上评估。

Result: 构建了合成胃数据集并训练出统计形状模型，经过半监督对齐后在真实CT持出测试集上表现出稳健泛化与拟合精度，模型和数据已公开。

Conclusion: 本文提出了首个胃部三维统计形状模型，通过合成数据生成、参数化建模及半监督对齐增强模型对真实CT的泛化能力，为手术模拟、术前规划及医疗教育提供了可用工具。

Abstract: Realistic and parameterized 3D models of human anatomy have become invaluable
in research, diagnostics, and surgical planning. However, the development of
detailed models for internal organs, such as the stomach, has been limited by
data availability and methodological challenges. In this paper, we propose a
novel pipeline for the generation of synthetic 3D stomach models, enabling the
creation of anatomically diverse morphologies informed by established studies
on stomach shape variability. Using this pipeline, we construct a dataset of
synthetic stomachs. Building on this dataset, we develop a 3D statistical shape
model of the stomach, trained to capture natural anatomical variability in a
low-dimensional shape space. The model is further refined using CT meshes
derived from publicly available datasets through a semi-supervised alignment
process, enhancing its ability to generalize to unseen anatomical variations.
We evaluated the model on a held-out test set of real stomach CT scans,
demonstrating robust generalization and fit accuracy. We make the statistical
shape model along with the synthetic dataset publicly available on GitLab:
https://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.
This work introduces the first statistical 3D shape model of the stomach, with
applications ranging from surgical simulation and pre-operative planning to
medical education and computational modeling. By combining synthetic data
generation, parametric modeling, and real-world validation, our approach
represents a significant advancement in organ modeling and opens new
possibilities for personalized healthcare solutions.

</details>


### [116] [Does DINOv3 Set a New Medical Vision Standard?](https://arxiv.org/abs/2509.06467)
*Che Liu,Yinda Chen,Haoyuan Shi,Jinpeng Lu,Bailiang Jian,Jiazhen Pan,Linghan Cai,Jiayi Wang,Yundi Zhang,Jun Li,Cosmin I. Bercea,Cheng Ouyang,Chen Chen,Zhiwei Xiong,Benedikt Wiestler,Christian Wachinger,Daniel Rueckert,Wenjia Bai,Rossella Arcucci*

Main category: cs.CV

TL;DR: 未经医疗预训练的DINOv3在多数医疗影像任务中是强大的统一编码器并建立新基线，但在高度专化模态和尺度上有局限，且表现的规模／分辨率扩展性不一致。


<details>
  <summary>Details</summary>
Motivation: 评估在自然图像上预训练的前沿视觉基础模型（DINOv3）能否直接迁移并在医疗影像领域提供强有力的统一编码器，省略领域特定预训练成本。

Method: 将未经医疗预训练的DINOv3在多种医疗任务（2D/3D分类、分割）和多模态影像上进行系统基线评估，分析不同模型规模与输入分辨率对性能的影响。

Result: DINOv3在多项任务上建立了新的强基线，部分任务上优于专门的医疗模型（如BiomedCLIP、CT-Net）；但对WSI、EM、PET等高专化领域表现不足，且不遵循一致的扩展规律。

Conclusion: DINOv3在许多医疗影像任务上表现出强大通用性，但在深度领域特化场景（如WSI、EM、PET）仍有明显缺陷；其可作为强基线与视觉先验，未来可用于多视图一致性等方向。

Abstract: The advent of large-scale vision foundation models, pre-trained on diverse
natural images, has marked a paradigm shift in computer vision. However, how
the frontier vision foundation models' efficacies transfer to specialized
domains remains such as medical imaging remains an open question. This report
investigates whether DINOv3, a state-of-the-art self-supervised vision
transformer (ViT) that features strong capability in dense prediction tasks,
can directly serve as a powerful, unified encoder for medical vision tasks
without domain-specific pre-training. To answer this, we benchmark DINOv3
across common medical vision tasks, including 2D/3D classification and
segmentation on a wide range of medical imaging modalities. We systematically
analyze its scalability by varying model sizes and input image resolutions. Our
findings reveal that DINOv3 shows impressive performance and establishes a
formidable new baseline. Remarkably, it can even outperform medical-specific
foundation models like BiomedCLIP and CT-Net on several tasks, despite being
trained solely on natural images. However, we identify clear limitations: The
model's features degrade in scenarios requiring deep domain specialization,
such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),
and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3
does not consistently obey scaling law in the medical domain; performance does
not reliably increase with larger models or finer feature resolutions, showing
diverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3
as a strong baseline, whose powerful visual features can serve as a robust
prior for multiple complex medical tasks. This opens promising future
directions, such as leveraging its features to enforce multiview consistency in
3D reconstruction.

</details>


### [117] [FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection](https://arxiv.org/abs/2509.06482)
*Zhongxiang Xie,Shuangxi Miao,Yuhan Jiang,Zhewei Zhang,Jing Yao,Xuecao Li,Jianxi Huang,Pedram Ghamisi*

Main category: cs.CV

TL;DR: FSG-Net: frequency-first, then spatial attention, plus gated fusion to suppress pseudo-changes and improve boundary delineation, achieving new SOTA on major change-detection benchmarks.


<details>
  <summary>Details</summary>
Motivation: To reduce false alarms caused by radiometric nuisance variations and to better fuse deep semantic and shallow detailed features to obtain accurate boundaries in change detection.

Method: FSG-Net processes images first in frequency domain via Discrepancy-Aware Wavelet Interaction Module (DAWIM) to reduce pseudo-changes, then refines features spatially with Synergistic Temporal-Spatial Attention Module (STSAM), and finally bridges semantic gap with Lightweight Gated Fusion Unit (LGFU) to fuse high-level and shallow features.

Result: Outperforms prior methods on CDD, GZ-CD, and LEVIR-CD with F1-scores 94.16%, 89.51%, and 91.27% respectively; code to be released.

Conclusion: The paper presents FSG-Net, a new method for change detection in high-resolution remote sensing images that addresses false alarms and semantic gap issues, achieving state-of-the-art F1 on several benchmarks.

Abstract: Change detection from high-resolution remote sensing images lies as a
cornerstone of Earth observation applications, yet its efficacy is often
compromised by two critical challenges. First, false alarms are prevalent as
models misinterpret radiometric variations from temporal shifts (e.g.,
illumination, season) as genuine changes. Second, a non-negligible semantic gap
between deep abstract features and shallow detail-rich features tends to
obstruct their effective fusion, culminating in poorly delineated boundaries.
To step further in addressing these issues, we propose the Frequency-Spatial
Synergistic Gated Network (FSG-Net), a novel paradigm that aims to
systematically disentangle semantic changes from nuisance variations.
Specifically, FSG-Net first operates in the frequency domain, where a
Discrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates
pseudo-changes by discerningly processing different frequency components.
Subsequently, the refined features are enhanced in the spatial domain by a
Synergistic Temporal-Spatial Attention Module (STSAM), which amplifies the
saliency of genuine change regions. To finally bridge the semantic gap, a
Lightweight Gated Fusion Unit (LGFU) leverages high-level semantics to
selectively gate and integrate crucial details from shallow layers.
Comprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate
the superiority of FSG-Net, establishing a new state-of-the-art with F1-scores
of 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at
https://github.com/zxXie-Air/FSG-Net after a possible publication.

</details>


### [118] [WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting](https://arxiv.org/abs/2509.06485)
*Andrea Marelli,Alberto Foresti,Leonardo Pesce,Giacomo Boracchi,Mario Grosso*

Main category: cs.CV

TL;DR: 提出利用操作者移除前后图像差作为弱监督来训练分割网络，发布多视角垃圾分拣数据集WS^2并提供基准流水线，推动工业分拣弱监督研究。


<details>
  <summary>Details</summary>
Motivation: 在工业质量控制（如垃圾分拣）中，人工仍需在移动异构物料流中手动剔除不需要的物体。完全监督方法标注成本高且不可扩展，因而探索利用操作员“移除行为”作为隐式弱监督信号的方向具有实用意义和研究价值。

Method: 方法包括构建基于“前后”图像对的监督信号：利用图像差异（可能结合运动补偿与多视角对齐）生成伪标签，训练分割网络。同时提供数据采集流程（多视角、高分辨率、11k帧）、数据预处理与后处理步骤，并在端到端流水线中评估现有弱监督分割算法。

Result: 贡献包括：提出Before-After Supervision范式，发布WS^2数据集（11000+高分辨率‘前后’视频帧、多视角），并提供稳健的端到端基准流水线。在WS^2上对若干弱监督分割方法进行了比较，展示了该范式的可行性与挑战（具体数值未在摘要中给出）。

Conclusion: 本文提出了Before-After Supervision（BAS）概念，通过仅利用工作人员移除物体前后图像的视觉差异，实现对分拣场景中不需要物体的弱监督分割，从而减少标注成本。作者还发布了首个多视角垃圾分拣弱监督数据集WS^2，并提供了一个稳健的端到端评估流水线，对若干最新弱监督分割方法进行了基准测试。

Abstract: In industrial quality control, to visually recognize unwanted items within a
moving heterogeneous stream, human operators are often still indispensable.
Waste-sorting stands as a significant example, where operators on multiple
conveyor belts manually remove unwanted objects to select specific materials.
To automate this recognition problem, computer vision systems offer great
potential in accurately identifying and segmenting unwanted items in such
settings. Unfortunately, considering the multitude and the variety of sorting
tasks, fully supervised approaches are not a viable option to address this
challange, as they require extensive labeling efforts. Surprisingly, weakly
supervised alternatives that leverage the implicit supervision naturally
provided by the operator in his removal action are relatively unexplored. In
this paper, we define the concept of Before-After Supervision, illustrating how
to train a segmentation network by leveraging only the visual differences
between images acquired \textit{before} and \textit{after} the operator. To
promote research in this direction, we introduce WS$^2$ (Weakly Supervised
segmentation for Waste-Sorting), the first multiview dataset consisting of more
than 11 000 high-resolution video frames captured on top of a conveyor belt,
including "before" and "after" images. We also present a robust end-to-end
pipeline, used to benchmark several state-of-the-art weakly supervised
segmentation methods on WS$^2$.

</details>


### [119] [TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement](https://arxiv.org/abs/2509.06499)
*Jibai Lin,Bo Ma,Yating Yang,Rong Ma,Turghun Osman,Ahtamjan Ahmat,Rui Dong,Lei Wang,Xi Zhou*

Main category: cs.CV

TL;DR: TIDE通过目标监督的三元组和偏好学习，在无测试时微调的情况下，学会在保持主体身份与遵从编辑指令之间取得最佳平衡，从而提升SDIG性能并具备多任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持主体身份和满足动态编辑指令之间取得良好平衡，且通常需要测试时微调或在某一方面牺牲性能。作者旨在通过无测试时微调的训练范式解决该权衡问题。

Method: 提出Target-Instructed Diffusion Enhancing (TIDE)框架；引入目标监督三元组对齐（reference image, instruction, target images），并采用Direct Subject Diffusion (DSD)目标，通过系统生成并使用定量指标评估得到的“胜出”与“失败”目标对进行训练，从而隐式建模保留-遵从平衡的奖励信号。

Result: 在标准基准上，TIDE在多项定量指标上优于基线方法，生成的图像在主体忠实度与指令遵从性之间取得更好平衡；并且成功扩展到结构条件生成、图像到图像生成和文本-图像插值等任务。

Conclusion: TIDE通过目标监督与偏好学习在不进行测试时微调的情况下，成功平衡了主体身份保持与指令遵从之间的矛盾，从而改进了主体驱动图像生成（SDIG）任务的性能。

Abstract: Subject-driven image generation (SDIG) aims to manipulate specific subjects
within images while adhering to textual instructions, a task crucial for
advancing text-to-image diffusion models. SDIG requires reconciling the tension
between maintaining subject identity and complying with dynamic edit
instructions, a challenge inadequately addressed by existing methods. In this
paper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,
which resolves this tension through target supervision and preference learning
without test-time fine-tuning. TIDE pioneers target-supervised triplet
alignment, modelling subject adaptation dynamics using a (reference image,
instruction, target images) triplet. This approach leverages the Direct Subject
Diffusion (DSD) objective, training the model with paired "winning" (balanced
preservation-compliance) and "losing" (distorted) targets, systematically
generated and evaluated via quantitative metrics. This enables implicit reward
modelling for optimal preservation-compliance balance. Experimental results on
standard benchmarks demonstrate TIDE's superior performance in generating
subject-faithful outputs while maintaining instruction compliance,
outperforming baseline methods across multiple quantitative metrics. TIDE's
versatility is further evidenced by its successful application to diverse
tasks, including structural-conditioned generation, image-to-image generation,
and text-image interpolation. Our code is available at
https://github.com/KomJay520/TIDE.

</details>


### [120] [Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach](https://arxiv.org/abs/2509.06511)
*Daniil Tikhonov,Matheus Scatolin,Mohor Banerjee,Qiankun Ji,Ahmed Jaheen,Mostafa Salem,Abdelrahman Elsayed,Hu Wang,Sarim Hashmi,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 将微调ResNet-18提取的图像表征与大量3D/临床放射组学特征融合，使用CatBoost对四类肿瘤应答进行预测，AUC 0.81，Macro F1 0.50，证明了混合特征策略在自动化治疗反应评估中的有效性。


<details>
  <summary>Details</summary>
Motivation: RANO标准虽提供了规范化的应答评估，但手工应用复杂且存在观察者变异性，故希望通过自动化方法提高评估的一致性与效率，并为BraTS 2025挑战提供可行方案。

Method: 基于微调的ResNet-18从四种MRI模态的二维ROI中提取深度特征，融合>4800个放射组学与临床特征（含3D肿瘤生长/缩小掩模、相对于最低点的体积变化、肿瘤重心位移等），最终使用CatBoost分类器进行四分类预测。

Result: 融合深度特征与领域特定放射组学特征后，CatBoost在四类（CR/PR/SD/PD）预测上达到平均ROC AUC 0.81与Macro F1 0.50，显示了特征协同的优越性。

Conclusion: 本文提出的混合框架有效结合了深度学习特征与大量放射组学和临床特征，实现了对多时间点MRI肿瘤应答的自动分类。实验结果（AUC 0.81，Macro F1 0.50）表明该方法在四类应答预测任务上具有竞争力，但仍有改善空间。

Abstract: Accurate evaluation of the response of glioblastoma to therapy is crucial for
clinical decision-making and patient management. The Response Assessment in
Neuro-Oncology (RANO) criteria provide a standardized framework to assess
patients' clinical response, but their application can be complex and subject
to observer variability. This paper presents an automated method for
classifying the intervention response from longitudinal MRI scans, developed to
predict tumor response during therapy as part of the BraTS 2025 challenge. We
propose a novel hybrid framework that combines deep learning derived feature
extraction and an extensive set of radiomics and clinically chosen features.
Our approach utilizes a fine-tuned ResNet-18 model to extract features from 2D
regions of interest across four MRI modalities. These deep features are then
fused with a rich set of more than 4800 radiomic and clinically driven
features, including 3D radiomics of tumor growth and shrinkage masks,
volumetric changes relative to the nadir, and tumor centroid shift. Using the
fused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a
Macro F1 score of 0.50 in the 4-class response prediction task (Complete
Response, Partial Response, Stable Disease, Progressive Disease). Our results
highlight that synergizing learned image representations with domain-targeted
radiomic features provides a robust and effective solution for automated
treatment response assessment in neuro-oncology.

</details>


### [121] [On the Reproducibility of "FairCLIP: Harnessing Fairness in Vision-Language Learning''](https://arxiv.org/abs/2509.06535)
*Hua Chang Bakker,Stan Fris,Angela Madelon Bernardy,Stan Deutekom*

Main category: cs.CV

TL;DR: 复现研究发现：FairCLIP能减少优化目标中的距离但并不改善CLIP在医学零样本分类任务上的性能与公平性。 请参阅论文以获取实现细节和更多实验结果。


<details>
  <summary>Details</summary>
Motivation: 检验FairCLIP方法的可复现性与有效性，验证其是否能在真实医疗图像与临床文本数据上同时提升CLIP的性能与群体公平性。

Method: 复现Luo et al. (2024)的实验设置，发现论文描述与官方实现存在差异，于是实现了A-FairCLIP以对齐设计选择；提出FairCLIP+用于扩展到多属性；通过在两个数据集上进行零样本青光眼分类实验评估性能和公平性。

Result: CLIP在Harvard-FairVLMed数据集的零样本青光眼分类中存在人口学偏差；FairCLIP及其实现变体（官方、A-FairCLIP、FairCLIP+）虽然降低了Sinkhorn距离，但未能提高分类性能或减小群体性能差距。

Conclusion: 作者结论是：在他们的实验中，FairCLIP并未如原论文所宣称那样改善CLIP在零样本青光眼分类任务上的性能和群体公平性，尽管其正则化目标能降低Sinkhorn距离。

Abstract: We investigated the reproducibility of FairCLIP, proposed by Luo et al.
(2024), for improving the group fairness of CLIP (Radford et al., 2021) by
minimizing image-text similarity score disparities across sensitive groups
using the Sinkhorn distance. The experimental setup of Luo et al. (2024) was
reproduced to primarily investigate the research findings for FairCLIP. The
model description by Luo et al. (2024) was found to differ from the original
implementation. Therefore, a new implementation, A-FairCLIP, is introduced to
examine specific design choices. Furthermore, FairCLIP+ is proposed to extend
the FairCLIP objective to include multiple attributes. Additionally, the impact
of the distance minimization on FairCLIP's fairness and performance was
explored. In alignment with the original authors, CLIP was found to be biased
towards certain demographics when applied to zero-shot glaucoma classification
using medical scans and clinical notes from the Harvard-FairVLMed dataset.
However, the experimental results on two datasets do not support their claim
that FairCLIP improves the performance and fairness of CLIP. Although the
regularization objective reduces Sinkhorn distances, both the official
implementation and the aligned implementation, A-FairCLIP, were not found to
improve performance nor fairness in zero-shot glaucoma classification.

</details>


### [122] [Benchmarking EfficientTAM on FMO datasets](https://arxiv.org/abs/2509.06536)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 提出FMOX JSON标注扩展四个FMO数据集并公开，基于此对EfficientTAM进行评测，结果显示其在TIoU上与专门方法相当，代码数据开源。


<details>
  <summary>Details</summary>
Motivation: 快速且轻量的目标跟踪仍具挑战性；现有FMO数据集缺乏统一、扩展的元数据格式，阻碍了不同跟踪方法的公平比较与复现。因此提出FMOX以标准化标注并促进基线模型评估与复现。

Method: 作者首先为四个开源FMO数据集制作并发布JSON元数据文件(FMOX)，在其中加入了物体尺寸等额外标注。然后利用该FMOX标注对最近提出的基础跟踪模型EfficientTAM进行测试，采用轨迹交并比(TIoU)作为评估指标，并将结果与为FMO数据集定制的最先进方法进行比较。

Result: 发布了包含目标尺寸信息的FMOX JSON标注；基于FMOX对EfficientTAM的实验表明其性能与为FMO优化的跟踪管线相当（以TIoU衡量）；提供了开源代码和数据，便于社区使用和比较。

Conclusion: 本文引入了一个名为FMOX的JSON元数据格式，扩展并标准化了四个快速移动物体(FMO)数据集的标注信息，包含对象尺寸等额外真值。作者使用FMOX对EfficientTAM这一基础跟踪模型进行了评估，并用轨迹交并比(TIoU)与原有为FMO设计的跟踪管线进行了比较，结果表明EfficientTAM表现良好。研究同时公开了代码和JSON，使FMOX可供其他机器学习管线使用。

Abstract: Fast and tiny object tracking remains a challenge in computer vision and in
this paper we first introduce a JSON metadata file associated with four open
source datasets of Fast Moving Objects (FMOs) image sequences. In addition, we
extend the description of the FMOs datasets with additional ground truth
information in JSON format (called FMOX) with object size information. Finally
we use our FMOX file to test a recently proposed foundational model for
tracking (called EfficientTAM) showing that its performance compares well with
the pipelines originally taylored for these FMO datasets. Our comparison of
these state-of-the-art techniques on FMOX is provided with Trajectory
Intersection of Union (TIoU) scores. The code and JSON is shared open source
allowing FMOX to be accessible and usable for other machine learning pipelines
aiming to process FMO datasets.

</details>


### [123] [Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval](https://arxiv.org/abs/2509.06566)
*Emil Demić,Luka Čehovin Zajc*

Main category: cs.CV

TL;DR: 通过专注于预训练、编码器选择与鲁棒损失设计，而非复杂模型改进，作者在场景级草图检索任务上实现了SOTA，实验证明训练设计至关重要，评测需改进。


<details>
  <summary>Details</summary>
Motivation: 动机是：现有工作多在模型结构上做改进，但忽视了现实草图中固有的歧义性与噪声；作者认为通过设计鲁棒的训练目标能更好地处理这些问题。

Method: 方法上，作者结合合适的预训练策略、编码器架构和鲁棒损失函数，使模型对手绘草图的模糊性与噪声具有更高容忍度，未引入额外复杂模块。

Result: 结果上，在FS-COCO和SketchyCOCO两个数据集上进行大量实验，证明该训练设计能达到或超过现有最优方法，同时指出训练设计在跨模态检索中的重要性与评测场景的局限性。

Conclusion: 本文结论是：通过重点关注训练设计（预训练、编码器结构与损失函数），而非增加模型复杂度，也能在场景级草图到图像检索任务中实现最先进性能，并且现有评测场景需改进。

Abstract: The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural
images matching the overall semantics and spatial layout of a free-hand sketch.
Unlike prior work focused on architectural augmentations of retrieval models,
we emphasize the inherent ambiguity and noise present in real-world sketches.
This insight motivates a training objective that is explicitly designed to be
robust to sketch variability. We show that with an appropriate combination of
pre-training, encoder architecture, and loss formulation, it is possible to
achieve state-of-the-art performance without the introduction of additional
complexity. Extensive experiments on a challenging FS-COCO and widely-used
SketchyCOCO datasets confirm the effectiveness of our approach and underline
the critical role of training design in cross-modal retrieval tasks, as well as
the need to improve the evaluation scenarios of scene-level SBIR.

</details>


### [124] [Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition](https://arxiv.org/abs/2509.06570)
*Runqing Yang,Yimin Fu,Changyuan Wu,Zhunga Liu*

Main category: cs.CV

TL;DR: 提出RARL，通过ETF角度空间对齐未知表示、VII训练压缩已知类、及分层校正缓解增量学习中的表示漂移与不平衡问题，在CIFAR100和TinyImageNet的IOSR任务上表现出领先效果。


<details>
  <summary>Details</summary>
Motivation: 传统OSR在静态设置下无法应对连续数据流中新出现类的增量学习，受限于无法访问之前训练数据导致边界判别力下降和类间混淆。

Method: 提出了基于等角紧框（ETF）构建的角度空间表示，鼓励未知表示围绕非活跃原型对齐；引入虚拟-固有交互（VII）训练策略，通过边界近旁虚拟类压缩已知表示并强化类间间隔；设计分层校正策略以处理旧/新和正/负样本不平衡造成的表示偏差和特征扭曲。

Result: 在CIFAR100和TinyImageNet上建立IOSR基准，各种任务设置下实现了优于现有方法的性能。

Conclusion: RARL能在增量开放集识别（IOSR）中有效保持判别边界，缓解表示漂移，提升旧新类区分能力。

Abstract: Existing open set recognition (OSR) methods are typically designed for static
scenarios, where models aim to classify known classes and identify unknown ones
within fixed scopes. This deviates from the expectation that the model should
incrementally identify newly emerging unknown classes from continuous data
streams and acquire corresponding knowledge. In such evolving scenarios, the
discriminability of OSR decision boundaries is hard to maintain due to
restricted access to former training data, causing severe inter-class
confusion. To solve this problem, we propose retentive angular representation
learning (RARL) for incremental open set recognition (IOSR). In RARL, unknown
representations are encouraged to align around inactive prototypes within an
angular space constructed under the equiangular tight frame, thereby mitigating
excessive representation drift during knowledge updates. Specifically, we adopt
a virtual-intrinsic interactive (VII) training strategy, which compacts known
representations by enforcing clear inter-class margins through
boundary-proximal virtual classes. Furthermore, a stratified rectification
strategy is designed to refine decision boundaries, mitigating representation
bias and feature space distortion caused by imbalances between old/new and
positive/negative class samples. We conduct thorough evaluations on CIFAR100
and TinyImageNet datasets and establish a new benchmark for IOSR. Experimental
results across various task setups demonstrate that the proposed method
achieves state-of-the-art performance.

</details>


### [125] [Approximating Condorcet Ordering for Vector-valued Mathematical Morphology](https://arxiv.org/abs/2509.06577)
*Marcos Eduardo Valle,Santiago Velasco-Forero,Joao Batista Florindo,Gustavo Jesus Angulo*

Main category: cs.CV

TL;DR: 提出用机器学习学习一个近似康多塞特的降维向量排序，以统一向量值形态学中的元素排序，初步实验在彩色图像上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 向量值图像（如彩色、超光谱）应用形态学时缺乏公认的向量元素排序规则，现有方法各有缺陷。借鉴投票理论中的康多塞特排序，期望通过学习得到一个可操作且接近集体偏好的简化排序，为形态学算子提供一致的序关系。

Method: 构建多个已有向量排序作为“选民”，定义康多塞特排序作为目标排名，采用机器学习（具体模型未详述）学习一个降维或简化的排序映射，使得映射后的单一序能近似康多塞特排序，进而用于构建形态学算子。

Result: 初步计算实验（在彩色图像上）表明，学习得到的降维排序能有效定义向量形态学算子，性能优于或可替代采用单一传统向量排序的方法。

Conclusion: 本文提出通过机器学习学习一种降维排序映射，以逼近由多个向量排序构成的康多塞特排序，从而为向量值形态学算子提供统一排序基础，实验表明该方法在彩色图像形态学中有效。

Abstract: Mathematical morphology provides a nonlinear framework for image and spatial
data processing and analysis. Although there have been many successful
applications of mathematical morphology to vector-valued images, such as color
and hyperspectral images, there is still no consensus on the most suitable
vector ordering for constructing morphological operators. This paper addresses
this issue by examining a reduced ordering approximating the Condorcet ranking
derived from a set of vector orderings. Inspired by voting problems, the
Condorcet ordering ranks elements from most to least voted, with voters
representing different orderings. In this paper, we develop a machine learning
approach that learns a reduced ordering that approximates the Condorcet
ordering. Preliminary computational experiments confirm the effectiveness of
learning the reduced mapping to define vector-valued morphological operators
for color images.

</details>


### [126] [CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis](https://arxiv.org/abs/2509.06579)
*Xin Kong,Daniel Watson,Yannick Strümpler,Michael Niemeyer,Federico Tombari*

Main category: cs.CV

TL;DR: 提出自回归多视图扩散模型CausNVS：用因果mask、逐帧噪声与成对相机位姿编码训练，推理时用滑窗、缓存和噪声增强以减轻漂移，支持任意视图配置并在多场景下保持高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有多视图扩散模型多为非自回归，限制了其在场景建模中的适用性：只能处理固定帧数且因需同时对所有帧去噪导致推理缓慢。作者希望实现支持任意输入-输出视图配置、顺序生成并提升推理效率与灵活性的多视图扩散模型。

Method: 在训练阶段采用自回归框架，结合因果mask（causal masking）和逐帧噪声注入，并使用成对相对相机位姿编码（CaPE）作为精确的相机控制信息。推理时通过空间感知滑动窗口（spatially-aware sliding-window）、键值缓存（key-value caching）与噪声条件增强（noise conditioning augmentation）来缓解随序列生成的漂移。

Result: CausNVS在多种相机轨迹与配置下均能生成高质量视图，支持灵活的自回归新视图合成，并在视觉质量方面表现稳健。

Conclusion: CausNVS通过引入自回归生成、多视图因果mask与逐帧噪声、以及相对相机位姿编码，实现了任意输入-输出视图配置的灵活新视图合成；推理时的滑窗缓存与噪声条件增强减轻了累计漂移。总体上，该方法拓展了扩散模型在场景建模中的适用性，并在多种轨迹与设置下保持稳定的视觉质量。

Abstract: Multi-view diffusion models have shown promise in 3D novel view synthesis,
but most existing methods adopt a non-autoregressive formulation. This limits
their applicability in world modeling, as they only support a fixed number of
views and suffer from slow inference due to denoising all frames
simultaneously. To address these limitations, we propose CausNVS, a multi-view
diffusion model in an autoregressive setting, which supports arbitrary
input-output view configurations and generates views sequentially. We train
CausNVS with causal masking and per-frame noise, using pairwise-relative camera
pose encodings (CaPE) for precise camera control. At inference time, we combine
a spatially-aware sliding-window with key-value caching and noise conditioning
augmentation to mitigate drift. Our experiments demonstrate that CausNVS
supports a broad range of camera trajectories, enables flexible autoregressive
novel view synthesis, and achieves consistently strong visual quality across
diverse settings. Project page: https://kxhit.github.io/CausNVS.html.

</details>


### [127] [Detection of trade in products derived from threatened species using machine learning and a smartphone](https://arxiv.org/abs/2509.06585)
*Ritwik Kulkarni,WU Hanqin,Enrico Di Minin*

Main category: cs.CV

TL;DR: 基于图像的深度学习模型能有效检测象、穿山甲和虎的盗猎制品，并通过手机应用实现实时识别，辅助野生动物贸易监管。


<details>
  <summary>Details</summary>
Motivation: 数字市场和社交媒体上非法野生动物贸易泛滥，人工监测难以应对大量图像数据，特别是需要自动识别象牙等具体野生动物制品，因此需要自动化识别工具帮助监管与执法。

Method: 收集并标注来自象（象牙与皮革）、穿山甲（鳞片与爪）和虎（皮与骨）三种物种的非法贸易或被查获产品图像；构建基于深度学习的目标检测/识别模型，尝试多种训练策略并比较两种损失函数；训练物种专用模型与一个多物种通用模型；评估模型在不同类别上的准确率，并将最佳模型部署到智能手机应用以进行实时检测。

Result: 最佳模型总体准确率84.2%；按物种分别为象产品71.1%、穿山甲90.2%、虎93.5%。移动端应用实现后总体准确率提高到91.3%，可实时拍照识别潜在禁售产品，适用于网络监测和实体市场执法。

Conclusion: 该研究表明基于机器学习的目标识别模型能够在图像中自动识别和定位非法野生动物产品，整体表现良好，且可通过移动应用便于执法和监管使用。

Abstract: Unsustainable trade in wildlife is a major threat to biodiversity and is now
increasingly prevalent in digital marketplaces and social media. With the sheer
volume of digital content, the need for automated methods to detect wildlife
trade listings is growing. These methods are especially needed for the
automatic identification of wildlife products, such as ivory. We developed
machine learning-based object recognition models that can identify wildlife
products within images and highlight them. The data consists of images of
elephant, pangolin, and tiger products that were identified as being sold
illegally or that were confiscated by authorities. Specifically, the wildlife
products included elephant ivory and skins, pangolin scales, and claws (raw and
crafted), and tiger skins and bones. We investigated various combinations of
training strategies and two loss functions to identify the best model to use in
the automatic detection of these wildlife products. Models were trained for
each species while also developing a single model to identify products from all
three species. The best model showed an overall accuracy of 84.2% with
accuracies of 71.1%, 90.2% and 93.5% in detecting products derived from
elephants, pangolins, and tigers, respectively. We further demonstrate that the
machine learning model can be made easily available to stakeholders, such as
government authorities and law enforcement agencies, by developing a
smartphone-based application that had an overall accuracy of 91.3%. The
application can be used in real time to click images and help identify
potentially prohibited products of target species. Thus, the proposed method is
not only applicable for monitoring trade on the web but can also be used e.g.
in physical markets for monitoring wildlife trade.

</details>


### [128] [Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising](https://arxiv.org/abs/2509.06591)
*Yichao Liu,YueYang Teng*

Main category: cs.CV

TL;DR: 提出一种轻量的Hybrid Swin Attention Network用于LDCT/PET去噪，结合EGA与混合上采样，实验证明性能更好且易部署。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT和PET虽减少辐射但噪声和伪影增多，影响诊断，因而需要有效的去噪方法在不增加辐射的前提下提升图像质量。

Method: 引入Hybrid Swin Attention Network (HSANet)，包含Efficient Global Attention (EGA)模块以增强空间和通道交互，以及混合上采样模块以降低对噪声的过拟合风险。

Result: 在公开LDCT/PET数据集上，HSANet在定量指标和视觉质量上均优于现有方法，且模型参数较少，内存占用低，便于临床部署。

Conclusion: 本文提出的HSANet在公开LDCT/PET数据集上显示出优于现有方法的去噪能力，同时模型轻量化，适合常规GPU部署，具有较高的临床实用性。

Abstract: Low-dose computed tomography (LDCT) and positron emission tomography (PET)
have emerged as safer alternatives to conventional imaging modalities by
significantly reducing radiation exposure. However, this reduction often
results in increased noise and artifacts, which can compromise diagnostic
accuracy. Consequently, denoising for LDCT/PET has become a vital area of
research aimed at enhancing image quality while maintaining radiation safety.
In this study, we introduce a novel Hybrid Swin Attention Network (HSANet),
which incorporates Efficient Global Attention (EGA) modules and a hybrid
upsampling module. The EGA modules enhance both spatial and channel-wise
interaction, improving the network's capacity to capture relevant features,
while the hybrid upsampling module mitigates the risk of overfitting to noise.
We validate the proposed approach using a publicly available LDCT/PET dataset.
Experimental results demonstrate that HSANet achieves superior denoising
performance compared to existing methods, while maintaining a lightweight model
size suitable for deployment on GPUs with standard memory configurations. This
makes our approach highly practical for real-world clinical applications.

</details>


### [129] [Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework](https://arxiv.org/abs/2509.06625)
*Aswini Kumar Patra*

Main category: cs.CV

TL;DR: 利用多模态时序图像，结合CNN+LSTM的时空深度学习管线，可在复杂的水分与杂草胁迫条件下，以98%准确率对氮胁迫程度进行三分类，优于仅空间模型和传统方法。


<details>
  <summary>Details</summary>
Motivation: 在自然环境中多种胁迫常共存，氮缺乏与干旱和杂草竞争交互使得早期检测困难，迫切需要能捕捉时空特征以实现早期、准确的氮胁迫分级。

Method: 使用四模态成像（RGB、多光谱、两种红外波段）获取时间序列冠层图像，构建CNN提取每帧空间特征，并用LSTM建模时间依赖，同时对比仅空间CNN管线；在不同氮供给（水旱胁迫和杂草压力交互存在）下进行三类别分类。

Result: CNN-LSTM在测试集上达98%准确率，显著高于空间CNN（80.45%）和以前报道的机器学习方法（76%），表明时序信息对区分氮胁迫严重度极为关键。

Conclusion: 本文提出的时空深度学习框架能够在复杂的多重胁迫环境下，高精度区分氮胁迫严重程度，结论是CNN-LSTM模型显著优于仅空间CNN和传统方法。

Abstract: Plants in their natural habitats endure an array of interacting stresses,
both biotic and abiotic, that rarely occur in isolation. Nutrient
stress-particularly nitrogen deficiency-becomes even more critical when
compounded with drought and weed competition, making it increasingly difficult
to distinguish and address its effects. Early detection of nitrogen stress is
therefore crucial for protecting plant health and implementing effective
management strategies. This study proposes a novel deep learning framework to
accurately classify nitrogen stress severity in a combined stress environment.
Our model uses a unique blend of four imaging modalities-RGB, multispectral,
and two infrared wavelengths-to capture a wide range of physiological plant
responses from canopy images. These images, provided as time-series data,
document plant health across three levels of nitrogen availability (low,
medium, and high) under varying water stress and weed pressures. The core of
our approach is a spatio-temporal deep learning pipeline that merges a
Convolutional Neural Network (CNN) for extracting spatial features from images
with a Long Short-Term Memory (LSTM) network to capture temporal dependencies.
We also devised and evaluated a spatial-only CNN pipeline for comparison. Our
CNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively
surpassing the spatial-only model's 80.45% and other previously reported
machine learning method's 76%. These results bring actionable insights based on
the power of our CNN-LSTM approach in effectively capturing the subtle and
complex interactions between nitrogen deficiency, water stress, and weed
pressure. This robust platform offers a promising tool for the timely and
proactive identification of nitrogen stress severity, enabling better crop
management and improved plant health.

</details>


### [130] [Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery](https://arxiv.org/abs/2509.06660)
*Cailei Liang,Adrian Bodenmann,Emma J Curtis,Samuel Simmons,Kazunori Nagano,Stan Brown,Adam Riese,Blair Thornton*

Main category: cs.CV

TL;DR: 位置元数据作为正则项能稳定提高SSL在海底影像上的分类表现，低维潜空间时益处更明显；高维ViT预训练模型则已具备强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提高机器人采集海底影像的自动化解释效率，探索位置元数据在不同SSL策略、模型架构与数据集上的普适性与收益。

Method: 在三个海底图像数据集上，对六种最先进的SSL框架（包含CNN与ViT，不同潜空间维度）加入位置正则化并与标准SSL及预训练模型比较，通过F1分数评估下游分类性能。

Result: 位置正则化在所有测试情况下均带来平均F1提升：CNN提升约4.9±4.0%，ViT提升约6.3±8.9%。CNN在高维潜空间（512）上受益于通用预训练，而数据集自适应SSL在512与128维上表现相近。与预训练模型比较，位置正则化对CNN分别在高低维带来2.7±2.7%与10.1±9.4%的提升；ViT的高维表示对预训练与自监督均有利。预训练高维ViT表现与最佳位置正则化SSL相当（F1≈0.795±0.075）。

Conclusion: 位置元数据正则化能一致提升自监督学习（SSL）在海底影像上的下游分类性能，尤其对低维潜空间更有益，但高维的ViT在很多情况下已具备很强的泛化能力。

Abstract: High-throughput interpretation of robotically gathered seafloor visual
imagery can increase the efficiency of marine monitoring and exploration.
Although recent research has suggested that location metadata can enhance
self-supervised feature learning (SSL), its benefits across different SSL
strategies, models and seafloor image datasets are underexplored. This study
evaluates the impact of location-based regularisation on six state-of-the-art
SSL frameworks, which include Convolutional Neural Network (CNN) and Vision
Transformer (ViT) models with varying latent-space dimensionality. Evaluation
across three diverse seafloor image datasets finds that location-regularisation
consistently improves downstream classification performance over standard SSL,
with average F1-score gains of $4.9 \pm 4.0%$ for CNNs and $6.3 \pm 8.9%$ for
ViTs, respectively. While CNNs pretrained on generic datasets benefit from
high-dimensional latent representations, dataset-optimised SSL achieves similar
performance across the high (512) and low (128) dimensional latent
representations. Location-regularised SSL improves CNN performance over
pre-trained models by $2.7 \pm 2.7%$ and $10.1 \pm 9.4%$ for high and
low-dimensional latent representations, respectively. For ViTs,
high-dimensionality benefits both pre-trained and dataset-optimised SSL.
Although location-regularisation improves SSL performance compared to standard
SSL methods, pre-trained ViTs show strong generalisation, matching the
best-performing location-regularised SSL with F1-scores of $0.795 \pm 0.075$
and $0.795 \pm 0.077$, respectively. The findings highlight the value of
location metadata for SSL regularisation, particularly when using
low-dimensional latent representations, and demonstrate strong generalisation
of high-dimensional ViTs for seafloor image analysis.

</details>


### [131] [Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations](https://arxiv.org/abs/2509.06678)
*Cailei Liang,Adrian Bodenmann,Sam Fenton,Blair Thornton*

Main category: cs.CV

TL;DR: 提出一种实时、无监督的在线聚类框架，通过维护代表性样本实现可扩展且自适应的海底影像解释，实验表明在精度与计算效率上优于其它在线方法，适合长期海洋自主勘测场景。


<details>
  <summary>Details</summary>
Motivation: 随着长时间驻留与海床驻留AUV能力增强，需要实时、持续的海底影像解释以支持自适应任务与通信优化，而传统离线方法依赖全量数据和人工标注，无法满足实时性与自适应性需求。

Method: OCF通过维护一组代表性样本来近似并持续更新特征分布，实现常量时间内的回顾与合并/拆分聚类操作；支持动态聚类合并与拆分而无需重处理历史影像。评估比较了不同代表性采样策略对聚类精度和计算成本的影响。

Result: 在三个多样化海底影像数据集上，OCF在所有在线聚类对比方法中取得最高平均F1=0.68（跨三条不同航迹的标准差3%），并且随着数据量增加保持较低且有界的计算时间，显示出对航迹变化的鲁棒性与可扩展性。

Conclusion: 本文提出了一种适用于长航时海底自治水下航行器的在线无监督聚类框架（OCF），能在实时数据流中自适应地解释海底影像，无需完整数据集或人工标注。

Abstract: As long-endurance and seafloor-resident AUVs become more capable, there is an
increasing need for extended, real-time interpretation of seafloor imagery to
enable adaptive missions and optimise communication efficiency. Although
offline image analysis methods are well established, they rely on access to
complete datasets and human-labelled examples to manage the strong influence of
environmental and operational conditions on seafloor image
appearance-requirements that cannot be met in real-time settings. To address
this, we introduce an online clustering framework (OCF) capable of interpreting
seafloor imagery without supervision, which is designed to operate in real-time
on continuous data streams in a scalable, adaptive, and self-consistent manner.
The method enables the efficient review and consolidation of common patterns
across the entire data history in constant time by identifying and maintaining
a set of representative samples that capture the evolving feature distribution,
supporting dynamic cluster merging and splitting without reprocessing the full
image history. We evaluate the framework on three diverse seafloor image
datasets, analysing the impact of different representative sampling strategies
on both clustering accuracy and computational cost. The OCF achieves the
highest average F1 score of 0.68 across the three datasets among all
comparative online clustering approaches, with a standard deviation of 3%
across three distinct survey trajectories, demonstrating its superior
clustering capability and robustness to trajectory variation. In addition, it
maintains consistently lower and bounded computational time as the data volume
increases. These properties are beneficial for generating survey data summaries
and supporting informative path planning in long-term, persistent autonomous
marine exploration.

</details>


### [132] [VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes](https://arxiv.org/abs/2509.06685)
*Shengkai Zhang,Yuhe Liu,Guanjun Wu,Jianhua He,Xinggang Wang,Mozi Chen,Kezhong Liu*

Main category: cs.CV

TL;DR: 用视觉-惯性SfM稀疏深度修正大型模型单目深度，通过对象分割传播和动态精化模块，为Gaussian Splatting在大场景提供高质量深度，显著提升新视图合成效果。


<details>
  <summary>Details</summary>
Motivation: 传统GS依赖RGB-D/立体相机提供准确深度，但这些传感器对远景受限，难以扩展到大场景；单目图像缺乏深度信息导致NVS效果差；虽然大型模型可估计单目深度，但存在跨帧不一致、远景不准和纹理误导问题。作者希望融合SfM与LFMs优点，生成适合GS的高质量密集深度。

Method: 提出对象分割深度传播算法，将稀疏SfM深度在结构化对象范围内传播以生成更致密的初始深度；并引入动态深度精化模块，处理动态对象和修正LFMs的错误深度。整体流程是用LFMs提供初始稠密深度，SfM提供稀疏准确深度，二者通过分割引导传播与动态精化融合，最终用于GS渲染。

Result: 在公开和定制数据集上的实验表明，VIM-GS在大场景中生成的渲染质量优于基线方法，特别在远景和动态对象表现更稳定、更逼真。

Conclusion: VIM-GS通过融合稀疏但准确的视觉-惯性SfM深度与大型模型预测的稠密但粗糙单目深度，成功在大场景中为Gaussian Splatting提供高质量初始深度，从而显著提升了新视图合成效果。

Abstract: VIM-GS is a Gaussian Splatting (GS) framework using monocular images for
novel-view synthesis (NVS) in large scenes. GS typically requires accurate
depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited
depth sensing range makes it difficult for GS to work in large scenes.
Monocular images, however, lack depth to guide the learning and lead to
inferior NVS results. Although large foundation models (LFMs) for monocular
depth estimation are available, they suffer from cross-frame inconsistency,
inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This
paper aims to generate dense, accurate depth images from monocular RGB inputs
for high-definite GS rendering. The key idea is to leverage the accurate but
sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the
dense but coarse depth from LFMs. To bridge the sparse input and dense output,
we propose an object-segmented depth propagation algorithm that renders the
depth of pixels of structured objects. Then we develop a dynamic depth
refinement module to handle the crippled SfM depth of dynamic objects and
refine the coarse LFM depth. Experiments using public and customized datasets
demonstrate the superior rendering quality of VIM-GS in large scenes.

</details>


### [133] [BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring](https://arxiv.org/abs/2509.06690)
*Usman Haider,Lukasz Szemet,Daniel Kelly,Vasileios Sergis,Andrew C. Daly,Karl Mason*

Main category: cs.CV

TL;DR: 为实时闭环生物打印提出了高效轻量的BioLite U-Net，在小数据集和树莓派上实现高精度且低延迟的三类分割。


<details>
  <summary>Details</summary>
Motivation: 实时监控生物打印过程中喷嘴与挤出物的语义分割对于保证打印质量和生物活性至关重要，但受限于有限影像数据与嵌入式硬件算力，需设计高效轻量的分割模型。

Method: 构建了787张手工标注的RGB生物打印图像数据集（标签包括喷嘴、生物墨水、背景）；设计BioLite U-Net，采用Depthwise separable卷积以降低计算量；将模型与MobileNetV2/V3基线（用于分割）在Raspberry Pi 4B上比较，使用mIoU、Dice和像素精度评估。

Result: BioLite U-Net在测试集上达到mIoU 92.85%、Dice 96.17%，在Raspberry Pi 4B上每帧推理时间约335 ms，模型参数量比MobileNetV2-DeepLabV3+小超过1300倍，综合精度与效率优于MobileNet基线。

Conclusion: 提出了一种适用于实时生物打印的轻量级语义分割框架（BioLite U-Net），在资源受限的嵌入式设备上实现了高效推理与高精度分割。

Abstract: Bioprinting is a rapidly advancing field that offers a transformative
approach to fabricating tissue and organ models through the precise deposition
of cell-laden bioinks. Ensuring the fidelity and consistency of printed
structures in real-time remains a core challenge, particularly under
constraints imposed by limited imaging data and resource-constrained embedded
hardware. Semantic segmentation of the extrusion process, differentiating
between nozzle, extruded bioink, and surrounding background, enables in situ
monitoring critical to maintaining print quality and biological viability. In
this work, we introduce a lightweight semantic segmentation framework tailored
for real-time bioprinting applications. We present a novel, manually annotated
dataset comprising 787 RGB images captured during the bioprinting process,
labeled across three classes: nozzle, bioink, and background. To achieve fast
and efficient inference suitable for integration with bioprinting systems, we
propose a BioLite U-Net architecture that leverages depthwise separable
convolutions to drastically reduce computational load without compromising
accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based
segmentation baselines using mean Intersection over Union (mIoU), Dice score,
and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess
real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%
and a Dice score of 96.17%, while being over 1300x smaller than
MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,
demonstrating near real-time capability. Compared to MobileNet baselines,
BioLite U-Net offers a superior tradeoff between segmentation accuracy,
efficiency, and deployability, making it highly suitable for intelligent,
closed-loop bioprinting systems.

</details>


### [134] [STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment](https://arxiv.org/abs/2509.06693)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Qunyi Zhang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: STAGE通过背景引导的去噪、分级异常分支和显式掩码对齐，生成更细致且与背景一致的异常，提升了合成数据用于异常分割的效果。


<details>
  <summary>Details</summary>
Motivation: 现有SIAS方法合成的异常缺乏细节、与背景对齐不足且难以生成细粒度像素级异常，限制了数据扩增对分割模型的提升效果。

Method: 提出了含背景先验的异常推断策略来引导去噪分布；构建分级扩散框架并增加异常专用分支以在正/反过程显式记录局部异常；采用显式掩码对齐（EMA）逐步使合成异常与背景语境一致。

Result: 在MVTec和BTAD数据集上，STAGE在SIAS任务上达到了最先进性能，并显著提升了基于合成数据训练的异常分割效果。

Conclusion: STAGE通过引入干净背景先验、分级扩散和显式掩码对齐，有效提升了合成异常的细节一致性和像素级表现，进而改善了下游异常分割性能。

Abstract: Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal
role in enhancing the performance of downstream anomaly segmentation, as it
provides an effective means of expanding abnormal data. However, existing SIAS
methods face several critical limitations: (i) the synthesized anomalies often
lack intricate texture details and fail to align precisely with the surrounding
background, and (ii) they struggle to generate fine-grained, pixel-level
anomalies. To address these challenges, we propose Segmentation-oriented
Anomaly synthesis via Graded diffusion with Explicit mask alignment, termed
STAGE. STAGE introduces a novel anomaly inference strategy that incorporates
clean background information as a prior to guide the denoising distribution,
enabling the model to more effectively distinguish and highlight abnormal
foregrounds. Furthermore, it employs a graded diffusion framework with an
anomaly-only branch to explicitly record local anomalies during both the
forward and reverse processes, ensuring that subtle anomalies are not
overlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)
strategy to progressively align the synthesized anomalies with the background,
resulting in context-consistent and structurally coherent generations.
Extensive experiments on the MVTec and BTAD datasets demonstrate that STAGE
achieves state-of-the-art performance in SIAS, which in turn enhances
downstream anomaly segmentation.

</details>


### [135] [Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention](https://arxiv.org/abs/2509.06705)
*Mohamed Zayaan S*

Main category: cs.CV

TL;DR: 提出Cortex Synth，一种端到端可微分网络，通过层次图注意力、可微谱拓扑优化与对抗几何一致性训练，从单张2D图像重建准确且拓扑正确的3D骨架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决单视图下准确恢复骨架几何与拓扑结构的挑战，兼顾形状精度与拓扑正确性，以便应用于机器人操作、医学影像与自动角色绑定等场景。

Method: 框架包含伪3D点云生成器、改进的PointNet编码器、骨架坐标解码器及可微分图构建网络(DGCN)，并引入层次图注意力、多尺度骨架细化、基于拉普拉斯特征分解的可微分谱拓扑优化及对抗几何一致性训练。

Result: 在ShapeNet上，MPJPE降低18.7%，图编辑距离改善27.3%，拓扑错误减少42%，证明了方法能同时提升几何精度与拓扑一致性。

Conclusion: Cortex Synth 提出了一种从单张2D图像联合合成3D骨架几何与拓扑的端到端可微分框架，实验显示在多个指标上优于现有方法。

Abstract: We present Cortex Synth, a novel end-to-end differentiable framework for
joint 3D skeleton geometry and topology synthesis from single 2D images. Our
architecture introduces three key innovations: (1) A hierarchical graph
attention mechanism with multi-scale skeletal refinement, (2) Differentiable
spectral topology optimization via Laplacian eigen decomposition, and (3)
Adversarial geometric consistency training for pose structure alignment. The
framework integrates four synergistic modules: a pseudo 3D point cloud
generator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a
novel Differentiable Graph Construction Network (DGCN). Our experiments
demonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and
27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological
errors by 42 percent compared to previous approaches. The model's end-to-end
differentiability enables applications in robotic manipulation, medical
imaging, and automated character rigging.

</details>


### [136] [MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture](https://arxiv.org/abs/2509.06713)
*Mustafa Yurdakul,Şakir Taşdemir*

Main category: cs.CV

TL;DR: 作者在3064张MRI上用EfficientNetV2+注意力型MLP-Mixer进行三类脑肿瘤分类，五折验证准确率≈99.5%，并用Grad-CAM增强可解释性，结果优于已有研究。


<details>
  <summary>Details</summary>
Motivation: MRI脑肿瘤诊断依赖专家且易出错，故需可靠且可解释的自动化深度学习诊断系统以提高临床决策支持。

Method: 先对九种常见CNN进行比较以选择骨干网络，选择EfficientNetV2作为基础网络；然后将注意力模块的MLP-Mixer集成到EfficientNetV2以增强特征建模；使用五折交叉验证评估模型性能，并用Grad-CAM进行可视化解释；与基础CNN和文献方法进行对比。

Result: 在Figshare的3064张T1加权对比增强MRI图像上，模型达到99.50%准确率、99.47%精确率、99.52%召回率和99.49%F1分数；Grad-CAM显示模型聚焦于相关病灶区域，提升临床可信度。

Conclusion: 提出的模型（EfficientNetV2结合注意力型MLP-Mixer）在Figshare数据集上的脑肿瘤分类表现优异，五折交叉验证下准确率、精确率、召回率和F1均接近99.5%，并通过Grad-CAM提高可解释性，宣称优于文献方法。

Abstract: Brain tumors are serious health problems that require early diagnosis due to
their high mortality rates. Diagnosing tumors by examining Magnetic Resonance
Imaging (MRI) images is a process that requires expertise and is prone to
error. Therefore, the need for automated diagnosis systems is increasing day by
day. In this context, a robust and explainable Deep Learning (DL) model for the
classification of brain tumors is proposed. In this study, a publicly available
Figshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI
images of three tumor types was used. First, the classification performance of
nine well-known CNN architectures was evaluated to determine the most effective
backbone. Among these, EfficientNetV2 demonstrated the best performance and was
selected as the backbone for further development. Subsequently, an
attention-based MLP-Mixer architecture was integrated into EfficientNetV2 to
enhance its classification capability. The performance of the final model was
comprehensively compared with basic CNNs and the methods in the literature.
Additionally, Grad-CAM visualization was used to interpret and validate the
decision-making process of the proposed model. The proposed model's performance
was evaluated using the five-fold cross-validation method. The proposed model
demonstrated superior performance with 99.50% accuracy, 99.47% precision,
99.52% recall and 99.49% F1 score. The results obtained show that the model
outperforms the studies in the literature. Moreover, Grad-CAM visualizations
demonstrate that the model effectively focuses on relevant regions of MRI
images, thus improving interpretability and clinical reliability. A robust deep
learning model for clinical decision support systems has been obtained by
combining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy
and interpretability in brain tumor classification.

</details>


### [137] [Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training](https://arxiv.org/abs/2509.06723)
*Ruicheng Zhang,Jun Zhou,Zunnan Xu,Zihao Liu,Jiehui Huang,Mingyang Zhang,Yu Sun,Xiu Li*

Main category: cs.CV

TL;DR: Zo3T: zero-shot test-time-training for trajectory-guided I2V using depth-aware kinematic projection, ephemeral LoRA co-adaptation with regional consistency, and guidance rectification to achieve realistic, accurate motion without dataset fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing I2V methods depend on expensive fine-tuning or produce unrealistic motion in zero-shot settings due to neglecting 3D perspective and misalignment between manipulated latents and network noise predictions. Zo3T aims to enable accurate trajectory control zero-shot while preserving realism and leveraging pre-trained models.

Method: Three core components: (1) 3D-Aware Kinematic Projection uses inferred depth to compute perspective-correct affine transforms for target regions. (2) Trajectory-Guided Test-Time LoRA injects and optimizes short-lived LoRA adapters during denoising with regional feature consistency loss to co-adapt model and latent states. (3) Guidance Field Rectification refines the denoising path via one-step lookahead optimization of the conditional guidance field.

Result: Zo3T significantly improves 3D realism and motion accuracy compared to both training-based and zero-shot baselines, yielding more perspective-correct and faithful trajectory-constrained videos.

Conclusion: Zo3T presents an effective zero-shot test-time-training framework that improves trajectory-guided image-to-video generation by integrating 3D-aware transformations, dynamic LoRA adaptation, and guidance rectification, leading to better 3D realism and motion accuracy without costly dataset fine-tuning.

Abstract: Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos
that adhere to user-specified motion instructions. Existing methods typically
rely on computationally expensive fine-tuning on scarce annotated datasets.
Although some zero-shot methods attempt to trajectory control in the latent
space, they may yield unrealistic motion by neglecting 3D perspective and
creating a misalignment between the manipulated latents and the network's noise
predictions. To address these challenges, we introduce Zo3T, a novel zero-shot
test-time-training framework for trajectory-guided generation with three core
innovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging
inferring scene depth to derive perspective-correct affine transformations for
target regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a
mechanism that dynamically injects and optimizes ephemeral LoRA adapters into
the denoising network alongside the latent state. Driven by a regional feature
consistency loss, this co-adaptation effectively enforces motion constraints
while allowing the pre-trained model to locally adapt its internal
representations to the manipulated latent, thereby ensuring generative fidelity
and on-manifold adherence. Finally, we develop Guidance Field Rectification,
which refines the denoising evolutionary path by optimizing the conditional
guidance field through a one-step lookahead strategy, ensuring efficient
generative progression towards the target trajectory. Zo3T significantly
enhances 3D realism and motion accuracy in trajectory-controlled I2V
generation, demonstrating superior performance over existing training-based and
zero-shot approaches.

</details>


### [138] [Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation](https://arxiv.org/abs/2509.06740)
*Qing Xu,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 提出Co-Seg，通过RP-Encoder和MP-Decoder实现组织与细胞核的协同分割，在PUMA上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有工作往往分别处理组织语义分割与细胞核实例分割，忽视两者间内在关联，导致病理理解不充分。本工作旨在通过协同建模两类任务来弥补这一缺陷。

Method: 设计了区域感知提示编码器（RP-Encoder）生成语义和实例区域提示；提出了互相提示掩码解码器（MP-Decoder），通过跨任务引导实现上下文一致性，协同计算语义和实例分割掩码。

Result: 在PUMA数据集上，Co-Seg在语义分割、实例分割及全景分割任务上均优于现有最先进方法，实验验证了协同提示机制的有效性。

Conclusion: 该论文提出了Co-Seg框架，通过联合组织组织语义分割和细胞核实例分割任务，提升了病理图像理解与分割性能。

Abstract: Histopathology image analysis is critical yet challenged by the demand of
segmenting tissue regions and nuclei instances for tumor microenvironment and
cellular morphology analysis. Existing studies focused on tissue semantic
segmentation or nuclei instance segmentation separately, but ignored the
inherent relationship between these two tasks, resulting in insufficient
histopathology understanding. To address this issue, we propose a Co-Seg
framework for collaborative tissue and nuclei segmentation. Specifically, we
introduce a novel co-segmentation paradigm, allowing tissue and nuclei
segmentation tasks to mutually enhance each other. To this end, we first devise
a region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and
instance region prompts as prior constraints. Moreover, we design a mutual
prompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen
the contextual consistency of both tasks, collaboratively computing semantic
and instance segmentation masks. Extensive experiments on the PUMA dataset
demonstrate that the proposed Co-Seg surpasses state-of-the-arts in the
semantic, instance and panoptic segmentation of tumor tissues and nuclei
instances. The source code is available at https://github.com/xq141839/Co-Seg.

</details>


### [139] [Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light](https://arxiv.org/abs/2509.06741)
*Christian Geckeler,Niklas Neugebauer,Manasi Muglikar,Davide Scaramuzza,Stefano Mintchev*

Main category: cs.CV

TL;DR: 本文提出一种通过调制结构光波长的事件光谱单传感器系统，实现同时低延迟高分辨率深度重建与650–850 nm波段多光谱成像；实验显示深度RMSE最多减小60%，光谱精度可与参考设备相当，结合深度的材料区分效果比颜色仅法提高30%以上，适用于无人机在雨林等复杂自然环境的感知与数据采集。


<details>
  <summary>Details</summary>
Motivation: 传统被动多光谱与RGB成像在林冠下存在延迟、深度分辨率差及对环境光依赖强的问题，限制了UAV在森林环境中对安全导航和精确数据采集的能力。提出主动投射可控波长的结构光以克服这些限制。

Method: 利用结构光进行深度重建，投射结构光时调制投射光的波长以在650–850 nm之间捕获受控波段的光谱信息；实现单传感器同时获取深度与多光谱数据。还开发了便携版限于RGB三波段用于实地采集。

Result: 在实验室与真实雨林环境中验证：深度估计在RMSE上对比商用深度传感器提升达约60%；光谱测量与参考分光仪及商用多光谱相机相当；便携3波段原型在Masola雨林采集数据，基于光谱+深度的叶片与树枝区分比纯色彩方法提升超过30%的准确率。系统能实现高性能的深度估计、RGB重建与材料区分，适合集成到无人机用于复杂自然环境感知。

Conclusion: 本文提出了一个通过单一传感器实现高分辨率、低延迟深度重建与多光谱成像的事件光谱系统，适用于森林环境中的UAV感知。实验表明在深度重建上对比商业深度传感器RMSE可提升约60%，并在光谱精度上与参考分光仪和商用多光谱相机相当。在真实雨林环境的便携RGB三波段原型机测试中，结合深度信息的材料区分准确率较仅用颜色的方法提高了30%以上。

Abstract: Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest
environments for tasks such as environmental monitoring and search and rescue,
which require safe navigation through dense foliage and precise data
collection. Traditional sensing approaches, including passive multispectral and
RGB imaging, suffer from latency, poor depth resolution, and strong dependence
on ambient light - especially under forest canopies. In this work, we present a
novel event spectroscopy system that simultaneously enables high-resolution,
low-latency depth reconstruction and multispectral imaging using a single
sensor. Depth is reconstructed using structured light, and by modulating the
wavelength of the projected structured light, our system captures spectral
information in controlled bands between 650 nm and 850 nm. We demonstrate up to
$60\%$ improvement in RMSE over commercial depth sensors and validate the
spectral accuracy against a reference spectrometer and commercial multispectral
cameras, demonstrating comparable performance. A portable version limited to
RGB (3 wavelengths) is used to collect real-world depth and spectral data from
a Masoala Rainforest. We demonstrate the use of this prototype for color image
reconstruction and material differentiation between leaves and branches using
spectral and depth data. Our results show that adding depth (available at no
extra effort with our setup) to material differentiation improves the accuracy
by over $30\%$ compared to color-only method. Our system, tested in both lab
and real-world rainforest environments, shows strong performance in depth
estimation, RGB reconstruction, and material differentiation - paving the way
for lightweight, integrated, and robust UAV perception and data collection in
complex natural environments.

</details>


### [140] [Pothole Detection and Recognition based on Transfer Learning](https://arxiv.org/abs/2509.06750)
*Mang Hu,Qianqian Xia*

Main category: cs.CV

TL;DR: 本文通过数据预处理与迁移学习构建ResNet50-EfficientNet-RegNet模型，实现了高达98.89%的坑洞识别准确率并兼顾高识别速度，优于几种传统方法。


<details>
  <summary>Details</summary>
Motivation: 自动、高效、准确地从路面图像中识别坑洞对道路维护与社会发展具有重要意义，因此需要构建鲁棒且高效的图像识别模型以替代繁琐的人工作业。

Method: 首先对原始数据集进行标准化、归一化和数据增强；然后基于迁移学习构建融合ResNet50、EfficientNet和RegNet特征的深度网络（ResNet50-EfficientNet-RegNet）；在训练过程中通过实验不断调整参数并优化网络；最后与传统机器学习方法（Random Forest、MLP、SVM、LightGBM）在Accuracy、Recall、Precision、F1-score和FPS等指标上进行对比评估。

Result: 提出的迁移学习模型在初始90样本测试集中达到97.78%（88/90）的分类准确率，在扩展的900样本测试集中达到98.89%（890/900）；在识别速度（FPS）和综合评价指标上优于Random Forest、MLP、SVM和LightGBM等模型。

Conclusion: 该论文提出并验证了一种基于迁移学习的深度特征提取网络（ResNet50-EfficientNet-RegNet），用于路面坑洞的图像自动识别。通过预处理、增强和模型微调，模型在测试集上表现出高准确率和较快的识别速度。

Abstract: With the rapid development of computer vision and machine learning, automated
methods for pothole detection and recognition based on image and video data
have received significant attention. It is of great significance for social
development to conduct an in-depth analysis of road images through feature
extraction, thereby achieving automatic identification of the pothole condition
in new images. Consequently, this is the main issue addressed in this study.
Based on preprocessing techniques such as standardization, normalization, and
data augmentation applied to the collected raw dataset, we continuously
improved the network model based on experimental results. Ultimately, we
constructed a deep learning feature extraction network
ResNet50-EfficientNet-RegNet model based on transfer learning. This model
exhibits high classification accuracy and computational efficiency. In terms of
model evaluation, this study employed a comparative evaluation approach by
comparing the performance of the proposed transfer learning model with other
models, including Random Forest, MLP, SVM, and LightGBM. The comparison
analysis was conducted based on metrics such as Accuracy, Recall, Precision,
F1-score, and FPS, to assess the classification performance of the transfer
learning model proposed in this paper. The results demonstrate that our model
exhibits high performance in terms of recognition speed and accuracy,
surpassing the performance of other models. Through careful parameter selection
and model optimization, our transfer learning model achieved a classification
accuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%
(890/900) on the expanded test set.

</details>


### [141] [Raw2Event: Converting Raw Frame Camera into Event Camera](https://arxiv.org/abs/2509.06767)
*Zijie Ning,Enmin Lin,Sudarshan R. Iyengar,Patrick Vandewalle*

Main category: cs.CV

TL;DR: Raw2Event通过直接处理Bayer RAW并基于DVS-Voltmeter，在低成本相机上实时生成高质量事件流，适合事件视觉研究与快速原型化。


<details>
  <summary>Details</summary>
Motivation: 降低事件相机高成本和分辨率受限问题，为早期开发与原型制作提供高性价比、更高动态范围与分辨率的替代方案，同时保留自动对焦等传统相机优势。

Method: 通过直接访问Bayer RAW数据并绕过ISP，基于DVS-Voltmeter模型构建可配置仿真框架，设计同步数据采集流水线，并在树莓派上部署实现实时性能。

Result: 生成的事件流与真实事件相机高度相似，同时具备更高分辨率与自动对焦能力，支持嵌入式平台实时运行并提供参数可调性以适配多种应用。

Conclusion: Raw2Event成功用低成本原始RAW相机实时生成事件流，为事件视觉研究与原型开发提供可行方案。

Abstract: Event cameras offer unique advantages such as high temporal resolution, low
latency, and high dynamic range, making them more and more popular for vision
tasks under challenging light conditions. However, their high cost, limited
resolution, and lack of features such as autofocus hinder their broad adoption,
particularly for early-stage development and prototyping. In this work, we
present Raw2Event, a complete hardware-software system that enables real-time
event generation from low-cost raw frame-based cameras. By leveraging direct
access to raw Bayer data and bypassing traditional image signal processors
(ISP), our system is able to utilize the full potential of camera hardware,
delivering higher dynamic range, higher resolution, and more faithful output
than RGB-based frame-to-event converters.
  Built upon the DVS-Voltmeter model, Raw2Event features a configurable
simulation framework optimized for deployment on embedded platforms. We further
design a data acquisition pipeline that supports synchronized recording of raw,
RGB, and event streams, facilitating downstream evaluation and dataset
creation. Experimental results show that Raw2Event can generate event streams
closely resembling those from real event cameras, while benefiting from higher
resolution and autofocus capabilities. The system also supports user-intuitive
parameter tuning, enabling flexible adaptation to various application
requirements. Finally, we deploy the system on a Raspberry Pi for real-time
operation, providing a scalable and cost-effective solution for event-based
vision research and early-stage system development.
  The codes are available online:
https://anonymous.4open.science/r/raw2event-BFF2/README.md.

</details>


### [142] [D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning](https://arxiv.org/abs/2509.06771)
*Sai Kartheek Reddy Kasu,Mohammad Zia Ur Rehman,Shahid Shafi Dar,Rishi Bharat Junghare,Dhanvin Sanjay Namboodiri,Nagendra Kumar*

Main category: cs.CV

TL;DR: 作者构建了4,379条标注暗黑幽默的Reddit表情包数据集，提出用VLM生成并自我优化解释的推理增强三流融合网络（TCRNet），显著提高了暗黑幽默及其目标与强度的识别性能，代码与数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 暗黑幽默依赖隐含、敏感和文化相关线索，缺乏专门的多模态数据集和有效方法用于检测与强度分类，因而需要构建数据集并设计能整合视觉、文本与推理信息的模型。

Method: 提出了基于大视觉语言模型（VLM）的推理增强框架：先生成结构化解释，经Role-Reversal Self-Loop迭代完善，再从OCR文本与自我推理中提取文本特征，用视觉Transformer提取图像特征，最后通过三流交叉推理网络（TCRNet）对三流特征进行两两注意力融合用于分类。

Result: 在暗黑幽默检测、目标识别和强度预测三项任务上，该方法优于若干强基线；并公布了包含4,379条带注释的Reddit表情包数据集与代码。

Conclusion: 本文提出的数据集与方法有效提升了暗黑幽默多模态识别性能，但在文化偏见、标注一致性与模型泛化性方面存在局限。

Abstract: Dark humor in online memes poses unique challenges due to its reliance on
implicit, sensitive, and culturally contextual cues. To address the lack of
resources and methods for detecting dark humor in multimodal content, we
introduce a novel dataset of 4,379 Reddit memes annotated for dark humor,
target category (gender, mental health, violence, race, disability, and other),
and a three-level intensity rating (mild, moderate, severe). Building on this
resource, we propose a reasoning-augmented framework that first generates
structured explanations for each meme using a Large Vision-Language Model
(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective
to iteratively refine its explanations, ensuring completeness and alignment. We
then extract textual features from both the OCR transcript and the self-refined
reasoning via a text encoder, while visual features are obtained using a vision
transformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three
streams, text, image, and reasoning, via pairwise attention mechanisms,
producing a unified representation for classification. Experimental results
demonstrate that our approach outperforms strong baselines across three tasks:
dark humor detection, target identification, and intensity prediction. The
dataset, annotations, and code are released to facilitate further research in
multimodal humor understanding and content moderation. Code and Dataset are
available at:
https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning

</details>


### [143] [UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets](https://arxiv.org/abs/2509.06781)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: UrbanTwin通过精细数字孪生合成与真实点云高度一致的大规模带注释激光雷达数据，能有效训练并提升3D感知模型，提供替代或扩充现实数据的可行方案。


<details>
  <summary>Details</summary>
Motivation: 减少获取高质量标注实车点云数据的成本与难度，为自动驾驶及感知算法提供可扩展且可控的训练数据，且能够通过修改孪生场景测试特定场景。

Method: 基于真实地点的环境建模（几何、车道对齐、路口拓扑与车辆运动模式），在数字孪生中仿真激光雷达传感器生成10K帧带注释的合成数据，提供3D包围框、实例分割、跟踪ID（6类）和语义分割标签（9类），并用统计/结构相似性分析及纯合成训练-实地测试的任务基准来评估效果。

Result: 生成了三个与公开数据集（LUMPI、V2X-Real-IC、TUMTraf-I）对应的每个10K帧合成数据集；相似性分析显示高相似度；用合成数据训练的3D检测模型在真实测试集上表现优于基于真实训练的数据，表明合成数据具有替代/增强价值。

Conclusion: 本文声称通过精细数字孪生生成的合成点云数据（UrbanTwin）在统计和结构上与真实道路点云高度一致，且用于训练时能在真实数据上获得可比较或更好的3D目标检测表现，从而可以替代或增强同场景的真实数据集。

Abstract: This article presents UrbanTwin datasets - high-fidelity, realistic replicas
of three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.
Each UrbanTwin dataset contains 10K annotated frames corresponding to one of
the public datasets. Annotations include 3D bounding boxes, instance
segmentation labels, and tracking IDs for six object classes, along with
semantic segmentation labels for nine classes. These datasets are synthesized
using emulated lidar sensors within realistic digital twins, modeled based on
surrounding geometry, road alignment at lane level, and the lane topology and
vehicle movement patterns at intersections of the actual locations
corresponding to each real dataset. Due to the precise digital twin modeling,
the synthetic datasets are well aligned with their real counterparts, offering
strong standalone and augmentative value for training deep learning models on
tasks such as 3D object detection, tracking, and semantic and instance
segmentation. We evaluate the alignment of the synthetic replicas through
statistical and structural similarity analysis with real data, and further
demonstrate their utility by training 3D object detection models solely on
synthetic data and testing them on real, unseen data. The high similarity
scores and improved detection performance, compared to the models trained on
real data, indicate that the UrbanTwin datasets effectively enhance existing
benchmark datasets by increasing sample size and scene diversity. In addition,
the digital twins can be adapted to test custom scenarios by modifying the
design and dynamics of the simulations. To our knowledge, these are the first
digitally synthesized datasets that can replace in-domain real-world datasets
for lidar perception tasks. UrbanTwin datasets are publicly available at
https://dataverse.harvard.edu/dataverse/ucf-ut.

</details>


### [144] [P3-SAM: Native 3D Part Segmentation](https://arxiv.org/abs/2509.06784)
*Changfeng Ma,Yang Li,Xinhao Yan,Jiachen Xu,Yunhan Yang,Chunshi Wang,Zibo Zhao,Yanwen Guo,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: P3-SAM：面向点提示的原生3D分割框架，结合多头分割与掩码合并算法，在大规模数据上训练，提供强鲁棒性的自动与交互式零件分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂3D对象时鲁棒性不足且无法完全自动化，需一种能接受点提示并支持全自动分割的通用3D分割模型。

Method: 模型包含一个3D特征提取器、多个分割头和IoU预测器，并引入了一个自动选择与合并预测掩码的算法以实现零件实例分割。训练使用了近370万带合理标签的模型数据集。

Result: 在多个实验比较中，P3-SAM在精度和鲁棒性上达到或优于现有最先进方法，能精确分割任意复杂对象。

Conclusion: P3-SAM提出了一种点提示的3D零件分割模型，通过借鉴SAM的模块化设计，实现了交互与自动化分割。实验表明在复杂对象上具有较高精度和鲁棒性。

Abstract: Segmenting 3D assets into their constituent parts is crucial for enhancing 3D
understanding, facilitating model reuse, and supporting various applications
such as part generation. However, current methods face limitations such as poor
robustness when dealing with complex objects and cannot fully automate the
process. In this paper, we propose a native 3D point-promptable part
segmentation model termed P3-SAM, designed to fully automate the segmentation
of any 3D objects into components. Inspired by SAM, P3-SAM consists of a
feature extractor, multiple segmentation heads, and an IoU predictor, enabling
interactive segmentation for users. We also propose an algorithm to
automatically select and merge masks predicted by our model for part instance
segmentation. Our model is trained on a newly built dataset containing nearly
3.7 million models with reasonable segmentation labels. Comparisons show that
our method achieves precise segmentation results and strong robustness on any
complex objects, attaining state-of-the-art performance. Our code will be
released soon.

</details>


### [145] [AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results](https://arxiv.org/abs/2509.06793)
*George Ciubotariu,Florin-Alexandru Vasluianu,Zhuyun Zhou,Nancy Mehta,Radu Timofte,Ke Wu,Long Sun,Lingshun Kong,Zhongbao Yang,Jinshan Pan,Jiangxin Dong,Jinhui Tang,Hao Chen,Yinghui Fang,Dafeng Zhang,Yongqi Song,Jiangbo Guo,Shuhua Jin,Zeyu Xiao,Rui Zhao,Zhuoyuan Li,Cong Zhang,Yufeng Peng,Xin Lu,Zhijing Sun,Chengjie Ge,Zihao Li,Zishun Liao,Ziang Zhou,Qiyu Kang,Xueyang Fu,Zheng-Jun Zha,Yuqian Zhang,Shuai Liu,Jie Liu,Zhuhao Zhang,Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Juncheng Zhou,Jufeng Yang,Qianfeng Yang,Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin*

Main category: cs.CV

TL;DR: AIM 2025高帧率非均匀运动去模糊挑战赛回顾：汇总参赛方法、比较性能、展示在新MIORe数据集上的显著进展，9支有效提交、推动高FPS单图像去模糊研究。


<details>
  <summary>Details</summary>
Motivation: 动机是寻找能在多样且复杂条件下恢复清晰、视觉一致图像的有效网络，尤其针对高帧率场景中出现的非均匀运动模糊，推动该领域性能与泛化能力的提升。

Method: 论文以挑战赛回顾为主，整理并比较各参赛队伍提交的网络架构、训练策略及数据增强方法，结合定量评估指标和定性视觉结果分析方法，展示了高FPS单图像运动去模糊的最新技术路线。

Result: 共有68名参赛者注册，9支队伍提交有效结果。评测显示多种基于深度学习的模型在MIORe数据集上均有不同程度的性能提升，部分方法在定量指标和视觉质量上表现优异，证明了领域的进展。

Conclusion: 本文总结了AIM 2025高帧率非均匀运动去模糊挑战赛的成果，指出参赛方法在处理复杂运动类型聚合时取得显著进展，并强调了MIORe数据集在提供挑战性运动模式样本方面的价值。

Abstract: This paper presents a comprehensive review of the AIM 2025 High FPS
Non-Uniform Motion Deblurring Challenge, highlighting the proposed solutions
and final results. The objective of this challenge is to identify effective
networks capable of producing clearer and visually compelling images in diverse
and challenging conditions, by learning representative visual cues for complex
aggregations of motion types. A total of 68 participants registered for the
competition, and 9 teams ultimately submitted valid entries. This paper
thoroughly evaluates the state-of-the-art advances in high-FPS single image
motion deblurring, showcasing the significant progress in the field, while
leveraging samples of the novel dataset, MIORe, that introduces challenging
examples of movement patterns.

</details>


### [146] [SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis](https://arxiv.org/abs/2509.06798)
*Zhengqing Chen,Ruohong Mei,Xiaoyang Guo,Qingjie Wang,Yubin Hu,Wei Yin,Weiqiang Ren,Qian Zhang*

Main category: cs.CV

TL;DR: 提出一个基于3D生成的real2sim2real系统，自动化资产挖掘与稀有场景合成，解决现有方法在多样性、通用性和数据需求上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前CG方法缺乏多样性和可扩展性，学习方法受限于目标类别且需大量配套数据，难以生成大量稀有且多样的场景用于感知模型训练。

Method: 通过结合基于生成的3D资产自动化挖掘与合成，并整合到传感器仿真流程中，实现从真实数据到模拟再回到真实的闭环；系统应支持多类别通用物体而非仅限车辆，并减少对多传感器配准数据的依赖。

Result: 系统能自动生成多类别3D资产并合成稀有场景，提升模拟数据多样性和覆盖率，降低对昂贵多传感器实测数据的依赖，推动自动驾驶感知模型在稀有场景上的泛化性能。

Conclusion: 该工作提出了一个可扩展的real2sim2real管道，利用3D生成自动化地进行资产挖掘、生成和稀有场景数据合成，从而弥补现有CG和学习方法的不足。

Abstract: In the field of autonomous driving, sensor simulation is essential for
generating rare and diverse scenarios that are difficult to capture in
real-world environments. Current solutions fall into two categories: 1)
CG-based methods, such as CARLA, which lack diversity and struggle to scale to
the vast array of rare cases required for robust perception training; and 2)
learning-based approaches, such as NeuSim, which are limited to specific object
categories (vehicles) and require extensive multi-sensor data, hindering their
applicability to generic objects. To address these limitations, we propose a
scalable real2sim2real system that leverages 3D generation to automate asset
mining, generation, and rare-case data synthesis.

</details>


### [147] [MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration](https://arxiv.org/abs/2509.06803)
*George Ciubotariu,Zhuyun Zhou,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: 提出两个基于1000FPS采集的多任务数据集MIORe与VAR-MIORe，采用光流引导的帧平均生成一致模糊并保留清晰帧，VAR-MIORe额外提供运动幅度可控性，显著扩展了运动恢复基准的复杂性与评估范围。


<details>
  <summary>Details</summary>
Motivation: 现有运动恢复基准在模糊一致性、运动幅度覆盖和真实场景复杂性（如自运动、多人交互、景深相关模糊）等方面存在不足，导致算法在真实应用中难以泛化与可靠评估。

Method: 使用1000 FPS高帧率与专业镜头拍摄，基于光流度量自适应地对若干帧进行平均以生成一致且可控的运动模糊，同时保留用于插帧与光流估计的清晰帧；VAR-MIORe通过系统性调整被摄体与相机的相对运动幅度，构建跨从极小到极大运动量的样本集合。

Result: 提供高分辨率、可扩展的地面真值（清晰帧、光流等），并构建涵盖复杂自运动、多主体交互与景深依赖模糊的样本。通过对运动幅度的显式控制，VAR-MIORe使算法在从轻微到极端运动条件下都能被评估，揭示现有方法在极端条件下的性能瓶颈。

Conclusion: MIORe与VAR-MIORe通过高帧率采集与专业光学设备，显著提升了运动恢复基准数据集的真实感与多样性，解决了现有数据集中运动幅度受限、模糊生成不一致和难以控制等问题。VAR-MIORe进一步提供了运动幅度可控性，为算法评估提供了更细粒度的挑战。

Abstract: We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address
critical limitations in current motion restoration benchmarks. Designed with
high-frame-rate (1000 FPS) acquisition and professional-grade optics, our
datasets capture a broad spectrum of motion scenarios, which include complex
ego-camera movements, dynamic multi-subject interactions, and depth-dependent
blur effects. By adaptively averaging frames based on computed optical flow
metrics, MIORe generates consistent motion blur, and preserves sharp inputs for
video frame interpolation and optical flow estimation. VAR-MIORe further
extends by spanning a variable range of motion magnitudes, from minimal to
extreme, establishing the first benchmark to offer explicit control over motion
amplitude. We provide high-resolution, scalable ground truths that challenge
existing algorithms under both controlled and adverse conditions, paving the
way for next-generation research of various image and video restoration tasks.

</details>


### [148] [UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward](https://arxiv.org/abs/2509.06818)
*Yufeng Cheng,Wenxu Wu,Shaojin Wu,Mengqi Huang,Fei Ding,Qian He*

Main category: cs.CV

TL;DR: UMO提出将多参考—多生成任务视为全局指派问题，并通过在扩散模型上用强化学习优化，配合新数据集和混淆度量，有效提高多身份定制的一致性并降低身份混淆，达成更强的身份可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有图像定制方法在处理多参考人脸时易造成身份混淆，难以保持身份一致性与可扩展性。由于人类对面部身份敏感，需改进多身份定制能力以避免身份混淆。

Method: 提出“multi-to-multi matching”范式，将多重参考图像与多重生成目标配对为全局指派问题；在扩散模型上采用强化学习优化该指派策略；构建包含合成与真实部分的可扩展多参考定制数据集；引入新的身份混淆度量指标。

Result: 在多个图像定制方法上，UMO显著提升身份一致性并减少身份混淆，在开源方法中在身份保真维度达到新的最先进水平。公开了代码与模型。

Conclusion: UMO通过将多身份定制重构为全局分配优化问题，并在扩散模型上以强化学习方式训练，实现了多身份一致性和减少身份混淆，从而提升了身份保真度和可扩展性。

Abstract: Recent advancements in image customization exhibit a wide range of
application prospects due to stronger customization capabilities. However,
since we humans are more sensitive to faces, a significant challenge remains in
preserving consistent identity while avoiding identity confusion with
multi-reference images, limiting the identity scalability of customization
models. To address this, we present UMO, a Unified Multi-identity Optimization
framework, designed to maintain high-fidelity identity preservation and
alleviate identity confusion with scalability. With "multi-to-multi matching"
paradigm, UMO reformulates multi-identity generation as a global assignment
optimization problem and unleashes multi-identity consistency for existing
image customization methods generally through reinforcement learning on
diffusion models. To facilitate the training of UMO, we develop a scalable
customization dataset with multi-reference images, consisting of both
synthesised and real parts. Additionally, we propose a new metric to measure
identity confusion. Extensive experiments demonstrate that UMO not only
improves identity consistency significantly, but also reduces identity
confusion on several image customization methods, setting a new
state-of-the-art among open-source methods along the dimension of identity
preserving. Code and model: https://github.com/bytedance/UMO

</details>


### [149] [Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning](https://arxiv.org/abs/2509.06826)
*Dipta Neogi,Nourash Azmine Chowdhury,Muhammad Rafsan Kabir,Mohammad Ashrafuzzaman Khan*

Main category: cs.CV

TL;DR: 论文通过在LRCN+Bahdanau注意力上应用三种对比学习框架，比较多种对比损失，提出在上下文对比学习下的最佳方案，达88%准确率并实现实时部署，改进了视频MPAA分级的判别性与泛化。


<details>
  <summary>Details</summary>
Motivation: 应对视频分级中标注数据稀缺、泛化差和特征学习效率低的问题，希望通过对比学习提高判别性和适应性，减少对大量标注的依赖。

Method: 使用三种对比学习框架（实例辨别、上下文对比、多视图对比）训练LRCN（CNN+LSTM）骨干并加入Bahdanau注意力，比较NT-Xent、NT-logistic、Margin Triplet等损失函数，最终在上下文对比框架达到最佳。

Result: 在Contextual Contrastive Learning框架下，模型达到88%准确率和0.8815 F1，能有效区分PG-13与R等边界级别，并已部署为实时网页应用。

Conclusion: 该论文提出结合对比学习与混合时序-空间网络（CNN+LSTM+Bahdanau注意力）用于视频MPAA分级，取得了较高的准确率与F1，且部署为实时网络应用。

Abstract: The rapid growth of visual content consumption across platforms necessitates
automated video classification for age-suitability standards like the MPAA
rating system (G, PG, PG-13, R). Traditional methods struggle with large
labeled data requirements, poor generalization, and inefficient feature
learning. To address these challenges, we employ contrastive learning for
improved discrimination and adaptability, exploring three frameworks: Instance
Discrimination, Contextual Contrastive Learning, and Multi-View Contrastive
Learning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a
Bahdanau attention mechanism, achieving state-of-the-art performance in the
Contextual Contrastive Learning framework, with 88% accuracy and an F1 score of
0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,
and attention mechanisms for dynamic frame prioritization, the model excels in
fine-grained borderline distinctions, such as differentiating PG-13 and R-rated
content. We evaluate the model's performance across various contrastive loss
functions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating
the robustness of our proposed architecture. To ensure practical application,
the model is deployed as a web application for real-time MPAA rating
classification, offering an efficient solution for automated content compliance
across streaming platforms.

</details>


### [150] [Curia: A Multi-Modal Foundation Model for Radiology](https://arxiv.org/abs/2509.06830)
*Corentin Dancette,Julien Khlaut,Antoine Saporta,Helene Philippe,Elodie Ferreres,Baptiste Callard,Théo Danielou,Léo Alberge,Léo Machado,Daniel Tordjman,Julie Dupuis,Korentin Le Floch,Jean Du Terrail,Mariam Moshiri,Laurent Dercle,Tom Boeken,Jules Gregory,Maxime Ronot,François Legou,Pascal Roux,Marc Sapoval,Pierre Manceron,Paul Hérent*

Main category: cs.CV

TL;DR: Curia, trained on 150k real-world cross-sectional imaging exams (130 TB), is a radiology foundation model that outperforms or matches radiologists and prior FMs across 19 tasks, excelling in cross-modality and low-data scenarios; weights released on Hugging Face.


<details>
  <summary>Details</summary>
Motivation: Existing AI in radiology relies on narrow single-task models insufficient for the wide variety of imaging modalities, diseases, and findings; foundation models promise broader generalization but have not yet been realized in radiology.

Method: Train a foundation model (Curia) on 150,000 cross-sectional imaging exams (130 TB) from a major hospital over several years; evaluate on a newly curated 19-task external validation benchmark; assess organ identification, condition detection, tumor staging outcome prediction, cross-modality and low-data performance.

Result: Curia accurately identifies organs, detects critical conditions (e.g., intracranial hemorrhage, myocardial infarction), predicts tumor staging outcomes, meets or surpasses radiologist and recent foundation model performance, and shows emergent advantages in cross-modality and low-data settings; base model weights released publicly.

Conclusion: Curia demonstrates that a large, real-world cross-sectional imaging corpus can produce a radiology foundation model that matches or exceeds radiologist and recent FM performance across diverse tasks.

Abstract: AI-assisted radiological interpretation is based on predominantly narrow,
single-task models. This approach is impractical for covering the vast spectrum
of imaging modalities, diseases, and radiological findings. Foundation models
(FMs) hold the promise of broad generalization across modalities and in
low-data settings. However, this potential has remained largely unrealized in
radiology. We introduce Curia, a foundation model trained on the entire
cross-sectional imaging output of a major hospital over several years, which to
our knowledge is the largest such corpus of real-world data-encompassing
150,000 exams (130 TB). On a newly curated 19-task external validation
benchmark, Curia accurately identifies organs, detects conditions like brain
hemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.
Curia meets or surpasses the performance of radiologists and recent foundation
models, and exhibits clinically significant emergent properties in
cross-modality, and low-data regimes. To accelerate progress, we release our
base model's weights at https://huggingface.co/raidium/curia.

</details>


### [151] [Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis](https://arxiv.org/abs/2509.06831)
*Simon Pezold,Jérôme A. Kurylec,Jan S. Liechti,Beat P. Müller,Joël L. Lavanchy*

Main category: cs.CV

TL;DR: 微调通用视频基础模型并在多模态框架中整合手术室时序数据，可显著提升外科视频任务性能，验证了域自适应与互补模态融合的价值。


<details>
  <summary>Details</summary>
Motivation: 目标是探索如何利用通用的视觉基础模型与手术室中的互补模态数据，通过迁移学习和多模态融合，提高外科相关任务的自动化与决策支持能力。

Method: 作者以V-JEPA作为视频模态基础模型，对公开和内部外科视频数据进行实验：首先用预训练的V-JEPA作为单模态基线；其次对V-JEPA在未标注的外科视频上进行微调以实现域自适应；再次采用模块化决策支持思路，为其它手术室数据流（时间分辨）训练独立编码器，使其与V-JEPA的嵌入共享表示空间，从而构建多模态模型；在内置肝外科数据集上预测住院时长与术后并发症，在HeiCo公开数据集上进行手术阶段识别。

Result: 1) 在未标注外科视频上对V-JEPA进行微调可以提升各任务性能；2) 在内置肝外科数据集中，融合额外时间分辨的数据流进一步提升了预测住院时长与术后并发症的效果；3) 在HeiCo数据集中，预训练的视频单模态基线已接近EndoVis2017挑战的最佳提交，域内微调还能带来额外增益；4) 研究代码与模型权重已公开。

Conclusion: 本研究表明，将通用视觉基础模型（V-JEPA）在外科领域进行域自适应微调，并在多模态框架中融合手术室中时间分辨的辅助数据，能显著提升下游任务的性能，从而推动外科数据科学的发展。

Abstract: We investigate how both the adaptation of a generic foundation model via
transfer learning and the integration of complementary modalities from the
operating room (OR) can support surgical data science. To this end, we use
V-JEPA as the single-modality foundation of a multimodal model for minimally
invasive surgery support. We analyze how the model's downstream performance can
benefit (a) from finetuning on unlabeled surgical video data and (b) from
providing additional time-resolved data streams from the OR in a multimodal
setup.
  In an in-house dataset of liver surgery videos, we analyze the tasks of
predicting hospital length of stay and postoperative complications. In videos
of the public HeiCo dataset, we analyze the task of surgical phase recognition.
As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on
unlabeled, held-out videos to investigate its change in performance after
domain adaptation. Following the idea of modular decision support networks, we
integrate additional data streams from the OR by training a separate encoder to
form a shared representation space with V-JEPA's embeddings.
  Our experiments show that finetuning on domain-specific data increases model
performance. On the in-house data, integrating additional time-resolved data
likewise benefits the model. On the HeiCo data, accuracy of the pretrained
video-only, single-modality baseline setup is on par with the top-performing
submissions of the EndoVis2017 challenge, while finetuning on domain-specific
data increases accuracy further. Our results thus demonstrate how surgical data
science can leverage public, generic foundation models. Likewise, they indicate
the potential of domain adaptation and of integrating suitable complementary
data streams from the OR. To support further research, we release our code and
model weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.

</details>


### [152] [Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset](https://arxiv.org/abs/2509.06835)
*Nabeyou Tadessa,Balaji Iyangar,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 基于LISA数据集训练的交通标志分类CNN在FGSM和PGD攻击下表现出明显脆弱性，准确率随扰动增大快速下降，需要研究针对现实场景的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击研究多集中在MNIST等数据集，缺乏对现实世界交通标志识别系统脆弱性的系统研究，故研究基于LISA数据集的交通标志分类器在对抗攻击下的鲁棒性。

Method: 使用LISA交通标志数据集训练卷积神经网络，对47类交通标志分类；采用FGSM和PGD两种白盒对抗攻击生成对抗样本并评估模型在不同扰动幅度下的表现。

Result: 实验结果显示，随着FGSM和PGD扰动幅度增加，模型分类准确率急剧下降，表明模型对抗鲁棒性不足；论文为后续设计适用于真实交通场景的防御机制提供了基础。

Conclusion: 该论文证明了交通标志分类器对对抗样本高度脆弱，在扰动幅度增加时，分类准确率显著下降。

Abstract: Adversarial attacks pose significant threats to machine learning models by
introducing carefully crafted perturbations that cause misclassification. While
prior work has primarily focused on MNIST and similar datasets, this paper
investigates the vulnerability of traffic sign classifiers using the LISA
Traffic Sign dataset. We train a convolutional neural network to classify 47
different traffic signs and evaluate its robustness against Fast Gradient Sign
Method (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a
sharp decline in classification accuracy as the perturbation magnitude
increases, highlighting the models susceptibility to adversarial examples. This
study lays the groundwork for future exploration into defense mechanisms
tailored for real-world traffic sign recognition systems.

</details>


### [153] [ToonOut: Fine-tuned Background-Removal for Anime Characters](https://arxiv.org/abs/2509.06839)
*Matteo Muratori,Joël Seytre*

Main category: cs.CV

TL;DR: Fine-tuning BiRefNet on a 1,228-image annotated anime dataset raises background removal pixel accuracy from 95.3% to 99.5%; resources released publicly.


<details>
  <summary>Details</summary>
Motivation: Standard background removal models perform well on realistic photos but struggle on specialized domains like anime due to unique visual characteristics (stylized lines, hair complexity, transparency).

Method: Collected and annotated 1,228 high-quality anime images; fine-tuned open-source BiRefNet on this dataset; evaluated using a new Pixel Accuracy metric and compared to baseline.

Result: Pixel Accuracy improved from 95.3% to 99.5% after fine-tuning. Code, weights, and dataset are open-sourced at provided GitHub link.

Conclusion: Fine-tuning BiRefNet on a tailored anime dataset significantly improves background removal for anime images, achieving substantial gains in pixel accuracy and handling challenging features like hair and transparency.

Abstract: While state-of-the-art background removal models excel at realistic imagery,
they frequently underperform in specialized domains such as anime-style
content, where complex features like hair and transparency present unique
challenges. To address this limitation, we collected and annotated a custom
dataset of 1,228 high-quality anime images of characters and objects, and
fine-tuned the open-sourced BiRefNet model on this dataset. This resulted in
marked improvements in background removal accuracy for anime-style images,
increasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.
We are open-sourcing the code, the fine-tuned model weights, as well as the
dataset at: https://github.com/MatteoKartoon/BiRefNet.

</details>


### [154] [Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice](https://arxiv.org/abs/2509.06854)
*Hajar Moradmand,Lei Ren*

Main category: cs.CV

TL;DR: 提出基于ResNet50/UNet.3/YOLOv7和多种回归模型（最佳为ViT）的ARTSS框架，实现对全手X光的自动TSS评分，处理关节消失与可变关节数量，关节检测准确率99%，ViT Huber loss=0.87，能减少耗时与读者变异，提升临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 手动TSS评分耗时且主观性强，存在较大的组内外阅读者差异；此外临床存在关节消失和影像序列长度可变的挑战，需自动化、稳健的评分系统。

Method: 四阶段流程：I) ResNet50用于图像预处理和重定向；II) UNet.3用于手部分割；III) YOLOv7用于关节识别；IV) 使用多种深度学习模型（VGG16/19、ResNet50、DenseNet201、EfficientNetB0、ViT）进行TSS预测。训练采用3折交叉验证（每折452训练、227验证），外部独立测试集291例。评估指标包括IoU、mAP、MAE、RMSE和Huber loss。

Result: 关节识别模型准确率达99%；ViT在TSS预测上获得最低Huber loss 0.87；整体显示深度学习可有效自动化RA评分、节省时间并提高准确性与一致性。

Conclusion: 该研究提出的ARTSS框架能够在全手X光片上自动评估RA的TSS，显著降低了评分所需时间并减少读者变异，处理关节消失和可变关节数量问题，ViT模型在TSS预测上表现最好（Huber loss=0.87）。

Abstract: Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van
Der Heijde Score (TSS) is crucial, but manual scoring is often time-consuming
and subjective. This study introduces an Automated Radiographic Sharp Scoring
(ARTSS) framework that leverages deep learning to analyze full-hand X-ray
images, aiming to reduce inter- and intra-observer variability. The research
uniquely accommodates patients with joint disappearance and variable-length
image sequences. We developed ARTSS using data from 970 patients, structured
into four stages: I) Image pre-processing and re-orientation using ResNet50,
II) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and
IV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,
EfficientNetB0, and Vision Transformer (ViT). We evaluated model performance
with Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute
error (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS
from two radiologists was used as the ground truth. Model training employed
3-fold cross-validation, with each fold consisting of 452 training and 227
validation samples, and external testing included 291 unseen subjects. Our
joint identification model achieved 99% accuracy. The best-performing model,
ViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results
demonstrate the potential of deep learning to automate RA scoring, which can
significantly enhance clinical practice. Our approach addresses the challenge
of joint disappearance and variable joint numbers, offers timesaving benefits,
reduces inter- and intra-reader variability, improves radiologist accuracy, and
aids rheumatologists in making more informed decisions.

</details>


### [155] [Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach](https://arxiv.org/abs/2509.06862)
*Aymen Merrouche,Stefanie Wuhrer,Edmond Boyer*

Main category: cs.CV

TL;DR: 提出拓扑自适应变形模型，联合优化模板拓扑与对齐，在处理拓扑伪影和强非等距形变时无需数据驱动先验且匹配效果优秀。


<details>
  <summary>Details</summary>
Motivation: 许多现有方法假设变形近等距（Functional Maps）或近ARAP，但在存在拓扑伪影（如逐帧多视角重建）时这些假设被打破，需一种能应对拓扑变化并保持双射和变形约束的匹配方法。

Method: 构建一个可改变拓扑的模板网格，并在优化过程中联合优化模板拓扑及其与目标形状的对齐，利用拓扑自适应的变形模型在保持ARAP和双射关联约束的同时允许拓扑变化以解释观测形状的拓扑伪影，从而提取对应关系。

Result: 在无监督（无数据驱动先验）条件下，方法能适用于高度非等距和存在拓扑伪影的网格，包括带噪声的多视角逐帧重建，并在3D对齐质量上优于一些在大数据上训练的现有方法。

Conclusion: 本文提出了一种拓扑自适应的变形模型，通过在变形过程中允许拓扑变化来匹配含有拓扑伪影的非刚性3D网格，从而在ARAP和双射约束下对形状进行对齐，能在不依赖数据驱动先验的情况下处理强非等距和有拓扑伪影的形状，匹配质量优于部分基于大数据训练的方法。

Abstract: Non-rigid 3D mesh matching is a critical step in computer vision and computer
graphics pipelines. We tackle matching meshes that contain topological
artefacts which can break the assumption made by current approaches. While
Functional Maps assume the deformation induced by the ground truth
correspondences to be near-isometric, ARAP-like deformation-guided approaches
assume the latter to be ARAP. Neither assumption holds in certain topological
configurations of the input shapes. We are motivated by real-world scenarios
such as per-frame multi-view reconstructions, often suffering from topological
artefacts. To this end, we propose a topology-adaptive deformation model
allowing changes in shape topology to align shape pairs under ARAP and
bijective association constraints. Using this model, we jointly optimise for a
template mesh with adequate topology and for its alignment with the shapes to
be matched to extract correspondences. We show that, while not relying on any
data-driven prior, our approach applies to highly non-isometric shapes and
shapes with topological artefacts, including noisy per-frame multi-view
reconstructions, even outperforming methods trained on large datasets in 3D
alignment quality.

</details>


### [156] [A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition](https://arxiv.org/abs/2509.06868)
*Behnoud Shafiezadeh,Amir Mashmool,Farshad Eshghi,Manoochehr Kelarestaghi*

Main category: cs.CV

TL;DR: 选择性Deblur-GAN+YOLOv5在公开的近实景车牌数据集上实现了高精度、低延迟的ALPR，尤其显著改善模糊图像下的识别效果。


<details>
  <summary>Details</summary>
Motivation: ALPR在实际场景中常受模糊影响，传统方法难以兼顾准确性与实时性；采用深度学习尤其GAN用于去模糊，并用高效的YOLOv5提升检测和识别性能。

Method: 使用选择性生成对抗网络(Deblur-GAN)作为可选的预处理模块，结合YOLOv5完成车牌检测、字符分割与识别；在必要时对输入图像去模糊，从而避免对清晰图像进行不必要的处理。

Result: 在伊朗车牌数据集（公开发布，含模糊样本）上，YOLOv5在车牌检测和字符识别阶段分别取得95%和97%准确率；检测时延约0.026秒；加入Deblur-GAN后，在模糊场景下整体检测准确率提升近40%。

Conclusion: 本文提出将选择性Deblur-GAN与YOLOv5结合用于车牌识别预处理和检测，能提高模糊图像下的检测准确率并实现实时性能。

Abstract: Automatic License-Plate Recognition (ALPR) plays a pivotal role in
Intelligent Transportation Systems (ITS) as a fundamental element of Smart
Cities. However, due to its high variability, ALPR faces challenging issues
more efficiently addressed by deep learning techniques. In this paper, a
selective Generative Adversarial Network (GAN) is proposed for deblurring in
the preprocessing step, coupled with the state-of-the-art You-Only-Look-Once
(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and
the integrated Character Segmentation (CS) and Character Recognition (CR)
steps. The selective preprocessing bypasses unnecessary and sometimes
counter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high
accuracy and low computing cost. As a result, YOLOv5 achieves a detection time
of 0.026 seconds for both LP and CR detection stages, facilitating real-time
applications with exceptionally rapid responsiveness. Moreover, the proposed
model achieves accuracy rates of 95\% and 97\% in the LPD and CR detection
phases, respectively. Furthermore, the inclusion of the Deblur-GAN
pre-processor significantly improves detection accuracy by nearly 40\%,
especially when encountering blurred License Plates (LPs).To train and test the
learning components, we generated and publicly released our blur and ALPR
datasets (using Iranian license plates as a use-case), which are more
representative of close-to-real-life ad-hoc situations. The findings
demonstrate that employing the state-of-the-art YOLO model results in excellent
overall precision and detection time, making it well-suited for portable
applications. Additionally, integrating the Deblur-GAN model as a preliminary
processing step enhances the overall effectiveness of our comprehensive model,
particularly when confronted with blurred scenes captured by the camera as
input.

</details>


### [157] [Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers](https://arxiv.org/abs/2509.06885)
*Morteza Kiani Haftlang,Mohammadhossein Malmir,Foroutan Parand,Umberto Michelucci,Safouane El Ghazouali*

Main category: cs.CV

TL;DR: 提出Barlow Twins预训练的浅层Swin风格编码器+U-Net解码器，用于轻量实时二值医学图像分割，性能接近现有方法但更小更快。


<details>
  <summary>Details</summary>
Motivation: 传统卷积网络（如U-Net）感受野有限，难以建模全局上下文；使用Transformer虽可改善但通常模型深且计算开销大，不利于实时应用。动机是设计一个既能捕获全局信息又轻量高效、适合实时临床部署的分割架构，并利用自监督学习减少对大规模标注数据的依赖。

Method: 构建一个较浅的Transformer编码器与U-Net式解码器，通过跳跃连接保留空间细节；先对编码器进行Barlow Twins自监督预训练以学习鲁棒特征，然后在分割任务上端到端微调整个网络。

Result: 在基准二值分割数据集上，模型在精度上与现有方法竞争，同时参数数量和推理时间显著降低，展示了在资源受限环境中的实用性。代码已开源。

Conclusion: 该论文提出了一种轻量级端到端的医学图像二值分割模型，结合了Swin Transformer风格的编码器和U-Net风格的解码器，并采用Barlow Twins自监督预训练以增强特征学习，在保持精准度的同时显著减少参数量并提高推理速度，适用于实时与资源受限的临床场景。

Abstract: Medical image segmentation is a critical task in clinical workflows,
particularly for the detection and delineation of pathological regions. While
convolutional architectures like U-Net have become standard for such tasks,
their limited receptive field restricts global context modeling. Recent efforts
integrating transformers have addressed this, but often result in deep,
computationally expensive models unsuitable for real-time use. In this work, we
present a novel end-to-end lightweight architecture designed specifically for
real-time binary medical image segmentation. Our model combines a Swin
Transformer-like encoder with a U-Net-like decoder, connected via skip pathways
to preserve spatial detail while capturing contextual information. Unlike
existing designs such as Swin Transformer or U-Net, our architecture is
significantly shallower and competitively efficient. To improve the encoder's
ability to learn meaningful features without relying on large amounts of
labeled data, we first train it using Barlow Twins, a self-supervised learning
method that helps the model focus on important patterns by reducing unnecessary
repetition in the learned features. After this pretraining, we fine-tune the
entire model for our specific task. Experiments on benchmark binary
segmentation tasks demonstrate that our model achieves competitive accuracy
with substantially reduced parameter count and faster inference, positioning it
as a practical alternative for deployment in real-time and resource-limited
clinical environments. The code for our method is available at Github
repository: https://github.com/mkianih/Barlow-Swin.

</details>


### [158] [Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization](https://arxiv.org/abs/2509.06890)
*Minheng Chen,Youyong Kong*

Main category: cs.CV

TL;DR: 作者在球面特征空间上学习相似性，并在SO(4)用黎曼距离近似测地距离，结合CNN-Transformer和可微分Levenberg-Marquardt优化，提升了2D/3D术中配准的鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有用欧几里得近似测地距离的方法会扭曲流形结构并导致收敛变慢，无法很好处理大扰动和复杂姿态分布，因而需要非欧几里得的表征与更几何一致的距离度量。

Method: 用CNN-Transformer编码器提取特征嵌入，投影到球面空间，并在SO(4)上用双不变黎曼距离近似其测地距离；训练时采用可微分相似性学习框架，推理时用可微分Levenberg-Marquardt优化代替梯度下降以加速收敛。

Result: 在真实和合成数据集上进行实验，在病人特异和非特异场景中均表现出更高的配准精度和更快的收敛。

Conclusion: 使用球面特征空间和SO(4)上的黎曼距离近似，可以更好地保持流形结构，提高配准的表达能力和区分姿态差异的能力，从而改进2D/3D术中配准的准确性与鲁棒性。

Abstract: Intraoperative 2D/3D registration aligns preoperative 3D volumes with
real-time 2D radiographs, enabling accurate localization of instruments and
implants. A recent fully differentiable similarity learning framework
approximates geodesic distances on SE(3), expanding the capture range of
registration and mitigating the effects of substantial disturbances, but
existing Euclidean approximations distort manifold structure and slow
convergence. To address these limitations, we explore similarity learning in
non-Euclidean spherical feature spaces to better capture and fit complex
manifold structure. We extract feature embeddings using a CNN-Transformer
encoder, project them into spherical space, and approximate their geodesic
distances with Riemannian distances in the bi-invariant SO(4) space. This
enables a more expressive and geometrically consistent deep similarity metric,
enhancing the ability to distinguish subtle pose differences. During inference,
we replace gradient descent with fully differentiable Levenberg-Marquardt
optimization to accelerate convergence. Experiments on real and synthetic
datasets show superior accuracy in both patient-specific and patient-agnostic
scenarios.

</details>


### [159] [BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration](https://arxiv.org/abs/2509.06904)
*Cem Eteke,Alexander Griessel,Wolfgang Kellerer,Eckehard Steinbach*

Main category: cs.CV

TL;DR: BIR-Adapter 是一种轻量级适配器，通过在预训练扩散模型内部提取并融合退化图像特征，扩展自注意力并引入采样引导，实现在盲图像修复上低复杂度且高性能的解决方案，且易于集成到其他模型。


<details>
  <summary>Details</summary>
Motivation: 利用大规模预训练扩散模型的先验能力来处理盲图像修复问题，同时避免训练高成本的额外特征提取网络，从而实现低复杂度且易于集成的解决方案。

Method: 直接在预训练扩散模型内部提取退化图像特征，无需额外特征提取器；将这些特征融入自注意力模块作为适配器；引入采样引导机制以抑制伪影/幻觉；适配器结构轻量，能插入到其他扩散模型以扩展功能。

Result: 在合成及真实退化数据上，BIR-Adapter 在指标上与或优于最先进方法，同时显著减少模型复杂度。实例展示了将仅用于超分的模型通过适配器扩展以应对未知退化并提升性能。

Conclusion: BIR-Adapter 利用预训练扩散模型的鲁棒性，通过从退化图像中提取特征并扩展自注意力机制，实现了低复杂度的盲图像修复，达到或优于现有方法的性能，并便于集成到其他扩散模型。

Abstract: This paper introduces BIR-Adapter, a low-complexity blind image restoration
adapter for diffusion models. The BIR-Adapter enables the utilization of the
prior of pre-trained large-scale diffusion models on blind image restoration
without training any auxiliary feature extractor. We take advantage of the
robustness of pretrained models. We extract features from degraded images via
the model itself and extend the self-attention mechanism with these degraded
features. We introduce a sampling guidance mechanism to reduce hallucinations.
We perform experiments on synthetic and real-world degradations and demonstrate
that BIR-Adapter achieves competitive or better performance compared to
state-of-the-art methods while having significantly lower complexity.
Additionally, its adapter-based design enables integration into other diffusion
models, enabling broader applications in image restoration tasks. We showcase
this by extending a super-resolution-only model to perform better under
additional unknown degradations.

</details>


### [160] [FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data](https://arxiv.org/abs/2509.06907)
*Bing Han,Chen Zhu,Dong Han,Rui Yu,Songliang Cao,Jianhui Wu,Scott Chapman,Zijian Wang,Bangyou Zheng,Wei Guo,Marie Weiss,Benoit de Solan,Andreas Hund,Lukas Roth,Kirchgessner Norbert,Andrea Visioni,Yufeng Ge,Wenjuan Li,Alexis Comar,Dong Jiang,Dejun Han,Fred Baret,Yanfeng Ding,Hao Lu,Shouyang Liu*

Main category: cs.CV

TL;DR: 用2.5M小麦图像自监督预训练的FoMo4Wheat比通用预训练模型在田间视觉任务上普遍更好，增强了鲁棒性与跨作物迁移能力。


<details>
  <summary>Details</summary>
Motivation: 通用域预训练模型在复杂多变的田间环境和细粒度冠层结构下泛化性差，需要面向作物的基础模型以提高田间视觉监测的可靠性与可迁移性。

Method: 在ImAg4Wheat（2.5M高分辨率小麦图像，10年、30站点、>2000基因型、>500环境）上采用自监督预训练构建FoMo4Wheat基础模型，并在十个田间视觉任务（冠层及器官层面）上与通用域预训练模型对比评估。

Result: FoMo4Wheat在十个任务上持续优于现有最先进的通用预训练模型，其表示对小麦鲁棒并可迁移到其他作物和杂草，支持构建跨物种、跨任务的通用作物基础模型。

Conclusion: FoMo4Wheat表明面向作物的自监督预训练在小麦与跨作物视觉任务中优于通用数据集预训练模型，提升了田间感知的鲁棒性与迁移能力。

Abstract: Vision-driven field monitoring is central to digital agriculture, yet models
built on general-domain pretrained backbones often fail to generalize across
tasks, owing to the interaction of fine, variable canopy structures with
fluctuating field conditions. We present FoMo4Wheat, one of the first
crop-domain vision foundation model pretrained with self-supervision on
ImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5
million high-resolution images collected over a decade at 30 global sites,
spanning >2,000 genotypes and >500 environmental conditions). This
wheat-specific pretraining yields representations that are robust for wheat and
transferable to other crops and weeds. Across ten in-field vision tasks at
canopy and organ levels, FoMo4Wheat models consistently outperform
state-of-the-art models pretrained on general-domain dataset. These results
demonstrate the value of crop-specific foundation models for reliable in-field
perception and chart a path toward a universal crop foundation model with
cross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat
dataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat
and https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:
https://fomo4wheat.phenix-lab.com/.

</details>


### [161] [Interleaving Reasoning for Better Text-to-Image Generation](https://arxiv.org/abs/2509.06945)
*Wenxuan Huang,Shuang Chen,Zheyong Xie,Shaosheng Cao,Shixiang Tang,Yufan Shen,Qingyu Yin,Wenbo Hu,Xiaoman Wang,Yuntian Tang,Junbo Qiao,Yue Guo,Yao Hu,Zhenfei Yin,Philip Torr,Yu Cheng,Wanli Ouyang,Shaohui Lin*

Main category: cs.CV

TL;DR: 通过交替的文本思考与图像生成并配合两阶段学习，IRG显著提升了T2I的细节保真与指令遵循，基准上有5-10点的绝对增益。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型虽提升图像生成，但在指令遵循与细节保真上仍落后于紧耦合理解与生成系统；受交替推理（interleaving reasoning）启发，探究此类机制能否改善T2I细节与指令执行。

Method: 提出Interleaving Reasoning Generation (IRG)框架，交替执行文本思考（生成文本引导）与图像合成；并提出两阶段训练方案IRGL：先强化思考-生成能力，再用完整思考-图像轨迹高效微调。构建IRGL-300K数据集，包含六种分解学习模式，支持思考和思考-图像轨迹学习。基于原生输出交替文本-图像的统一基础模型进行训练。

Result: 在多项基准上取得SoTA，GenEval、WISE、TIIF、GenAI-Bench与OneIG-EN上绝对提升5-10点，并在视觉质量与细粒度保真度上有显著改进。将开源代码、模型与数据集。

Conclusion: IRG通过在文本思考与图像生成之间交替进行，显著提升了T2I在细节保持、指令遵循和视觉质量上的表现。

Abstract: Unified multimodal understanding and generation models recently have achieve
significant improvement in image generation capability, yet a large gap remains
in instruction following and detail preservation compared to systems that
tightly couple comprehension with generation such as GPT-4o. Motivated by
recent advances in interleaving reasoning, we explore whether such reasoning
can further improve Text-to-Image (T2I) generation. We introduce Interleaving
Reasoning Generation (IRG), a framework that alternates between text-based
thinking and image synthesis: the model first produces a text-based thinking to
guide an initial image, then reflects on the result to refine fine-grained
details, visual quality, and aesthetics while preserving semantics. To train
IRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),
which targets two sub-goals: (1) strengthening the initial think-and-generate
stage to establish core content and base quality, and (2) enabling high-quality
textual reflection and faithful implementation of those refinements in a
subsequent image. We curate IRGL-300K, a dataset organized into six decomposed
learning modes that jointly cover learning text-based thinking, and full
thinking-image trajectories. Starting from a unified foundation model that
natively emits interleaved text-image outputs, our two-stage training first
builds robust thinking and reflection, then efficiently tunes the IRG pipeline
in the full thinking-image trajectory data. Extensive experiments show SoTA
performance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,
GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality
and fine-grained fidelity. The code, model weights and datasets will be
released in: https://github.com/Osilly/Interleaving-Reasoning-Generation .

</details>


### [162] [H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers](https://arxiv.org/abs/2509.06956)
*Wenhao Li,Mengyuan Liu,Hong Liu,Pichao Wang,Shijian Lu,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出H2OT：通过分层剪枝代表性帧tokens并在末端恢复，显著加速视频Transformer的3D人体姿态估计，兼顾效率与精度，且可插入多种VPT模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频姿态Transformer计算开销高，难以在资源受限设备上实时部署。观察到完整时间序列的pose信息有冗余，可通过选取代表性帧tokens压缩计算并在末端恢复精细信息。

Method: 在Transformer内部逐层剪枝姿态tokens（Token Pruning Module, TPM），保留代表性帧的少量tokens；随后用Token Recovering Module (TRM)基于保留下来的tokens恢复到原始时间分辨率输出；框架为插拔式，可用于seq2seq和seq2frame两类VPT模型，支持多种剪枝和恢复策略。

Result: 在多个基准数据集上进行实验，结果显示H2OT在保持甚至提升3D姿态估计精度的同时，显著降低推理时间和计算成本，证明了方法的有效性与通用性。

Conclusion: 本文提出H2OT层次化时间通道的剪枝与恢复框架，有效减少视频姿态Transformer的计算成本，同时保持甚至提升估计精度。

Abstract: Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a hierarchical plug-and-play pruning-and-recovering
framework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient
transformer-based 3D human pose estimation from videos. H$_{2}$OT begins with
progressively pruning pose tokens of redundant frames and ends with recovering
full-length sequences, resulting in a few pose tokens in the intermediate
transformer blocks and thus improving the model efficiency. It works with two
key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module
(TRM). TPM dynamically selects a few representative tokens to eliminate the
redundancy of video frames, while TRM restores the detailed spatio-temporal
information based on the selected tokens, thereby expanding the network output
to the original full-length temporal resolution for fast inference. Our method
is general-purpose: it can be easily incorporated into common VPT models on
both seq2seq and seq2frame pipelines while effectively accommodating different
token pruning and recovery strategies. In addition, our H$_{2}$OT reveals that
maintaining the full pose sequence is unnecessary, and a few pose tokens of
representative frames can achieve both high efficiency and estimation accuracy.
Extensive experiments on multiple benchmark datasets demonstrate both the
effectiveness and efficiency of the proposed method. Code and models are
available at https://github.com/NationalGAILab/HoT.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [163] [A Unified Framework for Cultural Heritage Data Historicity and Migration: The ARGUS Approach](https://arxiv.org/abs/2509.06044)
*Lingxiao Kong,Apostolos Sarris,Miltiadis Polidorou,Victor Klingenberg,Vasilis Sevetlidis,Vasilis Arampatzakis,George Pavlidis,Cong Yang,Zeyd Boukhers*

Main category: cs.DB

TL;DR: 提出并实现一个符合FAIR的文化遗产数据历史性与迁移框架，涵盖标准化、增强、集成与基于LLM的查询，已在五个欧洲试点验证，提升了数据可用性与保护决策支持。


<details>
  <summary>Details</summary>
Motivation: 文化遗产保护需要整合来自多源、不同格式和尺度的数据以支持长期监测与保护决策，现有数据分散且稀疏，缺乏互操作性与可查询性。

Method: 构建了系统化数据处理流水线：数据标准化（遵循FAIR原则）、缺失值插补与扩展、数据库级别集成、可视化展示、基于LLM的自然语言查询增强以及数据发布机制。该流程在五个欧洲试点站点部署验证。

Result: 在五个试点应用中，该框架改善了原始数据的结构化程度、填补了稀疏数据、提高了互操作性与查询效率，最终提升了分析能力与保护决策效果。

Conclusion: 该论文提出并实现了一个面向文化遗产数据的历史性与迁移框架，解决了异构、多源、多尺度数据的标准化、增强、集成与发布问题，从而提高了数据可访问性与决策支持能力。

Abstract: Cultural heritage preservation faces significant challenges in managing
diverse, multi-source, and multi-scale data for effective monitoring and
conservation. This paper documents a comprehensive data historicity and
migration framework implemented within the ARGUS project, which addresses the
complexities of processing heterogeneous cultural heritage data. We describe a
systematic data processing pipeline encompassing standardization, enrichment,
integration, visualization, ingestion, and publication strategies. The
framework transforms raw, disparate datasets into standardized formats
compliant with FAIR principles. It enhances sparse datasets through established
imputation techniques, ensures interoperability through database integration,
and improves querying capabilities through LLM-powered natural language
processing. This approach has been applied across five European pilot sites
with varying preservation challenges, demonstrating its adaptability to diverse
cultural heritage contexts. The implementation results show improved data
accessibility, enhanced analytical capabilities, and more effective
decision-making for conservation efforts.

</details>


### [164] [Language Native Lightly Structured Databases for Large Language Model Driven Composite Materials Research](https://arxiv.org/abs/2509.06093)
*Yuze Liu,Zhaoyuan Zhang,Xiangsheng Zeng,Yihe Zhang,Leping Yu,Lejia Wang,Xi Yu*

Main category: cs.DB

TL;DR: 论文构建了一个面向BNNS-聚合物热导复合材料的语言原生异构数据库，通过证据链接的轻结构化记录与复合检索，结合RAG与工具代理，实现可验证的专家级文献综合与可操作SOP生成，旨在为LLM驱动的材料发现提供语言丰富的底层支撑。


<details>
  <summary>Details</summary>
Motivation: 传统化学与材料研究信息多以语言叙述为主，常规数据库和机器学习工具难以充分利用这些非结构化的知识，作者希望建立一种语言原生的数据库以捕获和组织这些信息，从而支持LLM驱动的材料研究与发现。

Method: 构建异构语言数据库，采用证据链接的短文本记录（lightly structured snippets），设计复合检索机制（语义检索、关键词、数值/值过滤器），并结合RAG与工具增强代理，让检索与推理交替进行以生成可操作SOP。

Result: 实现了一个专注于BNNS-聚合物热导复合材料的语言原生数据库原型，能够把文献综合为准确、可核查的专家风格建议，支持高保真、高效的RAG流程和工具增强代理，推动可操作SOP的自动生成并为材料发现提供坚实语言基础。

Conclusion: 该论文提出了一种以语言为中心的材料数据库框架，专注于硼化氮纳米片（BNNS）-高分子热导复合材料，能捕获论文中轻结构化的信息（制备、表征、理论计算、机理推理等），并关联证据片段，支持语义+关键词+数值筛选的混合检索，旨在提升文献综合、可验证的专家式指导，并为基于大模型的材料发现提供语言丰富的基底。

Abstract: Chemical and materials research has traditionally relied heavily on knowledge
narrative, with progress often driven by language-based descriptions of
principles, mechanisms, and experimental experiences, rather than tables,
limiting what conventional databases and ML can exploit. We present a
language-native database for boron nitride nanosheet (BNNS) polymer thermally
conductive composites that captures lightly structured information from papers
across preparation, characterization, theory-computation, and mechanistic
reasoning, with evidence-linked snippets. Records are organized in a
heterogeneous database and queried via composite retrieval with semantics, key
words and value filters. The system can synthesizes literature into accurate,
verifiable, and expert style guidance. This substrate enables high fidelity
efficient Retrieval Augmented Generation (RAG) and tool augmented agents to
interleave retrieval with reasoning and deliver actionable SOP. The framework
supplies the language rich foundation required for LLM-driven materials
discovery.

</details>


### [165] [MCTuner: Spatial Decomposition-Enhanced Database Tuning via LLM-Guided Exploration](https://arxiv.org/abs/2509.06298)
*Zihan Yan,Rui Xi,Mengshu Hou*

Main category: cs.DB

TL;DR: MCTuner用LLM专家识别关键参数并通过递归空间分解+贝叶斯优化在子空间内高效搜索，显著提升数据库调参效率与性能。


<details>
  <summary>Details</summary>
Motivation: 数据库具有大量连续或离散的调参项，配置空间巨大，传统学习方法要么忽略领域知识、要么难以在高维空间有效探索，导致调优成本高且效果欠佳。

Method: 提出基于Mixture-of-Experts的关键参数识别（使用专门LLM专家）和首个空间分解算法：递归划分配置空间为层次子空间，并在每个子空间上执行贝叶斯优化以加速高维搜索。

Result: 在OLAP、OLTP和HTAP基准上，MCTuner相比最先进方法最高带来19.2%的性能提升，并在每次迭代上实现1.4倍的更快配置发现。

Conclusion: MCTuner有效减少在低效配置区域的探索，通过MoE机制与专用LLM识别关键参数，并提出层次空间分解算法在子空间上进行贝叶斯优化，从而显著提升调优效率与性能。

Abstract: Database knob tuning is essential for optimizing the performance of modern
database management systems, which often expose hundreds of knobs with
continuous or categorical values. However, the large number of knobs and the
vast configuration space make it difficult to identify optimal settings
efficiently. Although learning-based tuning has shown promise, existing
approaches either ignore domain knowledge by relying solely on benchmark
feedback or struggle to explore the high-dimensional knob space, resulting in
high tuning costs and suboptimal performance. To address these challenges, we
propose MCTuner, an adaptive knob tuning framework that minimizes exploration
in ineffective regions of the configuration space. MCTuner employs a
Mixture-of-Experts (MoE) mechanism with specialized LLMs to identify
performance-critical knobs. In further, MCTuner introduces the first spatial
decomposition algorithm that recursively partitions the space into hierarchical
subspaces, on which Bayesian Optimization is performed to efficiently search
for near-optimal configurations. Evaluated on different benchmarks (OLAP, OLTP,
and HTAP), MCTuner achieves up to 19.2% performance gains and 1.4x faster
configuration discovery per iteration compared to state-of-the-art methods.

</details>


### [166] [Relational Algebras for Subset Selection and Optimisation](https://arxiv.org/abs/2509.06439)
*David Robert Pratten,Luke Mathieson,Fahimeh Ramezani*

Main category: cs.DB

TL;DR: 论文通过引入“关系指数”和“解集”两种代数构造，为子集选择与优化查询提供了第一个统一且可组合的关系代数基础，既具有强大的表达能力（达NP复杂度），又能映射到现有关系代数和评估算法。


<details>
  <summary>Details</summary>
Motivation: 数据库领域缺乏对子集选择与优化查询的统一关系查询语言；现有工作零散、基于SQL的扩展各异，既影响用户表达也限制查询优化器的推理能力。论文旨在提供理论清晰、可组合的统一框架以支持这些查询。

Method: 作者扩展了关系代数：首先通过定义域关系（由特征函数而非显式元组列举的关系）实现与NP完全/困难问题等价的表达能力且保证输入有限时的查询安全；其次引入“解集”（solution sets），这是一个作用于关系集合的高阶代数，表达为函数 f: Base -> Decision, 其候选关系数为 |Decision|^{|Base|}；最后给出从解集到标准关系代数的保持结构的翻译语义，以便将高阶表达机械地转为现有评估算法。

Result: 构建了包含关系指数与解集的扩展关系代数，证明该框架既能表示先前最强有力的方法的表达能力，又提供了到传统关系代数的翻译路径；并展示了在多态SQL中如何用标准子句统一表达数据管理、子集选择与优化查询。

Conclusion: 该论文提出了一个统一的代数性基础，通过引入关系指数（relational exponentiation）将子集选择和优化查询纳入关系代数，从而弥补了以并（加）与笛卡尔积（乘）为核心的传统关系代数的缺失。

Abstract: The database community lacks a unified relational query language for subset
selection and optimisation queries, limiting both user expression and query
optimiser reasoning about such problems. Decades of research (latterly under
the rubric of prescriptive analytics) have produced powerful evaluation
algorithms with incompatible, ad-hoc SQL extensions that specify and filter
through distinct mechanisms. We present the first unified algebraic foundation
for these queries, introducing relational exponentiation to complete the
fundamental algebraic operations alongside union (addition) and cross product
(multiplication). First, we extend relational algebra to complete domain
relations-relations defined by characteristic functions rather than explicit
extensions-achieving the expressiveness of NP-complete/hard problems, while
simultaneously providing query safety for finite inputs. Second, we introduce
solution sets, a higher-order relational algebra over sets of relations that
naturally expresses search spaces as functions f: Base to Decision, yielding
|Decision|^|Base| candidate relations. Third, we provide structure-preserving
translation semantics from solution sets to standard relational algebra,
enabling mechanical translation to existing evaluation algorithms. This
framework achieves the expressiveness of the most powerful prior approaches
while providing the theoretical clarity and compositional properties absent in
previous work. We demonstrate the capabilities these algebras open up through a
polymorphic SQL where standard clauses seamlessly express data management,
subset selection, and optimisation queries within a single paradigm.

</details>
