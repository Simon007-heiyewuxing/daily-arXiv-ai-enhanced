{"id": "2511.00011", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00011", "abs": "https://arxiv.org/abs/2511.00011", "authors": ["Alexander Okupnik", "Johannes Schneider", "Kyriakos Flouris"], "title": "Generative human motion mimicking through feature extraction in denoising diffusion settings", "comment": null, "summary": "Recent success with large language models has sparked a new wave of verbal\nhuman-AI interaction. While such models support users in a variety of creative\ntasks, they lack the embodied nature of human interaction. Dance, as a primal\nform of human expression, is predestined to complement this experience. To\nexplore creative human-AI interaction exemplified by dance, we build an\ninteractive model based on motion capture (MoCap) data. It generates an\nartificial other by partially mimicking and also \"creatively\" enhancing an\nincoming sequence of movement data. It is the first model, which leverages\nsingle-person motion data and high level features in order to do so and, thus,\nit does not rely on low level human-human interaction data. It combines ideas\nof two diffusion models, motion inpainting, and motion style transfer to\ngenerate movement representations that are both temporally coherent and\nresponsive to a chosen movement reference. The success of the model is\ndemonstrated by quantitatively assessing the convergence of the feature\ndistribution of the generated samples and the test set which serves as\nsimulating the human performer. We show that our generations are first steps to\ncreative dancing with AI as they are both diverse showing various deviations\nfrom the human partner while appearing realistic.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4ec5\u57fa\u4e8e\u5355\u4ebaMoCap\u548c\u9ad8\u5c42\u7279\u5f81\u7684\u4ea4\u4e92\u5f0f\u821e\u8e48\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u6269\u6563\u3001\u52a8\u4f5c\u4fee\u8865\u4e0e\u98ce\u683c\u8fc1\u79fb\uff0c\u80fd\u751f\u6210\u65f6\u95f4\u8fde\u8d2f\u3001\u56de\u5e94\u53c2\u8003\u52a8\u4f5c\u4e14\u5177\u6709\u521b\u610f\u504f\u79bb\u7684\u821e\u8e48\u52a8\u4f5c\uff0c\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u7ed3\u679c\u65e2\u591a\u6837\u53c8\u771f\u5b9e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u4e86\u4eba\u673a\u53e3\u5934\u4ea4\u4e92\u7684\u53d1\u5c55\uff0c\u4f46\u7f3a\u4e4f\u5177\u8eab\u6027\uff1b\u821e\u8e48\u4f5c\u4e3a\u4eba\u7c7b\u539f\u59cb\u8868\u8fbe\u5f62\u5f0f\uff0c\u53ef\u4e3a\u8fd9\u79cd\u4e92\u52a8\u8865\u5145\u5177\u8eab\u7ef4\u5ea6\uff0c\u63a2\u7d22\u521b\u610f\u4eba\u673a\u4e92\u52a8\u7684\u53ef\u80fd\u6027\u3002", "method": "\u6a21\u578b\u7ed3\u5408\u4e86\u4e24\u7c7b\u6269\u6563\u6a21\u578b\u3001\u52a8\u4f5c\u4fee\u8865\uff08motion inpainting\uff09\u548c\u52a8\u4f5c\u98ce\u683c\u8fc1\u79fb\uff08motion style transfer\uff09\u601d\u60f3\uff0c\u5229\u7528\u9ad8\u5c42\u6b21\u7279\u5f81\u800c\u975e\u4f4e\u5c42\u4eba\u9645\u4ea4\u4e92\u6570\u636e\uff0c\u751f\u6210\u65f6\u95f4\u4e0a\u8fde\u8d2f\u4e14\u5bf9\u53c2\u8003\u52a8\u4f5c\u54cd\u5e94\u7684\u8fd0\u52a8\u8868\u793a\u3002", "result": "\u901a\u8fc7\u91cf\u5316\u8bc4\u4f30\u751f\u6210\u6837\u672c\u4e0e\u6d4b\u8bd5\u96c6\uff08\u6a21\u62df\u4eba\u7c7b\u8868\u6f14\u8005\uff09\u7279\u5f81\u5206\u5e03\u7684\u6536\u655b\u6027\uff0c\u8bc1\u660e\u751f\u6210\u52a8\u4f5c\u65e2\u591a\u6837\uff08\u5728\u4fdd\u6301\u73b0\u5b9e\u611f\u7684\u540c\u65f6\u51fa\u73b0\u5bf9\u4eba\u7c7b\u4f19\u4f34\u7684\u5404\u79cd\u504f\u79bb\uff09\uff0c\u53c8\u73b0\u5b9e\u53ef\u4fe1\uff0c\u4f53\u73b0\u4e3a\u521b\u610f\u821e\u8e48\u7684\u521d\u6b65\u6210\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u6210\u529f\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5355\u4ebaMoCap\u6570\u636e\u7684\u4ea4\u4e92\u5f0f\u821e\u8e48\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u90e8\u5206\u6a21\u4eff\u5e76\u5bcc\u6709\u521b\u9020\u6027\u5730\u589e\u5f3a\u8f93\u5165\u52a8\u4f5c\u5e8f\u5217\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u5171\u821e\u4e2d\u521b\u610f\u4e92\u52a8\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.00021", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00021", "abs": "https://arxiv.org/abs/2511.00021", "authors": ["Julio Jerison E. Macrohon", "Gordon Hung"], "title": "Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets", "comment": "15 pages, 10 figures", "summary": "Coral reefs support numerous marine organisms and are an important source of\ncoastal protection from storms and floods, representing a major part of marine\necosystems. However coral reefs face increasing threats from pollution, ocean\nacidification, and sea temperature anomalies, making efficient protection and\nmonitoring heavily urgent. Therefore, this study presents a novel\nmachine-learning-based coral bleaching classification system based on a diverse\nglobal dataset with samples of healthy and bleached corals under varying\nenvironmental conditions, including deep seas, marshes, and coastal zones. We\nbenchmarked and compared three state-of-the-art models: Residual Neural Network\n(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).\nAfter comprehensive hyperparameter tuning, the CNN model achieved the highest\naccuracy of 88%, outperforming existing benchmarks. Our findings offer\nimportant insights into autonomous coral monitoring and present a comprehensive\nanalysis of the most widely used computer vision models.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u5e76\u6bd4\u8f83\u4e86ResNet\u3001ViT\u4e0eCNN\u4e09\u7c7b\u6a21\u578b\u7528\u4e8e\u591a\u73af\u5883\u4e0b\u73ca\u745a\u767d\u5316\u5206\u7c7b\uff0c\u7ecf\u8fc7\u8c03\u53c2\u540eCNN\u4ee588%\u51c6\u786e\u7387\u9886\u5148\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u73ca\u745a\u76d1\u6d4b\u7684\u53ef\u884c\u6027\u4e0e\u6a21\u578b\u9009\u62e9\u7684\u5b9e\u8bc1\u5bf9\u6bd4\u3002", "motivation": "\u73ca\u745a\u7901\u9762\u4e34\u6c61\u67d3\u3001\u6d77\u6c34\u9178\u5316\u548c\u6d77\u6e29\u5f02\u5e38\u7b49\u5a01\u80c1\uff0c\u8feb\u5207\u9700\u8981\u9ad8\u6548\u7684\u76d1\u6d4b\u4e0e\u4fdd\u62a4\u624b\u6bb5\uff0c\u56e0\u800c\u7814\u7a76\u81ea\u52a8\u5316\u73ca\u745a\u767d\u5316\u68c0\u6d4b\u4e0e\u5206\u7c7b\u65b9\u6cd5\u3002", "method": "\u6536\u96c6\u5305\u542b\u5065\u5eb7\u4e0e\u767d\u5316\u73ca\u745a\u7684\u591a\u73af\u5883\uff08\u6df1\u6d77\u3001\u6cbc\u6cfd\u3001\u6cbf\u6d77\uff09\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5bf9ResNet\u3001ViT\u548cCNN\u8fdb\u884c\u57fa\u7ebf\u6d4b\u8bd5\u5e76\u901a\u8fc7\u5168\u9762\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u6700\u7ec8\u9009\u62e9\u6700\u4f73\u6a21\u578b\u3002", "result": "\u7ecf\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u540e\uff0c\u81ea\u5efaCNN\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u6700\u9ad888%\u51c6\u786e\u7387\uff0c\u4f18\u4e8eResNet\u4e0eViT\uff0c\u8868\u660e\u4f20\u7edf\u5377\u79ef\u7f51\u7edc\u5728\u8be5\u4efb\u52a1\u4e0a\u4ecd\u5177\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u73ca\u745a\u767d\u5316\u5206\u7c7b\u7cfb\u7edf\uff0c\u5e76\u5728\u591a\u6837\u5316\u7684\u5168\u7403\u6570\u636e\u96c6\u4e0a\u6bd4\u8f83\u4e86ResNet\u3001ViT\u548c\u81ea\u5efaCNN\u4e09\u79cd\u6a21\u578b\uff0c\u6700\u7ec8CNN\u53d6\u5f97\u4e8688%\u51c6\u786e\u7387\u3002"}}
{"id": "2511.00022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00022", "abs": "https://arxiv.org/abs/2511.00022", "authors": ["Jules Gerard", "Leandro Di Bella", "Filip Huyghe", "Marc Kochzius"], "title": "Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline", "comment": "Accepted to EUVIP2025, student session", "summary": "Coral reef monitoring in the Western Indian Ocean is limited by the labor\ndemands of underwater visual censuses. This work evaluates a YOLOv8-based deep\nlearning pipeline for automating family-level fish identification from video\ntransects collected in Kenya and Tanzania. A curated dataset of 24 families was\ntested under different configurations, providing the first region-specific\nbenchmark for automated reef fish monitoring in the Western Indian Ocean. The\nbest model achieved mAP@0.5 of 0.52, with high accuracy for abundant families\nbut weaker detection of rare or complex taxa. Results demonstrate the potential\nof deep learning as a scalable complement to traditional monitoring methods.", "AI": {"tldr": "\u4f7f\u7528YOLOv8\u5728\u80af\u5c3c\u4e9a\u548c\u5766\u6851\u5c3c\u4e9a\u89c6\u9891\u4e0a\u8fdb\u884c24\u79d1\u73ca\u745a\u7901\u9c7c\u81ea\u52a8\u8bc6\u522b\uff0c\u6700\u4f73mAP@0.5=0.52\uff0c\u9002\u5408\u5e38\u89c1\u79d1\uff0c\u7a00\u6709/\u590d\u6742\u79d1\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u51cf\u8f7b\u6c34\u4e0b\u76ee\u89c6\u666e\u67e5\u7684\u4eba\u5de5\u8d1f\u62c5\uff0c\u63d0\u5347\u73ca\u745a\u7901\u9c7c\u7c7b\u76d1\u6d4b\u7684\u89c4\u6a21\u5316\u4e0e\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u63d0\u4f9b\u533a\u57df\u7279\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u57fa\u4e8eYOLOv8\u7684\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u7ba1\u7ebf\uff0c\u5bf9\u80af\u5c3c\u4e9a\u548c\u5766\u6851\u5c3c\u4e9a\u7684\u89c6\u9891\u6837\u5e26\u8fdb\u884c\u5bb6\u65cf\u7ea7\u6807\u6ce8\u4e0e\u8bad\u7ec3\uff0c\u4f7f\u7528\u542b24\u4e2a\u79d1\u7684\u7cbe\u5fc3\u6311\u9009\u6570\u636e\u96c6\u8fdb\u884c\u591a\u914d\u7f6e\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728mAP@0.5\u4e0a\u8fbe\u52300.52\uff0c\u5bf9\u4e30\u5ea6\u8f83\u9ad8\u7684\u79d1\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u7a00\u6709\u6216\u5f62\u6001\u590d\u6742\u7684\u79d1\u68c0\u6d4b\u8f83\u5f31\u3002\u8be5\u5de5\u4f5c\u4e3a\u897f\u5370\u5ea6\u6d0b\u81ea\u52a8\u5316\u7901\u9c7c\u76d1\u6d4b\u63d0\u4f9b\u4e86\u9996\u4e2a\u533a\u57df\u6027\u57fa\u51c6\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u80fd\u5728\u897f\u5370\u5ea6\u6d0b\u73ca\u745a\u7901\u9c7c\u7c7b\u76d1\u6d4b\u4e2d\u4f5c\u4e3a\u4f20\u7edf\u76ee\u89c6\u8c03\u67e5\u7684\u53ef\u6269\u5c55\u8865\u5145\uff0c\u4f46\u76ee\u524d\u5bf9\u7a00\u6709\u6216\u5f62\u6001\u590d\u6742\u79d1\u7684\u68c0\u6d4b\u4ecd\u6709\u9650\u3002"}}
{"id": "2511.00028", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00028", "abs": "https://arxiv.org/abs/2511.00028", "authors": ["Hanyang Chen", "Yanchao Yang"], "title": "Mutual Information guided Visual Contrastive Learning", "comment": "Tech Report - Undergraduate Thesis - 2023", "summary": "Representation learning methods utilizing the InfoNCE loss have demonstrated\nconsiderable capacity in reducing human annotation effort by training invariant\nneural feature extractors. Although different variants of the training\nobjective adhere to the information maximization principle between the data and\nlearned features, data selection and augmentation still rely on human\nhypotheses or engineering, which may be suboptimal. For instance, data\naugmentation in contrastive learning primarily focuses on color jittering,\naiming to emulate real-world illumination changes. In this work, we investigate\nthe potential of selecting training data based on their mutual information\ncomputed from real-world distributions, which, in principle, should endow the\nlearned features with better generalization when applied in open environments.\nSpecifically, we consider patches attached to scenes that exhibit high mutual\ninformation under natural perturbations, such as color changes and motion, as\npositive samples for learning with contrastive loss. We evaluate the proposed\nmutual-information-informed data augmentation method on several benchmarks\nacross multiple state-of-the-art representation learning frameworks,\ndemonstrating its effectiveness and establishing it as a promising direction\nfor future research.", "AI": {"tldr": "\u901a\u8fc7\u57fa\u4e8e\u81ea\u7136\u6270\u52a8\u4e0b\u4e92\u4fe1\u606f\u6311\u9009\u6b63\u6837\u672c\u7528\u4e8e\u5bf9\u6bd4\u5b66\u4e60\uff0c\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u6570\u636e\u589e\u5f3a\uff0c\u80fd\u66f4\u597d\u5730\u5b66\u4e60\u6cdb\u5316\u4e0d\u53d8\u7279\u5f81\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6548\u679c\u663e\u8457\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eInfoNCE\u7684\u8868\u793a\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\uff08\u5982\u989c\u8272\u6296\u52a8\uff09\u6765\u6a21\u62df\u73b0\u5b9e\u6270\u52a8\uff0c\u4f46\u8fd9\u4e9b\u5047\u8bbe\u53ef\u80fd\u4e0d\u5145\u5206\u6216\u6b21\u4f18\uff1b\u5229\u7528\u771f\u5b9e\u4e16\u754c\u4e92\u4fe1\u606f\u9009\u62e9\u6837\u672c\u53ef\u66f4\u76f4\u63a5\u6355\u6349\u81ea\u7136\u6270\u52a8\u4e0b\u7684\u4e0d\u53d8\u6027\u3002", "method": "\u8ba1\u7b97\u573a\u666f\u4e2d\u4e0d\u540cpatch\u5728\u81ea\u7136\u6270\u52a8\uff08\u5982\u989c\u8272\u53d8\u5316\u548c\u8fd0\u52a8\uff09\u4e0b\u7684\u4e92\u4fe1\u606f\uff0c\u5e76\u5c06\u4e92\u4fe1\u606f\u9ad8\u7684patch\u5bf9\u4f5c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u7684\u6b63\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff1b\u5728\u591a\u79cd\u4ee3\u8868\u6027\u5b66\u4e60\u6846\u67b6\u4e0e\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u5bf9\u6bd4\u3002", "result": "\u5728\u591a\u79cd\u6700\u65b0\u7684\u8868\u5f81\u5b66\u4e60\u6846\u67b6\u548c\u57fa\u51c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5c55\u793a\u4e86\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u5b66\u4e60\u7279\u5f81\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u8868\u73b0\uff0c\u8868\u660e\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u9009\u62e9\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u7684\u4e92\u4fe1\u606f\u6765\u9009\u62e9\u8bad\u7ec3\u6837\u672c\u4f5c\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u6b63\u6837\u672c\uff0c\u4ece\u800c\u66ff\u4ee3\u6216\u8865\u5145\u624b\u5de5\u8bbe\u8ba1\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u4ee5\u63d0\u5347\u8868\u5f81\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.00290", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00290", "abs": "https://arxiv.org/abs/2511.00290", "authors": ["Ashwin Gerard Colaco", "Sharad Mehrotra", "Michael J De Lucia", "Kevin Hamlen", "Murat Kantarcioglu", "Latifur Khan", "Ananthram Swami", "Bhavani Thuraisingham"], "title": "NOMAD - Navigating Optimal Model Application to Datastreams", "comment": null, "summary": "NOMAD (Navigating Optimal Model Application for Datastreams) is an\nintelligent framework for data enrichment during ingestion that optimizes\nrealtime multiclass classification by dynamically constructing model chains,\ni.e ,sequences of machine learning models with varying cost-quality tradeoffs,\nselected via a utilitybased criterion. Inspired by predicate ordering\ntechniques from database query processing, NOMAD leverages cheaper models as\ninitial filters, proceeding to more expensive models only when necessary, while\nguaranteeing classification quality remains comparable to a designated role\nmodel through a formal chain safety mechanism. It employs a dynamic belief\nupdate strategy to adapt model selection based on per event predictions and\nshifting data distributions, and extends to scenarios with dependent models\nsuch as earlyexit DNNs and stacking ensembles. Evaluation across multiple\ndatasets demonstrates that NOMAD achieves significant computational savings\ncompared to static and naive approaches while maintaining classification\nquality comparable to that achieved by the most accurate (and often the most\nexpensive) model.", "AI": {"tldr": "NOMAD \u901a\u8fc7\u52a8\u6001\u3001\u57fa\u4e8e\u6548\u7528\u7684\u6a21\u578b\u94fe\u7ba1\u7406\u548c\u94fe\u5b89\u5168\u6027\u4fdd\u969c\uff0c\u5728\u6570\u636e\u6444\u53d6\u65f6\u5b9e\u73b0\u4f4e\u6210\u672c\u4e14\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\u3002", "motivation": "\u5728\u7ebf\u6570\u636e\u6d41\u6444\u53d6\u4e2d\u9700\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\uff0c\u5355\u4e00\u9ad8\u8d28\u91cf\u6a21\u578b\u8ba1\u7b97\u4ee3\u4ef7\u9ad8\uff0c\u9759\u6001\u6a21\u578b\u7ec4\u5408/\u9009\u62e9\u7f3a\u4e4f\u7075\u6d3b\u6027\u4e0e\u9002\u5e94\u6027\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u4fdd\u8bc1\u5206\u7c7b\u8d28\u91cf\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u52a8\u6001\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u6548\u7528\u51c6\u5219\u52a8\u6001\u9009\u53d6\u4e0d\u540c\u4ee3\u4ef7-\u8d28\u91cf\u6743\u8861\u7684\u6a21\u578b\u5e8f\u5217\uff08\u6a21\u578b\u94fe\uff09\uff0c\u501f\u9274\u6570\u636e\u5e93\u8c13\u8bcd\u6392\u5e8f\u601d\u60f3\uff0c\u7528\u5ec9\u4ef7\u6a21\u578b\u505a\u521d\u7ea7\u8fc7\u6ee4\u5e76\u5728\u5fc5\u8981\u65f6\u8c03\u7528\u66f4\u6602\u8d35\u7684\u6a21\u578b\uff1b\u5f15\u5165\u94fe\u5b89\u5168\u673a\u5236\u4fdd\u8bc1\u8d28\u91cf\u4e0d\u4f4e\u4e8e\u89d2\u8272\u6a21\u578b\uff1b\u91c7\u7528\u52a8\u6001\u4fe1\u5ff5\u66f4\u65b0\u6839\u636e\u6bcf\u4e2a\u4e8b\u4ef6\u9884\u6d4b\u548c\u6570\u636e\u5206\u5e03\u6f02\u79fb\u81ea\u9002\u5e94\u9009\u62e9\u6a21\u578b\uff1b\u5e76\u6269\u5c55\u5230\u6a21\u578b\u76f8\u5173\u6027\u573a\u666f\uff08\u5982\u65e9\u9000DNN\u548c\u5806\u53e0\u96c6\u6210\uff09\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cNOMAD \u5728\u4fdd\u6301\u4e0e\u6700\u51c6\u786e\u6a21\u578b\u76f8\u5f53\u7684\u5206\u7c7b\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u76f8\u8f83\u4e8e\u9759\u6001\u6216\u7b80\u5355\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u8282\u7701\u3002", "conclusion": "NOMAD \u63d0\u4f9b\u4e86\u4e00\u79cd\u5728\u6570\u636e\u6444\u53d6\u65f6\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u6a21\u578b\u94fe\u6765\u4f18\u5316\u5b9e\u65f6\u591a\u7c7b\u5206\u7c7b\u7684\u667a\u80fd\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u4e0e\u6307\u5b9a\u89d2\u8272\u6a21\u578b\u76f8\u5f53\u7684\u5206\u7c7b\u8d28\u91cf\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2511.00037", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.00037", "abs": "https://arxiv.org/abs/2511.00037", "authors": ["Riya Gupta", "Alexander Chowdhury", "Sahil Nalawade"], "title": "Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra", "comment": null, "summary": "Federated Learning (FL) has emerged as a transformative paradigm in medical\nAI, enabling collaborative model training across institutions without direct\ndata sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,\nFlower, and Owkin Substra to evaluate their suitability for medical imaging\napplications in real-world settings. Using the PathMNIST dataset, we assess\nmodel performance, convergence efficiency, communication overhead, scalability,\nand developer experience. Results indicate that NVIDIA FLARE offers superior\nproduction scalability, Flower provides flexibility for prototyping and\nacademic research, and Owkin Substra demonstrates exceptional privacy and\ncompliance features. Each framework exhibits strengths optimized for distinct\nuse cases, emphasizing their relevance to practical deployment in healthcare\nenvironments.", "AI": {"tldr": "\u5728\u533b\u7597\u5f71\u50cf\u7684\u8054\u90a6\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1aNVIDIA FLARE\u9002\u5408\u751f\u4ea7\u90e8\u7f72\uff0cFlower\u9002\u5408\u7814\u7a76\u4e0e\u539f\u578b\uff0cOwkin Substra\u9002\u5408\u9ad8\u5ea6\u5408\u89c4\u9700\u6c42\uff1b\u9009\u62e9\u5e94\u57fa\u4e8e\u573a\u666f\u4e0e\u6743\u8861\u3002", "motivation": "\u533b\u7597\u5f71\u50cf\u9886\u57df\u5bf9\u6570\u636e\u9690\u79c1\u4e0e\u8de8\u673a\u6784\u534f\u4f5c\u6709\u8feb\u5207\u9700\u6c42\uff0c\u8054\u90a6\u5b66\u4e60\u80fd\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u4e0b\u8054\u5408\u8bad\u7ec3\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u533b\u7597\u673a\u6784\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u5bf9\u4e3b\u6d41FL\u6846\u67b6\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u8868\u73b0\u7684\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u5e2e\u52a9\u9009\u578b\u4e0e\u90e8\u7f72\u51b3\u7b56\u3002", "method": "\u57fa\u4e8ePathMNIST\u6570\u636e\u96c6\u5728\u4e09\u4e2a\u6846\u67b6\u4e0a\u8bad\u7ec3\u76f8\u540c\u7684\u6a21\u578b\uff0c\u6bd4\u8f83\u6a21\u578b\u6027\u80fd\uff08\u51c6\u786e\u7387/\u6307\u6807\uff09\u3001\u6536\u655b\u901f\u5ea6\u3001\u901a\u4fe1\u5f00\u9500\u3001\u6269\u5c55\u6027\uff08\u8282\u70b9\u589e\u591a\u65f6\u7684\u8868\u73b0\uff09\u53ca\u5f00\u53d1\u8005\u4f53\u9a8c\uff08\u90e8\u7f72\u590d\u6742\u5ea6\u3001\u6587\u6863\u4e0eAPI\u53cb\u597d\u6027\uff09\u3002\u7edf\u8ba1\u591a\u6b21\u5b9e\u9a8c\u7ed3\u679c\u5e76\u8bb0\u5f55\u8bad\u7ec3\u65f6\u95f4\u3001\u901a\u4fe1\u8f6e\u6b21\u4e0e\u6a21\u578b\u6027\u80fd\u53d8\u5316\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a\u5728\u76f8\u540c\u6a21\u578b\u4e0e\u6570\u636e\u5212\u5206\u4e0b\uff0c\u4e09\u6846\u67b6\u7684\u6700\u7ec8\u6a21\u578b\u6027\u80fd\u76f8\u8fd1\uff1bFLARE\u5728\u591a\u8282\u70b9\u6269\u5c55\u4e0e\u751f\u4ea7\u90e8\u7f72\u4e2d\u8bad\u7ec3\u65f6\u95f4\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\u66f4\u4f73\uff1bFlower\u5728\u5feb\u901f\u8fed\u4ee3\u3001\u5b9a\u5236\u901a\u4fe1\u7b56\u7565\u548c\u5b9e\u9a8c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u6700\u4f18\uff1bSubstra\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u3001\u5408\u89c4\u4e0e\u5ba1\u8ba1\u529f\u80fd\uff0c\u4f46\u90e8\u7f72\u4e0e\u96c6\u6210\u6210\u672c\u8f83\u9ad8\u3002\u901a\u4fe1\u5f00\u9500\u548c\u6536\u655b\u8f6e\u6b21\u5728\u5404\u6846\u67b6\u95f4\u5dee\u5f02\u6709\u9650\uff0c\u66f4\u591a\u53d6\u51b3\u4e8e\u7b97\u6cd5\u4e0e\u7f51\u7edc\u5e26\u5bbd\u914d\u7f6e\u3002", "conclusion": "\u4e09\u79cd\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u5404\u6709\u4fa7\u91cd\uff1aNVIDIA FLARE\u5728\u751f\u4ea7\u7ea7\u53ef\u6269\u5c55\u6027\u548c\u90e8\u7f72\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1bFlower\u5728\u539f\u578b\u9a8c\u8bc1\u548c\u5b66\u672f\u7814\u7a76\u4e2d\u6700\u7075\u6d3b\uff1bOwkin Substra\u5728\u9690\u79c1\u5408\u89c4\u4e0e\u6570\u636e\u6cbb\u7406\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u6ca1\u6709\u5355\u4e00\u6846\u67b6\u9002\u7528\u4e8e\u6240\u6709\u573a\u666f\uff0c\u5e94\u57fa\u4e8e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u3002"}}
{"id": "2511.00414", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00414", "abs": "https://arxiv.org/abs/2511.00414", "authors": ["Sirintra Vaiwsri", "Thilina Ranbaduge"], "title": "Embedding based Encoding Scheme for Privacy Preserving Record Linkage", "comment": "12 pages", "summary": "To discover new insights from data, there is a growing need to share\ninformation that is often held by different organisations. One key task in data\nintegration is the calculation of similarities between records in different\ndatabases to identify pairs or sets of records that correspond to the same\nreal-world entities. Due to privacy and confidentiality concerns, however, the\nowners of sensitive databases are often not allowed or willing to exchange or\nshare their data with other organisations to allow such similarity\ncalculations. Privacy-preserving record linkage (PPRL) is the process of\nmatching records that refer to the same entity across sensitive databases held\nby different organisations while ensuring no information about the entities is\nrevealed to the participating parties. In this paper, we study how embedding\nbased encoding techniques can be applied in the PPRL context to ensure the\nprivacy of the entities that are being linked. We first convert individual\nq-grams into the embedded space and then convert the embedding of a set of\nq-grams of a given record into a binary representation. The final binary\nrepresentations can be used to link records into matches and non-matches. We\nempirically evaluate our proposed encoding technique against different\nreal-world datasets. The results suggest that our proposed encoding approach\ncan provide better linkage accuracy and protect the privacy of individuals\nagainst attack compared to state-of-the-art techniques for short record values.", "AI": {"tldr": "\u63d0\u51fa\u628aq-gram\u5d4c\u5165\u5e76\u4e8c\u503c\u5316\u4e3a\u9690\u79c1\u4fdd\u62a4\u7f16\u7801\u7684\u65b9\u6cd5\u7528\u4e8ePPRL\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u77ed\u6587\u672c\u8bb0\u5f55\u4e0a\u51c6\u786e\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e0d\u540c\u7ec4\u7ec7\u95f4\u9700\u8981\u5408\u5e76\u654f\u611f\u6570\u636e\u4ee5\u53d1\u73b0\u65b0\u6d1e\u89c1\uff0c\u4f46\u51fa\u4e8e\u9690\u79c1\u4e0e\u5408\u89c4\u539f\u56e0\u4e0d\u80fd\u76f4\u63a5\u5171\u4eab\u6570\u636e\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5728\u4e0d\u6cc4\u9732\u5b9e\u4f53\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u5e93\u5339\u914d\u7684\u6280\u672f\u3002", "method": "\u5c06\u5355\u4e2aq-gram\u6620\u5c04\u5230\u5411\u91cf\u5d4c\u5165\u7a7a\u95f4\uff0c\u968f\u540e\u5c06\u4e00\u6761\u8bb0\u5f55\u4e2d\u6240\u6709q-gram\u7684\u5d4c\u5165\u805a\u5408\u5e76\u91cf\u5316\u4e3a\u4e8c\u8fdb\u5236\u8868\u793a\uff08\u7f16\u7801\uff09\uff0c\u6700\u7ec8\u4f7f\u7528\u8fd9\u4e9b\u4e8c\u8fdb\u5236\u8868\u793a\u8ba1\u7b97\u8bb0\u5f55\u95f4\u76f8\u4f3c\u6027\u4ee5\u8fdb\u884c\u5339\u914d\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u7f16\u7801\u5728\u77ed\u8bb0\u5f55\u503c\uff08\u5982\u59d3\u540d\u3001\u5730\u5740\u7247\u6bb5\u7b49\uff09\u4e0a\u6bd4\u73b0\u6709PPRL\u65b9\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u94fe\u63a5\u7cbe\u5ea6\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u62b5\u6297\u9488\u5bf9\u9690\u79c1\u7684\u653b\u51fb\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5d4c\u5165\u7684\u7f16\u7801\u65b9\u6cd5\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u8bb0\u5f55\u94fe\u63a5\uff08PPRL\uff09\uff0c\u901a\u8fc7\u5c06q-gram\u5148\u6620\u5c04\u5230\u5d4c\u5165\u7a7a\u95f4\uff0c\u518d\u5c06\u8bb0\u5f55\u7684q-gram\u96c6\u5408\u5d4c\u5165\u8f6c\u6362\u4e3a\u4e8c\u8fdb\u5236\u8868\u793a\uff0c\u4ece\u800c\u5728\u4e0d\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u8fdb\u884c\u5339\u914d\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u77ed\u6587\u672c\u8bb0\u5f55\u4e0a\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u80fd\u63d0\u4f9b\u66f4\u597d\u7684\u5339\u914d\u51c6\u786e\u6027\u5e76\u589e\u5f3a\u5bf9\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002"}}
{"id": "2511.00046", "categories": ["cs.CV", "68U10, 94A08", "I.4.3; I.4.4; I.5.1; J.3"], "pdf": "https://arxiv.org/pdf/2511.00046", "abs": "https://arxiv.org/abs/2511.00046", "authors": ["Rupjyoti Chutia", "Dibya Jyoti Bora"], "title": "Enhancing rice leaf images: An overview of image denoising techniques", "comment": "18 pages, 6 figures. Research Article published in the International\n  Journal of Agricultural and Natural Sciences (IJANS), Vol. 18, Issue 2, 2025.\n  This paper presents a comparative study of image denoising and CLAHE\n  techniques for enhancing rice leaf images corrupted by Gaussian,\n  Salt-and-pepper, Speckle, and Random noise for agricultural analysis", "summary": "Digital image processing involves the systematic handling of images using\nadvanced computer algorithms, and has gained significant attention in both\nacademic and practical fields. Image enhancement is a crucial preprocessing\nstage in the image-processing chain, improving image quality and emphasizing\nfeatures. This makes subsequent tasks (segmentation, feature extraction,\nclassification) more reliable. Image enhancement is essential for rice leaf\nanalysis, aiding in disease detection, nutrient deficiency evaluation, and\ngrowth analysis. Denoising followed by contrast enhancement are the primary\nsteps. Image filters, generally employed for denoising, transform or enhance\nvisual characteristics like brightness, contrast, and sharpness, playing a\ncrucial role in improving overall image quality and enabling the extraction of\nuseful information. This work provides an extensive comparative study of\nwell-known image-denoising methods combined with CLAHE (Contrast Limited\nAdaptive Histogram Equalization) for efficient denoising of rice leaf images.\nThe experiments were performed on a rice leaf image dataset to ensure the data\nis relevant and representative. Results were examined using various metrics to\ncomprehensively test enhancement methods. This approach provides a strong basis\nfor assessing the effectiveness of methodologies in digital image processing\nand reveals insights useful for future adaptation in agricultural research and\nother domains.", "AI": {"tldr": "\u6587\u7ae0\u5728\u7a3b\u53f6\u56fe\u50cf\u4e0a\u6bd4\u8f83\u4e86\u591a\u79cd\u53bb\u566a\u65b9\u6cd5\u52a0CLAHE\u7684\u7ec4\u5408\uff0c\u53d1\u73b0\u8fb9\u7f18\u4fdd\u7559\u578b\u53bb\u566a+CLAHE\u5728\u4fdd\u7ec6\u8282\u4e14\u63d0\u5347\u5bf9\u6bd4\u5ea6\u65b9\u9762\u6548\u679c\u6700\u4f73\uff0c\u6709\u52a9\u4e8e\u540e\u7eed\u4f5c\u7269\u75c5\u5bb3\u5206\u6790\u3002", "motivation": "\u63d0\u9ad8\u7a3b\u53f6\u56fe\u50cf\u7684\u8d28\u91cf\u4ee5\u589e\u5f3a\u75c5\u5bb3\u68c0\u6d4b\u3001\u8425\u517b\u7f3a\u9677\u8bc4\u4f30\u548c\u751f\u957f\u5206\u6790\u7684\u51c6\u786e\u6027\uff1b\u9274\u4e8e\u566a\u58f0\u548c\u5bf9\u6bd4\u5ea6\u4e0d\u4f73\u4f1a\u5f71\u54cd\u540e\u7eed\u5206\u5272\u4e0e\u7279\u5f81\u63d0\u53d6\uff0c\u7814\u7a76\u7ec4\u5408\u53bb\u566a\u4e0eCLAHE\u7684\u6548\u679c\u5177\u6709\u5b9e\u9645\u610f\u4e49\u3002", "method": "\u4ee5\u7a3b\u53f6\u56fe\u50cf\u6570\u636e\u96c6\u4e3a\u57fa\u7840\uff0c\u5148\u5bf9\u56fe\u50cf\u8fdb\u884c\u53bb\u566a\uff08\u6bd4\u8f83\u591a\u79cd\u5e38\u89c1\u6ee4\u6ce2\u5668\uff0c\u5982\u5747\u503c\u6ee4\u6ce2\u3001\u4e2d\u503c\u6ee4\u6ce2\u3001\u9ad8\u65af\u6ee4\u6ce2\u3001\u53cc\u8fb9\u6ee4\u6ce2\u3001\u975e\u5c40\u90e8\u5747\u503c\u7b49\uff09\uff0c\u7136\u540e\u5bf9\u53bb\u566a\u7ed3\u679c\u5e94\u7528CLAHE\u8fdb\u884c\u5bf9\u6bd4\u5ea6\u9650\u5236\u7684\u81ea\u9002\u5e94\u76f4\u65b9\u56fe\u5747\u8861\u5316\u3002\u91c7\u7528\u591a\u79cd\u56fe\u50cf\u8d28\u91cf\u8bc4\u4ef7\u6307\u6807\uff08\u5982PSNR\u3001SSIM\u3001MSE\u3001\u71b5\u3001\u8fb9\u7f18\u4fdd\u5b58\u6307\u6807\u7b49\uff09\u548c\u53ef\u89c6\u5316\u5bf9\u6bd4\u6765\u8bc4\u4f30\u65b9\u6cd5\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u57fa\u4e8e\u8fb9\u7f18\u4fdd\u6301\u80fd\u529b\u8f83\u597d\u7684\u53bb\u566a\u65b9\u6cd5\uff08\u4f8b\u5982\u53cc\u8fb9\u6ee4\u6ce2\u6216\u975e\u5c40\u90e8\u5747\u503c\uff09\u7ed3\u5408CLAHE\u901a\u5e38\u5728PSNR\u4e0eSSIM\u6307\u6807\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u540c\u65f6\u80fd\u66f4\u597d\u5730\u4fdd\u7559\u53f6\u8109\u548c\u75c5\u6591\u7ec6\u8282\uff1b\u7b80\u5355\u7684\u5747\u503c\u6ee4\u6ce2\u867d\u80fd\u53bb\u566a\u4f46\u4f1a\u6a21\u7cca\u7ec6\u8282\uff0c\u5f71\u54cd\u540e\u7eed\u5206\u6790\u3002\u4e0d\u540c\u65b9\u6cd5\u5728\u4e0d\u540c\u56fe\u50cf\u566a\u58f0\u7c7b\u578b\u4e0b\u8868\u73b0\u5dee\u5f02\u660e\u663e\u3002", "conclusion": "\u672c\u6587\u6bd4\u8f83\u4e86\u591a\u79cd\u56fe\u50cf\u53bb\u566a\u65b9\u6cd5\u4e0eCLAHE\u7ed3\u5408\u7528\u4e8e\u7a3b\u53f6\u56fe\u50cf\u589e\u5f3a\uff0c\u7ed3\u8bba\u4e3a\u67d0\u4e9b\u53bb\u566a\u65b9\u6cd5\u5728\u4fdd\u8fb9\u7f18\u548c\u7ec6\u8282\u540c\u65f6\u4e0eCLAHE\u914d\u5408\u53ef\u663e\u8457\u63d0\u5347\u7a3b\u53f6\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u4ece\u800c\u6709\u5229\u4e8e\u540e\u7eed\u75c5\u5bb3\u68c0\u6d4b\u4e0e\u7279\u5f81\u63d0\u53d6\u3002"}}
{"id": "2511.00693", "categories": ["cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00693", "abs": "https://arxiv.org/abs/2511.00693", "authors": ["Saba Latif", "Huma Latif", "Muhammad Rameez Ur Rahman"], "title": "Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries", "comment": "12 pages, 4 figures, PROFES2025 conference", "summary": "Object Centric Event Data (OCED) has gained attention in recent years within\nthe field of process mining. However, there are still many challenges, such as\nconnecting the XES format to object-centric approaches to enable more\ninsightful analysis. It is important for a process miner to understand the\ninsights and dependencies of events in the event log to see what is going on in\nour processes. In previous standards, the dependencies of event logs are only\nused to show events, but not their dependencies among each other and actions in\ndetail as described in OCEDO. There is more information in the event log when\nit is revealed using the OCEDO model. It becomes more understandable and easier\nto grasp the concepts and deal with the processes. This paper proposes the use\nof Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of\nthe XES standard in event logs for process mining. We demonstrate how the OCEDO\napproach, integrated with SPARQL queries, can be applied to the BPIC 2013\ndataset to make the relationships between events and objects more explicit. It\ndescribes dealing with the meta descriptions of the OCEDO model on a business\nprocess challenge as an event log. It improves the completeness and readability\nof process data, suggesting that object-centric modeling allows for richer\nanalyses than traditional approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06 OCEDO \u4e0e SPARQL \u7ed3\u5408\u7528\u4e8e BPIC2013 \u6570\u636e\u96c6\uff0c\u514b\u670d XES \u5728\u5bf9\u8c61-\u4e8b\u4ef6\u4f9d\u8d56\u8868\u8fbe\u4e0a\u7684\u9650\u5236\uff0c\u63d0\u5347\u65e5\u5fd7\u53ef\u8bfb\u6027\u4e0e\u5206\u6790\u6df1\u5ea6\u3002", "motivation": "XES \u6807\u51c6\u5728\u8868\u8fbe\u4e8b\u4ef6\u95f4\u4f9d\u8d56\u548c\u5bf9\u8c61\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u96be\u4ee5\u63ed\u793a\u66f4\u4e30\u5bcc\u7684\u8fc7\u7a0b\u4fe1\u606f\uff1bOCEDO \u53ef\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u4f7f\u8fc7\u7a0b\u77ff\u5de5\u66f4\u597d\u7406\u89e3\u4e8b\u4ef6\u4f9d\u8d56\u4e0e\u8fc7\u7a0b\u52a8\u6001\u3002", "method": "\u63d0\u51fa\u5c06 OCEDO \u672c\u4f53\u4e0e SPARQL \u67e5\u8be2\u7ed3\u5408\uff0c\u9488\u5bf9 BPIC2013 \u6570\u636e\u96c6\u8fdb\u884c\u5efa\u6a21\u4e0e\u67e5\u8be2\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u4e0e\u5bf9\u8c61\u5173\u7cfb\u7684\u663e\u5f0f\u5316\u8868\u793a\u3002", "result": "\u5728 BPIC2013 \u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a OCEDO \u80fd\u63d0\u9ad8\u65e5\u5fd7\u7684\u5b8c\u6574\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u4f7f\u5f97\u5bf9\u8c61\u4e2d\u5fc3\u7684\u5efa\u6a21\u6bd4\u4f20\u7edf\u65b9\u6cd5\u80fd\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u5206\u6790\u3002", "conclusion": "OCEDO \u63d0\u5347\u4e86\u4e8b\u4ef6\u65e5\u5fd7\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4f7f\u5bf9\u8c61-\u4e8b\u4ef6\u5173\u7cfb\u548c\u4f9d\u8d56\u66f4\u6e05\u6670\uff0c\u4ece\u800c\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u8fc7\u7a0b\u5206\u6790\u3002"}}
{"id": "2511.00060", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00060", "abs": "https://arxiv.org/abs/2511.00060", "authors": ["Zhiqi Qi", "Runxin Zhao", "Hanyang Zhuang", "Chunxiang Wang", "Ming Yang"], "title": "Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?", "comment": null, "summary": "LiDAR-based roadside perception is a cornerstone of advanced Intelligent\nTransportation Systems (ITS). While considerable research has addressed optimal\nLiDAR placement for infrastructure, the profound impact of differing LiDAR\nscanning patterns on perceptual performance remains comparatively\nunder-investigated. The inherent nature of various scanning modes - such as\ntraditional repetitive (mechanical/solid-state) versus emerging non-repetitive\n(e.g. prism-based) systems - leads to distinct point cloud distributions at\nvarying distances, critically dictating the efficacy of object detection and\noverall environmental understanding. To systematically investigate these\ndifferences in infrastructure-based contexts, we introduce the \"InfraLiDARs'\nBenchmark,\" a novel dataset meticulously collected in the CARLA simulation\nenvironment using concurrently operating infrastructure-based LiDARs exhibiting\nboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensive\nstatistical analysis of the respective LiDAR scanning abilities and evaluate\nthe impact of these distinct patterns on the performance of various leading 3D\nobject detection algorithms. Our findings reveal that non-repetitive scanning\nLiDAR and the 128-line repetitive LiDAR were found to exhibit comparable\ndetection performance across various scenarios. Despite non-repetitive LiDAR's\nlimited perception range, it's a cost-effective option considering its low\nprice. Ultimately, this study provides insights for setting up roadside\nperception system with optimal LiDAR scanning patterns and compatible\nalgorithms for diverse roadside applications, and publicly releases the\n\"InfraLiDARs' Benchmark\" dataset to foster further research.", "AI": {"tldr": "\u5728CARLA\u4eff\u771f\u4e2d\u6bd4\u8f83\u91cd\u590d\u4e0e\u975e\u91cd\u590d\u626b\u63cf\u8def\u4fa7LiDAR\uff0c\u6784\u5efa\u4e86\u516c\u5f00\u57fa\u51c6\u5e76\u53d1\u73b0\u975e\u91cd\u590d\u626b\u63cf\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u53ef\u4e0e128\u7ebf\u91cd\u590d\u626b\u63cf\u76f8\u5f53\uff0c\u4e14\u66f4\u5177\u6210\u672c\u6548\u76ca\uff0c\u5c3d\u7ba1\u611f\u77e5\u8ddd\u79bb\u53d7\u9650\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u8def\u4fa7LiDAR\u6700\u4f18\u5e03\u7f6e\u5df2\u6709\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u4e0d\u540c\u626b\u63cf\u6a21\u5f0f\uff08\u91cd\u590d vs \u975e\u91cd\u590d\uff09\u5bf9\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u7531\u4e8e\u626b\u63cf\u6a21\u5f0f\u6539\u53d8\u70b9\u4e91\u968f\u8ddd\u79bb\u7684\u5206\u5e03\uff0c\u53ef\u80fd\u663e\u8457\u5f71\u54cd\u76ee\u6807\u68c0\u6d4b\u548c\u73af\u5883\u7406\u89e3\uff0c\u4e9f\u9700\u91cf\u5316\u6bd4\u8f83\u3002", "method": "\u5728CARLA\u4eff\u771f\u4e2d\u540c\u6b65\u90e8\u7f72\u591a\u53f0\u57fa\u7840\u8bbe\u65bdLiDAR\uff08\u5305\u542b\u91cd\u590d\u4e0e\u975e\u91cd\u590d\u626b\u63cf\u6a21\u5f0f\uff09\uff0c\u6784\u5efa\u4e86\u2018InfraLiDARs' Benchmark\u2019\u6570\u636e\u96c6\uff1b\u5bf9\u70b9\u4e91\u7a7a\u95f4\u5206\u5e03\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\uff1b\u4f7f\u7528\u591a\u79cd\u4e3b\u6d413D\u76ee\u6807\u68c0\u6d4b\u5668\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6bd4\u8f83\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u4e0d\u540c\u626b\u63cf\u6a21\u5f0f\u5bf9\u68c0\u6d4b\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7edf\u8ba1\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\u975e\u91cd\u590d\u626b\u63cfLiDAR\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4e0e128\u7ebf\u91cd\u590d\u626b\u63cfLiDAR\u603b\u4f53\u76f8\u8fd1\uff1b\u975e\u91cd\u590dLiDAR\u611f\u77e5\u8303\u56f4\u8f83\u5c0f\u4f46\u6210\u672c\u4f4e\uff1b\u4e0d\u540c\u626b\u63cf\u6a21\u5f0f\u5728\u4e0d\u540c\u573a\u666f\u548c\u8ddd\u79bb\u4e0b\u5448\u73b0\u5dee\u5f02\uff0c\u4f46\u6574\u4f53\u53ef\u901a\u8fc7\u7b97\u6cd5\u8c03\u6574\u4e92\u76f8\u8865\u507f\u3002\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86\u4e0d\u540cLiDAR\u626b\u63cf\u6a21\u5f0f\u5bf9\u8def\u4fa7\u611f\u77e5\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7ed3\u8bba\u662f\u975e\u91cd\u590d\u626b\u63cfLiDAR\u5728\u68c0\u6d4b\u6027\u80fd\u4e0a\u4e0e128\u7ebf\u91cd\u590d\u626b\u63cfLiDAR\u76f8\u5f53\uff0c\u4e14\u5728\u6210\u672c\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u5c3d\u7ba1\u5176\u611f\u77e5\u8ddd\u79bb\u53d7\u9650\u3002"}}
{"id": "2511.00748", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00748", "abs": "https://arxiv.org/abs/2511.00748", "authors": ["Yi Yang", "Jian Pei", "Jun Yang", "Jichun Xie"], "title": "Finding Non-Redundant Simpson's Paradox from Multidimensional Data", "comment": "20 pages, 7 figures", "summary": "Simpson's paradox, a long-standing statistical phenomenon, describes the\nreversal of an observed association when data are disaggregated into\nsub-populations. It has critical implications across statistics, epidemiology,\neconomics, and causal inference. Existing methods for detecting Simpson's\nparadox overlook a key issue: many paradoxes are redundant, arising from\nequivalent selections of data subsets, identical partitioning of\nsub-populations, and correlated outcome variables, which obscure essential\npatterns and inflate computational cost. In this paper, we present the first\nframework for discovering non-redundant Simpson's paradoxes. We formalize three\ntypes of redundancy - sibling child, separator, and statistic equivalence - and\nshow that redundancy forms an equivalence relation. Leveraging this insight, we\npropose a concise representation framework for systematically organizing\nredundant paradoxes and design efficient algorithms that integrate depth-first\nmaterialization of the base table with redundancy-aware paradox discovery.\nExperiments on real-world datasets and synthetic benchmarks show that redundant\nparadoxes are widespread, on some real datasets constituting over 40% of all\nparadoxes, while our algorithms scale to millions of records, reduce run time\nby up to 60%, and discover paradoxes that are structurally robust under data\nperturbation. These results demonstrate that Simpson's paradoxes can be\nefficiently identified, concisely summarized, and meaningfully interpreted in\nlarge multidimensional datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8bc6\u522b\u5e76\u53bb\u91cd\u8f9b\u666e\u68ee\u6096\u8bba\u7684\u7406\u8bba\u4e0e\u7b97\u6cd5\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u5197\u4f59\u7c7b\u578b\u3001\u8bc1\u660e\u7b49\u4ef7\u5173\u7cfb\u5e76\u5b9e\u73b0\u5197\u4f59\u611f\u77e5\u7684\u9ad8\u6548\u53d1\u73b0\uff0c\u663e\u8457\u964d\u4f4e\u5197\u4f59\u6bd4\u4f8b\u4e0e\u8ba1\u7b97\u5f00\u9500\uff0c\u7ed3\u679c\u5728\u89c4\u6a21\u4e0e\u7a33\u5b9a\u6027\u4e0a\u5747\u6709\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u8f9b\u666e\u68ee\u6096\u8bba\u68c0\u6d4b\u65b9\u6cd5\u5e38\u4ea7\u751f\u5927\u91cf\u5197\u4f59\u6096\u8bba\uff08\u6765\u81ea\u7b49\u4ef7\u5b50\u96c6\u9009\u62e9\u3001\u76f8\u540c\u5b50\u7fa4\u5212\u5206\u6216\u76f8\u5173\u7ed3\u679c\u53d8\u91cf\uff09\uff0c\u8fd9\u4e0d\u4ec5\u63a9\u76d6\u4e86\u5173\u952e\u6a21\u5f0f\u4e5f\u6d6a\u8d39\u8ba1\u7b97\u8d44\u6e90\uff0c\u56e0\u800c\u9700\u8981\u4e00\u4e2a\u80fd\u53bb\u91cd\u5e76\u9ad8\u6548\u53d1\u73b0\u5b9e\u8d28\u6096\u8bba\u7684\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u5b9a\u4e49\u4e86\u4e09\u79cd\u5197\u4f59\u7c7b\u578b\uff08sibling child\u3001separator\u3001statistic equivalence\uff09\uff0c\u8bc1\u660e\u5197\u4f59\u6784\u6210\u7b49\u4ef7\u5173\u7cfb\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u5408\u6df1\u5ea6\u4f18\u5148\u57fa\u8868\u7269\u5316\u4e0e\u5197\u4f59\u611f\u77e5\u7684\u6096\u8bba\u53d1\u73b0\u7b97\u6cd5\uff0c\u7528\u4ee5\u7cfb\u7edf\u5316\u7ec4\u7ec7\u548c\u5254\u9664\u7b49\u4ef7\u6096\u8bba\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5408\u6210\u57fa\u51c6\u4e0a\uff0c\u4f5c\u8005\u53d1\u73b0\u5197\u4f59\u6096\u8bba\u666e\u904d\u5b58\u5728\uff08\u67d0\u4e9b\u6570\u636e\u96c6\u5360\u6bd4\u8d85\u8fc740%\uff09\uff0c\u6240\u63d0\u7b97\u6cd5\u53ef\u6269\u5c55\u81f3\u767e\u4e07\u7ea7\u8bb0\u5f55\uff0c\u8fd0\u884c\u65f6\u95f4\u6700\u591a\u51cf\u5c1160%\uff0c\u5e76\u80fd\u53d1\u73b0\u5bf9\u6570\u636e\u6270\u52a8\u7ed3\u6784\u4e0a\u7a33\u5065\u7684\u6096\u8bba\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u53d1\u73b0\u975e\u5197\u4f59\u8f9b\u666e\u68ee\u6096\u8bba\uff08Simpson's paradox\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u4e09\u7c7b\u5197\u4f59\u5e76\u5c06\u5176\u89c6\u4f5c\u7b49\u4ef7\u5173\u7cfb\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5197\u4f59\u6096\u8bba\u7684\u7d27\u51d1\u8868\u793a\u4e0e\u9ad8\u6548\u53d1\u73b0\u3002"}}
{"id": "2511.00062", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00062", "abs": "https://arxiv.org/abs/2511.00062", "authors": ["NVIDIA", ":", "Arslan Ali", "Junjie Bai", "Maciej Bala", "Yogesh Balaji", "Aaron Blakeman", "Tiffany Cai", "Jiaxin Cao", "Tianshi Cao", "Elizabeth Cha", "Yu-Wei Chao", "Prithvijit Chattopadhyay", "Mike Chen", "Yongxin Chen", "Yu Chen", "Shuai Cheng", "Yin Cui", "Jenna Diamond", "Yifan Ding", "Jiaojiao Fan", "Linxi Fan", "Liang Feng", "Francesco Ferroni", "Sanja Fidler", "Xiao Fu", "Ruiyuan Gao", "Yunhao Ge", "Jinwei Gu", "Aryaman Gupta", "Siddharth Gururani", "Imad El Hanafi", "Ali Hassani", "Zekun Hao", "Jacob Huffman", "Joel Jang", "Pooya Jannaty", "Jan Kautz", "Grace Lam", "Xuan Li", "Zhaoshuo Li", "Maosheng Liao", "Chen-Hsuan Lin", "Tsung-Yi Lin", "Yen-Chen Lin", "Huan Ling", "Ming-Yu Liu", "Xian Liu", "Yifan Lu", "Alice Luo", "Qianli Ma", "Hanzi Mao", "Kaichun Mo", "Seungjun Nah", "Yashraj Narang", "Abhijeet Panaskar", "Lindsey Pavao", "Trung Pham", "Morteza Ramezanali", "Fitsum Reda", "Scott Reed", "Xuanchi Ren", "Haonan Shao", "Yue Shen", "Stella Shi", "Shuran Song", "Bartosz Stefaniak", "Shangkun Sun", "Shitao Tang", "Sameena Tasmeen", "Lyne Tchapmi", "Wei-Cheng Tseng", "Jibin Varghese", "Andrew Z. Wang", "Hao Wang", "Haoxiang Wang", "Heng Wang", "Ting-Chun Wang", "Fangyin Wei", "Jiashu Xu", "Dinghao Yang", "Xiaodong Yang", "Haotian Ye", "Seonghyeon Ye", "Xiaohui Zeng", "Jing Zhang", "Qinsheng Zhang", "Kaiwen Zheng", "Andrew Zhu", "Yuke Zhu"], "title": "World Simulation with Video Foundation Models for Physical AI", "comment": null, "summary": "We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World\nFoundation Models for Physical AI. Built on a flow-based architecture,\n[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation\nin a single model and leverages [Cosmos-Reason1], a Physical AI vision-language\nmodel, to provide richer text grounding and finer control of world simulation.\nTrained on 200M curated video clips and refined with reinforcement\nlearning-based post-training, [Cosmos-Predict2.5] achieves substantial\nimprovements over [Cosmos-Predict1] in video quality and instruction alignment,\nwith models released at 2B and 14B scales. These capabilities enable more\nreliable synthetic data generation, policy evaluation, and closed-loop\nsimulation for robotics and autonomous systems. We further extend the family\nwith [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and\nReal2Real world translation. Despite being 3.5$\\times$ smaller than\n[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video\ngeneration. Together, these advances establish [Cosmos-Predict2.5] and\n[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To\naccelerate research and deployment in Physical AI, we release source code,\npretrained checkpoints, and curated benchmarks under the NVIDIA Open Model\nLicense at https://github.com/nvidia-cosmos/cosmos-predict2.5 and\nhttps://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open\nresources lower the barrier to adoption and foster innovation in building the\nnext generation of embodied intelligence.", "AI": {"tldr": "Cosmos-Predict2.5\uff1a\u57fa\u4e8eflow\u7684\u591a\u6a21\u6001\u4e16\u754c\u751f\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u7269\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0eRL\u540e\u8bad\u7ec3\uff0c\u63d0\u5347\u89c6\u9891\u5408\u6210\u8d28\u91cf\u548c\u6307\u4ee4\u5bf9\u9f50\uff1bCosmos-Transfer2.5\uff1a\u66f4\u5c0f\u66f4\u5f3a\u7684Sim2Real/Real2Real\u7ffb\u8bd1\u6846\u67b6\uff1b\u4e24\u8005\u52a9\u529b\u5b9e\u4f53\u667a\u80fd\u7814\u7a76\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "\u964d\u4f4e\u6784\u5efa\u5927\u89c4\u6a21\u3001\u53ef\u63a7\u7269\u7406\u4e16\u754c\u6a21\u62df\u4e0e\u5408\u6210\u6570\u636e\u7684\u95e8\u69db\uff0c\u63a8\u52a8\u673a\u5668\u4eba\u548c\u81ea\u52a8\u7cfb\u7edf\u7684\u95ed\u73af\u4eff\u771f\u3001\u7b56\u7565\u8bc4\u4f30\u4e0e\u5b9e\u4f53\u667a\u80fd\u6269\u5c55\u3002", "method": "\u57fa\u4e8eflow\u7684\u5355\u4e00\u67b6\u6784\u7edf\u4e00Text2World\u3001Image2World\u548cVideo2World\uff0c\u7ed3\u5408[Cosmos-Reason1]\u8fdb\u884c\u66f4\u5f3a\u7684\u6587\u672c\u4e0e\u89c6\u89c9\u5bf9\u9f50\uff1b\u5728200M\u89c6\u9891\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u7cbe\u5316\uff1b\u5e76\u901a\u8fc7\u63a7\u5236\u7f51\u98ce\u683c\u7684[Cosmos-Transfer2.5]\u5b9e\u73b0\u57df\u7ffb\u8bd1\u3002", "result": "\u57282B\u548c14B\u5c3a\u5ea6\u4e0a\u76f8\u8f83\u4e8e[Cosmos-Predict1]\u5728\u89c6\u9891\u8d28\u91cf\u4e0e\u6307\u4ee4\u5bf9\u9f50\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff1b[Cosmos-Transfer2.5]\u5728\u4f53\u91cf\u66f4\u5c0f\uff083.5\u00d7\uff09\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u7684\u957f\u65f6\u89c6\u9891\u751f\u6210\uff1b\u5e76\u5f00\u6e90\u4ee3\u7801\u4e0e\u6a21\u578b\u3002", "conclusion": "[Cosmos-Predict2.5]\u663e\u8457\u63d0\u5347\u4e86\u7269\u7406AI\u4e2d\u4e16\u754c\u751f\u6210\u548c\u4eff\u771f\u7684\u8d28\u91cf\u4e0e\u6307\u4ee4\u5bf9\u9f50\u80fd\u529b\uff0c\u5e76\u901a\u8fc7[Cosmos-Transfer2.5]\u6539\u8fdb\u4e86Sim2Real/Real2Real\u7684\u957f\u89c6\u91ce\u89c6\u9891\u7ffb\u8bd1\u3002"}}
{"id": "2511.00772", "categories": ["cs.DB", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2511.00772", "abs": "https://arxiv.org/abs/2511.00772", "authors": ["Raymond M. Xiong", "Panyu Chen", "Tianze Dong", "Jian Lu", "Benjamin Goldstein", "Danyang Zhuo", "Anru R. Zhang"], "title": "Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints", "comment": null, "summary": "Electronic health records (EHRs) are central to modern healthcare delivery\nand research; yet, many researchers lack the database expertise necessary to\nwrite complex SQL queries or generate effective visualizations, limiting\nefficient data use and scientific discovery. To address this barrier, we\nintroduce CELEC, a large language model (LLM)-powered framework for automated\nEHR data extraction and analytics. CELEC translates natural language queries\ninto SQL using a prompting strategy that integrates schema information,\nfew-shot demonstrations, and chain-of-thought reasoning, which together improve\naccuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves\nexecution accuracy comparable to prior systems while maintaining low latency,\ncost efficiency, and strict privacy by exposing only database metadata to the\nLLM. CELEC also adheres to strict privacy protocols: the LLM accesses only\ndatabase metadata (e.g., table and column names), while all query execution\noccurs securely within the institutional environment, ensuring that no\npatient-level data is ever transmitted to or shared with the LLM. Ablation\nstudies confirm that each component of the SQL generation pipeline,\nparticularly the few-shot demonstrations, plays a critical role in performance.\nBy lowering technical barriers and enabling medical researchers to query EHR\ndatabases directly, CELEC streamlines research workflows and accelerates\nbiomedical discovery.", "AI": {"tldr": "CELEC \u7528 LLM \u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u4e3a SQL\uff0c\u7ed3\u5408\u5143\u6570\u636e\u3001\u5c11\u6837\u672c\u4e0e\u94fe\u5f0f\u601d\u7ef4\uff0c\u65e2\u4fdd\u9690\u79c1\u53c8\u63d0\u9ad8 EHR \u67e5\u8be2\u53ef\u7528\u6027\uff0c\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "motivation": "\u8bb8\u591a\u7814\u7a76\u8005\u7f3a\u4e4f\u7f16\u5199\u590d\u6742 SQL \u548c\u751f\u6210\u6709\u6548\u53ef\u89c6\u5316\u7684\u6570\u636e\u5e93\u6280\u80fd\uff0c\u9650\u5236\u4e86 EHR \u6570\u636e\u7684\u9ad8\u6548\u5229\u7528\u4e0e\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u901a\u8fc7\u878d\u5408\u67b6\u6784\u4fe1\u606f\u3001\u5c11\u6837\u672c\u793a\u4f8b\u4e0e\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\u9a71\u52a8 LLM \u751f\u6210 SQL\uff0c\u5e76\u5728\u673a\u6784\u5185\u6267\u884c\u4ee5\u4fdd\u969c\u9690\u79c1\u3002", "result": "\u5728 EHRSQL \u57fa\u51c6\u5b50\u96c6\u4e0a\uff0cCELEC \u8fbe\u5230\u4e0e\u65e2\u6709\u7cfb\u7edf\u53ef\u6bd4\u7684\u6267\u884c\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u3001\u6210\u672c\u6548\u76ca\u4e0e\u4e25\u683c\u9690\u79c1\uff08\u4ec5\u66b4\u9732\u5143\u6570\u636e\u7ed9 LLM\uff09\u3002\u6d88\u878d\u7814\u7a76\u663e\u793a\u5c11\u6837\u672c\u793a\u4f8b\u5c24\u4e3a\u5173\u952e\u3002", "conclusion": "CELEC \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u4e3a SQL \u5e76\u5728\u673a\u6784\u5185\u90e8\u5b89\u5168\u6267\u884c\uff0c\u4ece\u800c\u964d\u4f4e\u7814\u7a76\u8005\u8bbf\u95ee EHR \u6570\u636e\u5e93\u7684\u95e8\u69db\u3002"}}
{"id": "2511.00073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00073", "abs": "https://arxiv.org/abs/2511.00073", "authors": ["Harald Kristen", "Daniel Kulmer", "Manuela Hirschmugl"], "title": "Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures", "comment": null, "summary": "Rapid climate change and other disturbances in alpine ecosystems demand\nfrequent habitat monitoring, yet manual mapping remains prohibitively expensive\nfor the required temporal resolution. We employ deep learning for change\ndetection using long-term alpine habitat data from Gesaeuse National Park,\nAustria, addressing a major gap in applying geospatial foundation models (GFMs)\nto complex natural environments with fuzzy class boundaries and highly\nimbalanced classes. We compare two paradigms: post-classification change\ndetection (CD) versus direct CD. For post-classification CD, we evaluate GFMs\nPrithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the\ntransformer ChangeViT against U-Net baselines. Using high-resolution multimodal\ndata (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes\nover 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus\nU-Net's 41% for multi-class habitat change, while both reach 67% for binary\nchange detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but\nonly 28% accuracy for multi-class detection. Cross-temporal evaluation reveals\nGFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's\n23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.\nAlthough overall accuracies are lower than in more homogeneous landscapes, they\nreflect realistic performance for complex alpine habitats. Future work will\nintegrate object-based post-processing and physical constraints to enhance\napplicability.", "AI": {"tldr": "\u5728\u9ad8\u5c71\u590d\u6742\u751f\u5883\u4e2d\uff0cGFM\uff08\u5c24\u5176Clay v1.0\uff09\u5bf9\u591a\u7c7b\u53d8\u5316\u68c0\u6d4b\u8868\u73b0\u66f4\u597d\uff0c\u76f4\u63a5CD\u5728\u4e8c\u5143\u4efb\u52a1\u4e0aIoU\u66f4\u9ad8\uff0cLiDAR\u878d\u5408\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u4f46\u603b\u4f53\u7cbe\u5ea6\u4ecd\u53d7\u590d\u6742\u6027\u9650\u5236\u3002", "motivation": "\u9ad8\u5c71\u751f\u6001\u7cfb\u7edf\u5feb\u901f\u53d8\u5316\u9700\u8981\u9ad8\u9891\u6b21\u76d1\u6d4b\uff0c\u4eba\u5de5\u5236\u56fe\u6210\u672c\u9ad8\uff0c\u4e14\u590d\u6742\u751f\u5883\u5b58\u5728\u6a21\u7cca\u7c7b\u522b\u8fb9\u754c\u548c\u7c7b\u522b\u4e25\u91cd\u4e0d\u5747\u8861\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u80dc\u4efb\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u8303\u5f0f\uff1a\u5148\u5206\u7c7b\u540e\u53d8\u5316\u68c0\u6d4b\uff08\u4f7f\u7528Prithvi-EO-2.0\u3001Clay v1.0\u548cU-Net\uff09\u4e0e\u76f4\u63a5\u53d8\u5316\u68c0\u6d4b\uff08ChangeViT\u4e0eU-Net\u57fa\u7ebf\uff09\u3002\u4f7f\u7528\u9ad8\u5206\u8fa8\u7387\u591a\u6a21\u6001\u6570\u636e\uff08RGB\u3001NIR\u3001LiDAR\u3001\u5730\u5f62\u5c5e\u6027\uff09\u548c4,480\u4e2a\u53d8\u5316\u6837\u672c\u8fdb\u884c\u8bc4\u4f30\uff0c\u540c\u65f6\u8fdb\u884c\u8de8\u65f6\u5e8f\u6d4b\u8bd5\u4e0eLiDAR\u878d\u5408\u5b9e\u9a8c\u3002", "result": "Clay v1.0\u5728\u591a\u7c7b\u751f\u5883\u53d8\u5316\u68c0\u6d4b\u4e0a\u8fbe51%\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4f18\u4e8eU-Net\u768441%\uff1b\u5728\u4e8c\u5143\u53d8\u5316\u68c0\u6d4b\u4e0a\u4e24\u8005\u5747\u8fbe67%\u3002\u76f4\u63a5\u53d8\u5316\u68c0\u6d4b\u5728\u4e8c\u5143IoU\u4e0a\u4f18\u4e8e\u540e\u5206\u7c7b\uff080.53 vs 0.35\uff09\uff0c\u4f46\u591a\u7c7b\u51c6\u786e\u7387\u4ec528%\u3002\u8de8\u65f6\u5e8f\u8bc4\u4f30\u663e\u793aClay\u5bf9\u672a\u89c1\u5e74\u4efd\u7a33\u5065\uff082020\u5e7433% vs U-Net 23%\uff09\u3002\u878d\u5408LiDAR\u5c06\u8bed\u4e49\u5206\u5272\u51c6\u786e\u7387\u4ece30%\u63d0\u5347\u523050%\u3002", "conclusion": "\u672c\u6587\u8868\u660e\u5728\u590d\u6742\u7684\u9ad8\u5c71\u751f\u5883\u4e2d\uff0c\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u6a21\u578b\uff08GFM\uff09\u5728\u591a\u7c7b\u53d8\u5316\u68c0\u6d4b\u4e0a\u4f18\u4e8e\u4f20\u7edfU-Net\uff0c\u4f46\u603b\u4f53\u7cbe\u5ea6\u4ecd\u4f4e\u4e8e\u5747\u8d28\u666f\u89c2\uff0c\u53cd\u6620\u4efb\u52a1\u96be\u5ea6\u3002"}}
{"id": "2511.00826", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00826", "abs": "https://arxiv.org/abs/2511.00826", "authors": ["Shatha Algarni", "Boris Glavic", "Seokki Lee", "Adriane Chapman"], "title": "Efficient Query Repair for Aggregate Constraints", "comment": "19 pages, 63 figures", "summary": "In many real-world scenarios, query results must satisfy domain-specific\nconstraints. For instance, a minimum percentage of interview candidates\nselected based on their qualifications should be female. These requirements can\nbe expressed as constraints over an arithmetic combination of aggregates\nevaluated on the result of the query. In this work, we study how to repair a\nquery to fulfill such constraints by modifying the filter predicates of the\nquery. We introduce a novel query repair technique that leverages bounds on\nsets of candidate solutions and interval arithmetic to efficiently prune the\nsearch space. We demonstrate experimentally, that our technique significantly\noutperforms baselines that consider a single candidate at a time.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5019\u9009\u96c6\u5408\u8fb9\u754c\u548c\u533a\u95f4\u7b97\u672f\u7684\u67e5\u8be2\u4fee\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u526a\u679d\u964d\u4f4e\u641c\u7d22\u6210\u672c\uff0c\u5b9e\u9a8c\u4f18\u4e8e\u9010\u5019\u9009\u57fa\u7ebf\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u67e5\u8be2\u7ed3\u679c\u9700\u6ee1\u8db3\u7279\u5b9a\u9886\u57df\u7ea6\u675f\uff08\u4f8b\u5982\u6027\u522b\u6bd4\u4f8b\uff09\uff0c\u8fd9\u4e9b\u7ea6\u675f\u53ef\u8868\u793a\u4e3a\u5bf9\u67e5\u8be2\u7ed3\u679c\u4e0a\u82e5\u5e72\u805a\u5408\u7b97\u672f\u7ec4\u5408\u7684\u7ea6\u675f\u3002\u5df2\u6709\u65b9\u6cd5\u9010\u4e00\u68c0\u67e5\u5019\u9009\u4fee\u6539\u6548\u7387\u4f4e\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4fee\u590d\u7b56\u7565\u3002", "method": "\u57fa\u4e8e\u5019\u9009\u89e3\u96c6\u5408\u7684\u8fb9\u754c\u4f30\u8ba1\u548c\u533a\u95f4\u7b97\u672f\u8fdb\u884c\u526a\u679d\uff1a\u8bba\u6587\u6784\u9020\u5019\u9009\u8c13\u8bcd\u7684\u96c6\u5408\uff0c\u5bf9\u8fd9\u4e9b\u96c6\u5408\u8ba1\u7b97\u805a\u5408\u8868\u8fbe\u5f0f\u7684\u4e0a\u4e0b\u754c\uff0c\u4ece\u800c\u5feb\u901f\u6392\u9664\u4e0d\u53ef\u80fd\u6ee1\u8db3\u7ea6\u675f\u7684\u96c6\u5408\uff0c\u51cf\u5c11\u9700\u8981\u9010\u4e00\u68c0\u67e5\u7684\u5019\u9009\u6570\u76ee\u3002\u5b9e\u73b0\u65f6\u4f7f\u7528\u641c\u7d22\u7b56\u7565\u904d\u5386\u53ef\u80fd\u7684\u8c13\u8bcd\u4fee\u6539\u5e76\u7ed3\u5408\u533a\u95f4\u8fd0\u7b97\u8fdb\u884c\u65e9\u671f\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u79cd\u57fa\u51c6\u4e0a\u7684\u8fd0\u884c\u65f6\u95f4\u548c\u9700\u8981\u68c0\u67e5\u7684\u5019\u9009\u6570\u5747\u663e\u8457\u4f18\u4e8e\u9010\u4e2a\u5019\u9009\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u660e\u8fb9\u754c+\u533a\u95f4\u7b97\u672f\u526a\u679d\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u67e5\u8be2\u4fee\u590d\u6548\u7387\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4fee\u6539\u67e5\u8be2\u7684\u7b5b\u9009\u8c13\u8bcd\u6765\u4fee\u590d\u4e0d\u6ee1\u8db3\u9886\u57df\u7ea6\u675f\u7684\u67e5\u8be2\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5019\u9009\u89e3\u96c6\u5408\u7684\u754c\u9650\u548c\u533a\u95f4\u7b97\u672f\u9ad8\u6548\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff0c\u663e\u8457\u4f18\u4e8e\u9010\u4e2a\u5019\u9009\u8003\u8651\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.00090", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00090", "abs": "https://arxiv.org/abs/2511.00090", "authors": ["Huanlin Gao", "Ping Chen", "Fuyuan Shi", "Chao Tan", "Zhaoxiang Liu", "Fang Zhao", "Kai Wang", "Shiguo Lian"], "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation", "comment": "NeurIPS 2025", "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa", "AI": {"tldr": "LeMiCa\u901a\u8fc7\u56fe\u4f18\u5316\u7684\u7f13\u5b58\u8c03\u5ea6\u5728\u4e0d\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u5168\u5c40\u8bef\u5dee\u7d2f\u79ef\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u4e00\u81f4\u6027\u4e0e\u901f\u5ea6\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u7f13\u5b58\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u7f13\u5b58\u7b56\u7565\u591a\u5173\u6ce8\u5c40\u90e8\u542f\u53d1\u5f0f\u8bef\u5dee\uff0c\u5ffd\u89c6\u5168\u5c40\u8bef\u5dee\u7d2f\u79ef\u5bfc\u81f4\u52a0\u901f\u540e\u89c6\u9891\u4e0e\u539f\u89c6\u9891\u5185\u5bb9\u98ce\u683c\u4e0d\u4e00\u81f4\u3002", "method": "\u5c06\u7f13\u5b58\u8c03\u5ea6\u5efa\u6a21\u4e3a\u5e26\u8bef\u5dee\u6743\u91cd\u8fb9\u7684\u6709\u5411\u56fe\uff0c\u5e76\u63d0\u51fa\u8bcd\u5178\u5f0f\uff08Lexicographic\uff09\u6781\u5c0f\u5316\u6700\u5c0f\u5316\u6700\u5927\u8def\u5f84\u8bef\u5dee\uff08Minimax Path Optimization\uff09\u7b56\u7565\uff0c\u5bf9\u6700\u5dee\u8def\u5f84\u8bef\u5dee\u8fdb\u884c\u663e\u5f0f\u7ea6\u675f\u4ee5\u51cf\u5c11\u5168\u5c40\u8bef\u5dee\u7d2f\u79ef\u3002", "result": "\u5728\u591a\u9879\u6587\u672c\u5230\u89c6\u9891\u57fa\u51c6\u4e0a\u53cc\u5411\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\uff1a\u5982\u5728Latte\u6a21\u578b\u4e0a\u5b9e\u73b02.9x\u52a0\u901f\uff0c\u5728Open-Sora\u4e0aLPIPS\u8fbe\u52300.05\uff0c\u4e14\u4e3b\u89c2\u611f\u77e5\u8d28\u91cf\u4ec5\u6709\u6781\u5c0f\u964d\u635f\u3002", "conclusion": "LeMiCa\u80fd\u5728\u4e0d\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u56fe\u8bba\u4f18\u5316\u7f13\u5b58\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u89c6\u9891\u751f\u6210\u7684\u4e00\u81f4\u6027\u4e0e\u6548\u7387\uff0c\u517c\u987e\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002"}}
{"id": "2511.00855", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00855", "abs": "https://arxiv.org/abs/2511.00855", "authors": ["Zhonggen Li", "Yougen Li", "Yifan Zhu", "Zhaoqiang Chen", "Yunjun Gao"], "title": "All-in-one Graph-based Indexing for Hybrid Search on GPUs", "comment": null, "summary": "Hybrid search has emerged as a promising paradigm to overcome the limitations\nof single-path retrieval, enhancing accuracy for applications like\nrecommendations, information retrieval, and Retrieval-Augmented Generation.\nHowever, existing methods are constrained by a trilemma: they sacrifice\nflexibility for efficiency, suffer from accuracy degradation due to separate\nretrievals, or incur prohibitive storage overhead for flexible combinations of\nretrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph\nindex accelerated by GPUs for efficient hybrid search. We first analyze the\nlimitations of existing retrieval paradigms and distill key design principles\nfor an effective hybrid search index. Guided by these principles, we architect\na unified graph-based index that flexibly integrates four retrieval paths-dense\nvector, sparse vector, full-text, and knowledge graph-within a single, cohesive\nstructure. To enable efficient construction, we design a GPU-accelerated\npipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,\nand keyword-aware neighbor recycling. For query processing, we introduce a\ndynamic fusion framework that supports any combination of retrieval paths and\nweights without index reconstruction, leveraging logical edges from the\nknowledge graph to resolve complex multi-hop queries. Extensive experiments on\n6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end\nquery accuracy and outperforms state-of-the-art methods by 1.5-186.4x in\nthroughput, while significantly reducing storage overhead.", "AI": {"tldr": "Allan-Poe\u901a\u8fc7GPU\u52a0\u901f\u7684\u7edf\u4e00\u56fe\u7d22\u5f15\u4e0e\u52a8\u6001\u878d\u5408\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u6df7\u5408\u68c0\u7d22\u7684\u4e09\u96be\u56f0\u5883\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u3001\u4f4e\u5b58\u50a8\u7684\u591a\u8def\u5f84\u68c0\u7d22\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\u5728\u6548\u7387\u3001\u7075\u6d3b\u6027\u548c\u5b58\u50a8\u5f00\u9500\u4e4b\u95f4\u5b58\u5728\u4e09\u96be\u56f0\u5883\uff1a\u90e8\u5206\u65b9\u6cd5\u4e3a\u6548\u7387\u727a\u7272\u7075\u6d3b\u6027\uff1b\u5355\u72ec\u68c0\u7d22\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff1b\u7075\u6d3b\u7ec4\u5408\u8def\u5f84\u5219\u9700\u8981\u9ad8\u5b58\u50a8\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u56fe\u7d22\u5f15\uff0c\u5c06\u7a20\u5bc6\u5411\u91cf\u3001\u7a00\u758f\u5411\u91cf\u3001\u5168\u6587\u672c\u548c\u77e5\u8bc6\u56fe\u8c31\u56db\u79cd\u68c0\u7d22\u8def\u5f84\u6574\u5408\u8fdb\u5355\u4e00\u7ed3\u6784\uff1b\u8bbe\u8ba1GPU\u52a0\u901f\u6784\u5efa\u6d41\u6c34\u7ebf\uff08warp\u7ea7\u6df7\u5408\u8ddd\u79bb\u6838\u3001RNG-IP\u8054\u5408\u526a\u679d\u3001\u5173\u952e\u8bcd\u611f\u77e5\u90bb\u5c45\u56de\u6536\uff09\uff1b\u5f15\u5165\u52a8\u6001\u878d\u5408\u6846\u67b6\u652f\u6301\u4efb\u610f\u8def\u5f84\u7ec4\u5408\u4e0e\u6743\u91cd\u5e76\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u903b\u8f91\u8fb9\u5904\u7406\u591a\u8df3\u67e5\u8be2\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAllan-Poe\u5728\u7aef\u5230\u7aef\u67e5\u8be2\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u541e\u5410\u91cf\u8f83\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u53471.5-186.4\u500d\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u5f00\u9500\u3002", "conclusion": "Allan-Poe\u5728\u4fdd\u6301\u9ad8\u541e\u5410\u548c\u4f4e\u5b58\u50a8\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u4e14\u7cbe\u786e\u7684\u6df7\u5408\u68c0\u7d22\uff0c\u9002\u7528\u4e8e\u591a\u8def\u5f84\u68c0\u7d22\u573a\u666f\u3002"}}
{"id": "2511.00091", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.00091", "abs": "https://arxiv.org/abs/2511.00091", "authors": ["Wenli Xiao", "Haotian Lin", "Andy Peng", "Haoru Xue", "Tairan He", "Yuqi Xie", "Fengyuan Hu", "Jimmy Wu", "Zhengyi Luo", "Linxi \"Jim\" Fan", "Guanya Shi", "Yuke Zhu"], "title": "Self-Improving Vision-Language-Action Models with Data Generation via Residual RL", "comment": "26 pages", "summary": "Supervised fine-tuning (SFT) has become the de facto post-training strategy\nfor large vision-language-action (VLA) models, but its reliance on costly human\ndemonstrations limits scalability and generalization. We propose Probe, Learn,\nDistill (PLD), a three-stage plug-and-play framework that improves VLAs through\nresidual reinforcement learning (RL) and distribution-aware data collection. In\nStage 1, we train lightweight residual actors to probe failure regions of the\nVLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns\ncollected trajectories with the generalist's deployment distribution while\ncapturing recovery behaviors. In Stage 3, we distill the curated trajectories\nback into the generalist with standard SFT. PLD achieves near-saturated 99%\ntask success on LIBERO, over 50% gains in SimplerEnv, and 100% success on\nreal-world Franka and YAM arm manipulation tasks. Ablations show that residual\nprobing and distribution-aware replay are key to collecting deployment-aligned\ndata that improves both seen and unseen tasks, offering a scalable path toward\nself-improving VLA models.", "AI": {"tldr": "PLD\u63d0\u51fa\u6b8b\u5deeRL\u4e0e\u90e8\u7f72\u611f\u77e5\u7684\u6570\u636e\u6536\u96c6\u6d41\u7a0b\uff0c\u81ea\u52a8\u53d1\u73b0\u5e76\u6539\u6b63VLA\u901a\u7528\u6a21\u578b\u5931\u8d25\uff0c\u663e\u8457\u63d0\u5347\u6a21\u62df\u4e0e\u771f\u5b9e\u673a\u68b0\u81c2\u4efb\u52a1\u6210\u529f\u7387\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u8def\u5f84\u3002", "motivation": "\u73b0\u6709SFT\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u7c7b\u793a\u8303\uff0c\u9650\u5236\u4e86\u89c4\u6a21\u5316\u548c\u6cdb\u5316\u80fd\u529b\uff1b\u76ee\u6807\u662f\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u6536\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347VLA\u6a21\u578b\u6027\u80fd\u5e76\u6269\u5c55\u5176\u80fd\u529b\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bad\u7ec3\u8f7b\u91cf\u6b8b\u5deeactor\u53bb\u63a2\u6d4b\u901a\u7528VLA\u7684\u5931\u8d25\u533a\u57df\uff1b2) \u91c7\u7528\u6df7\u5408rollout\u65b9\u6848\uff08\u7ed3\u5408\u901a\u7528\u6a21\u578b\u90e8\u7f72\u5206\u5e03\u4e0e\u6062\u590d\u884c\u4e3a\uff09\u6536\u96c6\u8f68\u8ff9\uff1b3) \u5c06\u7b5b\u9009\u540e\u7684\u8f68\u8ff9\u901a\u8fc7\u6807\u51c6SFT\u84b8\u998f\u56de\u901a\u7528\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u548c\u771f\u5b9e\u8bbe\u5907\u4e0a\u83b7\u5f97\u5927\u5e45\u63d0\u5347\uff1aLIBERO\u8fd1\u9971\u548c99%\u4efb\u52a1\u6210\u529f\u7387\uff0cSimplerEnv\u63d0\u534750%\u4ee5\u4e0a\uff0c\u4ee5\u53caFranka\u548cYAM\u673a\u68b0\u81c2\u4e0a\u7684100%\u6210\u529f\u7387\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6b8b\u5dee\u63a2\u6d4b\u4e0e\u5206\u5e03\u611f\u77e5\u91cd\u653e\u662f\u5173\u952e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86PLD\uff08Probe, Learn, Distill\uff09\uff0c\u901a\u8fc7\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u4e0e\u5206\u5e03\u611f\u77e5\u7684\u6570\u636e\u6536\u96c6\uff0c\u5728SFT\u6d41\u7a0b\u5916\u5b9e\u73b0VLA\u6a21\u578b\u81ea\u6211\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5c24\u5176\u5728\u771f\u5b9e\u673a\u68b0\u81c2\u4efb\u52a1\u4e0a\u8fbe\u5230100%\u6210\u529f\u3002"}}
{"id": "2511.00865", "categories": ["cs.DB", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.00865", "abs": "https://arxiv.org/abs/2511.00865", "authors": ["Hangdong Zhao", "Zhenghong Yu", "Srinag Rao", "Simon Frisk", "Zhiwei Fan", "Paraschos Koutris"], "title": "FlowLog: Efficient and Extensible Datalog via Incrementality", "comment": "Accepted to VLDB 2026", "summary": "Datalog-based languages are regaining popularity as a powerful abstraction\nfor expressing recursive computations in domains such as program analysis and\ngraph processing. However, existing systems often face a trade-off between\nefficiency and extensibility. Engines like Souffle achieve high efficiency\nthrough domain-specific designs, but lack general-purpose flexibility. Others,\nlike RecStep, offer modularity by layering Datalog on traditional databases,\nbut struggle to integrate Datalog-specific optimizations.\n  This paper bridges this gap by presenting FlowLog, a new Datalog engine that\nuses an explicit relational IR per-rule to cleanly separate recursive control\n(e.g., semi-naive execution) from each rule's logical plan. This boundary lets\nus retain fine-grained, Datalog-aware optimizations at the logical layer, but\nalso reuse off-the-shelf database primitives at execution. At the logical level\n(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan\nreuse. To address high volatility in recursive workloads, we adopt a\nrobustness-first approach that pairs a structural optimizer (avoiding\nworst-case joins) with sideways information passing (early filtering). Built\natop Differential Dataflow--a mature framework for streaming analytics--FlowLog\nsupports both batch and incremental Datalog and adds novel recursion-aware\noptimizations called Boolean (or algebraic) specialization. Our evaluation\nshows that FlowLog outperforms state-of-the-art Datalog engines and modern\ndatabases across a broad range of recursive workloads, achieving superior\nscalability while preserving a simple and extensible architecture.", "AI": {"tldr": "\u63d0\u51fa FlowLog\uff1a\u57fa\u4e8e\u6bcf\u6761\u89c4\u5219\u7684\u5173\u7cfb IR\uff0c\u5c06\u9012\u5f52\u63a7\u5236\u4e0e\u903b\u8f91\u8ba1\u5212\u5206\u79bb\uff0c\u7ed3\u5408 SQL \u4f18\u5316\u4e0e\u9012\u5f52\u611f\u77e5\u4f18\u5316\uff08\u5e03\u5c14\u7279\u5316\uff09\uff0c\u6784\u5efa\u4e8e Differential Dataflow\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u9c81\u68d2\u7684 Datalog \u6267\u884c\uff0c\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709 Datalog \u7cfb\u7edf\u5728\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u4e13\u7528\u5f15\u64ce\uff08\u5982 Souffle\uff09\u9ad8\u6548\u4f46\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u57fa\u4e8e\u6570\u636e\u5e93\u7684\u89e3\u51b3\u65b9\u6848\uff08\u5982 RecStep\uff09\u6a21\u5757\u5316\u4f46\u96be\u4ee5\u6574\u5408 Datalog \u4e13\u6709\u4f18\u5316\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301 Datalog \u4f18\u5316\u80fd\u529b\u53c8\u80fd\u590d\u7528\u6570\u636e\u5e93\u6267\u884c\u57fa\u7840\u8bbe\u65bd\u7684\u8bbe\u8ba1\u3002", "method": "\u5728\u6bcf\u6761\u89c4\u5219\u4f7f\u7528\u663e\u5f0f\u5173\u7cfb IR\uff0c\u5c06\u9012\u5f52\u63a7\u5236\uff08\u5982\u534a\u6734\u7d20\u6267\u884c\uff09\u4e0e\u89c4\u5219\u7684\u903b\u8f91\u8ba1\u5212\u5206\u79bb\uff1b\u5728 IR \u5c42\u5e94\u7528 SQL \u7c7b\u4f18\u5316\uff08\u903b\u8f91\u878d\u5408\u3001\u5b50\u8ba1\u5212\u91cd\u7528\uff09\uff1b\u91c7\u7528\u7ed3\u6784\u6027\u4f18\u5316\u5668\uff08\u907f\u514d\u6700\u574f\u60c5\u51b5\u7684\u7b1b\u5361\u5c14/\u7b1b\u5408\u5e76\uff09\u548c\u4fa7\u5411\u4fe1\u606f\u4f20\u9012\uff08\u65e9\u671f\u8fc7\u6ee4\uff09\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff1b\u6784\u5efa\u4e8e Differential Dataflow \u4e4b\u4e0a\uff0c\u652f\u6301\u6279\u5904\u7406\u4e0e\u589e\u91cf\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u5e03\u5c14/\u4ee3\u6570\u7279\u5316\u7b49\u9012\u5f52\u611f\u77e5\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e FlowLog \u5728\u5404\u79cd\u9012\u5f52\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb Datalog \u5f15\u64ce\u548c\u73b0\u4ee3\u6570\u636e\u5e93\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7b80\u5355\u4e14\u53ef\u6269\u5c55\u7684\u67b6\u6784\u3002", "conclusion": "FlowLog \u5728\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u901a\u8fc7\u5728\u6bcf\u6761\u89c4\u5219\u7ea7\u522b\u5f15\u5165\u663e\u5f0f\u5173\u7cfb\u4e2d\u95f4\u8868\u793a\uff08IR\uff09\uff0c\u5c06\u9012\u5f52\u63a7\u5236\u4e0e\u903b\u8f91\u8ba1\u5212\u5206\u79bb\uff0c\u4ece\u800c\u65e2\u80fd\u4fdd\u7559 Datalog \u7279\u5b9a\u4f18\u5316\uff0c\u53c8\u80fd\u590d\u7528\u6570\u636e\u5e93\u6267\u884c\u539f\u8bed\uff0c\u6700\u7ec8\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.00095", "categories": ["cs.CV", "cs.AI", "92C55", "I.2.10"], "pdf": "https://arxiv.org/pdf/2511.00095", "abs": "https://arxiv.org/abs/2511.00095", "authors": ["Jiaming Liu", "Dingwei Fan", "Junyong Zhao", "Chunlin Li", "Haipeng Si", "Liang Sun"], "title": "SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation", "comment": "2 Tables,5 Figures,16 Equations", "summary": "The anatomical structure segmentation of the spine and adjacent structures\nfrom computed tomography (CT) images is a key step for spinal disease diagnosis\nand treatment. However, the segmentation of CT images is impeded by low\ncontrast and complex vertebral boundaries. Although advanced models such as the\nSegment Anything Model (SAM) have shown promise in various segmentation tasks,\ntheir performance in spinal CT imaging is limited by high annotation\nrequirements and poor domain adaptability. To address these limitations, we\npropose SpinalSAM-R1, a multimodal vision-language interactive system that\nintegrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.\nSpecifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism\nto improve spine segmentation performance, and a semantics-driven interaction\nprotocol powered by DeepSeek-R1, enabling natural language-guided refinement.\nThe SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient\nadaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with\nCT images. Experimental results suggest that our method achieves superior\nsegmentation performance. Meanwhile, we develop a PyQt5-based interactive\nsoftware, which supports point, box, and text-based prompts. The system\nsupports 11 clinical operations with 94.3\\% parsing accuracy and sub-800 ms\nresponse times. The software is released on\nhttps://github.com/6jm233333/spinalsam-r1.", "AI": {"tldr": "\u63d0\u51faSpinalSAM-R1\uff1a\u7ed3\u5408LoRA\u5fae\u8c03\u7684SAM\u4e0eDeepSeek-R1\uff0c\u52a0\u5165\u89e3\u5256\u5b66\u6ce8\u610f\u529b\u4e0e\u8bed\u4e49\u4ea4\u4e92\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u7684\u810a\u67f1CT\u5206\u5272\uff0c\u5e76\u63d0\u4f9b\u54cd\u5e94\u8fc5\u901f\u7684PyQt5\u4ea4\u4e92\u8f6f\u4ef6\u3002", "motivation": "\u810a\u67f1CT\u5206\u5272\u53d7\u4f4e\u5bf9\u6bd4\u5ea6\u4e0e\u590d\u6742\u8fb9\u754c\u5f71\u54cd\uff0c\u73b0\u6709\u901a\u7528\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u5728\u6ce8\u91ca\u9700\u6c42\u9ad8\u4e0e\u9886\u57df\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u76f4\u63a5\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u53ef\u4ea4\u4e92\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LoRA\u5bf9SAM\u8fdb\u884c\u4f4e\u79e9\u9002\u914d\u5fae\u8c03\uff0c\u52a0\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u589e\u5f3a\u810a\u67f1\u7ed3\u6784\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u901a\u8fc7DeepSeek-R1\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u7684\u7ec6\u5316\uff1b\u5f00\u53d1PyQt5\u4ea4\u4e92\u8f6f\u4ef6\u652f\u6301\u70b9/\u6846/\u6587\u672c\u63d0\u793a\u3002", "result": "\u5728\u810a\u67f1\u89e3\u5256\u7ed3\u6784CT\u5206\u5272\u9a8c\u8bc1\u4e2d\uff0c\u65b9\u6cd5\u663e\u793a\u51fa\u4f18\u8d8a\u7684\u5206\u5272\u6027\u80fd\uff1b\u8f6f\u4ef6\u5b9e\u73b011\u9879\u4e34\u5e8a\u64cd\u4f5c\uff0c\u89e3\u6790\u51c6\u786e\u738794.3%\uff0c\u54cd\u5e94\u65f6\u95f4\u4f4e\u4e8e800 ms\u3002", "conclusion": "SpinalSAM-R1\u901a\u8fc7\u5c06\u5fae\u8c03\u7684SAM\u4e0eDeepSeek-R1\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u89e3\u5256\u5b66\u5f15\u5bfc\u6ce8\u610f\u529b\u548c\u8bed\u4e49\u9a71\u52a8\u4ea4\u4e92\u534f\u8bae\uff0c\u80fd\u5728\u810a\u67f1CT\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u5206\u5272\u6027\u80fd\u4e0e\u4ea4\u4e92\u6548\u7387\u3002"}}
{"id": "2511.00985", "categories": ["cs.DB", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00985", "abs": "https://arxiv.org/abs/2511.00985", "authors": ["Yiwen Jiao", "Tonghui Ren", "Yuche Gao", "Zhenying He", "Yinan Jing", "Kai Zhang", "X. Sean Wang"], "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL", "comment": "16 pages, 4 figures, preprint", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries.", "AI": {"tldr": "ORANGE\u4ece\u7ffb\u8bd1\u65e5\u5fd7\u589e\u91cf\u6784\u5efa\u6570\u636e\u5e93\u77e5\u8bc6\u5e93\uff0c\u7ed3\u5408\u5d4c\u5957CoT\u4e0etuple\u8ffd\u8e2a\u964d\u4f4e\u8bed\u4e49\u9519\u8bef\uff0c\u6301\u7eed\u63d0\u5347Text-to-SQL\u6027\u80fd\u3002", "motivation": "LLM\u867d\u80fd\u5c06\u81ea\u7136\u8bed\u8a00\u7ffb\u6210SQL\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7279\u5b9a\u6570\u636e\u5e93\u6a21\u5f0f\u548c\u6570\u636e\u8bed\u4e49\u7684\u957f\u671f\u79ef\u7d2f\uff0c\u5386\u53f2\u7ffb\u8bd1\u65e5\u5fd7\u8574\u542b\u4e86\u8fd0\u884c\u65f6\u7684\u771f\u5b9e\u4f7f\u7528\u6a21\u5f0f\uff0c\u53ef\u7528\u4e8e\u5f25\u8865\u8fd9\u4e00\u9886\u57df\u77e5\u8bc6\u7f3a\u53e3\u3002", "method": "\u63d0\u51fa\u5728\u7ebf\u81ea\u6f14\u5316\u6846\u67b6ORANGE\uff1a\u89e3\u6790\u7ffb\u8bd1\u65e5\u5fd7\u4e2d\u7684SQL\u751f\u6210\u5305\u542bschema\u548c\u6570\u636e\u8bed\u4e49\u7684\u77e5\u8bc6\u9879\uff1b\u91c7\u7528\u5d4c\u5957Chain-of-Thought\u7684SQL-to-Text\u7b56\u7565\u5e76\u52a0\u5165tuple\u8bed\u4e49\u8ffd\u8e2a\u4ee5\u964d\u4f4e\u8bed\u4e49\u9519\u8bef\uff1b\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u66f4\u65b0\u77e5\u8bc6\u5e93\u5e76\u7528\u4e8e\u540e\u7eed\u7ffb\u8bd1\u63d0\u793a\u6216\u68c0\u7d22\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cORANGE\u5728\u5904\u7406\u590d\u6742\u4e0e\u9886\u57df\u7279\u5b9a\u67e5\u8be2\u65f6\u663e\u8457\u63d0\u5347Text-to-SQL\u7684\u51c6\u786e\u7387\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ORANGE\u901a\u8fc7\u4ece\u5386\u53f2\u7ffb\u8bd1\u65e5\u5fd7\u4e2d\u89e3\u6790SQL\u6784\u5efa\u6570\u636e\u5e93\u4e13\u5c5e\u77e5\u8bc6\u5e93\uff0c\u9010\u6b65\u5f25\u5408\u901a\u7528LLM\u4e0e\u6570\u636e\u5e93\u9886\u57df\u8bed\u4e49\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u5347Text-to-SQL\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.00098", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00098", "abs": "https://arxiv.org/abs/2511.00098", "authors": ["Nils Porsche", "Flurin M\u00fcller-Diesing", "Sweta Banerjee", "Miguel Goncalves", "Marc Aubreville"], "title": "A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning", "comment": null, "summary": "Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging\nmodality that can be used for in-situ, in-vivo imaging and the microstructural\nanalysis of mucous structures. The diagnosis using CLE is, however, complicated\nby images being hard to interpret for non-experienced physicians. Utilizing\nmachine learning as an augmentative tool would hence be beneficial, but is\ncomplicated by the shortage of histopathology-correlated CLE imaging sequences\nwith respect to the plurality of patterns in this domain, leading to\noverfitting of machine learning models. To overcome this, self-supervised\nlearning (SSL) can be employed on larger unlabeled datasets. CLE is a\nvideo-based modality with high inter-frame correlation, leading to a\nnon-stratified data distribution for SSL training. In this work, we propose a\nfilter functionality on CLE video sequences to reduce the dataset redundancy in\nSSL training and improve SSL training convergence and training efficiency. We\nuse four state-of-the-art baseline networks and a SSL teacher-student network\nwith a vision transformer small backbone for the evaluation. These networks\nwere evaluated on downstream tasks for a sinonasal tumor dataset and a squamous\ncell carcinoma of the skin dataset. On both datasets, we found the highest test\naccuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both\nconsiderably outperforming their non-SSL baselines. Our results show that SSL\nis an effective method for CLE pretraining. Further, we show that our proposed\nCLE video filter can be utilized to improve training efficiency in\nself-supervised scenarios, resulting in a reduction of 67% in training time.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5bf9CLE\u89c6\u9891\u5e8f\u5217\u53bb\u5197\u4f59\u7684\u8fc7\u6ee4\u7b56\u7565\uff0c\u7ed3\u5408SSL\uff08ViT-small\u5e08\u751f\u7f51\u7edc\uff09\uff0c\u5728\u5c11\u91cf\u6807\u6ce8\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u4e0b\u6e38\u75c5\u53d8\u5206\u7c7b\u51c6\u786e\u7387\u5e76\u5c06\u81ea\u76d1\u7763\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u7ea6\u4e09\u5206\u4e4b\u4e8c\u3002", "motivation": "CLE\u4e3a\u5b9e\u65f6\u5fae\u7ed3\u6784\u6210\u50cf\u5de5\u5177\uff0c\u4f46\u56fe\u50cf\u5bf9\u65e0\u7ecf\u9a8c\u533b\u751f\u96be\u4ee5\u89e3\u91ca\uff1b\u53d7\u9650\u4e8e\u5e26\u75c5\u7406\u6807\u7b7e\u7684CLE\u5e8f\u5217\u7a00\u7f3a\uff0c\u76d1\u7763\u5f0f\u6a21\u578b\u6613\u8fc7\u62df\u5408\u3002\u5229\u7528\u5927\u91cf\u672a\u6807\u6ce8\u6570\u636e\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5e76\u7ed3\u5408\u53bb\u5197\u4f59\u7684\u89c6\u9891\u8fc7\u6ee4\u80fd\u6539\u5584\u9884\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51fa\u5bf9CLE\u89c6\u9891\u5e8f\u5217\u7684\u8fc7\u6ee4\u529f\u80fd\u4ee5\u51cf\u5c11\u9ad8\u5e27\u95f4\u76f8\u5173\u6027\u5bfc\u81f4\u7684\u6570\u636e\u5197\u4f59\uff1b\u4f7f\u7528\u56db\u79cdSOTA\u57fa\u7ebf\u7f51\u7edc\u548c\u57fa\u4e8eViT-small\u7684SSL\u5e08\u751f\u7f51\u7edc\u8fdb\u884c\u9884\u8bad\u7ec3\uff1b\u5728\u4e24\u9879\u4e0b\u6e38\u4efb\u52a1\uff08\u9f3b\u7aa6\u80bf\u7624\u6570\u636e\u96c6\u4e0e\u76ae\u80a4\u9cde\u72b6\u7ec6\u80de\u764c\u6570\u636e\u96c6\uff09\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u8fc7\u6ee4\u540e\u7684SSL\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e24\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u5206\u522b\u8fbe\u523067.48%\u548c73.52%\uff0c\u5747\u660e\u663e\u4f18\u4e8e\u672a\u4f7f\u7528SSL\u7684\u57fa\u7ebf\uff1b\u8fc7\u6ee4\u673a\u5236\u4f7f\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u7ea667%\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728CLE\u5f71\u50cf\u9884\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u7684\u89c6\u9891\u5e8f\u5217\u8fc7\u6ee4\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u6570\u636e\u5197\u4f59\u3001\u63d0\u5347\u8bad\u7ec3\u6536\u655b\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.00995", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.00995", "abs": "https://arxiv.org/abs/2511.00995", "authors": ["Tianming Wu", "Dixin Tang"], "title": "PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search", "comment": null, "summary": "Filtered approximate nearest neighbor search (ANNS) restricts the search to\ndata objects whose attributes satisfy a given filter and retrieves the top-$K$\nobjects that are most semantically similar to the query object. Many\ngraph-based ANNS indexes are proposed to enable efficient filtered ANNS but\nremain limited in applicability or performance: indexes optimized for a\nspecific attribute achieve high efficiency for filters on that attribute but\nfail to support complex filters with arbitrary conjunctions and disjunctions\nover multiple attributes. Inspired by the design of relational databases, this\npaper presents PathFinder, a new indexing framework that allows users to\nselectively create ANNS indexes optimized for filters on specific attributes\nand employs a cost-based optimizer to efficiently utilize them for processing\ncomplex filters. PathFinder includes three novel techniques: 1) a new\noptimization metric that captures the tradeoff between query execution time and\naccuracy, 2) a two-phase optimization for handling filters with conjunctions\nand disjunctions, and 3) an index borrowing optimization that uses an\nattribute-specific index to process filters on another attribute. Experiments\non four real-world datasets show that PathFinder outperforms the best baseline\nby up to 9.8x in query throughput at recall 0.95.", "AI": {"tldr": "\u63d0\u51faPathFinder\uff1a\u53ef\u6309\u5c5e\u6027\u6784\u5efa\u56feANNS\u7d22\u5f15\u5e76\u7528\u4ee3\u4ef7\u4f18\u5316\u5668\u7ec4\u5408\u4ee5\u9ad8\u6548\u5904\u7406\u590d\u6742\u8fc7\u6ee4\uff0c\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u6700\u591a9.8x\uff0crecall=0.95\uff09\u3002", "motivation": "\u73b0\u6709\u56fe\u7d22\u5f15\u5728\u5e26\u8fc7\u6ee4\u6761\u4ef6\u7684ANNS\u4e0a\u8981\u4e48\u53ea\u5bf9\u5355\u4e00\u5c5e\u6027\u9ad8\u6548\u4f46\u4e0d\u652f\u6301\u590d\u6742\u591a\u5c5e\u6027\u8fc7\u6ee4\uff0c\u8981\u4e48\u4e0d\u80fd\u5728\u51c6\u786e\u7387\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff1b\u53d7\u5173\u7cfb\u578b\u6570\u636e\u5e93\u591a\u7d22\u5f15\u4e0e\u4f18\u5316\u5668\u601d\u60f3\u542f\u53d1\uff0c\u63d0\u51fa\u53ef\u9009\u62e9\u5efa\u7acb\u5c5e\u6027\u7d22\u5f15\u5e76\u7531\u4f18\u5316\u5668\u5229\u7528\u5b83\u4eec\u5904\u7406\u590d\u6742\u8fc7\u6ee4\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6846\u67b6\uff0c\u652f\u6301\u6309\u5c5e\u6027\u521b\u5efa\u4f18\u5316\u7684\u56fe\u7d22\u5f15\uff0c\u5e76\u5f15\u5165\u4e09\u4e2a\u6280\u672f\uff1a\u65b0\u7684\u4f18\u5316\u5ea6\u91cf\uff08\u5728\u67e5\u8be2\u65f6\u95f4\u4e0e\u51c6\u786e\u7387\u95f4\u6743\u8861\uff09\u3001\u7528\u4e8e\u5904\u7406\u4e0e/\u6216\u6df7\u5408\u8fc7\u6ee4\u7684\u4e24\u9636\u6bb5\u4f18\u5316\u6d41\u7a0b\u3001\u4ee5\u53ca\u7d22\u5f15\u501f\u7528\uff08\u7528\u67d0\u5c5e\u6027\u7d22\u5f15\u5904\u7406\u53e6\u4e00\u5c5e\u6027\u8fc7\u6ee4\uff09\u3002\u7cfb\u7edf\u5728\u6267\u884c\u65f6\u57fa\u4e8e\u4ee3\u4ef7\u6a21\u578b\u9009\u53d6\u5e76\u7ec4\u5408\u5df2\u6709\u7d22\u5f15\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cPathFinder\u5728\u53ec\u56de\u73870.95\u4e0b\u67e5\u8be2\u541e\u5410\u91cf\u4f18\u4e8e\u6700\u4f18\u57fa\u7ebf\uff0c\u6700\u9ad8\u63d0\u5347\u7ea69.8\u500d\u3002", "conclusion": "PathFinder\u901a\u8fc7\u5141\u8bb8\u9009\u62e9\u6027\u6784\u5efa\u9488\u5bf9\u5177\u4f53\u5c5e\u6027\u4f18\u5316\u7684ANNS\u7d22\u5f15\u5e76\u7528\u57fa\u4e8e\u4ee3\u4ef7\u7684\u4f18\u5316\u5668\u7ec4\u5408\u8fd9\u4e9b\u7d22\u5f15\u6765\u5904\u7406\u590d\u6742\u8fc7\u6ee4\u6761\u4ef6\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u53ec\u56de\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8fc7\u6ee4\u8fd1\u4f3c\u6700\u8fd1\u90bb\u67e5\u8be2\u6548\u7387\u3002"}}
{"id": "2511.00103", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00103", "abs": "https://arxiv.org/abs/2511.00103", "authors": ["Rotem Ezra", "Hedi Zisling", "Nimrod Berman", "Ilan Naiman", "Alexey Gorkor", "Liran Nochumsohn", "Eliya Nachmani", "Omri Azencot"], "title": "FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video", "comment": null, "summary": "Diffusion models have become state-of-the-art generative models for images,\naudio, and video, yet enabling fine-grained controllable generation, i.e.,\ncontinuously steering specific concepts without disturbing unrelated content,\nremains challenging. Concept Sliders (CS) offer a promising direction by\ndiscovering semantic directions through textual contrasts, but they require\nper-concept training and architecture-specific fine-tuning (e.g., LoRA),\nlimiting scalability to new modalities. In this work we introduce FreeSliders,\na simple yet effective approach that is fully training-free and\nmodality-agnostic, achieved by partially estimating the CS formula during\ninference. To support modality-agnostic evaluation, we extend the CS benchmark\nto include both video and audio, establishing the first suite for fine-grained\nconcept generation control with multiple modalities. We further propose three\nevaluation properties along with new metrics to improve evaluation quality.\nFinally, we identify an open problem of scale selection and non-linear\ntraversals and introduce a two-stage procedure that automatically detects\nsaturation points and reparameterizes traversal for perceptually uniform,\nsemantically meaningful edits. Extensive experiments demonstrate that our\nmethod enables plug-and-play, training-free concept control across modalities,\nimproves over existing baselines, and establishes new tools for principled\ncontrollable generation. An interactive presentation of our benchmark and\nmethod is available at: https://azencot-group.github.io/FreeSliders/", "AI": {"tldr": "FreeSliders\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u3001\u6a21\u6001\u4e0d\u53ef\u77e5\u7684\u63a8\u7406\u7ea7\u522b\u8fd1\u4f3cCS\u65b9\u6cd5\uff0c\u652f\u6301\u56fe\u50cf/\u89c6\u9891/\u97f3\u9891\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u63a7\u5236\uff0c\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u5c3a\u5ea6\u9009\u62e9\u4e0e\u91cd\u53c2\u6570\u5316\u5b9e\u73b0\u5747\u5300\u611f\u77e5\u7f16\u8f91\u3002", "motivation": "\u73b0\u6709\u7684Concept Sliders\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u6982\u5ff5\u8bad\u7ec3\u5e76\u4f9d\u8d56\u7279\u5b9a\u6a21\u578b\u5fae\u8c03\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u65b0\u6a21\u6001\uff1b\u76ee\u6807\u662f\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u8de8\u6a21\u6001\u4e14\u53ef\u63d2\u62d4\u7684\u7ec6\u7c92\u5ea6\u6982\u5ff5\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u8fd1\u4f3c\u6c42\u89e3CS\u516c\u5f0f\uff0cFreeSliders\u907f\u514d\u4e86\u9010\u6982\u5ff5\u8bad\u7ec3\u548c\u6a21\u578b\u5fae\u8c03\uff08\u5982LoRA\uff09\uff1b\u540c\u65f6\u63d0\u51fa\u4e24\u9636\u6bb5\u7b56\u7565\uff1a\u81ea\u52a8\u68c0\u6d4b\u9971\u548c\u70b9\u5e76\u5bf9\u904d\u5386\u8fdb\u884c\u518d\u53c2\u6570\u5316\u4ee5\u5b9e\u73b0\u611f\u77e5\u4e0a\u5747\u5300\u7684\u975e\u7ebf\u6027\u7f16\u8f91\u3002\u6269\u5c55\u4e86CS\u57fa\u51c6\u81f3\u89c6\u9891\u548c\u97f3\u9891\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e09\u9879\u8bc4\u4f30\u5c5e\u6027\u4e0e\u65b0\u6307\u6807\u6765\u89c4\u8303\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u6a21\u6001\uff08\u56fe\u50cf/\u89c6\u9891/\u97f3\u9891\uff09\u57fa\u51c6\u4e0a\uff0cFreeSliders\u5728\u4e0d\u8fdb\u884c\u8bad\u7ec3\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6982\u5ff5\u63a7\u5236\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u5c3a\u5ea6\u9009\u62e9\u4e0e\u975e\u7ebf\u6027\u904d\u5386\u63d0\u5347\u7f16\u8f91\u7684\u611f\u77e5\u4e00\u81f4\u6027\u3002", "conclusion": "FreeSliders\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4e0e\u6a21\u6001\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u9636\u6bb5\u90e8\u5206\u4f30\u8ba1Concept Sliders\uff08CS\uff09\u516c\u5f0f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u53ef\u63a7\u751f\u6210\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u53ef\u5373\u63d2\u5373\u7528\u5730\u5728\u56fe\u50cf\u3001\u89c6\u9891\u548c\u97f3\u9891\u4e0a\u8fdb\u884c\u6982\u5ff5\u63a7\u5236\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u7684\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6307\u6807\u3002"}}
{"id": "2511.01025", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2511.01025", "abs": "https://arxiv.org/abs/2511.01025", "authors": ["Huihui Yang", "Pingpeng Yuan"], "title": "Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index", "comment": null, "summary": "Reachability queries ask whether there exists a path from the source vertex\nto the target vertex on a graph. Recently, several powerful reachability\nqueries, such as Label-Constrained Reachability (LCR) queries and Regular Path\nQueries (RPQ), have been proposed for emerging complex edge-labeled digraphs.\nHowever, they cannot allow users to describe complex query requirements by\ncomposing query patterns. Here, we introduce composite patterns, a logical\nexpression of patterns that can express complex constraints on the set of\nlabels. Based on pattern, we propose pattern-constrained reachability queries\n(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the\nperformance to answer PCR queries, we build a two-dimensional reachability (TDR\nfor short) index which consists of a multi-way index (horizontal dimension) and\na path index (vertical dimension). Because the number of combinations of both\nlabels and vertices is exponential, it is very expensive to build full indices\nthat contain all the reachability information. Thus, the reachable vertices of\na vertex are decomposed into blocks, each of which is hashed into the\nhorizontal dimension index and the vertical dimension index, respectively. The\nindices in the horizontal dimension and the vertical dimension serve as a\nglobal filter and a local filter, respectively, to prune the search space.\nExperimental results demonstrate that our index size and indexing time\noutperform the state-of-the-art label-constrained reachability indexing\ntechnique on 16 real datasets. TDR can efficiently answer pattern-constrained\nreachability queries, including label-constrained reachability queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u8868\u8fbe\u590d\u6742\u6807\u7b7e\u7ea6\u675f\u7684\u590d\u5408\u6a21\u5f0f\u548c\u76f8\u5e94\u7684PCR\u95ee\u9898\uff1b\u7531\u4e8eNP-hard\uff0c\u8bbe\u8ba1\u4e86\u4e24\u7ef4\u54c8\u5e0c+\u8def\u5f84\u7684TDR\u7d22\u5f15\u5c06\u53ef\u8fbe\u96c6\u5206\u5757\u5e76\u53cc\u91cd\u8fc7\u6ee4\u4ee5\u526a\u679d\uff1b\u5b9e\u9a8c\u572816\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc1\u660e\u7d22\u5f15\u66f4\u5c0f\u3001\u6784\u5efa\u66f4\u5feb\u3001\u67e5\u8be2\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u6807\u7b7e\u7ea6\u675f\u53ef\u8fbe\u6027\uff08LCR\uff09\u548c\u6b63\u5219\u8def\u5f84\u67e5\u8be2\uff08RPQ\uff09\u65e0\u6cd5\u901a\u8fc7\u7ec4\u5408\u67e5\u8be2\u6a21\u5f0f\u6765\u8868\u8fbe\u590d\u6742\u7684\u6807\u7b7e\u7ea6\u675f\uff0c\u9700\u66f4\u7075\u6d3b\u7684\u67e5\u8be2\u6a21\u5f0f\u6765\u6ee1\u8db3\u590d\u6742\u5e94\u7528\u9700\u6c42\u3002", "method": "\u5f15\u5165\u590d\u5408\u6a21\u5f0f\uff08logical expressions of label sets\uff09\u5b9a\u4e49PCR\u95ee\u9898\uff0c\u8bc1\u660ePCR\u4e3aNP-hard\uff1b\u8bbe\u8ba1TDR\u7d22\u5f15\uff0c\u6c34\u5e73\u7ef4\u5ea6\u4e3a\u591a\u8def\u54c8\u5e0c\u7d22\u5f15\u5b58\u50a8\u6309\u6807\u7b7e\u7ec4\u5408\u5206\u5757\u7684\u53ef\u8fbe\u9876\u70b9\uff0c\u5782\u76f4\u7ef4\u5ea6\u4e3a\u8def\u5f84\u7d22\u5f15\uff1b\u5c06\u6bcf\u4e2a\u9876\u70b9\u7684\u53ef\u8fbe\u9876\u70b9\u5206\u89e3\u4e3a\u591a\u4e2a\u5757\uff0c\u5206\u522b\u54c8\u5e0c\u5230\u4e24\u7ef4\u7d22\u5f15\uff0c\u67e5\u8be2\u65f6\u5148\u7528\u6c34\u5e73\u7d22\u5f15\u505a\u5168\u5c40\u8fc7\u6ee4\u518d\u7528\u5782\u76f4\u7d22\u5f15\u505a\u5c40\u90e8\u7cbe\u68c0\u4ee5\u526a\u679d\u641c\u7d22\u7a7a\u95f4\u3002", "result": "\u901a\u8fc7\u572816\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\uff0cTDR\u5728\u7d22\u5f15\u5927\u5c0f\u548c\u6784\u5efa\u65f6\u95f4\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LCR\u7d22\u5f15\u6280\u672f\uff0c\u540c\u65f6\u80fd\u591f\u9ad8\u6548\u5730\u5904\u7406PCR\u67e5\u8be2\uff08\u5305\u542bLCR\u4f5c\u4e3a\u7279\u4f8b\uff09\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u590d\u6742\u8fb9\u6807\u8bb0\u6709\u5411\u56fe\u7684\u6a21\u5f0f\u7ea6\u675f\u53ef\u8fbe\u6027\u67e5\u8be2\uff08PCR\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e8c\u7ef4\u53ef\u8fbe\u6027\u7d22\u5f15\uff08TDR\uff09\u6765\u63d0\u9ad8\u67e5\u8be2\u6027\u80fd\u3002TDR\u901a\u8fc7\u6c34\u5e73\u7684\u591a\u8def\u7d22\u5f15\u548c\u5782\u76f4\u7684\u8def\u5f84\u7d22\u5f15\u4f5c\u4e3a\u5168\u5c40\u4e0e\u5c40\u90e8\u8fc7\u6ee4\u5668\uff0c\u5229\u7528\u5757\u54c8\u5e0c\u51cf\u5c11\u6307\u6570\u7ec4\u5408\u5e26\u6765\u7684\u5f00\u9500\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u7d22\u5f15\u5927\u5c0f\u548c\u6784\u5efa\u65f6\u95f4\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u80fd\u9ad8\u6548\u56de\u7b54PCR\u548cLCR\u67e5\u8be2\u3002"}}
{"id": "2511.00107", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.00107", "abs": "https://arxiv.org/abs/2511.00107", "authors": ["Piyushkumar Patel"], "title": "AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency", "comment": null, "summary": "Text to video generation has emerged as a critical frontier in generative\nartificial intelligence, yet existing approaches struggle with maintaining\ntemporal consistency, compositional understanding, and fine grained control\nover visual narratives. We present MOVAI (Multimodal Original Video AI), a\nnovel hierarchical framework that integrates compositional scene understanding\nwith temporal aware diffusion models for high fidelity text to video synthesis.\nOur approach introduces three key innovations: (1) a Compositional Scene Parser\n(CSP) that decomposes textual descriptions into hierarchical scene graphs with\ntemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that\nensures coherent motion dynamics across frames while preserving spatial\ndetails, and (3) a Progressive Video Refinement (PVR) module that iteratively\nenhances video quality through multi-scale temporal reasoning. Extensive\nexperiments on standard benchmarks demonstrate that MOVAI achieves\nstate-of-the-art performance, improving video quality metrics by 15.3% in\nLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing\nmethods. Our framework shows particular strength in generating complex\nmulti-object scenes with realistic temporal dynamics and fine-grained semantic\ncontrol.", "AI": {"tldr": "MOVAI\u901a\u8fc7CSP\u3001TSAM\u548cPVR\u4e09\u5927\u6a21\u5757\uff0c\u663e\u8457\u6539\u5584\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u65b9\u6cd5\u5728\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u573a\u666f\u7ec4\u5408\u80fd\u529b\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u4e0a\u7684\u4e0d\u8db3\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u52a8\u6001\u5408\u7406\u7684\u89c6\u9891\u3002", "method": "\u6846\u67b6\u5305\u542b\u4e09\u90e8\u5206\uff1a1) Compositional Scene Parser (CSP)\u5c06\u6587\u672c\u89e3\u6784\u4e3a\u5e26\u65f6\u95f4\u6ce8\u91ca\u7684\u5c42\u7ea7\u573a\u666f\u56fe\uff1b2) Temporal-Spatial Attention Mechanism (TSAM)\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u4fdd\u6301\u8fd0\u52a8\u8fde\u8d2f\u6027\u548c\u7a7a\u95f4\u7ec6\u8282\uff1b3) Progressive Video Refinement (PVR)\u901a\u8fc7\u591a\u5c3a\u5ea6\u65f6\u5e8f\u63a8\u7406\u8fed\u4ee3\u63d0\u5347\u89c6\u9891\u8d28\u91cf\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\uff0cMOVAI\u5728LPIPS\u3001FVD\u7b49\u89c6\u9891\u8d28\u91cf\u6307\u6807\u4e0a\u5206\u522b\u63d0\u534715.3%\u548c12.7%\uff0c\u5e76\u5728\u7528\u6237\u504f\u597d\u7814\u7a76\u4e2d\u63d0\u534718.9%\uff0c\u7279\u522b\u64c5\u957f\u751f\u6210\u590d\u6742\u591a\u5bf9\u8c61\u573a\u666f\u53ca\u771f\u5b9e\u65f6\u5e8f\u52a8\u6001\u3002", "conclusion": "MOVAI\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u6784\u6210\u573a\u666f\u7406\u89e3\u4e0e\u65f6\u95f4\u611f\u77e5\u6269\u6563\u6a21\u578b\u7684\u5206\u5c42\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u3001\u7ec4\u5408\u7406\u89e3\u548c\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002"}}
{"id": "2511.01602", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01602", "abs": "https://arxiv.org/abs/2511.01602", "authors": ["Xinyue Yang", "Chen Zheng", "Yaoyang Hou", "Renhao Zhang", "Yiyan Zhang", "Yanjun Wu", "Heng Zhang"], "title": "L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3", "comment": null, "summary": "Configuration tuning is critical for database performance. Although recent\nadvancements in database tuning have shown promising results in throughput and\nlatency improvement, challenges remain. First, the vast knob space makes direct\noptimization unstable and slow to converge. Second, reinforcement learning\npipelines often lack effective warm-start guidance and require long offline\ntraining. Third, transferability is limited: when hardware or workloads change,\nexisting models typically require substantial retraining to recover\nperformance.\n  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid\ndatabase tuning framework that features a three-stage pipeline: Stage one\nperforms a warm start that simultaneously generates uniform samples across the\nknob space and logs them into a shared pool; Stage two leverages a large\nlanguage model to mine and prioritize tuning hints from manuals and community\ndocuments for rapid convergence. Stage three uses the warm-start sample pool to\nreduce the dimensionality of knobs and state features, then fine-tunes the\nconfiguration with the Twin Delayed Deep Deterministic Policy Gradient\nalgorithm.\n  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared\nwith the best-performing alternative, our approach improves performance by an\naverage of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with\nmodels trained with reinforcement learning, it achieves rapid convergence in\nthe offline tuning stage on a single server. Moreover, during the online tuning\nstage, it only takes 30 steps to achieve best results.", "AI": {"tldr": "L2T-Tune\u7ed3\u5408LLM\u63d0\u793a\u4e0eTD3\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6e29\u542f\u52a8\u6837\u672c\u6c60\u3001LLM\u6316\u6398\u63d0\u793a\u4e0e\u964d\u7ef4\u5fae\u8c03\uff0c\u663e\u8457\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u534737.1%\uff0c\u5728\u7ebf\u4ec5\u970030\u6b65\u8fbe\u5230\u6700\u4f18\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u514b\u670d\u5f53\u524d\u6570\u636e\u5e93\u8c03\u4f18\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff1a\u9ad8\u7ef4\u65cb\u94ae\u7a7a\u95f4\u5bfc\u81f4\u7684\u4f18\u5316\u4e0d\u7a33\u5b9a\u4e14\u6536\u655b\u6162\uff1b\u5f3a\u5316\u5b66\u4e60\u7ba1\u7ebf\u7f3a\u4e4f\u6709\u6548\u7684\u70ed\u542f\u52a8\u4e0e\u9700\u957f\u65f6\u95f4\u79bb\u7ebf\u8bad\u7ec3\uff1b\u4ee5\u53ca\u6a21\u578b\u5728\u786c\u4ef6\u6216\u8d1f\u8f7d\u53d8\u5316\u65f6\u8fc1\u79fb\u80fd\u529b\u5dee\uff0c\u9700\u8981\u5927\u91cf\u91cd\u8bad\u7ec3\u3002", "method": "\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u6e29\u542f\u52a8\u9636\u6bb5\uff1a\u751f\u6210\u5e76\u8bb0\u5f55\u5747\u5300\u8986\u76d6\u65cb\u94ae\u7a7a\u95f4\u7684\u6837\u672c\u5230\u5171\u4eab\u6c60\uff1b2) LLM\u5f15\u5bfc\u9636\u6bb5\uff1a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u624b\u518c\u548c\u793e\u533a\u6587\u6863\u4e2d\u6316\u6398\u5e76\u4f18\u5148\u7ea7\u6392\u5e8f\u8c03\u4f18\u63d0\u793a\uff0c\u52a0\u901f\u6536\u655b\uff1b3) \u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\uff1a\u7528\u6e29\u542f\u52a8\u6837\u672c\u6c60\u505a\u964d\u7ef4\uff08\u65cb\u94ae\u4e0e\u72b6\u6001\u7279\u5f81\uff09\uff0c\u7136\u540e\u7528TD3\u7b97\u6cd5\u5fae\u8c03\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u5bf9\u6bd4\u8868\u660eL2T-Tune\u5728\u6240\u6709\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e3a37.1%\uff0c\u5728TPC-C\u4e0a\u6700\u9ad8\u8fbe73%\u3002\u4e0e\u7eaf\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u79bb\u7ebf\u8c03\u4f18\u9636\u6bb5\u6536\u655b\u66f4\u5feb\uff1b\u5728\u7ebf\u9636\u6bb5\u4ec5\u970030\u6b65\u5373\u53ef\u8fbe\u5230\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "L2T-Tune\u901a\u8fc7\u6df7\u5408LLM\u5f15\u5bfc\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u9636\u6bb5\u7ba1\u7ebf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6570\u636e\u5e93\u53c2\u6570\u8c03\u4f18\u4e2d\u6536\u655b\u6162\u3001\u70ed\u542f\u52a8\u4e0d\u8db3\u4e0e\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2511.00110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00110", "abs": "https://arxiv.org/abs/2511.00110", "authors": ["YingQiao Wang", "Eric Bigelow", "Boyi Li", "Tomer Ullman"], "title": "Chain of Time: In-Context Physical Simulation with Image Generation Models", "comment": null, "summary": "We propose a novel cognitively-inspired method to improve and interpret\nphysical simulation in vision-language models. Our ``Chain of Time\" method\ninvolves generating a series of intermediate images during a simulation, and it\nis motivated by in-context reasoning in machine learning, as well as mental\nsimulation in humans. Chain of Time is used at inference time, and requires no\nadditional fine-tuning. We apply the Chain-of-Time method to synthetic and\nreal-world domains, including 2-D graphics simulations and natural 3-D videos.\nThese domains test a variety of particular physical properties, including\nvelocity, acceleration, fluid dynamics, and conservation of momentum. We found\nthat using Chain-of-Time simulation substantially improves the performance of a\nstate-of-the-art image generation model. Beyond examining performance, we also\nanalyzed the specific states of the world simulated by an image model at each\ntime step, which sheds light on the dynamics underlying these simulations. This\nanalysis reveals insights that are hidden from traditional evaluations of\nphysical reasoning, including cases where an image generation model is able to\nsimulate physical properties that unfold over time, such as velocity, gravity,\nand collisions. Our analysis also highlights particular cases where the image\ngeneration model struggles to infer particular physical parameters from input\nimages, despite being capable of simulating relevant physical processes.", "AI": {"tldr": "\u63d0\u51faChain of Time\uff1a\u5728\u63a8\u7406\u9636\u6bb5\u751f\u6210\u4e2d\u95f4\u5e27\u4ee5\u589e\u5f3a\u5e76\u89e3\u91ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u6a21\u62df\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u57df\u4e0a\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u5e76\u63ed\u793a\u6a21\u578b\u5185\u90e8\u52a8\u529b\u5b66\u4e0e\u5c40\u9650\u3002", "motivation": "\u53d7\u5230\u4eba\u7c7b\u5fc3\u7406\u6a21\u62df\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u4e0a\u4e0b\u6587\u5185\u63a8\u7406\uff08in-context reasoning\uff09\u7684\u542f\u53d1\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u968f\u65f6\u95f4\u5c55\u5f00\u7684\u7269\u7406\u8fc7\u7a0b\uff08\u5982\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u3001\u52a8\u91cf\u5b88\u6052\u7b49\uff09\u7684\u6a21\u62df\u80fd\u529b\u5e76\u589e\u52a0\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728\u4e0d\u505a\u989d\u5916\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u7406\u65f6\u751f\u6210\u65f6\u95f4\u94fe\uff08\u4e2d\u95f4\u5e27\uff09\u5e76\u4ee5\u6b64\u5f15\u5bfc\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7269\u7406\u9884\u6d4b\uff1b\u5206\u6790\u6bcf\u4e00\u65f6\u95f4\u6b65\u7684\u4e16\u754c\u72b6\u6001\u6765\u7406\u89e3\u6a21\u578b\u5982\u4f55\u7f16\u7801\u548c\u6f14\u7ece\u7269\u7406\u89c4\u5f8b\u3002", "result": "\u57282D\u5408\u6210\u4e0e3D\u771f\u5b9e\u89c6\u9891\u7b49\u591a\u79cd\u57df\u4e0a\uff0cChain of Time\u5728\u6027\u80fd\u4e0a\u5927\u5e45\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\uff1b\u5206\u6790\u8868\u660e\u6a21\u578b\u80fd\u6a21\u62df\u8bf8\u5982\u901f\u5ea6\u3001\u91cd\u529b\u548c\u78b0\u649e\u7b49\u968f\u65f6\u95f4\u5c55\u5f00\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u4f46\u5728\u4ece\u9759\u6001\u8f93\u5165\u56fe\u50cf\u63a8\u65ad\u67d0\u4e9b\u7269\u7406\u53c2\u6570\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "Chain of Time\u80fd\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u751f\u6210\u4e00\u7cfb\u5217\u4e2d\u95f4\u56fe\u50cf\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u7269\u7406\u6a21\u62df\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u5bf9\u6a21\u578b\u5185\u90e8\u52a8\u6001\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2511.01625", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.01625", "abs": "https://arxiv.org/abs/2511.01625", "authors": ["Han Weng", "Zhou Liu", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen", "Wentao Zhang"], "title": "UniDataBench: Evaluating Data Analytics Agents Across Structured and Unstructured Data", "comment": null, "summary": "In the real business world, data is stored in a variety of sources, including\nstructured relational databases, unstructured databases (e.g., NoSQL\ndatabases), or even CSV/excel files. The ability to extract reasonable insights\nacross these diverse source is vital for business success. Existing benchmarks,\nhowever, are limited in assessing agents' capabilities across these diverse\ndata types. To address this gap, we introduce UniDataBench, a comprehensive\nbenchmark designed to evaluate the performance of data analytics agents in\nhandling diverse data sources. Specifically, UniDataBench is originating from\nreal-life industry analysis report and we then propose a pipeline to remove the\nprivacy and sensitive information. It encompasses a wide array of datasets,\nincluding relational databases, CSV files to NoSQL data, reflecting real-world\nbusiness scenarios, and provides unified framework to assess how effectively\nagents can explore multiple data formats, extract valuable insights, and\ngenerate meaningful summaries and recommendations. Based on UniDataBench, we\npropose a novel LLM-based agent named ReActInsight, an autonomous agent that\nperforms end-to-end analysis over diverse data sources by automatically\ndiscovering cross-source linkages, decomposing goals, and generating robust,\nself-correcting code to extract actionable insights. Our benchmark and agent\ntogether provide a powerful framework for advancing the capabilities of data\nanalytics agents in real-world applications.", "AI": {"tldr": "\u63d0\u51faUniDataBench\u57fa\u51c6\u4e0eLLM\u4ee3\u7406ReActInsight\uff0c\u7528\u4e8e\u8bc4\u6d4b\u548c\u5b9e\u73b0\u8de8\u5173\u7cfb\u578b\u3001CSV\u4e0eNoSQL\u7b49\u591a\u6e90\u6570\u636e\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u5316\u5206\u6790\u3002", "motivation": "\u73b0\u5b9e\u4e1a\u52a1\u4e2d\u6570\u636e\u5206\u6563\u5728\u591a\u79cd\u683c\u5f0f\u548c\u5b58\u50a8\u7cfb\u7edf\uff0c\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u4ee3\u7406\u8de8\u591a\u6e90\u6570\u636e\u7684\u5206\u6790\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u66f4\u8d34\u8fd1\u5de5\u4e1a\u573a\u666f\u7684\u8bc4\u6d4b\u57fa\u51c6\u5e76\u8bbe\u8ba1\u76f8\u5e94\u7684\u667a\u80fd\u4ee3\u7406\u3002", "method": "\u4ece\u771f\u5b9e\u884c\u4e1a\u5206\u6790\u62a5\u544a\u51fa\u53d1\uff0c\u6784\u5efa\u5305\u542b\u5173\u7cfb\u578b\u6570\u636e\u5e93\u3001CSV\u6587\u4ef6\u53caNoSQL\u6570\u636e\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u9690\u79c1\u654f\u611f\u4fe1\u606f\u53bb\u9664\u7ba1\u9053\uff1b\u63d0\u51fa\u7edf\u4e00\u8bc4\u6d4b\u6846\u67b6\u8bc4\u4ef7\u4ee3\u7406\u5728\u63a2\u7d22\u591a\u683c\u5f0f\u6570\u636e\u3001\u63d0\u53d6\u6d1e\u5bdf\u4e0e\u751f\u6210\u603b\u7ed3\u5efa\u8bae\u7684\u80fd\u529b\uff1b\u5e76\u5b9e\u73b0ReActInsight\u2014\u2014\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u4ee3\u7406\uff0c\u5305\u542b\u8de8\u6e90\u94fe\u8def\u53d1\u73b0\u3001\u4efb\u52a1\u5206\u89e3\u4e0e\u81ea\u6821\u6b63\u4ee3\u7801\u751f\u6210\u6a21\u5757\u3002", "result": "\u6784\u5efa\u4e86UniDataBench\u5e76\u57fa\u4e8e\u5176\u5f00\u53d1\u4e86ReActInsight\u3002\u8bba\u6587\u58f0\u79f0\u8be5\u7ec4\u5408\u80fd\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u548c\u63d0\u5347\u6570\u636e\u5206\u6790\u4ee3\u7406\u5728\u8de8\u6e90\u3001\u591a\u683c\u5f0f\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5c55\u793a\u4e86ReActInsight\u5728\u81ea\u52a8\u53d1\u73b0\u5173\u8054\u3001\u751f\u6210\u81ea\u6821\u6b63\u4ee3\u7801\u548c\u4ea7\u51fa\u53ef\u64cd\u4f5c\u6d1e\u5bdf\u65b9\u9762\u7684\u6709\u6548\u6027\uff08\u5177\u4f53\u5b9e\u9a8c\u6307\u6807\u548c\u6570\u503c\u9700\u770b\u6b63\u6587\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9762\u5411\u591a\u6e90\u6570\u636e\u5206\u6790\u4ee3\u7406\u8bc4\u6d4b\u7684\u57fa\u51c6UniDataBench\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u53bb\u9690\u79c1\u5316\u7684\u771f\u5b9e\u884c\u4e1a\u62a5\u544a\u5230\u6570\u636e\u96c6\u7684\u6784\u5efa\u6d41\u7a0b\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u4ee3\u7406ReActInsight\uff0c\u7528\u4ee5\u81ea\u52a8\u53d1\u73b0\u8de8\u6e90\u5173\u8054\u3001\u5206\u89e3\u4efb\u52a1\u5e76\u751f\u6210\u81ea\u6821\u6b63\u4ee3\u7801\uff0c\u5b8c\u6210\u7aef\u5230\u7aef\u5206\u6790\u3002"}}
{"id": "2511.00114", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00114", "abs": "https://arxiv.org/abs/2511.00114", "authors": ["Hanae Elmekki", "Amanda Spilkin", "Ehsan Zakeri", "Antonela Mariel Zanuttini", "Ahmed Alagha", "Hani Sami", "Jamal Bentahar", "Lyes Kadem", "Wen-Fang Xie", "Philippe Pibarot", "Rabeb Mizouni", "Hadi Otrok", "Azzam Mourad", "Sami Muhaidat"], "title": "End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning", "comment": null, "summary": "Cardiac ultrasound (US) is among the most widely used diagnostic tools in\ncardiology for assessing heart health, but its effectiveness is limited by\noperator dependence, time constraints, and human error. The shortage of trained\nprofessionals, especially in remote areas, further restricts access. These\nissues underscore the need for automated solutions that can ensure consistent,\nand accessible cardiac imaging regardless of operator skill or location. Recent\nprogress in artificial intelligence (AI), especially in deep reinforcement\nlearning (DRL), has gained attention for enabling autonomous decision-making.\nHowever, existing DRL-based approaches to cardiac US scanning lack\nreproducibility, rely on proprietary data, and use simplified models. Motivated\nby these gaps, we present the first end-to-end framework that integrates\ngenerative AI and DRL to enable autonomous and reproducible cardiac US\nscanning. The framework comprises two components: (i) a conditional generative\nsimulator combining Generative Adversarial Networks (GANs) with Variational\nAutoencoders (VAEs), that models the cardiac US environment producing realistic\naction-conditioned images; and (ii) a DRL module that leverages this simulator\nto learn autonomous, accurate scanning policies. The proposed framework\ndelivers AI-driven guidance through expert-validated models that classify image\ntype and assess quality, supports conditional generation of realistic US\nimages, and establishes a reproducible foundation extendable to other organs.\nTo ensure reproducibility, a publicly available dataset of real cardiac US\nscans is released. The solution is validated through several experiments. The\nVAE-GAN is benchmarked against existing GAN variants, with performance assessed\nusing qualitative and quantitative approaches, while the DRL-based scanning\nsystem is evaluated under varying configurations to demonstrate effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eVAE-GAN\u7684\u6761\u4ef6\u6a21\u62df\u5668+DRL\u7684\u9996\u4e2a\u7aef\u5230\u7aef\u81ea\u4e3b\u5fc3\u810f\u8d85\u58f0\u626b\u63cf\u6846\u67b6\uff0c\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6a21\u62df\u5668\u4e0eDRL\u7b56\u7565\u6709\u6548\u3002", "motivation": "\u5f53\u524d\u5fc3\u810f\u8d85\u58f0\u53d7\u64cd\u4f5c\u5458\u4f9d\u8d56\u5f3a\u3001\u8bad\u7ec3\u8005\u532e\u4e4f\u548c\u4eba\u4e3a\u8bef\u5dee\u5f71\u54cd\uff0c\u5df2\u6709DRL\u65b9\u6cd5\u5b58\u5728\u53ef\u590d\u73b0\u6027\u5dee\u3001\u4f9d\u8d56\u4e13\u6709\u6570\u636e\u4e0e\u6a21\u578b\u7b80\u5316\u7684\u95ee\u9898\uff0c\u56e0\u800c\u9700\u8981\u4e00\u4e2a\u53ef\u590d\u73b0\u4e14\u751f\u6210\u903c\u771f\u56fe\u50cf\u4ee5\u652f\u6301DRL\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6761\u4ef6\u751f\u6210\u6a21\u62df\u5668\uff08VAE-GAN\u6df7\u5408\u6a21\u578b\uff09\u7528\u4e8e\u751f\u6210\u52a8\u4f5c\u6761\u4ef6\u4e0b\u7684\u903c\u771f\u8d85\u58f0\u56fe\u50cf\uff1b\u57fa\u4e8e\u8be5\u6a21\u62df\u5668\u8bad\u7ec3DRL\u4ee3\u7406\u4ee5\u5b66\u4e60\u626b\u63cf\u7b56\u7565\uff1b\u5e76\u52a0\u5165\u4e13\u5bb6\u9a8c\u8bc1\u7684\u56fe\u50cf\u5206\u7c7b\u4e0e\u8d28\u91cf\u8bc4\u4f30\u6a21\u5757\u3002\u53d1\u5e03\u4e86\u516c\u5f00\u771f\u5b9e\u5fc3\u810f\u8d85\u58f0\u6570\u636e\u96c6\u4ee5\u4fdd\u8bc1\u53ef\u590d\u73b0\u6027\u3002", "result": "VAE-GAN\u5728\u5b9a\u6027\u4e0e\u5b9a\u91cf\u8bc4\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709GAN\u53d8\u4f53\uff1bDRL\u626b\u63cf\u7cfb\u7edf\u5728\u591a\u79cd\u914d\u7f6e\u4e0b\u8868\u73b0\u6709\u6548\u3002\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\uff0c\u652f\u6301\u5411\u5176\u4ed6\u5668\u5b98\u6269\u5c55\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u5c06\u751f\u6210\u5f0fAI\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6574\u5408\u4ee5\u5b9e\u73b0\u81ea\u4e3b\u3001\u53ef\u590d\u73b0\u7684\u5fc3\u810f\u8d85\u58f0\uff08US\uff09\u626b\u63cf\u7aef\u5230\u7aef\u6846\u67b6\u3002"}}
{"id": "2511.01716", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01716", "abs": "https://arxiv.org/abs/2511.01716", "authors": ["Jiale Lao", "Andreas Zimmerer", "Olga Ovcharenko", "Tianji Cong", "Matthew Russo", "Gerardo Vitagliano", "Michael Cochez", "Fatma \u00d6zcan", "Gautam Gupta", "Thibaud Hottelier", "H. V. Jagadish", "Kris Kissel", "Sebastian Schelter", "Andreas Kipf", "Immanuel Trummer"], "title": "SemBench: A Benchmark for Semantic Query Processing Engines", "comment": null, "summary": "We present a benchmark targeting a novel class of systems: semantic query\nprocessing engines. Those systems rely inherently on generative and reasoning\ncapabilities of state-of-the-art large language models (LLMs). They extend SQL\nwith semantic operators, configured by natural language instructions, that are\nevaluated via LLMs and enable users to perform various operations on multimodal\ndata.\n  Our benchmark introduces diversity across three key dimensions: scenarios,\nmodalities, and operators. Included are scenarios ranging from movie review\nanalysis to medical question-answering. Within these scenarios, we cover\ndifferent data modalities, including images, audio, and text. Finally, the\nqueries involve a diverse set of operators, including semantic filters, joins,\nmappings, ranking, and classification operators.\n  We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and\nThalamusDB) and one industrial system, Google BigQuery. Although these results\nreflect a snapshot of systems under continuous development, our study offers\ncrucial insights into their current strengths and weaknesses, illuminating\npromising directions for future research.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u8bed\u4e49\u67e5\u8be2\u5904\u7406\u5f15\u64ce\u7684\u591a\u573a\u666f\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u6269\u5c55SQL\u652f\u6301\u81ea\u7136\u8bed\u8a00\u914d\u7f6e\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\uff0c\u8bc4\u4f30\u4e86\u56db\u5957\u7cfb\u7edf\u5e76\u63ed\u793a\u5176\u80fd\u529b\u8fb9\u754c\u4e0e\u5f85\u6539\u8fdb\u70b9\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u7cfb\u7edf\u5174\u8d77\uff0c\u51fa\u73b0\u4e86\u65b0\u578b\u7684\u8bed\u4e49\u67e5\u8be2\u5904\u7406\u5f15\u64ce\uff0c\u73b0\u6709\u8bc4\u6d4b\u4e0d\u8db3\u4ee5\u8861\u91cf\u5176\u5728\u591a\u6a21\u6001\u3001\u591a\u573a\u666f\u3001\u590d\u6742\u8bed\u4e49\u64cd\u4f5c\u4e0a\u7684\u80fd\u529b\uff0c\u6545\u9700\u8bbe\u8ba1\u4e13\u95e8\u57fa\u51c6\u4ee5\u63ed\u793a\u7cfb\u7edf\u4f18\u52a3\u4e0e\u7814\u7a76\u65b9\u5411\u3002", "method": "\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u7ef4\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e09\u7c7b\u591a\u6837\u6027\uff1a\u573a\u666f\uff08\u5982\u7535\u5f71\u8bc4\u8bba\u5206\u6790\u3001\u533b\u5b66\u95ee\u7b54\uff09\u3001\u6a21\u6001\uff08\u56fe\u50cf\u3001\u97f3\u9891\u3001\u6587\u672c\uff09\u548c\u64cd\u4f5c\u7b26\uff08\u8bed\u4e49\u8fc7\u6ee4\u3001\u8fde\u63a5\u3001\u6620\u5c04\u3001\u6392\u5e8f\u3001\u5206\u7c7b\u7b49\uff09\u3002\u5728\u4e09\u4e2a\u5b66\u672f\u7cfb\u7edf\uff08LOTUS\u3001Palimpzest\u3001ThalamusDB\uff09\u53ca\u5de5\u4e1a\u7cfb\u7edf\uff08Google BigQuery\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8bc4\u6d4b\u63ed\u793a\u4e86\u5404\u7cfb\u7edf\u5728\u4e0d\u540c\u573a\u666f\u548c\u64cd\u4f5c\u4e0a\u7684\u5f3a\u9879\u4e0e\u77ed\u677f\uff0c\u5c3d\u7ba1\u5177\u4f53\u7ed3\u679c\u53d7\u7cfb\u7edf\u6301\u7eed\u6539\u8fdb\u5f71\u54cd\uff0c\u4f46\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d1e\u89c1\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u201c\u8bed\u4e49\u67e5\u8be2\u5904\u7406\u5f15\u64ce\u201d\u7684\u57fa\u51c6\u8bc4\u6d4b\uff0c\u5f3a\u8c03\u5176\u4f9d\u8d56\u751f\u6210\u5f0f\u5927\u6a21\u578b\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6269\u5c55SQL\u52a0\u5165\u7531\u81ea\u7136\u8bed\u8a00\u914d\u7f6e\u7684\u8bed\u4e49\u64cd\u4f5c\u7b26\u6765\u5bf9\u591a\u6a21\u6001\u6570\u636e\u6267\u884c\u67e5\u8be2\u3002"}}
{"id": "2511.00120", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00120", "abs": "https://arxiv.org/abs/2511.00120", "authors": ["Md Selim Sarowar", "Sungho Kim"], "title": "VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images", "comment": "This paper has been accepted to IEIE( The Institute Of Electronics\n  and Information Engineering, South Korea) Fall,2025 Conference", "summary": "The primary challenge in computer vision is precisely calculating the pose of\n6D objects, however many current approaches are still fragile and have trouble\ngeneralizing from synthetic data to real-world situations with fluctuating\nlighting, textureless objects, and significant occlusions. To address these\nlimitations, VLM6D, a novel dual-stream architecture that leverages the\ndistinct strengths of visual and geometric data from RGB-D input for robust and\nprecise pose estimation. Our framework uniquely integrates two specialized\nencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the\nRGB modality, harnessing its rich, pre-trained understanding of visual grammar\nto achieve remarkable resilience against texture and lighting variations.\nConcurrently, a PointNet++ encoder processes the 3D point cloud derived from\ndepth data, enabling robust geometric reasoning that excels even with the\nsparse, fragmented data typical of severe occlusion. These complementary\nfeature streams are effectively fused to inform a multi task prediction head.\nWe demonstrate through comprehensive experiments that VLM6D obtained new SOTA\nperformance on the challenging Occluded-LineMOD, validating its superior\nrobustness and accuracy.", "AI": {"tldr": "VLM6D\u4f7f\u7528DINOv2+PointNet++\u53cc\u6d41\u878d\u5408RGB-D\u7279\u5f81\uff0c\u7528\u4e8e\u66f4\u9c81\u68d2\u76846D\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5728Occluded-LineMOD\u4e0a\u8fbe\u6210SOTA\u3002", "motivation": "\u5f53\u524d6D\u4f4d\u59ff\u4f30\u8ba1\u5728\u4ece\u5408\u6210\u5230\u73b0\u5b9e\u8fc1\u79fb\u3001\u7eb9\u7406\u7f3a\u5931\u3001\u5149\u7167\u53d8\u5316\u548c\u5f3a\u906e\u6321\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u7ed3\u5408\u89c6\u89c9\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u4ee5\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u67b6\u6784\uff1aRGB\u7531DINOv2\uff08\u81ea\u76d1\u7763ViT\uff09\u7f16\u7801\uff0c\u6df1\u5ea6\u70b9\u4e91\u7531PointNet++\u7f16\u7801\uff0c\u968f\u540e\u7279\u5f81\u878d\u5408\u5e76\u8f93\u5165\u591a\u4efb\u52a1\u9884\u6d4b\u5934\uff0c\u8f93\u51fa\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728Occluded-LineMOD\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u65b0\u7684SOTA\u6210\u7ee9\uff0c\u663e\u793a\u5bf9\u7eb9\u7406\u548c\u5149\u7167\u53d8\u5316\u3001\u7a00\u758f/\u788e\u7247\u5316\u70b9\u4e91\u7684\u5f3a\u9c81\u68d2\u6027\u4e0e\u7cbe\u786e\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684VLM6D\u901a\u8fc7\u878d\u5408DINOv2\u89c6\u89c9\u7f16\u7801\u5668\u548cPointNet++\u51e0\u4f55\u7f16\u7801\u5668\uff0c\u6539\u5584\u4e866D\u4f4d\u59ff\u4f30\u8ba1\u5728\u7eb9\u7406\u3001\u5149\u7167\u53d8\u5316\u548c\u4e25\u91cd\u906e\u6321\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u5728Occluded-LineMOD\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\uff0c\u7ed3\u8bba\u53ef\u4fe1\u4f46\u9700\u66f4\u591a\u7ec6\u8282\u9a8c\u8bc1\u3002"}}
{"id": "2511.00123", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00123", "abs": "https://arxiv.org/abs/2511.00123", "authors": ["Gaby Maroun", "Salah Eddine Bekhouche", "Fadi Dornaika"], "title": "Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation", "comment": null, "summary": "Age estimation from facial images is a complex and multifaceted challenge in\ncomputer vision. In this study, we present a novel hybrid architecture that\ncombines ConvNeXt, a state-of-the-art advancement of convolutional neural\nnetworks (CNNs), with Vision Transformers (ViT). While each model independently\ndelivers excellent performance on a variety of tasks, their integration\nleverages the complementary strengths of the CNNs localized feature extraction\ncapabilities and the Transformers global attention mechanisms. Our proposed\nConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age\nestimation datasets, including MORPH II, CACD, and AFAD, and achieved superior\nperformance in terms of mean absolute error (MAE). To address computational\nconstraints, we leverage pre-trained models and systematically explore\ndifferent configurations, using linear layers and advanced regularization\ntechniques to optimize the architecture. Comprehensive ablation studies\nhighlight the critical role of individual components and training strategies,\nand in particular emphasize the importance of adapted attention mechanisms\nwithin the CNN framework to improve the model focus on age-relevant facial\nfeatures. The results show that the ConvNeXt-ViT hybrid not only outperforms\ntraditional methods, but also provides a robust foundation for future advances\nin age estimation and related visual tasks. This work underscores the\ntransformative potential of hybrid architectures and represents a promising\ndirection for the seamless integration of CNNs and transformers to address\ncomplex computer vision challenges.", "AI": {"tldr": "\u5c06ConvNeXt\u4e0eViT\u6df7\u5408\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u3001\u6b63\u5219\u5316\u548c\u6ce8\u610f\u529b\u9002\u914d\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4eMAE\uff0c\u8868\u660e\u6df7\u5408\u67b6\u6784\u662f\u5e74\u9f84\u4f30\u8ba1\u7684\u6709\u6548\u65b9\u5411\u3002", "motivation": "\u5e74\u9f84\u4f30\u8ba1\u5bf9\u5c40\u90e8\u7eb9\u7406\u4e0e\u5168\u5c40\u7ed3\u6784\u4fe1\u606f\u5747\u654f\u611f\uff0c\u5355\u4e00\u67b6\u6784\u96be\u4ee5\u540c\u65f6\u517c\u987e\uff0c\u56e0\u6b64\u5c1d\u8bd5\u878d\u5408CNN\u4e0eTransformer\u7684\u4e92\u8865\u4f18\u52bf\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3ConvNeXt\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u9aa8\u5e72\uff0c\u63a5\u5165ViT\u6216\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u6dfb\u52a0\u7ebf\u6027\u56de\u5f52\u5c42\u4ee5\u8f93\u51fa\u5e74\u9f84\uff0c\u4f7f\u7528\u6b63\u5219\u5316\u4e0e\u4e0d\u540c\u914d\u7f6e\u7684\u6d88\u878d\u5b9e\u9a8c\u4f18\u5316\u67b6\u6784\u3002", "result": "\u5728MORPH II\u3001CACD\u548cAFAD\u57fa\u51c6\u4e0a\uff0c\u6df7\u5408\u6a21\u578b\u5728MAE\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6d88\u878d\u7814\u7a76\u8868\u660e\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u6a21\u5757\u5bf9\u6539\u5584\u5e74\u9f84\u76f8\u5173\u7279\u5f81\u5173\u6ce8\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5c06ConvNeXt\u4e0eViT\u6df7\u5408\uff0c\u4ee5\u7ed3\u5408CNN\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\u4e0eTransformer\u5168\u5c40\u6ce8\u610f\u529b\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5e74\u9f84\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2511.00141", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00141", "abs": "https://arxiv.org/abs/2511.00141", "authors": ["Janghoon Cho", "Jungsoo Lee", "Munawar Hayat", "Kyuwoong Hwang", "Fatih Porikli", "Sungha Choi"], "title": "FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding", "comment": null, "summary": "Recent studies in long video understanding have harnessed the advanced\nvisual-language reasoning capabilities of Large Multimodal Models (LMMs),\ndriving the evolution of video-LMMs specialized for processing extended video\nsequences. However, the scalability of these models is severely limited by the\noverwhelming volume of visual tokens generated from extended video sequences.\nTo address this challenge, this paper proposes FLoC, an efficient visual token\ncompression framework based on the facility location function, a principled\napproach that swiftly selects a compact yet highly representative and diverse\nsubset of visual tokens within a predefined budget on the number of visual\ntokens. By integrating the lazy greedy algorithm, our method achieves\nremarkable efficiency gains by swiftly selecting a compact subset of tokens,\ndrastically reducing the number of visual tokens while guaranteeing\nnear-optimal performance. Notably, our approach is training-free,\nmodel-agnostic, and query-agnostic, providing a versatile solution that\nseamlessly integrates with diverse video-LLMs and existing workflows. Extensive\nevaluations on large-scale benchmarks, such as Video-MME, MLVU, and\nLongVideoBench, demonstrate that our framework consistently surpasses recent\ncompression techniques, highlighting not only its effectiveness and robustness\nin addressing the critical challenges of long video understanding, but also its\nefficiency in processing speed.", "AI": {"tldr": "FLoC\u5229\u7528facility location\u4e0elazy greedy\u5feb\u901f\u9009\u53d6\u4ee3\u8868\u6027\u89c6\u89c9token\uff0c\u8bad\u7ec3/\u6a21\u578b/\u67e5\u8be2\u65e0\u5173\uff0c\u5927\u5e45\u51cf\u5c11\u957f\u89c6\u9891token\u91cf\uff0c\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u3002", "motivation": "\u957f\u89c6\u9891\u4ea7\u751f\u5927\u91cf\u89c6\u89c9token\uff0c\u5bfc\u81f4\u89c6\u9891-LLM\u4e0d\u53ef\u6269\u5c55\uff1b\u9700\u8981\u4e00\u79cd\u5feb\u901f\u3001\u5ec9\u4ef7\u4e14\u4e0d\u9700\u91cd\u8bad\u7ec3\u7684\u538b\u7f29\u7b56\u7565\u4ee5\u4fdd\u7559\u4fe1\u606f\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u5c06facility location\u51fd\u6570\u4f5c\u4e3a\u76ee\u6807\uff0c\u91c7\u7528lazy greedy\u7b97\u6cd5\u5728\u9884\u8bbe\u7684token\u9884\u7b97\u4e0b\u5feb\u901f\u9009\u62e9\u5177\u6709\u4ee3\u8868\u6027\u4e0e\u591a\u6837\u6027\u7684\u89c6\u89c9token\u5b50\u96c6\uff0c\u51cf\u5c11\u8f93\u5165token\u6570\uff1b\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u8bad\u7ec3\uff0c\u9002\u914d\u591a\u79cd\u89c6\u9891-LLM\u5e76\u4e0e\u73b0\u6709\u6d41\u7a0b\u517c\u5bb9\u3002", "result": "\u5728Video-MME\u3001MLVU\u548cLongVideoBench\u7b49\u5927\u578b\u57fa\u51c6\u4e0a\uff0cFLoC\u5728\u7cbe\u5ea6\u548c\u5904\u7406\u901f\u5ea6\u4e0a\u5747\u4f18\u4e8e\u8fd1\u671f\u538b\u7f29\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5728\u6548\u7387\u3001\u7a33\u5065\u6027\u548c\u6548\u679c\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "FLoC\u901a\u8fc7\u57fa\u4e8efacility location\u7684\u8d2a\u5fc3\u5b50\u96c6\u9009\u62e9\uff0c\u6709\u6548\u538b\u7f29\u957f\u89c6\u9891\u7684\u89c6\u89c9token\u6570\u91cf\uff0c\u5728\u4e0d\u8bad\u7ec3\u6a21\u578b\u4e14\u4e0e\u6a21\u578b/\u67e5\u8be2\u65e0\u5173\u7684\u6761\u4ef6\u4e0b\uff0c\u63d0\u5347\u4e86\u89c6\u9891-LLM\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6548\u7387\u4e0e\u6027\u80fd\u3002"}}
{"id": "2511.00143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00143", "abs": "https://arxiv.org/abs/2511.00143", "authors": ["Jinsu Kim", "Yunhun Nam", "Minseon Kim", "Sangpil Kim", "Jongheon Jeong"], "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing", "comment": "36 pages; NeurIPS 2025; Code is available at\n  https://github.com/jsu-kim/BlurGuard", "summary": "Recent advances in text-to-image models have increased the exposure of\npowerful image editing techniques as a tool, raising concerns about their\npotential for malicious use. An emerging line of research to address such\nthreats focuses on implanting \"protective\" adversarial noise into images before\ntheir public release, so future attempts to edit them using text-to-image\nmodels can be impeded. However, subsequent works have shown that these\nadversarial noises are often easily \"reversed,\" e.g., with techniques as simple\nas JPEG compression, casting doubt on the practicality of the approach. In this\npaper, we argue that adversarial noise for image protection should not only be\nimperceptible, as has been a primary focus of prior work, but also\nirreversible, viz., it should be difficult to detect as noise provided that the\noriginal image is hidden. We propose a surprisingly simple method to enhance\nthe robustness of image protection methods against noise reversal techniques.\nSpecifically, it applies an adaptive per-region Gaussian blur on the noise to\nadjust the overall frequency spectrum. Through extensive experiments, we show\nthat our method consistently improves the per-sample worst-case protection\nperformance of existing methods against a wide range of reversal techniques on\ndiverse image editing scenarios, while also reducing quality degradation due to\nnoise in terms of perceptual metrics. Code is available at\nhttps://github.com/jsu-kim/BlurGuard.", "AI": {"tldr": "\u63d0\u51faBlurGuard\uff1a\u5bf9\u6297\u566a\u58f0\u505a\u81ea\u9002\u5e94\u5206\u533a\u9ad8\u65af\u6a21\u7cca\u4ee5\u8c03\u6574\u9891\u8c31\uff0c\u4ece\u800c\u63d0\u9ad8\u9632\u62a4\u566a\u58f0\u5bf9\u9006\u8f6c\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u517c\u987e\u56fe\u50cf\u611f\u77e5\u8d28\u91cf\uff0c\u5b9e\u9a8c\u8868\u660e\u6548\u679c\u7a33\u5065\u4e14\u5b9e\u7528\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u6709\u7684\u56fe\u50cf\u4fdd\u62a4\u5bf9\u6297\u566a\u58f0\u867d\u4e0d\u53ef\u89c1\u4f46\u6613\u88ab\u9006\u8f6c\uff0c\u65e0\u6cd5\u6709\u6548\u963b\u6b62\u6076\u610f\u56fe\u50cf\u7f16\u8f91\uff1b\u56e0\u6b64\u9700\u8981\u65e2\u4e0d\u53ef\u611f\u77e5\u53c8\u4e0d\u53ef\u9006\u7684\u566a\u58f0\u4fdd\u62a4\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u4e3a\u5728\u539f\u6709\u5bf9\u6297\u566a\u58f0\u4e0a\u65bd\u52a0\u57fa\u4e8e\u56fe\u50cf\u5206\u533a\u7684\u81ea\u9002\u5e94\u9ad8\u65af\u6a21\u7cca\uff0c\u8c03\u6574\u566a\u58f0\u7684\u9891\u8c31\u5206\u5e03\u4ee5\u964d\u4f4e\u566a\u58f0\u88ab\u9006\u8f6c\uff08\u5982JPEG\u3001\u8fc7\u6ee4\u5668\u3001\u53bb\u566a\u7b49\uff09\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u7ed3\u5408\u611f\u77e5\u8d28\u91cf\u5ea6\u91cf\u8fdb\u884c\u53c2\u6570\u9009\u62e9\u3002", "result": "\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u9006\u8f6c\u6280\u672f\u548c\u7f16\u8f91\u573a\u666f\u4e0b\u5747\u80fd\u4e00\u81f4\u63d0\u9ad8\u6837\u672c\u7684\u6700\u574f\u60c5\u51b5\u9632\u62a4\u6548\u679c\uff0c\u540c\u65f6\u5728\u611f\u77e5\u6307\u6807\u4e0a\u6bd4\u672a\u6a21\u7cca\u7684\u566a\u58f0\u66f4\u5c11\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\uff1a\u901a\u8fc7\u5bf9\u6297\u566a\u58f0\u505a\u81ea\u9002\u5e94\u7684\u5206\u533a\u9ad8\u65af\u6a21\u7cca\u53ef\u663e\u8457\u63d0\u9ad8\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5bf9\u566a\u58f0\u9006\u8f6c\u6280\u672f\u7684\u9c81\u68d2\u6027\uff0c\u4f7f\u5f97\u5728\u4fdd\u6301\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\u66f4\u96be\u88ab\u6062\u590d\u6216\u79fb\u9664\uff0c\u4ece\u800c\u6709\u6548\u963b\u6b62\u57fa\u4e8e\u6587\u672c\u5230\u56fe\u50cf\u7684\u7f16\u8f91\u6ee5\u7528\u3002"}}
{"id": "2511.00171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00171", "abs": "https://arxiv.org/abs/2511.00171", "authors": ["Rahul Ghosh", "Baishali Chaudhury", "Hari Prasanna Das", "Meghana Ashok", "Ryan Razkenari", "Sungmin Hong", "Chun-Hao Liu"], "title": "CompAgent: An Agentic Framework for Visual Compliance Verification", "comment": "Under review", "summary": "Visual compliance verification is a critical yet underexplored problem in\ncomputer vision, especially in domains such as media, entertainment, and\nadvertising where content must adhere to complex and evolving policy rules.\nExisting methods often rely on task-specific deep learning models trained on\nmanually labeled datasets, which are costly to build and limited in\ngeneralizability. While recent multi-modal large language models (MLLMs) offer\nbroad real-world knowledge and policy understanding, they struggle to reason\nover fine-grained visual details and apply structured compliance rules\neffectively on their own. In this paper, we propose CompAgent, the first\nagentic framework for visual compliance verification. CompAgent augments MLLMs\nwith a suite of visual tools - such as object detectors, face analyzers, NSFW\ndetectors, and captioning models - and introduces a planning agent that\ndynamically selects appropriate tools based on the compliance policy. A\nverification agent then integrates image, tool outputs, and policy context to\nperform multi-modal reasoning. Experiments on public benchmarks show that\nCompAgent outperforms specialized classifiers, direct MLLM prompting, and\ncurated routing baselines, achieving up to 76% F1 score and a 10% improvement\nover the state-of-the-art on the UnsafeBench dataset. Our results demonstrate\nthe effectiveness of agentic planning and tool-augmented reasoning for\nscalable, accurate, and adaptable visual compliance verification.", "AI": {"tldr": "CompAgent\u901a\u8fc7\u89c4\u5212\u667a\u80fd\u4f53\u52a8\u6001\u8c03\u7528\u89c6\u89c9\u5de5\u5177\u5e76\u7531\u9a8c\u8bc1\u667a\u80fd\u4f53\u6574\u5408\u5de5\u5177\u8f93\u51fa\u4e0e\u7b56\u7565\uff0c\u7528MLLM\u6267\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u89c6\u89c9\u5408\u89c4\u68c0\u6d4b\u6027\u80fd\uff0cUnsafeBench\u4e0aF1\u6700\u9ad876%\uff0c\u9886\u5148SOTA\u7ea610%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6807\u6ce8\u6570\u636e\u8bad\u7ec3\u4e13\u7528\u6a21\u578b\uff0c\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u6027\u5dee\uff1bMLLM\u867d\u5177\u5907\u4e30\u5bcc\u77e5\u8bc6\u4f46\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u4e0e\u7ed3\u6784\u5316\u7b56\u7565\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u5de5\u5177\u80fd\u529b\u4e0e\u667a\u80fd\u4f53\u89c4\u5212\u7ed3\u5408\u6765\u63d0\u5347\u89c6\u89c9\u5408\u89c4\u6027\u9a8c\u8bc1\u3002", "method": "\u8bbe\u8ba1\u4e24\u7ea7\u667a\u80fd\u4f53\u4f53\u7cfb\uff1a\u89c4\u5212\u667a\u80fd\u4f53\u6839\u636e\u5408\u89c4\u7b56\u7565\u52a8\u6001\u9009\u53d6\u5408\u9002\u89c6\u89c9\u5de5\u5177\uff1b\u9a8c\u8bc1\u667a\u80fd\u4f53\u5c06\u56fe\u50cf\u3001\u5de5\u5177\u8f93\u51fa\u4e0e\u7b56\u7565\u4e0a\u4e0b\u6587\u6574\u5408\uff0c\u5229\u7528MLLM\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u5224\u65ad\u3002\u4f7f\u7528\u591a\u79cd\u5de5\u5177\uff08\u68c0\u6d4b\u5668\u3001\u5206\u6790\u5668\u3001NSFW\u3001captioner\uff09\u589e\u5f3aMLLM\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u3002", "result": "\u5728\u516c\u5171\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e13\u7528\u5206\u7c7b\u5668\u3001\u76f4\u63a5MLLM\u63d0\u793a\u548c\u4eba\u5de5\u8def\u7531\u57fa\u7ebf\u3002\u5728UnsafeBench\u4e0a\u8fbe\u5230\u4e86\u6700\u9ad876% F1\uff0c\u8f83SOTA\u63d0\u5347\u7ea610%\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u4f53\u89c4\u5212\u4e0e\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u5728\u53ef\u6269\u5c55\u3001\u51c6\u786e\u548c\u53ef\u9002\u5e94\u89c6\u89c9\u5408\u89c4\u6027\u9a8c\u8bc1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompAgent\u7684\u9996\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u4e0e\u4e00\u5957\u89c6\u89c9\u5de5\u5177\uff08\u76ee\u6807\u68c0\u6d4b\u3001\u4eba\u8138\u5206\u6790\u3001NSFW\u68c0\u6d4b\u3001\u56fe\u50cf\u63cf\u8ff0\u7b49\uff09\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u89c4\u5212\u667a\u80fd\u4f53\u52a8\u6001\u9009\u62e9\u5de5\u5177\u4e0e\u9a8c\u8bc1\u667a\u80fd\u4f53\u6574\u5408\u4fe1\u606f\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u89c6\u89c9\u5408\u89c4\u6027\u68c0\u6d4b\u3002"}}
{"id": "2511.00181", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2511.00181", "abs": "https://arxiv.org/abs/2511.00181", "authors": ["Mengfei Liang", "Yiting Qu", "Yukun Jiang", "Michael Backes", "Yang Zhang"], "title": "From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection", "comment": "20 pages, 6 figures", "summary": "The rapid evolution of AI-generated images poses unprecedented challenges to\ninformation integrity and media authenticity. Existing detection approaches\nsuffer from fundamental limitations: traditional classifiers lack\ninterpretability and fail to generalize across evolving generative models,\nwhile vision-language models (VLMs), despite their promise, remain constrained\nto single-shot analysis and pixel-level reasoning. To address these challenges,\nwe introduce AIFo (Agent-based Image Forensics), a novel training-free\nframework that emulates human forensic investigation through multi-agent\ncollaboration. Unlike conventional methods, our framework employs a set of\nforensic tools, including reverse image search, metadata extraction,\npre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based\nagents that collect, synthesize, and reason over cross-source evidence. When\nevidence is conflicting or insufficient, a structured multi-agent debate\nmechanism allows agents to exchange arguments and reach a reliable conclusion.\nFurthermore, we enhance the framework with a memory-augmented reasoning module\nthat learns from historical cases to improve future detection accuracy. Our\ncomprehensive evaluation spans 6,000 images across both controlled laboratory\nsettings and challenging real-world scenarios, including images from modern\ngenerative platforms and diverse online sources. AIFo achieves 97.05% accuracy,\nsubstantially outperforming traditional classifiers and state-of-the-art VLMs.\nThese results demonstrate that agent-based procedural reasoning offers a new\nparadigm for more robust, interpretable, and adaptable AI-generated image\ndetection.", "AI": {"tldr": "AIFo\u662f\u4e00\u79cd\u8bad\u7ec3-free\u7684\u591a\u4ee3\u7406\u56fe\u50cf\u53d6\u8bc1\u6846\u67b6\uff0c\u96c6\u5408\u591a\u79cd\u5de5\u5177\u5e76\u7531LLM\u4ee3\u7406\u534f\u4f5c\u4e0e\u8fa9\u8bba\u3001\u8f85\u4ee5\u8bb0\u5fc6\u6a21\u5757\uff0c\u5b9e\u73b0\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u9ad8\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u68c0\u6d4b\uff08\u5b9e\u9a8c\u4e2d97.05%\uff09\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u5f25\u8865\u4f20\u7edf\u5206\u7c7b\u5668\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u6027\u3001\u4ee5\u53ca\u73b0\u6709VLM\u65b9\u6cd5\u4ec5\u9650\u5355\u6b21\u5206\u6790\u4e0e\u50cf\u7d20\u7ea7\u63a8\u7406\u7684\u4e0d\u8db3\uff0c\u9762\u5bf9\u4e0d\u65ad\u6f14\u8fdb\u7684AI\u56fe\u50cf\u751f\u6210\u6280\u672f\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5177\u7a0b\u5e8f\u5316\u3001\u53ef\u89e3\u91ca\u548c\u8de8\u6e90\u8bc1\u636e\u6574\u5408\u80fd\u529b\u7684\u68c0\u6d4b\u8303\u5f0f\u3002", "method": "\u65b9\u6cd5\u4e3a\u6784\u5efa\u82e5\u5e72\u529f\u80fd\u6027\u53d6\u8bc1\u5de5\u5177\u5e76\u8d4b\u4e88\u4e13\u95e8LLM\u4ee3\u7406\u4ee5\u6536\u96c6\u3001\u6574\u5408\u4e0e\u63a8\u7406\u8de8\u6e90\u8bc1\u636e\uff1b\u5728\u8bc1\u636e\u51b2\u7a81\u6216\u4e0d\u8db3\u65f6\u91c7\u7528\u7ed3\u6784\u5316\u591a\u4ee3\u7406\u8fa9\u8bba\u673a\u5236\u8fbe\u6210\u7ed3\u8bba\uff1b\u5e76\u5f15\u5165\u8bb0\u5fc6\u589e\u5f3a\u63a8\u7406\u6a21\u5757\u4ee5\u4ece\u5386\u53f2\u6848\u4f8b\u4e2d\u5b66\u4e60\u4ee5\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u3002\u8be5\u6846\u67b6\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u6d41\u7a0b\u5316\u4ee3\u7406\u534f\u4f5c\u5b8c\u6210\u5224\u5b9a\u3002", "result": "\u57286000\u5f20\u56fe\u50cf\uff08\u542b\u5b9e\u9a8c\u5ba4\u63a7\u5236\u4e0e\u73b0\u5b9e\u573a\u666f\u3001\u73b0\u4ee3\u751f\u6210\u5e73\u53f0\u4e0e\u5728\u7ebf\u591a\u6e90\u56fe\u50cf\uff09\u4e0a\u8bc4\u4f30\uff0cAIFo\u8fbe\u621097.05%\u51c6\u786e\u7387\uff0c\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u5206\u7c7b\u5668\u4e0e\u6700\u65b0VLM\u65b9\u6cd5\uff0c\u8868\u660e\u57fa\u4e8e\u4ee3\u7406\u7684\u6d41\u7a0b\u5316\u63a8\u7406\u5728\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u4e0a\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u4ee3\u7406\u5ba1\u67e5\u6846\u67b6AIFo\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u53d6\u8bc1\u6d41\u7a0b\uff0c\u7ed3\u5408\u53cd\u5411\u56fe\u50cf\u641c\u7d22\u3001\u5143\u6570\u636e\u63d0\u53d6\u3001\u9884\u8bad\u7ec3\u5206\u7c7b\u5668\u4e0eVLM\u5206\u6790\u7b49\u5de5\u5177\uff0c\u5e76\u7531\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u534f\u8c03\u3001\u7efc\u5408\u4e0e\u8bba\u8bc1\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u68c0\u6d4b\u3002"}}
{"id": "2511.00191", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00191", "abs": "https://arxiv.org/abs/2511.00191", "authors": ["Ziliang Chen", "Xin Huang", "Quanlong Guan", "Liang Lin", "Weiqi Luo"], "title": "A Retrospect to Multi-prompt Learning across Vision and Language", "comment": "ICCV", "summary": "The vision community is undergoing the unprecedented progress with the\nemergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays\nas the holy grail of accessing VLMs since it enables their fast adaptation to\ndownstream tasks with limited resources. Whereas existing researches milling\naround single-prompt paradigms, rarely investigate the technical potential\nbehind their multi-prompt learning counterparts. This paper aims to provide a\nprincipled retrospect for vision-language multi-prompt learning. We extend the\nrecent constant modality gap phenomenon to learnable prompts and then, justify\nthe superiority of vision-language transfer with multi-prompt augmentation,\nempirically and theoretically. In terms of this observation, we propose an\nEnergy-based Multi-prompt Learning (EMPL) to generate multiple prompt\nembeddings by drawing instances from an energy-based distribution, which is\nimplicitly defined by VLMs. So our EMPL is not only parameter-efficient but\nalso rigorously lead to the balance between in-domain and out-of-domain\nopen-vocabulary generalization. Comprehensive experiments have been conducted\nto justify our claims and the excellence of EMPL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7528\u80fd\u91cf\u5206\u5e03\u4eceVLM\u9690\u5f0f\u5b9a\u4e49\u7684\u6a21\u578b\u4e2d\u91c7\u6837\u751f\u6210\u591aprompt\u5d4c\u5165\u7684EMPL\u65b9\u6cd5\uff0c\u7406\u8bba+\u5b9e\u9a8c\u8bc1\u660e\u5176\u5728\u53c2\u6570\u6548\u7387\u548c\u57df\u5185/\u5916\u6cdb\u5316\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u4f18\u4e8e\u4f20\u7edf\u5355prompt\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5de5\u4f5c\u591a\u805a\u7126\u5355prompt\u8303\u5f0f\uff0c\u5ffd\u89c6\u591aprompt\u65b9\u6cd5\u6f5c\u529b\u3002\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u591aprompt\u5728\u89c6\u89c9-\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u7406\u8bba\u4e0e\u5b9e\u8bc1\u4f18\u52bf\uff0c\u63d0\u5347\u9002\u5e94\u6027\u4e0e\u6cdb\u5316\u6027\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\u3002", "method": "\u4f5c\u8005\u5148\u6269\u5c55\u5e76\u9a8c\u8bc1\u4e86\u5e38\u91cf\u6a21\u6001\u5dee\uff08constant modality gap\uff09\u73b0\u8c61\u5230\u53ef\u5b66\u4e60prompt\uff0c\u7136\u540e\u7406\u8bba\u4e0e\u5b9e\u8bc1\u5206\u6790\u591aprompt\u589e\u5f3a\u7684\u4f18\u8d8a\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u5206\u5e03\u3001\u7531VLM\u9690\u5f0f\u5b9a\u4e49\u7684\u91c7\u6837\u673a\u5236\u6765\u751f\u6210\u591a\u4e2aprompt\u5d4c\u5165\uff0c\u5b9e\u73b0\u591aprompt\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u663e\u793aEMPL\u5728\u5f00\u57df\uff08out-of-domain\uff09\u4e0e\u57df\u5185\uff08in-domain\uff09\u4efb\u52a1\u4e0a\u5747\u6709\u7ade\u4e89\u4f18\u52bf\uff0c\u4e14\u53c2\u6570\u5f00\u9500\u5c0f\uff0c\u7406\u8bba\u5206\u6790\u652f\u6301\u5176\u5728\u8c03\u548c\u57df\u95f4\u6027\u80fd\u4e0a\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Energy-based Multi-prompt Learning (EMPL)\u901a\u8fc7\u4ece\u80fd\u91cf\u5206\u5e03\u4e2d\u91c7\u6837\u751f\u6210\u591a\u4e2a\u53ef\u5b66\u4e60\u7684prompt\u5d4c\u5165\uff0c\u80fd\u5728\u53c2\u6570\u6548\u7387\u4e0e\u57df\u5185/\u57df\u5916\u6cdb\u5316\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4ece\u800c\u63d0\u5347Vision-Language\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u7684\u8868\u73b0\u3002"}}
{"id": "2511.00211", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00211", "abs": "https://arxiv.org/abs/2511.00211", "authors": ["Wenxuan Zhang", "Peng Hu"], "title": "An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals", "comment": null, "summary": "The increasing adoption of satellite Internet with low-Earth-orbit (LEO)\nsatellites in mega-constellations allows ubiquitous connectivity to rural and\nremote areas. However, weather events have a significant impact on the\nperformance and reliability of satellite Internet. Adverse weather events such\nas snow and rain can disturb the performance and operations of satellite\nInternet's essential ground terminal components, such as satellite antennas,\nsignificantly disrupting the space-ground link conditions between LEO\nsatellites and ground stations. This challenge calls for not only region-based\nweather forecasts but also fine-grained detection capability on ground terminal\ncomponents of fine-grained weather conditions. Such a capability can assist in\nfault diagnostics and mitigation for reliable satellite Internet, but its\nsolutions are lacking, not to mention the effectiveness and generalization that\nare essential in real-world deployments. This paper discusses an efficient\ntransfer learning (TL) method that can enable a ground component to locally\ndetect representative weather-related conditions. The proposed method can\ndetect snow, wet, and other conditions resulting from adverse and typical\nweather events and shows superior performance compared to the typical deep\nlearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL\nmethod also shows the advantage of being generalizable to various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u5730\u9762\u7ec8\u7aef\u5929\u6c14\u76f8\u5173\u72b6\u6001\u68c0\u6d4b\u65b9\u6cd5\uff0c\u80fd\u5728\u6709\u9650\u6570\u636e\u548c\u8d44\u6e90\u4e0b\u51c6\u786e\u68c0\u6d4b\u96ea\u3001\u6e7f\u7b49\u6761\u4ef6\uff0c\u4f18\u4e8eYOLOv7/9\u3001Faster R-CNN\u7b49\uff0c\u5e76\u5177\u5907\u8f83\u597d\u6cdb\u5316\u6027\u3002", "motivation": "\u968f\u7740LEO\u536b\u661f\u661f\u5ea7\u5e7f\u6cdb\u90e8\u7f72\uff0c\u5730\u9762\u7ec8\u7aef\u5728\u519c\u6751\u548c\u504f\u8fdc\u5730\u533a\u7684\u666e\u53ca\u4f7f\u5f97\u5929\u6c14\uff08\u5982\u96ea\u3001\u96e8\uff09\u5bf9\u5929\u7ebf\u7b49\u5173\u952e\u7ec4\u4ef6\u7684\u5f71\u54cd\u65e5\u76ca\u7a81\u663e\uff0c\u4e9f\u9700\u7ec6\u7c92\u5ea6\u7684\u7ec4\u4ef6\u72b6\u6001\u68c0\u6d4b\u80fd\u529b\u4ee5\u652f\u6301\u6545\u969c\u8bca\u65ad\u4e0e\u7f13\u89e3\u63aa\u65bd\uff0c\u4f46\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5728\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u8fc1\u79fb\u5b66\u4e60\u65b9\u6848\uff0c\u5c06\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u76f8\u5173\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u9002\u914d\u5730\u9762\u7ec8\u7aef\u7684\u6709\u9650\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff1b\u5e76\u901a\u8fc7\u4e0eYOLOv7\u3001YOLOv9\u3001Faster R-CNN\u3001R-YOLO\u7b49\u5178\u578b\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0e\u6cdb\u5316\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "result": "\u8be5\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522b\u96ea\u3001\u6e7f\u6da6\u53ca\u5176\u4ed6\u5929\u6c14\u76f8\u5173\u6761\u4ef6\uff0c\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u6240\u9009\u7684\u4e3b\u6d41\u76ee\u6807\u68c0\u6d4b\u7f51\u7edc\uff0c\u5e76\u5728\u591a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5730\u9762\u7ec8\u7aef\u672c\u5730\u68c0\u6d4b\u4e0e\u5929\u6c14\u76f8\u5173\u7684\u72b6\u6001\uff08\u5982\u96ea\u3001\u6e7f\u6ed1\u7b49\uff09\uff0c\u4ece\u800c\u63d0\u5347LEO\u536b\u661f\u4e92\u8054\u7f51\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00218", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00218", "abs": "https://arxiv.org/abs/2511.00218", "authors": ["Rajatsubhra Chakraborty", "Ana Espinosa-Momox", "Riley Haskin", "Depeng Xu", "Rosario Porras-Aguilar"], "title": "DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy", "comment": "5 pages, 4 figures", "summary": "Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces\nchallenges from traditional thresholding methods that are sensitive to noise\nand cell density, while deep learning approaches using simple channel\nconcatenation fail to exploit the complementary nature of polarized intensity\nimages and phase maps. We introduce DM-QPMNet, a dual-encoder network that\ntreats these as distinct modalities with separate encoding streams. Our\narchitecture fuses modality-specific features at intermediate depth via\nmulti-head attention, enabling polarized edge and texture representations to\nselectively integrate complementary phase information. This content-aware\nfusion preserves training stability while adding principled multi-modal\nintegration through dual-source skip connections and per-modality normalization\nat minimal overhead. Our approach demonstrates substantial improvements over\nmonolithic concatenation and single-modality baselines, showing that\nmodality-specific encoding with learnable fusion effectively exploits ssQPM's\nsimultaneous capture of complementary illumination and phase cues for robust\ncell segmentation.", "AI": {"tldr": "\u5c06\u504f\u632f\u5f3a\u5ea6\u4e0e\u76f8\u4f4d\u4f5c\u4e3a\u72ec\u7acb\u6a21\u6001\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u548c\u591a\u5934\u6ce8\u610f\u529b\u4e2d\u5c42\u878d\u5408\uff0cDM-QPMNet\u5728ssQPM\u7ec6\u80de\u5206\u5272\u4e0a\u4f18\u4e8e\u62fc\u63a5\u4e0e\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9608\u503c\u6cd5\u5bf9\u566a\u58f0\u548c\u7ec6\u80de\u5bc6\u5ea6\u654f\u611f\uff0c\u7b80\u5355\u901a\u9053\u62fc\u63a5\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u504f\u632f\u5f3a\u5ea6\u4e0e\u76f8\u4f4d\u56fe\u7684\u4e92\u8865\u4fe1\u606f\uff0c\u9700\u8bbe\u8ba1\u6a21\u6001\u611f\u77e5\u7684\u878d\u5408\u673a\u5236\u3002", "method": "\u63d0\u51fa\u53cc\u6d41\u7f16\u7801\u5668\u5206\u522b\u5bf9\u504f\u632f\u5f3a\u5ea6\u56fe\u548c\u76f8\u4f4d\u56fe\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u5728\u4e2d\u5c42\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u8fdb\u884c\u5185\u5bb9\u611f\u77e5\u7684\u6a21\u6001\u878d\u5408\uff0c\u4f7f\u7528\u53cc\u6e90\u8df3\u8dc3\u8fde\u63a5\u548c\u9010\u6a21\u6001\u5f52\u4e00\u5316\u4ee5\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u7f51\u7edc\u5f00\u9500\u5c0f\u3002", "result": "\u5728\u4e0e\u901a\u9053\u62fc\u63a5\u548c\u5355\u6a21\u6001\u57fa\u7ebf\u6bd4\u8f83\u4e2d\uff0cDM-QPMNet\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb\uff08\u6587\u4e2d\u5ba3\u79f0\u4e3a\u201c\u5927\u5e45\u5ea6\u201d\u63d0\u5347\uff09\uff0c\u8868\u660e\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u52a0\u53ef\u5b66\u4e60\u878d\u5408\u80fd\u66f4\u7a33\u5065\u5730\u5229\u7528ssQPM\u63d0\u4f9b\u7684\u7167\u660e\u4e0e\u76f8\u4f4d\u7ebf\u7d22\u3002", "conclusion": "DM-QPMNet\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u4e0e\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86ssQPM\u7ec6\u80de\u5206\u5272\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5c06\u504f\u632f\u5f3a\u5ea6\u548c\u76f8\u4f4d\u56fe\u89c6\u4e3a\u4e92\u8865\u6a21\u6001\u5e76\u5206\u522b\u7f16\u7801\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.00231", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00231", "abs": "https://arxiv.org/abs/2511.00231", "authors": ["Fuming Yang", "Yicong Li", "Hanspeter Pfister", "Jeff W. Lichtman", "Yaron Meirovitch"], "title": "Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior", "comment": null, "summary": "Petascale electron microscopy (EM) datasets push storage, transfer, and\ndownstream analysis toward their current limits. We present a vector-quantized\nvariational autoencoder-based (VQ-VAE) compression framework for EM that spans\n16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme\ncompression, with an optional Transformer prior that predicts bottom tokens\n(without changing the compression ratio) to restore texture via feature-wise\nlinear modulation (FiLM) and concatenation; we further introduce an ROI-driven\nworkflow that performs selective high-resolution reconstruction from\n1024x-compressed latents only where needed.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u6309\u9700\u89e3\u7801\u7684VQ-VAE EM\u538b\u7f29\u6846\u67b6\uff0816x\u20131024x\uff09\uff0c\u901a\u8fc7Transformer\u5148\u9a8c+FiLM\u6062\u590d\u7eb9\u7406\uff0c\u5e76\u7528ROI\u7b56\u7565\u9009\u62e9\u6027\u91cd\u5efa\u9ad8\u5206\u8fa8\u7387\u533a\u57df\u3002", "motivation": "Petascale\u7ea7\u522b\u7684EM\u6570\u636e\u5728\u5b58\u50a8\u3001\u4f20\u8f93\u548c\u4e0b\u6e38\u5206\u6790\u65b9\u9762\u5e26\u6765\u5de8\u5927\u538b\u529b\uff0c\u9700\u8981\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u538b\u7f29\u65b9\u6848\uff0c\u65e2\u80fd\u63d0\u4f9b\u6781\u81f4\u538b\u7f29\u4ee5\u8282\u7701\u8d44\u6e90\uff0c\u53c8\u80fd\u5728\u9700\u8981\u65f6\u4ee5\u53ef\u63a7\u6210\u672c\u6062\u590d\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\u3002", "method": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u53d8\u5206\u81ea\u7f16\u7801\u5668(VQ-VAE)\u6784\u5efa\u591a\u5c42\u7f16\u7801\u5668/\u89e3\u7801\u5668\uff0c\u5141\u8bb8\u4ec5\u89e3\u7801\u9876\u5c42\uff08top-only\uff09\u4ee5\u5b9e\u73b0\u6781\u7aef\u538b\u7f29\u3002\u4e3a\u6062\u590d\u7ec6\u8282\uff0c\u5f15\u5165\u4e00\u4e2aTransformer\u5148\u9a8c\u9884\u6d4b\u5e95\u5c42\u4ee4\u724c\uff0c\u7ed3\u5408FiLM\u548c\u62fc\u63a5\u64cd\u4f5c\u5c06\u9884\u6d4b\u7684\u5e95\u5c42\u4fe1\u606f\u6ce8\u5165\u89e3\u7801\u5668\u800c\u4e0d\u6539\u53d8\u538b\u7f29\u6bd4\u3002\u8fd8\u5b9e\u73b0\u4e86\u57fa\u4e8eROI\u7684\u6d41\u7a0b\uff0c\u4ec5\u5bf91024x\u538b\u7f29\u7684\u6f5c\u5728\u8868\u793a\u4e2d\u9700\u8981\u7684\u533a\u57df\u8fdb\u884c\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u4ece\u800c\u8282\u7701\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\u3002", "result": "\u6846\u67b6\u8986\u76d616x\u81f31024x\u7684\u538b\u7f29\u8303\u56f4\uff0c\u652f\u6301\u6309\u9700\u89e3\u7801\uff08top-only\uff09\u548c\u53ef\u9009\u7684Transformer\u9a71\u52a8\u7684\u5e95\u5c42\u4ee4\u724c\u9884\u6d4b\u4ee5\u63d0\u5347\u7eb9\u7406\u6062\u590d\u8d28\u91cf\uff1bROI\u9a71\u52a8\u7684\u9009\u62e9\u6027\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u663e\u8457\u964d\u4f4e\u4e86\u603b\u4f53\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u7559\u91cd\u8981\u533a\u57df\u7684\u7ec6\u8282\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eVQ-VAE\u7684\u7535\u5b50\u663e\u5fae\u955c\uff08EM\uff09\u56fe\u50cf\u538b\u7f29\u6846\u67b6\uff0c\u652f\u630116x\u52301024x\u7684\u538b\u7f29\u7387\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u89e3\u7801\u4e0e\u53ef\u9009Transformer\u5148\u9a8c\u5b9e\u73b0\u6309\u9700\u89e3\u7801\u4e0e\u7eb9\u7406\u6062\u590d\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\u7684\u9ad8\u5206\u8fa8\u7387\u91cd\u5efa\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2511.00244", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00244", "abs": "https://arxiv.org/abs/2511.00244", "authors": ["Yan Bin Ng", "Xianfeng Gu"], "title": "Hyperbolic Optimal Transport", "comment": "65 pages, 21 figures", "summary": "The optimal transport (OT) problem aims to find the most efficient mapping\nbetween two probability distributions under a given cost function, and has\ndiverse applications in many fields such as machine learning, computer vision\nand computer graphics. However, existing methods for computing optimal\ntransport maps are primarily developed for Euclidean spaces and the sphere. In\nthis paper, we explore the problem of computing the optimal transport map in\nhyperbolic space, which naturally arises in contexts involving hierarchical\ndata, networks, and multi-genus Riemann surfaces. We propose a novel and\nefficient algorithm for computing the optimal transport map in hyperbolic space\nusing a geometric variational technique by extending methods for Euclidean and\nspherical geometry to the hyperbolic setting. We also perform experiments on\nsynthetic data and multi-genus surface models to validate the efficacy of the\nproposed method.", "AI": {"tldr": "\u62d3\u5c55\u6700\u4f18\u4f20\u8f93\u5230\u53cc\u66f2\u7a7a\u95f4\uff1a\u63d0\u51fa\u51e0\u4f55\u53d8\u5206\u7b97\u6cd5\u5e76\u5728\u5408\u6210\u4e0e\u591a\u66f2\u9762\u6570\u636e\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6700\u4f18\u4f20\u8f93\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u6b27\u6c0f\u4e0e\u7403\u9762\uff0c\u800c\u8bb8\u591a\u5b9e\u9645\u95ee\u9898\uff08\u5982\u5c42\u6b21\u5316\u6570\u636e\u3001\u7f51\u7edc\u3001\u4ee5\u53ca\u591a\u66f2\u9762\u51e0\u4f55\uff09\u81ea\u7136\u5d4c\u5165\u5728\u53cc\u66f2\u7a7a\u95f4\uff0c\u4e9f\u9700\u6709\u6548\u7684\u53cc\u66f2\u6700\u4f18\u4f20\u8f93\u7b97\u6cd5\u3002", "method": "\u57fa\u4e8e\u5c06\u6b27\u6c0f\u548c\u7403\u9762\u51e0\u4f55\u7684\u53d8\u5206\u6280\u672f\u63a8\u5e7f\u5230\u53cc\u66f2\u51e0\u4f55\uff0c\u6784\u9020\u9002\u7528\u4e8e\u8d1f\u5e38\u66f2\u7387\u7a7a\u95f4\u7684\u76ee\u6807\u6cdb\u51fd\u4e0eEuler\u2013Lagrange\u65b9\u7a0b\uff0c\u5e76\u8bbe\u8ba1\u6570\u503c\u4f18\u5316\u7b97\u6cd5\u5b9e\u73b0\u6620\u5c04\u6c42\u89e3\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u591a\u66f2\u9762\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u7a33\u5b9a\u8ba1\u7b97\u53cc\u66f2\u7a7a\u95f4\u4e2d\u7684\u6620\u5c04\uff0c\u5177\u6709\u8f83\u597d\u7684\u6570\u503c\u6027\u80fd\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u6700\u4f18\u4f20\u8f93\u6620\u5c04\u7684\u51e0\u4f55\u53d8\u5206\u65b9\u6cd5\u6269\u5c55\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5c42\u6b21\u5316\u6570\u636e\u548c\u591a\u66f2\u9762\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.00248", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation.", "AI": {"tldr": "\u63d0\u51faMSDI\u4e0eMSDS\u7ed3\u5408LLM\u8bed\u4e49\u7684\u96f6\u6837\u672c\u5bf9\u8c61\u611f\u77e54D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u5408\u7406\u3001\u8bed\u4e49\u4e00\u81f4\u4e14\u53ef\u6269\u5c55\u7684\u4ea4\u4e92\u5f0f\u52a8\u4f5c\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u7684\u540c\u65f6\u4ecd\u51fa\u73b0\u4e0d\u73b0\u5b9e\u7684\u5f62\u53d8\u3001\u8bed\u4e49\u51b2\u7a81\u4e0e\u7269\u7406\u4e0d\u4e00\u81f4\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f3D\u7269\u7406\u5148\u9a8c\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u51653D\u9ad8\u65af\u8868\u793a\u548c\u8fd0\u52a8\u6269\u6563\u5148\u9a8c\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u89c6\u9891\u4e2d\u4eba\u4f53\u4e0e\u7269\u4f53\u4ea4\u4e92\u7684\u7269\u7406\u5408\u7406\u6027\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u9884\u751f\u6210\u76843D\u4eba\u4f53\u4e0e\u7269\u4f53\u51e0\u4f55\u8868\u793a\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u7a7a\u95f4\u4e0e\u8bed\u4e49\u63d0\u793a\uff0c\u63d0\u51faMotion Diffusion Score Distillation Sampling(MSDS)\u4ee5\u4ece\u9884\u8bad\u7ec3\u7684\u8fd0\u52a8\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u5206\u6570\u68af\u5ea6\uff0c\u5e76\u901a\u8fc7Motion Score Distilled Interaction(MSDI)\u6267\u884c\u9762\u5411\u7a7a\u95f4\u7684\u8fd0\u52a8\u4f18\u5316\u6765\u8c03\u6574\u4eba\u4f53\u52a8\u4f5c\u4ee5\u9002\u914d\u7269\u4f53\u4e0e\u8bed\u4e49\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u5728\u4e0d\u91cd\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u6761\u4ef6\u4e0b\u4ea7\u751f\u9075\u5faa3D\u7a7a\u95f4\u8bed\u5883\u7684\u81ea\u7136\u4e14\u7269\u7406\u5408\u7406\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u5206\u5e03\u5916\u7684\u5bf9\u8c61\u611f\u77e5\u4eba\u4f53\u52a8\u4f5c\u573a\u666f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e3D\u9ad8\u65af\u8868\u793a\u4e0e\u8fd0\u52a8\u6269\u6563\u5148\u9a8c\u7684\u5bf9\u8c61\u611f\u77e54D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u751f\u6210\u7b26\u5408\u8bed\u4e49\u548c\u7269\u7406\u7ea6\u675f\u7684\u81ea\u7136\u4eba\u4f53\u52a8\u4f5c\u3002"}}
{"id": "2511.00252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00252", "abs": "https://arxiv.org/abs/2511.00252", "authors": ["Aaron Sun", "Subhransu Maji", "Grant Van Horn"], "title": "Merlin L48 Spectrogram Dataset", "comment": "Accepted to 39th Conference on Neural Information Processing Systems\n  (NeurIPS 2025) Track on Datasets and Benchmarks", "summary": "In the single-positive multi-label (SPML) setting, each image in a dataset is\nlabeled with the presence of a single class, while the true presence of other\nclasses remains unknown. The challenge is to narrow the performance gap between\nthis partially-labeled setting and fully-supervised learning, which often\nrequires a significant annotation budget. Prior SPML methods were developed and\nbenchmarked on synthetic datasets created by randomly sampling single positive\nlabels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and\nCUB200. However, this synthetic approach does not reflect real-world scenarios\nand fails to capture the fine-grained complexities that can lead to difficult\nmisclassifications. In this work, we introduce the L48 dataset, a fine-grained,\nreal-world multi-label dataset derived from recordings of bird sounds. L48\nprovides a natural SPML setting with single-positive annotations on a\nchallenging, fine-grained domain, as well as two extended settings in which\ndomain priors give access to additional negative labels. We benchmark existing\nSPML methods on L48 and observe significant performance differences compared to\nsynthetic datasets and analyze method weaknesses, underscoring the need for\nmore realistic and difficult benchmarks.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86L48\uff0c\u4e00\u4e2a\u6765\u81ea\u9e1f\u58f0\u7684\u771f\u5b9e\u7ec6\u7c92\u5ea6SPML\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u4e0a\u8bc4\u4f30\u73b0\u6709\u65b9\u6cd5\uff0c\u53d1\u73b0\u5408\u6210\u57fa\u51c6\u4e0d\u80fd\u4ee3\u8868\u73b0\u5b9e\u590d\u6742\u6027\uff0c\u547c\u5401\u66f4\u73b0\u5b9e\u7684\u8bc4\u6d4b\u57fa\u51c6\u4e0e\u65b9\u6cd5\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709SPML\u5de5\u4f5c\u4e3b\u8981\u57fa\u4e8e\u7531\u5168\u6807\u6ce8\u6570\u636e\u968f\u673a\u91c7\u6837\u5f97\u5230\u7684\u5408\u6210\u5355\u6b63\u6807\u7b7e\u6570\u636e\u96c6\uff0c\u8fd9\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u7ec6\u7c92\u5ea6\u590d\u6742\u6027\u548c\u8bef\u5206\u7c7b\u6311\u6218\uff0c\u56e0\u800c\u9700\u8981\u771f\u5b9e\u7684\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u65b9\u6cd5\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u6765\u81ea\u9e1f\u58f0\u5f55\u97f3\u7684\u7ec6\u7c92\u5ea6\u591a\u6807\u7b7e\u6570\u636e\u96c6L48\uff0c\u4fdd\u7559\u5355\u6b63\u6807\u6ce8\u5e76\u8bbe\u8ba1\u4e24\u79cd\u6269\u5c55\u8bbe\u7f6e\u4ee5\u5229\u7528\u9886\u57df\u5148\u9a8c\u83b7\u5f97\u989d\u5916\u8d1f\u6807\u7b7e\uff1b\u5728L48\u4e0a\u5bf9\u73b0\u6709SPML\u65b9\u6cd5\u8fdb\u884c\u7cfb\u7edf\u57fa\u51c6\u8bc4\u6d4b\u4e0e\u6bd4\u8f83\uff0c\u5206\u6790\u6027\u80fd\u5dee\u5f02\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "result": "\u5728L48\u4e0a\uff0c\u73b0\u6709SPML\u65b9\u6cd5\u603b\u4f53\u6027\u80fd\u4e0b\u964d\u5e76\u8868\u73b0\u51fa\u4e0e\u5408\u6210\u6570\u636e\u4e0d\u540c\u7684\u5f31\u70b9\uff1b\u5177\u4f53\u65b9\u6cd5\u5728\u771f\u5b9e\u7ec6\u7c92\u5ea6\u9e1f\u7c7b\u58f0\u97f3\u6570\u636e\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u5408\u6210\u57fa\u51c6\u4e0d\u80fd\u5145\u5206\u9884\u6d4b\u73b0\u5b9e\u573a\u666f\u8868\u73b0\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u4e86L48\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u771f\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u5355\u6b63\u591a\u6807\u7b7e\uff08SPML\uff09\u9e1f\u7c7b\u58f0\u97f3\u6570\u636e\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bc4\u4f30\u73b0\u6709SPML\u65b9\u6cd5\uff0c\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4e0e\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u57fa\u51c6\u5dee\u5f02\u663e\u8457\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8584\u5f31\u70b9\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u73b0\u5b9e\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.00255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00255", "abs": "https://arxiv.org/abs/2511.00255", "authors": ["Fangxun Liu", "S M Rayeed", "Samuel Stevens", "Alyson East", "Cheng Hsuan Chiang", "Colin Lee", "Daniel Yi", "Junke Yang", "Tejas Naik", "Ziyi Wang", "Connor Kilrain", "Elijah H Buckwalter", "Jiacheng Hou", "Saul Ibaven Bueno", "Shuheng Wang", "Xinyue Ma", "Yifan Liu", "Zhiyuan Tao", "Ziheng Zhang", "Eric Sokol", "Michael Belitz", "Sydne Record", "Charles V. Stewart", "Wei-Lun Chao"], "title": "BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing", "comment": "4 pages, NeurIPS 2025 Workshop Imageomics", "summary": "In entomology and ecology research, biologists often need to collect a large\nnumber of insects, among which beetles are the most common species. A common\npractice for biologists to organize beetles is to place them on trays and take\na picture of each tray. Given the images of thousands of such trays, it is\nimportant to have an automated pipeline to process the large-scale data for\nfurther research. Therefore, we develop a 3-stage pipeline to detect all the\nbeetles on each tray, sort and crop the image of each beetle, and do\nmorphological segmentation on the cropped beetles. For detection, we design an\niterative process utilizing a transformer-based open-vocabulary object detector\nand a vision-language model. For segmentation, we manually labeled 670 beetle\nimages and fine-tuned two variants of a transformer-based segmentation model to\nachieve fine-grained segmentation of beetles with relatively high accuracy. The\npipeline integrates multiple deep learning methods and is specialized for\nbeetle image processing, which can greatly improve the efficiency to process\nlarge-scale beetle data and accelerate biological research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7531\u5f00\u57dfTransformer\u68c0\u6d4b\u3001\u88c1\u526a\u548c\u5fae\u8c03\u5206\u5272\u6a21\u578b\u7ec4\u6210\u7684\u4e09\u9636\u6bb5\u7532\u866b\u56fe\u50cf\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528670\u5f20\u624b\u5de5\u6807\u6ce8\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7ec6\u7c92\u5ea6\u5206\u5272\uff0c\u4ece\u800c\u81ea\u52a8\u5316\u5e76\u52a0\u901f\u5927\u89c4\u6a21\u7532\u866b\u56fe\u50cf\u7684\u6574\u7406\u4e0e\u5206\u6790\u3002", "motivation": "\u751f\u7269\u5b66\u5bb6\u9700\u5904\u7406\u6210\u5343\u4e0a\u4e07\u6258\u76d8\u62cd\u6444\u7684\u7532\u866b\u56fe\u7247\uff0c\u4eba\u5de5\u9010\u5f20\u5904\u7406\u8017\u65f6\u8d39\u529b\uff0c\u9700\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\u4ee5\u52a0\u901f\u6837\u672c\u6574\u7406\u4e0e\u540e\u7eed\u5f62\u6001\u5b66\u5206\u6790\u3002", "method": "\u6784\u5efa\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1\uff09\u57fa\u4e8eTransformer\u7684\u5f00\u57df\u76ee\u6807\u68c0\u6d4b\u5668\u4e0e\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u8fed\u4ee3\u68c0\u6d4b\u7b56\u7565\uff0c\u7528\u4e8e\u5b9a\u4f4d\u6258\u76d8\u56fe\u4e2d\u6240\u6709\u7532\u866b\uff1b2\uff09\u5bf9\u68c0\u6d4b\u6846\u8fdb\u884c\u6392\u5e8f\u4e0e\u88c1\u526a\uff0c\u751f\u6210\u5355\u4e2a\u7532\u866b\u56fe\u50cf\uff1b3\uff09\u624b\u5de5\u6807\u6ce8670\u5f20\u7532\u866b\u56fe\u50cf\u5e76\u5fae\u8c03\u4e24\u79cdTransformer\u5206\u5272\u6a21\u578b\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5f62\u6001\u5206\u5272\u3002", "result": "\u901a\u8fc7\u5fae\u8c03\u7684\u5206\u5272\u6a21\u578b\u53d6\u5f97\u8f83\u9ad8\u7cbe\u5ea6\u7684\u7532\u866b\u7ec6\u7c92\u5ea6\u5206\u5272\uff1b\u68c0\u6d4b\u9636\u6bb5\u91c7\u7528\u8fed\u4ee3\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u4e86\u68c0\u6d4b\u8986\u76d6\u7387\uff1b\u6574\u4f53\u6d41\u6c34\u7ebf\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u7532\u866b\u6570\u636e\u5904\u7406\u6548\u7387\uff0c\u5229\u4e8e\u751f\u7269\u5b66\u7814\u7a76\u52a0\u901f\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u5957\u9488\u5bf9\u6258\u76d8\u4e0a\u5927\u91cf\u7532\u866b\u56fe\u50cf\u7684\u81ea\u52a8\u5316\u5904\u7406\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u68c0\u6d4b\u3001\u88c1\u526a\u548c\u5f62\u6001\u5206\u5272\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u7532\u866b\u56fe\u50cf\u5904\u7406\u6548\u7387\u3002"}}
{"id": "2511.00260", "categories": ["cs.CV", "68T07 (Primary) 68T45, 92C55 (Secondary)"], "pdf": "https://arxiv.org/pdf/2511.00260", "abs": "https://arxiv.org/abs/2511.00260", "authors": ["Linzhe Jiang", "Jiayuan Huang", "Sophia Bano", "Matthew J. Clarkson", "Zhehua Mao", "Mobarak I. Hoque"], "title": "MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba", "comment": "12 pages, 4 figures, 3 tables, IPCAI conference", "summary": "Accurate 3D point cloud registration underpins reliable image-guided\ncolonoscopy, directly affecting lesion localization, margin assessment, and\nnavigation safety. However, biological tissue exhibits repetitive textures and\nlocally homogeneous geometry that cause feature degeneracy, while substantial\ndomain shifts between pre-operative anatomy and intra-operative observations\nfurther degrade alignment stability. To address these clinically critical\nchallenges, we introduce a novel 3D registration method tailored for endoscopic\nnavigation and a high-quality, clinically grounded dataset to support rigorous\nand reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale\nbenchmark dataset with 10,014 geometrically aligned point cloud pairs derived\nfrom clinical CT data. We propose MambaNetLK, a novel correspondence-free\nregistration framework, which enhances the PointNetLK architecture by\nintegrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.\nAs a result, the proposed framework efficiently captures long-range\ndependencies with linear-time complexity. The alignment is achieved iteratively\nusing the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,\nMambaNetLK achieves the best performance compared with the state-of-the-art\nmethods, reducing median rotation error by 56.04% and RMSE translation error by\n26.19% over the second-best method. The model also demonstrates strong\ngeneralization on ModelNet40 and superior robustness to initial pose\nperturbations. MambaNetLK provides a robust foundation for 3D registration in\nsurgical navigation. The combination of a globally expressive SSM-based feature\nextractor and a large-scale clinical dataset enables more accurate and reliable\nguidance systems in minimally invasive procedures like colonoscopy.", "AI": {"tldr": "\u63d0\u51faMambaNetLK\u548c\u5927\u89c4\u6a21\u4e34\u5e8a\u70b9\u4e91\u914d\u51c6\u57fa\u51c6C3VD-Raycasting-10k\uff0c\u7ed3\u5408SSM\u7279\u5f81\u63d0\u53d6\u4e0eLK\u8fed\u4ee3\u5bf9\u9f50\uff0c\u5728\u4e34\u5e8a\u6570\u636e\u4e0a\u663e\u8457\u63d0\u5347\u914d\u51c6\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u5185\u955c\u7ec4\u7ec7\u7eb9\u7406\u91cd\u590d\u3001\u5c40\u90e8\u51e0\u4f55\u540c\u8d28\u5bfc\u81f4\u7279\u5f81\u9000\u5316\uff0c\u672f\u524d\u4e0e\u672f\u4e2d\u6570\u636e\u57df\u5dee\u5f02\u5927\uff0c\u5f71\u54cd\u70b9\u4e91\u914d\u51c6\u7a33\u5b9a\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u914d\u51c6\u65b9\u6cd5\u548c\u4e34\u5e8a\u7ea7\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u5728PointNetLK\u57fa\u7840\u4e0a\u5f15\u5165Mamba\u72b6\u6001\u7a7a\u95f4\u6a21\u578b(SSM)\u4f5c\u4e3a\u8de8\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5229\u7528\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\uff0c\u914d\u5408Lucas-Kanade\u8fed\u4ee3\u5bf9\u9f50\u3002", "result": "\u5728C3VD-Raycasting-10k\u6570\u636e\u96c6\u4e0a\uff0cMambaNetLK\u8f83\u7b2c\u4e8c\u540d\u5c06\u4e2d\u4f4d\u65cb\u8f6c\u8bef\u5dee\u51cf\u5c1156.04%\uff0cRMSE\u5e73\u79fb\u8bef\u5dee\u51cf\u5c1126.19%\uff1b\u5728ModelNet40\u4e0a\u6cdb\u5316\u826f\u597d\uff0c\u5bf9\u521d\u59cb\u4f4d\u59ff\u6270\u52a8\u66f4\u9c81\u68d2\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7528\u4e8e\u5185\u955c\u5bfc\u822a\u7684\u5bf9\u5e94\u65e0\u5173\u7684\u914d\u51c6\u6846\u67b6MambaNetLK\uff0c\u5e76\u53d1\u5e03\u4e86\u4e34\u5e8a\u6570\u636e\u96c6C3VD-Raycasting-10k\u3002"}}
{"id": "2511.00261", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00261", "abs": "https://arxiv.org/abs/2511.00261", "authors": ["Neha Balamurugan", "Sarah Wu", "Adam Chun", "Gabe Gaw", "Cristobal Eyzaguirre", "Tobias Gerstenberg"], "title": "Spot The Ball: A Benchmark for Visual Social Inference", "comment": null, "summary": "Humans excel at visual social inference, the ability to infer hidden elements\nof a scene from subtle behavioral cues such as other people's gaze, pose, and\norientation. This ability drives everyday social reasoning in humans and is\ncritical for developing more human-like AI agents. We introduce Spot The Ball,\na challenging benchmark for evaluating visual social inference in\nvision-language models (VLMs) using sports as a test domain. The task is to\nlocalize a removed sports ball from soccer, basketball, and volleyball images.\nWe present a curated evaluation set with human baselines and a scalable\npipeline for generating additional test items. We evaluate four\nstate-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting\nstrategies, finding that humans are consistently two to three times more\naccurate (20-34%) than models ($\\leq$ 17%) across all sports. Our analyses show\nthat models rely on superficial spatial heuristics--such as guessing near the\nimage center or nearby players--while humans leverage social cues like gaze\ndirection and body pose. These findings reveal a persistent human-model gap in\nvisual social reasoning and underscore the need for architectures that\nexplicitly encode structured behavioral cues to achieve robust, human-like\ninference.", "AI": {"tldr": "\u63d0\u51faSpot The Ball\u57fa\u51c6\uff0c\u663e\u793a\u73b0\u6709VLM\u5728\u5229\u7528\u793e\u4ea4\u884c\u4e3a\u7ebf\u7d22\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u843d\u540e\u4e8e\u4eba\u7c7b\uff0c\u6a21\u578b\u591a\u4f9d\u8d56\u7b80\u5355\u7a7a\u95f4\u542f\u53d1\u5f0f\uff0c\u9700\u65b0\u67b6\u6784\u663e\u5f0f\u7f16\u7801\u884c\u4e3a\u7ed3\u6784\u4fe1\u606f\u4ee5\u7f29\u5c0f\u5dee\u8ddd\u3002", "motivation": "\u901a\u8fc7\u4f53\u80b2\u573a\u666f\u6d4b\u8bd5\u89c6\u89c9\u793e\u4ea4\u63a8\u7406\uff0c\u8bc4\u4f30\u6a21\u578b\u80fd\u5426\u4ece\u51dd\u89c6\u3001\u59ff\u6001\u7b49\u884c\u4e3a\u7ebf\u7d22\u63a8\u65ad\u88ab\u906e\u6321\u6216\u79fb\u9664\u7684\u7269\u4f53\u4f4d\u7f6e\uff0c\u4ee5\u63a8\u52a8\u66f4\u7c7b\u4eba\u5316\u7684AI\u4ee3\u7406\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u201c\u627e\u7403\u201d\u57fa\u51c6\uff1a\u5220\u9664\u4f53\u80b2\u56fe\u7247\u4e2d\u7684\u7403\uff0c\u8981\u6c42\u6a21\u578b\u5b9a\u4f4d\u7403\u4f4d\u7f6e\uff1b\u63d0\u4f9b\u4eba\u5de5\u57fa\u7ebf\u5e76\u8bbe\u8ba1\u53ef\u6269\u5c55\u7684\u6570\u636e\u751f\u6210\u7ba1\u7ebf\uff1b\u7528\u4e09\u79cd\u63d0\u793a\u7b56\u7565\u8bc4\u4f30\u56db\u4e2a\u9886\u5148VLM\uff08Gemini\u3001GPT\u3001LLaMA\u3001Qwen\uff09\u3002", "result": "\u4eba\u7c7b\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u6a21\u578b\uff0820\u201334% vs \u226417%\uff09\uff1b\u6a21\u578b\u503e\u5411\u4e8e\u4e2d\u5fc3\u6216\u9760\u8fd1\u7403\u5458\u7b49\u542f\u53d1\u5f0f\u731c\u6d4b\uff0c\u4eba\u7c7b\u5229\u7528\u89c6\u7ebf\u548c\u8eab\u4f53\u59ff\u6001\u7b49\u793e\u4ea4\u7ebf\u7d22\u3002", "conclusion": "\u8bba\u6587\u8868\u660e\u5f53\u524d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u8fdc\u4e0d\u53ca\u4eba\u7c7b\uff0c\u4e14\u4f9d\u8d56\u8868\u9762\u7a7a\u95f4\u542f\u53d1\u5f0f\u800c\u975e\u884c\u4e3a\u7ebf\u7d22\u3002"}}
{"id": "2511.00269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00269", "abs": "https://arxiv.org/abs/2511.00269", "authors": ["Long Li", "Jiajia Li", "Dong Chen", "Lina Pu", "Haibo Yao", "Yanbo Huang"], "title": "FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture", "comment": null, "summary": "Accurate classification plays a pivotal role in smart agriculture, enabling\napplications such as crop monitoring, fruit recognition, and pest detection.\nHowever, conventional centralized training often requires large-scale data\ncollection, which raises privacy concerns, while standard federated learning\nstruggles with non-independent and identically distributed (non-IID) data and\nincurs high communication costs. To address these challenges, we propose a\nfederated learning framework that integrates a frozen Contrastive\nLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight\ntransformer classifier. By leveraging the strong feature extraction capability\nof the pre-trained CLIP ViT, the framework avoids training large-scale models\nfrom scratch and restricts federated updates to a compact classifier, thereby\nreducing transmission overhead significantly. Furthermore, to mitigate\nperformance degradation caused by non-IID data distribution, a small subset\n(1%) of CLIP-extracted feature representations from all classes is shared\nacross clients. These shared features are non-reversible to raw images,\nensuring privacy preservation while aligning class representation across\nparticipants. Experimental results on agricultural classification tasks show\nthat the proposed method achieve 86.6% accuracy, which is more than 4 times\nhigher compared to baseline federated learning approaches. This demonstrates\nthe effectiveness and efficiency of combining vision-language model features\nwith federated learning for privacy-preserving and scalable agricultural\nintelligence.", "AI": {"tldr": "\u7528\u51bb\u7ed3\u7684CLIP ViT\u63d0\u7279\u5f81\u3001\u53ea\u8054\u90a6\u8bad\u7ec3\u5c0f\u578bTransformer\u5206\u7c7b\u5668\u5e76\u5171\u4eab\u6bcf\u7c7b1%\u4e0d\u53ef\u9006\u7279\u5f81\uff0c\u80fd\u5728\u975eIID\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u519c\u4e1a\u5206\u7c7b\u6027\u80fd\u5e76\u964d\u4f4e\u901a\u4fe1\u4e0e\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u5728\u519c\u4e1a\u667a\u80fd\u4e2d\u9700\u8981\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u4f46\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u5e26\u6765\u9690\u79c1\u95ee\u9898\uff0c\u6807\u51c6\u8054\u90a6\u5b66\u4e60\u5728\u975eIID\u6570\u636e\u4e0b\u6027\u80fd\u5dee\u4e14\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u6545\u9700\u4e00\u79cd\u4fdd\u9690\u79c1\u3001\u4f4e\u901a\u4fe1\u3001\u9ad8\u6548\u4e14\u9002\u5e94\u975eIID\u6570\u636e\u7684\u65b9\u6848\u3002", "method": "\u51bb\u7ed3\u9884\u8bad\u7ec3\u7684CLIP ViT\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u4ec5\u5728\u5ba2\u6237\u7aef/\u670d\u52a1\u5668\u4e4b\u95f4\u4f20\u8f93\u548c\u8054\u5408\u8bad\u7ec3\u4e00\u4e2a\u5c0f\u578bTransformer\u5206\u7c7b\u5668\uff1b\u6b64\u5916\u5171\u4eab\u6bcf\u7c7b1%\u7684CLIP\u7279\u5f81\u5b50\u96c6\u4ee5\u5bf9\u9f50\u7c7b\u522b\u8868\u793a\uff0c\u4ece\u800c\u7f13\u89e3\u975eIID\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\u3002", "result": "\u5728\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b086.6%\u51c6\u786e\u7387\uff0c\u76f8\u8f83\u4e8e\u57fa\u7ebf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u8d85\u8fc74\u500d\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5c06\u51bb\u7ed3\u7684CLIP ViT\u4e0e\u8f7b\u91cf\u7ea7Transformer\u5206\u7c7b\u5668\u7ed3\u5408\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u5e76\u7f13\u89e3\u975eIID\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u519c\u4e1a\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u7387\u5e76\u63d0\u9ad8\u6548\u7387\u3002"}}
{"id": "2511.00293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00293", "abs": "https://arxiv.org/abs/2511.00293", "authors": ["Hengjia Li", "Jianjin Xu", "Keli Cheng", "Lei Wang", "Ning Bi", "Boxi Wu", "Fernando De la Torre", "Deng Cai"], "title": "Multi-View Consistent Human Image Customization via In-Context Learning", "comment": null, "summary": "Recent advances in personalized generative models demonstrate impressive\nresults in creating identity-consistent images of the same person under diverse\nsettings. Yet, we note that most methods cannot control the viewpoint of the\ngenerated image, nor generate consistent multiple views of the person. To\naddress this problem, we propose a lightweight adaptation method, PersonalView,\ncapable of enabling an existing model to acquire multi-view generation\ncapability with as few as 100 training samples. PersonalView consists of two\nkey components: First, we design a conditioning architecture to take advantage\nof the in-context learning ability of the pre-trained diffusion transformer.\nSecond, we preserve the original generative ability of the pretrained model\nwith a new Semantic Correspondence Alignment Loss. We evaluate the multi-view\nconsistency, text alignment, identity similarity, and visual quality of\nPersonalView and compare it to recent baselines with potential capability of\nmulti-view customization. PersonalView significantly outperforms baselines\ntrained on a large corpus of multi-view data with only 100 training samples.", "AI": {"tldr": "\u63d0\u51faPersonalView\uff0c\u901a\u8fc7\u6761\u4ef6\u5316\u7ed3\u6784\u4e0e\u8bed\u4e49\u5bf9\u5e94\u5bf9\u9f50\u635f\u5931\uff0c\u5728\u4ec5100\u5f20\u6837\u672c\u4e0b\u5b9e\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u591a\u89c6\u89d2\u4e2a\u6027\u5316\u751f\u6210\uff0c\u6027\u80fd\u8d85\u8d8a\u591a\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u63a7\u5236\u751f\u6210\u56fe\u50cf\u7684\u89c6\u89d2\uff0c\u4e5f\u96be\u4ee5\u751f\u6210\u4e00\u81f4\u7684\u591a\u89c6\u89d2\u4eba\u7269\uff0c\u9700\u4e00\u79cd\u80fd\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u591a\u89c6\u89d2\u5b9a\u5236\u7684\u9ad8\u6548\u9002\u914d\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u6761\u4ef6\u5316\u7ed3\u6784\u4ee5\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563Transformer\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u5bf9\u5e94\u5bf9\u9f50\u635f\u5931\u4ee5\u4fdd\u6301\u539f\u59cb\u751f\u6210\u80fd\u529b\uff1b\u4ec5\u5bf9\u5c11\u91cf\u53c2\u6570\u8fdb\u884c\u9002\u914d\u3002", "result": "\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3001\u6587\u672c\u5bf9\u9f50\u3001\u8eab\u4efd\u4fdd\u6301\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u8bc4\u6d4b\u4e2d\uff0cPersonalView\u5728\u4ec5100\u5f20\u8bad\u7ec3\u6837\u672c\u7684\u6761\u4ef6\u4e0b\u663e\u8457\u4f18\u4e8e\u4f7f\u7528\u5927\u91cf\u591a\u89c6\u89d2\u6570\u636e\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86PersonalView\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4e2a\u6027\u5316\u9002\u914d\u65b9\u6cd5\uff0c\u80fd\u5728\u4ec5100\u5f20\u6837\u672c\u4e0b\u8d4b\u4e88\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u591a\u89c6\u89d2\u751f\u6210\u529f\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u3001\u6587\u672c\u5bf9\u9f50\u3001\u8eab\u4efd\u76f8\u4f3c\u5ea6\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u3002"}}
{"id": "2511.00328", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00328", "abs": "https://arxiv.org/abs/2511.00328", "authors": ["Isai Daniel Chac\u00f3n", "Paola Ruiz Puentes", "Jillian Pearse", "Pablo Arbel\u00e1ez"], "title": "Towards Automated Petrography", "comment": null, "summary": "Petrography is a branch of geology that analyzes the mineralogical\ncomposition of rocks from microscopical thin section samples. It is essential\nfor understanding rock properties across geology, archaeology, engineering,\nmineral exploration, and the oil industry. However, petrography is a\nlabor-intensive task requiring experts to conduct detailed visual examinations\nof thin section samples through optical polarization microscopes, thus\nhampering scalability and highlighting the need for automated techniques. To\naddress this challenge, we introduce the Large-scale Imaging and Thin section\nOptical-polarization Set (LITHOS), the largest and most diverse publicly\navailable experimental framework for automated petrography. LITHOS includes\n211,604 high-resolution RGB patches of polarized light and 105,802\nexpert-annotated grains across 25 mineral categories. Each annotation consists\nof the mineral class, spatial coordinates, and expert-defined major and minor\naxes represented as intersecting vector paths, capturing grain geometry and\norientation. We evaluate multiple deep learning techniques for mineral\nclassification in LITHOS and propose a dual-encoder transformer architecture\nthat integrates both polarization modalities as a strong baseline for future\nreference. Our method consistently outperforms single-polarization models,\ndemonstrating the value of polarization synergy in mineral classification. We\nhave made the LITHOS Benchmark publicly available, comprising our dataset,\ncode, and pretrained models, to foster reproducibility and further research in\nautomated petrographic analysis.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u516c\u5f00LITHOS\u5927\u89c4\u6a21\u504f\u5149\u8584\u7247\u6570\u636e\u96c6\uff08211k\u56fe\u50cf\u5757\u3001106k\u9897\u7c92\u6807\u6ce8\u300125\u7c7b\u77ff\u7269\uff09\uff0c\u5e76\u7ed9\u51fa\u4e00\u4e2a\u878d\u5408\u4e24\u79cd\u504f\u5149\u6a21\u6001\u7684\u53cc\u7f16\u7801\u5668Transformer\u57fa\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u624b\u5de5\u8584\u7247\u5ca9\u76f8\u5b66\u8017\u65f6\u4e14\u9700\u4e13\u5bb6\u7ecf\u9a8c\uff0c\u9650\u5236\u4e86\u89c4\u6a21\u5316\u5e94\u7528\uff1b\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u6280\u672f\u548c\u516c\u5f00\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u4e0e\u7814\u7a76\u53d1\u5c55\u3002", "method": "\u6536\u96c6\u5e76\u6807\u6ce8211,604\u5f20\u9ad8\u5206\u8fa8\u7387RGB\u504f\u5149\u56fe\u50cf\u5757\u548c105,802\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u9897\u7c92\uff0825\u7c7b\u77ff\u7269\uff09\uff0c\u6bcf\u4e2a\u6807\u6ce8\u5305\u542b\u7c7b\u522b\u3001\u7a7a\u95f4\u5750\u6807\u53ca\u8868\u793a\u9897\u7c92\u51e0\u4f55\u4e0e\u65b9\u5411\u7684\u4e3b\u6b21\u8f74\u5411\u91cf\u8def\u5f84\uff1b\u8bc4\u4f30\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5e76\u63d0\u51fa\u4e00\u4e2a\u6574\u5408\u4e24\u79cd\u504f\u5149\u6a21\u6001\u7684\u53cc\u7f16\u7801\u5668Transformer\u67b6\u6784\u4f5c\u4e3a\u5f3a\u57fa\u7ebf\u3002", "result": "LITHOS\u6210\u4e3a\u76ee\u524d\u6700\u5927\u4e14\u6700\u5177\u591a\u6837\u6027\u7684\u516c\u5f00\u8584\u7247\u504f\u5149\u56fe\u50cf\u4e0e\u6807\u6ce8\u57fa\u51c6\uff0c\u53cc\u7f16\u7801\u5668Transformer\u5728\u77ff\u7269\u5206\u7c7b\u4e0a\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\u6a21\u578b\uff0c\u8868\u660e\u504f\u5149\u6a21\u6001\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002\u6570\u636e\u96c6\u3001\u4ee3\u7801\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u5df2\u516c\u5f00\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u504f\u5149\u663e\u5fae\u8584\u7247\u56fe\u50cf\u6570\u636e\u96c6LITHOS\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7f16\u7801\u5668Transformer\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u7ed3\u5408\u4e24\u79cd\u504f\u5149\u6a21\u6001\u80fd\u63d0\u9ad8\u77ff\u7269\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2511.00335", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00335", "abs": "https://arxiv.org/abs/2511.00335", "authors": ["Weidong Zhang", "Pak Lun Kevin Ding", "Huan Liu"], "title": "Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models", "comment": "10 pages, 5 tables, 1 figure, 3 equations, 11 mobile models, 7\n  datasets", "summary": "Lightweight vision classification models such as MobileNet, ShuffleNet, and\nEfficientNet are increasingly deployed in mobile and embedded systems, yet\ntheir performance has been predominantly benchmarked on ImageNet. This raises\ncritical questions: Do models that excel on ImageNet also generalize across\nother domains? How can cross-dataset robustness be systematically quantified?\nAnd which architectural elements consistently drive generalization under tight\nresource constraints? Here, we present the first systematic evaluation of 11\nlightweight vision models (2.5M parameters), trained under a fixed 100-epoch\nschedule across 7 diverse datasets. We introduce the Cross-Dataset Score\n(xScore), a unified metric that quantifies the consistency and robustness of\nmodel performance across diverse visual domains. Our results show that (1)\nImageNet accuracy does not reliably predict performance on fine-grained or\nmedical datasets, (2) xScore provides a scalable predictor of mobile model\nperformance that can be estimated from just four datasets, and (3) certain\narchitectural components--such as isotropic convolutions with higher spatial\nresolution and channel-wise attention--promote broader generalization, while\nTransformer-based blocks yield little additional benefit, despite incurring\nhigher parameter overhead. This study provides a reproducible framework for\nevaluating lightweight vision models beyond ImageNet, highlights key design\nprinciples for mobile-friendly architectures, and guides the development of\nfuture models that generalize robustly across diverse application domains.", "AI": {"tldr": "TL;DR\uff1a\u5728\u79fb\u52a8/\u5d4c\u5165\u5f0f\u573a\u666f\u4e0b\uff0cImageNet\u4e0d\u8db3\u4ee5\u4ee3\u8868\u6a21\u578b\u8de8\u57df\u6cdb\u5316\uff0cxScore\u662f\u66f4\u597d\u7684\u7edf\u4e00\u5ea6\u91cf\uff1b\u4fdd\u6301\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u5404\u5411\u540c\u6027\u5377\u79ef\u548c\u901a\u9053\u6ce8\u610f\u529b\u6bd4\u5f15\u5165Transformer\u6a21\u5757\u66f4\u6709\u52a9\u4e8e\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u5e7f\u6cdb\u6cdb\u5316\u3002", "motivation": "\u52a8\u673a\uff1a\u5f53\u524d\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e3b\u8981\u4ee5ImageNet\u4f5c\u4e3a\u6027\u80fd\u57fa\u51c6\uff0c\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u79fb\u52a8/\u5d4c\u5165\u5f0f\u5e94\u7528\u4e2d\u9762\u5bf9\u591a\u6837\u5316\u89c6\u89c9\u57df\uff08\u5982\u7ec6\u7c92\u5ea6\u3001\u533b\u7597\uff09\u65f6\u7684\u6cdb\u5316\u80fd\u529b\uff1b\u9700\u8981\u4e00\u4e2a\u7cfb\u7edf\u5316\u8bc4\u4f30\u6846\u67b6\u4e0e\u7edf\u4e00\u6307\u6807\u6765\u8861\u91cf\u8de8\u6570\u636e\u96c6\u9c81\u68d2\u6027\u5e76\u6307\u5bfc\u79fb\u52a8\u53cb\u597d\u67b6\u6784\u8bbe\u8ba1\u3002", "method": "\u65b9\u6cd5\uff1a\u5bf911\u4e2a\u7ea62.5M\u53c2\u6570\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5728\u7edf\u4e00\u7684100\u8f6e\u8bad\u7ec3\u8ba1\u5212\u4e0b\uff0c\u57287\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff1b\u63d0\u51faxScore\u4f5c\u4e3a\u7edf\u4e00\u6307\u6807\uff0c\u8861\u91cf\u6a21\u578b\u5728\u4e0d\u540c\u89c6\u89c9\u57df\u95f4\u7684\u4e00\u81f4\u6027\u4e0e\u9c81\u68d2\u6027\uff1b\u901a\u8fc7\u6d88\u878d\u548c\u7ed3\u6784\u5206\u6790\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u7ec4\u4ef6\uff08\u5377\u79ef\u7c7b\u578b\u3001\u5206\u8fa8\u7387\u3001\u6ce8\u610f\u529b\u3001Transformer\u5757\uff09\u7684\u5f71\u54cd\uff1b\u8fd8\u8bc4\u4f30\u4e86xScore\u5728\u51cf\u5c11\u6570\u636e\u96c6\u6570\u91cf\u65f6\u7684\u53ef\u9884\u6d4b\u6027\u3002", "result": "\u7ed3\u679c\uff1a1) ImageNet\u51c6\u786e\u7387\u4e0e\u7ec6\u7c92\u5ea6/\u533b\u7597\u6570\u636e\u96c6\u8868\u73b0\u76f8\u5173\u6027\u8f83\u5f31\uff1b2) xScore\u80fd\u4f5c\u4e3a\u79fb\u52a8\u6a21\u578b\u8de8\u57df\u8868\u73b0\u7684\u53ef\u6269\u5c55\u9884\u6d4b\u5668\uff0c\u4e14\u53ea\u97004\u4e2a\u6570\u636e\u96c6\u5373\u53ef\u4f30\u7b97\uff1b3) \u67b6\u6784\u5206\u6790\u663e\u793a\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5404\u5411\u540c\u6027\u5377\u79ef\u4e0e\u901a\u9053\u6ce8\u610f\u529b\u663e\u8457\u63d0\u5347\u8de8\u57df\u6cdb\u5316\uff0cTransformer\u5757\u5e26\u6765\u7684\u589e\u76ca\u6709\u9650\u4e14\u53c2\u6570\u5f00\u9500\u8f83\u5927\u3002", "conclusion": "\u8bba\u6587\u7ed3\u8bba\uff1a\u5728\u8f7b\u91cf\u7ea7\u89c6\u89c9\u5206\u7c7b\u6a21\u578b\u4e2d\uff0cImageNet\u51c6\u786e\u7387\u4e0d\u80fd\u53ef\u9760\u9884\u6d4b\u5728\u7ec6\u7c92\u5ea6\u6216\u533b\u7597\u7b49\u5176\u4ed6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff1b\u5f15\u5165\u7684Cross-Dataset Score\uff08xScore\uff09\u53ef\u91cf\u5316\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u4ece\u56db\u4e2a\u6570\u636e\u96c6\u4f30\u7b97\u79fb\u52a8\u6a21\u578b\u7684\u6574\u4f53\u8868\u73b0\uff1b\u67d0\u4e9b\u67b6\u6784\u7ec4\u4ef6\uff08\u5982\u66f4\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u5404\u5411\u540c\u6027\u5377\u79ef\u548c\u901a\u9053\u6ce8\u610f\u529b\uff09\u6709\u52a9\u4e8e\u66f4\u5e7f\u6cdb\u7684\u6cdb\u5316\uff0c\u800cTransformer\u6a21\u5757\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u7f6e\u4e0b\u76ca\u5904\u6709\u9650\u3002"}}
{"id": "2511.00338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00338", "abs": "https://arxiv.org/abs/2511.00338", "authors": ["Yuhao Fang", "Zijian Wang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction", "comment": null, "summary": "This work presents a novel hybrid approach that integrates Deep Operator\nNetworks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex\ninverse problem. The method effectively addresses tasks such as source\nlocalization governed by the Navier-Stokes equations and image reconstruction,\novercoming challenges related to nonlinearity, sparsity, and noisy data. By\nincorporating physics-informed constraints and task-specific regularization\ninto the loss function, the framework ensures solutions that are both\nphysically consistent and accurate. Validation on diverse synthetic and real\ndatasets demonstrates its robustness, scalability, and precision, showcasing\nits broad potential applications in computational physics and imaging sciences.", "AI": {"tldr": "\u5c06DeepONet\u4e0eNTK\u76f8\u7ed3\u5408\u5e76\u5f15\u5165\u7269\u7406\u635f\u5931\u4e0e\u4efb\u52a1\u6b63\u5219\u5316\uff0c\u53ef\u6709\u6548\u89e3\u51b3\u975e\u7ebf\u6027\u3001\u7a00\u758f\u4e0e\u566a\u58f0\u4e0b\u7684\u9006\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u6d41\u4f53\u6e90\u5b9a\u4f4d\u4e0e\u56fe\u50cf\u91cd\u5efa\uff0c\u5e76\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u5176\u7a33\u5065\u6027\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u76ee\u6807\u662f\u514b\u670d\u4f20\u7edf\u9006\u95ee\u9898\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5ea6\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff08\u4f8b\u5982Navier\u2013Stokes\u9a71\u52a8\u7684\u95ee\u9898\uff09\u3001\u7a00\u758f\u89c2\u6d4b\u548c\u566a\u58f0\u65f6\u7684\u56f0\u96be\uff0c\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7b97\u5b50\u5b66\u4e60\u4e0e\u7406\u8bba\u4e0a\u53ef\u89e3\u91ca\u7684\u6838\u65b9\u6cd5\u6765\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u65b9\u6cd5\u901a\u8fc7\u5c06Deep Operator Network\u7528\u4e8e\u5b66\u4e60\u7b97\u5b50\u6620\u5c04\uff0c\u5e76\u5229\u7528Neural Tangent Kernel\u5206\u6790\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u548c\u505a\u6b63\u5219\u5316\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\uff08\u5982Navier\u2013Stokes\u65b9\u7a0b\uff09\u4e0e\u4efb\u52a1\u7279\u5b9a\u6b63\u5219\u9879\uff0c\u5171\u540c\u6784\u5efa\u635f\u5931\u51fd\u6570\u3002\u8bad\u7ec3\u65f6\u5f15\u5165NTK\u76f8\u5173\u9879\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3001\u63d0\u9ad8\u6cdb\u5316\uff0c\u5e76\u5728\u635f\u5931\u4e2d\u52a0\u5165\u7a00\u758f/\u5148\u9a8c\u7ea6\u675f\u4ee5\u5904\u7406\u6b20\u5b9a\u6216\u566a\u58f0\u89c2\u6d4b\u3002", "result": "\u5728\u82e5\u5e72\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4efb\u52a1\u5305\u62ec\u57fa\u4e8eNavier\u2013Stokes\u7684\u6e90\u5b9a\u4f4d\u548c\u56fe\u50cf\u91cd\u5efa\u3002\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8bef\u5dee\u3001\u9c81\u68d2\u6027\uff08\u5bf9\u566a\u58f0\u4e0e\u7a00\u758f\u89c2\u6d4b\uff09\u4ee5\u53ca\u53ef\u6269\u5c55\u6027\u65b9\u9762\u4f18\u4e8e\u82e5\u5e72\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u91cd\u5efa\u89e3\u6ee1\u8db3\u7269\u7406\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06DeepONet\u4e0eNTK\u7ed3\u5408\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u590d\u6742\u9006\u95ee\u9898\uff0c\u80fd\u591f\u5728\u975e\u7ebf\u6027\u3001\u7a00\u758f\u548c\u566a\u58f0\u6570\u636e\u6761\u4ef6\u4e0b\u6062\u590d\u7269\u7406\u573a\u6216\u56fe\u50cf\uff0c\u4e14\u5728\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u7a33\u5065\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2511.00344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00344", "abs": "https://arxiv.org/abs/2511.00344", "authors": ["Xihang Qiu", "Jiarong Cheng", "Yuhao Fang", "Wanpeng Zhang", "Yao Lu", "Ye Zhang", "Chun Li"], "title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities", "comment": null, "summary": "Multimodal Emotion Recognition in Conversations (MERC) enhances emotional\nunderstanding through the fusion of multimodal signals. However, unpredictable\nmodality absence in real-world scenarios significantly degrades the performance\nof existing methods. Conventional missing-modality recovery approaches, which\ndepend on training with complete multimodal data, often suffer from semantic\ndistortion under extreme data distributions, such as fixed-modality absence. To\naddress this, we propose the Federated Dialogue-guided and Semantic-Consistent\nDiffusion (FedDISC) framework, pioneering the integration of federated learning\ninto missing-modality recovery. By federated aggregation of modality-specific\ndiffusion models trained on clients and broadcasting them to clients missing\ncorresponding modalities, FedDISC overcomes single-client reliance on modality\ncompleteness. Additionally, the DISC-Diffusion module ensures consistency in\ncontext, speaker identity, and semantics between recovered and available\nmodalities, using a Dialogue Graph Network to capture conversational\ndependencies and a Semantic Conditioning Network to enforce semantic alignment.\nWe further introduce a novel Alternating Frozen Aggregation strategy, which\ncyclically freezes recovery and classifier modules to facilitate collaborative\noptimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI\ndatasets demonstrate that FedDISC achieves superior emotion classification\nperformance across diverse missing modality patterns, outperforming existing\napproaches.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u8054\u90a6\u5b66\u4e60\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u6269\u6563\u6062\u590d\u76f8\u7ed3\u5408\u7684FedDISC\uff0c\u7528\u5bf9\u8bdd\u56fe\u4e0e\u8bed\u4e49\u6761\u4ef6\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u4ea4\u66ff\u51bb\u7ed3\u805a\u5408\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u5728\u7f3a\u5931\u6a21\u6001\u60c5\u5883\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\u6a21\u6001\u53ef\u80fd\u7f3a\u5931\uff0c\u4e14\u4f20\u7edf\u4f9d\u8d56\u5b8c\u6574\u6a21\u6001\u8bad\u7ec3\u7684\u6062\u590d\u65b9\u6cd5\u5728\u6781\u7aef\u5206\u5e03\uff08\u5982\u56fa\u5b9a\u6a21\u6001\u7f3a\u5931\uff09\u4e0b\u4f1a\u4ea7\u751f\u8bed\u4e49\u5931\u771f\uff0c\u5355\u5ba2\u6237\u7aef\u65b9\u6cd5\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u5f15\u5165\u8054\u90a6\u5b66\u4e60\u89e3\u51b3\u6570\u636e\u5206\u5e03\u4e0e\u9690\u79c1\u95ee\u9898\uff0c\u63d0\u9ad8\u6062\u590d\u7684\u6cdb\u5316\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "method": "\u5728\u5ba2\u6237\u7aef\u8bad\u7ec3\u6a21\u6001\u4e13\u5c5e\u6269\u6563\u6062\u590d\u6a21\u578b\u5e76\u8fdb\u884c\u8054\u90a6\u805a\u5408\uff0c\u5411\u7f3a\u5931\u8be5\u6a21\u6001\u7684\u5ba2\u6237\u7aef\u5e7f\u64ad\uff1b\u5f15\u5165DISC-Diffusion\u6a21\u5757\uff0c\u7ed3\u5408\u5bf9\u8bdd\u56fe\u7f51\u7edc\u5efa\u6a21\u4f1a\u8bdd\u4f9d\u8d56\u3001\u8bed\u4e49\u6761\u4ef6\u7f51\u7edc\u8fdb\u884c\u8bed\u4e49\u5bf9\u9f50\uff1b\u91c7\u7528\u4ea4\u66ff\u51bb\u7ed3\u805a\u5408\u7b56\u7565\u4ea4\u66ff\u51bb\u7ed3\u6062\u590d\u5668\u548c\u5206\u7c7b\u5668\u4ee5\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5728IEMOCAP\u3001CMUMOSI\u548cCMUMOSEI\u6570\u636e\u96c6\u4e0a\uff0cFedDISC\u5728\u591a\u79cd\u6a21\u6001\u7f3a\u5931\u6a21\u5f0f\u4e0b\u663e\u8457\u63d0\u5347\u60c5\u7eea\u5206\u7c7b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u7f3a\u5931\u6a21\u6001\u6062\u590d\u4e0e\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u3002", "conclusion": "FedDISC\u901a\u8fc7\u8054\u90a6\u5b66\u4e60\u6574\u5408\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u7f3a\u5931\u6a21\u6001\u6062\u590d\u5e76\u63d0\u5347\u5bf9\u8bdd\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\uff1b\u91c7\u7528\u5bf9\u8bdd\u56fe\u7f51\u7edc\u4e0e\u8bed\u4e49\u6761\u4ef6\u7f51\u7edc\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff1b\u4ea4\u66ff\u51bb\u7ed3\u805a\u5408\u7b56\u7565\u4fc3\u8fdb\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u79cd\u7f3a\u5931\u6a21\u5f0f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.00345", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00345", "abs": "https://arxiv.org/abs/2511.00345", "authors": ["Amir Ziashahabi", "Narges Ghasemi", "Sajjad Shahabi", "John Krumm", "Salman Avestimehr", "Cyrus Shahabi"], "title": "OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data", "comment": "Accepted at NeurIPS 2025 UrbanAI Workshop", "summary": "Accurate and up-to-date geospatial data are essential for urban planning,\ninfrastructure monitoring, and environmental management. Yet, automating urban\nmonitoring remains difficult because curated datasets of specific urban\nfeatures and their changes are scarce. We introduce OSMGen, a generative\nframework that creates realistic satellite imagery directly from raw\nOpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen\nuses the full richness of OSM JSON, including vector geometries, semantic tags,\nlocation, and time, giving fine-grained control over how scenes are generated.\nA central feature of the framework is the ability to produce consistent\nbefore-after image pairs: user edits to OSM inputs translate into targeted\nvisual changes, while the rest of the scene is preserved. This makes it\npossible to generate training data that addresses scarcity and class imbalance,\nand to give planners a simple way to preview proposed interventions by editing\nmap data. More broadly, OSMGen produces paired (JSON, image) data for both\nstatic and changed states, paving the way toward a closed-loop system where\nsatellite imagery can automatically drive structured OSM updates. Source code\nis available at https://github.com/amir-zsh/OSMGen.", "AI": {"tldr": "OSMGen\u7528\u539f\u59cbOSM JSON\u751f\u6210\u53ef\u63a7\u7684\u536b\u661f\u5f71\u50cf\u5bf9\uff0c\u89e3\u51b3\u57ce\u5e02\u9065\u611f\u6570\u636e\u7a00\u7f3a\u4e0e\u4e0d\u5e73\u8861\uff0c\u5e76\u652f\u6301\u53ef\u89c6\u5316\u89c4\u5212\u4e0e\u95ed\u73af\u66f4\u65b0\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u57ce\u5e02\u76d1\u6d4b\u53d7\u9650\u4e8e\u4e13\u95e8\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u4e14\u4ee5\u6805\u683c\u5316\u5730\u56fe\u4e3a\u8f93\u5165\u4f1a\u4e22\u5931\u8bed\u4e49\u4e0e\u51e0\u4f55\u7ec6\u8282\u3002\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u4e30\u5bccOSM\u7ed3\u6784\u5316\u6570\u636e\u751f\u6210\u6210\u5bf9\uff08JSON, image\uff09\u8bad\u7ec3\u6837\u672c\u7684\u65b9\u6cd5\uff0c\u4ee5\u7f13\u89e3\u6570\u636e\u77ed\u7f3a\u5e76\u652f\u6301\u53ef\u63a7\u53d8\u5316\u5408\u6210\u3002", "method": "OSMGen\u5229\u7528\u5b8c\u6574OSM JSON\uff08\u5411\u91cf\u51e0\u4f55\u3001\u8bed\u4e49\u6807\u7b7e\u3001\u4f4d\u7f6e\u548c\u65f6\u95f4\uff09\uff0c\u800c\u975e\u6805\u683c\u74e6\u7247\uff0c\u4f5c\u4e3a\u751f\u6210\u8f93\u5165\u3002\u6846\u67b6\u5c06OSM\u6570\u636e\u6620\u5c04\u4e3a\u5408\u6210\u536b\u661f\u56fe\u50cf\uff0c\u652f\u6301\u5bf9OSM\u8f93\u5165\u8fdb\u884c\u7528\u6237\u7f16\u8f91\u5e76\u751f\u6210\u5bf9\u5e94\u7684\u524d\u540e\u56fe\u50cf\u5bf9\uff0c\u4fdd\u8bc1\u975e\u7f16\u8f91\u533a\u57df\u89c6\u89c9\u4e0a\u4fdd\u6301\u4e00\u81f4\u3002", "result": "OSMGen\u80fd\u751f\u6210\u73b0\u5b9e\u611f\u5f3a\u7684\u5355\u5e27\u536b\u661f\u56fe\u50cf\u548c\u7f16\u8f91\u9a71\u52a8\u7684\u524d\u540e\u5bf9\u6bd4\u5f71\u50cf\uff0c\u652f\u6301\u9488\u5bf9\u6027\u5408\u6210\u8bad\u7ec3\u6570\u636e\u4ee5\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4e14\u4e3a\u57ce\u5e02\u89c4\u5212\u8005\u63d0\u4f9b\u901a\u8fc7\u7f16\u8f91OSM\u9884\u89c8\u5e72\u9884\u6548\u679c\u7684\u5de5\u5177\u3002\u6e90\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86OSMGen\uff0c\u4e00\u4e2a\u76f4\u63a5\u4ece\u539f\u59cbOpenStreetMap\uff08OSM\uff09JSON\u751f\u6210\u903c\u771f\u536b\u661f\u5f71\u50cf\u7684\u6846\u67b6\uff0c\u80fd\u751f\u6210\u524d\u540e\u5bf9\u6bd4\u56fe\u50cf\u5e76\u4fdd\u6301\u573a\u666f\u4e00\u81f4\u6027\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u4e0e\u7c7b\u4e0d\u5e73\u8861\u95ee\u9898\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u7528\u4e8e\u89c4\u5212\u9884\u89c8\u548c\u95ed\u73af\u66f4\u65b0\u6d41\u7a0b\u3002"}}
{"id": "2511.00352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00352", "abs": "https://arxiv.org/abs/2511.00352", "authors": ["Mohd Ruhul Ameen", "Akif Islam"], "title": "Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach", "comment": "6 pages, 8 figures, 4 Tables, submitted to ICECTE 2026", "summary": "The rapid rise of generative diffusion models has made distinguishing\nauthentic visual content from synthetic imagery increasingly challenging.\nTraditional deepfake detection methods, which rely on frequency or pixel-level\nartifacts, fail against modern text-to-image systems such as Stable Diffusion\nand DALL-E that produce photorealistic and artifact-free results. This paper\nintroduces a diffusion-based forensic framework that leverages multi-strength\nimage reconstruction dynamics, termed diffusion snap-back, to identify\nAI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and\nPSNR) evolve across varying noise strengths, we extract interpretable\nmanifold-based features that differentiate real and synthetic images. Evaluated\non a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under\ncross-validation and remains robust to common distortions such as compression\nand noise. Despite using limited data and a single diffusion backbone (Stable\nDiffusion v1.5), the proposed method demonstrates strong generalization and\ninterpretability, offering a foundation for scalable, model-agnostic synthetic\nmedia forensics.", "AI": {"tldr": "\u5229\u7528\u5bf9\u56fe\u50cf\u5728\u4e0d\u540c\u566a\u58f0\u5f3a\u5ea6\u4e0b\u7684\u6269\u6563\u91cd\u5efa\u8f68\u8ff9\uff08LPIPS/SSIM/PSNR\u66f2\u7ebf\uff09\u63d0\u53d6\u53ef\u89e3\u91ca\u7279\u5f81\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u4e14\u9c81\u68d2\u7684AI\u56fe\u50cf\u9274\u522b\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4f9d\u8d56\u9891\u57df\u6216\u50cf\u7d20\u7ea7\u75d5\u8ff9\uff0c\u5bf9\u73b0\u4ee3\u6587\u672c\u5230\u56fe\u50cf\u7cfb\u7edf\u6548\u679c\u6b20\u4f73\u3002\u9700\u8981\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u3001\u53ef\u89e3\u91ca\u4e14\u5728\u538b\u7f29\u3001\u566a\u58f0\u7b49\u5e38\u89c1\u5931\u771f\u4e0b\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u9274\u522b\u9ad8\u903c\u771f\u5ea6\u7684\u5408\u6210\u56fe\u50cf\u3002", "method": "\u901a\u8fc7\u5bf9\u56fe\u50cf\u5728\u4e0d\u540c\u566a\u58f0\u5f3a\u5ea6\u4e0b\u4f7f\u7528Stable Diffusion v1.5\u8fdb\u884c\u591a\u5f3a\u5ea6\u91cd\u5efa\uff0c\u8bb0\u5f55LPIPS\u3001SSIM\u548cPSNR\u968f\u566a\u58f0\u5f3a\u5ea6\u53d8\u5316\u7684\u66f2\u7ebf\uff0c\u63d0\u53d6\u57fa\u4e8e\u6d41\u5f62\u7684\u53ef\u89e3\u91ca\u7279\u5f81\u7528\u4e8e\u5206\u7c7b\uff08\u4f8b\u5982\u66f2\u7ebf\u659c\u7387\u3001\u66f2\u7ebf\u5f62\u72b6\u7edf\u8ba1\u91cf\uff09\uff0c\u5e76\u57284,000\u5f20\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3/\u9a8c\u8bc1\u5206\u7c7b\u5668\u3002", "result": "\u57284,000\u5f20\u56fe\u50cf\u7684\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e0b\u8fbe\u52300.993 AUROC\uff1b\u5bf9\u538b\u7f29\u548c\u566a\u58f0\u5931\u771f\u4ecd\u8868\u73b0\u7a33\u5065\uff1b\u5373\u4f7f\u53ea\u7528\u6709\u9650\u6570\u636e\u548c\u5355\u4e00\u6269\u6563\u9aa8\u5e72\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u6269\u6563\u6a21\u578b\u91cd\u5efa\u52a8\u6001\uff08diffusion snap-back\uff09\u533a\u5206\u771f\u5b9e\u4e0eAI\u5408\u6210\u56fe\u50cf\uff0c\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u4e0e\u5355\u4e00\u6269\u6563\u9aa8\u5e72\u4e0b\u4ecd\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\u5e76\u5177\u6709\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.00357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00357", "abs": "https://arxiv.org/abs/2511.00357", "authors": ["Niklas W\u00f6lki", "Lukas Kondmann", "Christian Molli\u00e8re", "Martin Langer", "Julia Gottfriedsen", "Martin Werner"], "title": "Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation", "comment": "This work was presented at the TerraBytes Workshop at the 42nd\n  International Conference on Machine Learning. This version is not part of the\n  official ICML proceedings", "summary": "Onboard cloud segmentation is a critical yet underexplored task in thermal\nEarth observation (EO), particularly for CubeSat missions constrained by\nlimited hardware and spectral information. CubeSats often rely on a single\nthermal band and lack sufficient labeled data, making conventional cloud\nmasking techniques infeasible. This work addresses these challenges by applying\ntransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using\na UNet with a lightweight MobileNet encoder. We pretrain the model on the\npublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small\nset of mission-specific samples in a joint-training setup, improving the macro\nF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a\nTensorRT engine and demonstrate full-image inference in under 5 seconds on an\nNVIDIA Jetson Nano. These results show that leveraging public datasets and\nlightweight architectures can enable accurate, efficient thermal-only cloud\nmasking on-orbit, supporting real-time decision-making in data-limited EO\nmissions.", "AI": {"tldr": "\u5728\u5355\u70ed\u7ea2\u5916\u3001\u6570\u636e\u7a00\u7f3a\u7684CubeSat\u73af\u5883\u4e2d\uff0c\u5229\u7528Landsat-7\u9884\u8bad\u7ec3\u548c\u8f7b\u91cfUNet\uff08MobileNet\u7f16\u7801\u5668\uff09\u53ef\u5c06\u5b8fF1\u4ece0.850\u63d0\u5347\u52300.877\uff0c\u5e76\u5728Jetson Nano\u4e0a\u5b9e\u73b0<5s\u7684\u5168\u56fe\u63a8\u7406\uff0c\u9002\u5408\u5728\u8f68\u5b9e\u65f6\u4e91\u63a9\u819c\u3002", "motivation": "CubeSat\u901a\u5e38\u4ec5\u6709\u5355\u6ce2\u6bb5\u70ed\u7ea2\u5916\u89c2\u6d4b\u4e14\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u73b0\u6709\u4e91\u63a9\u819c\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\uff0c\u6545\u9700\u63a2\u7d22\u8f7b\u91cf\u5316\u3001\u4f9d\u8d56\u516c\u5171\u6570\u636e\u7684\u8fc1\u79fb\u5b66\u4e60\u65b9\u6848\u4ee5\u5b9e\u73b0\u673a\u8f7d\u5b9e\u65f6\u4e91\u63a9\u819c\u3002", "method": "\u4f7f\u7528UNet\u67b6\u6784\u914d\u5408\u8f7b\u91cf\u7684MobileNet\u7f16\u7801\u5668\uff0c\u5728Landsat-7\u4e91\u63a9\u819c\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u518d\u5728\u5c11\u91cfFOREST-2\u4efb\u52a1\u7279\u5b9a\u6837\u672c\u4e0a\u8054\u5408\u5fae\u8c03\uff1b\u968f\u540e\u5c06\u6a21\u578b\u8f6c\u6362\u4e3aTensorRT\u5f15\u64ce\u5e76\u5728NVIDIA Jetson Nano\u4e0a\u9a8c\u8bc1\u63a8\u7406\u901f\u5ea6\u3002", "result": "\u901a\u8fc7\u9884\u8bad\u7ec3+\u8054\u5408\u5fae\u8c03\uff0c\u5b8fF1\u4ece\u4ec5\u7528FOREST-2\u57fa\u7ebf\u76840.850\u63d0\u5347\u52300.877\uff1b\u5728Jetson Nano\u4e0a\u5b8c\u6210\u5168\u56fe\u63a8\u7406\u8017\u65f6\u4f4e\u4e8e5\u79d2\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684CubeSat\u4e0a\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u4e0e\u8f7b\u91cf\u5316\u7f51\u7edc\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u70ed\u7ea2\u5916\u4e91\u63a9\u819c\u63a8\u65ad\uff0c\u652f\u6301\u5b9e\u65f6\u5728\u8f68\u51b3\u7b56\u3002"}}
{"id": "2511.00362", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations.", "AI": {"tldr": "Oitijjo-3D \u7528\u514d\u8d39\u8857\u666f\u56fe\u548c\u751f\u6210\u5f0f AI\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u4f4e\u6210\u672c\u3001\u65e0\u9700\u4e13\u5bb6\u7684\u6587\u5316\u9057\u4ea7\u4e09\u7ef4\u91cd\u5efa\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u56fd\u5bb6\u63a8\u5e7f\u3002", "motivation": "\u5b5f\u52a0\u62c9\u56fd\u6587\u5316\u9057\u4ea7\u9762\u4e34\u8d44\u91d1\u4e0e\u6280\u672f\u77ed\u7f3a\uff0c\u4f20\u7edf\u4e09\u7ef4\u6570\u5b57\u5316\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u9700\u4e13\u5bb6\u64cd\u4f5c\uff0c\u8bb8\u591a\u53e4\u8ff9\u672a\u88ab\u6570\u5b57\u5316\u3002\u63d0\u51fa\u96f6\u6210\u672c\u3001\u6613\u666e\u53ca\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u6c11\u4e3b\u5316\u9057\u4ea7\u4fdd\u62a4\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1) \u4f7f\u7528 Gemini 2.5 Flash Image \u8fdb\u884c\u591a\u6a21\u6001\u89c6\u89c9\u63a8\u7406\u4ee5\u5408\u6210\u7ed3\u6784\u4e0e\u7eb9\u7406\u4fe1\u606f\uff1b2) \u4f7f\u7528 Hexagen \u5c06\u5408\u6210\u56fe\u50cf\u8f6c\u6362\u4e3a\u51e0\u4f55\u4f53\u4ee5\u6062\u590d\u4e09\u7ef4\u5f62\u72b6\u3002\u6574\u4e2a\u6d41\u7a0b\u4f9d\u8d56\u516c\u5f00 Google Street View \u56fe\u50cf\uff0c\u65e0\u9700\u4e13\u4e1a\u8bbe\u5907\u6216\u4eba\u5de5\u76d1\u7763\uff0c\u901f\u5ea6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf SfM\u3002", "result": "\u5728 Ahsan Manzil\u3001Choto Sona Mosque\u3001Paharpur \u7b49\u5730\u6807\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cOitijjo-3D \u5728\u79d2\u7ea7\u65f6\u95f4\u5185\u751f\u6210\u5149\u5ea6\u771f\u5b9e\u4e14\u5c3a\u5ea6\u4e00\u81f4\u7684\u91cd\u5efa\uff0c\u4fdd\u6301\u89c6\u89c9\u4e0e\u7ed3\u6784\u7ec6\u8282\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u7ecf\u6d4e\u4e0e\u6280\u672f\u95e8\u69db\u3002", "conclusion": "Oitijjo-3D \u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u516c\u5f00\u8857\u666f\u56fe\u50cf\u4e0e\u751f\u6210\u5f0f AI \u7684\u96f6\u6210\u672c\u4e09\u7ef4\u6587\u5316\u9057\u4ea7\u91cd\u5efa\u6846\u67b6\uff0c\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5feb\u901f\u751f\u6210\u5177\u6709\u89c6\u89c9\u4e0e\u7ed3\u6784\u4fdd\u771f\u5ea6\u7684\u4e09\u7ef4\u6a21\u578b\u3002"}}
{"id": "2511.00370", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00370", "abs": "https://arxiv.org/abs/2511.00370", "authors": ["Chaochen Wu", "Guan Luo", "Meiyun Zuo", "Zhitao Fan"], "title": "Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict", "comment": null, "summary": "Video moment retrieval uses a text query to locate a moment from a given\nuntrimmed video reference. Locating corresponding video moments with text\nqueries helps people interact with videos efficiently. Current solutions for\nthis task have not considered conflict within location results from different\nmodels, so various models cannot integrate correctly to produce better results.\nThis study introduces a reinforcement learning-based video moment retrieval\nmodel that can scan the whole video once to find the moment's boundary while\nproducing its locational evidence. Moreover, we proposed a multi-agent system\nframework that can use evidential learning to resolve conflicts between agents'\nlocalization output. As a side product of observing and dealing with conflicts\nbetween agents, we can decide whether a query has no corresponding moment in a\nvideo (out-of-scope) without additional training, which is suitable for\nreal-world applications. Extensive experiments on benchmark datasets show the\neffectiveness of our proposed methods compared with state-of-the-art\napproaches. Furthermore, the results of our study reveal that modeling\ncompetition and conflict of the multi-agent system is an effective way to\nimprove RL performance in moment retrieval and show the new role of evidential\nlearning in the multi-agent framework.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bc1\u636e\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u89c6\u9891\u65f6\u523b\u68c0\u7d22\u6846\u67b6\uff0c\u5355\u6b21\u89c6\u9891\u626b\u63cf\u5b9a\u4f4d\u5e76\u878d\u5408\u51b2\u7a81\uff0c\u80fd\u65e0\u76d1\u7763\u8bc6\u522b\u65e0\u5339\u914d\u67e5\u8be2\uff0c\u5b9e\u9a8c\u4f18\u4e8eSOTA\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u4e0d\u540c\u6a21\u578b\u6216\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u5b9a\u4f4d\u51b2\u7a81\uff0c\u5bfc\u81f4\u591a\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u878d\u5408\uff1b\u9700\u8981\u9ad8\u6548\u4e14\u80fd\u7ed9\u51fa\u5b9a\u4f4d\u8bc1\u636e\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5224\u65ad\u67e5\u8be2\u662f\u5426\u65e0\u5bf9\u5e94\u65f6\u523b\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u80fd\u5728\u89c6\u9891\u4e0a\u5355\u904d\u626b\u63cf\u5b9a\u4f4d\u8fb9\u754c\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5e76\u6784\u5efa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5f15\u5165\u8bc1\u636e\u5b66\u4e60\u7528\u4e8e\u878d\u5408\u548c\u89e3\u51b3\u5404\u667a\u80fd\u4f53\u95f4\u5b9a\u4f4d\u51b2\u7a81\uff1b\u901a\u8fc7\u7ade\u4e89\u4e0e\u51b2\u7a81\u5efa\u6a21\u63d0\u5347RL\u6027\u80fd\uff1b\u5229\u7528\u51b2\u7a81\u5904\u7406\u673a\u5236\u65e0\u76d1\u7763\u8bc6\u522bout-of-scope\u67e5\u8be2\u3002", "result": "\u5728\u591a\u9879\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u65b9\u6cd5\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff1b\u540c\u65f6\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7ade\u4e89/\u51b2\u7a81\u5efa\u6a21\u548c\u8bc1\u636e\u5b66\u4e60\u5728\u63d0\u5347RL\u68c0\u7d22\u6027\u80fd\u4e0eout-of-scope\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u667a\u80fd\u4f53\u7684\u89c6\u5c4f\u65f6\u523b\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b21\u626b\u63cf\u4ea7\u751f\u8fb9\u754c\u5e76\u8f93\u51fa\u8bc1\u636e\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u4e0d\u540c\u6a21\u578b\u5b9a\u4f4d\u51b2\u7a81\uff0c\u5e76\u53ef\u65e0\u989d\u5916\u8bad\u7ec3\u5224\u65ad\u67e5\u8be2\u662f\u5426\u8d85\u51fa\u8303\u56f4\uff0c\u5b9e\u9a8c\u8868\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.00381", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.00381", "abs": "https://arxiv.org/abs/2511.00381", "authors": ["Jiaming Li", "Junlei Wu", "Sheng Wang", "Honglin Xiong", "Jiangdong Cai", "Zihao Zhao", "Yitao Zhu", "Yuan Yin", "Dinggang Shen", "Qian Wang"], "title": "VisionCAD: An Integration-Free Radiology Copilot Framework", "comment": null, "summary": "Widespread clinical deployment of computer-aided diagnosis (CAD) systems is\nhindered by the challenge of integrating with existing hospital IT\ninfrastructure. Here, we introduce VisionCAD, a vision-based radiological\nassistance framework that circumvents this barrier by capturing medical images\ndirectly from displays using a camera system. The framework operates through an\nautomated pipeline that detects, restores, and analyzes on-screen medical\nimages, transforming camera-captured visual data into diagnostic-quality images\nsuitable for automated analysis and report generation. We validated VisionCAD\nacross diverse medical imaging datasets, demonstrating that our modular\narchitecture can flexibly utilize state-of-the-art diagnostic models for\nspecific tasks. The system achieves diagnostic performance comparable to\nconventional CAD systems operating on original digital images, with an F1-score\ndegradation typically less than 2\\% across classification tasks, while natural\nlanguage generation metrics for automated reports remain within 1\\% of those\nderived from original images. By requiring only a camera device and standard\ncomputing resources, VisionCAD offers an accessible approach for AI-assisted\ndiagnosis, enabling the deployment of diagnostic capabilities in diverse\nclinical settings without modifications to existing infrastructure.", "AI": {"tldr": "VisionCAD \u7528\u76f8\u673a\u62cd\u6444\u663e\u793a\u5668\u5e76\u901a\u8fc7\u68c0\u6d4b\u2014\u6062\u590d\u2014\u5206\u6790\u6d41\u6c34\u7ebf\u5c06\u5c4f\u5e55\u5f71\u50cf\u8f6c\u6210\u8bca\u65ad\u7ea7\u56fe\u50cf\uff0c\u8fbe\u5230\u4e0e\u539f\u59cb\u6570\u5b57\u56fe\u50cf\u8fd1\u4f3c\u7684AI\u8bca\u65ad\u548c\u81ea\u52a8\u62a5\u544a\u6548\u679c\uff0c\u4fbf\u4e8e\u4f4e\u6210\u672c\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709CAD\u7cfb\u7edf\u96be\u4ee5\u6574\u5408\u5230\u533b\u9662\u65e2\u6709IT\u57fa\u7840\u8bbe\u65bd\uff0c\u5bfc\u81f4\u90e8\u7f72\u53d7\u9650\u3002\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u6539\u52a8\u533b\u9662\u7cfb\u7edf\u3001\u4ec5\u9700\u76f8\u673a\u548c\u8ba1\u7b97\u8d44\u6e90\u5373\u53ef\u5de5\u4f5c\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u7ba1\u9053\uff1a\u5c4f\u5e55\u56fe\u50cf\u68c0\u6d4b\u3001\u56fe\u50cf\u6062\u590d\uff08\u53bb\u566a\u3001\u51e0\u4f55\u6821\u6b63\u3001\u989c\u8272/\u5bf9\u6bd4\u4fee\u590d\uff09\u3001\u4ee5\u53ca\u4e0b\u6e38\u8bca\u65ad\u4e0e\u62a5\u544a\u751f\u6210\uff1b\u53ef\u63d2\u62d4\u5730\u91c7\u7528\u73b0\u6709\u5148\u8fdb\u8bca\u65ad\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u533b\u5b66\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8868\u660e\uff1a\u5206\u7c7b\u4efb\u52a1F1\u4ec5\u4e0b\u964d<2%\uff0c\u81ea\u52a8\u62a5\u544a\u7684\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6307\u6807\u4e0e\u539f\u59cb\u56fe\u50cf\u76f8\u6bd4\u4e0b\u964d<1%\uff0c\u8868\u660e\u6027\u80fd\u63a5\u8fd1\u4f20\u7edfCAD\u3002", "conclusion": "VisionCAD \u901a\u8fc7\u6444\u50cf\u5934\u76f4\u63a5\u91c7\u96c6\u5c4f\u5e55\u4e0a\u7684\u533b\u5b66\u56fe\u50cf\uff0c\u7ed5\u8fc7\u533b\u9662IT\u96c6\u6210\u969c\u788d\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6570\u5b57\u6e90\u56fe\u50cf\u7684\u8bca\u65ad\u6027\u80fd\uff0c\u4fbf\u4e8e\u5728\u591a\u79cd\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72AI\u8f85\u52a9\u8bca\u65ad\u3002"}}
{"id": "2511.00389", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00389", "abs": "https://arxiv.org/abs/2511.00389", "authors": ["Fan Zhang", "Haoxuan Li", "Shengju Qian", "Xin Wang", "Zheng Lian", "Hao Wu", "Zhihong Zhu", "Yuan Gao", "Qiankun Li", "Yefeng Zheng", "Zhouchen Lin", "Pheng-Ann Heng"], "title": "Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have revolutionized numerous\nresearch fields, including computer vision and affective computing. As a\npivotal challenge in this interdisciplinary domain, facial expression\nrecognition (FER) has evolved from separate, domain-specific models to more\nunified approaches. One promising avenue to unify FER tasks is converting\nconventional FER datasets into visual question-answering (VQA) formats,\nenabling the direct application of powerful generalist MLLMs for inference.\nHowever, despite the success of cutting-edge MLLMs in various tasks, their\nperformance on FER tasks remains largely unexplored. To address this gap, we\nprovide FERBench, a systematic benchmark that incorporates 20 state-of-the-art\nMLLMs across four widely used FER datasets. Our results reveal that, while\nMLLMs exhibit good classification performance, they still face significant\nlimitations in reasoning and interpretability. To this end, we introduce\npost-training strategies aimed at enhancing the facial expression reasoning\ncapabilities of MLLMs. Specifically, we curate two high-quality and large-scale\ndatasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K\nfor reinforcement learning with verifiable rewards (RLVR), respectively.\nBuilding upon them, we develop a unified and interpretable FER foundation model\ntermed UniFER-7B, which outperforms many open-sourced and closed-source\ngeneralist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).", "AI": {"tldr": "\u63d0\u51faFERBench\u5e76\u6784\u5efa\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6570\u636e\uff08UniFER-CoT-230K, UniFER-RLVR-360K\uff09\uff0c\u901a\u8fc7\u540e\u8bad\u7ec3\u5f97\u5230UniFER-7B\uff0c\u63d0\u5347MLLM\u5728FER\u4e0a\u7684\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u591a\u6b3e\u5927\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u901a\u7528MLLM\u867d\u5728\u591a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08FER\uff09\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e14\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u3002\u4e3a\u4fc3\u8fdb\u7edf\u4e00FER\u65b9\u6cd5\u5e76\u63d0\u5347MLLM\u5728FER\u7684\u6027\u80fd\uff0c\u4f5c\u8005\u63d0\u51fa\u5c06FER\u4efb\u52a1\u8f6c\u4e3aVQA\u4ee5\u76f4\u63a5\u5229\u7528MLLM\u5e76\u8bbe\u8ba1\u540e\u8bad\u7ec3\u7b56\u7565\u6539\u8fdb\u6a21\u578b\u7684\u8868\u60c5\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u4f20\u7edfFER\u6570\u636e\u96c6\u8f6c\u6362\u4e3aVQA\u683c\u5f0f\u4ee5\u9002\u914dMLLM\uff0c\u63d0\u51faFERBench\u8bc4\u4f30\u6846\u67b6\uff1b\u6784\u5efa\u4e24\u5927\u540e\u8bad\u7ec3\u6570\u636e\u96c6\uff1aUniFER-CoT-230K\u7528\u4e8eChain-of-Thought\u98ce\u683c\u7684\u51b7\u542f\u52a8\u8bad\u7ec3\uff0cUniFER-RLVR-360K\u7528\u4e8e\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff1b\u57fa\u4e8e\u8fd9\u4e9b\u6570\u636e\u5bf9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u540e\u8bad\u7ec3\u5f97\u5230UniFER-7B\uff0c\u5e76\u4e0e\u591a\u6b3e\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "FERBench\u8bc4\u4f30\u663e\u793a20\u4e2a\u5148\u8fdbMLLM\u5728\u56db\u4e2aFER\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u80fd\u529b\u603b\u4f53\u4e0d\u9519\uff0c\u4f46\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\u5dee\uff1b\u901a\u8fc7\u540e\u8bad\u7ec3\u7b56\u7565\u5f97\u5230\u7684UniFER-7B\u5728FER\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u4f18\u4e8e\u4f17\u591a\u5f00\u6e90\u548c\u95ed\u6e90\u5927\u6a21\u578b\uff08\u5982Gemini-2.5-Pro\u3001Qwen2.5-VL-72B\uff09\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86FERBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8620\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u6a21\u578b\u5728\u56db\u4e2a\u8868\u60c5\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0MLLMs\u5728\u5206\u7c7b\u4e0a\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u63a8\u7406\u4e0e\u53ef\u89e3\u91ca\u6027\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002\u4f5c\u8005\u8fdb\u4e00\u6b65\u901a\u8fc7\u4e24\u9636\u6bb5\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff08\u51b7\u542f\u52a8\u7684UniFER-CoT-230K\u4e0e\u7528\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684RL\u7684UniFER-RLVR-360K\uff09\u63d0\u5347\u6a21\u578b\u7684\u8868\u60c5\u63a8\u7406\u80fd\u529b\uff0c\u6700\u7ec8\u5f97\u5230\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u57fa\u7840\u6a21\u578bUniFER-7B\uff0c\u6027\u80fd\u8d85\u8d8a\u591a\u6b3e\u5f00\u6e90\u4e0e\u95ed\u6e90\u901a\u7528MLLM\u3002"}}
{"id": "2511.00391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00391", "abs": "https://arxiv.org/abs/2511.00391", "authors": ["Xuanle Zhao", "Deyang Jiang", "Zhixiong Zeng", "Lei Chen", "Haibo Qiu", "Jing Huang", "Yufeng Zhong", "Liming Zheng", "Yilin Cao", "Lin Ma"], "title": "VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning", "comment": "Preprint Version, Work in Progress", "summary": "Multimodal code generation has garnered significant interest within the\nresearch community. Despite the notable success of recent vision-language\nmodels (VLMs) on specialized tasks like Chart-to-code generation, their\nreliance on single-task training regimens fosters a narrow paradigm that\nhinders the development of generalized \\textbf{VI}sio\\textbf{N} \\textbf{C}ode\n\\textbf{I}ntelligence. In this work, we introduce \\textbf{VinciCoder}, a\nunified multimodal code generation model that addresses this limitation via a\ntwo-stage training framework. We begin by constructing a large-scale Supervised\nFinetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving\ndirect code generation and visual-based code refinement. Subsequently, we\nintroduce a Visual Reinforcement Learning (ViRL) strategy, which employs a\ncoarse-to-fine reward mechanism to improve visual fidelity by calculating\nvisual similarity across local and global image patches. Extensive experiments\non various multimodal code generation benchmarks demonstrate that VinciCoder\nachieves state-of-the-art performance, underscoring the effectiveness of our\ncoarse-to-fine ViRL strategy. The code and model will be available at\nhttps://github.com/DocTron-hub/VinciCoder.", "AI": {"tldr": "\u63d0\u51faVinciCoder\uff1a1.6M SFT\u8bed\u6599 + \u7c97\u5230\u7ec6\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff0c\u7edf\u4e00\u63d0\u5347\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u4e0e\u89c6\u89c9\u4ee3\u7801\u4fee\u6b63\uff0c\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u591a\u4e3a\u5355\u4efb\u52a1\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u901a\u7528\u89c6\u89c9\u4ee3\u7801\u667a\u80fd\uff08Vision Code Intelligence, VCI\uff09\u7684\u53d1\u5c55\uff1b\u56e0\u6b64\u9700\u8981\u7edf\u4e00\u7684\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u6a21\u578b\u4e0e\u8bad\u7ec3\u6846\u67b6\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9996\u5148\u6784\u5efa1.6M\u56fe\u50cf-\u4ee3\u7801\u5bf9\u7684SFT\u8bed\u6599\uff0c\u8986\u76d6\u76f4\u63a5\u751f\u6210\u4e0e\u57fa\u4e8e\u89c6\u89c9\u7684\u4ee3\u7801\u7cbe\u4fee\uff1b\u968f\u540e\u5f15\u5165Visual Reinforcement Learning (ViRL)\uff0c\u91c7\u7528\u7c97\u5230\u7ec6\u7684\u5956\u52b1\u673a\u5236\uff0c\u57fa\u4e8e\u5c40\u90e8\u4e0e\u5168\u5c40\u56fe\u50cf\u5757\u7684\u89c6\u89c9\u76f8\u4f3c\u5ea6\u6765\u4f18\u5316\u89c6\u89c9\u4fdd\u771f\u6027\u3002", "result": "\u5728\u591a\u79cd\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e0a\uff0cVinciCoder\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7c97\u5230\u7ec6ViRL\u7b56\u7565\u548c\u5927\u89c4\u6a21SFT\u8bed\u6599\u7684\u6709\u6548\u6027\u3002", "conclusion": "VinciCoder\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u6709\u76d1\u7763\u5fae\u8c03\u548c\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u4fe1\u606f\u4e0b\u7684\u4ee3\u7801\u751f\u6210\u4e0e\u4ee3\u7801\u4fee\u6b63\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u591a\u9879\u57fa\u51c6\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2511.00396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00396", "abs": "https://arxiv.org/abs/2511.00396", "authors": ["Long Li", "Shuichen Ji", "Ziyang Luo", "Nian Liu", "Dingwen Zhang", "Junwei Han"], "title": "CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks", "comment": "14 pages,10 figures", "summary": "We present the first unified framework that jointly handles three\noperationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting\neach as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model\n(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT\nquality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a\nlightweight single-sample algorithm that leverages the discrepancy between\nreward and model confidence as a per-sample advantage signal. This design\nnaturally focuses updates on informative responses while eliminating group\nsampling, thereby addressing GRPO's key limitations: confidence-agnostic\nlearning, signal dilution, and prohibitive computational overhead. We also\nintroduce an \"output-to-reasoning\" strategy to construct high-fidelity SFT data\nthat ensures logical consistency with ground-truth masks. Experiments show our\nmodel matches or outperforms specialized SOTA methods and strong closed-source\nVLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for\nCoSOD, surpassing the prior best by 8.0 percentage points, despite using far\nless training data.", "AI": {"tldr": "\u63d0\u51fa\u5728VLM\u4e2d\u7528CoT\u7edf\u4e00SOD/CoSOD/SIS\u4efb\u52a1\uff0c\u7ed3\u5408SFT+RL\u548c\u65b0\u9896\u7684CGPO\u7b97\u6cd5\u53ca\u8f93\u51fa\u5230\u63a8\u7406\u7684\u6570\u636e\u6784\u9020\uff0c\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e0e\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4e09\u7c7b\u663e\u8457\u6027\u4efb\u52a1\u5728\u64cd\u4f5c\u4e0a\u5f02\u6784\uff0c\u96be\u4ee5\u7528\u5355\u4e00\u6a21\u578b\u5904\u7406\uff1b\u5e0c\u671b\u901a\u8fc7\u5728VLM\u4e2d\u7528CoT\u7edf\u4e00\u8868\u8ff0\u4efb\u52a1\uff0c\u4ee5\u51cf\u5c11\u4efb\u52a1\u5dee\u5f02\u5e76\u5229\u7528\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4e24\u9636\u6bb5CoT\u8bad\u7ec3\uff1a\u5148\u6709\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u518d\u7528\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u3002\u5f15\u5165\u4e86Confidence-Guided Policy Optimization\uff08CGPO\uff09\uff0c\u4e00\u79cd\u5355\u6837\u672cRL\u7b97\u6cd5\uff0c\u7528\u5956\u52b1\u4e0e\u6a21\u578b\u7f6e\u4fe1\u5dee\u4f5c\u4e3a\u6837\u672c\u4f18\u52bf\u4fe1\u53f7\uff0c\u907f\u514d\u7ec4\u91c7\u6837\u5e76\u805a\u7126\u6709\u4fe1\u606f\u7684\u54cd\u5e94\uff1b\u540c\u65f6\u63d0\u51fa\u201c\u8f93\u51fa\u5230\u63a8\u7406\u201d\u7b56\u7565\u751f\u6210\u9ad8\u4fdd\u771fSFT\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e13\u7528SOTA\u548c\u5f3a\u95ed\u6e90VLM\uff0c\u5c24\u5176\u5728CoSOD\u7684CoCA\u6570\u636e\u96c6\u4e0aS-measure\u8fbe\u52300.899\uff0c\u6bd4\u4e4b\u524d\u6700\u597d\u63d0\u53478.0\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8fdc\u5c11\u4e8e\u5bf9\u624b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06SOD\u3001CoSOD\u548cSIS\u4e09\u79cd\u5f02\u6784\u663e\u8457\u6027\u4efb\u52a1\u7edf\u4e00\u4e3a\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u4ee5\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u63a8\u7406\u7684\u5f62\u5f0f\u6765\u89e3\u51b3\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u95e8\u65b9\u6cd5\u548c\u95ed\u6e90\u5f3a\u6a21\u578b\u3002"}}
{"id": "2511.00419", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00419", "abs": "https://arxiv.org/abs/2511.00419", "authors": ["Thanh Hieu Cao", "Trung Khang Tran", "Gia Thinh Pham", "Tuong Nghiem Diep", "Thanh Binh Nguyen"], "title": "LGCA: Enhancing Semantic Representation via Progressive Expansion", "comment": "15 pages, 5 figures, to appear in SoICT 2025", "summary": "Recent advancements in large-scale pretraining in natural language processing\nhave enabled pretrained vision-language models such as CLIP to effectively\nalign images and text, significantly improving performance in zero-shot image\nclassification tasks. Subsequent studies have further demonstrated that\ncropping images into smaller regions and using large language models to\ngenerate multiple descriptions for each caption can further enhance model\nperformance. However, due to the inherent sensitivity of CLIP, random image\ncrops can introduce misinformation and bias, as many images share similar\nfeatures at small scales. To address this issue, we propose\nLocalized-Globalized Cross-Alignment (LGCA), a framework that first captures\nthe local features of an image and then repeatedly selects the most salient\nregions and expands them. The similarity score is designed to incorporate both\nthe original and expanded images, enabling the model to capture both local and\nglobal features while minimizing misinformation. Additionally, we provide a\ntheoretical analysis demonstrating that the time complexity of LGCA remains the\nsame as that of the original model prior to the repeated expansion process,\nhighlighting its efficiency and scalability. Extensive experiments demonstrate\nthat our method substantially improves zero-shot performance across diverse\ndatasets, outperforming state-of-the-art baselines.", "AI": {"tldr": "LGCA\u901a\u8fc7\u9009\u62e9\u5e76\u6269\u5c55\u6700\u663e\u8457\u5c40\u90e8\u533a\u57df\u5e76\u7ed3\u5408\u539f\u56fe\u8fdb\u884c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u89e3\u51b3\u968f\u673a\u88c1\u526a\u5e26\u6765\u7684\u8bef\u5bfc\uff0c\u663e\u8457\u63d0\u5347CLIP\u7c7b\u6a21\u578b\u7684\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u65f6\u95f4\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u968f\u673a\u88c1\u526a\u7684\u65b9\u6cd5\u4f1a\u56e0CLIP\u5bf9\u5c0f\u5c3a\u5ea6\u7279\u5f81\u7684\u654f\u611f\u6027\u800c\u5f15\u5165\u8bef\u5bfc\u4fe1\u606f\u548c\u504f\u5dee\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5229\u7528\u5c40\u90e8\u663e\u8457\u533a\u57df\u4fe1\u606f\u53c8\u80fd\u907f\u514d\u8bef\u5bfc\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u96f6\u6837\u672c\u5206\u7c7b\u8868\u73b0\u3002", "method": "\u63d0\u51faLocalized-Globalized Cross-Alignment (LGCA)\u6846\u67b6\uff1a\u9996\u5148\u5bf9\u56fe\u50cf\u8fdb\u884c\u5c40\u90e8\u7279\u5f81\u63d0\u53d6\uff0c\u53cd\u590d\u9009\u62e9\u6700\u663e\u8457\u533a\u57df\u5e76\u8fdb\u884c\u6269\u5c55\uff1b\u8bbe\u8ba1\u65b0\u7684\u76f8\u4f3c\u5ea6\u8bc4\u5206\uff0c\u5c06\u539f\u56fe\u4e0e\u6269\u5c55\u56fe\u540c\u65f6\u7eb3\u5165\u8ba1\u7b97\uff0c\u878d\u5408\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u606f\uff1b\u4fdd\u7559\u539f\u6709\u6a21\u578b\u63a8\u7406\u6d41\u7a0b\uff0c\u6269\u5c55\u6b65\u9aa4\u5728\u4e0d\u589e\u52a0\u6574\u4f53\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u5b8c\u6210\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u8bc1\u660e\uff0cLGCA\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\uff1b\u7406\u8bba\u5206\u6790\u8868\u660e\u5176\u65f6\u95f4\u590d\u6742\u5ea6\u4e0e\u672a\u6269\u5c55\u7684\u539f\u59cb\u6a21\u578b\u76f8\u540c\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u9ad8\u6548\u6027\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "LGCA\u901a\u8fc7\u5148\u6355\u83b7\u56fe\u50cf\u5c40\u90e8\u7279\u5f81\u518d\u6269\u5c55\u6700\u663e\u8457\u533a\u57df\uff0c\u5b9e\u73b0\u672c\u5730-\u5168\u5c40\u7279\u5f81\u5bf9\u9f50\uff0c\u6709\u6548\u964d\u4f4e\u968f\u673a\u88c1\u526a\u5f15\u5165\u7684\u8bef\u5bfc\u4fe1\u606f\u548c\u504f\u5dee\u3002\u8be5\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u4e14\u5728\u7406\u8bba\u4e0a\u4fdd\u6301\u4e0e\u539f\u6a21\u578b\u76f8\u540c\u7684\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u517c\u5177\u6027\u80fd\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00427", "abs": "https://arxiv.org/abs/2511.00427", "authors": ["Daichi Zhang", "Tong Zhang", "Jianmin Bao", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection", "comment": null, "summary": "With the rapid development of generative models, detecting generated fake\nimages to prevent their malicious use has become a critical issue recently.\nExisting methods frame this challenge as a naive binary image classification\ntask. However, such methods focus only on visual clues, yielding trained\ndetectors susceptible to overfitting specific image patterns and incapable of\ngeneralizing to unseen models. In this paper, we address this issue from a\nmulti-modal perspective and find that fake images cannot be properly aligned\nwith corresponding captions compared to real images. Upon this observation, we\npropose a simple yet effective detector termed ITEM by leveraging the\nimage-text misalignment in a joint visual-language space as discriminative\nclues. Specifically, we first measure the misalignment of the images and\ncaptions in pre-trained CLIP's space, and then tune a MLP head to perform the\nusual detection task. Furthermore, we propose a hierarchical misalignment\nscheme that first focuses on the whole image and then each semantic object\ndescribed in the caption, which can explore both global and fine-grained local\nsemantic misalignment as clues. Extensive experiments demonstrate the\nsuperiority of our method against other state-of-the-art competitors with\nimpressive generalization and robustness on various recent generative models.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faITEM\uff0c\u901a\u8fc7\u5728CLIP\u7a7a\u95f4\u6d4b\u91cf\u56fe\u6587\u5bf9\u9f50\u5dee\u5f02\u5e76\u7528\u5206\u5c42\u7b56\u7565\u63d0\u53d6\u5168\u5c40\u4e0e\u5c40\u90e8\u7ebf\u7d22\uff0c\u501f\u52a9\u8f7b\u91cfMLP\u5b9e\u73b0\u5bf9\u751f\u6210\u56fe\u50cf\u7684\u9c81\u68d2\u68c0\u6d4b\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4ec5\u57fa\u4e8e\u89c6\u89c9\u7279\u5f81\u7684\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u56fe\u50cf\u6a21\u5f0f\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u89c1\u7684\u751f\u6210\u6a21\u578b\uff0c\u4f5c\u8005\u53d1\u73b0\u751f\u6210\u56fe\u50cf\u4e0e\u5176\u63cf\u8ff0\u6587\u672c\u5728\u8bed\u4e49\u5bf9\u9f50\u4e0a\u5b58\u5728\u5f02\u5e38\uff0c\u56e0\u800c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u9f50\u5dee\u5f02\u6765\u63d0\u9ad8\u68c0\u6d4b\u9c81\u68d2\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3CLIP\u7f16\u7801\u5668\u8ba1\u7b97\u56fe\u50cf\u4e0e\u5bf9\u5e94\u63cf\u8ff0\u6587\u672c\u7684\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u7279\u5f81\uff0c\u8bbe\u8ba1\u5206\u5c42\u8bef\u914d\u7b56\u7565\uff08\u5148\u5168\u56fe\u518d\u5bf9\u8c61\u7ea7\uff09\u63d0\u53d6\u5168\u5c40\u4e0e\u5c40\u90e8\u5bf9\u9f50\u5dee\u5f02\uff0c\u6700\u540e\u5728\u8fd9\u4e9b\u7279\u5f81\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cfMLP\u8fdb\u884c\u4e8c\u5206\u7c7b\u68c0\u6d4b\u3002", "result": "\u5728\u591a\u79cd\u65b0\u8fd1\u751f\u6210\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0cITEM\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8fc7\u82e5\u5e72\u6700\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u56fe\u6587\u5bf9\u9f50\u5dee\u5f02\u662f\u6709\u6548\u7684\u5224\u522b\u7ebf\u7d22\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u56fe\u6587\u5bf9\u9f50\u5dee\u5f02\u7684\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5ITEM\uff0c\u901a\u8fc7\u5728CLIP\u7a7a\u95f4\u6d4b\u91cf\u56fe\u50cf\u4e0e\u6587\u672c\u7684\u5bf9\u9f50\u5ea6\u5e76\u5fae\u8c03MLP\u5206\u7c7b\u5934\u6765\u533a\u5206\u771f\u4f2a\u56fe\u50cf\uff1b\u91c7\u7528\u5206\u5c42\u5bf9\u9f50\u65b9\u6848\u517c\u987e\u5168\u5c40\u4e0e\u5c40\u90e8\u8bed\u4e49\uff0c\u5b9e\u9a8c\u8868\u660e\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.00429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00429", "abs": "https://arxiv.org/abs/2511.00429", "authors": ["Daichi Zhang", "Tong Zhang", "Shiming Ge", "Sabine S\u00fcsstrunk"], "title": "Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection", "comment": null, "summary": "Diffusion models have achieved remarkable success in image synthesis, but the\ngenerated high-quality images raise concerns about potential malicious use.\nExisting detectors often struggle to capture discriminative clues across\ndifferent models and settings, limiting their generalization to unseen\ndiffusion models and robustness to various perturbations. To address this\nissue, we observe that diffusion-generated images exhibit progressively larger\ndifferences from natural real images across low- to high-frequency bands. Based\non this insight, we propose a simple yet effective representation by enhancing\nthe Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we\nintroduce a frequency-selective function which serves as a weighted filter to\nthe Fourier spectrum, suppressing less discriminative bands while enhancing\nmore informative ones. This approach, grounded in a comprehensive analysis of\nfrequency-based differences between natural real and diffusion-generated\nimages, enables general detection of images from unseen diffusion models and\nprovides robust resilience to various perturbations. Extensive experiments on\nvarious diffusion-generated image datasets demonstrate that our method\noutperforms state-of-the-art detectors with superior generalization and\nrobustness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u9891\u57df\u7684\u9891\u7387\u9009\u62e9\u52a0\u6743\u7b56\u7565\u6765\u589e\u5f3a\u9891\u7387\u4f2a\u9020\u7ebf\u7d22\uff0c\u80fd\u66f4\u597d\u533a\u5206\u6269\u6563\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u56fe\u50cf\uff0c\u63d0\u5347\u5bf9\u672a\u89c1\u6a21\u578b\u53ca\u6270\u52a8\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u9ad8\u8d28\u91cf\u56fe\u50cf\u5e26\u6765\u6ee5\u7528\u98ce\u9669\uff0c\u800c\u73b0\u6709\u68c0\u6d4b\u5668\u96be\u4ee5\u8de8\u4e0d\u540c\u6269\u6563\u6a21\u578b\u548c\u6270\u52a8\u4fdd\u6301\u5224\u522b\u80fd\u529b\uff0c\u9700\u5bfb\u627e\u66f4\u5177\u666e\u9002\u6027\u7684\u5224\u522b\u7ebf\u7d22\u3002", "method": "\u5206\u6790\u771f\u5b9e\u56fe\u50cf\u4e0e\u6269\u6563\u751f\u6210\u56fe\u50cf\u5728\u4f4e\u5230\u9ad8\u9891\u6bb5\u7684\u5dee\u5f02\uff0c\u5f15\u5165\u4e00\u4e2a\u9891\u7387\u9009\u62e9\u51fd\u6570\u5bf9\u5085\u91cc\u53f6\u8c31\u505a\u52a0\u6743\u6ee4\u6ce2\uff0c\u6291\u5236\u5224\u522b\u529b\u5f31\u7684\u9891\u6bb5\uff0c\u589e\u5f3a\u5224\u522b\u529b\u5f3a\u7684\u9891\u6bb5\uff0c\u4ece\u800c\u5f97\u5230\u589e\u5f3a\u7684F^2C\u8868\u5f81\u7528\u4e8e\u8bad\u7ec3\u68c0\u6d4b\u5668\u3002", "result": "\u5728\u591a\u79cd\u6269\u6563\u751f\u6210\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u589e\u5f3a\u8de8\u9891\u5e26\u7684\u9891\u7387\u4f2a\u9020\u7ebf\u7d22(F^2C)\u6765\u68c0\u6d4b\u6269\u6563\u751f\u6210\u56fe\u50cf\uff0c\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u80fd\u63d0\u5347\u5bf9\u672a\u89c1\u6269\u6563\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u589e\u5f3a\u5bf9\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.00446", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00446", "abs": "https://arxiv.org/abs/2511.00446", "authors": ["Xin Yao", "Haiyang Zhao", "Yimin Chen", "Jiawei Guo", "Kecheng Huang", "Ming Zhao"], "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training", "comment": "Accepted by NeurIPS 2025", "summary": "The Contrastive Language-Image Pretraining (CLIP) model has significantly\nadvanced vision-language modeling by aligning image-text pairs from large-scale\nweb data through self-supervised contrastive learning. Yet, its reliance on\nuncurated Internet-sourced data exposes it to data poisoning and backdoor\nrisks. While existing studies primarily investigate image-based attacks, the\ntext modality, which is equally central to CLIP's training, remains\nunderexplored. In this work, we introduce ToxicTextCLIP, a framework for\ngenerating high-quality adversarial texts that target CLIP during the\npre-training phase. The framework addresses two key challenges: semantic\nmisalignment caused by background inconsistency with the target class, and the\nscarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively\napplies: 1) a background-aware selector that prioritizes texts with background\ncontent aligned to the target class, and 2) a background-driven augmenter that\ngenerates semantically coherent and diverse poisoned samples. Extensive\nexperiments on classification and retrieval tasks show that ToxicTextCLIP\nachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while\nbypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be\naccessed via https://github.com/xinyaocse/ToxicTextCLIP/.", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411CLIP\u9884\u8bad\u7ec3\u7684\u6587\u672c\u4e2d\u6bd2\u6846\u67b6ToxicTextCLIP\uff0c\u901a\u8fc7\u80cc\u666f\u611f\u77e5\u9009\u62e9\u4e0e\u80cc\u666f\u9a71\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u4e2d\u6bd2\u6587\u672c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u653b\u51fb\u6548\u679c\u5f3a\u4e14\u80fd\u89c4\u907f\u591a\u79cd\u9632\u5fa1\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eCLIP\u5b89\u5168\u6027\u7684\u7814\u7a76\u591a\u805a\u7126\u56fe\u50cf\u5c42\u9762\u7684\u653b\u51fb\uff0c\u5ffd\u89c6\u4e86\u540c\u7b49\u91cd\u8981\u7684\u6587\u672c\u6a21\u6001\uff1b\u672a\u7ecf\u7b5b\u9009\u7684\u7f51\u7edc\u6587\u672c\u53ef\u80fd\u88ab\u6076\u610f\u5229\u7528\u7528\u4e8e\u6570\u636e\u4e2d\u6bd2\u4e0e\u540e\u95e8\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u6846\u67b6\uff0c\u5305\u62ec\u80cc\u666f\u611f\u77e5\u9009\u62e9\u5668\uff08\u7b5b\u9009\u4e0e\u76ee\u6807\u7c7b\u80cc\u666f\u4e00\u81f4\u7684\u6587\u672c\uff09\u4e0e\u80cc\u666f\u9a71\u52a8\u6269\u589e\u5668\uff08\u751f\u6210\u8bed\u4e49\u8fde\u8d2f\u4e14\u591a\u6837\u7684\u4e2d\u6bd2\u6587\u672c\uff09\uff0c\u5728\u9884\u8bad\u7ec3\u6570\u636e\u4e2d\u6ce8\u5165\u8fd9\u4e9b\u6587\u672c\u4ee5\u5b9e\u73b0\u5bf9CLIP\u7684\u4e2d\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u3002", "result": "\u5728\u5206\u7c7b\u4e0e\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u5230\u9ad8\u4e2d\u6bd2\u6210\u529f\u7387\uff08\u6700\u9ad895.83%\uff09\u548c\u9ad8\u540e\u95e8Hit@1\uff08\u6700\u9ad898.68%\uff09\uff0c\u4e14\u80fd\u7ed5\u8fc7RoCLIP\u3001CleanCLIP\u548cSafeCLIP\u7b49\u9632\u5fa1\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51faToxicTextCLIP\uff0c\u5c55\u793a\u4e86CLIP\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u6587\u672c\u6a21\u6001\u8fdb\u884c\u9ad8\u6548\u4e2d\u6bd2\u4e0e\u540e\u95e8\u690d\u5165\u7684\u53ef\u884c\u6027\uff0c\u5f3a\u8c03\u6587\u672c\u653b\u51fb\u7684\u5371\u9669\u6027\u5e76\u9a8c\u8bc1\u4e86\u5176\u80fd\u7ed5\u8fc7\u73b0\u6709\u9632\u5fa1\u3002"}}
{"id": "2511.00456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": null, "summary": "This study proposes a weakly supervised deep learning framework for pneumonia\nclassification and localization from chest X-rays, utilizing Grad-CAM\nexplanations. Instead of costly pixel-level annotations, our approach utilizes\nimage-level labels to generate clinically meaningful heatmaps that highlight\nregions affected by pneumonia. We evaluate seven ImageNet-pretrained\narchitectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and\nViT-B16 under identical training conditions with focal loss and patient-wise\nsplits to prevent data leakage. Experimental results on the Kermany CXR dataset\ndemonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test\naccuracy of 98\\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides\nan optimal trade-off between accuracy and computational cost. Grad-CAM\nvisualizations confirm that the proposed models focus on clinically relevant\nlung regions, supporting the use of interpretable AI for radiological\ndiagnostics. This work highlights the potential of weakly supervised\nexplainable models that enhance pneumonia screening transparency, and clinical\ntrust in AI-assisted medical imaging.\n  https://github.com/kiranshahi/pneumonia-analysis", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u7ea7\u6807\u7b7e\u548cGrad-CAM\u7684\u5f31\u76d1\u7763\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u5728\u65e0\u50cf\u7d20\u7ea7\u6807\u6ce8\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684\u80ba\u708e\u5206\u7c7b\u4e0e\u5b9a\u4f4d\uff0cResNet-18\u548cEfficientNet-B0\u8868\u73b0\u6700\u4f73\uff0cMobileNet-V2\u5177\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u52a8\u673a\u662f\u51cf\u5c11\u6602\u8d35\u7684\u50cf\u7d20\u7ea7\u6ce8\u91ca\u9700\u6c42\uff0c\u901a\u8fc7\u5f31\u76d1\u7763\u4e0e\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08Grad-CAM\uff09\u5b9e\u73b0\u80ba\u708e\u7684\u81ea\u52a8\u5206\u7c7b\u4e0e\u5b9a\u4f4d\uff0c\u4ece\u800c\u589e\u8fdbAI\u5728\u653e\u5c04\u8bca\u65ad\u4e2d\u7684\u900f\u660e\u5ea6\u4e0e\u53ef\u4fe1\u5ea6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u4f7f\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u8bad\u7ec3\u591a\u79cdImageNet\u9884\u8bad\u7ec3\u7684CNN\u4e0eViT\uff08ResNet-18/50\u3001DenseNet-121\u3001EfficientNet-B0\u3001MobileNet-V2/V3\u3001ViT-B16\uff09\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6761\u4ef6\u4e0b\u91c7\u7528focal loss\u3001\u60a3\u8005\u7ea7\u6570\u636e\u5212\u5206\u4ee5\u9632\u6b62\u6cc4\u6f0f\uff0c\u5e76\u7528Grad-CAM\u751f\u6210\u53ef\u89c6\u5316\u70ed\u56fe\u8fdb\u884c\u5b9a\u4f4d\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u5728Kermany CXR\u6570\u636e\u96c6\u4e0a\u8868\u660e\uff1aResNet-18\u548cEfficientNet-B0\u8fbe\u5230\u6700\u597d\u6d4b\u8bd5\u51c6\u786e\u738798%\u3001ROC-AUC=0.997\u3001F1=0.987\uff1bMobileNet-V2\u5728\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6210\u672c\u95f4\u53d6\u5f97\u8f83\u4f18\u6298\u8877\u3002Grad-CAM\u53ef\u89c6\u5316\u663e\u793a\u6a21\u578b\u5173\u6ce8\u4e8e\u4e34\u5e8a\u76f8\u5173\u7684\u80ba\u90e8\u533a\u57df\u3002", "conclusion": "\u8be5\u8bba\u6587\u7ed3\u8bba\u4e3a\uff1a\u5f31\u76d1\u7763\u7684\u57fa\u4e8eGrad-CAM\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u80f8\u7247\u80ba\u708e\u5206\u7c7b\u4e0e\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u5728\u4ec5\u7528\u56fe\u50cf\u7ea7\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4e34\u5e8a\u76f8\u5173\u7684\u70ed\u56fe\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4e34\u5e8a\u4fe1\u4efb\u3002"}}
{"id": "2511.00468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00468", "abs": "https://arxiv.org/abs/2511.00468", "authors": ["Panwang Pan", "Tingting Shen", "Chenxin Li", "Yunlong Lin", "Kairun Wen", "Jingjing Zhao", "Yixuan Yuan"], "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation", "comment": "Accepted to NeurIPS 2025; Project page: [this\n  URL](https://paulpanwang.github.io/HumanCrafter)", "summary": "Recent advances in generative models have achieved high-fidelity in 3D human\nreconstruction, yet their utility for specific tasks (e.g., human 3D\nsegmentation) remains constrained. We propose HumanCrafter, a unified framework\nthat enables the joint modeling of appearance and human-part semantics from a\nsingle image in a feed-forward manner. Specifically, we integrate human\ngeometric priors in the reconstruction stage and self-supervised semantic\npriors in the segmentation stage. To address labeled 3D human datasets\nscarcity, we further develop an interactive annotation procedure for generating\nhigh-quality data-label pairs. Our pixel-aligned aggregation enables cross-task\nsynergy, while the multi-task objective simultaneously optimizes texture\nmodeling fidelity and semantic consistency. Extensive experiments demonstrate\nthat HumanCrafter surpasses existing state-of-the-art methods in both 3D\nhuman-part segmentation and 3D human reconstruction from a single image.", "AI": {"tldr": "\u63d0\u51faHumanCrafter\uff0c\u4e00\u79cd\u7ed3\u5408\u51e0\u4f55\u4e0e\u81ea\u76d1\u7763\u8bed\u4e49\u5148\u9a8c\u7684\u5355\u56fe\u50cf\u524d\u9988\u5f0f\u591a\u4efb\u52a1\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u5bf9\u9f50\u548c\u4ea4\u4e92\u5f0f\u6807\u6ce8\u63d0\u5347\u6570\u636e\u4e0e\u6a21\u578b\u8d28\u91cf\uff0c\u57283D\u4eba\u4f53\u91cd\u5efa\u4e0e\u90e8\u4f4d\u5206\u5272\u4e0a\u8d85\u8d8aSOTA\u3002", "motivation": "\u5c3d\u7ba1\u751f\u6210\u6a21\u578b\u57283D\u4eba\u4f53\u91cd\u5efa\u4e0a\u53d6\u5f97\u9ad8\u4fdd\u771f\u5ea6\uff0c\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\uff08\u59823D\u4eba\u4f53\u90e8\u4f4d\u5206\u5272\uff09\u4e0a\u7684\u9002\u7528\u6027\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u8054\u5408\u5904\u7406\u5916\u89c2\u4e0e\u8bed\u4e49\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4e14\u8981\u7f13\u89e3\u5e26\u6807\u7b7e3D\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "method": "\u5728\u91cd\u5efa\u9636\u6bb5\u6574\u5408\u4eba\u4f53\u51e0\u4f55\u5148\u9a8c\uff0c\u5728\u5206\u5272\u9636\u6bb5\u91c7\u7528\u81ea\u76d1\u7763\u8bed\u4e49\u5148\u9a8c\uff1b\u8bbe\u8ba1\u50cf\u7d20\u5bf9\u9f50\u805a\u5408\u6a21\u5757\u7528\u4e8e\u8de8\u4efb\u52a1\u4fe1\u606f\u878d\u5408\uff0c\u5e76\u91c7\u7528\u591a\u4efb\u52a1\u635f\u5931\u540c\u65f6\u4f18\u5316\u7eb9\u7406\u91cd\u5efa\u8d28\u91cf\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\uff1b\u5f00\u53d1\u4ea4\u4e92\u5f0f\u6ce8\u91ca\u6d41\u7a0b\u4ee5\u6269\u51453D\u4eba\u4f53\u5e26\u6807\u7b7e\u6570\u636e\u96c6\u3002", "result": "\u5728\u5355\u5f20\u56fe\u50cf\u76843D\u4eba\u4f53\u91cd\u5efa\u4e0e\u4eba\u4f53\u90e8\u4f4d\u5206\u5272\u4efb\u52a1\u4e0a\uff0cHumanCrafter\u5728\u5927\u91cf\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u5206\u5272\u51c6\u786e\u7387\u548c\u91cd\u5efa\u4fdd\u771f\u5ea6\u3002", "conclusion": "HumanCrafter\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u524d\u9988\u6846\u67b6\uff0c\u540c\u65f6\u5efa\u6a21\u5916\u89c2\u548c\u4eba\u4f53\u90e8\u4f4d\u8bed\u4e49\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u4eba\u4f53\u5e76\u8fdb\u884c\u8bed\u4e49\u5206\u5272\u3002\u901a\u8fc7\u5728\u91cd\u5efa\u4e2d\u5f15\u5165\u51e0\u4f55\u5148\u9a8c\u3001\u5728\u5206\u5272\u4e2d\u5f15\u5165\u81ea\u76d1\u7763\u8bed\u4e49\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u50cf\u7d20\u5bf9\u9f50\u805a\u5408\u5b9e\u73b0\u4efb\u52a1\u95f4\u534f\u540c\uff0c\u914d\u5408\u4ea4\u4e92\u5f0f\u6807\u6ce8\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2511.00472", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00472", "abs": "https://arxiv.org/abs/2511.00472", "authors": ["Navodini Wijethilake", "Marina Ivory", "Oscar MacCormac", "Siddhant Kumar", "Aaron Kujawa", "Lorena Garcia-Foncillas Macias", "Rebecca Burger", "Amanda Hitchings", "Suki Thomson", "Sinan Barazi", "Eleni Maratos", "Rupert Obholzer", "Dan Jiang", "Fiona McClenaghan", "Kazumi Chia", "Omar Al-Salihi", "Nick Thomas", "Steve Connor", "Tom Vercauteren", "Jonathan Shapey"], "title": "Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations", "comment": null, "summary": "Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance\nImaging (MRI) is essential for patient management but often requires\ntime-intensive manual annotations by experts. While recent advances in deep\nlearning (DL) have facilitated automated segmentation, challenges remain in\nachieving robust performance across diverse datasets and complex clinical\ncases. We present an annotated dataset stemming from a bootstrapped DL-based\nframework for iterative segmentation and quality refinement of VS in MRI. We\ncombine data from multiple centres and rely on expert consensus for\ntrustworthiness of the annotations. We show that our approach enables effective\nand resource-efficient generalisation of automated segmentation models to a\ntarget data distribution. The framework achieved a significant improvement in\nsegmentation accuracy with a Dice Similarity Coefficient (DSC) increase from\n0.9125 to 0.9670 on our target internal validation dataset, while maintaining\nstable performance on representative external datasets. Expert evaluation on\n143 scans further highlighted areas for model refinement, revealing nuanced\ncases where segmentation required expert intervention. The proposed approach is\nestimated to enhance efficiency by approximately 37.4% compared to the\nconventional manual annotation process. Overall, our human-in-the-loop model\ntraining approach achieved high segmentation accuracy, highlighting its\npotential as a clinically adaptable and generalisable strategy for automated VS\nsegmentation in diverse clinical settings. The dataset includes 190 patients,\nwith tumour annotations available for 534 longitudinal contrast-enhanced\nT1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans\nfrom 6 patients. This dataset is publicly accessible on The Cancer Imaging\nArchive (TCIA) (https://doi.org/10.7937/bq0z-xa62).", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u4e3e\u5f0f\u4eba\u673a\u4ea4\u4e92\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u4e2d\u5fc3\u4e13\u5bb6\u5171\u8bc6\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u524d\u5ead\u795e\u7ecf\u9798\u7624\u5728MRI\u4e0a\u7684\u81ea\u52a8\u5206\u5272\u7cbe\u5ea6\uff08DSC 0.9125\u21920.9670\uff09\uff0c\u5728\u5916\u90e8\u6570\u636e\u4e0a\u4fdd\u6301\u7a33\u5065\uff0c\u6ce8\u91ca\u6548\u7387\u7ea6\u63d0\u534737%\uff0c\u5e76\u516c\u5f00\u4e86534\u4f8b\u5e26\u6ce8\u91ca\u7684T1CE\u626b\u63cf\u6570\u636e\u3002", "motivation": "\u624b\u52a8\u6807\u6ce8VS\u5728MRI\u4e0a\u65e2\u8017\u65f6\u53c8\u9700\u4e13\u5bb6\u53c2\u4e0e\uff0c\u4e14\u73b0\u6709\u81ea\u52a8\u5316\u5206\u5272\u65b9\u6cd5\u5728\u8de8\u4e2d\u5fc3\u3001\u591a\u6837\u5316\u4e34\u5e8a\u573a\u666f\u4e0b\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4f5c\u8005\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u4e14\u53ef\u63a8\u5e7f\u7684\u6807\u6ce8\u4e0e\u8bad\u7ec3\u7b56\u7565\uff0c\u63d0\u9ad8\u81ea\u52a8\u5206\u5272\u6a21\u578b\u5728\u76ee\u6807\u5206\u5e03\u4e0a\u7684\u6027\u80fd\u5e76\u51cf\u5c11\u4eba\u5de5\u8d1f\u62c5\u3002", "method": "\u6784\u5efa\u591a\u4e2d\u5fc3\u3001\u7ecf\u4e13\u5bb6\u5171\u8bc6\u5ba1\u6838\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff1b\u91c7\u7528\u81ea\u4e3e\u5f0fDL\u6a21\u578b\u8fdb\u884c\u521d\u59cb\u5206\u5272\uff1b\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u7684\u8fed\u4ee3\u6d41\u7a0b\u5bf9\u5206\u5272\u7ed3\u679c\u8fdb\u884c\u8d28\u91cf\u5ba1\u67e5\u548c\u4fee\u6b63\uff0c\u5f62\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff1b\u5c06\u8fd9\u4e9b\u6807\u6ce8\u7528\u4e8e\u518d\u8bad\u7ec3\u6a21\u578b\u4ee5\u9002\u5e94\u76ee\u6807\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u5185\u90e8\u76ee\u6807\u9a8c\u8bc1\u96c6\u4e0a\uff0c\u5206\u5272\u7684DSC\u75310.9125\u63d0\u5347\u52300.9670\uff1b\u5728\u4ee3\u8868\u6027\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff1b\u4e13\u5bb6\u5bf9143\u4f8b\u626b\u63cf\u7684\u8bc4\u4f30\u6307\u51fa\u82e5\u5e72\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u7ec6\u5fae\u60c5\u51b5\uff1b\u76f8\u8f83\u4e8e\u4f20\u7edf\u5168\u624b\u5de5\u6807\u6ce8\u6d41\u7a0b\uff0c\u4f30\u8ba1\u6548\u7387\u63d0\u5347\u7ea637.4%\u3002\u6570\u636e\u96c6\u5305\u542b190\u540d\u60a3\u8005\uff0c184\u540d\u60a3\u8005\u7684534\u6b21T1CE\u5e26\u6ce8\u91ca\uff0c\u53e6\u67096\u540d\u60a3\u8005\u7684T2\u5e8f\u5217\u672a\u6ce8\u91ca\uff0c\u5e76\u5df2\u516c\u5f00\u53d1\u5e03\u4e8eTCIA\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u673a\u4ea4\u4e92\uff08human-in-the-loop\uff09\u548c\u81ea\u4e3e\uff08bootstrapped\uff09\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8fed\u4ee3\u5730\u751f\u6210\u548c\u4f18\u5316\u524d\u5ead\u795e\u7ecf\u9798\u7624(VS)\u5728MRI\u4e0a\u7684\u5206\u5272\u6807\u6ce8\u3002\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u76ee\u6807\u6570\u636e\u5206\u5e03\u4e0a\u7684\u5206\u5272\u7cbe\u5ea6\uff0c\u540c\u65f6\u5728\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u6ce8\u91ca\u6548\u7387\u4e0a\u5e26\u6765\u660e\u663e\u6539\u8fdb\u3002"}}
{"id": "2511.00480", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00480", "abs": "https://arxiv.org/abs/2511.00480", "authors": ["Weihao Bo", "Yanpeng Sun", "Yu Wang", "Xinyu Zhang", "Zechao Li"], "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts", "comment": null, "summary": "In this paper, we introduce FedMGP, a new paradigm for personalized federated\nprompt learning in vision-language models. FedMGP equips each client with\nmultiple groups of paired textual and visual prompts, enabling the model to\ncapture diverse, fine-grained semantic and instance-level cues. A diversity\nloss is introduced to drive each prompt group to specialize in distinct and\ncomplementary semantic aspects, ensuring that the groups collectively cover a\nbroader range of local characteristics. During communication, FedMGP employs a\ndynamic prompt aggregation strategy based on similarity-guided probabilistic\nsampling: each client computes the cosine similarity between its prompt groups\nand the global prompts from the previous round, then samples s groups via a\nsoftmax-weighted distribution. This soft selection mechanism preferentially\naggregates semantically aligned knowledge while still enabling exploration of\nunderrepresented patterns effectively balancing the preservation of common\nknowledge with client-specific features. Notably, FedMGP maintains parameter\nefficiency by redistributing a fixed prompt capacity across multiple groups,\nachieving state-of-the-art performance with the lowest communication parameters\namong all federated prompt learning methods. Theoretical analysis shows that\nour dynamic aggregation strategy promotes robust global representation learning\nby reinforcing shared semantics while suppressing client-specific noise.\nExtensive experiments demonstrate that FedMGP consistently outperforms prior\napproaches in both personalization and domain generalization across diverse\nfederated vision-language benchmarks. The code will be released on\nhttps://github.com/weihao-bo/FedMGP.git.", "AI": {"tldr": "\u63d0\u51faFedMGP\uff1a\u6bcf\u5ba2\u6237\u7aef\u591a\u7ec4\u6587\u672c-\u89c6\u89c9prompt+\u591a\u6837\u6027\u635f\u5931\uff0c\u6309\u76f8\u4f3c\u5ea6\u6982\u7387\u91c7\u6837\u805a\u5408\uff0c\u517c\u987e\u4e2a\u6027\u5316\u4e0e\u5171\u4eab\u77e5\u8bc6\uff0c\u53c2\u6570\u9ad8\u6548\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8054\u90a6\u63d0\u793a\u5b66\u4e60\u5f80\u5f80\u53ea\u4f7f\u7528\u5355\u4e00\u63d0\u793a\u6216\u65e0\u6cd5\u517c\u987e\u591a\u6837\u6027\u4e0e\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u5bf9\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u5b9e\u4f8b\u7ea7\u4fe1\u606f\u5efa\u6a21\u4e0d\u8db3\uff0c\u4e14\u901a\u4fe1\u6548\u7387\u548c\u53c2\u6570\u91cf\u6709\u5f85\u4f18\u5316\u3002", "method": "\u4e3a\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5206\u914d\u591a\u4e2aprompt\u7ec4\uff1b\u8bbe\u8ba1\u591a\u6837\u6027\u635f\u5931\u4fc3\u4f7f\u5404\u7ec4\u4e13\u6ce8\u4e0d\u540c\u8bed\u4e49\uff1b\u901a\u4fe1\u65f6\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97softmax\u6982\u7387\u91c7\u6837s\u7ec4\u8fdb\u884c\u805a\u5408\uff1b\u4fdd\u6301\u56fa\u5b9aprompt\u5bb9\u91cf\u4ee5\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u52a8\u6001\u805a\u5408\u6709\u52a9\u4e8e\u589e\u5f3a\u5171\u4eab\u8bed\u4e49\u5e76\u6291\u5236\u5ba2\u6237\u7aef\u566a\u58f0\uff1b\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\u5728\u4e2a\u6027\u5316\u4e0e\u57df\u6cdb\u5316\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u901a\u4fe1\u53c2\u6570\u4e0a\u4e5f\u6700\u4f18\u3002", "conclusion": "FedMGP\u901a\u8fc7\u591a\u7ec4\u6210\u5bf9\u6587\u672c\u4e0e\u89c6\u89c9prompt\u3001\u5f15\u5165\u591a\u6837\u6027\u635f\u5931\u548c\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u7684\u6982\u7387\u805a\u5408\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u7559\u516c\u5171\u77e5\u8bc6\u7684\u540c\u65f6\u5b66\u4e60\u5ba2\u6237\u7aef\u7279\u6709\u7279\u5f81\uff0c\u517c\u987e\u4e2a\u6027\u5316\u4e0e\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.00503", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00503", "abs": "https://arxiv.org/abs/2511.00503", "authors": ["Panwang Pan", "Chenguo Lin", "Jingjing Zhao", "Chenxin Li", "Yuchen Lin", "Haopeng Li", "Honglei Yan", "Kairun Wen", "Yunlong Lin", "Yixuan Yuan", "Yadong Mu"], "title": "Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models", "comment": null, "summary": "We introduce Diff4Splat, a feed-forward method that synthesizes controllable\nand explicit 4D scenes from a single image. Our approach unifies the generative\npriors of video diffusion models with geometry and motion constraints learned\nfrom large-scale 4D datasets. Given a single input image, a camera trajectory,\nand an optional text prompt, Diff4Splat directly predicts a deformable 3D\nGaussian field that encodes appearance, geometry, and motion, all in a single\nforward pass, without test-time optimization or post-hoc refinement. At the\ncore of our framework lies a video latent transformer, which augments video\ndiffusion models to jointly capture spatio-temporal dependencies and predict\ntime-varying 3D Gaussian primitives. Training is guided by objectives on\nappearance fidelity, geometric accuracy, and motion consistency, enabling\nDiff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate\nthe effectiveness of Diff4Splatacross video generation, novel view synthesis,\nand geometry extraction, where it matches or surpasses optimization-based\nmethods for dynamic scene synthesis while being significantly more efficient.", "AI": {"tldr": "Diff4Splat\u901a\u8fc7\u89c6\u9891\u6f5c\u5728\u53d8\u6362\u5668\u7ed3\u5408\u89c6\u9891\u6269\u6563\u5148\u9a8c\u4e0e4D\u51e0\u4f55/\u8fd0\u52a8\u7ea6\u675f\uff0c\u5b9e\u73b0\u4ece\u5355\u5f20\u56fe\u50cf\u5728\u4e00\u6b21\u524d\u5411\u4f20\u9012\u4e2d\u5feb\u901f\u751f\u6210\u53ef\u63a7\u4e14\u663e\u5f0f\u7684\u56db\u7ef4\u573a\u666f\uff0c\u8d28\u91cf\u4e0e\u4f18\u5316\u65b9\u6cd5\u76f8\u5f53\u4f46\u901f\u5ea6\u5feb\u5f97\u591a\uff08\u7ea630s\uff09\u3002", "motivation": "\u52a8\u673a\u662f\u5e0c\u671b\u5c06\u5f3a\u5927\u7684\u751f\u6210\u5148\u9a8c\uff08\u89c6\u9891\u6269\u6563\u6a21\u578b\uff09\u4e0e\u51e0\u4f55\u548c\u8fd0\u52a8\u77e5\u8bc6\u7ed3\u5408\uff0c\u5b9e\u73b0\u5728\u65e0\u9700\u6602\u8d35\u4f18\u5316\u7684\u60c5\u51b5\u4e0b\u4ece\u5355\u56fe\u50cf\u9ad8\u6548\u751f\u6210\u53ef\u63a7\u76844D\u52a8\u6001\u573a\u666f\u3002", "method": "\u65b9\u6cd5\u6838\u5fc3\u662f\u4e00\u4e2a\u89c6\u9891\u6f5c\u5728\u53d8\u6362\u5668\uff08video latent transformer\uff09\uff0c\u7528\u4e8e\u6269\u5c55\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u8054\u5408\u5efa\u6a21\u65f6\u7a7a\u4f9d\u8d56\u5e76\u76f4\u63a5\u9884\u6d4b\u968f\u65f6\u95f4\u53d8\u5316\u76843D\u9ad8\u65af\u57fa\u5143\uff08deformable 3D Gaussian field\uff09\u3002\u8f93\u5165\u5305\u62ec\u5355\u5f20\u56fe\u50cf\u3001\u76f8\u673a\u8f68\u8ff9\u548c\u53ef\u9009\u6587\u672c\u63d0\u793a\uff0c\u8bad\u7ec3\u901a\u8fc7\u5916\u89c2\u4fdd\u771f\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u7b49\u76ee\u6807\u6765\u7ea6\u675f\u3002", "result": "Diff4Splat\u5728\u89c6\u9891\u751f\u6210\u3001\u65b0\u89c6\u89d2\u5408\u6210\u548c\u51e0\u4f55\u63d0\u53d6\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u80fd\u5339\u914d\u6216\u8d85\u8d8a\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u6548\u7387\u663e\u8457\u66f4\u9ad8\uff0c\u53ef\u5728\u7ea630\u79d2\u5185\u5408\u6210\u9ad8\u8d28\u91cf4D\u573a\u666f\u3002", "conclusion": "Diff4Splat\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u5feb\u901f\u751f\u6210\u53ef\u63a7\u4e14\u660e\u786e\u76844D\u573a\u666f\u7684\u524d\u9988\u65b9\u6cd5\uff0c\u517c\u5bb9\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u5e76\u7ed3\u54084D\u6570\u636e\u96c6\u7684\u51e0\u4f55\u548c\u8fd0\u52a8\u7ea6\u675f\uff0c\u80fd\u5728\u5355\u6b21\u524d\u5411\u63a8\u65ad\u4e2d\u9884\u6d4b\u53ef\u53d8\u5f62\u76843D\u9ad8\u65af\u573a\uff0c\u4ece\u800c\u65e0\u9700\u6d4b\u8bd5\u65f6\u4f18\u5316\u6216\u540e\u5904\u7406\u3002"}}
{"id": "2511.00504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00504", "abs": "https://arxiv.org/abs/2511.00504", "authors": ["Hai-Dang Nguyen", "Ha-Hieu Pham", "Hao T. Nguyen", "Huy-Hieu Pham"], "title": "VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning", "comment": "ISBI submission. Contains 5 pages, 2 figures, and 6 tables. Code &\n  data: https://huggingface.co/datasets/Dangindev/VinDR-CXR-VQA", "summary": "We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable\nMedical Visual Question Answering (Med-VQA) with spatial grounding. The dataset\ncontains 17,597 question-answer pairs across 4,394 images, each annotated with\nradiologist-verified bounding boxes and clinical reasoning explanations. Our\nquestion taxonomy spans six diagnostic types-Where, What, Is there, How many,\nWhich, and Yes/No-capturing diverse clinical intents. To improve reliability,\nwe construct a balanced distribution of 41.7% positive and 58.3% negative\nsamples, mitigating hallucinations in normal cases. Benchmarking with\nMedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over\nbaseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance\nreproducible and clinically grounded Med-VQA research. The dataset and\nevaluation tools are publicly available at\nhuggingface.co/datasets/Dangindev/VinDR-CXR-VQA.", "AI": {"tldr": "VinDr-CXR-VQA\u662f\u4e00\u4e2a\u7ecf\u653e\u5c04\u79d1\u533b\u751f\u9a8c\u8bc1\u3001\u5e26\u8fb9\u754c\u6846\u548c\u4e34\u5e8a\u63a8\u7406\u89e3\u91ca\u7684\u5927\u89c4\u6a21\u80f8\u7247VQA\u6570\u636e\u96c6\uff084,394\u5f20\u56fe\uff0c17,597\u95ee\u7b54\uff09\uff0c\u8986\u76d66\u7c7b\u8bca\u65ad\u95ee\u9898\u5e76\u5e73\u8861\u9633\u6027/\u9634\u6027\u6837\u672c\uff0c\u516c\u5f00\u53ef\u7528\uff0c\u80fd\u63d0\u5347Med-VQA\u6a21\u578b\u7684\u56de\u7b54\u51c6\u786e\u6027\u4e0e\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u73b0\u6709Med-VQA\u7f3a\u4e4f\u5927\u89c4\u6a21\u5e26\u7a7a\u95f4\u5b9a\u4f4d\u4e0e\u53ef\u89e3\u91ca\u6027\u6807\u6ce8\u7684\u6570\u636e\uff0c\u4e14\u5728\u6b63\u5e38\u6848\u4f8b\u4e2d\u5bb9\u6613\u4ea7\u751f\u5e7b\u89c9\uff08hallucination\uff09\uff1b\u56e0\u6b64\u9700\u4e00\u4e2a\u4e34\u5e8a\u9a8c\u8bc1\u3001\u53ef\u590d\u73b0\u4e14\u5177\u5b9a\u4f4d\u89e3\u91ca\u80fd\u529b\u7684\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u7814\u7a76\u3002", "method": "\u6784\u5efa\u5305\u542b4,394\u5f20\u80f8\u7247\u548c17,597\u4e2a\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6240\u6709\u95ee\u7b54\u5747\u914d\u5907\u653e\u5c04\u79d1\u533b\u751f\u9a8c\u8bc1\u7684\u8fb9\u754c\u6846\u548c\u4e34\u5e8a\u63a8\u7406\u89e3\u91ca\uff1b\u8bbe\u8ba1\u6db5\u76d66\u79cd\u8bca\u65ad\u7c7b\u578b\u7684\u95ee\u9898\u5206\u7c7b\uff0c\u5e76\u5e73\u8861\u6b63\u8d1f\u6837\u672c\uff0841.7%\u9633\u6027\uff0c58.3%\u9634\u6027\uff09\uff1b\u7528MedGemma-4B-it\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u5e76\u8bc4\u4f30\u56de\u7b54\u4e0e\u5b9a\u4f4d\u6027\u80fd\u3002", "result": "\u53d1\u5e03\u4e86VinDr-CXR-VQA\u6570\u636e\u96c6\u5e76\u516c\u5f00\u8bc4\u6d4b\u5de5\u5177\uff1b\u5728MedGemma-4B-it\u57fa\u51c6\u4e0a\u63d0\u9ad8\u4e86\u6027\u80fd\uff08F1=0.624\uff0c\u8f83\u57fa\u7ebf\u63d0\u534711.8%\uff09\uff0c\u4e14\u652f\u6301\u75c5\u7076\u5b9a\u4f4d\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u5177\u5907\u5b9a\u4f4d\u89e3\u91ca\u80fd\u529b\u7684\u80f8\u7247\u95ee\u7b54\u6570\u636e\u96c6VinDr-CXR-VQA\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u8d44\u6e90\u3002"}}
{"id": "2511.00510", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.00510", "abs": "https://arxiv.org/abs/2511.00510", "authors": ["Kai Luo", "Hao Shi", "Kunyu Peng", "Fei Teng", "Sheng Wu", "Kaiwei Wang", "Kailun Yang"], "title": "OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback", "comment": "Extended version of CVPR 2025 paper arXiv:2503.04565. Datasets and\n  code will be made publicly available at https://github.com/xifen523/OmniTrack", "summary": "This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,\nwhich introduces unique challenges including a 360{\\deg} Field of View (FoV),\nresolution dilution, and severe view-dependent distortions. Conventional MOT\nmethods designed for narrow-FoV pinhole cameras generalize unsatisfactorily\nunder these conditions. To address panoramic distortion, large search space,\nand identity ambiguity under a 360{\\deg} FoV, OmniTrack++ adopts a\nfeedback-driven framework that progressively refines perception with trajectory\ncues. A DynamicSSM block first stabilizes panoramic features, implicitly\nalleviating geometric distortion. On top of normalized representations,\nFlexiTrack Instances use trajectory-informed feedback for flexible localization\nand reliable short-term association. To ensure long-term robustness, an\nExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts\ndesign, enabling recovery from fragmented tracks and reducing identity drift.\nFinally, a Tracklet Management module adaptively switches between end-to-end\nand tracking-by-detection modes according to scene dynamics, offering a\nbalanced and scalable solution for panoramic MOT. To support rigorous\nevaluation, we establish the EmboTrack benchmark, a comprehensive dataset for\npanoramic MOT that includes QuadTrack, captured with a quadruped robot, and\nBipTrack, collected with a bipedal wheel-legged robot. Together, these datasets\nspan wide-angle environments and diverse motion patterns, providing a\nchallenging testbed for real-world panoramic perception. Extensive experiments\non JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art\nperformance, yielding substantial HOTA improvements of +25.5% on JRDB and\n+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be\nmade publicly available at https://github.com/xifen523/OmniTrack.", "AI": {"tldr": "\u9488\u5bf9360\u00b0\u5168\u666fMOT\uff0cOmniTrack++\u7528\u7279\u5f81\u7a33\u5b9a\u5316\u3001\u8f68\u8ff9\u53cd\u9988\u5b9e\u4f8b\u5316\u3001\u4e13\u5bb6\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u8ddf\u7ba1\u7406\u56db\u6a21\u5757\u8054\u5408\u63d0\u5347\u77ed\u671f\u5173\u8054\u4e0e\u957f\u671f\u8eab\u4efd\u4fdd\u6301\uff0c\u5728\u65b0\u6570\u636e\u96c6EmboTrack\u548cJRDB\u4e0a\u663e\u8457\u8d85\u8d8a\u539f\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7a84\u89c6\u573a\u76f8\u673a\u7684MOT\u65b9\u6cd5\u5728360\u00b0\u5168\u666f\u56fe\u50cf\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u53d7\u5206\u8fa8\u7387\u7a00\u91ca\u3001\u89c6\u89d2\u4f9d\u8d56\u7578\u53d8\u548c\u5e7f\u57df\u641c\u7d22\u7a7a\u95f4\u5f71\u54cd\uff0c\u9700\u65b0\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u5168\u666f\u573a\u666f\u7684\u7279\u6709\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86DynamicSSM\u7528\u4e8e\u7a33\u5b9a\u5316\u5168\u666f\u7279\u5f81\uff0cFlexiTrack Instances\u5728\u5f52\u4e00\u5316\u8868\u793a\u4e0a\u5229\u7528\u8f68\u8ff9\u53cd\u9988\u8fdb\u884c\u67d4\u6027\u5b9a\u4f4d\u4e0e\u77ed\u671f\u5173\u8054\uff0cExpertTrack Memory\u901a\u8fc7Mixture-of-Experts\u6574\u5408\u5916\u89c2\u7ebf\u7d22\u7528\u4e8e\u957f\u671f\u6062\u590d\uff0c\u4ee5\u53caTracklet Management\u5728\u7aef\u5230\u7aef\u4e0e\u68c0\u6d4b\u8ddf\u8e2a\u6a21\u5f0f\u95f4\u81ea\u9002\u5e94\u5207\u6362\u3002", "result": "\u5728JRDB\u548c\u81ea\u5efaEmboTrack\uff08\u542bQuadTrack\u4e0eBipTrack\uff09\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u76f8\u8f83\u539fOmniTrack\u5728JRDB\u4e0aHOTA\u63d0\u9ad8+25.5%\uff0c\u5728QuadTrack\u4e0a\u63d0\u9ad8+43.07%\u3002", "conclusion": "OmniTrack++\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u7684\u591a\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5168\u666f\u56fe\u50cf\u4e0b\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c0f\u4e86\u56e0\u5168\u666f\u7578\u53d8\u548c\u89c6\u89d2\u53d8\u5316\u5e26\u6765\u7684\u8eab\u4efd\u6f02\u79fb\u548c\u8ddf\u8e2a\u788e\u7247\u5316\u95ee\u9898\u3002"}}
{"id": "2511.00511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00511", "abs": "https://arxiv.org/abs/2511.00511", "authors": ["Panwang Pan", "Jingjing Zhao", "Yuchen Lin", "Chenguo Lin", "Chenxin Li", "Haopeng Li", "Honglei Yan", "Tingting Shen", "Yadong Mu"], "title": "ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation", "comment": null, "summary": "Video generative models pretrained on large-scale datasets can produce\nhigh-quality videos, but are often conditioned on text or a single image,\nlimiting controllability and applicability. We introduce ID-Composer, a novel\nframework that addresses this gap by tackling multi-subject video generation\nfrom a text prompt and reference images. This task is challenging as it\nrequires preserving subject identities, integrating semantics across subjects\nand modalities, and maintaining temporal consistency. To faithfully preserve\nthe subject consistency and textual information in synthesized videos,\nID-Composer designs a \\textbf{hierarchical identity-preserving attention\nmechanism}, which effectively aggregates features within and across subjects\nand modalities. To effectively allow for the semantic following of user\nintention, we introduce \\textbf{semantic understanding via pretrained\nvision-language model (VLM)}, leveraging VLM's superior semantic understanding\nto provide fine-grained guidance and capture complex interactions between\nmultiple subjects. Considering that standard diffusion loss often fails in\naligning the critical concepts like subject ID, we employ an \\textbf{online\nreinforcement learning phase} to drive the overall training objective of\nID-Composer into RLVR. Extensive experiments demonstrate that our model\nsurpasses existing methods in identity preservation, temporal consistency, and\nvideo quality.", "AI": {"tldr": "\u63d0\u51faID-Composer\uff1a\u7ed3\u5408\u5c42\u7ea7\u8eab\u4efd\u6ce8\u610f\u529b\u3001VLM\u8bed\u4e49\u7406\u89e3\u4e0e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u5584\u8eab\u4efd\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u9075\u5faa\u4e0e\u65f6\u95f4\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\u591a\u88ab\u9650\u5236\u4e8e\u6587\u672c\u6216\u5355\u5f20\u56fe\u50cf\u6761\u4ef6\uff0c\u7f3a\u4e4f\u5bf9\u591a\u4e3b\u4f53\u89c6\u9891\u751f\u6210\u7684\u53ef\u63a7\u6027\u4e0e\u9002\u7528\u6027\uff0c\u4e14\u8981\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u3001\u8de8\u4e3b\u4f53\u4e0e\u8de8\u6a21\u6001\u8bed\u4e49\u6574\u5408\u53ca\u65f6\u95f4\u4e00\u81f4\u6027\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u5c42\u7ea7\u8eab\u4efd\u4fdd\u6301\u6ce8\u610f\u529b\uff08\u5728\u4e3b\u4f53\u5185\u4e0e\u4e3b\u4f53\u95f4\u4ee5\u53ca\u8de8\u6a21\u6001\u805a\u5408\u7279\u5f81\uff09\u3001\u5f15\u5165\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5f15\u5bfc\uff0c\u4ee5\u53ca\u5728\u6807\u51c6\u6269\u6563\u8bad\u7ec3\u540e\u52a0\u5165\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u4ee5\u5f3a\u5316\u5173\u952e\u6982\u5ff5\u5bf9\u9f50\uff08\u5982\u4e3b\u4f53ID\uff09\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cID-Composer\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u9891\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ID-Composer\u63d0\u51fa\u4e86\u4e00\u4e2a\u80fd\u591f\u4ece\u6587\u672c\u63d0\u793a\u548c\u591a\u5f20\u53c2\u8003\u56fe\u50cf\u751f\u6210\u591a\u4e3b\u4f53\u89c6\u9891\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u7ea7\u5316\u7684\u8eab\u4efd\u4fdd\u6301\u6ce8\u610f\u529b\u673a\u5236\u3001\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\u4ee5\u53ca\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8bad\u7ec3\u76ee\u6807\uff08RLVR\uff09\u6765\u63d0\u5347\u8eab\u4efd\u4fdd\u6301\u3001\u8bed\u4e49\u9075\u5faa\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.00523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00523", "abs": "https://arxiv.org/abs/2511.00523", "authors": ["Fangyu Wu", "Yujun Cai"], "title": "SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation", "comment": null, "summary": "Vision language models such as CLIP have shown remarkable performance in zero\nshot classification, but remain susceptible to spurious correlations, where\nirrelevant visual features influence predictions. Existing debiasing methods\noften require access to training data and explicit group labels to perform\nfine-tuning or adjust embeddings, which limits their practicality in real-world\nsettings. Test-time methods attempt to avoid this constraint, but many still\ndepend on prior knowledge of dataset specific biases, limiting their\ngeneralizability in open set settings. In this work, we propose a test-time\ndebiasing method for ViT based CLIP models that requires no additional training\nor assumptions of bias annotations. Our approach uses a pretrained segmentation\nmodel to isolate the target visual attribute, then adjusts the non target\nregions so that their embeddings are uniformly similar to all class specific\ntext prompts. This procedure removes unintended bias signals from confounding\nvisual regions while preserving the target attribute. Experiments on Waterbirds\nand CelebA show that our method outperforms existing test-time debiasing\napproaches in both group robustness metrics and Attention IoU. These results\ndemonstrate the effectiveness of segmentation guided interventions for scalable\nand annotation free bias mitigation in vision language models.", "AI": {"tldr": "\u901a\u8fc7\u5206\u5272\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9694\u79bb\u76ee\u6807\u5c5e\u6027\u5e76\u5747\u5300\u5316\u975e\u76ee\u6807\u533a\u57df\u5d4c\u5165\uff0c\u672c\u6587\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u4e0e\u504f\u89c1\u6ce8\u91ca\u7684CLIP\u53bb\u504f\uff0c\u63d0\u5347\u4e86\u7fa4\u4f53\u9c81\u68d2\u6027\u548c\u6ce8\u610f\u529b\u5bf9\u9f50\u6027\u3002", "motivation": "CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u79c0\u4f46\u6613\u53d7\u4e0e\u4efb\u52a1\u65e0\u5173\u7684\u89c6\u89c9\u7279\u5f81\u5f71\u54cd\uff0c\u65e2\u5f80\u53bb\u504f\u65b9\u6cd5\u5e38\u9700\u8bad\u7ec3\u6570\u636e\u6216\u504f\u89c1\u6807\u7b7e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff1b\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8fd9\u4e9b\u989d\u5916\u8d44\u6e90\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6cdb\u5316\u4e0e\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5bf9\u8f93\u5165\u56fe\u50cf\u4f7f\u7528\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u5206\u79bb\u76ee\u6807\u533a\u57df\uff1b\u5bf9\u975e\u76ee\u6807\u533a\u57df\u8c03\u6574\uff08\u5982\u66ff\u6362\u6216\u5747\u5300\u5316\uff09\u4f7f\u5176\u5728CLIP\u6587\u672c\u63d0\u793a\u7a7a\u95f4\u4e2d\u5bf9\u6240\u6709\u7c7b\u522b\u7684\u5d4c\u5165\u76f8\u4f3c\uff1b\u4fdd\u6301\u76ee\u6807\u533a\u57df\u4e0d\u53d8\u4ee5\u4fdd\u7559\u5408\u6cd5\u5224\u522b\u4fe1\u606f\uff1b\u5728\u6d4b\u8bd5\u65f6\u76f4\u63a5\u5e94\u7528\uff0c\u65e0\u9700Fine-tune\u6216\u989d\u5916\u6807\u7b7e\u3002", "result": "\u5728Waterbirds\u548cCelebA\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7fa4\u4f53\u9c81\u68d2\u6027\u6307\u6807\u548cAttention IoU\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u5206\u5272\u5f15\u5bfc\u5e72\u9884\u5728\u6d88\u9664\u504f\u89c1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u548c\u504f\u89c1\u6807\u6ce8\u7684\u6d4b\u8bd5\u65f6\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5272\u6a21\u578b\u9694\u79bb\u76ee\u6807\u5c5e\u6027\u5e76\u5bf9\u975e\u76ee\u6807\u533a\u57df\u8fdb\u884c\u5e72\u9884\uff0c\u4ece\u800c\u6d88\u9664\u6df7\u6dc6\u89c6\u89c9\u4fe1\u53f7\uff0c\u63d0\u5347ViT\u57faCLIP\u5728\u7fa4\u4f53\u9c81\u68d2\u6027\u548c\u6ce8\u610f\u529bIoU\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2511.00524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00524", "abs": "https://arxiv.org/abs/2511.00524", "authors": ["Jihao Gu", "Kun Li", "He Wang", "Kaan Ak\u015fit"], "title": "Text-guided Fine-Grained Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) aims to identify anomalous events within video\nsegments. In scenarios such as surveillance or industrial process monitoring,\nanomaly detection is of critical importance. While existing approaches are\nsemi-automated, requiring human assessment for anomaly detection, traditional\nVADs offer limited output as either normal or anomalous. We propose Text-guided\nFine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large\nVision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)\nthat performs pixel-wise visual-textual feature alignment to generate\nfine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly\nEncoder (RAE) that transforms the heatmaps into learnable textual embeddings,\nguiding the LVLM to accurately identify and localize anomalous events in\nvideos. This significantly enhances both the granularity and interactivity of\nanomaly detection. The proposed method achieving SOTA performance by\ndemonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and\n67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,\nand subjectively verified more preferable textual description on the\nShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;\nYes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for\ntargets, 78.10 for trajectories; Yes/No accuracy: 89.73%).", "AI": {"tldr": "\u57fa\u4e8eLVLM\u7684T-VAD\u901a\u8fc7AHD\u548cRAE\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u53ef\u6587\u672c\u5316\u7684\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u53ef\u4ea4\u4e92\u6027\u5e76\u8fbeSOTA\u3002", "motivation": "\u5f25\u8865\u4f20\u7edfVAD\u4ec5\u8f93\u51fa\u6b63\u5e38/\u5f02\u5e38\u4e8c\u5143\u6807\u7b7e\u3001\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u4e0e\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\u7684\u4e0d\u8db3\uff0c\u4f7f\u68c0\u6d4b\u7ed3\u679c\u66f4\u53ef\u89e3\u91ca\u4e0e\u4ea4\u4e92\u5f0f\u3002", "method": "\u5f15\u5165\u5f02\u5e38\u70ed\u56fe\u89e3\u7801\u5668\uff08AHD\uff09\u8fdb\u884c\u50cf\u7d20\u7ea7\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u5bf9\u9f50\u751f\u6210\u5f02\u5e38\u70ed\u56fe\uff1b\u8bbe\u8ba1\u533a\u57df\u611f\u77e5\u5f02\u5e38\u7f16\u7801\u5668\uff08RAE\uff09\u5c06\u70ed\u56fe\u8f6c\u4e3a\u53ef\u5b66\u4e60\u6587\u672c\u5d4c\u5165\uff0c\u6307\u5bfcLVLM\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u5f02\u5e38\u3002", "result": "\u5728UBnormal\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5faeAUC 94.8%\u3001\u70ed\u56feRBDC/TBDC\u51c6\u786e\u738767.8%/76.7%\uff1b\u5728ShanghaiTech-based\u548cUBnormal\u4e0a\u5206\u522b\u5728\u4eba\u7c7b\u53ef\u8bfb\u6587\u672c\u63cf\u8ff0\uff08BLEU-4\u4e0eYes/No\u51c6\u786e\u7387\uff09\u4e0a\u53d6\u5f97\u9ad8\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51faT-VAD\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7528\u4e8e\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5f02\u5e38\u5b9a\u4f4d\u4e0e\u6587\u672c\u5316\u63cf\u8ff0\u3002"}}
{"id": "2511.00540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00540", "abs": "https://arxiv.org/abs/2511.00540", "authors": ["Wenbing Zhu", "Chengjie Wang", "Bin-Bin Gao", "Jiangning Zhang", "Guannan Jiang", "Jie Hu", "Zhenye Gan", "Lidong Wang", "Ziqing Zhou", "Linjie Cheng", "Yurui Pan", "Bo Peng", "Mingmin Chi", "Lizhuang Ma"], "title": "Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era", "comment": "13 pages, 4 figures and 5 tables", "summary": "Industrial Anomaly Detection (IAD) is critical for enhancing operational\nsafety, ensuring product quality, and optimizing manufacturing efficiency\nacross global industries. However, the IAD algorithms are severely constrained\nby the limitations of existing public benchmarks. Current datasets exhibit\nrestricted category diversity and insufficient scale, frequently resulting in\nmetric saturation and limited model transferability to real-world scenarios. To\naddress this gap, we introduce Real-IAD Variety, the largest and most diverse\nIAD benchmark, comprising 198,960 high-resolution images across 160 distinct\nobject categories. Its diversity is ensured through comprehensive coverage of\n28 industries, 24 material types, and 22 color variations. Our comprehensive\nexperimental analysis validates the benchmark's substantial challenge:\nstate-of-the-art multi-class unsupervised anomaly detection methods experience\nsignificant performance degradation when scaled from 30 to 160 categories.\nCrucially, we demonstrate that vision-language models exhibit remarkable\nrobustness to category scale-up, with minimal performance variation across\ndifferent category counts, significantly enhancing generalization capabilities\nin diverse industrial contexts. The unprecedented scale and complexity of\nReal-IAD Variety position it as an essential resource for training and\nevaluating next-generation foundation models for anomaly detection. By\nproviding this comprehensive benchmark with rigorous evaluation protocols\nacross multi-class unsupervised, multi-view, and zero-/few-shot settings, we\naim to accelerate research beyond domain-specific constraints, enabling the\ndevelopment of scalable, general-purpose anomaly detection systems. Real-IAD\nVariety will be made publicly available to facilitate innovation in this\ncritical field.", "AI": {"tldr": "\u63d0\u51faReal-IAD Variety\u2014\u2014\u6700\u5927\u7684\u591a\u6837\u5316\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff08198,960\u5f20\u56fe\uff0c160\u7c7b\uff09\uff0c\u5e76\u8bc1\u660e\u5f53\u524d\u591a\u7c7b\u65e0\u76d1\u7763\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7c7b\u522b\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u7c7b\u522b\u6269\u5c55\u9c81\u68d2\u6027\uff0c\u4fc3\u8fdb\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709IAD\u516c\u5f00\u57fa\u51c6\u5728\u7c7b\u522b\u591a\u6837\u6027\u548c\u89c4\u6a21\u4e0a\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u8fc1\u79fb\u6027\u548c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u9700\u8981\u66f4\u5927\u3001\u66f4\u590d\u6742\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u901a\u7528\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b198,960\u5f20\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u3001160\u4e2a\u7c7b\u522b\u3001\u8986\u76d628\u4e2a\u884c\u4e1a\u300124\u79cd\u6750\u6599\u548c22\u79cd\u989c\u8272\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff1b\u5e76\u8bbe\u8ba1\u4e25\u683c\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5305\u542b\u591a\u7c7b\u65e0\u76d1\u7763\u3001\u591a\u89c6\u89d2\u3001\u96f6/\u5c11\u6837\u672c\u7b49\u8bbe\u7f6e\uff1b\u5bf9\u6bd4\u73b0\u6709\u591a\u7c7b\u65e0\u76d1\u7763\u65b9\u6cd5\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u6570\u636e\u96c6\u96be\u5ea6\u548c\u6a21\u578b\u8868\u73b0\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u5f53\u7c7b\u522b\u4ece30\u6269\u5c55\u5230160\u65f6\uff0c\u73b0\u6709\u591a\u7c7b\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b2\uff09\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5bf9\u7c7b\u522b\u6269\u5c55\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\uff0c\u6027\u80fd\u968f\u7c7b\u522b\u6570\u53d8\u5316\u5f88\u5c0f\uff0c\u4f53\u73b0\u4e86\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86Real-IAD Variety\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u7c7b\u522b\u5355\u4e00\u3001\u89c4\u6a21\u4e0d\u8db3\u5bfc\u81f4\u7684\u6307\u6807\u9971\u548c\u548c\u6a21\u578b\u6cdb\u5316\u5dee\u95ee\u9898\u3002"}}
{"id": "2511.00542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00542", "abs": "https://arxiv.org/abs/2511.00542", "authors": ["Kailun Su", "Ziqi He", "Xi Wang", "Yang Zhou"], "title": "MIFO: Learning and Synthesizing Multi-Instance from One Image", "comment": "17 pages, 30 figures", "summary": "This paper proposes a method for precise learning and synthesizing\nmulti-instance semantics from a single image. The difficulty of this problem\nlies in the limited training data, and it becomes even more challenging when\nthe instances to be learned have similar semantics or appearance. To address\nthis, we propose a penalty-based attention optimization to disentangle similar\nsemantics during the learning stage. Then, in the synthesis, we introduce and\noptimize box control in attention layers to further mitigate semantic leakage\nwhile precisely controlling the output layout. Experimental results demonstrate\nthat our method achieves disentangled and high-quality semantic learning and\nsynthesis, strikingly balancing editability and instance consistency. Our\nmethod remains robust when dealing with semantically or visually similar\ninstances or rare-seen objects. The code is publicly available at\nhttps://github.com/Kareneveve/MIFO", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u60e9\u7f5a\u7684\u6ce8\u610f\u529b\u4f18\u5316\u4e0e\u6ce8\u610f\u529b\u5c42\u76d2\u63a7\u7684\u5355\u56fe\u591a\u5b9e\u4f8b\u8bed\u4e49\u5b66\u4e60\u4e0e\u5408\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u7f20\u76f8\u4f3c\u5b9e\u4f8b\u5e76\u9632\u6b62\u8bed\u4e49\u6cc4\u9732\uff0c\u7ed3\u679c\u663e\u793a\u9ad8\u8d28\u91cf\u4e14\u53ef\u7f16\u8f91\u3002", "motivation": "\u5728\u4ec5\u7528\u5355\u5f20\u56fe\u50cf\u8fdb\u884c\u591a\u5b9e\u4f8b\u8bed\u4e49\u5b66\u4e60\u65f6\uff0c\u8bad\u7ec3\u6570\u636e\u6781\u5ea6\u6709\u9650\uff0c\u4e14\u5f53\u5b9e\u4f8b\u8bed\u4e49\u6216\u5916\u89c2\u76f8\u8fd1\u65f6\u66f4\u96be\u4ee5\u533a\u5206\u4e0e\u5408\u6210\uff0c\u56e0\u800c\u9700\u8981\u65b0\u7684\u673a\u5236\u63d0\u9ad8\u89e3\u7f20\u80fd\u529b\u4e0e\u5e03\u5c40\u63a7\u5236\u3002", "method": "\u8bad\u7ec3\u9636\u6bb5\uff1a\u5f15\u5165\u57fa\u4e8e\u60e9\u7f5a\u7684\u6ce8\u610f\u529b\u4f18\u5316\uff08penalty-based attention optimization\uff09\u4ee5\u89e3\u7f20\u76f8\u4f3c\u8bed\u4e49\uff1b\u5408\u6210\u9636\u6bb5\uff1a\u5728\u6ce8\u610f\u529b\u5c42\u4e2d\u52a0\u5165\u5e76\u4f18\u5316\u76d2\u63a7\uff08box control\uff09\u4ee5\u7cbe\u786e\u63a7\u5236\u8f93\u51fa\u5e03\u5c40\u5e76\u8fdb\u4e00\u6b65\u51cf\u5c11\u8bed\u4e49\u6cc4\u9732\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bed\u4e49\u89e3\u7f20\u4e0e\u5408\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u76f8\u4f3c\u8bed\u4e49/\u89c6\u89c9\u5b9e\u4f8b\u53ca\u5c11\u89c1\u5bf9\u8c61\u4f9d\u7136\u4fdd\u6301\u9c81\u68d2\uff0c\u4e14\u5728\u53ef\u7f16\u8f91\u6027\u4e0e\u5b9e\u4f8b\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u7cbe\u786e\u5b66\u4e60\u4e0e\u5408\u6210\u591a\u5b9e\u4f8b\u8bed\u4e49\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8bed\u4e49\u89e3\u7f20\u5e76\u52a0\u5165\u8fb9\u754c\u6846\u63a7\u5236\u4ee5\u9632\u6b62\u8bed\u4e49\u6cc4\u9732\uff0c\u5728\u7f16\u8f91\u6027\u4e0e\u5b9e\u4f8b\u4e00\u81f4\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2511.00560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00560", "abs": "https://arxiv.org/abs/2511.00560", "authors": ["Chun-Tin Wu", "Jun-Cheng Chen"], "title": "4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting", "comment": "10 pages, 7 figures", "summary": "Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel\nview synthesis, extending it to dynamic scenes still results in substantial\nmemory overhead from replicating Gaussians across frames. To address this\nchallenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines\nvoxel-based representations with neural Gaussian splatting for efficient\ndynamic scene modeling. Instead of generating separate Gaussian sets per\ntimestamp, our method employs a compact set of neural voxels with learned\ndeformation fields to model temporal dynamics. The design greatly reduces\nmemory consumption and accelerates training while preserving high image\nquality. We further introduce a novel view refinement stage that selectively\nimproves challenging viewpoints through targeted optimization, maintaining\nglobal efficiency while enhancing rendering quality for difficult viewing\nangles. Experiments demonstrate that our method outperforms state-of-the-art\napproaches with significant memory reduction and faster training, enabling\nreal-time rendering with superior visual fidelity.", "AI": {"tldr": "\u7528\u795e\u7ecf\u4f53\u7d20+\u5b66\u4e60\u5f62\u53d8\u573a\u66ff\u4ee3\u6bcf\u5e27\u9ad8\u65af\u590d\u5236\uff0c\u5e26\u6765\u66f4\u4f4e\u5185\u5b58\u3001\u66f4\u5feb\u8bad\u7ec3\u548c\u9ad8\u8d28\u91cf\u52a8\u6001\u573a\u666f\u5b9e\u65f6\u6e32\u67d3\uff1b\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u89c6\u56fe\u7cbe\u5316\u63d0\u5347\u96be\u89c6\u89d2\u6548\u679c\u3002", "motivation": "\u76f4\u63a5\u5bf9\u6bcf\u5e27\u590d\u5236\u9ad8\u65af\u4f1a\u5e26\u6765\u5de8\u5927\u7684\u5185\u5b58\u5f00\u9500\u548c\u8ba1\u7b97\u5197\u4f59\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u9ad8\u5206\u8fa8\u7387\u6216\u957f\u65f6\u95f4\u52a8\u6001\u5e8f\u5217\uff1b\u4e3a\u4e86\u89e3\u51b3\u5185\u5b58\u4e0e\u6548\u7387\u74f6\u9888\uff0c\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u63d0\u51fa\u7528\u4f53\u7d20+\u5f62\u53d8\u573a\u7684\u7d27\u51d1\u8868\u793a\u3002", "method": "\u6838\u5fc3\u65b9\u6cd5\u662f\u4f7f\u7528\u7d27\u51d1\u7684\u795e\u7ecf\u4f53\u7d20\u7f51\u683c\u4f5c\u4e3a\u9759\u6001\u573a\u666f\u57fa\u5e95\uff0c\u5b66\u4e60\u65f6\u95f4\u76f8\u5173\u7684\u5f62\u53d8\u573a\u5c06\u4f53\u7d20\u5728\u56db\u7ef4\uff08\u7a7a\u95f4+\u65f6\u95f4\uff09\u4e2d\u53d8\u5f62\u4ee5\u8868\u793a\u52a8\u6001\u5185\u5bb9\uff1b\u6e32\u67d3\u91c7\u7528\u795e\u7ecf\u9ad8\u65afsplatting\u601d\u60f3\u5bf9\u4f53\u7d20\u8fdb\u884c\u9ad8\u6548\u63d2\u503c\u4e0e\u5149\u7167\u5408\u6210\uff1b\u989d\u5916\u8bbe\u8ba1\u4e86\u9488\u5bf9\u96be\u89c6\u89d2\u7684\u9009\u62e9\u6027\u7cbe\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u5c40\u90e8\u4f18\u5316\u63d0\u9ad8\u7279\u5b9a\u89c6\u89d2\u7684\u6e32\u67d3\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u5185\u5b58\u5360\u7528\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u76f8\u6bd43D Gaussian Splatting\u7b49\u65b9\u6cd5\u6709\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u5728\u50cf\u7d20\u8d28\u91cf\uff08PSNR/LPIPS\u7b49\uff09\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0a\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u5e76\u80fd\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u4e864D-NVS\uff0c\u901a\u8fc7\u7528\u795e\u7ecf\u4f53\u7d20+\u5f62\u53d8\u573a\u66ff\u4ee3\u6bcf\u5e27\u9ad8\u65af\u96c6\u5408\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5e76\u52a0\u901f\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u8d28\u91cf\u6e32\u67d3\uff1b\u5f15\u5165\u89c6\u56fe\u7cbe\u5316\u9636\u6bb5\u63d0\u5347\u96be\u89c6\u89d2\u8868\u73b0\uff0c\u6700\u7ec8\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\u5e76\u5728\u5185\u5b58\u548c\u8bad\u7ec3\u901f\u5ea6\u4e0a\u4f18\u4e8eSOTA\u3002"}}
{"id": "2511.00573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00573", "abs": "https://arxiv.org/abs/2511.00573", "authors": ["Wei Feng", "Zongyuan Ge"], "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective", "comment": "29 pages, 5 figures", "summary": "Generalized Category Discovery (GCD) aims to leverage labeled samples from\nknown categories to cluster unlabeled data that may include both known and\nunknown categories. While existing methods have achieved impressive results\nunder standard conditions, their performance often deteriorates in the presence\nof distribution shifts. In this paper, we explore a more realistic task:\nDomain-Shifted Generalized Category Discovery (DS\\_GCD), where the unlabeled\ndata includes not only unknown categories but also samples from unknown\ndomains. To tackle this challenge, we propose a\n\\textbf{\\underline{F}}requency-guided Gene\\textbf{\\underline{r}}alized\nCat\\textbf{\\underline{e}}gory Discov\\textbf{\\underline{e}}ry framework (FREE)\nthat enhances the model's ability to discover categories under distributional\nshift by leveraging frequency-domain information. Specifically, we first\npropose a frequency-based domain separation strategy that partitions samples\ninto known and unknown domains by measuring their amplitude differences. We\nthen propose two types of frequency-domain perturbation strategies: a\ncross-domain strategy, which adapts to new distributions by exchanging\namplitude components across domains, and an intra-domain strategy, which\nenhances robustness to intra-domain variations within the unknown domain.\nFurthermore, we extend the self-supervised contrastive objective and semantic\nclustering loss to better guide the training process. Finally, we introduce a\nclustering-difficulty-aware resampling technique to adaptively focus on\nharder-to-cluster categories, further enhancing model performance. Extensive\nexperiments demonstrate that our method effectively mitigates the impact of\ndistributional shifts across various benchmark datasets and achieves superior\nperformance in discovering both known and unknown categories.", "AI": {"tldr": "\u63d0\u51faFREE\uff1a\u4e00\u4e2a\u57fa\u4e8e\u9891\u57df\u7684DS_GCD\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u57df\u5206\u79bb\u3001\u8de8/\u57df\u5185\u9891\u57df\u6270\u52a8\u3001\u76ee\u6807\u635f\u5931\u6269\u5c55\u548c\u56f0\u96be\u5ea6\u91cd\u91c7\u6837\uff0c\u5e94\u5bf9\u672a\u77e5\u57df\u4e0e\u672a\u77e5\u7c7b\u5171\u5b58\u7684\u6311\u6218\uff0c\u5b9e\u9a8c\u8868\u660e\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709GCD\u65b9\u6cd5\u5728\u5206\u5e03\u504f\u79fb\uff08\u5c24\u5176\u5b58\u5728\u672a\u77e5\u57df\u6837\u672c\u65f6\uff09\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u5b9e\u573a\u666f\u4e2d\u672a\u6807\u6ce8\u6570\u636e\u5f80\u5f80\u65e2\u5305\u542b\u672a\u77e5\u7c7b\u522b\u4e5f\u5305\u542b\u672a\u77e5\u57df\uff0c\u9700\u8bbe\u8ba1\u80fd\u62b5\u6297\u57df\u504f\u79fb\u7684GCD\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u9891\u57df\u57df\u5206\u79bb\uff1a\u901a\u8fc7\u5e45\u5ea6\u5dee\u5f02\u5c06\u6837\u672c\u5212\u5206\u4e3a\u5df2\u77e5\u57df\u4e0e\u672a\u77e5\u57df\uff1b2\uff09\u9891\u57df\u6270\u52a8\uff1a\u8de8\u57df\u7b56\u7565\uff08\u4ea4\u6362\u5e45\u5ea6\u6210\u5206\uff09\u4e0e\u57df\u5185\u7b56\u7565\uff08\u589e\u5f3a\u57df\u5185\u53d8\u5f02\u9c81\u68d2\u6027\uff09\uff1b3\uff09\u635f\u5931\u6269\u5c55\uff1a\u5c06\u81ea\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u548c\u8bed\u4e49\u805a\u7c7b\u635f\u5931\u7ed3\u5408\u7528\u4e8e\u8bad\u7ec3\uff1b4\uff09\u56f0\u96be\u5ea6\u611f\u77e5\u91cd\u91c7\u6837\uff1a\u5bf9\u96be\u805a\u7c7b\u7684\u7c7b\u522b\u8fdb\u884c\u81ea\u9002\u5e94\u91c7\u6837\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cFREE\u663e\u8457\u51cf\u8f7b\u4e86\u5206\u5e03\u504f\u79fb\u7684\u5f71\u54cd\uff0c\u5728\u53d1\u73b0\u5df2\u77e5\u548c\u672a\u77e5\u7c7b\u522b\u4e0a\u5747\u53d6\u5f97\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u57df\u504f\u79fb\u4e0b\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u4efb\u52a1\uff08DS_GCD\uff09\u7684\u9891\u57df\u5f15\u5bfc\u6846\u67b6FREE\uff0c\u901a\u8fc7\u5229\u7528\u9891\u57df\u4fe1\u606f\u8fdb\u884c\u57df\u5206\u79bb\u3001\u9891\u57df\u6270\u52a8\u548c\u81ea\u76d1\u7763/\u8bed\u4e49\u805a\u7c7b\u635f\u5931\u6269\u5c55\uff0c\u63d0\u5347\u5728\u672a\u77e5\u7c7b\u522b\u548c\u672a\u77e5\u57df\u6df7\u5408\u60c5\u51b5\u4e0b\u7684\u53d1\u73b0\u6027\u80fd\u3002"}}
{"id": "2511.00580", "categories": ["cs.CV", "cs.AI", "68T07, 68T45, 68U10", "I.2.10; I.5.4; I.4.8; C.3"], "pdf": "https://arxiv.org/pdf/2511.00580", "abs": "https://arxiv.org/abs/2511.00580", "authors": ["Yousuf Ahmed Siddiqui", "Sufiyaan Usmani", "Umer Tariq", "Jawwad Ahmed Shamsi", "Muhammad Burhan Khan"], "title": "TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection", "comment": "10 pages, 5 figures", "summary": "Video anomalies often depend on contextual information available and temporal\nevolution. Non-anomalous action in one context can be anomalous in some other\ncontext. Most anomaly detectors, however, do not notice this type of context,\nwhich seriously limits their capability to generalize to new, real-life\nsituations. Our work addresses the context-aware zero-shot anomaly detection\nchallenge, in which systems need to learn adaptively to detect new events by\ncorrelating temporal and appearance features with textual traces of memory in\nreal time. Our approach defines a memory-augmented pipeline, correlating\ntemporal signals with visual embeddings using cross-attention, and real-time\nzero-shot anomaly classification by contextual similarity scoring. We achieve\n90.4\\% AUC on UCF-Crime and 83.67\\% AP on XD-Violence, a new state-of-the-art\namong zero-shot models. Our model achieves real-time inference with high\nprecision and explainability for deployment. We show that, by fusing\ncross-attention temporal fusion and contextual memory, we achieve high fidelity\nanomaly detection, a step towards the applicability of zero-shot models in\nreal-world surveillance and infrastructure monitoring.", "AI": {"tldr": "\u63d0\u51fa\u8bb0\u5fc6\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u654f\u611f\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u65f6\u5e8f\u4e0e\u89c6\u89c9\u7279\u5f81\u5e76\u4e0e\u6587\u672c\u8bb0\u5fc6\u6bd4\u5bf9\u5b9e\u73b0\u5b9e\u65f6\u96f6\u6837\u672c\u5206\u7c7b\uff0c\u5728UCF-Crime\u4e0eXD-Violence\u4e0a\u62a5\u51faSOTA\u6027\u80fd\uff0c\u5f3a\u8c03\u53ef\u5b9e\u65f6\u90e8\u7f72\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u591a\u6570\u5f02\u5e38\u68c0\u6d4b\u5668\u5ffd\u89c6\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff1a\u67d0\u4e9b\u52a8\u4f5c\u5728\u4e00\u79cd\u4e0a\u4e0b\u6587\u4e2d\u662f\u6b63\u5e38\u7684\uff0c\u5728\u53e6\u4e00\u79cd\u4e0a\u4e0b\u6587\u4e2d\u53ef\u80fd\u662f\u5f02\u5e38\uff0c\u4e14\u5f02\u5e38\u8fd8\u4f9d\u8d56\u4e8e\u65f6\u95f4\u6f14\u53d8\u3002\u4e3a\u63d0\u9ad8\u96f6\u6837\u672c\u6a21\u578b\u5728\u5b9e\u9645\u76d1\u63a7\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u8981\u5c06\u65f6\u5e8f\u4fe1\u606f\u3001\u89c6\u89c9\u7279\u5f81\u4e0e\u8bed\u4e49\u8bb0\u5fc6\uff08\u6587\u672c\uff09\u5173\u8054\u8d77\u6765\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u65b0\u4e8b\u4ef6\u7684\u81ea\u9002\u5e94\u68c0\u6d4b\u3002", "method": "\u6784\u5efa\u8bb0\u5fc6\u589e\u5f3a\u7ba1\u7ebf\uff1a1) \u4f7f\u7528\u89c6\u89c9\u7f16\u7801\u5668\u63d0\u53d6\u5e27\u6216\u77ed\u65f6\u7247\u6bb5\u7684\u89c6\u89c9\u5d4c\u5165\uff1b2) \u4f7f\u7528\u65f6\u5e8f\u4fe1\u53f7\uff08\u5982\u52a8\u4f5c\u7279\u5f81\u6216\u65f6\u95f4\u7f16\u7801\uff09\u5e76\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u5c06\u5176\u4e0e\u89c6\u89c9\u5d4c\u5165\u878d\u5408\u4ee5\u6355\u6349\u65f6\u7a7a\u5173\u8054\uff1b3) \u5728\u5916\u90e8\u6587\u672c\u8bb0\u5fc6\u5e93\u4e2d\u7ef4\u62a4\u4e0a\u4e0b\u6587\u6587\u672c/\u8bed\u4e49\u75d5\u8ff9\uff08\u5982\u4e8b\u4ef6\u63cf\u8ff0\u3001\u573a\u666f\u8bf4\u660e\uff09\uff1b4) \u901a\u8fc7\u89c6\u89c9\u2014\u6587\u672c\u76f8\u4f3c\u5ea6\uff08\u4e0a\u4e0b\u6587\u76f8\u4f3c\u5ea6\u8bc4\u5206\uff09\u8fdb\u884c\u96f6\u6837\u672c\u5f02\u5e38\u5206\u7c7b\uff1b5) \u8bbe\u8ba1\u5b9e\u65f6\u63a8\u7406\u6d41\u7a0b\u548c\u53ef\u89e3\u91ca\u6027\u673a\u5236\uff08\u4f8b\u5982\u6ce8\u610f\u529b\u70ed\u56fe\u6216\u56de\u6eaf\u5230\u6587\u672c\u8bb0\u5fc6\u7684\u76f8\u5173\u6761\u76ee\uff09\u3002", "result": "\u6587\u4e2d\u62a5\u544a\u7684\u4e3b\u8981\u91cf\u5316\u7ed3\u679c\u4e3a\uff1aUCF-Crime\u4e0a90.4% AUC\uff0cXD-Violence\u4e0a83.67% AP\uff0c\u58f0\u79f0\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u65b0\u7684\u6700\u4f18\uff1b\u6b64\u5916\u5ba3\u79f0\u65b9\u6cd5\u5177\u5907\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3001\u9ad8\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4e0a\u4e0b\u6587\u7684\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u6a21\u5757\u5c06\u65f6\u5e8f\u4fe1\u606f\u4e0e\u89c6\u89c9\u5d4c\u5165\u8fdb\u884c\u5173\u8054\uff0c\u4f7f\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u65f6\u5e8f\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u4e0a\u4e0b\u6587\u76f8\u4f3c\u5ea6\u8bc4\u5206\u8fdb\u884c\u5b9e\u65f6\u96f6\u6837\u672c\u5f02\u5e38\u5206\u7c7b\u3002\u4f5c\u8005\u5ba3\u79f0\u5728UCF-Crime\u548cXD-Violence\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f9790.4% AUC\u548c83.67% AP\uff0c\u8fbe\u5230\u96f6\u6837\u672c\u6a21\u578b\u7684\u65b0SOTA\uff0c\u5e76\u5177\u5907\u5b9e\u65f6\u63a8\u7406\u3001\u9ad8\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u76d1\u63a7\u7b49\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2511.00613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00613", "abs": "https://arxiv.org/abs/2511.00613", "authors": ["Yating Yu", "Congqi Cao", "Zhaoying Wang", "Weihua Meng", "Jie Li", "Yuxin Li", "Zihao Wei", "Zhongpei Shen", "Jiajun Zhang"], "title": "CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World", "comment": null, "summary": "How far are deep models from real-world video anomaly understanding (VAU)?\nCurrent works typically emphasize on detecting unexpected occurrences deviated\nfrom normal patterns or comprehending anomalous events with interpretable\ndescriptions. However, they exhibit only a superficial comprehension of\nreal-world anomalies, with limited breadth in complex principles and subtle\ncontext that distinguish the anomalies from normalities, e.g., climbing cliffs\nwith safety gear vs. without it. To this end, we introduce CueBench, the first\nof its kind Benchmark, devoted to Context-aware video anomalies within a\nUnified Evaluation framework. We comprehensively establish an event-centric\nhierarchical taxonomy that anchors two core event types: 14 conditional and 18\nabsolute anomaly events, defined by their refined semantics from diverse\ncontexts across 174 scenes and 198 attributes. Based on this, we propose to\nunify and benchmark context-aware VAU with various challenging tasks across\nrecognition, temporal grounding, detection, and anticipation. This also serves\nas a rigorous and fair probing evaluation suite for generative-discriminative\nas well as generalized-specialized vision-language models (VLMs). To address\nthe challenges underlying CueBench, we further develop Cue-R1 based on R1-style\nreinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined\nrewards in a unified generative manner. Extensive results on CueBench reveal\nthat, existing VLMs are still far from satisfactory real-world anomaly\nunderstanding, while our Cue-R1 surpasses these state-of-the-art approaches by\nover 24% on average.", "AI": {"tldr": "\u63d0\u51faCueBench\u57fa\u51c6\u4e0e\u4e8b\u4ef6\u5206\u5c42\u4f53\u7cfb\uff0c\u7edf\u4e00\u8bc4\u6d4b\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u89c6\u9891\u5f02\u5e38\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7Cue-R1\uff08\u4e00\u79cdR1\u98ce\u683c\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff09\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u771f\u5b9e\u5f02\u5e38\u7406\u89e3\u4e0a\u7684\u6027\u80fd\uff0c\u5e73\u5747\u63d0\u5347\u7ea624%\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u68c0\u6d4b/\u63cf\u8ff0\u65b9\u6cd5\u5bf9\u201c\u771f\u5b9e\u4e16\u754c\u201d\u7684\u5f02\u5e38\u7406\u89e3\u8fc7\u4e8e\u6d45\u663e\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u8bed\u4e49\u4e0e\u4e0a\u4e0b\u6587\uff08\u5982\u5e26\u5b89\u5168\u88c5\u5907\u6500\u722c\u4e0e\u4e0d\u5e26\u7684\u533a\u522b\uff09\u7684\u7ec6\u7c92\u5ea6\u5224\u522b\u80fd\u529b\uff0c\u9700\u6784\u5efa\u66f4\u4e25\u8c28\u7684\u8bc4\u6d4b\u4e0e\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e8b\u4ef6\u4e2d\u5fc3\u7684\u5206\u5c42\u5206\u7c7b\u4f53\u7cfb\uff0814\u7c7b\u6761\u4ef6\u5f02\u5e38\u300118\u7c7b\u7edd\u5bf9\u5f02\u5e38\uff09\uff0c\u6536\u96c6174\u573a\u666f\u3001198\u5c5e\u6027\uff1b\u7edf\u4e00\u4efb\u52a1\u5305\u62ec\u8bc6\u522b\u3001\u65f6\u5e8f\u5b9a\u4f4d\u3001\u68c0\u6d4b\u4e0e\u9884\u6d4b\uff1b\u8bbe\u8ba1Cue-R1\uff0c\u57fa\u4e8eR1\u98ce\u683c\u5f3a\u5316\u5fae\u8c03\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u3001\u4efb\u52a1\u5bf9\u9f50\u548c\u5c42\u7ea7\u7ec6\u5316\u7684\u5956\u52b1\uff0c\u4ee5\u751f\u6210\u5f0f\u65b9\u5f0f\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728CueBench\u4e0a\uff0c\u73b0\u6709VLMs\u8868\u73b0\u5dee\u8ddd\u5927\uff0cCue-R1\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u5e73\u5747\u9886\u5148\u8d85\u8fc724%\uff0c\u663e\u793a\u5f3a\u5316\u5fae\u8c03\u4e0e\u5956\u8d4f\u8bbe\u8ba1\u53ef\u663e\u8457\u63d0\u5347\u60c5\u5883\u611f\u77e5\u5f02\u5e38\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86CueBench\uff0c\u4e00\u4e2a\u9762\u5411\u60c5\u5883\u611f\u77e5\u89c6\u9891\u5f02\u5e38\u7406\u89e3\uff08VAU\uff09\u7684\u7edf\u4e00\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8868\u660e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u5f02\u5e38\u7406\u89e3\u4e0a\u4ecd\u8fdc\u4e0d\u8db3\uff0c\u5e76\u63d0\u51faCue-R1\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2511.00643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00643", "abs": "https://arxiv.org/abs/2511.00643", "authors": ["Oluwatosin Alabi", "Meng Wei", "Charlie Budd", "Tom Vercauteren", "Miaojing Shi"], "title": "Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach", "comment": null, "summary": "Understanding surgical instrument-tissue interactions requires not only\nidentifying which instrument performs which action on which anatomical target,\nbut also grounding these interactions spatially within the surgical scene.\nExisting surgical action triplet recognition methods are limited to learning\nfrom frame-level classification, failing to reliably link actions to specific\ninstrument instances.Previous attempts at spatial grounding have primarily\nrelied on class activation maps, which lack the precision and robustness\nrequired for detailed instrument-tissue interaction analysis.To address this\ngap, we propose grounding surgical action triplets with instrument instance\nsegmentation, or triplet segmentation for short, a new unified task which\nproduces spatially grounded <instrument, verb, target> outputs.We start by\npresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000\nannotated frames, linking instrument instance masks with action verb and\nanatomical target annotations, and establishing the first benchmark for\nstrongly supervised, instance-level triplet grounding and evaluation.To learn\ntriplet segmentation, we propose TargetFusionNet, a novel architecture that\nextends Mask2Former with a target-aware fusion mechanism to address the\nchallenge of accurate anatomical target prediction by fusing weak anatomy\npriors with instrument instance queries.Evaluated across recognition,\ndetection, and triplet segmentation metrics, TargetFusionNet consistently\nimproves performance over existing baselines, demonstrating that strong\ninstance supervision combined with weak target priors significantly enhances\nthe accuracy and robustness of surgical action understanding.Triplet\nsegmentation establishes a unified framework for spatially grounding surgical\naction triplets. The proposed benchmark and architecture pave the way for more\ninterpretable, surgical scene understanding.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u5b9a\u4e49\u4e86triplet segmentation\u4efb\u52a1\uff0c\u53d1\u5e03CholecTriplet-Seg\u6570\u636e\u96c6\uff0c\u5e76\u8bbe\u8ba1TargetFusionNet\u5c06\u5668\u68b0\u5b9e\u4f8b\u5206\u5272\u4e0e\u76ee\u6807\u9884\u6d4b\u878d\u5408\uff0c\u6709\u6548\u5b9e\u73b0\u7a7a\u95f4\u7ea7\u7684\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5e27\u7ea7\u5206\u7c7b\u6216\u7528\u7c7b\u6fc0\u6d3b\u6620\u5c04\u8fdb\u884c\u7a7a\u95f4\u5b9a\u4f4d\uff0c\u65e0\u6cd5\u53ef\u9760\u5730\u628a\u52a8\u4f5c\u4e0e\u5177\u4f53\u5668\u68b0\u5b9e\u4f8b\u5173\u8054\uff0c\u4e14CAM\u5b9a\u4f4d\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u96be\u4ee5\u652f\u6301\u7cbe\u7ec6\u7684\u5668\u68b0-\u7ec4\u7ec7\u4ea4\u4e92\u5206\u6790\u3002", "method": "\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6CholecTriplet-Seg\uff083\u4e07+\u5e27\uff09\uff0c\u5c06\u5668\u68b0\u5b9e\u4f8b\u63a9\u7801\u4e0e\u52a8\u4f5c\u52a8\u8bcd\u548c\u89e3\u5256\u76ee\u6807\u914d\u5bf9\uff1b\u63d0\u51faTargetFusionNet\uff0c\u57fa\u4e8eMask2Former\u5e76\u52a0\u5165\u76ee\u6807\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u5c06\u5f31\u89e3\u5256\u5148\u9a8c\u4e0e\u5668\u68b0\u5b9e\u4f8b\u67e5\u8be2\u878d\u5408\u4ee5\u63d0\u5347\u76ee\u6807\u9884\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u8bc6\u522b\u3001\u68c0\u6d4b\u548c\u4e09\u5143\u7ec4\u5206\u5272\u6307\u6807\u4e0a\uff0cTargetFusionNet\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5747\u6709\u6301\u7eed\u63d0\u5347\uff0c\u8868\u660e\u5f3a\u5b9e\u4f8b\u76d1\u7763\u7ed3\u5408\u5f31\u76ee\u6807\u5148\u9a8c\u53ef\u663e\u8457\u63d0\u5347\u624b\u672f\u52a8\u4f5c\u7406\u89e3\u7684\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u5c06\u624b\u672f\u52a8\u4f5c\u4e09\u5143\u7ec4\u4e0e\u5668\u68b0\u5b9e\u4f8b\u5206\u5272\u7ed3\u5408\u7684\u201ctriplet segmentation\u201d\u65b0\u4efb\u52a1\uff0c\u5f3a\u8c03\u4e86\u7a7a\u95f4\u5b9a\u4f4d\u5728\u624b\u672f\u573a\u666f\u7406\u89e3\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u96c6\u548c\u6a21\u578b\u8bc1\u660e\u4e86\u4f18\u52bf\u3002"}}
{"id": "2511.00653", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00653", "abs": "https://arxiv.org/abs/2511.00653", "authors": ["Lassi Ruoppa", "Tarmo Hietala", "Verneri Sepp\u00e4nen", "Josef Taher", "Teemu Hakala", "Xiaowei Yu", "Antero Kukko", "Harri Kaartinen", "Juha Hyypp\u00e4"], "title": "Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset", "comment": "39 pages, 9 figures", "summary": "Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for\napplications such as forest inventory, carbon monitoring and biodiversity\nassessment. Traditionally, ITS has been achieved with unsupervised\ngeometry-based algorithms, while more recent advances have shifted toward\nsupervised deep learning (DL). In the past, progress in method development was\nhindered by the lack of large-scale benchmark datasets, and the availability of\nnovel data formats, particularly multispectral (MS) LiDAR, remains limited to\nthis day, despite evidence that MS reflectance can improve the accuracy of ITS.\nThis study introduces FGI-EMIT, the first large-scale MS airborne laser\nscanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550\nnm, the dataset consists of 1,561 manually annotated trees, with a particular\nfocus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked\nfour conventional unsupervised algorithms and four supervised DL approaches.\nHyperparameters of unsupervised methods were optimized using a Bayesian\napproach, while DL models were trained from scratch. Among the unsupervised\nmethods, Treeiso achieved the highest test set F1-score of 52.7%. The DL\napproaches performed significantly better overall, with the best model,\nForestFormer3D, attaining an F1-score of 73.3%. The most significant difference\nwas observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9\npercentage points. An ablation study demonstrated that current DL-based\napproaches generally fail to leverage MS reflectance information when it is\nprovided as additional input features, although single channel reflectance can\nimprove accuracy marginally, especially for understory trees. A performance\nanalysis across point densities further showed that DL methods consistently\nremain superior to unsupervised algorithms, even at densities as low as 10\npoints/m$^2$.", "AI": {"tldr": "FGI-EMIT\u662f\u9996\u4e2a\u5927\u89c4\u6a21MS ALS\u4e2a\u4f53\u6811\u5206\u5272\u57fa\u51c6\uff0c\u8bc4\u6d4b\u663e\u793aDL\u65b9\u6cd5\uff08ForestFormer3D\uff09\u660e\u663e\u4f18\u4e8e\u65e0\u76d1\u7763\u7b97\u6cd5\uff0cMS\u53cd\u5c04\u7387\u4f5c\u4e3a\u989d\u5916\u7279\u5f81\u5bf9\u5f53\u524dDL\u6a21\u578b\u6539\u8fdb\u6709\u9650\uff0c\u4f46\u5355\u901a\u9053\u6709\u52a9\u4e8e\u6797\u4e0b\u5c0f\u6811\u68c0\u6d4b\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u591a\u5149\u8c31LiDAR\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u9650\u5236\u4e86\u57fa\u4e8eMS\u4fe1\u606f\u6539\u8fdb\u4e2a\u4f53\u6811\u5206\u5272\uff08ITS\uff09\u65b9\u6cd5\u7684\u53d1\u5c55\u4e0e\u6bd4\u8f83\uff1b\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u8bc4\u4f30MS\u6570\u636e\u4e0e\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728ITS\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e09\u6ce2\u6bb5\uff08532\u3001905\u30011550 nm\uff09\u53cd\u5c04\u7387\u7684MS\u70b9\u4e91\u6570\u636e\u96c6\uff0c\u624b\u5de5\u6807\u6ce81561\u68f5\u6811\uff08\u5f3a\u8c03\u5c0f\u578b\u6797\u4e0b\u6811\uff09\uff1b\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u8c03\u6574\u65e0\u76d1\u7763\u65b9\u6cd5\u8d85\u53c2\u6570\uff1b\u4ece\u5934\u8bad\u7ec3\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff1b\u8fdb\u884c\u6d88\u878d\u7814\u7a76\u4ee5\u68c0\u9a8c\u591a\u5149\u8c31\u4fe1\u606f\u7684\u4f5c\u7528\uff1b\u8bc4\u4f30\u4e0d\u540c\u70b9\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2dTreeiso\u5728\u6d4b\u8bd5\u96c6\u4e0aF1\u6700\u9ad8\u4e3a52.7%\uff1b\u6700\u4f73\u6df1\u5ea6\u5b66\u4e60\u6a21\u578bForestFormer3D\u8fbe73.3% F1\uff0c\u4e14\u5728\u6797\u4e0b\u5c0f\u6811\u4e0a\u6bd4Treeiso\u9ad825.9\u4e2a\u767e\u5206\u70b9\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u76f4\u63a5\u5c06\u591a\u5149\u8c31\u53cd\u5c04\u7387\u4f5c\u4e3a\u989d\u5916\u7279\u5f81\u5bf9\u4e8e\u73b0\u6709DL\u65b9\u6cd5\u5e2e\u52a9\u6709\u9650\uff0c\u4f46\u5355\u901a\u9053\u53cd\u5c04\u7387\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u63d0\u5347\u5c0f\u6811\u68c0\u6d4b\uff1b\u5728\u6700\u4f4e10\u70b9/m^2\u70b9\u5bc6\u5ea6\u4e0bDL\u65b9\u6cd5\u4ecd\u4f18\u4e8e\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u5149\u8c31\u673a\u8f7d\u6fc0\u5149\u626b\u63cf\uff08MS ALS\uff09\u4e2a\u4f53\u6811\u5206\u5272\u57fa\u51c6\u6570\u636e\u96c6FGI-EMIT\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4f20\u7edf\u65e0\u76d1\u7763\u65b9\u6cd5\u4e0e\u56db\u79cd\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6574\u4f53\u4f18\u4e8e\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u6797\u4e0b\u5c0f\u6811\u68c0\u6d4b\u4e0a\u5dee\u8ddd\u663e\u8457\u3002\u6b64\u5916\uff0c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bf9\u591a\u5149\u8c31\u53cd\u5c04\u7387\u7684\u5229\u7528\u6709\u9650\uff0c\u70b9\u4e91\u5bc6\u5ea6\u964d\u4f4e\u65f6\u4ecd\u4fdd\u6301\u4f18\u52bf\u3002"}}
{"id": "2511.00681", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00681", "abs": "https://arxiv.org/abs/2511.00681", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Virginia Fernandez", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control", "comment": null, "summary": "Magnetic Resonance Imaging suffers from substantial data heterogeneity and\nthe absence of standardized contrast labels across scanners, protocols, and\ninstitutions, which severely limits large-scale automated analysis. A unified\nrepresentation of MRI contrast would enable a wide range of downstream\nutilities, from automatic sequence recognition to harmonization and quality\ncontrol, without relying on manual annotations. To this end, we introduce\nMR-CLIP, a metadata-guided framework that learns MRI contrast representations\nby aligning volumetric images with their DICOM acquisition parameters. The\nresulting embeddings shows distinct clusters of MRI sequences and outperform\nsupervised 3D baselines under data scarcity in few-shot sequence\nclassification. Moreover, MR-CLIP enables unsupervised data quality control by\nidentifying corrupted or inconsistent metadata through image-metadata embedding\ndistances. By transforming routinely available acquisition metadata into a\nsupervisory signal, MR-CLIP provides a scalable foundation for label-efficient\nMRI analysis across diverse clinical datasets.", "AI": {"tldr": "MR-CLIP\u7528DICOM\u91c7\u96c6\u53c2\u6570\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\u8fdb\u884c\u56fe\u50cf-\u5143\u6570\u636e\u5bf9\u9f50\uff0c\u5f97\u5230\u53ef\u533a\u5206MRI\u5e8f\u5217\u7684\u5411\u91cf\u8868\u793a\uff0c\u652f\u6301\u5c11\u6837\u672c\u5206\u7c7b\u4e0e\u65e0\u76d1\u7763\u8d28\u91cf\u63a7\u5236\uff0c\u6709\u52a9\u4e8e\u8de8\u673a\u6784\u7684\u6807\u7b7e\u9ad8\u6548MRI\u5206\u6790\u3002", "motivation": "MRI\u5728\u4e0d\u540c\u626b\u63cf\u4eea\u3001\u534f\u8bae\u548c\u673a\u6784\u95f4\u5b58\u5728\u663e\u8457\u6570\u636e\u5f02\u8d28\u6027\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u5bf9\u6bd4\u6807\u7b7e\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u81ea\u52a8\u5316\u5206\u6790\uff0c\u6545\u9700\u4e00\u79cd\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u7edf\u4e00\u8868\u793aMRI\u5bf9\u6bd4\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eCLIP\u601d\u60f3\uff0c\u4f7f\u7528\u56fe\u50cf-\u5143\u6570\u636e\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u4f53\u79ef\u5f71\u50cf\u7f16\u7801\u4e0e\u91c7\u96c6\u53c2\u6570\uff08DICOM\uff09\u6587\u672c/\u5411\u91cf\u7f16\u7801\u5bf9\u9f50\uff0c\u4ece\u800c\u83b7\u5f97\u5bf9\u6bd4\u8868\u793a\u3002", "result": "\u5b66\u5f97\u7684\u5d4c\u5165\u5728\u5e8f\u5217\u805a\u7c7b\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u7c07\u7ed3\u6784\uff0c\u5728\u5c11\u6837\u672c\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6709\u76d1\u77633D\u57fa\u7ebf\uff0c\u5e76\u53ef\u901a\u8fc7\u56fe\u50cf-\u5143\u6570\u636e\u5d4c\u5165\u8ddd\u79bb\u8fdb\u884c\u65e0\u76d1\u7763\u6570\u636e\u8d28\u91cf\u63a7\u5236\uff0c\u8bc6\u522b\u635f\u574f\u6216\u4e0d\u4e00\u81f4\u7684\u5143\u6570\u636e\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86MR-CLIP\uff0c\u901a\u8fc7\u5c06MRI\u4f53\u79ef\u56fe\u50cf\u4e0eDICOM\u91c7\u96c6\u53c2\u6570\u5bf9\u9f50\u5b66\u4e60\u5bf9\u6bd4\u8868\u793a\uff0c\u89e3\u51b3\u4e86MRI\u5bf9\u6bd4\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002"}}
{"id": "2511.00682", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00682", "abs": "https://arxiv.org/abs/2511.00682", "authors": ["Hailing Wang", "jianglin Lu", "Yitian Zhang", "Yun Fu"], "title": "Outlier-Aware Post-Training Quantization for Image Super-Resolution", "comment": null, "summary": "Quantization techniques, including quantization-aware training (QAT) and\npost-training quantization (PTQ), have become essential for inference\nacceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has\ngarnered significant attention as it eliminates the need for ground truth and\nmodel retraining. However, existing PTQ methods for SR often fail to achieve\nsatisfactory performance as they overlook the impact of outliers in activation.\nOur empirical analysis reveals that these prevalent activation outliers are\nstrongly correlated with image color information, and directly removing them\nleads to significant performance degradation. Motivated by this, we propose a\ndual-region quantization strategy that partitions activations into an outlier\nregion and a dense region, applying uniform quantization to each region\nindependently to better balance bit-width allocation. Furthermore, we observe\nthat different network layers exhibit varying sensitivities to quantization,\nleading to different levels of performance degradation. To address this, we\nintroduce sensitivity-aware finetuning that encourages the model to focus more\non highly sensitive layers, further enhancing quantization performance.\nExtensive experiments demonstrate that our method outperforms existing PTQ\napproaches across various SR networks and datasets, while achieving performance\ncomparable to QAT methods in most scenarios with at least a 75 speedup.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u6fc0\u6d3b\u5206\u533a\u5e76\u5bf9\u9ad8\u654f\u611f\u5c42\u8fdb\u884c\u4f18\u5148\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u5728PTQ\u4e0b\u663e\u8457\u63d0\u5347SR\u6027\u80fd\uff0c\u63a5\u8fd1QAT\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709SR\u7f51\u7edc\u7684PTQ\u65b9\u6cd5\u5ffd\u89c6\u6fc0\u6d3b\u4e2d\u7684\u5f02\u5e38\u503c\uff08\u4e0e\u56fe\u50cf\u989c\u8272\u76f8\u5173\uff09\uff0c\u76f4\u63a5\u53bb\u9664\u5f02\u5e38\u503c\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u9700\u8981\u66f4\u5408\u7406\u7684\u91cf\u5316\u7b56\u7565\u3002", "method": "\u5c06\u6fc0\u6d3b\u5206\u4e3a\u5f02\u5e38\u533a\u57df\u548c\u7a20\u5bc6\u533a\u57df\uff0c\u5bf9\u6bcf\u4e2a\u533a\u57df\u72ec\u7acb\u8fdb\u884c\u5747\u5300\u91cf\u5316\uff0c\u540c\u65f6\u5f15\u5165\u654f\u611f\u6027\u611f\u77e5\u5fae\u8c03\u4ee5\u9488\u5bf9\u5bf9\u91cf\u5316\u66f4\u654f\u611f\u7684\u5c42\u8fdb\u884c\u91cd\u70b9\u5fae\u8c03\u3002", "result": "\u5728\u591a\u79cdSR\u7f51\u7edc\u548c\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u5df2\u6709PTQ\u65b9\u6cd5\uff0c\u5e76\u5728\u5927\u591a\u6570\u573a\u666f\u4e0b\u8fbe\u5230\u4e0eQAT\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u5b9e\u73b0\u81f3\u5c1175\u500d\u63a8\u7406\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u53cc\u533a\u57df\u91cf\u5316\u7b56\u7565\u548c\u654f\u611f\u6027\u611f\u77e5\u5fae\u8c03\uff0c\u6709\u6548\u5904\u7406\u6fc0\u6d3b\u4e2d\u7684\u5f02\u5e38\u503c\u5bfc\u81f4\u7684\u91cf\u5316\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347SR\u7f51\u7edc\u7684PTQ\u6027\u80fd\uff0c\u8fbe\u5230\u63a5\u8fd1QAT\u4e14\u6709\u5927\u5e45\u52a0\u901f\u3002"}}
{"id": "2511.00686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00686", "abs": "https://arxiv.org/abs/2511.00686", "authors": ["Alex Inch", "Passawis Chaiyapattanaporn", "Yuchen Zhu", "Yuan Lu", "Ting-Wen Ko", "Davide Paglieri"], "title": "Evolve to Inspire: Novelty Search for Diverse Image Generation", "comment": "14 pages, 10 figures, Accepted to Neurips 2025 GenProCC Workshop", "summary": "Text-to-image diffusion models, while proficient at generating high-fidelity\nim- ages, often suffer from limited output diversity, hindering their\napplication in exploratory and ideation tasks. Existing prompt optimization\ntechniques typically target aesthetic fitness or are ill-suited to the creative\nvisual domain. To address this shortcoming, we introduce WANDER, a novelty\nsearch-based approach to generating diverse sets of images from a single input\nprompt. WANDER operates directly on natural language prompts, employing a Large\nLanguage Model (LLM) for semantic evolution of diverse sets of images, and\nusing CLIP embeddings to quantify novelty. We additionally apply emitters to\nguide the search into distinct regions of the prompt space, and demonstrate\nthat they boost the diversity of the generated images. Empirical evaluations\nusing FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that\nWANDER significantly outperforms existing evolutionary prompt optimization\nbaselines in diversity metrics. Ablation studies confirm the efficacy of\nemitters.", "AI": {"tldr": "WANDER\u901a\u8fc7\u5728\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7a7a\u95f4\u4e2d\u7ed3\u5408LLM\u9a71\u52a8\u7684\u8bed\u4e49\u53d8\u5f02\u3001CLIP\u65b0\u9896\u6027\u8bc4\u4ef7\u4e0eemitters\u5f15\u5bfc\uff0c\u6210\u529f\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u591a\u6837\u6027\u7684\u56fe\u50cf\u96c6\u5408\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u8f93\u51fa\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u63a2\u7d22\u4e0e\u521b\u610f\u4efb\u52a1\uff1b\u4f20\u7edf\u63d0\u793a\u4f18\u5316\u591a\u5173\u6ce8\u7f8e\u5b66\u6216\u4e0d\u9002\u7528\u4e8e\u89c6\u89c9\u521b\u610f\u9886\u57df\uff0c\u9700\u4e00\u79cd\u4e13\u95e8\u9762\u5411\u591a\u6837\u6027\u7684\u4f18\u5316\u7b56\u7565\u3002", "method": "\u65b9\u6cd5\u5728\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7a7a\u95f4\u4e0a\u76f4\u63a5\u5de5\u4f5c\uff0c\u4f7f\u7528LLM\uff08\u5982GPT-4o-mini\uff09\u8fdb\u884c\u8bed\u4e49\u53d8\u5f02\uff0c\u901a\u8fc7CLIP\u5d4c\u5165\u8bc4\u4f30\u56fe\u50cf\u65b0\u9896\u6027\uff0c\u5e76\u5f15\u5165\"emitters\"\u5f15\u5bfc\u641c\u7d22\u5230\u63d0\u793a\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\u4ee5\u63d0\u5347\u591a\u6837\u6027\u3002", "result": "\u5728FLUX-DEV\u4e3a\u751f\u6210\u5668\u3001GPT-4o-mini\u4e3a\u53d8\u5f02\u5668\u7684\u5b9e\u9a8c\u4e2d\uff0cWANDER\u5728\u591a\u6837\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8fdb\u5316\u63d0\u793a\u4f18\u5316\u57fa\u7ebf\uff1b\u6d88\u878d\u7814\u7a76\u8868\u660eemitters\u6709\u6548\u63d0\u5347\u591a\u6837\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51faWANDER\uff0c\u4e00\u79cd\u57fa\u4e8e\u65b0\u9896\u6027\u641c\u7d22\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u65e8\u5728\u4ece\u5355\u4e00\u6587\u672c\u63d0\u793a\u751f\u6210\u591a\u6837\u5316\u56fe\u50cf\u96c6\u3002"}}
{"id": "2511.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00698", "abs": "https://arxiv.org/abs/2511.00698", "authors": ["Taifour Yousra", "Beghdadi Azeddine", "Marie Luong", "Zuheng Ming"], "title": "Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics", "comment": null, "summary": "Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to\nmitigate high exposure side effects, but often suffers from noise and artifacts\nthat affect diagnostic accuracy. To tackle this issue, deep learning models\nhave been developed to enhance LDCT images. Various loss functions have been\nemployed, including classical approaches such as Mean Square Error and\nadversarial losses, as well as customized loss functions(LFs) designed for\nspecific architectures. Although these models achieve remarkable performance in\nterms of PSNR and SSIM, these metrics are limited in their ability to reflect\nperceptual quality, especially for medical images. In this paper, we focus on\none of the most critical elements of DL-based architectures, namely the loss\nfunction. We conduct an objective analysis of the relevance of different loss\nfunctions for LDCT image quality enhancement and their consistency with image\nquality metrics. Our findings reveal inconsistencies between LFs and quality\nmetrics, and highlight the need of consideration of image quality metrics when\ndeveloping a new loss function for image quality enhancement.", "AI": {"tldr": "\u5206\u6790\u8868\u660eLDCT\u589e\u5f3a\u4e2d\u7684\u5e38\u7528\u635f\u5931\u51fd\u6570\u4e0e\u56fe\u50cf\u8d28\u91cf\u6307\u6807\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u5efa\u8bae\u5728\u8bbe\u8ba1\u635f\u5931\u65f6\u7ed3\u5408\u611f\u77e5/\u4e34\u5e8a\u76f8\u5173\u6307\u6807\u4ee5\u63d0\u9ad8\u5b9e\u9645\u8bca\u65ad\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728LDCT\u53bb\u566a\u4e0e\u589e\u5f3a\u65b9\u9762\u5728PSNR/SSIM\u4e0a\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0d\u80fd\u5145\u5206\u53cd\u6620\u533b\u5b66\u56fe\u50cf\u7684\u611f\u77e5/\u8bca\u65ad\u8d28\u91cf\uff0c\u5bfc\u81f4\u4f18\u5316\u76ee\u6807\u4e0e\u4e34\u5e8a\u53ef\u7528\u6027\u53ef\u80fd\u4e0d\u4e00\u81f4\u3002\u4f5c\u8005\u65e8\u5728\u8bc4\u4f30\u4e0d\u540c\u635f\u5931\u51fd\u6570\u4e0e\u8d28\u91cf\u6307\u6807\u7684\u4e00\u81f4\u6027\uff0c\u4ee5\u6307\u5bfc\u66f4\u5408\u7406\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u3002", "method": "\u5bf9\u6bd4\u5206\u6790\u6cd5\uff1a\u68b3\u7406\u5e76\u5206\u7c7b\u73b0\u6709\u635f\u5931\u51fd\u6570\uff08MSE\u3001\u5bf9\u6297\u6027\u635f\u5931\u3001\u81ea\u5b9a\u4e49\u7ed3\u6784\u5316\u635f\u5931\u7b49\uff09\uff0c\u5e76\u5c06\u5b83\u4eec\u5728\u591a\u4e2a\u56fe\u50cf\u8d28\u91cf\u6307\u6807\uff08PSNR\u3001SSIM\u53ca\u611f\u77e5\u76f8\u5173\u6307\u6807\uff09\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u5ba2\u89c2\u6bd4\u8f83\u548c\u4e00\u81f4\u6027\u68c0\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u591a\u79cd\u5e38\u7528\u635f\u5931\u51fd\u6570\u5728PSNR/SSIM\u4e0a\u7684\u63d0\u5347\u5e76\u4e0d\u5fc5\u7136\u5bf9\u5e94\u611f\u77e5\u8d28\u91cf\u6216\u533b\u5b66\u5224\u8bfb\u8d28\u91cf\u7684\u6539\u5584\uff0c\u90e8\u5206\u5b9a\u5236\u635f\u5931\u751a\u81f3\u4e0e\u611f\u77e5\u6307\u6807\u8d1f\u76f8\u5173\u3002\u8bba\u6587\u5efa\u8bae\u5728\u5f00\u53d1\u65b0\u635f\u5931\u65f6\u5e94\u663e\u5f0f\u7eb3\u5165\u4e0e\u4e34\u5e8a\u76f8\u5173\u7684\u56fe\u50cf\u8d28\u91cf\u5ea6\u91cf\u5e76\u8fdb\u884c\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u6307\u51fa\u73b0\u6709\u7528\u4e8e\u4f4e\u5242\u91cfCT\uff08LDCT\uff09\u589e\u5f3a\u7684\u635f\u5931\u51fd\u6570\u4e0e\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u5982PSNR\u3001SSIM\uff09\u4e4b\u95f4\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u5f3a\u8c03\u5728\u8bbe\u8ba1\u65b0\u635f\u5931\u51fd\u6570\u65f6\u5e94\u8003\u8651\u4e0e\u611f\u77e5\u53ca\u533b\u5b66\u8bca\u65ad\u76f8\u5173\u7684\u8d28\u91cf\u6307\u6807\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00728", "abs": "https://arxiv.org/abs/2511.00728", "authors": ["Hugo Massaroli", "Hernan Chaves", "Pilar Anania", "Mauricio Farez", "Emmanuel Iarussi", "Viviana Siless"], "title": "Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data", "comment": "7 pages, 2 figures", "summary": "Deep learning models have shown strong performance in diagnosing Alzheimer's\ndisease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with\ntraining datasets largely composed of North American cohorts such as those in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their\ngeneralization to underrepresented populations remains underexplored. In this\nstudy, we benchmark convolutional and Transformer-based models on the ADNI\ndataset and assess their generalization performance on a novel Latin American\nclinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show\nthat while all models achieve high AUCs on ADNI (up to .96, .97), their\nperformance drops substantially on FLENI (down to .82, .80, respectively),\nrevealing a significant domain shift. The tested architectures demonstrated\nsimilar performance, calling into question the supposed advantages of\ntransformers for this specific task. Through ablation studies, we identify\nper-image normalization and a correct sampling selection as key factors for\ngeneralization. Occlusion sensitivity analysis further reveals that models\ntrained on ADNI, generally attend to canonical hypometabolic regions for the AD\nclass, but focus becomes unclear for the other classes and for FLENI scans.\nThese findings highlight the need for population-aware validation of diagnostic\nAI models and motivate future work on domain adaptation and cohort\ndiversification.", "AI": {"tldr": "ADNI\u8bad\u7ec3\u7684PET\u5f71\u50cf\u6df1\u5ea6\u6a21\u578b\u5728\u62c9\u7f8e\u4e34\u5e8a\u961f\u5217\u6cdb\u5316\u663e\u8457\u4e0b\u964d\uff0c\u5f52\u4e00\u5316\u4e0e\u91c7\u6837\u662f\u5173\u952e\uff0c\u67b6\u6784\u5dee\u5f02\u5c0f\uff0c\u9700\u91cd\u89c6\u4eba\u7fa4\u591a\u6837\u6027\u4e0e\u57df\u9002\u5e94\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u5f53\u524d\u57fa\u4e8e18F-FDG PET\u7684\u6df1\u5ea6\u5b66\u4e60AD\u8bca\u65ad\u6a21\u578b\u5bf9\u88ab\u4f4e\u4f30\u6216\u672a\u5145\u5206\u4ee3\u8868\u7684\u4eba\u7fa4\uff08\u5982\u62c9\u4e01\u7f8e\u6d32\u4e34\u5e8a\u961f\u5217\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63ed\u793a\u6f5c\u5728\u57df\u504f\u79fb\u5e76\u627e\u5230\u63d0\u5347\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u5728ADNI\u6570\u636e\u96c6\u4e0a\u57fa\u51c6\u6d4b\u8bd5\u82e5\u5e72\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u6a21\u578b\uff0c\u5e76\u5728FLENI\uff08\u963f\u6839\u5ef7\uff09\u4e34\u5e8a\u961f\u5217\u4e0a\u8bc4\u4f30\u6cdb\u5316\u6027\u80fd\uff1b\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u9884\u5904\u7406\uff08\u6bcf\u56fe\u50cf\u5f52\u4e00\u5316\uff09\u548c\u91c7\u6837\u9009\u62e9\u5bf9\u6cdb\u5316\u7684\u5f71\u54cd\uff1b\u4f7f\u7528\u906e\u6321\u654f\u611f\u6027\u5206\u6790\uff08occlusion\uff09\u68c0\u67e5\u6a21\u578b\u6ce8\u610f\u529b\u5206\u5e03\u3002", "result": "\u5728ADNI\u4e0a\u6a21\u578bAUC\u6700\u9ad8\u53ef\u8fbe0.96-0.97\uff0c\u4f46\u5728FLENI\u4e0a\u964d\u81f30.80-0.82\uff1b\u4e0d\u540c\u67b6\u6784\u8868\u73b0\u76f8\u8fd1\uff1b\u6bcf\u56fe\u50cf\u5f52\u4e00\u5316\u548c\u6b63\u786e\u7684\u91c7\u6837\u9009\u62e9\u663e\u8457\u6539\u5584\u6cdb\u5316\uff1b\u906e\u6321\u5206\u6790\u663e\u793a\u6a21\u578b\u5728AD\u7c7b\u5173\u6ce8\u5178\u578b\u4f4e\u4ee3\u8c22\u533a\uff0c\u4f46\u5728\u5176\u4ed6\u7c7b\u4e0eFLENI\u626b\u63cf\u4e0a\u5173\u6ce8\u533a\u57df\u4e0d\u660e\u786e\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\u5728ADNI\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u62c9\u4e01\u7f8e\u6d32\u771f\u5b9e\u4e34\u5e8a\u961f\u5217\uff08FLENI\uff09\u4e0a\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u5b58\u5728\u660e\u663e\u7684\u57df\u504f\u79fb\u95ee\u9898\uff0c\u4e14\u4e0d\u540c\u67b6\u6784\uff08\u5377\u79ef\u4e0eTransformer\uff09\u95f4\u6027\u80fd\u5dee\u5f02\u4e0d\u5927\u3002"}}
{"id": "2511.00738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00738", "abs": "https://arxiv.org/abs/2511.00738", "authors": ["Dmitrii Khizbullin", "Maksim Konoplia"], "title": "Towards classification-based representation learning for place recognition on LiDAR scans", "comment": null, "summary": "Place recognition is a crucial task in autonomous driving, allowing vehicles\nto determine their position using sensor data. While most existing methods rely\non contrastive learning, we explore an alternative approach by framing place\nrecognition as a multi-class classification problem. Our method assigns\ndiscrete location labels to LiDAR scans and trains an encoder-decoder model to\nclassify each scan's position directly. We evaluate this approach on the\nNuScenes dataset and show that it achieves competitive performance compared to\ncontrastive learning-based methods while offering advantages in training\nefficiency and stability.", "AI": {"tldr": "\u628aLiDAR\u5730\u70b9\u8bc6\u522b\u6539\u4e3a\u591a\u7c7b\u5206\u7c7b\uff1a\u7ed9\u626b\u63cf\u6253\u79bb\u6563\u4f4d\u7f6e\u6807\u7b7e\uff0c\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u5206\u7c7b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728NuScenes\u4e0a\u6027\u80fd\u63a5\u8fd1\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f46\u8bad\u7ec3\u66f4\u9ad8\u6548\u3001\u66f4\u7a33\u5b9a\u3002", "motivation": "\u76ee\u524d\u5927\u591a\u6570\u4f4d\u7f6e\u4fe1\u606f\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5bf9\u6bd4\u5b66\u4e60\uff0c\u9700\u8981\u590d\u6742\u7684\u6837\u672c\u6784\u9020\u548c\u96be\u4ee5\u7a33\u5b9a\u7684\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5c06\u95ee\u9898\u89c6\u4e3a\u591a\u7c7b\u5206\u7c7b\u53ef\u4ee5\u7b80\u5316\u8bad\u7ec3\u76ee\u6807\uff0c\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6536\u655b\u7a33\u5b9a\u6027\u3002", "method": "\u5c06\u573a\u666f\u79bb\u6563\u5316\u4e3a\u82e5\u5e72\u4f4d\u7f6e\u6807\u7b7e\uff0c\u5bf9\u6bcf\u4e2aLiDAR\u626b\u63cf\u5206\u914d\u4f4d\u7f6e\u7c7b\u522b\u6807\u7b7e\uff0c\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e00\u4e2a\u7f16\u7801\u5668-\u89e3\u7801\u5668\uff08encoder-decoder\uff09\u6a21\u578b\uff0c\u76f4\u63a5\u5bf9\u8f93\u5165\u626b\u63cf\u8fdb\u884c\u4f4d\u7f6e\u5206\u7c7b\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a\u5206\u7c7b\u65b9\u6cd5\u7684\u68c0\u7d22/\u8bc6\u522b\u6027\u80fd\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u8bad\u7ec3\u65f6\u95f4\u66f4\u77ed\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u66f4\u7a33\u5b9a\uff0c\u53ef\u80fd\u964d\u4f4e\u8d85\u53c2\u6570\u8c03\u8282\u9700\u6c42\u3002", "conclusion": "\u672c\u6587\u5c06\u57fa\u4e8eLiDAR\u7684\u4f4d\u7f6e\u4fe1\u606f\u8bc6\u522b\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\uff0c\u7ed3\u8bba\u662f\u8fd9\u79cd\u65b9\u5f0f\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u80fd\u5f97\u5230\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u8bad\u7ec3\u6548\u7387\u548c\u7a33\u5b9a\u6027\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2511.00749", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.00749", "abs": "https://arxiv.org/abs/2511.00749", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Gavin Abercrombie", "Ioannis Konstas"], "title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models", "comment": "This is a preprint under review", "summary": "Social media has exacerbated the promotion of Western beauty norms, leading\nto negative self-image, particularly in women and girls, and causing harm such\nas body dysmorphia. Increasingly content on the internet has been artificially\ngenerated, leading to concerns that these norms are being exaggerated. The aim\nof this work is to study how generative AI models may encode 'beauty' and erase\n'ugliness', and discuss the implications of this for society. To investigate\nthese aims, we create two image generation pipelines: a text-to-image model and\na text-to-language model-to image model. We develop a structured beauty\ntaxonomy which we use to prompt three language models (LMs) and two\ntext-to-image models to cumulatively generate 5984 images using our two\npipelines. We then recruit women and non-binary social media users to evaluate\n1200 of the images through a Likert-scale within-subjects study. Participants\nshow high agreement in their ratings. Our results show that 86.5% of generated\nimages depicted people with lighter skin tones, 22% contained explicit content\ndespite Safe for Work (SFW) training, and 74% were rated as being in a younger\nage demographic. In particular, the images of non-binary individuals were rated\nas both younger and more hypersexualised, indicating troubling intersectional\neffects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such\nas \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings\nregardless of gender. This work sheds light on the pervasive demographic biases\nrelated to beauty standards present in generative AI models -- biases that are\nactively perpetuated by model developers, such as via negative prompting. We\nconclude by discussing the implications of this on society, which include\npollution of the data streams and active erasure of features that do not fall\ninside the stereotype of what is considered beautiful by developers.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u7f8e\u5b66\u8868\u73b0\u4e0a\u5b58\u5728\u5f3a\u70c8\u504f\u89c1\uff1a\u504f\u5411\u5e74\u8f7b\u3001\u767d\u7699\u548c\u6027\u5316\u7684\u5f62\u8c61\uff0c\u8d1f\u9762\u63d0\u793a\u53cd\u800c\u5bfc\u81f4\u66f4\u591aNSFW\u8f93\u51fa\uff0c\u63d0\u793a\u6a21\u578b\u4f1a\u64e6\u9664\u5e76\u653e\u5927\u975e\u4e3b\u6d41\u7f8e\u7684\u7279\u5f81\uff0c\u5177\u6709\u91cd\u8981\u793e\u4f1a\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7f16\u7801\u201c\u7f8e\u201d\u4e0e\u201c\u4e11\u201d\uff0c\u5e76\u63a2\u8ba8\u8fd9\u79cd\u7f16\u7801\u5bf9\u793e\u4f1a\uff08\u5982\u81ea\u6211\u5f62\u8c61\u3001\u6587\u5316\u523b\u677f\u5370\u8c61\u3001\u6570\u636e\u6d41\u6c61\u67d3\u7b49\uff09\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e24\u6761\u751f\u6210\u7ba1\u9053\uff08\u6587\u672c\u5230\u56fe\u50cf\uff0c\u6587\u672c\u5230\u8bed\u8a00\u518d\u5230\u56fe\u50cf\uff09\uff0c\u5236\u5b9a\u7ed3\u6784\u5316\u7f8e\u5bb9\u5206\u7c7b\u6cd5\uff0c\u7528\u8be5\u5206\u7c7b\u6cd5\u63d0\u793a\u4e09\u4e2a\u8bed\u8a00\u6a21\u578b\u548c\u4e24\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u7d2f\u8ba1\u751f\u62105984\u5f20\u56fe\u50cf\uff0c\u5e76\u62db\u52df\u5973\u6027\u548c\u975e\u4e8c\u5143\u793e\u5a92\u7528\u6237\u7528Likert\u91cf\u8868\u8bc4\u4ef71200\u5f20\u56fe\u50cf\uff1b\u7edf\u8ba1\u5206\u6790\u80a4\u8272\u3001\u5e74\u9f84\u5206\u5e03\u3001NSFW\u6bd4\u4f8b\u53ca\u4ea4\u53c9\u4eba\u53e3\u5b66\u6548\u679c\u3002", "result": "\u751f\u6210\u56fe\u50cf\u4e2d86.5%\u4e3a\u80a4\u8272\u8f83\u6d45\uff0c22%\u542bNSFW\u5185\u5bb9\uff0c74%\u88ab\u8bc4\u4e3a\u66f4\u5e74\u8f7b\u4eba\u7fa4\uff1b\u975e\u4e8c\u5143\u4e2a\u4f53\u56fe\u50cf\u88ab\u8bc4\u4e3a\u66f4\u5e74\u8f7b\u4e14\u66f4\u5177\u6027\u5316\uff1b\u5e26\u6709\u8d1f\u9762\u7f8e\u5b66\u63d0\u793a\u7684\u56fe\u50cfNSFW\u8bc4\u5206\u66f4\u9ad8\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u7f8e\u5b66\u523b\u677f\u5370\u8c61\u4e0a\u8868\u73b0\u51fa\u660e\u663e\u504f\u89c1\uff0c\u503e\u5411\u4e8e\u4ea7\u751f\u66f4\u5e74\u8f7b\u3001\u80a4\u8272\u66f4\u6d45\u3001\u5e76\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u66f4\u5177\u6027\u5316\u7684\u5f62\u8c61\uff0c\u5c24\u5176\u5bf9\u975e\u4e8c\u5143\u6027\u522b\u6709\u6240\u653e\u5927\uff1b\u6b64\u5916\uff0c\u8d1f\u9762\u63d0\u793a\uff08\u4f8b\u5982\u201c\u4e11\u201d\u6216\u5177\u4f53\u7279\u5f81\u5982\u201c\u5bbd\u9f3b\u201d\uff09\u4f1a\u5bfc\u81f4\u66f4\u591aNSFW\u8f93\u51fa\uff0c\u8bf4\u660e\u6a21\u578b\u53ca\u5176\u8bad\u7ec3/\u8c03\u4f18\u8fc7\u7a0b\u5728\u523b\u610f\u6216\u65e0\u610f\u4e2d\u64e6\u9664\u975e\u4e3b\u6d41\u201c\u7f8e\u201d\u7684\u7279\u5f81\u3002"}}
{"id": "2511.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00777", "abs": "https://arxiv.org/abs/2511.00777", "authors": ["Anis Suttan Shahrir", "Zakiah Ayop", "Syarulnaziah Anawar", "Norulzahrah Mohd Zainudin"], "title": "A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection", "comment": null, "summary": "Durian plantation suffers from animal intrusions that cause crop damage and\nfinancial loss. The traditional farming practices prove ineffective due to the\nunavailability of monitoring without human intervention. The fast growth of\nmachine learning and Internet of Things (IoT) technology has led to new ways to\ndetect animals. However, current systems are limited by dependence on single\nobject detection algorithms, less accessible notification platforms, and\nlimited deterrent mechanisms. This research suggests an IoT-enabled animal\ndetection system for durian crops. The system integrates YOLOv5 and SSD object\ndetection algorithms to improve detection accuracy. The system provides\nreal-time monitoring, with detected intrusions automatically reported to\nfarmers via Telegram notifications for rapid response. An automated sound\nmechanism (e.g., tiger roar) is triggered once the animal is detected. The\nYOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,\n85% and 70%, respectively. The system shows the highest accuracy in daytime and\ndecreases at night, regardless of whether the image is still or a video.\nOverall, this study contributes a comprehensive and practical framework that\ncombines detection, notification, and deterrence, paving the way for future\ninnovations in automated farming solutions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u5957\u57fa\u4e8eIoT\u7684\u69b4\u83b2\u56ed\u52a8\u7269\u68c0\u6d4b\u4e0e\u9a71\u8d76\u7cfb\u7edf\uff0c\u878d\u5408YOLOv5\u4e0eSSD\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u652f\u6301Telegram\u544a\u8b66\u4e0e\u81ea\u52a8\u58f0\u5b66\u5a01\u6151\uff1b\u767d\u5929\u6548\u679c\u826f\u597d\uff0c\u591c\u95f4\u6027\u80fd\u4e0b\u964d\u3002", "motivation": "\u69b4\u83b2\u79cd\u690d\u53d7\u52a8\u7269\u5165\u4fb5\u9020\u6210\u4ea7\u91cf\u4e0e\u7ecf\u6d4e\u635f\u5931\uff0c\u4f46\u4f20\u7edf\u4eba\u5de5\u5de1\u67e5\u6210\u672c\u9ad8\u4e14\u96be\u4ee524\u5c0f\u65f6\u76d1\u63a7\uff1b\u56e0\u6b64\u5229\u7528\u673a\u5668\u5b66\u4e60\u4e0e\u7269\u8054\u7f51\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u5b9e\u65f6\u7684\u5165\u4fb5\u68c0\u6d4b\u4e0e\u54cd\u5e94\u5177\u6709\u91cd\u8981\u73b0\u5b9e\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542b\u6444\u50cf\u5934\u3001\u8fb9\u7f18\u8bbe\u5907\u548cIoT\u901a\u4fe1\u7684\u76d1\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528YOLOv5\u548cSSD\u4e24\u79cd\u6a21\u578b\u7684\u878d\u5408\u7b56\u7565\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff1b\u68c0\u6d4b\u5230\u52a8\u7269\u540e\u7cfb\u7edf\u901a\u8fc7Telegram Bot\u53d1\u9001\u5b9e\u65f6\u901a\u77e5\u7ed9\u519c\u6237\uff0c\u5e76\u9a71\u52a8\u64ad\u653e\u9884\u8bbe\u7684\u5a01\u6151\u97f3\u6548\u3002\u7cfb\u7edf\u5728\u767d\u5929\u4e0e\u591c\u95f4\u3001\u9759\u6001\u56fe\u7247\u4e0e\u89c6\u9891\u6d41\u7b49\u4e0d\u540c\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u878d\u5408YOLOv5\u4e0eSSD\u7684\u6a21\u578b\u5728\u767d\u5929\u53d6\u5f97\u8f83\u9ad8\u8bc6\u522b\u7387\uff1a\u5927\u8c6190%\u3001\u91ce\u732a85%\u3001\u7334\u5b5070%\uff1b\u6574\u4f53\u5728\u591c\u95f4\u8bc6\u522b\u6027\u80fd\u4e0b\u964d\uff1b\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u57fa\u4e8eTelegram\u7684\u5b9e\u65f6\u544a\u8b66\u4e0e\u81ea\u52a8\u58f0\u97f3\u5a01\u6151\u529f\u80fd\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u8054\u7f51\u7684\u69b4\u83b2\u56ed\u52a8\u7269\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408YOLOv5\u4e0eSSD\u4e24\u79cd\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u68c0\u6d4b\u3001\u901a\u77e5\u4e0e\u5a01\u6151\u673a\u5236\u7684\u7efc\u5408\u65b9\u6848\u3002\u5b9e\u9a8c\u8bc1\u660e\u878d\u5408\u6a21\u578b\u5728\u767d\u5929\u5bf9\u5927\u8c61\u3001\u91ce\u732a\u548c\u7334\u5b50\u7684\u68c0\u6d4b\u51c6\u786e\u7387\u5206\u522b\u4e3a90%\u300185%\u548c70%\uff0c\u591c\u95f4\u4ee5\u53ca\u89c6\u9891\u573a\u666f\u4e0b\u51c6\u786e\u7387\u4e0b\u964d\u3002\u7cfb\u7edf\u901a\u8fc7Telegram\u5b9e\u65f6\u901a\u77e5\u519c\u6237\uff0c\u5e76\u5728\u68c0\u6d4b\u5230\u5165\u4fb5\u65f6\u89e6\u53d1\u81ea\u52a8\u58f0\u97f3\u9a71\u8d76\uff08\u5982\u864e\u5578\uff09\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u52a8\u7269\u5165\u4fb5\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u5373\u65f6\u5a01\u6151\u80fd\u529b\u3002"}}
{"id": "2511.00785", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00785", "abs": "https://arxiv.org/abs/2511.00785", "authors": ["Juan Wang", "Yasutomo Kawanishi", "Tomo Miyazaki", "Zhijie Wang", "Shinichiro Omachi"], "title": "Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking", "comment": "Under review in Pattern Recognition", "summary": "3D instance segmentation is an important task for real-world applications. To\navoid costly manual annotations, existing methods have explored generating\npseudo labels by transferring 2D masks from foundation models to 3D. However,\nthis approach is often suboptimal since the video frames are processed\nindependently. This causes inconsistent segmentation granularity and\nconflicting 3D pseudo labels, which degrades the accuracy of final\nsegmentation. To address this, we introduce a Granularity-Consistent automatic\n2D Mask Tracking approach that maintains temporal correspondences across\nframes, eliminating conflicting pseudo labels. Combined with a three-stage\ncurriculum learning framework, our approach progressively trains from\nfragmented single-view data to unified multi-view annotations, ultimately\nglobally coherent full-scene supervision. This structured learning pipeline\nenables the model to progressively expose to pseudo-labels of increasing\nconsistency. Thus, we can robustly distill a consistent 3D representation from\ninitially fragmented and contradictory 2D priors. Experimental results\ndemonstrated that our method effectively generated consistent and accurate 3D\nsegmentations. Furthermore, the proposed method achieved state-of-the-art\nresults on standard benchmarks and open-vocabulary ability.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u5e27\u63a9\u7801\u8ddf\u8e2a\u4fdd\u8bc1\u4f2a\u6807\u7b7e\u7c92\u5ea6\u4e00\u81f4\uff0c\u914d\u5408\u9010\u6b65\u7684\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u80fd\u4ece\u788e\u7247\u53162D\u5148\u9a8c\u7a33\u5065\u84b8\u998f\u51fa\u4e00\u81f4\u76843D\u5b9e\u4f8b\u5206\u5272\uff0c\u8fbe\u6210SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u5c062D foundation model\u63a9\u7801\u8fc1\u79fb\u52303D\u7684\u65b9\u6cd5\u5ffd\u7565\u89c6\u9891\u5e27\u95f4\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u5206\u5272\u7c92\u5ea6\u4e0d\u4e00\u81f4\u548c\u4f2a\u6807\u7b7e\u51b2\u7a81\uff0c\u5f71\u54cd\u6700\u7ec83D\u5206\u5272\u7cbe\u5ea6\u3002", "method": "\u8bbe\u8ba1\u4e86Granularity-Consistent\u81ea\u52a82D\u63a9\u7801\u8ddf\u8e2a\u4ee5\u5728\u5e27\u95f4\u7ef4\u62a4\u65f6\u5e8f\u5bf9\u5e94\uff0c\u6d88\u9664\u51b2\u7a81\u4f2a\u6807\u7b7e\uff1b\u7ed3\u5408\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u4ece\u5355\u89c6\u89d2\u788e\u7247\u5316\u6807\u7b7e\u5230\u591a\u89c6\u89d2\u7edf\u4e00\u6ce8\u91ca\uff0c\u518d\u5230\u5168\u666f\u4e00\u81f4\u76d1\u7763\uff0c\u9010\u6b65\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff1b\u5b9e\u9a8c\u663e\u793a\u751f\u6210\u7684\u4e00\u81f4\u4e14\u51c6\u786e\u76843D\u5206\u5272\u3002", "conclusion": "\u63d0\u51fa\u7684GC\u81ea\u52a82D\u63a9\u7801\u8ddf\u8e2a\u548c\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u9ad8\u4ece2D\u4f2a\u6807\u7b7e\u5230\u4e00\u81f4\u60273D\u5b9e\u4f8b\u5206\u5272\u7684\u6548\u679c\uff0c\u89e3\u51b3\u4e86\u8de8\u5e27\u4e0d\u4e00\u81f4\u548c\u51b2\u7a81\u4f2a\u6807\u7b7e\u95ee\u9898\u3002"}}
{"id": "2511.00795", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00795", "abs": "https://arxiv.org/abs/2511.00795", "authors": ["Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru", "Sai Teja Erukude"], "title": "FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data", "comment": "Published in IEEE", "summary": "Federated Learning (FL) allows multiple institutions to cooperatively train\nmachine learning models while retaining sensitive data at the source, which has\ngreat utility in privacy-sensitive environments. However, FL systems remain\nvulnerable to membership-inference attacks and data heterogeneity. This paper\npresents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using\nsynthetic oncologic CT scans with tumor annotations. It evaluates segmentation\nperformance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and\nFedAvg with DP-SGD. Results show a distinct trade-off between privacy and\nutility: FedAvg is high performance (Dice around 0.85) with more privacy\nleakage (attack AUC about 0.72), while DP-SGD provides a higher level of\nprivacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx\nand FedBN offer balanced performance under heterogeneous data, especially with\nnon-identical distributed client data. FedOnco-Bench serves as a standardized,\nopen-source platform for benchmarking and developing privacy-preserving FL\nmethods for medical image segmentation.", "AI": {"tldr": "FedOnco-Bench\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u653e\u57fa\u51c6\uff0c\u7528\u5408\u6210\u80bf\u7624CT\u8bc4\u6d4b\u8054\u90a6\u5b66\u4e60\u5728\u533b\u7597\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u9690\u79c1\u4e0e\u6548\u7528\u6743\u8861\uff0c\u663e\u793aFedAvg\u9ad8\u6548\u4f46\u9690\u79c1\u5dee\uff0cDP-SGD\u9690\u79c1\u597d\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff0cFedProx/FedBN\u5728\u5f02\u6784\u6570\u636e\u4e0b\u66f4\u5e73\u8861\u3002", "motivation": "\u5728\u533b\u7597\u5f71\u50cf\u5206\u5272\u4e2d\uff0c\u6570\u636e\u654f\u611f\u5bfc\u81f4\u96be\u4ee5\u5171\u4eab\uff0c\u8054\u90a6\u5b66\u4e60\u53ef\u4ee5\u89e3\u51b3\u6570\u636e\u7559\u5728\u6e90\u5934\u7684\u95ee\u9898\uff0c\u4f46\u4ecd\u9762\u4e34\u6210\u5458\u63a8\u65ad\u653b\u51fb\u4e0e\u5f02\u8d28\u6027\u6570\u636e\u6311\u6218\uff0c\u9700\u8981\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30\u9690\u79c1-\u6548\u7528\u6298\u8877\u3002", "method": "\u4f7f\u7528\u5408\u6210\u7684\u80bf\u7624CT\u6570\u636e\uff0c\u6bd4\u8f83FedAvg\u3001FedProx\u3001FedBN\u4ee5\u53ca\u5728FedAvg\u4e0a\u5e94\u7528DP-SGD\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u5272Dice\u548c\u6210\u5458\u63a8\u65ad\u653b\u51fbAUC\u6765\u8bc4\u4f30\u6548\u7528\u548c\u9690\u79c1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff1aFedAvg\u5728\u7cbe\u5ea6\u4e0a\u6700\u597d\uff08Dice\u22480.85\uff09\u4f46\u9690\u79c1\u6cc4\u9732\u9ad8\uff08\u653b\u51fbAUC\u22480.72\uff09\uff1b\u52a0\u5165DP-SGD\u663e\u8457\u964d\u4f4e\u9690\u79c1\u6cc4\u9732\uff08AUC\u22480.25\uff09\u4f46\u7cbe\u5ea6\u4e0b\u964d\uff08Dice\u22480.79\uff09\uff1bFedProx\u548cFedBN\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\u573a\u666f\u4e0b\u63d0\u4f9b\u66f4\u5e73\u8861\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u6784\u5efa\u4e86FedOnco-Bench\uff0c\u4e00\u4e2a\u7528\u4e8e\u9690\u79c1\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u590d\u73b0\u57fa\u51c6\uff0c\u57fa\u4e8e\u5408\u6210\u80bf\u7624CT\u4e0e\u6807\u6ce8\uff0c\u8bc4\u4f30\u5206\u5272\u6027\u80fd\u4e0e\u9690\u79c1\u6cc4\u9732\u7684\u6743\u8861\u3002"}}
{"id": "2511.00801", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.00801", "abs": "https://arxiv.org/abs/2511.00801", "authors": ["Zhihui Chen", "Mengling Feng"], "title": "Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing", "comment": null, "summary": "Recent advances in multimodal large language models have enabled remarkable\nmedical image editing capabilities. However, the research community's progress\nremains constrained by the absence of large-scale, high-quality, and openly\naccessible datasets built specifically for medical image editing with strict\nanatomical and clinical constraints. We introduce Med-Banana-50K, a\ncomprehensive 50K-image dataset for instruction-based medical image editing\nspanning three modalities (chest X-ray, brain MRI, fundus photography) and 23\ndisease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image\nto generate bidirectional edits (lesion addition and removal) from real medical\nimages. What distinguishes Med-Banana-50K from general-domain editing datasets\nis our systematic approach to medical quality control: we employ LLM-as-Judge\nwith a medically grounded rubric (instruction compliance, structural\nplausibility, realism, and fidelity preservation) and history-aware iterative\nrefinement up to five rounds. Beyond single-turn editing, Med-Banana-50K\nincludes 37K failed attempts with full conversation logs for preference\nlearning and alignment research. By providing this large-scale, medically\nvalidated, and fully documented resource, Med-Banana-50K establishes a\nfoundation for training and evaluating the next generation of medical image\nediting models.Our dataset and code are publicly available at\n[https://github.com/richardChenzhihui/med-banana-50k].", "AI": {"tldr": "Med-Banana-50K \u662f\u4e00\u4e2a\u7ecf\u8fc7 LLM \u533b\u5b66\u8d28\u63a7\u548c\u591a\u8f6e\u4fee\u6b63\u7684\u3001\u9762\u5411\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u768450K\u7ea7\u53cc\u5411\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u9644\u5e26\u5927\u91cf\u5931\u8d25\u6837\u672c\u4e0e\u5bf9\u8bdd\u8bb0\u5f55\uff0c\u65e8\u5728\u652f\u6301\u8bad\u7ec3\u8bc4\u4f30\u533b\u5b66\u7f16\u8f91\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\u7f3a\u4e4f\u6ee1\u8db3\u4e25\u683c\u89e3\u5256\u4e0e\u4e34\u5e8a\u7ea6\u675f\u7684\u533b\u5b66\u4e13\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5236\u7ea6\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528 Gemini-2.5-Flash-Image \u5bf9\u771f\u5b9e\u533b\u5b66\u56fe\u50cf\u751f\u6210\u6dfb\u52a0\u4e0e\u53bb\u9664\u75c5\u7076\u7684\u7f16\u8f91\u6837\u672c\uff0c\u91c7\u7528 LLM-as-Judge \u6309\u533b\u5b66\u8bc4\u5206\u91cf\u8868\uff08\u6307\u4ee4\u5408\u89c4\u6027\u3001\u7ed3\u6784\u5408\u7406\u6027\u3001\u903c\u771f\u5ea6\u3001\u4fdd\u771f\u6027\uff09\u8fdb\u884c\u591a\u8f6e\uff08\u6700\u591a5\u8f6e\uff09\u8fed\u4ee3\u4fee\u6b63\uff0c\u5e76\u4fdd\u7559\u5b8c\u6574\u7684\u5931\u8d25\u5c1d\u8bd5\u5bf9\u8bdd\u65e5\u5fd7\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b50K\u5f20\u7ecf\u8fc7\u533b\u5b66\u9a8c\u8bc1\u7684\u7f16\u8f91\u56fe\u50cf\u6570\u636e\u96c6\uff08\u542b37K\u6761\u5931\u8d25\u5c1d\u8bd5\u4e0e\u5bf9\u8bdd\u65e5\u5fd7\uff09\uff0c\u8986\u76d6\u80f8\u7247\u3001\u8111MRI\u3001\u773c\u5e95\u7167\u4e09\u79cd\u6a21\u6001\u548c23\u79cd\u75be\u75c5\uff0c\u5e76\u516c\u5f00\u6570\u636e\u4e0e\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u793e\u533a\u7814\u7a76\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a Med-Banana-50K \u7684\u5927\u89c4\u6a21\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u6570\u636e\u96c6\uff0c\u8986\u76d6\u4e09\u79cd\u6a21\u6001\u548c23\u79cd\u75be\u75c5\uff0c\u91c7\u7528\u53cc\u5411\u7f16\u8f91\u5e76\u7ed3\u5408\u57fa\u4e8eLLM\u7684\u533b\u5b66\u8d28\u63a7\u6d41\u7a0b\uff0c\u65e8\u5728\u63a8\u52a8\u533b\u5b66\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u3002"}}
{"id": "2511.00810", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00810", "abs": "https://arxiv.org/abs/2511.00810", "authors": ["Shijie Zhou", "Viet Dac Lai", "Hao Tan", "Jihyung Kil", "Wanrong Zhu", "Changyou Chen", "Ruiyi Zhang"], "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding", "comment": null, "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u9f50MLLM\u5185\u5728\u6ce8\u610f\u529b\u4e0e\u8865\u4e01\u7ea7\u76d1\u7763\u4fe1\u53f7\uff0cGUI-AIMA\u5728\u4f4e\u6570\u636e\u4e0b\u5b9e\u73b0\u9ad8\u6548\u3001\u5750\u6807\u81ea\u7531\u7684GUI grounding\uff0c\u4e14\u652f\u6301zoom-in\u4ee5\u63d0\u5347\u7cbe\u5ea6\uff0c\u57283B\u6a21\u578b\u4e0a\u8fbeSOTA\u3002", "motivation": "\u76f4\u63a5\u4ece\u89c6\u89c9\u8f93\u5165\u751f\u6210\u7cbe\u786e\u5750\u6807\u56f0\u96be\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\uff1b\u66f4\u76f4\u89c2\u7684\u505a\u6cd5\u662f\u5148\u9009\u76f8\u5173\u89c6\u89c9\u8865\u4e01\u518d\u5728\u8865\u4e01\u5185\u786e\u5b9a\u70b9\u51fb\u4f4d\u7f6e\uff1b\u89c2\u5bdf\u5230\u901a\u7528MLLM\u5df2\u5177\u5907\u4e00\u5b9a\u7684\u539f\u751fgrounding\u80fd\u529b\uff0c\u6545\u8bbe\u8ba1\u65b9\u6cd5\u4ee5\u6fc0\u6d3b\u5e76\u5229\u7528\u8fd9\u79cd\u80fd\u529b\u3002", "method": "\u5229\u7528MLLM\u7684\u67e5\u8be2-\u89c6\u89c9\u6ce8\u610f\u77e9\u9635\uff0c\u8fdb\u884c\u591a\u5934\u805a\u5408\u4ee5\u81ea\u9002\u5e94\u751f\u6210\u8865\u4e01\u7ea7\u7684grounding\u4fe1\u53f7\uff1b\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7f\u6a21\u578b\u6ce8\u610f\u529b\u4e0e\u8fd9\u4e9b\u8865\u4e01\u4fe1\u53f7\u5bf9\u9f50\uff1b\u91c7\u7528\u5750\u6807\u81ea\u7531\u8bbe\u8ba1\u5e76\u652f\u6301\u63d2\u62d4\u5f0f\u653e\u5927\uff08zoom-in\uff09\u9636\u6bb5\u4ee5\u63d0\u5347\u7cbe\u786e\u5ea6\u3002", "result": "\u5728\u4ec5\u4f7f\u752885k\u622a\u56fe\u8bad\u7ec3\u4e0b\uff0cGUI-AIMA-3B\u57283B\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\uff0cScreenSpot-Pro\u5e73\u5747\u51c6\u786e\u738758.6%\uff0cOSWorld-G\u4e3a62.2%\uff0c\u663e\u793a\u51fa\u51fa\u8272\u7684\u6570\u636e\u6548\u7387\u4e0e\u6027\u80fd\u3002", "conclusion": "GUI-AIMA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u4e14\u65e0\u5750\u6807\u7684\u76d1\u7763\u5fae\u8c03\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u7684GUI grounding\uff0c\u901a\u8fc7\u5bf9\u9f50\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLM\uff09\u5185\u5728\u7684\u6ce8\u610f\u529b\u4e0e\u8865\u4e01\u7ea7\u7684\u5b9a\u4f4d\u4fe1\u53f7\uff0c\u80fd\u5728\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e0b\u89e6\u53d1\u6a21\u578b\u7684\u539f\u751f\u5b9a\u4f4d\u80fd\u529b\u3002"}}
{"id": "2511.00815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00815", "abs": "https://arxiv.org/abs/2511.00815", "authors": ["Yue Gou", "Fanghui Song", "Yuming Xing", "Shengzhu Shi", "Zhichang Guo", "Boying Wu"], "title": "TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation", "comment": "14 pages, 7 figures", "summary": "Pancreas segmentation in medical image processing is a persistent challenge\ndue to its small size, low contrast against adjacent tissues, and significant\ntopological variations. Traditional level set methods drive boundary evolution\nusing gradient flows, often ignoring pointwise topological effects. Conversely,\ndeep learning-based segmentation networks extract rich semantic features but\nfrequently sacrifice structural details. To bridge this gap, we propose a novel\nmodel named TA-LSDiff, which combined topology-aware diffusion probabilistic\nmodel and level set energy, achieving segmentation without explicit geometric\nevolution. This energy function guides implicit curve evolution by integrating\nthe input image and deep features through four complementary terms. To further\nenhance boundary precision, we introduce a pixel-adaptive refinement module\nthat locally modulates the energy function using affinity weighting from\nneighboring evidence. Ablation studies systematically quantify the contribution\nof each proposed component. Evaluations on four public pancreas datasets\ndemonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming\nexisting methods. These results establish TA-LSDiff as a practical and accurate\nsolution for pancreas segmentation.", "AI": {"tldr": "TA-LSDiff\u5c06\u62d3\u6251\u611f\u77e5\u6269\u6563\u6a21\u578b\u4e0e\u6c34\u5e73\u96c6\u80fd\u91cf\u7ed3\u5408\uff0c\u8f85\u4ee5\u50cf\u7d20\u81ea\u9002\u5e94\u7ec6\u5316\uff0c\u5728\u80f0\u817a\u5206\u5272\u4e0a\u5b9e\u73b0\u66f4\u597d\u7ed3\u6784\u4fdd\u7559\u548c\u8fb9\u754c\u7cbe\u5ea6\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u80f0\u817a\u5206\u5272\u56f0\u96be\u5728\u4e8e\u5668\u5b98\u5c0f\u3001\u4e0e\u5468\u56f4\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u4f4e\u53ca\u62d3\u6251\u53d8\u5f02\u5927\uff1b\u4f20\u7edf\u6c34\u5e73\u96c6\u6ce8\u91cd\u5c40\u90e8\u68af\u5ea6\u9a71\u52a8\u5ffd\u89c6\u62d3\u6251\u4fe1\u606f\uff0c\u6df1\u5ea6\u7f51\u7edc\u867d\u63d0\u53d6\u8bed\u4e49\u4f46\u6613\u4e22\u5931\u7ed3\u6784\u7ec6\u8282\uff0c\u56e0\u800c\u9700\u8981\u7ed3\u5408\u62d3\u6251\u7ea6\u675f\u4e0e\u6df1\u5ea6\u7279\u5f81\u4ee5\u517c\u987e\u8bed\u4e49\u4e0e\u7ed3\u6784\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u6784\u5efa\u62d3\u6251\u611f\u77e5\u6269\u6563\u6982\u7387\u6a21\u578b\u4e0e\u4e00\u4e2a\u542b\u6709\u56db\u9879\u4e92\u8865\u80fd\u91cf\u9879\u7684\u6c34\u5e73\u96c6\u80fd\u91cf\u51fd\u6570\uff0c\u80fd\u91cf\u9879\u7ed3\u5408\u8f93\u5165\u56fe\u50cf\u548c\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u4ee5\u6307\u5bfc\u9690\u5f0f\u66f2\u7ebf\u6f14\u5316\uff1b\u5f15\u5165\u50cf\u7d20\u81ea\u9002\u5e94\u7ec6\u5316\u6a21\u5757\uff0c\u5229\u7528\u90bb\u57df\u4eb2\u548c\u6743\u91cd\u5c40\u90e8\u8c03\u5236\u80fd\u91cf\u4ee5\u7cbe\u7ec6\u5316\u8fb9\u754c\uff1b\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u8d21\u732e\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5f00\u80f0\u817a\u6570\u636e\u96c6\u4e0a\uff0cTA-LSDiff\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u4f5c\u8005\u79f0\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff09\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5206\u6790\u5c55\u793a\u5404\u6a21\u5757\u5bf9\u6027\u80fd\u7684\u8d21\u732e\uff0c\u4e14\u50cf\u7d20\u81ea\u9002\u5e94\u7ec6\u5316\u663e\u8457\u63d0\u5347\u8fb9\u754c\u7cbe\u7ec6\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u6269\u6563\u6982\u7387\u6a21\u578b\u4e0e\u6c34\u5e73\u96c6\u80fd\u91cf\u7684\u80f0\u817a\u5206\u5272\u65b9\u6cd5TA-LSDiff\uff0c\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u95f4\u63a5\u9a71\u52a8\u66f2\u7ebf\u6f14\u5316\uff0c\u907f\u514d\u663e\u5f0f\u51e0\u4f55\u6f14\u5316\uff0c\u540c\u65f6\u878d\u5165\u6df1\u5ea6\u7279\u5f81\u4e0e\u56fe\u50cf\u4fe1\u606f\uff0c\u8f85\u4ee5\u50cf\u7d20\u81ea\u9002\u5e94\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u8fb9\u754c\u51c6\u786e\u6027\u3002\u5b9e\u9a8c\u5728\u56db\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u5e76\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5b9e\u7528\u6027\u4e0e\u7cbe\u5ea6\u3002"}}
{"id": "2511.00821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00821", "abs": "https://arxiv.org/abs/2511.00821", "authors": ["Ruoxiang Huang", "Xindian Ma", "Rundong Kong", "Zhen Yuan", "Peng Zhang"], "title": "OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated strong performance across\nvarious multimodal tasks, where position encoding plays a vital role in\nmodeling both the sequential structure of textual information and the spatial\nstructure of visual information. However, current VLMs commonly adopt\nmodality-unified 1D or 2D positional indexing strategies, which treat textual\nand visual tokens uniformly without accounting for their distinct structural\nproperties and sequential continuity for text and spatial coherence for vision.\nTo address this limitation, we propose OMEGA, a novel position encoding\nframework that employs Modality-Specific Position Encoding (MSPE) to assign\npositional indices while preserving the inherent structures of each modality\nacross separate coordinate dimensions. Additionally, to align the information\ndensity of multimodal data in the positional index space, OMEGA introduces\nGlobal Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the\nposition encoding step size of visual tokens based on the embedding entropy of\nboth modalities. Experimental results demonstrate that OMEGA consistently\nenhances VLM performance across diverse architectures and VQA benchmarks. On\nvisual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline\nposition encoding strategies on Qwen2.5-VL-3B, with consistent gains observed\nacross larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.", "AI": {"tldr": "\u63d0\u51faOMEGA\uff1a\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u4f4d\u7f6e\u7f16\u7801\u4e0e\u81ea\u9002\u5e94\u6b65\u957f\u7f29\u653e\u5bf9\u9f50\u4fe1\u606f\u5bc6\u5ea6\uff0c\u6539\u8fdbVLM\u4f4d\u7f6e\u8868\u5f81\uff0c\u5728\u591a\u6a21\u578b\u591a\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5c24\u5176\u5bf9\u89c6\u89c9\u5bc6\u96c6\u4efb\u52a1\u6709\u6548\u3002", "motivation": "\u73b0\u6709VLM\u4f7f\u7528\u7edf\u4e00\u7684\u4e00\u7ef4\u6216\u4e8c\u7ef4\u4f4d\u7f6e\u7d22\u5f15\u7b56\u7565\uff0c\u672a\u533a\u5206\u6587\u672c\u4e0e\u89c6\u89c9\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u5bfc\u81f4\u4f4d\u7f6e\u7f16\u7801\u65e0\u6cd5\u5145\u5206\u8868\u5f81\u6587\u672c\u7684\u5e8f\u5217\u7279\u6027\u4e0e\u89c6\u89c9\u7684\u7a7a\u95f4\u7279\u6027\uff0c\u5f71\u54cd\u591a\u6a21\u6001\u5bf9\u9f50\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faModality-Specific Position Encoding\uff08MSPE\uff09\uff0c\u5bf9\u6587\u672c\u548c\u89c6\u89c9\u5206\u522b\u5728\u72ec\u7acb\u5750\u6807\u7ef4\u5ea6\u4e0a\u5206\u914d\u4f4d\u7f6e\u7d22\u5f15\uff0c\u4fdd\u6301\u6587\u672c\u7684\u5e8f\u5217\u8fde\u7eed\u6027\u548c\u89c6\u89c9\u7684\u7a7a\u95f4\u8fde\u8d2f\u6027\uff1b\u5f15\u5165Global Adaptive Encoding Step Scaling\uff08GAESS\uff09\uff0c\u57fa\u4e8e\u4e24\u79cd\u6a21\u6001\u7684\u5d4c\u5165\u71b5\u81ea\u9002\u5e94\u8c03\u6574\u89c6\u89c9\u4f4d\u7f6e\u7f16\u7801\u6b65\u957f\uff0c\u4ee5\u5728\u4f4d\u7f6e\u7d22\u5f15\u7a7a\u95f4\u5bf9\u9f50\u4fe1\u606f\u5bc6\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548cVQA\u57fa\u51c6\u4e0a\u5747\u5e26\u6765\u4e00\u81f4\u63d0\u5347\u3002\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\uff0cQwen2.5-VL-3B\u8f83\u57fa\u7ebf\u4f4d\u7f6e\u7f16\u7801\u7b56\u7565\u6700\u9ad8\u63d0\u53473.43%\uff0c\u5728\u66f4\u5927\u6a21\u578b\uff08Qwen2.5-VL-7B\u3001LLaVA-v1.5-7B\uff09\u4e0a\u4e5f\u6709\u7a33\u5b9a\u589e\u76ca\u3002", "conclusion": "OMEGA\u901a\u8fc7\u4e3a\u4e0d\u540c\u6a21\u6001\u8bbe\u8ba1\u72ec\u7acb\u7684\u4f4d\u7f6e\u7f16\u7801\u7ef4\u5ea6\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u6b65\u957f\u7f29\u653e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLM\u7edf\u4e00\u4f4d\u7f6e\u7d22\u5f15\u5ffd\u89c6\u6a21\u6001\u5185\u5728\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u4e86VLM\u5728\u591a\u6a21\u6001\u4efb\u52a1\u7279\u522b\u662f\u89c6\u89c9\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2511.00831", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00831", "abs": "https://arxiv.org/abs/2511.00831", "authors": ["Xin Liu", "Aoyang Zhou", "Aoyang Zhou"], "title": "Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack", "comment": "Accepted by NAACL2025 findings", "summary": "Visual-Language Pre-training (VLP) models have achieved significant\nperformance across various downstream tasks. However, they remain vulnerable to\nadversarial examples. While prior efforts focus on improving the adversarial\ntransferability of multimodal adversarial examples through cross-modal\ninteractions, these approaches suffer from overfitting issues, due to a lack of\ninput diversity by relying excessively on information from adversarial examples\nin one modality when crafting attacks in another. To address this issue, we\ndraw inspiration from strategies in some adversarial training methods and\npropose a novel attack called Local Shuffle and Sample-based Attack (LSSA).\nLSSA randomly shuffles one of the local image blocks, thus expanding the\noriginal image-text pairs, generating adversarial images, and sampling around\nthem. Then, it utilizes both the original and sampled images to generate the\nadversarial texts. Extensive experiments on multiple models and datasets\ndemonstrate that LSSA significantly enhances the transferability of multimodal\nadversarial examples across diverse VLP models and downstream tasks. Moreover,\nLSSA outperforms other advanced attacks on Large Vision-Language Models.", "AI": {"tldr": "\u63d0\u51faLSSA\uff1a\u901a\u8fc7\u5c40\u90e8\u5757\u6253\u4e71+\u5468\u8fb9\u91c7\u6837\u6269\u5145\u56fe\u6587\u5bf9\u591a\u6837\u6027\uff0c\u751f\u6210\u66f4\u5177\u8fc1\u79fb\u6027\u7684\u591a\u6a21\u6001\u5bf9\u6297\u6837\u672c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u56e0\u5728\u4e00\u79cd\u6a21\u6001\u4e0a\u8fc7\u5ea6\u4f9d\u8d56\u5bf9\u6297\u6837\u672c\u5bfc\u81f4\u8f93\u5165\u591a\u6837\u6027\u4e0d\u8db3\u5e76\u4ea7\u751f\u8fc7\u62df\u5408\uff0c\u8fdb\u800c\u9650\u5236\u4e86\u653b\u51fb\u5728\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u7684\u8fc1\u79fb\u6027\u3002\u4f5c\u8005\u501f\u9274\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u591a\u6837\u5316\u7b56\u7565\u4ee5\u63d0\u9ad8\u653b\u51fb\u6cdb\u5316\u6027\u3002", "method": "LSSA\u5148\u5bf9\u56fe\u50cf\u8fdb\u884c\u5c40\u90e8\u5757\u968f\u673a\u6253\u4e71\u4ee5\u6269\u589e\u56fe\u50cf-\u6587\u672c\u914d\u5bf9\uff0c\u751f\u6210\u521d\u59cb\u5bf9\u6297\u56fe\u50cf\u5e76\u5728\u5176\u5468\u56f4\u8fdb\u884c\u91c7\u6837\uff0c\u968f\u540e\u5229\u7528\u539f\u59cb\u53ca\u91c7\u6837\u5f97\u5230\u7684\u56fe\u50cf\u5171\u540c\u751f\u6210\u5bf9\u6297\u6587\u672c\uff0c\u5f3a\u8c03\u5728\u56fe\u50cf\u4e0e\u6587\u672c\u95f4\u7684\u591a\u6837\u5316\u4fe1\u606f\u4ea4\u4e92\u4ee5\u63d0\u9ad8\u8fc1\u79fb\u80fd\u529b\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eLSSA\u5728\u591a\u4e2aVLP\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u4e0a\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6210\u529f\u7387\uff0c\u5e76\u5728\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8d85\u8fc7\u5176\u4ed6\u5148\u8fdb\u653b\u51fb\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684LSSA\u901a\u8fc7\u5c40\u90e8\u5757\u968f\u673a\u6253\u4e71\u53ca\u6837\u672c\u91c7\u6837\u6269\u5145\u4e86\u56fe\u6587\u5bf9\u7684\u591a\u6837\u6027\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u8de8\u6a21\u6001\u5bf9\u6297\u653b\u51fb\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u6297\u6837\u672c\u7684\u8fc1\u79fb\u6027\uff0c\u4e14\u5728\u591a\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u9ad8\u7ea7\u653b\u51fb\u65b9\u6cd5\u3002"}}
{"id": "2511.00833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00833", "abs": "https://arxiv.org/abs/2511.00833", "authors": ["Yifan Pu", "Jixuan Ying", "Qixiu Li", "Tianzhu Ye", "Dongchen Han", "Xiaochen Wang", "Ziyi Wang", "Xinyu Shao", "Gao Huang", "Xiu Li"], "title": "Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials", "comment": "NeurIPS 2025", "summary": "Vision Transformers (ViTs) have become a universal backbone for both image\nrecognition and image generation. Yet their Multi-Head Self-Attention (MHSA)\nlayer still performs a quadratic query-key interaction for every token pair,\nspending the bulk of computation on visually weak or redundant correlations. We\nintroduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that\ninjects an explicit notion of discrimination while reducing the theoretical\ncomplexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's\ndense query field into a handful of spatially pooled visual-contrast tokens,\nthen splits them into a learnable positive and negative stream whose\ndifferential interaction highlights what truly separates one region from\nanother. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,\nrequires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA\nlifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and\nimproves three strong hierarchical ViTs by up to 3.1%, while in\nclass-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points\nacross both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm\nthat (i) spatial pooling supplies low-variance global cues, (ii) dual\npositional embeddings are indispensable for contrastive reasoning, and (iii)\ncombining the two in both stages yields the strongest synergy. VCA therefore\noffers a simple path towards faster and sharper Vision Transformers. The source\ncode is available at https://github.com/LeapLabTHU/LinearDiff.", "AI": {"tldr": "\u63d0\u51faVisual-Contrast Attention\uff1a\u7528\u7a7a\u95f4\u6c60\u5316\u751f\u6210\u5c11\u91cf\u5bf9\u6bd4tokens\u5e76\u5206\u6210\u6b63\u8d1f\u6d41\u8fdb\u884c\u5dee\u5206\u4ea4\u4e92\uff0c\u964d\u4f4e\u6ce8\u610f\u529b\u590d\u6742\u5ea6\u5e76\u589e\u5f3a\u5224\u522b\u529b\uff0c\u5e26\u6765\u5206\u7c7b\u4e0e\u751f\u6210\u4efb\u52a1\u7684\u663e\u8457\u6027\u80fd\u63d0\u5347\u4e14\u4ee3\u4ef7\u6781\u5c0f\u3002", "motivation": "\u4f20\u7edfMHSA\u5bf9\u6240\u6709token\u5bf9\u505a\u4e8c\u6b21\u4ea4\u4e92\uff0c\u6d6a\u8d39\u8ba1\u7b97\u5728\u4e0d\u91cd\u8981\u6216\u5197\u4f59\u7684\u89c6\u89c9\u76f8\u5173\u6027\u4e0a\uff1b\u5e0c\u671b\u901a\u8fc7\u663e\u5f0f\u5bf9\u6bd4\u548c\u7a00\u758f\u5316\u6ce8\u610f\u529b\u6765\u63d0\u9ad8\u6548\u7387\u4e0e\u5224\u522b\u529b\u3002", "method": "\u5728\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u4e2d\u5148\u7528\u7a7a\u95f4\u6c60\u5316\u5c06\u7a20\u5bc6\u67e5\u8be2\u573a\u84b8\u998f\u4e3a\u5c11\u91cf\u89c6\u89c9\u5bf9\u6bd4tokens\uff0c\u518d\u5c06tokens\u5206\u4e3a\u53ef\u5b66\u4e60\u7684\u6b63\u8d1f\u6d41\uff0c\u5229\u7528\u4e24\u8005\u7684\u5dee\u5206\u4ea4\u4e92\u7a81\u51fa\u533a\u57df\u95f4\u5224\u522b\u6027\uff1b\u8be5\u6a21\u5757\u4fdd\u6301\u53c2\u6570\u5c11\u3001FLOPs\u4e0d\u589e\u3001\u53ef\u66ff\u6362MHSA\u3002", "result": "\u5728DeiT-Tiny\u4e0aImageNet-1K top-1\u4ece72.2%\u63d0\u5347\u523075.6%\uff08+3.4\uff09\uff1b\u5bf9\u4e09\u79cd\u5206\u5c42ViT\u63d0\u5347\u6700\u591a3.1%\uff1b\u5728\u6761\u4ef6ImageNet\u751f\u6210\u4efb\u52a1\u4e0a\uff0cDiT\u4e0eSiT\u7684FID-50K\u5206\u522b\u964d\u4f4e2.1\u81f35.2\u70b9\uff1b\u6a21\u5757\u4ec5\u589e\u52a0<0.3M\u53c2\u6570\uff0c\u65e0\u989d\u5916FLOPs\u3002", "conclusion": "VCA\u901a\u8fc7\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u6b63\u8d1f\u5bf9\u6bd4\u6d41\u4e0e\u7a7a\u95f4\u6c60\u5316\u7684\u89c6\u89c9\u5bf9\u6bd4tokens\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u81ea\u6ce8\u610f\u529b\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u589e\u5f3a\u5224\u522b\u80fd\u529b\uff0c\u4ece\u800c\u5728\u5206\u7c7b\u4e0e\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u83b7\u5f97\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2511.00836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00836", "abs": "https://arxiv.org/abs/2511.00836", "authors": ["Xin Liu", "Yichen Yang", "Kun He", "John E. Hopcroft"], "title": "Parameter Interpolation Adversarial Training for Robust Image Classification", "comment": "Accepted by TIFS 2025", "summary": "Though deep neural networks exhibit superior performance on various tasks,\nthey are still plagued by adversarial examples. Adversarial training has been\ndemonstrated to be the most effective method to defend against adversarial\nattacks. However, existing adversarial training methods show that the model\nrobustness has apparent oscillations and overfitting issues in the training\nprocess, degrading the defense efficacy. To address these issues, we propose a\nnovel framework called Parameter Interpolation Adversarial Training (PIAT).\nPIAT tunes the model parameters between each epoch by interpolating the\nparameters of the previous and current epochs. It makes the decision boundary\nof model change more moderate and alleviates the overfitting issue, helping the\nmodel converge better and achieving higher model robustness. In addition, we\nsuggest using the Normalized Mean Square Error (NMSE) to further improve the\nrobustness by aligning the relative magnitude of logits between clean and\nadversarial examples rather than the absolute magnitude. Extensive experiments\nconducted on several benchmark datasets demonstrate that our framework could\nprominently improve the robustness of both Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs).", "AI": {"tldr": "PIAT\u901a\u8fc7\u8de8epoch\u53c2\u6570\u63d2\u503c\u548c\u5e73\u6ed1logits\u7684NMSE\u635f\u5931\uff0c\u7f13\u89e3\u5bf9\u6297\u8bad\u7ec3\u632f\u8361\u4e0e\u8fc7\u62df\u5408\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u5bf9\u6297\u8bad\u7ec3\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u632f\u8361\u548c\u8fc7\u62df\u5408\uff0c\u5bfc\u81f4\u9632\u5fa1\u6548\u679c\u4e0b\u964d\uff0c\u9700\u4e00\u79cd\u80fd\u5e73\u6ed1\u53c2\u6570\u66f4\u65b0\u5e76\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faParameter Interpolation Adversarial Training\uff08PIAT\uff09\uff0c\u5728\u6bcf\u4e2aepoch\u7ed3\u675f\u65f6\u5c06\u5f53\u524d\u53c2\u6570\u4e0e\u524d\u4e00epoch\u53c2\u6570\u6309\u6bd4\u4f8b\u7ebf\u6027\u63d2\u503c\u66f4\u65b0\uff1b\u5e76\u5f15\u5165Normalized Mean Square Error\uff08NMSE\uff09\u635f\u5931\uff0c\u7528\u4e8e\u5bf9\u9f50\u5e72\u51c0\u6837\u672c\u4e0e\u5bf9\u6297\u6837\u672clogits\u7684\u76f8\u5bf9\u5e45\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cPIAT\u663e\u8457\u63d0\u5347\u4e86CNN\u4e0eViT\u7684\u9c81\u68d2\u6027\uff0c\u663e\u793a\u51fa\u6bd4\u4f20\u7edf\u5bf9\u6297\u8bad\u7ec3\u66f4\u597d\u7684\u6536\u655b\u6027\u4e0e\u9632\u5fa1\u6548\u679c\u3002", "conclusion": "PIAT\u901a\u8fc7\u5728\u8bad\u7ec3\u8f6e\u6b21\u95f4\u5bf9\u6a21\u578b\u53c2\u6570\u8fdb\u884c\u7ebf\u6027\u63d2\u503c\uff0c\u4f7f\u51b3\u7b56\u8fb9\u754c\u53d8\u5316\u66f4\u5e73\u7f13\uff0c\u4ece\u800c\u7f13\u89e3\u5bf9\u6297\u8bad\u7ec3\u4e2d\u7684\u632f\u8361\u548c\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.00846", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00846", "abs": "https://arxiv.org/abs/2511.00846", "authors": ["Zhihao Peng", "Cheng Wang", "Shengyuan Liu", "Zhiying Liang", "Yixuan Yuan"], "title": "OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks", "comment": null, "summary": "Brain imaging analysis is vital for diagnosing and treating brain disorders,\nand multimodal large language models (MLLMs) are increasingly assisting in that\nanalysis. However, current brain-oriented visual question-answering (VQA)\nbenchmarks either cover a few imaging modalities or are limited to\ncoarse-grained pathological descriptions, hindering a comprehensive assessment\nof MLLMs throughout the full clinical continuum. To address these, we introduce\nOmniBrainBench, the first comprehensive multimodal VQA benchmark specifically\ndesigned to assess the multimodal comprehension capabilities of MLLMs in brain\nimaging analysis.OmniBrainBench consists of 15 distinct brain imaging\nmodalities collected from 30 verified medical sources, yielding 9,527 validated\nVQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15\nmulti-stage clinical tasks rigorously validated by a professional radiologist.\nEvaluation of 24 state-of-the-art models, including open-source, medical, and\nproprietary MLLMs, highlights the substantial challenges posed by\nOmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)\nbeat open-source and medical models but lag physicians; (2) medical MLLMs vary\nwidely in performance; (3) open-source MLLMs trail overall but excel in\nspecific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,\nrevealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new\nstandard for evaluating and advancing MLLMs in brain imaging analysis,\nhighlighting gaps compared to expert clinical reasoning. We release it at\nbenchmark \\& code.", "AI": {"tldr": "OmniBrainBench\u662f\u9996\u4e2a\u5168\u9762\u7684\u8111\u5f71\u50cf\u591a\u6a21\u6001VQA\u57fa\u51c6\uff0c\u8986\u76d6\u591a\u6a21\u6001\u4e0e\u4e34\u5e8a\u4efb\u52a1\uff0c\u8bc4\u6d4b24\u4e2a\u6a21\u578b\u540e\u53d1\u73b0\u73b0\u6709MLLM\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4e0a\u4ecd\u6709\u663e\u8457\u5dee\u8ddd\uff0c\u63d0\u793a\u672a\u6765\u9700\u5173\u6ce8\u89c6\u89c9-\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8111\u5f71\u50cfVQA\u57fa\u51c6\u8981\u4e48\u6a21\u6001\u5355\u4e00\uff0c\u8981\u4e48\u53ea\u5173\u6ce8\u7c97\u7c92\u5ea6\u75c5\u7406\u63cf\u8ff0\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30MLLM\u5728\u5b9e\u9645\u4e34\u5e8a\u6d41\u7a0b\u4e2d\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u6545\u9700\u6784\u5efa\u66f4\u5168\u9762\u3001\u4e34\u5e8a\u76f8\u5173\u7684\u57fa\u51c6\u3002", "method": "\u6536\u96c6\u81ea30\u4e2a\u7ecf\u9a8c\u8bc1\u7684\u533b\u5b66\u6765\u6e90\u768415\u79cd\u8111\u5f71\u50cf\u6a21\u6001\u6570\u636e\uff0c\u6784\u5efa9527\u5bf9\u7ecf\u9a8c\u8bc1\u7684VQA\u95ee\u7b54\u5bf9\u548c31706\u5f20\u56fe\u50cf\uff0c\u6a21\u62df\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u5e76\u8bbe\u8ba115\u4e2a\u591a\u9636\u6bb5\u4e34\u5e8a\u4efb\u52a1\uff0c\u7531\u4e13\u4e1a\u653e\u5c04\u79d1\u533b\u5e08\u4e25\u683c\u9a8c\u8bc1\uff1b\u968f\u540e\u572824\u4e2aSOTA\u6a21\u578b\uff08\u5f00\u653e\u6e90\u4ee3\u7801\u3001\u533b\u7597\u5b9a\u5236\u548c\u4e13\u6709\u6a21\u578b\uff09\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9a\u91cf\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86OmniBrainBench\u5e76\u516c\u5f00\u53d1\u5e03\uff1b\u5b9e\u9a8c\u53d1\u73b0\uff1a\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-5\uff09\u4f18\u4e8e\u5f00\u6e90\u548c\u533b\u7597\u6a21\u578b\u4f46\u4ecd\u843d\u540e\u533b\u751f\uff1b\u533b\u7597\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u5927\uff1b\u5f00\u6e90\u6a21\u578b\u603b\u4f53\u843d\u540e\u4f46\u5728\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\u7a81\u51fa\uff1b\u6240\u6709\u6a21\u578b\u5728\u590d\u6742\u672f\u524d\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u66b4\u9732\u89c6\u89c9\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOmniBrainBench\u7684\u591a\u6a21\u6001\u8111\u5f71\u50cfVQA\u57fa\u51c6\uff0c\u8986\u76d615\u79cd\u5f71\u50cf\u6a21\u6001\u548c15\u4e2a\u4e34\u5e8a\u4efb\u52a1\uff0c\u65e8\u5728\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8111\u5f71\u50cf\u5206\u6790\u4e2d\u7684\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u76ee\u524d\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4e0a\u4ecd\u843d\u540e\u4e8e\u533b\u751f\u3002"}}
{"id": "2511.00858", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.00858", "abs": "https://arxiv.org/abs/2511.00858", "authors": ["Yu Liu", "Zhijie Liu", "Zedong Yang", "You-Fu Li", "He Kong"], "title": "Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction", "comment": "This manuscript has been accepted to the IEEE Transactions on\n  Intelligent Transportation Systems as a regular paper", "summary": "Predicting pedestrian crossing intentions is crucial for the navigation of\nmobile robots and intelligent vehicles. Although recent deep learning-based\nmodels have shown significant success in forecasting intentions, few consider\nincomplete observation under occlusion scenarios. To tackle this challenge, we\npropose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded\nmotion patterns and leverages them to guide future intention prediction. During\nthe denoising stage, we introduce an occlusion-aware diffusion transformer\narchitecture to estimate noise features associated with occluded patterns,\nthereby enhancing the model's ability to capture contextual relationships in\noccluded semantic scenarios. Furthermore, an occlusion mask-guided reverse\nprocess is introduced to effectively utilize observation information, reducing\nthe accumulation of prediction errors and enhancing the accuracy of\nreconstructed motion features. The performance of the proposed method under\nvarious occlusion scenarios is comprehensively evaluated and compared with\nexisting methods on popular benchmarks, namely PIE and JAAD. Extensive\nexperimental results demonstrate that the proposed method achieves more robust\nperformance than existing methods in the literature.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u906e\u6321\u611f\u77e5\u7684\u6269\u6563\u6a21\u578b\uff08ODM\uff09\uff0c\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u6321\u7684\u8fd0\u52a8\u6a21\u5f0f\u5e76\u7ed3\u5408\u906e\u6321\u63a9\u7801\u5f15\u5bfc\u7684\u53cd\u5411\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u5728\u906e\u6321\u573a\u666f\u4e0b\u7684\u884c\u4eba\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u9c81\u68d2\u6027\uff0c\u4e14\u5728PIE\u548cJAAD\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u52a8\u673a\u662f\u4f20\u7edf\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u65b9\u6cd5\u5728\u906e\u6321\u6216\u89c2\u6d4b\u4e0d\u5b8c\u6574\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u5f88\u5c11\u4e13\u95e8\u5904\u7406\u906e\u6321\u5e26\u6765\u7684\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u91cd\u5efa\u906e\u6321\u8fd0\u52a8\u6a21\u5f0f\u5e76\u7528\u5176\u6539\u8fdb\u610f\u56fe\u9884\u6d4b\u7684\u6a21\u578b\u3002\n", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1\uff09\u5728\u6269\u6563\u6a21\u578b\u7684\u53bb\u566a\u9636\u6bb5\u5f15\u5165\u4e00\u79cd\u906e\u6321\u611f\u77e5\u6269\u6563Transformer\uff0c\u7528\u4ee5\u4f30\u7b97\u4e0e\u88ab\u906e\u6321\u6a21\u5f0f\u76f8\u5173\u7684\u566a\u58f0\u7279\u5f81\uff0c\u589e\u5f3a\u6a21\u578b\u5728\u906e\u6321\u8bed\u4e49\u573a\u666f\u4e0b\u5bf9\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u6355\u6349\uff1b2\uff09\u8bbe\u8ba1\u4e00\u4e2a\u906e\u6321\u63a9\u7801\u5f15\u5bfc\u7684\u53cd\u5411\u8fc7\u7a0b\uff08reverse process\uff09\uff0c\u5728\u91cd\u6784\u65f6\u6709\u6548\u5229\u7528\u53ef\u89c1\u89c2\u6d4b\u4fe1\u606f\uff0c\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\uff0c\u4ece\u800c\u63d0\u9ad8\u88ab\u906e\u6321\u8fd0\u52a8\u7279\u5f81\u7684\u91cd\u5efa\u7cbe\u5ea6\u3002\n", "result": "\u5728PIE\u548cJAAD\u6570\u636e\u96c6\u4e0a\u7684\u591a\u79cd\u906e\u6321\u573a\u666f\u4e0b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eODM\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u9884\u6d4b\u6027\u80fd\u66f4\u7a33\u5065\uff0c\u91cd\u5efa\u548c\u610f\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u5747\u6709\u63d0\u5347\u3002\n", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u906e\u6321\u60c5\u5f62\u4e0b\u884c\u4eba\u8fc7\u9a6c\u8def\u610f\u56fe\u9884\u6d4b\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\u2014\u2014\u906e\u6321\u611f\u77e5\u6269\u6563\u6a21\u578b\uff08ODM\uff09\uff0c\u901a\u8fc7\u91cd\u5efa\u88ab\u906e\u6321\u7684\u8fd0\u52a8\u6a21\u5f0f\u5e76\u5229\u7528\u5176\u5f15\u5bfc\u672a\u6765\u610f\u56fe\u9884\u6d4b\uff0c\u4ece\u800c\u63d0\u5347\u5728\u4e0d\u5b8c\u6574\u89c2\u6d4b\u4e0b\u7684\u9884\u6d4b\u9c81\u68d2\u6027\u3002\n"}}
{"id": "2511.00859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00859", "abs": "https://arxiv.org/abs/2511.00859", "authors": ["Jaehyun Park", "Konyul Park", "Daehun Kim", "Junseo Park", "Jun Won Choi"], "title": "Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion", "comment": "Accepted to NeurIPS 2025", "summary": "In autonomous driving, transparency in the decision-making of perception\nmodels is critical, as even a single misperception can be catastrophic. Yet\nwith multi-sensor inputs, it is difficult to determine how each modality\ncontributes to a prediction because sensor information becomes entangled within\nthe fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a\npost-hoc, model-agnostic interpretability method that disentangles\nmodality-specific information across all layers of a pretrained fusion model.\nTo our knowledge, LMD is the first approach to attribute the predictions of a\nperception model to individual input modalities in a sensor-fusion system for\nautonomous driving. We evaluate LMD on pretrained fusion models under\ncamera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous\ndriving. Its effectiveness is validated using structured perturbation-based\nmetrics and modality-wise visual decompositions, demonstrating practical\napplicability to interpreting high-capacity multimodal architectures. Code is\navailable at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.", "AI": {"tldr": "LMD\u4e3a\u540e\u9a8c\u3001\u6a21\u578b\u65e0\u5173\u7684\u9010\u5c42\u6a21\u6001\u5206\u89e3\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u5c06\u591a\u4f20\u611f\u5668\u878d\u5408\u611f\u77e5\u6a21\u578b\u7684\u9884\u6d4b\u5f52\u56e0\u5230\u5355\u72ec\u6a21\u6001\uff0c\u5e76\u5728\u76f8\u673a/\u96f7\u8fbe/LiDAR\u878d\u5408\u573a\u666f\u4e0a\u901a\u8fc7\u8bc4\u4f30\u6307\u6807\u4e0e\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u591a\u4f20\u611f\u5668\u878d\u5408\u80fd\u63d0\u9ad8\u611f\u77e5\u9c81\u68d2\u6027\uff0c\u4f46\u4f20\u611f\u5668\u4fe1\u606f\u5728\u7f51\u7edc\u5185\u90e8\u9ad8\u5ea6\u8026\u5408\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5224\u5b9a\u5404\u6a21\u6001\u5bf9\u5355\u6b21\u9884\u6d4b\u7684\u5177\u4f53\u8d21\u732e\uff1b\u800c\u5355\u6b21\u8bef\u5224\u53ef\u80fd\u81f4\u547d\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u9010\u5c42\u5206\u89e3\u6a21\u6001\u4fe1\u606f\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "method": "LMD\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u9010\u5c42\u63d0\u53d6\u548c\u5206\u79bb\u878d\u5408\u7f51\u7edc\u4e2d\u6a21\u6001\u7279\u5b9a\u7684\u4fe1\u606f\uff0c\u751f\u6210\u6a21\u6001\u7ea7\u7684\u8868\u793a\u4e0e\u53ef\u89c6\u5316\u5206\u89e3\uff0c\u8fdb\u800c\u91cf\u5316\u5404\u6a21\u6001\u5bf9\u6700\u7ec8\u9884\u6d4b\u7684\u8d21\u732e\u3002\u65b9\u6cd5\u9762\u5411\u9884\u8bad\u7ec3\u7684\u878d\u5408\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u53cc\u6a21\u6001\u548c\u4e09\u6a21\u6001\uff08\u76f8\u673a-\u96f7\u8fbe\u3001\u76f8\u673a-LiDAR\u3001\u76f8\u673a-\u96f7\u8fbe-LiDAR\uff09\u8bbe\u7f6e\u3002", "result": "\u5728\u591a\u79cd\u9884\u8bad\u7ec3\u878d\u5408\u6a21\u578b\u548c\u4e0d\u540c\u4f20\u611f\u5668\u7ec4\u5408\u4e0a\u9a8c\u8bc1\uff0cLMD\u901a\u8fc7\u7ed3\u6784\u5316\u6270\u52a8\u8bc4\u4f30\u6307\u6807\u548c\u6a21\u6001\u53ef\u89c6\u5316\u5206\u89e3\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bc1\u660e\u53ef\u5e94\u7528\u4e8e\u9ad8\u5bb9\u91cf\u591a\u6a21\u6001\u67b6\u6784\u4ee5\u89e3\u91ca\u6a21\u578b\u51b3\u7b56\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684LMD\u80fd\u5bf9\u591a\u4f20\u611f\u5668\u878d\u5408\u611f\u77e5\u6a21\u578b\u8fdb\u884c\u9010\u5c42\u6a21\u6001\u5206\u89e3\uff0c\u5b9e\u73b0\u5bf9\u6bcf\u4e2a\u8f93\u5165\u6a21\u6001\u5bf9\u9884\u6d4b\u8d21\u732e\u7684\u540e\u9a8c\u53ef\u89e3\u91ca\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u900f\u660e\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2511.00908", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.00908", "abs": "https://arxiv.org/abs/2511.00908", "authors": ["Heng Zheng", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks", "comment": null, "summary": "Visual geo-localization requires extensive geographic knowledge and\nsophisticated reasoning to determine image locations without GPS metadata.\nTraditional retrieval methods are constrained by database coverage and quality.\nRecent Large Vision-Language Models (LVLMs) enable direct location reasoning\nfrom image content, yet individual models struggle with diverse geographic\nregions and complex scenes. Existing multi-agent systems improve performance\nthrough model collaboration but treat all agent interactions uniformly. They\nlack mechanisms to handle conflicting predictions effectively. We propose\n\\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph\nneural networks for visual geo-localization. Our approach models diverse debate\nrelationships through typed edges, distinguishing supportive collaboration,\ncompetitive argumentation, and knowledge transfer. We introduce a dual-level\ndebate mechanism combining node-level refinement and edge-level argumentation\nmodeling. A cross-level topology refinement strategy enables co-evolution\nbetween graph structure and agent representations. Experiments on multiple\nbenchmarks demonstrate GraphGeo significantly outperforms state-of-the-art\nmethods. Our framework transforms cognitive conflicts between agents into\nenhanced geo-localization accuracy through structured debate.", "AI": {"tldr": "GraphGeo \u7528\u5f02\u8d28\u56fe+\u53cc\u5c42\u4e89\u8bba\u628a\u591a\u6a21\u578b\u51b2\u7a81\u8f6c\u4e3a\u6709\u76ca\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u5730\u7406\u5b9a\u4f4d\u3002", "motivation": "\u5355\u4e00 LVLM \u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u548c\u590d\u6742\u573a\u666f\u4e0b\u8868\u73b0\u6b20\u4f73\uff0c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5904\u7406\u6a21\u578b\u95f4\u51b2\u7a81\uff0c\u56e0\u6b64\u901a\u8fc7\u7ed3\u6784\u5316\u4e89\u8bba\u63d0\u5347\u534f\u4f5c\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u5e26\u7c7b\u578b\u8fb9\u7684\u5f02\u8d28\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408\u8282\u70b9\u7ea7\u7cbe\u70bc\u4e0e\u8fb9\u7ea7\u8bba\u8bc1\u7684\u53cc\u5c42\u4e89\u8bba\u673a\u5236\uff0c\u4ee5\u53ca\u8de8\u5c42\u62d3\u6251\u7cbe\u70bc\u7b56\u7565\u4ee5\u5b9e\u73b0\u56fe\u7ed3\u6784\u4e0e\u8868\u5f81\u7684\u534f\u540c\u6f14\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cGraphGeo \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7ed3\u6784\u5316\u4e89\u8bba\u8f6c\u6362\u8ba4\u77e5\u51b2\u7a81\u53ef\u63d0\u5347\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\u3002", "conclusion": "GraphGeo \u5c06\u591a\u6a21\u578b\u4e89\u8bba\u7ed3\u6784\u5316\u4e3a\u5e26\u7c7b\u578b\u8fb9\u7684\u5f02\u8d28\u56fe\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u5730\u7406\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.00916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00916", "abs": "https://arxiv.org/abs/2511.00916", "authors": ["Yan Shu", "Chi Liu", "Robin Chen", "Derek Li", "Bryan Dai"], "title": "Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\neffectiveness in various general-domain scenarios, such as visual question\nanswering and image captioning. Recently, researchers have increasingly focused\non empowering MLLMs with medical conversational abilities, which hold\nsignificant promise for clinical applications. However, medical data presents\nunique challenges due to its heterogeneous nature -- encompassing diverse\nmodalities including 2D images, 3D volumetric scans, and temporal video\nsequences. The substantial domain gap and data format inconsistencies across\nthese modalities have hindered the development of unified medical MLLMs. To\naddress these challenges, we propose Fleming-VL, a unified end-to-end framework\nfor comprehensive medical visual understanding across heterogeneous modalities.\nFleming-VL tackles this problem from a data-centric perspective through three\nkey strategies: (1) scaling up pretraining by integrating long-context data\nfrom both natural and medical-specific domains; (2) complementing fine-tuning\nwith rare medical data, including holistic video analysis and underrepresented\n2D modalities such as ultrasound and dermoscopy images; (3) extending existing\nevaluation frameworks to incorporate 3D volumetric and video understanding\nbenchmarks. Through supervised fine-tuning (SFT) and group relative policy\noptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive\nexperiments demonstrate that Fleming-VL achieves state-of-the-art performance\nacross multiple benchmarks, including medical VQA, video QA, and 3D medical\nimage understanding. We publicly release Fleming-VL to promote transparent,\nreproducible, and auditable progress in medical AI.", "AI": {"tldr": "\u63d0\u51faFleming-VL\uff1a\u901a\u8fc7\u6269\u5c55\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u3001\u8865\u5145\u7a00\u6709\u533b\u5b66\u6a21\u6001\u5fae\u8c03\u5e76\u6269\u5c55\u8bc4\u6d4b\uff0c\u7ed3\u5408SFT\u4e0eGRPO\u8bad\u7ec3\uff0c\u6784\u5efa\u652f\u63012D/3D/\u89c6\u9891\u7684\u7edf\u4e00\u533b\u5b66\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u5728\u591a\u9879\u533b\u5b66\u89c6\u89c9\u4efb\u52a1\u4e0a\u8fbeSOTA\u5e76\u516c\u5f00\u53d1\u5e03\u3002", "motivation": "\u533b\u5b66\u591a\u6a21\u6001\u6570\u636e\u5f02\u6784\uff082D\u56fe\u50cf\u30013D\u4f53\u79ef\u3001\u65f6\u5e8f\u89c6\u9891\uff09\u5bfc\u81f4\u9886\u57df\u5dee\u8ddd\u5927\u4e14\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u7edf\u4e00\u533b\u5b66MLLM\u7684\u6784\u5efa\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u517c\u987e\u591a\u6a21\u6001\u3001\u6269\u5c55\u6027\u548c\u53ef\u8bc4\u4f30\u6027\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86Fleming-VL\u7edf\u4e00\u7aef\u5230\u7aef\u6846\u67b6\uff1b\u6570\u636e\u5c42\u9762\u91c7\u7528\u4e09\u5927\u7b56\u7565\uff1a\u6269\u5927\u9884\u8bad\u7ec3\u8bed\u6599\uff08\u81ea\u7136+\u533b\u5b66\u957f\u4e0a\u4e0b\u6587\u6570\u636e\uff09\u3001\u5728\u5fae\u8c03\u9636\u6bb5\u52a0\u5165\u7a00\u6709\u533b\u5b66\u6a21\u6001\uff08\u8d85\u58f0\u3001\u76ae\u80a4\u955c\u3001\u89c6\u9891\uff09\u4ee5\u53ca\u6269\u5c55\u8bc4\u6d4b\u52303D\u548c\u89c6\u9891\u57fa\u51c6\uff1b\u8bad\u7ec3\u4f7f\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff1b\u5e76\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\u9a8c\u8bc1\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6210\u7ee9\uff0c\u5305\u62ec\u533b\u5b66VQA\u3001\u89c6\u9891\u95ee\u7b54\u548c3D\u533b\u5b66\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1b\u5e76\u516c\u5f00\u53d1\u5e03\u6a21\u578b\u4ee5\u652f\u6301\u900f\u660e\u4e0e\u53ef\u91cd\u590d\u7814\u7a76\u3002", "conclusion": "Fleming-VL\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u9762\u5411\u5f02\u6784\u533b\u5b66\u6a21\u6001\uff082D\u30013D\u3001\u89c6\u9891\uff09\u7684\u7edf\u4e00\u591a\u6a21\u6001\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u7b56\u7565\uff08\u6269\u5c55\u9884\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u3001\u8865\u5145\u7a00\u6709\u533b\u5b66\u6a21\u6001\u6570\u636e\u3001\u6269\u5c55\u8bc4\u6d4b\u4efb\u52a1\uff09\u5e76\u7ed3\u5408SFT\u4e0eGRPO\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u591a\u9879\u533b\u5b66\u89c6\u89c9\u7406\u89e3\u4efb\u52a1\u7684SOTA\u8868\u73b0\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4ee5\u63a8\u52a8\u53ef\u91cd\u590d\u4e0e\u53ef\u5ba1\u8ba1\u7684\u7814\u7a76\u3002"}}
{"id": "2511.00925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00925", "abs": "https://arxiv.org/abs/2511.00925", "authors": ["Hanwen Su", "Ge Song", "Jiyan Wang", "Yuanbo Zhu"], "title": "Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval", "comment": null, "summary": "The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved\nincreasing attention due to its wide applications, e.g. e-commerce. Despite\nprogress made in this field, previous works suffer from using imbalanced\nsamples of modalities and inconsistent low-quality information during training,\nresulting in sub-optimal performance. Therefore, in this paper, we introduce an\napproach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It\nconsists of three components: (i) a Uni-modal Feature Extraction Module that\nincludes a CLIP text encoder and a ViT for extracting textual and visual\ntokens, (ii) a Cross-modal Multi-level Weighting Module that produces an\nalignment weight list by the local and global aggregation blocks to measure the\naligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss\nModule aiming to improve the balance of domains in the triplet loss.\nExperiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and\nQuickDraw, show our method delivers superior performances over the\nstate-of-the-art ZS-SBIR methods.", "AI": {"tldr": "\u63d0\u51faDMWA-Net\uff1a\u7528CLIP+ViT\u63d0\u53d6\u7279\u5f81\uff0c\u57fa\u4e8e\u5c40\u90e8\u4e0e\u5168\u5c40\u805a\u5408\u751f\u6210\u6837\u672c\u5bf9\u9f50\u6743\u91cd\uff0c\u5e76\u5c06\u6743\u91cd\u878d\u5165\u52a0\u6743\u56db\u5143\u7ec4\u635f\u5931\uff0c\u4ee5\u7f13\u89e3\u6a21\u6001\u4e0d\u5e73\u8861\u4e0e\u4f4e\u8d28\u5e72\u6270\uff1b\u5728\u4e09\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u9886\u5148\u3002", "motivation": "\u4f5c\u8005\u6307\u51fa\u73b0\u6709ZS-SBIR\u65b9\u6cd5\u5728\u8bad\u7ec3\u9636\u6bb5\u5b58\u5728\u6a21\u6001\u6837\u672c\u4e0d\u5e73\u8861\u4e0e\u4e0d\u4e00\u81f4\u7684\u4f4e\u8d28\u91cf\u4fe1\u606f\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\uff0c\u63d0\u51fa\u901a\u8fc7\u52a8\u6001\u591a\u5c42\u52a0\u6743\u6765\u8d4b\u4e88\u53ef\u9760\u6837\u672c\u66f4\u9ad8\u6743\u91cd\u4ee5\u63d0\u5347\u5bf9\u9f50\u4e0e\u68c0\u7d22\u6548\u679c\u3002", "method": "\u65b9\u6cd5\u5305\u542b\u4e09\u90e8\u5206\uff1a1\uff09\u5355\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u6a21\u5757\uff1a\u4f7f\u7528CLIP\u6587\u672c\u7f16\u7801\u5668\u4e0eViT\u63d0\u53d6\u6587\u672c\u4e0e\u89c6\u89c9tokens\uff1b2\uff09\u8de8\u6a21\u6001\u591a\u5c42\u52a0\u6743\u6a21\u5757\uff1a\u901a\u8fc7\u5c40\u90e8\u4e0e\u5168\u5c40\u805a\u5408\u5757\u751f\u6210\u5bf9\u9f50\u6743\u91cd\u5217\u8868\uff0c\u7528\u4e8e\u8861\u91cf\u7d20\u63cf\u4e0e\u56fe\u50cf\u6837\u672c\u7684\u5bf9\u9f50\u8d28\u91cf\uff1b3\uff09\u52a0\u6743\u56db\u5143\u7ec4\u635f\u5931\u6a21\u5757\uff1a\u5728\u4e09\u5143\u7ec4/\u56db\u5143\u7ec4\u635f\u5931\u4e2d\u5f15\u5165\u6837\u672c\u5bf9\u9f50\u6743\u91cd\u4ee5\u6539\u5584\u57df\u5e73\u8861\u3002", "result": "\u5728Sketchy\u3001TU-Berlin\u3001QuickDraw\u4e09\u4e2a\u57fa\u51c6\u4e0a\uff0c\u4f5c\u8005\u62a5\u544a\u7684\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u5177\u4f53\u63d0\u5347\u5e45\u5ea6\u3001\u8bc4\u4ef7\u6307\u6807\uff08\u5982P@K\u3001mAP\uff09\u4e0e\u7edf\u8ba1\u663e\u8457\u6027\u9700\u5728\u6b63\u6587\u6838\u5b9e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u52a8\u6001\u591a\u5c42\u52a0\u6743\u5bf9\u9f50\u7f51\u7edc\uff08DMWA-Net\uff09\u9488\u5bf9ZS-SBIR\u4efb\u52a1\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u52a0\u6743\u6765\u7f13\u89e3\u8de8\u6a21\u6001\u6837\u672c\u4e0d\u5e73\u8861\u548c\u4e0d\u4e00\u81f4\u4f4e\u8d28\u91cf\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5b9e\u9a8c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7ed3\u8bba\u5408\u7406\u4f46\u9700\u66f4\u591a\u6d88\u878d\u4e0e\u7ec6\u8282\u9a8c\u8bc1\u3002"}}
{"id": "2511.00956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00956", "abs": "https://arxiv.org/abs/2511.00956", "authors": ["Liuzhuozheng Li", "Yue Gong", "Shanyuan Liu", "Bo Cheng", "Yuhang Ma", "Liebucha Wu", "Dengyang Jiang", "Zanyi Wang", "Dawei Leng", "Yuhui Yin"], "title": "EVTAR: End-to-End Try on with Additional Unpaired Visual Reference", "comment": null, "summary": "We propose EVTAR, an End-to-End Virtual Try-on model with Additional\nReference, that directly fits the target garment onto the person image while\nincorporating reference images to enhance try-on accuracy. Most existing\nvirtual try-on approaches rely on complex inputs such as agnostic person\nimages, human pose, densepose, or body keypoints, making them labor-intensive\nand impractical for real-world applications. In contrast, EVTAR adopts a\ntwo-stage training strategy, enabling simple inference with only the source\nimage and the target garment inputs. Our model generates try-on results without\nmasks, densepose, or segmentation maps. Moreover, EVTAR leverages additional\nreference images of different individuals wearing the same clothes to preserve\ngarment texture and fine-grained details better. This mechanism is analogous to\nhow humans consider reference models when choosing outfits, thereby simulating\na more realistic and high-quality dressing effect. We enrich the training data\nwith supplementary references and unpaired person images to support these\ncapabilities. We evaluate EVTAR on two widely used benchmarks and diverse\ntasks, and the results consistently validate the effectiveness of our approach.", "AI": {"tldr": "EVTAR\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\uff0c\u901a\u8fc7\u989d\u5916\u53c2\u8003\u56fe\u50cf\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5728\u65e0\u9700\u590d\u6742\u4eba\u4f53\u5148\u9a8c\u8f93\u5165\u4e0b\u66f4\u51c6\u786e\u5730\u4fdd\u7559\u670d\u88c5\u7eb9\u7406\u4e0e\u7ec6\u8282\u3002", "motivation": "\u5f53\u524d\u8bd5\u7a7f\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u4eba\u4f53\u5148\u9a8c\uff08\u4f8b\u5982\u4eba\u4f53\u65e0\u5173\u56fe\u50cf\u3001\u59ff\u6001\u3001densepose\u6216\u5173\u952e\u70b9\uff09\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u4fbf\uff1b\u901a\u8fc7\u5f15\u5165\u989d\u5916\u53c2\u8003\u56fe\u50cf\u5e76\u7b80\u5316\u8f93\u5165\uff0c\u63d0\u5347\u53ef\u7528\u6027\u4e0e\u89c6\u89c9\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u6a21\u578bEVTAR\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u8bad\u7ec3\u9636\u6bb5\u878d\u5408\u989d\u5916\u53c2\u8003\u56fe\u50cf\u4e0e\u672a\u914d\u5bf9\u4eba\u7269\u56fe\u50cf\u4ee5\u589e\u5f3a\u7eb9\u7406\u548c\u7ec6\u8282\u4fdd\u7559\uff1b\u63a8\u7406\u65f6\u4ec5\u9700\u6e90\u4eba\u50cf\u4e0e\u76ee\u6807\u670d\u88c5\u56fe\u50cf\uff0c\u65e0\u9700mask\u3001densepose\u6216\u5173\u952e\u70b9\u3002", "result": "\u5728\u4e24\u4e2a\u5e38\u7528\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u79cd\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8bc1\u660eEVTAR\u5728\u7ec6\u8282\u4fdd\u7559\u548c\u6574\u4f53\u8bd5\u7a7f\u6548\u679c\u65b9\u9762\u5747\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "EVTAR\u901a\u8fc7\u5229\u7528\u989d\u5916\u53c2\u8003\u56fe\u50cf\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u590d\u6742\u4eba\u4f53\u5148\u9a8c\uff08\u5982densepose\u6216\u5206\u5272\u56fe\uff09\u7684\u7aef\u5230\u7aef\u865a\u62df\u8bd5\u7a7f\uff0c\u80fd\u66f4\u597d\u4fdd\u7559\u670d\u88c5\u7ec6\u8282\u5e76\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6548\u679c\u3002"}}
{"id": "2511.00962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00962", "abs": "https://arxiv.org/abs/2511.00962", "authors": ["Dongheng Lin", "Mengxue Qu", "Kunyang Han", "Jianbo Jiao", "Xiaojie Jin", "Yunchao Wei"], "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis", "comment": "NeurIPS 2025 poster", "summary": "Most video-anomaly research stops at frame-wise detection, offering little\ninsight into why an event is abnormal, typically outputting only frame-wise\nanomaly scores without spatial or semantic context. Recent video anomaly\nlocalization and video anomaly understanding methods improve explainability but\nremain data-dependent and task-specific. We propose a unified reasoning\nframework that bridges the gap between temporal detection, spatial\nlocalization, and textual explanation. Our approach is built upon a chained\ntest-time reasoning process that sequentially connects these tasks, enabling\nholistic zero-shot anomaly analysis without any additional training.\nSpecifically, our approach leverages intra-task reasoning to refine temporal\ndetections and inter-task chaining for spatial and semantic understanding,\nyielding improved interpretability and generalization in a fully zero-shot\nmanner. Without any additional data or gradients, our method achieves\nstate-of-the-art zero-shot performance across multiple video anomaly detection,\nlocalization, and explanation benchmarks. The results demonstrate that careful\nprompt design with task-wise chaining can unlock the reasoning power of\nfoundation models, enabling practical, interpretable video anomaly analysis in\na fully zero-shot manner. Project Page:\nhttps://rathgrith.github.io/Unified_Frame_VAA/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u94fe\u5f0f\u6d4b\u8bd5\u65f6\u63a8\u7406\u7684\u7edf\u4e00\u96f6\u6837\u672c\u89c6\u9891\u5f02\u5e38\u5206\u6790\u6846\u67b6\uff0c\u53ef\u5728\u4e0d\u9700\u8bad\u7ec3\u6216\u989d\u5916\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u65f6\u95f4\u68c0\u6d4b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u4e0e\u6587\u672c\u89e3\u91ca\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\u5f3a\u53ef\u89e3\u91ca\u6027\u4e0eSOTA\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5f02\u5e38\u7814\u7a76\u591a\u505c\u7559\u5728\u5e27\u7ea7\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u89e3\u91ca\u6027\uff08\u4e3a\u4f55\u5f02\u5e38\u3001\u5728\u54ea\u513f\u3001\u8bed\u4e49\u539f\u56e0\uff09\u3002\u5df2\u6709\u5b9a\u4f4d\u4e0e\u7406\u89e3\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\uff0c\u8981\u4e48\u4e3a\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u89e3\u91ca\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6846\u67b6\u6765\u6865\u63a5\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u89e3\u91ca\u4e09\u8005\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u673a\u5236\uff1a1) intra-task reasoning\uff1a\u5728\u5355\u9879\u4efb\u52a1\u5185\u90e8\u901a\u8fc7\u8fed\u4ee3\u6216\u63d0\u793a\u5de5\u7a0b\u7cbe\u70bc\u65f6\u95f4\u68c0\u6d4b\u7ed3\u679c\uff1b2) inter-task chaining\uff1a\u5c06\u65f6\u95f4\u68c0\u6d4b\u7ed3\u679c\u4f5c\u4e3a\u6761\u4ef6\u4f9d\u6b21\u89e6\u53d1\u7a7a\u95f4\u5b9a\u4f4d\u4e0e\u8bed\u4e49\u89e3\u91ca\uff0c\u4ece\u800c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u89e3\u91ca\u3002\u6574\u4e2a\u6d41\u7a0b\u4e0d\u4f7f\u7528\u989d\u5916\u6570\u636e\u6216\u68af\u5ea6\u66f4\u65b0\uff0c\u4ec5\u4f9d\u8d56\u57fa\u7840\u6a21\u578b\u548c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4efb\u52a1\u7ea7\u63d0\u793a\u4e32\u8054\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u3001\u5b9a\u4f4d\u4e0e\u89e3\u91ca\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u7eaf\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u793a\u51fa\u826f\u597d\u7684\u6cdb\u5316\u4e0e\u53ef\u89e3\u91ca\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5408\u7406\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u4efb\u52a1\u7ea7\u94fe\u5f0f\u63a8\u7406\u80fd\u591f\u6316\u6398\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u7528\u7684\u3001\u53ef\u89e3\u91ca\u7684\u89c6\u9891\u5f02\u5e38\u5206\u6790\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u96f6\u6837\u672c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u94fe\u5f0f\u6d4b\u8bd5\u65f6\u63a8\u7406\uff08chained test-time reasoning\uff09\u5c06\u65f6\u95f4\u68c0\u6d4b\u3001\u7a7a\u95f4\u5b9a\u4f4d\u548c\u6587\u672c\u89e3\u91ca\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\u65e0\u987b\u989d\u5916\u8bad\u7ec3\u7684\u6570\u636e\u65e0\u5173\u7684\u5f02\u5e38\u5206\u6790\u3002"}}
{"id": "2511.00981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00981", "abs": "https://arxiv.org/abs/2511.00981", "authors": ["Suzhong Fu", "Rui Sun", "Xuan Ding", "Jingqi Dong", "Yiming Yang", "Yao Zhu", "Min Chang Jordan Ren", "Delin Deng", "Angelica Aviles-Rivero", "Shuguang Cui", "Zhen Li"], "title": "VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel", "comment": null, "summary": "Accurate vessel segmentation is critical for clinical applications such as\ndisease diagnosis and surgical planning, yet remains challenging due to thin,\nbranching structures and low texture contrast. While foundation models like the\nSegment Anything Model (SAM) have shown promise in generic segmentation, they\nperform sub-optimally on vascular structures. In this work, we present VesSAM,\na powerful and efficient framework tailored for 2D vessel segmentation. VesSAM\nintegrates (1) a convolutional adapter to enhance local texture features, (2) a\nmulti-prompt encoder that fuses anatomical prompts, including skeletons,\nbifurcation points, and segment midpoints, via hierarchical cross-attention,\nand (3) a lightweight mask decoder to reduce jagged artifacts. We also\nintroduce an automated pipeline to generate structured multi-prompt\nannotations, and curate a diverse benchmark dataset spanning 8 datasets across\n5 imaging modalities. Experimental results demonstrate that VesSAM consistently\noutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%\nIoU, and achieves competitive performance compared to fully fine-tuned methods,\nwith significantly fewer parameters. VesSAM also generalizes well to\nout-of-distribution (OoD) settings, outperforming all baselines in average OoD\nDice and IoU.", "AI": {"tldr": "\u9488\u5bf9\u8840\u7ba1\u5206\u5272\u6539\u9020SAM\uff1a\u5377\u79ef\u9002\u914d\u5668+\u591a\u63d0\u793a\u7f16\u7801\u5668+\u8f7b\u91cf\u63a9\u7801\u89e3\u7801\u5668\uff0c\u81ea\u52a8\u751f\u6210\u591a\u63d0\u793a\u6807\u6ce8\uff0c\u8de8\u6a21\u6001\u3001\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u660e\u663e\u8457\u63d0\u5347Dice/IoU\u4e14\u6cdb\u5316\u6027\u5f3a\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u5206\u5272\u6a21\u578b\uff08\u5982SAM\uff09\u5728\u8840\u7ba1\u8fd9\u7c7b\u7ec6\u957f\u3001\u5206\u652f\u4e14\u5bf9\u6bd4\u5ea6\u4f4e\u7684\u7ed3\u6784\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u5316\u8bbe\u8ba1\u4ee5\u63d0\u5347\u5c40\u90e8\u7eb9\u7406\u611f\u77e5\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u5728SAM\u57fa\u7840\u4e0a\u5f15\u5165\u5377\u79ef\u9002\u914d\u5668\u589e\u5f3a\u5c40\u90e8\u7eb9\u7406\u7279\u5f81\uff0c\u8bbe\u8ba1\u591a\u63d0\u793a\u7f16\u7801\u5668\u901a\u8fc7\u5206\u5c42\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u9aa8\u67b6\u3001\u5206\u53c9\u70b9\u548c\u7ebf\u6bb5\u4e2d\u70b9\u7b49\u89e3\u5256\u63d0\u793a\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u63a9\u7801\u89e3\u7801\u5668\u51cf\u5c11\u952f\u9f7f\u4f2a\u5f71\uff1b\u540c\u65f6\u6784\u5efa\u81ea\u52a8\u5316\u7ba1\u7ebf\u751f\u6210\u7ed3\u6784\u5316\u591a\u63d0\u793a\u6807\u6ce8\u3002", "result": "\u57288\u4e2a\u6570\u636e\u96c6\u30015\u79cd\u6210\u50cf\u6a21\u6001\u6784\u6210\u7684\u57fa\u51c6\u4e0a\uff0cVesSAM\u76f8\u8f83\u4e8e\u57fa\u4e8ePEFT\u7684SAM\u53d8\u4f53\u5728Dice\u4e0a\u63d0\u5347>10%\u3001IoU\u63d0\u5347>13%\uff1b\u5728\u53c2\u6570\u8fdc\u5c11\u4e8e\u5b8c\u5168\u5fae\u8c03\u65b9\u6cd5\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u5e76\u5728OoD\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u5e73\u5747Dice\u548cIoU\u3002", "conclusion": "VesSAM\u662f\u4e00\u79cd\u9488\u5bf92D\u8840\u7ba1\u5206\u5272\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8584\u3001\u5206\u652f\u8840\u7ba1\u7ed3\u6784\u7684\u5206\u5272\u6027\u80fd\uff0c\u4e14\u53c2\u6570\u91cf\u5c0f\u3001\u6cdb\u5316\u6027\u597d\u3002"}}
{"id": "2511.00997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.00997", "abs": "https://arxiv.org/abs/2511.00997", "authors": ["Chang Nie", "Tianchen Deng", "Zhe Liu", "Hesheng Wang"], "title": "MID: A Self-supervised Multimodal Iterative Denoising Framework", "comment": null, "summary": "Data denoising is a persistent challenge across scientific and engineering\ndomains. Real-world data is frequently corrupted by complex, non-linear noise,\nrendering traditional rule-based denoising methods inadequate. To overcome\nthese obstacles, we propose a novel self-supervised multimodal iterative\ndenoising (MID) framework. MID models the collected noisy data as a state\nwithin a continuous process of non-linear noise accumulation. By iteratively\nintroducing further noise, MID learns two neural networks: one to estimate the\ncurrent noise step and another to predict and subtract the corresponding noise\nincrement. For complex non-linear contamination, MID employs a first-order\nTaylor expansion to locally linearize the noise process, enabling effective\niterative removal. Crucially, MID does not require paired clean-noisy datasets,\nas it learns noise characteristics directly from the noisy inputs. Experiments\nacross four classic computer vision tasks demonstrate MID's robustness,\nadaptability, and consistent state-of-the-art performance. Moreover, MID\nexhibits strong performance and adaptability in tasks within the biomedical and\nbioinformatics domains.", "AI": {"tldr": "MID\u662f\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u7684\u81ea\u76d1\u7763\u8fed\u4ee3\u53bb\u566a\u6846\u67b6\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u5316\u975e\u7ebf\u6027\u566a\u58f0\u5e76\u5b66\u4e60\u4f30\u8ba1\u4e0e\u51cf\u566a\u7f51\u7edc\uff0c\u80fd\u5728\u591a\u9886\u57df\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7a33\u5065\u4e14\u9886\u5148\u7684\u53bb\u566a\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5e38\u53d7\u590d\u6742\u975e\u7ebf\u6027\u566a\u58f0\u6c61\u67d3\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e14\u5f80\u5f80\u4f9d\u8d56\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6837\u672c\u3002\u4e3a\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u914d\u5bf9\u6570\u636e\u3001\u80fd\u9002\u5e94\u975e\u7ebf\u6027\u566a\u58f0\u5e76\u5177\u5907\u7a33\u5065\u6027\u7684\u901a\u7528\u53bb\u566a\u6846\u67b6\u3002", "method": "MID\u901a\u8fc7\u5efa\u6a21\u566a\u58f0\u5728\u8fde\u7eed\u8fc7\u7a0b\u4e2d\u7684\u79ef\u7d2f\uff0c\u5f15\u5165\u4e24\u7c7b\u795e\u7ecf\u7f51\u7edc\uff1a\u4e00\u7c7b\u4f30\u8ba1\u5f53\u524d\u566a\u58f0\u6b65\uff08\u566a\u58f0\u72b6\u6001\uff09\uff0c\u53e6\u4e00\u7c7b\u9884\u6d4b\u5e76\u51cf\u53bb\u76f8\u5e94\u7684\u566a\u58f0\u589e\u91cf\u3002\u5bf9\u4e8e\u590d\u6742\u975e\u7ebf\u6027\u6c61\u67d3\uff0c\u4f7f\u7528\u4e00\u9636\u6cf0\u52d2\u5c55\u5f00\u5728\u5c40\u90e8\u7ebf\u6027\u5316\u566a\u58f0\u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u8fed\u4ee3\u53bb\u9664\u3002\u8bad\u7ec3\u8fc7\u7a0b\u901a\u8fc7\u5728\u539f\u59cb\u566a\u58f0\u6570\u636e\u4e0a\u8fdb\u4e00\u6b65\u52a0\u566a\u6765\u5f62\u6210\u81ea\u76d1\u7763\u4fe1\u53f7\u3002", "result": "\u5728\u56db\u4e2a\u7ecf\u5178\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u4e0a\uff0cMID\u8868\u73b0\u51fa\u7a33\u5065\u6027\u3001\u9002\u5e94\u6027\u548c\u4e00\u81f4\u7684\u9886\u5148\u6027\u80fd\uff1b\u540c\u65f6\u5728\u751f\u7269\u533b\u5b66\u548c\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u591a\u6a21\u6001\u8fed\u4ee3\u53bb\u566a\uff08MID\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u566a\u58f0\u89c6\u4e3a\u975e\u7ebf\u6027\u7d2f\u79ef\u8fc7\u7a0b\u5e76\u5f15\u5165\u8fed\u4ee3\u52a0\u566a\u4e0e\u51cf\u566a\u7684\u7b56\u7565\uff0c\u5b9e\u73b0\u65e0\u9700\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6570\u636e\u7684\u53bb\u566a\u5b66\u4e60\u3002"}}
{"id": "2511.01000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01000", "abs": "https://arxiv.org/abs/2511.01000", "authors": ["Hassan Ugail", "Ismail Lujain Jaleel"], "title": "Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya", "comment": null, "summary": "Art authentication of Francisco Goya's works presents complex computational\nchallenges due to his heterogeneous stylistic evolution and extensive\nhistorical patterns of forgery. We introduce a novel multimodal machine\nlearning framework that applies identical feature extraction techniques to both\nvisual and X-ray radiographic images of Goya paintings. The unified feature\nextraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,\nLocal Binary Patterns, entropy measures, energy calculations, and colour\ndistribution analysis applied consistently across both imaging modalities. The\nextracted features from both visual and X-ray images are processed through an\noptimised One-Class Support Vector Machine with hyperparameter tuning. Using a\ndataset of 24 authenticated Goya paintings with corresponding X-ray images,\nsplit into an 80/20 train-test configuration with 10-fold cross-validation, the\nframework achieves 97.8% classification accuracy with a 0.022 false positive\nrate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy\nof our pipeline, achieving 92.3% authentication confidence through unified\nmultimodal feature analysis. Our results indicate substantial performance\nimprovement over single-modal approaches, establishing the effectiveness of\napplying identical computational methods to both visual and radiographic\nimagery in art authentication applications.", "AI": {"tldr": "\u5bf9\u6208\u96c5\u4f5c\u54c1\uff0c\u8bba\u6587\u901a\u8fc7\u5728\u53ef\u89c1\u5149\u4e0eX\u5149\u5f71\u50cf\u4e0a\u7edf\u4e00\u63d0\u53d6\u7eb9\u7406/\u989c\u8272\u7b49\u7279\u5f81\u5e76\u7528\u4e00\u7c7bSVM\u8bc6\u522b\u771f\u4f2a\uff0c24\u5e45\u771f\u8ff9\u5b9e\u9a8c\u663e\u793a97.8%\u51c6\u786e\u7387\uff0c\u6848\u4f8b\u201cUn Gigante\u201d\u7f6e\u4fe1\u5ea692.3%\u3002", "motivation": "\u6208\u96c5\u4f5c\u54c1\u98ce\u683c\u591a\u8b8a\u4e14\u4f2a\u4f5c\u5386\u53f2\u60a0\u4e45\uff0c\u5355\u4e00\u6a21\u6001\u6216\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u5904\u7406\u96be\u4ee5\u6355\u6349\u8de8\u6a21\u6001\u7684\u9274\u522b\u4fe1\u606f\uff0c\u56e0\u6b64\u63d0\u51fa\u5bf9\u89c6\u89c9\u4e0eX\u5c04\u7ebf\u5f71\u50cf\u91c7\u7528\u76f8\u540c\u7684\u7279\u5f81\u63d0\u53d6\u4ee5\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u9274\u5b9a\u3002", "method": "\u8bba\u6587\u5728\u4e24\u79cd\u5f71\u50cf\u6a21\u6001\u4e0a\u7edf\u4e00\u8ba1\u7b97\u7eb9\u7406\uff08GLCM\uff09\u3001\u5c40\u90e8\u4e8c\u503c\u6a21\u5f0f\uff08LBP\uff09\u3001\u71b5\u3001\u80fd\u91cf\u548c\u989c\u8272\u5206\u5e03\u7b49\u7279\u5f81\uff1b\u5c06\u7279\u5f81\u8f93\u5165\u7ecf\u8fc7\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u4e00\u7c7bSVM\u8fdb\u884c\u8bad\u7ec3\u4e0e\u68c0\u6d4b\uff1b\u4f7f\u752880/20\u5206\u5272\u4e0e10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u572824\u5e45\u771f\u8ff9\u6570\u636e\u96c6\u4e0a\u62a5\u905397.8%\u51c6\u786e\u7387\u4e0e0.022\u7684\u5047\u9633\u6027\u7387\uff1b\u5bf9\u201cUn Gigante\u201d\u4e00\u4f8b\u7ed9\u51fa92.3%\u7684\u9274\u5b9a\u7f6e\u4fe1\u5ea6\uff0c\u4e14\u58f0\u79f0\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5728\u53ef\u89c1\u5149\u548cX\u5c04\u7ebf\u5f71\u50cf\u4e0a\u4f7f\u7528\u7edf\u4e00\u7279\u5f81\u63d0\u53d6\u6d41\u7a0b\u7684\u591a\u6a21\u6001\u827a\u672f\u9274\u5b9a\u6846\u67b6\uff0c\u5e76\u91c7\u7528\u4e00\u7c7b\u652f\u6301\u5411\u91cf\u673a\u8fdb\u884c\u9274\u5b9a\uff0c\u5b9e\u9a8c\u572824\u5e45\u6709X\u5149\u7684\u6208\u96c5\u771f\u8ff9\u4e0a\u5f97\u5230\u9ad8\u51c6\u786e\u7387\u3002"}}
{"id": "2511.01013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01013", "abs": "https://arxiv.org/abs/2511.01013", "authors": ["Mohammad Amanour Rahman"], "title": "HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images", "comment": "This manuscript has been submitted to Informatics in Medicine\n  Unlocked", "summary": "B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,\noperator dependency, and indistinct boundaries. Existing deep learning suffers\nfrom single-task learning, architectural constraints (CNNs lack global context,\nTransformers local features), and black-box decision-making. These gaps hinder\nclinical adoption.\n  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous\nsegmentation and classification with intrinsic interpretability. Its\ndual-branch encoder integrates EfficientNet-B3 and Swin Transformer via\nmulti-scale hierarchical fusion blocks. An attention-gated decoder provides\nprecision and explainability. We introduce dual-pipeline interpretability: (1)\nintrinsic attention validation with quantitative IoU verification (mean: 0.86),\nand (2) Grad-CAM for classification reasoning.\n  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and\naccuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant\nRecall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling\nyields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant\nRecall, eliminating false negatives. Ablation studies confirm multi-scale\nfusion contributes +16.8% Dice and attention gates add +5.9%.\n  Crucially, we conduct the first cross-dataset generalization study for hybrid\nCNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),\nconfirming domain shift. However, progressive fine-tuning with only 10%\ntarget-domain data (68 images) recovers 92.5% performance. With 50% data, our\nmodel achieves 77.3% Dice, exceeding source-domain performance (76.1%) and\ndemonstrating true generalization.", "AI": {"tldr": "HyFormer-Net\u901a\u8fc7CNN-Transformer\u878d\u5408\u4e0e\u53ef\u89e3\u91ca\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e73\u817aB\u8d85\u7684\u5206\u5272\u4e0e\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u5728\u5c0f\u6837\u672c\u5fae\u8c03\u4e0b\u5b9e\u73b0\u826f\u597d\u8de8\u57df\u6cdb\u5316\u3002", "motivation": "\u4f20\u7edfB\u8d85\u56fe\u50cf\u5b58\u5728\u6591\u70b9\u566a\u58f0\u3001\u64cd\u4f5c\u8005\u4f9d\u8d56\u4e0e\u8fb9\u754c\u6a21\u7cca\uff1b\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u4efb\u52a1\u5b66\u4e60\u3001\u7f51\u7edc\u67b6\u6784\u5c40\u9650\uff08CNN\u7f3a\u5168\u5c40\u3001Transformer\u7f3a\u5c40\u90e8\uff09\u53ca\u9ed1\u76d2\u51b3\u7b56\uff0c\u963b\u788d\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51faHyFormer-Net\uff1a\u53cc\u5206\u652f\u7f16\u7801\u5668\uff08EfficientNet-B3\u4e0eSwin Transformer\uff09\u901a\u8fc7\u591a\u5c3a\u5ea6\u5c42\u6b21\u878d\u5408\u6a21\u5757\u96c6\u6210\uff0c\u6ce8\u610f\u529b\u95e8\u63a7\u89e3\u7801\u5668\u7528\u4e8e\u7cbe\u7ec6\u5206\u5272\u4e0e\u89e3\u91ca\uff1b\u540c\u65f6\u91c7\u7528\u53cc\u901a\u9053\u53ef\u89e3\u91ca\u6027\u7b56\u7565\uff08\u6ce8\u610f\u529b\u56fe\u4e0eGrad-CAM\uff09\u5e76\u8fdb\u884c\u6d88\u878d\u4e0e\u96c6\u6210\u5b9e\u9a8c\u3002", "result": "\u5728BUSI\u6570\u636e\u96c6\u4e0a\uff0c\u5355\u6a21\u578bDice 0.761\u00b10.072\u3001\u51c6\u786e\u738793.2%\u3001\u6076\u6027\u53ec\u56de92.1\u00b12.2%\uff1b\u96c6\u6210\u540eDice 0.902\u3001\u51c6\u786e\u738799.5%\u3001\u6076\u6027\u53ec\u56de100%\uff1b\u6d88\u878d\u663e\u793a\u591a\u5c3a\u5ea6\u878d\u5408\u63d0\u9ad8Dice+16.8%\u3001\u6ce8\u610f\u529b\u95e8\u63d0\u9ad8+5.9%\uff1b\u8de8\u57df\u96f6\u6837\u672c\u8f6c\u79fbDice\u4ec50.058\uff0c\u4f46\u752810%\u76ee\u6807\u57df\u6570\u636e\u5fae\u8c03\u53ef\u6062\u590d\u81f392.5%\uff0c50%\u6570\u636e\u53ef\u8fbe\u523077.3%\u4e14\u8d85\u8d8a\u6e90\u57df\u6027\u80fd\u3002", "conclusion": "HyFormer-Net\u5728\u4e73\u817aB\u8d85\u5206\u5272\u4e0e\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u624b\u6bb5\u5e76\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u4e0a\u5c55\u793a\u4e86\u5f3a\u9002\u5e94\u6027\u4e0e\u5c0f\u6837\u672c\u5fae\u8c03\u6548\u679c\u3002"}}
{"id": "2511.01026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01026", "abs": "https://arxiv.org/abs/2511.01026", "authors": ["JunXi Yuan"], "title": "FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning", "comment": "17pages , 10figures , 12tables", "summary": "We present FastBoost, a parameter-efficient neural architecture that achieves\nstate-of-the-art performance on CIFAR benchmarks through a novel Dynamically\nScaled Progressive Attention (DSPA) mechanism. Our design establishes new\nefficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and\n93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and\n74.85% (0.44M parameters) The breakthrough stems from three fundamental\ninnovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention\nblending with dynamic weights. (2) Phase Scaling: Training-stage-aware\nintensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized\nskip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced\nMBConv blocks, FastBoost achieves a 2.1 times parameter reduction over\nMobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The\narchitecture features dual attention pathways with real-time weight adjustment,\ncascaded refinement layers (increasing gradient flow by 12.7%), and a\nhardware-friendly design (0.28G FLOPs). This co-optimization of dynamic\nattention and efficient convolution operations demonstrates unprecedented\nparameter-accuracy trade-offs, enabling deployment in resource-constrained edge\ndevices without accuracy degradation.", "AI": {"tldr": "FastBoost \u5229\u7528 DSPA \u7684\u4e09\u9879\u521b\u65b0\u5728\u6781\u5c11\u53c2\u6570\u4e0b\u5b9e\u73b0 CIFAR \u6570\u636e\u96c6\u7684 SOTA\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u90e8\u7f72\u3002", "motivation": "\u52a8\u673a\u662f\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u8bbe\u8ba1\u4e00\u4e2a\u5728\u53c2\u6570\u548c\u8ba1\u7b97\u4e0a\u90fd\u9ad8\u6548\u4f46\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7a81\u7834\u73b0\u6709\u8f7b\u91cf\u7f51\u7edc\uff08\u5982 MobileNetV3\uff09\u5728\u53c2\u6570-\u7cbe\u5ea6\u6743\u8861\u4e0a\u7684\u5c40\u9650\u3002", "method": "\u5728\u65b9\u6cd5\u4e0a\uff0c\u4f5c\u8005\u5c06 DSPA \u96c6\u6210\u5230\u589e\u5f3a\u7684 MBConv \u5757\u4e2d\uff0c\u6784\u5efa\u53cc\u6ce8\u610f\u529b\u8def\u5f84\u5e76\u5f15\u5165\u5b9e\u65f6\u6743\u91cd\u8c03\u6574\u4e0e\u7ea7\u8054\u7ec6\u5316\u5c42\uff0c\u4f7f\u7528\u8bad\u7ec3\u9636\u6bb5\u611f\u77e5\u7684\u5f3a\u5ea6\u8c03\u5236\uff08Phase Scaling\uff09\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u95e8\u63a7\uff08Residual Adaptation\uff09\u6765\u63a7\u5236\u4fe1\u606f\u6d41\uff0c\u63d0\u51fa\u4e86\u5b66\u4e60\u7684\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u878d\u5408\uff08Adaptive Fusion\uff09\u3002\u67b6\u6784\u6ce8\u91cd\u786c\u4ef6\u53cb\u597d\u6027\uff0c\u62a5\u544a\u4e86\u4f4e FLOPs \u548c\u53c2\u6570\u89c4\u6a21\u3002", "result": "\u5728 CIFAR-10 \u4e0a\u8fbe\u5230 95.57%\uff080.85M \u53c2\u6570\uff09\u548c 93.80%\uff080.37M \u53c2\u6570\uff09\uff1b\u5728 CIFAR-100 \u4e0a\u8fbe\u5230 81.37%\uff080.92M \u53c2\u6570\uff09\u548c 74.85%\uff080.44M \u53c2\u6570\uff09\u3002\u4e0e MobileNetV3 \u76f8\u6bd4\u5728\u53c2\u6570\u51cf\u5c11\u7ea6 2.1 \u500d\u7684\u540c\u65f6\uff0cCIFAR-10 \u7cbe\u5ea6\u63d0\u5347\u7ea6 +3.2%\u3002\u62a5\u544a\u8fd8\u58f0\u79f0\u7ea7\u8054\u5c42\u589e\u52a0\u4e86 12.7% \u7684\u68af\u5ea6\u6d41\uff0c\u5e76\u63d0\u4f9b 0.28G FLOPs \u7684\u786c\u4ef6\u53cb\u597d\u6307\u6807\u3002", "conclusion": "FastBoost \u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u52a8\u6001\u6ce8\u610f\u529b\u548c\u9ad8\u6548\u5377\u79ef\u7684\u53c2\u6570\u9ad8\u6548\u795e\u7ecf\u67b6\u6784\uff0c\u5728 CIFAR \u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u4f18\u5f02\u6027\u80fd\uff0c\u5176\u6838\u5fc3\u8d21\u732e\u662f\u63d0\u51fa\u4e86 Dynamically Scaled Progressive Attention (DSPA) \u673a\u5236\uff0c\u5e76\u901a\u8fc7\u4e09\u9879\u5173\u952e\u8bbe\u8ba1\uff08Adaptive Fusion\u3001Phase Scaling\u3001Residual Adaptation\uff09\u4f18\u5316\u6ce8\u610f\u529b\u878d\u5408\u3001\u8bad\u7ec3\u9636\u6bb5\u7f29\u653e\u4e0e\u6b8b\u5dee\u8fde\u63a5\uff0c\u4ece\u800c\u5728\u53c2\u6570\u6570\u76ee\u4e0e\u7cbe\u5ea6\u95f4\u8fbe\u6210\u826f\u597d\u6298\u4e2d\u3002"}}
{"id": "2511.01079", "categories": ["cs.CV", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2511.01079", "abs": "https://arxiv.org/abs/2511.01079", "authors": ["Nikolay I. Kalmykov", "Razan Dibo", "Kaiyu Shen", "Xu Zhonghan", "Anh-Huy Phan", "Yipeng Liu", "Ivan Oseledets"], "title": "T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression", "comment": "Submitted to Information Systems. Code will be released upon journal\n  publication", "summary": "Neural image compression (NIC) has become the state-of-the-art for\nrate-distortion performance, yet its security vulnerabilities remain\nsignificantly less understood than those of classifiers. Existing adversarial\nattacks on NICs are often naive adaptations of pixel-space methods, overlooking\nthe unique, structured nature of the compression pipeline. In this work, we\npropose a more advanced class of vulnerabilities by introducing T-MLA, the\nfirst targeted multiscale log--exponential attack framework. Our approach\ncrafts adversarial perturbations in the wavelet domain by directly targeting\nthe quality of the attacked and reconstructed images. This allows for a\nprincipled, offline attack where perturbations are strategically confined to\nspecific wavelet subbands, maximizing distortion while ensuring perceptual\nstealth. Extensive evaluation across multiple state-of-the-art NIC\narchitectures on standard image compression benchmarks reveals a large drop in\nreconstruction quality while the perturbations remain visually imperceptible.\nOur findings reveal a critical security flaw at the core of generative and\ncontent delivery pipelines.", "AI": {"tldr": "T-MLA\uff1a\u5728\u5c0f\u6ce2\u5b50\u5e26\u4e2d\u6709\u7b56\u7565\u5730\u65bd\u52a0\u591a\u5c3a\u5ea6\u5bf9\u6570-\u6307\u6570\u6270\u52a8\uff0c\u53ef\u5728\u4e0d\u660e\u663e\u6539\u53d8\u89c6\u89c9\u611f\u53d7\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u7684\u91cd\u6784\u8d28\u91cf\uff0c\u63ed\u793a\u4e86\u538b\u7f29\u7cfb\u7edf\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9NIC\u7684\u5bf9\u6297\u653b\u51fb\u591a\u4e3a\u50cf\u7d20\u57df\u65b9\u6cd5\u7684\u7b80\u5355\u79fb\u690d\uff0c\u5ffd\u89c6\u4e86\u538b\u7f29\u7ba1\u7ebf\u7684\u591a\u5c3a\u5ea6\u3001\u5c0f\u6ce2\u7ed3\u6784\u4e0e\u91cf\u5316\u654f\u611f\u6027\uff0c\u5bfc\u81f4\u6548\u679c\u6709\u9650\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u7b26\u5408\u538b\u7f29\u7279\u6027\u3001\u80fd\u6709\u6548\u7834\u574f\u91cd\u6784\u8d28\u91cf\u4e14\u96be\u4ee5\u88ab\u89c6\u89c9\u5bdf\u89c9\u7684\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6709\u9488\u5bf9\u6027\u7684\u591a\u5c3a\u5ea6\u5bf9\u6570-\u6307\u6570\uff08T-MLA\uff09\u653b\u51fb\u6846\u67b6\uff0c\u5728\u5c0f\u6ce2\u57df\u5bf9\u7279\u5b9a\u5b50\u5e26\u65bd\u52a0\u6270\u52a8\uff0c\u901a\u8fc7\u76f4\u63a5\u4f18\u5316\u88ab\u653b\u51fb\u4e0e\u91cd\u6784\u56fe\u50cf\u8d28\u91cf\u6765\u751f\u6210\u79bb\u7ebf\u6270\u52a8\uff0c\u517c\u987e\u626d\u66f2\u6700\u5927\u5316\u4e0e\u611f\u77e5\u9690\u853d\u6027\u3002", "result": "\u5728\u591a\u79cd\u4e3b\u6d41\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u67b6\u6784\u548c\u6807\u51c6\u538b\u7f29\u57fa\u51c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u4fdd\u6301\u89c6\u89c9\u4e0d\u53ef\u5bdf\u89c9\u7684\u524d\u63d0\u4e0b\uff0c\u91cd\u6784\u8d28\u91cf\u5927\u5e45\u4e0b\u964d\uff0c\u8bc1\u660eT-MLA\u80fd\u6709\u6548\u7834\u574fNIC\u7684\u9c81\u68d2\u6027\u5e76\u66b4\u9732\u5173\u952e\u5b89\u5168\u9690\u60a3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684T-MLA\u5728\u6ce2\u6bb5\u57df\u65bd\u52a0\u9488\u5bf9\u6027\u7684\u6270\u52a8\uff0c\u80fd\u5728\u4e0d\u663e\u8457\u589e\u52a0\u53ef\u89c1\u566a\u58f0\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u795e\u7ecf\u56fe\u50cf\u538b\u7f29\u91cd\u6784\u8d28\u91cf\uff0c\u63ed\u793a\u4e86NIC\u5728\u5185\u5bb9\u5206\u53d1\u548c\u751f\u6210\u6d41\u6c34\u7ebf\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2511.01082", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01082", "abs": "https://arxiv.org/abs/2511.01082", "authors": ["Narges Ghasemi", "Amir Ziashahabi", "Salman Avestimehr", "Cyrus Shahabi"], "title": "GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction", "comment": "Accepted to IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Image geolocalization, the task of determining an image's geographic origin,\nposes significant challenges, largely due to visual similarities across\ndisparate locations and the large search space. To address these issues, we\npropose a hierarchical sequence prediction approach inspired by how humans\nnarrow down locations from broad regions to specific addresses. Analogously,\nour model predicts geographic tokens hierarchically, first identifying a\ngeneral region and then sequentially refining predictions to increasingly\nprecise locations. Rather than relying on explicit semantic partitions, our\nmethod uses S2 cells, a nested, multiresolution global grid, and sequentially\npredicts finer-level cells conditioned on visual inputs and previous\npredictions. This procedure mirrors autoregressive text generation in large\nlanguage models. Much like in language modeling, final performance depends not\nonly on training but also on inference-time strategy. We investigate multiple\ntop-down traversal methods for autoregressive sampling, incorporating\ntechniques from test-time compute scaling used in language models.\nSpecifically, we integrate beam search and multi-sample inference while\nexploring various selection strategies to determine the final output. This\nenables the model to manage uncertainty by exploring multiple plausible paths\nthrough the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k\ndatasets against two distinct sets of baselines: those that operate without a\nMultimodal Large Language Model (MLLM) and those that leverage one. In the\nMLLM-free setting, our model surpasses other comparable baselines on nearly all\nmetrics, achieving state-of-the-art performance with accuracy gains of up to\n13.9%. When augmented with an MLLM, our model outperforms all baselines,\nsetting a new state-of-the-art across all metrics. The source code is available\nat https://github.com/NNargesNN/GeoToken.", "AI": {"tldr": "\u628a\u5730\u7406\u5b9a\u4f4d\u5efa\u6a21\u4e3a\u5728S2\u5c42\u6b21\u7f51\u683c\u4e0a\u7684\u81ea\u56de\u5f52\u5e8f\u5217\u751f\u6210\uff0c\u8bad\u7ec3+\u63a8\u7406\u65f6\u501f\u9274\u8bed\u8a00\u6a21\u578b\u7684\u6280\u5de7\uff08\u675f\u641c\u7d22/\u591a\u91c7\u6837\uff09\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4e24\u4e2a\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u52a8\u673a\u6e90\u4e8e\u4eba\u7c7b\u5b9a\u4f4d\u65f6\u4ece\u5927\u8303\u56f4\u5230\u5177\u4f53\u4f4d\u7f6e\u9010\u6b65\u7f29\u5c0f\u641c\u7d22\u8303\u56f4\uff0c\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u89c6\u89c9\u76f8\u4f3c\u6027\u548c\u5927\u91cf\u641c\u7d22\u7a7a\u95f4\u7684\u6311\u6218\uff0c\u56e0\u6b64\u501f\u9274\u8bed\u8a00\u5efa\u6a21\u7684\u81ea\u56de\u5f52\u5e8f\u5217\u751f\u6210\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6269\u5c55\u7b56\u7565\u4ee5\u6539\u5584\u5730\u7406\u5b9a\u4f4d\u6027\u80fd\u3002", "method": "\u65b9\u6cd5\u4f7f\u7528S2\u591a\u5206\u8fa8\u7387\u7f51\u683c\u4f5c\u4e3a\u5730\u7406token\uff0c\u6a21\u578b\u57fa\u4e8e\u89c6\u89c9\u8f93\u5165\u81ea\u56de\u5f52\u5730\u9884\u6d4b\u4ece\u7c97\u5230\u7ec6\u7684S2 cell\u5e8f\u5217\uff1b\u5728\u8bad\u7ec3\u4e2d\u5b66\u4e60\u6761\u4ef6\u6982\u7387\uff0c\u5728\u63a8\u7406\u4e2d\u91c7\u7528\u9876-down\u904d\u5386\u7b56\u7565\uff0c\u5305\u62ec\u675f\u641c\u7d22\u3001\u591a\u4e2a\u6837\u672c\u5e76\u7ed3\u5408\u4e0d\u540c\u9009\u62e9\u7b56\u7565\u6765\u786e\u5b9a\u6700\u7ec8\u8f93\u51fa\u3002", "result": "\u5728Im2GPS3k\u548cYFCC4k\u4e0a\uff0c\u6a21\u578b\u5728\u4e0d\u4f7f\u7528MLLM\u7684\u6761\u4ef6\u4e0b\u5728\u51e0\u4e4e\u6240\u6709\u6307\u6807\u4e0a\u4f18\u4e8e\u53ef\u6bd4\u57fa\u7ebf\uff0c\u6700\u9ad8\u63d0\u534713.9%\uff1b\u4e0eMLLM\u7ed3\u5408\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u5168\u9762\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0c\u8fbe\u5230\u65b0\u7684SOTA\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5206\u5c42\u5e8f\u5217\u9884\u6d4b\u5c06\u5168\u7403\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u5efa\u6a21\u4e3a\u5728S2\u6805\u683c\u4e0a\u7684\u81ea\u56de\u5f52\u591a\u5c42\u7ea7\u5355\u5143\u9884\u6d4b\uff0c\u4ece\u7c97\u5230\u7ec6\u9010\u6b65\u7ec6\u5316\u4f4d\u7f6e\u4fe1\u606f\uff1b\u5728\u63a8\u7406\u9636\u6bb5\u501f\u9274\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u7b56\u7565\uff08\u5982\u675f\u641c\u7d22\u548c\u591a\u91c7\u6837\uff09\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u6700\u7ec8\u5728Im2GPS3k\u548cYFCC4k\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e86\u65e0MLLM\u548c\u6709MLLM\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002"}}
{"id": "2511.01087", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01087", "abs": "https://arxiv.org/abs/2511.01087", "authors": ["Md. Abid Hasan Rafi", "Mst. Fatematuj Johora", "Pankaj Bhowmik"], "title": "SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices", "comment": null, "summary": "The emergence of 5G and 6G networks has established network slicing as a\nsignificant part of future service-oriented architectures, demanding refined\nidentification methods supported by robust datasets. The article presents\nSliceVision-F2I, a dataset of synthetic samples for studying feature\nvisualization in network slicing for next-generation networking systems. The\ndataset transforms multivariate Key Performance Indicator (KPI) vectors into\nvisual representations through four distinct encoding methods: physically\ninspired mappings, Perlin noise, neural wallpapering, and fractal branching.\nFor each encoding method, 30,000 samples are generated, each comprising a raw\nKPI vector and a corresponding RGB image at low-resolution pixels. The dataset\nsimulates realistic and noisy network conditions to reflect operational\nuncertainties and measurement imperfections. SliceVision-F2I is suitable for\ntasks involving visual learning, network state classification, anomaly\ndetection, and benchmarking of image-based machine learning techniques applied\nto network data. The dataset is publicly available and can be reused in various\nresearch contexts, including multivariate time series analysis, synthetic data\ngeneration, and feature-to-image transformations.", "AI": {"tldr": "SliceVision-F2I\uff1a120k\u5408\u6210KPI\u2192\u56fe\u50cf\u6837\u672c\uff0c4\u79cd\u7f16\u7801\uff0c\u6a21\u62df\u566a\u58f0\uff0c\u9762\u5411\u89c6\u89c9\u5b66\u4e60\u4e0e\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u968f\u77405G/6G\u7f51\u7edc\u4e0e\u7f51\u7edc\u5207\u7247\u7684\u5174\u8d77\uff0c\u8feb\u5207\u9700\u8981\u53ef\u9760\u7684\u6570\u636e\u96c6\u4e0e\u66f4\u7cbe\u7ec6\u7684\u7279\u5f81\u8bc6\u522b\u65b9\u6cd5\u6765\u652f\u6301\u670d\u52a1\u5bfc\u5411\u67b6\u6784\u548c\u7f51\u7edc\u72b6\u6001\u76d1\u6d4b\u3002", "method": "\u5c06\u591a\u53d8\u91cfKPI\u5411\u91cf\u901a\u8fc7\u56db\u79cd\u7f16\u7801\u65b9\u6cd5\uff08\u7269\u7406\u542f\u53d1\u6620\u5c04\u3001Perlin\u566a\u58f0\u3001\u795e\u7ecf\u58c1\u7eb8\u3001\u5206\u5f62\u5206\u652f\uff09\u8f6c\u6362\u4e3a\u4f4e\u5206\u8fa8\u7387RGB\u56fe\u50cf\uff1b\u6bcf\u79cd\u7f16\u7801\u751f\u621030,000\u4e2a\u6837\u672c\uff1b\u540c\u65f6\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u5f15\u5165\u566a\u58f0\u4ee5\u6a21\u62df\u771f\u5b9e\u7f51\u7edc\u6761\u4ef6\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b\u539f\u59cbKPI\u5411\u91cf\u4e0e\u5bf9\u5e94RGB\u56fe\u50cf\u7684SliceVision-F2I\u6570\u636e\u96c6\uff0c\u5171120,000\u4e2a\u6837\u672c\uff0c\u6570\u636e\u516c\u5f00\u53ef\u7528\uff0c\u80fd\u591f\u652f\u6301\u591a\u79cd\u7814\u7a76\u4efb\u52a1\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86SliceVision-F2I\u6570\u636e\u96c6\uff0c\u4e3a\u7f51\u7edc\u5207\u7247\u9886\u57df\u63d0\u4f9b\u4e86\u4ece\u591a\u53d8\u91cfKPI\u5411\u56fe\u50cf\u7684\u7279\u5f81\u53ef\u89c6\u5316\u7814\u7a76\u8d44\u6e90\uff0c\u9002\u7528\u4e8e\u89c6\u89c9\u5b66\u4e60\u3001\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u4efb\u52a1\u3002"}}
{"id": "2511.01098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01098", "abs": "https://arxiv.org/abs/2511.01098", "authors": ["Veronica Marsico", "Antonio Quintero-Rincon", "Hadj Batatia"], "title": "Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images", "comment": "12 pages, 6 figures, 3 tables", "summary": "This study presents a novel method for diagnosing respiratory diseases using\nimage data. It combines Epanechnikov's non-parametric kernel density estimation\n(EKDE) with a bimodal logistic regression classifier in a\nstatistical-model-based learning scheme. EKDE's flexibility in modeling data\ndistributions without assuming specific shapes and its adaptability to pixel\nintensity variations make it valuable for extracting key features from medical\nimages. The method was tested on 13808 randomly selected chest X-rays from the\nCOVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of\n59.26%, and a specificity of 74.18%, demonstrating moderate performance in\ndetecting respiratory disease while showing room for improvement in\nsensitivity. While clinical expertise remains essential for further refining\nthe model, this study highlights the potential of EKDE-based approaches to\nenhance diagnostic accuracy and reliability in medical imaging.", "AI": {"tldr": "\u4f5c\u8005\u63d0\u51faEKDE+\u53cc\u6a21\u6001\u903b\u8f91\u56de\u5f52\u7528\u4e8e\u80f8\u7247\u75be\u75c5\u8bca\u65ad\uff0c\u7ed3\u679c\u663e\u793a\u51c6\u786e\u7387\u4e2d\u7b49\u4f46\u5bf9\u9633\u6027\u6837\u672c\u68c0\u6d4b\u654f\u611f\u6027\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4e0e\u4e34\u5e8a\u9a8c\u8bc1\u3002", "motivation": "\u5229\u7528EKDE\u65e0\u9700\u9884\u8bbe\u5206\u5e03\u5f62\u72b6\u7684\u7075\u6d3b\u6027\uff0c\u9002\u5e94\u50cf\u7d20\u5f3a\u5ea6\u53d8\u5316\uff0c\u4ee5\u63d0\u9ad8\u533b\u5b66\u5f71\u50cf\u4e2d\u75c5\u7076\u7279\u5f81\u63d0\u53d6\u7684\u51c6\u786e\u6027\u4e0e\u7a33\u5b9a\u6027\u3002", "method": "\u57fa\u4e8eEpanechnikov\u6838\u7684\u975e\u53c2\u6570\u6838\u5bc6\u5ea6\u4f30\u8ba1\uff08EKDE\uff09\u4ece\u50cf\u7d20\u5f3a\u5ea6\u5206\u5e03\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u518d\u901a\u8fc7\u4e8c\u5143/\u53cc\u6a21\u6001\u903b\u8f91\u56de\u5f52\u8fdb\u884c\u5206\u7c7b\uff0c\u6784\u6210\u7edf\u8ba1\u6a21\u578b\u9a71\u52a8\u7684\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u572813808\u5f20\u968f\u673a\u9009\u53d6\u7684\u80f8\u7247\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u738770.14%\uff0c\u654f\u611f\u602759.26%\uff0c\u7279\u5f02\u602774.18%\uff0c\u8868\u73b0\u4e3a\u4e2d\u7b49\u603b\u4f53\u6027\u80fd\u4f46\u654f\u611f\u6027\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86EKDE\u4e0e\u53cc\u6a21\u6001\u903b\u8f91\u56de\u5f52\u7ed3\u5408\u7528\u4e8e\u80f8\u7247\u547c\u5438\u75be\u75c5\u8bca\u65ad\u7684\u53ef\u884c\u6027\uff0c\u4f46\u5728\u654f\u611f\u6027\u4e0a\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u4ecd\u9700\u6539\u8fdb\u548c\u66f4\u591a\u9a8c\u8bc1\u3002"}}
{"id": "2511.01109", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01109", "abs": "https://arxiv.org/abs/2511.01109", "authors": ["Alexander Thorley", "Agis Chartsias", "Jordan Strom", "Jeremy Slivnick", "Dipak Kotecha", "Alberto Gomez", "Jinming Duan"], "title": "Anatomically Constrained Transformers for Echocardiogram Analysis", "comment": null, "summary": "Video transformers have recently demonstrated strong potential for\nechocardiogram (echo) analysis, leveraging self-supervised pre-training and\nflexible adaptation across diverse tasks. However, like other models operating\non videos, they are prone to learning spurious correlations from non-diagnostic\nregions such as image backgrounds. To overcome this limitation, we propose the\nVideo Anatomically Constrained Transformer (ViACT), a novel framework that\nintegrates anatomical priors directly into the transformer architecture. ViACT\nrepresents a deforming anatomical structure as a point set and encodes both its\nspatial geometry and corresponding image patches into transformer tokens.\nDuring pre-training, ViACT follows a masked autoencoding strategy that masks\nand reconstructs only anatomical patches, enforcing that representation\nlearning is focused on the anatomical region. The pre-trained model can then be\nfine-tuned for tasks localized to this region. In this work we focus on the\nmyocardium, demonstrating the framework on echo analysis tasks such as left\nventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)\ndetection. The anatomical constraint focuses transformer attention within the\nmyocardium, yielding interpretable attention maps aligned with regions of known\nCA pathology. Moreover, ViACT generalizes to myocardium point tracking without\nrequiring task-specific components such as correlation volumes used in\nspecialized tracking networks.", "AI": {"tldr": "ViACT\u901a\u8fc7\u5c06\u89e3\u5256\u70b9\u96c6\u4e0e\u56fe\u50cf\u8865\u4e01\u5171\u540c\u4f5c\u4e3atoken\u5e76\u5728\u9884\u8bad\u7ec3\u4e2d\u4ec5\u91cd\u5efa\u89e3\u5256\u8865\u4e01\uff0c\u5f3a\u5236\u6a21\u578b\u5173\u6ce8\u5fc3\u808c\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8d85\u58f0\u89c6\u9891\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u4efb\u52a1\u6cdb\u5316\u3002", "motivation": "\u89c6\u9891Transformer\u5728\u5fc3\u810f\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6613\u4ece\u80cc\u666f\u7b49\u975e\u8bca\u65ad\u533a\u57df\u5b66\u4e60\u865a\u5047\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5c06\u89e3\u5256\u5b66\u5148\u9a8c\u76f4\u63a5\u6574\u5408\u5230\u6a21\u578b\u4e2d\u4ee5\u5f15\u5bfc\u6ce8\u610f\u529b\u96c6\u4e2d\u4e8e\u5173\u952e\u89e3\u5256\u7ed3\u6784\u3002", "method": "\u5c06\u53d8\u5f62\u7684\u89e3\u5256\u7ed3\u6784\u8868\u793a\u4e3a\u70b9\u96c6\uff0c\u5e76\u5c06\u5176\u51e0\u4f55\u7a7a\u95f4\u4fe1\u606f\u4e0e\u5bf9\u5e94\u56fe\u50cf\u8865\u4e01\u4e00\u5e76\u7f16\u7801\u4e3aTransformer token\uff1b\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u91c7\u7528\u53ea\u5bf9\u89e3\u5256\u8865\u4e01\u8fdb\u884c\u906e\u76d6\u5e76\u91cd\u5efa\u7684masked autoencoding\u7b56\u7565\uff0c\u4f7f\u8868\u5f81\u5b66\u4e60\u805a\u7126\u4e8e\u89e3\u5256\u533a\u57df\uff1b\u5fae\u8c03\u9636\u6bb5\u5728\u8be5\u5c40\u90e8\u533a\u57df\u4e0a\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u5b66\u4e60\u3002", "result": "\u5728\u5de6\u5ba4\u5c04\u8840\u5206\u6570\u56de\u5f52\u548c\u5fc3\u808c\u6dc0\u7c89\u6837\u53d8\u6027\u68c0\u6d4b\u7b49\u4efb\u52a1\u4e0a\uff0cViACT\u4f7f\u6ce8\u610f\u529b\u56fe\u66f4\u53ef\u89e3\u91ca\u5e76\u4e0e\u5df2\u77e5\u75c5\u53d8\u533a\u57df\u5bf9\u9f50\uff1b\u540c\u65f6\u53ef\u63a8\u5e7f\u81f3\u5fc3\u808c\u70b9\u8ddf\u8e2a\u4efb\u52a1\uff0c\u65e0\u9700\u4f7f\u7528\u4e13\u95e8\u7684\u76f8\u5173\u4f53\u79ef\u7b49\u8ffd\u8e2a\u7ec4\u4ef6\u3002", "conclusion": "ViACT\u901a\u8fc7\u5728\u89c6\u9891Transformer\u4e2d\u76f4\u63a5\u5f15\u5165\u89e3\u5256\u5b66\u5148\u9a8c\uff0c\u6709\u6548\u6291\u5236\u4e86\u6a21\u578b\u5bf9\u80cc\u666f\u7b49\u975e\u8bca\u65ad\u533a\u57df\u7684\u865a\u5047\u76f8\u5173\u5b66\u4e60\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8d85\u58f0\u5fc3\u52a8\u56fe\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01129", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.01129", "abs": "https://arxiv.org/abs/2511.01129", "authors": ["Fabio Diniz Rossi"], "title": "Boosting performance of computer vision applications through embedded GPUs on the edge", "comment": "4 pages, 6 figures", "summary": "Computer vision applications, especially those using augmented reality\ntechnology, are becoming quite popular in mobile devices. However, this type of\napplication is known as presenting significant demands regarding resources. In\norder to enable its utilization in devices with more modest resources, edge\ncomputing can be used to offload certain high intensive tasks. Still, edge\ncomputing is usually composed of devices with limited capacity, which may\nimpact in users quality of experience when using computer vision applications.\nThis work proposes the use of embedded devices with graphics processing units\n(GPUs) to overcome such limitation. Experiments performed shown that GPUs can\nattain a performance gain when compared to using only CPUs, which guarantee a\nbetter experience to users using such kind of application.", "AI": {"tldr": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\uff0c\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u5378\u8f7d\u5230\u5e26GPU\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u80fd\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u4f18\u4e8e\u4ec5\u7528CPU\u7684\u65b9\u6848\u3002", "motivation": "\u52a8\u673a\u662f\u79fb\u52a8\u8bbe\u5907\u4e0aAugmented Reality\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u5bf9\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u4f20\u7edf\u8fb9\u7f18\u8ba1\u7b97\u8282\u70b9\u901a\u5e38\u8d44\u6e90\u6709\u9650\uff0c\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff0c\u56e0\u6b64\u63a2\u7d22\u7528\u5e26GPU\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u4ece\u79fb\u52a8\u8bbe\u5907\u6216\u8f7b\u91cf\u7ea7\u8fb9\u7f18\u8282\u70b9\u5378\u8f7d\u5230\u5e26GPU\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\uff0c\u5e76\u5bf9\u6bd4\u4e86\u4ec5\u4f7f\u7528CPU\u7684\u6027\u80fd\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cGPU\u76f8\u8f83\u4e8e\u4ec5\u7528CPU\u5728\u5b9e\u9a8c\u573a\u666f\u4e2d\u5e26\u6765\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4ece\u800c\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u73af\u5883\u4e2d\u6539\u5584\u5e94\u7528\u54cd\u5e94\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u672c\u6587\u5f97\u51fa\u7ed3\u8bba\uff1a\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\uff0c\u91c7\u7528\u5e26GPU\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u673a\u89c6\u89c9\u589e\u5f3a\u73b0\u5b9e\u5e94\u7528\u7684\u6027\u80fd\uff0c\u4ece\u800c\u6539\u5584\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2511.01131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01131", "abs": "https://arxiv.org/abs/2511.01131", "authors": ["Md Nahiduzzaman", "Steven Korevaar", "Alireza Bab-Hadiashar", "Ruwan Tennakoon"], "title": "Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis", "comment": null, "summary": "Human-interpretable predictions are essential for deploying AI in medical\nimaging, yet most interpretable-by-design (IBD) frameworks require concept\nannotations for training data, which are costly and impractical to obtain in\nclinical contexts. Recent attempts to bypass annotation, such as zero-shot\nvision-language models or concept-generation frameworks, struggle to capture\ndomain-specific medical features, leading to poor reliability. In this paper,\nwe propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised\nframework that enables concept answer prediction without explicit supervision\nor reliance on language models. PCP leverages class-level concept priors as\nweak supervision and incorporates a refinement mechanism with KL divergence and\nentropy regularization to align predictions with clinical reasoning.\nExperiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves\nconcept-level F1-score by over 33% compared to zero-shot baselines, while\ndelivering competitive classification performance on four medical datasets\n(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept\nbottleneck models (CBMs) and V-IP.", "AI": {"tldr": "\u63d0\u51faPCP\uff0c\u7528\u7c7b\u7ea7\u6982\u5ff5\u5148\u9a8c\u4e0eKL\u548c\u71b5\u6b63\u5219\u8fdb\u884c\u5f31\u76d1\u7763\u6982\u5ff5\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6982\u5ff5F1\u5e76\u4fdd\u6301\u7ade\u4e89\u6027\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "Concept annotations are costly in medical imaging; zero-shot and LLM-based methods fail to capture domain-specific features; need weakly supervised approach to produce human-interpretable concept predictions aligned with clinical reasoning.", "method": "Introduce Prior-guided Concept Predictor (PCP) that uses class-level concept priors as weak supervision; employs a refinement module with KL divergence and entropy regularization to align predictions to priors and encourage confident, clinically-reasoned outputs; trained end-to-end with image-level labels only.", "result": "On PH2 and WBCatt, PCP improves concept-level F1 by >33% over zero-shot baselines; classification performance competitive on PH2, WBCatt, HAM10000, and CXR4 compared to fully supervised CBMs and V-IP.", "conclusion": "PCP can predict concept answers without concept-level annotations by using class-level priors and regularized refinement, improving concept prediction and keeping competitive classification performance."}}
{"id": "2511.01139", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01139", "abs": "https://arxiv.org/abs/2511.01139", "authors": ["Yoshihiro Maruyama"], "title": "Learning with Category-Equivariant Architectures for Human Activity Recognition", "comment": null, "summary": "We propose CatEquiv, a category-equivariant neural network for Human Activity\nRecognition (HAR) from inertial sensors that systematically encodes temporal,\namplitude, and structural symmetries. In particular, we introduce the\ncategorical symmetry product where cyclic time shifts, positive gains and the\nsensor-hierarchy poset together capture the categorical symmetry structure of\nthe data. CatEquiv achieves equivariance with respect to the categorical\nsymmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv\nattains markedly higher robustness compared with circularly padded CNNs and\nplain CNNs. These results demonstrate that enforcing categorical symmetries\nyields strong invariance and generalization without additional model capacity.", "AI": {"tldr": "\u901a\u8fc7\u628a\u65f6\u95f4\u5e73\u79fb\u3001\u5e45\u503c\u7f29\u653e\u548c\u4f20\u611f\u5668\u5c42\u7ea7\u4ee5\u8303\u7574\u5316\u7684\u65b9\u5f0f\u5efa\u6a21\u5e76\u6784\u9020\u7b49\u53d8\u7f51\u7edc\uff0cCatEquiv\u5728HAR\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u7a33\u5065\u7684\u6cdb\u5316\u3002", "motivation": "HAR\u6570\u636e\u5177\u6709\u5929\u7136\u7684\u5468\u671f\u6027\uff08\u65f6\u95f4\u5e73\u79fb\uff09\u3001\u5e45\u503c\u53d8\u52a8\uff08\u589e\u76ca\uff09\u548c\u4f20\u611f\u5668\u95f4\u7684\u5c42\u7ea7\u7ed3\u6784\uff0c\u73b0\u6709CNN\u672a\u7cfb\u7edf\u5229\u7528\u8fd9\u4e9b\u5bf9\u79f0\u6027\u5bfc\u81f4\u6cdb\u5316\u53d7\u9650\uff0c\u6545\u63d0\u51fa\u5229\u7528\u8303\u7574\u7406\u8bba/\u5bf9\u79f0\u6027\u6765\u63d0\u5347\u7a33\u5065\u6027\u4e0e\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u5206\u7c7b\u5bf9\u79f0\u6027\u4e58\u79ef\uff08categorical symmetry product\uff09\u5c06\u4e09\u7c7b\u53d8\u6362\uff08\u5faa\u73af\u65f6\u95f4\u5e73\u79fb\u3001\u6b63\u589e\u76ca\u3001\u4f20\u611f\u5668\u5c42\u7ea7poset\uff09\u7ed3\u5408\uff0c\u8bbe\u8ba1\u5bf9\u5e94\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u4f7f\u7f51\u7edc\u5bf9\u8be5\u4e58\u79ef\u7fa4\u4fdd\u6301\u7b49\u53d8\u6027\uff0c\u4ece\u800c\u5728\u7279\u5f81\u63d0\u53d6\u8fc7\u7a0b\u4e2d\u7ed3\u6784\u5316\u5730\u7ea6\u675f\u8868\u793a\u3002", "result": "\u5728UCI-HAR\u6570\u636e\u96c6\u7684OOD\u6270\u52a8\u573a\u666f\uff08\u5982\u65f6\u95f4\u504f\u79fb\u3001\u589e\u76ca\u53d8\u5316\u3001\u4f20\u611f\u5668\u7f3a\u5931/\u91cd\u6392\u7b49\uff09\u4e0b\uff0cCatEquiv\u4f18\u4e8e\u5e26\u5faa\u73af\u586b\u5145\u7684CNN\u548c\u666e\u901aCNN\uff0c\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\uff0c\u800c\u6a21\u578b\u53c2\u6570\u91cf\u672a\u589e\u52a0\u3002", "conclusion": "CatEquiv\u901a\u8fc7\u5bf9\u65f6\u95f4\u5faa\u73af\u5e73\u79fb\u3001\u6b63\u589e\u76ca\u7f29\u653e\u4ee5\u53ca\u4f20\u611f\u5668\u5c42\u7ea7\u504f\u5e8f\u7ed3\u6784\u5efa\u6a21\uff0c\u7cfb\u7edf\u6027\u5730\u5b9e\u73b0\u4e86\u5bf9\u5206\u7c7b\u5bf9\u79f0\u6027\u7684\u7b49\u53d8\u6027\uff0c\u4ece\u800c\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u4e86\u5bf9OOD\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.01143", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01143", "abs": "https://arxiv.org/abs/2511.01143", "authors": ["Ziyi Wang", "Yuanmei Zhang", "Dorna Esrafilzadeh", "Ali R. Jalili", "Suncheng Xiang"], "title": "MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation", "comment": "Work in progress", "summary": "Early and accurate segmentation of colorectal polyps is critical for reducing\ncolorectal cancer mortality, which has been extensively explored by academia\nand industry. However, current deep learning-based polyp segmentation models\neither compromise clinical decision-making by providing ambiguous polyp margins\nin segmentation outputs or rely on heavy architectures with high computational\ncomplexity, resulting in insufficient inference speeds for real-time colorectal\nendoscopic applications. To address this problem, we propose MicroAUNet, a\nlight-weighted attention-based segmentation network that combines\ndepthwise-separable dilated convolutions with a single-path, parameter-shared\nchannel-spatial attention block to strengthen multi-scale boundary features. On\nthe basis of it, a progressive two-stage knowledge-distillation scheme is\nintroduced to transfer semantic and boundary cues from a high-capacity teacher.\nExtensive experiments on benchmarks also demonstrate the state-of-the-art\naccuracy under extremely low model complexity, indicating that MicroAUNet is\nsuitable for real-time clinical polyp segmentation. The code is publicly\navailable at https://github.com/JeremyXSC/MicroAUNet.", "AI": {"tldr": "MicroAUNet\uff1a\u8f7b\u91cf\u6ce8\u610f\u529b\u5206\u5272\u7f51\u7edc+\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4f4e\u590d\u6742\u5ea6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u3001\u5b9e\u65f6\u7684\u7ed3\u76f4\u80a0\u606f\u8089\u5206\u5272\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u8f93\u51fa\u8fb9\u754c\u6a21\u7cca\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\uff0c\u8981\u4e48\u6a21\u578b\u8fc7\u5927\u5bfc\u81f4\u5b9e\u65f6\u63a8\u7406\u4e0d\u8db3\uff0c\u6545\u9700\u8981\u517c\u987e\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u8f7b\u91cf\u7ea7\u5206\u5272\u7f51\u7edc\u3002", "method": "\u6a21\u578b\u7ed3\u6784\u4e0a\u4f7f\u7528depthwise-separable dilated convolutions\u4e0e\u5355\u8def\u5f84\u3001\u53c2\u6570\u5171\u4eab\u7684channel-spatial attention\u5757\uff1b\u8bad\u7ec3\u4e0a\u5f15\u5165\u4e86\u6e10\u8fdb\u5f0f\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff0c\u5148\u84b8\u998f\u8bed\u4e49\u4fe1\u606f\u518d\u84b8\u998f\u8fb9\u754c\u4fe1\u606f\uff08\u6216\u540c\u65f6\u84b8\u998f\u4e24\u8005\u7684\u4e0d\u540c\u9636\u6bb5\uff09\uff0c\u4ee5\u63d0\u5347\u8f7b\u91cf\u7f51\u7edc\u7684\u8868\u73b0\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\uff0c\u5728\u6781\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u4e0b\u53d6\u5f97\u4e86SOTA\u51c6\u786e\u7387\uff0c\u5e76\u5177\u5907\u9002\u7528\u4e8e\u5b9e\u65f6\u5185\u955c\u7684\u63a8\u7406\u901f\u5ea6\uff1b\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6587\u63d0\u51fa\u4e86MicroAUNet\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6ce8\u610f\u529b\u5206\u5272\u7f51\u7edc\uff0c\u901a\u8fc7\u6df1\u5ea6\u53ef\u5206\u79bb\u81a8\u80c0\u5377\u79ef\u548c\u5355\u8def\u5f84\u53c2\u6570\u5171\u4eab\u7684\u901a\u9053-\u7a7a\u95f4\u6ce8\u610f\u529b\u6a21\u5757\u6765\u589e\u5f3a\u591a\u5c3a\u5ea6\u8fb9\u754c\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u4ee5\u4ece\u9ad8\u5bb9\u91cf\u6559\u5e08\u6a21\u578b\u4f20\u9012\u8bed\u4e49\u548c\u8fb9\u754c\u4fe1\u606f\uff0c\u8fbe\u5230\u4e86\u5728\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\u4e0b\u7684SOTA\u7cbe\u5ea6\uff0c\u9002\u5408\u5b9e\u65f6\u4e34\u5e8a\u606f\u8089\u5206\u5272\u3002"}}
{"id": "2511.01163", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01163", "abs": "https://arxiv.org/abs/2511.01163", "authors": ["Yongyuan Liang", "Wei Chow", "Feng Li", "Ziqiao Ma", "Xiyao Wang", "Jiageng Mao", "Jiuhai Chen", "Jiatao Gu", "Yue Wang", "Furong Huang"], "title": "ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation", "comment": "Project Page: https://roverbench.github.io/", "summary": "Unified multimodal models (UMMs) have emerged as a powerful paradigm for\nseamlessly unifying text and image understanding and generation. However,\nprevailing evaluations treat these abilities in isolation, such that tasks with\nmultimodal inputs and outputs are scored primarily through unimodal reasoning,\ni.e., textual benchmarks emphasize language-based reasoning, while visual\nbenchmarks emphasize reasoning outcomes manifested in the pixels. We introduce\nROVER to address this pressing need to test reciprocal cross-modal reasoning,\nthe use of one modality to guide, verify, or refine outputs in the other, an\nability central to the vision of unified multimodal intelligence. ROVER is a\nhuman-annotated benchmark that explicitly targets reciprocal cross-modal\nreasoning, which contains 1312 tasks grounded in 1876 images, spanning two\ncomplementary settings. Verbally-augmented reasoning for visual generation\nevaluates whether models can use verbal prompts and reasoning chains to guide\nfaithful image synthesis. Visually-augmented reasoning for verbal generation\nevaluates whether models can generate intermediate visualizations that\nstrengthen their own reasoning processes for question answering. Experiments on\n17 unified models reveal two key findings: (i) Cross-modal reasoning determines\nvisual generation quality, with interleaved models significantly outperforming\nnon-interleaved ones; notably, combining strong unimodal models fails to\nachieve comparable reasoning. (ii) Models show dissociation between physical\nand symbolic reasoning: they succeed at interpreting perceptual concepts\nliterally but fail to construct visual abstractions for symbolic tasks, where\nfaulty reasoning harms performance. These results highlight reciprocal\ncross-modal reasoning as a critical frontier for enabling true omnimodal\ngeneration.", "AI": {"tldr": "ROVER\u662f\u9996\u4e2a\u805a\u7126\u4e92\u60e0\u8de8\u6a21\u6001\u63a8\u7406\u7684\u4eba\u5de5\u6ce8\u91ca\u57fa\u51c6\uff0c\u663e\u793a\u51fa\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u5728\u7528\u8bed\u8a00\u63a8\u52a8\u89c6\u89c9\u751f\u6210\u548c\u7528\u89c6\u89c9\u5f3a\u5316\u8bed\u8a00\u63a8\u7406\u4e24\u65b9\u9762\u7684\u5c40\u9650\uff0c\u5f3a\u8c03\u6784\u5efa\u4ea4\u9519\u5f0f\u878d\u5408\u4e0e\u89c6\u89c9\u62bd\u8c61\u80fd\u529b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u6d4b\u5c06\u6587\u672c\u4e0e\u56fe\u50cf\u80fd\u529b\u5b64\u7acb\u8003\u5bdf\uff0c\u5ffd\u89c6\u4e86\u8de8\u6a21\u6001\u76f8\u4e92\u5229\u7528\uff08\u7528\u4e00\u79cd\u6a21\u6001\u6307\u5bfc/\u9a8c\u8bc1\u53e6\u4e00\u6a21\u6001\uff09\u7684\u5173\u952e\u80fd\u529b\u3002\u4e3a\u5b9e\u73b0\u771f\u6b63\u7684\u7edf\u4e00\u591a\u6a21\u6001\u667a\u80fd\uff0c\u9700\u8981\u4e13\u95e8\u57fa\u51c6\u68c0\u9a8c\u4e92\u60e0\u8de8\u6a21\u6001\u63a8\u7406\u3002", "method": "\u6784\u5efa\u5e76\u4eba\u5de5\u6ce8\u91ca\u4e86ROVER\u57fa\u51c6\uff081312\u4e2a\u4efb\u52a1\uff0c1876\u5f20\u56fe\uff09\uff0c\u5305\u542b\u4e24\u79cd\u4e92\u8865\u4efb\u52a1\u8bbe\u7f6e\uff1a\u7528\u8bed\u8a00\u589e\u5f3a\u7684\u89c6\u89c9\u751f\u6210\uff08\u9a8c\u8bc1\u8bed\u8a00/\u63a8\u7406\u94fe\u662f\u5426\u80fd\u6307\u5bfc\u56fe\u50cf\u5408\u6210\uff09\u548c\u7528\u89c6\u89c9\u589e\u5f3a\u7684\u8bed\u8a00\u751f\u6210\uff08\u8bc4\u4f30\u751f\u6210\u4e2d\u95f4\u53ef\u89c6\u5316\u6765\u5f3a\u5316\u95ee\u7b54\u63a8\u7406\uff09\uff1b\u572817\u4e2a\u7edf\u4e00\u6a21\u578b\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u6d4b\u5e76\u5bf9\u6bd4\u4ea4\u9519\u4e0e\u975e\u4ea4\u9519\u67b6\u6784\u53ca\u7ec4\u5408\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1) \u8de8\u6a21\u6001\u63a8\u7406\u76f4\u63a5\u51b3\u5b9a\u89c6\u89c9\u751f\u6210\u8d28\u91cf\uff0c\u4ea4\u9519\u5f0f\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u4ea4\u9519\u5f0f\uff1b2) \u6a21\u578b\u5728\u7269\u7406/\u611f\u77e5\u5c42\u9762\u63a8\u7406\u80fd\u529b\u8f83\u5f3a\uff0c\u4f46\u5728\u7b26\u53f7\u62bd\u8c61\u6784\u5efa\u65b9\u9762\u5931\u8d25\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "ROVER\u63ed\u793a\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u6a21\u578b\u5728\u4e92\u60e0\u8de8\u6a21\u6001\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u5c24\u5176\u5728\u6784\u5efa\u89c6\u89c9\u62bd\u8c61\u4ee5\u652f\u6301\u7b26\u53f7\u63a8\u7406\u65f6\u8868\u73b0\u4e0d\u8db3\uff1b\u4ea4\u9519\u5f0f\u6a21\u578b\u5728\u7528\u8bed\u8a00\u5f15\u5bfc\u89c6\u89c9\u751f\u6210\u4e0a\u4f18\u4e8e\u975e\u4ea4\u9519\u6a21\u578b\uff0c\u800c\u7b80\u5355\u7ec4\u5408\u5f3a\u5927\u7684\u5355\u6a21\u6001\u6a21\u578b\u65e0\u6cd5\u5339\u654c\u3002"}}
{"id": "2511.01169", "categories": ["cs.CV", "I.2.10; I.4.5"], "pdf": "https://arxiv.org/pdf/2511.01169", "abs": "https://arxiv.org/abs/2511.01169", "authors": ["Brian Nlong Zhao", "Jiajun Wu", "Shangzhe Wu"], "title": "Web-Scale Collection of Video Data for 4D Animal Reconstruction", "comment": "NeurIPS 2025 Datasets and Benchmarks", "summary": "Computer vision for animals holds great promise for wildlife research but\noften depends on large-scale data, while existing collection methods rely on\ncontrolled capture setups. Recent data-driven approaches show the potential of\nsingle-view, non-invasive analysis, yet current animal video datasets are\nlimited--offering as few as 2.4K 15-frame clips and lacking key processing for\nanimal-centric 3D/4D tasks. We introduce an automated pipeline that mines\nYouTube videos and processes them into object-centric clips, along with\nauxiliary annotations valuable for downstream tasks like pose estimation,\ntracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos\n(2M frames)--an order of magnitude more than prior works. To demonstrate its\nutility, we focus on the 4D quadruped animal reconstruction task. To support\nthis task, we present Animal-in-Motion (AiM), a benchmark of 230 manually\nfiltered sequences with 11K frames showcasing clean, diverse animal motions. We\nevaluate state-of-the-art model-based and model-free methods on\nAnimal-in-Motion, finding that 2D metrics favor the former despite unrealistic\n3D shapes, while the latter yields more natural reconstructions but scores\nlower--revealing a gap in current evaluation. To address this, we enhance a\nrecent model-free approach with sequence-level optimization, establishing the\nfirst 4D animal reconstruction baseline. Together, our pipeline, benchmark, and\nbaseline aim to advance large-scale, markerless 4D animal reconstruction and\nrelated tasks from in-the-wild videos. Code and datasets are available at\nhttps://github.com/briannlongzhao/Animal-in-Motion.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u81ea\u52a8\u5316YouTube\u6316\u6398\u4e0e\u5904\u7406\uff0c\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u52a8\u7269\u89c6\u9891\u8d44\u6e90\u5e76\u63d0\u51faAiM\u57fa\u51c6\uff0c\u63ed\u793a\u4e862D\u8bc4\u4ef7\u4e0e3D\u771f\u5b9e\u611f\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5e76\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u5efa\u7acb\u9996\u4e2a4D\u56db\u8db3\u52a8\u7269\u91cd\u5efa\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u52a8\u7269\u89c6\u9891\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u7f3a\u4e4f\u9762\u54113D/4D\u4efb\u52a1\u7684\u5904\u7406\uff0c\u9650\u5236\u4e86\u65e0\u521b\u3001\u5355\u89c6\u89d2\u52a8\u7269\u4e09\u56db\u7ef4\u5206\u6790\u7684\u53d1\u5c55\uff1b\u9700\u5927\u89c4\u6a21\u3001\u975e\u53d7\u63a7\u73af\u5883\u6570\u636e\u4e0e\u8bc4\u4ef7\u57fa\u51c6\u6765\u63a8\u8fdb\u7814\u7a76\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u722c\u53d6\u4e0e\u5904\u7406\u6d41\u7a0b\uff1a\u4eceYouTube\u91c7\u96c6\u89c6\u9891\uff0c\u8fdb\u884c\u7269\u4f53\u4e2d\u5fc3\u5316\u88c1\u526a\u3001\u5173\u952e\u5e27/\u8ffd\u8e2a\u3001\u9644\u52a0\u6ce8\u91ca\u751f\u6210\uff1b\u6536\u96c630K\u89c6\u9891\uff08\u7ea62M\u5e27\uff09\uff1b\u4eba\u5de5\u7b5b\u9009\u51fa230\u6761\u9ad8\u8d28\u91cf\u5e8f\u5217\u7ec4\u6210AiM\uff0811K\u5e27\uff09\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\u6539\u8fdb\u73b0\u6709\u65e0\u6a21\u578b\u65b9\u6cd5\uff0c\u52a0\u5165\u5e8f\u5217\u7ea7\u4f18\u5316\uff0c\u5f62\u6210\u9996\u4e2a4D\u91cd\u5efa\u57fa\u7ebf\u5e76\u4e0e\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u6bd4\u8f83\u3002", "result": "\u6784\u5efa\u5e76\u516c\u5f00\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0830K\u89c6\u9891\u30012M\u5e27\uff09\u4e0eAiM\u57fa\u51c6\uff08230\u5e8f\u5217\u300111K\u5e27\uff09\uff1b\u5b9e\u9a8c\u8bc1\u660e\u5f53\u524d\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u504f\u5dee\u2014\u20142D\u6307\u6807\u504f\u597d\u6a21\u578b\u9a71\u52a8\u65b9\u6cd5\u4f46\u4ea7\u751f\u4e0d\u771f\u5b9e3D\u5f62\u72b6\uff1b\u6539\u8fdb\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\u901a\u8fc7\u5e8f\u5217\u7ea7\u4f18\u5316\u5728\u81ea\u7136\u6027\u4e0e\u4e00\u81f4\u6027\u4e0a\u53d6\u5f97\u66f4\u597d\u8868\u73b0\uff0c\u5efa\u7acb\u4e864D\u91cd\u5efa\u521d\u59cb\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4eceYouTube\u6316\u6398\u5e76\u5904\u7406\u52a8\u7269\u89c6\u9891\uff0c\u751f\u6210\u9762\u5411\u5bf9\u8c61\u7684\u526a\u8f91\u53ca\u8f85\u52a9\u6ce8\u91ca\uff0c\u663e\u8457\u6269\u5145\u4e86\u52a8\u7269\u89c6\u9891\u6570\u636e\u89c4\u6a21\uff1b\u57fa\u4e8e\u6b64\u6784\u5efa\u4e86AiM\u57fa\u51c6\uff08Animal-in-Motion\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a4D\u56db\u8db3\u52a8\u7269\u91cd\u5efa\u57fa\u7ebf\u3002"}}
{"id": "2511.01175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01175", "abs": "https://arxiv.org/abs/2511.01175", "authors": ["Peng Du", "Hui Li", "Han Xu", "Paul Barom Jeon", "Dongwook Lee", "Daehyun Ji", "Ran Yang", "Feng Zhu"], "title": "Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution", "comment": null, "summary": "Discrete Wavelet Transform (DWT) has been widely explored to enhance the\nperformance of image superresolution (SR). Despite some DWT-based methods\nimproving SR by capturing fine-grained frequency signals, most existing\napproaches neglect the interrelations among multiscale frequency sub-bands,\nresulting in inconsistencies and unnatural artifacts in the reconstructed\nimages. To address this challenge, we propose a Diffusion Transformer model\nbased on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the\nsuperiority of diffusion models and transformers to capture the interrelations\namong multiscale frequency sub-bands, leading to a more consistence and\nrealistic SR image. Specifically, we use a Multi-level Discrete Wavelet\nTransform (MDWT) to decompose images into wavelet spectra. A pyramid\ntokenization method is proposed which embeds the spectra into a sequence of\ntokens for transformer model, facilitating to capture features from both\nspatial and frequency domain. A dual-decoder is designed elaborately to handle\nthe distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,\nwithout omitting their alignment in image generation. Extensive experiments on\nmultiple benchmark datasets demonstrate the effectiveness of our method, with\nhigh performance on both perception quality and fidelity.", "AI": {"tldr": "DTWSR\uff1a\u57fa\u4e8e\u591a\u5c42\u5c0f\u6ce2\u8c31\u7684\u6269\u6563Transformer\uff0c\u7528\u91d1\u5b57\u5854\u5206\u8bcd\u4e0e\u53cc\u89e3\u7801\u5668\u5efa\u6a21\u591a\u5c3a\u5ea6\u9891\u5e26\u5173\u7cfb\uff0c\u63d0\u9ad8\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u611f\u77e5\u4e0e\u4fdd\u771f\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDWT\u7684\u65b9\u6cd5\u591a\u5173\u6ce8\u6355\u6349\u7ec6\u7c92\u5ea6\u9891\u7387\u4fe1\u53f7\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e0d\u540c\u5c3a\u5ea6\u9891\u7387\u5b50\u5e26\u4e4b\u95f4\u7684\u76f8\u4e92\u5173\u7cfb\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u51fa\u73b0\u4e0d\u4e00\u81f4\u6027\u548c\u4f2a\u5f71\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8054\u5408\u5efa\u6a21\u591a\u5c3a\u5ea6\u5b50\u5e26\u5173\u7cfb\u6765\u6539\u5584\u8d85\u5206\u8fa8\u7387\u7ed3\u679c\u7684\u81ea\u7136\u6027\u4e0e\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u591a\u5c42\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362\uff08MDWT\uff09\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u6ce2\u8c31\uff0c\u63d0\u51fa\u91d1\u5b57\u5854\u5206\u8bcd\uff08pyramid tokenization\uff09\u5c06\u5404\u5b50\u5e26\u5d4c\u5165\u4e3a\u5e8f\u5217token\u8f93\u5165Transformer\uff1b\u91c7\u7528\u6269\u6563\u6a21\u578b\u6846\u67b6\u751f\u6210\u56fe\u50cf\uff1b\u8bbe\u8ba1\u53cc\u89e3\u7801\u5668\u5206\u522b\u5904\u7406\u4f4e\u9891\u4e0e\u9ad8\u9891\u5b50\u5e26\uff0c\u540c\u65f6\u4fdd\u6301\u5b83\u4eec\u5728\u751f\u6210\u4e2d\u7684\u5bf9\u9f50\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eDTWSR\u5728\u611f\u77e5\u8d28\u91cf\u548c\u4fdd\u771f\u5ea6\u4e24\u4e2a\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\uff0c\u8868\u73b0\u4f18\u4e8e\u82e5\u5e72\u5bf9\u6bd4\u65b9\u6cd5\uff08\u6587\u4e2d\u5ba3\u79f0\u4f46\u6458\u8981\u672a\u7ed9\u51fa\u5177\u4f53\u6570\u503c\uff09\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06\u6269\u6563\u6a21\u578b\u3001Transformer\u4e0e\u591a\u5c42\u5c0f\u6ce2\u53d8\u6362\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86DTWSR\u7528\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff0c\u65e8\u5728\u5efa\u6a21\u591a\u5c3a\u5ea6\u9891\u5e26\u4e4b\u95f4\u7684\u5185\u5728\u5173\u8054\u4ee5\u63d0\u5347\u91cd\u5efa\u4e00\u81f4\u6027\u4e0e\u611f\u77e5\u8d28\u91cf\u3002"}}
{"id": "2511.01194", "categories": ["cs.CV", "cs.AI", "68T07 (Artificial neural networks and deep learning), 68U10\n  (Computer graphics, computational geometry)"], "pdf": "https://arxiv.org/pdf/2511.01194", "abs": "https://arxiv.org/abs/2511.01194", "authors": ["Minmin Zeng"], "title": "A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment", "comment": "10 pages, 5 figures. Submitted as a computer vision paper in the\n  cs.CV category", "summary": "Action Quality Assessment (AQA) requires fine-grained understanding of human\nmotion and precise evaluation of pose similarity. This paper proposes a\ntopology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,\nwhich models the human skeleton as a graph to learn discriminative,\ntopology-sensitive pose embeddings. Using a Siamese architecture trained with a\ncontrastive regression objective, our method outperforms coordinate-based\nbaselines and achieves competitive performance on AQA-7 and FineDiving\nbenchmarks. Experimental results and ablation studies validate the\neffectiveness of leveraging skeletal topology for pose similarity and action\nquality assessment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9aa8\u67b6\u62d3\u6251\u7684GCN-PSN\u4e0e\u5b6a\u751f\u5bf9\u6bd4\u56de\u5f52\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u59ff\u6001\u76f8\u4f3c\u5ea6\u5b66\u4e60\u4e0e\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\uff0c\u5728AQA-7\u548cFineDiving\u4e0a\u8868\u73b0\u826f\u597d\u3002", "motivation": "AQA\u9700\u8981\u5bf9\u4eba\u4f53\u5fae\u5c0f\u4e14\u590d\u6742\u7684\u8fd0\u52a8\u5dee\u5f02\u8fdb\u884c\u7cbe\u7ec6\u7406\u89e3\uff0c\u4f20\u7edf\u57fa\u4e8e\u5750\u6807\u7684\u65b9\u6cd5\u5ffd\u89c6\u9aa8\u67b6\u62d3\u6251\u7ed3\u6784\uff0c\u5bfc\u81f4\u5bf9\u59ff\u6001\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u4e0d\u8db3\uff1b\u56e0\u6b64\u5e0c\u671b\u5229\u7528\u9aa8\u67b6\u62d3\u6251\u4fe1\u606f\u6765\u63d0\u5347\u5d4c\u5165\u7684\u5224\u522b\u6027\u548c\u8bc4\u4f30\u7cbe\u5ea6\u3002", "method": "\u5c06\u4eba\u4f53\u9aa8\u67b6\u5efa\u6a21\u4e3a\u56fe\u7ed3\u6784\uff0c\u4f7f\u7528\u62d3\u6251\u611f\u77e5\u7684GCN\u5b66\u4e60\u5224\u522b\u6027\u4e14\u5bf9\u62d3\u6251\u654f\u611f\u7684\u59ff\u6001\u5d4c\u5165\uff1b\u91c7\u7528Siamese\uff08\u5b6a\u751f\uff09\u7ed3\u6784\u5e76\u4ee5\u5bf9\u6bd4\u56de\u5f52\uff08contrastive regression\uff09\u76ee\u6807\u8bad\u7ec3\uff0c\u4ee5\u76f4\u63a5\u5b66\u4e60\u59ff\u6001\u95f4\u76f8\u4f3c\u5ea6\u4e0e\u52a8\u4f5c\u8d28\u91cf\u8bc4\u5206\u6620\u5c04\u3002", "result": "\u5728AQA-7\u548cFineDiving\u57fa\u51c6\u4e0a\uff0cGCN-PSN\u4f18\u4e8e\u5750\u6807\u57fa\u7ebf\u65b9\u6cd5\u5e76\u53d6\u5f97\u7ade\u4e89\u6027\u8868\u73b0\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u5f15\u5165\u9aa8\u67b6\u62d3\u6251\u548cGCN\u7ed3\u6784\u5bf9\u59ff\u6001\u76f8\u4f3c\u5ea6\u548c\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\u5747\u6709\u663e\u8457\u8d21\u732e\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5f15\u5165\u62d3\u6251\u611f\u77e5\u7684\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN-PSN\uff09\uff0c\u5728\u57fa\u4e8e\u9aa8\u67b6\u7684\u4eba\u4f53\u52a8\u4f5c\u8d28\u91cf\u8bc4\u4f30\uff08AQA\uff09\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u59ff\u6001\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u8bc4\u5206\u6027\u80fd\uff0c\u4f18\u4e8e\u5750\u6807\u57fa\u7ebf\u5e76\u5728AQA-7\u4e0eFineDiving\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.01200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01200", "abs": "https://arxiv.org/abs/2511.01200", "authors": ["Mengyuan Liu", "Sheng Yan", "Yong Wang", "Yingjie Li", "Gui-Bin Bian", "Hong Liu"], "title": "MoSa: Motion Generation with Scalable Autoregressive Modeling", "comment": null, "summary": "We introduce MoSa, a novel hierarchical motion generation framework for\ntext-driven 3D human motion generation that enhances the Vector\nQuantization-guided Generative Transformers (VQ-GT) paradigm through a\ncoarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale\nToken Preservation Strategy (MTPS) integrated into a hierarchical residual\nvector quantization variational autoencoder (RQ-VAE). MTPS employs\ninterpolation at each hierarchical quantization to effectively retain\ncoarse-to-fine multi-scale tokens. With this, the generative transformer\nsupports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,\nunlike traditional methods that predict only one token at each step.\nConsequently, MoSa requires only 10 inference steps, matching the number of\nRQ-VAE quantization layers. To address potential reconstruction degradation\nfrom frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive\nconvolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and\nincorporates attention mechanisms to better capture global dependencies.\nExtensive experiments show that MoSa achieves state-of-the-art generation\nquality and efficiency, outperforming prior methods in both fidelity and speed.\nOn the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)\nwhile reducing inference time by 27 percent. Moreover, MoSa generalizes well to\ndownstream tasks such as motion editing, requiring no additional fine-tuning.\nThe code is available at https://mosa-web.github.io/MoSa-web", "AI": {"tldr": "MoSa\u901a\u8fc7\u5c42\u7ea7\u6b8b\u5dee\u91cf\u5316\u4e0e\u591a\u5c3a\u5ea6token\u4fdd\u7559\uff0c\u5e76\u7ed3\u5408\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u751f\u6210\u4e0eCAQ-VAE\uff0c\u5b9e\u73b0\u5728\u8d28\u91cf\uff08FID 0.06\uff09\u4e0e\u6548\u7387\uff08\u63a8\u7406\u5feb27%\uff09\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5177\u5907\u826f\u597d\u6cdb\u5316\u6027\u3002", "motivation": "\u63d0\u5347VQ-GT\u8303\u5f0f\u5728\u6587\u672c\u52303D\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u51cf\u5c11\u81ea\u56de\u5f52\u6b65\u6570\u540c\u65f6\u4fdd\u6301\u6216\u589e\u5f3a\u91cd\u5efa\u4e0e\u751f\u6210\u4fdd\u771f\u5ea6\uff0c\u5e76\u4f7f\u6a21\u578b\u6613\u4e8e\u6cdb\u5316\u5230\u7f16\u8f91\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86MTPS\uff08\u591a\u5c3a\u5ea6token\u4fdd\u7559\u7b56\u7565\uff09\u4e0e\u5c42\u7ea7\u6b8b\u5dee\u5411\u91cf\u91cf\u5316VAE\uff08RQ-VAE\uff09\uff0c\u5728\u6bcf\u5c42\u91cf\u5316\u65f6\u8fdb\u884c\u63d2\u503c\u4ee5\u4fdd\u7559\u7c97\u7ec6\u7c92\u5ea6token\uff1b\u5f15\u5165\u53ef\u6269\u5c55\u81ea\u56de\u5f52\uff08SAR\uff09\u751f\u6210\u5668\u4e00\u6b21\u9884\u6d4b\u591a\u5c3a\u5ea6token\uff0c\u4ece\u800c\u5c06\u63a8\u7406\u6b65\u9aa4\u51cf\u5c11\u5230\u4e0e\u91cf\u5316\u5c42\u6570\u4e00\u81f4\uff0810\u6b65\uff09\uff1b\u4e3a\u5e94\u5bf9\u63d2\u503c\u5e26\u6765\u7684\u91cd\u5efa\u9000\u5316\uff0c\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u4f46 expressive \u7684\u5377\u79ef-\u6ce8\u610f\u529b\u6df7\u5408VQ-VAE\uff08CAQ-VAE\uff09\uff0c\u6539\u8fdb\u6b8b\u5dee\u6a21\u5757\u5e76\u52a0\u5165\u6ce8\u610f\u529b\u4ee5\u6355\u6349\u5168\u5c40\u4f9d\u8d56\u3002", "result": "\u5728Motion-X\u6570\u636e\u96c6\u4e0a\u53d6\u5f97FID=0.06\uff0c\u4f18\u4e8eMoMask\u76840.20\uff1b\u63a8\u7406\u65f6\u95f4\u51cf\u5c1127%\uff1b\u5728\u8d28\u91cf\u4e0e\u901f\u5ea6\u4e0a\u5747\u8d85\u8d8a\u6b64\u524d\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u7528\u4e8e\u52a8\u4f5c\u7f16\u8f91\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "MoSa\u901a\u8fc7\u591a\u5c3a\u5ea6\u5206\u5c42\u91cf\u5316\u548c\u53ef\u6269\u5c55\u81ea\u56de\u5f52\u5efa\u6a21\u5728\u6587\u672c\u9a71\u52a8\u76843D\u4eba\u4f53\u52a8\u4f5c\u751f\u6210\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\uff0c\u517c\u987e\u8d28\u91cf\u4e0e\u901f\u5ea6\uff0c\u5e76\u5177\u6709\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01210", "abs": "https://arxiv.org/abs/2511.01210", "authors": ["Heyu Guo", "Shanmu Wang", "Ruichun Ma", "Shiqi Jiang", "Yasaman Ghasempour", "Omid Abari", "Baining Guo", "Lili Qi"], "title": "OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA", "comment": null, "summary": "Vision-language-action (VLA) models have shown strong generalization for\naction prediction through large-scale vision-language pretraining. However,\nmost existing models rely solely on RGB cameras, limiting their perception and,\nconsequently, manipulation capabilities. We present OmniVLA, an omni-modality\nVLA model that integrates novel sensing modalities for physically-grounded\nspatial intelligence beyond RGB perception. The core of our approach is the\nsensor-masked image, a unified representation that overlays spatially grounded\nand physically meaningful masks onto the RGB images, derived from sensors\nincluding an infrared camera, a mmWave radar, and a microphone array. This\nimage-native unification keeps sensor input close to RGB statistics to\nfacilitate training, provides a uniform interface across sensor hardware, and\nenables data-efficient learning with lightweight per-sensor projectors. Built\non this, we present a multisensory vision-language-action model architecture\nand train the model based on an RGB-pretrained VLA backbone. We evaluate\nOmniVLA on challenging real-world tasks where sensor-modality perception is\nneeded to guide the manipulation. OmniVLA achieves an average task success rate\nof 84%, significantly outperforms both RGB-only and raw-sensor-input baseline\nmodels by 59% and 28% respectively, meanwhile showing higher learning\nefficiency and stronger generalization capability.", "AI": {"tldr": "\u5c06\u591a\u79cd\u7269\u7406\u4f20\u611f\u5668\u4fe1\u606f\u4ee5\u906e\u7f69\u56fe\u50cf\u53e0\u52a0\u5230RGB\u4e0a\uff0c\u8bad\u7ec3\u57fa\u4e8eRGB\u9884\u8bad\u7ec3\u9aa8\u5e72\u7684\u591a\u611f\u77e5VLA\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5927\u591a\u4ec5\u4f9d\u8d56RGB\u611f\u77e5\uff0c\u9650\u5236\u4e86\u5bf9\u7269\u7406\u4e16\u754c\u7684\u611f\u77e5\u80fd\u529b\u548c\u64cd\u4f5c\u6027\u80fd\uff1b\u901a\u8fc7\u5f15\u5165\u7269\u7406\u4f20\u611f\u5668\u6a21\u6001\uff08\u7ea2\u5916\u3001\u96f7\u8fbe\u3001\u58f0\u5b66\uff09\u5e76\u4ee5\u56fe\u50cf\u539f\u751f\u65b9\u5f0f\u7edf\u4e00\u8868\u793a\uff0c\u53ef\u4ee5\u589e\u5f3a\u7a7a\u95f4\u548c\u7269\u7406\u611f\u77e5\uff0c\u63d0\u5347\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "method": "\u63d0\u51fa\u4f20\u611f\u5668\u906e\u7f69\u56fe\u50cf\uff08sensor-masked image\uff09\u4f5c\u4e3a\u7edf\u4e00\u8868\u793a\uff0c\u5c06\u7ea2\u5916\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001\u9ea6\u514b\u98ce\u9635\u5217\u7b49\u4f20\u611f\u5668\u7684\u7a7a\u95f4\u4fe1\u606f\u4ee5\u53ef\u53e0\u52a0\u7684\u906e\u7f69\u5f62\u5f0f\u6295\u5f71\u5230RGB\u56fe\u50cf\uff0c\u4ece\u800c\u4fdd\u6301\u56fe\u50cf\u7edf\u8ba1\u7279\u6027\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u6bcf\u4f20\u611f\u5668\u6295\u5f71\u5668\u4e0e\u57fa\u4e8eRGB\u9884\u8bad\u7ec3\u7684VLA\u9aa8\u5e72\u7ed3\u5408\u8bad\u7ec3\u591a\u611f\u77e5\u6a21\u578b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u9700\u8981\u4f20\u611f\u5668\u611f\u77e5\u4ee5\u5f15\u5bfc\u64cd\u4f5c\u7684\u4efb\u52a1\u4e0a\uff0cOmniVLA\u5e73\u5747\u4efb\u52a1\u6210\u529f\u738784%\uff0c\u5206\u522b\u6bd4\u4ec5\u7528RGB\u7684\u57fa\u7ebf\u9ad8\u51fa59%\u548c\u6bd4\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u4f20\u611f\u5668\u8f93\u5165\u7684\u57fa\u7ebf\u9ad8\u51fa28%\uff0c\u5e76\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OmniVLA\u901a\u8fc7\u5c06\u591a\u6a21\u6001\u4f20\u611f\u5668\u4fe1\u606f\u4ee5\u4f20\u611f\u5668\u906e\u7f69\u56fe\u50cf\u7684\u65b9\u5f0f\u4e0eRGB\u56fe\u50cf\u7edf\u4e00\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7269\u7406\u611f\u77e5\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u4efb\u52a1\u6210\u529f\u7387\u3001\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01213", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01213", "abs": "https://arxiv.org/abs/2511.01213", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Parijat Deshpande", "Venkataramana Runkana"], "title": "Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering", "comment": "10 pages, 11 figures, 6 tables", "summary": "The immense diversity in the culture and culinary of Indian cuisines calls\nattention to the major shortcoming of the existing Visual Question\nAnswering(VQA) systems which are inclined towards the foods from Western\nregion. Recent attempt towards building a VQA dataset for Indian food is a step\ntowards addressing this challenge. However, their approach towards VQA follows\na two-step process in which the answer is generated first, followed by the\nexplanation of the expected answer. In this work, we claim that food VQA\nrequires to follow a multi-step reasoning process to arrive at an accurate\nanswer, especially in the context of India food, which involves understanding\ncomplex culinary context and identifying relationships between various food\nitems. With this hypothesis we create reasoning chains upon the QA with minimal\nhuman intervention. We fine-tune smaller LLMs and VLMs with auto-validated\nreasoning chains and further train them using reinforcement learning with\nlarger data. With augmentation of reasoning chains, we observed accuracy\nimprovement of an average 10 percentage points on the baseline. We provide\ndetailed analysis in terms the effect of addition of reasoning chains for the\nIndian Food VQA task.\n  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge\nGraph.", "AI": {"tldr": "\u9488\u5bf9\u5370\u5ea6\u98df\u54c1VQA\uff0c\u4f5c\u8005\u81ea\u52a8\u6784\u5efa\u5e76\u9a8c\u8bc1\u591a\u6b65\u63a8\u7406\u94fe\u7528\u4e8e\u5fae\u8c03\u4e0e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u540e\u4f7f\u5c0f\u578b\u6a21\u578b\u6027\u80fd\u5e73\u5747\u63d0\u5347\u7ea610%\u3002", "motivation": "\u73b0\u6709VQA\u7cfb\u7edf\u504f\u5411\u897f\u65b9\u98df\u7269\uff0c\u5370\u5ea6\u7f8e\u98df\u591a\u6837\u5316\u4e0e\u590d\u6742\u7684\u70f9\u996a\u8bed\u5883\u9700\u8981\u591a\u6b65\u63a8\u7406\u6765\u8bc6\u522b\u98df\u7269\u95f4\u5173\u7cfb\u4e0e\u80cc\u666f\u77e5\u8bc6\uff0c\u5355\u6b65\u751f\u6210\u7b54\u6848\u5e76\u89e3\u91ca\u7684\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u6355\u6349\u8fd9\u4e9b\u590d\u6742\u6027\u3002", "method": "\u57fa\u4e8e\u5df2\u6709\u5370\u5ea6\u98df\u54c1VQA\u6570\u636e\uff0c\u81ea\u52a8\u6784\u5efa\u6700\u5c0f\u4eba\u5de5\u5e72\u9884\u7684\u63a8\u7406\u94fe\uff08reasoning chains\uff09\uff0c\u7528\u4e8e\u5fae\u8c03\u8f83\u5c0f\u7684LLM\u548cVLM\uff0c\u968f\u540e\u4f7f\u7528\u66f4\u5927\u89c4\u6a21\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u8bad\u7ec3\u6a21\u578b\uff1b\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5173\u7cfb\u7406\u89e3\uff0c\u5e76\u91c7\u7528\u81ea\u52a8\u9a8c\u8bc1\u673a\u5236\u7b5b\u9009\u9ad8\u8d28\u91cf\u63a8\u7406\u94fe\u3002", "result": "\u5728\u52a0\u5165\u63a8\u7406\u94fe\u7684\u589e\u5f3a\u8bad\u7ec3\u540e\uff0c\u6a21\u578b\u5728\u57fa\u7ebf\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u63d0\u5347\u7ea610\u4e2a\u767e\u5206\u70b9\uff1b\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u63a8\u7406\u94fe\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u7684\u8be6\u7ec6\u6d88\u878d\u4e0e\u5206\u6790\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5728\u5370\u5ea6\u98df\u54c1VQA\u4efb\u52a1\u4e2d\u5f15\u5165\u81ea\u52a8\u751f\u6210\u7684\u591a\u6b65\u63a8\u7406\u94fe\u5e76\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u578bLLM\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u51c6\u786e\u6027\u3002"}}
{"id": "2511.01223", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01223", "abs": "https://arxiv.org/abs/2511.01223", "authors": ["Zahra Mehraban", "Sebastien Glaser", "Michael Milford", "Ronald Schroeter"], "title": "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering", "comment": null, "summary": "Domain adaptation is required for automated driving models to generalize well\nacross diverse road conditions. This paper explores a training method for\ndomain adaptation to adapt PilotNet, an end-to-end deep learning-based model,\nfor left-hand driving conditions using real-world Australian highway data. Four\ntraining methods were evaluated: (1) a baseline model trained on U.S.\nright-hand driving data, (2) a model trained on flipped U.S. data, (3) a model\npretrained on U.S. data and then fine-tuned on Australian highways, and (4) a\nmodel pretrained on flipped U.S. data and then finetuned on Australian\nhighways. This setup examines whether incorporating flipped data enhances the\nmodel adaptation by providing an initial left-hand driving alignment. The paper\ncompares model performance regarding steering prediction accuracy and\nattention, using saliency-based analysis to measure attention shifts across\nsignificant road regions. Results show that pretraining on flipped data alone\nworsens prediction stability due to misaligned feature representations, but\nsignificantly improves adaptation when followed by fine-tuning, leading to\nlower prediction error and stronger focus on left-side cues. To validate this\napproach across different architectures, the same experiments were done on\nResNet, which confirmed similar adaptation trends. These findings emphasize the\nimportance of preprocessing techniques, such as flipped-data pretraining,\nfollowed by fine-tuning to improve model adaptation with minimal retraining\nrequirements.", "AI": {"tldr": "\u5728\u53f3\u4fa7\u9a7e\u9a76\u6570\u636e\u4e0a\u5148\u505a\u6c34\u5e73\u7ffb\u8f6c\u9884\u8bad\u7ec3\u3001\u518d\u5728\u5de6\u4fa7\u9a7e\u9a76\u76ee\u6807\u57df\u5fae\u8c03\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u57df\u9002\u5e94\u7b56\u7565\uff0c\u80fd\u63d0\u9ad8\u8f6c\u5411\u9884\u6d4b\u51c6\u786e\u6027\u5e76\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u5de6\u4fa7\u91cd\u8981\u8def\u51b5\u7279\u5f81\u3002", "motivation": "\u76ee\u6807\u662f\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5728\u4e0d\u540c\u9a7e\u9a76\u4fa7\uff08\u53f3\u4fa7->\u5de6\u4fa7\uff09\u548c\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u95f4\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a2\u7d22\u7b80\u5355\u7684\u6570\u636e\u9884\u5904\u7406\uff08\u6c34\u5e73\u7ffb\u8f6c\uff09\u662f\u5426\u80fd\u4f5c\u4e3a\u6709\u6548\u7684\u57df\u9002\u5e94\u8d77\u70b9\u4ee5\u51cf\u5c11\u5728\u76ee\u6807\u57df\u4e0a\u7684\u91cd\u8bad\u7ec3\u6210\u672c\u3002", "method": "\u6bd4\u8f83\u56db\u79cd\u8bad\u7ec3\u7b56\u7565\uff1a1) \u57fa\u7ebf\uff1a\u4ec5\u5728\u7f8e\u56fd\u53f3\u4fa7\u9a7e\u9a76\u6570\u636e\u8bad\u7ec3\uff1b2) \u5728\u7f8e\u56fd\u6570\u636e\u4e0a\u8fdb\u884c\u6c34\u5e73\u7ffb\u8f6c\u540e\u8bad\u7ec3\uff1b3) \u5728\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3\u540e\u5728\u6fb3\u5927\u5229\u4e9a\u6570\u636e\u5fae\u8c03\uff1b4) \u5728\u7ffb\u8f6c\u7684\u7f8e\u56fd\u6570\u636e\u9884\u8bad\u7ec3\u540e\u518d\u5728\u6fb3\u5927\u5229\u4e9a\u5fae\u8c03\u3002\u8bc4\u4f30\u6307\u6807\u4e3a\u8f6c\u5411\u9884\u6d4b\u8bef\u5dee\u4e0e\u57fa\u4e8e\u663e\u8457\u56fe\u7684\u6ce8\u610f\u529b\u5206\u6790\uff08\u5173\u6ce8\u91cd\u8981\u8def\u533a\u7684\u6ce8\u610f\u529b\u79fb\u4f4d\uff09\u3002\u5728\u4e24\u79cd\u7f51\u7edc\u67b6\u6784\uff08PilotNet\u4e0eResNet\uff09\u4e0a\u590d\u73b0\u5b9e\u9a8c\u4ee5\u9a8c\u8bc1\u901a\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\uff1a\u5355\u72ec\u7ffb\u8f6c\u9884\u8bad\u7ec3\u4f1a\u5f15\u5165\u7279\u5f81\u8868\u793a\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0b\u964d\uff1b\u4f46\u7ffb\u8f6c\u9884\u8bad\u7ec3\u518d\u5fae\u8c03\u80fd\u663e\u8457\u964d\u4f4e\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u4f7f\u6a21\u578b\u66f4\u96c6\u4e2d\u5173\u6ce8\u5de6\u4fa7\u8def\u51b5\u7ebf\u7d22\u3002ResNet\u4e0a\u7684\u91cd\u590d\u5b9e\u9a8c\u5c55\u793a\u4e86\u76f8\u4f3c\u7684\u8d8b\u52bf\uff0c\u8868\u660e\u65b9\u6cd5\u5177\u6709\u8de8\u67b6\u6784\u4e00\u5b9a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4f5c\u8005\u603b\u7ed3\uff1a\u5728\u8fc1\u79fb\u5230\u5de6\u4fa7\u9a7e\u9a76(\u6fb3\u5927\u5229\u4e9a)\u65f6\uff0c\u5148\u7528\u7ffb\u8f6c\u7684\u53f3\u4fa7\u9a7e\u9a76\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u76ee\u6807\u57df\u4e0a\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u578b\u7684\u8f6c\u79fb\u6027\u80fd\u548c\u6ce8\u610f\u529b\u96c6\u4e2d\u4e8e\u5de6\u4fa7\u7ebf\u7d22\uff0c\u4ece\u800c\u964d\u4f4e\u8f6c\u5411\u9884\u6d4b\u8bef\u5dee\u3002\u5355\u72ec\u7528\u7ffb\u8f6c\u6570\u636e\u9884\u8bad\u7ec3\u4f1a\u56e0\u7279\u5f81\u9519\u914d\u5bfc\u81f4\u9884\u6d4b\u4e0d\u7a33\u5b9a\uff0c\u4f46\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6b65\u9aa4\u7ed3\u5408\u5fae\u8c03\u5219\u6548\u679c\u6700\u4f73\u3002\u8be5\u7ed3\u8bba\u5bf9\u4e0d\u540c\u67b6\u6784\uff08PilotNet\u3001ResNet\uff09\u5747\u6210\u7acb\uff0c\u5f3a\u8c03\u9884\u5904\u7406\u4e0e\u5fae\u8c03\u7ed3\u5408\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2511.01233", "categories": ["cs.CV", "cs.GR", "cs.HC", "I.3; I.2"], "pdf": "https://arxiv.org/pdf/2511.01233", "abs": "https://arxiv.org/abs/2511.01233", "authors": ["Rajmund Nagy", "Hendric Voss", "Thanh Hoang-Minh", "Mihail Tsakov", "Teodor Nikolov", "Zeyi Zhang", "Tenglong Ao", "Sicheng Yang", "Shaoli Huang", "Yongkang Cheng", "M. Hamza Mughal", "Rishabh Dabral", "Kiran Chhatre", "Christian Theobalt", "Libin Liu", "Stefan Kopp", "Rachel McDonnell", "Michael Neff", "Taras Kucherenko", "Youngwoo Yoon", "Gustav Eje Henter"], "title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark", "comment": "23 pages, 10 figures. The last two authors made equal contributions", "summary": "We review human evaluation practices in automated, speech-driven 3D gesture\ngeneration and find a lack of standardisation and frequent use of flawed\nexperimental setups. This leads to a situation where it is impossible to know\nhow different methods compare, or what the state of the art is. In order to\naddress common shortcomings of evaluation design, and to standardise future\nuser studies in gesture-generation works, we introduce a detailed human\nevaluation protocol for the widely-used BEAT2 motion-capture dataset. Using\nthis protocol, we conduct large-scale crowdsourced evaluation to rank six\nrecent gesture-generation models -- each trained by its original authors --\nacross two key evaluation dimensions: motion realism and speech-gesture\nalignment. Our results provide strong evidence that 1) newer models do not\nconsistently outperform earlier approaches; 2) published claims of high motion\nrealism or speech-gesture alignment may not hold up under rigorous evaluation;\nand 3) the field must adopt disentangled assessments of motion quality and\nmultimodal alignment for accurate benchmarking in order to make progress.\nFinally, in order to drive standardisation and enable new evaluation research,\nwe will release five hours of synthetic motion from the benchmarked models;\nover 750 rendered video stimuli from the user studies -- enabling new\nevaluations without model reimplementation required -- alongside our\nopen-source rendering script, and the 16,000 pairwise human preference votes\ncollected for our benchmark.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u4e3aBEAT2\u6570\u636e\u96c6\u5236\u5b9a\u89c4\u8303\u5316\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u5e76\u8fdb\u884c\u5927\u89c4\u6a21\u4f17\u5305\u8bc4\u6d4b\uff0c\u63ed\u793a\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u6df7\u4e71\u95ee\u9898\uff0c\u8bc1\u5b9e\u8fd1\u671f\u6a21\u578b\u5e76\u975e\u59cb\u7ec8\u4f18\u4e8e\u65e9\u671f\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9700\u5206\u79bb\u8bc4\u4f30\u8fd0\u52a8\u8d28\u91cf\u4e0e\u8bed\u97f3-\u624b\u52bf\u5bf9\u9f50\uff0c\u5e76\u516c\u5f00\u5927\u91cf\u6570\u636e\u4e0e\u5de5\u5177\u4ee5\u63a8\u52a8\u6807\u51c6\u5316\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u56e0\u8bc4\u4f30\u65b9\u6cd5\u4e0d\u6807\u51c6\u548c\u8bbe\u8ba1\u7f3a\u9677\uff0c\u65e0\u6cd5\u53ef\u9760\u6bd4\u8f83\u4e0d\u540c\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u53ca\u5224\u65ad\u6700\u5148\u8fdb\u6280\u672f\uff1b\u56e0\u6b64\u9700\u8981\u7edf\u4e00\u4e14\u4e25\u683c\u7684\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u4ee5\u63a8\u52a8\u771f\u5b9e\u8fdb\u5c55\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u5e76\u53d1\u5e03\u4e86\u9488\u5bf9BEAT2\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6\u7684\u8be6\u7ec6\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\uff1b\u5229\u7528\u8be5\u534f\u8bae\u5728\u4f17\u5305\u5e73\u53f0\u4e0a\u5bf9\u516d\u4e2a\u8fd1\u671f\u6a21\u578b\uff08\u539f\u4f5c\u8005\u8bad\u7ec3\u7684\u7248\u672c\uff09\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u6d4b\uff0c\u8bc4\u4f30\u7ef4\u5ea6\u4e3a\u8fd0\u52a8\u771f\u5b9e\u6027\u548c\u8bed\u97f3-\u624b\u52bf\u5bf9\u9f50\u5ea6\uff1b\u5e76\u516c\u5f00\u5408\u6210\u52a8\u4f5c\u3001\u6e32\u67d3\u89c6\u9891\u3001\u8bc4\u6d4b\u6295\u7968\u7b49\u8d44\u6e90\u4ee5\u63a8\u8fdb\u6807\u51c6\u5316\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u7528\u6237\u7814\u7a76\uff0c\u53d1\u73b01) \u65b0\u6a21\u578b\u5e76\u4e0d\u603b\u662f\u4f18\u4e8e\u65e9\u671f\u65b9\u6cd5\uff1b2) \u8bb8\u591a\u8bba\u6587\u4e2d\u5173\u4e8e\u9ad8\u8fd0\u52a8\u771f\u5b9e\u6027\u6216\u826f\u597d\u8bed\u97f3-\u624b\u52bf\u5bf9\u9f50\u7684\u58f0\u660e\uff0c\u5728\u4e25\u683c\u8bc4\u6d4b\u4e0b\u4e0d\u6210\u7acb\uff1b3) \u5fc5\u987b\u5206\u522b\u8bc4\u4f30\u8fd0\u52a8\u8d28\u91cf\u4e0e\u591a\u6a21\u6001\u5bf9\u9f50\u624d\u80fd\u83b7\u5f97\u51c6\u786e\u57fa\u51c6\u3002\u4f5c\u8005\u8fd8\u5c06\u53d1\u5e03\u7ea65\u5c0f\u65f6\u5408\u6210\u52a8\u4f5c\u3001750+\u6e32\u67d3\u89c6\u9891\u548c1.6\u4e07\u6b21\u504f\u597d\u6295\u7968\u6570\u636e\u3002", "conclusion": "\u8be5\u8bba\u6587\u6307\u51fa\u5f53\u524d\u8bed\u97f3\u9a71\u52a8\u4e09\u7ef4\u624b\u52bf\u751f\u6210\u9886\u57df\u5728\u4eba\u5de5\u8bc4\u4f30\u65b9\u9762\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\uff0c\u8bb8\u591a\u5b9e\u9a8c\u8bbe\u8ba1\u5b58\u5728\u7f3a\u9677\uff0c\u5bfc\u81f4\u65b9\u6cd5\u95f4\u4e0d\u53ef\u6bd4\u5e76\u59a8\u788d\u9886\u57df\u8fdb\u5c55\u3002"}}
{"id": "2511.01237", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01237", "abs": "https://arxiv.org/abs/2511.01237", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video", "comment": "Accepted at RAAI 2025", "summary": "Human gaze offers rich supervisory signals for understanding visual attention\nin complex visual environments. In this paper, we propose Eyes on Target, a\nnovel depth-aware and gaze-guided object detection framework designed for\negocentric videos. Our approach injects gaze-derived features into the\nattention mechanism of a Vision Transformer (ViT), effectively biasing spatial\nfeature selection toward human-attended regions. Unlike traditional object\ndetectors that treat all regions equally, our method emphasises\nviewer-prioritised areas to enhance object detection. We validate our method on\nan egocentric simulator dataset where human visual attention is critical for\ntask assessment, illustrating its potential in evaluating human performance in\nsimulation scenarios. We evaluate the effectiveness of our gaze-integrated\nmodel through extensive experiments and ablation studies, demonstrating\nconsistent gains in detection accuracy over gaze-agnostic baselines on both the\ncustom simulator dataset and public benchmarks, including Ego4D Ego-Motion and\nEgo-CH-Gaze datasets. To interpret model behaviour, we also introduce a\ngaze-aware attention head importance metric, revealing how gaze cues modulate\ntransformer attention dynamics.", "AI": {"tldr": "\u672c\u6587\u7528\u6df1\u5ea6\u611f\u77e5\u7684\u51dd\u89c6\u5f15\u5bfc\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdbViT\u76ee\u6807\u68c0\u6d4b\uff0c\u4e13\u6ce8\u89c2\u5bdf\u8005\u5173\u6ce8\u533a\u57df\uff0c\u5728\u6a21\u62df\u5668\u4e0e\u516c\u5f00egocentric\u6570\u636e\u96c6\u4e0a\u5747\u5e26\u6765\u53ef\u9760\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u63d0\u4f9b\u6ce8\u610f\u529b\u5934\u7ea7\u522b\u7684\u89e3\u91ca\u6027\u5206\u6790\u3002", "motivation": "\u4eba\u7c7b\u51dd\u89c6\u643a\u5e26\u5173\u4e8e\u89c6\u89c9\u5173\u6ce8\u7684\u5f3a\u76d1\u7763\u4fe1\u53f7\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u80fd\u591f\u6307\u793a\u91cd\u8981\u533a\u57df\uff1b\u5c06\u6b64\u4fe1\u606f\u7528\u4e8e\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u76ee\u6807\u68c0\u6d4b\uff0c\u53ef\u66f4\u597d\u5730\u8bc4\u4f30\u6a21\u62df\u5668\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u5e76\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "method": "\u5728Vision Transformer\u4e2d\u5f15\u5165\u57fa\u4e8e\u773c\u52a8\u7684\u7279\u5f81\uff0c\u901a\u8fc7\u4e0e\u6df1\u5ea6\u4fe1\u606f\u878d\u5408\u751f\u6210\u51dd\u89c6\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u504f\u7f6e\uff0c\u5f3a\u8c03\u89c2\u5bdf\u8005\u4f18\u5148\u7684\u7a7a\u95f4\u533a\u57df\uff1b\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u548c\u5934\u91cd\u8981\u6027\u5ea6\u91cf\u6765\u5206\u6790\u51dd\u89c6\u5bf9\u6ce8\u610f\u529b\u5934\u7684\u8c03\u5236\u4f5c\u7528\u3002", "result": "\u5728\u81ea\u5236\u7684egocentric\u6a21\u62df\u5668\u6570\u636e\u96c6\u4ee5\u53ca\u516c\u5f00\u6570\u636e\u96c6\uff08Ego4D Ego-Motion\u3001Ego-CH-Gaze\uff09\u4e0a\uff0c\u76f8\u8f83\u4e8e\u4e0d\u4f7f\u7528\u51dd\u89c6\u4fe1\u606f\u7684\u57fa\u7ebf\uff0c\u6a21\u578b\u5728\u68c0\u6d4b\u51c6\u786e\u7387\u4e0a\u6301\u7eed\u63d0\u9ad8\uff1b\u5e76\u901a\u8fc7\u51dd\u89c6\u611f\u77e5\u7684\u6ce8\u610f\u529b\u5934\u91cd\u8981\u6027\u6307\u6807\u89e3\u91ca\u4e86\u51dd\u89c6\u5982\u4f55\u5f71\u54cdTransformer\u7684\u6ce8\u610f\u529b\u5206\u914d\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u5c06\u51dd\u89c6\uff08gaze\uff09\u4fe1\u606f\u4e0e\u6df1\u5ea6\u4fe1\u606f\u6ce8\u5165\u5230ViT\u7684\u6ce8\u610f\u529b\u673a\u5236\u4e2d\uff0c\u4ee5\u504f\u5411\u4eba\u7c7b\u5173\u6ce8\u533a\u57df\uff0c\u4ece\u800c\u63d0\u5347\u81ea\u6211\u4e2d\u5fc3\uff08egocentric\uff09\u89c6\u9891\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2511.01240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01240", "abs": "https://arxiv.org/abs/2511.01240", "authors": ["Zhixuan Zhang", "Pingyu Wang", "Xingjian Zheng", "Linbo Qing", "Qi Liu"], "title": "Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability", "comment": "Accepted by Pattern Recognition in Nov 01,2025", "summary": "Transferable attacks generate adversarial examples on surrogate models to\nfool unknown victim models, posing real-world threats and growing research\ninterest. Despite focusing on flat losses for transferable adversarial\nexamples, recent studies still fall into suboptimal regions, especially the\nflat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce\na novel black-box gradient-based transferable attack from a perspective of\ndual-order information. Specifically, we feasibly propose Adversarial Flatness\n(AF) to the deceptive flatness problem and a theoretical assurance for\nadversarial transferability. Based on this, using an efficient approximation of\nour objective, we instantiate our attack as Adversarial Flatness Attack (AFA),\naddressing the altered gradient sign issue. Additionally, to further improve\nthe attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by\nenhancing the inner-loop sampling efficiency. The comprehensive results on\nImageNet-compatible dataset demonstrate superiority over six baselines,\ngenerating adversarial examples in flatter regions and boosting transferability\nacross model architectures. When tested on input transformation attacks or the\nBaidu Cloud API, our method outperforms baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8c\u9636\u4fe1\u606f\u7684Adversarial Flatness\u6982\u5ff5\uff0c\u6784\u5efaAFA\u548cMCAS\u63d0\u5347\u5bf9\u6297\u6837\u672c\u8fc1\u79fb\u6027\uff0c\u5728\u591a\u79cd\u6d4b\u8bd5\u573a\u666f\u4e0b\u4f18\u4e8e\u516d\u4e2a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u53ef\u8fc1\u79fb\u653b\u51fb\u5c3d\u7ba1\u5173\u6ce8\u5e73\u5766\u635f\u5931\uff0c\u4f46\u4ecd\u5bb9\u6613\u9677\u5165\u201c\u5e73\u5766\u5374\u9661\u5ced\u201d\u7684\u5b50\u6700\u4f18\u533a\u57df\uff08\u6b3a\u9a97\u6027\u5e73\u5766\u6027\uff09\uff0c\u5bfc\u81f4\u8fc1\u79fb\u6027\u4e0d\u8db3\uff1b\u56e0\u6b64\u5f15\u5165\u4e8c\u9636\u4fe1\u606f\u4e0e\u65b0\u7684\u5e73\u5766\u6027\u5ea6\u91cf\u4ee5\u63d0\u5347\u8fc1\u79fb\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u53cc\u9636\u4fe1\u606f\u5f15\u5165Adversarial Flatness\uff08AF\uff09\u6307\u6807\u4ee5\u5ea6\u91cf\u5bf9\u6297\u6837\u672c\u6240\u5728\u635f\u5931\u66f2\u9762\u5e73\u5766\u6027\uff0c\u63a8\u5bfc\u7406\u8bba\u4fdd\u8bc1\u5e76\u6784\u9020\u53ef\u9ad8\u6548\u8ba1\u7b97\u7684\u8fd1\u4f3c\u76ee\u6807\uff1b\u8bbe\u8ba1AFA\u653b\u51fb\u4ee5\u5904\u7406\u68af\u5ea6\u7b26\u53f7\u6539\u53d8\u95ee\u9898\uff0c\u5e76\u63d0\u51faMCAS\u63d0\u5347\u5185\u5faa\u73af\u91c7\u6837\u6548\u7387\u4ee5\u589e\u5f3a\u653b\u51fb\u80fd\u529b\u3002", "result": "\u5728ImageNet\u517c\u5bb9\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u516d\u4e2a\u57fa\u7ebf\u65b9\u6cd5\uff0cAFA\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u4f4d\u4e8e\u66f4\u5e73\u5766\u7684\u635f\u5931\u533a\u57df\u5e76\u663e\u8457\u63d0\u9ad8\u4e0d\u540c\u67b6\u6784\u95f4\u7684\u8fc1\u79fb\u7387\uff1b\u5728\u8f93\u5165\u53d8\u6362\u9632\u62a4\u548c\u767e\u5ea6\u4e91API\u9ed1\u7bb1\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4ece\u4e8c\u9636\u4fe1\u606f\u89d2\u5ea6\u89e3\u51b3\u53ef\u8fc1\u79fb\u5bf9\u6297\u653b\u51fb\u4e2d\u201c\u6b3a\u9a97\u6027\u5e73\u5766\u6027\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49Adversarial Flatness\uff08AF\uff09\u5e76\u7ed9\u51fa\u7406\u8bba\u4fdd\u8bc1\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u8fd1\u4f3c\u76ee\u6807\u7684Adversarial Flatness Attack\uff08AFA\uff09\uff0c\u4ee5\u53ca\u63d0\u5347\u91c7\u6837\u6548\u7387\u7684MonteCarlo Adversarial Sampling\uff08MCAS\uff09\uff0c\u5728ImageNet\u517c\u5bb9\u6570\u636e\u96c6\u548c\u9ed1\u7bb1API\u4e0a\u5747\u4f18\u4e8e\u516d\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2511.01243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01243", "abs": "https://arxiv.org/abs/2511.01243", "authors": ["Yu Tian", "Zhongheng Yang", "Chenshi Liu", "Yiyun Su", "Ziwei Hong", "Zexi Gong", "Jingyuan Xu"], "title": "CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation", "comment": null, "summary": "Brain lesion segmentation remains challenging due to small, low-contrast\nlesions, anisotropic sampling, and cross-slice discontinuities. We propose\nCenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and\ntrains only lightweight adapters for efficient fine-tuning. At its core is the\nCenterMamba encoder, which employs a novel 3x3 corner-axis-center\nshort-sequence scanning strategy to enable center-prioritized, axis-reinforced,\nand diagonally compensated information aggregation. This design enhances\nsensitivity to weak boundaries and tiny foci while maintaining sparse yet\neffective feature representation. A memory-driven structural prompt generator\nmaintains a prototype bank across neighboring slices, enabling automatic\nsynthesis of reliable prompts without user interaction, thereby improving\ninter-slice coherence. The memory-augmented multi-scale decoder integrates\nmemory attention modules at multiple levels, combining deep supervision with\nprogressive refinement to restore fine details while preserving global\nconsistency. Extensive experiments on public benchmarks demonstrate that\nCenterMamba-SAM achieves state-of-the-art performance in brain lesion\nsegmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u51bb\u7ed3\u9aa8\u5e72\u3001\u8bad\u7ec3\u8f7b\u91cf\u9002\u914d\u5668\u7684CenterMamba-SAM\uff0c\u6838\u5fc3\u4e3a3x3 corner-axis-center\u626b\u63cf\u7684\u7f16\u7801\u5668\u3001\u8de8\u5207\u7247\u539f\u578b\u8bb0\u5fc6\u63d0\u793a\u751f\u6210\u5668\u4e0e\u591a\u5c3a\u5ea6\u8bb0\u5fc6\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u8111\u635f\u4f24\u5206\u5272\u7684\u5fae\u5c0f\u75c5\u7076\u8bc6\u522b\u4e0e\u8de8\u5207\u7247\u4e00\u81f4\u6027\uff0c\u5e76\u8fbe\u5230SOTA\u3002", "motivation": "\u8111\u635f\u4f24\u5206\u5272\u5b58\u5728\u5c0f\u76ee\u6807\u3001\u4f4e\u5bf9\u6bd4\u3001\u91c7\u6837\u5404\u5411\u5f02\u6027\u548c\u5207\u7247\u95f4\u4e0d\u8fde\u7eed\u7b49\u56f0\u96be\uff0c\u9700\u63d0\u9ad8\u5bf9\u5fae\u5c0f\u4f4e\u5bf9\u6bd4\u754c\u9762\u7684\u654f\u611f\u6027\u5e76\u4fdd\u6301\u8de8\u5207\u7247\u4e00\u81f4\u6027\u540c\u65f6\u4fdd\u8bc1\u8ba1\u7b97\u9ad8\u6548\u3002", "method": "\u63d0\u51faCenterMamba\u7f16\u7801\u5668\uff083x3 corner-axis-center\u77ed\u5e8f\u5217\u626b\u63cf\u7b56\u7565\uff09\u3001\u8bb0\u5fc6\u9a71\u52a8\u7ed3\u6784\u63d0\u793a\u751f\u6210\u5668\uff08\u8de8\u5207\u7247\u539f\u578b\u5e93\uff09\u4e0e\u8bb0\u5fc6\u589e\u5f3a\u591a\u5c3a\u5ea6\u89e3\u7801\u5668\uff08\u591a\u5c42\u8bb0\u5fc6\u6ce8\u610f\u3001\u6df1\u5ea6\u76d1\u7763\u548c\u6e10\u8fdb\u7ec6\u5316\uff09\u3002\u6574\u4f53\u91c7\u7528\u51bb\u7ed3\u9aa8\u5e72+\u8f7b\u91cf\u9002\u914d\u5668\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65b9\u6848\u3002", "result": "\u5728\u516c\u5f00\u57fa\u51c6\u4e0a\uff0cCenterMamba-SAM\u5728\u8111\u635f\u4f24\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u65b0\u7684SOTA\u6027\u80fd\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u5c0f\u75c5\u7076\u548c\u5f31\u8fb9\u754c\u533a\u57df\u8868\u73b0\u4f18\u8d8a\uff0c\u540c\u65f6\u5177\u5907\u8f83\u9ad8\u7684\u8de8\u5207\u7247\u8fde\u8d2f\u6027\u4e0e\u8d44\u6e90\u6548\u7387\u3002", "conclusion": "CenterMamba-SAM\u662f\u9488\u5bf9\u8111\u635f\u4f24\u5206\u5272\u8bbe\u8ba1\u7684\u9ad8\u6548\u8f7b\u91cf\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u9aa8\u5e72\u5e76\u4ec5\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u8d44\u6e90\u53cb\u597d\u7684\u8fc1\u79fb\u5b66\u4e60\uff0c\u540c\u65f6\u5728\u7ed3\u6784\u4e0a\u517c\u987e\u7ec6\u8282\u6062\u590d\u4e0e\u8de8\u5207\u7247\u4e00\u81f4\u6027\u3002"}}
{"id": "2511.01250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01250", "abs": "https://arxiv.org/abs/2511.01250", "authors": ["YoungJae Cheong", "Jhonghyun An"], "title": "Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop", "comment": null, "summary": "LiDAR semantic segmentation degrades in adverse weather because refraction,\nscattering, and point dropouts corrupt geometry. Prior work in weather\nsimulation, mixing-based augmentation, domain randomization, and uncertainty or\nboundary regularization improves robustness but still overlooks structural\nvulnerabilities near boundaries, corners, and sparse regions. We present a\nLight Geometry-aware adapter. The module aligns azimuth and applies horizontal\ncircular padding to preserve neighbor continuity across the 0~360 degree\nwrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points\nand computes simple local statistics, which are compressed into compact\ngeometry-aware cues. During training, these cues drive region-aware\nregularization that stabilizes predictions in structurally fragile areas. The\nadapter is plug and play, complements augmentation, and can be enabled only\nduring training with negligible inference cost. We adopt a source-only\ncross-weather setup where models train on SemanticKITTI and are evaluated on\nSemanticSTF without target labels or fine-tuning. The adapter improves mIoU by\n7.9 percentage points over the data-centric augmentation baseline and by 0.6\npoints over the class-centric regularization baseline. These results indicate\nthat geometry-driven regularization is a key direction for all-weather LiDAR\nsegmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8bad\u7ec3\u671f\u542f\u7528\u7684\u8f7b\u91cf\u51e0\u4f55\u611f\u77e5\u9002\u914d\u5668\uff0c\u901a\u8fc7\u5bf9\u9f50\u65b9\u4f4d\u89d2\u3001\u73af\u5f62\u586b\u5145\u548c\u5c40\u90e8KNN\u7edf\u8ba1\u751f\u6210\u51e0\u4f55\u7279\u5f81\uff0c\u5e76\u7528\u4ee5\u533a\u57df\u611f\u77e5\u6b63\u5219\u5316\uff0c\u663e\u8457\u63d0\u5347\u8de8\u5929\u6c14\u6761\u4ef6\u4e0b\u7684LiDAR\u8bed\u4e49\u5206\u5272\u9c81\u68d2\u6027\uff0c\u63a8\u7406\u5f00\u9500\u53ef\u5ffd\u7565\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u901a\u8fc7\u589e\u5f3a\u3001\u9886\u57df\u968f\u673a\u5316\u6216\u4e0d\u786e\u5b9a\u6027\u6b63\u5219\u7b49\u624b\u6bb5\u6539\u8fdb\u9c81\u68d2\u6027\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u8fb9\u754c\u3001\u62d0\u89d2\u548c\u7a00\u758f\u533a\u57df\u7b49\u7ed3\u6784\u8584\u5f31\u70b9\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u8fd9\u4e9b\u533a\u57df\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faLight Geometry-aware adapter\uff1a\u5bf9\u6781\u5750\u6807\u7684\u65b9\u4f4d\u89d2\u8fdb\u884c\u5bf9\u9f50\u5e76\u91c7\u7528\u6c34\u5e73\u73af\u5f62\u586b\u5145\u4ee5\u4fdd\u63010~360\u5ea6\u7684\u90bb\u63a5\u8fde\u7eed\u6027\uff1b\u5728\u5c40\u90e8\u7a97\u53e3\u5185\u4f7f\u7528KNN\u805a\u5408\u90bb\u70b9\u5e76\u8ba1\u7b97\u7b80\u5355\u5c40\u90e8\u7edf\u8ba1\u91cf\uff0c\u538b\u7f29\u4e3a\u7d27\u51d1\u7684\u51e0\u4f55\u611f\u77e5\u7279\u5f81\uff1b\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u9a71\u52a8\u533a\u57df\u611f\u77e5\u6b63\u5219\u5316\u4ee5\u7a33\u5b9a\u7ed3\u6784\u8106\u5f31\u533a\u57df\u7684\u9884\u6d4b\u3002\u8be5\u6a21\u5757\u53ef\u5373\u63d2\u5373\u7528\uff0c\u4ec5\u5728\u8bad\u7ec3\u65f6\u542f\u7528\uff0c\u5bf9\u63a8\u7406\u5f00\u9500\u51e0\u4e4e\u4e3a\u96f6\u3002", "result": "\u5728\u6e90\u57df\uff08SemanticKITTI\uff09\u8bad\u7ec3\u3001\u76ee\u6807\u57df\uff08SemanticSTF\uff09\u65e0\u6807\u7b7e\u8bc4\u4f30\u7684\u8bbe\u7f6e\u4e0b\uff0c\u9002\u914d\u5668\u76f8\u6bd4\u6570\u636e\u589e\u5f3a\u57fa\u7ebf\u63d0\u5347mIoU 7.9\u4e2a\u767e\u5206\u70b9\uff0c\u76f8\u6bd4\u7c7b\u4e2d\u5fc3\u6b63\u5219\u57fa\u7ebf\u63d0\u53470.6\u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u51e0\u4f55\u9a71\u52a8\u6b63\u5219\u5316\u5bf9\u5168\u5929\u6c14LiDAR\u5206\u5272\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7684\u51e0\u4f55\u611f\u77e5\u9002\u914d\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u4e0bLiDAR\u8bed\u4e49\u5206\u5272\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u8fb9\u754c\u3001\u89d2\u843d\u548c\u7a00\u758f\u533a\u57df\u7684\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0a\u6709\u6240\u6539\u5584\u3002"}}
{"id": "2511.01266", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01266", "abs": "https://arxiv.org/abs/2511.01266", "authors": ["Joonghyuk Shin", "Zhengqi Li", "Richard Zhang", "Jun-Yan Zhu", "Jaesik Park", "Eli Schechtman", "Xun Huang"], "title": "MotionStream: Real-Time Video Generation with Interactive Motion Controls", "comment": "Project webpage: https://joonghyuk.com/motionstream-web/", "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.", "AI": {"tldr": "\u63d0\u51faMotionStream\uff0c\u901a\u8fc7\u5c06\u53d7\u63a7\u6587\u672c\u89c6\u9891\u6559\u5e08\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u5e76\u5f15\u5165\u6ed1\u52a8\u7a97\u53e3\u56e0\u679c\u6ce8\u610f\u529b\u4e0e\u6ce8\u610f\u529b\u6c47\uff0c\u7ed3\u5408\u81ea\u56de\u6eda\u8bad\u7ec3\u4e0eKV\u7f13\u5b58\u6eda\u52a8\uff0c\u5b9e\u73b0\u5355GPU\u4e0a\u4e9a\u79d2\u5ef6\u8fdf\u3001\u6700\u9ad829FPS\u7684\u65e0\u9650\u957f\u5ea6\u5b9e\u65f6\u6d41\u5f0f\u89c6\u9891\u751f\u6210\uff0c\u8d28\u91cf\u4e0e\u8fd0\u52a8\u8ddf\u968f\u8fbe\u5230SOTA\uff0c\u901f\u5ea6\u5feb\u7ea6100\u500d\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u6761\u4ef6\u89c6\u9891\u751f\u6210\u5ef6\u8fdf\u9ad8\u4e14\u975e\u56e0\u679c\uff0c\u65e0\u6cd5\u5b9e\u65f6\u4ea4\u4e92\uff1b\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9075\u5faa\u5168\u5c40\u6587\u672c\u4e0e\u5c40\u90e8\u8fd0\u52a8\u63a7\u5236\uff0c\u53c8\u80fd\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u65e0\u9650\u957f\u5ea6\u6d41\u5f0f\u751f\u6210\u7684\u65b9\u6cd5\u3002", "method": "\u5148\u6269\u5c55\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4ee5\u6dfb\u52a0\u8fd0\u52a8\u63a7\u5236\uff0c\u5f97\u5230\u9ad8\u8d28\u91cf\u7684\u53cc\u5411\u6559\u5e08\u6a21\u578b\uff1b\u7136\u540e\u901a\u8fc7\u81ea\u5f3a\u5236\uff08Self Forcing\uff09\u4e0e\u5206\u5e03\u5339\u914d\u84b8\u998f\uff08Distribution Matching Distillation\uff09\u5c06\u6559\u5e08\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u3002\u5173\u952e\u6280\u672f\u5305\u62ec\u6ed1\u52a8\u7a97\u53e3\u56e0\u679c\u6ce8\u610f\u529b\u3001\u6ce8\u610f\u529b\u6c47\uff08attention sinks\uff09\u3001\u81ea\u56de\u6eda\uff08self-rollout\uff09\u8bad\u7ec3\u4e0eKV\u7f13\u5b58\u6eda\u52a8\uff0c\u6a21\u62df\u63a8\u7406\u65f6\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u4e0b\u7684\u5916\u63a8\u3002", "result": "\u6a21\u578b\u5728\u8fd0\u52a8\u8ddf\u968f\u548c\u89c6\u9891\u8d28\u91cf\u4e0a\u8fbe\u5230SOTA\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u5347\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u652f\u6301\u5b9e\u65f6\u7ed8\u5236\u8f68\u8ff9\u3001\u76f8\u673a\u63a7\u5236\u4e0e\u52a8\u4f5c\u8fc1\u79fb\uff0c\u9996\u6b21\u5b9e\u73b0\u65e0\u9650\u957f\u5ea6\u6d41\u5f0f\u4ea4\u4e92\u4f53\u9a8c\u3002", "conclusion": "MotionStream\u5b9e\u73b0\u4e86\u5728\u5355GPU\u4e0a\u5b9e\u65f6\u6d41\u5f0f\u751f\u6210\uff08\u4e9a\u79d2\u7ea7\u5ef6\u8fdf\uff0c\u6700\u9ad829FPS\uff09\uff0c\u901a\u8fc7\u5c06\u53d7\u63a7\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\u7cbe\u84b8\u998f\u4e3a\u56e0\u679c\u5b66\u751f\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u957f\u671f\u3001\u65e0\u9650\u65f6\u57df\u751f\u6210\u7684\u6311\u6218\u3002"}}
{"id": "2511.01274", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01274", "abs": "https://arxiv.org/abs/2511.01274", "authors": ["Tan Tang", "Yanhong Wu", "Junming Gao", "Yingcai Wu"], "title": "PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers", "comment": null, "summary": "Ancient Chinese paintings are a valuable cultural heritage that is damaged by\nirreversible color degradation. Reviving color-degraded paintings is\nextraordinarily difficult due to the complex chemistry mechanism. Progress is\nfurther slowed by the lack of comprehensive, high-quality datasets, which\nhampers the creation of end-to-end digital restoration tools. To revive colors,\nwe propose PRevivor, a prior-guided color transformer that learns from recent\npaintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and\nSong Dynasty). To develop PRevivor, we decompose color restoration into two\nsequential sub-tasks: luminance enhancement and hue correction. For luminance\nenhancement, we employ two variational U-Nets and a multi-scale mapping module\nto translate faded luminance into restored counterparts. For hue correction, we\ndesign a dual-branch color query module guided by localized hue priors\nextracted from faded paintings. Specifically, one branch focuses attention on\nregions guided by masked priors, enforcing localized hue correction, whereas\nthe other branch remains unconstrained to maintain a global reasoning\ncapability. To evaluate PRevivor, we conduct extensive experiments against\nstate-of-the-art colorization methods. The results demonstrate superior\nperformance both quantitatively and qualitatively.", "AI": {"tldr": "\u63d0\u51faPRevivor\uff1a\u4e00\u79cd\u57fa\u4e8e\u5148\u9a8c\u7684\u8272\u5f69Transformer\uff0c\u901a\u8fc7\u4eae\u5ea6\u6062\u590d\u4e0e\u53cc\u5206\u652f\u8272\u76f8\u7ea0\u6b63\u8054\u5408\u5c40\u90e8\u5148\u9a8c\u6210\u529f\u590d\u539f\u53e4\u4ee3\u4e2d\u56fd\u753b\u4f5c\u8272\u5f69\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u4e0a\u8272\u65b9\u6cd5\u3002", "motivation": "\u53e4\u4ee3\u4e2d\u56fd\u753b\u4f5c\u4e0d\u53ef\u9006\u7684\u8272\u5f69\u9000\u5316\u96be\u4ee5\u590d\u539f\uff0c\u4e14\u7f3a\u4e4f\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0e\u7aef\u5230\u7aef\u5de5\u5177\uff0c\u4f5c\u8005\u501f\u52a9\u8fd1\u4ee3\u753b\u4f5c\u7684\u8272\u5f69\u5148\u9a8c\u6765\u6307\u5bfc\u53e4\u753b\u7684\u6062\u590d\u3002", "method": "\u5148\u5c06\u8272\u5f69\u6062\u590d\u5206\u4e3a\u4eae\u5ea6\u589e\u5f3a\u4e0e\u8272\u76f8\u7ea0\u6b63\u4e24\u9636\u6bb5\uff1a\u4eae\u5ea6\u589e\u5f3a\u4f7f\u7528\u4e24\u4e2a\u53d8\u5206U-Net\u4e0e\u591a\u5c3a\u5ea6\u6620\u5c04\u6a21\u5757\u6062\u590d\u892a\u8272\u4eae\u5ea6\uff1b\u8272\u76f8\u7ea0\u6b63\u901a\u8fc7\u53cc\u5206\u652f\u989c\u8272\u67e5\u8be2\u6a21\u5757\u5e76\u7ed3\u5408\u5c40\u90e8\u8272\u76f8\u5148\u9a8c\u8fdb\u884c\u5c40\u90e8\u4e0e\u5168\u5c40\u76f8\u4e92\u8865\u7684\u4fee\u590d\u3002", "result": "\u4e0e\u591a\u79cd\u6700\u65b0\u4e0a\u8272\u65b9\u6cd5\u5bf9\u6bd4\uff0cPRevivor\u5728\u5b9a\u91cf\u6307\u6807\u4e0e\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u663e\u793a\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "PRevivor\u901a\u8fc7\u5229\u7528\u8fd1\u4ee3\u771f\u5e45\u8272\u5f69\u5148\u9a8c\u5e76\u5206\u89e3\u4e3a\u4eae\u5ea6\u589e\u5f3a\u4e0e\u8272\u76f8\u7ea0\u6b63\u4e24\u6b65\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53e4\u4ee3\u753b\u4f5c\u4e0d\u53ef\u9006\u892a\u8272\u7684\u9ad8\u8d28\u91cf\u6570\u5b57\u590d\u539f\u3002"}}
{"id": "2511.01284", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01284", "abs": "https://arxiv.org/abs/2511.01284", "authors": ["Karma Phuntsho", "Abdullah", "Kyungmi Lee", "Ickjai Lee", "Euijoon Ahn"], "title": "Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions", "comment": null, "summary": "Foundation models (FMs) have emerged as a transformative paradigm in medical\nimage analysis, offering the potential to provide generalizable, task-agnostic\nsolutions across a wide range of clinical tasks and imaging modalities. Their\ncapacity to learn transferable representations from large-scale data has the\npotential to address the limitations of conventional task-specific models.\nHowever, adaptation of FMs to real-world clinical practice remains constrained\nby key challenges, including domain shifts, limited availability of\nhigh-quality annotated data, substantial computational demands, and strict\nprivacy requirements. This review presents a comprehensive assessment of\nstrategies for adapting FMs to the specific demands of medical imaging. We\nexamine approaches such as supervised fine-tuning, domain-specific pretraining,\nparameter-efficient fine-tuning, self-supervised learning, hybrid methods, and\nmultimodal or cross-modal frameworks. For each, we evaluate reported\nperformance gains, clinical applicability, and limitations, while identifying\ntrade-offs and unresolved challenges that prior reviews have often overlooked.\nBeyond these established techniques, we also highlight emerging directions\naimed at addressing current gaps. These include continual learning to enable\ndynamic deployment, federated and privacy-preserving approaches to safeguard\nsensitive data, hybrid self-supervised learning to enhance data efficiency,\ndata-centric pipelines that combine synthetic generation with human-in-the-loop\nvalidation, and systematic benchmarking to assess robust generalization under\nreal-world clinical variability. By outlining these strategies and associated\nresearch gaps, this review provides a roadmap for developing adaptive,\ntrustworthy, and clinically integrated FMs capable of meeting the demands of\nreal-world medical imaging.", "AI": {"tldr": "\u7efc\u8ff0\u6307\u51faFMs\u5728\u533b\u7597\u5f71\u50cf\u4e2d\u5177\u5907\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u3001\u6301\u7eed\u5b66\u4e60\u3001\u6570\u636e\u9ad8\u6548\u7b56\u7565\u4e0e\u4e25\u683c\u57fa\u51c6\u6765\u5b9e\u73b0\u5b89\u5168\u53ef\u4fe1\u7684\u4e34\u5e8a\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u96be\u4ee5\u6cdb\u5316\u3001\u6807\u6ce8\u8d44\u6e90\u532e\u4e4f\u4e14\u4e34\u5e8a\u5e94\u7528\u5bf9\u9c81\u68d2\u6027\u4e0e\u9690\u79c1\u8981\u6c42\u9ad8\uff0c\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u53ef\u8fc1\u79fb\u8868\u5f81\u7684\u6f5c\u529b\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u5982\u4f55\u5c06\u5176\u53ef\u9760\u3001\u5b89\u5168\u5730\u5e94\u7528\u4e8e\u771f\u5b9e\u533b\u7597\u5f71\u50cf\u573a\u666f\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u5e76\u5206\u7c7b\u8bc4\u4f30\u4e86\u591a\u7c7b\u9002\u914d\u65b9\u6cd5\uff1a\u76d1\u7763\u5fae\u8c03\u3001\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u65e0\u76d1\u7763/\u81ea\u76d1\u7763\u5b66\u4e60\u3001\u6df7\u5408\u65b9\u6cd5\u4ee5\u53ca\u591a\u6a21\u6001/\u8de8\u6a21\u6001\u6846\u67b6\uff1b\u5bf9\u6bcf\u7c7b\u65b9\u6cd5\u7684\u6027\u80fd\u63d0\u5347\u3001\u4e34\u5e8a\u9002\u7528\u6027\u3001\u5c40\u9650\u6027\u548c\u6743\u8861\u8fdb\u884c\u6279\u5224\u6027\u6bd4\u8f83\u3002", "result": "\u603b\u7ed3\u5404\u65b9\u6cd5\u5728\u6587\u732e\u4e2d\u62a5\u544a\u7684\u6027\u80fd\u6539\u8fdb\u4e0e\u5c40\u9650\u6027\uff0c\u6307\u51fa\u591a\u6570\u7814\u7a76\u5728\u771f\u5b9e\u4e34\u5e8a\u5206\u5e03\u4e0b\u6cdb\u5316\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u5b9e\u8bc1\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u82e5\u5e72\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u586b\u8865\u7f3a\u53e3\uff0c\u4f8b\u5982\u6301\u7eed\u5b66\u4e60\u3001\u5dee\u5206\u9690\u79c1/\u8054\u90a6\u5b66\u4e60\u3001\u5408\u6210\u6570\u636e\u4e0e\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\u3001\u4ee5\u53ca\u8de8\u673a\u6784\u7cfb\u7edf\u6027\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u7efc\u8ff0\u4e86\u5c06\u57fa\u7840\u6a21\u578b(FMs)\u9002\u5e94\u533b\u7597\u5f71\u50cf\u7684\u591a\u79cd\u7b56\u7565\uff0c\u8ba4\u4e3a\u5c3d\u7ba1FMs\u5728\u53ef\u8fc1\u79fb\u8868\u793a\u5b66\u4e60\u4e0a\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u5728\u9886\u57df\u8fc1\u79fb\u3001\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u3001\u8ba1\u7b97\u6210\u672c\u548c\u9690\u79c1\u7b49\u65b9\u9762\u4ecd\u9762\u4e34\u91cd\u8981\u6311\u6218\uff0c\u9700\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u3001\u8054\u90a6/\u9690\u79c1\u4fdd\u62a4\u3001\u6df7\u5408\u81ea\u76d1\u7763\u3001\u6570\u636e\u4e2d\u5fc3\u5316\u751f\u6210\u4e0e\u4eba\u673a\u4ea4\u4e92\u9a8c\u8bc1\u53ca\u7cfb\u7edf\u6027\u57fa\u51c6\u6d4b\u8bd5\u7b49\u65b9\u5411\u63a8\u8fdb\u4e34\u5e8a\u843d\u5730\u3002"}}
{"id": "2511.01293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01293", "abs": "https://arxiv.org/abs/2511.01293", "authors": ["Yonggang Zhang", "Jun Nie", "Xinmei Tian", "Mingming Gong", "Kun Zhang", "Bo Han"], "title": "Detecting Generated Images by Fitting Natural Image Distributions", "comment": "25 pages, 9 figures, NeurIPS 2025 spotlight", "summary": "The increasing realism of generated images has raised significant concerns\nabout their potential misuse, necessitating robust detection methods. Current\napproaches mainly rely on training binary classifiers, which depend heavily on\nthe quantity and quality of available generated images. In this work, we\npropose a novel framework that exploits geometric differences between the data\nmanifolds of natural and generated images. To exploit this difference, we\nemploy a pair of functions engineered to yield consistent outputs for natural\nimages but divergent outputs for generated ones, leveraging the property that\ntheir gradients reside in mutually orthogonal subspaces. This design enables a\nsimple yet effective detection method: an image is identified as generated if a\ntransformation along its data manifold induces a significant change in the loss\nvalue of a self-supervised model pre-trained on natural images. Further more,\nto address diminishing manifold disparities in advanced generative models, we\nleverage normalizing flows to amplify detectable differences by extruding\ngenerated images away from the natural image manifold. Extensive experiments\ndemonstrate the efficacy of this method. Code is available at\nhttps://github.com/tmlr-group/ConV.", "AI": {"tldr": "\u5229\u7528\u6d41\u5f62\u51e0\u4f55\u5dee\u5f02\u4e0e\u81ea\u76d1\u7763\u635f\u5931\u654f\u611f\u6027\u68c0\u6d4b\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u7528\u5f52\u4e00\u5316\u6d41\u653e\u5927\u5dee\u5f02\uff0c\u6446\u8131\u5bf9\u5927\u91cf\u751f\u6210\u6837\u672c\u7684\u4f9d\u8d56\uff0c\u5b9e\u9a8c\u6709\u6548\u5e76\u5f00\u6e90\u3002", "motivation": "\u73b0\u6709\u4e8c\u5206\u7c7b\u68c0\u6d4b\u4f9d\u8d56\u5927\u91cf\u9ad8\u8d28\u91cf\u751f\u6210\u56fe\u50cf\u6837\u672c\uff0c\u96be\u4ee5\u6cdb\u5316\u3002\u4f5c\u8005\u8ba4\u4e3a\u81ea\u7136\u4e0e\u751f\u6210\u56fe\u50cf\u5206\u5e03\u5728\u6d41\u5f62\u51e0\u4f55\u4e0a\u5b58\u5728\u53ef\u5229\u7528\u7684\u5dee\u5f02\uff0c\u53ef\u7528\u4e8e\u66f4\u7a33\u5065\u7684\u68c0\u6d4b\u3002", "method": "\u8bbe\u8ba1\u4e24\u51fd\u6570\u5bf9\uff08\u8f93\u51fa\u4e00\u81f4\u6027+\u68af\u5ea6\u5b50\u7a7a\u95f4\u6b63\u4ea4\uff09\uff0c\u7528\u9884\u8bad\u7ec3\u81ea\u76d1\u7763\u6a21\u578b\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8ba1\u7b97\u635f\u5931\u53d8\u5316\u4f5c\u4e3a\u5224\u522b\u4f9d\u636e\uff1b\u5f53\u751f\u6210\u6a21\u578b\u66f4\u903c\u8fd1\u771f\u5b9e\u6d41\u5f62\u65f6\uff0c\u91c7\u7528\u5f52\u4e00\u5316\u6d41\u5c06\u751f\u6210\u56fe\u50cf\u6324\u51fa\u771f\u5b9e\u6d41\u5f62\u4ee5\u653e\u5927\u5dee\u5f02\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u6709\u6548\uff0c\u7279\u522b\u5728\u9ad8\u7ea7\u751f\u6210\u5668\u6837\u672c\u4e0a\u901a\u8fc7\u5f52\u4e00\u5316\u6d41\u589e\u5f3a\u540e\u4ecd\u80fd\u4fdd\u6301\u8f83\u9ad8\u68c0\u6d4b\u6027\u80fd\uff1b\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u6d41\u5f62\u51e0\u4f55\u5dee\u5f02\u7684\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5229\u7528\u4e00\u5bf9\u51fd\u6570\u5728\u81ea\u7136\u56fe\u50cf\u4e0a\u8f93\u51fa\u4e00\u81f4\u800c\u5728\u751f\u6210\u56fe\u50cf\u4e0a\u8f93\u51fa\u76f8\u5f02\uff0c\u4e14\u68af\u5ea6\u4f4d\u4e8e\u76f8\u4e92\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff0c\u4ece\u800c\u901a\u8fc7\u6cbf\u6570\u636e\u6d41\u5f62\u53d8\u6362\u65f6\u81ea\u76d1\u7763\u6a21\u578b\u635f\u5931\u663e\u8457\u53d8\u5316\u6765\u5224\u522b\u751f\u6210\u56fe\u50cf\uff0c\u5e76\u7528\u5f52\u4e00\u5316\u6d41\u653e\u5927\u5dee\u5f02\u3002"}}
{"id": "2511.01295", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01295", "abs": "https://arxiv.org/abs/2511.01295", "authors": ["Feng Han", "Yibin Wang", "Chenglin Li", "Zheming Liang", "Dianyi Wang", "Yang Jiao", "Zhipeng Wei", "Chao Gong", "Cheng Jin", "Jingjing Chen", "Jiaqi Wang"], "title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark", "comment": "Project page: https://maplebb.github.io/UniREditBench", "summary": "Recent advances in multi-modal generative models have driven substantial\nimprovements in image editing. However, current generative models still\nstruggle with handling diverse and complex image editing tasks that require\nimplicit reasoning, underscoring the need for a comprehensive benchmark to\nsystematically assess their performance across various reasoning scenarios.\nExisting benchmarks primarily focus on single-object attribute transformation\nin realistic scenarios, which, while effective, encounter two key challenges:\n(1) they largely overlook multi-object interactions as well as game-world\nscenarios that involve human-defined rules, which are common in real-life\napplications; (2) they only rely on textual references to evaluate the\ngenerated images, potentially leading to systematic misjudgments, especially in\ncomplex reasoning scenarios. To this end, this work proposes UniREditBench, a\nunified benchmark for reasoning-based image editing evaluation. It comprises\n2,700 meticulously curated samples, covering both real- and game-world\nscenarios across 8 primary dimensions and 18 sub-dimensions. To improve\nevaluation reliability, we introduce multimodal dual-reference evaluation,\nproviding both textual and ground-truth image references for each sample\nassessment. Furthermore, we design an automated multi-scenario data synthesis\npipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with\nhigh-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel\non this dataset and develop UniREdit-Bagel, demonstrating substantial\nimprovements in both in-domain and out-of-distribution settings. Through\nthorough benchmarking of both open-source and closed-source image editing\nmodels, we reveal their strengths and weaknesses across various aspects.", "AI": {"tldr": "\u63d0\u51faUniREditBench\u4e0e10\u4e07\u5408\u6210\u63a8\u7406\u6570\u636e\u5e76\u5fae\u8c03\u5f97\u5230UniREdit-Bagel\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u76f8\u5173\u7684\u56fe\u50cf\u7f16\u8f91\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4e0e\u6539\u8fdb\u9014\u5f84\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u7f16\u8f91\u57fa\u51c6\u8fc7\u4e8e\u96c6\u4e2d\u4e8e\u5355\u7269\u4f53\u5c5e\u6027\u53d8\u6362\u4e14\u53ea\u7528\u6587\u672c\u53c2\u8003\u8bc4\u4f30\uff0c\u5ffd\u89c6\u591a\u5bf9\u8c61\u4ea4\u4e92\u3001\u6e38\u620f\u4e16\u754c\u89c4\u5219\u4ee5\u53ca\u4ec5\u6587\u672c\u8bc4\u4f30\u5728\u590d\u6742\u63a8\u7406\u573a\u666f\u4e0b\u53ef\u80fd\u5bfc\u81f4\u7684\u7cfb\u7edf\u6027\u8bef\u5224\u3002", "method": "\u6784\u5efa2,700\u6761\u5305\u542b\u771f\u5b9e\u4e0e\u6e38\u620f\u573a\u666f\u7684\u591a\u7ef4\u8bc4\u4f30\u96c6\uff1b\u5f15\u5165\u591a\u6a21\u6001\u53cc\u53c2\u8003\u8bc4\u4f30\uff08\u6587\u672c+\u56fe\u50cf\u771f\u503c\uff09\uff1b\u8bbe\u8ba1\u81ea\u52a8\u5316\u591a\u573a\u666f\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u751f\u6210\u5e26\u94fe\u5f0f\u601d\u7ef4\u6ce8\u91ca\u768410\u4e07\u6761\u5408\u6210\u6570\u636e\uff08UniREdit-Data-100K\uff09\uff1b\u5728\u8be5\u6570\u636e\u4e0a\u5fae\u8c03Bagel\uff0c\u5f97\u5230UniREdit-Bagel\uff0c\u5e76\u5bf9\u591a\u6a21\u578b\u8fdb\u884c\u6a2a\u5411\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u63d0\u51fa\u7684\u57fa\u51c6\u8986\u76d68\u4e2a\u4e3b\u7ef4\u5ea6\u4e0e18\u4e2a\u5b50\u7ef4\u5ea6\uff1b\u591a\u6a21\u6001\u53cc\u53c2\u8003\u63d0\u9ad8\u8bc4\u4f30\u53ef\u9760\u6027\uff1b\u57fa\u4e8e\u5408\u6210\u6570\u636e\u5fae\u8c03\u7684UniREdit-Bagel\u5728\u5185\u5916\u57df\u5747\u663e\u8457\u63d0\u5347\uff1b\u901a\u8fc7\u57fa\u51c6\u63ed\u793a\u73b0\u6709\u5f00\u95ed\u6e90\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u65b9\u9762\u7684\u4f18\u52a3\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u57fa\u4e8e\u63a8\u7406\u7684\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u7684\u7edf\u4e00\u57fa\u51c6UniREditBench\uff0c\u5e76\u901a\u8fc7\u5408\u6210\u6570\u636e\u4e0e\u5fae\u8c03\u6a21\u578b\u63d0\u5347\u7f16\u8f91\u6a21\u578b\u5728\u63a8\u7406\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002"}}
{"id": "2511.01302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01302", "abs": "https://arxiv.org/abs/2511.01302", "authors": ["Nu-Fnag Xiao", "De-Xing Huang", "Le-Tian Wang", "Mei-Jiang Gui", "Qi Fu", "Xiao-Liang Xie", "Shi-Qi Liu", "Shuangyi Wang", "Zeng-Guang Hou", "Ying-Wei Wang", "Xiao-Hu Zhou"], "title": "REASON: Probability map-guided dual-branch fusion framework for gastric content assessment", "comment": "Under Review. 12 pages, 10 figures, 6 tables", "summary": "Accurate assessment of gastric content from ultrasound is critical for\nstratifying aspiration risk at induction of general anesthesia. However,\ntraditional methods rely on manual tracing of gastric antra and empirical\nformulas, which face significant limitations in both efficiency and accuracy.\nTo address these challenges, a novel two-stage probability map-guided\ndual-branch fusion framework (REASON) for gastric content assessment is\nproposed. In stage 1, a segmentation model generates probability maps that\nsuppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch\nclassifier fuses information from two standard views, right lateral decubitus\n(RLD) and supine (SUP), to improve the discrimination of learned features.\nExperimental results on a self-collected dataset demonstrate that the proposed\nframework outperforms current state-of-the-art approaches by a significant\nmargin. This framework shows great promise for automated preoperative\naspiration risk assessment, offering a more robust, efficient, and accurate\nsolution for clinical practice.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6982\u7387\u56fe\u5f15\u5bfc\u7684\u53cc\u89c6\u56fe\u878d\u5408\u6846\u67b6\uff08REASON\uff09\uff0c\u901a\u8fc7\u5206\u5272\u751f\u6210\u6982\u7387\u56fe\u5e76\u878d\u5408RLD/SUP\u89c6\u56fe\u7684\u53cc\u652f\u8def\u5206\u7c7b\u5668\uff0c\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u672f\u524d\u8bef\u5438\u98ce\u9669\u8bc4\u4f30\u3002", "motivation": "\u52a8\u673a\u662f\u4f20\u7edf\u8d85\u58f0\u8bc4\u4f30\u4f9d\u8d56\u624b\u5de5\u63cf\u8bb0\u548c\u7ecf\u9a8c\u516c\u5f0f\uff0c\u6548\u7387\u4f4e\u4e14\u51c6\u786e\u6027\u53d7\u9650\uff0c\u9700\u8981\u81ea\u52a8\u5316\u3001\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u80c3\u5185\u5bb9\u7269\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u6539\u5584\u672f\u524d\u8bef\u5438\u98ce\u9669\u5206\u5c42\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a\u9636\u6bb51\u4f7f\u7528\u5206\u5272\u6a21\u578b\u751f\u6210\u6982\u7387\u56fe\u4ee5\u6291\u5236\u4f2a\u5f71\u5e76\u7a81\u51fa\u80c3\u89e3\u5256\u7ed3\u6784\uff1b\u9636\u6bb52\u91c7\u7528\u6765\u81eaRLD\u548cSUP\u4e24\u89c6\u56fe\u7684\u53cc\u652f\u8def\u5206\u7c7b\u5668\u878d\u5408\u7279\u5f81\u4ee5\u63d0\u9ad8\u5224\u522b\u80fd\u529b\u3002\u6574\u4f53\u4e3a\u7aef\u5230\u7aef\u7684\u6982\u7387\u56fe\u5f15\u5bfc\u53cc\u5206\u652f\u878d\u5408\u6846\u67b6\u3002", "result": "\u5728\u4f5c\u8005\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cREASON\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u9c81\u68d2\u6027\uff0c\u9002\u5408\u7528\u4e8e\u672f\u524d\u81ea\u52a8\u5316\u8bef\u5438\u98ce\u9669\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684REASON\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u6d41\u7a0b\uff08\u6982\u7387\u56fe\u5f15\u5bfc\u7684\u5206\u5272 + \u53cc\u652f\u8def\u89c6\u56fe\u878d\u5408\u5206\u7c7b\uff09\uff0c\u5728\u80c3\u5185\u5185\u5bb9\u7269\u8d85\u58f0\u8bc4\u4f30\u4e0a\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u7ed3\u8bba\u53ef\u4fe1\u4e14\u5177\u6709\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.01304", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01304", "abs": "https://arxiv.org/abs/2511.01304", "authors": ["Chentao Li", "Behzad Bozorgtabar", "Yifang Ping", "Pan Huang", "Jing Qin"], "title": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation", "comment": "Our code is available at https://github.com/Prince-Lee-PathAI/PG-CIDL", "summary": "Multiple instance learning (MIL) has been widely used for representing\nwhole-slide pathology images. However, spatial, semantic, and decision\nentanglements among instances limit its representation and interpretability. To\naddress these challenges, we propose a latent factor grouping-boosted\ncluster-reasoning instance disentangled learning framework for whole-slide\nimage (WSI) interpretable representation in three phases. First, we introduce a\nnovel positive semi-definite latent factor grouping that maps instances into a\nlatent subspace, effectively mitigating spatial entanglement in MIL. To\nalleviate semantic entanglement, we employs instance probability counterfactual\ninference and optimization via cluster-reasoning instance disentangling.\nFinally, we employ a generalized linear weighted decision via instance effect\nre-weighting to address decision entanglement. Extensive experiments on\nmulticentre datasets demonstrate that our model outperforms all\nstate-of-the-art models. Moreover, it attains pathologist-aligned\ninterpretability through disentangled representations and a transparent\ndecision-making process.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u5b9e\u4f8b\u89e3\u7f20\u5b66\u4e60\u6846\u67b6\uff08\u6f5c\u5728\u56e0\u5b50\u5206\u7ec4\u3001\u805a\u7c7b\u63a8\u7406\u53cd\u4e8b\u5b9e\u89e3\u7f20\u3001\u5b9e\u4f8b\u91cd\u52a0\u6743\u51b3\u7b56\uff09\uff0c\u663e\u8457\u63d0\u5347WSI\u7684\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u52a8\u673a\u5728\u4e8e\u4f20\u7edfMIL\u5728\u5904\u7406WSI\u65f6\u5b58\u5728\u5b9e\u4f8b\u4e4b\u95f4\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u4e0e\u51b3\u7b56\u7ea0\u7f20\uff0c\u9650\u5236\u4e86\u8868\u793a\u80fd\u529b\u4e0e\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u89e3\u6784\u8fd9\u4e9b\u7ea0\u7f20\u5e76\u63d0\u4f9b\u75c5\u7406\u5b66\u4e00\u81f4\u6027\u89e3\u91ca\u7684\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u4e09\u9636\u6bb5\uff1a1) \u6b63\u534a\u5b9a\u6f5c\u5728\u56e0\u5b50\u5206\u7ec4\uff0c\u5c06\u5b9e\u4f8b\u6620\u5c04\u5230\u6f5c\u5728\u5b50\u7a7a\u95f4\u4ee5\u7f13\u89e3\u7a7a\u95f4\u7ea0\u7f20\uff1b2) \u901a\u8fc7\u5b9e\u4f8b\u6982\u7387\u53cd\u4e8b\u5b9e\u63a8\u65ad\u4e0e\u805a\u7c7b\u63a8\u7406\u8fdb\u884c\u5b9e\u4f8b\u89e3\u7f20\u4ee5\u7f13\u89e3\u8bed\u4e49\u7ea0\u7f20\uff1b3) \u91c7\u7528\u5e7f\u4e49\u7ebf\u6027\u52a0\u6743\u51b3\u7b56\u548c\u5b9e\u4f8b\u6548\u5e94\u91cd\u52a0\u6743\u6765\u89e3\u51b3\u51b3\u7b56\u7ea0\u7f20\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u6240\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u89e3\u7f20\u8868\u793a\u4e0e\u900f\u660e\u51b3\u7b56\u6d41\u7a0b\u5b9e\u73b0\u4e0e\u75c5\u7406\u5b66\u5bb6\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5168\u5207\u7247\u56fe\u50cf\uff08WSI\uff09\u53ef\u89e3\u91ca\u8868\u793a\u7684\u5b9e\u4f8b\u89e3\u7f20\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u56e0\u5b50\u5206\u7ec4\u3001\u805a\u7c7b\u63a8\u7406\u7684\u53cd\u4e8b\u5b9e\u63a8\u65ad\u4e0e\u5b9e\u4f8b\u91cd\u52a0\u6743\uff0c\u89e3\u51b3\u4e86MIL\u4e2d\u7684\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u51b3\u7b56\u4e09\u7c7b\u7ea0\u7f20\u95ee\u9898\u3002"}}
{"id": "2511.01307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01307", "abs": "https://arxiv.org/abs/2511.01307", "authors": ["Tae-Young Lee", "Juwon Seo", "Jong Hwan Ko", "Gyeong-Moon Park"], "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models", "comment": "26 pages, 9 figures, 16 tables, NeurIPS 2025", "summary": "Recent advances in diffusion models have enabled high-quality synthesis of\nspecific subjects, such as identities or objects. This capability, while\nunlocking new possibilities in content creation, also introduces significant\nprivacy risks, as personalization techniques can be misused by malicious users\nto generate unauthorized content. Although several studies have attempted to\ncounter this by generating adversarially perturbed samples designed to disrupt\npersonalization, they rely on unrealistic assumptions and become ineffective in\nthe presence of even a few clean images or under simple image transformations.\nTo address these challenges, we shift the protection target from the images to\nthe diffusion model itself to hinder the personalization of specific subjects,\nthrough our novel framework called Anti-Personalized Diffusion Models (APDM).\nWe first provide a theoretical analysis demonstrating that a naive approach of\nexisting loss functions to diffusion models is inherently incapable of ensuring\nconvergence for robust anti-personalization. Motivated by this finding, we\nintroduce Direct Protective Optimization (DPO), a novel loss function that\neffectively disrupts subject personalization in the target model without\ncompromising generative quality. Moreover, we propose a new dual-path\noptimization strategy, coined Learning to Protect (L2P). By alternating between\npersonalization and protection paths, L2P simulates future personalization\ntrajectories and adaptively reinforces protection at each step. Experimental\nresults demonstrate that our framework outperforms existing methods, achieving\nstate-of-the-art performance in preventing unauthorized personalization. The\ncode is available at https://github.com/KU-VGI/APDM.", "AI": {"tldr": "\u63d0\u51faAPDM\u6846\u67b6\uff0c\u901a\u8fc7DPO\u635f\u5931\u548cL2P\u53cc\u8def\u5f84\u4f18\u5316\uff0c\u5728\u6a21\u578b\u5c42\u9762\u963b\u65ad\u6269\u6563\u6a21\u578b\u5bf9\u7279\u5b9a\u4e3b\u4f53\u7684\u4e2a\u6027\u5316\uff0c\u4ece\u800c\u6bd4\u56fe\u50cf\u7ea7\u5bf9\u6297\u6270\u52a8\u66f4\u9c81\u68d2\u5730\u9632\u6b62\u672a\u6388\u6743\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u6297\u6270\u52a8\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5728\u5b58\u5728\u5c11\u91cf\u5e72\u51c0\u56fe\u50cf\u6216\u56fe\u50cf\u7b80\u5355\u53d8\u6362\u65f6\u5931\u6548\uff0c\u4e14\u5047\u8bbe\u4e0d\u73b0\u5b9e\u3002\u4f5c\u8005\u56e0\u6b64\u5c06\u9632\u62a4\u5bf9\u8c61\u4e0a\u79fb\u5230\u6269\u6563\u6a21\u578b\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faDirect Protective Optimization\uff08DPO\uff09\u635f\u5931\u51fd\u6570\u7528\u4e8e\u5728\u4e0d\u7834\u574f\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u7834\u574f\u76ee\u6807\u6a21\u578b\u7684\u4e3b\u4f53\u4e2a\u6027\u5316\u80fd\u529b\uff1b\u540c\u65f6\u8bbe\u8ba1\u53cc\u8def\u5f84\u4f18\u5316\u7b56\u7565Learning to Protect\uff08L2P\uff09\uff0c\u901a\u8fc7\u5728\u4e2a\u6027\u5316\u8def\u5f84\u548c\u4fdd\u62a4\u8def\u5f84\u4e4b\u95f4\u4ea4\u66ff\u4f18\u5316\uff0c\u6a21\u62df\u672a\u6765\u4e2a\u6027\u5316\u8f68\u8ff9\u5e76\u5728\u6bcf\u4e00\u6b65\u81ea\u9002\u5e94\u52a0\u5f3a\u4fdd\u62a4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAPDM\u5728\u963b\u6b62\u672a\u6388\u6743\u4e2a\u6027\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u4fdd\u62a4\u76ee\u6807\u4ece\u56fe\u50cf\u8f6c\u5411\u6269\u6563\u6a21\u578b\u672c\u8eab\uff0c\u901a\u8fc7\u5728\u6a21\u578b\u5c42\u9762\u963b\u6b62\u9488\u5bf9\u7279\u5b9a\u4e3b\u4f53\u7684\u4e2a\u6027\u5316\uff0c\u4ece\u800c\u66f4\u7a33\u5065\u5730\u9632\u6b62\u672a\u6388\u6743\u7684\u4e2a\u6027\u5316\u751f\u6210\u3002"}}
{"id": "2511.01315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01315", "abs": "https://arxiv.org/abs/2511.01315", "authors": ["Jianfei Jiang", "Qiankun Liu", "Hongyuan Liu", "Haochen Yu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MVSMamba: Multi-View Stereo with State Space Model", "comment": "Accepted by NeurIPS 2025", "summary": "Robust feature representations are essential for learning-based Multi-View\nStereo (MVS), which relies on accurate feature matching. Recent MVS methods\nleverage Transformers to capture long-range dependencies based on local\nfeatures extracted by conventional feature pyramid networks. However, the\nquadratic complexity of Transformer-based MVS methods poses challenges to\nbalance performance and efficiency. Motivated by the global modeling capability\nand linear complexity of the Mamba architecture, we propose MVSMamba, the first\nMamba-based MVS network. MVSMamba enables efficient global feature aggregation\nwith minimal computational overhead. To fully exploit Mamba's potential in MVS,\nwe propose a Dynamic Mamba module (DM-module) based on a novel\nreference-centered dynamic scanning strategy, which enables: (1) Efficient\nintra- and inter-view feature interaction from the reference to source views,\n(2) Omnidirectional multi-view feature representations, and (3) Multi-scale\nglobal feature aggregation. Extensive experimental results demonstrate MVSMamba\noutperforms state-of-the-art MVS methods on the DTU dataset and the\nTanks-and-Temples benchmark with both superior performance and efficiency. The\nsource code is available at https://github.com/JianfeiJ/MVSMamba.", "AI": {"tldr": "\u5c06\u7ebf\u6027\u590d\u6742\u5ea6\u7684Mamba\u67b6\u6784\u5e94\u7528\u4e8eMVS\uff0c\u63d0\u51fa\u52a8\u6001Mamba\u6a21\u5757\u4ee5\u5b9e\u73b0\u53c2\u8003\u4e2d\u5fc3\u52a8\u6001\u626b\u63cf\u7684\u8de8\u89c6\u56fe\u53ca\u591a\u5c3a\u5ea6\u5168\u5c40\u7279\u5f81\u805a\u5408\uff0c\u4ece\u800c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u5747\u8d85\u8fc7\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709Transformer-based MVS\u5728\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u65b9\u9762\u6709\u6548\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\uff0c\u96be\u4ee5\u5728\u6027\u80fd\u4e0e\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002Mamba\u67b6\u6784\u5177\u5907\u7ebf\u6027\u590d\u6742\u5ea6\u4e0e\u5168\u5c40\u5efa\u6a21\u80fd\u529b\uff0c\u56e0\u800c\u88ab\u7528\u4e8e\u63d0\u5347MVS\u7684\u6548\u7387\u4e0e\u8868\u73b0\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eMamba\u7684MVSMamba\u7f51\u7edc\uff0c\u6838\u5fc3\u4e3a\u63d0\u51fa\u7684\u52a8\u6001Mamba\u6a21\u5757\uff08DM-module\uff09\uff0c\u91c7\u7528reference-centered\u52a8\u6001\u626b\u63cf\u7b56\u7565\u4ee5\u5b9e\u73b0\u81ea\u89c6\u56fe\u4e0e\u6e90\u89c6\u56fe\u95f4\u9ad8\u6548\u7279\u5f81\u4ea4\u4e92\u3001\u5168\u65b9\u4f4d\u591a\u89c6\u56fe\u7279\u5f81\u8868\u793a\u4ee5\u53ca\u591a\u5c3a\u5ea6\u5168\u5c40\u7279\u5f81\u805a\u5408\u3002", "result": "\u5728DTU\u548cTanks-and-Temples\u57fa\u51c6\u4e0a\uff0cMVSMamba\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684MVS\u65b9\u6cd5\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51faMVSMamba\uff0c\u5c06Mamba\u67b6\u6784\u5f15\u5165\u591a\u89c6\u56fe\u7acb\u4f53\u91cd\u5efa\uff08MVS\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u5c40\u7279\u5f81\u805a\u5408\u5e76\u6539\u5584\u5339\u914d\u6027\u80fd\u3002"}}
{"id": "2511.01317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01317", "abs": "https://arxiv.org/abs/2511.01317", "authors": ["Sampriti Soor", "Alik Pramanick", "Jothiprakash K", "Arijit Sur"], "title": "A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model", "comment": "18 pages, 3 figures", "summary": "The rapid growth of deep learning has brought about powerful models that can\nhandle various tasks, like identifying images and understanding language.\nHowever, adversarial attacks, an unnoticed alteration, can deceive models,\nleading to inaccurate predictions. In this paper, a generative adversarial\nattack method is proposed that uses the CLIP model to create highly effective\nand visually imperceptible adversarial perturbations. The CLIP model's ability\nto align text and image representation helps incorporate natural language\nsemantics with a guided loss to generate effective adversarial examples that\nlook identical to the original inputs. This integration allows extensive scene\nmanipulation, creating perturbations in multi-object environments specifically\ndesigned to deceive multilabel classifiers. Our approach integrates the\nconcentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with\nthe dissimilar text embeddings similar to Generative Adversarial Multi-Object\nScene Attacks (GAMA), resulting in perturbations that both deceive\nclassification models and maintain high structural similarity to the original\nimages. The model was tested on various tasks across diverse black-box victim\nmodels. The experimental results show that our method performs competitively,\nachieving comparable or superior results to existing techniques, while\npreserving greater visual fidelity.", "AI": {"tldr": "\u5229\u7528CLIP\u8bed\u4e49\u5bf9\u9f50\u548c\u96c6\u4e2d\u5f0f\u6270\u52a8\u751f\u6210\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u76ee\u6807\u573a\u666f\u4e0b\u5bf9\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u8fdb\u884c\u9ad8\u4fdd\u771f\u5bf9\u6297\u653b\u51fb\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u52a8\u673a\u662f\u5229\u7528CLIP\u5c06\u81ea\u7136\u8bed\u8a00\u8bed\u4e49\u4e0e\u56fe\u50cf\u8868\u793a\u5bf9\u9f50\u7684\u80fd\u529b\uff0c\u4ee5\u5728\u591a\u76ee\u6807\u573a\u666f\u4e2d\u751f\u6210\u66f4\u6709\u6548\u4e14\u89c6\u89c9\u4e0a\u96be\u4ee5\u5bdf\u89c9\u7684\u5bf9\u6297\u6270\u52a8\uff0c\u4e13\u95e8\u7528\u4e8e\u6b3a\u9a97\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u3002", "method": "\u65b9\u6cd5\u7ed3\u5408\u4e86CLIP\u7684\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u80fd\u529b\u4e0e\u5f15\u5bfc\u635f\u5931\uff0c\u5229\u7528SSAE\u7684\u96c6\u4e2d\u6270\u52a8\u7b56\u7565\u4ee5\u53ca\u7c7b\u4f3cGAMA\u7684\u4e0d\u76f8\u4f3c\u6587\u672c\u5d4c\u5165\u751f\u6210\u5668\uff0c\u751f\u6210\u9488\u5bf9\u591a\u76ee\u6807\u573a\u666f\u7684\u6270\u52a8\u3002", "result": "\u5728\u591a\u79cd\u9ed1\u76d2\u53d7\u5bb3\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u66f4\u9ad8\u7ed3\u6784\u76f8\u4f3c\u5ea6\u7684\u540c\u65f6\uff0c\u80fd\u8fbe\u5230\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u653b\u51fb\u6548\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u751f\u6210\u5f0f\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\uff0c\u751f\u6210\u5bf9\u591a\u6807\u7b7e\u5206\u7c7b\u5668\u5177\u6709\u6b3a\u9a97\u6027\u7684\u6270\u52a8\u3002"}}
{"id": "2511.01328", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01328", "abs": "https://arxiv.org/abs/2511.01328", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation", "comment": null, "summary": "Medical image segmentation is essential for computer-assisted diagnosis and\ntreatment planning, yet substantial anatomical variability and boundary\nambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,\na segmentation network that unifies local modeling with global context to\nstrengthen boundary delineation and detail preservation. RDTE-UNet employs a\nhybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for\nadaptive boundary enhancement, HVDA for fine-grained feature modeling, and\nEulerFF for fusion weighting guided by Euler's formula. Together, these\ncomponents improve structural consistency and boundary accuracy across\nmorphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has\nachieved a comparable level in terms of segmentation accuracy and boundary\nquality.", "AI": {"tldr": "RDTE-UNet\u7ed3\u5408ResBlock+Transformer\u9aa8\u5e72\u4e0eASBE\u3001HVDA\u3001EulerFF\u4e09\u6a21\u5757\uff0c\u7edf\u4e00\u5c40\u90e8\u4e0e\u5168\u5c40\u5efa\u6a21\u4ee5\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u7ec6\u8282\u548c\u8fb9\u754c\u5206\u5272\uff0c\u5728Synapse\u4e0eBUSI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u53ef\u6bd4\u3002", "motivation": "\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\uff0c\u89e3\u5256\u53d8\u5f02\u6027\u5927\u4e14\u8fb9\u754c\u6a21\u7cca\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7a33\u5b9a\u5206\u5272\u7ec6\u5c0f\u7ed3\u6784\uff0c\u6545\u9700\u4e00\u79cd\u5728\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6a21\u578b\u4ee5\u63d0\u9ad8\u8fb9\u754c\u63cf\u7ed8\u4e0e\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eResBlock\u7684detail-aware Transformer\u9aa8\u5e72\uff0c\u7ed3\u5408\u4e09\u6a21\u5757\uff1aASBE\uff08\u81ea\u9002\u5e94\u8fb9\u754c\u589e\u5f3a\uff09\u3001HVDA\uff08\u7ec6\u7c92\u5ea6\u7279\u5f81\u5efa\u6a21\uff09\u548cEulerFF\uff08\u57fa\u4e8e\u6b27\u62c9\u516c\u5f0f\u7684\u878d\u5408\u6743\u91cd\uff09\u3002\u6574\u4f53\u91c7\u7528\u6df7\u5408\u5c40\u90e8\u5377\u79ef\u4e0e\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u8fb9\u754c\u548c\u7ec6\u8282\u4fdd\u7559\u3002", "result": "\u5728Synapse\u4e0eBUSI\u6570\u636e\u96c6\u4e0a\uff0cRDTE-UNet\u5728\u5206\u5272\u7cbe\u5ea6\u548c\u8fb9\u754c\u8d28\u91cf\u4e0a\u8fbe\u5230\u53ef\u6bd4\uff08\u6216\u4f18\uff09\u6c34\u5e73\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5f62\u6001\u3001\u65b9\u5411\u548c\u5c3a\u5ea6\u7684\u7ed3\u6784\u4e0a\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "RDTE-UNet\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7ec6\u8282\u5efa\u6a21\u4e0e\u5168\u5c40\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u80fd\u5728\u89e3\u5256\u591a\u6837\u6027\u548c\u8fb9\u754c\u6a21\u7cca\u573a\u666f\u4e0b\u6539\u5584\u7ec6\u5c0f\u7ed3\u6784\u7684\u5206\u5272\u548c\u8fb9\u754c\u7cbe\u5ea6\uff0c\u5c55\u793a\u51fa\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u8d28\u91cf\u7684\u63d0\u5347\u3002"}}
{"id": "2511.01340", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01340", "abs": "https://arxiv.org/abs/2511.01340", "authors": ["Trishanu Das", "Abhilash Nandy", "Khush Bajaj", "Deepiha S"], "title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles", "comment": "7 pages, 5 figures, 4 tables", "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse\nbenchmark of $1,333$ English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and\n$20-30\\%$ using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning.", "AI": {"tldr": "\u63d0\u4f9b1333\u9898\u7684Rebus\u6570\u636e\u96c6\u4e0eRebusDescProgICE\u6846\u67b6\uff0c\u901a\u8fc7\u63cf\u8ff0+\u4ee3\u7801\u5f0f\u63a8\u7406\u548c\u63a8\u7406\u5bfc\u5411\u793a\u4f8b\u9009\u62e9\uff0c\u5927\u5e45\u63d0\u5347VLM\u5728\u56fe\u8c1c\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u56fe\u8c1c\u9700\u8981\u56fe\u50cf\u8bc6\u522b\u4e0e\u590d\u6742\u7684\u6587\u5b57\u6e38\u620f\u63a8\u7406\uff0c\u73b0\u6709 VLM \u5728\u8fd9\u7c7b\u4efb\u52a1\u4e0a\u4e0d\u8db3\uff0c\u7f3a\u5c11\u5927\u89c4\u6a21\u3001\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u4e0e\u6709\u6548\u63a8\u7406\u8303\u5f0f\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u6570\u636e\u96c6\u4e0e\u65b0\u6846\u67b6\u63d0\u5347\u6a21\u578b\u5728\u56fe\u8c1c\u7406\u89e3\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u5305\u542b1333\u4e2a\u82f1\u6587\u56fe\u8c1c\u300118\u7c7b\u4e3b\u9898\u7684\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa RebusDescProgICE \u6846\u67b6\uff1a\u7ed3\u5408\u65e0\u7ed3\u6784\u7684\u56fe\u50cf\u63cf\u8ff0\uff08description\uff09\u3001\u57fa\u4e8e\u4ee3\u7801\u7684\u7ed3\u6784\u5316\u63a8\u7406\uff08program\uff09\u548c\u57fa\u4e8e\u63a8\u7406\u80fd\u529b\u7684\u793a\u4f8b\u9009\u62e9\uff08ICE\uff09\u3002\u4ed6\u4eec\u5728\u95ed\u6e90\u4e0e\u5f00\u6e90\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5e76\u4e0eChain-of-Thought\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728 |\u27f3[BUS]| \u6570\u636e\u96c6\u4e0a\uff0cRebusDescProgICE \u76f8\u8f83\u4e8eChain-of-Thought\uff0c\u4f7f\u95ed\u6e90\u6a21\u578b\u63d0\u53472.1\u20134.1%\uff0c\u5f00\u6e90\u6a21\u578b\u63d0\u534720\u201330%\u3002\u6570\u636e\u96c6\u8986\u76d618\u7c7b\u3001\u591a\u6837\u827a\u672f\u98ce\u683c\uff0c\u4e14\u63d0\u4f9b\u5206\u96be\u5ea6\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a |\u27f3[BUS]| \u7684\u5927\u578b\u4e2d\u6587\u6ce8\u91ca\uff1a\u82f1\u6587 Rebus\uff08\u56fe\u8c1c\uff09\u6570\u636e\u96c6\u4e0e\u4e00\u79cd\u901a\u7528\u7684\u6a21\u578b\u589e\u5f3a\u6846\u67b6 RebusDescProgICE\u3002\u4f5c\u8005\u8ba4\u4e3a\u56fe\u8c1c\u7406\u89e3\u9700\u8981\u89c6\u89c9\u8bc6\u522b\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u591a\u6b65\u7b26\u53f7\u63a8\u7406\u7b49\u591a\u79cd\u80fd\u529b\uff0c\u4f20\u7edf\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u6b20\u4f73\u3002"}}
{"id": "2511.01345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01345", "abs": "https://arxiv.org/abs/2511.01345", "authors": ["Jierui Qu", "Jianchun Zhao"], "title": "MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement", "comment": null, "summary": "Accurate segmentation of medical images is fundamental to tumor diagnosis and\ntreatment planning. SAM-based interactive segmentation has gained attention for\nits strong generalization, but most methods follow a\nsingle-point-to-single-object paradigm, which limits multi-lesion segmentation.\nMoreover, ViT backbones capture global context but often miss high-fidelity\nlocal details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework\nwith a competitive query optimization strategy that shifts from\nsingle-point-to-single-mask to single-point-to-multi-instance. A\nprompt-conditioned instance-query generator transforms a single point prompt\ninto multiple specialized queries, enabling retrieval of all semantically\nsimilar lesions across the 3D volume from a single exemplar. A hybrid\nCNN-Transformer encoder injects CNN-derived boundary saliency into ViT\nself-attention via spatial gating. A competitively optimized query decoder then\nenables end-to-end, parallel, multi-instance prediction through inter-query\ncompetition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels\nand exhibits strong robustness to prompts, providing a practical solution for\nefficient annotation of clinically relevant multi-lesion cases.", "AI": {"tldr": "\u63d0\u51faMIQ-SAM3D\uff1a\u5c06\u5355\u70b9\u63d0\u793a\u6269\u5c55\u4e3a\u591a\u5b9e\u4f8b\u67e5\u8be2\uff0c\u7ed3\u5408CNN-Transformer\u7684\u8fb9\u754c\u611f\u77e5\u7f16\u7801\u548c\u7ade\u4e89\u5f0f\u89e3\u7801\uff0c\u5b9e\u73b03D\u591a\u75c5\u7076\u5e76\u884c\u5206\u5272\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u53ef\u6bd4\u4e14\u5bf9\u63d0\u793a\u9c81\u68d2\u3002", "motivation": "\u52a8\u673a\u662f\u89e3\u51b3\u73b0\u6709SAM\u5f0f\u4ea4\u4e92\u5206\u5272\u901a\u5e38\u91c7\u7528\u5355\u70b9\u5bf9\u5355\u5bf9\u8c61\u8303\u5f0f\uff0c\u96be\u4ee5\u5904\u7406\u591a\u75c5\u7076\u573a\u666f\uff1b\u540c\u65f6ViT\u867d\u6355\u6349\u5168\u5c40\u4fe1\u606f\u4f46\u7f3a\u4e4f\u9ad8\u4fdd\u771f\u5c40\u90e8\u7ec6\u8282\uff0c\u9650\u5236\u533b\u5b66\u56fe\u50cf\u4e2d\u7ec6\u5c0f/\u8fb9\u754c\u6e05\u6670\u5ea6\u9700\u6c42\u3002", "method": "\u65b9\u6cd5\u5305\u62ec(1) \u63d0\u793a\u6761\u4ef6\u5b9e\u4f8b\u67e5\u8be2\u751f\u6210\u5668\uff1a\u5c06\u5355\u70b9\u63d0\u793a\u6620\u5c04\u4e3a\u591a\u6761\u4e13\u95e8\u5316\u67e5\u8be2\u4ee5\u68c0\u7d22\u8bed\u4e49\u76f8\u4f3c\u7684\u6240\u6709\u75c5\u7076\uff1b(2) \u6df7\u5408CNN-Transformer\u7f16\u7801\u5668\uff1a\u901a\u8fc7\u7a7a\u95f4\u95e8\u63a7\u5c06CNN\u63d0\u53d6\u7684\u8fb9\u754c\u663e\u8457\u6027\u6ce8\u5165ViT\u81ea\u6ce8\u610f\u529b\uff0c\u4ee5\u589e\u5f3a\u5c40\u90e8\u7ec6\u8282\uff1b(3) \u7ade\u4e89\u5f0f\u67e5\u8be2\u89e3\u7801\u5668\uff1a\u901a\u8fc7\u67e5\u8be2\u95f4\u7ade\u4e89\u673a\u5236\u5b9e\u73b0\u7aef\u5230\u7aef\u5e76\u884c\u591a\u5b9e\u4f8b\u9884\u6d4b\u3002", "result": "\u5728LiTS17\u548cKiTS21\u4e0a\uff0cMIQ-SAM3D\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u63d0\u793a\u4f4d\u7f6e\u548c\u6570\u76ee\u7684\u8f83\u5f3a\u9c81\u68d2\u6027\uff0c\u80fd\u9ad8\u6548\u6807\u6ce8\u591a\u75c5\u7076\u75c5\u4f8b\u3002", "conclusion": "MIQ-SAM3D\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u70b9\u63d0\u793a\u6269\u5c55\u5230\u591a\u5b9e\u4f8b\u9884\u6d4b\u7684\u4e09\u7ef4\u4ea4\u4e92\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u751f\u6210\u5668\u3001\u6df7\u5408CNN-Transformer\u7f16\u7801\u5668\u548c\u7ade\u4e89\u5f0f\u89e3\u7801\u5668\uff0c\u5b9e\u73b0\u5bf9\u591a\u75c5\u7076\u7684\u5e76\u884c\u68c0\u7d22\u4e0e\u5206\u5272\u3002\u5b9e\u9a8c\u5728LiTS17\u548cKiTS21\u4e0a\u8868\u73b0\u51fa\u53ef\u6bd4\u7684\u5206\u5272\u6027\u80fd\u548c\u5bf9\u63d0\u793a\u9c81\u68d2\u6027\u7684\u589e\u5f3a\uff0c\u5177\u6709\u4e34\u5e8a\u6ce8\u91ca\u6548\u7387\u63d0\u5347\u6f5c\u529b\u3002"}}
{"id": "2511.01355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01355", "abs": "https://arxiv.org/abs/2511.01355", "authors": ["Linhao Huang"], "title": "Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion", "comment": null, "summary": "Recent advancements in text-to-image diffusion models have significantly\nimproved the personalization and stylization of generated images. However,\nprevious studies have only assessed content similarity under a single style\nintensity. In our experiments, we observe that increasing style intensity leads\nto a significant loss of content features, resulting in a suboptimal\ncontent-style frontier. To address this, we propose a novel approach to expand\nthe content-style frontier by leveraging Content-Style Subspace Blending and a\nContent-Style Balance loss. Our method improves content similarity across\nvarying style intensities, significantly broadening the content-style frontier.\nExtensive experiments demonstrate that our approach outperforms existing\ntechniques in both qualitative and quantitative evaluations, achieving superior\ncontent-style trade-off with significantly lower Inverted Generational Distance\n(IGD) and Generational Distance (GD) scores compared to current methods.", "AI": {"tldr": "\u63d0\u51fa\u5185\u5bb9-\u98ce\u683c\u5b50\u7a7a\u95f4\u6df7\u5408\u4e0e\u5e73\u8861\u635f\u5931\uff0c\u6269\u5927\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\uff0c\u5728\u66f4\u9ad8\u98ce\u683c\u5f3a\u5ea6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u5185\u5bb9\uff0cIGD/GD\u663e\u8457\u964d\u4f4e\u3002", "motivation": "\u89c2\u5bdf\u5230\u968f\u7740\u98ce\u683c\u5f3a\u5ea6\u589e\u52a0\uff0c\u751f\u6210\u56fe\u50cf\u7684\u5185\u5bb9\u7279\u5f81\u663e\u8457\u4e22\u5931\uff0c\u5bfc\u81f4\u5f53\u524d\u65b9\u6cd5\u7684\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\u53d7\u9650\uff0c\u56e0\u6b64\u5e0c\u671b\u6269\u5c55\u524d\u6cbf\u4ee5\u63d0\u5347\u98ce\u683c\u591a\u6837\u6027\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "method": "\u6784\u5efa\u5185\u5bb9-\u98ce\u683c\u5b50\u7a7a\u95f4\u5e76\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u5b50\u7a7a\u95f4\u6df7\u5408\uff0c\u540c\u65f6\u5f15\u5165\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u635f\u5931\u4ee5\u7ea6\u675f\u5728\u9ad8\u98ce\u683c\u5f3a\u5ea6\u4e0b\u4fdd\u7559\u5185\u5bb9\u7279\u5f81\uff1b\u8bad\u7ec3\u6216\u5fae\u8c03\u6a21\u578b\u4ee5\u4f18\u5316\u8be5\u635f\u5931\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u597d\u7684\u5185\u5bb9-\u98ce\u683c\u6743\u8861\u3002", "result": "\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u5728\u4e0d\u540c\u98ce\u683c\u5f3a\u5ea6\u4e0b\u5747\u80fd\u4fdd\u6301\u66f4\u9ad8\u7684\u5185\u5bb9\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u964d\u4f4eIGD\u548cGD\uff0c\u8868\u660e\u5185\u5bb9-\u98ce\u683c\u6743\u8861\u66f4\u4f18\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u5185\u5bb9-\u98ce\u683c\u5b50\u7a7a\u95f4\u6df7\u5408\u4e0e\u5185\u5bb9-\u98ce\u683c\u5e73\u8861\u635f\u5931\u6269\u5c55\u5185\u5bb9-\u98ce\u683c\u524d\u6cbf\uff0c\u80fd\u5728\u4e0d\u540c\u98ce\u683c\u5f3a\u5ea6\u4e0b\u4fdd\u6301\u66f4\u597d\u7684\u5185\u5bb9\u76f8\u4f3c\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728IGD\u548cGD\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.01357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01357", "abs": "https://arxiv.org/abs/2511.01357", "authors": ["Qiangguo Jin", "Xianyao Zheng", "Hui Cui", "Changming Sun", "Yuqi Fang", "Cong Cong", "Ran Su", "Leyi Wei", "Ping Xuan", "Junbo Wang"], "title": "CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering", "comment": "The paper has been accepted by the 33rd Pacific Conference on\n  Computer Graphics and Applications (Pacific Graphics 2025)", "summary": "Medical visual question answering (Med-VQA) is a crucial multimodal task in\nclinical decision support and telemedicine. Recent self-attention based methods\nstruggle to effectively handle cross-modal semantic alignments between vision\nand language. Moreover, classification-based methods rely on predefined answer\nsets. Treating this task as a simple classification problem may make it unable\nto adapt to the diversity of free-form answers and overlook the detailed\nsemantic information of free-form answers. In order to tackle these challenges,\nwe introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)\nframework that learns cross-modal feature representations from images and\ntexts. CMI-MTL comprises three key modules: fine-grained visual-text feature\nalignment (FVTA), cross-modal interleaved feature representation (CIFR), and\nfree-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most\nrelevant regions in image-text pairs through fine-grained visual-text feature\nalignment. CIFR captures cross-modal sequential interactions via cross-modal\ninterleaved feature representation. FFAE leverages auxiliary knowledge from\nopen-ended questions through free-form answer-enhanced multi-task learning,\nimproving the model's capability for open-ended Med-VQA. Experimental results\nshow that CMI-MTL outperforms the existing state-of-the-art methods on three\nMed-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more\ninterpretability experiments to prove the effectiveness. The code is publicly\navailable at https://github.com/BioMedIA-repo/CMI-MTL.", "AI": {"tldr": "\u63d0\u51faCMI-MTL\uff1a\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3001\u4ea4\u53c9\u6a21\u6001\u4ea4\u9519\u8868\u793a\u548c\u81ea\u7531\u7b54\u6848\u589e\u5f3a\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u6539\u5584Med-VQA\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5f00\u653e\u5f0f\u56de\u7b54\u80fd\u529b\uff0c\u5728\u591a\u6570\u636e\u96c6\u4e0a\u8fbe\u6210SOTA\u4e14\u5177\u66f4\u597d\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u6ce8\u610f\u529b\u65b9\u6cd5\u96be\u4ee5\u5145\u5206\u5bf9\u9f50\u89c6\u89c9\u4e0e\u8bed\u8a00\u8bed\u4e49\uff0c\u4e14\u5206\u7c7b\u5f0f\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b54\u6848\u96c6\uff0c\u65e0\u6cd5\u9002\u5e94\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u7684\u591a\u6837\u6027\u548c\u7ec6\u8282\u8bed\u4e49\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u8de8\u6a21\u6001\u8868\u793a\u4e0e\u81ea\u7531\u7b54\u6848\u589e\u5f3a\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e09\u6a21\u5757\u6846\u67b6\uff1aFVTA\u7528\u4e8e\u56fe\u6587\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4ee5\u5b9a\u4f4d\u76f8\u5173\u533a\u57df\uff0cCIFR\u7528\u4e8e\u6355\u6349\u8de8\u6a21\u6001\u5e8f\u5217\u4ea4\u4e92\uff0cFFAE\u901a\u8fc7\u5c06\u81ea\u7531\u5f62\u5f0f\u7b54\u6848\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u63d0\u5347\u5f00\u653e\u5f0f\u56de\u7b54\u80fd\u529b\uff0c\u6574\u4f53\u91c7\u7528\u4ea4\u53c9\u6a21\u6001\u4ea4\u4e92\u7684\u591a\u4efb\u52a1\u8bad\u7ec3\u3002", "result": "\u5728VQA-RAD\u3001SLAKE\u548cOVQA\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cCMI-MTL\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff1b\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5bf9\u76f8\u5173\u56fe\u50cf\u533a\u57df\u4e0e\u6587\u672c\u4fe1\u606f\u7684\u6709\u6548\u6355\u83b7\u3002", "conclusion": "CMI-MTL\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u3001\u4ea4\u53c9\u6a21\u6001\u4ea4\u9519\u8868\u793a\u548c\u81ea\u7531\u6587\u672c\u589e\u5f3a\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86Med-VQA\u5728\u5f00\u653e\u5f0f\u95ee\u7b54\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2511.01381", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01381", "abs": "https://arxiv.org/abs/2511.01381", "authors": ["Hitesh Kyatham", "Arjun Suresh", "Aadi Palnitkar", "Yiannis Aloimonos"], "title": "EREBUS: End-to-end Robust Event Based Underwater Simulation", "comment": "Accepted to ICRA AQUA2SIM Workshop 2025, 6 pages, 3 figures,\n  conference paper", "summary": "The underwater domain presents a vast array of challenges for roboticists and\ncomputer vision researchers alike, such as poor lighting conditions and high\ndynamic range scenes. In these adverse conditions, traditional vision\ntechniques struggle to adapt and lead to suboptimal performance. Event-based\ncameras present an attractive solution to this problem, mitigating the issues\nof traditional cameras by tracking changes in the footage on a frame-by-frame\nbasis. In this paper, we introduce a pipeline which can be used to generate\nrealistic synthetic data of an event-based camera mounted to an AUV (Autonomous\nUnderwater Vehicle) in an underwater environment for training vision models. We\ndemonstrate the effectiveness of our pipeline using the task of rock detection\nwith poor visibility and suspended particulate matter, but the approach can be\ngeneralized to other underwater tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u751f\u6210\u6c34\u4e0b\u4e8b\u4ef6\u76f8\u673a\u5408\u6210\u6570\u636e\u7684\u6d41\u6c34\u7ebf\uff0c\u5e76\u5728\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u5176\u5728\u4f4e\u80fd\u89c1\u5ea6\u73af\u5883\u4e0b\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u5e27\u5f0f\u76f8\u673a\u5728\u6c34\u4e0b\u5149\u7167\u5dee\u3001\u9ad8\u52a8\u6001\u8303\u56f4\u548c\u80fd\u89c1\u5ea6\u5dee\u7684\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e8b\u4ef6\u76f8\u673a\u901a\u8fc7\u4ec5\u8bb0\u5f55\u50cf\u7d20\u4eae\u5ea6\u53d8\u5316\u53ef\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u5408\u6210\u7684\u6570\u636e\u6765\u8bad\u7ec3\u548c\u8bc4\u4f30\u57fa\u4e8e\u4e8b\u4ef6\u76f8\u673a\u7684\u6c34\u4e0b\u89c6\u89c9\u7b97\u6cd5\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4eff\u771f\u6d41\u6c34\u7ebf\uff0c\u5c06\u4e8b\u4ef6\u76f8\u673a\u6a21\u578b\u4e0eAUV\u8fd0\u52a8\u548c\u6c34\u4e0b\u5149\u5b66\u6761\u4ef6\uff08\u5982\u6563\u5c04\u3001\u5438\u6536\u548c\u60ac\u6d6e\u9897\u7c92\uff09\u8026\u5408\uff0c\u751f\u6210\u903c\u771f\u7684\u4e8b\u4ef6\u6d41\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5728\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\uff0c\u6d41\u6c34\u7ebf\u751f\u6210\u7684\u6570\u636e\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u5728\u4f4e\u80fd\u89c1\u5ea6\u53ca\u60ac\u6d6e\u9897\u7c92\u73af\u5883\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u63a8\u5e7f\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5728\u6c34\u4e0b\u73af\u5883\u4e2d\u751f\u6210\u4e8b\u4ef6\u76f8\u673a\uff08event-based camera\uff09\u5408\u6210\u6570\u636e\u7684\u6d41\u6c34\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u80fd\u89c1\u5ea6\u4f4e\u548c\u60ac\u6d6e\u9897\u7c92\u5b58\u5728\u6761\u4ef6\u4e0b\u7528\u4e8e\u5ca9\u77f3\u68c0\u6d4b\u4efb\u52a1\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.01390", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01390", "abs": "https://arxiv.org/abs/2511.01390", "authors": ["Xinyu Mao", "Junsi Li", "Haoji Zhang", "Yu Liang", "Ming Sun"], "title": "SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment", "comment": null, "summary": "Fine-grained cross-modal alignment aims to establish precise local\ncorrespondences between vision and language, forming a cornerstone for visual\nquestion answering and related multimodal applications. Current approaches face\nchallenges in addressing patch redundancy and ambiguity, which arise from the\ninherent information density disparities across modalities. Recently,\nMultimodal Large Language Models (MLLMs) have emerged as promising solutions to\nbridge this gap through their robust semantic generation capabilities. However,\nthe dense textual outputs from MLLMs may introduce conflicts with the original\nsparse captions. Furthermore, accurately quantifying semantic relevance between\nrich visual patches and concise textual descriptions remains a core challenge.\nTo overcome these limitations, we introduce the Semantic-Enhanced Patch\nSlimming (SEPS) framework, which systematically addresses patch redundancy and\nambiguity. Our approach employs a two-stage mechanism to integrate unified\nsemantics from both dense and sparse texts, enabling the identification of\nsalient visual patches. Additionally, it leverages relevance-aware selection\nwith mean value computation to highlight crucial patch-word correspondences,\nthereby improving cross-modal similarity assessment. Comprehensive experiments\non Flickr30K and MS-COCO datasets validate that SEPS achieves superior\nperformance, surpassing existing approaches by 23\\%-86\\% in rSum across diverse\nmodel architectures, with notable enhancements in text-to-image retrieval\nscenarios. Our implementation is available at\nhttps://github.com/Sweet4tars/seps.git.", "AI": {"tldr": "\u63d0\u51faSEPS\uff0c\u7528\u7a00\u758f\u4e0e\u81f4\u5bc6\u6587\u672c\u7edf\u4e00\u8bed\u4e49\u5e76\u8fdb\u884c\u76f8\u5173\u6027\u611f\u77e5\u7684patch\u7b5b\u9009\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u4e0e\u68c0\u7d22\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u56e0\u6a21\u6001\u4fe1\u606f\u5bc6\u5ea6\u5dee\u5f02\u5bfc\u81f4\u7684patch\u5197\u4f59\u4e0e\u8bed\u4e49\u6b67\u4e49\uff1bMLLM\u867d\u80fd\u751f\u6210\u4e30\u5bcc\u8bed\u4e49\u4f46\u6587\u672c\u81f4\u5bc6\u5ea6\u4e0e\u539f\u59cb\u7a00\u758fcaption\u51b2\u7a81\uff0c\u4e14\u96be\u4ee5\u91cf\u5316rich patch\u4e0econcise text\u7684\u8bed\u4e49\u76f8\u5173\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u673a\u5236\uff1a\u4e00\u662f\u4eceMLLM\u751f\u6210\u7684\u81f4\u5bc6\u6587\u672c\u4e0e\u539f\u59cb\u7a00\u758f\u63cf\u8ff0\u878d\u5408\u7edf\u4e00\u8bed\u4e49\u4ee5\u8bc6\u522b\u663e\u8457\u8865\u4e01\uff1b\u4e8c\u662f\u57fa\u4e8e\u76f8\u5173\u6027\u548c\u5747\u503c\u8ba1\u7b97\u7684\u9009\u62e9\u7b56\u7565\uff0c\u5f3a\u5316\u5173\u952epatch-word\u5bf9\u5e94\u5e76\u7528\u4e8e\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u3002", "result": "\u5728Flickr30K\u548cMS-COCO\u4e0a\uff0cSEPS\u5728rSum\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534723%-86%\uff0c\u5728text-to-image\u68c0\u7d22\u4efb\u52a1\u4e0a\u63d0\u5347\u5c24\u4e3a\u663e\u8457\u3002", "conclusion": "SEPS\u901a\u8fc7\u7ed3\u5408\u7a00\u758f\u4e0e\u81f4\u5bc6\u6587\u672c\u8bed\u4e49\u5e76\u8fdb\u884c\u76f8\u5173\u6027\u611f\u77e5\u7684patch\u7b5b\u9009\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u89c9\u8865\u4e01\u7684\u5197\u4f59\u4e0e\u6a21\u7cca\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5bf9\u9f50\u4e0e\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2511.01399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01399", "abs": "https://arxiv.org/abs/2511.01399", "authors": ["Ya Wen", "Yutong Qiao", "Chi Chiu Lam", "Ioannis Brilakis", "Sanghoon Lee", "Mun On Wong"], "title": "Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction", "comment": null, "summary": "Inventory management of firefighting assets is crucial for emergency\npreparedness, risk assessment, and on-site fire response. However, conventional\nmethods are inefficient due to limited capabilities in automated asset\nrecognition and reconstruction. To address the challenge, this research\nintroduces the Fire-ART dataset and develops a panoramic image-based\nreconstruction approach for semantic enrichment of firefighting assets into BIM\nmodels. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626\nimages and 6,627 instances, making it an extensive and publicly accessible\ndataset for asset recognition. In addition, the reconstruction approach\nintegrates modified cube-map conversion and radius-based spherical camera\nprojection to enhance recognition and localization accuracy. Through\nvalidations with two real-world case studies, the proposed approach achieves\nF1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,\nrespectively. The Fire-ART dataset and the reconstruction approach offer\nvaluable resources and robust technical solutions to enhance the accurate\ndigital management of fire safety equipment.", "AI": {"tldr": "\u53d1\u5e03\u4e86Fire-ART\u6570\u636e\u96c6\u5e76\u63d0\u51fa\u57fa\u4e8e\u6539\u8fdb\u7acb\u65b9\u6620\u5c04\u4e0e\u7403\u9762\u6295\u5f71\u7684\u5168\u666f\u56fe\u5230BIM\u7684\u6d88\u9632\u8d44\u4ea7\u8bed\u4e49\u5316\u91cd\u5efa\u65b9\u6cd5\uff0c\u5b9e\u6d4bF1 73%/88%\u3001\u5b9a\u4f4d\u8bef\u5dee0.620/0.428m\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u81ea\u52a8\u8bc6\u522b\u4e0e\u91cd\u5efa\u6d88\u9632\u8d44\u4ea7\u65b9\u9762\u6548\u7387\u4f4e\u3001\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u9488\u5bf9\u5168\u666f\u56fe\u7684\u91cd\u5efa\u65b9\u6cd5\u6765\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u7684\u8d44\u4ea7\u8bed\u4e49\u5316\u5165BIM\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b15\u7c7b\u6d88\u9632\u8d44\u4ea7\u7684Fire-ART\u6570\u636e\u96c6\uff082626\u5f20\u56fe\u7247\u30016627\u4e2a\u5b9e\u4f8b\uff09\uff1b\u63d0\u51fa\u57fa\u4e8e\u5168\u666f\u56fe\u7684\u91cd\u5efa\u6d41\u7a0b\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u7acb\u65b9\u6620\u5c04\u548c\u57fa\u4e8e\u534a\u5f84\u7684\u7403\u9762\u76f8\u673a\u6295\u5f71\u7528\u4e8e\u63d0\u9ad8\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u6848\u4f8b\u9a8c\u8bc1\u4e2d\uff0c\u65b9\u6cd5\u5206\u522b\u8fbe\u5230\u4e86F1\u5206\u657073%\u548c88%\uff0c\u5b9a\u4f4d\u8bef\u5dee\u5206\u522b\u4e3a0.620\u7c73\u548c0.428\u7c73\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u4e0a\u5177\u6709\u5b9e\u7528\u6027\u4e0e\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u5168\u666f\u56fe\u50cf\u5230BIM\u7684\u8d44\u4ea7\u8bed\u4e49\u91cd\u5efa\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86Fire-ART\u6d88\u9632\u8d44\u4ea7\u6570\u636e\u96c6\uff0c\u4e3a\u6d88\u9632\u8bbe\u5907\u7684\u6570\u5b57\u5316\u7ba1\u7406\u63d0\u4f9b\u4e86\u6570\u636e+\u65b9\u6cd5\u652f\u6301\u3002"}}
{"id": "2511.01411", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01411", "abs": "https://arxiv.org/abs/2511.01411", "authors": ["Reza Karimzadeh", "Albert Alonso", "Frans Zdyb", "Julius B. Kirkegaard", "Bulat Ibragimov"], "title": "Extremal Contours: Gradient-driven contours for compact visual attribution", "comment": null, "summary": "Faithful yet compact explanations for vision models remain a challenge, as\ncommonly used dense perturbation masks are often fragmented and overfitted,\nneeding careful post-processing. Here, we present a training-free explanation\nmethod that replaces dense masks with smooth tunable contours. A star-convex\nregion is parameterized by a truncated Fourier series and optimized under an\nextremal preserve/delete objective using the classifier gradients. The approach\nguarantees a single, simply connected mask, cuts the number of free parameters\nby orders of magnitude, and yields stable boundary updates without cleanup.\nRestricting solutions to low-dimensional, smooth contours makes the method\nrobust to adversarial masking artifacts. On ImageNet classifiers, it matches\nthe extremal fidelity of dense masks while producing compact, interpretable\nregions with improved run-to-run consistency. Explicit area control also\nenables importance contour maps, yielding a transparent fidelity-area profiles.\nFinally, we extend the approach to multi-contour and show how it can localize\nmultiple objects within the same framework. Across benchmarks, the method\nachieves higher relevance mass and lower complexity than gradient and\nperturbation based baselines, with especially strong gains on self-supervised\nDINO models where it improves relevance mass by over 15% and maintains positive\nfaithfulness correlations.", "AI": {"tldr": "\u7528\u622a\u65ad\u5085\u91cc\u53f6\u8868\u793a\u7684\u661f\u72b6\u51f8\u5149\u6ed1\u8f6e\u5ed3\u66ff\u4ee3\u7a20\u5bc6\u63a9\u7801\uff0c\u901a\u8fc7\u5728\u4fdd\u7559/\u5220\u9664\u76ee\u6807\u4e0b\u7528\u5206\u7c7b\u5668\u68af\u5ea6\u4f18\u5316\uff0c\u751f\u6210\u5355\u4e00\u8fde\u901a\u3001\u7d27\u51d1\u3001\u9c81\u68d2\u4e14\u8bad\u7ec3\u514d\u8d39\u7684\u89c6\u89c9\u89e3\u91ca\uff0c\u6548\u679c\u5728ImageNet\u53caDINO\u6a21\u578b\u4e0a\u663e\u8457\u4f18\u4e8e\u82e5\u5e72\u57fa\u7ebf\u3002", "motivation": "\u52a8\u673a\u662f\u73b0\u6709\u7a20\u5bc6\u6270\u52a8\u63a9\u7801\u5f80\u5f80\u788e\u7247\u5316\u3001\u8fc7\u62df\u5408\u4e14\u9700\u8981\u540e\u5904\u7406\uff0c\u4e14\u5bb9\u6613\u53d7\u5bf9\u6297\u6027\u63a9\u7801\u4f2a\u88c5\u3002\u5e0c\u671b\u7528\u66f4\u7d27\u51d1\u3001\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u8f6e\u5ed3\u5316\u63a9\u7801\u6765\u63d0\u5347\u5fe0\u5b9e\u6027\u4e0e\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u53c2\u6570\u590d\u6742\u5ea6\u548c\u5bf9\u540e\u5904\u7406\u7684\u4f9d\u8d56\u3002", "method": "\u65b9\u6cd5\u5c06\u661f\u72b6\u51f8\uff08star-convex\uff09\u533a\u57df\u53c2\u6570\u5316\u4e3a\u622a\u65ad\u5085\u91cc\u53f6\u7ea7\u6570\uff0c\u5e76\u5728\u6781\u503c\u4fdd\u7559/\u5220\u9664\uff08extremal preserve/delete\uff09\u76ee\u6807\u4e0b\u4f7f\u7528\u5206\u7c7b\u5668\u68af\u5ea6\u8fdb\u884c\u4f18\u5316\u3002\u901a\u8fc7\u9650\u5236\u4e3a\u4f4e\u7ef4\u5149\u6ed1\u8f6e\u5ed3\uff0c\u5927\u5e45\u51cf\u5c11\u81ea\u7531\u53c2\u6570\uff0c\u4fdd\u8bc1\u7b80\u5355\u8fde\u901a\u7684\u63a9\u7801\u3001\u7a33\u5b9a\u7684\u8fb9\u754c\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u663e\u5f0f\u9762\u79ef\u63a7\u5236\u751f\u6210\u91cd\u8981\u6027\u8f6e\u5ed3\u56fe\u3002\u6269\u5c55\u5230\u591a\u8f6e\u5ed3\u53ef\u4ee5\u5728\u540c\u4e00\u6846\u67b6\u4e0b\u5b9a\u4f4d\u591a\u4e2a\u5bf9\u8c61\u3002", "result": "\u5728ImageNet\u4e0e\u5404\u79cd\u57fa\u7ebf\u6bd4\u8f83\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u5339\u914d\u7a20\u5bc6\u63a9\u7801\u7684\u6781\u503c\u5fe0\u5b9e\u5ea6\u7684\u540c\u65f6\uff0c\u8f93\u51fa\u66f4\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u4e14\u8fd0\u884c\u95f4\u4e00\u81f4\u6027\u66f4\u597d\u7684\u533a\u57df\uff1b\u5bf9\u81ea\u76d1\u7763DINO\u6a21\u578b\u5c24\u4e3a\u660e\u663e\uff0c\u76f8\u5173\u6027\u8d28\u91cf\u63d0\u9ad8\u8d85\u8fc715%\uff0c\u4e14\u5728\u76f8\u5173\u6027\u8d28\u91cf\uff08relevance mass\uff09\u4e0e\u590d\u6742\u5ea6\uff08complexity\uff09\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u68af\u5ea6\u548c\u6270\u52a8\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\uff08training-free\uff09\u7684\u89c6\u89c9\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\uff0c\u7528\u53ef\u8c03\u5149\u6ed1\u8f6e\u5ed3\u66ff\u4ee3\u5e38\u89c1\u7684\u7a20\u5bc6\u6270\u52a8\u63a9\u7801\uff0c\u4ece\u800c\u751f\u6210\u5355\u4e00\u3001\u8fde\u901a\u3001\u7d27\u51d1\u4e14\u7a33\u5b9a\u7684\u89e3\u91ca\u533a\u57df\u3002"}}
{"id": "2511.01419", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01419", "abs": "https://arxiv.org/abs/2511.01419", "authors": ["Yongqi Yang", "Huayang Huang", "Xu Peng", "Xiaobin Hu", "Donghao Luo", "Jiangning Zhang", "Chengjie Wang", "Yu Wu"], "title": "Towards One-step Causal Video Generation via Adversarial Self-Distillation", "comment": "Under double-blind review as a conference paper", "summary": "Recent hybrid video generation models combine autoregressive temporal\ndynamics with diffusion-based spatial denoising, but their sequential,\niterative nature leads to error accumulation and long inference times. In this\nwork, we propose a distillation-based framework for efficient causal video\ngeneration that enables high-quality synthesis with extremely limited denoising\nsteps. Our approach builds upon the Distribution Matching Distillation (DMD)\nframework and proposes a novel Adversarial Self-Distillation (ASD) strategy,\nwhich aligns the outputs of the student model's n-step denoising process with\nits (n+1)-step version at the distribution level. This design provides smoother\nsupervision by bridging small intra-student gaps and more informative guidance\nby combining teacher knowledge with locally consistent student behavior,\nsubstantially improving training stability and generation quality in extremely\nfew-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame\nEnhancement (FFE) strategy, which allocates more denoising steps to the initial\nframes to mitigate error propagation while applying larger skipping steps to\nlater frames. Extensive experiments on VBench demonstrate that our method\nsurpasses state-of-the-art approaches in both one-step and two-step video\ngeneration. Notably, our framework produces a single distilled model that\nflexibly supports multiple inference-step settings, eliminating the need for\nrepeated re-distillation and enabling efficient, high-quality video synthesis.", "AI": {"tldr": "\u63d0\u51faASD+FFE\u7684\u84b8\u998f\u6846\u67b6\uff0c\u4f7f\u56e0\u679c\u89c6\u9891\u6269\u6563\u6a21\u578b\u57281-2\u6b65\u5185\u9ad8\u6548\u7a33\u5b9a\u751f\u6210\uff0c\u514b\u670d\u8bef\u5dee\u7d2f\u79ef\u5e76\u652f\u6301\u591a\u6b65\u6570\u7075\u6d3b\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u81ea\u56de\u5f52\u65f6\u5e8f\u4e0e\u6269\u6563\u7a7a\u95f4\u53bb\u566a\u7684\u65b9\u6cd5\u63a8\u7406\u8fc7\u7a0b\u8fed\u4ee3\u4e14\u6613\u7d2f\u79ef\u8bef\u5dee\uff0c\u5bfc\u81f4\u63a8\u7406\u6162\u4e0e\u8d28\u91cf\u4e0b\u964d\u3002\u76ee\u6807\u662f\u5728\u6781\u5c11\u53bb\u566a\u6b65\u6570\u4e0b\u4fdd\u6301\u9ad8\u8d28\u91cf\u5e76\u52a0\u901f\u63a8\u7406\u3002", "method": "\u5728Distribution Matching Distillation(DMD)\u57fa\u7840\u4e0a\u5f15\u5165Adversarial Self-Distillation(ASD)\uff0c\u5c06\u5b66\u751f\u6a21\u578bn\u6b65\u8f93\u51fa\u4e0e(n+1)\u6b65\u8f93\u51fa\u5728\u5206\u5e03\u5c42\u9762\u5bf9\u9f50\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u4ee5\u63d0\u4f9b\u66f4\u5e73\u6ed1\u548c\u66f4\u5177\u4fe1\u606f\u6027\u7684\u76d1\u7763\uff1b\u540c\u65f6\u63d0\u51faFirst-Frame Enhancement(FFE)\uff0c\u5bf9\u9996\u5e27\u5206\u914d\u66f4\u591a\u53bb\u566a\u6b65\u6570\u4ee5\u7f13\u89e3\u8bef\u5dee\u7d2f\u79ef\uff0c\u5bf9\u540e\u7eed\u5e27\u91c7\u7528\u66f4\u5927\u8df3\u6b65\u3002", "result": "\u5728VBench\u6570\u636e\u96c6\u4e0a\uff0c\u4f5c\u8005\u7684\u65b9\u6cd5\u5728\u4e00\u6b65\u548c\u4e24\u6b65\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u6700\u65b0\u65b9\u6cd5\uff1b\u751f\u6210\u6a21\u578b\u4e3a\u5355\u4e00\u84b8\u998f\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u63a8\u7406\u6b65\u6570\u8bbe\u7f6e\uff0c\u7701\u53bb\u53cd\u590d\u84b8\u998f\u5e76\u9ad8\u6548\u5408\u6210\u9ad8\u8d28\u91cf\u89c6\u9891\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u84b8\u998f\u7684\u9ad8\u6548\u56e0\u679c\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u5b66\u751f\u6a21\u578b\u4e0d\u540c\u6b65\u6570\u53bb\u566a\u8f93\u51fa\u8fdb\u884c\u5206\u5e03\u7ea7\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u6781\u5c11\u6b65\u6570\uff081-2\u6b65\uff09\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u8d28\u91cf\u751f\u6210\uff0c\u4e14\u652f\u6301\u4e00\u4e2a\u6a21\u578b\u5bf9\u5e94\u591a\u79cd\u63a8\u7406\u6b65\u6570\u3002"}}
{"id": "2511.01427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01427", "abs": "https://arxiv.org/abs/2511.01427", "authors": ["Yinchao Ma", "Yuyang Tang", "Wenfei Yang", "Tianzhu Zhang", "Xu Zhou", "Feng Wu"], "title": "UniSOT: A Unified Framework for Multi-Modality Single Object Tracking", "comment": "The paper has been accepted by TPAMI", "summary": "Single object tracking aims to localize target object with specific reference\nmodalities (bounding box, natural language or both) in a sequence of specific\nvideo modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different\nreference modalities enable various human-machine interactions, and different\nvideo modalities are demanded in complex scenarios to enhance tracking\nrobustness. Existing trackers are designed for single or several video\nmodalities with single or several reference modalities, which leads to separate\nmodel designs and limits practical applications. Practically, a unified tracker\nis needed to handle various requirements. To the best of our knowledge, there\nis still no tracker that can perform tracking with these above reference\nmodalities across these video modalities simultaneously. Thus, in this paper,\nwe present a unified tracker, UniSOT, for different combinations of three\nreference modalities and four video modalities with uniform parameters.\nExtensive experimental results on 18 visual tracking, vision-language tracking\nand RGB+X tracking benchmarks demonstrate that UniSOT shows superior\nperformance against modality-specific counterparts. Notably, UniSOT outperforms\nprevious counterparts by over 3.0\\% AUC on TNL2K across all three reference\nmodalities and outperforms Un-Track by over 2.0\\% main metric across all three\nRGB+X video modalities.", "AI": {"tldr": "\u63d0\u51faUniSOT\uff0c\u4e00\u4e2a\u7edf\u4e00\u4e14\u53c2\u6570\u5171\u4eab\u7684\u5355\u76ee\u6807\u8ddf\u8e2a\u5668\uff0c\u80fd\u540c\u65f6\u652f\u6301\u591a\u79cd\u53c2\u8003\u4e0e\u89c6\u9891\u6a21\u6001\uff0c\u5728\u591a\u9879\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8ddf\u8e2a\u5668\u901a\u5e38\u53ea\u9488\u5bf9\u5c11\u6570\u89c6\u9891\u6216\u53c2\u8003\u6a21\u6001\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u6a21\u578b\u5206\u88c2\u3001\u5b9e\u9645\u5e94\u7528\u53d7\u9650\u3002\u4e3a\u6ee1\u8db3\u5b9e\u9645\u573a\u666f\u4e2d\u591a\u6837\u5316\u7684\u4eba\u673a\u4ea4\u4e92\u9700\u6c42\u4e0e\u590d\u6742\u89c6\u9891\u611f\u77e5\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u4e2a\u80fd\u540c\u65f6\u5904\u7406\u591a\u79cd\u53c2\u8003\u4e0e\u89c6\u9891\u6a21\u6001\u7684\u7edf\u4e00\u8ddf\u8e2a\u5668\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7edf\u4e00\u67b6\u6784\uff08\u672a\u5728\u6458\u8981\u7ed9\u51fa\u5177\u4f53\u7ec6\u8282\uff09\uff0c\u80fd\u5904\u7406\u4e0d\u540c\u7684\u53c2\u8003\u8f93\u5165\u4e0e\u89c6\u9891\u8f93\u5165\uff0c\u5e76\u572818\u4e2a\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4e0e\u6d4b\u8bd5\uff0c\u4ece\u800c\u5728\u5355\u6a21\u6001\u3001\u89c6\u89c9-\u8bed\u8a00\u548cRGB+X\u8ddf\u8e2a\u4efb\u52a1\u4e0a\u5171\u4eab\u53c2\u6570\u5e76\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u572818\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cUniSOT\u4f18\u4e8e\u6a21\u6001\u4e13\u7528\u65b9\u6cd5\uff1a\u5728TNL2K\u6570\u636e\u96c6\u4e0a\u5bf9\u4e09\u79cd\u53c2\u8003\u6a21\u6001\u7684\u5e73\u5747AUC\u63d0\u5347\u8d85\u8fc73.0%\uff1b\u5728\u591a\u79cdRGB+X\u89c6\u9891\u6a21\u6001\u4e0a\u76f8\u8f83Un-Track\u5728\u4e3b\u8981\u6307\u6807\u4e0a\u63d0\u5347\u8d85\u8fc72.0%\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5355\u76ee\u6807\u8ddf\u8e2a\u5668UniSOT\uff0c\u80fd\u5728\u76f8\u540c\u53c2\u6570\u4e0b\u652f\u6301\u4e09\u79cd\u53c2\u8003\u6a21\u6001\uff08\u8fb9\u754c\u6846\u3001\u81ea\u7136\u8bed\u8a00\u6216\u4e24\u8005\uff09\u4e0e\u56db\u79cd\u89c6\u9891\u6a21\u6001\uff08RGB\u3001RGB+Depth\u3001RGB+Thermal\u3001RGB+Event\uff09\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u7684\u8ddf\u8e2a\u7edf\u4e00\u3002"}}
{"id": "2511.01434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01434", "abs": "https://arxiv.org/abs/2511.01434", "authors": ["Seongkyu Choi", "Jhonghyun An"], "title": "Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation", "comment": null, "summary": "Off-road semantic segmentation suffers from thick, inconsistent boundaries,\nsparse supervision for rare classes, and pervasive label noise. Designs that\nfuse only at low resolution blur edges and propagate local errors, whereas\nmaintaining high-resolution pathways or repeating high-resolution fusions is\ncostly and fragile to noise. We introduce a resolutionaware token decoder that\nbalances global semantics, local consistency, and boundary fidelity under\nimperfect supervision. Most computation occurs at a low-resolution bottleneck;\na gated cross-attention injects fine-scale detail, and only a sparse,\nuncertainty-selected set of pixels is refined. The components are co-designed\nand tightly integrated: global self-attention with lightweight dilated\ndepthwise refinement restores local coherence; a gated cross-attention\nintegrates fine-scale features from a standard high-resolution encoder stream\nwithout amplifying noise; and a class-aware point refinement corrects residual\nambiguities with negligible overhead. During training, we add a boundary-band\nconsistency regularizer that encourages coherent predictions in a thin\nneighborhood around annotated edges, with no inference-time cost. Overall, the\nresults indicate competitive performance and improved stability across\ntransitions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u4f4e\u5206\u8fa8\u7387\u74f6\u9888\u8fdb\u884c\u5927\u90e8\u5206\u8ba1\u7b97\u3001\u7528\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\u5e76\u5bf9\u4e0d\u786e\u5b9a\u50cf\u7d20\u7a00\u758f\u7cbe\u70bc\u7684\u5206\u8fa8\u7387\u611f\u77e5token\u89e3\u7801\u5668\uff1b\u914d\u5408\u8fb9\u754c\u5e26\u4e00\u81f4\u6027\u6b63\u5219\u5316\uff0c\u63d0\u5347\u8d8a\u91ce\u8bed\u4e49\u5206\u5272\u7684\u8fb9\u754c\u4fdd\u771f\u3001\u5c40\u90e8\u4e00\u81f4\u6027\u53ca\u566a\u58f0\u9c81\u68d2\u6027\u3002", "motivation": "\u8d8a\u91ce\u8bed\u4e49\u5206\u5272\u9762\u4e34\u8fb9\u754c\u7c97\u7cd9\u4e14\u4e0d\u4e00\u81f4\u3001\u7f55\u89c1\u7c7b\u522b\u76d1\u7763\u7a00\u758f\u53ca\u6807\u7b7e\u566a\u58f0\u666e\u904d\u7684\u95ee\u9898\u3002\u4f4e\u5206\u8fa8\u7387\u878d\u5408\u4f1a\u6a21\u7cca\u8fb9\u754c\u5e76\u4f20\u64ad\u5c40\u90e8\u9519\u8bef\uff0c\u800c\u4fdd\u6301\u6216\u91cd\u590d\u9ad8\u5206\u8fa8\u7387\u878d\u5408\u4ee3\u4ef7\u9ad8\u4e14\u5bf9\u566a\u58f0\u654f\u611f\u3002\u6545\u9700\u4e00\u79cd\u5728\u8ba1\u7b97\u6210\u672c\u3001\u8fb9\u754c\u4fdd\u771f\u4e0e\u5bf9\u566a\u58f0\u9c81\u68d2\u6027\u4e4b\u95f4\u53d6\u5f97\u6743\u8861\u7684\u89e3\u7801\u5668\u8bbe\u8ba1\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728\u4f4e\u5206\u8fa8\u7387\u74f6\u9888\u5904\u8fdb\u884c\u5927\u89c4\u6a21\u8ba1\u7b97\u5e76\u4f7f\u7528\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u6062\u590d\u5168\u5c40\u8bed\u4e49\uff1b2) \u8f7b\u91cf\u7ea7\u81a8\u80c0\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7528\u4e8e\u5c40\u90e8\u7cbe\u70bc\u4ee5\u6062\u590d\u5c40\u90e8\u4e00\u81f4\u6027\uff1b3) \u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u4ece\u9ad8\u5206\u8fa8\u7387\u7f16\u7801\u5668\u6d41\u4e2d\u9009\u62e9\u6027\u5f15\u5165\u7ec6\u5c3a\u5ea6\u7279\u5f81\u4ee5\u907f\u514d\u566a\u58f0\u653e\u5927\uff1b4) \u7c7b\u522b\u611f\u77e5\u70b9\u7cbe\u70bc\u5bf9\u4e0d\u786e\u5b9a\u50cf\u7d20\u8fdb\u884c\u7a00\u758f\u4fee\u6b63\uff1b5) \u8bad\u7ec3\u671f\u95f4\u5f15\u5165\u8fb9\u754c\u5e26\u4e00\u81f4\u6027\u6b63\u5219\u5316\u4ee5\u5728\u6ce8\u91ca\u8fb9\u7f18\u9644\u8fd1\u9f13\u52b1\u9884\u6d4b\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u4e0d\u540c\u8fc7\u6e21\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u7684\u6027\u80fd\u4ee5\u53ca\u66f4\u597d\u7684\u7a33\u5b9a\u6027\uff0c\u5c24\u5176\u5728\u8fb9\u754c\u4fdd\u771f\u548c\u5bf9\u6807\u6ce8\u566a\u58f0\u7684\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u5206\u8fa8\u7387\u611f\u77e5\u7684token\u89e3\u7801\u5668\uff0c\u65e8\u5728\u5728\u4e0d\u5b8c\u7f8e\u6807\u6ce8\u4e0b\u5728\u4f4e\u5206\u8fa8\u7387\u74f6\u9888\u5904\u8fdb\u884c\u5927\u90e8\u5206\u8ba1\u7b97\uff0c\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u6ce8\u5165\u7ec6\u8282\uff0c\u5e76\u4ec5\u5bf9\u4e0d\u786e\u5b9a\u50cf\u7d20\u8fdb\u884c\u7a00\u758f\u7ec6\u5316\uff0c\u4ece\u800c\u5728\u5168\u5c40\u8bed\u4e49\u3001\u5c40\u90e8\u4e00\u81f4\u6027\u548c\u8fb9\u754c\u4fdd\u771f\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2511.01435", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01435", "abs": "https://arxiv.org/abs/2511.01435", "authors": ["SiWoo Kim", "JhongHyun An"], "title": "Contrast-Guided Cross-Modal Distillation for Thermal Object Detection", "comment": null, "summary": "Robust perception at night remains challenging for thermal-infrared\ndetection: low contrast and weak high-frequency cues lead to duplicate,\noverlapping boxes, missed small objects, and class confusion. Prior remedies\neither translate TIR to RGB and hope pixel fidelity transfers to detection --\nmaking performance fragile to color or structure artifacts -- or fuse RGB and\nTIR at test time, which requires extra sensors, precise calibration, and higher\nruntime cost. Both lines can help in favorable conditions, but do not directly\nshape the thermal representation used by the detector. We keep mono-modality\ninference and tackle the root causes during training. Specifically, we\nintroduce training-only objectives that sharpen instance-level decision\nboundaries by pulling together features of the same class and pushing apart\nthose of different classes -- suppressing duplicate and confusing detections --\nand that inject cross-modal semantic priors by aligning the student's\nmulti-level pyramid features with an RGB-trained teacher, thereby strengthening\ntexture-poor thermal features without visible input at test time. In\nexperiments, our method outperformed prior approaches and achieved\nstate-of-the-art performance.", "AI": {"tldr": "\u5728\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u7279\u5f81\u805a\u5408/\u5206\u79bb\u548c\u591a\u5c42RGB\u6559\u5e08\u84b8\u998f\uff0c\u5f3a\u5316\u70ed\u7ea2\u5916\u68c0\u6d4b\u5668\u7684\u8bed\u4e49\u4e0e\u5224\u522b\u8fb9\u754c\uff0c\u6d4b\u8bd5\u65f6\u4fdd\u6301\u5355\u6a21\u6001\u63a8\u7406\uff0c\u4ece\u800c\u5728\u591c\u95f4\u70ed\u7ea2\u5916\u68c0\u6d4b\u4e0a\u53d6\u5f97SOTA\u6210\u7ee9\u3002", "motivation": "\u70ed\u7ea2\u5916\u591c\u95f4\u68c0\u6d4b\u53d7\u4f4e\u5bf9\u6bd4\u5ea6\u4e0e\u9ad8\u9891\u4fe1\u606f\u7f3a\u5931\u5f71\u54cd\uff0c\u5bfc\u81f4\u91cd\u590d\u91cd\u53e0\u6846\u3001\u5c0f\u76ee\u6807\u6f0f\u68c0\u4e0e\u7c7b\u522b\u6df7\u6dc6\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u901a\u8fc7\u56fe\u50cf\u7ffb\u8bd1\uff08TIR->RGB\uff09\u5f15\u5165\u50cf\u7d20\u7ea7\u4f2a\u5f69\u8272\u5bfc\u81f4\u8106\u5f31\u6027\uff0c\u8981\u4e48\u5728\u6d4b\u8bd5\u65f6\u4f9d\u8d56RGB\u878d\u5408\u589e\u52a0\u4f20\u611f\u5668\u4e0e\u8ba1\u7b97\u5f00\u9500\uff0c\u5747\u672a\u76f4\u63a5\u5851\u9020\u70ed\u6a21\u6001\u7684\u68c0\u6d4b\u8868\u5f81\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u589e\u52a0\u63a8\u7406\u590d\u6742\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u4ece\u8bad\u7ec3\u9636\u6bb5\u6539\u5584\u70ed\u6a21\u6001\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u8bad\u7ec3\u4e13\u7528\u76ee\u6807\uff1a1) \u5b9e\u4f8b\u7ea7\u5bf9\u6bd4\u5f0f\u635f\u5931\uff08\u6216\u4e2d\u5fc3/\u805a\u7c7b\u7ea6\u675f\uff09\uff0c\u5c06\u540c\u7c7b\u76ee\u6807\u7684\u7279\u5f81\u62c9\u8fd1\u3001\u5f02\u7c7b\u7279\u5f81\u63a8\u8fdc\uff0c\u4ee5\u51cf\u5c11\u91cd\u590d\u548c\u6df7\u6dc6\u68c0\u6d4b\uff1b2) \u591a\u5c42\u7279\u5f81\u84b8\u998f\uff0c\u901a\u8fc7\u5bf9\u9f50\u5b66\u751f\uff08\u70ed\u7ea2\u5916\uff09\u91d1\u5b57\u5854\u7279\u5f81\u4e0e\u5728RGB\u4e0a\u8bad\u7ec3\u7684\u6559\u5e08\u7f51\u7edc\u7684\u591a\u5c3a\u5ea6\u8bed\u4e49\u8868\u793a\uff0c\u6ce8\u5165\u7eb9\u7406\u548c\u8bed\u4e49\u5148\u9a8c\uff0c\u589e\u5f3a\u70ed\u56fe\u50cf\u7684\u8868\u5f81\u80fd\u529b\u3002\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528\u8fd9\u4e9b\u8f85\u52a9\u635f\u5931\uff0c\u6d4b\u8bd5\u9636\u6bb5\u4ec5\u4fdd\u7559\u5355\u4e00\u70ed\u7ea2\u5916\u68c0\u6d4b\u5668\uff0c\u65e0\u9700RGB\u8f93\u5165\u3002", "result": "\u5728\u82e5\u5e72\u70ed\u7ea2\u5916\u68c0\u6d4b\u6570\u636e\u96c6\u4e0e\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u65b9\u6cd5\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u56fe\u50cf\u7ffb\u8bd1\u6216\u591a\u6a21\u6001\u878d\u5408\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u5c11\u7684\u91cd\u590d\u6846\u3001\u66f4\u9ad8\u7684\u5c0f\u76ee\u6807\u53ec\u56de\u7387\u4e0e\u66f4\u4f4e\u7684\u7c7b\u522b\u6df7\u6dc6\uff0c\u6574\u4f53\u8fbe\u5230\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65f6\u7684\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u7c7b\u522b\u5185\u7279\u5f81\u805a\u5408\u4e0e\u7c7b\u522b\u95f4\u7279\u5f81\u5206\u79bb\u5e76\u7ed3\u5408\u4eceRGB\u6559\u5e08\u7f51\u7edc\u84b8\u998f\u7684\u591a\u5c42\u8bed\u4e49\u5148\u9a8c\uff0c\u63d0\u5347\u70ed\u7ea2\u5916\u68c0\u6d4b\u5668\u5728\u591c\u95f4\u7684\u9c81\u68d2\u6027\uff0c\u5728\u5355\u6a21\u6001\u63a8\u7406\u4e0b\u51cf\u5c11\u91cd\u590d\u6846\u3001\u6f0f\u68c0\u548c\u5206\u7c7b\u6df7\u6dc6\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u6027\u80fd\u5e76\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2511.01449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01449", "abs": "https://arxiv.org/abs/2511.01449", "authors": ["Riddhi Jain", "Manasi Patwardhan", "Aayush Mishra", "Parijat Deshpande", "Beena Rai"], "title": "Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction", "comment": "9 pages, 1 figure, 4 tables", "summary": "To effectively manage the wastage of perishable fruits, it is crucial to\naccurately predict their freshness or shelf life using non-invasive methods\nthat rely on visual data. In this regard, deep learning techniques can offer a\nviable solution. However, obtaining fine-grained fruit freshness labels from\nexperts is costly, leading to a scarcity of data. Closed proprietary Vision\nLanguage Models (VLMs), such as Gemini, have demonstrated strong performance in\nfruit freshness detection task in both zero-shot and few-shot settings.\nNonetheless, food retail organizations are unable to utilize these proprietary\nmodels due to concerns related to data privacy, while existing open-source VLMs\nyield sub-optimal performance for the task. Fine-tuning these open-source\nmodels with limited data fails to achieve the performance levels of proprietary\nmodels. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning\n(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes\nmeta-learning to address data sparsity and leverages label ordinality, thereby\nachieving state-of-the-art performance in the fruit freshness classification\ntask under both zero-shot and few-shot settings. Our method achieves an\nindustry-standard accuracy of 92.71%, averaged across all fruits.\n  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,\nOrdinal Regression", "AI": {"tldr": "\u63d0\u51faMAOML\uff0c\u7ed3\u5408\u5e8f\u6570\u635f\u5931\u4e0e\u5143\u5b66\u4e60\uff0c\u63d0\u5347\u5c0f\u578b\u5f00\u6e90VLM\u5728\u6c34\u679c\u65b0\u9c9c\u5ea6\u9884\u6d4b\u4e0a\u7684\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u738792.71%\u3002", "motivation": "\u83b7\u53d6\u7ec6\u7c92\u5ea6\u7684\u6c34\u679c\u65b0\u9c9c\u5ea6\u6807\u7b7e\u6210\u672c\u9ad8\uff0c\u5bfc\u81f4\u6570\u636e\u7a00\u7f3a\uff1b\u95ed\u6e90\u5927\u6a21\u578b\uff08\u5982Gemini\uff09\u867d\u6548\u679c\u597d\uff0c\u4f46\u56e0\u9690\u79c1\u548c\u4f01\u4e1a\u4f7f\u7528\u9650\u5236\u65e0\u6cd5\u666e\u53ca\uff1b\u73b0\u6709\u5f00\u6e90VLM\u6027\u80fd\u4e0d\u8db3\u4e14\u5c11\u91cf\u6570\u636e\u5fae\u8c03\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u4e00\u79cd\u80fd\u5728\u5c0f\u6570\u636e\u4e0b\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u5e8f\u6570\u5143\u5b66\u4e60\u7b97\u6cd5\uff08MAOML\uff09\uff0c\u5c06\u5e8f\u6570\u56de\u5f52\u548c\u5143\u5b66\u4e60\u6846\u67b6\u7ed3\u5408\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u5c0f\u6837\u672c\u9002\u914d\u80fd\u529b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5143\u4efb\u52a1\u3001\u4efb\u52a1\u5185\u5916\u4f18\u5316\u53ca\u5e8f\u6570\u635f\u5931\u6765\u5b66\u4e60\u66f4\u9c81\u68d2\u7684\u8868\u5f81\uff1b\u53ef\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u5e94\u7528\u3002", "result": "\u5728\u6c34\u679c\u65b0\u9c9c\u5ea6\u5206\u7c7b\u4efb\u52a1\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0cMAOML\u76f8\u6bd4\u76f4\u63a5\u5fae\u8c03\u548c\u73b0\u6709\u5f00\u6e90VLM\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523092.71%\uff0c\u5e76\u8fbe\u5230\u884c\u4e1a\u53ef\u63a5\u53d7\u6807\u51c6\uff1b\u8bba\u6587\u8fd8\u5c55\u793a\u4e86\u65b9\u6cd5\u5bf9\u4e0d\u540c\u6c34\u679c\u548c\u6837\u672c\u89c4\u6a21\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684MAOML\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5143\u5b66\u4e60\u548c\u5e8f\u6570\u4fe1\u606f\uff0c\u9488\u5bf9\u5c0f\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u751f\u9c9c\u6c34\u679c\u4fdd\u9c9c\u671f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u6e90\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e0e\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u6700\u7ec8\u5728\u6240\u6709\u6c34\u679c\u4e0a\u5e73\u5747\u8fbe\u523092.71%\u7684\u884c\u4e1a\u6807\u51c6\u51c6\u786e\u7387\u3002"}}
{"id": "2511.01450", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01450", "abs": "https://arxiv.org/abs/2511.01450", "authors": ["Jie Du", "Xinyu Gong", "Qingshan Tan", "Wen Li", "Yangming Cheng", "Weitao Wang", "Chenlu Zhan", "Suhui Wu", "Hao Zhang", "Jun Zhang"], "title": "Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation", "comment": null, "summary": "Recent studies have identified Direct Preference Optimization (DPO) as an\nefficient and reward-free approach to improving video generation quality.\nHowever, existing methods largely follow image-domain paradigms and are mainly\ndeveloped on small-scale models (approximately 2B parameters), limiting their\nability to address the unique challenges of video tasks, such as costly data\nconstruction, unstable training, and heavy memory consumption. To overcome\nthese limitations, we introduce a GT-Pair that automatically builds\nhigh-quality preference pairs by using real videos as positives and\nmodel-generated videos as negatives, eliminating the need for any external\nannotation. We further present Reg-DPO, which incorporates the SFT loss as a\nregularization term into the DPO objective to enhance training stability and\ngeneration fidelity. Additionally, by combining the FSDP framework with\nmultiple memory optimization techniques, our approach achieves nearly three\ntimes higher training capacity than using FSDP alone. Extensive experiments on\nboth I2V and T2V tasks across multiple datasets demonstrate that our method\nconsistently outperforms existing approaches, delivering superior video\ngeneration quality.", "AI": {"tldr": "\u63d0\u51faGT-Pair\u81ea\u52a8\u6784\u5efa\u504f\u597d\u5bf9\u4e0eReg-DPO\u6b63\u5219\u5316DPO\uff0c\u5e76\u7ed3\u5408\u5185\u5b58\u4f18\u5316\u6269\u5c55\u8bad\u7ec3\u5bb9\u91cf\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u751f\u6210\u6548\u679c\u5e76\u7701\u53bb\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u591a\u6cbf\u7528\u56fe\u50cf\u9886\u57df\u8303\u5f0f\u3001\u5728\u5c0f\u89c4\u6a21\u6a21\u578b\u4e0a\u5f00\u53d1\uff0c\u96be\u4ee5\u5e94\u5bf9\u89c6\u9891\u4efb\u52a1\u7684\u6570\u636e\u6784\u5efa\u6210\u672c\u9ad8\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u65e0\u76d1\u7763\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u6784\u5efaGT-Pair\uff08\u771f\u5b9e\u89c6\u9891\u4e3a\u6b63\u3001\u6a21\u578b\u751f\u6210\u89c6\u9891\u4e3a\u8d1f\uff09\u81ea\u52a8\u751f\u6210\u504f\u597d\u5bf9\uff1b\u5c06SFT\u635f\u5931\u4f5c\u4e3a\u6b63\u5219\u9879\u5e76\u5165DPO\u76ee\u6807\uff08Reg-DPO\uff09\uff1b\u5728FSDP\u57fa\u7840\u4e0a\u7ed3\u5408\u591a\u79cd\u5185\u5b58\u4f18\u5316\u6280\u672f\u6269\u5c55\u8bad\u7ec3\u5bb9\u91cf\u8fd13\u500d\u3002", "result": "\u5728I2V\u548cT2V\u591a\u6570\u636e\u96c6\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u5728\u89c6\u9891\u751f\u6210\u8d28\u91cf\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684GT-Pair\u548cReg-DPO\u7ed3\u5408\u591a\u9879\u5185\u5b58\u4f18\u5316\u4e0eFSDP\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u751f\u6210\u8d28\u91cf\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u6807\u6ce8\u3002"}}
{"id": "2511.01458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01458", "abs": "https://arxiv.org/abs/2511.01458", "authors": ["Dennis Pierantozzi", "Luca Carlini", "Mauro Orazio Drago", "Chiara Lena", "Cesare Hassan", "Elena De Momi", "Danail Stoyanov", "Sophia Bano", "Mobarak I. Hoque"], "title": "When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA", "comment": null, "summary": "Safety and reliability are essential for deploying Visual Question Answering\n(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.\nMost surgical VQA research focuses on accuracy or linguistic quality while\noverlooking safety behaviors such as ambiguity awareness, referral to human\nexperts, or triggering a second opinion. Inspired by Automatic Failure\nDetection (AFD), we study uncertainty estimation as a key enabler of safer\ndecision making. We introduce Question Aligned Semantic Nearest Neighbor\nEntropy (QA-SNNE), a black box uncertainty estimator that incorporates question\nsemantics into prediction confidence. It measures semantic entropy by comparing\ngenerated answers with nearest neighbors in a medical text embedding space,\nconditioned on the question. We evaluate five models, including domain specific\nParameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large\nVision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models\ndegrade under mild paraphrasing, while LVLMs are more resilient. Across three\nLVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template\nsettings and enhances hallucination detection. The Area Under the ROC Curve\n(AUROC) increases by 15-38% for zero-shot models, with gains maintained under\nout-of-template stress. QA-SNNE offers a practical and interpretable step\ntoward AFD in surgical VQA by linking semantic uncertainty to question context.\nCombining LVLM backbones with question aligned uncertainty estimation can\nimprove safety and clinician trust. The code and model are available at\nhttps://github.com/DennisPierantozzi/QASNNE", "AI": {"tldr": "\u9488\u5bf9\u624b\u672fVQA\u7684\u5b89\u5168\u95ee\u9898\uff0c\u63d0\u51faQA-SNNE\u901a\u8fc7\u95ee\u9898\u5bf9\u9f50\u7684\u6587\u672c\u6700\u8fd1\u90bb\u8bed\u4e49\u71b5\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u96f6-shot LVLM\u7684AUROC\u4e0e\u5e7b\u89c9\u68c0\u6d4b\uff0c\u589e\u5f3a\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u624b\u672f\u7528VQA\u5bf9\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\u8981\u6c42\u9ad8\uff0c\u4ee5\u5f80\u7814\u7a76\u591a\u5173\u6ce8\u51c6\u786e\u6027\u6216\u8bed\u8a00\u8d28\u91cf\uff0c\u5ffd\u89c6\u4e86\u6a21\u7cca\u8bc6\u522b\u3001\u4e0a\u62a5\u4e13\u5bb6\u6216\u89e6\u53d1\u590d\u8bae\u7b49\u5b89\u5168\u884c\u4e3a\uff1b\u53d7\u81ea\u52a8\u6545\u969c\u68c0\u6d4b\uff08AFD\uff09\u542f\u53d1\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4f5c\u4e3a\u5b9e\u73b0\u66f4\u5b89\u5168\u51b3\u7b56\u7684\u5173\u952e\u624b\u6bb5\u3002", "method": "\u63d0\u51faQuestion Aligned Semantic Nearest Neighbor Entropy\uff08QA-SNNE\uff09\uff0c\u5728\u533b\u7597\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e2d\u8ba1\u7b97\u57fa\u4e8e\u95ee\u9898\u6761\u4ef6\u7684\u8bed\u4e49\u71b5\uff1b\u901a\u8fc7\u5c06\u751f\u6210\u7b54\u6848\u4e0e\u6700\u8fd1\u90bb\u6587\u672c\u6bd4\u8f83\u5f97\u5230\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff0c\u4f5c\u4e3a\u9ed1\u76d2\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5668\u3002\u8bc4\u4f30\u4e94\u4e2a\u6a21\u578b\uff083\u4e2aLVLMs\uff0c2\u4e2aPEFT\uff09\uff0c\u5728EndoVis18-VQA\u4e0ePitVQA\u6570\u636e\u96c6\u4e0a\u5bf9\u6bd4\u6a21\u677f\u5185\u4e0e\u6a21\u677f\u5916\u9c81\u68d2\u6027\u4ee5\u53ca\u5bf9\u8f7b\u5fae\u6539\u5199\u7684\u654f\u611f\u6027\u3002", "result": "QA-SNNE\u5728\u591a\u6570\u6a21\u677f\u5185\u573a\u666f\u4e0b\u63d0\u9ad8\u4e86AUROC\uff1b\u5bf9\u96f6-shot\u6a21\u578b\uff0cAUROC\u63d0\u534715-38%\uff0c\u4e14\u5728\u6a21\u677f\u5916\u538b\u529b\u6d4b\u8bd5\u4e0b\u4fdd\u6301\u63d0\u5347\uff1b\u8fd8\u6539\u5584\u4e86\u6a21\u578b\u7684\u5e7b\u89c9\u68c0\u6d4b\u80fd\u529b\u3002PEFT\u6a21\u578b\u5bf9\u8f7b\u5fae\u95ee\u9898\u6539\u5199\u6027\u80fd\u4e0b\u964d\uff0cLVLMs\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u624b\u672f\u573a\u666f\u7684VQA\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5QA-SNNE\uff0c\u5c06\u95ee\u9898\u8bed\u4e49\u878d\u5165\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u4ece\u800c\u63d0\u9ad8\u5b89\u5168\u6027\u4e0e\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5728\u96f6-shot\u5927\u6a21\u578b\u4e0a\u663e\u8457\u6539\u5584AUROC\u5e76\u589e\u5f3a\u5e7b\u89c9\u68c0\u6d4b\uff0c\u7ed3\u5408LVLM\u9aa8\u5e72\u53ef\u63d0\u5347\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.01462", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01462", "abs": "https://arxiv.org/abs/2511.01462", "authors": ["Peng Xia", "Junbiao Pang", "Tianyang Cai"], "title": "Efficiently Training A Flat Neural Network Before It has been Quantizated", "comment": "ongoing work, more results would be added", "summary": "Post-training quantization (PTQ) for vision transformers (ViTs) has garnered\nsignificant attention due to its efficiency in compressing models. However,\nexisting methods typically overlook the relationship between a well-trained NN\nand the quantized model, leading to considerable quantization error for PTQ.\nHowever, it is unclear how to efficiently train a model-agnostic neural network\nwhich is tailored for a predefined precision low-bit model. In this paper, we\nfirstly discover that a flat full precision neural network is crucial for\nlow-bit quantization. To achieve this, we propose a framework that proactively\npre-conditions the model by measuring and disentangling the error sources.\nSpecifically, both the Activation Quantization Error (AQE) and the Weight\nQuantization Error (WQE) are statistically modeled as independent Gaussian\nnoises. We study several noise injection optimization methods to obtain a flat\nminimum. Experimental results attest to the effectiveness of our approach.\nThese results open novel pathways for obtaining low-bit PTQ models.", "AI": {"tldr": "\u901a\u8fc7\u5c06AQE\u4e0eWQE\u89c6\u4e3a\u72ec\u7acb\u9ad8\u65af\u566a\u58f0\u5e76\u7528\u566a\u58f0\u6ce8\u5165\u4f18\u5316\u5f97\u5230\u5e73\u5766\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u4f4e\u4f4dPTQ\u6548\u679c\u3002", "motivation": "\u73b0\u6709PTQ\u65b9\u6cd5\u5ffd\u89c6\u4e86\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u4e0e\u91cf\u5316\u6a21\u578b\u95f4\u7684\u5173\u7cfb\uff0c\u5bfc\u81f4\u663e\u8457\u91cf\u5316\u8bef\u5dee\uff1b\u9700\u8981\u9ad8\u6548\u8bad\u7ec3\u5bf9\u7279\u5b9a\u4f4e\u4f4d\u91cf\u5316\u9c81\u68d2\u7684\u6a21\u578b\u3002", "method": "\u672c\u6587\u5c06\u6fc0\u6d3b\u91cf\u5316\u8bef\u5dee(AQE)\u4e0e\u6743\u91cd\u91cf\u5316\u8bef\u5dee(WQE)\u5206\u522b\u7edf\u8ba1\u5efa\u6a21\u4e3a\u72ec\u7acb\u9ad8\u65af\u566a\u58f0\uff0c\u5e76\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u7684\u4f18\u5316\u65b9\u6cd5\u5bfb\u627e\u5e73\u5766\u6781\u5c0f\u503c\u6765\u9884\u8c03\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u91cf\u5316\u8bef\u5dee\u5e76\u63d0\u9ad8\u4f4e\u4f4dPTQ\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u9020\u5c31\u5e73\u5766\u7684\u5168\u7cbe\u5ea6\u6a21\u578b\u6765\u964d\u4f4e\u4f4e\u4f4d\u91cf\u5316\u8bef\u5dee\uff0c\u4ece\u800c\u63d0\u5347PTQ\u6027\u80fd\u3002"}}
{"id": "2511.01463", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.2.10; I.3.7"], "pdf": "https://arxiv.org/pdf/2511.01463", "abs": "https://arxiv.org/abs/2511.01463", "authors": ["Lei Hu", "Yongjing Ye", "Shihong Xia"], "title": "HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA", "comment": "10 pages, 5figures. The Thirty-Ninth Annual Conference on Neural\n  Information Processing Systems", "summary": "The expansion of instruction-tuning data has enabled foundation language\nmodels to exhibit improved instruction adherence and superior performance\nacross diverse downstream tasks. Semantically-rich 3D human motion is being\nprogressively integrated with these foundation models to enhance multimodal\nunderstanding and cross-modal generation capabilities. However, the modality\ngap between human motion and text raises unresolved concerns about catastrophic\nforgetting during this integration. In addition, developing\nautoregressive-compatible pose representations that preserve generalizability\nacross heterogeneous downstream tasks remains a critical technical barrier. To\naddress these issues, we propose the Human Motion-Vision-Language Model\n(HMVLM), a unified framework based on the Mixture of Expert Low-Rank\nAdaption(MoE LoRA) strategy. The framework leverages the gating network to\ndynamically allocate LoRA expert weights based on the input prompt, enabling\nsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting\nduring instruction-tuning, we introduce a novel zero expert that preserves the\npre-trained parameters for general linguistic tasks. For pose representation,\nwe implement body-part-specific tokenization by partitioning the human body\ninto different joint groups, enhancing the spatial resolution of the\nrepresentation. Experiments show that our method effectively alleviates\nknowledge forgetting during instruction-tuning and achieves remarkable\nperformance across diverse human motion downstream tasks.", "AI": {"tldr": "\u63d0\u51faHMVLM\uff1a\u57fa\u4e8eMoE LoRA\u7684\u591a\u4e13\u5bb6\u95e8\u63a7\u5fae\u8c03\u6846\u67b6\uff0c\u52a0\u5165\u96f6\u4e13\u5bb6\u4fdd\u7559\u9884\u8bad\u7ec3\u80fd\u529b\uff0c\u5e76\u7528\u6309\u8eab\u4f53\u90e8\u4f4d\u7684token\u5316\u6539\u8fdbpose\u8868\u793a\uff0c\u65e8\u5728\u5728\u4e0d\u4e22\u5931\u8bed\u8a00\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\u878d\u54083D\u4eba\u4f53\u52a8\u4f5c\u5b9e\u73b0\u4f18\u79c0\u7684\u591a\u6a21\u6001\u4e0b\u6e38\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u6269\u5c55\uff0c\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u4e0a\u6539\u8fdb\uff0c\u4f46\u5c06\u8bed\u4e49\u4e30\u5bcc\u76843D\u4eba\u4f53\u52a8\u4f5c\u6574\u5408\u8fdb\u8fd9\u4e9b\u6a21\u578b\u65f6\u5b58\u5728\u6a21\u6001\u5dee\u8ddd\uff0c\u5e26\u6765\u707e\u96be\u6027\u9057\u5fd8\u98ce\u9669\uff1b\u540c\u65f6\uff0c\u81ea\u56de\u5f52\u517c\u5bb9\u4e14\u80fd\u6cdb\u5316\u5230\u5f02\u8d28\u4e0b\u6e38\u4efb\u52a1\u7684\u59ff\u6001\u8868\u5f81\u4ecd\u662f\u5173\u952e\u6280\u672f\u74f6\u9888\u3002\u8bba\u6587\u65e8\u5728\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u65b9\u9762\u95ee\u9898\uff0c\u5b9e\u73b0\u52a8\u4f5c-\u89c6\u89c9-\u8bed\u8a00\u7edf\u4e00\u5efa\u6a21\u3002", "method": "\u91c7\u7528Mixture of Expert Low-Rank Adaption\uff08MoE LoRA\uff09\u7b56\u7565\uff1a1\uff09\u95e8\u63a7\u7f51\u7edc\u6839\u636e\u8f93\u5165\u63d0\u793a\u52a8\u6001\u9009\u62e9\u548c\u52a0\u6743\u4e0d\u540cLoRA\u4e13\u5bb6\uff0c\u5141\u8bb8\u5728\u5355\u4e00\u6a21\u578b\u4e2d\u540c\u6b65\u5904\u7406\u591a\u79cd\u4efb\u52a1\uff1b2\uff09\u5f15\u5165\u96f6\u4e13\u5bb6\uff08zero expert\uff09\u4fdd\u6301\u9884\u8bad\u7ec3\u53c2\u6570\u4e0d\u53d8\uff0c\u7528\u4ee5\u4fdd\u7559\u8bed\u8a00\u80fd\u529b\uff0c\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff1b3\uff09\u4eba\u4f53\u52a8\u4f5c\u7684\u8868\u793a\u91c7\u7528\u6309\u8eab\u4f53\u90e8\u4f4d\uff08\u5173\u8282\u7ec4\uff09\u5212\u5206\u7684token\u5316\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u5e76\u517c\u5bb9\u81ea\u56de\u5f52\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHMVLM\u5728\u6307\u4ee4\u8c03\u4f18\u8fc7\u7a0b\u4e2d\u6709\u6548\u7f13\u89e3\u4e86\u77e5\u8bc6\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u4eba\u4f53\u52a8\u4f5c\u4e0b\u6e38\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86MoE LoRA\u4e0e\u96f6\u4e13\u5bb6\u53ca\u6309\u90e8\u4f4dtoken\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86HMVLM\uff0c\u4e00\u79cd\u57fa\u4e8eMoE LoRA\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u6309\u63d0\u793a\u52a8\u6001\u5206\u914dLoRA\u4e13\u5bb6\u6743\u91cd\uff0c\u5b9e\u73b0\u591a\u4efb\u52a1\u540c\u6b65\u5fae\u8c03\uff0c\u540c\u65f6\u5f15\u5165\u96f6\u4e13\u5bb6\u4ee5\u7f13\u89e3\u6307\u4ee4\u8c03\u4f18\u671f\u95f4\u7684\u707e\u96be\u6027\u9057\u5fd8\uff0c\u5e76\u901a\u8fc7\u6309\u8eab\u4f53\u90e8\u4f4d\u5206\u7ec4\u7684token\u5316\u63d0\u9ad8\u5173\u8282\u7a7a\u95f4\u5206\u8fa8\u7387\u3002\u6574\u4f53\u76ee\u6807\u662f\u5c06\u8bed\u4e49\u4e30\u5bcc\u7684\u4e09\u7ef4\u4eba\u4f53\u52a8\u4f5c\u4e0e\u5927\u57fa\u7840\u8bed\u8a00\u6a21\u578b\u878d\u5408\uff0c\u4fdd\u7559\u8bed\u8a00\u80fd\u529b\u5e76\u63d0\u5347\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u8de8\u6a21\u6001\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2511.01466", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01466", "abs": "https://arxiv.org/abs/2511.01466", "authors": ["Changyuan Zhao", "Jiacheng Wang", "Ruichen Zhang", "Dusit Niyato", "Hongyang Du", "Zehui Xiong", "Dong In Kim", "Ping Zhang"], "title": "SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks", "comment": "13 pages, 6 figures", "summary": "Deep joint source-channel coding (JSCC) has emerged as a promising paradigm\nfor semantic communication, delivering significant performance gains over\nconventional separate coding schemes. However, existing JSCC frameworks remain\nvulnerable to physical-layer adversarial threats, such as pilot spoofing and\nsubcarrier jamming, compromising semantic fidelity. In this paper, we propose\nSecDiff, a plug-and-play, diffusion-aided decoding framework that significantly\nenhances the security and robustness of deep JSCC under adversarial wireless\nenvironments. Different from prior diffusion-guided JSCC methods that suffer\nfrom high inference latency, SecDiff employs pseudoinverse-guided sampling and\nadaptive guidance weighting, enabling flexible step-size control and efficient\nsemantic reconstruction. To counter jamming attacks, we introduce a power-based\nsubcarrier masking strategy and recast recovery as a masked inpainting problem,\nsolved via diffusion guidance. For pilot spoofing, we formulate channel\nestimation as a blind inverse problem and develop an expectation-minimization\n(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and\na channel operator. Notably, our method alternates between pilot recovery and\nchannel estimation, enabling joint refinement of both variables throughout the\ndiffusion process. Extensive experiments over orthogonal frequency-division\nmultiplexing (OFDM) channels under adversarial conditions show that SecDiff\noutperforms existing secure and generative JSCC baselines by achieving a\nfavorable trade-off between reconstruction quality and computational cost. This\nbalance makes SecDiff a promising step toward practical, low-latency, and\nattack-resilient semantic communications.", "AI": {"tldr": "SecDiff \u7528\u4f2a\u9006\u5f15\u5bfc\u91c7\u6837\u4e0e\u81ea\u9002\u5e94\u6743\u91cd\u52a0\u901f\u6269\u6563\u89e3\u7801\uff0c\u7ed3\u5408\u5b50\u8f7d\u6ce2\u63a9\u853d\u548c EM \u9a71\u52a8\u76f2\u4fe1\u9053\u4f30\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u6df1\u5ea6 JSCC \u5728\u5bfc\u9891\u6b3a\u9a97\u4e0e\u5b50\u8f7d\u6ce2\u963b\u585e\u4e0b\u7684\u9c81\u68d2\u6027\u4e0e\u4f4e\u5ef6\u8fdf\u5b9e\u73b0\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6 JSCC \u5728\u9762\u5bf9\u7269\u7406\u5c42\u5bf9\u6297\uff08\u5982\u5bfc\u9891\u6b3a\u9a97\u3001\u5b50\u8f7d\u6ce2\u963b\u585e\uff09\u65f6\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0b\u964d\uff0c\u4e14\u5df2\u6709\u6269\u6563\u5f15\u5bfc\u65b9\u6cd5\u63a8\u7406\u5ef6\u8fdf\u9ad8\uff0c\u9700\u4e00\u79cd\u517c\u987e\u9c81\u68d2\u6027\u4e0e\u4f4e\u5ef6\u8fdf\u7684\u901a\u7528\u53ef\u63d2\u62d4\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4e3b\u8981\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f2a\u9006\u5f15\u5bfc\u91c7\u6837\u4e0e\u81ea\u9002\u5e94\u5f15\u5bfc\u6743\u91cd\u7528\u4e8e\u52a0\u901f\u6269\u6563\u91cd\u6784\uff1b2) \u57fa\u4e8e\u529f\u7387\u7684\u5b50\u8f7d\u6ce2\u63a9\u853d\uff0c\u5c06\u5e72\u6270\u4e0b\u7684\u6062\u590d\u8f6c\u5316\u4e3a\u63a9\u853d\u4fee\u8865\u95ee\u9898\u5e76\u7528\u6269\u6563\u5f15\u5bfc\u6c42\u89e3\uff1b3) \u5bf9\u4e8e\u5bfc\u9891\u6b3a\u9a97\uff0c\u5c06\u4fe1\u9053\u4f30\u8ba1\u8868\u8ff0\u4e3a\u76f2\u9006\u95ee\u9898\uff0c\u8bbe\u8ba1 EM \u9a71\u52a8\u7684\u4ea4\u66ff\u6062\u590d\u7b97\u6cd5\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\u4f18\u5316\u5bfc\u9891\u6062\u590d\u4e0e\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u5728 OFDM \u4fe1\u9053\u5bf9\u6297\u573a\u666f\u4e0b\uff0cSecDiff \u5728\u91cd\u6784\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u6298\u4e2d\uff0c\u4f18\u4e8e\u73b0\u6709\u5b89\u5168\u6216\u751f\u6210\u5f0f JSCC \u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u4e3a\u8f83\u9ad8\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e0e\u8f83\u4f4e\u7684\u8fd0\u884c\u5ef6\u8fdf\u3002", "conclusion": "SecDiff \u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4f2a\u9006\u5f15\u5bfc\u91c7\u6837\u548c\u81ea\u9002\u5e94\u5f15\u5bfc\u6743\u91cd\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u9ad8\u6548\u8bed\u4e49\u91cd\u6784\uff0c\u540c\u65f6\u7ed3\u5408\u5b50\u8f7d\u6ce2\u63a9\u853d\u548c EM \u9a71\u52a8\u7684\u76f2\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5e72\u6270\uff08\u5b50\u8f7d\u6ce2\u963b\u585e\uff09\u548c\u5bfc\u9891\u6b3a\u9a97\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.01498", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01498", "abs": "https://arxiv.org/abs/2511.01498", "authors": ["Zhiyang Jia", "Hongyan Cui", "Ge Gao", "Bo Li", "Minjie Zhang", "Zishuo Gao", "Huiwen Huang", "Caisheng Zhuo"], "title": "EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance", "comment": "12 page, 5 figures", "summary": "Person re-identification (ReID) plays a pivotal role in computer vision,\nparticularly in surveillance and security applications within IoT-enabled smart\nenvironments. This study introduces the Enhanced Pedestrian Alignment Network\n(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.\nEPAN employs a dual-branch architecture to mitigate the impact of perspective\nand environmental changes, extracting alignment information under varying\nscales and viewpoints. Here, we demonstrate EPAN's strong feature extraction\ncapabilities, achieving outstanding performance on the Inspection-Personnel\ndataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of\n78.82%. This highlights EPAN's potential for real-world IoT applications,\nenabling effective and reliable person ReID across diverse cameras in\nsurveillance and security systems. The code and data are available at:\nhttps://github.com/ggboy2580/EPAN", "AI": {"tldr": "EPAN\u901a\u8fc7\u53cc\u5206\u652f\u5bf9\u9f50\u4e0e\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\uff0c\u63d0\u9ad8\u4e86IoT\u76d1\u63a7\u573a\u666f\u4e0b\u7684\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7ed3\u679c\u5e76\u5df2\u5f00\u6e90\u3002", "motivation": "IoT\u73af\u5883\u4e0b\u6444\u50cf\u5934\u5206\u5e03\u5e7f\u3001\u89c6\u89d2\u591a\u53d8\u3001\u5149\u7167\u4e0e\u906e\u6321\u590d\u6742\uff0c\u4f20\u7edfReID\u65b9\u6cd5\u5728\u8de8\u6444\u50cf\u5934\u4e00\u81f4\u6027\u548c\u5bf9\u9f50\u80fd\u529b\u4e0a\u4e0d\u8db3\uff0c\u9700\u4e00\u79cd\u9c81\u68d2\u7684\u5bf9\u9f50\u548c\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u5206\u652f\u7f51\u7edc\u7ed3\u6784\uff1a\u4e00\u652f\u7528\u4e8e\u63d0\u53d6\u5c3a\u5ea6\u4e0d\u53d8\u7684\u5168\u5c40\u7279\u5f81\uff0c\u53e6\u4e00\u652f\u4e13\u6ce8\u4e8e\u89c6\u89d2/\u900f\u89c6\u6821\u6b63\u7684\u5bf9\u9f50\u4fe1\u606f\uff1b\u4e24\u5206\u652f\u7279\u5f81\u878d\u5408\u540e\u7528\u4e8e\u6700\u7ec8ReID\u5224\u522b\u3002\u7f51\u7edc\u53ef\u80fd\u5f15\u5165\u591a\u5c3a\u5ea6\u6c60\u5316\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u5bf9\u9f50\u6a21\u5757\u4ee5\u5e94\u5bf9\u89c6\u89d2\u4e0e\u73af\u5883\u53d8\u5316\u3002", "result": "\u5728Inspection-Personnel\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86Rank-1=90.09%\u548cmAP=78.82%\uff0c\u8868\u660e\u65b9\u6cd5\u5728\u7cbe\u786e\u68c0\u7d22\u548c\u6392\u5e8f\u4e0a\u6548\u679c\u4f18\u79c0\uff0c\u4ee3\u7801\u4e0e\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "EPAN\u5728IoT\u76d1\u63a7\u573a\u666f\u4e0b\u80fd\u663e\u8457\u63d0\u5347\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5c24\u5176\u5728Inspection-Personnel\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u5176\u5728\u5b9e\u9645\u591a\u6444\u50cf\u5934\u73af\u5883\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.01501", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01501", "abs": "https://arxiv.org/abs/2511.01501", "authors": ["Yufeng Jin", "Niklas Funk", "Vignesh Prasad", "Zechu Li", "Mathias Franzius", "Jan Peters", "Georgia Chalvatzaki"], "title": "SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation", "comment": null, "summary": "Object pose estimation is a fundamental problem in robotics and computer\nvision, yet it remains challenging due to partial observability, occlusions,\nand object symmetries, which inevitably lead to pose ambiguity and multiple\nhypotheses consistent with the same observation. While deterministic deep\nnetworks achieve impressive performance under well-constrained conditions, they\nare often overconfident and fail to capture the multi-modality of the\nunderlying pose distribution. To address these challenges, we propose a novel\nprobabilistic framework that leverages flow matching on the SE(3) manifold for\nestimating 6D object pose distributions. Unlike existing methods that regress a\nsingle deterministic output, our approach models the full pose distribution\nwith a sample-based estimate and enables reasoning about uncertainty in\nambiguous cases such as symmetric objects or severe occlusions. We achieve\nstate-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our\nsample-based pose estimates can be leveraged in downstream robotic manipulation\ntasks such as active perception for disambiguating uncertain viewpoints or\nguiding grasp synthesis in an uncertainty-aware manner.", "AI": {"tldr": "\u63d0\u51fa\u5728SE(3)\u6d41\u5339\u914d\u76846D\u4f4d\u59ff\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6837\u672c\u4f30\u8ba1\u5efa\u6a21\u591a\u5cf0\u4f4d\u59ff\u5206\u5e03\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fbeSOTA\u5e76\u80fd\u7528\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4e0e\u6293\u53d6\u3002", "motivation": "Deterministic networks are overconfident and fail to model pose ambiguity from occlusions and symmetries; a probabilistic, sample-based approach on SE(3) better captures multi-modality and uncertainty.", "method": "They propose a flow-matching approach on the SE(3) manifold to learn sample-based pose distributions, replacing deterministic regression with a probabilistic model that generates multiple pose hypotheses. Likely involves manifold-aware flows, loss functions aligning distributions, and sampling strategies for downstream use.", "result": "State-of-the-art results on Real275, YCB-V, LM-O; improved handling of ambiguous/symmetric objects; benefits for active perception and uncertainty-aware grasping.", "conclusion": "This paper introduces a probabilistic SE(3) flow-matching framework for 6D object pose estimation, effectively modeling multi-modal pose distributions and improving uncertainty handling. It demonstrates state-of-the-art performance on standard benchmarks and practical benefits for downstream robotic tasks."}}
{"id": "2511.01502", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01502", "abs": "https://arxiv.org/abs/2511.01502", "authors": ["Mengtan Zhang", "Zizhan Guo", "Hongbo Zhao", "Yi Feng", "Zuyi Xiong", "Yue Wang", "Shaoyi Du", "Hanli Wang", "Rui Fan"], "title": "Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning", "comment": "18 pages, 14 figures", "summary": "Unsupervised learning of depth and ego-motion, two fundamental 3D perception\ntasks, has made significant strides in recent years. However, most methods\ntreat ego-motion as an auxiliary task, either mixing all motion types or\nexcluding depth-independent rotational motions in supervision. Such designs\nlimit the incorporation of strong geometric constraints, reducing reliability\nand robustness under diverse conditions. This study introduces a discriminative\ntreatment of motion components, leveraging the geometric regularities of their\nrespective rigid flows to benefit both depth and ego-motion estimation. Given\nconsecutive video frames, network outputs first align the optical axes and\nimaging planes of the source and target cameras. Optical flows between frames\nare transformed through these alignments, and deviations are quantified to\nimpose geometric constraints individually on each ego-motion component,\nenabling more targeted refinement. These alignments further reformulate the\njoint learning process into coaxial and coplanar forms, where depth and each\ntranslation component can be mutually derived through closed-form geometric\nrelationships, introducing complementary constraints that improve depth\nrobustness. DiMoDE, a general depth and ego-motion joint learning framework\nincorporating these designs, achieves state-of-the-art performance on multiple\npublic datasets and a newly collected diverse real-world dataset, particularly\nunder challenging conditions. Our source code will be publicly available at\nmias.group/DiMoDE upon publication.", "AI": {"tldr": "DiMoDE\u901a\u8fc7\u5bf9\u8fd0\u52a8\u5206\u91cf\u7684\u51e0\u4f55\u5bf9\u9f50\u4e0e\u5224\u522b\u5f0f\u7ea6\u675f\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u6df1\u5ea6\u4e0e\u81ea\u6211\u8fd0\u52a8\u8054\u5408\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u9c81\u68d2\u6027\u4e0e\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u6df1\u5ea6\u4e0e\u81ea\u6211\u8fd0\u52a8\u5b66\u4e60\u5c06\u81ea\u6211\u8fd0\u52a8\u89c6\u4e3a\u8f85\u52a9\u4efb\u52a1\u6216\u6df7\u5408\u5404\u79cd\u8fd0\u52a8\u7c7b\u578b\uff0c\u5bfc\u81f4\u51e0\u4f55\u7ea6\u675f\u5f31\u3001\u5728\u591a\u6837\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u5dee\u3002", "method": "\u63d0\u51faDiMoDE\u6846\u67b6\uff1a\u5148\u5bf9\u6e90/\u76ee\u6807\u76f8\u673a\u7684\u5149\u8f74\u548c\u6210\u50cf\u5e73\u9762\u8fdb\u884c\u5bf9\u9f50\uff0c\u53d8\u6362\u5149\u6d41\u5e76\u5206\u522b\u5bf9\u5404\u8fd0\u52a8\u5206\u91cf\u65bd\u52a0\u51e0\u4f55\u7ea6\u675f\uff0c\u5c06\u8054\u5408\u5b66\u4e60\u8f6c\u5316\u4e3a\u5171\u8f74(coaxial)\u548c\u5171\u9762(coplanar)\u5f62\u5f0f\uff0c\u5e76\u901a\u8fc7\u95ed\u5f0f\u51e0\u4f55\u5173\u7cfb\u4e92\u76f8\u63a8\u5bfc\u6df1\u5ea6\u4e0e\u5e73\u79fb\u5206\u91cf\u3002", "result": "DiMoDE\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u53ca\u4e00\u4efd\u65b0\u91c7\u96c6\u7684\u591a\u6837\u5316\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u9886\u5148\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6311\u6218\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u901a\u8fc7\u5bf9\u4e0d\u540c\u8fd0\u52a8\u5206\u91cf\u7684\u5224\u522b\u5f0f\u5904\u7406\uff0c\u5229\u7528\u5404\u81ea\u521a\u6027\u5149\u6d41\u7684\u51e0\u4f55\u89c4\u5f8b\u6765\u63d0\u5347\u6df1\u5ea6\u548c\u81ea\u6211\u8fd0\u52a8\u4f30\u8ba1\uff0c\u4ece\u800c\u6539\u8fdb\u9c81\u68d2\u6027\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2511.01510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01510", "abs": "https://arxiv.org/abs/2511.01510", "authors": ["Derong Kong", "Zhixiong Yang", "Shengxi Li", "Shuaifeng Zhi", "Li Liu", "Zhen Liu", "Jingyuan Xia"], "title": "Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement", "comment": "Accepted at NeurIPS 2025", "summary": "Low-light image enhancement (LLIE) faces persistent challenges in balancing\nreconstruction fidelity with cross-scenario generalization. While existing\nmethods predominantly focus on deterministic pixel-level mappings between\npaired low/normal-light images, they often neglect the continuous physical\nprocess of luminance transitions in real-world environments, leading to\nperformance drop when normal-light references are unavailable. Inspired by\nempirical analysis of natural luminance dynamics revealing power-law\ndistributed intensity transitions, this paper introduces Luminance-Aware\nStatistical Quantification (LASQ), a novel framework that reformulates LLIE as\na statistical sampling process over hierarchical luminance distributions. Our\nLASQ re-conceptualizes luminance transition as a power-law distribution in\nintensity coordinate space that can be approximated by stratified power\nfunctions, therefore, replacing deterministic mappings with probabilistic\nsampling over continuous luminance layers. A diffusion forward process is\ndesigned to autonomously discover optimal transition paths between luminance\nlayers, achieving unsupervised distribution emulation without normal-light\nreferences. In this way, it considerably improves the performance in practical\nsituations, enabling more adaptable and versatile light restoration. This\nframework is also readily applicable to cases with normal-light references,\nwhere it achieves superior performance on domain-specific datasets alongside\nbetter generalization-ability across non-reference datasets.", "AI": {"tldr": "\u5c06LLIE\u4ece\u786e\u5b9a\u6027\u6620\u5c04\u8f6c\u4e3a\u57fa\u4e8e\u529f\u7387\u5f8b\u5206\u5c42\u4eae\u5ea6\u7684\u7edf\u8ba1\u91c7\u6837\u4e0e\u6269\u6563\u524d\u5411\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u65e0\u76d1\u7763\u6cdb\u5316\u4e0e\u6709\u53c2\u8003\u65f6\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u50cf\u7d20\u7ea7\u786e\u5b9a\u6027\u6620\u5c04\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u4eae\u5ea6\u8fde\u7eed\u53d8\u5316\u7684\u7269\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u5728\u65e0\u6b63\u5e38\u5149\u7167\u53c2\u8003\u7684\u60c5\u51b5\u4e0b\u6cdb\u5316\u6027\u80fd\u4e0b\u964d\uff1b\u57fa\u4e8e\u5bf9\u81ea\u7136\u4eae\u5ea6\u52a8\u6001\u7684\u7ecf\u9a8c\u5206\u6790\uff0c\u53d1\u73b0\u4eae\u5ea6\u8fc7\u6e21\u670d\u4ece\u529f\u7387\u5f8b\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u51fa\u7edf\u8ba1\u5316\u5efa\u6a21\u3002", "method": "\u5f15\u5165\u4eae\u5ea6\u611f\u77e5\u7edf\u8ba1\u91cf\u5316\uff08LASQ\uff09\uff0c\u4ee5\u529f\u7387\u5f8b\u5206\u5e03\u8fd1\u4f3c\u81ea\u7136\u4eae\u5ea6\u8fc7\u6e21\uff0c\u4f7f\u7528\u5206\u5c42\u529f\u7387\u51fd\u6570\u8fdb\u884c\u5206\u6bb5\u903c\u8fd1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6269\u6563\u5f0f\u524d\u5411\u8fc7\u7a0b\u4ee5\u5728\u4eae\u5ea6\u5c42\u95f4\u81ea\u52a8\u641c\u7d22\u6700\u4f18\u8fc7\u6e21\u8def\u5f84\uff0c\u5b9e\u73b0\u4e86\u65e0\u53c2\u8003\u7684\u5206\u5e03\u4eff\u771f\u4e0e\u91c7\u6837\u6062\u590d\u3002", "result": "\u5728\u6709\u65e0\u6b63\u5e38\u5149\u7167\u53c2\u8003\u7684\u573a\u666f\u4e0b\u5747\u8868\u73b0\u826f\u597d\uff1a\u65e0\u53c2\u8003\u65f6\u63d0\u9ad8\u4e86\u8de8\u573a\u666f\u6cdb\u5316\u4e0e\u5b9e\u9645\u60c5\u51b5\u7684\u6062\u590d\u80fd\u529b\uff1b\u6709\u53c2\u8003\u65f6\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u66f4\u597d\u6027\u80fd\uff0c\u540c\u65f6\u5bf9\u672a\u89c1\u6570\u636e\u96c6\u4e5f\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86LASQ\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4f4e\u5149\u7167\u589e\u5f3a\u4efb\u52a1\u4ece\u786e\u5b9a\u6027\u50cf\u7d20\u6620\u5c04\u8f6c\u4e3a\u57fa\u4e8e\u5206\u5c42\u4eae\u5ea6\u5206\u5e03\u7684\u7edf\u8ba1\u91c7\u6837\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u65e0\u76d1\u7763\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u53ef\u9002\u5e94\u6027\u3002"}}
{"id": "2511.01513", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2511.01513", "abs": "https://arxiv.org/abs/2511.01513", "authors": ["Andrei-Timotei Ardelean", "Tim Weyrich"], "title": "Example-Based Feature Painting on Textures", "comment": "\"\\c{opyright} 2025 Andrei-Timotei Ardelean, Tim Weyrich. This is the\n  author's version of the work. It is posted here for your personal use. Not\n  for redistribution. The definitive Version of Record was published in ACM\n  Trans. Graph., Vol. 44, No. 6, https://doi.org/10.1145/3763301", "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u6807\u6ce8\u7684\u5f02\u5e38\u68c0\u6d4b+\u81ea\u52a8\u805a\u7c7b+\u6761\u4ef6\u6269\u6563\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u5bf9\u6709\u5c40\u90e8\u7455\u75b5\u7eb9\u7406\u7684\u53ef\u63a7\u3001\u53ef\u4ea4\u4e92\u7f16\u8f91\u4e0e\u65e0\u9650\u6269\u5c55\u751f\u6210\u3002", "motivation": "\u81ea\u7136\u754c\u4e2d\u5927\u91cf\u6750\u6599\u8868\u9762\u5b58\u5728\u5c40\u90e8\u5916\u89c2\u53d8\u5316\uff0c\u8981\u751f\u6210\u771f\u5b9e\u611f\u7eb9\u7406\u5fc5\u987b\u80fd\u5408\u6210\u8fd9\u4e9b\u75d5\u8ff9\u5e76\u5141\u8bb8\u7528\u6237\u53ef\u63a7\u7f16\u8f91\uff0c\u4f46\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6982\u62ec\u591a\u6837\u6027\uff0c\u56e0\u6b64\u91c7\u7528\u65e0\u76d1\u7763\u68c0\u6d4b\u4e0e\u5b66\u4e60\u6765\u81ea\u52a8\u53d1\u73b0\u5e76\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4ece\u65e0\u6807\u6ce8\u6837\u672c\u4e2d\u68c0\u6d4b\u5916\u89c2\u6539\u53d8\u7279\u5f81\uff0c\u968f\u540e\u5bf9\u8fd9\u4e9b\u7279\u5f81\u8fdb\u884c\u81ea\u52a8\u805a\u7c7b\u4ee5\u5f97\u5230\u8bed\u4e49\u4e00\u81f4\u7684\u7c7b\u522b\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\u7c7b\u522b\u8bad\u7ec3\u6761\u4ef6\u751f\u6210\u6a21\u578b\uff1b\u5f15\u5165\u6269\u6563\uff08diffusion\uff09\u57fa\u7840\u7684\u7f16\u8f91\u7b97\u6cd5\u548c\u65e0\u9650\u5e73\u7a33\u7eb9\u7406\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u4ece\u5c11\u91cf\u56fe\u50cf\u51fa\u53d1\uff0c\u6784\u5efa\u51fa\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u751f\u6210\u6a21\u578b\uff0c\u7528\u6237\u53ef\u5728\u4efb\u610f\u5c3a\u5bf8\u7eb9\u7406\u4e0a\u7ed8\u5236\u4e0e\u521b\u5efa\u7279\u5f81\uff1b\u63d0\u51fa\u7684\u6269\u6563\u7f16\u8f91\u4e0e\u65e0\u9650\u7eb9\u7406\u751f\u6210\u7b97\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u573a\u666f\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5177\u6709\u5c40\u90e8\u7279\u5f81\uff08\u5982\u6c61\u6e0d\u3001\u7834\u6d1e\u3001\u78e8\u635f\u3001\u892a\u8272\u7b49\uff09\u7684\u7eb9\u7406\u8fdb\u884c\u53ef\u63a7\u751f\u6210\u4e0e\u7f16\u8f91\u3002"}}
{"id": "2511.01517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01517", "abs": "https://arxiv.org/abs/2511.01517", "authors": ["Serkan Ozturk", "Samet Hicsonmez", "Pinar Duygulu"], "title": "NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation", "comment": "Under review", "summary": "Current text conditioned image generation methods output realistic looking\nimages, but they fail to capture specific styles. Simply finetuning them on the\ntarget style datasets still struggles to grasp the style features. In this\nwork, we present a novel contrastive learning framework to improve the\nstylization capability of large text-to-image diffusion models. Motivated by\nthe astonishing advance in image generation models that makes synthetic data an\nintrinsic part of model training in various computer vision tasks, we exploit\nsynthetic image generation in our approach. Usually, the generated synthetic\ndata is dependent on the task, and most of the time it is used to enlarge the\navailable real training dataset. With NSYNC, alternatively, we focus on\ngenerating negative synthetic sets to be used in a novel contrastive training\nscheme along with real positive images. In our proposed training setup, we\nforward negative data along with positive data and obtain negative and positive\ngradients, respectively. We then refine the positive gradient by subtracting\nits projection onto the negative gradient to get the orthogonal component,\nbased on which the parameters are updated. This orthogonal component eliminates\nthe trivial attributes that are present in both positive and negative data and\ndirects the model towards capturing a more unique style. Experiments on various\nstyles of painters and illustrators show that our approach improves the\nperformance over the baseline methods both quantitatively and qualitatively.\nOur code is available at https://github.com/giddyyupp/NSYNC.", "AI": {"tldr": "NSYNC\u7528\u8d1f\u5408\u6210\u56fe\u50cf+\u68af\u5ea6\u6b63\u4ea4\u5316\u7684\u5bf9\u6bd4\u8bad\u7ec3\uff0c\u53bb\u9664\u5171\u6709\u5c5e\u6027\u5e72\u6270\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6761\u4ef6\u6587\u672c\u751f\u6210\u6a21\u578b\u867d\u80fd\u4ea7\u751f\u903c\u771f\u56fe\u50cf\uff0c\u4f46\u96be\u4ee5\u7cbe\u786e\u6355\u6349\u7279\u5b9a\u827a\u672f\u98ce\u683c\uff1b\u5355\u7eaf\u5fae\u8c03\u5728\u76ee\u6807\u98ce\u683c\u6570\u636e\u4e0a\u5f80\u5f80\u65e0\u6cd5\u5b66\u5230\u98ce\u683c\u7279\u5f81\u3002\u4f5c\u8005\u5229\u7528\u5408\u6210\u6570\u636e\u6784\u5efa\u5bf9\u6bd4\u5b66\u4e60\u4fe1\u53f7\uff0c\u8feb\u4f7f\u6a21\u578b\u533a\u5206\u76ee\u6807\u98ce\u683c\u4e0e\u76f8\u8fd1\u4f46\u4e0d\u5c5e\u4e8e\u76ee\u6807\u7684\u5c5e\u6027\u3002", "method": "\u5229\u7528\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u76ee\u6807\u98ce\u683c\u76f8\u4f3c\u4f46\u975e\u76ee\u6807\u98ce\u683c\u7684\u8d1f\u5408\u6210\u6570\u636e\uff1b\u5728\u8bad\u7ec3\u4e2d\u540c\u65f6\u8f93\u5165\u6b63\uff08\u771f\u5b9e\u76ee\u6807\u98ce\u683c\uff09\u548c\u8d1f\u6837\u672c\uff0c\u5206\u522b\u8ba1\u7b97\u68af\u5ea6\uff1b\u5c06\u6b63\u68af\u5ea6\u5728\u8d1f\u68af\u5ea6\u4e0a\u7684\u6295\u5f71\u53bb\u9664\uff0c\u53ea\u4fdd\u7559\u6b63\u68af\u5ea6\u7684\u6b63\u4ea4\u5206\u91cf\u7528\u4e8e\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4f4d\u753b\u5bb6\u548c\u63d2\u753b\u5bb6\u7684\u98ce\u683c\u8fc1\u79fb\u4efb\u52a1\u4e0a\uff0cNSYNC\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u89c6\u89c9\u6548\u679c\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u8d1f\u5408\u6210\u6837\u672c\u4e0e\u68af\u5ea6\u6b63\u4ea4\u5316\u80fd\u66f4\u597d\u5730\u63d0\u53d6\u72ec\u7279\u98ce\u683c\u7279\u5f81\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51faNSYNC\uff0c\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u201c\u8d1f\u201d\u5408\u6210\u56fe\u50cf\u5e76\u5728\u68af\u5ea6\u7a7a\u95f4\u4e2d\u53bb\u9664\u4e0e\u8d1f\u6837\u672c\u5171\u4eab\u7684\u6210\u5206\uff0c\u4ece\u800c\u589e\u5f3a\u5927\u89c4\u6a21\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u5316\u80fd\u529b\u3002"}}
{"id": "2511.01541", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01541", "abs": "https://arxiv.org/abs/2511.01541", "authors": ["Arthur Hubert", "Gamal Elghazaly", "Rapha\u00ebl Frank"], "title": "Driving scenario generation and evaluation using a structured layer representation and foundational models", "comment": null, "summary": "Rare and challenging driving scenarios are critical for autonomous vehicle\ndevelopment. Since they are difficult to encounter, simulating or generating\nthem using generative models is a popular approach. Following previous efforts\nto structure driving scenario representations in a layer model, we propose a\nstructured five-layer model to improve the evaluation and generation of rare\nscenarios. We use this model alongside large foundational models to generate\nnew driving scenarios using a data augmentation strategy. Unlike previous\nrepresentations, our structure introduces subclasses and characteristics for\nevery agent of the scenario, allowing us to compare them using an embedding\nspecific to our layer-model. We study and adapt two metrics to evaluate the\nrelevance of a synthetic dataset in the context of a structured representation:\nthe diversity score estimates how different the scenarios of a dataset are from\none another, while the originality score calculates how similar a synthetic\ndataset is from a real reference set. This paper showcases both metrics in\ndifferent generation setup, as well as a qualitative evaluation of synthetic\nvideos generated from structured scenario descriptions. The code and extended\nresults can be found at https://github.com/Valgiz/5LMSG.", "AI": {"tldr": "\u63d0\u51fa\u4e94\u5c42\u7ed3\u6784\u5316\u9a7e\u9a76\u573a\u666f\u6a21\u578b\uff0c\u7ed3\u5408\u5927\u6a21\u578b\u751f\u6210\u7a00\u6709\u573a\u666f\uff0c\u5e76\u7528\u591a\u6837\u6027\u4e0e\u539f\u521b\u6027\u6307\u6807\u8bc4\u4f30\u751f\u6210\u6570\u636e\u7684\u8d28\u91cf\u4e0e\u76f8\u5173\u6027\u3002", "motivation": "\u7f55\u89c1\u9a7e\u9a76\u573a\u666f\u96be\u4ee5\u91c7\u96c6\uff0c\u9700\u901a\u8fc7\u6a21\u62df\u6216\u751f\u6210\u65b9\u6cd5\u6269\u5145\u6570\u636e\u4ee5\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7814\u53d1\uff1b\u4e3a\u66f4\u597d\u5730\u8bc4\u4f30\u4e0e\u751f\u6210\u6b64\u7c7b\u573a\u666f\uff0c\u63d0\u51fa\u66f4\u7ec6\u5316\u7684\u7ed3\u6784\u5316\u8868\u793a\u5e76\u914d\u5957\u6307\u6807\u3002", "method": "\u6784\u5efa\u4e94\u5c42\u5c42\u7ea7\u8868\u793a\uff08\u5f15\u5165\u6bcf\u4e2a\u53c2\u4e0e\u8005\u7684\u5b50\u7c7b\u522b\u4e0e\u7279\u5f81\uff09\uff0c\u4f7f\u7528\u5927\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u751f\u6210\uff0c\u57fa\u4e8e\u8be5\u5c42\u6a21\u578b\u5b66\u4e60\u5d4c\u5165\u5e76\u91c7\u7528\u4e24\u79cd\u6307\u6807\uff08\u591a\u6837\u6027\u4e0e\u539f\u521b\u6027\uff09\u8bc4\u4f30\u5408\u6210\u6570\u636e\uff0c\u540c\u65f6\u8fdb\u884c\u5b9a\u6027\u5408\u6210\u89c6\u9891\u5c55\u793a\u3002", "result": "\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u751f\u6210\u8bbe\u7f6e\u4e0b\u591a\u6837\u6027\u4e0e\u539f\u521b\u6027\u6307\u6807\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5b9a\u6027\u89c6\u9891\u9a8c\u8bc1\u5408\u6210\u573a\u666f\u7684\u53ef\u7528\u6027\uff0c\u63d0\u4f9b\u4ee3\u7801\u4e0e\u6269\u5c55\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u4e94\u5c42\u9a7e\u9a76\u573a\u666f\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u5927\u6a21\u578b\u751f\u6210\u4e0e\u8bc4\u4ef7\u7f55\u89c1\u9a7e\u9a76\u573a\u666f\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u591a\u6837\u4e14\u4e0e\u771f\u5b9e\u6570\u636e\u6709\u4e00\u5b9a\u539f\u521b\u6027\u7684\u5408\u6210\u573a\u666f\u3002"}}
{"id": "2511.01546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01546", "abs": "https://arxiv.org/abs/2511.01546", "authors": ["Ge Gao", "Zishuo Gao", "Hongyan Cui", "Zhiyang Jia", "Zhuang Luo", "ChaoPeng Liu"], "title": "PCD-ReID: Occluded Person Re-Identification for Base Station Inspection", "comment": "11 pages, 7 figures", "summary": "Occluded pedestrian re-identification (ReID) in base station environments is\na critical task in computer vision, particularly for surveillance and security\napplications. This task faces numerous challenges, as occlusions often obscure\nkey body features, increasing the complexity of identification. Traditional\nResNet-based ReID algorithms often fail to address occlusions effectively,\nnecessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component\nDiscrepancy) algorithm to address these issues. The contributions of this work\nare as follows: To tackle the occlusion problem, we design a Transformer-based\nPCD network capable of extracting shared component features, such as helmets\nand uniforms. To mitigate overfitting on public datasets, we collected new\nreal-world patrol surveillance images for model training, covering six months,\n10,000 individuals, and over 50,000 images. Comparative experiments with\nexisting ReID algorithms demonstrate that our model achieves a mean Average\nPrecision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1\nimprovement over ResNet50-based methods. Experimental evaluations indicate that\nPCD-ReID effectively achieves occlusion-aware ReID performance for personnel in\ntower inspection scenarios, highlighting its potential for practical deployment\nin surveillance and security applications.", "AI": {"tldr": "\u63d0\u51faTransformer\u4e3a\u6838\u5fc3\u7684PCD-ReID\uff0c\u901a\u8fc7\u7ec4\u4ef6\u7ea7\u7279\u5f81\u4e0e\u5927\u89c4\u6a21\u5de1\u903b\u6570\u636e\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u906e\u6321\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5177\u5907\u5b9e\u5883\u90e8\u7f72\u4ef7\u503c\u3002", "motivation": "\u4f20\u7edfResNet\u57fa\u7684ReID\u65b9\u6cd5\u5728\u906e\u6321\u573a\u666f\u4e0b\u6548\u679c\u5dee\uff0c\u5b9e\u9645\u76d1\u63a7\u6570\u636e\u5206\u5e03\u4e0e\u516c\u5f00\u6570\u636e\u4e0d\u540c\uff0c\u9700\u65b0\u65b9\u6cd5\u548c\u6570\u636e\u63d0\u5347\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684PCD\u7f51\u7edc\uff0c\u805a\u7126\u63d0\u53d6\u5171\u4eab\u7ec4\u4ef6\u7279\u5f81\uff08\u5982\u5934\u76d4\u3001\u5236\u670d\uff09\uff0c\u5e76\u901a\u8fc7\u65b0\u6536\u96c6\u7684\u5de1\u903b\u76d1\u63a7\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4ee5\u51cf\u8f7b\u8fc7\u62df\u5408\u3002", "result": "\u5728\u65b0\u6570\u636e\u548c\u5bf9\u6bd4\u5b9e\u9a8c\u4e2d\uff0cPCD-ReID\u8fbe\u6210mAP=79.0%\u3001Rank-1=82.7%\uff0cRank-1\u76f8\u8f83ResNet50\u63d0\u534715.9%\u3002", "conclusion": "PCD-ReID\u5728\u5854\u68c0\u573a\u666f\u4e0b\u80fd\u663e\u8457\u63d0\u9ad8\u906e\u6321\u884c\u4eba\u91cd\u8bc6\u522b\u6027\u80fd\uff0c\u5c24\u5176\u5728Rank-1\u4e0a\u8f83ResNet50\u63d0\u5347\u660e\u663e\uff0c\u5177\u5907\u5b9e\u7528\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2511.01549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01549", "abs": "https://arxiv.org/abs/2511.01549", "authors": ["Mikhail Konov", "Lion J. Gleiter", "Khoa Co", "Monica Yabal", "Tingying Peng"], "title": "NOA: a versatile, extensible tool for AI-based organoid analysis", "comment": null, "summary": "AI tools can greatly enhance the analysis of organoid microscopy images, from\ndetection and segmentation to feature extraction and classification. However,\ntheir limited accessibility to biologists without programming experience\nremains a major barrier, resulting in labor-intensive and largely manual\nworkflows. Although a few AI models for organoid analysis have been developed,\nmost existing tools remain narrowly focused on specific tasks. In this work, we\nintroduce the Napari Organoid Analyzer (NOA), a general purpose graphical user\ninterface to simplify AI-based organoid analysis. NOA integrates modules for\ndetection, segmentation, tracking, feature extraction, custom feature\nannotation and ML-based feature prediction. It interfaces multiple\nstate-of-the-art algorithms and is implemented as an open-source napari plugin\nfor maximal flexibility and extensibility. We demonstrate the versatility of\nNOA through three case studies, involving the quantification of morphological\nchanges during organoid differentiation, assessment of phototoxicity effects,\nand prediction of organoid viability and differentiation state. Together, these\nexamples illustrate how NOA enables comprehensive, AI-driven organoid image\nanalysis within an accessible and extensible framework.", "AI": {"tldr": "NOA: an open-source napari plugin providing an accessible GUI that consolidates multiple AI tools for comprehensive organoid image analysis, validated on three biological case studies.", "motivation": "Existing AI tools for organoid microscopy are limited in accessibility and task scope, creating a need for an accessible, general-purpose GUI that enables biologists without programming skills to perform comprehensive AI-driven analysis.", "method": "NOA integrates multiple state-of-the-art algorithms as modules within a napari plugin, providing GUIs for detection, segmentation, tracking, feature extraction, custom annotation, and ML-based feature prediction; it interfaces with various algorithms and is open-source for flexibility.", "result": "Through three case studies\u2014morphological quantification during differentiation, phototoxicity assessment, and prediction of viability/differentiation\u2014NOA demonstrates comprehensive and effective AI-driven organoid image analysis in an accessible, extensible framework.", "conclusion": "NOA is a versatile, user-friendly napari plugin that makes AI-based organoid image analysis accessible to non-programmers, integrating detection, segmentation, tracking, feature extraction, annotation and ML prediction into one extensible GUI."}}
{"id": "2511.01571", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01571", "abs": "https://arxiv.org/abs/2511.01571", "authors": ["Wenqi Liang", "Gan Sun", "Yao He", "Jiahua Dong", "Suyan Dai", "Ivan Laptev", "Salman Khan", "Yang Cong"], "title": "PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model", "comment": "17pages,7 figures, 5 tabels", "summary": "Vision-Language-Action models (VLAs) are emerging as powerful tools for\nlearning generalizable visuomotor control policies. However, current VLAs are\nmostly trained on large-scale image-text-action data and remain limited in two\nkey ways: (i) they struggle with pixel-level scene understanding, and (ii) they\nrely heavily on textual prompts, which reduces their flexibility in real-world\nsettings. To address these challenges, we introduce PixelVLA, the first VLA\nmodel designed to support both pixel-level reasoning and multimodal prompting\nwith text and visual inputs. Our approach is built on a new visuomotor\ninstruction tuning framework that integrates a multiscale pixel-aware encoder\nwith a visual prompting encoder. To train PixelVLA effectively, we further\npropose a two-stage automated annotation pipeline that generates Pixel-160K, a\nlarge-scale dataset with pixel-level annotations derived from existing robot\ndata. Experiments on three standard VLA benchmarks and two VLA model variants\nshow that PixelVLA improves manipulation success rates by 10.1%-17.8% over\nOpenVLA, while requiring only 1.5% of its pretraining cost. These results\ndemonstrate that PixelVLA can be integrated into existing VLAs to enable more\naccurate, efficient, and versatile robot control in complex environments. The\ndataset and code will be released as open source.", "AI": {"tldr": "PixelVLA\u5f15\u5165\u50cf\u7d20\u7ea7\u63a8\u7406\u4e0e\u89c6\u89c9\u63d0\u793a\uff0c\u901a\u8fc7Pixel-160K\u6570\u636e\u548c\u65b0\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u5728\u66f4\u4f4e\u9884\u8bad\u7ec3\u6210\u672c\u4e0b\u663e\u8457\u63d0\u5347\u64cd\u63a7\u6210\u529f\u7387\uff0c\u589e\u5f3aVLA\u7684\u7cbe\u5ea6\u4e0e\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709Vision-Language-Action\u6a21\u578b\u5728\u50cf\u7d20\u7ea7\u573a\u666f\u7406\u89e3\u80fd\u529b\u5f31\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff0c\u9650\u5236\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u7075\u6d3b\u6027\u4e0e\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u591a\u5c3a\u5ea6\u50cf\u7d20\u611f\u77e5\u7f16\u7801\u5668\u548c\u89c6\u89c9\u63d0\u793a\u7f16\u7801\u5668\u7684\u89c6\u52a8\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\uff1b\u8bbe\u8ba1\u4e24\u9636\u6bb5\u81ea\u52a8\u6ce8\u91ca\u6d41\u6c34\u7ebf\uff0c\u4ece\u673a\u5668\u4eba\u6570\u636e\u751f\u6210Pixel-160K\u50cf\u7d20\u7ea7\u6807\u6ce8\uff1b\u5728\u4e09\u4e2aVLA\u57fa\u51c6\u4e0e\u4e24\u4e2a\u6a21\u578b\u53d8\u4f53\u4e0a\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6VLA\u57fa\u51c6\u4e0e\u4e24\u4e2a\u6a21\u578b\u53d8\u4f53\u4e0a\uff0cPixelVLA\u76f8\u6bd4OpenVLA\u5c06\u64cd\u4f5c\u6210\u529f\u7387\u63d0\u9ad8\u4e8610.1%\u81f317.8%\uff0c\u540c\u65f6\u4ec5\u9700OpenVLA 1.5%\u7684\u9884\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "PixelVLA\u901a\u8fc7\u5f15\u5165\u50cf\u7d20\u7ea7\u7f16\u7801\u5668\u4e0e\u89c6\u89c9\u63d0\u793a\u7f16\u7801\u5668\uff0c\u5e76\u5229\u7528\u81ea\u52a8\u6807\u6ce8\u751f\u6210Pixel-160K\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u5728\u50cf\u7d20\u7ea7\u7406\u89e3\u4e0e\u591a\u6a21\u6001\u63d0\u793a\u4e0b\u7684\u64cd\u63a7\u6027\u80fd\u3002"}}
{"id": "2511.01574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01574", "abs": "https://arxiv.org/abs/2511.01574", "authors": ["Md Sumon Ali", "Muzammil Behzad"], "title": "Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images", "comment": "9 pagers, 8 Figures", "summary": "Compared to traditional methods, Deep Learning (DL) becomes a key technology\nfor computer vision tasks. Synthetic data generation is an interesting use case\nfor DL, especially in the field of medical imaging such as Magnetic Resonance\nImaging (MRI). The need for this task since the original MRI data is limited.\nThe generation of realistic medical images is completely difficult and\nchallenging. Generative Adversarial Networks (GANs) are useful for creating\nsynthetic medical images. In this paper, we propose a DL based methodology for\ncreating synthetic MRI data using the Deep Convolutional Generative Adversarial\nNetwork (DC-GAN) to address the problem of limited data. We also employ a\nConvolutional Neural Network (CNN) classifier to classify the brain tumor using\nsynthetic data and real MRI data. CNN is used to evaluate the quality and\nutility of the synthetic images. The classification result demonstrates\ncomparable performance on real and synthetic images, which validates the\neffectiveness of GAN-generated images for downstream tasks.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u7528DC-GAN\u751f\u6210\u5408\u6210\u8111MRI\u5e76\u7528CNN\u9a8c\u8bc1\u5176\u5206\u7c7b\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u5408\u6210\u56fe\u50cf\u53ef\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u66ff\u4ee3\u90e8\u5206\u771f\u5b9e\u6570\u636e\u3002", "motivation": "\u7531\u4e8e\u771f\u5b9eMRI\u6570\u636e\u6709\u9650\u4e14\u83b7\u53d6\u96be\u5ea6\u5927\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u751f\u6210\u903c\u771f\u5408\u6210\u533b\u5b66\u5f71\u50cf\u6765\u6269\u5145\u6570\u636e\u96c6\uff0c\u4ee5\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528Deep Convolutional GAN\u751f\u6210\u5408\u6210MRI\u56fe\u50cf\uff0c\u5e76\u4f7f\u7528CNN\u5206\u7c7b\u5668\u5bf9\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\u6bd4\u8f83\uff0c\u7528\u5206\u7c7b\u6027\u80fd\u8bc4\u4f30\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7528\u5408\u6210\u56fe\u50cf\u8bad\u7ec3\u7684CNN\u5728\u8111\u80bf\u7624\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u7528\u771f\u5b9e\u56fe\u50cf\u8bad\u7ec3\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u8bc1\u660eGAN\u751f\u6210\u7684\u6570\u636e\u5177\u6709\u53ef\u7528\u6027\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4f7f\u7528DC-GAN\u751f\u6210MRI\u8111\u80bf\u7624\u56fe\u50cf\u5e76\u7528CNN\u8bc4\u4f30\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u7ed3\u8bba\u662fGAN\u751f\u6210\u56fe\u50cf\u5728\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u4e2d\u53ef\u8fbe\u5230\u4e0e\u771f\u5b9e\u56fe\u50cf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2511.01593", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01593", "abs": "https://arxiv.org/abs/2511.01593", "authors": ["Yizhu Chen", "Chen Ju", "Zhicheng Wang", "Shuai Xiao", "Xu Chen", "Jinsong Lan", "Xiaoyong Zhu", "Ying Chen"], "title": "Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation", "comment": null, "summary": "The unification of understanding and generation within a single multi-modal\nlarge model (MLLM) remains one significant challenge, largely due to the\ndichotomy between continuous and discrete visual tokenizations. Continuous\ntokenizer (CT) achieves strong performance by bridging multiple\nindependently-trained understanding modules and generation modules, but suffers\nfrom complex multi-stage pipelines and substantial engineering overhead.\nConversely, discrete tokenizers (DT) offer a conceptually elegant idea by\nquantizing each image into a primitive, but inevitably leading to information\nloss and performance degradation. To resolve this tension, we question the\nbinary choice between CT and DT, inspired by the wave-particle duality of\nlight, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).\nWe treat visual data as a flexible composition of image primitives derived from\nquantized codebooks, with the crucial insight that the primitive number\nassigned to each visual sample is adaptively determined according to its\ncomplexity: simple instances use a few primitives, emulating discrete\ntokenization, while complex instances use many, approximating continuous\ntokenization. Two core components are designed: Diverse Quantitative\nPrimitives, which encourage primitives orthogonality to better populate\ninformation space, and Dynamic Primitive Allocator, which assesses sample\ncomplexity to determine the optimal set of primitives. Extensive experiments on\nreconstruction, retrieval and classification show that CDD-VT achieves superior\nperformance over to specialized CT and DT, effectively getting strong result\nwithin a concise and scalable MLLM.", "AI": {"tldr": "\u63d0\u51faCDD-VT\uff1a\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u5206\u914d\u91cf\u5316\u57fa\u5143\u6570\u91cf\u7684\u89c6\u89c9\u6807\u8bb0\u5668\uff0c\u901a\u8fc7\u5728\u8fde\u7eed\u4e0e\u79bb\u6563\u4e4b\u95f4\u52a8\u6001\u5e73\u8861\uff0c\u63d0\u5347\u91cd\u5efa\u3001\u68c0\u7d22\u4e0e\u5206\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u964d\u4f4e\u5de5\u7a0b\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u5927\u6a21\u578b\u4e2d\u8fde\u7eed\u6807\u8bb0\u5668\u4e0e\u79bb\u6563\u6807\u8bb0\u5668\u4e4b\u95f4\u7684\u77db\u76fe\uff1a\u524d\u8005\u6027\u80fd\u597d\u4f46\u5de5\u7a0b\u590d\u6742\uff0c\u540e\u8005\u7b80\u6d01\u4f46\u4fe1\u606f\u4e22\u5931\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51faDualistic Visual Tokenizer\uff0c\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1aDiverse Quantitative Primitives\uff08\u4fc3\u8fdb\u57fa\u5143\u6b63\u4ea4\u4ee5\u6269\u5c55\u4fe1\u606f\u8986\u76d6\uff09\u548cDynamic Primitive Allocator\uff08\u6839\u636e\u6837\u672c\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u5206\u914d\u57fa\u5143\u6570\u91cf\uff09\u3002\u57fa\u4e8e\u91cf\u5316\u7801\u672c\u5bf9\u56fe\u50cf\u8fdb\u884c\u8868\u5f81\uff0c\u590d\u6742\u56fe\u50cf\u4f7f\u7528\u66f4\u591a\u57fa\u5143\uff0c\u7b80\u5355\u56fe\u50cf\u4f7f\u7528\u66f4\u5c11\u57fa\u5143\uff0c\u4ece\u800c\u5728\u8fde\u7eed\u4e0e\u79bb\u6563\u4e4b\u95f4\u5e73\u6ed1\u5207\u6362\u3002", "result": "\u5728\u91cd\u5efa\u3001\u68c0\u7d22\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cCDD-VT\u4f18\u4e8e\u4e13\u95e8\u7684\u8fde\u7eed\u6216\u79bb\u6563\u65b9\u6cd5\uff0c\u4e14\u5728\u5b9e\u73b0\u4e0a\u66f4\u7b80\u6d01\u3001\u53ef\u6269\u5c55\uff0c\u517c\u987e\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\u3002", "conclusion": "CDD-VT\u901a\u8fc7\u5728\u8fde\u7eed\u548c\u79bb\u6563\u89c6\u89c9\u6807\u8bb0\u5316\u4e4b\u95f4\u5efa\u7acb\u81ea\u9002\u5e94\u4e2d\u95f4\u5730\u5e26\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u4e24\u8005\u7684\u5404\u81ea\u7f3a\u9677\uff0c\u517c\u987e\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\uff0c\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u3002"}}
{"id": "2511.01600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01600", "abs": "https://arxiv.org/abs/2511.01600", "authors": ["Agnar Martin Bj\u00f8rnstad", "Elias Stenhede", "Arian Ranjbar"], "title": "Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography", "comment": null, "summary": "Accurate tumor size measurement is a cornerstone of evaluating cancer\ntreatment response. The most widely adopted standard for this purpose is the\nResponse Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on\nmeasuring the longest tumor diameter in a single plane. However, volumetric\nmeasurements have been shown to provide a more reliable assessment of treatment\neffect. Their clinical adoption has been limited, though, due to the\nlabor-intensive nature of manual volumetric annotation. In this paper, we\npresent Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed\nfor efficient volumetric tumor segmentation from CT scans annotated with RECIST\nannotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:\nPan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice\nSimilarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of\n63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an\naverage inference time of 14.4 s on CPU on the public validation dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u6b3e\u9762\u5411RECIST\u6ce8\u91ca\u7684\u8f7b\u91cf\u5316ENSAM\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a5\u53d7\u7684\u5206\u5272\u6027\u80fd\u4e0e\u8f83\u4f4e\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u5408\u5728CPU\u4e0a\u8fdb\u884c\u9ad8\u6548\u4f53\u79ef\u5206\u5272\u3002", "motivation": "\u4f20\u7edfRECIST\u57fa\u4e8e\u5355\u5e73\u9762\u6700\u957f\u76f4\u5f84\uff0c\u4f53\u79ef\u6d4b\u91cf\u66f4\u80fd\u53ef\u9760\u53cd\u6620\u6cbb\u7597\u6548\u679c\uff0c\u4f46\u4eba\u5de5\u4f53\u79ef\u5206\u5272\u8017\u65f6\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\uff1b\u56e0\u6b64\u9700\u8981\u8f7b\u91cf\u3001\u9ad8\u6548\u7684\u81ea\u52a8\u4f53\u79ef\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u5728ENSAM\u57fa\u7840\u4e0a\u8fdb\u884c\u67b6\u6784\u7b80\u5316\u4e0e\u53c2\u6570\u538b\u7f29\uff0c\u4f18\u5316\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\uff0c\u4f7f\u4e4b\u5728CPU\u4e0a\u4e5f\u80fd\u8f83\u5feb\u63a8\u7406\uff1b\u4f7f\u7528RECIST\u6807\u6ce8\u4f5c\u4e3a\u5f31\u76d1\u7763\u4fe1\u53f7\u8bad\u7ec3\u4f53\u79ef\u5206\u5272\u6a21\u578b\uff0c\u53c2\u52a0MICCAI FLARE 2025\u4efb\u52a1\u5e76\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728MICCAI FLARE 2025\u516c\u5f00\u9a8c\u8bc1\u96c6\u548c\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e2d\uff0cLite ENSAM\u5728\u9690\u85cf\u6d4b\u8bd5\u96c6\u4e0a\u53d6\u5f97DSC=60.7%\u3001NSD=63.6%\uff1b\u5728\u516c\u5f00\u9a8c\u8bc1\u96c6\u4e0aCPU\u5e73\u5747\u5185\u5b58\u5360\u7528\u7ea650.6 GB\u3001\u5e73\u5747\u63a8\u7406\u65f6\u95f414.4 s\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86Lite ENSAM\uff0c\u4e00\u79cd\u8f7b\u91cf\u5316\u7684ENSAM\u67b6\u6784\u6539\u8fdb\uff0c\u7528\u4e8e\u4ece\u5e26\u6709RECIST\u6807\u6ce8\u7684CT\u5f71\u50cf\u9ad8\u6548\u8fdb\u884c\u80bf\u7624\u4f53\u79ef\u5206\u5272\u3002"}}
{"id": "2511.01610", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01610", "abs": "https://arxiv.org/abs/2511.01610", "authors": ["Mahmut Selman Gokmen", "Cody Bumgardner"], "title": "DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning", "comment": null, "summary": "Vision Foundation Models (VFMs) have advanced representation learning through\nself-supervised methods. However, existing training pipelines are often\ninflexible, domain-specific, or computationally expensive, which limits their\nusability across different domains and resource settings. DINO-MX is a modular\nand extensible training framework that combines the core principles of DINO,\nDINOv2 and DINOv3 within a unified configuration-driven system. It supports a\nvariety of transformer-based architectures and is fully compatible with the\nHugging Face ecosystem. The framework includes multiple training strategies\nsuch as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,\nalong with support for distributed training through both Distributed Data\nParallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to\nwork with both natural and specialized data types, including single- and\nmulti-channel images. Experimental results on diverse datasets show that\nDINO-MX achieves competitive performance while significantly reducing\ncomputational costs. Additionally, it offers interpretability tools and a\nlabel-guided data augmentation method that improves attention-based\nlocalization without the need for extra detection or segmentation heads.\nDINO-MX provides a reproducible and scalable foundation for developing,\nadapting, and benchmarking self-supervised vision models across a range of\nresearch and real-world applications.", "AI": {"tldr": "DINO-MX \u662f\u4e00\u4e2a\u914d\u7f6e\u9a71\u52a8\u7684\u3001\u6a21\u5757\u5316\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u517c\u5bb9 DINO \u7cfb\u5217\u601d\u60f3\u4e0e Hugging Face\uff0c\u652f\u6301\u8282\u7ea6\u7b97\u529b\u7684\u8bad\u7ec3\u7b56\u7565\u4e0e\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5e76\u5728\u591a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u4e0e\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u89c6\u89c9\u8bad\u7ec3\u6d41\u7a0b\u5728\u7075\u6d3b\u6027\u3001\u9886\u57df\u901a\u7528\u6027\u4e0e\u8ba1\u7b97\u6210\u672c\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u8de8\u57df\u548c\u4e0d\u540c\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u53ef\u7528\u6027\u3002\u76ee\u6807\u662f\u6784\u5efa\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4fbf\u4e8e\u5728\u591a\u79cd\u573a\u666f\u4e2d\u5f00\u53d1\u4e0e\u57fa\u51c6\u6d4b\u8bd5 VFMs\u3002", "method": "\u5c06 DINO \u7cfb\u5217\uff08DINO\u3001DINOv2\u3001DINOv3\uff09\u7684\u6838\u5fc3\u539f\u7406\u6574\u5408\u5230\u7edf\u4e00\u914d\u7f6e\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u79cd transformer \u67b6\u6784\u3001\u5355/\u591a\u901a\u9053\u56fe\u50cf\u8f93\u5165\uff1b\u5b9e\u73b0\u8bad\u7ec3\u7b56\u7565\u5305\u62ec LoRA\u3001\u5c42\u51bb\u7ed3\u3001\u77e5\u8bc6\u84b8\u998f\uff1b\u652f\u6301 DDP \u4e0e FSDP \u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff1b\u63d0\u4f9b\u89e3\u91ca\u6027\u5de5\u5177\u4e0e\u6807\u7b7e\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u7528\u4e8e\u6ce8\u610f\u529b\u5b9a\u4f4d\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a DINO-MX \u5728\u6027\u80fd\u4e0a\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u5f00\u9500\uff1b\u6807\u7b7e\u5f15\u5bfc\u7684\u6570\u636e\u589e\u5f3a\u80fd\u5728\u4e0d\u5f15\u5165\u989d\u5916\u68c0\u6d4b/\u5206\u5272\u5934\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u5b9a\u4f4d\u6548\u679c\uff1b\u6846\u67b6\u4e0e Hugging Face \u517c\u5bb9\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7684\u8bad\u7ec3\u4e0e\u6269\u5c55\u3002", "conclusion": "DINO-MX \u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u914d\u7f6e\u9a71\u52a8\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u517c\u5bb9\u591a\u79cd Transformer \u67b6\u6784\u4e0e Hugging Face \u751f\u6001\uff0c\u652f\u6301 LoRA\u3001\u5c42\u51bb\u7ed3\u3001\u84b8\u998f\u53ca\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u5e76\u80fd\u5904\u7406\u81ea\u7136\u4e0e\u4e13\u7528\u6570\u636e\u7c7b\u578b\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728\u591a\u6837\u6570\u636e\u96c6\u4e0a\u5176\u6027\u80fd\u6709\u7ade\u4e89\u529b\u4e14\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u4e0e\u57fa\u4e8e\u6807\u7b7e\u7684\u589e\u5f3a\u63d0\u5347\u6ce8\u610f\u529b\u5b9a\u4f4d\u3002"}}
{"id": "2511.01613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01613", "abs": "https://arxiv.org/abs/2511.01613", "authors": ["Tom\u00e1\u0161 Krsi\u010dka", "Tibor Kub\u00edk"], "title": "Benchmark-Ready 3D Anatomical Shape Classification", "comment": "Shape in Medical Imaging, ShapeMI 2025, Held in Conjunction with\n  MICCAI 2025", "summary": "Progress in anatomical 3D shape classification is limited by the complexity\nof mesh data and the lack of standardized benchmarks, highlighting the need for\nrobust learning methods and reproducible evaluation. We introduce two key steps\ntoward clinically and benchmark-ready anatomical shape classification via\nself-supervised graph autoencoding. We propose Precomputed Structural Pooling\n(PSPooling), a non-learnable mesh pooling operator designed for efficient and\nstructure-preserving graph coarsening in 3D anatomical shape analysis.\nPSPooling precomputes node correspondence sets based on geometric proximity,\nenabling parallelizable and reversible pooling and unpooling operations with\nguaranteed support structure. This design avoids the sparsity and\nreconstruction issues of selection-based methods and the sequential overhead of\nedge contraction approaches, making it particularly suitable for\nhigh-resolution medical meshes. To demonstrate its effectiveness, we integrate\nPSPooling into a self-supervised graph autoencoder that learns anatomy-aware\nrepresentations from unlabeled surface meshes. We evaluate the downstream\nbenefits on MedShapeNet19, a new curated benchmark dataset we derive from\nMedShapeNet, consisting of 19 anatomical classes with standardized training,\nvalidation, and test splits. Experiments show that PSPooling significantly\nimproves reconstruction fidelity and classification accuracy in low-label\nregimes, establishing a strong baseline for medical 3D shape learning. We hope\nthat MedShapeNet19 will serve as a widely adopted benchmark for anatomical\nshape classification and further research in medical 3D shape analysis. Access\nthe complete codebase, model weights, and dataset information here:\nhttps://github.com/TomasKrsicka/MedShapeNet19-PSPooling.", "AI": {"tldr": "\u63d0\u51faPSPooling\u7528\u4e8e\u53ef\u5e76\u884c\u53ef\u9006\u7684\u7ed3\u6784\u4fdd\u7559\u7f51\u683c\u6c60\u5316\uff0c\u7ed3\u5408\u81ea\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u5728\u65b0\u5efa\u7684MedShapeNet19\u57fa\u51c6\u4e0a\uff0c\u5728\u4f4e\u6807\u7b7e\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u91cd\u6784\u4e0e\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89e3\u52563D\u5f62\u72b6\u5206\u7c7b\u53d7\u7f51\u683c\u6570\u636e\u590d\u6742\u6027\u548c\u7f3a\u4e4f\u6807\u51c6\u57fa\u51c6\u9650\u5236\uff0c\u9700\u8981\u7a33\u5065\u7684\u5b66\u4e60\u65b9\u6cd5\u548c\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faPSPooling\uff1a\u901a\u8fc7\u51e0\u4f55\u90bb\u8fd1\u9884\u8ba1\u7b97\u8282\u70b9\u5bf9\u5e94\u96c6\uff0c\u5b9e\u73b0\u7ed3\u6784\u4fdd\u7559\u7684\u5e76\u884c\u53ef\u9006\u6c60\u5316/\u53cd\u6c60\u5316\uff1b\u5176\u4e0e\u81ea\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\u96c6\u6210\uff0c\u5b66\u4e60\u65e0\u6807\u7b7e\u8868\u9762\u7f51\u683c\u7684\u89e3\u5256\u611f\u77e5\u8868\u793a\u3002\u6784\u5efa\u5e76\u4f7f\u7528MedShapeNet19\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728MedShapeNet19\u4e0a\uff0cPSPooling\u63d0\u5347\u4e86\u91cd\u6784\u8d28\u91cf\u548c\u5728\u5c11\u91cf\u6807\u6ce8\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u533b\u7597\u7f51\u683c\u4e0a\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4ee3\u7801\u3001\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76\u3002", "conclusion": "\u672c\u6587\u63d0\u51faPSPooling\uff0c\u4e00\u79cd\u9884\u8ba1\u7b97\u7684\u4e0d\u53ef\u5b66\u4e60\u7684\u7f51\u683c\u6c60\u5316\u7b97\u5b50\uff0c\u9002\u7528\u4e8e\u9ad8\u5206\u8fa8\u7387\u533b\u5b66\u7f51\u683c\u7684\u5e76\u884c\u3001\u53ef\u9006\u4e0b\u91c7\u6837\u4e0e\u4e0a\u91c7\u6837\uff0c\u907f\u514d\u4e86\u57fa\u4e8e\u9009\u62e9\u65b9\u6cd5\u7684\u7a00\u758f\u548c\u91cd\u6784\u95ee\u9898\u4ee5\u53ca\u57fa\u4e8e\u8fb9\u6536\u7f29\u65b9\u6cd5\u7684\u987a\u5e8f\u5f00\u9500\u3002\u7ed3\u5408\u81ea\u76d1\u7763\u56fe\u81ea\u7f16\u7801\u5668\uff0c\u5728MedShapeNet19\u57fa\u51c6\u4e0a\uff0c\u5728\u4f4e\u6807\u7b7e\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u91cd\u6784\u4fdd\u771f\u5ea6\u548c\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6784\u5efa\u4e86\u533b\u5b663D\u5f62\u72b6\u5b66\u4e60\u7684\u5f3a\u57fa\u7ebf\u3002"}}
{"id": "2511.01617", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.01617", "abs": "https://arxiv.org/abs/2511.01617", "authors": ["Mohamed Eltahir", "Ali Habibullah", "Lama Ayash", "Tanveer Hussain", "Naeemullah Khan"], "title": "Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers", "comment": null, "summary": "In the retrieval domain, candidates' fusion from heterogeneous retrievers is\na long-standing challenge, particularly for complex, multi-modal data such as\nvideos. While typical fusion techniques are training-free, they rely solely on\nrank or score signals, disregarding candidates' representations. This work\nintroduces Vote-in-Context (ViC), a generalized, training-free framework that\nre-thinks list-wise reranking and fusion as a zero-shot reasoning task for a\nVision-Language Model (VLM). The core insight is to serialize both content\nevidence and retriever metadata directly within the VLM's prompt, allowing the\nmodel to adaptively weigh retriever consensus against visual-linguistic\ncontent. We demonstrate the generality of this framework by applying it to the\nchallenging domain of cross-modal video retrieval. To this end, we introduce\nthe S-Grid, a compact serialization map that represents each video as an image\ngrid, optionally paired with subtitles to enable list-wise reasoning over video\ncandidates. ViC is evaluated both as a single-list reranker, where it\ndramatically improves the precision of individual retrievers, and as an\nensemble fuser, where it consistently outperforms strong baselines like\nCombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the\nframework establishes new state-of-the-art zero-shot retrieval performance,\ndemonstrating its effectiveness in handling complex visual and temporal signals\nalongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%\n(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive\ngains of up to +40 Recall@1 over previous state-of-the-art baselines. We\npresent ViC as a simple, reproducible, and highly effective recipe for turning\nmodern VLMs into powerful zero-shot rerankers and fusers. Code and resources\nare publicly available at: https://github.com/mohammad2012191/ViC", "AI": {"tldr": "\u628a\u5019\u9009\u5185\u5bb9\u4e0e\u68c0\u7d22\u5668\u5143\u4fe1\u606f\u4e00\u8d77\u5582\u7ed9VLM\u4f5c\u4e3a\u96f6\u6837\u672c\u63d0\u793a\uff08\u4f7f\u7528S-Grid\u8868\u793a\u89c6\u9891\uff09\uff0c\u5b9e\u73b0\u57fa\u4e8eVLM\u7684\u5217\u8868\u7ea7\u6295\u7968\u91cd\u6392\u4e0e\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65e0\u8bad\u7ec3\u878d\u5408\u65b9\u6cd5\u4ec5\u5229\u7528\u5206\u6570\u6216\u6392\u540d\uff0c\u5ffd\u7565\u4e86\u5019\u9009\u7684\u8868\u793a\u4e0e\u89c6\u89c9-\u8bed\u8a00\u8bc1\u636e\uff0c\u5bfc\u81f4\u590d\u6742\u591a\u6a21\u6001\uff08\u5c24\u5176\u89c6\u9891\uff09\u68c0\u7d22\u8868\u73b0\u53d7\u9650\uff1b\u5e0c\u671b\u5229\u7528\u5f3a\u5927\u7684VLM\u8fdb\u884c\u8bed\u5883\u5316\u5224\u65ad\u4ee5\u6539\u8fdb\u878d\u5408\u4e0e\u91cd\u6392\u3002", "method": "\u63d0\u51faVote-in-Context\u6846\u67b6\uff1a\u628a\u5019\u9009\u89c6\u9891\u7684\u89c6\u89c9\u5185\u5bb9\uff08S-Grid\u56fe\u50cf\u7f51\u683c\uff09\u4e0e\u53ef\u9009\u5b57\u5e55\u53ca\u68c0\u7d22\u5668\u7684\u6392\u540d/\u5206\u6570\u5143\u4fe1\u606f\u4e00\u5e76\u5e8f\u5217\u5316\u8fdbVLM\u63d0\u793a\uff0c\u4f5c\u4e3a\u5217\u8868\u5f0f\u8f93\u5165\u8fdb\u884c\u96f6\u6837\u672c\u6295\u7968\u5f0f\u91cd\u6392\u4e0e\u878d\u5408\uff1b\u9002\u914d\u5355\u5217rerank\u548c\u591a\u68c0\u7d22\u5668ensemble fuser\u4e24\u79cd\u573a\u666f\u3002", "result": "\u5728\u591a\u4e2a\u89c6\u9891\u68c0\u7d22\u57fa\u51c6\uff08\u5982MSR-VTT\u3001ActivityNet\u3001VATEX\uff09\u4e0a\u53d6\u5f97\u65b0\u7684\u96f6\u6837\u672cSOTA\uff1aMSR-VTT Recall@1 \u8fbe\u523087.1%\uff08\u6587\u672c\u5230\u89c6\u9891\uff09/89.0%\uff08\u89c6\u9891\u5230\u6587\u672c\uff09\uff0cVATEX v2t Recall@1 \u8fbe\u523099.6%\uff0c\u76f8\u8f83\u5148\u524d\u65b9\u6cd5\u6700\u9ad8\u63d0\u5347\u7ea6+40 Recall@1\uff0c\u5e76\u4f18\u4e8eCombSUM\u7b49\u5f3a\u57fa\u7ebf\u3002", "conclusion": "ViC\u5c06\u68c0\u7d22\u878d\u5408\u4e0e\u91cd\u6392\u95ee\u9898\u89c6\u4e3aVLM\u7684\u96f6\u6837\u672c\u63a8\u7406\uff0c\u901a\u8fc7\u5728\u63d0\u793a\u4e2d\u5e8f\u5217\u5316\u5019\u9009\u5185\u5bb9\u4e0e\u68c0\u7d22\u5668\u5143\u4fe1\u606f\uff0c\u4f7f\u6a21\u578b\u80fd\u81ea\u9002\u5e94\u5730\u5728\u89c6\u89c9-\u8bed\u8a00\u8bc1\u636e\u4e0e\u68c0\u7d22\u5668\u4e00\u81f4\u6027\u95f4\u6743\u8861\uff0c\u663e\u8457\u63d0\u5347\u89c6\u9891\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002"}}
{"id": "2511.01618", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.01618", "abs": "https://arxiv.org/abs/2511.01618", "authors": ["Xiaoyu Zhan", "Wenxuan Huang", "Hao Sun", "Xinyu Fu", "Changfeng Ma", "Shaosheng Cao", "Bohan Jia", "Shaohui Lin", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding.", "AI": {"tldr": "\u6784\u5efa100K\u89c6\u70b9\u914d\u5bf9\u6570\u636e\u96c6\u5e76\u91c7\u7528SFT+GRPO\u4e24\u9636\u6bb5\u5fae\u8c03\u4e0e\u6df7\u5408\u51b7\u542f\u52a8\uff0c\u663e\u8457\u63d0\u5347MLLM\u7684\u89c6\u70b9\u4e00\u81f4\u6027\u4e0e\u4e09\u7ef4\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u673a\u5668\u4eba\u4e0e3D\u573a\u666f\u7406\u89e3\u5e94\u7528\u3002", "motivation": "\u5f53\u524dMLLM\u5728\u4e8c\u7ef4\u89c6\u89c9\u7406\u89e3\u4e0a\u8fdb\u5c55\u663e\u8457\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u4e09\u7ef4\u7a7a\u95f4\u4fe1\u606f\u548c\u89c6\u89d2\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u4e0e\u63d0\u5347\uff0c\u5f71\u54cd\u5728\u771f\u5b9e\u4e16\u754c\u4e09\u7ef4\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5fae\u8c03\uff1a1) \u5728Viewpoint-100K\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6ce8\u5165\u57fa\u7840\u89c6\u70b9\u77e5\u8bc6\uff1b2) \u4f7f\u7528\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08GRPO\uff09\u5728\u66f4\u5e7f\u6cdb\u7684\u95ee\u9898\u96c6\u4e0a\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u6df7\u5408\u51b7\u542f\u52a8\u521d\u59cb\u5316\u540c\u65f6\u5b66\u4e60\u89c6\u70b9\u8868\u5f81\u548c\u4fdd\u6301\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u6fc0\u6d3bMLLM\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u9886\u57df\u5185\u4e0e\u9886\u57df\u5916\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u6784\u5efa\u57fa\u7840\u7a7a\u95f4\u6280\u80fd\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u901a\u8fc7\u4e13\u95e8\u6784\u5efa\u7684Viewpoint-100K\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u89c6\u70b9\u4e00\u81f4\u6027\u548c\u4e09\u7ef4\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2511.01645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01645", "abs": "https://arxiv.org/abs/2511.01645", "authors": ["Xiaogang Xu", "Ruihang Chu", "Jian Wang", "Kun Zhou", "Wenjie Shu", "Harry Yang", "Ser-Nam Lim", "Hao Chen", "Liang Lin"], "title": "Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward", "comment": null, "summary": "Reinforcement Learning (RL) has recently been incorporated into diffusion\nmodels, e.g., tasks such as text-to-image. However, directly applying existing\nRL methods to diffusion-based image restoration models is suboptimal, as the\nobjective of restoration fundamentally differs from that of pure generation: it\nplaces greater emphasis on fidelity. In this paper, we investigate how to\neffectively integrate RL into diffusion-based restoration models. First,\nthrough extensive experiments with various reward functions, we find that an\neffective reward can be derived from an Image Quality Assessment (IQA) model,\ninstead of intuitive ground-truth-based supervision, which has already been\noptimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,\nour strategy focuses on using RL for challenging samples that are significantly\ndistant from the ground truth, and our RL approach is innovatively implemented\nusing MLLM-based IQA models to align distributions with high-quality images\ninitially. As the samples approach the ground truth's distribution, RL is\nadaptively combined with SFT for more fine-grained alignment. This dynamic\nprocess is facilitated through an automatic weighting strategy that adjusts\nbased on the relative difficulty of the training samples. Our strategy is\nplug-and-play that can be seamlessly applied to diffusion-based restoration\nmodels, boosting its performance across various restoration tasks. Extensive\nexperiments across multiple benchmarks demonstrate the effectiveness of our\nproposed RL framework.", "AI": {"tldr": "\u7528IQA\u9a71\u52a8\u7684\u5956\u52b1\u548c\u57fa\u4e8e\u6837\u672c\u96be\u5ea6\u7684\u81ea\u9002\u5e94RL+SFT\u6df7\u5408\u7b56\u7565\uff0c\u5c06RL\u9ad8\u6548\u5e94\u7528\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\uff0c\u663e\u8457\u63d0\u5347\u6062\u590d\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u76f4\u63a5\u7528\u4e8e\u6269\u6563\u57fa\u6062\u590d\u6a21\u578b\u6548\u679c\u6b20\u4f73\uff0c\u56e0\u4e3a\u6062\u590d\u4efb\u52a1\u66f4\u6ce8\u91cd\u4fdd\u771f\u5ea6\uff0c\u9700\u8981\u4e0d\u540c\u7684\u5956\u52b1\u8bbe\u8ba1\u4e0e\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u591a\u79cd\u5956\u52b1\u51fd\u6570\uff0c\u53d1\u73b0IQA\u6a21\u578b\u63d0\u4f9b\u7684\u5956\u52b1\u4f18\u4e8e\u57fa\u4e8eGT\u7684\u76d1\u7763\uff1b\u91c7\u7528MLLM-based IQA\u5bf9\u9ad8\u8d28\u91cf\u56fe\u50cf\u5206\u5e03\u8fdb\u884c\u5bf9\u9f50\uff1b\u5bf9\u96be\u6837\u672c\u91cd\u70b9\u4f7f\u7528RL\uff0c\u5e76\u5728\u6837\u672c\u63a5\u8fd1GT\u5206\u5e03\u65f6\u81ea\u9002\u5e94\u5730\u5c06RL\u4e0eSFT\u7ed3\u5408\uff0c\u91c7\u7528\u81ea\u52a8\u6743\u91cd\u8c03\u6574\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u4fee\u590d\u4efb\u52a1\u548c\u57fa\u51c6\u4e0a\uff0c\u63d0\u51fa\u7684RL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u5728\u4e3b\u89c2\u8d28\u91cf\u548c\u96be\u6837\u672c\u6062\u590d\u4e0a\u6548\u679c\u660e\u663e\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u5c06RL\u6709\u6548\u6574\u5408\u5230\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u4fee\u590d\u4efb\u52a1\u4e2d\uff0c\u5f3a\u8c03\u57fa\u4e8eIQA\u6a21\u578b\u7684\u5956\u52b1\u51fd\u6570\u4e0e\u52a8\u6001\u6743\u91cd\u7b56\u7565\u80fd\u63d0\u5347\u6062\u590d\u4efb\u52a1\u7684\u4fdd\u771f\u5ea6\u4e0e\u4e3b\u89c2\u8d28\u91cf\u3002"}}
{"id": "2511.01678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01678", "abs": "https://arxiv.org/abs/2511.01678", "authors": ["Ropeway Liu", "Hangjie Yuan", "Bo Dong", "Jiazheng Xing", "Jinwang Wang", "Rui Zhao", "Yan Xing", "Weihua Chen", "Fan Wang"], "title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback", "comment": "NeurIPS 2025", "summary": "Relighting is a crucial task with both practical demand and artistic value,\nand recent diffusion models have shown strong potential by enabling rich and\ncontrollable lighting effects. However, as they are typically optimized in\nsemantic latent space, where proximity does not guarantee physical correctness\nin visual space, they often produce unrealistic results, such as overexposed\nhighlights, misaligned shadows, and incorrect occlusions. We address this with\nUniLumos, a unified relighting framework for both images and videos that brings\nRGB-space geometry feedback into a flow matching backbone. By supervising the\nmodel with depth and normal maps extracted from its outputs, we explicitly\nalign lighting effects with the scene structure, enhancing physical\nplausibility. Nevertheless, this feedback requires high-quality outputs for\nsupervision in visual space, making standard multi-step denoising\ncomputationally expensive. To mitigate this, we employ path consistency\nlearning, allowing supervision to remain effective even under few-step training\nregimes. To enable fine-grained relighting control and supervision, we design a\nstructured six-dimensional annotation protocol capturing core illumination\nattributes. Building upon this, we propose LumosBench, a disentangled\nattribute-level benchmark that evaluates lighting controllability via large\nvision-language models, enabling automatic and interpretable assessment of\nrelighting precision across individual dimensions. Extensive experiments\ndemonstrate that UniLumos achieves state-of-the-art relighting quality with\nsignificantly improved physical consistency, while delivering a 20x speedup for\nboth image and video relighting. Code is available at\nhttps://github.com/alibaba-damo-academy/Lumos-Custom.", "AI": {"tldr": "UniLumos\u5728\u6d41\u5339\u914d\u6846\u67b6\u4e2d\u5f15\u5165RGB\u7a7a\u95f4\u7684\u6df1\u5ea6\u4e0e\u6cd5\u7ebf\u53cd\u9988\u4e0e\u8def\u5f84\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u5e76\u914d\u5957\u516d\u7ef4\u6807\u6ce8\u4e0eLum osBench\uff0c\u5b9e\u73b0\u4e86\u66f4\u771f\u5b9e\u4e14\u5feb\u901f\u7684\u56fe\u50cf/\u89c6\u9891\u91cd\u5149\u7167\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u7684\u6269\u6563\u6a21\u578b\u5e38\u4ea7\u751f\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u5149\u7167\u4f2a\u5f71\uff08\u8fc7\u66dd\u9ad8\u5149\u3001\u9634\u5f71\u9519\u4f4d\u3001\u906e\u6321\u9519\u8bef\uff09\uff0c\u9700\u8981\u5728\u89c6\u89c9\u7a7a\u95f4\u7ed3\u5408\u51e0\u4f55\u4fe1\u606f\u4ee5\u63d0\u5347\u7269\u7406\u5408\u7406\u6027\u3002", "method": "\u5728\u6d41\u5339\u914d\uff08flow matching\uff09\u9aa8\u5e72\u4e0a\uff0c\u6a21\u578b\u8f93\u51fa\u7684\u6df1\u5ea6\u4e0e\u6cd5\u7ebf\u56fe\u88ab\u56de\u9988\u7528\u4e8e\u76d1\u7763\uff0c\u4f7f\u5f97\u5149\u7167\u6548\u679c\u4e0e\u573a\u666f\u7ed3\u6784\u663e\u5f0f\u5bf9\u9f50\uff1b\u540c\u65f6\u4f7f\u7528\u8def\u5f84\u4e00\u81f4\u6027\u5b66\u4e60\u5728\u5c11\u6b65\u8bad\u7ec3\u4e0b\u4ecd\u4fdd\u6301\u76d1\u7763\u6709\u6548\uff1b\u5e76\u8bbe\u8ba1\u4e86\u516d\u7ef4\u7167\u660e\u5c5e\u6027\u6807\u6ce8\u534f\u8bae\u548c\u57fa\u4e8e\u5927\u6a21\u578b\u7684Lum osBench\u8bc4\u4f30\u57fa\u51c6\u3002", "result": "\u5728\u56fe\u50cf\u4e0e\u89c6\u9891\u91cd\u5149\u7167\u4efb\u52a1\u4e0a\uff0cUniLumos\u663e\u8457\u63d0\u5347\u7269\u7406\u4e00\u81f4\u6027\u5e76\u5b9e\u73b0\u7ea620\u500d\u52a0\u901f\uff0c\u540c\u65f6\u5728Lum osBench\u4e0a\u5c55\u73b0\u66f4\u7ec6\u7c92\u5ea6\u53ef\u63a7\u7684\u5149\u7167\u8c03\u8282\u80fd\u529b\u3002", "conclusion": "UniLumos\u901a\u8fc7\u5728RGB\u7a7a\u95f4\u5f15\u5165\u51e0\u4f55\u53cd\u9988\u5e76\u7ed3\u5408\u8def\u5f84\u4e00\u81f4\u6027\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u5728\u56fe\u50cf\u548c\u89c6\u9891\u91cd\u5149\u7167\u4efb\u52a1\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u63d0\u5347\u548c\u901f\u5ea6\u52a0\u901f\uff0c\u8fbe\u5230\u4e86SOTA\u8d28\u91cf\u3002"}}
{"id": "2511.01698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01698", "abs": "https://arxiv.org/abs/2511.01698", "authors": ["Yuhang Kang", "Ziyu Su", "Tianyang Wang", "Zaibo Li", "Wei Chen", "Muhammad Khalid Khan Niazi"], "title": "Progressive Translation of H&E to IHC with Enhanced Structural Fidelity", "comment": null, "summary": "Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not\nonly maintains the structural features of tissue samples, but also provides\nhigh-resolution protein localization, which is essential for aiding in\npathology diagnosis. Despite its diagnostic value, IHC remains a costly and\nlabor-intensive technique. Its limited scalability and constraints in\nmultiplexing further hinder widespread adoption, especially in resource-limited\nsettings. Consequently, researchers are increasingly exploring computational\nstain translation techniques to synthesize IHC-equivalent images from\nH&E-stained slides, aiming to extract protein-level information more\nefficiently and cost-effectively. However, most existing stain translation\ntechniques rely on a linearly weighted summation of multiple loss terms within\na single objective function, strategy that often overlooks the interdepedence\namong these components-resulting in suboptimal image quality and an inability\nto simultaneously preserve structural authenticity and color fidelity. To\naddress this limitation, we propose a novel network architecture that follows a\nprogressive structure, incorporating color and cell border generation logic,\nwhich enables each visual aspect to be optimized in a stage-wise and decoupled\nmanner. To validate the effectiveness of our proposed network architecture, we\nbuild upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We\nintroduce additional loss functions based on 3,3'-diaminobenzidine (DAB)\nchromogen concentration and image gradient, enhancing color fidelity and cell\nboundary clarity in the generated IHC images. By reconstructing the generation\npipeline using our structure-color-cell boundary progressive mechanism,\nexperiments on HER2 and ER datasets demonstrated that the model significantly\nimproved visual quality and achieved finer structural details.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u6784-\u989c\u8272-\u7ec6\u80de\u8fb9\u754c\u6e10\u8fdb\u751f\u6210\u7f51\u7edc\uff0c\u7ed3\u5408DAB\u6d53\u5ea6\u548c\u68af\u5ea6\u635f\u5931\uff0c\u663e\u8457\u6539\u5584H&E\u5230IHC\u7684\u67d3\u8272\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "IHC\u6210\u672c\u9ad8\u4e14\u52b3\u52a8\u5f3a\u5ea6\u5927\uff0c\u73b0\u6709\u8ba1\u7b97\u67d3\u8272\u65b9\u6cd5\u901a\u5e38\u628a\u591a\u79cd\u635f\u5931\u7ebf\u6027\u52a0\u6743\uff0c\u5ffd\u7565\u635f\u5931\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\uff0c\u5bfc\u81f4\u7ed3\u6784\u548c\u989c\u8272\u4e0d\u80fd\u517c\u987e\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316\u5404\u89c6\u89c9\u6210\u5206\u6765\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u771f\u5b9e\u6027\u548c\u989c\u8272\u4fdd\u771f\u6027\u3002", "method": "\u57fa\u4e8eASP\u6846\u67b6\u4f5c\u4e3a\u57fa\u7ebf\uff0c\u5f15\u5165\u7ed3\u6784-\u989c\u8272-\u7ec6\u80de\u8fb9\u754c\u7684\u6e10\u8fdb\u5f0f\u751f\u6210\u673a\u5236\uff1b\u65b0\u589e\u57fa\u4e8eDAB\u6d53\u5ea6\u548c\u56fe\u50cf\u68af\u5ea6\u7684\u635f\u5931\u9879\u4ee5\u589e\u5f3a\u989c\u8272\u4fdd\u771f\u5ea6\u548c\u8fb9\u754c\u6e05\u6670\u5ea6\u3002", "result": "\u5728HER2\u548cER\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u89c6\u89c9\u8d28\u91cf\u548c\u7ed3\u6784\u7ec6\u8282\u4e0a\u6709\u660e\u663e\u63d0\u5347\uff0c\u751f\u6210\u7684IHC\u56fe\u50cf\u5728\u989c\u8272\u548c\u7ec6\u80de\u8fb9\u754c\u4e0a\u66f4\u7cbe\u7ec6\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u7f51\u7edc\u7ed3\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u7ed3\u6784\u3001\u989c\u8272\u548c\u7ec6\u80de\u8fb9\u754c\u7684\u751f\u6210\u903b\u8f91\u6765\u63d0\u5347\u4eceH&E\u5230IHC\u7684\u67d3\u8272\u7ffb\u8bd1\u6548\u679c\u3002"}}
{"id": "2511.01704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01704", "abs": "https://arxiv.org/abs/2511.01704", "authors": ["Xin Qiao", "Matteo Poggi", "Xing Wei", "Pengchao Deng", "Yanhui Zhou", "Stefano Mattoccia"], "title": "Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond", "comment": null, "summary": "Under-display ToF imaging aims to achieve accurate depth sensing through a\nToF camera placed beneath a screen panel. However, transparent OLED (TOLED)\nlayers introduce severe degradations-such as signal attenuation, multi-path\ninterference (MPI), and temporal noise-that significantly compromise depth\nquality. To alleviate this drawback, we propose Learnable Fractional\nReaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the\nexpressive power of neural networks with the interpretability of physical\nmodeling. Specifically, we implement a time-fractional reaction-diffusion\nmodule that enables iterative depth refinement with dynamically generated\ndifferential orders, capturing long-term dependencies. In addition, we\nintroduce an efficient continuous convolution operator via coefficient\nprediction and repeated differentiation to further improve restoration quality.\nExperiments on four benchmark datasets demonstrate the effectiveness of our\napproach. The code is publicly available at https://github.com/wudiqx106/LFRD2.", "AI": {"tldr": "\u63d0\u51faLFRD2\uff1a\u7ed3\u5408\u53ef\u5b66\u4e60\u65f6\u95f4\u5206\u6570\u9636\u53cd\u5e94-\u6269\u6563\u548c\u9ad8\u6548\u8fde\u7eed\u5377\u79ef\u7684\u6df7\u5408\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347TOLED\u4e0bToF\u6df1\u5ea6\u6210\u50cf\u8d28\u91cf\u3002", "motivation": "\u76ee\u6807\u662f\u5728\u5c4f\u5e55\u4e0b\u7684ToF\u76f8\u673a\uff08\u5c24\u5176TOLED\u9762\u677f\uff09\u4e2d\u6062\u590d\u51c6\u786e\u6df1\u5ea6\u5730\u56fe\uff0c\u56e0TOLED\u5f15\u5165\u4fe1\u53f7\u8870\u51cf\u3001MPI\u548c\u65f6\u5e8f\u566a\u58f0\u7b49\u4e25\u91cd\u9000\u5316\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5efa\u6a21\u590d\u6742\u65f6\u7a7a\u975e\u5c40\u90e8\u6548\u5e94\u4e0e\u7269\u7406\u9000\u5316\uff0c\u6545\u63d0\u51fa\u7ed3\u5408\u53ef\u5b66\u4e60\u6a21\u578b\u4e0e\u7269\u7406\u53ef\u89e3\u91ca\u6027\u7684\u5206\u6570\u9636\u52a8\u529b\u5b66\u65b9\u6cd5\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u8bbe\u8ba1\u65f6\u95f4\u5206\u6570\u9636\u53cd\u5e94-\u6269\u6563\u6a21\u5757\uff0c\u5b9e\u73b0\u5e26\u52a8\u6001\u751f\u6210\u7684\u5206\u6570\u9636\uff08\u53ef\u5b66\u4e60\u9636\u6570\uff09\u7684\u8fed\u4ee3\u6df1\u5ea6\u7cbe\u5316\uff0c\u4ee5\u6355\u6349\u957f\u65f6\u4f9d\u8d56\uff1b2) \u63d0\u51fa\u57fa\u4e8e\u7cfb\u6570\u9884\u6d4b\u548c\u91cd\u590d\u5fae\u5206\u7684\u9ad8\u6548\u8fde\u7eed\u5377\u79ef\u7b97\u5b50\uff0c\u7528\u4e8e\u589e\u5f3a\u6062\u590d\u80fd\u529b\uff1b3) \u5c06\u53ef\u5b66\u4e60\u795e\u7ecf\u7f51\u7edc\u4e0e\u7269\u7406\u5efa\u6a21\u76f8\u7ed3\u5408\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u900f\u660eOLED\u5f15\u8d77\u7684\u9000\u5316\u6062\u590d\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLFRD2\u5728\u6df1\u5ea6\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6291\u5236MPI\u548c\u566a\u58f0\u5e76\u6062\u590d\u7ec6\u8282\u3002\u4ee3\u7801\u5df2\u516c\u5f00\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u7684\u53ef\u5b66\u4e60\u5206\u6570\u9636\u53cd\u5e94-\u6269\u6563\u52a8\u529b\u5b66(LFRD2)\u6846\u67b6\uff0c\u7528\u4e8e\u6539\u5584\u900f\u660eOLED\u4e0b\u7684ToF\u6df1\u5ea6\u6210\u50cf\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u5206\u6570\u9636\u53cd\u5e94-\u6269\u6563\u6a21\u5757\u548c\u9ad8\u6548\u8fde\u7eed\u5377\u79ef\u7b97\u5b50\uff0c\u6709\u6548\u7f13\u89e3\u4fe1\u53f7\u8870\u51cf\u3001\u591a\u8def\u5f84\u5e72\u6270\u548c\u65f6\u5e8f\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u6df1\u5ea6\u6062\u590d\u8d28\u91cf\u3002"}}
{"id": "2511.01724", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.01724", "abs": "https://arxiv.org/abs/2511.01724", "authors": ["Yi Zhang", "Zheng Wang", "Chen Zhen", "Wenjie Ruan", "Qing Guo", "Siddartha Khastgir", "Carsten Maple", "Xingyu Zhao"], "title": "Probabilistic Robustness for Free? Revisiting Training via a Benchmark", "comment": null, "summary": "Deep learning models are notoriously vulnerable to imperceptible\nperturbations. Most existing research centers on adversarial robustness (AR),\nwhich evaluates models under worst-case scenarios by examining the existence of\ndeterministic adversarial examples (AEs). In contrast, probabilistic robustness\n(PR) adopts a statistical perspective, measuring the probability that\npredictions remain correct under stochastic perturbations. While PR is widely\nregarded as a practical complement to AR, dedicated training methods for\nimproving PR are still relatively underexplored, albeit with emerging progress.\nAmong the few PR-targeted training methods, we identify three limitations: i\nnon-comparable evaluation protocols; ii limited comparisons to strong AT\nbaselines despite anecdotal PR gains from AT; and iii no unified framework to\ncompare the generalization of these methods. Thus, we introduce PRBench, the\nfirst benchmark dedicated to evaluating improvements in PR achieved by\ndifferent robustness training methods. PRBench empirically compares most common\nAT and PR-targeted training methods using a comprehensive set of metrics,\nincluding clean accuracy, PR and AR performance, training efficiency, and\ngeneralization error (GE). We also provide theoretical analysis on the GE of PR\nperformance across different training methods. Main findings revealed by\nPRBench include: AT methods are more versatile than PR-targeted training\nmethods in terms of improving both AR and PR performance across diverse\nhyperparameter settings, while PR-targeted training methods consistently yield\nlower GE and higher clean accuracy. A leaderboard comprising 222 trained models\nacross 7 datasets and 10 model architectures is publicly available at\nhttps://tmpspace.github.io/PRBenchLeaderboard/.", "AI": {"tldr": "\u63d0\u51faPRBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u6bd4\u8f83AT\u4e0ePR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u53d1\u73b0AT\u66f4\u901a\u7528\u63d0\u5347AR/PR\uff0c\u800cPR\u8bad\u7ec3\u5728\u6cdb\u5316\u4e0e\u6d01\u51c0\u7cbe\u5ea6\u4e0a\u6709\u4f18\u52bf\uff1b\u5e76\u516c\u5f00\u4e86222\u6a21\u578b\u6392\u884c\u699c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6700\u574f\u60c5\u51b5\u7684\u5bf9\u6297\u9c81\u68d2\u6027(AR)\uff0c\u800cPR\u4f5c\u4e3a\u7edf\u8ba1\u89c6\u89d2\u7684\u4e92\u8865\u6307\u6807\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6bd4\u4e0e\u8bc4\u4f30\u57fa\u51c6\uff1b\u73b0\u6709PR\u8bad\u7ec3\u65b9\u6cd5\u5728\u8bc4\u4f30\u534f\u8bae\u3001\u4e0d\u5145\u5206\u4e0e\u5f3aAT\u6bd4\u8f83\u3001\u65e0\u7edf\u4e00\u6cdb\u5316\u6846\u67b6\u7b49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "PRBench\u6536\u96c6\u5e76\u7edf\u4e00\u8bc4\u4f30\u591a\u79cd\u5bf9\u6297\u8bad\u7ec3(AT)\u4e0ePR\u76ee\u6807\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e00\u81f4\u7684\u8bc4\u4f30\u534f\u8bae\u4e0e\u6307\u6807\uff08\u6d01\u51c0\u7cbe\u5ea6\u3001PR\u3001AR\u3001\u8bad\u7ec3\u6548\u7387\u3001\u6cdb\u5316\u8bef\u5dee\uff09\uff0c\u5e76\u8fdb\u884c\u7406\u8bbaGE\u5206\u6790\uff0c\u540c\u65f6\u53d1\u5e03\u5305\u542b222\u4e2a\u8bad\u7ec3\u6a21\u578b\u7684\u6392\u884c\u699c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1aAT\u65b9\u6cd5\u5728\u5e7f\u6cdb\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u66f4\u80fd\u540c\u65f6\u63d0\u5347AR\u4e0ePR\uff1bPR\u76ee\u6807\u8bad\u7ec3\u65b9\u6cd5\u5728\u6cdb\u5316\u8bef\u5dee\u66f4\u4f4e\u4e14\u6d01\u51c0\u7cbe\u5ea6\u66f4\u9ad8\uff1bPRBench\u516c\u5f00\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u6392\u884c\u699c\u7528\u4e8e\u540e\u7eed\u6bd4\u8f83\u3002", "conclusion": "PRBench\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u8bc4\u4f30\u6982\u7387\u9c81\u68d2\u6027(PR)\u8bad\u7ec3\u65b9\u6cd5\u7684\u57fa\u51c6\uff0c\u5e76\u63ed\u793a\u5bf9\u6bd4\u5b9e\u9a8c\u4e0e\u7406\u8bba\u5206\u6790\u7684\u4e3b\u8981\u7ed3\u8bba\u3002"}}
{"id": "2511.01728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01728", "abs": "https://arxiv.org/abs/2511.01728", "authors": ["Tom Odem"], "title": "Toward Strategy Identification and Subtask Decomposition In Task Exploration", "comment": null, "summary": "This research builds on work in anticipatory human-machine interaction, a\nsubfield of human-machine interaction where machines can facilitate\nadvantageous interactions by anticipating a user's future state. The aim of\nthis research is to further a machine's understanding of user knowledge, skill,\nand behavior in pursuit of implicit coordination. A task explorer pipeline was\ndeveloped that uses clustering techniques, paired with factor analysis and\nstring edit distance, to automatically identify key global and local strategies\nthat are used to complete tasks. Global strategies identify generalized sets of\nactions used to complete tasks, while local strategies identify sequences that\nused those sets of actions in a similar composition. Additionally, meaningful\nsubtasks of various lengths are identified within the tasks. The task explorer\npipeline was able to automatically identify key strategies used to complete\ntasks and encode user runs with hierarchical subtask structures. In addition, a\nTask Explorer application was developed to easily review pipeline results. The\ntask explorer pipeline can be easily modified to any action-based time-series\ndata and the identified strategies and subtasks help to inform humans and\nmachines on user knowledge, skill, and behavior.", "AI": {"tldr": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4ee5\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u7f16\u8f91\u8ddd\u79bb\u4e3a\u6838\u5fc3\u7684\u4efb\u52a1\u63a2\u7d22\u6d41\u6c34\u7ebf\uff0c\u81ea\u52a8\u53d1\u73b0\u4efb\u52a1\u5b8c\u6210\u7b56\u7565\u4e0e\u5b50\u4efb\u52a1\u7ed3\u6784\uff0c\u8f85\u52a9\u7406\u89e3\u4e0e\u9884\u6d4b\u7528\u6237\u77e5\u8bc6\u3001\u6280\u80fd\u4e0e\u884c\u4e3a\u3002", "motivation": "\u5728\u5177\u5907\u9884\u671f\u6027\u7684\u4eba\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u673a\u5668\u9700\u8981\u63d0\u524d\u7406\u89e3\u7528\u6237\u7684\u77e5\u8bc6\u4e0e\u884c\u4e3a\u4ee5\u5b9e\u73b0\u9690\u5f0f\u914d\u5408\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u65b9\u6cd5\u4ece\u52a8\u4f5c\u65f6\u95f4\u5e8f\u5217\u4e2d\u63d0\u53d6\u5173\u952e\u7b56\u7565\u4e0e\u5b50\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4efb\u52a1\u63a2\u7d22\u6d41\u6c34\u7ebf\uff1a\u5bf9\u52a8\u4f5c\u5e8f\u5217\u8fdb\u884c\u56e0\u5b50\u5206\u6790\u4e0e\u964d\u7ef4\uff0c\u4f7f\u7528\u805a\u7c7b\u53d1\u73b0\u5168\u5c40\u7b56\u7565\uff1b\u57fa\u4e8e\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\u5bf9\u5e8f\u5217\u8fdb\u884c\u76f8\u4f3c\u6027\u5ea6\u91cf\u4ee5\u8bc6\u522b\u5c40\u90e8\u7b56\u7565\uff1b\u7ed3\u5408\u5206\u5c42\u7ed3\u6784\u7f16\u7801\u7528\u6237\u8fd0\u884c\u5e76\u63d0\u53d6\u4e0d\u540c\u957f\u5ea6\u7684\u5b50\u4efb\u52a1\uff1b\u5f00\u53d1\u4e86\u53ef\u89c6\u5316\u7684Task Explorer\u5e94\u7528\u4ee5\u4fbf\u5ba1\u67e5\u7ed3\u679c\u3002", "result": "\u6d41\u6c34\u7ebf\u6210\u529f\u81ea\u52a8\u8bc6\u522b\u4e86\u5173\u952e\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\uff0c\u80fd\u591f\u5c06\u7528\u6237\u8fd0\u884c\u7f16\u7801\u4e3a\u5206\u5c42\u5b50\u4efb\u52a1\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7Task Explorer\u5e94\u7528\u652f\u6301\u7ed3\u679c\u5ba1\u67e5\uff1b\u65b9\u6cd5\u53ef\u63a8\u5e7f\u5230\u4efb\u610f\u57fa\u4e8e\u52a8\u4f5c\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u4efb\u52a1\u63a2\u7d22\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u805a\u7c7b\u3001\u56e0\u5b50\u5206\u6790\u548c\u5b57\u7b26\u4e32\u7f16\u8f91\u8ddd\u79bb\u7b49\u65b9\u6cd5\uff0c\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u5b8c\u6210\u4efb\u52a1\u65f6\u7684\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\uff0c\u5e76\u5206\u5272\u6709\u610f\u4e49\u7684\u5b50\u4efb\u52a1\uff0c\u4ece\u800c\u4e3a\u7406\u89e3\u7528\u6237\u7684\u77e5\u8bc6\u3001\u6280\u80fd\u4e0e\u884c\u4e3a\u63d0\u4f9b\u91cf\u5316\u652f\u6301\u3002"}}
{"id": "2511.01730", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01730", "abs": "https://arxiv.org/abs/2511.01730", "authors": ["Yefeng Wu", "Yucheng Song", "Ling Wu", "Shan Wan", "Yecheng Zhao"], "title": "CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays", "comment": null, "summary": "Pneumonia remains a leading cause of morbidity and mortality worldwide,\nnecessitating accurate and efficient automated detection systems. While recent\ntransformer-based detectors like RT-DETR have shown promise in object detection\ntasks, their application to medical imaging, particularly pneumonia detection\nin chest X-rays, remains underexplored. This paper presents CGF-DETR, an\nenhanced real-time detection transformer specifically designed for pneumonia\ndetection. We introduce XFABlock in the backbone to improve multi-scale feature\nextraction through convolutional attention mechanisms integrated with CSP\narchitecture. To achieve efficient feature aggregation, we propose SPGA module\nthat replaces standard multi-head attention with dynamic gating mechanisms and\nsingle-head self-attention. Additionally, GCFC3 is designed for the neck to\nenhance feature representation through multi-path convolution fusion while\nmaintaining real-time performance via structural re-parameterization. Extensive\nexperiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR\nachieves 82.2\\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\\% while\nmaintaining comparable inference speed at 48.1 FPS. Our ablation studies\nconfirm that each proposed module contributes meaningfully to the overall\nperformance improvement, with the complete model achieving 50.4\\%\nmAP@[0.5:0.95]", "AI": {"tldr": "\u63d0\u51fa\u9762\u5411\u80f8\u7247\u80ba\u708e\u68c0\u6d4b\u7684CGF-DETR\uff0c\u901a\u8fc7XFABlock\u3001SPGA\u548cGCFC3\u4e09\u6a21\u5757\u6539\u9020RT-DETR\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347mAP\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "\u5c3d\u7ba1RT-DETR\u7b49\u53d8\u6362\u5668\u68c0\u6d4b\u5668\u5728\u76ee\u6807\u68c0\u6d4b\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u533b\u5b66\u5f71\u50cf\u5c24\u5176\u662f\u80f8\u7247\u80ba\u708e\u68c0\u6d4b\u4e0a\u7684\u5e94\u7528\u5c1a\u5c11\uff0c\u4e14\u9700\u517c\u987e\u7cbe\u5ea6\u4e0e\u5b9e\u65f6\u6027\uff0c\u6545\u63d0\u51fa\u9488\u5bf9\u6027\u6539\u8fdb\u4ee5\u63d0\u5347\u5728\u533b\u7597\u573a\u666f\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u5728RT-DETR\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e09\u5927\u6539\u8fdb\u6a21\u5757\uff1abackbone\u4e2d\u5f15\u5165XFABlock\uff08\u7ed3\u5408\u5377\u79ef\u6ce8\u610f\u529b\u4e0eCSP\u7ed3\u6784\u4ee5\u5f3a\u5316\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\uff09\uff1b\u66ff\u6362\u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\u7684SPGA\u6a21\u5757\uff08\u4f7f\u7528\u52a8\u6001\u95e8\u63a7\u673a\u5236\u4e0e\u5355\u5934\u81ea\u6ce8\u610f\u529b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u805a\u5408\uff09\uff1bneck\u4e2d\u8bbe\u8ba1GCFC3\uff08\u591a\u8def\u5f84\u5377\u79ef\u878d\u5408\u5e76\u901a\u8fc7\u7ed3\u6784\u91cd\u53c2\u6570\u5316\u4fdd\u6301\u5b9e\u65f6\u6027\uff09\u3002", "result": "\u5728RSNA\u6570\u636e\u96c6\u4e0a\uff0cCGF-DETR\u8fbe\u523082.2% mAP@0.5\uff0c\u8f83RT-DETR-l\u63d0\u53473.7%\uff0c\u63a8\u7406\u901f\u5ea6\u4fdd\u630148.1 FPS\uff1b\u5b8c\u6574\u6a21\u578bmAP@[0.5:0.95]\u4e3a50.4%\uff1b\u6d88\u878d\u5b9e\u9a8c\u8868\u660e\u6bcf\u4e2a\u6a21\u5757\u5747\u5bf9\u6027\u80fd\u6709\u8d21\u732e\u3002", "conclusion": "CGF-DETR\u5728RSNA Pneumonia Detection\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u901f\u5ea6\uff0c\u8bc1\u660e\u6240\u63d0\u6a21\u5757\u5728\u80f8\u7247\u80ba\u708e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u6709\u6548\u3002"}}
{"id": "2511.01755", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.01755", "abs": "https://arxiv.org/abs/2511.01755", "authors": ["Rong Li", "Yuhao Dong", "Tianshuai Hu", "Ao Liang", "Youquan Liu", "Dongyue Lu", "Liang Pan", "Lingdong Kong", "Junwei Liang", "Ziwei Liu"], "title": "3EED: Ground Everything Everywhere in 3D", "comment": "NeurIPS 2025 DB Track; 29 pages, 17 figures, 10 tables; Project Page\n  at https://project-3eed.github.io/", "summary": "Visual grounding in 3D is the key for embodied agents to localize\nlanguage-referred objects in open-world environments. However, existing\nbenchmarks are limited to indoor focus, single-platform constraints, and small\nscale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark\nfeaturing RGB and LiDAR data from vehicle, drone, and quadruped platforms. We\nprovide over 128,000 objects and 22,000 validated referring expressions across\ndiverse outdoor scenes -- 10x larger than existing datasets. We develop a\nscalable annotation pipeline combining vision-language model prompting with\nhuman verification to ensure high-quality spatial grounding. To support\ncross-platform learning, we propose platform-aware normalization and\ncross-modal alignment techniques, and establish benchmark protocols for\nin-domain and cross-platform evaluations. Our findings reveal significant\nperformance gaps, highlighting the challenges and opportunities of\ngeneralizable 3D grounding. The 3EED dataset and benchmark toolkit are released\nto advance future research in language-driven 3D embodied perception.", "AI": {"tldr": "3EED\u662f\u4e00\u4e2a\u8de8\u5e73\u53f0\u3001\u591a\u6a21\u6001\u3001\u8d85\u5927\u89c4\u6a21\u7684\u6237\u59163D\u89c6\u89c9\u6307\u79f0\u6570\u636e\u96c6\u4e0e\u57fa\u51c6\uff0c\u4f7f\u7528VLM\u63d0\u793a+\u4eba\u5de5\u9a8c\u8bc1\u6784\u5efa\u6807\u6ce8\uff0c\u5e76\u63d0\u51fa\u5e73\u53f0\u5f52\u4e00\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u57fa\u51c6\u7ed3\u679c\u8868\u660e\u8de8\u5e73\u53f0\u6cdb\u5316\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u73b0\u67093D\u53ef\u89c6\u5316\u5b9a\u4f4d\u57fa\u51c6\u591a\u4e3a\u5ba4\u5185\u3001\u5c0f\u89c4\u6a21\u3001\u5355\u4e00\u5e73\u53f0\uff0c\u96be\u4ee5\u6ee1\u8db3\u6237\u5916\u5f00\u653e\u4e16\u754c\u4e2d\u5177\u8eab\u667a\u80fd\u4f53\u7684\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5927\u89c4\u6a21\u3001\u591a\u5e73\u53f0\u3001\u591a\u6a21\u6001\u7684\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e09\u4e2a\u5e73\u53f0\u7684RGB\u548cLiDAR\u6570\u636e\u96c6\uff1b\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5019\u9009\u63cf\u8ff0\u5e76\u7ed3\u5408\u4eba\u5de5\u9a8c\u8bc1\u5f62\u6210\u9ad8\u8d28\u91cf\u6807\u6ce8\uff1b\u63d0\u51fa\u4e86\u5e73\u53f0\u611f\u77e5\u5f52\u4e00\u5316\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\u4ee5\u652f\u6301\u8de8\u5e73\u53f0\u5b66\u4e60\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57df\u5185\u4e0e\u8de8\u5e73\u53f0\u8bc4\u6d4b\u534f\u8bae\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b128k\u5bf9\u8c61\u548c22k\u7ecf\u9a8c\u8bc1\u7684\u6307\u79f0\u77ed\u8bed\u7684\u6570\u636e\u96c6\uff0c\u89c4\u6a21\u7ea6\u4e3a\u73b0\u6709\u6570\u636e\u96c6\u768410\u500d\uff1b\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u65b9\u6cd5\u5728\u8de8\u5e73\u53f0\u548c\u8de8\u573a\u666f\u6cdb\u5316\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u8de8\u5e73\u53f0\u76843D\u89c6\u89c9\u5b9a\u4f4d\u57fa\u51c63EED\uff0c\u6269\u5c55\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u573a\u666f\u3001\u5e73\u53f0\u548c\u89c4\u6a21\u4e0a\u7684\u8986\u76d6\uff0c\u80fd\u591f\u63a8\u52a8\u6237\u5916\u3001\u8de8\u5e73\u53f0\u7684\u8bed\u8a00\u9a71\u52a83D\u611f\u77e5\u7814\u7a76\u3002"}}
{"id": "2511.01756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01756", "abs": "https://arxiv.org/abs/2511.01756", "authors": ["Kai Zhai", "Ziyan Huang", "Qiang Nie", "Xiang Li", "Bo Ouyang"], "title": "HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain", "comment": null, "summary": "2D-to-3D human pose lifting is a fundamental challenge for 3D human pose\nestimation in monocular video, where graph convolutional networks (GCNs) and\nattention mechanisms have proven to be inherently suitable for encoding the\nspatial-temporal correlations of skeletal joints. However, depth ambiguity and\nerrors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous\nstudies have attempted to restrict jitters in the time domain, for instance, by\nconstraining the differences between adjacent frames while neglecting the\nglobal spatial-temporal correlations of skeletal joint motion. To tackle this\nproblem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid\nfeature aggregation and 3D trajectory consistency in the frequency domain.\nSpecifically, we propose a hop-hybrid graph attention (HGA) module and a\nTransformer encoder to model global joint spatial-temporal correlations. The\nHGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group\nto enlarge the receptive field and applies the attention mechanism to discover\nthe latent correlations of these groups globally. We then exploit global\ntemporal correlations by constraining trajectory consistency in the frequency\ndomain. To provide 3D information for depth inference across frames and\nmaintain coherence over time, a preliminary network is applied to estimate the\n3D pose. Extensive experiments were conducted on two standard benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed\nHGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional\naccuracy and temporal consistency.", "AI": {"tldr": "\u63d0\u51faHGFreNet\uff1a\u901a\u8fc7HGA\u6a21\u5757\u6269\u5c55\u611f\u53d7\u91ce\u5e76\u7528Transformer\u5efa\u6a21\u5168\u5c40\u65f6\u7a7a\u5173\u7cfb\uff0c\u7ed3\u5408\u9891\u57df\u76843D\u8f68\u8ff9\u4e00\u81f4\u6027\u7ea6\u675f\u4e0e\u521d\u6b653D\u4f30\u8ba1\u7f51\u7edc\uff0c\u6709\u6548\u63d0\u5347\u5355\u76ee\u89c6\u9891\u76843D\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u4e0e\u65f6\u5e8f\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4ec5\u5728\u65f6\u95f4\u57df\u9650\u5236\u76f8\u90bb\u5e27\u5dee\u5206\u4ee5\u6291\u5236\u6296\u52a8\uff0c\u4f46\u5ffd\u7565\u4e86\u9aa8\u9abc\u5173\u8282\u8fd0\u52a8\u7684\u5168\u5c40\u65f6\u7a7a\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u6df1\u5ea6\u6b67\u4e49\u548c2D\u4f30\u8ba1\u8bef\u5dee\u5f15\u53d1\u76843D\u8f68\u8ff9\u4e0d\u8fde\u8d2f\u3002", "method": "\u8bbe\u8ba1\u4e86HGA\u6a21\u5757\uff0c\u5c06\u6bcf\u4e2a\u5173\u8282\u70b9\u7684k\u8df3\u90bb\u5c45\u5206\u7ec4\u4ee5\u6269\u5c55\u611f\u53d7\u91ce\uff0c\u5e76\u5bf9\u8fd9\u4e9b\u7ec4\u65bd\u52a0\u5168\u5c40\u6ce8\u610f\u529b\u4ee5\u53d1\u73b0\u6f5c\u5728\u76f8\u5173\u6027\uff1b\u5f15\u5165Transformer\u7f16\u7801\u5668\u5efa\u6a21\u5168\u5c40\u65f6\u7a7a\u76f8\u5173\u6027\uff1b\u5728\u9891\u57df\u5bf93D\u8f68\u8ff9\u4e00\u81f4\u6027\u8fdb\u884c\u7ea6\u675f\u4ee5\u51cf\u5c11\u6df1\u5ea6\u6a21\u7cca\u548c2D\u4f30\u8ba1\u8bef\u5dee\u5f15\u8d77\u7684\u6296\u52a8\uff1b\u4f7f\u7528\u521d\u6b65\u7f51\u7edc\u63d0\u4f9b\u8de8\u5e27\u6df1\u5ea6\u4fe1\u606f\u4ee5\u7ef4\u62a4\u65f6\u95f4\u8fde\u8d2f\u6027\u3002", "result": "\u5728Human3.6M\u4e0eMPI-INF-3DHP\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHGFreNet\u5728\u4f4d\u7f6e\u7cbe\u5ea6\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51faHGFreNet\uff0c\u901a\u8fc7\u7ed3\u5408Hop-Hybrid Graph Attention\uff08HGA\uff09\u6a21\u5757\u4e0eTransformer\u7f16\u7801\u5668\uff0c\u4ee5\u53ca\u5728\u9891\u57df\u4e0a\u7ea6\u675f3D\u8f68\u8ff9\u4e00\u81f4\u6027\uff0c\u63d0\u5347\u4e86\u5355\u76ee\u89c6\u9891\u76842D-to-3D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5728\u4f4d\u7f6e\u7cbe\u5ea6\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002"}}
{"id": "2511.01767", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.01767", "abs": "https://arxiv.org/abs/2511.01767", "authors": ["Yuxiao Yang", "Xiao-Xiao Long", "Zhiyang Dou", "Cheng Lin", "Yuan Liu", "Qingsong Yan", "Yuexin Ma", "Haoqian Wang", "Zhiqiang Wu", "Wei Yin"], "title": "Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image", "comment": "21 pages, 19 figures, accepted by TPAMI", "summary": "In this work, we introduce \\textbf{Wonder3D++}, a novel method for\nefficiently generating high-fidelity textured meshes from single-view images.\nRecent methods based on Score Distillation Sampling (SDS) have shown the\npotential to recover 3D geometry from 2D diffusion priors, but they typically\nsuffer from time-consuming per-shape optimization and inconsistent geometry. In\ncontrast, certain works directly produce 3D information via fast network\ninferences, but their results are often of low quality and lack geometric\ndetails. To holistically improve the quality, consistency, and efficiency of\nsingle-view reconstruction tasks, we propose a cross-domain diffusion model\nthat generates multi-view normal maps and the corresponding color images. To\nensure the consistency of generation, we employ a multi-view cross-domain\nattention mechanism that facilitates information exchange across views and\nmodalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that\ndrives high-quality surfaces from the multi-view 2D representations in only\nabout $3$ minute in a coarse-to-fine manner. Our extensive evaluations\ndemonstrate that our method achieves high-quality reconstruction results,\nrobust generalization, and good efficiency compared to prior works. Code\navailable at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.", "AI": {"tldr": "\u63d0\u51faWonder3D++\uff1a\u901a\u8fc7\u8de8\u57df\u6269\u6563\u751f\u6210\u591a\u89c6\u89d2\u6cd5\u7ebf\u548c\u5f69\u8272\u56fe\uff0c\u7ed3\u5408\u591a\u89c6\u89d2\u8de8\u57df\u6ce8\u610f\u529b\u4e0e\u7ea7\u8054\u7f51\u683c\u63d0\u53d6\uff0c\u5b9e\u73b0\u57283\u5206\u949f\u7ea7\u522b\u7684\u5355\u89c6\u56fe\u9ad8\u8d28\u91cf\u7eb9\u7406\u7f51\u683c\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eSDS\u7684\u65b9\u6cd5\u867d\u80fd\u4ece2D\u6269\u6563\u5148\u9a8c\u6062\u590d3D\u51e0\u4f55\u4f46\u4f18\u5316\u6162\u4e14\u51e0\u4f55\u4e0d\u4e00\u81f4\uff1b\u76f4\u63a5\u7f51\u7edc\u63a8\u7406\u65b9\u6cd5\u901f\u5ea6\u5feb\u4f46\u8d28\u91cf\u4f4e\u4e14\u7f3a\u4e4f\u7ec6\u8282\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u4e0e\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u8de8\u57df\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u6cd5\u7ebf\u56fe\u4e0e\u5bf9\u5e94\u5f69\u8272\u56fe\u50cf\uff0c\u91c7\u7528\u591a\u89c6\u89d2\u8de8\u57df\u6ce8\u610f\u529b\u673a\u5236\u4fdd\u8bc1\u89c6\u89d2\u548c\u6a21\u6001\u95f4\u4fe1\u606f\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u7ea7\u8054\u7c97\u5230\u7ec6\u76843D\u7f51\u683c\u63d0\u53d6\u7b97\u6cd5\u4ece2D\u8868\u793a\u5feb\u901f\u63d0\u53d6\u9ad8\u8d28\u91cf\u8868\u9762\uff08\u7ea63\u5206\u949f\uff09\u3002", "result": "\u5728\u5927\u91cf\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u3001\u6cdb\u5316\u6027\u4e0e\u6548\u7387\u4e0a\u4f18\u4e8e\u5148\u524d\u5de5\u4f5c\uff0c\u80fd\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u7f51\u683c\u6a21\u578b\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Wonder3D++\uff0c\u7528\u4e8e\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u9ad8\u6548\u751f\u6210\u9ad8\u4fdd\u771f\u5e26\u7eb9\u7406\u7f51\u683c\uff0c\u7efc\u5408\u63d0\u9ad8\u8d28\u91cf\u3001\u4e00\u81f4\u6027\u4e0e\u6548\u7387\u3002"}}
{"id": "2511.01768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01768", "abs": "https://arxiv.org/abs/2511.01768", "authors": ["Zhe Liu", "Jinghua Hou", "Xiaoqing Ye", "Jingdong Wang", "Hengshuang Zhao", "Xiang Bai"], "title": "UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs", "comment": null, "summary": "Although transformers have demonstrated remarkable capabilities across\nvarious domains, their quadratic attention mechanisms introduce significant\ncomputational overhead when processing long-sequence data. In this paper, we\npresent a unified autonomous driving model, UniLION, which efficiently handles\nlarge-scale LiDAR point clouds, high-resolution multi-view images, and even\ntemporal sequences based on the linear group RNN operator (i.e., performs\nlinear RNN for grouped features). Remarkably, UniLION serves as a single\nversatile architecture that can seamlessly support multiple specialized\nvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal\ntemporal fusion configurations) without requiring explicit temporal or\nmulti-modal fusion modules. Moreover, UniLION consistently delivers competitive\nand even state-of-the-art performance across a wide range of core tasks,\nincluding 3D perception (e.g., 3D object detection, 3D object tracking, 3D\noccupancy prediction, BEV map segmentation), prediction (e.g., motion\nprediction), and planning (e.g., end-to-end planning). This unified paradigm\nnaturally simplifies the design of multi-modal and multi-task autonomous\ndriving systems while maintaining superior performance. Ultimately, we hope\nUniLION offers a fresh perspective on the development of 3D foundation models\nin autonomous driving. Code is available at\nhttps://github.com/happinesslz/UniLION", "AI": {"tldr": "\u7528\u7ebf\u6027group RNN\u66ff\u4ee3Transformer\u6ce8\u610f\u529b\uff0cUniLION\u5b9e\u73b0\u4e86\u4e00\u4e2a\u9ad8\u6548\u7edf\u4e00\u7684\u81ea\u52a8\u9a7e\u9a76\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u5728\u591a\u98793D\u611f\u77e5\u3001\u9884\u6d4b\u4e0e\u89c4\u5212\u4efb\u52a1\u4e0a\u53d6\u5f97\u5f3a\u52b2\u8868\u73b0\u3002", "motivation": "Transformer\u7684\u4e8c\u6b21\u6ce8\u610f\u529b\u5728\u957f\u5e8f\u5217\u8f93\u5165\uff08\u5982\u5927\u89c4\u6a21\u70b9\u4e91\u4e0e\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u5e8f\u5217\uff09\u4e0b\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u5bfb\u627e\u4e00\u4e2a\u8ba1\u7b97\u66f4\u9ad8\u6548\u4e14\u80fd\u81ea\u7136\u652f\u6301\u591a\u6a21\u6001\u591a\u4efb\u52a1\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u7ebf\u6027group RNN\u7b97\u5b50\uff08\u5bf9\u5206\u7ec4\u7279\u5f81\u6267\u884c\u7ebf\u6027RNN\uff09\uff0c\u66ff\u4ee3\u4f20\u7edfTransformer\u7684\u4e8c\u6b21\u6ce8\u610f\u529b\uff0c\u5f62\u6210\u5355\u4e00\u7f51\u7edc\u7ed3\u6784\u53ef\u901a\u8fc7\u4e0d\u540c\u8f93\u5165\u914d\u7f6e\uff08LiDAR-only\u3001temporal LiDAR\u3001\u591a\u6a21\u6001\u53ca\u5176\u65f6\u95f4\u5e8f\u5217\uff09\u5f97\u5230\u4e13\u7528\u53d8\u4f53\u800c\u65e0\u9700\u989d\u5916\u7684\u65f6\u5e8f\u6216\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u3002", "result": "UniLION\u57283D\u611f\u77e5\uff08\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u5360\u7528\u9884\u6d4b\u3001BEV\u5206\u5272\uff09\u3001\u8f68\u8ff9\u9884\u6d4b\u4e0e\u7aef\u5230\u7aef\u89c4\u5212\u7b49\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0e\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ade\u4e89\u529b\u751a\u81f3SOTA\u6210\u7ee9\uff0c\u4e14\u6a21\u578b\u7ed3\u6784\u7b80\u6d01\uff0c\u6613\u6269\u5c55\u3002", "conclusion": "UniLION\u901a\u8fc7\u5c06\u7ebf\u6027\u7fa4RNN\u7b97\u5b50\u5e94\u7528\u4e8e\u5206\u7ec4\u7279\u5f81\uff0c\u6210\u529f\u5728\u7edf\u4e00\u67b6\u6784\u4e0b\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21LiDAR\u70b9\u4e91\u3001\u9ad8\u5206\u8fa8\u7387\u591a\u89c6\u89d2\u56fe\u50cf\u53ca\u65f6\u95f4\u5e8f\u5217\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u652f\u6301\u3002"}}
{"id": "2511.01775", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.01775", "abs": "https://arxiv.org/abs/2511.01775", "authors": ["Zhen Chen", "Qing Xu", "Jinlin Wu", "Biao Yang", "Yuhao Zhai", "Geng Guo", "Jing Zhang", "Yinlu Ding", "Nassir Navab", "Jiebo Luo"], "title": "How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment", "comment": null, "summary": "Foundation models in video generation are demonstrating remarkable\ncapabilities as potential world models for simulating the physical world.\nHowever, their application in high-stakes domains like surgery, which demand\ndeep, specialized causal knowledge rather than general physical rules, remains\na critical unexplored gap. To systematically address this challenge, we present\nSurgVeo, the first expert-curated benchmark for video generation model\nevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,\nfour-tiered framework tailored to assess model outputs from basic appearance to\ncomplex surgical strategy. On the basis of the SurgVeo benchmark, we task the\nadvanced Veo-3 model with a zero-shot prediction task on surgical clips from\nlaparoscopic and neurosurgical procedures. A panel of four board-certified\nsurgeons evaluates the generated videos according to the SPP. Our results\nreveal a distinct \"plausibility gap\": while Veo-3 achieves exceptional Visual\nPerceptual Plausibility, it fails critically at higher levels of the SPP,\nincluding Instrument Operation Plausibility, Environment Feedback Plausibility,\nand Surgical Intent Plausibility. This work provides the first quantitative\nevidence of the chasm between visually convincing mimicry and causal\nunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish a\ncrucial foundation and roadmap for developing future models capable of\nnavigating the complexities of specialized, real-world healthcare domains.", "AI": {"tldr": "\u63d0\u51faSurgVeo\u57fa\u51c6\u4e0eSPP\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u5927\u578b\u89c6\u9891\u6a21\u578b\u5728\u5916\u79d1\u56e0\u679c\u5c42\u9762\u5931\u8d25\uff0c\u5f3a\u8c03\u9700\u9488\u5bf9\u533b\u7597\u573a\u666f\u8bbe\u8ba1\u5177\u6709\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5bf9\u7269\u7406\u4e16\u754c\u7684\u6cdb\u5316\u826f\u597d\uff0c\u4f46\u80fd\u5426\u5728\u8981\u6c42\u6df1\u5ea6\u4e13\u4e1a\u56e0\u679c\u77e5\u8bc6\u7684\u9ad8\u98ce\u9669\u5916\u79d1\u9886\u57df\u80dc\u4efb\u5c1a\u4e0d\u660e\u786e\uff0c\u8feb\u5207\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0e\u57fa\u51c6\u3002", "method": "\u6784\u5efaSurgVeo\u5916\u79d1\u89c6\u9891\u57fa\u51c6\u5e76\u63d0\u51fa\u56db\u5c42Surgical Plausibility Pyramid\uff08\u5916\u79d1\u5408\u7406\u6027\u91d1\u5b57\u5854, SPP\uff09\uff1b\u4f7f\u7528Veo-3\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u751f\u6210\u672f\u4e2d\u89c6\u9891\uff0c\u5e76\u7531\u56db\u4f4d\u4e3b\u5200\u5916\u79d1\u533b\u5e08\u4f9d\u636eSPP\u8fdb\u884c\u76f2\u8bc4\u3002", "result": "Veo-3\u5728\u89c6\u89c9\u611f\u77e5\u5c42\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5668\u68b0\u64cd\u4f5c\u5408\u7406\u6027\u3001\u73af\u5883\u53cd\u9988\u5408\u7406\u6027\u4e0e\u5916\u79d1\u610f\u56fe\u5408\u7406\u6027\u4e09\u5c42\u5747\u8868\u73b0\u4e0d\u8db3\uff0c\u5b9a\u91cf\u4e0a\u4f53\u73b0\u4e3a\u8bc4\u5206\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u89c6\u89c9\u903c\u771f\u4e0e\u56e0\u679c\u7406\u89e3\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u672c\u6587\u8bc1\u5b9e\u73b0\u6709\u89c6\u9891\u57fa\u7840\u6a21\u578b\u5728\u5916\u79d1\u573a\u666f\u4e0b\u5b58\u5728\u201c\u53ef\u884c\u6027\u9e3f\u6c9f\u201d\uff1a\u5c3d\u7ba1\u89c6\u89c9\u8868\u73b0\u903c\u771f\uff0c\u4f46\u5728\u56e0\u679c\u64cd\u4f5c\u4e0e\u7b56\u7565\u5c42\u9762\u4e25\u91cd\u5931\u6548\uff0c\u9700\u53d1\u5c55\u4e13\u95e8\u7684\u3001\u56e0\u679c\u611f\u77e5\u7684\u6a21\u578b\u3002"}}
{"id": "2511.01802", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01802", "abs": "https://arxiv.org/abs/2511.01802", "authors": ["Tejas Sarnaik", "Manan Shah", "Ravi Hegde"], "title": "PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution", "comment": "Accepted in PReMI 2025", "summary": "Retrieval-Augmented Generation (RAG) has become a robust framework for\nenhancing Large Language Models (LLMs) with external knowledge. Recent advances\nin RAG have investigated graph based retrieval for intricate reasoning;\nhowever, the influence of prompt design on enhancing the retrieval and\nreasoning process is still considerably under-examined. In this paper, we\npresent a prompt-driven GraphRAG framework that underscores the significance of\nprompt formulation in facilitating entity extraction, fact selection, and\npassage reranking for multi-hop question answering. Our approach creates a\nsymbolic knowledge graph from text data by encoding entities and factual\nrelationships as structured facts triples. We use LLMs selectively during\nonline retrieval to perform semantic filtering and answer generation. We also\nuse entity-guided graph traversal through Personalized PageRank (PPR) to\nsupport efficient, scalable retrieval based on the knowledge graph we built.\nOur system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,\nwith F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,\nrespectively. These results show that prompt design is an important part of\nimproving retrieval accuracy and response quality. This research lays the\ngroundwork for more efficient and comprehensible multi-hop question-answering\nsystems, highlighting the importance of prompt-aware graph reasoning.", "AI": {"tldr": "\u63d0\u51faPrompt-driven GraphRAG\uff1a\u901a\u8fc7\u63d0\u793a\u8bed\u9a71\u52a8\u7684\u5b9e\u4f53\u62bd\u53d6\u3001\u4e09\u5143\u7ec4\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u3001PPR\u56fe\u904d\u5386\u68c0\u7d22\u548cLLM\u8bed\u4e49\u8fc7\u6ee4/\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u5e76\u8fbe\u6210SOTA\u3002", "motivation": "\u52a8\u673a\u662f\u5f53\u524d\u57fa\u4e8e\u56fe\u7684RAG\u5728\u590d\u6742\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u63d0\u793a\u8bed\uff08prompt\uff09\u5bf9\u68c0\u7d22\u4e0e\u63a8\u7406\u6d41\u7a0b\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5c06prompt\u4e0e\u56fe\u68c0\u7d22\u7ed3\u5408\uff0c\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u7684\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4ece\u6587\u672c\u4e2d\u62bd\u53d6\u5b9e\u4f53\u5e76\u6784\u5efa\u4e09\u5143\u7ec4\u5f62\u5f0f\u7684\u7b26\u53f7\u77e5\u8bc6\u56fe\u8c31\uff1b2) \u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u8bed\u7684LLM\u5728\u5728\u7ebf\u68c0\u7d22\u9636\u6bb5\u8fdb\u884c\u8bed\u4e49\u8fc7\u6ee4\u4e0e\u4e8b\u5b9e\u9009\u62e9\uff1b3) \u4f7f\u7528\u5b9e\u4f53\u5f15\u5bfc\u7684\u4e2a\u6027\u5316PageRank\uff08PPR\uff09\u56fe\u904d\u5386\u8fdb\u884c\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u591a\u8df3\u68c0\u7d22\uff1b4) \u6700\u7ec8\u5229\u7528LLM\u8fdb\u884c\u7b54\u6848\u751f\u6210\u4e0e\u6bb5\u843d\u91cd\u6392\u5e8f\u3002", "result": "\u5728HotpotQA\u548c2WikiMultiHopQA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff1aF1\u5206\u522b\u4e3a80.7%\u548c78.9%\uff0cRecall@5\u5206\u522b\u4e3a97.1%\u548c98.1%\u3002\u7ed3\u679c\u8868\u660e\u63d0\u793a\u8bed\u8bbe\u8ba1\u80fd\u663e\u8457\u6539\u5584\u68c0\u7d22\u51c6\u786e\u7387\u4e0e\u56de\u7b54\u8d28\u91cf\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684Prompt-driven GraphRAG\u6846\u67b6\u5f3a\u8c03\u4e86\u63d0\u793a\u8bed\u8bbe\u8ba1\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u5bf9\u5b9e\u4f53\u62bd\u53d6\u3001\u4e8b\u5b9e\u9009\u62e9\u548c\u6bb5\u843d\u91cd\u6392\u5e8f\u7684\u5173\u952e\u4f5c\u7528\uff0c\u901a\u8fc7\u5c06\u6587\u672c\u6784\u5efa\u4e3a\u7b26\u53f7\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u5e76\u7ed3\u5408LLM\u8fdb\u884c\u8bed\u4e49\u8fc7\u6ee4\u4e0e\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u63d0\u5347\u68c0\u7d22\u4e0e\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2511.01817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01817", "abs": "https://arxiv.org/abs/2511.01817", "authors": ["Sagi Eppel", "Alona Strugatski"], "title": "SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art", "comment": null, "summary": "The ability to connect visual patterns with the processes that form them\nrepresents one of the deepest forms of visual understanding. Textures of clouds\nand waves, the growth of cities and forests, or the formation of materials and\nlandscapes are all examples of patterns emerging from underlying mechanisms. We\npresent the Scitextures dataset, a large-scale collection of textures and\nvisual patterns from all domains of science, tech, and art, along with the\nmodels and code that generate these images. Covering over 1,200 different\nmodels and 100,000 images of patterns and textures from physics, chemistry,\nbiology, sociology, technology, mathematics, and art, this dataset offers a way\nto explore the connection between the visual patterns that shape our world and\nthe mechanisms that produce them. Created by an agentic AI pipeline that\nautonomously collects and implements models in standardized form, we use\nSciTextures to evaluate the ability of leading AI models to link visual\npatterns to the models and code that generate them, and to identify different\npatterns that emerged from the same process. We also test AIs ability to infer\nand recreate the mechanisms behind visual patterns by providing a natural image\nof a real-world pattern and asking the AI to identify, model, and code the\nmechanism that formed the pattern, then run this code to generate a simulated\nimage that is compared to the real image. These benchmarks show that\nvision-language models (VLMs) can understand and simulate the physical system\nbeyond a visual pattern. The dataset and code are available at:\nhttps://zenodo.org/records/17485502", "AI": {"tldr": "SciTextures\u63d0\u4f9b\u4e861,200+\u6a21\u578b\u3001100k+\u7eb9\u7406\u56fe\u50cf\u53ca\u5176\u751f\u6210\u4ee3\u7801\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u5c06\u56fe\u50cf\u6a21\u5f0f\u6620\u5c04\u5230\u5f62\u6210\u673a\u5236\u53ca\u91cd\u5efa\u8fc7\u7a0b\u7684\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793aVLM\u5728\u6b64\u4efb\u52a1\u4e0a\u5177\u5907\u4e00\u5b9a\u7406\u89e3\u4e0e\u6a21\u62df\u80fd\u529b\u3002", "motivation": "\u5efa\u7acb\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u5927\u89c4\u6a21\u7eb9\u7406\u4e0e\u751f\u6210\u6a21\u578b\u96c6\uff0c\u7814\u7a76\u5e76\u8bc4\u4f30AI\u5c06\u89c6\u89c9\u6a21\u5f0f\u4e0e\u5f62\u6210\u673a\u5236\u8fde\u63a5\u3001\u7406\u89e3\u5e76\u91cd\u5efa\u7269\u7406/\u793e\u4f1a/\u827a\u672f\u7b49\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u81ea\u4e3b\u7684AI\u6d41\u6c34\u7ebf\u81ea\u52a8\u6536\u96c6\u5e76\u6807\u51c6\u5316\u5b9e\u73b0\u8d85\u8fc71,200\u4e2a\u6a21\u578b\u7684\u4ee3\u7801\uff0c\u751f\u6210\u8d85\u8fc7100,000\u5f20\u6765\u81ea\u5404\u5b66\u79d1\u9886\u57df\u7684\u7eb9\u7406\u56fe\u50cf\uff1b\u5e76\u8bbe\u8ba1\u57fa\u51c6\u4efb\u52a1\uff0c\u5305\u62ec\u5339\u914d\u89c6\u89c9\u6a21\u5f0f\u5230\u6a21\u578b/\u4ee3\u7801\uff0c\u4ee5\u53ca\u4ece\u771f\u5b9e\u56fe\u50cf\u63a8\u65ad\u5e76\u8fd0\u884c\u751f\u6210\u673a\u5236\u4ee5\u6bd4\u8f83\u6a21\u62df\u7ed3\u679c\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\u9886\u5148\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7406\u89e3\u5e76\u6a21\u62df\u751f\u6210\u7269\u7406\u7cfb\u7edf\u7684\u673a\u5236\uff0c\u80fd\u591f\u8bc6\u522b\u6a21\u5f0f\u5bf9\u5e94\u7684\u6a21\u578b\u5e76\u751f\u6210\u76f8\u4f3c\u7684\u6a21\u62df\u56fe\u50cf\uff1b\u540c\u65f6\u63ed\u793a\u4efb\u52a1\u5b58\u5728\u6311\u6218\u548c\u6a21\u578b\u5c40\u9650\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u53d1\u5e03\u4e86SciTextures\u6570\u636e\u96c6\uff0c\u89c4\u6a21\u5927\u3001\u8986\u76d6\u9762\u5e7f\uff0c\u65e8\u5728\u5c06\u53ef\u89c6\u7eb9\u7406\u4e0e\u751f\u6210\u673a\u5236\u8fde\u63a5\u8d77\u6765\uff0c\u5e76\u7528\u8be5\u6570\u636e\u96c6\u8bc4\u4f30AI\u8bc6\u522b\u548c\u91cd\u5efa\u751f\u6210\u673a\u5236\u7684\u80fd\u529b\u3002"}}
{"id": "2511.01833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.01833", "abs": "https://arxiv.org/abs/2511.01833", "authors": ["Ming Li", "Jike Zhong", "Shitian Zhao", "Haoquan Zhang", "Shaoheng Lin", "Yuxiang Lai", "Wei Chen", "Konstantinos Psounis", "Kaipeng Zhang"], "title": "TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning", "comment": "Preprint", "summary": "The frontier of visual reasoning is shifting toward models like OpenAI o3,\nwhich can intelligently create and operate tools to transform images for\nproblem-solving, also known as thinking-\\textit{with}-images in\nchain-of-thought. Yet existing benchmarks fail to fully capture this advanced\ncapability. Even Visual Search, the most common benchmark for current\nthinking-\\textit{with}-images methods, tests only basic operations such as\nlocalization and cropping, offering little insight into more complex, dynamic,\nand tool-dependent reasoning. We introduce \\textbf{TIR-Bench}, a comprehensive\nbenchmark for evaluating agentic thinking-with-images across 13 diverse tasks,\neach requiring novel tool use for image processing and manipulation in\nchain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from\nleading open-sourced and proprietary models to those with explicit tool-use\naugmentation. Results show that TIR-Bench is universally challenging, and\nstrong performance requires genuine thinking-with-images capabilities. Finally,\nwe present a pilot study comparing direct versus agentic fine-tuning.", "AI": {"tldr": "\u63d0\u51faTIR-Bench\u8bc4\u4f30\u5177\u5de5\u5177\u5316\u56fe\u50cf\u94fe\u5f0f\u601d\u7ef4\u7684\u6a21\u578b\uff0c\u6d4b\u8bd522\u6b3eMLLMs\uff0c\u7ed3\u679c\u663e\u793a\u5927\u591a\u6570\u6a21\u578b\u8868\u73b0\u5dee\uff0c\u5f3a\u80fd\u529b\u9700\u8981\u771f\u6b63\u7684agentic\u56fe\u50cf\u601d\u7ef4\uff1b\u5e76\u6bd4\u8f83\u4e86\u5fae\u8c03\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\uff08\u5982Visual Search\uff09\u4ec5\u6d4b\u8bd5\u7b80\u5355\u64cd\u4f5c\uff0c\u96be\u4ee5\u8861\u91cf\u6700\u65b0\u80fd\u591f\u521b\u9020\u5e76\u64cd\u4f5c\u5de5\u5177\u4ee5\u89e3\u51b3\u56fe\u50cf\u95ee\u9898\u7684\u6a21\u578b\u80fd\u529b\uff0c\u9700\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa13\u4e2a\u4efb\u52a1\u96c6\uff0c\u8bbe\u8ba1\u9700\u8981\u94fe\u5f0f\u601d\u7ef4\u548c\u5de5\u5177\u4f7f\u7528\u7684\u56fe\u50cf\u5904\u7406/\u64cd\u4f5c\u4efb\u52a1\uff1b\u5bf922\u4e2aMLLMs\uff08\u5f00\u653e\u6e90\u4e0e\u4e13\u6709\u3001\u542b\u5de5\u5177\u589e\u5f3a\u4e0e\u5426\uff09\u8fdb\u884c\u7edf\u4e00\u8bc4\u6d4b\uff1b\u5e76\u8fdb\u884cpilot study\u5bf9\u6bd4\u76f4\u63a5\u5fae\u8c03\u4e0eagentic\u5fae\u8c03\u6548\u679c\u3002", "result": "TIR-Bench\u5bf9\u88ab\u6d4b\u6a21\u578b\u666e\u904d\u5177\u6709\u6311\u6218\u6027\uff0c\u53ea\u6709\u5177\u5907\u771f\u6b63thinking-with-images\u80fd\u529b\u7684\u6a21\u578b\u624d\u80fd\u8868\u73b0\u826f\u597d\uff1b\u53e6\u5916\u63d0\u4f9b\u4e86\u5173\u4e8e\u5fae\u8c03\u7b56\u7565\u7684\u521d\u6b65\u6bd4\u8f83\u7ed3\u679c\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86TIR-Bench\uff0c\u4e00\u4e2a\u8bc4\u4f30\u201cagentic thinking-with-images\u201d\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u8986\u76d613\u9879\u9700\u8981\u5de5\u5177\u5316\u56fe\u50cf\u5904\u7406\u7684\u4efb\u52a1\uff0c\u8868\u660e\u73b0\u6709\u6a21\u578b\u666e\u904d\u56f0\u96be\uff0c\u5f3a\u6027\u80fd\u9700\u8981\u771f\u5b9e\u7684\u56fe\u50cf\u601d\u7ef4\u80fd\u529b\uff0c\u5e76\u6bd4\u8f83\u4e86\u76f4\u63a5\u4e0eagentic\u5fae\u8c03\u3002"}}
