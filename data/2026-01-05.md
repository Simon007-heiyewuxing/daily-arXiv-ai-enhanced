<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [TeleWorld: Towards Dynamic Multimodal Synthesis with a 4D World Model](https://arxiv.org/abs/2601.00051)
*Yabo Chen,Yuanzhi Liang,Jiepeng Wang,Tingxi Chen,Junfei Cheng,Zixiao Gu,Yuyang Huang,Zicheng Jiang,Wei Li,Tian Li,Weichen Li,Zuoxin Li,Guangce Liu,Jialun Liu,Junqi Liu,Haoyuan Wang,Qizhen Weng,Xuan'er Wu,Xunzhi Xiang,Xiaoyan Yang,Xin Zhang,Shiwen Zhang,Junyu Zhou,Chengcheng Zhou,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleWorld 提出一个闭环的生成-重建-引导 4D 世界建模框架，结合 MMPL 与 DMD，在实时性、长期一致性与动态场景记忆方面显著改进，推动世界模型向实用化方向发展。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成在实时交互、长期一致性和对动态场景的持久记忆方面不足，限制其作为实用的世界模型的发展，因此需要统一生成、重建与长期记忆的框架。

Method: 提出生成-重建-引导范式，将生成的视频流连续重建为4D时空表示，并用该重构结果引导后续生成；采用自回归扩散视频模型，并引入 Macro-from-Micro Planning (MMPL) 的分层规划和 Distribution Matching Distillation (DMD) 以实现低时延长时域生成。

Result: 在静态与动态世界理解、长期一致性与实时生成效率上取得良好表现，证明 TeleWorld 有助于实现面向多模态生成与具身智能的交互式记忆型世界模型。

Conclusion: TeleWorld 构建了一个实时、多模态、4D 的世界建模框架，通过生成-重建-引导闭环实现时空一致性与长期记忆，展示了在交互式场景中兼顾动态对象与静态场景建模的能力。

Abstract: World models aim to endow AI systems with the ability to represent, generate, and interact with dynamic environments in a coherent and temporally consistent manner. While recent video generation models have demonstrated impressive visual quality, they remain limited in real-time interaction, long-horizon consistency, and persistent memory of dynamic scenes, hindering their evolution into practical world models. In this report, we present TeleWorld, a real-time multimodal 4D world modeling framework that unifies video generation, dynamic scene reconstruction, and long-term world memory within a closed-loop system. TeleWorld introduces a novel generation-reconstruction-guidance paradigm, where generated video streams are continuously reconstructed into a dynamic 4D spatio-temporal representation, which in turn guides subsequent generation to maintain spatial, temporal, and physical consistency. To support long-horizon generation with low latency, we employ an autoregressive diffusion-based video model enhanced with Macro-from-Micro Planning (MMPL)--a hierarchical planning method that reduces error accumulation from frame-level to segment-level-alongside efficient Distribution Matching Distillation (DMD), enabling real-time synthesis under practical computational budgets. Our approach achieves seamless integration of dynamic object modeling and static scene representation within a unified 4D framework, advancing world models toward practical, interactive, and computationally accessible systems. Extensive experiments demonstrate that TeleWorld achieves strong performance in both static and dynamic world understanding, long-term consistency, and real-time generation efficiency, positioning it as a practical step toward interactive, memory-enabled world models for multimodal generation and embodied intelligence.

</details>


### [2] [It's Never Too Late: Noise Optimization for Collapse Recovery in Trained Diffusion Models](https://arxiv.org/abs/2601.00090)
*Anne Harrington,A. Sophia Koepke,Shyamgopal Karthik,Trevor Darrell,Alexei A. Efros*

Main category: cs.CV

TL;DR: 通过在噪声空间进行目标优化并利用不同频谱的噪声初始化，能有效缓解文本到图像模型的模式坍缩，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 观察到同一文本提示下生成结果高度重复，现有方法多靠引导或生成大量候选并筛选，作者希望直接从噪声层面提升多样性以更有效且保持保真度。

Method: 构建一个简单的噪声优化目标，通过梯度或搜索方法在噪声空间中优化初始噪声；分析噪声的频域特性，探索不同频谱初始化（高频/低频混合）以改善优化和搜索效果。

Result: 实验表明噪声优化在生成质量和多样性方面优于基线方法，且不同频谱初始化能进一步提升效果。

Conclusion: 本文提出通过对生成模型的初始噪声进行优化来缓解文本到图像模型的模式坍缩（mode collapse），在保持基模型生成保真度的同时提高多样性。

Abstract: Contemporary text-to-image models exhibit a surprising degree of mode collapse, as can be seen when sampling several images given the same text prompt. While previous work has attempted to address this issue by steering the model using guidance mechanisms, or by generating a large pool of candidates and refining them, in this work we take a different direction and aim for diversity in generations via noise optimization. Specifically, we show that a simple noise optimization objective can mitigate mode collapse while preserving the fidelity of the base model. We also analyze the frequency characteristics of the noise and show that alternative noise initializations with different frequency profiles can improve both optimization and search. Our experiments demonstrate that noise optimization yields superior results in terms of generation quality and variety.

</details>


### [3] [Spatial4D-Bench: A Versatile 4D Spatial Intelligence Benchmark](https://arxiv.org/abs/2601.00092)
*Pan Wang,Yang Liu,Guile Wu,Eduardo R. Corral-Soto,Chengjie Huang,Binbin Xu,Dongfeng Bai,Xu Yan,Yuan Ren,Xingxin Chen,Yizhe Wu,Tao Huang,Wenjun Wan,Xin Wu,Pei Zhou,Xuyang Dai,Kangbo Lv,Hongbo Zhang,Yosef Fried,Aixue Ye,Bailan Feng,Zhenyu Chen,Zhen Li,Yingcong Chen,Yiyi Liao,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出了大规模的4D空间智能基准Spatial4D-Bench（~40k QA，18任务，6类认知）并评测多种MLLMs，结果显示现有模型在时空推理上仍有明显不足。


<details>
  <summary>Details</summary>
Motivation: 评估并推动MLLMs在类人4D空间智能（即同时理解空间与时间变化）的能力发展；现有基准往往规模小、任务不全或多样性不足，难以全面测评时空推理能力。

Method: 构建大规模多任务数据集（~40k QA），将任务分为6类（物体理解、场景理解、空间关系、时空关系、空间推理、时空推理），并对多种开源与专有MLLMs进行系统评估，比较模型在各任务上的表现并分析失败案例。

Result: Spatial4D-Bench提供了全面评测结果，显示大多数当前MLLM在多个4D任务上表现远未达到人类水平，尤其在路径规划、动作识别、以及物理合理性推断方面存在显著缺陷；并提供分析与建议以促进后续模型改进。

Conclusion: Spatial4D-Bench是一个面向多模态大模型（MLLMs）4D空间推理能力的大规模基准，覆盖18个任务、约4万问答，按6类认知能力组织，揭示现有模型在路径规划、动作识别和物理合理性推理等方面存在显著不足。

Abstract: 4D spatial intelligence involves perceiving and processing how objects move or change over time. Humans naturally possess 4D spatial intelligence, supporting a broad spectrum of spatial reasoning abilities. To what extent can Multimodal Large Language Models (MLLMs) achieve human-level 4D spatial intelligence? In this work, we present Spatial4D-Bench, a versatile 4D spatial intelligence benchmark designed to comprehensively assess the 4D spatial reasoning abilities of MLLMs. Unlike existing spatial intelligence benchmarks that are often small-scale or limited in diversity, Spatial4D-Bench provides a large-scale, multi-task evaluation benchmark consisting of ~40,000 question-answer pairs covering 18 well-defined tasks. We systematically organize these tasks into six cognitive categories: object understanding, scene understanding, spatial relationship understanding, spatiotemporal relationship understanding, spatial reasoning and spatiotemporal reasoning. Spatial4D-Bench thereby offers a structured and comprehensive benchmark for evaluating the spatial cognition abilities of MLLMs, covering a broad spectrum of tasks that parallel the versatility of human spatial intelligence. We benchmark various state-of-the-art open-source and proprietary MLLMs on Spatial4D-Bench and reveal their substantial limitations in a wide variety of 4D spatial reasoning aspects, such as route plan, action recognition, and physical plausibility reasoning. We hope that the findings provided in this work offer valuable insights to the community and that our benchmark can facilitate the development of more capable MLLMs toward human-level 4D spatial intelligence. More resources can be found on our project page.

</details>


### [4] [A Spatially Masked Adaptive Gated Network for multimodal post-flood water extent mapping using SAR and incomplete multispectral data](https://arxiv.org/abs/2601.00123)
*Hyunho Lee,Wenwen Li*

Main category: cs.CV

TL;DR: 提出SMAGNet，通过自适应门控和空间掩码机制在SAR主导的洪水后期水体映射中有效融合部分可用MSI，提升了预测性能与对缺失数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提高洪水事件期间水体范围映射的准确性与鲁棒性，特别在MSI（多光谱影像）数据部分缺失或延迟可用时，通过融合SAR与MSI信息提升灾害响应阶段的时效性与可靠性。

Method: 设计了一种多模态网络框架：以SAR影像作为基础输入，采用空间掩码标识MSI可用区域，并通过自适应门控机制在特征层进行融合；在C2S-MS Floods数据集上进行了多种MSI可用性条件下的对比实验，评估性能和统计显著性。

Result: 提出了Spatially Masked Adaptive Gated Network (SMAGNet)，一种以SAR为主输入并通过特征融合自适应引入部分可用MSI的多模态深度学习模型。在C2S-MS Floods数据集上，SMAGNet在不同MSI可用性水平下均优于其他多模态模型；即使MSI完全缺失，SMAGNet性能仍与仅用SAR训练的U-Net无显著差异。

Conclusion: SMAGNet在强化多模态洪水映射模型对部分或完全缺失MSI的适应性方面有效，具有实际应用价值，可在真实灾害响应场景中利用不完整的遥感数据提高水体范围映射精度。

Abstract: Mapping water extent during a flood event is essential for effective disaster management throughout all phases: mitigation, preparedness, response, and recovery. In particular, during the response stage, when timely and accurate information is important, Synthetic Aperture Radar (SAR) data are primarily employed to produce water extent maps. Recently, leveraging the complementary characteristics of SAR and MSI data through a multimodal approach has emerged as a promising strategy for advancing water extent mapping using deep learning models. This approach is particularly beneficial when timely post-flood observations, acquired during or shortly after the flood peak, are limited, as it enables the use of all available imagery for more accurate post-flood water extent mapping. However, the adaptive integration of partially available MSI data into the SAR-based post-flood water extent mapping process remains underexplored. To bridge this research gap, we propose the Spatially Masked Adaptive Gated Network (SMAGNet), a multimodal deep learning model that utilizes SAR data as the primary input for post-flood water extent mapping and integrates complementary MSI data through feature fusion. In experiments on the C2S-MS Floods dataset, SMAGNet consistently outperformed other multimodal deep learning models in prediction performance across varying levels of MSI data availability. Furthermore, we found that even when MSI data were completely missing, the performance of SMAGNet remained statistically comparable to that of a U-Net model trained solely on SAR data. These findings indicate that SMAGNet enhances the model robustness to missing data as well as the applicability of multimodal deep learning in real-world flood management scenarios.

</details>


### [5] [Compressed Map Priors for 3D Perception](https://arxiv.org/abs/2601.00139)
*Brady Zhou,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: 提出CMP，通过二值化哈希地图学习历史遍历的空间先验，存储仅32KB/km^2，集成到3D感知系统中在nuScenes上显著提升3D目标检测表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶视觉系统通常假设每个位置是首次遇到，忽略了大量历史遍历数据的先验信息，作者借鉴人类驾驶经常行驶在已知道路的事实，希望提升感知性能并降低存储成本。

Method: 使用二值化哈希地图（binarized hashmap）压缩历史位置信息，作为空间先验融合进现有3D目标检测网络，存储密度为32KB每平方公里，约为稠密存储的1/20。

Result: Compressed Map Priors (CMP) framework

Conclusion: CMP在保持极低存储和几乎无额外计算开销下，能为现有3D感知架构提供稳定的性能提升，证明利用历史遍历的空间先验在自动驾驶视觉任务中有效。

Abstract: Human drivers rarely travel where no person has gone before. After all, thousands of drivers use busy city roads every day, and only one can claim to be the first. The same holds for autonomous computer vision systems. The vast majority of the deployment area of an autonomous vision system will have been visited before. Yet, most autonomous vehicle vision systems act as if they are encountering each location for the first time. In this work, we present Compressed Map Priors (CMP), a simple but effective framework to learn spatial priors from historic traversals. The map priors use a binarized hashmap that requires only $32\text{KB}/\text{km}^2$, a $20\times$ reduction compared to the dense storage. Compressed Map Priors easily integrate into leading 3D perception systems at little to no extra computational costs, and lead to a significant and consistent improvement in 3D object detection on the nuScenes dataset across several architectures.

</details>


### [6] [Attention to Detail: Global-Local Attention for High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2601.00141)
*Lawrence Han*

Main category: cs.CV

TL;DR: GLASS通过全局缩放+分层采样的原始分辨率局部裁剪并用注意力聚合，兼顾全局与细节，在AI图像检测任务中提升性能且计算可行。


<details>
  <summary>Details</summary>
Motivation: 常见检测方法会在送入模型前对图像降采样，导致细粒度伪迹丢失，而AI生成图像的细节痕迹对检测很重要；需要一种在计算受限下保留原始分辨率局部信息的方案。

Method: 提出在输入中同时使用全局缩放后的图像和通过空间分层采样得到的多个随机原始分辨率局部裁剪；用注意力机制对这些局部裁剪进行评分并聚合；可将该机制插入ViT、ResNet、ConvNeXt等视觉骨干网络。

Result: 在若干骨干模型和任务上，GLASS在可接受的计算预算内优于标准迁移学习方法，取得更高的预测性能。

Conclusion: GLASS通过结合全局缩放视图和多尺度原始分辨率局部裁剪，能在保持计算可行性的同时恢复细粒度信息，从而提升AI生成图像检测性能。

Abstract: The rapid development of generative AI has made AI-generated images increasingly realistic and high-resolution. Most AI-generated image detection architectures typically downsample images before inputting them into models, risking the loss of fine-grained details. This paper presents GLASS (Global-Local Attention with Stratified Sampling), an architecture that combines a globally resized view with multiple randomly sampled local crops. These crops are original-resolution regions efficiently selected through spatially stratified sampling and aggregated using attention-based scoring. GLASS can be integrated into vision models to leverage both global and local information in images of any size. Vision Transformer, ResNet, and ConvNeXt models are used as backbones, and experiments show that GLASS outperforms standard transfer learning by achieving higher predictive performance within feasible computational constraints.

</details>


### [7] [FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications](https://arxiv.org/abs/2601.00150)
*Yehui Yang,Dalu Yang,Wenshuo Zhou,Fangxin Shang,Yifan Liu,Jie Ren,Haojun Fei,Qing Yang,Tao Chen*

Main category: cs.CV

TL;DR: 提出了FCMBench-V1.0：一个覆盖18类证件、4,043张合成隐私合规图片和8,446个QA样本的金融信贷多模态基准，包含感知、推理和鲁棒性三大维度评估，并在23款主流VLM上进行了广泛测试，表明当前模型在获取噪声下性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI在信贷审批与文档审查中的广泛应用，亟需一个既真实反映信贷文档与流程又满足隐私合规的领域基准，以评估模型的理解能力与实用鲁棒性。

Method: 通过手工合成文档模板并在内部以场景感知方式拍摄生成数据，构建感知任务、信贷特定推理任务和10类采集伪影扰动，随后在23个最先进VLM上进行横向评测。

Result: FCMBench-V1.0 is a privacy-compliant, domain-specific multimodal benchmark for financial credit tasks, offering perception, reasoning, and robustness evaluations across synthesized document images.

Conclusion: FCMBench能有效区分模型在财信领域的性能和鲁棒性，合成-捕获数据流既保证隐私合规也降低预训练数据泄露，实验显示领域专用模型能超越通用商用与开源模型，但整体鲁棒性仍需加强。

Abstract: As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.

</details>


### [8] [Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions](https://arxiv.org/abs/2601.00156)
*Kaiwen Zheng,Junchen Fu,Songpei Xu,Yaoqing He,Joemon M. Jose,Han Hu,Xuri Ge*

Main category: cs.CV

TL;DR: 本文构建了面向任意面部区域的多属性自然语言描述数据集，并提出基于Qwen2.5-VL的多阶段微调模型Focal-RegionFace，实现可解释的年龄、AU和情绪检测，实验证明在新基准和新指标上表现最好。


<details>
  <summary>Details</summary>
Motivation: 传统面部分析多关注整体或单一属性，缺乏对任意选定面部区域的多属性自然语言描述和识别能力；通过让系统能聚焦局部区域，可提升解释性、可控性与细粒度理解。

Method: 构建包含任意选定面部区域的多属性描述数据集；基于Qwen2.5-VL进行多阶段渐进式微调（逐步细化对局部特征的关注），训练生成并识别包含AU、情绪、年龄估计的自然语言描述的模型Focal-RegionFace；使用传统指标与新提出的指标在新基准上评估性能。

Result: 在新构建的FaceFocalDesc基准上，Focal-RegionFace在传统评价指标及作者提出的新指标上均取得最优性能，证明了其在细粒度多属性面部区域分析任务中的有效性与多功能性。

Conclusion: 本文提出FaceFocalDesc问题并构建数据集，展示了区域级多属性自然语言描述对面部分析的作用；所提模型Focal-RegionFace通过多阶段微调提升对局部面部特征的聚焦能力，实现可解释的年龄估计、面部动作单元和情绪检测，实验在新基准和新指标上表现优异，验证了方法有效性与通用性。

Abstract: In this paper, we introduce an underexplored problem in facial analysis: generating and recognizing multi-attribute natural language descriptions, containing facial action units (AUs), emotional states, and age estimation, for arbitrarily selected face regions (termed FaceFocalDesc). We argue that the system's ability to focus on individual facial areas leads to better understanding and control. To achieve this capability, we construct a new multi-attribute description dataset for arbitrarily selected face regions, providing rich region-level annotations and natural language descriptions. Further, we propose a fine-tuned vision-language model based on Qwen2.5-VL, called Focal-RegionFace for facial state analysis, which incrementally refines its focus on localized facial features through multiple progressively fine-tuning stages, resulting in interpretable age estimation, FAU and emotion detection. Experimental results show that Focal-RegionFace achieves the best performance on the new benchmark in terms of traditional and widely used metrics, as well as new proposed metrics. This fully verifies its effectiveness and versatility in fine-grained multi-attribute face region-focal analysis scenarios.

</details>


### [9] [DichroGAN: Towards Restoration of in-air Colours of Seafloor from Satellite Imagery](https://arxiv.org/abs/2601.00194)
*Salma Gonzalez-Sabbagh,Antonio Robles-Kelly,Shang Gao*

Main category: cs.CV

TL;DR: DichroGAN通过四生成器的cGAN框架结合高光谱数据与水下成像模型，旨在从卫星影像中恢复海底空中颜色，实验证明其表现接近现有顶级方法。


<details>
  <summary>Details</summary>
Motivation: 卫星影像受水体中光随深度指数衰减的影响，直接从卫星RGB恢复海底真实颜色困难。利用高光谱信息和生成对抗网络结合成像物理模型，有望更准确地校正水下光学效应并重建空中颜色。

Method: 提出一种条件GAN（DichroGAN），采用四个生成器并行训练：前两个从高光谱数据估计漫反射和镜面反射以恢复大气场景辐亮度；第三个利用场景辐亮度重建各波段特征；第四个估计水下光传输，基于水下成像方程共同去除吸收与散射影响。

Result: 在一个源自PRISMA的数据集上训练并在卫星与水下数据集上做广泛实验，结果显示DichroGAN在恢复质量上与当前先进的水下图像复原方法具有竞争力。

Conclusion: DichroGAN可以在一定程度上恢复海底的空中颜色，但受限于数据、物理模型和实际水体多样性，效果并非在所有情形都优越。

Abstract: Recovering the in-air colours of seafloor from satellite imagery is a challenging task due to the exponential attenuation of light with depth in the water column. In this study, we present DichroGAN, a conditional generative adversarial network (cGAN) designed for this purpose. DichroGAN employs a two-steps simultaneous training: first, two generators utilise a hyperspectral image cube to estimate diffuse and specular reflections, thereby obtaining atmospheric scene radiance. Next, a third generator receives as input the generated scene radiance containing the features of each spectral band, while a fourth generator estimates the underwater light transmission. These generators work together to remove the effects of light absorption and scattering, restoring the in-air colours of seafloor based on the underwater image formation equation. DichroGAN is trained on a compact dataset derived from PRISMA satellite imagery, comprising RGB images paired with their corresponding spectral bands and masks. Extensive experiments on both satellite and underwater datasets demonstrate that DichroGAN achieves competitive performance compared to state-of-the-art underwater restoration techniques.

</details>


### [10] [MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing](https://arxiv.org/abs/2601.00204)
*Xiaokun Sun,Zeyu Cai,Hao Tang,Ying Tai,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 提出 MorphAny3D：通过在注意力中融合源/目标 SLAT 特征（MCA）与时间融合自注意力（TFSA），实现训练自由、跨类别的高质量 3D 形变与扩展应用。


<details>
  <summary>Details</summary>
Motivation: 3D 形变难点在于跨类别生成语义一致与时间平滑的变形序列，现有方法在保结构和连贯性上表现不足。

Method: 在 3D 生成器的注意力模块中引入两种关键机制：Morphing Cross-Attention (MCA) 用于在跨类别形变时保持结构一致性；Temporal-Fused Self-Attention (TFSA) 通过融合前一帧特征提升时间一致性；并采用姿态校正策略减少形变过程中的姿态歧义。该方法为训练无关，可扩展到其他 SLAT 基生成模型。

Result: 实验表明在标准和跨类别任务上均实现了最先进的形变序列生成，且支持解耦形变、3D 风格迁移等扩展应用。

Conclusion: MorphAny3D 提出了一种无训练框架，基于 SLAT 表示并在注意力机制中智能融合源/目标特征，实现了高质量、语义一致且时间平滑的 3D 形变。

Abstract: 3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/.

</details>


### [11] [CropNeRF: A Neural Radiance Field-Based Framework for Crop Counting](https://arxiv.org/abs/2601.00207)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 提出一种基于NeRF的多视角3D实例分割作物计数方法，通过作物可见性和掩码一致性评分结合3D信息，实现无需作物特定调参的精确计数，在三个数据集上优于现有方法并公开了棉花数据集。


<details>
  <summary>Details</summary>
Motivation: 在户外田间环境中，作物部分遮挡和从单视角难以区分聚簇作物与单个作物的固有歧义，导致基于图像的分割和计数困难，因此需要结合多视角和3D信息的精确计数方法。

Method: 利用多视角2D图像与独立实例掩码为NeRF提供监督，学习可用于3D实例分割的隐式场。提出作物可见性分数和掩码一致性分数，结合NeRF生成的3D信息来关联和优化实例掩码，从而生成一致的3D实例分割并进行计数。

Result: 在三个数据集（棉铃、苹果、梨）上进行了验证，方法在计数任务上优于现有最先进方法，并显示对颜色、形状、尺寸变化具有鲁棒性；同时发布了一个棉花植株数据集以推进研究。

Conclusion: 该文提出了基于NeRF的三维实例分割作物计数框架，通过多视角2D图像与独立实例掩码的NeRF视图合成，实现对作物的三维分割与精确计数。引入了作物可见性和掩码一致性评分，结合NeRF的3D信息，避免了作物特定参数调优，并在棉铃、苹果、梨数据集上验证，表现优于现有方法并提供了一个棉花植株数据集。

Abstract: Rigorous crop counting is crucial for effective agricultural management and informed intervention strategies. However, in outdoor field environments, partial occlusions combined with inherent ambiguity in distinguishing clustered crops from individual viewpoints poses an immense challenge for image-based segmentation methods. To address these problems, we introduce a novel crop counting framework designed for exact enumeration via 3D instance segmentation. Our approach utilizes 2D images captured from multiple viewpoints and associates independent instance masks for neural radiance field (NeRF) view synthesis. We introduce crop visibility and mask consistency scores, which are incorporated alongside 3D information from a NeRF model. This results in an effective segmentation of crop instances in 3D and highly-accurate crop counts. Furthermore, our method eliminates the dependence on crop-specific parameter tuning. We validate our framework on three agricultural datasets consisting of cotton bolls, apples, and pears, and demonstrate consistent counting performance despite major variations in crop color, shape, and size. A comparative analysis against the state of the art highlights superior performance on crop counting tasks. Lastly, we contribute a cotton plant dataset to advance further research on this topic.

</details>


### [12] [IntraStyler: Exemplar-based Style Synthesis for Cross-modality Domain Adaptation](https://arxiv.org/abs/2601.00212)
*Han Liu,Yubo Fan,Hao Li,Dewei Hu,Daniel Moyer,Zhoubing Xu,Benoit M. Dawant,Ipek Oguz*

Main category: cs.CV

TL;DR: IntraStyler用示例引导和基于对比学习的风格编码器，无需先验地合成多样目标域风格，改善了无监督域适配中的合成数据多样性并提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像级域对齐方法侧重源-目标域间差异，忽略目标域内的多样性；现有风格合成常需预先指定域内变异类型，不切实际。因此提出一种无需先验、能捕获域内多样性的示例驱动方法以增强合成目标域数据的多样性。

Method: 提出一种示例驱动的风格合成框架（IntraStyler）：使用风格编码器提取仅与风格相关的表征（通过对比学习进行判别性训练），并在图像翻译过程中以示例图像引导合成，使输出样式匹配示例；生成多样化合成目标域样本用于训练分割器。

Result: 在CrossMoDA 2023数据集上进行评估，实验表明IntraStyler能够实现可控且多样的风格合成，并用多样化合成数据提升了下游分割任务的性能。

Conclusion: IntraStyler通过基于示例的风格合成与对比学习的风格编码器，实现了无先验的域内风格多样化，从而改进了图像级无监督域适配的合成目标域数据质量，提高了下游分割性能。

Abstract: Image-level domain alignment is the de facto approach for unsupervised domain adaptation, where unpaired image translation is used to minimize the domain gap. Prior studies mainly focus on the domain shift between the source and target domains, whereas the intra-domain variability remains under-explored. To address the latter, an effective strategy is to diversify the styles of the synthetic target domain data during image translation. However, previous methods typically require intra-domain variations to be pre-specified for style synthesis, which may be impractical. In this paper, we propose an exemplar-based style synthesis method named IntraStyler, which can capture diverse intra-domain styles without any prior knowledge. Specifically, IntraStyler uses an exemplar image to guide the style synthesis such that the output style matches the exemplar style. To extract the style-only features, we introduce a style encoder to learn styles discriminatively based on contrastive learning. We evaluate the proposed method on the largest public dataset for cross-modality domain adaptation, CrossMoDA 2023. Our experiments show the efficacy of our method in controllable style synthesis and the benefits of diverse synthetic data for downstream segmentation. Code is available at https://github.com/han-liu/IntraStyler.

</details>


### [13] [From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning](https://arxiv.org/abs/2601.00215)
*Omar Sharif,Eftekhar Hossain,Patrick Ng*

Main category: cs.CV

TL;DR: 该工作通过设计多样化奖励并结合GRPO，用无监督/弱监督的强化学习方法增强开源多模态模型的视觉感知与长链推理能力，有效提升视觉推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 动机是现有通过RL生成的推理链在多模态大模型中没有充分整合视觉信息，导致在需要精确视觉感知的问题（如视觉谜题）上表现欠佳。作者发现将图像转换为文本描述能显著提升性能，说明视觉感知是瓶颈，因此希望通过奖励驱动的RL来改进视觉推理而无需昂贵标注。

Method: 方法包括：设计六种针对图像理解、思维步骤和答案准确性的奖励函数；采用Group Relative Policy Optimization (GRPO)训练策略以激励更长、更结构化的推理并抑制绕过视觉信息的行为；在Qwen-2.5-VL-7B等开源模型上进行实验评估，并与基线及不同Claude模型在文本化图像描述设置下比较性能。

Result: 结果显示：将图像转为文本描述对Claude 3.5和3.7分别带来26.7%和23.6%的性能提升；在开源模型Qwen-2.5-VL-7B上，采用所提的奖励与GRPO训练后在视觉推理任务上相较基线提升约5.56%，并在域内与域外场景均有稳定增益。

Conclusion: 该论文结论是：视觉感知是限制多模态大模型在视觉推理任务上能力的关键瓶颈；通过奖励驱动的强化学习（使用GRPO和设计的六种奖励函数）可以在无需大量监督下，促使开源MLLM生成更长、更有结构且融合视觉信息的推理链，从而提升模型在视觉推理任务上的表现。

Abstract: Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7.
  To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.

</details>


### [14] [LooC: Effective Low-Dimensional Codebook for Compositional Vector Quantization](https://arxiv.org/abs/2601.00222)
*Jie Li,Kwan-Yee K. Wong,Kai Han*

Main category: cs.CV

TL;DR: LooC用低维可组合码本+无参数插值外推，使VQ在更小码本下更有效、稳健且可插拔，实验验证胜过现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据与模型复杂性增长需要更高容量但更紧凑的VQ方法，现有码本要么体积大要么性能受限，故提出在压缩码本容量的同时提升表示能力的方法。

Method: 重新定义码向量与特征向量关系，将码向量视为特征的低维可组合单元并进行组合，同时引入参数免费插值外推以在量化过程中增强和平滑特征，保证码本充分利用。

Result: 在多任务、多数据集和多种架构上广泛评估，LooC以显著更小的码本实现了最先进的性能并避免码表塌陷。

Conclusion: LooC通过低维可组合码表与无参数插值外推机制，实现了在更小码本容量下的更高性能并避免塌陷，且可插拔应用于多种下游任务，实验显示优于现有VQ方法。

Abstract: Vector quantization (VQ) is a prevalent and fundamental technique that discretizes continuous feature vectors by approximating them using a codebook. As the diversity and complexity of data and models continue to increase, there is an urgent need for high-capacity, yet more compact VQ methods. This paper aims to reconcile this conflict by presenting a new approach called LooC, which utilizes an effective Low-dimensional codebook for Compositional vector quantization. Firstly, LooC introduces a parameter-efficient codebook by reframing the relationship between codevectors and feature vectors, significantly expanding its solution space. Instead of individually matching codevectors with feature vectors, LooC treats them as lower-dimensional compositional units within feature vectors and combines them, resulting in a more compact codebook with improved performance. Secondly, LooC incorporates a parameter-free extrapolation-by-interpolation mechanism to enhance and smooth features during the VQ process, which allows for better preservation of details and fidelity in feature approximation. The design of LooC leads to full codebook usage, effectively utilizing the compact codebook while avoiding the problem of collapse. Thirdly, LooC can serve as a plug-and-play module for existing methods for different downstream tasks based on VQ. Finally, extensive evaluations on different tasks, datasets, and architectures demonstrate that LooC outperforms existing VQ methods, achieving state-of-the-art performance with a significantly smaller codebook.

</details>


### [15] [Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions](https://arxiv.org/abs/2601.00225)
*Aobo Li,Jinjian Wu,Yongxu Liu,Leida Li,Weisheng Dong*

Main category: cs.CV

TL;DR: 论文发现合成数据分布导致特征聚类化从而限制BIQA泛化，提出SynDR-IQA通过增强样本多样性与下采样高密度簇来重塑分布，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据训练的BIQA在泛化到真实/其他合成数据时性能有限，关键问题在于合成数据导致的特征表示离散并聚类化，影响回归学习。

Method: 提出SynDR-IQA框架，包含两大策略：1）分布感知的多样内容上采样（增强视觉多样性且保留内容分布）；2）密度感知的冗余簇下采样（通过降低高密度簇样本数量平衡样本）。理论上基于样本多样性与冗余对泛化误差影响的推导。

Result: 在三类跨数据集设置（合成到真实、合成到算法性、合成到合成）上进行大量实验，显示所提方法显著提升泛化性能。

Conclusion: 本论文指出合成数据分布导致的表征离散聚类问题限制了BIQA的泛化，并提出SynDR-IQA通过重塑合成数据分布来提升泛化性。

Abstract: Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA.

</details>


### [16] [Application Research of a Deep Learning Model Integrating CycleGAN and YOLO in PCB Infrared Defect Detection](https://arxiv.org/abs/2601.00237)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: 用CycleGAN把可见光PCB图像无配对转换为伪红外图像，混合有限真实IR数据训练轻量YOLOv8，缓解IR数据稀缺，显著提升缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决PCB缺陷检测中红外数据稀缺问题，通过跨模态数据增强提高检测器在低数据条件下的泛化与精度。

Method: 使用CycleGAN进行无配对图像到图像翻译，生成保留结构语义且模拟热分布的伪IR样本；随后将这些伪样本与有限真实IR样本按异构训练策略混合，用于训练轻量级YOLOv8检测器。

Result: Proposes cross-modal augmentation using CycleGAN to translate visible PCB images to pseudo-IR, then trains YOLOv8 with mixed real and pseudo-IR data.

Conclusion: 伪红外合成可有效扩充IR数据，以低成本改善PCB缺陷检测，在低样本情形下接近全监督性能，适用于工业检测场景。

Abstract: This paper addresses the critical bottleneck of infrared (IR) data scarcity in Printed Circuit Board (PCB) defect detection by proposing a cross-modal data augmentation framework integrating CycleGAN and YOLOv8. Unlike conventional methods relying on paired supervision, we leverage CycleGAN to perform unpaired image-to-image translation, mapping abundant visible-light PCB images into the infrared domain. This generative process synthesizes high-fidelity pseudo-IR samples that preserve the structural semantics of defects while accurately simulating thermal distribution patterns. Subsequently, we construct a heterogeneous training strategy that fuses generated pseudo-IR data with limited real IR samples to train a lightweight YOLOv8 detector. Experimental results demonstrate that this method effectively enhances feature learning under low-data conditions. The augmented detector significantly outperforms models trained on limited real data alone and approaches the performance benchmarks of fully supervised training, proving the efficacy of pseudo-IR synthesis as a robust augmentation strategy for industrial inspection.

</details>


### [17] [Context-Aware Pesticide Recommendation via Few-Shot Pest Recognition for Precision Agriculture](https://arxiv.org/abs/2601.00243)
*Anirudha Ghosh,Ritam Sarkar,Debaditya Barman*

Main category: cs.CV

TL;DR: 本文提出面向手机/无人机的轻量级害虫检测与农药决策支持系统，采用小型CNN+原型元学习与环境感知的农药推荐，兼顾精度与低算力部署，推动可持续植保。


<details>
  <summary>Details</summary>
Motivation: 提出针对小农户及移动低算力设备的轻量级害虫检测与农药推荐框架，解决传统人工巡查与化学农药依赖高成本、耗时、环境污染等问题。

Method: 构建紧凑CNN并结合原型网络进行少样本学习；收集合并多数据源形成含不同视角、尺度与背景的综合害虫图像集；实现环境感知的决策规则或学习模块输出农药推荐；在低算力硬件上评估精度与复杂度指标。

Result: 设计了两个模块：基于轻量级CNN和原型元学习的害虫检测模块，以及结合作物类型与生育期等环境因素的农药推荐模块；合并公开数据集构建多样化图像数据集；在保持与先进模型相当精度的同时大幅降低计算复杂度，并实现实时性与可部署性。

Conclusion: 所提框架在保证精度的前提下显著减小模型复杂度并支持实时精确检测与生态友好型农药建议，适合资源受限环境的精准农业应用。

Abstract: Effective pest management is crucial for enhancing agricultural productivity, especially for crops such as sugarcane and wheat that are highly vulnerable to pest infestations. Traditional pest management methods depend heavily on manual field inspections and the use of chemical pesticides. These approaches are often costly, time-consuming, labor-intensive, and can have a negative impact on the environment. To overcome these challenges, this study presents a lightweight framework for pest detection and pesticide recommendation, designed for low-resource devices such as smartphones and drones, making it suitable for use by small and marginal farmers.
  The proposed framework includes two main components. The first is a Pest Detection Module that uses a compact, lightweight convolutional neural network (CNN) combined with prototypical meta-learning to accurately identify pests even when only a few training samples are available. The second is a Pesticide Recommendation Module that incorporates environmental factors like crop type and growth stage to suggest safe and eco-friendly pesticide recommendations. To train and evaluate our framework, a comprehensive pest image dataset was developed by combining multiple publicly available datasets. The final dataset contains samples with different viewing angles, pest sizes, and background conditions to ensure strong generalization.
  Experimental results show that the proposed lightweight CNN achieves high accuracy, comparable to state-of-the-art models, while significantly reducing computational complexity. The Decision Support System additionally improves pest management by reducing dependence on traditional chemical pesticides and encouraging sustainable practices, demonstrating its potential for real-time applications in precision agriculture.

</details>


### [18] [TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models](https://arxiv.org/abs/2601.00260)
*Kohei Yamamoto,Tomohiro Kikuchi*

Main category: cs.CV

TL;DR: 提出一种基于器官分离和大规模自动配对的3D-CT放射学基础模型，通过VideoMAE自监督与体积—文本对比学习，在多项零样本临床任务上优于或可比于已有VLM，具备现实可行性。


<details>
  <summary>Details</summary>
Motivation: 面对3D-CT体积数据计算开销大与临床多任务需求，提出通过器官分离和自动化构建配对数据来实现高效的影像—语言基础模型。

Method: 利用分割自动生成器官体积与发现句对，结合VideoMAE自监督预训练与体积—文本对比学习（contrastive learning），在大规模14万序列数据上训练模型以降低计算成本同时提升性能。

Result: 在零样本器官级病变分类中，TotalFM比CT-CLIP在83%器官上F1更高、比Merlin在64%器官上更优；在零样本发现类别的AUROC上比Merlin在83%类别更高，同时在放射报告生成上达到可比性能。

Conclusion: 提出的TotalFM通过器官分离理念，在3D-CT与文本的对应学习上兼顾计算效率与表示能力，能作为3D-CT基础模型设计的现实可行方案。

Abstract: While foundation models in radiology are expected to be applied to various clinical tasks, computational cost constraints remain a major challenge when training on 3D-CT volumetric data. In this study, we propose TotalFM, a radiological foundation model that efficiently learns the correspondence between 3D-CT images and linguistic expressions based on the concept of organ separation, utilizing a large-scale dataset of 140,000 series. By automating the creation of organ volume and finding-sentence pairs through segmentation techniques and Large Language Model (LLM)-based radiology report processing, and by combining self-supervised pre-training via VideoMAE with contrastive learning using volume-text pairs, we aimed to balance computational efficiency and representation capability. In zero-shot organ-wise lesion classification tasks, the proposed model achieved higher F1 scores in 83% (5/6) of organs compared to CT-CLIP and 64% (9/14) of organs compared to Merlin. These results suggest that the proposed model exhibits high generalization performance in a clinical evaluation setting using actual radiology report sentences. Furthermore, in zero-shot finding-wise lesion classification tasks, our model achieved a higher AUROC in 83% (25/30) of finding categories compared to Merlin. We also confirmed performance comparable to existing Vision-Language Models (VLMs) in radiology report generation tasks. Our results demonstrate that the organ-separated learning framework can serve as a realistic and effective design guideline for the practical implementation of 3D-CT foundation models.

</details>


### [19] [S1-MMAlign: A Large-Scale, Multi-Disciplinary Dataset for Scientific Figure-Text Understanding](https://arxiv.org/abs/2601.00264)
*He Wang,Longteng Guo,Pengkang Huo,Xuanxu Lin,Yichen Yuan,Jie Jiang,Jing Liu*

Main category: cs.CV

TL;DR: 作者收集了15.5M图文对，覆盖物理、生物、工程等，使用Qwen-VL结合摘要与引文上下文重写图注，显著提升了文本图像对齐质量（CLIP提升18.21%）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法在通用领域表现出色，但科学图像与稀疏文本之间存在严重语义鸿沟，阻碍了科学发现任务的多模态应用；因此需要大规模高质量的科学图文对及语义增强方法。

Method: 通过从250万开放获取论文中提取图像与原始图注，采样并使用Qwen-VL结合论文摘要与引文上下文生成重写图注；并用SciBERT伪困惑度和CLIP分数进行质量验证。

Result: S1-MMAlign构建了大规模跨学科的科学图像-文本配对数据集，并提出语义增强流水线以改善弱对齐问题。

Conclusion: S1-MMAlign为科学多模态研究提供了高质量、可公开访问的基线数据资源，有助于推动跨模态科学推理与理解。

Abstract: Multimodal learning has revolutionized general domain tasks, yet its application in scientific discovery is hindered by the profound semantic gap between complex scientific imagery and sparse textual descriptions. We present S1-MMAlign, a large-scale, multi-disciplinary multimodal dataset comprising over 15.5 million high-quality image-text pairs derived from 2.5 million open-access scientific papers. Spanning disciplines from physics and biology to engineering, the dataset captures diverse visual modalities including experimental setups, heatmaps, and microscopic imagery. To address the pervasive issue of weak alignment in raw scientific captions, we introduce an AI-ready semantic enhancement pipeline that utilizes the Qwen-VL multimodal large model series to recaption images by synthesizing context from paper abstracts and citation contexts. Technical validation demonstrates that this enhancement significantly improves data quality: SciBERT-based pseudo-perplexity metrics show reduced semantic ambiguity, while CLIP scores indicate an 18.21% improvement in image-text alignment. S1-MMAlign provides a foundational resource for advancing scientific reasoning and cross-modal understanding in the era of AI for Science. The dataset is publicly available at https://huggingface.co/datasets/ScienceOne-AI/S1-MMAlign.

</details>


### [20] [ActErase: A Training-Free Paradigm for Precise Concept Erasure via Activation Patching](https://arxiv.org/abs/2601.00267)
*Yi Sun,Xinhao Zhong,Hongyan Li,Yimin Zhou,Junhao Li,Bin Chen,Xuan Wang*

Main category: cs.CV

TL;DR: ActErase是一种训练-free、插拔式的概念擦除方法，通过识别并替换特定激活成分，在多项敏感概念擦除任务中实现了高效且稳健的表现，同时避免了昂贵的微调。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法多依赖大量数据与昂贵的微调，限制了实用性。观察到模型激活主要由通用概念构成，目标概念仅占少量成分，从而激发了通过直接操控激活实现高效擦除的想法。

Method: 基于提示对比分析定位激活差异区域，提取目标激活并在推理时动态替换输入激活，整个过程无需对模型进行微调或额外训练。

Result: 在裸露、艺术风格和对象移除三项任务上，ActErase在擦除效果上达到了SOTA水平，同时保持了模型的整体生成能力，对抗攻击下也表现出较强鲁棒性。

Conclusion: 本文提出了一种无需训练的概念擦除方法ActErase，通过在前向传播中替换激活来移除目标概念，从而在不损失模型生成能力的情况下实现对敏感概念（裸露、艺术风格、对象等）的擦除。

Abstract: Recent advances in text-to-image diffusion models have demonstrated remarkable generation capabilities, yet they raise significant concerns regarding safety, copyright, and ethical implications. Existing concept erasure methods address these risks by removing sensitive concepts from pre-trained models, but most of them rely on data-intensive and computationally expensive fine-tuning, which poses a critical limitation. To overcome these challenges, inspired by the observation that the model's activations are predominantly composed of generic concepts, with only a minimal component can represent the target concept, we propose a novel training-free method (ActErase) for efficient concept erasure. Specifically, the proposed method operates by identifying activation difference regions via prompt-pair analysis, extracting target activations and dynamically replacing input activations during forward passes. Comprehensive evaluations across three critical erasure tasks (nudity, artistic style, and object removal) demonstrates that our training-free method achieves state-of-the-art (SOTA) erasure performance, while effectively preserving the model's overall generative capability. Our approach also exhibits strong robustness against adversarial attacks, establishing a new plug-and-play paradigm for lightweight yet effective concept manipulation in diffusion models.

</details>


### [21] [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/abs/2601.00269)
*Chaodong Tong,Qi Zhang,Chen Li,Lei Jiang,Yanbing Liu*

Main category: cs.CV

TL;DR: FaithSCAN 通过融合解码不确定性、视觉中间表示和跨模态对齐等内部信号，并用自动生成的模型依赖监督训练，轻量高效地检测 VQA 的可置信性幻觉，性能与效率均领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部验证的方法昂贵且依赖外部资源质量；不确定性驱动的方法只捕捉有限不确定性方面，无法充分利用模型内部丰富信号来检测多样化失败模式。需一种高效且鲁棒的方法，利用内部信号以提升检测性能并降低开销。

Method: 设计轻量级检测网络，输入包括逐标记解码不确定性、模型中间视觉特征与跨模态对齐向量；采用分支式证据编码分别处理不同信号，再用不确定性感知注意力融合；提出低成本、模型依赖的自动标注策略扩展 LLM-as-a-Judge 到 VQA 并用于监督训练。

Result: 在多个 VQA 基准上，FaithSCAN 在检测效果与计算效率上均显著优于现有方法；深入分析表明虚假回答来自视觉感知、中跨模态推理与语言解码的系统性内部状态变化，不同内部信号互为补充，且不同 VLM 架构呈现不同虚假模式。

Conclusion: FaithSCAN 有效利用 VLM 的内部信号（解码不确定性、中间视觉表示、跨模态对齐特征）并通过分支式证据编码与不确定性感知注意力融合这些信号，从而高效检测 VQA 中的虚假回答，显著优于现有外部验证与不确定性驱动方法。

Abstract: Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.

</details>


### [22] [Disentangling Hardness from Noise: An Uncertainty-Driven Model-Agnostic Framework for Long-Tailed Remote Sensing Classification](https://arxiv.org/abs/2601.00278)
*Chi Ding,Junxiao Xue,Xinyi Yin,Shi Chen,Yunyun Shi,Yiduo Wang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 提出DUAL：将不确定性分解为EU与AU，EU用于重权尾类样本，AU用于自适应标签平滑以抑制噪声，在遥感长尾分类上显著优于TGN和SADE。


<details>
  <summary>Details</summary>
Motivation: 遥感数据中存在严重长尾分布，且低置信度样本既包含难学的尾类样本也包含噪声模糊样本。现有方法通常把所有低置信度样本一视同仁，导致对噪声的过拟合。需设计方法区分难样本与噪声样本，以更有针对性地处理长尾问题。

Method: 基于Evidential Deep Learning，DUAL首先估计每个样本的EU和AU：EU用于指示样本稀缺性并指导样本重权；AU用于衡量数据固有模糊性并通过自适应标签平滑来抑制噪声影响。框架为模型无关，可与不同骨干网络结合；训练过程中采用动态重权和基于AU的平滑策略。

Result: 在多个数据集和不同骨干网络上，DUAL优于强基线（如TGN、SADE），证明了其在提升长尾分类性能和泛化能力方面的有效性。消融实验表明EU与AU的分离、重权策略与自适应标签平滑为性能提升的关键因素。

Conclusion: 本文提出了一种模型无关的基于不确定性的长尾分类框架DUAL，通过将预测不确定性动态分解为认知不确定性（EU）和本质不确定性（AU），在重权难学尾类样本与抑制噪声样本之间实现区分，从而提高长尾遥感目标识别的泛化能力。

Abstract: Long-Tailed distributions are pervasive in remote sensing due to the inherently imbalanced occurrence of grounded objects. However, a critical challenge remains largely overlooked, i.e., disentangling hard tail data samples from noisy ambiguous ones. Conventional methods often indiscriminately emphasize all low-confidence samples, leading to overfitting on noisy data. To bridge this gap, building upon Evidential Deep Learning, we propose a model-agnostic uncertainty-aware framework termed DUAL, which dynamically disentangles prediction uncertainty into Epistemic Uncertainty (EU) and Aleatoric Uncertainty (AU). Specifically, we introduce EU as an indicator of sample scarcity to guide a reweighting strategy for hard-to-learn tail samples, while leveraging AU to quantify data ambiguity, employing an adaptive label smoothing mechanism to suppress the impact of noise. Extensive experiments on multiple datasets across various backbones demonstrate the effectiveness and generalization of our framework, surpassing strong baselines such as TGN and SADE. Ablation studies provide further insights into the crucial choices of our design.

</details>


### [23] [SV-GS: Sparse View 4D Reconstruction with Skeleton-Driven Gaussian Splatting](https://arxiv.org/abs/2601.00285)
*Jun-Jee Chao,Volkan Isler*

Main category: cs.CV

TL;DR: 提出SV-GS: 骨架驱动的时序关节位姿+细粒度形变混合模型，能在稀疏时间/视角观测下显著提升动态重建质量，并可用扩散先验替代静态初始化。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，动态目标的多视角多时间稠密观测难以获得，现有方法在稀疏时间和视角下性能大幅下降。论文旨在解决在时间与视角均稀疏情况下的动态重建问题。

Method: 方法以粗糙的骨架图和初始静态重建为初始化，优化由粗粒度骨架关节位姿估计器和用于细粒度形变的模块组成的骨架驱动变形场；仅关节位姿估计器随时间依赖，其他细节由时不变的形变场表示。还演示可用扩散模型生成的先验替代初始静态重建。

Result: 在合成数据集上，SV-GS相比现有方法在PSNR上提高最多34%；在真实数据集上，即使用明显更少的帧，其性能可与稠密单目视频方法相媲美；并展示了用扩散生成先验替代初始静态重建的可行性，从而提高实用性。

Conclusion: 该论文提出了一个在稀疏观测下同时估计骨架驱动变形与时间相关运动的框架SV-GS，通过仅使关节位姿估计器随时间变化来实现平滑插值并保持几何细节。

Abstract: Reconstructing a dynamic target moving over a large area is challenging. Standard approaches for dynamic object reconstruction require dense coverage in both the viewing space and the temporal dimension, typically relying on multi-view videos captured at each time step. However, such setups are only possible in constrained environments. In real-world scenarios, observations are often sparse over time and captured sparsely from diverse viewpoints (e.g., from security cameras), making dynamic reconstruction highly ill-posed. We present SV-GS, a framework that simultaneously estimates a deformation model and the object's motion over time under sparse observations. To initialize SV-GS, we leverage a rough skeleton graph and an initial static reconstruction as inputs to guide motion estimation. (Later, we show that this input requirement can be relaxed.) Our method optimizes a skeleton-driven deformation field composed of a coarse skeleton joint pose estimator and a module for fine-grained deformations. By making only the joint pose estimator time-dependent, our model enables smooth motion interpolation while preserving learned geometric details. Experiments on synthetic datasets show that our method outperforms existing approaches under sparse observations by up to 34% in PSNR, and achieves comparable performance to dense monocular video methods on real-world datasets despite using significantly fewer frames. Moreover, we demonstrate that the input initial static reconstruction can be replaced by a diffusion-based generative prior, making our method more practical for real-world scenarios.

</details>


### [24] [Towards Automated Differential Diagnosis of Skin Diseases Using Deep Learning and Imbalance-Aware Strategies](https://arxiv.org/abs/2601.00286)
*Ali Anaissi,Ali Braytee,Weidong Huang,Junaid Akram,Alaa Farhat,Jie Hua*

Main category: cs.CV

TL;DR: 基于预训练和数据增强的Swin Transformer模型在ISIC2019上达87.71%准确率，具有临床辅助和患者自查潜力，但需进一步验证泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着皮肤病发病率上升且皮肤科医生资源有限，开发智能辅助诊断工具可提高疾病识别效率，支持临床决策并为患者提供自查帮助。

Method: 作者采用迁移学习，在公开皮肤病图像数据集上进行预训练并微调，改进模型架构、优化数据预处理流程并进行针对性数据增强，最终基于Swin Transformer在ISIC2019数据集上进行八类皮损分类。

Result: 最终模型在ISIC2019八类皮损分类任务上取得87.71%的预测准确率，显示出较高的分类能力。

Conclusion: 该论文展示了基于Swin Transformer的皮肤病分类模型具有良好性能，可作为诊断辅助工具和自我评估工具的潜力，但仍需注意泛化性、临床可解释性和伦理合规等问题。

Abstract: As dermatological conditions become increasingly common and the availability of dermatologists remains limited, there is a growing need for intelligent tools to support both patients and clinicians in the timely and accurate diagnosis of skin diseases. In this project, we developed a deep learning based model for the classification and diagnosis of skin conditions. By leveraging pretraining on publicly available skin disease image datasets, our model effectively extracted visual features and accurately classified various dermatological cases. Throughout the project, we refined the model architecture, optimized data preprocessing workflows, and applied targeted data augmentation techniques to improve overall performance. The final model, based on the Swin Transformer, achieved a prediction accuracy of 87.71 percent across eight skin lesion classes on the ISIC2019 dataset. These results demonstrate the model's potential as a diagnostic support tool for clinicians and a self assessment aid for patients.

</details>


### [25] [TimeColor: Flexible Reference Colorization via Temporal Concatenation](https://arxiv.org/abs/2601.00296)
*Bryan Constantine Sadihin,Yihao Meng,Michael Hua Wang,Matteo Jiahao Chen,Hang Su*

Main category: cs.CV

TL;DR: TimeColor 通过时间串联参考帧、时空掩码注意力与模态隔离 RoPE，有效利用多样参考提升视频上色的保真性与一致性。


<details>
  <summary>Details</summary>
Motivation: 传统色彩化模型通常只使用单一参考（如首帧），忽略了角色设定图、背景图或任意已上色帧等多样参考信息，导致色彩保真和身份一致性不足。TimeColor 致力于利用多源参考改善上色质量并避免调色板信息泄漏。

Method: 将异质、可变数量的参考图像编码为额外潜在帧并与输入帧时间串联，使它们在每个扩散步骤中并行处理；引入时空对应掩码注意力以强化参考与主体的绑定；使用模态不相交的 RoPE（旋转位置编码）索引以避免不同模态间的混淆；显式的每参考区域分配来控制参考影响范围。

Result: 在 SAKUGA-42M 数据集的单参考和多参考协议下，TimeColor 在颜色保真度、身份一致性与时间稳定性方面均优于先前基线方法，证明其在多参考场景中的有效性。

Conclusion: TimeColor 在多参考图像的视频上色任务中，通过将参考帧作为额外的潜在帧并进行时间串联、使用时空对应掩码注意力和模态隔离的 RoPE 索引来提高颜色保真度、身份一致性和时间稳定性。该方法有效降低了跨身份调色板泄漏和捷径问题，在 SAKUGA-42M 数据集上优于现有基线。

Abstract: Most colorization models condition only on a single reference, typically the first frame of the scene. However, this approach ignores other sources of conditional data, such as character sheets, background images, or arbitrary colorized frames. We propose TimeColor, a sketch-based video colorization model that supports heterogeneous, variable-count references with the use of explicit per-reference region assignment. TimeColor encodes references as additional latent frames which are concatenated temporally, permitting them to be processed concurrently in each diffusion step while keeping the model's parameter count fixed. TimeColor also uses spatiotemporal correspondence-masked attention to enforce subject-reference binding in addition to modality-disjoint RoPE indexing. These mechanisms mitigate shortcutting and cross-identity palette leakage. Experiments on SAKUGA-42M under both single- and multi-reference protocols show that TimeColor improves color fidelity, identity consistency, and temporal stability over prior baselines.

</details>


### [26] [VisNet: Efficient Person Re-Identification via Alpha-Divergence Loss, Feature Fusion and Dynamic Multi-Task Learning](https://arxiv.org/abs/2601.00307)
*Anns Ijaz,Muhammad Azeem Javed*

Main category: cs.CV

TL;DR: VisNet通过多尺度自注意力融合、基于体部的语义聚类、动态权重平均和FIDI损失，实现在Market-1501上高效准确的ReID，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 在实时监控与移动端应用中，需要在保持高识别精度的同时尽量降低计算开销，现有最先进方法精度高但计算代价大，故提出轻量而有效的模型VisNet。

Method: 多尺度特征融合（融合ResNet50的stage1~stage4）、每尺度自动注意力、基于人体结构的语义聚类（规则伪标签）、动态权重平均用于平衡分类正则化、并使用FIDI损失加强度量学习。

Result: 在Market-1501上达到87.05% Rank-1与77.65% mAP，模型参数32.41M，4.601 GFLOPs，兼顾准确率与计算效率，适合实时部署。

Conclusion: VisNet 在资源受限场景中实现了较好的ReID性能与效率折中，展示了其实用性。

Abstract: Person re-identification (ReID) is an extremely important area in both surveillance and mobile applications, requiring strong accuracy with minimal computational cost. State-of-the-art methods give good accuracy but with high computational budgets. To remedy this, this paper proposes VisNet, a computationally efficient and effective re-identification model suitable for real-world scenarios. It is the culmination of conceptual contributions, including feature fusion at multiple scales with automatic attention on each, semantic clustering with anatomical body partitioning, a dynamic weight averaging technique to balance classification semantic regularization, and the use of loss function FIDI for improved metric learning tasks. The multiple scales fuse ResNet50's stages 1 through 4 without the use of parallel paths, with semantic clustering introducing spatial constraints through the use of rule-based pseudo-labeling. VisNet achieves 87.05% Rank-1 and 77.65% mAP on the Market-1501 dataset, having 32.41M parameters and 4.601 GFLOPs, hence, proposing a practical approach for real-time deployment in surveillance and mobile applications where computational resources are limited.

</details>


### [27] [ReMA: A Training-Free Plug-and-Play Mixing Augmentation for Video Behavior Recognition](https://arxiv.org/abs/2601.00311)
*Feng-Qi Cui,Jinyang Huang,Sirui Zhao,Jinglong Guo,Qifan Cai,Xin Yan,Zhi Liu*

Main category: cs.CV

TL;DR: ReMA mixes video representations using alignment-constrained intra-class mixing (RAM) and motion-aware masks (DSM) to improve robustness without extra supervision or parameters.


<details>
  <summary>Details</summary>
Motivation: Existing video augmentations are perturbation-driven and can introduce uncontrolled variations that increase non-discriminative factors, harming intra-class distributional structure and causing representation drift across temporal scales.

Method: ReMA uses two mechanisms: Representation Alignment Mechanism (structured intra-class mixing under distribution alignment constraints) and Dynamic Selection Mechanism (motion-aware spatiotemporal masks to localize perturbations). Mixing is formulated as controlled replacement of representations.

Result: Representation-aware Mixing Augmentation (ReMA) is an augmentation method for video behavior recognition that mixes samples in a controlled, representation-aware manner to preserve class-conditional stability and temporal coherence.

Conclusion: ReMA improves generalization and robustness across benchmarks by controlling how (RAM) and where (DSM) mixing is applied, reducing intra-class drift and maintaining temporal coherence.

Abstract: Video behavior recognition demands stable and discriminative representations under complex spatiotemporal variations. However, prevailing data augmentation strategies for videos remain largely perturbation-driven, often introducing uncontrolled variations that amplify non-discriminative factors, which finally weaken intra-class distributional structure and representation drift with inconsistent gains across temporal scales. To address these problems, we propose Representation-aware Mixing Augmentation (ReMA), a plug-and-play augmentation strategy that formulates mixing as a controlled replacement process to expand representations while preserving class-conditional stability. ReMA integrates two complementary mechanisms. Firstly, the Representation Alignment Mechanism (RAM) performs structured intra-class mixing under distributional alignment constraints, suppressing irrelevant intra-class drift while enhancing statistical reliability. Then, the Dynamic Selection Mechanism (DSM) generates motion-aware spatiotemporal masks to localize perturbations, guiding them away from discrimination-sensitive regions and promoting temporal coherence. By jointly controlling how and where mixing is applied, ReMA improves representation robustness without additional supervision or trainable parameters. Extensive experiments on diverse video behavior benchmarks demonstrate that ReMA consistently enhances generalization and robustness across different spatiotemporal granularities.

</details>


### [28] [Depth-Synergized Mamba Meets Memory Experts for All-Day Image Reflection Separation](https://arxiv.org/abs/2601.00322)
*Siyan Fang,Long Peng,Yuntao Wang,Ruonan Wei,Yuehuan Wang*

Main category: cs.CV

TL;DR: 提出基于深度感知的状态空间与记忆补偿的DMDNet及夜间数据集，改善了图像反射分离的准确性，尤其在夜间场景。


<details>
  <summary>Details</summary>
Motivation: 单张图像信息受限，尤其在夜间当透射层与反射层对比相近时，现有方法容易混淆两层，需引入深度信息与记忆机制以约束解耦过程。

Method: 方法包括Depth-Aware Scanning (DAScan)引导Mamba聚焦显著结构，Depth-Synergized State-Space Model (DS-SSM)基于深度调节状态激活敏感性，以及Memory Expert Compensation Module (MECM)利用跨图像历史知识做类别特定补偿。

Result: 作者构建了NightIRS夜间数据集，并在多项实验中显示DMDNet较现有最优方法在白天与夜间均有明显提升。

Conclusion: 本文提出DMDNet，通过深度引导的状态空间建模和记忆补偿模块，有效改善了白天和夜间的图像反射分离性能。

Abstract: Image reflection separation aims to disentangle the transmission layer and the reflection layer from a blended image. Existing methods rely on limited information from a single image, tending to confuse the two layers when their contrasts are similar, a challenge more severe at night. To address this issue, we propose the Depth-Memory Decoupling Network (DMDNet). It employs the Depth-Aware Scanning (DAScan) to guide Mamba toward salient structures, promoting information flow along semantic coherence to construct stable states. Working in synergy with DAScan, the Depth-Synergized State-Space Model (DS-SSM) modulates the sensitivity of state activations by depth, suppressing the spread of ambiguous features that interfere with layer disentanglement. Furthermore, we introduce the Memory Expert Compensation Module (MECM), leveraging cross-image historical knowledge to guide experts in providing layer-specific compensation. To address the lack of datasets for nighttime reflection separation, we construct the Nighttime Image Reflection Separation (NightIRS) dataset. Extensive experiments demonstrate that DMDNet outperforms state-of-the-art methods in both daytime and nighttime.

</details>


### [29] [HarmoniAD: Harmonizing Local Structures and Global Semantics for Anomaly Detection](https://arxiv.org/abs/2601.00327)
*Naiqi Zhang,Chuancheng Shi,Jingtong Dou,Wenhua Wu,Fei Shen,Jianhua Cao*

Main category: cs.CV

TL;DR: HarmoniAD通过频域分支将结构与语义互补：高频FSAM检测微小缺陷，低频GSCM保持语义一致，多类联合训练在多个数据集上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决工业质检中难以检测微小缺陷的问题，克服结构导向与语义导向方法之间的取舍。

Method: 使用CLIP图像编码器提取特征，转换到频域后分解成高频与低频分支；高频支路集成细粒度结构注意力模块(FSAM)，低频支路使用全局结构上下文模块(GSCM)，并结合多类联合训练策略；在公开数据集上验证性能。

Result: 提出HarmoniAD框架：CLIP编码器提取特征后变换到频域，分为高低频两条支路。高频支路用FSAM增强细粒度结构（纹理、边缘），低频支路用GSCM捕捉长程依赖和语义一致性；并采用多类联合训练，在MVTec-AD、VisA、BTAD上实现了最先进的性能。

Conclusion: 频域引导的双支路设计有效平衡了细节敏感性与语义鲁棒性，显著提升了工业缺陷检测对微小缺陷的检测能力。

Abstract: Anomaly detection is crucial in industrial product quality inspection. Failing to detect tiny defects often leads to serious consequences. Existing methods face a structure-semantics trade-off: structure-oriented models (such as frequency-based filters) are noise-sensitive, while semantics-oriented models (such as CLIP-based encoders) often miss fine details. To address this, we propose HarmoniAD, a frequency-guided dual-branch framework. Features are first extracted by the CLIP image encoder, then transformed into the frequency domain, and finally decoupled into high- and low-frequency paths for complementary modeling of structure and semantics. The high-frequency branch is equipped with a fine-grained structural attention module (FSAM) to enhance textures and edges for detecting small anomalies, while the low-frequency branch uses a global structural context module (GSCM) to capture long-range dependencies and preserve semantic consistency. Together, these branches balance fine detail and global semantics. HarmoniAD further adopts a multi-class joint training strategy, and experiments on MVTec-AD, VisA, and BTAD show state-of-the-art performance with both sensitivity and robustness.

</details>


### [30] [Joint Geometry-Appearance Human Reconstruction in a Unified Latent Space via Bridge Diffusion](https://arxiv.org/abs/2601.00328)
*Yingzhi Tang,Qijian Zhang,Junhui Hou*

Main category: cs.CV

TL;DR: JGA-LBD用3D高斯表示+稀疏VAE将异构条件统一到潜在空间，结合桥式扩散完成从部分观测到完整3D人像的联合重建，提升了几何与外观一致性与质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法将几何估计与外观合成分开，导致重建不一致；直接融合异构条件训练困难，需统一表示与联合建模。

Method: 将异构条件统一为3D高斯表示，并用共享的稀疏VAE压缩到统一潜在空间；采用桥式扩散从部分观测潜在编码推断缺失部分；最后用解码模块提取完整几何并进行新视角渲染。

Result: 在几何保真度和外观质量上优于现有最先进方法，并在真实场景（in-the-wild）中表现良好。

Conclusion: JGA-LBD通过联合潜在表示和桥式扩散实现从单幅RGB图像到高保真3D人像几何与外观的统一重建，克服了以往解耦流程造成的不一致性。

Abstract: Achieving consistent and high-fidelity geometry and appearance reconstruction of 3D digital humans from a single RGB image is inherently a challenging task. Existing studies typically resort to decoupled pipelines for geometry estimation and appearance synthesis, often hindering unified reconstruction and causing inconsistencies. This paper introduces \textbf{JGA-LBD}, a novel framework that unifies the modeling of geometry and appearance into a joint latent representation and formulates the generation process as bridge diffusion. Observing that directly integrating heterogeneous input conditions (e.g., depth maps, SMPL models) leads to substantial training difficulties, we unify all conditions into the 3D Gaussian representations, which can be further compressed into a unified latent space through a shared sparse variational autoencoder (VAE). Subsequently, the specialized form of bridge diffusion enables to start with a partial observation of the target latent code and solely focuses on inferring the missing components. Finally, a dedicated decoding module extracts the complete 3D human geometric structure and renders novel views from the inferred latent representation. Experiments demonstrate that JGA-LBD outperforms current state-of-the-art approaches in terms of both geometry fidelity and appearance quality, including challenging in-the-wild scenarios. Our code will be made publicly available at https://github.com/haiantyz/JGA-LBD.

</details>


### [31] [Intelligent Traffic Surveillance for Real-Time Vehicle Detection, License Plate Recognition, and Speed Estimation](https://arxiv.org/abs/2601.00344)
*Bruce Mugizi,Sudi Murindanyi,Olivia Nakacwa,Andrew Katumba*

Main category: cs.CV

TL;DR: 提出面向资源受限环境的实时智能交通监控系统：YOLOv8车牌检测mAP 97.9%，transformer OCR CER 1.79%，速度估计误差约10 km/h，并支持数据库关联与SMS自动开罚单。


<details>
  <summary>Details</summary>
Motivation: Develop a real-time intelligent traffic surveillance system for resource-constrained developing regions (e.g., Uganda) to reduce speeding-related fatalities by automating vehicle detection, license plate recognition, speed estimation, and ticketing.

Method: 数据采集使用测速枪、Canon相机和手机；车牌检测用YOLOv8；字符识别比较CNN与transformer模型；速度估计通过定义源目标ROI并测距；建立数据库并调用Africa's Talking API发送短信罚单。

Result: Collected dataset (speed gun, Canon camera, mobile phone). YOLOv8 license plate detection mAP 97.9%. CNN OCR CER 3.85%; transformer OCR CER 1.79%. Speed estimation with ROI gave ~±10 km/h error. Integrated database and automated SMS ticketing via Africa's Talking API.

Conclusion: 系统在车牌识别与目标检测方面表现良好，OCR采用transformer显著提升准确性，速度估计可用于执法但误差需改进；总体上为发展中国家实现自动交通执法提供可行方案。

Abstract: Speeding is a major contributor to road fatalities, particularly in developing countries such as Uganda, where road safety infrastructure is limited. This study proposes a real-time intelligent traffic surveillance system tailored to such regions, using computer vision techniques to address vehicle detection, license plate recognition, and speed estimation. The study collected a rich dataset using a speed gun, a Canon Camera, and a mobile phone to train the models. License plate detection using YOLOv8 achieved a mean average precision (mAP) of 97.9%. For character recognition of the detected license plate, the CNN model got a character error rate (CER) of 3.85%, while the transformer model significantly reduced the CER to 1.79%. Speed estimation used source and target regions of interest, yielding a good performance of 10 km/h margin of error. Additionally, a database was established to correlate user information with vehicle detection data, enabling automated ticket issuance via SMS via Africa's Talking API. This system addresses critical traffic management needs in resource-constrained environments and shows potential to reduce road accidents through automated traffic enforcement in developing countries where such interventions are urgently needed.

</details>


### [32] [OmniVaT: Single Domain Generalization for Multimodal Visual-Tactile Learning](https://arxiv.org/abs/2601.00352)
*Liuxiang Qiu,Hui Da,Yuzhen Niu,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出OmniVaT框架，结合MFFA与DTG两模块，有效解决VIS与TAC间模态差异及未知域迁移问题，从而提升SDG-VTL任务的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-触觉学习存在模态差异和由非标准化触觉传感器及不一致采集流程导致的域差异，且缺乏针对单域泛化的解决方案。

Method: 提出了两大模块：1）多模态分数傅里叶适配器（MFFA），将VIS和TAC嵌入映射到统一的嵌入-频域空间以减小模态差异；2）离散树生成（DTG）模块，通过层级树结构生成多样且可靠的分数表示以增强对未知域的适应性。

Result: 在提出的单域泛化多模态VTL（SDG-VTL）任务上，OmniVaT通过大量实验表现出优越的跨域泛化能力。

Conclusion: OmniVaT有效缓解了视觉与触觉模态差异及传感器/采集带来的域间差异，显著提升了单域泛化的多模态视觉-触觉学习性能。

Abstract: Visual-tactile learning (VTL) enables embodied agents to perceive the physical world by integrating visual (VIS) and tactile (TAC) sensors. However, VTL still suffers from modality discrepancies between VIS and TAC images, as well as domain gaps caused by non-standardized tactile sensors and inconsistent data collection procedures. We formulate these challenges as a new task, termed single domain generalization for multimodal VTL (SDG-VTL). In this paper, we propose an OmniVaT framework that, for the first time, successfully addresses this task. On the one hand, OmniVaT integrates a multimodal fractional Fourier adapter (MFFA) to map VIS and TAC embeddings into a unified embedding-frequency space, thereby effectively mitigating the modality gap without multi-domain training data or careful cross-modal fusion strategies. On the other hand, it also incorporates a discrete tree generation (DTG) module that obtains diverse and reliable multimodal fractional representations through a hierarchical tree structure, thereby enhancing its adaptivity to fluctuating domain shifts in unseen domains. Extensive experiments demonstrate the superior cross-domain generalization performance of OmniVaT on the SDG-VTL task.

</details>


### [33] [Efficient Prediction of Dense Visual Embeddings via Distillation and RGB-D Transformers](https://arxiv.org/abs/2601.00359)
*Söhnke Benedikt Fischedick,Daniel Seichter,Benedict Stephan,Robin Schmidt,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出DVEFormer：通过Alpha-CLIP蒸馏学习像素级文本对齐视觉嵌入的RGB-D Transformer，支持灵活文本查询与实时3D建图，是传统分割方法的可替代方案。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在家庭环境中需要对周围环境有细致、语义丰富且可由非专业用户直观查询的理解；传统固定类别分割限制了灵活性，因而提出学习文本对齐的像素嵌入以支持自然语言交互及3D映射。

Method: 基于RGB-D的Transformer学生网络（DVEFormer）在像素级通过一个Alpha-CLIP教师的嵌入进行知识蒸馏，输出密集的文本对齐视觉嵌入（DVE）。通过线性探测或文本查询实现经典分割或更灵活的自然语言检索；实现了两种模型尺寸以权衡速度与精度。

Result: 在常见室内数据集上取得了有竞争力的性能，并达到实时运行速度：完整模型在NVIDIA Jetson AGX Orin上约26.3 FPS，小型变体约77.0 FPS；同时展示了定性示例证明了实际应用可行性。

Conclusion: DVEFormer通过知识蒸馏从Alpha-CLIP教师网络学习细粒度的像素级文本对齐视觉嵌入，在提供与经典语义分割兼容的同时，支持基于文本的灵活查询与3D建图，满足实时性能要求，适合移动机器人在室内场景中使用。

Abstract: In domestic environments, robots require a comprehensive understanding of their surroundings to interact effectively and intuitively with untrained humans. In this paper, we propose DVEFormer - an efficient RGB-D Transformer-based approach that predicts dense text-aligned visual embeddings (DVE) via knowledge distillation. Instead of directly performing classical semantic segmentation with fixed predefined classes, our method uses teacher embeddings from Alpha-CLIP to guide our efficient student model DVEFormer in learning fine-grained pixel-wise embeddings. While this approach still enables classical semantic segmentation, e.g., via linear probing, it further enables flexible text-based querying and other applications, such as creating comprehensive 3D maps. Evaluations on common indoor datasets demonstrate that our approach achieves competitive performance while meeting real-time requirements, operating at 26.3 FPS for the full model and 77.0 FPS for a smaller variant on an NVIDIA Jetson AGX Orin. Additionally, we show qualitative results that highlight the effectiveness and possible use cases in real-world applications. Overall, our method serves as a drop-in replacement for traditional segmentation approaches while enabling flexible natural-language querying and seamless integration into 3D mapping pipelines for mobile robotics.

</details>


### [34] [Mask-Conditioned Voxel Diffusion for Joint Geometry and Color Inpainting](https://arxiv.org/abs/2601.00368)
*Aarya Sumuk*

Main category: cs.CV

TL;DR: 提出了先预测2D切片损伤聚合为体素掩码，再用掩码条件的3D扩散U-Net在32^3体素上联合恢复几何和颜色的两阶段轻量框架，相比对称性方法能更好地修复损失的形状与纹理。


<details>
  <summary>Details</summary>
Motivation: 针对文物数字修复，现有方法（如基于对称性的方法）在处理复杂纹理和几何缺损时存在局限，故提出显式掩码引导的体素扩散模型以更好地恢复完整几何和一致颜色。

Method: 第一阶段：使用2D卷积网络在从体素化对象提取的RGB切片上预测损伤掩码，将这些预测聚合为体素级掩码。第二阶段：基于扩散的3D U-Net在体素网格上进行掩码条件的修复，联合预测占据（几何）和颜色，使用占据重建、掩码颜色重建及感知正则化的复合损失。

Result: 在经过合成损伤的有纹理文物数据集上进行评估，按照标准几何和颜色指标在32^3分辨率下，相比对称性基线，方法在几何完整性和颜色一致性上均有提升。

Conclusion: 该工作提出了一个轻量的两阶段框架，通过先定位损伤再进行重建，实现了对损坏3D物体的几何和颜色修复，适用于数字化文物修复场景。

Abstract: We present a lightweight two-stage framework for joint geometry and color inpainting of damaged 3D objects, motivated by the digital restoration of cultural heritage artifacts. The pipeline separates damage localization from reconstruction. In the first stage, a 2D convolutional network predicts damage masks on RGB slices extracted from a voxelized object, and these predictions are aggregated into a volumetric mask. In the second stage, a diffusion-based 3D U-Net performs mask-conditioned inpainting directly on voxel grids, reconstructing geometry and color while preserving observed regions. The model jointly predicts occupancy and color using a composite objective that combines occupancy reconstruction with masked color reconstruction and perceptual regularization. We evaluate the approach on a curated set of textured artifacts with synthetically generated damage using standard geometric and color metrics. Compared to symmetry-based baselines, our method produces more complete geometry and more coherent color reconstructions at a fixed 32^3 resolution. Overall, the results indicate that explicit mask conditioning is a practical way to guide volumetric diffusion models for joint 3D geometry and color inpainting.

</details>


### [35] [BHaRNet: Reliability-Aware Body-Hand Modality Expertized Networks for Fine-grained Skeleton Action Recognition](https://arxiv.org/abs/2601.00369)
*Seungyeon Cho,Tae-kyun Kim*

Main category: cs.CV

TL;DR: 提出一个无校准、基于Noisy-OR的概率双流骨架-视觉融合框架，增强对手部细粒度动作的识别并在多基准上验证了其鲁棒性与有效性。


<details>
  <summary>Details</summary>
Motivation: 现有图网络多侧重整体身体运动，忽视细致的手部关节动作；同时在不确定性与噪声条件下，多模态融合的可靠性未被充分建模。

Method: 框架包括：去校正的预处理（在原生坐标系下学习而非标准化到规范空间）、基于Noisy-OR的概率融合用于无监督置信度的可靠性感知双流学习，以及一种从模态内部到跨模态的集成，耦合四种骨架模态（关节、骨骼、关节运动、骨骼运动）与RGB视觉表示。

Result: 在NTU RGB+D 60/120、PKU-MMD、N-UCLA以及作者构建的手部中心基准上，方法在准确率与鲁棒性方面均取得一致提升，且在噪声或模态异构条件下表现稳定。

Conclusion: 该论文提出了一种概率性双流框架，通过可靠性建模与多模态融合来提升骨架手势识别的精细表征能力。

Abstract: Skeleton-based human action recognition (HAR) has achieved remarkable progress with graph-based architectures. However, most existing methods remain body-centric, focusing on large-scale motions while neglecting subtle hand articulations that are crucial for fine-grained recognition. This work presents a probabilistic dual-stream framework that unifies reliability modeling and multi-modal integration, generalizing expertized learning under uncertainty across both intra-skeleton and cross-modal domains. The framework comprises three key components: (1) a calibration-free preprocessing pipeline that removes canonical-space transformations and learns directly from native coordinates; (2) a probabilistic Noisy-OR fusion that stabilizes reliability-aware dual-stream learning without requiring explicit confidence supervision; and (3) an intra- to cross-modal ensemble that couples four skeleton modalities (Joint, Bone, Joint Motion, and Bone Motion) to RGB representations, bridging structural and visual motion cues in a unified cross-modal formulation. Comprehensive evaluations across multiple benchmarks (NTU RGB+D~60/120, PKU-MMD, N-UCLA) and a newly defined hand-centric benchmark exhibit consistent improvements and robustness under noisy and heterogeneous conditions.

</details>


### [36] [NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos](https://arxiv.org/abs/2601.00393)
*Yuxue Yang,Lue Fan,Ziqi Shi,Junran Peng,Feng Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: NeoVerse是一个面向单目视频的可扩展4D世界模型，提出姿态无感前馈重建与在线退化模拟等技术，实现强生成与重建性能并提升通用性。


<details>
  <summary>Details</summary>
Motivation: 识别当前4D世界建模方法在可扩展性上的瓶颈：依赖昂贵/专用多视4D数据或繁重的训练预处理，使得在大规模真实单目视频上应用受限。NeoVerse希望构建一个可扩展、对真实单目视频友好的通用4D建模框架。

Method: 核心方法包括：1) 基于单目视频的姿态无感（pose-free）前馈4D重建模块，避免昂贵的多视或繁琐的预处理；2) 在线单目退化模式模拟，用以桥接合成与真实数据分布差异；3) 一系列对齐技术与训练策略以增强通用性与鲁棒性；整体流水线允许直接在多样的in-the-wild数据上训练。

Result: 在标准4D重建与新轨迹视频生成基准上，NeoVerse达到了或超过了现有最先进水平，同时展示了在不同领域（多种in-the-wild场景）上的良好泛化能力。

Conclusion: NeoVerse提出了一个可扩展的4D世界建模框架，能在裸眼单目视频上实现姿态无感的前馈式4D重建、在线单目退化模拟及多种下游任务，兼具通用性与生成/重建性能优势。

Abstract: In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io

</details>


### [37] [RoLID-11K: A Dashcam Dataset for Small-Object Roadside Litter Detection](https://arxiv.org/abs/2601.00398)
*Tao Wu,Qing Xu,Xiangjian He,Oakleigh Weekes,James Brown,Wenting Duan*

Main category: cs.CV

TL;DR: 发表了首个大规模行车摄像头路侧垃圾数据集 RoLID-11K（>11k 图像），并基准测试多类检测器，发现 transformer 在定位上更优，实时模型在小目标场景仍受限，提出了用于极小目标检测与道路环境监测的挑战性基准。


<details>
  <summary>Details</summary>
Motivation: 现有垃圾检测数据集多为街景静态图像、航拍或水域场景，无法覆盖行车摄像头中垃圾极小、稀疏且背景复杂的特点；需要数据集促进可扩展、低成本的路侧垃圾监测系统研发。

Method: 构建并标注了超过11k张英国内不同驾驶条件下的行车摄像头图像，定义11类垃圾（隐含），并对现代检测器（Transformer 系列与 YOLO 系列等）进行广泛基准测试与性能分析。

Result: 数据集呈现明显长尾与小目标分布。实验显示 CO-DETR 等 transformer 模型在定位精度上最佳，但实时模型（如 YOLO 系列）受限于粗糙的特征层次，对极小目标检测效果较差。

Conclusion: RoLID-11K 填补了路侧垃圾检测在行车摄像头场景上的数据空白，为极小目标检测提供了具有挑战性的基准。

Abstract: Roadside litter poses environmental, safety and economic challenges, yet current monitoring relies on labour-intensive surveys and public reporting, providing limited spatial coverage. Existing vision datasets for litter detection focus on street-level still images, aerial scenes or aquatic environments, and do not reflect the unique characteristics of dashcam footage, where litter appears extremely small, sparse and embedded in cluttered road-verge backgrounds. We introduce RoLID-11K, the first large-scale dataset for roadside litter detection from dashcams, comprising over 11k annotated images spanning diverse UK driving conditions and exhibiting pronounced long-tail and small-object distributions. We benchmark a broad spectrum of modern detectors, from accuracy-oriented transformer architectures to real-time YOLO models, and analyse their strengths and limitations on this challenging task. Our results show that while CO-DETR and related transformers achieve the best localisation accuracy, real-time models remain constrained by coarse feature hierarchies. RoLID-11K establishes a challenging benchmark for extreme small-object detection in dynamic driving scenes and aims to support the development of scalable, low-cost systems for roadside-litter monitoring. The dataset is available at https://github.com/xq141839/RoLID-11K.

</details>


### [38] [ABFR-KAN: Kolmogorov-Arnold Networks for Functional Brain Analysis](https://arxiv.org/abs/2601.00416)
*Tyler Ward,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出一种结合变换器与KAN的FC表示与分类方法ABFR-KAN，以减少分区偏差并增强个体特异性，在ABIDE I上对ASD分类表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于脑图谱的分区可能引入选择偏差且忽视个体差异，导致功能连接估计不可靠。为此提出一种减少结构偏差并增强个体特异性的FC表示与分类框架。

Method: 本文提出的ABFR-KAN包含基于变换器的主干用于特征提取，并引入先进脑功能表示组件以及KAN模块以重构或修正FC矩阵，搭配不同骨干网络和KAN配置进行消融研究与交站点评估。

Result: 在ABIDE I上进行的大量实验（含交站点评估与消融实验）显示，ABFR-KAN在ASD分类任务上稳定优于现有最先进方法。作者已公开代码。

Conclusion: ABFR-KAN通过将变换器分类网络与Kolmogorov-Arnold网络(KAN)相结合，在估计功能连接(FC)时减少结构偏差、提高解剖一致性并增强受试者特异性，从而在ABIDE I数据集上的ASD分类任务中优于多种基线方法。

Abstract: Functional connectivity (FC) analysis, a valuable tool for computer-aided brain disorder diagnosis, traditionally relies on atlas-based parcellation. However, issues relating to selection bias and a lack of regard for subject specificity can arise as a result of such parcellations. Addressing this, we propose ABFR-KAN, a transformer-based classification network that incorporates novel advanced brain function representation components with the power of Kolmogorov-Arnold Networks (KANs) to mitigate structural bias, improve anatomical conformity, and enhance the reliability of FC estimation. Extensive experiments on the ABIDE I dataset, including cross-site evaluation and ablation studies across varying model backbones and KAN configurations, demonstrate that ABFR-KAN consistently outperforms state-of-the-art baselines for autism spectrum distorder (ASD) classification. Our code is available at https://github.com/tbwa233/ABFR-KAN.

</details>


### [39] [Robust Assembly Progress Estimation via Deep Metric Learning](https://arxiv.org/abs/2601.00422)
*Kazuma Miura,Sarthak Pathak,Kazunori Umeda*

Main category: cs.CV

TL;DR: 針對小變化與遮擋情況，透過四元組損失與策略性樣本選擇，提升裝配進度估計的穩健性，實驗在桌機裝配數據集上顯示小幅但穩定改進。


<details>
  <summary>Details</summary>
Motivation: 手工跨多日進行的裝配任務視覺變化細微且常有遮擋，導致現有基於視覺特徵的進度估計（如Anomaly Triplet-Net）易誤分，需提高在小樣本與微變化場景下的魯棒性。

Method: 基於Quadruplet Loss的深度度量學習，並設計自定義數據加載器，策略性選擇訓練樣本以強化對異常圖像及相鄰任務區分的能力。

Result: 在桌面PC裝配圖像數據集上，Anomaly Quadruplet-Net將估計準確率提升1.3%，並將相鄰任務之間的誤分類降低1.9%。

Conclusion: 提出的Anomaly Quadruplet-Net在桌面PC裝配數據集上比現有方法有小幅提升，能更穩健地處理遮擋與微小視覺變化情況。

Abstract: In recent years, the advancement of AI technologies has accelerated the development of smart factories. In particular, the automatic monitoring of product assembly progress is crucial for improving operational efficiency, minimizing the cost of discarded parts, and maximizing factory productivity. However, in cases where assembly tasks are performed manually over multiple days, implementing smart factory systems remains a challenge. Previous work has proposed Anomaly Triplet-Net, which estimates assembly progress by applying deep metric learning to the visual features of products. Nevertheless, when visual changes between consecutive tasks are subtle, misclassification often occurs. To address this issue, this paper proposes a robust system for estimating assembly progress, even in cases of occlusion or minimal visual change, using a small-scale dataset. Our method leverages a Quadruplet Loss-based learning approach for anomaly images and introduces a custom data loader that strategically selects training samples to enhance estimation accuracy. We evaluated our approach using a image datasets: captured during desktop PC assembly. The proposed Anomaly Quadruplet-Net outperformed existing methods on the dataset. Specifically, it improved the estimation accuracy by 1.3% and reduced misclassification between adjacent tasks by 1.9% in the desktop PC dataset and demonstrating the effectiveness of the proposed method.

</details>


### [40] [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/abs/2601.00501)
*Ahmad Rezaei,Mohsen Gholami,Saeed Ranjbar Alvar,Kevin Cannons,Mohammad Asiful Hossain,Zhou Weimin,Shunbo Zhou,Yong Zhang,Mohammad Akbari*

Main category: cs.CV

TL;DR: CPPO finds perception tokens using entropy changes under image perturbations and applies contrastive loss in RL fine-tuning to improve VLM multimodal reasoning without extra models.


<details>
  <summary>Details</summary>
Motivation: Improve multimodal reasoning by finetuning VLMs to better handle perception tokens during RL, without extra models or ground-truth separation.

Method: Detect perception tokens via entropy shifts when perturbing input images; define CPL that contrasts outputs under preserving vs removing perturbations; combine with RL objective to finetune VLMs.

Result: CPPO detects perception tokens via entropy shifts under perturbed images and adds a Contrastive Perception Loss to enforce consistency for information-preserving perturbations and sensitivity for information-removing ones, outperforming prior methods and more efficient/scalable.

Conclusion: CPPO improves perception-rewarding finetuning by detecting perception tokens automatically via entropy shifts and using CPL, leading to better performance and efficiency compared to prior methods.

Abstract: We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.

</details>


### [41] [MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation](https://arxiv.org/abs/2601.00504)
*Miaowei Wang,Jakub Zadrożny,Oisin Mac Aodha,Amir Vaxman*

Main category: cs.CV

TL;DR: MotionPhysics用多模态大模型+视频扩散的可学习蒸馏损失，从自然语言提示直接推断3D场景的物理参数，实现高真实感动态仿真，无需真值轨迹或标注视频。


<details>
  <summary>Details</summary>
Motivation: 减少专家手工调参成本，使非专家通过自然语言即可指定期望动态行为，并在没有真值轨迹/视频标注的情况下推断物理参数。

Method: 方法包括两部分：1) 使用多模态大模型根据自然语言估计材料参数并将其限制在合理范围内；2) 提出可学习的运动蒸馏损失，从预训练视频扩散模型中提取稳健的运动先验，最小化外观与几何偏置以引导物理仿真。整体框架可微并自动确定参数。

Result: 在30+场景（真实、人工设计、AI生成），覆盖弹性固体、金属、泡沫、沙子、牛顿/非牛顿流体等材料，MotionPhysics在视觉真实感和参数合理性上超越现有方法。

Conclusion: MotionPhysics提出了一种端到端可微框架，通过自然语言提示推断3D场景中材料的物理参数，实现无需轨迹或标注视频的动态仿真参数估计。

Abstract: Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.

</details>


### [42] [All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations](https://arxiv.org/abs/2601.00533)
*Wenrui Li,Hongtao Chen,Yao Xiao,Wangmeng Zuo,Jiantao Zhou,Yonghong Tian,Xiaopeng Fan*

Main category: cs.CV

TL;DR: 提出SEUD情景与ORCANet，用于视频中平滑演变的未知退化恢复；设计合成管线、CIED粗估计去雾初始化、FPG生成静态/动态prompt及标签感知监督，显示优于基线。


<details>
  <summary>Details</summary>
Motivation: 现实视频退化随时间平滑演变，现有方法忽略时间连续性与复合退化；需单模型处理多类、动态强度的退化。

Method: 设计时序一致的合成数据生成；CIED模块基于物理先验估计雾强并给出粗去雾特征；FPG提取退化特征，生成静态（段级）和动态（帧级）prompt；标签感知监督提升静态prompt判别性；使用循环条件与自适应prompt进行恢复。

Result: ORCANet for SEUD scenario

Conclusion: 在SEUD场景下，ORCANet能更好恢复质量、时间一致性与鲁棒性，适用于单一、复合与演变退化的视频恢复。

Abstract: All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.

</details>


### [43] [FreeText: Training-Free Text Rendering in Diffusion Transformers via Attention Localization and Spectral Glyph Injection](https://arxiv.org/abs/2601.00535)
*Ruiqiang Zhang,Hengyi Wang,Chang Liu,Guanjie Wang,Zehua Ma,Weiming Zhang*

Main category: cs.CV

TL;DR: FreeText是一个不需训练、可插拔的框架，通过利用Diffusion Transformer内部机制改善大规模文本到图像扩散模型的文本渲染。它将任务分为“在哪里写”（基于image-to-text注意力的token级空间归因与拓扑优化生成写入掩码）和“写什么”（引入谱调制字形注入SGMI，用频域带通调制的噪声对齐字形先验以强化字形结构并抑制语义泄露）。在多个模型和基准上实验证明提升了可读性，同时保持语义和美学。


<details>
  <summary>Details</summary>
Motivation: 现有大规模T2I扩散模型文本渲染在多行布局、密集排版及长尾文字（如中文）上表现欠佳。现有方法往往需要昂贵的再训练或依赖刚性的外部布局限制，导致美学下降或灵活性受限。FreeText旨在无训练、保持美学与语义的前提下改进文本可读性。

Method: 1) Where-to-write: 从内生的image-to-text注意力读取token级空间归因，使用“水槽式”token作为稳定空间锚点并结合拓扑感知细化得到高置信写入掩码。2) What-to-write: 提出Spectral-Modulated Glyph Injection (SGMI)，将与噪声对齐的字形先验注入生成过程，并在频域施加带通调制以强化字形频谱、抑制语义内容扩散。整个框架无需训练，可插拔地作用于现有DiT-based扩散模型。

Result: 在Qwen-Image、FLUX.1-dev及SD3变体上，基于longText-Benchmark、CVTG和自建CLT-Bench的广泛实验显示，FreeText在文本可读性上持续带来提升，同时在语义对齐与视觉美学上基本保持一致，仅带来适度的推理开销。

Conclusion: FreeText能在不重训练的前提下，通过定位写入区域和频域调制的字形注入，有效提升T2I扩散模型对多行文本、稠密排版和长尾文字（如中文）的渲染可读性，在保持语义一致性与美学的同时仅带来小幅推理开销。

Abstract: Large-scale text-to-image (T2I) diffusion models excel at open-domain synthesis but still struggle with precise text rendering, especially for multi-line layouts, dense typography, and long-tailed scripts such as Chinese. Prior solutions typically require costly retraining or rigid external layout constraints, which can degrade aesthetics and limit flexibility. We propose \textbf{FreeText}, a training-free, plug-and-play framework that improves text rendering by exploiting intrinsic mechanisms of \emph{Diffusion Transformer (DiT)} models. \textbf{FreeText} decomposes the problem into \emph{where to write} and \emph{what to write}. For \emph{where to write}, we localize writing regions by reading token-wise spatial attribution from endogenous image-to-text attention, using sink-like tokens as stable spatial anchors and topology-aware refinement to produce high-confidence masks. For \emph{what to write}, we introduce Spectral-Modulated Glyph Injection (SGMI), which injects a noise-aligned glyph prior with frequency-domain band-pass modulation to strengthen glyph structure and suppress semantic leakage (rendering the concept instead of the word). Extensive experiments on Qwen-Image, FLUX.1-dev, and SD3 variants across longText-Benchmark, CVTG, and our CLT-Bench show consistent gains in text readability while largely preserving semantic alignment and aesthetic quality, with modest inference overhead.

</details>


### [44] [Boosting Segment Anything Model to Generalize Visually Non-Salient Scenarios](https://arxiv.org/abs/2601.00537)
*Guangqian Guo,Pengfei Chen,Yong Guo,Huafeng Chen,Boqiang Zhang,Shan Gao*

Main category: cs.CV

TL;DR: 本文提出VNS-SAM，通过两项轻量级模块利用SAM低层特征改善视觉不显著场景的分割，并构建了35K+的VNS-SEG数据集，实验证明在零样本条件下性能显著提升。


<details>
  <summary>Details</summary>
Motivation: SAM在低对比、视觉不显著场景下分割性能下降，现有方法难以准确捕捉目标轮廓，亟需在保持零样本泛化性的同时增强此类场景的感知能力。

Method: 提出Mask-Edge Token Interactive decoder和Non-Salient Feature Mining模块，利用SAM的低层特征与边缘信息进行交互，参数和计算开销仅小幅增加，能在约4小时内优化新增参数。

Result: 在多种视觉不显著分割任务上进行大量实验，VNS-SAM在零样本设置下显著优于原SAM及其他方法，验证了其在真实场景中广泛应用的潜力。

Conclusion: VNS-SAM有效增强了SAM在视觉不显著场景的分割能力，在保持零样本泛化性的前提下，通过低层特征挖掘与边缘-掩码交互解码器两项轻量设计提升性能，附带的VNS-SEG数据集支持更全面的训练与评估。

Abstract: Segment Anything Model (SAM), known for its remarkable zero-shot segmentation capabilities, has garnered significant attention in the community. Nevertheless, its performance is challenged when dealing with what we refer to as visually non-salient scenarios, where there is low contrast between the foreground and background. In these cases, existing methods often cannot capture accurate contours and fail to produce promising segmentation results. In this paper, we propose Visually Non-Salient SAM (VNS-SAM), aiming to enhance SAM's perception of visually non-salient scenarios while preserving its original zero-shot generalizability. We achieve this by effectively exploiting SAM's low-level features through two designs: Mask-Edge Token Interactive decoder and Non-Salient Feature Mining module. These designs help the SAM decoder gain a deeper understanding of non-salient characteristics with only marginal parameter increments and computational requirements. The additional parameters of VNS-SAM can be optimized within 4 hours, demonstrating its feasibility and practicality. In terms of data, we established VNS-SEG, a unified dataset for various VNS scenarios, with more than 35K images, in contrast to previous single-task adaptations. It is designed to make the model learn more robust VNS features and comprehensively benchmark the model's segmentation performance and generalizability on VNS scenarios. Extensive experiments across various VNS segmentation tasks demonstrate the superior performance of VNS-SAM, particularly under zero-shot settings, highlighting its potential for broad real-world applications. Codes and datasets are publicly available at https://guangqian-guo.github.io/VNS-SAM.

</details>


### [45] [DynaDrag: Dynamic Drag-Style Image Editing by Motion Prediction](https://arxiv.org/abs/2601.00542)
*Jiacheng Sui,Yujie Zhou,Li Niu*

Main category: cs.CV

TL;DR: DynaDrag是一种新的predict-and-move拖拽图像编辑方法，通过迭代预测控制点移动并执行拖拽，结合动态控制点调整，在人脸和人体编辑任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式图像编辑多数采用move-and-track框架，存在跟踪失败和模糊跟踪问题，或在其它框架下存在源图与目标图差距大、生成不合理的中间点等问题，导致可编辑性差。为避免这些问题，作者提出predict-and-move框架。

Method: 在每次迭代中先进行Motion Prediction，预测控制点的移动位置，然后由Motion Supervision根据预测结果执行拖拽操作；同时引入动态有效控制点调整策略来剔除或加入控制点以提升性能。

Result: 在人脸和人体数据集上的实验显示，DynaDrag在编辑效果、稳定性和可编辑性方面优于之前的方法。

Conclusion: DynaDrag提出了一个新的predict-and-move框架以避免move-and-track框架的跟踪错误和模糊追踪问题，通过迭代进行Motion Prediction和Motion Supervision，并动态调整有效控制点，从而提高拖拽式像素级图像编辑的稳定性和可编辑性。

Abstract: To achieve pixel-level image manipulation, drag-style image editing which edits images using points or trajectories as conditions is attracting widespread attention. Most previous methods follow move-and-track framework, in which miss tracking and ambiguous tracking are unavoidable challenging issues. Other methods under different frameworks suffer from various problems like the huge gap between source image and target edited image as well as unreasonable intermediate point which can lead to low editability. To avoid these problems, we propose DynaDrag, the first dragging method under predict-and-move framework. In DynaDrag, Motion Prediction and Motion Supervision are performed iteratively. In each iteration, Motion Prediction first predicts where the handle points should move, and then Motion Supervision drags them accordingly. We also propose to dynamically adjust the valid handle points to further improve the performance. Experiments on face and human datasets showcase the superiority over previous works.

</details>


### [46] [SingBAG Pro: Accelerating point cloud-based iterative reconstruction for 3D photoacoustic imaging under arbitrary array](https://arxiv.org/abs/2601.00551)
*Shuang Li,Yibing Wang,Jian Gao,Chulhong Kim,Seongwook Choi,Yu Zhang,Qian Chen,Yao Yao,Changhui Li*

Main category: cs.CV

TL;DR: 提出SlingBAG Pro：对任意不规则阵列友好的点云迭代重建方法，层次优化加速收敛，保持质量同时最多2.2×提速，并公开了源码。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统迭代重建在不规则探头阵列下计算复杂度高、内存需求大和重建时间长的问题，以便用更少探头实现高质量3D光声成像。

Method: 基于Sliding ball adaptive growth的点云迭代框架，扩展至任意阵列几何，采用层次优化策略：零梯度滤波结合迭代中逐步增加的时间采样率，快速剔除冗余空间点云以加速收敛。

Result: 与原始SlingBAG相比，在不规则阵列几何条件下点云基3D光声重建速度提高最多达2.2倍；通过仿真和体内小鼠实验验证了方法有效性，代码开源。

Conclusion: SlingBAG Pro在不规则阵列几何下显著提升了点云法三维光声重建的速度与效率，同时保持重建质量并减少所需探头数量。

Abstract: High-quality three-dimensional (3D) photoacoustic imaging (PAI) is gaining increasing attention in clinical applications. To address the challenges of limited space and high costs, irregular geometric transducer arrays that conform to specific imaging regions are promising for achieving high-quality 3D PAI with fewer transducers. However, traditional iterative reconstruction algorithms struggle with irregular array configurations, suffering from high computational complexity, substantial memory requirements, and lengthy reconstruction times. In this work, we introduce SlingBAG Pro, an advanced reconstruction algorithm based on the point cloud iteration concept of the Sliding ball adaptive growth (SlingBAG) method, while extending its compatibility to arbitrary array geometries. SlingBAG Pro maintains high reconstruction quality, reduces the number of required transducers, and employs a hierarchical optimization strategy that combines zero-gradient filtering with progressively increased temporal sampling rates during iteration. This strategy rapidly removes redundant spatial point clouds, accelerates convergence, and significantly shortens overall reconstruction time. Compared to the original SlingBAG algorithm, SlingBAG Pro achieves up to a 2.2-fold speed improvement in point cloud-based 3D PA reconstruction under irregular array geometries. The proposed method is validated through both simulation and in vivo mouse experiments, and the source code is publicly available at https://github.com/JaegerCQ/SlingBAG_Pro.

</details>


### [47] [A Comprehensive Dataset for Human vs. AI Generated Image Detection](https://arxiv.org/abs/2601.00553)
*Rajarshi Roy,Nasrin Imanpour,Ashhar Aziz,Shashwat Bajpai,Gurpreet Singh,Shwetangshu Biswas,Kapil Wanaskar,Parth Patwa,Subhankar Ghosh,Shreyas Dixit,Nilesh Ranjan Pal,Vipula Rawte,Ritvik Garimella,Gaytri Jena,Vasu Sharma,Vinija Jain,Aman Chadha,Aishwarya Naresh Reganti,Amitava Das*

Main category: cs.CV

TL;DR: 发布MS COCOAI数据集（96k样本，5个生成器），目标：判断图像真伪及识别生成模型，推动生成图像检测研究。


<details>
  <summary>Details</summary>
Motivation: 随着多模态生成模型进步，合成图像越来越难以与真实照片区分，导致误导性内容传播风险急剧上升，因此需要可靠的检测与模型归属方法。

Method: 使用MS COCO作为真实图像来源，利用五个生成器（Stable Diffusion 3、Stable Diffusion 2.1、SDXL、DALL-E 3、MidJourney v6）生成96,000个合成图像，构建包含真实与合成样本的数据集；在此基础上定义两项基准任务并提供数据集下载链接。

Result: 构建了包含96,000个真实与合成样本的公开数据集MS COCOAI，并提出二分类与模型识别两个任务，促进生成图像检测研究。数据已托管在Hugging Face并提供下载。

Conclusion: 论文提出并发布了一个名为MS COCOAI的大规模数据集，用于检测AI生成的图像，并提出了两个任务（真图/生成图分类及生成模型识别）。

Abstract: Multimodal generative AI systems like Stable Diffusion, DALL-E, and MidJourney have fundamentally changed how synthetic images are created. These tools drive innovation but also enable the spread of misleading content, false information, and manipulated media. As generated images become harder to distinguish from photographs, detecting them has become an urgent priority. To combat this challenge, We release MS COCOAI, a novel dataset for AI generated image detection consisting of 96000 real and synthetic datapoints, built using the MS COCO dataset. To generate synthetic images, we use five generators: Stable Diffusion 3, Stable Diffusion 2.1, SDXL, DALL-E 3, and MidJourney v6. Based on the dataset, we propose two tasks: (1) classifying images as real or generated, and (2) identifying which model produced a given synthetic image. The dataset is available at https://huggingface.co/datasets/Rajarshi-Roy-research/Defactify_Image_Dataset.

</details>


### [48] [AEGIS: Exploring the Limit of World Knowledge Capabilities for Unified Mulitmodal Models](https://arxiv.org/abs/2601.00561)
*Jintao Lin,Bowen Dong,Weikang Shi,Chenyang Lei,Suiyun Zhang,Rui Liu,Xihui Liu*

Main category: cs.CV

TL;DR: AEGIS是一个覆盖理解/生成/编辑/交互任务的多任务基准，配合DCE的确定性Y/N评估，揭示了UMMs在世界知识与复杂推理方面的严重缺陷，且插件式推理能部分缓解。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅限单任务、诊断能力有限，难以衡量UMMs在跨任务应用世界知识的能力，需建立一个覆盖理解、生成、编辑和交互生成的综合评测。

Method: 构建多任务基准AEGIS，包含1050手工注释问题、21个主题和6类推理；提出确定性核对表评估（DCE）以将评分简化为原子Y/N判断；在多款UMMs上进行广泛实验并测试插件式推理模块效果。

Result: 通过AEGIS和DCE评估，发现大多数UMMs在世界知识覆盖和复杂推理上表现不足；复杂性提高时性能显著下降；插件式推理模块能带来部分提升。

Conclusion: AEGIS展示了UMMs在基于世界知识的推理任务上存在显著短板，复杂推理导致性能大幅下降；简单的插件式推理模块能部分改善这些弱点。

Abstract: The capability of Unified Multimodal Models (UMMs) to apply world knowledge across diverse tasks remains a critical, unresolved challenge. Existing benchmarks fall short, offering only siloed, single-task evaluations with limited diagnostic power. To bridge this gap, we propose AEGIS (\emph{i.e.}, \textbf{A}ssessing \textbf{E}diting, \textbf{G}eneration, \textbf{I}nterpretation-Understanding for \textbf{S}uper-intelligence), a comprehensive multi-task benchmark covering visual understanding, generation, editing, and interleaved generation. AEGIS comprises 1,050 challenging, manually-annotated questions spanning 21 topics (including STEM, humanities, daily life, etc.) and 6 reasoning types. To concretely evaluate the performance of UMMs in world knowledge scope without ambiguous metrics, we further propose Deterministic Checklist-based Evaluation (DCE), a protocol that replaces ambiguous prompt-based scoring with atomic ``Y/N'' judgments, to enhance evaluation reliability. Our extensive experiments reveal that most UMMs exhibit severe world knowledge deficits and that performance degrades significantly with complex reasoning. Additionally, simple plug-in reasoning modules can partially mitigate these vulnerabilities, highlighting a promising direction for future research. These results highlight the importance of world-knowledge-based reasoning as a critical frontier for UMMs.

</details>


### [49] [A Cascaded Information Interaction Network for Precise Image Segmentation](https://arxiv.org/abs/2601.00562)
*Hewen Xiao,Jie Mei,Guangfu Ma,Weiren Wu*

Main category: cs.CV

TL;DR: 本文通过在级联CNN中加入全局信息引导模块，跨层融合低级纹理与高级语义，显著提高了复杂场景下的图像分割精度，优于现有方法，具备实用潜力。


<details>
  <summary>Details</summary>
Motivation: 视觉感知在自主行为中至关重要，但在复杂场景中鲁棒分割仍具有挑战，尤其是单一尺度特征难以同时兼顾细节与语义信息。为此提出跨层全局引导以提升分割效果。

Method: 设计了一个级联CNN架构，并在其中引入全局信息引导模块（Global Information Guidance Module，GIGM），用于在多层特征之间传递和融合信息，弥补单尺度特征提取的不足，增强对模糊与杂乱背景的分割能力。

Result: 在基准图像分割数据集上的实验表明，该方法在精度上优于现有多种最先进方法，特别在视觉混乱或模糊环境下提升明显，适合机器人实际应用。

Conclusion: 该论文提出了一个级联卷积神经网络，结合全局信息引导模块，有效融合低层纹理和高层语义，实现多层跨尺度特征融合，从而提升分割在复杂场景下的鲁棒性。

Abstract: Visual perception plays a pivotal role in enabling autonomous behavior, offering a cost-effective and efficient alternative to complex multi-sensor systems. However, robust segmentation remains a challenge in complex scenarios. To address this, this paper proposes a cascaded convolutional neural network integrated with a novel Global Information Guidance Module. This module is designed to effectively fuse low-level texture details with high-level semantic features across multiple layers, thereby overcoming the inherent limitations of single-scale feature extraction. This architectural innovation significantly enhances segmentation accuracy, particularly in visually cluttered or blurred environments where traditional methods often fail. Experimental evaluations on benchmark image segmentation datasets demonstrate that the proposed framework achieves superior precision, outperforming existing state-of-the-art methods. The results highlight the effectiveness of the approach and its promising potential for deployment in practical robotic applications.

</details>


### [50] [GranAlign: Granularity-Aware Alignment Framework for Zero-Shot Video Moment Retrieval](https://arxiv.org/abs/2601.00584)
*Mingyu Jeon,Sunjae Yoon,Jonghee Kim,Junyeoung Kim*

Main category: cs.CV

TL;DR: GranAlign通过生成多层次查询和查询感知/无感知字幕、无训练地桥接文本与视频粒度差异，显著提高零样本视频片段检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练跨模态表示的方法虽然质量高，但两种模态在语义粒度上不平衡，导致检索不准确；因此需要在不依赖任务特定训练的条件下缩小语义粒度差距。

Method: 不需要额外训练，包含两大技术：基于粒度的查询重写（生成不同语义粒度的查询）和查询感知字幕生成（将查询意图注入视频字幕），并将多层次查询与查询无关/相关字幕配对用于对齐。

Result: 在三个主要基准（QVHighlights、Charades-STA、ActivityNet-Captions）上均达新SOTA，特别是在QVHighlights上mAP@avg提升3.23%。

Conclusion: 提出了一种训练-free的细粒度对齐框架GranAlign，旨在解决文本查询与视频内容语义粒度不匹配的问题，通过生成多层次查询与查询相关/无关的字幕对齐，从而提升零样本视频片段检索性能。

Abstract: Zero-shot video moment retrieval (ZVMR) is the task of localizing a temporal moment within an untrimmed video using a natural language query without relying on task-specific training data. The primary challenge in this setting lies in the mismatch in semantic granularity between textual queries and visual content. Previous studies in ZVMR have attempted to achieve alignment by leveraging high-quality pre-trained knowledge that represents video and language in a joint space. However, these approaches failed to balance the semantic granularity between the pre-trained knowledge provided by each modality for a given scene. As a result, despite the high quality of each modality's representations, the mismatch in granularity led to inaccurate retrieval. In this paper, we propose a training-free framework, called Granularity-Aware Alignment (GranAlign), that bridges this gap between coarse and fine semantic representations. Our approach introduces two complementary techniques: granularity-based query rewriting to generate varied semantic granularities, and query-aware caption generation to embed query intent into video content. By pairing multi-level queries with both query-agnostic and query-aware captions, we effectively resolve semantic mismatches. As a result, our method sets a new state-of-the-art across all three major benchmarks (QVHighlights, Charades-STA, ActivityNet-Captions), with a notable 3.23% mAP@avg improvement on the challenging QVHighlights dataset.

</details>


### [51] [SafeMo: Linguistically Grounded Unlearning for Trustworthy Text-to-Motion Generation](https://arxiv.org/abs/2601.00590)
*Yiling Wang,Zeyu Zhang,Yiran Wang,Hao Tang*

Main category: cs.CV

TL;DR: SafeMo通过连续空间的最小运动遗忘和SafeMoVAE-29K数据集，有效消除了不安全提示导致的有害运动生成，同时避免了离散codebook替换的负面影响，兼顾安全性与生成实用性。


<details>
  <summary>Details</summary>
Motivation: 针对现有基于离散VQ-VAE codebook替换的方法存在的两大问题：对良性任务的漂移（替换被重用的codebook条目导致）以及离散化造成的量化和不平滑问题；并且现有数据集包含不安全意图与动作，不利于安全学习。

Method: 提出Minimal Motion Unlearning (MMU)双阶段机器遗忘策略，在连续空间进行运动微调以避免离散codebook替换带来的量化与平滑性损失；构建SafeMoVAE-29K数据集，包含重写的安全文本提示与连续精炼运动；基于DiP扩展用于生成自然过渡的安全人体运动。

Result: 实验表明SafeMo在HumanML3D和Motion-X上对不安全集合的忘却效果显著，分别达到比之前SOTA LCR高2.5倍和14.4倍的forget-set FID，同时在安全提示上的生成质量保持更好或可比。TLDR包含于下文。

Conclusion: SafeMo提出了一种基于连续运动空间的最小运动遗忘（MMU）策略与SafeMoVAE-29K数据集，有效提升对不安全提示的遗忘同时保持或改善对安全提示的生成质量。

Abstract: Text-to-motion (T2M) generation with diffusion backbones achieves strong realism and alignment. Safety concerns in T2M methods have been raised in recent years; existing methods replace discrete VQ-VAE codebook entries to steer the model away from unsafe behaviors. However, discrete codebook replacement-based methods have two critical flaws: firstly, replacing codebook entries which are reused by benign prompts leads to drifts on everyday tasks, degrading the model's benign performance; secondly, discrete token-based methods introduce quantization and smoothness loss, resulting in artifacts and jerky transitions. Moreover, existing text-to-motion datasets naturally contain unsafe intents and corresponding motions, making them unsuitable for safety-driven machine learning. To address these challenges, we propose SafeMo, a trustworthy motion generative framework integrating Minimal Motion Unlearning (MMU), a two-stage machine unlearning strategy, enabling safe human motion generation in continuous space, preserving continuous kinematics without codebook loss and delivering strong safety-utility trade-offs compared to current baselines. Additionally, we present the first safe text-to-motion dataset SafeMoVAE-29K integrating rewritten safe text prompts and continuous refined motion for trustworthy human motion unlearning. Built upon DiP, SafeMo efficiently generates safe human motions with natural transitions. Experiments demonstrate effective unlearning performance of SafeMo by showing strengthened forgetting on unsafe prompts, reaching 2.5x and 14.4x higher forget-set FID on HumanML3D and Motion-X respectively, compared to the previous SOTA human motion unlearning method LCR, with benign performance on safe prompts being better or comparable. Code: https://github.com/AIGeeksGroup/SafeMo. Website: https://aigeeksgroup.github.io/SafeMo.

</details>


### [52] [Modality Dominance-Aware Optimization for Embodied RGB-Infrared Perception](https://arxiv.org/abs/2601.00598)
*Xianhui Liu,Siqi Jiang,Yi Xie,Yuqing Lin,Siao Liu*

Main category: cs.CV

TL;DR: 本文通过MDI量化模态主导性，并提出MDACL（含HCG和AER）调节跨模态优化，显著改善RGB-IR检测的融合效果与性能。


<details>
  <summary>Details</summary>
Motivation: RGB-IR模态在信息密度和特征质量上存在差异，导致训练过程中出现模态主导性（优化偏置），进而影响跨模态融合效果和检测性能。

Method: 定义Modality Dominance Index (MDI)联合建模特征熵和梯度贡献；提出MDACL框架，包括Hierarchical Cross-modal Guidance (HCG)增强特征对齐，以及Adversarial Equilibrium Regularization (AER)在融合阶段平衡优化动态。

Result: 在三个RGB-IR基准数据集上的大量实验表明，MDACL有效缓解了优化偏置并实现了SOTA性能。

Conclusion: 该论文提出MDI用于量化模态主导性，并基于此构建MDACL框架，从优化层面缓解RGB-IR模态不对称带来的训练偏置，最终提高跨模态检测性能。

Abstract: RGB-Infrared (RGB-IR) multimodal perception is fundamental to embodied multimedia systems operating in complex physical environments. Although recent cross-modal fusion methods have advanced RGB-IR detection, the optimization dynamics caused by asymmetric modality characteristics remain underexplored. In practice, disparities in information density and feature quality introduce persistent optimization bias, leading training to overemphasize a dominant modality and hindering effective fusion. To quantify this phenomenon, we propose the Modality Dominance Index (MDI), which measures modality dominance by jointly modeling feature entropy and gradient contribution. Based on MDI, we develop a Modality Dominance-Aware Cross-modal Learning (MDACL) framework that regulates cross-modal optimization. MDACL incorporates Hierarchical Cross-modal Guidance (HCG) to enhance feature alignment and Adversarial Equilibrium Regularization (AER) to balance optimization dynamics during fusion. Extensive experiments on three RGB-IR benchmarks demonstrate that MDACL effectively mitigates optimization bias and achieves SOTA performance.

</details>


### [53] [Noise-Robust Tiny Object Localization with Flows](https://arxiv.org/abs/2601.00617)
*Huixin Sun,Linlin Yang,Ronyu Chen,Kerui Gu,Baochang Zhang,Angela Yao,Xianbin Cao*

Main category: cs.CV

TL;DR: 针对tiny对象标注噪声敏感性，提出TOLF：基于正则化流的误差建模与不确定性引导的梯度调制，从而在噪声监督下更稳健地学习定位，能建模非高斯分布并抑制高不确定样本学习，在多数据集上显著提升AP（AI-TOD上提升1.2%）。


<details>
  <summary>Details</summary>
Motivation: 现有检测器对tiny目标性能远落后于常规模型；分析发现tiny目标对标注噪声高度敏感，严格定位目标容易过拟合噪声，需更鲁棒的定位方法。

Method: 提出TOLF：使用正则化流（normalizing flows）对定位误差分布进行灵活建模以捕捉复杂、非高斯误差，并估计不确定性；基于不确定性的梯度调制策略降低高不确定（噪声倾向）样本的学习权重，从而抑制噪声过拟合并稳定训练。

Result: 在三个数据集上广泛实验验证方法有效性；在AI-TOD上，TOLF使DINO基线提升1.2% AP。

Conclusion: TOLF通过流式误差建模与不确定性引导的优化，为tiny目标定位在噪声监督下提供了一种更稳健的解决方案，显著提升检测性能。

Abstract: Despite significant advances in generic object detection, a persistent performance gap remains for tiny objects compared to normal-scale objects. We demonstrate that tiny objects are highly sensitive to annotation noise, where optimizing strict localization objectives risks noise overfitting. To address this, we propose Tiny Object Localization with Flows (TOLF), a noise-robust localization framework leveraging normalizing flows for flexible error modeling and uncertainty-guided optimization. Our method captures complex, non-Gaussian prediction distributions through flow-based error modeling, enabling robust learning under noisy supervision. An uncertainty-aware gradient modulation mechanism further suppresses learning from high-uncertainty, noise-prone samples, mitigating overfitting while stabilizing training. Extensive experiments across three datasets validate our approach's effectiveness. Especially, TOLF boosts the DINO baseline by 1.2% AP on the AI-TOD dataset.

</details>


### [54] [RePose: A Real-Time 3D Human Pose Estimation and Biomechanical Analysis Framework for Rehabilitation](https://arxiv.org/abs/2601.00625)
*Junxiao Xue,Pavel Smirnov,Ziao Li,Yunyun Shi,Shi Chen,Xinyi Yin,Xiaohan Yue,Lei Wang,Yiduo Wang,Feng Lin,Yijia Chen,Xiao Ma,Xiaoran Yan,Qing Zhang,Fengjian Xue,Xuecheng Wu*

Main category: cs.CV

TL;DR: 本文提出RePose，一个面向康复训练的实时3D人体姿态估计与动作分析系统。系统使用多摄像头RGB视频，结合快速多人体跟踪、改进的SmoothNet平滑模块与Unity可视化，实现低延迟姿态估计、姿态修正与肌肉应力实时显示。


<details>
  <summary>Details</summary>
Motivation: 康复训练中需要实时、准确的动作监测与纠正以辅助患者恢复肌肉力量与运动功能，现有方法在实时性、多人干扰场景的跟踪性能以及姿态平滑还存在不足。

Method: 1) 设计统一的端到端实时管线：多摄像头RGB输入，3D姿态估计与动作分析；2) 提出用于医疗康复场景的快速跟踪方法，单帧跟踪耗时<1ms，能应对多人干扰；3) 修改SmoothNet以适配实时姿态估计，降低估计误差并恢复真实运动状态；4) 在Unity中实现实时监控、评估与肌肉应力可视化，提供即时反馈与指导。

Result: 系统实现了实时性要求（跟踪<1ms），改进的平滑模块提升了姿态稳定性并降低误差，多摄像头管线与Unity平台实现了可视化评估与肌肉应力展示，能为康复训练提供即时反馈。

Conclusion: RePose为康复训练场景提供了一套实时、实用的3D姿态估计与动作分析解决方案，具备高效跟踪、误差抑制与可视化能力，可辅助患者正确完成训练并优化康复效果。

Abstract: We propose a real-time 3D human pose estimation and motion analysis method termed RePose for rehabilitation training. It is capable of real-time monitoring and evaluation of patients'motion during rehabilitation, providing immediate feedback and guidance to assist patients in executing rehabilitation exercises correctly. Firstly, we introduce a unified pipeline for end-to-end real-time human pose estimation and motion analysis using RGB video input from multiple cameras which can be applied to the field of rehabilitation training. The pipeline can help to monitor and correct patients'actions, thus aiding them in regaining muscle strength and motor functions. Secondly, we propose a fast tracking method for medical rehabilitation scenarios with multiple-person interference, which requires less than 1ms for tracking for a single frame. Additionally, we modify SmoothNet for real-time posture estimation, effectively reducing pose estimation errors and restoring the patient's true motion state, making it visually smoother. Finally, we use Unity platform for real-time monitoring and evaluation of patients' motion during rehabilitation, and to display the muscle stress conditions to assist patients with their rehabilitation training.

</details>


### [55] [HyperPriv-EPN: Hypergraph Learning with Privileged Knowledge for Ependymoma Prognosis](https://arxiv.org/abs/2601.00626)
*Shuren Gabriel Yu,Sikang Ren,Yongji Tian*

Main category: cs.CV

TL;DR: 该论文提出HyperPriv-EPN，一种基于超图的LUPI框架，通过‘切断图策略’和双流蒸馏，让仅依赖术前MRI的学生模型从包含术后文本信息的教师图中学习语义社区结构，从而在推断时无需文本也能实现更好的诊断与生存分层。在311例多中心数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 术前Ependymoma预后对治疗决策关键，但术前MRI缺乏术后病理/手术报告等语义性特征，现有多模态方法在推理时若缺少文本就无法利用历史丰富术后信息。需要一种能在训练时借助特权信息（术后报告），推理时仅依赖影像的方法。

Method: 提出HyperPriv-EPN：1) 基于超图构建教师图（包含术后文本/手术信息增强的节点/超边）和学生图（仅术前MRI）；2) 采用共享编码器处理两图，并引入‘切断图策略’使图结构在两张图间差异显式化；3) 通过双流蒸馏——在结构级（社区/超边分布）和表征级对学生进行约束，训练学生从视觉特征“幻觉”出语义社区结构；4) 在多中心数据上评估分类/生存分层性能。

Result: 在311例多中心队列上，HyperPriv-EPN在诊断准确率和生存风险分层上均优于对比方法，达到或接近SOTA。学生模型在推理时无需文本即可表现出由教师提供的语义结构知识，多中心验证显示良好泛化。

Conclusion: HyperPriv-EPN成功将历史术后专家文本知识通过超图-蒸馏机制转移到术前影像模型，使得推理阶段无需特权信息亦能获益，从而为术前Ependymoma的诊断和生存预后提供了实用且可泛化的方法。

Abstract: Preoperative prognosis of Ependymoma is critical for treatment planning but challenging due to the lack of semantic insights in MRI compared to post-operative surgical reports. Existing multimodal methods fail to leverage this privileged text data when it is unavailable during inference. To bridge this gap, we propose HyperPriv-EPN, a hypergraph-based Learning Using Privileged Information (LUPI) framework. We introduce a Severed Graph Strategy, utilizing a shared encoder to process both a Teacher graph (enriched with privileged post-surgery information) and a Student graph (restricted to pre-operation data). Through dual-stream distillation, the Student learns to hallucinate semantic community structures from visual features alone. Validated on a multi-center cohort of 311 patients, HyperPriv-EPN achieves state-of-the-art diagnostic accuracy and survival stratification. This effectively transfers expert knowledge to the preoperative setting, unlocking the value of historical post-operative data to guide the diagnosis of new patients without requiring text at inference.

</details>


### [56] [Quality Detection of Stored Potatoes via Transfer Learning: A CNN and Vision Transformer Approach](https://arxiv.org/abs/2601.00645)
*Shrikant Kapse,Priyankkumar Dhrangdhariya,Priya Kedia,Manasi Patwardhan,Shankar Kausley,Soumyadipta Maiti,Beena Rai,Shirish Karande*

Main category: cs.CV

TL;DR: 使用ResNet、VGG、DenseNet和ViT等预训练网络，通过图像和重量随时间数据，成功实现了非破坏性的马铃薯发芽检测与粗粒度保质期/失重预测，DenseNet表现最优；细粒度预测受限于视觉差异微弱和样本量不足。


<details>
  <summary>Details</summary>
Motivation: 提出一种基于图像的深度学习方法，用于在储存期间非破坏性、可扩展地监测马铃薯的质量，解决发芽检测、失重估计和保质期预测等关键问题。

Method: 收集为期200天的图像及对应重量数据，在受控温湿条件下采样；基于预训练ResNet、VGG、DenseNet、ViT构建二分类（发芽与否）与多分类（失重/剩余保质期区间）模型，比较各种网络表现，评估不同类划分粒度对精度的影响。

Result: 构建了两类模型：高精度二分类器用于发芽检测（DenseNet在该任务上达98.03%准确率）；多类预测器用于估计失重和预测剩余保质期，在粗粒度（2-5类）下表现最好（准确率>89.83%），在细粒度（6-8类）时准确率下降。

Conclusion: 图像驱动的模型可被整合进自动分拣与库存系统，实现早期识别发芽马铃薯和基于储存阶段的动态分类，从而改进库存管理、差异化定价并减少食品浪费；未来需在更多品种和储存条件下训练泛化性更强的模型。

Abstract: Image-based deep learning provides a non-invasive, scalable solution for monitoring potato quality during storage, addressing key challenges such as sprout detection, weight loss estimation, and shelf-life prediction. In this study, images and corresponding weight data were collected over a 200-day period under controlled temperature and humidity conditions. Leveraging powerful pre-trained architectures of ResNet, VGG, DenseNet, and Vision Transformer (ViT), we designed two specialized models: (1) a high-precision binary classifier for sprout detection, and (2) an advanced multi-class predictor to estimate weight loss and forecast remaining shelf-life with remarkable accuracy. DenseNet achieved exceptional performance, with 98.03% accuracy in sprout detection. Shelf-life prediction models performed best with coarse class divisions (2-5 classes), achieving over 89.83% accuracy, while accuracy declined for finer divisions (6-8 classes) due to subtle visual differences and limited data per class. These findings demonstrate the feasibility of integrating image-based models into automated sorting and inventory systems, enabling early identification of sprouted potatoes and dynamic categorization based on storage stage. Practical implications include improved inventory management, differential pricing strategies, and reduced food waste across supply chains. While predicting exact shelf-life intervals remains challenging, focusing on broader class divisions ensures robust performance. Future research should aim to develop generalized models trained on diverse potato varieties and storage conditions to enhance adaptability and scalability. Overall, this approach offers a cost-effective, non-destructive method for quality assessment, supporting efficiency and sustainability in potato storage and distribution.

</details>


### [57] [Reconstructing Building Height from Spaceborne TomoSAR Point Clouds Using a Dual-Topology Network](https://arxiv.org/abs/2601.00658)
*Zhaiyu Chen,Yuanyuan Wang,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 首次提出从原始TomoSAR点云直接生成城市建筑高度图的双拓扑学习框架，通过点-栅格联合去噪与插值，在慕尼黑和柏林验证有效，并可与光学影像联合提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统光学方法在天气/照明条件下受限，而TomoSAR提供天气无关且获取立面结构的侧视观测，但原始TomoSAR点云存在噪声、点分布各向异性以及非相干面数据空洞，影响高度重建精度，因此需要专门的处理与学习框架来直接从TomoSAR点云生成连续建筑高度图。

Method: 提出双拓扑网络：交替运行的点分支（建模不规则散射体特征）和栅格分支（强制空间一致性）。网络联合处理两种表示以去噪并对缺失区域进行插补，输出连续高度估计。实现上可能使用点云网络（如PointNet/PointNet++变体）与卷积/UNet风格的栅格网络，通过跨分支融合模块交换特征。

Result: 在慕尼黑和柏林的大规模数据上进行了广泛实验，结果显示提出方法能够有效去噪并填补空洞，生成高质量、高分辨率的建筑高度地图；此外，融合光学卫星影像能进一步提升重建质量。作者已开源代码。

Conclusion: 本文提出了一个从空间SAR层析(TomoSAR)点云直接生成高分辨率建筑高度图的学习框架，通过联合点云和栅格两类拓扑结构，解决噪声、各向异性分布和数据空洞问题，实验在慕尼黑和柏林验证了方法有效性，并可扩展融合光学影像。

Abstract: Reliable building height estimation is essential for various urban applications. Spaceborne SAR tomography (TomoSAR) provides weather-independent, side-looking observations that capture facade-level structure, offering a promising alternative to conventional optical methods. However, TomoSAR point clouds often suffer from noise, anisotropic point distributions, and data voids on incoherent surfaces, all of which hinder accurate height reconstruction. To address these challenges, we introduce a learning-based framework for converting raw TomoSAR points into high-resolution building height maps. Our dual-topology network alternates between a point branch that models irregular scatterer features and a grid branch that enforces spatial consistency. By jointly processing these representations, the network denoises the input points and inpaints missing regions to produce continuous height estimates. To our knowledge, this is the first proof of concept for large-scale urban height mapping directly from TomoSAR point clouds. Extensive experiments on data from Munich and Berlin validate the effectiveness of our approach. Moreover, we demonstrate that our framework can be extended to incorporate optical satellite imagery, further enhancing reconstruction quality. The source code is available at https://github.com/zhu-xlab/tomosar2height.

</details>


### [58] [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/abs/2601.00659)
*Neeraj Anand,Samyak Jha,Udbhav Bamba,Rahul Rahaman*

Main category: cs.CV

TL;DR: CRoPS: create hallucinated models by removing key text tokens and combine multiple such models via Generalized Contrastive Decoding to mitigate LVLM hallucinations without training, achieving strong empirical gains


<details>
  <summary>Details</summary>
Motivation: Existing training-free methods for LVLM hallucination mitigation have narrow assumptions and weaken at later decoding steps; removing visual tokens alone is insufficient because visual info still propagates to text, so need better hallucinated models

Method: Propose a training-free hallucination mitigation framework named CRoPS

Result: CRoPS selectively removes key text tokens to model hallucinations, introduces Generalized Contrastive Decoding integrating multiple hallucinated models, and improves CHAIR by 20% with consistent gains across six benchmarks and three LVLM families, outperforming SOTA training-free methods

Conclusion: Selective text-token removal plus combining diverse hallucinated models yields an effective training-free approach to reduce hallucinations in LVLMs, improving accuracy and robustness across datasets and models.

Abstract: Despite the rapid success of Large Vision-Language Models (LVLMs), a persistent challenge is their tendency to generate hallucinated content, undermining reliability in real-world use. Existing training-free methods address hallucinations but face two limitations: (i) they rely on narrow assumptions about hallucination sources, and (ii) their effectiveness declines toward the end of generation, where hallucinations are most likely to occur. A common strategy is to build hallucinated models by completely or partially removing visual tokens and contrasting them with the original model. Yet, this alone proves insufficient, since visual information still propagates into generated text. Building on this insight, we propose a novel hallucinated model that captures hallucination effects by selectively removing key text tokens. We further introduce Generalized Contrastive Decoding, which integrates multiple hallucinated models to represent diverse hallucination sources. Together, these ideas form CRoPS, a training-free hallucination mitigation framework that improves CHAIR scores by 20% and achieves consistent gains across six benchmarks and three LVLM families, outperforming state-of-the-art training-free methods.

</details>


### [59] [Pixel-to-4D: Camera-Controlled Image-to-Video Generation with Dynamic 3D Gaussians](https://arxiv.org/abs/2601.00678)
*Melonie de Almeida,Daniela Ivanova,Tong Shi,John H. Williamson,Paul Henderson*

Main category: cs.CV

TL;DR: 提出用3D高斯场景表示结合一次性采样对象运动的单图像到视频生成方法，实现了高质量、可控且高效的相机引导视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有单图像到视频的方法虽然在时间连贯性和3D一致性上有进步，但在用户可控（如相机路径修改）方面能力不足；基于显式3D中间表示可以提升相机控制下的视频一致性，但点云渲染加后期运动注入仍难以保证完整的时序一致性。

Method: 方法构建了一个3D高斯场景表示（Gaussian scene representation），直接在该表示上采样物体运动并渲染视频帧，从而避免了基于点云的两阶段流程和迭代去噪注入运动的过程，实现了快速的相机引导视频生成。

Result: 在KITTI、Waymo、RealEstate10K和DL3DV-10K上进行的大量实验表明，该方法在视频质量和推理效率上均达到或优于现有最先进方法。

Conclusion: 该论文提出了一种基于3D高斯场景表示的单图像到视频生成框架，能够在单次前向推理中采样合理的物体运动并根据给定相机轨迹生成一致的动态图像，克服了现有方法在相机可控性、时序一致性和几何完整性方面的不足。

Abstract: Humans excel at forecasting the future dynamics of a scene given just a single image. Video generation models that can mimic this ability are an essential component for intelligent systems. Recent approaches have improved temporal coherence and 3D consistency in single-image-conditioned video generation. However, these methods often lack robust user controllability, such as modifying the camera path, limiting their applicability in real-world applications. Most existing camera-controlled image-to-video models struggle with accurately modeling camera motion, maintaining temporal consistency, and preserving geometric integrity. Leveraging explicit intermediate 3D representations offers a promising solution by enabling coherent video generation aligned with a given camera trajectory. Although these methods often use 3D point clouds to render scenes and introduce object motion in a later stage, this two-step process still falls short in achieving full temporal consistency, despite allowing precise control over camera movement. We propose a novel framework that constructs a 3D Gaussian scene representation and samples plausible object motion, given a single image in a single forward pass. This enables fast, camera-guided video generation without the need for iterative denoising to inject object motion into render frames. Extensive experiments on the KITTI, Waymo, RealEstate10K and DL3DV-10K datasets demonstrate that our method achieves state-of-the-art video quality and inference efficiency. The project page is available at https://melonienimasha.github.io/Pixel-to-4D-Website.

</details>


### [60] [Efficient Deep Demosaicing with Spatially Downsampled Isotropic Networks](https://arxiv.org/abs/2601.00703)
*Cory Fan,Wenchao Zhang*

Main category: cs.CV

TL;DR: 本文挑战常见做法，证明对各向同性马赛克消除网络进行显著空间下采样能提高移动端效率与恢复效果，提出并验证了下采样网络JD3Net。


<details>
  <summary>Details</summary>
Motivation: 移动平台对计算和能耗有严格限制，而现有多数各向同性（残差-in-残差）网络完全避免空间下采样，导致在移动端难以部署；因此探索是否适度下采样可以提高效率且不损失甚至提升恢复质量。

Method: 使用改编自DeepMAD的数学架构设计技术，构建了有下采样和无下采样的简单全卷积网络，并对比它们在马赛克消除和联合去噪-马赛克消除任务上的表现；提出了下采样变体JD3Net并进行广泛实验评估。

Result: 实验表明，相比完全不下采样的各向同性网络，加入显著下采样的模型在计算效率和若干基准数据集上的恢复性能上都有提升；JD3Net在多种马赛克消除和联合去噪-马赛克消除任务上表现良好。

Conclusion: 作者主张在各向同性残差网络中引入较大比例的空间下采样能提升性能和效率，并通过设计对比实验验证了这一点。

Abstract: In digital imaging, image demosaicing is a crucial first step which recovers the RGB information from a color filter array (CFA). Oftentimes, deep learning is utilized to perform image demosaicing. Given that most modern digital imaging applications occur on mobile platforms, applying deep learning to demosaicing requires lightweight and efficient networks. Isotropic networks, also known as residual-in-residual networks, have been often employed for image demosaicing and joint-demosaicing-and-denoising (JDD). Most demosaicing isotropic networks avoid spatial downsampling entirely, and thus are often prohibitively expensive computationally for mobile applications. Contrary to previous isotropic network designs, this paper claims that spatial downsampling to a signficant degree can improve the efficiency and performance of isotropic networks. To validate this claim, we design simple fully convolutional networks with and without downsampling using a mathematical architecture design technique adapted from DeepMAD, and find that downsampling improves empirical performance. Additionally, empirical testing of the downsampled variant, JD3Net, of our fully convolutional networks reveals strong empirical performance on a variety of image demosaicing and JDD tasks.

</details>


### [61] [RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization](https://arxiv.org/abs/2601.00705)
*Wei-Tse Cheng,Yen-Jen Chiou,Yuan-Fu Yang*

Main category: cs.CV

TL;DR: 用DINOv3对应点一次性三角化生成高斯初值，替代残差增密，提升稳定性与收敛速度≈20%，在TUM和Replica上实现有竞争力的定位/重建并可达925 FPS。


<details>
  <summary>Details</summary>
Motivation: 原有GS-SLAM依靠残差逐步添加高斯导致早期建图不稳定且收敛慢，尤其在纹理丰富或场景杂乱时表现欠佳；希望通过基于对应点的初始化提供更好初值，提高稳定性与收敛速度。

Method: 使用DINOv3描述子提取密集多视图对应点，对应点通过置信度感知的内点分类器过滤后进行一次性三角化，生成分布均匀且结构感知的高斯种子；随后进入常规的高斯优化流程（Gaussian-splatting优化），替代原先的残差增密阶段。

Result: 在TUM RGB-D和Replica数据集上，RGS-SLAM在定位与重建精度上与最先进的高斯或点云SLAM系统相比表现竞争或更优；收敛速度提升约20%，并在实时映射下达到最高925 FPS。

Conclusion: RGS-SLAM通过用训练免费的一次性三角化初始化替代GS-SLAM的残差驱动增密，提升了早期建图稳定性并加速收敛，从而在复杂纹理与杂乱场景中获得更高的渲染保真度，同时与现有GS-SLAM管线兼容。

Abstract: We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS.

</details>


### [62] [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/abs/2601.00716)
*Hao Guan,Li Zhou*

Main category: cs.CV

TL;DR: 提出DomainSAT工具箱用于输入级漂移检测，并引入无标签置信度退化指标；实验表明两者结合能有效监测病理VLM在数据漂移下的性能退化。


<details>
  <summary>Details</summary>
Motivation: 动机在于实际部署的VLM在遇到与开发阶段不同的数据分布时可能出现性能下降，而缺乏标注数据使得监测模型退化变得困难；因此需要既能检测输入分布变化又能在无标签条件下估计性能退化的手段。

Method: 方法包括两条主线：1) 构建并使用DomainSAT工具箱对输入数据分布漂移进行系统化分析，整合多种代表性漂移检测算法并提供图形界面用于交互探索；2) 提出一种无标签的基于置信度的输出级退化指标，用于直接捕捉模型预测置信度的变化，并将其与输入级检测结果结合进行可靠性监控。实验在大规模病理肿瘤分类数据集上评估。

Result: 结果显示：输入分布漂移检测能有效识别分布变化并提供早期诊断信号，但并不总是与性能退化一致；所提出的基于置信度的输出指标与实际性能退化高度相关；将两者结合能更可靠地检测和解释VLM在数据漂移下的性能退化。

Conclusion: 本文结论是：在病理视觉-语言模型（VLM）面临数据分布漂移时，输入层面的分布变化检测与基于输出置信度的退化指标互为补充，二者结合能更可靠地识别模型性能下降并提供解释性信号。

Abstract: Vision-Language Models have demonstrated strong potential in medical image analysis and disease diagnosis. However, after deployment, their performance may deteriorate when the input data distribution shifts from that observed during development. Detecting such performance degradation is essential for clinical reliability, yet remains challenging for large pre-trained VLMs operating without labeled data. In this study, we investigate performance degradation detection under data shift in a state-of-the-art pathology VLM. We examine both input-level data shift and output-level prediction behavior to understand their respective roles in monitoring model reliability. To facilitate systematic analysis of input data shift, we develop DomainSAT, a lightweight toolbox with a graphical interface that integrates representative shift detection algorithms and enables intuitive exploration of data shift. Our analysis shows that while input data shift detection is effective at identifying distributional changes and providing early diagnostic signals, it does not always correspond to actual performance degradation. Motivated by this observation, we further study output-based monitoring and introduce a label-free, confidence-based degradation indicator that directly captures changes in model prediction confidence. We find that this indicator exhibits a close relationship with performance degradation and serves as an effective complement to input shift detection. Experiments on a large-scale pathology dataset for tumor classification demonstrate that combining input data shift detection and output confidence-based indicators enables more reliable detection and interpretation of performance degradation in VLMs under data shift. These findings provide a practical and complementary framework for monitoring the reliability of foundation models in digital pathology.

</details>


### [63] [Multi-Level Feature Fusion for Continual Learning in Visual Quality Inspection](https://arxiv.org/abs/2601.00725)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: MLFF通过融合预训练网络不同深度特征，实现低成本快速适应变化的质检任务，能匹配端到端性能并减少遗忘，适合再制造等动态场景。


<details>
  <summary>Details</summary>
Motivation: 再制造和类似波动较大的制造场景中，产品和缺陷模式频繁变化，已部署的深度模型需要频繁高效地适应新条件，同时避免灾难性遗忘。全量端到端微调计算成本高且易遗忘，故需一种既高效又稳健的适配策略。

Method: 作者采用预训练网络并从多个层级抽取特征，使用融合模块将这些多尺度表示整合以训练少量额外参数（而非端到端微调整个网络）。该方法在不同视觉质检任务上进行对比实验，评估训练参数量、性能、灾难性遗忘和对新领域/缺陷的泛化能力。

Result: MLFF在多个质检任务上达到与端到端训练相当的性能，但仅需显著更少的可训练参数；同时在持续学习场景下表现出更低的遗忘率，并在对新产品类型或缺陷的泛化测试中表现更稳健。

Conclusion: 本文提出的多层特征融合（MLFF）方法通过利用预训练网络不同深度的表示，能够在再制造场景下实现快速且高效的模型适应。实验表明，MLFF在训练可学习参数显著减少的同时，性能可匹配端到端训练，并在减少灾难性遗忘和提升对新产品/缺陷的泛化鲁棒性方面有优势。

Abstract: Deep neural networks show great potential for automating various visual quality inspection tasks in manufacturing. However, their applicability is limited in more volatile scenarios, such as remanufacturing, where the inspected products and defect patterns often change. In such settings, deployed models require frequent adaptation to novel conditions, effectively posing a continual learning problem. To enable quick adaptation, the necessary training processes must be computationally efficient while still avoiding effects like catastrophic forgetting. This work presents a multi-level feature fusion (MLFF) approach that aims to improve both aspects simultaneously by utilizing representations from different depths of a pretrained network. We show that our approach is able to match the performance of end-to-end training for different quality inspection problems while using significantly less trainable parameters. Furthermore, it reduces catastrophic forgetting and improves generalization robustness to new product types or defects.

</details>


### [64] [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/abs/2601.00730)
*Janez Perš,Jon Muhovič,Andrej Košir,Boštjan Murovec*

Main category: cs.CV

TL;DR: 提出一个以多模态LLM为后端、保留传统手写考试流程的可审计自动评分系统，在真实测验上与讲师评分接近（MAE≈8分），并通过多阶段与模板化设计提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 手写STEM考试能反映开放性推理与图示思路，但人工批改耗时且难以扩展，需一个保持传统考试流程且能自动化评分的可靠方案。

Method: 方法包括：教师仅提供手写参考答案和简短评分规则；将参考答案转为文本摘要用于条件评分而不暴露扫描件；多阶段设计保证可靠性：格式/存在性检测以防空答卷、独立评分员集成、监督聚合，以及使用刚性模板与确定性校验生成机器可解析的可审计报告。使用GPT-5.2和Gemini-3 Pro作为后端。

Result: 在斯洛文尼亚语真实课程测验（包含手绘电路图）上，冻结管线的表现为：与讲师评分的平均绝对差约8分，偏差低；在阈值D_max=40时估计需要人工复审的比例约17%。消融实验显示简单提示或移除参考答案会严重降低准确率并引入过度评分的系统性偏差，表明结构化提示与参考定锚至关重要。

Conclusion: 本文提出了一个端到端的评分流程，使用多模态大语言模型（含图像输入）对扫描的手写工程测验进行自动评分，在保持传统A4手写考试形式的同时实现可审计、可扩展的评分。

Abstract: Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\approx$17% at $D_{\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.

</details>


### [65] [Unified Primitive Proxies for Structured Shape Completion](https://arxiv.org/abs/2601.00759)
*Zhaiyu Chen,Yuqing Wang,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 提出UniCo，在单次前向推理中使用可学习的原型查询预测完整的几何原始元、语义和点内点关系，通过专用通路解码原始元并使用在线目标更新将原始元与点耦合。相比基线在多个基准和四种装配求解器上显著提升CD和法线一致性。


<details>
  <summary>Details</summary>
Motivation: 当前方法多采用级联策略让点和原始元交互；作者重新思考两者关系，认为应在专用路径中独立解码原始元并借助共享特征，这能更好地生成装配就绪的结构化表示。

Method: 模型使用“原始元代理”（learnable queries）作为上下文化的查询，在单次前向传递中预测一组原始元的完整几何、语义标签和点内点掩码。设计了一个专用的原始元解码通路，以便更有效地从共享形状特征中生成装配就绪输出，并通过在线目标更新在训练中将原始元与点耦合。评估包含合成与真实场景，并使用四种独立的装配求解器测量下游重建质量。

Result: UniCo: unified primitive-based structured shape completion

Conclusion: 将原始元解码放在专用路径并使用原始元代理与在线目标更新耦合，是从不完整点云恢复结构化3D形状的有效方法。UniCo在重建精度和法线一致性上优于现有方法，证明了该配方的实用性。

Abstract: Structured shape completion recovers missing geometry as primitives rather than as unstructured points, which enables primitive-based surface reconstruction. Instead of following the prevailing cascade, we rethink how primitives and points should interact, and find it more effective to decode primitives in a dedicated pathway that attends to shared shape features. Following this principle, we present UniCo, which in a single feed-forward pass predicts a set of primitives with complete geometry, semantics, and inlier membership. To drive this unified representation, we introduce primitive proxies, learnable queries that are contextualized to produce assembly-ready outputs. To ensure consistent optimization, our training strategy couples primitives and points with online target updates. Across synthetic and real-world benchmarks with four independent assembly solvers, UniCo consistently outperforms recent baselines, lowering Chamfer distance by up to 50% and improving normal consistency by up to 7%. These results establish an attractive recipe for structured 3D understanding from incomplete data. Project page: https://unico-completion.github.io.

</details>


### [66] [Fusion-SSAT: Unleashing the Potential of Self-supervised Auxiliary Task by Feature Fusion for Generalized Deepfake Detection](https://arxiv.org/abs/2601.00789)
*Shukesh Reddy,Srijan Das,Abhijit Das*

Main category: cs.CV

TL;DR: Generate a 'too long; didn't read' summary in one sentence.


<details>
  <summary>Details</summary>
Motivation: Please describe the motivation of the paper in one sentence.

Method: Please analyze the method used in the paper and give a concise but precise description.

Result: Please summarize the main experimental results in one short paragraph.

Conclusion: Provide a concise conclusion of the paper in one sentence.

Abstract: In this work, we attempted to unleash the potential of self-supervised learning as an auxiliary task that can optimise the primary task of generalised deepfake detection. To explore this, we examined different combinations of the training schemes for these tasks that can be most effective. Our findings reveal that fusing the feature representation from self-supervised auxiliary tasks is a powerful feature representation for the problem at hand. Such a representation can leverage the ultimate potential and bring in a unique representation of both the self-supervised and primary tasks, achieving better performance for the primary task. We experimented on a large set of datasets, which includes DF40, FaceForensics++, Celeb-DF, DFD, FaceShifter, UADFV, and our results showed better generalizability on cross-dataset evaluation when compared with current state-of-the-art detectors.

</details>


### [67] [Two Deep Learning Approaches for Automated Segmentation of Left Ventricle in Cine Cardiac MRI](https://arxiv.org/abs/2601.00794)
*Wenhui Chu,Nikolaos V. Tsekos*

Main category: cs.CV

TL;DR: 通过在U-Net中分别采用Layer Normalization（LNU-Net）与Instance-Batch融合归一化（IBU-Net），在805张心脏MRI上实现更好的左心室分割（Dice与APD指标提升）。


<details>
  <summary>Details</summary>
Motivation: 临床上左心室分割对心功能评估至关重要，常规U-Net在医疗影像小样本和强变形场景下可能受归一化方式影响，研究者希望通过改进归一化方法提升分割鲁棒性。

Method: 在经典U-Net骨架上，LNU-Net在每个卷积块内使用Layer Normalization；IBU-Net在第一卷积块内并行或顺序融合Instance Normalization与Batch Normalization并将结果传入下一层。网络采用下采样-上采样结构并结合仿射与弹性形变的数据增强进行训练。

Result: LNU-Net和IBU-Net在短轴cine MRI的左心室分割任务中取得了优于U-Net的性能，分别通过在卷积块中引入Layer Normalization或在首层融合Instance与Batch Normalization来提升特征归一化效果。结合仿射和弹性变换的数据增强策略，在805张来自45例患者的MRI图像上验证，两个模型在Dice系数和平均垂直距离上超越若干现有方法。

Conclusion: 引入不同归一化策略（LN或IB）能提高U-Net在左心室分割任务中的表现，特别是在小样本医疗影像数据上，通过适当的数据增强可进一步提升模型鲁棒性。

Abstract: Left ventricle (LV) segmentation is critical for clinical quantification and diagnosis of cardiac images. In this work, we propose two novel deep learning architectures called LNU-Net and IBU-Net for left ventricle segmentation from short-axis cine MRI images. LNU-Net is derived from layer normalization (LN) U-Net architecture, while IBU-Net is derived from the instance-batch normalized (IB) U-Net for medical image segmentation. The architectures of LNU-Net and IBU-Net have a down-sampling path for feature extraction and an up-sampling path for precise localization. We use the original U-Net as the basic segmentation approach and compared it with our proposed architectures. Both LNU-Net and IBU-Net have left ventricle segmentation methods: LNU-Net applies layer normalization in each convolutional block, while IBU-Net incorporates instance and batch normalization together in the first convolutional block and passes its result to the next layer. Our method incorporates affine transformations and elastic deformations for image data processing. Our dataset that contains 805 MRI images regarding the left ventricle from 45 patients is used for evaluation. We experimentally evaluate the results of the proposed approaches outperforming the dice coefficient and the average perpendicular distance than other state-of-the-art approaches.

</details>


### [68] [AdaGaR: Adaptive Gabor Representation for Dynamic Scene Reconstruction](https://arxiv.org/abs/2601.00796)
*Jiewen Chan,Zhenjun Zhao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出AdaGaR，一种用于从单目视频重建动态3D场景的框架。通过可学习频率权重和自适应能量补偿的可调谐Gabor表示代替高斯以兼顾高频细节与稳定性，并用带时域曲率正则的三次Hermite样条保证运动连续性。加入基于深度估计、点跟踪和前景掩码的自适应初始化以提高训练稳定性。实验在Tap-Vid和DAVIS上取得了领先指标，并展示了插帧、深度一致性、视频编辑和立体合成等泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有以单高斯基元为主的方法对细节捕捉能力受限（低通效应），标准Gabor虽然能表达高频但会引入能量不稳定，且缺乏时间连续性约束导致插值时出现运动伪影。因此需要一个既有频率自适应又保证时序平滑的动态显式场表示。

Method: 1) 提出Adaptive Gabor Representation：在高斯基础上引入可学习的频率权重与自适应能量补偿，平衡细节与能量稳定性；2) 时间上用三次Hermite样条（Cubic Hermite Splines）表示点云随时间的运动，并加入时域曲率正则化（Temporal Curvature Regularization）以约束运动平滑；3) 自适应初始化机制结合深度估计、点跟踪和前景掩码，建立训练初期的稳定点云分布；4) 整体框架为AdaGaR，并在多任务评估上进行测试。

Result: 在Tap-Vid和DAVIS数据集上取得PSNR 35.49、SSIM 0.9433、LPIPS 0.0723的领先结果，且在帧插值、深度一致性、视频编辑和立体视图合成等任务上表现出较好泛化能力。

Conclusion: AdaGaR通过频率自适应的Gabor表示与带曲率正则的时域样条，有效解决了细节丢失与运动伪影问题，实现了更稳定和高质量的动动态3D场重建，并在多个下游任务上展示良好性能。

Abstract: Reconstructing dynamic 3D scenes from monocular videos requires simultaneously capturing high-frequency appearance details and temporally continuous motion. Existing methods using single Gaussian primitives are limited by their low-pass filtering nature, while standard Gabor functions introduce energy instability. Moreover, lack of temporal continuity constraints often leads to motion artifacts during interpolation. We propose AdaGaR, a unified framework addressing both frequency adaptivity and temporal continuity in explicit dynamic scene modeling. We introduce Adaptive Gabor Representation, extending Gaussians through learnable frequency weights and adaptive energy compensation to balance detail capture and stability. For temporal continuity, we employ Cubic Hermite Splines with Temporal Curvature Regularization to ensure smooth motion evolution. An Adaptive Initialization mechanism combining depth estimation, point tracking, and foreground masks establishes stable point cloud distributions in early training. Experiments on Tap-Vid DAVIS demonstrate state-of-the-art performance (PSNR 35.49, SSIM 0.9433, LPIPS 0.0723) and strong generalization across frame interpolation, depth consistency, video editing, and stereo view synthesis. Project page: https://jiewenchan.github.io/AdaGaR/

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [From Metadata to Meaning: A Semantic Units Knowledge Graph for the Biodiversity Exploratories](https://arxiv.org/abs/2601.00002)
*Tarek Al Mustafa*

Main category: cs.DB

TL;DR: 提出并实现了将命名子图（语义单元）引入KG的思想，结合LLM与嵌入模型改善元数据结构化与丰富度，旨在提升生态领域KG的可用性与可发现性。


<details>
  <summary>Details</summary>
Motivation: KG虽对生态与生物多样性研究有巨大潜力，但SPARQL使用门槛高且KG设计侧重机器互操作性导致许多三元组对终端用户缺乏语义意义，需桥接用户需求与KG技术要求。

Method: 以德国Biodiversity Exploratories（BE）出版物和数据集元数据为源，构建知识图谱并实现首个SUs实现；使用大语言模型从标题与摘要中抽取结构化元数据类别，使用嵌入模型补充潜在元数据信息；评估SUs对SPARQL查询及用户交互的影响。

Result: 实现了基于BE元数据的KG与SUs原型，展示SUs在查询简化与语义聚合上的潜力；证明可用LLM自动抽取结构化元数据并用嵌入增强FAIR元数据创建流程。

Conclusion: 本文提出语义单元（Semantic Units，SUs）作为解决知识图谱（KG）与终端用户认知不匹配的手段，旨在提升认知互操作性并缓解建模复杂性。

Abstract: Knowledge Graphs (KGs) bear great potential for ecology and biodiversity researchers in their ability to support synthesis and integration efforts, meta-analyses, reasoning tasks, and overall machine interoperability of research data. However, this potential is yet to be realized as KGs are notoriously difficult to interact with via their query language SPARQL for many user groups alike. Additionally, a further hindrance for user-KG interaction is the fundamental disconnect between user requirements and requirements KGs have to fulfill regarding machine-interoperability, reasoning tasks, querying, and further technical requirements. Thus, many statements in a KG are of no semantic significance for end users. In this work, we investigate a potential remedy for this challenge: Semantic Units (SUs) are semantically significant, named subgraphs in a KG with the goal to enhance cognitive interoperability for users, and to provide responses to common KG modelling challenges. We model and construct a KG from publication and dataset metadata of the Biodiversity Exploratories (BE), a research platform for functional biodiversity research across research plots in Germany to contribute to biodiversity research from the perspective of computer science. We contribute further by delivering the first implementation of semantic units on a knowledge graph and investigate how SUs impact KG querying. Finally, we present two implementations of tasks that show how large language models (LLMs) can be used to extract structured metadata categories from publication and dataset titles and abstracts, and how embedding models can be used to enrich metadata with latent information, in an effort to support the creation of structured and FAIR (findable, accessible, interoperable, and reusable) metadata.

</details>


### [70] [Database Theory in Action: Yannakakis' Algorithm](https://arxiv.org/abs/2601.00098)
*Paraschos Koutris,Stijn Vansummeren,Qichen Wang,Yisu Remy Wang,Xiangyao Yu*

Main category: cs.DB

TL;DR: 综述近年来提升Yannakakis算法实用性的研究，梳理优化手段与实现经验，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管Yannakakis算法对无环连接查询具有最优性，但其在工业界和系统实现中鲜少采用，原因在于实际性能不佳和实现复杂，故需调研使其更实用的研究进展。

Method: 通过综述近年关于Yannakakis算法的工程优化、内存与I/O优化、并行与向量化实现、以及简化实现复杂度的工作，归纳各类技术路径与实验评估方法。

Result: 总结出若干使Yannakakis更实用的关键方向：改进预处理与筛选步骤、紧凑数据布局、增量与流式处理、并行化/向量化、易用的库与接口；并指出开放问题与未来研究方向。

Conclusion: 本文认为Yannakakis算法在理论最优的同时，实际应用受限，需改进实现与效率以推广应用。

Abstract: Yannakakis' seminal algorithm is optimal for acyclic joins, yet it has not been widely adopted due to its poor performance in practice. This paper briefly surveys recent advancements in making Yannakakis' algorithm more practical, in terms of both efficiency and ease of implementation, and points out several avenues for future research.

</details>


### [71] [Avoiding Thread Stalls and Switches in Key-Value Stores: New Latch-Free Techniques and More](https://arxiv.org/abs/2601.00208)
*David Lomet,Rui Wang*

Main category: cs.DB

TL;DR: 提出notices：一种针对B树维护的无锁增量通知机制，通过Delta记录更新减少无用工作和线程切换，从而在高并发下提高键值存储性能。


<details>
  <summary>Details</summary>
Motivation: 关键值存储系统在高负载下因资源竞争导致线程切换和停顿，严重影响性能。传统加锁会阻塞线程，而现有无锁方法若产生大量无用工作也限制了收益。作者希望设计一种既无锁又能有效减少浪费工作的机制。

Method: 作者基于Delta记录更新的思想，设计了一种名为notices的无锁机制：在修改B树节点时，不直接阻塞或切换线程，而是生成小型通知（包含必要的变更信息或补偿操作），使其他并发执行的线程能以增量方式应用或协助完成更新，避免重复执行，减少竞争导致的资源等待。

Result: 通过将notices用于B树索引维护，作者展示了可以在不引入线程切换或停顿的前提下完成节点更新和分裂等复杂操作，减少冲突时的重复工作，进而提升整体吞吐和延迟表现。论文还讨论了其它适合避免线程切换或停顿的场景与机会。

Conclusion: 本文提出的notices技术在无锁（latch-free）环境下通过对Delta记录更新并传递轻量级通知来减少无用工作，从而在B树索引维护中显著降低线程切换和停顿，提升性能。

Abstract: A significant impediment to high performance in key-value stores is the high cost of thread switching or stalls. While there are many sources for this, a major one is the contention for resources. And this cost increases with load as conflicting operations more frequently try to access data concurrently. Traditional latch-based approaches usually handle these situations by blocking one or more contending threads. Latch-free techniques can avoid this behavior. But the payoff may be limited if latch-free techniques require executing wasted work. In this paper, we show how latch-free techniques exploit delta record updating and can significantly reduce wasted work by using notices, a new latch-free approach. This paper explains how notices work and can solve B-tree index maintenance problems, while avoiding thread switches or stalls. Other opportunities for avoiding thread switches or stalls are also discussed.

</details>


### [72] [Combining Time-Series and Graph Data: A Survey of Existing Systems and Approaches](https://arxiv.org/abs/2601.00304)
*Mouna Ammar,Marvin Hofer,Erhard Rahm*

Main category: cs.DB

TL;DR: 综述并分类当前图与时间序列结合的系统，提出四类架构，比较其实现特性与权衡，帮助理解与选择适合的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的应用同时包含图结构与时间演化数据，缺乏系统性的分类与比较，导致用户难以在一致性、性能与可用性间做出选择，因此需要一个全面的概览帮助决策。

Method: 通过文献调研与系统分析，将现有方法和系统按架构特点归为四类，并从需求适配、实现特性（如数据模型、查询语言、存储与索引策略、延迟/吞吐优化）等维度对比分析。

Result: 给出四类架构分类，并为每类总结其优势与局限，以及在跨模型查询、实时性、可扩展性与开放性方面的表现。该综述为研究者和工程师在选择或设计图-时序融合系统时提供参考框架。

Conclusion: 本文综述了将图与时间序列数据结合的系统与方法，指出当前研究在跨模型集成程度、系统成熟度与开放性方面存在权衡，提出四类架构分类以帮助理解与评估选项与折衷。

Abstract: We provide a comprehensive overview of current approaches and systems for combining graphs and time series data. We categorize existing systems into four architectural categories and analyze how these systems meet different requirements and exhibit distinct implementation characteristics to support both data types in a unified manner. Our overview aims to help readers understand and evaluate current options and trade-offs, such as the degree of cross-model integration, maturity, and openness.

</details>


### [73] [KELP: Robust Online Log Parsing Through Evolutionary Grouping Trees](https://arxiv.org/abs/2601.00633)
*Satyam Singh,Sai Niranjan Ramachandran*

Main category: cs.DB

TL;DR: KELP通过Evolutionary Grouping Tree把模板发现变为持续的在线聚类，动态分裂/合并节点以适应日志模式漂移，并配套引入更真实的基准，实验证明在准确率和吞吐量上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有在线日志解析器依赖静态模板或启发式规则，无法适应生产环境中的模式漂移，会导致解析失败、告警丢失和运维成本增加，因此需要一种能在线自适应的解析方法和更真实的评估基准。

Method: 将日志模板发现视作连续的在线聚类过程，使用Evolutionary Grouping Tree对到达的日志条目进行增量组织与重构；节点根据统计频率分布触发分裂、合并和根节点重评估，从而避免静态规则的局限。

Result: 在作者提出的新基准上，KELP在保持高吞吐量的同时显著优于传统启发式方法，能够在结构模糊和模板漂移场景下维持高准确率。

Conclusion: KELP是一种针对生产环境中日志模式漂移问题设计的在线、高吞吐量解析器，其核心贡献是引入Evolutionary Grouping Tree数据结构，实现基于频率分布的动态聚类，通过节点的分裂与合并持续自适应模板变化。

Abstract: Real-time log analysis is the cornerstone of observability for modern infrastructure. However, existing online parsers are architecturally unsuited for the dynamism of production environments. Built on fundamentally static template models, they are dangerously brittle: minor schema drifts silently break parsing pipelines, leading to lost alerts and operational toil. We propose \textbf{KELP} (\textbf{K}elp \textbf{E}volutionary \textbf{L}og \textbf{P}arser), a high-throughput parser built on a novel data structure: the Evolutionary Grouping Tree. Unlike heuristic approaches that rely on fixed rules, KELP treats template discovery as a continuous online clustering process. As logs arrive, the tree structure evolves, nodes split, merge, and re-evaluate roots based on changing frequency distributions. Validating this adaptability requires a dataset that models realistic production complexity, yet we identify that standard benchmarks rely on static, regex-based ground truths that fail to reflect this. To enable rigorous evaluation, we introduce a new benchmark designed to reflect the structural ambiguity of modern production systems. Our evaluation demonstrates that KELP maintains high accuracy on this rigorous dataset where traditional heuristic methods fail, without compromising throughput. Our code and dataset can be found at codeberg.org/stonebucklabs/kelp

</details>


### [74] [DeXOR: Enabling XOR in Decimal Space for Streaming Lossless Compression of Floating-point Data](https://arxiv.org/abs/2601.00695)
*Chuanyi Lv,Huan Li,Dingyu Yang,Zhongle Xie,Lu Chen,Christian S. Jensen*

Main category: cs.DB

TL;DR: DeXOR在十进制空间用XOR式前后缀复用并结合容错十进制比特管理与异常处理，大幅提升浮点流压缩性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 流式浮点数越来越常见，需在兼顾高精度与无平滑性等极端情况下实现高效压缩，现有方案在某些极端场景下失效或效率不足。

Method: 提出了decimal XOR框架：在十进制空间识别最长公共前缀和后缀并复用；采用scaled truncation与容错舍入处理二进制-十进制转换误差；引入针对十进制XOR优化的位管理策略；并实现异常处理器管理浮点指数。

Result: 在22个数据集上，DeXOR平均提升压缩比15%，解压速度提升20%，压缩速度保持竞争力；同时在极端条件下比对手更稳定可扩展。

Conclusion: DeXOR通过在十进制空间进行XOR式前后缀复用和冗余消除，显著提升了浮点流数据压缩的比率与解压速度，在各种数据集和极端条件下都表现出鲁棒性。

Abstract: With streaming floating-point numbers being increasingly prevalent, effective and efficient compression of such data is critical. Compression schemes must be able to exploit the similarity, or smoothness, of consecutive numbers and must be able to contend with extreme conditions, such as high-precision values or the absence of smoothness. We present DeXOR, a novel framework that enables decimal XOR procedure to encode decimal-space longest common prefixes and suffixes, achieving optimal prefix reuse and effective redundancy elimination. To ensure accurate and low-cost decompression even with binary-decimal conversion errors, DeXOR incorporates 1) scaled truncation with error-tolerant rounding and 2) different bit management strategies optimized for decimal XOR. Additionally, a robust exception handler enhances stability by managing floating-point exponents, maintaining high compression ratios under extreme conditions. In evaluations across 22 datasets, DeXOR surpasses state-of-the-art schemes, achieving a 15% higher compression ratio and a 20% faster decompression speed while maintaining a competitive compression speed. DeXOR also offers scalability under varying conditions and exhibits robustness in extreme scenarios where other schemes fail.

</details>
