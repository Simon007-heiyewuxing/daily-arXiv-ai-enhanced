<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [FedeCouple: Fine-Grained Balancing of Global-Generalization and Local-Adaptability in Federated Learning](https://arxiv.org/abs/2511.09599)
*Ming Yang,Dongrun Li,Xin Wang,Feng Li,Lisheng Fan,Chunxiao Wang,Xiaoming Wu,Peng Cheng*

Main category: cs.CV

TL;DR: FedeCouple通过细粒度平衡全局泛化与本地适应、动态蒸馏和本地锚点，提升个性化联邦学习性能，兼顾隐私与通信效率并具备收敛性理论证据。


<details>
  <summary>Details</summary>
Motivation: 在异构客户端数据下，现有个性化联邦学习多关注特征一致性与局部分类个性化，但忽视提取器的本地适应性与分类器的全局泛化，导致组件之间耦合弱、性能下降。

Method: 联合训练全局和本地特征提取器与个性化分类器；使用动态知识蒸馏强化分类器的全局泛化；在客户端引入本地锚点以精炼特征空间且不传输以保护隐私；算法在通信轮次间同步全局参数并保留本地私有参数。

Result: 在五个图像分类数据集上，FedeCouple在有效性、稳定性、可扩展性和安全性方面均优于九个基线方法；在效果对比中平均优于最佳基线4.3%。并给出非凸目标下的收敛性证明。

Conclusion: FedeCouple实现了在个性化联邦学习中平衡全局泛化与局部适应，通过联合学习全局与本地特征表示、动态知识蒸馏和引入本地锚点，提升了模型性能并保持隐私与通信效率，收敛性在非凸目标下有理论保证。

Abstract: In privacy-preserving mobile network transmission scenarios with heterogeneous client data, personalized federated learning methods that decouple feature extractors and classifiers have demonstrated notable advantages in enhancing learning capability. However, many existing approaches primarily focus on feature space consistency and classification personalization during local training, often neglecting the local adaptability of the extractor and the global generalization of the classifier. This oversight results in insufficient coordination and weak coupling between the components, ultimately degrading the overall model performance. To address this challenge, we propose FedeCouple, a federated learning method that balances global generalization and local adaptability at a fine-grained level. Our approach jointly learns global and local feature representations while employing dynamic knowledge distillation to enhance the generalization of personalized classifiers. We further introduce anchors to refine the feature space; their strict locality and non-transmission inherently preserve privacy and reduce communication overhead. Furthermore, we provide a theoretical analysis proving that FedeCouple converges for nonconvex objectives, with iterates approaching a stationary point as the number of communication rounds increases. Extensive experiments conducted on five image-classification datasets demonstrate that FedeCouple consistently outperforms nine baseline methods in effectiveness, stability, scalability, and security. Notably, in experiments evaluating effectiveness, FedeCouple surpasses the best baseline by a significant margin of 4.3%.

</details>


### [2] [MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation](https://arxiv.org/abs/2511.09611)
*Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li*

Main category: cs.CV

TL;DR: paper finds autoregressive reasoning harms image synthesis via error propagation; proposes ParaBench, MMaDA-Parallel, ParaRL to improve cross-modal alignment, achieving +6.9% Output Alignment


<details>
  <summary>Details</summary>
Motivation: identify failure mode in thinking-aware generation; improve cross-modal alignment

Method: analysis of method

Result: propose ParaBench benchmark; MMaDA-Parallel framework; ParaRL; 6.9% improvement over Bagel

Conclusion: parallel multimodal diffusion with trajectory-level semantic rewards improves thinking-aware image synthesis and alignment

Abstract: While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at https://github.com/tyfeld/MMaDA-Parallel

</details>


### [3] [PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild](https://arxiv.org/abs/2511.09675)
*Felix B. Mueller,Jan F. Meier,Timo Lueddecke,Richard Vogg,Roger L. Freixanet,Valentin Hassler,Tiffany Bosshard,Elif Karakoc,William J. O'Hearn,Sofia M. Pereira,Sandro Sehner,Kaja Wierucka,Judith Burkart,Claudia Fichtel,Julia Fischer,Alexander Gail,Catherine Hobaiter,Julia Ostner,Liran Samuni,Oliver Schülke,Neda Shahidi,Erin G. Wessling,Alexander S. Ecker*

Main category: cs.CV

TL;DR: 提出PriVi：一个包含424小时以灵长类为中心的视频预训练数据集（174小时实验行为+250小时网络视频），并用V-JEPA在其上进行无监督/自监督预训练，结合轻量冻结分类器，在四个基准数据集上显著优于以往方法，尤其在低标签情境下表现出更好的泛化和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法过度依赖以人类为中心的预训练模型且多集中于单一数据集，导致对灵长类视频分析泛化能力差；通过构建大规模灵长类专用预训练数据集并进行表征学习，可以提高模型在各种灵长类下游任务上的表现和数据效率。

Method: 构建PriVi数据集（174小时行为研究视频+250小时网络视频），通过可扩展的数据策划流程进行清洗与标注；在此数据集上用V-JEPA模型执行视频表征预训练；评估使用轻量的冻结线性/小型分类器在四个灵长类行为/检测基准上，不对主模型做大规模微调以测试表征泛化能力。

Result: Primatology-focused video pretraining dataset and model improves downstream primate behavior recognition

Conclusion: 以数据为中心的灵长类专用视频预训练能显著提升下游任务性能与标签效率，优于以往以人类预训练模型或单数据集方法为主的研究，表明构建大规模物种专用数据集是提高生态/行为学计算工具的有效路径。

Abstract: Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.

</details>


### [4] [Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression](https://arxiv.org/abs/2511.09702)
*Katie Matton,Purvaja Balaji,Hamzeh Ghasemzadeh,Jameson C. Cooper,Daryush D. Mehta,Jarrad H. Van Stan,Robert E. Hillman,Rosalind Picard,John Guttag,S. Mazdak Abulnaga*

Main category: cs.CV

TL;DR: 提出首个自动从声带图像分类语音创伤严重度的方法；采用序数回归并提出对软标签的损失修改以反映标注者分布；模型接近专家性能并给出校准不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 语音创伤严重度评估依赖昂贵且主观的专家判断，存在可靠性问题；需要自动化方法以实现大规模、客观和可重复的评估。

Method: 使用常见的序数回归框架，并创新性地修改序数回归损失以支持软标签（反映多标注者评分分布），在数据上训练深度模型进行严重度分类并输出不确定性估计。

Result: 软序数回归模型在预测性能上接近临床专家，同时产生良好校准的不确定性估计。

Conclusion: 所提出的软序数回归方法能在声带图像上有效评估语音创伤严重度，性能接近临床专家并能提供可信的不确定性估计，能促进大规模研究和临床应用。

Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.

</details>


### [5] [SliderEdit: Continuous Image Editing with Fine-Grained Instruction Control](https://arxiv.org/abs/2511.09715)
*Arman Zarei,Samyadeep Basu,Mobina Pournemat,Sayan Nag,Ryan Rossi,Soheil Feizi*

Main category: cs.CV

TL;DR: SliderEdit为指令式图像编辑引入可连续调节的全局滑块，使用共享低秩适配机制实现对多种编辑的细粒度强度控制，改善可控性和一致性，并适配现有模型如FLUX-Kontext与Qwen-Image-Edit。


<details>
  <summary>Details</summary>
Motivation: 解决指令驱动图像编辑中对每条指令强度无法连续精细控制的问题，提升用户对单独编辑的可控性和可解释性。

Method: 将多段编辑指令解耦为独立维度，对每个维度暴露一个全局训练的滑块；学习一组共享的低秩适配矩阵（LoRA-like）来调整模型响应，通过连续插值滑块系数实现强度控制；在保持空间局部性与全局语义一致性的前提下，将该机制集成到现有指令式图像编辑模型并验证效果。

Result: 提出SliderEdit框架：将复合指令分解为独立可调的全局滑块，通过学习一组低秩适配矩阵实现跨编辑、属性和组合指令的泛化，能在保持空间局部性和语义一致性的同时对单个编辑维度进行连续插值。

Conclusion: SliderEdit实现了首次在指令式图像编辑中对多条指令提供连续、可解释和组合式强度控制，提升了编辑可控性、视觉一致性和用户可引导性，为交互式指令驱动图像操作提供新范式。

Abstract: Instruction-based image editing models have recently achieved impressive performance, enabling complex edits to an input image from a multi-instruction prompt. However, these models apply each instruction in the prompt with a fixed strength, limiting the user's ability to precisely and continuously control the intensity of individual edits. We introduce SliderEdit, a framework for continuous image editing with fine-grained, interpretable instruction control. Given a multi-part edit instruction, SliderEdit disentangles the individual instructions and exposes each as a globally trained slider, allowing smooth adjustment of its strength. Unlike prior works that introduced slider-based attribute controls in text-to-image generation, typically requiring separate training or fine-tuning for each attribute or concept, our method learns a single set of low-rank adaptation matrices that generalize across diverse edits, attributes, and compositional instructions. This enables continuous interpolation along individual edit dimensions while preserving both spatial locality and global semantic consistency. We apply SliderEdit to state-of-the-art image editing models, including FLUX-Kontext and Qwen-Image-Edit, and observe substantial improvements in edit controllability, visual consistency, and user steerability. To the best of our knowledge, we are the first to explore and propose a framework for continuous, fine-grained instruction control in instruction-based image editing models. Our results pave the way for interactive, instruction-driven image manipulation with continuous and compositional control.

</details>


### [6] [Density Estimation and Crowd Counting](https://arxiv.org/abs/2511.09723)
*Balachandra Devarangadi Sunil,Rakshith Venkatesh,Shantanu Todmal*

Main category: cs.CV

TL;DR: 提出将图像级人群密度估计算法扩展到视频场景，采用扩散去噪模型生成高质量密度图，使用窄高斯核和多重密度输出，并加入回归分支与基于相似度的融合机制；通过基于Farneback光流的事件驱动采样选择关键帧以节省计算与存储。评估包括定量MAE与定性叠加图，结果表明能在稠密与稀疏场景下有效捕捉人群动态，并在采样效率上保持关键事件。


<details>
  <summary>Details</summary>
Motivation: 弥补原先仅针对单帧图像的人群密度估计算法在时间一致性和效率上的不足，使其适用于视频监控中需实时响应和节省计算资源的应用。

Method: 使用去噪扩散概率模型生成多张窄高斯核的密度图，加入回归分支提取精确特征，以相似度评分将多张密度图融合为最终输出；通过Farneback光流计算帧间运动，并基于运动事件进行采样以减少帧数。

Result: 在定量（MAE）与定性（叠加可视化）评估中，方法在稠密与稀疏人群场景均表现良好；事件驱动采样显著减少帧数同时保留关键人群事件，提升系统可扩展性与实时性。

Conclusion: 将扩散生成模型、窄核多输出与回归分支结合，并配合光流驱动的事件采样，可在视频环境中实现高质量、计算高效的人群密度估计，适用于实时监控场景。

Abstract: This study enhances a crowd density estimation algorithm originally designed for image-based analysis by adapting it for video-based scenarios. The proposed method integrates a denoising probabilistic model that utilizes diffusion processes to generate high-quality crowd density maps. To improve accuracy, narrow Gaussian kernels are employed, and multiple density map outputs are generated. A regression branch is incorporated into the model for precise feature extraction, while a consolidation mechanism combines these maps based on similarity scores to produce a robust final result. An event-driven sampling technique, utilizing the Farneback optical flow algorithm, is introduced to selectively capture frames showing significant crowd movements, reducing computational load and storage by focusing on critical crowd dynamics. Through qualitative and quantitative evaluations, including overlay plots and Mean Absolute Error (MAE), the model demonstrates its ability to effectively capture crowd dynamics in both dense and sparse settings. The efficiency of the sampling method is further assessed, showcasing its capability to decrease frame counts while maintaining essential crowd events. By addressing the temporal challenges unique to video analysis, this work offers a scalable and efficient framework for real-time crowd monitoring in applications such as public safety, disaster response, and event management.

</details>


### [7] [PALMS+: Modular Image-Based Floor Plan Localization Leveraging Depth Foundation Model](https://arxiv.org/abs/2511.09724)
*Yunqian Cheng,Benjamin Princen,Roberto Manduchi*

Main category: cs.CV

TL;DR: PALMS+用单目尺度深度将RGB图像转成点云并与楼层平面图匹配，实现基于图像的无基础设施室内定位，在多数据集上优于现有方法且无需训练


<details>
  <summary>Details</summary>
Motivation: 解决室内无GPS环境下定位，克服手机LiDAR短距与布局歧义问题，支持无额外基础设施的定位

Method: 从有位姿的RGB图像用Foundation单目深度模型(Depth Pro)估计尺度对齐深度，构建3D点云；将点云投影到平面图并通过卷积进行几何布局匹配，得到位置-朝向后验；可直接使用或与粒子滤波结合做序列定位；无训练需求

Result: 提出PALMS+：用单目深度模型从已知相机位姿RGB图像重建尺度对齐点云，再与平面图进行几何匹配，输出位置和朝向后验，支持静态和序列定位；在Structured3D与自建校园数据集上优于PALMS和F3Loc，集成粒子滤波后序列定位误差更低，无需训练

Conclusion: PALMS+在静态与序列室内定位任务中均显著提升准确性与鲁棒性，适用于基础设施免费场景，并已开源代码与数据

Abstract: Indoor localization in GPS-denied environments is crucial for applications like emergency response and assistive navigation. Vision-based methods such as PALMS enable infrastructure-free localization using only a floor plan and a stationary scan, but are limited by the short range of smartphone LiDAR and ambiguity in indoor layouts. We propose PALMS$+$, a modular, image-based system that addresses these challenges by reconstructing scale-aligned 3D point clouds from posed RGB images using a foundation monocular depth estimation model (Depth Pro), followed by geometric layout matching via convolution with the floor plan. PALMS$+$ outputs a posterior over the location and orientation, usable for direct or sequential localization. Evaluated on the Structured3D and a custom campus dataset consisting of 80 observations across four large campus buildings, PALMS$+$ outperforms PALMS and F3Loc in stationary localization accuracy -- without requiring any training. Furthermore, when integrated with a particle filter for sequential localization on 33 real-world trajectories, PALMS$+$ achieved lower localization errors compared to other methods, demonstrating robustness for camera-free tracking and its potential for infrastructure-free applications. Code and data are available at https://github.com/Head-inthe-Cloud/PALMS-Plane-based-Accessible-Indoor-Localization-Using-Mobile-Smartphones

</details>


### [8] [Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction](https://arxiv.org/abs/2511.09735)
*Ahmed Alia,Mohcine Chraibi,Armin Seyfried*

Main category: cs.CV

TL;DR: 通过在Social LSTM中加入密度敏感的占用空间惩罚，本文在真实多密度数据集上同时降低碰撞率并改善位移预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法将行人视为点，忽略个体占据的物理空间，导致在拥挤场景中容易产生不现实的碰撞；因此需要将个体空间占用与场景密度纳入损失以学习更现实的轨迹。

Method: 在Social LSTM基础上引入结合平均位移误差与密度敏感碰撞惩罚的新损失函数（Dynamic Occupied Space loss），并使用在Lyon 2022 Festival of Lights实测轨迹生成的五个数据集进行训练与评估。

Result: 在五个数据集上，模型在碰撞率上最多降低31%，平均位移误差减少约5%，最终位移误差减少约6%，并在大多数测试集中优于若干最先进模型。

Conclusion: 本文提出的Dynamic Occupied Space损失函数能引导Social LSTM在不同密度场景中学习避免碰撞，同时在位移误差上不退步，实验显示在多密度真实数据集上显著降低碰撞率并小幅改善位移预测精度。

Abstract: In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.

</details>


### [9] [Soiling detection for Advanced Driver Assistance Systems](https://arxiv.org/abs/2511.09740)
*Filip Beránek,Václav Diviš,Ivan Gruber*

Main category: cs.CV

TL;DR: 将摄像头污损检测作为语义分割问题，比较了多种方法并修正了Woodscape数据集的泄露与标注问题，提供了更小的高质量子集和代码开源。


<details>
  <summary>Details</summary>
Motivation: 提高高级驾驶辅助系统在各种外部条件（如天气、灰尘）下的鲁棒性，需要准确检测摄像头污损，作者旨在通过语义分割方法提升检测精度并纠正现有数据集的问题。

Method: 对多种流行的语义分割方法进行了全面比较，并与tile级别分类方法进行了对照实验；对Woodscape数据集进行详尽分析以发现数据泄露与标注问题；构建并公开一个清洗后的数据子集和相应的代码与数据切分。

Result: 分割方法在性能上优于tile级分类，使用作者构建的更小但经过清洗的数据子集可以在更短训练时间内达到可比的结果，且公开了代码与数据切分以便复现。

Conclusion: 本文将汽车相机的污损检测视为语义分割问题，并证明分割方法在性能上优于基于tile的分类方法，同时指出Woodscape数据集存在数据泄露与标注不精确问题，提出了一个更小但更干净的数据子集。

Abstract: Soiling detection for automotive cameras is a crucial part of advanced driver assistance systems to make them more robust to external conditions like weather, dust, etc. In this paper, we regard the soiling detection as a semantic segmentation problem. We provide a comprehensive comparison of popular segmentation methods and show their superiority in performance while comparing them to tile-level classification approaches. Moreover, we present an extensive analysis of the Woodscape dataset showing that the original dataset contains a data-leakage and imprecise annotations. To address these problems, we create a new data subset, which, despite being much smaller, provides enough information for the segmentation method to reach comparable results in a much shorter time. All our codes and dataset splits are available at https://github.com/filipberanek/woodscape_revision.

</details>


### [10] [Feature Quality and Adaptability of Medical Foundation Models: A Comparative Evaluation for Radiographic Classification and Segmentation](https://arxiv.org/abs/2511.09742)
*Frank Li,Theo Dapamede,Mohammadreza Chavoshi,Young Seok Jeon,Bardia Khosravi,Abdulhameed Dere,Beatrice Brown-Mulry,Rohan Satya Isaac,Aawez Mansuri,Chiratidzo Sanyika,Janice Newsome,Saptarshi Purkayastha,Imon Banerjee,Hari Trivedi,Judy Gichoya*

Main category: cs.CV

TL;DR: 医疗预训练提升初始特征质量，但对细微病变定位效果有限；架构（多尺度等）与监督方式比文本对齐更关键，有监督基线仍很强。


<details>
  <summary>Details</summary>
Motivation: 需评估不同领域、范式和架构的基础模型（FMs）在胸部X光任务中对嵌入质量的影响，帮助选择最优编码器。

Method: 比较八个医疗和通用领域的视觉编码器，在胸片上通过线性探测与微调评估分类（气胸、心脏肥大）与分割（气胸、心界）任务，并做子组分析以检测模型是否利用混淆提示；比较不同预训练范式（文本-图像、图像-仅、标签监督）与有监督端到端基线。

Result: 医疗领域预训练显著优于通用领域模型在线性探测；预训练嵌入对全局分类和显著解剖结构分割有效，但对复杂、细微病变（如气胸）的定位效果差，需大量微调；昂贵的文本-图像对齐非必要，部分图像或标签监督模型表现优异；有监督端到端基线在分割任务中仍具竞争力。

Conclusion: 医疗预训练有利但不足以解决复杂定位问题；选择模型时应重视架构和任务匹配，针对细微病变需更多微调或专门设计的模型/数据以避免被混淆变量误导。

Abstract: Foundation models (FMs) promise to generalize medical imaging, but their effectiveness varies. It remains unclear how pre-training domain (medical vs. general), paradigm (e.g., text-guided), and architecture influence embedding quality, hindering the selection of optimal encoders for specific radiology tasks. To address this, we evaluate vision encoders from eight medical and general-domain FMs for chest X-ray analysis. We benchmark classification (pneumothorax, cardiomegaly) and segmentation (pneumothorax, cardiac boundary) using linear probing and fine-tuning. Our results show that domain-specific pre-training provides a significant advantage; medical FMs consistently outperformed general-domain models in linear probing, establishing superior initial feature quality. However, feature utility is highly task-dependent. Pre-trained embeddings were strong for global classification and segmenting salient anatomy (e.g., heart). In contrast, for segmenting complex, subtle pathologies (e.g., pneumothorax), all FMs performed poorly without significant fine-tuning, revealing a critical gap in localizing subtle disease. Subgroup analysis showed FMs use confounding shortcuts (e.g., chest tubes for pneumothorax) for classification, a strategy that fails for precise segmentation. We also found that expensive text-image alignment is not a prerequisite; image-only (RAD-DINO) and label-supervised (Ark+) FMs were among top performers. Notably, a supervised, end-to-end baseline remained highly competitive, matching or exceeding the best FMs on segmentation tasks. These findings show that while medical pre-training is beneficial, architectural choices (e.g., multi-scale) are critical, and pre-trained features are not universally effective, especially for complex localization tasks where supervised models remain a strong alternative.

</details>


### [11] [Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations](https://arxiv.org/abs/2511.09749)
*Mahsa Mitcheff,Siamul Karim Khan,Adam Czajka*

Main category: cs.CV

TL;DR: 通过在GAN潜在空间上沿属性梯度遍历，生成同一身份且可控属性变换的虹膜图像，用于增强识别与防攻击数据集；方法支持任意可微属性和GAN反演真实图像。


<details>
  <summary>Details</summary>
Motivation: 提高虹膜识别与反欺骗检测的鲁棒性，生成同一身份但不同可控属性的虹膜图像，扩大训练数据多样性以覆盖几何、纹理与质量变化。

Method: 利用预训练生成对抗网络或通过GAN反演得到真实图像的潜在编码，在潜在空间中沿着目标属性的梯度方向移动编码。通过定义可微的属性损失（几何/纹理/质量相关），对潜在码进行优化以改变属性同时添加身份保持项，以确保生成图像与原图同一身份。

Result: 提出一种基于生成模型潜在空间遍历的虹膜图像增强策略，通过对特定可微属性的梯度引导，在保持身份信息的同时操控诸如清晰度、瞳孔/虹膜尺寸等属性，并支持GAN预训练生成或对真实图像进行反演。

Conclusion: 该方法能以身份保持为前提，灵活操控多种虹膜属性，易于扩展到任意可导属性，并可结合预训练GAN或GAN反演用于现实图像增强，预计可提升虹膜识别与反欺骗模型的泛化能力。

Abstract: Developing reliable iris recognition and presentation attack detection methods requires diverse datasets that capture realistic variations in iris features and a wide spectrum of anomalies. Because of the rich texture of iris images, which spans a wide range of spatial frequencies, synthesizing same-identity iris images while controlling specific attributes remains challenging. In this work, we introduce a new iris image augmentation strategy by traversing a generative model's latent space toward latent codes that represent same-identity samples but with some desired iris image properties manipulated. The latent space traversal is guided by a gradient of specific geometrical, textural, or quality-related iris image features (e.g., sharpness, pupil size, iris size, or pupil-to-iris ratio) and preserves the identity represented by the image being manipulated. The proposed approach can be easily extended to manipulate any attribute for which a differentiable loss term can be formulated. Additionally, our approach can use either randomly generated images using either a pre-train GAN model or real-world iris images. We can utilize GAN inversion to project any given iris image into the latent space and obtain its corresponding latent code.

</details>


### [12] [STORM: Segment, Track, and Object Re-Localization from a Single 3D Model](https://arxiv.org/abs/2511.09771)
*Yu Deng,Teng Cao,Hikaru Shindo,Jiahong Xue,Quentin Delfosse,Kristian Kersting*

Main category: cs.CV

TL;DR: 提出了STORM，一种无需手工标注的实时6D位姿估计与跟踪系统，结合视觉-语言理解与自监督特征匹配，并具备自动重注册以恢复跟踪失败，适用于复杂工业场景，实时且开源。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖第一帧的手工分割掩码，标注成本高且在遮挡或快速运动场景下性能下降，故提出无需手工注释且具自动恢复能力的系统以降低部署成本并提升鲁棒性。

Method: 三阶段管线：1) 利用视觉-语言（上下文描述）引导目标定位；2) 自交互注意力（self-cross-attention）确定候选区域，并用分割模型生成精确掩码以估计位姿；3) 自动重注册机制通过特征相似性监测检测跟踪失败并恢复。整个流程无额外训练且实时。

Result: 在包含多目标遮挡、高速运动和光照变化的复杂工业数据集上，STORM达到了最先进的准确性，并在实时速度下运行，无需额外训练，适合制造与质检等实际应用。

Conclusion: STORM在无需人工注释的情况下，实现了在复杂工业数据集上的实时、高精度6D位姿估计和鲁棒跟踪，能有效处理遮挡、快速运动和光照变化，并通过自动重注册机制恢复跟踪失败，显著降低部署成本。

Abstract: Accurate 6D pose estimation and tracking are fundamental capabilities for physical AI systems such as robots. However, existing approaches typically rely on a manually annotated segmentation mask of the target in the first frame, which is labor-intensive and leads to reduced performance when faced with occlusions or rapid movement. To address these limi- tations, we propose STORM (Segment, Track, and Object Re-localization from a single 3D Model), an open-source robust real-time 6D pose estimation system that requires no manual annotation. STORM employs a novel three-stage pipeline combining vision-language understanding with self-supervised feature matching: contextual object descriptions guide localization, self-cross-attention mechanisms identify candidate regions, and a segmentation model produces precise masks for accurate pose estimation. Another key innovation is our automatic re-registration mechanism that detects tracking failures through feature similarity monitoring and recovers from severe occlusions or rapid motion. STORM achieves state-of-the-art accuracy on challenging industrial datasets featuring multi-object occlusions, high-speed motion, and varying illumination, while operating at real-time speeds without additional training. This annotation-free approach significantly reduces deployment overhead, providing a practical solution for modern applications, such as flexible manufacturing and intelligent quality control.

</details>


### [13] [PANDA - Patch And Distribution-Aware Augmentation for Long-Tailed Exemplar-Free Continual Learning](https://arxiv.org/abs/2511.09791)
*Siddeshwar Raghavan,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出PANDA：用CLIP提取低频类补丁并移植到高频样本，同时根据历史任务分布自适应平衡，实现对双层不平衡EFCL的有效缓解，提升准确率并减少遗忘。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据流存在双层不平衡（任务内与任务间），在EFCL情境下特别严重，现有PTM基方法通常忽视此问题，导致泛化能力差和灾难性遗忘。

Method: 使用CLIP编码器识别低频类的代表性区域，并将这些补丁移植到同任务的高频类样本中，同时基于历史任务分布调整每个任务的采样/增强强度以平衡跨任务样本量，方法可与冻结的PTM结合使用。

Result: 实验与消融研究表明PANDA能与现有PTM基EFCL方法兼容，显著提高准确率并减少遗忘，证明补丁移植与分布自适应策略的有效性。

Conclusion: PANDA在处理双重不平衡的EFCL问题上有效，通过补丁级别的类内强化和基于先前任务分布的自适应平衡策略，缓解了灾难性遗忘并提升了性能。

Abstract: Exemplar-Free Continual Learning (EFCL) restricts the storage of previous task data and is highly susceptible to catastrophic forgetting. While pre-trained models (PTMs) are increasingly leveraged for EFCL, existing methods often overlook the inherent imbalance of real-world data distributions. We discovered that real-world data streams commonly exhibit dual-level imbalances, dataset-level distributions combined with extreme or reversed skews within individual tasks, creating both intra-task and inter-task disparities that hinder effective learning and generalization. To address these challenges, we propose PANDA, a Patch-and-Distribution-Aware Augmentation framework that integrates seamlessly with existing PTM-based EFCL methods. PANDA amplifies low-frequency classes by using a CLIP encoder to identify representative regions and transplanting those into frequent-class samples within each task. Furthermore, PANDA incorporates an adaptive balancing strategy that leverages prior task distributions to smooth inter-task imbalances, reducing the overall gap between average samples across tasks and enabling fairer learning with frozen PTMs. Extensive experiments and ablation studies demonstrate PANDA's capability to work with existing PTM-based CL methods, improving accuracy and reducing catastrophic forgetting.

</details>


### [14] [Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2511.09809)
*Konstantinos M. Dafnis,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: STS 在文本嵌入谱子空间上以少量每样本位移参数进行潜在空间引导，提供了一种无需反向传播或修改编码器的高效测试时自适应方法，兼顾性能与资源效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应方法（如提示微调）虽有效但通常需要对大规模编码器进行反向传播或修改核心组件，导致计算和内存开销大且推理变慢。因此提出一种轻量、无需修改编码器且高效的在线单样本自适应方法。

Method: 从文本嵌入中提取主成分谱子空间以定义语义主方向，然后为每个样本学习少量位移参数，在增强视图之间最小化熵以进行谱感知的潜在表征引导。整个过程在潜在空间进行，无需对冻结的编码器进行反向传播或修改。

Result: 在标准评测协议下，STS 在准确性上通常优于或与最先进的测试时自适应方法相当，且仅引入少量参数，推理速度最高可达 8 倍，内存占用比传统提示微调小 12 倍。

Conclusion: 本文提出的Spectrum-Aware Test-Time Steering (STS) 有效缓解了 VLM 在测试时域偏移下性能下降的问题，通过在文本嵌入中提取谱子空间并学习少量每样本位移参数来在潜在空间中引导表征，从而在不修改或反向传播穿过编码器的情况下实现轻量级的测试时自适应。

Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.

</details>


### [15] [Lumos3D: A Single-Forward Framework for Low-Light 3D Scene Restoration](https://arxiv.org/abs/2511.09818)
*Hanzhou Liu,Peng Jiang,Jia Huang,Mi Lu*

Main category: cs.CV

TL;DR: 该文提出Lumos3D，一种无需相机位姿且可泛化的3D低光恢复框架。训练一次后可对未配准的低光多视图图像进行前向推断，恢复亮度与结构，采用几何推动的骨干、跨光照蒸馏（teacher在正常光训练以传递深度等几何信息）以及专门的Lumos损失以保证3D光度一致性。结果显示在真实数据上具有高保真几何和良好泛化性，并可扩展到过曝矫正。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预先计算的相机位姿和场景特定优化，限制了在动态真实环境中的可扩展性；因此需要一种无需位姿且可泛化的3D低光恢复方法。

Method: 基于几何的骨干网络构建正常光3D高斯表示，用教师-学生跨光照蒸馏将正常光几何信息（如深度）传递给学生网络，并引入Lumos损失以在重建的3D空间内促进光度一致性。训练一次后以纯前馈方式对未配准低光多视图图像进行恢复，无需场景特定优化。

Result: 在真实世界数据集上，Lumos3D在几何准确性、光照恢复质量和对未见场景的泛化能力上表现良好，并能扩展处理过曝校正。

Conclusion: Lumos3D成功实现了无位姿、无场景调优的通用3D低光恢复，在保持结构细节的同时恢复光照，且对未见场景具有良好泛化，且可用于过曝修复。

Abstract: Restoring 3D scenes captured under low-light con- ditions remains a fundamental yet challenging problem. Most existing approaches depend on precomputed camera poses and scene-specific optimization, which greatly restricts their scala- bility to dynamic real-world environments. To overcome these limitations, we introduce Lumos3D, a generalizable pose-free framework for 3D low-light scene restoration. Trained once on a single dataset, Lumos3D performs inference in a purely feed- forward manner, directly restoring illumination and structure from unposed, low-light multi-view images without any per- scene training or optimization. Built upon a geometry-grounded backbone, Lumos3D reconstructs a normal-light 3D Gaussian representation that restores illumination while faithfully pre- serving structural details. During training, a cross-illumination distillation scheme is employed, where the teacher network is distilled on normal-light ground truth to transfer accurate geometric information, such as depth, to the student model. A dedicated Lumos loss is further introduced to promote photomet- ric consistency within the reconstructed 3D space. Experiments on real-world datasets demonstrate that Lumos3D achieves high- fidelity low-light 3D scene restoration with accurate geometry and strong generalization to unseen cases. Furthermore, the framework naturally extends to handle over-exposure correction, highlighting its versatility for diverse lighting restoration tasks.

</details>


### [16] [From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance](https://arxiv.org/abs/2511.09820)
*Jeongho Min,Dongyoung Kim,Jaehyup Lee*

Main category: cs.CV

TL;DR: 论文提出无需额外训练的街景到卫星图像检索方法，结合预训练视觉编码器与大模型，通过网络搜索与地理编码推断位置并检索卫星图块，在零样本设置下优于已有监督方法，还能自动构建配对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有监督数据和全景/UAV图像，限制实际部署。作者希望提出零训练、可扩展并适用于单目街景的实用检索方案。

Method: 对单目街景图像，先通过网络图像搜索和LLM推断地理线索，再用地理编码API生成卫星查询，利用预训练视觉编码器（如DINOv2）提取特征，结合PCA白化处理进行检索，无需监督或微调。

Result: 在零样本设置下超越先前基于学习的方法，并能构建大规模语义对齐的街景-卫星配对数据集，代码将开源。

Conclusion: 该方法在无需标注和微调的前提下，在基准数据集上实现了优于此前学习方法的检索性能，并可自动化生成语义对齐的数据集，显示出可扩展性与成本效益。

Abstract: Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.

</details>


### [17] [AHA! Animating Human Avatars in Diverse Scenes with Gaussian Splatting](https://arxiv.org/abs/2511.09827)
*Aymen Mir,Jian Wang,Riza Alp Guler,Chuan Guo,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: 用3D Gaussian Splatting 表示人和场景，渲染与动作合成解耦，通过高斯对齐的动作合成与高斯优化实现几何一致的人-场景动画，能在多数据集及单目视频编辑中得到良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格或点云的动画管线在自由视角渲染、场景交互一致性或单目视频扩展上受限，3DGS在新视角合成上效果显著但尚未用于人-场景动画，故探索其潜力。

Method: 将人和场景均表示为高斯元（Gaussian），渲染模块与动作合成模块解耦。设计Gaussian-aligned motion模块在无显式场景几何的情况下基于不透明度线索和高斯投影结构生成动作，并通过人-场景高斯优化保证接触与导航的自然性。

Result: 在Scannet++、SuperSplat和多视角/稀疏重建人像数据上进行评估，结果显示方法能实现几何一致的自由视角渲染、自然交互及单目视频中编辑后新人物的动画合成。

Conclusion: 本文提出将3D Gaussian Splatting (3DGS) 引入人-场景动画问题，证明了3DGS可用于几何一致的自由视角渲染并支持单目视频编辑与动画合成。

Abstract: We present a novel framework for animating humans in 3D scenes using 3D Gaussian Splatting (3DGS), a neural scene representation that has recently achieved state-of-the-art photorealistic results for novel-view synthesis but remains under-explored for human-scene animation and interaction. Unlike existing animation pipelines that use meshes or point clouds as the underlying 3D representation, our approach introduces the use of 3DGS as the 3D representation to the problem of animating humans in scenes. By representing humans and scenes as Gaussians, our approach allows for geometry-consistent free-viewpoint rendering of humans interacting with 3D scenes. Our key insight is that the rendering can be decoupled from the motion synthesis and each sub-problem can be addressed independently, without the need for paired human-scene data. Central to our method is a Gaussian-aligned motion module that synthesizes motion without explicit scene geometry, using opacity-based cues and projected Gaussian structures to guide human placement and pose alignment. To ensure natural interactions, we further propose a human-scene Gaussian refinement optimization that enforces realistic contact and navigation. We evaluate our approach on scenes from Scannet++ and the SuperSplat library, and on avatars reconstructed from sparse and dense multi-view human capture. Finally, we demonstrate that our framework allows for novel applications such as geometry-consistent free-viewpoint rendering of edited monocular RGB videos with new animated humans, showcasing the unique advantage of 3DGS for monocular video-based human animation.

</details>


### [18] [CertMask: Certifiable Defense Against Adversarial Patches via Theoretically Optimal Mask Coverage](https://arxiv.org/abs/2511.09834)
*Xuntao Lyu,Ching-Chi Lin,Abdullah Al Arafat,Georg von der Brüggen,Jian-Jia Chen,Zhishan Guo*

Main category: cs.CV

TL;DR: CertMask builds a mathematically guaranteed set of binary masks that covers patch locations k times, enabling single-round O(n) certified defense against adversarial patches, outperforming PatchCleanser in certified robustness.


<details>
  <summary>Details</summary>
Motivation: Need efficient certifiable defense against physical adversarial patches reducing PatchCleanser's quadratic cost while keeping robust guarantees

Method: CertMask single-round masking with provable coverage

Result: Constructed binary mask set guaranteeing each patch location covered >=k times; single-round masking O(n) inference; theoretical sufficiency proof; empirical +13.4% CRA over PatchCleanser on benchmarks with similar clean accuracy

Conclusion: CertMask provides provable, efficient (O(n)) certified robustness against localized patch attacks via a rigorously designed mask coverage strategy, improving certified accuracy substantially without sacrificing clean accuracy.

Abstract: Adversarial patch attacks inject localized perturbations into images to mislead deep vision models. These attacks can be physically deployed, posing serious risks to real-world applications. In this paper, we propose CertMask, a certifiably robust defense that constructs a provably sufficient set of binary masks to neutralize patch effects with strong theoretical guarantees. While the state-of-the-art approach (PatchCleanser) requires two rounds of masking and incurs $O(n^2)$ inference cost, CertMask performs only a single round of masking with $O(n)$ time complexity, where $n$ is the cardinality of the mask set to cover an input image. Our proposed mask set is computed using a mathematically rigorous coverage strategy that ensures each possible patch location is covered at least $k$ times, providing both efficiency and robustness. We offer a theoretical analysis of the coverage condition and prove its sufficiency for certification. Experiments on ImageNet, ImageNette, and CIFAR-10 show that CertMask improves certified robust accuracy by up to +13.4\% over PatchCleanser, while maintaining clean accuracy nearly identical to the vanilla model.

</details>


### [19] [CORONA-Fields: Leveraging Foundation Models for Classification of Solar Wind Phenomena](https://arxiv.org/abs/2511.09843)
*Daniela Martin,Jinsu Hong,Connor O'Brien,Valmir P Moraes Filho,Jasmine R. Kobayashi,Evangelia Samara,Joseph Gallego*

Main category: cs.CV

TL;DR: 作者把在SDO影像上训练的视觉基础模型迁移到原位太阳风分析，拼接位置和磁连通性特征并微调神经场模型，用PSP标签进行分类，结果有限但证明了方法可行并开源代码。


<details>
  <summary>Details</summary>
Motivation: 将遥感基础模型的视觉特征迁移到原位（in situ）太阳风结构分类，解决太阳风与日冕物质喷发等空间天气源的自动分类难题。

Method: 使用在SDO图像上训练的基础视觉模型提取嵌入；把嵌入与航天器位置和用傅里叶特征表示的太阳磁连通性拼接，输入到神经场（neural field）网络；在PSP测得的等离子体属性标签上微调整个架构执行多类别太阳风结构分类。

Result: 提出将SDO影像预训练的基础模型生成的嵌入，与航天器位置和磁连通性（傅里叶特征编码）拼接，构建基于神经场的深度学习架构并对下游帕克太阳探测器（PSP）数据进行微调，实现太阳风结构的分类。

Conclusion: 尽管分类性能一般（受限于标签粗糙、类别不平衡和预训练模型可迁移性），该工作展示了用影像基础模型嵌入处理原位太阳风任务的可行性，为更可靠的空间天气预测铺垫方向，并公开复现代码。

Abstract: Space weather at Earth, driven by the solar activity, poses growing risks to satellites around our planet as well as to critical ground-based technological infrastructure. Major space weather contributors are the solar wind and coronal mass ejections whose variable density, speed, temperature, and magnetic field make the automated classification of those structures challenging. In this work, we adapt a foundation model for solar physics, originally trained on Solar Dynamics Observatory imagery, to create embeddings suitable for solar wind structure analysis. These embeddings are concatenated with the spacecraft position and solar magnetic connectivity encoded using Fourier features which generates a neural field-based model. The full deep learning architecture is fine-tuned bridging the gap between remote sensing and in situ observations. Labels are derived from Parker Solar Probe measurements, forming a downstream classification task that maps plasma properties to solar wind structures. Although overall classification performance is modest, likely due to coarse labeling, class imbalance, and limited transferability of the pretrained model, this study demonstrates the feasibility of leveraging foundation model embeddings for in situ solar wind tasks. As a first proof-of-concept, it lays the groundwork for future improvements toward more reliable space weather predictions. The code and configuration files used in this study are publicly available to support reproducibility.

</details>


### [20] [IPCD: Intrinsic Point-Cloud Decomposition](https://arxiv.org/abs/2511.09866)
*Shogo Sato,Takuhiro Kaneko,Shoichiro Takeda,Tomoyasu Shimada,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida,Akisato Kimura*

Main category: cs.CV

TL;DR: 提出IPCD-Net，通过点级特征聚合处理非网格点云，并用投影基亮度分布（PLD）结合层次特征细化捕捉全局光照线索，实现点云的反照率与阴影分解，支持纹理编辑与重光照。


<details>
  <summary>Details</summary>
Motivation: 在AR与机器人等需真实可视化的场景中，点云的重光照和纹理编辑依赖准确分离反照率与阴影，但现有方法不适用于非网格点云且忽视全局光照方向，导致分解不准确，影响下游应用。

Method: 提出IPCD-Net架构：点级特征聚合模块处理非网格结构；引入Projection-based Luminance Distribution (PLD)通过多视角投影与层次特征细化编码全局光照方向信息；在合成数据集上训练与评估，进行消融与应用示例展示。

Result: 提出了IPCD（Intrinsic Point-Cloud Decomposition），用于将有色点云直接分解为反照率（albedo）和阴影（shade），解决点云非网格结构和全局光照方向缺失问题。

Conclusion: IPCD-Net在合成户外场景数据集上能有效减少反照率中的投射阴影，提升阴影颜色准确度，并展示了在纹理编辑、重光照及不同光照下点云配准的应用与真实世界适用性验证。

Abstract: Point clouds are widely used in various fields, including augmented reality (AR) and robotics, where relighting and texture editing are crucial for realistic visualization. Achieving these tasks requires accurately separating albedo from shade. However, performing this separation on point clouds presents two key challenges: (1) the non-grid structure of point clouds makes conventional image-based decomposition models ineffective, and (2) point-cloud models designed for other tasks do not explicitly consider global-light direction, resulting in inaccurate shade. In this paper, we introduce \textbf{Intrinsic Point-Cloud Decomposition (IPCD)}, which extends image decomposition to the direct decomposition of colored point clouds into albedo and shade. To overcome challenge (1), we propose \textbf{IPCD-Net} that extends image-based model with point-wise feature aggregation for non-grid data processing. For challenge (2), we introduce \textbf{Projection-based Luminance Distribution (PLD)} with a hierarchical feature refinement, capturing global-light ques via multi-view projection. For comprehensive evaluation, we create a synthetic outdoor-scene dataset. Experimental results demonstrate that IPCD-Net reduces cast shadows in albedo and enhances color accuracy in shade. Furthermore, we showcase its applications in texture editing, relighting, and point-cloud registration under varying illumination. Finally, we verify the real-world applicability of IPCD-Net.

</details>


### [21] [Remember Me: Bridging the Long-Range Gap in LVLMs with Three-Step Inference-Only Decay Resilience Strategies](https://arxiv.org/abs/2511.09868)
*Peng Gao,Yujian Lee,Xiaofeng Zhang,Zailong Chen,Hui Zhang*

Main category: cs.CV

TL;DR: 提出推理阶段的T-DRS（三步策略）来缓解ROPE导致的长距离注意力衰减：语义驱动放大、距离感知控制和平衡远依赖，训练不可变动下提升VQA表现。


<details>
  <summary>Details</summary>
Motivation: ROPE in LVLMs causes progressive attention decay over long token distances, impairing global context memory; need inference-only method to recover long-range dependencies without retraining.

Method: Three-step Decay Resilience Strategies (T-DRS)

Result: T-DRS (SD-DRS, DC-DRS, reRD-DRS) recovers suppressed long-range token pairs, improving VQA performance consistently in a training-free manner; code released.

Conclusion: T-DRS在不改变模型参数的前提下，有效恢复被抑制的长程依赖，保持局部偏置同时提高视觉问答任务性能。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive performance across a wide range of multimodal tasks. However, they still face critical challenges in modeling long-range dependencies under the usage of Rotary Positional Encoding (ROPE). Although it can facilitate precise modeling of token positions, it induces progressive attention decay as token distance increases, especially with progressive attention decay over distant token pairs, which severely impairs the model's ability to remember global context. To alleviate this issue, we propose inference-only Three-step Decay Resilience Strategies (T-DRS), comprising (1) Semantic-Driven DRS (SD-DRS), amplifying semantically meaningful but distant signals via content-aware residuals, (2) Distance-aware Control DRS (DC-DRS), which can purify attention by smoothly modulating weights based on positional distances, suppressing noise while preserving locality, and (3) re-Reinforce Distant DRS (reRD-DRS), consolidating the remaining informative remote dependencies to maintain global coherence. Together, the T-DRS recover suppressed long-range token pairs without harming local inductive biases. Extensive experiments on Vision Question Answering (VQA) benchmarks demonstrate that T-DRS can consistently improve performance in a training-free manner. The code can be accessed in https://github.com/labixiaoq-qq/Remember-me

</details>


### [22] [SAM-DAQ: Segment Anything Model with Depth-guided Adaptive Queries for RGB-D Video Salient Object Detection](https://arxiv.org/abs/2511.09870)
*Jia Lin,Xiaofei Zhou,Jiyuan Liu,Runmin Cong,Guodao Zhang,Zhi Liu,Jiyong Zhang*

Main category: cs.CV

TL;DR: 提出SAM-DAQ：在冻结的SAM2上通过深度引导并行适配器实现无提示多模态融合，并用查询驱动时间记忆模块处理时序，一致优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目标是将通用分割基础模型SAM推广到RGB-D视频显著性检测，克服手动提示依赖、顺序适配器高内存占用和记忆注意力计算成本三大挑战。

Method: 方法包括（1）PAMIE：在SAM编码器中以跳连方式插入深度引导并行适配器，在提示免费条件下融合RGB与深度特征；（2）QTM：将记忆库与提示嵌入统一为可学习的查询管线，使用帧级与视频级查询同时提取时序一致性并迭代更新查询的时间表示。

Result: Proposed SAM-DAQ integrates SAM2 with depth and temporal cues via PAMIE and QTM, fine-tunes frozen SAM encoder with depth-guided parallel adapters and uses query-driven temporal memory, achieving SOTA on three RGB-D VSOD datasets.

Conclusion: SAM-DAQ在RGB-D视频显著性目标检测任务中，通过深度引导的并行适配器和查询驱动的时间记忆，有效解决了手动提示依赖、高内存消耗和记忆注意力计算负担问题，并提高了指标性能。

Abstract: Recently segment anything model (SAM) has attracted widespread concerns, and it is often treated as a vision foundation model for universal segmentation. Some researchers have attempted to directly apply the foundation model to the RGB-D video salient object detection (RGB-D VSOD) task, which often encounters three challenges, including the dependence on manual prompts, the high memory consumption of sequential adapters, and the computational burden of memory attention. To address the limitations, we propose a novel method, namely Segment Anything Model with Depth-guided Adaptive Queries (SAM-DAQ), which adapts SAM2 to pop-out salient objects from videos by seamlessly integrating depth and temporal cues within a unified framework. Firstly, we deploy a parallel adapter-based multi-modal image encoder (PAMIE), which incorporates several depth-guided parallel adapters (DPAs) in a skip-connection way. Remarkably, we fine-tune the frozen SAM encoder under prompt-free conditions, where the DPA utilizes depth cues to facilitate the fusion of multi-modal features. Secondly, we deploy a query-driven temporal memory (QTM) module, which unifies the memory bank and prompt embeddings into a learnable pipeline. Concretely, by leveraging both frame-level queries and video-level queries simultaneously, the QTM module can not only selectively extract temporal consistency features but also iteratively update the temporal representations of the queries. Extensive experiments are conducted on three RGB-D VSOD datasets, and the results show that the proposed SAM-DAQ consistently outperforms state-of-the-art methods in terms of all evaluation metrics.

</details>


### [23] [RWKV-PCSSC: Exploring RWKV Model for Point Cloud Semantic Scene Completion](https://arxiv.org/abs/2511.09878)
*Wenzhe He,Xiaojun Chen,Wentang Chen,Hongyu Wang,Ying Liu,Ruihui Li*

Main category: cs.CV

TL;DR: Propose RWKV-PCSSC: lightweight SSC network with RWKV-SG and RWKV-PD modules, achieves better efficiency and SOTA accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce model complexity and resource demands in Semantic Scene Completion by designing a lightweight point cloud SSC network inspired by RWKV.

Method: Paper analysis

Result: RWKV-PCSSC reduces parameter count by 4.18x, improves memory efficiency by 1.37x, and achieves state-of-the-art performance on multiple datasets.

Conclusion: RWKV-PCSSC is an efficient and effective lightweight model for SSC, outperforming previous methods in accuracy and resource usage.

Abstract: Semantic Scene Completion (SSC) aims to generate a complete semantic scene from an incomplete input. Existing approaches often employ dense network architectures with a high parameter count, leading to increased model complexity and resource demands. To address these limitations, we propose RWKV-PCSSC, a lightweight point cloud semantic scene completion network inspired by the Receptance Weighted Key Value (RWKV) mechanism. Specifically, we introduce a RWKV Seed Generator (RWKV-SG) module that can aggregate features from a partial point cloud to produce a coarse point cloud with coarse features. Subsequently, the point-wise feature of the point cloud is progressively restored through multiple stages of the RWKV Point Deconvolution (RWKV-PD) modules. By leveraging a compact and efficient design, our method achieves a lightweight model representation. Experimental results demonstrate that RWKV-PCSSC reduces the parameter count by 4.18$\times$ and improves memory efficiency by 1.37$\times$ compared to state-of-the-art methods PointSSC. Furthermore, our network achieves state-of-the-art performance on established indoor (SSC-PC, NYUCAD-PC) and outdoor (PointSSC) scene dataset, as well as on our proposed datasets (NYUCAD-PC-V2, 3D-FRONT-PC).

</details>


### [24] [HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models](https://arxiv.org/abs/2511.09883)
*Liheng Zhang,Jin Wang,Hui Li,Bingfeng Zhang,Weifeng Liu*

Main category: cs.CV

TL;DR: HCC-3D通过GSC压缩全局结构并用ADM补偿细节，在保证近乎损失的情况下实现极高压缩率与SOTA性能，显著提高3D-VLM效率。


<details>
  <summary>Details</summary>
Motivation: 当前3D-VLM将全部3D token输入LLM导致计算瓶颈，研究如何显著降低token带来的计算开销同时保留关键信息。

Method: 先用全局结构压缩（GSC）通过全局查询将所有点云token压缩为少量关键token，再用自适应细节挖掘（ADM）模块通过互补打分选择性重压缩被忽视的重要特征进行补偿。

Result: 在约98%压缩率下，HCC-3D不仅极大减少计算量，还在多项任务上实现新的SOTA，兼顾效率与性能。

Conclusion: 提出了HCC-3D方法，通过层级压缩与补偿机制在极高压缩率下保持3D语义信息，提升效率并达到SOTA。

Abstract: 3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.

</details>


### [25] [Scale-Aware Relay and Scale-Adaptive Loss for Tiny Object Detection in Aerial Images](https://arxiv.org/abs/2511.09891)
*Jinfu Li,Yuqi Huang,Hong Song,Ting Wang,Jianghan Xia,Yucong Lin,Jingfan Fan,Jian Yang*

Main category: cs.CV

TL;DR: SARL enhances cross-layer feature sharing; SAL focuses loss on tiny objects; together they significantly boost tiny object detection in aerial images.


<details>
  <summary>Details</summary>
Motivation: Tiny objects lose features during deep propagation and get over-penalized by IoU losses, reducing detection performance in aerial images.

Method: Propose SARL and SAL for tiny object detection

Result: Introduced SARL with cross-scale spatial-channel attention and SAL that downweights larger objects; improved AP by 5.5% on YOLOv5/YOLOx and achieved 29.0% AP on noisy AI-TOD-v2.0.

Conclusion: SARL and SAL are plug-and-play modules improving tiny object detection and robustness across benchmarks.

Abstract: Recently, despite the remarkable advancements in object detection, modern detectors still struggle to detect tiny objects in aerial images. One key reason is that tiny objects carry limited features that are inevitably degraded or lost during long-distance network propagation. Another is that smaller objects receive disproportionately greater regression penalties than larger ones during training. To tackle these issues, we propose a Scale-Aware Relay Layer (SARL) and a Scale-Adaptive Loss (SAL) for tiny object detection, both of which are seamlessly compatible with the top-performing frameworks. Specifically, SARL employs a cross-scale spatial-channel attention to progressively enrich the meaningful features of each layer and strengthen the cross-layer feature sharing. SAL reshapes the vanilla IoU-based losses so as to dynamically assign lower weights to larger objects. This loss is able to focus training on tiny objects while reducing the influence on large objects. Extensive experiments are conducted on three benchmarks (\textit{i.e.,} AI-TOD, DOTA-v2.0 and VisDrone2019), and the results demonstrate that the proposed method boosts the generalization ability by 5.5\% Average Precision (AP) when embedded in YOLOv5 (anchor-based) and YOLOx (anchor-free) baselines. Moreover, it also promotes the robust performance with 29.0\% AP on the real-world noisy dataset (\textit{i.e.,} AI-TOD-v2.0).

</details>


### [26] [Regional Attention-Enhanced Swin Transformer for Clinically Relevant Medical Image Captioning](https://arxiv.org/abs/2511.09893)
*Zubia Naz,Farhan Asghar,Muhammad Ishfaq Hussain,Yahya Hadadi,Muhammad Aasim Rafique,Wookjin Choi,Moongu Jeon*

Main category: cs.CV

TL;DR: 提出Swin-BART结合轻量区域注意模块，在ROCO数据集上提升语义一致性并提供可解释性，ROUGE和BERTScore显著优于基线


<details>
  <summary>Details</summary>
Motivation: Improve automated radiology report generation by focusing on diagnostically salient regions while keeping model compact and interpretable

Method: Swin-BART encoder-decoder + regional attention

Result: State-of-the-art semantic fidelity on ROCO with improvements in ROUGE (0.603) and BERTScore (0.807) over baselines; competitive BLEU/CIDEr/METEOR; includes ablations, per-modality analysis, significance tests, and qualitative heatmaps

Conclusion: 方法在准确性与透明性间取得平衡，适合在有人监督下安全用于研究

Abstract: Automated medical image captioning translates complex radiological images into diagnostic narratives that can support reporting workflows. We present a Swin-BART encoder-decoder system with a lightweight regional attention module that amplifies diagnostically salient regions before cross-attention. Trained and evaluated on ROCO, our model achieves state-of-the-art semantic fidelity while remaining compact and interpretable. We report results as mean$\pm$std over three seeds and include $95\%$ confidence intervals. Compared with baselines, our approach improves ROUGE (proposed 0.603, ResNet-CNN 0.356, BLIP2-OPT 0.255) and BERTScore (proposed 0.807, BLIP2-OPT 0.645, ResNet-CNN 0.623), with competitive BLEU, CIDEr, and METEOR. We further provide ablations (regional attention on/off and token-count sweep), per-modality analysis (CT/MRI/X-ray), paired significance tests, and qualitative heatmaps that visualize the regions driving each description. Decoding uses beam search (beam size $=4$), length penalty $=1.1$, $no\_repeat\_ngram\_size$ $=3$, and max length $=128$. The proposed design yields accurate, clinically phrased captions and transparent regional attributions, supporting safe research use with a human in the loop.

</details>


### [27] [Simulating Distribution Dynamics: Liquid Temporal Feature Evolution for Single-Domain Generalized Object Detection](https://arxiv.org/abs/2511.09909)
*Zihao Zhang,Yang Li,Aming Wu,Yahong Han*

Main category: cs.CV

TL;DR: 提出LTFE：利用可控高斯噪声注入、多尺度高斯模糊、时间建模和液体参数调整，模拟连续的特征演化以提高单域到多未知域的检测泛化。


<details>
  <summary>Details</summary>
Motivation: 现有离散增强或静态扰动无法模拟现实中如天气或光照变化等导致的持续渐进域漂移，因而需要模拟连续的特征演化以捕捉细粒度跨域差异。

Method: 方法包括初始的可控高斯噪声注入与多尺度高斯模糊以生成特征扰动，随后通过时间建模模块和液体参数调节器生成自适应调制参数，实现连续平滑的域适配过程。

Result: Single-Domain Generalized Object Detection (Single-DGOD)提出一种新方法Liquid Temporal Feature Evolution (LTFE)，通过时间建模与液体神经网络驱动的参数调节，从源域到模拟潜在目标域实现连续渐进的特征演化，提升对未知域的泛化能力。

Conclusion: LTFE通过捕捉逐步的跨域特征演化并动态调控适配路径，有效缩小源域与未知域分布差距，在Diverse Weather和Real-to-Art基准上取得显著性能提升。

Abstract: In this paper, we focus on Single-Domain Generalized Object Detection (Single-DGOD), aiming to transfer a detector trained on one source domain to multiple unknown domains. Existing methods for Single-DGOD typically rely on discrete data augmentation or static perturbation methods to expand data diversity, thereby mitigating the lack of access to target domain data. However, in real-world scenarios such as changes in weather or lighting conditions, domain shifts often occur continuously and gradually. Discrete augmentations and static perturbations fail to effectively capture the dynamic variation of feature distributions, thereby limiting the model's ability to perceive fine-grained cross-domain differences. To this end, we propose a new method, Liquid Temporal Feature Evolution, which simulates the progressive evolution of features from the source domain to simulated latent distributions by incorporating temporal modeling and liquid neural network-driven parameter adjustment. Specifically, we introduce controllable Gaussian noise injection and multi-scale Gaussian blurring to simulate initial feature perturbations, followed by temporal modeling and a liquid parameter adjustment mechanism to generate adaptive modulation parameters, enabling a smooth and continuous adaptation across domains. By capturing progressive cross-domain feature evolution and dynamically regulating adaptation paths, our method bridges the source-unknown domain distribution gap, significantly boosting generalization and robustness to unseen shifts. Significant performance improvements on the Diverse Weather dataset and Real-to-Art benchmark demonstrate the superiority of our method. Our code is available at https://github.com/2490o/LTFE.

</details>


### [28] [MosaicDoc: A Large-Scale Bilingual Benchmark for Visually Rich Document Understanding](https://arxiv.org/abs/2511.09919)
*Ketong Chen,Yuhao Chen,Yang Xue*

Main category: cs.CV

TL;DR: 提出基于LLM的DocWeaver自动生成流程，构建了大型双语复杂版式基准MosaicDoc（72K图、600K+QA），揭示现有VLM在VRDU任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评测以英语为主、版式单一、任务有限，无法有效衡量VLM在视觉丰富文档理解（VRDU）中的表现，需构建更复杂、多语种和多任务的基准。

Method: 提出DocWeaver多智能体管道，利用大语言模型自动生成数据和多任务注释（OCR、VQA、阅读顺序、定位），从报刊与杂志采集多样复杂版式，生成72K图像与60万+问答对的基准数据集MosaicDoc，并对主流模型进行全面评估。

Result: 构建了包含72K图像、600K+QA、来自196家出版社的MosaicDoc数据集，覆盖多列与非曼哈顿布局、双语内容及多任务注释。基准评测显示当前SOTA模型在真实复杂文档上的表现仍有显著不足。

Conclusion: DocWeaver生成的大规模双语基准MosaicDoc显著推动了VRDU评估，揭示当前VLM在复杂版式和密集文本场景下的能力不足，并为未来研究指明方向。

Abstract: Despite the rapid progress of Vision-Language Models (VLMs), their capabilities are inadequately assessed by existing benchmarks, which are predominantly English-centric, feature simplistic layouts, and support limited tasks. Consequently, they fail to evaluate model performance for Visually Rich Document Understanding (VRDU), a critical challenge involving complex layouts and dense text. To address this, we introduce DocWeaver, a novel multi-agent pipeline that leverages Large Language Models to automatically generate a new benchmark. The result is MosaicDoc, a large-scale, bilingual (Chinese and English) resource designed to push the boundaries of VRDU. Sourced from newspapers and magazines, MosaicDoc features diverse and complex layouts (including multi-column and non-Manhattan), rich stylistic variety from 196 publishers, and comprehensive multi-task annotations (OCR, VQA, reading order, and localization). With 72K images and over 600K QA pairs, MosaicDoc serves as a definitive benchmark for the field. Our extensive evaluation of state-of-the-art models on this benchmark reveals their current limitations in handling real-world document complexity and charts a clear path for future research.

</details>


### [29] [Compensating Distribution Drifts in Class-incremental Learning of Pre-trained Vision Transformers](https://arxiv.org/abs/2511.09926)
*Xuan Rao,Simian Xu,Zheng Li,Bo Zhao,Derong Liu,Mingming Ha,Cesare Alippi*

Main category: cs.CV

TL;DR: Introduce SLDC to align feature distributions across tasks via learned linear/weakly nonlinear transition operators and KD, significantly improving sequential fine-tuning for CIL.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with pre-trained vision transformers suffers from distribution drift when sequential fine-tuning changes backbone features, causing classifier mismatch.

Method: Learn a linear operator via regularized least squares mapping pre- and post-fine-tuning features; extend to weakly nonlinear mappings; apply knowledge distillation; evaluate on CIL benchmarks.

Result: Propose SLDC: latent space transition operator with linear and weakly nonlinear variants plus knowledge distillation; improves SeqFT and matches joint training performance on benchmarks.

Conclusion: Combining KD and SLDC compensates representation and distribution drift, enabling SeqFT to reach joint-training-level performance on standard CIL datasets.

Abstract: Recent advances have shown that sequential fine-tuning (SeqFT) of pre-trained vision transformers (ViTs), followed by classifier refinement using approximate distributions of class features, can be an effective strategy for class-incremental learning (CIL). However, this approach is susceptible to distribution drift, caused by the sequential optimization of shared backbone parameters. This results in a mismatch between the distributions of the previously learned classes and that of the updater model, ultimately degrading the effectiveness of classifier performance over time. To address this issue, we introduce a latent space transition operator and propose Sequential Learning with Drift Compensation (SLDC). SLDC aims to align feature distributions across tasks to mitigate the impact of drift. First, we present a linear variant of SLDC, which learns a linear operator by solving a regularized least-squares problem that maps features before and after fine-tuning. Next, we extend this with a weakly nonlinear SLDC variant, which assumes that the ideal transition operator lies between purely linear and fully nonlinear transformations. This is implemented using learnable, weakly nonlinear mappings that balance flexibility and generalization. To further reduce representation drift, we apply knowledge distillation (KD) in both algorithmic variants. Extensive experiments on standard CIL benchmarks demonstrate that SLDC significantly improves the performance of SeqFT. Notably, by combining KD to address representation drift with SLDC to compensate distribution drift, SeqFT achieves performance comparable to joint training across all evaluated datasets. Code: https://github.com/raoxuan98-hash/sldc.git.

</details>


### [30] [Debiased Dual-Invariant Defense for Adversarially Robust Person Re-Identification](https://arxiv.org/abs/2511.09933)
*Yuhang Zhou,Yanxiang Zhao,Zhongyun Hua,Zhipu Liu,Zhaoquan Gu,Qing Liao,Leo Yu Zhang*

Main category: cs.CV

TL;DR: 本文针对行人重识别（ReID）在对抗攻击下的脆弱性，提出了一种去偏差的双不变性防御框架。方法包括基于扩散模型的数据重采样以平衡训练数据，以及一种包含最远负样本扩展的度量对抗训练与自我元学习机制，以提升对未见身份和未见攻击类型的鲁棒性。实验表明该方法显著优于现有防御。


<details>
  <summary>Details</summary>
Motivation: 现有分类任务的对抗防御方法难以直接迁移到度量学习/行人ReID，且现有ReID防御未充分解决模型偏差（如类别不平衡、样本多样性不足）和需要同时对未见身份与未见攻击泛化的复合问题。

Method: 两阶段框架：1) 数据平衡阶段：用扩散模型生成或重采样数据，增加训练集中少见或困难样本，缓解模型偏差并提升多样性。2) 双对抗自我元防御阶段：在度量学习框架下进行对抗训练，引入最远负样本扩展软化（farthest negative extension softening）以弥补无分类器时鲁棒性的下降，并结合对抗增强的自我元学习实现对未见身份和未见攻击类型的双重泛化。

Result: 在若干标准ReID数据集和攻击场景上，所提方法在鲁棒性指标上显著优于现有最先进防御，证明其在对抗条件下的有效性。

Conclusion: 提出的方法通过数据平衡和双重对抗自我元训练，能有效减轻模型偏差并提升对未见身份和攻击的泛化，从而在对抗性行人ReID任务上显著优于现有防御方法。

Abstract: Person re-identification (ReID) is a fundamental task in many real-world applications such as pedestrian trajectory tracking. However, advanced deep learning-based ReID models are highly susceptible to adversarial attacks, where imperceptible perturbations to pedestrian images can cause entirely incorrect predictions, posing significant security threats. Although numerous adversarial defense strategies have been proposed for classification tasks, their extension to metric learning tasks such as person ReID remains relatively unexplored. Moreover, the several existing defenses for person ReID fail to address the inherent unique challenges of adversarially robust ReID. In this paper, we systematically identify the challenges of adversarial defense in person ReID into two key issues: model bias and composite generalization requirements. To address them, we propose a debiased dual-invariant defense framework composed of two main phases. In the data balancing phase, we mitigate model bias using a diffusion-model-based data resampling strategy that promotes fairness and diversity in training data. In the bi-adversarial self-meta defense phase, we introduce a novel metric adversarial training approach incorporating farthest negative extension softening to overcome the robustness degradation caused by the absence of classifier. Additionally, we introduce an adversarially-enhanced self-meta mechanism to achieve dual-generalization for both unseen identities and unseen attack types. Experiments demonstrate that our method significantly outperforms existing state-of-the-art defenses.

</details>


### [31] [AdaptViG: Adaptive Vision GNN with Exponential Decay Gating](https://arxiv.org/abs/2511.09942)
*Mustafa Munir,Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: AdaptViG用静态轴向骨架+指数衰减门控的自适应图卷积，在保持高效的同时提升性能，在ImageNet和下游任务上优于现有Vision GNN与更大模型。


<details>
  <summary>Details</summary>
Motivation: 解决Vision GNN在图构建阶段计算开销大、效率低的问题，同时保留长程信息建模能力，通过高效的静态骨架+动态内容感知门控实现更优的准确率/效率平衡。

Method: 提出Adaptive Graph Convolution：基于静态轴向骨架构造基础连接，结合Exponential Decay Gating根据特征相似性调整长程连接权重；在网络前期用轻量门控模块，末期用完整全局注意力模块，形成混合架构。

Result: 提出AdaptViG，一种高效混合型视觉图神经网络，通过Adaptive Graph Convolution结合静态轴向骨架与动态指数衰减门控进行图构建，在早期使用高效门控、末期使用全局注意力以兼顾效率与聚合能力，取得了更优的准确率与效率折中。

Conclusion: AdaptViG在参数与计算量大幅减少的情况下，仍能超过或匹配现有更大模型与ViG变体，证明了混合静态+动态图构建策略的实用性。

Abstract: Vision Graph Neural Networks (ViGs) offer a new direction for advancements in vision architectures. While powerful, ViGs often face substantial computational challenges stemming from their graph construction phase, which can hinder their efficiency. To address this issue we propose AdaptViG, an efficient and powerful hybrid Vision GNN that introduces a novel graph construction mechanism called Adaptive Graph Convolution. This mechanism builds upon a highly efficient static axial scaffold and a dynamic, content-aware gating strategy called Exponential Decay Gating. This gating mechanism selectively weighs long-range connections based on feature similarity. Furthermore, AdaptViG employs a hybrid strategy, utilizing our efficient gating mechanism in the early stages and a full Global Attention block in the final stage for maximum feature aggregation. Our method achieves a new state-of-the-art trade-off between accuracy and efficiency among Vision GNNs. For instance, our AdaptViG-M achieves 82.6% top-1 accuracy, outperforming ViG-B by 0.3% while using 80% fewer parameters and 84% fewer GMACs. On downstream tasks, AdaptViG-M obtains 45.8 mIoU, 44.8 APbox, and 41.1 APmask, surpassing the much larger EfficientFormer-L7 by 0.7 mIoU, 2.2 APbox, and 2.1 APmask, respectively, with 78% fewer parameters.

</details>


### [32] [TSPE-GS: Probabilistic Depth Extraction for Semi-Transparent Surface Reconstruction via 3D Gaussian Splatting](https://arxiv.org/abs/2511.09944)
*Zhiyuan Xu,Nan Min,Yuhang Guo,Tong Wei*

Main category: cs.CV

TL;DR: TSPE-GS通过采样透射率并用TSDF逐步融合，建立像素多峰不透明度/深度分布，解决半透明场景的深度歧义，显著提升Gaussian Splatting在半透明对象的重建质量，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting方法通常假设每个像素只有单一深度，这在半透明场景中失效，导致无法正确重建可见的多重表面。需要一种能处理像素级多表面透明度与深度分布的方案。

Method: 提出TSPE-GS：对透射率进行均匀采样以建立像素级多模态不透明度和深度分布，替代单峰深度假设；逐步融合截断签名距离函数（TSDF），在统一框架内分离重建外部和内部表面；方法可扩展到其他基于高斯的重建管线，无需额外训练开销。

Result: 在公共与自建的半透明与不透明数据集上，大量实验表明TSPE-GS在半透明几何重建上显著优于基线方法，同时在不透明场景上保持竞争性能。

Conclusion: TSPE-GS通过在像素级建模多峰不透明度和深度分布，有效解决了半透明表面和跨表面深度歧义问题，从而显著提升了3D Gaussian Splatting在半透明场景的几何重建能力，同时对不透明场景保持性能。

Abstract: 3D Gaussian Splatting offers a strong speed-quality trade-off but struggles to reconstruct semi-transparent surfaces because most methods assume a single depth per pixel, which fails when multiple surfaces are visible. We propose TSPE-GS (Transparent Surface Probabilistic Extraction for Gaussian Splatting), which uniformly samples transmittance to model a pixel-wise multi-modal distribution of opacity and depth, replacing the prior single-peak assumption and resolving cross-surface depth ambiguity. By progressively fusing truncated signed distance functions, TSPE-GS reconstructs external and internal surfaces separately within a unified framework. The method generalizes to other Gaussian-based reconstruction pipelines without extra training overhead. Extensive experiments on public and self-collected semi-transparent and opaque datasets show TSPE-GS significantly improves semi-transparent geometry reconstruction while maintaining performance on opaque scenes.

</details>


### [33] [Beyond Cosine Similarity Magnitude-Aware CLIP for No-Reference Image Quality Assessment](https://arxiv.org/abs/2511.09948)
*Zhicheng Liao,Dongxu Wu,Zhenshan Shi,Sijie Mai,Hanwei Zhu,Lingyu Zhu,Yuncheng Jiang,Baoliang Chen*

Main category: cs.CV

TL;DR: 通过引入并归一化CLIP图像特征幅值，并与余弦相似度按置信度自适应融合，提出无需训练即可提升CLIP基于提示的NR-IQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有将CLIP用于无参考图像质量评估（NR-IQA）的方法仅用图像与文本提示间余弦相似度，忽略了图像特征幅值对感知质量的潜在相关性。作者发现该幅值与感知质量有显著相关性。

Method: 1) 取CLIP图像特征绝对值并汇聚为标量；2) 应用Box-Cox变换做统计归一以减少语义敏感性；3) 用该标量作为辅助质量线索与图像-文本余弦相似度结合；4) 设计基于置信度的自适应加权融合策略分别衡量两项的相对贡献；5) 在多个IQ A数据集上无监督评估。

Result: 提出自适应融合框架，结合余弦相似度与幅值敏感的质量线索；通过提取绝对CLIP图像特征并用Box-Cox变换进行统计归一化得到语义归一的标量辅助线索；设计置信度引导的融合方案按相对强度自适应加权两项。该方法在多个基准IQA数据集上，无需任务特定训练下超越标准CLIP-IQA与最先进基线。

Conclusion: 幅值信息是CLIP图像特征中有益的感知质量线索；在统计归一化并与余弦相似度自适应融合后，可不需训练显著提升NR-IQA表现。

Abstract: Recent efforts have repurposed the Contrastive Language-Image Pre-training (CLIP) model for No-Reference Image Quality Assessment (NR-IQA) by measuring the cosine similarity between the image embedding and textual prompts such as "a good photo" or "a bad photo." However, this semantic similarity overlooks a critical yet underexplored cue: the magnitude of the CLIP image features, which we empirically find to exhibit a strong correlation with perceptual quality. In this work, we introduce a novel adaptive fusion framework that complements cosine similarity with a magnitude-aware quality cue. Specifically, we first extract the absolute CLIP image features and apply a Box-Cox transformation to statistically normalize the feature distribution and mitigate semantic sensitivity. The resulting scalar summary serves as a semantically-normalized auxiliary cue that complements cosine-based prompt matching. To integrate both cues effectively, we further design a confidence-guided fusion scheme that adaptively weighs each term according to its relative strength. Extensive experiments on multiple benchmark IQA datasets demonstrate that our method consistently outperforms standard CLIP-based IQA and state-of-the-art baselines, without any task-specific training.

</details>


### [34] [Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching](https://arxiv.org/abs/2511.09955)
*Uday Bhaskar,Rishabh Bhattacharya,Avinash Patel,Sarthak Khoche,Praveen Anil Kulkarni,Naresh Manwani*

Main category: cs.CV

TL;DR: Use VLMs to pseudo-label images and train two YOLOs with per-object co-teaching that filters unreliable boxes per object using peer losses, yielding large mAP gains and real-time speed


<details>
  <summary>Details</summary>
Motivation: Enable real-time object detectors for autonomous driving by leveraging VLMs to produce pseudo-labels while mitigating VLM noise and latency; reduce expensive human annotation

Method: Per-object co-teaching with VLM pseudo-labels

Result: Per-object co-teaching trains two YOLO models that filter noisy boxes by peer per-object loss; significantly improves KITTI mAP@0.5 from 31.12% to 46.61%; with 10% GT reaches 57.97%; similar gains on ACDC and BDD100k

Conclusion: Per-object co-teaching on VLM-generated pseudo-labels produces robust, efficient detectors for autonomous driving, substantially reducing annotation needs and improving detection accuracy while keeping real-time latency.

Abstract: Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\%$ to $46.61\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\%$) leads to further performance gains, reaching $57.97\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.

</details>


### [35] [Equivariant Sampling for Improving Diffusion Model-based Image Restoration](https://arxiv.org/abs/2511.09965)
*Chenxu Wu,Qingpeng Kong,Peiang Zhao,Wendi Yang,Wenxin Ma,Fenghe Tang,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出EquS，通过双采样轨迹强制等变信息并提出时间步感知调度（TAS）生成EquS+，提高扩散模型无任务特定限制下的图像恢复表现与采样效率，兼容既有方法并显著提升性能且不增算力。


<details>
  <summary>Details</summary>
Motivation: Existing problem-agnostic diffusion-based IR methods underuse diffusion priors leading to suboptimal restoration; need to analyze sampling and inject equivariant info.

Method: EquS with dual sampling and TAS

Result: EquS imposes equivariance via dual sampling trajectories; EquS+ adds Timestep-Aware Schedule emphasizing deterministic steps, improving certainty and efficiency; shows compatibility and significant performance gains without extra compute.

Conclusion: EquS和EquS+通过在采样中注入等变性与时间步调度，有效利用扩散先验，显著提升问题无关的图像恢复性能，且兼容并增强现有方法，无额外计算开销。

Abstract: Recent advances in generative models, especially diffusion models, have significantly improved image restoration (IR) performance. However, existing problem-agnostic diffusion model-based image restoration (DMIR) methods face challenges in fully leveraging diffusion priors, resulting in suboptimal performance. In this paper, we address the limitations of current problem-agnostic DMIR methods by analyzing their sampling process and providing effective solutions. We introduce EquS, a DMIR method that imposes equivariant information through dual sampling trajectories. To further boost EquS, we propose the Timestep-Aware Schedule (TAS) and introduce EquS$^+$. TAS prioritizes deterministic steps to enhance certainty and sampling efficiency. Extensive experiments on benchmarks demonstrate that our method is compatible with previous problem-agnostic DMIR methods and significantly boosts their performance without increasing computational costs. Our code is available at https://github.com/FouierL/EquS.

</details>


### [36] [Difference Vector Equalization for Robust Fine-tuning of Vision-Language Models](https://arxiv.org/abs/2511.09973)
*Satoshi Suzuki,Shin'ya Yamaguchi,Shoichiro Takeda,Taiga Yamane,Naoki Makishima,Naotaka Kawata,Mana Ihori,Tomohiro Tanaka,Shota Orihashi,Ryo Masumura*

Main category: cs.CV

TL;DR: 该论文提出DiVE方法，在微调对比预训练的视觉-语言模型（如CLIP）时，通过约束“差向量”（微调前后相同样本嵌入之差）一致性，保持嵌入几何结构，从而在保持ID性能的同时显著提高OOD与零-shot表现。方法包括全局的AVL和局部的PVL损失。实验表明DiVE在ID、OOD与零-shot上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 观察到当前在微调对比预训练视觉-语言模型时，直接重用对比学习会破坏嵌入的几何结构，而几何结构对于模型在OOD与零-shot条件下的泛化至关重要。因而需要一种在微调过程中保持原始嵌入几何结构的机制，以兼顾ID与OOD/零-shot性能。

Method: 提出约束差向量等化的框架：对每个样本计算预训练与微调模型嵌入的差向量，使用AVL使这些差向量接近加权平均向量（全局一致性），并使用PVL通过成对约束维护局部多模态对齐（局部一致性）。与传统重新使用对比损失的微调方法不同，DiVE不直接优化对比目标，而是保持几何结构。

Result: 实验显示DiVE能有效保持嵌入几何结构，比现有方法在ID、OOD和零-shot评估指标上取得更好或更稳健的结果。

Conclusion: DiVE通过使差向量在样本间保持一致（全局通过AVL、局部通过PVL），成功保留了预训练嵌入的几何结构，避免了现有重用对比学习微调方法的几何扭曲，从而在微调后仍能保持或提升模型的OOD与零-shot泛化能力，同时维持ID性能。

Abstract: Contrastive pre-trained vision-language models, such as CLIP, demonstrate strong generalization abilities in zero-shot classification by leveraging embeddings extracted from image and text encoders. This paper aims to robustly fine-tune these vision-language models on in-distribution (ID) data without compromising their generalization abilities in out-of-distribution (OOD) and zero-shot settings. Current robust fine-tuning methods tackle this challenge by reusing contrastive learning, which was used in pre-training, for fine-tuning. However, we found that these methods distort the geometric structure of the embeddings, which plays a crucial role in the generalization of vision-language models, resulting in limited OOD and zero-shot performance. To address this, we propose Difference Vector Equalization (DiVE), which preserves the geometric structure during fine-tuning. The idea behind DiVE is to constrain difference vectors, each of which is obtained by subtracting the embeddings extracted from the pre-trained and fine-tuning models for the same data sample. By constraining the difference vectors to be equal across various data samples, we effectively preserve the geometric structure. Therefore, we introduce two losses: average vector loss (AVL) and pairwise vector loss (PVL). AVL preserves the geometric structure globally by constraining difference vectors to be equal to their weighted average. PVL preserves the geometric structure locally by ensuring a consistent multimodal alignment. Our experiments demonstrate that DiVE effectively preserves the geometric structure, achieving strong results across ID, OOD, and zero-shot metrics.

</details>


### [37] [STELLAR: Scene Text Editor for Low-Resource Languages and Real-World Data](https://arxiv.org/abs/2511.09977)
*Yongdeuk Seo,Hyun-seok Min,Sungchul Choi*

Main category: cs.CV

TL;DR: 该论文提出STELLAR，一种针对低资源语言和真实场景的场景文本编辑方法，通过语言自适应字形编码器和先合成后真实的多阶段训练策略提升多语言编辑能力；构建STIPLAR数据集并提出无需GT即可评估风格保持的TAS指标。实验显示在视觉一致性和识别准确率上优于现有方法，TAS平均提升2.2%。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基场景文本编辑方法在低资源语言支持、合成到真实域差距以及文本风格保留评估指标不足方面存在不足，限制了实际应用。作者欲通过改进模型架构、训练策略和评价方法来克服这些限制。

Method: 提出语言自适应字形编码器以适应不同语言的字形差异；采用多阶段训练：先在大量合成数据上预训练，再在真实图像上微调；使用扩散模型为基础的编辑网络；构建STIPLAR数据集并设计基于字形、颜色、背景的复合TAS评价函数。

Result: 在构建的STIPLAR数据集上，STELLAR相比基线在视觉一致性和识别准确率有明显提升，平均TAS提高2.2%，并在多语言场景下展现更稳定的编辑质量。

Conclusion: STELLAR在多语言和真实场景文本编辑上表现更优，能更好保留字体、颜色和背景风格，且在新的STIPLAR数据集和TAS指标下取得显著改进，适用于低资源语言和现实图像。

Abstract: Scene Text Editing (STE) is the task of modifying text content in an image while preserving its visual style, such as font, color, and background. While recent diffusion-based approaches have shown improvements in visual quality, key limitations remain: lack of support for low-resource languages, domain gap between synthetic and real data, and the absence of appropriate metrics for evaluating text style preservation. To address these challenges, we propose STELLAR (Scene Text Editor for Low-resource LAnguages and Real-world data). STELLAR enables reliable multilingual editing through a language-adaptive glyph encoder and a multi-stage training strategy that first pre-trains on synthetic data and then fine-tunes on real images. We also construct a new dataset, STIPLAR(Scene Text Image Pairs of Low-resource lAnguages and Real-world data), for training and evaluation. Furthermore, we propose Text Appearance Similarity (TAS), a novel metric that assesses style preservation by independently measuring font, color, and background similarity, enabling robust evaluation even without ground truth. Experimental results demonstrate that STELLAR outperforms state-of-the-art models in visual consistency and recognition accuracy, achieving an average TAS improvement of 2.2% across languages over the baselines.

</details>


### [38] [MOBA: A Material-Oriented Backdoor Attack against LiDAR-based 3D Object Detection Systems](https://arxiv.org/abs/2511.09999)
*Saket S. Chaturvedi,Gaurav Bagwe,Lan Zhang,Pan He,Xiaoyong Yuan*

Main category: cs.CV

TL;DR: MOBA picks robust trigger material (TiO2), models its LiDAR reflectance via angle-independent Oren-Nayar approximation and distance-aware scaling, enabling highly effective physical backdoor attacks with 93.5% success, outperforming prior methods by 41%.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks fail physically because digital triggers ignore material-dependent LiDAR reflections; MOBA aims to bridge that gap to create realistic, robust physical backdoors.

Method: Select robust material (TiO2); simulate material LiDAR intensity via angle-independent Oren-Nayar BRDF approximation; apply distance-aware scaling; integrate simulated triggers into training to create backdoor.

Result: Material-Oriented Backdoor Attack (MOBA) models material properties to create physically realizable LiDAR backdoor triggers, selecting TiO2 and simulating its LiDAR reflectance to bridge digital-physical gap.

Conclusion: Material-aware trigger design makes backdoors physically realizable and highly effective, exposing urgent need for defenses that consider material properties.

Abstract: LiDAR-based 3D object detection is widely used in safety-critical systems. However, these systems remain vulnerable to backdoor attacks that embed hidden malicious behaviors during training. A key limitation of existing backdoor attacks is their lack of physical realizability, primarily due to the digital-to-physical domain gap. Digital triggers often fail in real-world settings because they overlook material-dependent LiDAR reflection properties. On the other hand, physically constructed triggers are often unoptimized, leading to low effectiveness or easy detectability.This paper introduces Material-Oriented Backdoor Attack (MOBA), a novel framework that bridges the digital-physical gap by explicitly modeling the material properties of real-world triggers. MOBA tackles two key challenges in physical backdoor design: 1) robustness of the trigger material under diverse environmental conditions, 2) alignment between the physical trigger's behavior and its digital simulation. First, we propose a systematic approach to selecting robust trigger materials, identifying titanium dioxide (TiO_2) for its high diffuse reflectivity and environmental resilience. Second, to ensure the digital trigger accurately mimics the physical behavior of the material-based trigger, we develop a novel simulation pipeline that features: (1) an angle-independent approximation of the Oren-Nayar BRDF model to generate realistic LiDAR intensities, and (2) a distance-aware scaling mechanism to maintain spatial consistency across varying depths. We conduct extensive experiments on state-of-the-art LiDAR-based and Camera-LiDAR fusion models, showing that MOBA achieves a 93.50% attack success rate, outperforming prior methods by over 41%. Our work reveals a new class of physically realizable threats and underscores the urgent need for defenses that account for material-level properties in real-world environments.

</details>


### [39] [DBGroup: Dual-Branch Point Grouping for Weakly Supervised 3D Instance Segmentation](https://arxiv.org/abs/2511.10003)
*Xuexun Liu,Xiaoxu Xu,Qiudan Zhang,Lin Ma,Xu Wang*

Main category: cs.CV

TL;DR: 提出DBGroup，两阶段：双分支点群生成伪标签 + 多轮自训练，使用场景级标签替代昂贵人工标注，效果优于部分现有方法。


<details>
  <summary>Details</summary>
Motivation: 减轻3D实例分割中高昂的人工标注负担，提出利用更易获得的场景级注释结合多视图线索来生成高质量伪标签，从而实现高效可扩展的弱监督学习。

Method: 第一阶段：Dual-Branch Point Grouping从多视图语义和掩码信息生成伪标签，随后用Granularity-Aware Instance Merging与Semantic Selection and Propagation细化；第二阶段：在端到端实例分割网络上进行多轮自训练，并使用Instance Mask Filter处理伪标签不一致性。

Result: DBGroup提出一种基于场景级注释的弱监督三维实例分割框架，通过多视图图像的语义与掩码线索生成伪标签，并结合多轮自训练提高性能。

Conclusion: 使用场景级注释配合多视图信息和精细的伪标签优化策略，DBGroup在弱监督3D实例分割任务上能达到接近稀疏点级监督的方法性能，并优于现有场景级语义分割方法。

Abstract: Weakly supervised 3D instance segmentation is essential for 3D scene understanding, especially as the growing scale of data and high annotation costs associated with fully supervised approaches. Existing methods primarily rely on two forms of weak supervision: one-thing-one-click annotations and bounding box annotations, both of which aim to reduce labeling efforts. However, these approaches still encounter limitations, including labor-intensive annotation processes, high complexity, and reliance on expert annotators. To address these challenges, we propose \textbf{DBGroup}, a two-stage weakly supervised 3D instance segmentation framework that leverages scene-level annotations as a more efficient and scalable alternative. In the first stage, we introduce a Dual-Branch Point Grouping module to generate pseudo labels guided by semantic and mask cues extracted from multi-view images. To further improve label quality, we develop two refinement strategies: Granularity-Aware Instance Merging and Semantic Selection and Propagation. The second stage involves multi-round self-training on an end-to-end instance segmentation network using the refined pseudo-labels. Additionally, we introduce an Instance Mask Filter strategy to address inconsistencies within the pseudo labels. Extensive experiments demonstrate that DBGroup achieves competitive performance compared to sparse-point-level supervised 3D instance segmentation methods, while surpassing state-of-the-art scene-level supervised 3D semantic segmentation approaches. Code is available at https://github.com/liuxuexun/DBGroup.

</details>


### [40] [LampQ: Towards Accurate Layer-wise Mixed Precision Quantization for Vision Transformers](https://arxiv.org/abs/2511.10004)
*Minjun Kim,Jaeri Lee,Jongjin Kim,Jeongin Yun,Yongmo Kwon,U Kang*

Main category: cs.CV

TL;DR: LampQ提出逐层混合精度量化与类型感知Fisher度量并通过整数线性规划优化位宽分配，在多个视觉任务上实现了对预训练ViT的高效且高精度量化。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法使用统一精度忽视了不同ViT组件对量化的不同敏感性；先前的基于度量的混合精度方法存在粒度粗、不同组件度量尺度不匹配和分配不考虑量化影响等缺陷。

Method: 对每一层进行逐层（layer-wise）混合精度量化；引入类型感知的Fisher基准度量来评估不同组件对量化的敏感性；将位宽分配建模为整数线性规划问题求解最优分配，并通过迭代更新位宽以提升性能。

Result: 在多种任务（图像分类、目标检测、零样本量化）和多种预训练ViT模型上，LampQ在保持低位宽的同时实现了最先进的性能，优于现有MPQ方法。

Conclusion: LampQ通过逐层混合精度量化、类型感知Fisher灵敏度度量和整数线性规划的最优位宽分配，实现了对预训练视觉Transformer的高精度低位量化，减少了计算和内存开销同时保持甚至优于现有方法的准确率。

Abstract: How can we accurately quantize a pre-trained Vision Transformer model? Quantization algorithms compress Vision Transformers (ViTs) into low-bit formats, reducing memory and computation demands with minimal accuracy degradation. However, existing methods rely on uniform precision, ignoring the diverse sensitivity of ViT components to quantization. Metric-based Mixed Precision Quantization (MPQ) is a promising alternative, but previous MPQ methods for ViTs suffer from three major limitations: 1) coarse granularity, 2) mismatch in metric scale across component types, and 3) quantization-unaware bit allocation. In this paper, we propose LampQ (Layer-wise Mixed Precision Quantization for Vision Transformers), an accurate metric-based MPQ method for ViTs to overcome these limitations. LampQ performs layer-wise quantization to achieve both fine-grained control and efficient acceleration, incorporating a type-aware Fisher-based metric to measure sensitivity. Then, LampQ assigns bit-widths optimally through integer linear programming and further updates them iteratively. Extensive experiments show that LampQ provides the state-of-the-art performance in quantizing ViTs pre-trained on various tasks such as image classification, object detection, and zero-shot quantization.

</details>


### [41] [MIRNet: Integrating Constrained Graph-Based Reasoning with Pre-training for Diagnostic Medical Imaging](https://arxiv.org/abs/2511.10013)
*Shufeng Kong,Zijie Wang,Nuan Cui,Hao Tang,Yihan Meng,Yuanyuan Wei,Feifan Chen,Yingheng Wang,Zhuo Cai,Yaonan Wang,Yulong Zhang,Yuzheng Li,Zibin Zheng,Caihua Liu*

Main category: cs.CV

TL;DR: 提出MIRNet：结合MAE自监督预训练、GAT标签图推理与约束感知优化，并发布4,000张舌诊图谱TongueAtlas-4K，以解决舌诊中的标注稀缺与类别不平衡问题，达到SOTA水平并具可推广性。


<details>
  <summary>Details</summary>
Motivation: 医学图像自动诊断需精细建模视觉-语义关系，同时克服标注稀缺、标签不平衡与保证临床可解释性和合理性。舌诊为具有细粒度视觉与语义挑战的典型任务。

Method: 方法包括：1) 使用MAE进行自监督视觉表征学习；2) 使用专家定义的结构化图并通过GAT建模标签间相关性；3) 通过KL散度与正则化损失引入临床先验的约束感知优化；4) 采用ASL不对称损失与提升式集成缓解类别不平衡；并构建并在TongueAtlas-4K上训练与验证。

Result: 在TongueAtlas-4K及其他验证集上，MIRNet取得了领先性能，证明了自监督+图推理+约束优化的组合在文本所述任务上的有效性，且方法可推广到更广泛的诊断影像任务。

Conclusion: MIRNet提出了一种将自监督预训练与受限图推理结合的框架，专为解决医学影像（尤其舌诊）中标注稀缺、类别不平衡及临床先验约束而设计。

Abstract: Automated interpretation of medical images demands robust modeling of complex visual-semantic relationships while addressing annotation scarcity, label imbalance, and clinical plausibility constraints. We introduce MIRNet (Medical Image Reasoner Network), a novel framework that integrates self-supervised pre-training with constrained graph-based reasoning. Tongue image diagnosis is a particularly challenging domain that requires fine-grained visual and semantic understanding. Our approach leverages self-supervised masked autoencoder (MAE) to learn transferable visual representations from unlabeled data; employs graph attention networks (GAT) to model label correlations through expert-defined structured graphs; enforces clinical priors via constraint-aware optimization using KL divergence and regularization losses; and mitigates imbalance using asymmetric loss (ASL) and boosting ensembles. To address annotation scarcity, we also introduce TongueAtlas-4K, a comprehensive expert-curated benchmark comprising 4,000 images annotated with 22 diagnostic labels--representing the largest public dataset in tongue analysis. Validation shows our method achieves state-of-the-art performance. While optimized for tongue diagnosis, the framework readily generalizes to broader diagnostic medical imaging tasks.

</details>


### [42] [AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models](https://arxiv.org/abs/2511.10017)
*Xinyi Wang,Xun Yang,Yanlong Xu,Yuchen Wu,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: 本文提出Fine-grained 3D Embodied Reasoning任务，要求基于指令为3D场景中每个可交互元素预测位置、运动类型和运动轴。提出AffordBot框架，将MLLMs与链式思维结合，通过渲染环视图并将3D候选投影到2D视图来对齐几何信息；引入主动感知阶段选择最有信息的视角，再逐步推理以定位与推断交互动作。该方法在SceneFun3D上取得了SOTA，表现出基于点云的物理推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在物体层面或单独处理可交互性，缺乏统一的、由指令驱动的细粒度关联与推理。作者希望构建能在3D环境中、基于指令对可交互元素进行精确定位与物理一致的动作推断的系统。

Method: 1) 定义任务Fine-grained 3D Embodied Reasoning，输出每个被引用元素的(位置,运动类型,运动轴)三元组；2) 将3D点云渲染为多视角2D图像，并将3D候选元素投影到这些视图以形成与几何对齐的视觉表征；3) 设计CoT推理流程，先由MLLM在主动感知阶段选择最有信息视角，再逐步定位元素并推断交互动作；4) 在SceneFun3D上训练/评估并与基线比较。

Result: 在SceneFun3D上，AffordBot取得了SOTA表现，显示了强泛化能力和有物理依据的推理能力，且只使用3D点云输入与MLLMs即可实现竞争性效果。

Conclusion: AffordBot利用MLLM与特定CoT策略、环视渲染和主动视角选择，有效解决细粒度3D可交互推理任务，在SceneFun3D上实现领先性能，证明了仅用点云与MLLM即可进行物理有据推理和泛化。

Abstract: Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.

</details>


### [43] [Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation](https://arxiv.org/abs/2511.10020)
*Yuxin Jiang,Wei Luo,Hui Zhang,Qiyu Chen,Haiming Yao,Weiming Shen,Yunkang Cao*

Main category: cs.CV

TL;DR: Anomagic是一种零样本、跨模态提示引导的异常生成方法，配合AnomVerse数据集和对比精炼策略，能生成更真实多样的异常并提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 动机是解决异常样本稀缺问题，通过零样本合成语义一致且多样的异常来提升监督或无监督异常检测系统的鲁棒性与泛化能力。

Method: 方法包括：1) 跨模态提示编码，融合视觉和文本信息以引导修补(inpainting)生成；2) 基于修补的生成管线用于合成异常区域；3) 对比精炼策略使合成异常与掩码精确对齐；4) 构建AnomVerse用于训练，利用多模态大模型自动生成异常描述。

Result: Anomagic提出了一种无示例的异常生成方法，通过跨模态提示编码统一视觉和文本线索，引导基于修补的生成流程，并用对比优化精炼异常与掩码的对齐。数据集AnomVerse由12987个异常-掩码-描述三元组构成，描述由多模态大模型自动生成。实验表明Anomagic在可视真实感、多样性及下游异常检测性能上优于先前方法，并支持用户自定义提示生成任意正常类别图像的异常。

Conclusion: Anomagic能在无异常示例条件下生成语义一致的高质量异常，结合AnomVerse可显著提升下游异常检测，并支持用提示生成任意类别的异常，具备成为异常生成基础模型的潜力。

Abstract: We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.

</details>


### [44] [DGFusion: Dual-guided Fusion for Robust Multi-Modal 3D Object Detection](https://arxiv.org/abs/2511.10035)
*Feiyang Jia,Caiyan Jia,Ailin Liu,Shaoqing Xu,Qiming Xia,Lin Liu,Lei Yang,Yan Gong,Ziying Song*

Main category: cs.CV

TL;DR: 本文提出DGFusion，一种双引导（Dual-guided）多模态3D目标检测框架，通过Difficulty-aware Instance Pair Matcher按难度配对实例（easy/hard），并设计Dual-guided模块分别利用不同配对进行特征融合，从而提升远距、小型和遮挡目标的检测性能。在nuScenes上对比基线取得mAP、NDS和平均召回的稳定提升，并在多种困难因素上显示出鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D检测通常采用单一引导范式（如Point-guide-Image或Image-guide-Point），忽视了不同模态在困难实例上的信息密度差异，导致对远距、小型或遮挡目标效果不佳。为此提出双引导范式并基于实例难度进行针对性融合以增强对困难实例的感知能力。

Method: 提出Difficulty-aware Instance Pair Matcher（DIPM），对实例按难度进行匹配生成easy和hard实例对；设计Dual-guided Modules分别利用easy pair和hard pair的互补信息进行双向（点云引导图像与图像引导点云）特征融合；在融合策略上兼顾密度与语义互补，最终整合到多模态检测网络进行训练与评估。

Result: 在nuScenes数据集上相较基线提升+1.0% mAP、+0.8% NDS和+1.3%平均召回；在不同ego-distance、尺寸、可见性及小样本训练场景下，对困难实例检测表现均有一致的鲁棒性提升。

Conclusion: DGFusion通过融合Point-guide-Image与Image-guide-Point两种范式，并依据实例难度进行有针对性的配对与融合，能有效提升多模态3D检测器对困难实例（远距、体积小、遮挡等）的检测性能，实验在nuScenes上显示出明显且一致的改进。

Abstract: As a critical task in autonomous driving perception systems, 3D object detection is used to identify and track key objects, such as vehicles and pedestrians. However, detecting distant, small, or occluded objects (hard instances) remains a challenge, which directly compromises the safety of autonomous driving systems. We observe that existing multi-modal 3D object detection methods often follow a single-guided paradigm, failing to account for the differences in information density of hard instances between modalities. In this work, we propose DGFusion, based on the Dual-guided paradigm, which fully inherits the advantages of the Point-guide-Image paradigm and integrates the Image-guide-Point paradigm to address the limitations of the single paradigms. The core of DGFusion, the Difficulty-aware Instance Pair Matcher (DIPM), performs instance-level feature matching based on difficulty to generate easy and hard instance pairs, while the Dual-guided Modules exploit the advantages of both pair types to enable effective multi-modal feature fusion. Experimental results demonstrate that our DGFusion outperforms the baseline methods, with respective improvements of +1.0\% mAP, +0.8\% NDS, and +1.3\% average recall on nuScenes. Extensive experiments demonstrate consistent robustness gains for hard instance detection across ego-distance, size, visibility, and small-scale training scenarios.

</details>


### [45] [LoG3D: Ultra-High-Resolution 3D Shape Modeling via Local-to-Global Partitioning](https://arxiv.org/abs/2511.10040)
*Xinran Yang,Shuichang Lai,Jiangjing Lyu,Hongjie Li,Bowen Pan,Yuanqi Li,Jie Guo,Zhou Zhengkang,Yanwen Guo*

Main category: cs.CV

TL;DR: 提出基于UDF的3D VAE，采用UBlock分块、3D卷积+稀疏Transformer的LoG架构，并用Pad-Average平滑边界，支持最高2048^3分辨率，提升重建与生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决SDF需封闭预处理和点云采样噪声的问题，提供对开口及复杂内部结构鲁棒的高保真3D表示和生成方法。

Method: 构建以UDF为输入的VAE；将体素网格划分为UBlocks；在UBlock内用3D卷积学习局部特征，跨UBlock用稀疏Transformer建模全局一致性；重建时用Pad-Average融合子体积边界；训练在多分辨率策略下扩展到2048^3。

Result: Paper proposes a 3D VAE using unsigned distance fields (UDFs) with a local-to-global (LoG) architecture; partitions UDF into UBlocks, uses 3D convolutions for local detail and sparse transformers for global coherence; Pad-Average for boundary smoothing; scales to 2048^3; claims SOTA reconstruction and generative quality.

Conclusion: 使用UDF和LoG模块化设计可有效处理复杂非流形拓扑和不完整形状，在高分辨率下实现更平滑、更灵活的几何重建与生成，优于SDF和点云方法。

Abstract: Generating high-fidelity 3D contents remains a fundamental challenge due to the complexity of representing arbitrary topologies-such as open surfaces and intricate internal structures-while preserving geometric details. Prevailing methods based on signed distance fields (SDFs) are hampered by costly watertight preprocessing and struggle with non-manifold geometries, while point-cloud representations often suffer from sampling artifacts and surface discontinuities. To overcome these limitations, we propose a novel 3D variational autoencoder (VAE) framework built upon unsigned distance fields (UDFs)-a more robust and computationally efficient representation that naturally handles complex and incomplete shapes. Our core innovation is a local-to-global (LoG) architecture that processes the UDF by partitioning it into uniform subvolumes, termed UBlocks. This architecture couples 3D convolutions for capturing local detail with sparse transformers for enforcing global coherence. A Pad-Average strategy further ensures smooth transitions at subvolume boundaries during reconstruction. This modular design enables seamless scaling to ultra-high resolutions up to 2048^3-a regime previously unattainable for 3D VAEs. Experiments demonstrate state-of-the-art performance in both reconstruction accuracy and generative quality, yielding superior surface smoothness and geometric flexibility.

</details>


### [46] [FreDFT: Frequency Domain Fusion Transformer for Visible-Infrared Object Detection](https://arxiv.org/abs/2511.10046)
*Wencong Wu,Xiuwei Zhang,Hanlin Yin,Shun Dai,Hongxi Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出FreDFT，通过频域注意力与频域前馈、跨模态全局建模和局部增强模块，解决可见-红外模态信息不平衡问题，提升多模态目标检测性能，且在公开数据集上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在空间域用Transformer挖掘互补特征，忽视频域Transformer的潜力；同时可见与红外模态在复杂场景存在信息不均衡，影响融合效果。

Method: 提出多模态频域注意力（MFDA）在频域挖掘互补信息，设计频域前馈层（FDFFL）结合混合尺度频域特征融合策略；构建跨模态全局建模模块（CGMM）实现像素级的通道与空间交互；开发局部特征增强模块（LFEM）通过多种卷积和通道洗牌增强局部表征。

Result: 在多个公开数据集上，FreDFT较现有最先进方法取得了更优的检测性能，实验结果验证了方法的有效性。

Conclusion: 本文提出了基于频域变换器的多模态融合方法FreDFT，旨在缓解可见光与红外图像间的信息不平衡，从而提升在复杂天气和低照环境下的目标检测性能。

Abstract: Visible-infrared object detection has gained sufficient attention due to its detection performance in low light, fog, and rain conditions. However, visible and infrared modalities captured by different sensors exist the information imbalance problem in complex scenarios, which can cause inadequate cross-modal fusion, resulting in degraded detection performance. \textcolor{red}{Furthermore, most existing methods use transformers in the spatial domain to capture complementary features, ignoring the advantages of developing frequency domain transformers to mine complementary information.} To solve these weaknesses, we propose a frequency domain fusion transformer, called FreDFT, for visible-infrared object detection. The proposed approach employs a novel multimodal frequency domain attention (MFDA) to mine complementary information between modalities and a frequency domain feed-forward layer (FDFFL) via a mixed-scale frequency feature fusion strategy is designed to better enhance multimodal features. To eliminate the imbalance of multimodal information, a cross-modal global modeling module (CGMM) is constructed to perform pixel-wise inter-modal feature interaction in a spatial and channel manner. Moreover, a local feature enhancement module (LFEM) is developed to strengthen multimodal local feature representation and promote multimodal feature fusion by using various convolution layers and applying a channel shuffle. Extensive experimental results have verified that our proposed FreDFT achieves excellent performance on multiple public datasets compared with other state-of-the-art methods. The code of our FreDFT is linked at https://github.com/WenCongWu/FreDFT.

</details>


### [47] [MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples](https://arxiv.org/abs/2511.10047)
*Xurui Li,Feng Xue,Yu Zhou*

Main category: cs.CV

TL;DR: 提出MuSc-V2，基于2D/3D相似性邻域互评分机制与多模块融合，大幅提升零样本缺陷检测与分割性能。


<details>
  <summary>Details</summary>
Motivation: Reveal and leverage overlooked property that normal patches find many similar patches in 2D/3D while anomalies are isolated, to improve zero-shot anomaly classification/segmentation.

Method: IPG改善3D表示；SNAMD融合2D/3D多尺度邻域特征；MSM在模态内互评分；CAE跨模态融合得分；RsCon基于代表性样本相似性重评分。

Result: MuSc-V2 framework with IPG, SNAMD, MSM, CAE, RsCon; supports 2D/3D/multimodal; achieves +23.7% AP on MVTec 3D-AD and +19.3% on Eyecandies, outperforming zero-shot and many few-shot methods.

Conclusion: MuSc-V2通过改进3D表示与多模态邻域聚合，并引入互评分与重评分策略，有效抑制假阳性并恢复模态特有缺陷，实现对多产品线的鲁棒零样本AC/AS。

Abstract: Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\textbf{+23.7\%}$ AP gain on the MVTec 3D-AD dataset and a $\textbf{+19.3\%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \href{https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}.

</details>


### [48] [Image Aesthetic Reasoning via HCM-GRPO: Empowering Compact Model for Superior Performance](https://arxiv.org/abs/2511.10055)
*Zhiyuan Hu,Zheng Sun,Yi Wei,Long Yu*

Main category: cs.CV

TL;DR: 本文提出了针对图像筛选的完整解决方案：构建了128k样本的多视图图像筛选数据集，并提出了在GRPO框架下带动态奖励的困难样本挖掘方法HCM-GRPO，同时探索了多种CoT标注方式。实验显示闭源MLLM在图像美学推理上接近随机，但HCM-GRPO可用更小模型超越大型开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在图像筛选/美学推理任务上表现差、缺乏数据与模型的视觉审美推理能力，因此需要从数据和方法两方面改进。

Method: 数据方面：收集128k样本、约640k张图像的图像筛选数据集，每条样本含一张原图与四张生成图，评估四个维度（外观变形、物理阴影、放置布局、延展合理性）；标注方面：比较人工、自动与答案驱动的CoT获取方式；算法方面：在GRPO框架中提出HCM（困难样本挖掘）策略，并设计DPA（动态比例准确性）奖励，形成HCM-GRPO。

Result: 实验证明：闭源模型如GPT4o、Qwen-VL-Max在该任务上接近随机；采用HCM-GRPO的小模型在图像美学推理上超越了大规模开源及领先闭源模型。

Conclusion: 通过构建大规模、多维度的图像筛选数据集并引入HCM-GRPO训练策略（包含DPA奖励），可显著提升小模型在图像美学推理任务上的性能，优于当前主流开源与闭源MLLM。

Abstract: The performance of image generation has been significantly improved in recent years. However, the study of image screening is rare and its performance with Multimodal Large Language Models (MLLMs) is unsatisfactory due to the lack of data and the weak image aesthetic reasoning ability in MLLMs. In this work, we propose a complete solution to address these problems in terms of data and methodology. For data, we collect a comprehensive image screening dataset with over 128k samples, about 640k images. Each sample consists of an original image, four generated images. The dataset evaluates the image aesthetic reasoning ability under four aspects: appearance deformation, physical shadow, placement layout, and extension rationality. Regarding data annotation, we investigate multiple approaches, including purely manual, fully automated, and answer-driven annotations, to acquire high-quality chains of thought (CoT) data in the most cost-effective manner. Methodologically, we introduce a Hard Cases Mining (HCM) strategy with a Dynamic Proportional Accuracy (DPA) reward into the Group Relative Policy Optimization (GRPO) framework, called HCM-GRPO. This enhanced method demonstrates superior image aesthetic reasoning capabilities compared to the original GRPO. Our experimental results reveal that even state-of-the-art closed-source MLLMs, such as GPT4o and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic reasoning. In contrast, by leveraging the HCM-GRPO, we are able to surpass the scores of both large-scale open-source and leading closed-source models with a much smaller model.

</details>


### [49] [When Eyes and Ears Disagree: Can MLLMs Discern Audio-Visual Confusion?](https://arxiv.org/abs/2511.10059)
*Qilang Ye,Wei Zeng,Meng Liu,Jie Zhang,Yupeng Hu,Zitong Yu,Yu Zhou*

Main category: cs.CV

TL;DR: 本文提出AV-ConfuseBench基准，模拟视觉存在但音频缺失的“视听混淆”场景，发现现有多模态大模型在此任务上受视觉主导，难以判断声音是否存在；提出基于强化学习的RL-CoMM方法，利用外部大音频语言模型（LALM）提供音频推理参考，通过分步推理奖励和答案中心置信度优化提升音频-视觉推理能力，在少量训练下对AVQA和视听幻觉任务提升10~30%。


<details>
  <summary>Details</summary>
Motivation: 观察到现有MLLMs在多模态推理中视觉信息主导，导致无法区分视觉存在但音频缺失的对象，需构建专门基准评估并提出方法提升模型的音频感知与多模态协同推理能力。

Method: 提出AV-ConfuseBench构造方法（修改视频中对象对应声音，如静音化），并设计RL-CoMM两阶段训练：1) 使用外部LALM生成音频推理作为参考，设计Step-wise Reasoning Reward促使MLLM学习音频-视觉一致性；2) 引入Answer-centered Confidence Optimization降低不同推理路径间的不确定性，从而提高答案准确性。训练基于Qwen2.5-Omni，使用强化学习（奖励设计）与协同多模型机制。

Result: 在AVQA和音视幻觉测试上，RL-CoMM在有限训练数据下相比基线提升10~30%的准确率；实验证明LALM作为音频参考能有效引导MLLM改善音频-视觉辨别能力。

Conclusion: RL-CoMM能有效缓解视觉主导带来的视听混淆，在AV-ConfuseBench等任务上显著提升音频-视觉推理准确率，证明将LALM作为音频参考并通过强化学习优化是可行且有效的策略。

Abstract: Can Multimodal Large Language Models (MLLMs) discern confused objects that are visually present but audio-absent? To study this, we introduce a new benchmark, AV-ConfuseBench, which simulates an ``Audio-Visual Confusion'' scene by modifying the corresponding sound of an object in the video, e.g., mute the sounding object and ask MLLMs Is there a/an muted-object sound''. Experimental results reveal that MLLMs, such as Qwen2.5-Omni and Gemini 2.5, struggle to discriminate non-existent audio due to visually dominated reasoning. Motivated by this observation, we introduce RL-CoMM, a Reinforcement Learning-based Collaborative Multi-MLLM that is built upon the Qwen2.5-Omni foundation. RL-CoMM includes two stages: 1) To alleviate visually dominated ambiguities, we introduce an external model, a Large Audio Language Model (LALM), as the reference model to generate audio-only reasoning. Then, we design a Step-wise Reasoning Reward function that enables MLLMs to self-improve audio-visual reasoning with the audio-only reference. 2) To ensure an accurate answer prediction, we introduce Answer-centered Confidence Optimization to reduce the uncertainty of potential heterogeneous reasoning differences. Extensive experiments on audio-visual question answering and audio-visual hallucination show that RL-CoMM improves the accuracy by 10~30\% over the baseline model with limited training data. Follow: https://github.com/rikeilong/AVConfusion.

</details>


### [50] [Multivariate Gaussian Representation Learning for Medical Action Evaluation](https://arxiv.org/abs/2511.10060)
*Luming Yang,Haoxian Liu,Siqing Li,Alper Yilmaz*

Main category: cs.CV

TL;DR: New dataset CPREval-6k and Gaussian token-based GaussMedAct improve fine-grained medical action evaluation with high accuracy and efficiency


<details>
  <summary>Details</summary>
Motivation: Address lack of datasets and poor modeling of rapid actions in medical vision

Method: Paper Analysis

Result: Introduced CPREval-6k dataset and GaussMedAct method achieving 92.1% Top-1, outperforming ST-GCN by 5.9% with 10% FLOPs

Conclusion: GaussMedAct with multivariate Gaussian tokens and hybrid spatial encoding provides robust, efficient motion representation for medical action recognition.

Abstract: Fine-grained action evaluation in medical vision faces unique challenges due to the unavailability of comprehensive datasets, stringent precision requirements, and insufficient spatiotemporal dynamic modeling of very rapid actions. To support development and evaluation, we introduce CPREval-6k, a multi-view, multi-label medical action benchmark containing 6,372 expert-annotated videos with 22 clinical labels. Using this dataset, we present GaussMedAct, a multivariate Gaussian encoding framework, to advance medical motion analysis through adaptive spatiotemporal representation learning. Multivariate Gaussian Representation projects the joint motions to a temporally scaled multi-dimensional space, and decomposes actions into adaptive 3D Gaussians that serve as tokens. These tokens preserve motion semantics through anisotropic covariance modeling while maintaining robustness to spatiotemporal noise. Hybrid Spatial Encoding, employing a Cartesian and Vector dual-stream strategy, effectively utilizes skeletal information in the form of joint and bone features. The proposed method achieves 92.1% Top-1 accuracy with real-time inference on the benchmark, outperforming the ST-GCN baseline by +5.9% accuracy with only 10% FLOPs. Cross-dataset experiments confirm the superiority of our method in robustness.

</details>


### [51] [Perceive, Act and Correct: Confidence Is Not Enough for Hyperspectral Classification](https://arxiv.org/abs/2511.10068)
*Muzhou Yang,Wuzhou Quan,Mingqiang Wei*

Main category: cs.CV

TL;DR: CABIN通过不确定性估计、双重采样和细粒度伪标签分配，形成感知—行动—纠正闭环，改善半监督高光谱分类中的确认偏差和伪标签噪声，提高了性能和标注效率。


<details>
  <summary>Details</summary>
Motivation: 在高光谱图像分类中，单靠预测置信度常常误导模型，尤其在标注稀少或类别不平衡时，模型会高置信地犯错并被自训放大，导致确认偏差和泛化下降。

Method: CABIN首先估计模型的认知不确定性以识别易错区域；然后通过不确定性引导的双重采样策略（选择不确定样本进行探索，使用高置信样本作为稳定伪标签）进行行动；最后采用细粒度动态分配策略将伪标签数据划分为可靠、模糊和噪声三类，并对不同类别施加定制损失以纠正噪声监督。

Result: 在实验中，将CABIN集成到多种现有先进方法中均能带来性能提升和标注效率提高，证明其在减少确认偏差和改善伪标签质量方面有效。

Conclusion: 本文提出的CABIN通过闭环的感知-行动-纠正机制，显著缓解了高置信度误判带来的确认偏差，提升了半监督高光谱图像分类的泛化能力和标注效率。

Abstract: Confidence alone is often misleading in hyperspectral image classification, as models tend to mistake high predictive scores for correctness while lacking awareness of uncertainty. This leads to confirmation bias, especially under sparse annotations or class imbalance, where models overfit confident errors and fail to generalize. We propose CABIN (Cognitive-Aware Behavior-Informed learNing), a semi-supervised framework that addresses this limitation through a closed-loop learning process of perception, action, and correction. CABIN first develops perceptual awareness by estimating epistemic uncertainty, identifying ambiguous regions where errors are likely to occur. It then acts by adopting an Uncertainty-Guided Dual Sampling Strategy, selecting uncertain samples for exploration while anchoring confident ones as stable pseudo-labels to reduce bias. To correct noisy supervision, CABIN introduces a Fine-Grained Dynamic Assignment Strategy that categorizes pseudo-labeled data into reliable, ambiguous, and noisy subsets, applying tailored losses to enhance generalization. Experimental results show that a wide range of state-of-the-art methods benefit from the integration of CABIN, with improved labeling efficiency and performance.

</details>


### [52] [VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System](https://arxiv.org/abs/2511.10074)
*Gwangyeon Ahn,Jiwan Seo,Joonhyuk Kang*

Main category: cs.CV

TL;DR: VLF-MSC 通过传输单一视觉-语言特征实现多模态语义通信，避免为每种模态单独传输，提升频谱效率并在低SNR下保持更高语义准确性。


<details>
  <summary>Details</summary>
Motivation: 传统语义通信通常为每种模态单独处理，导致重复传输和频谱低效；提出通过统一视觉-语言表示来支持多模态生成，以减少带宽、提高适配性并增强在噪声环境中的鲁棒性。

Method: 1) 使用预训练视觉-语言模型将源图像编码为紧凑视觉-语言特征（VLF）；2) 对 VLF 进行信道编码并通过无线信道传输；3) 接收端以 VLF 为条件，分别用解码器语言模型生成文本描述，用扩散模型生成语义对齐图像；4) 评价语义准确性、带宽利用和抗噪能力。

Result: VLF-MSC 提出了一种基于视觉-语言特征的多模态语义通信系统，通过传输单一紧凑的视觉-语言表示（VLF）来同时支持接收端的图像和文本生成。系统利用预训练视觉-语言模型编码图像为 VLF，再通过无线信道传输；接收端使用基于解码器的语言模型和基于扩散的图像生成器，分别以 VLF 为条件生成描述性文本和语义一致的图像。与逐模态处理的方法相比，该方法节省带宽、提高频谱效率，并对信道噪声具有鲁棒性。实验表明在低信噪比和显著降低带宽下，VLF-MSC 在文本和图像语义准确性上均优于仅文本或仅图像的基线模型。

Conclusion: 使用预训练视觉-语言模型生成的统一特征可同时驱动语言和图像生成器，实现带宽节省与语义保真度提升，且在噪声信道下表现更优。

Abstract: We propose Vision-Language Feature-based Multimodal Semantic Communication (VLF-MSC), a unified system that transmits a single compact vision-language representation to support both image and text generation at the receiver. Unlike existing semantic communication techniques that process each modality separately, VLF-MSC employs a pre-trained vision-language model (VLM) to encode the source image into a vision-language semantic feature (VLF), which is transmitted over the wireless channel. At the receiver, a decoder-based language model and a diffusion-based image generator are both conditioned on the VLF to produce a descriptive text and a semantically aligned image. This unified representation eliminates the need for modality-specific streams or retransmissions, improving spectral efficiency and adaptability. By leveraging foundation models, the system achieves robustness to channel noise while preserving semantic fidelity. Experiments demonstrate that VLF-MSC outperforms text-only and image-only baselines, achieving higher semantic accuracy for both modalities under low SNR with significantly reduced bandwidth.

</details>


### [53] [Mitigating Error Accumulation in Co-Speech Motion Generation via Global Rotation Diffusion and Multi-Level Constraints](https://arxiv.org/abs/2511.10076)
*Xiangyue Zhang,Jianfang Li,Jianqiang Ren,Jiaxu Zhang*

Main category: cs.CV

TL;DR: 提出GlobalDiff，一种在全局关节旋转空间上进行扩散建模的共语伴随动作生成方法，通过多层结构约束（关节、骨骼、时间）补偿全局表示缺乏先验，减少层级误差累积，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在局部旋转（基于骨架层级）上建模会导致上游依赖与误差累积，尤其在末端执行器处表现为不稳定与不可信动作；因此提出直接在全局旋转空间建模以解耦依赖。

Method: 提出在全局关节旋转空间上构建扩散模型，并设计三类约束：关节结构约束（通过虚拟锚点捕捉细粒度朝向）、骨骼结构约束（保持骨骼角度一致性）、时间结构约束（多尺度变分编码器对齐时序模式）。这些约束在训练和采样阶段共同正则化扩散过程。

Result: 在标准共语数据集上广泛评估，GlobalDiff生成的动作更平滑、准确，在多说话者身份设置下相比当前SOTA提升了46.0%。

Conclusion: GlobalDiff在全局旋转空间上消除了层级依赖并通过多层约束恢复结构信息，从而生成更平滑、更准确的共语动作，在多个说话者场景下相比SOTA提升约46%。

Abstract: Reliable co-speech motion generation requires precise motion representation and consistent structural priors across all joints. Existing generative methods typically operate on local joint rotations, which are defined hierarchically based on the skeleton structure. This leads to cumulative errors during generation, manifesting as unstable and implausible motions at end-effectors. In this work, we propose GlobalDiff, a diffusion-based framework that operates directly in the space of global joint rotations for the first time, fundamentally decoupling each joint's prediction from upstream dependencies and alleviating hierarchical error accumulation. To compensate for the absence of structural priors in global rotation space, we introduce a multi-level constraint scheme. Specifically, a joint structure constraint introduces virtual anchor points around each joint to better capture fine-grained orientation. A skeleton structure constraint enforces angular consistency across bones to maintain structural integrity. A temporal structure constraint utilizes a multi-scale variational encoder to align the generated motion with ground-truth temporal patterns. These constraints jointly regularize the global diffusion process and reinforce structural awareness. Extensive evaluations on standard co-speech benchmarks show that GlobalDiff generates smooth and accurate motions, improving the performance by 46.0 % compared to the current SOTA under multiple speaker identities.

</details>


### [54] [GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2511.10081)
*Yuxiang Duan,Ao Li,Yingqin Li,Luyu Li,Pengwei Wang*

Main category: cs.CV

TL;DR: GridPrune replaces global Top-K with guide-globally select-locally zonal selection, improving efficiency and performance in MLLMs.


<details>
  <summary>Details</summary>
Motivation: Reduce computational overhead from many visual tokens in MLLMs by mimicking human two-stage attention (where to look, what to select).

Method: Paper analysis

Result: GridPrune uses text-conditional zonal token budget allocation then local selection; on LLaVA-NeXT-7B retains 96.98% performance with 11.1% tokens, +2.34% vs best baseline.

Conclusion: Text-guided zonal allocation plus local selection yields better token efficiency and accuracy than global Top-K pruning methods.

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities in a wide range of vision-language tasks. However, the large number of visual tokens introduces significant computational overhead. To address this issue, visual token pruning has emerged as a key technique for enhancing the efficiency of MLLMs. In cognitive science, humans tend to first determine which regions of a scene to attend to ("where to look") before deciding which specific elements within those regions to process in detail ("what to select"). This two-stage strategy enables the visual system to efficiently allocate attention at a coarse spatial level before performing fine-grained selection. However, existing pruning methods primarily focus on directly optimizing "what to select", typically using attention scores or similarity metrics. They rarely consider "where to look", which has been shown to lead to inefficient spatial allocation, positional bias, and the retention of irrelevant or redundant tokens. In this paper, we propose GridPrune, a method that replaces the global Top-K mechanism with a "guide-globally, select-locally" zonal selection system. GridPrune splits the pruning process into two steps: first, it uses text-conditional guidance to dynamically allocate a token budget across spatial zones; and then, it performs local selection within each budgeted zone. Experimental results demonstrate that GridPrune achieves superior performance across various MLLM architectures. On LLaVA-NeXT-7B, GridPrune retains 96.98% of the full performance while using 11.1% of the tokens, outperforming the best-performing baseline by 2.34% at the same pruning rate.

</details>


### [55] [SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition](https://arxiv.org/abs/2511.10091)
*Qilang Ye,Yu Zhou,Lian He,Jie Zhang,Xuanming Guo,Jiayu Zhang,Mingkui Tan,Weicheng Xie,Yue Sun,Tao Tan,Xiaochen Yuan,Ghada Khoriba,Zitong Yu*

Main category: cs.CV

TL;DR: 提出SUGAR框架，用大规模视频模型生成视觉/运动先验，监督骨架离散表示学习，再用未调权重的LLM理解表示完成动作分类与描述；引入Temporal Query Projection模块处理长时间骨架序列，在零样本与基准数据集上表现优于线性方法。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的丰富知识与迁移能力，弥补骨架数据在视觉信息上的缺失，使LLM能理解并区分骨架动作，实现更通用的动作识别与自然语言描述。

Method: 1) 用大规模视频模型抽取视觉与运动相关先验；2) 监督骨架编码器学习离散表示；3) 设计Temporal Query Projection模块以建模长序列骨架信号；4) 将离散表示输入未调权重的LLM生成动作类别与描述。

Result: 在若干骨架动作分类基准上SUGAR表现良好，并在零样本场景下相比线性基线更具泛化能力。

Conclusion: SUGAR能有效将视觉-运动知识注入骨架表示，使未调整权重的LLM能够理解并区分动作，实现有竞争力的有监督与零样本动作识别与描述。

Abstract: Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.

</details>


### [56] [MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models](https://arxiv.org/abs/2511.10098)
*Zihan Wang,Guansong Pang,Wenjun Miao,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: 提出MTAttack，一种针对大规模视觉语言模型的多目标后门攻击方法，通过潜在空间分区和触发原型锚定约束，同时优化多个触发器，实现在单次训练中对不同触发器映射到不同恶意目标，从而在多个基准上达到高攻击成功率并具备一定防御鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前后门攻击研究多聚焦于单目标触发器，现实应用中多目标后门更具危害且更难实现，LVLMs在多触发情况下由于特征干扰带来大量错误映射，需提出新的方法进行精确控制。

Method: 提出在潜在空间联合优化多触发器的框架，核心为Proxy Space Partitioning（将潜在空间分区以避免触发间干扰）和Trigger Prototype Anchoring（用原型约束保证每个触发器把图像映射到独特代理类别）两项约束，确保触发器独立且可分离。

Result: 在多个流行基准数据集上，MTAttack在多目标攻击成功率上显著优于现有方法；同时展示了对不同数据集的泛化能力以及对若干后门防御策略具备一定鲁棒性。

Conclusion: LVLMs对多目标后门攻击存在显著脆弱性。MTAttack成功实现了多个独立触发器到不同目标的准确映射，并在实验中表现出高成功率、良好泛化性及对常见防御方法的抗性，提示需要尽快采取缓解措施保护LVLM部署安全。

Abstract: Recent advances in Large Visual Language Models (LVLMs) have demonstrated impressive performance across various vision-language tasks by leveraging large-scale image-text pretraining and instruction tuning. However, the security vulnerabilities of LVLMs have become increasingly concerning, particularly their susceptibility to backdoor attacks. Existing backdoor attacks focus on single-target attacks, i.e., targeting a single malicious output associated with a specific trigger. In this work, we uncover multi-target backdoor attacks, where multiple independent triggers corresponding to different attack targets are added in a single pass of training, posing a greater threat to LVLMs in real-world applications. Executing such attacks in LVLMs is challenging since there can be many incorrect trigger-target mappings due to severe feature interference among different triggers. To address this challenge, we propose MTAttack, the first multi-target backdoor attack framework for enforcing accurate multiple trigger-target mappings in LVLMs. The core of MTAttack is a novel optimization method with two constraints, namely Proxy Space Partitioning constraint and Trigger Prototype Anchoring constraint. It jointly optimizes multiple triggers in the latent space, with each trigger independently mapping clean images to a unique proxy class while at the same time guaranteeing their separability. Experiments on popular benchmarks demonstrate a high success rate of MTAttack for multi-target attacks, substantially outperforming existing attack methods. Furthermore, our attack exhibits strong generalizability across datasets and robustness against backdoor defense strategies. These findings highlight the vulnerability of LVLMs to multi-target backdoor attacks and underscore the urgent need for mitigating such threats. Code is available at https://github.com/mala-lab/MTAttack.

</details>


### [57] [RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo](https://arxiv.org/abs/2511.10107)
*Jueun Ko,Hyewon Park,Hyesong Choi,Dongbo Min*

Main category: cs.CV

TL;DR: RobIA: instance-aware CTTA for stereo depth using attention-based MoE routing and a PEFT teacher providing dense pseudo-labels from sparse supervision, yielding better adaptation under domain shifts.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing TTA methods for stereo depth (static target domain assumption, input-invariant adaptation) and high cost of dense ground-truth; need robust continual adaptation with sparse supervision.

Method: Uses AttEx-MoE to route inputs to frozen experts via epipolar-aware self-attention and a PEFT-based Robust AdaptBN Teacher to generate dense pseudo supervision complementing sparse labels; combined in a continual TTA framework.

Result: RobIA, combining AttEx-MoE and Robust AdaptBN Teacher, improves generalization and achieves superior adaptation across dynamic domains while being computationally efficient.

Conclusion: RobIA offers input-specific adaptation and dense pseudo-supervision to handle continual domain shifts in stereo depth estimation, outperforming baselines while remaining efficient.

Abstract: Stereo Depth Estimation in real-world environments poses significant challenges due to dynamic domain shifts, sparse or unreliable supervision, and the high cost of acquiring dense ground-truth labels. While recent Test-Time Adaptation (TTA) methods offer promising solutions, most rely on static target domain assumptions and input-invariant adaptation strategies, limiting their effectiveness under continual shifts. In this paper, we propose RobIA, a novel Robust, Instance-Aware framework for Continual Test-Time Adaptation (CTTA) in stereo depth estimation. RobIA integrates two key components: (1) Attend-and-Excite Mixture-of-Experts (AttEx-MoE), a parameter-efficient module that dynamically routes input to frozen experts via lightweight self-attention mechanism tailored to epipolar geometry, and (2) Robust AdaptBN Teacher, a PEFT-based teacher model that provides dense pseudo-supervision by complementing sparse handcrafted labels. This strategy enables input-specific flexibility, broad supervision coverage, improving generalization under domain shift. Extensive experiments demonstrate that RobIA achieves superior adaptation performance across dynamic target domains while maintaining computational efficiency.

</details>


### [58] [Explicit Temporal-Semantic Modeling for Dense Video Captioning via Context-Aware Cross-Modal Interaction](https://arxiv.org/abs/2511.10134)
*Mingda Jia,Weiliang Meng,Zenghuang Fu,Yiheng Li,Qi Zeng,Yifan Zhang,Ju Xin,Rongtao Xu,Jiguang Zhang,Xiaopeng Zhang*

Main category: cs.CV

TL;DR: CACMI explicitly models temporal and semantic context via cross-modal frame aggregation and query-guided feature enhancement, improving dense video captioning and achieving SOTA on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of implicit, frame-level modeling in dense video captioning by explicitly modeling temporal coherence and semantic context.

Method: Two components: Cross-modal Frame Aggregation uses cross-modal retrieval to aggregate frames into event-aligned textual features; Context-aware Feature Enhancement applies query-guided attention to fuse visual dynamics with pseudo-event semantics.

Result: Propose CACMI framework with Cross-modal Frame Aggregation and Context-aware Feature Enhancement; achieves state-of-the-art on ActivityNet Captions and YouCook2.

Conclusion: Explicit temporal-semantic modeling that combines latent video temporal patterns and linguistic semantics improves dense video captioning performance.

Abstract: Dense video captioning jointly localizes and captions salient events in untrimmed videos. Recent methods primarily focus on leveraging additional prior knowledge and advanced multi-task architectures to achieve competitive performance. However, these pipelines rely on implicit modeling that uses frame-level or fragmented video features, failing to capture the temporal coherence across event sequences and comprehensive semantics within visual contexts. To address this, we propose an explicit temporal-semantic modeling framework called Context-Aware Cross-Modal Interaction (CACMI), which leverages both latent temporal characteristics within videos and linguistic semantics from text corpus. Specifically, our model consists of two core components: Cross-modal Frame Aggregation aggregates relevant frames to extract temporally coherent, event-aligned textual features through cross-modal retrieval; and Context-aware Feature Enhancement utilizes query-guided attention to integrate visual dynamics with pseudo-event semantics. Extensive experiments on the ActivityNet Captions and YouCook2 datasets demonstrate that CACMI achieves the state-of-the-art performance on dense video captioning task.

</details>


### [59] [Right Looks, Wrong Reasons: Compositional Fidelity in Text-to-Image Generation](https://arxiv.org/abs/2511.10136)
*Mayank Vatsa,Aparna Bharati,Richa Singh*

Main category: cs.CV

TL;DR: 现有文本到图像模型在处理逻辑组合（否定、计数、空间关系）时表现极差，问题源于训练数据、连续注意力架构与评价指标，简单放大或现有改进不足以解决，需根本性方法进步


<details>
  <summary>Details</summary>
Motivation: 调查当今文本生成图像模型在逻辑组合上的失败与局限性

Method: 对现有模型、基准与方法进行调查与分析，评估训练数据、架构与评估指标对组合性失败的贡献

Result: 展示在否定、计数与空间关系三类原语组合时性能崩溃，指出原因并评估现有方法不足

Conclusion: 需要在表示与推理上做根本性改进，而非现有架构的增量调整，才能实现真正的可组合性

Abstract: The architectural blueprint of today's leading text-to-image models contains a fundamental flaw: an inability to handle logical composition. This survey investigates this breakdown across three core primitives-negation, counting, and spatial relations. Our analysis reveals a dramatic performance collapse: models that are accurate on single primitives fail precipitously when these are combined, exposing severe interference. We trace this failure to three key factors. First, training data show a near-total absence of explicit negations. Second, continuous attention architectures are fundamentally unsuitable for discrete logic. Third, evaluation metrics reward visual plausibility over constraint satisfaction. By analyzing recent benchmarks and methods, we show that current solutions and simple scaling cannot bridge this gap. Achieving genuine compositionality, we conclude, will require fundamental advances in representation and reasoning rather than incremental adjustments to existing architectures.

</details>


### [60] [Split-Layer: Enhancing Implicit Neural Representation by Maximizing the Dimensionality of Feature Space](https://arxiv.org/abs/2511.10142)
*Zhicheng Cai,Hao Zhu,Linsen Chen,Qiu Shen,Xun Cao*

Main category: cs.CV

TL;DR: Introduce split-layer that splits MLP layers into parallel branches whose outputs are combined by elementwise product, enabling large polynomial feature spaces and improved INR performance with low overhead; validated on multiple tasks.


<details>
  <summary>Details</summary>
Motivation: INRs have limited representational capacity due to low-dimensional feature spaces in standard MLPs; widening MLPs increases capacity but is costly in compute/memory. Need a method to expand feature dimensions efficiently.

Method: Reformulate MLP layers into multiple parallel branches and integrate branch outputs through Hadamard (elementwise) product to construct high-degree polynomial feature spaces, increasing feature dimensionality more efficiently than simply widening layers.

Result: Propose split-layer: divide layer into parallel branches and combine outputs via Hadamard product to build high-degree polynomial space, boosting feature-space dimensionality with modest computational cost. Empirical gains across 2D image fitting, 2D CT, 3D shape, 5D novel view synthesis.

Conclusion: Split-layer effectively expands INR representational capacity and improves performance across various inverse problems without prohibitive computational costs, making it a practical enhancement to MLP-based INRs.

Abstract: Implicit neural representation (INR) models signals as continuous functions using neural networks, offering efficient and differentiable optimization for inverse problems across diverse disciplines. However, the representational capacity of INR defined by the range of functions the neural network can characterize, is inherently limited by the low-dimensional feature space in conventional multilayer perceptron (MLP) architectures. While widening the MLP can linearly increase feature space dimensionality, it also leads to a quadratic growth in computational and memory costs. To address this limitation, we propose the split-layer, a novel reformulation of MLP construction. The split-layer divides each layer into multiple parallel branches and integrates their outputs via Hadamard product, effectively constructing a high-degree polynomial space. This approach significantly enhances INR's representational capacity by expanding the feature space dimensionality without incurring prohibitive computational overhead. Extensive experiments demonstrate that the split-layer substantially improves INR performance, surpassing existing methods across multiple tasks, including 2D image fitting, 2D CT reconstruction, 3D shape representation, and 5D novel view synthesis.

</details>


### [61] [Decoupling Bias, Aligning Distributions: Synergistic Fairness Optimization for Deepfake Detection](https://arxiv.org/abs/2511.10150)
*Feng Ding,Wenhui Yi,Yunpeng Zhou,Xinan He,Hong Rao,Shu Hu*

Main category: cs.CV

TL;DR: 提出一种结合结构级解耦与特征级分布对齐的双重协同优化框架，在不牺牲检测精度的前提下提升不同人口子群体间及组内公平性


<details>
  <summary>Details</summary>
Motivation: Existing fairness-enhanced deepfake detectors trade detection accuracy for fairness across demographics; need to improve fairness without reducing accuracy

Method: Dual-mechanism collaborative optimization: structural fairness decoupling + global distribution alignment

Result: Framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains compared to other methods

Conclusion: 通过在模型架构中解耦敏感通道并在特征空间对齐总体与各群体分布，能同时提升公平性与保持准确度，具备跨域鲁棒性

Abstract: Fairness is a core element in the trustworthy deployment of deepfake detection models, especially in the field of digital identity security. Biases in detection models toward different demographic groups, such as gender and race, may lead to systemic misjudgments, exacerbating the digital divide and social inequities. However, current fairness-enhanced detectors often improve fairness at the cost of detection accuracy. To address this challenge, we propose a dual-mechanism collaborative optimization framework. Our proposed method innovatively integrates structural fairness decoupling and global distribution alignment: decoupling channels sensitive to demographic groups at the model architectural level, and subsequently reducing the distance between the overall sample distribution and the distributions corresponding to each demographic group at the feature level. Experimental results demonstrate that, compared with other methods, our framework improves both inter-group and intra-group fairness while maintaining overall detection accuracy across domains.

</details>


### [62] [GEA: Generation-Enhanced Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2511.10154)
*Hao Zou,Runqing Zhang,Xue Zhou,Jianxiao Zou*

Main category: cs.CV

TL;DR: GEA uses diffusion-generated images and cross-attention fusion to bridge text-image modality gap and improve text-to-image person retrieval, showing effectiveness on three TIPR benchmarks.


<details>
  <summary>Details</summary>
Motivation: Text queries sometimes fail to fully describe images and the modality gap between text and images hinders accurate cross-modal retrieval; overfitting to limited datasets also occurs.

Method: Two parallel modules: TGTE introduces diffusion-generated images as intermediate representations to enhance text tokens; GIF fuses features from generated images, real images, and text via cross-attention and trains unified representation with triplet alignment loss.

Result: Proposed Generation-Enhanced Alignment (GEA) with two modules: Text-Guided Token Enhancement (TGTE) using diffusion-generated images to enrich text semantics, and Generative Intermediate Fusion (GIF) combining cross-attention among generated images, original images, and text to create unified representations optimized by triplet alignment loss; validated on CUHK-PEDES, RSTPReid, and ICFG-PEDES with effective results.

Conclusion: GEA effectively leverages generated images as intermediate semantic anchors and cross-attention fusion to improve cross-modal alignment and retrieval performance on standard TIPR datasets.

Abstract: Text-to-Image Person Retrieval (TIPR) aims to retrieve person images based on natural language descriptions. Although many TIPR methods have achieved promising results, sometimes textual queries cannot accurately and comprehensively reflect the content of the image, leading to poor cross-modal alignment and overfitting to limited datasets. Moreover, the inherent modality gap between text and image further amplifies these issues, making accurate cross-modal retrieval even more challenging. To address these limitations, we propose the Generation-Enhanced Alignment (GEA) from a generative perspective. GEA contains two parallel modules: (1) Text-Guided Token Enhancement (TGTE), which introduces diffusion-generated images as intermediate semantic representations to bridge the gap between text and visual patterns. These generated images enrich the semantic representation of text and facilitate cross-modal alignment. (2) Generative Intermediate Fusion (GIF), which combines cross-attention between generated images, original images, and text features to generate a unified representation optimized by triplet alignment loss. We conduct extensive experiments on three public TIPR datasets, CUHK-PEDES, RSTPReid, and ICFG-PEDES, to evaluate the performance of GEA. The results justify the effectiveness of our method. More implementation details and extended results are available at https://github.com/sugelamyd123/Sup-for-GEA.

</details>


### [63] [Physically Interpretable Multi-Degradation Image Restoration via Deep Unfolding and Explainable Convolution](https://arxiv.org/abs/2511.10166)
*Hu Gao,Xiaoning Lei,Xichen Xu,Depeng Dang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 本文提出InterIR，通过将改进的二阶半光滑牛顿算法展开为网络模块，并引入可解释卷积单元，实现了对多种图像退化的可解释、高效恢复。


<details>
  <summary>Details</summary>
Motivation: 提出可解释的多退化图像恢复方法，以解决现有方法通常只针对单一退化类型、可解释性差的问题，并应对真实场景中多种退化同时存在的需求。

Method: 基于改进的二阶半光滑牛顿算法进行深度展开，保证每层模块具物理意义；设计可解释卷积模块，允许网络根据输入自适应调整参数；将两者紧密结合形成InterIR框架。

Result: 提出InterIR，一种基于深度展开网络和改进二阶半光滑牛顿算法的可解释性多退化图像恢复框架，包含受人脑信息处理启发的可解释卷积模块，在多退化和单一退化任务上均表现优异。

Conclusion: InterIR在保持物理可解释性的同时，具备处理多种混合退化的能力，且在多退化与单退化任务上均取得竞争性结果，展示了可解释性设计在图像恢复中的有效性。

Abstract: Although image restoration has advanced significantly, most existing methods target only a single type of degradation. In real-world scenarios, images often contain multiple degradations simultaneously, such as rain, noise, and haze, requiring models capable of handling diverse degradation types. Moreover, methods that improve performance through module stacking often suffer from limited interpretability. In this paper, we propose a novel interpretability-driven approach for multi-degradation image restoration, built upon a deep unfolding network that maps the iterative process of a mathematical optimization algorithm into a learnable network structure. Specifically, we employ an improved second-order semi-smooth Newton algorithm to ensure that each module maintains clear physical interpretability. To further enhance interpretability and adaptability, we design an explainable convolution module inspired by the human brain's flexible information processing and the intrinsic characteristics of images, allowing the network to flexibly leverage learned knowledge and autonomously adjust parameters for different input. The resulting tightly integrated architecture, named InterIR, demonstrates excellent performance in multi-degradation restoration while remaining highly competitive on single-degradation tasks.

</details>


### [64] [CephRes-MHNet: A Multi-Head Residual Network for Accurate and Robust Cephalometric Landmark Detection](https://arxiv.org/abs/2511.10173)
*Ahmed Jaheen,Islam Hassan,Mohanad Abouserie,Abdelaty Rehab,Adham Elasfar,Knzy Elmasry,Mostafa El-Dawlatly,Seif Eldawlatly*

Main category: cs.CV

TL;DR: CephRes-MHNet通过残差编码、双注意力与多头解码实现高效精确的颅线标志点检测，在Aariz数据集上以更少参数达到并超越现有最佳模型。


<details>
  <summary>Details</summary>
Motivation: 提高2D侧位头颅X光片中颅线标志点定位的准确性与效率，克服低对比度和解剖复杂性导致的自动化方法性能不足问题。

Method: 设计包含残差编码器、通道与空间双注意力模块、多头残差解码器的卷积网络；在Aariz 1000张辐射影像上训练与评估，使用MRE与SDR@2.0mm为主要指标，与多种基线模型对比，报告参数量与性能指标。

Result: 提出CephRes-MHNet，多头残差卷积网络，结合残差编码、双注意力机制和多头解码器，在Aariz数据集（1000张影像）上取得MRE=1.23mm，SDR@2.0mm=85.5%，优于基线AFPF-Net（MRE=1.25mm，SDR=84.1%），且参数量小于其25%。

Conclusion: 架构效率与注意力机制显著提升了标志点定位性能，CephRes-MHNet为临床正畸分析提供了实用且高准确度的自动化解决方案。

Abstract: Accurate localization of cephalometric landmarks from 2D lateral skull X-rays is vital for orthodontic diagnosis and treatment. Manual annotation is time-consuming and error-prone, whereas automated approaches often struggle with low contrast and anatomical complexity. This paper introduces CephRes-MHNet, a multi-head residual convolutional network for robust and efficient cephalometric landmark detection. The architecture integrates residual encoding, dual-attention mechanisms, and multi-head decoders to enhance contextual reasoning and anatomical precision. Trained on the Aariz Cephalometric dataset of 1,000 radiographs, CephRes-MHNet achieved a mean radial error (MRE) of 1.23 mm and a success detection rate (SDR) @ 2.0 mm of 85.5%, outperforming all evaluated models. In particular, it exceeded the strongest baseline, the attention-driven AFPF-Net (MRE = 1.25 mm, SDR @ 2.0 mm = 84.1%), while using less than 25% of its parameters. These results demonstrate that CephRes-MHNet attains state-of-the-art accuracy through architectural efficiency, providing a practical solution for real-world orthodontic analysis.

</details>


### [65] [Utilizing a Geospatial Foundation Model for Coastline Delineation in Small Sandy Islands](https://arxiv.org/abs/2511.10177)
*Tishya Chhabra,Manisha Bajpai,Walter Zesk,Skylar Tibbits*

Main category: cs.CV

TL;DR: Prithvi-EO-2.0 fine-tuned on 225-image Maldivian dataset excels at segmenting sandy island shorelines even with very small training sets, showing strong transfer learning potential.


<details>
  <summary>Details</summary>
Motivation: Assess transfer learning capability of geospatial foundation model for shoreline delineation in data-poor regions

Method: Fine-tuning Prithvi-EO-2.0 on shoreline delineation

Result: With 5–181 training images, both 300M and 600M models achieved high scores; e.g., with 5 images F1=0.94, IoU=0.79

Conclusion: Prithvi demonstrates strong few-shot performance for coastal monitoring and can support shoreline delineation in regions lacking labeled data; dataset is publicly released.

Abstract: We present an initial evaluation of NASA and IBM's Prithvi-EO-2.0 geospatial foundation model on shoreline delineation of small sandy islands using satellite images. We curated and labeled a dataset of 225 multispectral images of two Maldivian islands, which we publicly release, and fine-tuned both the 300M and 600M parameter versions of Prithvi on training subsets ranging from 5 to 181 images. Our experiments show that even with as few as 5 training images, the models achieve high performance (F1 of 0.94, IoU of 0.79). Our results demonstrate the strong transfer learning capability of Prithvi, underscoring the potential of such models to support coastal monitoring in data-poor regions.

</details>


### [66] [VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction](https://arxiv.org/abs/2511.10203)
*Stephane Da Silva Martins,Emanuel Aldea,Sylvie Le Hégarat-Mascle*

Main category: cs.CV

TL;DR: 提出VISTA，一种递归目标条件Transformer，结合长期意图与历史运动、社交token注意力和成对注意力图，实现多智能体轨迹预测，显著降低碰撞率并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉智能体的长期目标与细粒度社交交互，导致不现实的多智能体未来轨迹；为此需要一种能联合建模意图与交互、并提供可解释性的预测模型。

Method: VISTA包含三部分：（1）交叉注意融合模块，将长时程意图与过去运动信息结合；（2）社交token注意力，灵活建模智能体间交互；（3）成对注意力图，用于推断时解释社会影响模式。通过递归目标条件化，将单智能体目标预测扩展为连贯的多智能体预测框架。

Result: 在高密度MADRAS基准上将强基线平均碰撞率从2.14%降至0.03%；在SDD上实现零碰撞，并在ADE、FDE和minFDE上均有提升，证明其在准确性与社会合规性上的优势。

Conclusion: VISTA能生成社会合规、目标感知且可解释的多智能体轨迹，在MADRAS和SDD上取得SOTA并显著减少碰撞，适用于安全关键的自主系统。

Abstract: Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.

</details>


### [67] [LiNeXt: Revisiting LiDAR Completion with Efficient Non-Diffusion Architectures](https://arxiv.org/abs/2511.10209)
*Wenzhe He,Xiaojun Chen,Ruiqi Wang,Ruihui Li,Huilong Pi,Jiapeng Zhang,Zhuo Tang,Kenli Li*

Main category: cs.CV

TL;DR: 提出轻量非扩散网络LiNeXt：N2C单步去噪+Refine精化+距离感知重复策略，实现极大推理加速、显著误差下降和参数削减，适用于实时LiDAR场景补全。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然重构质量高，但多步采样代价高、实时性差；因此需要一种快速且参数少的替代方案用于自动驾驶的实时场景补全。

Method: 设计Noise-to-Coarse (N2C) 模块对输入有噪点云进行单次去噪生成粗略点云，再用Refine模块结合N2C中间特征进行精细化；提出Distance-aware Selected Repeat策略根据距离分布生成更均匀的噪点云；整体为轻量化网络，非扩散框架。

Result: 在SemanticKITTI上相比LiDiff实现199.8×推理加速、Chamfer Distance降低50.7%、参数量仅为LiDiff的6.1%，显示出显著的效率与效果提升。

Conclusion: LiNeXt以非扩散单步去噪+精炼策略成功替代多步扩散模型，实现了实时性与精度的平衡，适合在线场景补全。

Abstract: 3D LiDAR scene completion from point clouds is a fundamental component of perception systems in autonomous vehicles. Previous methods have predominantly employed diffusion models for high-fidelity reconstruction. However, their multi-step iterative sampling incurs significant computational overhead, limiting its real-time applicability. To address this, we propose LiNeXt-a lightweight, non-diffusion network optimized for rapid and accurate point cloud completion. Specifically, LiNeXt first applies the Noise-to-Coarse (N2C) Module to denoise the input noisy point cloud in a single pass, thereby obviating the multi-step iterative sampling of diffusion-based methods. The Refine Module then takes the coarse point cloud and its intermediate features from the N2C Module to perform more precise refinement, further enhancing structural completeness. Furthermore, we observe that LiDAR point clouds exhibit a distance-dependent spatial distribution, being densely sampled at proximal ranges and sparsely sampled at distal ranges. Accordingly, we propose the Distance-aware Selected Repeat strategy to generate a more uniformly distributed noisy point cloud. On the SemanticKITTI dataset, LiNeXt achieves a 199.8x speedup in inference, reduces Chamfer Distance by 50.7%, and uses only 6.1% of the parameters compared with LiDiff. These results demonstrate the superior efficiency and effectiveness of LiNeXt for real-time scene completion.

</details>


### [68] [HeatV2X: Scalable Heterogeneous Collaborative Perception via Efficient Alignment and Interaction](https://arxiv.org/abs/2511.10211)
*Yueran Zhao,Zhang Zhang,Chao Sun,Tianze Wang,Chao Yue,Nuoran Li*

Main category: cs.CV

TL;DR: HeatV2X通过异构图注意力基准代理+本地/全局适配器微调，低成本实现高性能的可扩展V2X协同感知。


<details>
  <summary>Details</summary>
Motivation: V2X协同感知中代理类型多样导致特征异构性，且参与代理数量增加使全参数训练不可行，需要同时解决跨代理对齐和可扩展适配问题。

Method: 先基于异构图注意力训练高性能基准代理；再设计本地异构微调（使用Hetero-Aware Adapters提取模态差异）和全局协同微调（使用Multi-Cognitive Adapter加强跨代理交互），实现高效对齐与协同融合。

Result: 在OPV2V-H和DAIR-V2X数据集上，HeatV2X在感知性能上超越现有SOTA方法，同时显著减少训练开销。

Conclusion: 提出HeatV2X，一种可扩展的异构协同感知框架，通过局部和全局微调适配多模态多代理场景，显著提升感知性能并降低训练开销。

Abstract: Vehicle-to-Everything (V2X) collaborative perception extends sensing beyond single vehicle limits through transmission. However, as more agents participate, existing frameworks face two key challenges: (1) the participating agents are inherently multi-modal and heterogeneous, and (2) the collaborative framework must be scalable to accommodate new agents. The former requires effective cross-agent feature alignment to mitigate heterogeneity loss, while the latter renders full-parameter training impractical, highlighting the importance of scalable adaptation. To address these issues, we propose Heterogeneous Adaptation (HeatV2X), a scalable collaborative framework. We first train a high-performance agent based on heterogeneous graph attention as the foundation for collaborative learning. Then, we design Local Heterogeneous Fine-Tuning and Global Collaborative Fine-Tuning to achieve effective alignment and interaction among heterogeneous agents. The former efficiently extracts modality-specific differences using Hetero-Aware Adapters, while the latter employs the Multi-Cognitive Adapter to enhance cross-agent collaboration and fully exploit the fusion potential. These designs enable substantial performance improvement of the collaborative framework with minimal training cost. We evaluate our approach on the OPV2V-H and DAIR-V2X datasets. Experimental results demonstrate that our method achieves superior perception performance with significantly reduced training overhead, outperforming existing state-of-the-art approaches. Our implementation will be released soon.

</details>


### [69] [Next-Frame Feature Prediction for Multimodal Deepfake Detection and Temporal Localization](https://arxiv.org/abs/2511.10212)
*Ashutosh Anshul,Shreyas Gopal,Deepu Rajan,Eng Siong Chng*

Main category: cs.CV

TL;DR: 提出一种无需额外真实样本预训练的单阶段训练框架，通过对单/跨模态特征做下一帧预测并结合窗口级注意力，提升跨域泛化能力并实现精确时序定位


<details>
  <summary>Details</summary>
Motivation: Avoid pretraining on real samples and detect manipulations that preserve audio-visual alignment by leveraging uni-/cross-modal temporal prediction to expose intra-modal artifacts

Method: single-stage next-frame prediction with window-level attention

Result: Model generalizes well across datasets and precise temporal localization of deepfake segments

Conclusion: 单阶段训练结合下一帧预测与窗口级注意力能够同时提升多模态深度伪造检测的泛化能力与时序定位性能

Abstract: Recent multimodal deepfake detection methods designed for generalization conjecture that single-stage supervised training struggles to generalize across unseen manipulations and datasets. However, such approaches that target generalization require pretraining over real samples. Additionally, these methods primarily focus on detecting audio-visual inconsistencies and may overlook intra-modal artifacts causing them to fail against manipulations that preserve audio-visual alignment. To address these limitations, we propose a single-stage training framework that enhances generalization by incorporating next-frame prediction for both uni-modal and cross-modal features. Additionally, we introduce a window-level attention mechanism to capture discrepancies between predicted and actual frames, enabling the model to detect local artifacts around every frame, which is crucial for accurately classifying fully manipulated videos and effectively localizing deepfake segments in partially spoofed samples. Our model, evaluated on multiple benchmark datasets, demonstrates strong generalization and precise temporal localization.

</details>


### [70] [TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding](https://arxiv.org/abs/2511.10241)
*Jinxuan Li,Yi Zhang,Jian-Fang Hu,Chaolei Tan,Tianming Liang,Beihao Xia*

Main category: cs.CV

TL;DR: 提出基于管条件重建与互约束的弱监督时空视频定位方法TubeRMC，生成文本关联候选管并通过三类重建策略与时空互约束精炼，减少目标识别错误和跟踪不一致，且在两个数据集上表现更好。


<details>
  <summary>Details</summary>
Motivation: 消除对精细标注（如边界框或时间戳）的依赖，通过弱监督方法在未剪辑视频中定位与语言查询对应的时空管（tube）。

Method: 使用预训练视觉 grounding模型生成文本条件候选时空管；设计三种重建器（时间、空间、时空）以管作为条件重建查询中的关键线索；引入空间与时间候选之间的互约束以提升用于重建的候选质量。

Result: 提出TubeRMC框架：用预训练视觉定位模型生成文本条件候选管，并通过时空约束下的管条件重建（含时、空、时空三种重建策略）及空间-时间互约束精炼管，提升识别和跟踪一致性。在VidSTG和HCSTVG上优于现有方法。

Conclusion: TubeRMC能够有效生成与文本相关的候选时空管并通过多角度重建与互约束提高其质量，从而缓解目标识别失败与跟踪不一致问题，并在公开基准上取得优越效果。

Abstract: Spatio-Temporal Video Grounding (STVG) aims to localize a spatio-temporal tube that corresponds to a given language query in an untrimmed video. This is a challenging task since it involves complex vision-language understanding and spatiotemporal reasoning. Recent works have explored weakly-supervised setting in STVG to eliminate reliance on fine-grained annotations like bounding boxes or temporal stamps. However, they typically follow a simple late-fusion manner, which generates tubes independent of the text description, often resulting in failed target identification and inconsistent target tracking. To address this limitation, we propose a Tube-conditioned Reconstruction with Mutual Constraints (\textbf{TubeRMC}) framework that generates text-conditioned candidate tubes with pre-trained visual grounding models and further refine them via tube-conditioned reconstruction with spatio-temporal constraints. Specifically, we design three reconstruction strategies from temporal, spatial, and spatio-temporal perspectives to comprehensively capture rich tube-text correspondences. Each strategy is equipped with a Tube-conditioned Reconstructor, utilizing spatio-temporal tubes as condition to reconstruct the key clues in the query. We further introduce mutual constraints between spatial and temporal proposals to enhance their quality for reconstruction. TubeRMC outperforms existing methods on two public benchmarks VidSTG and HCSTVG. Further visualization shows that TubeRMC effectively mitigates both target identification errors and inconsistent tracking.

</details>


### [71] [FineSkiing: A Fine-grained Benchmark for Skiing Action Quality Assessment](https://arxiv.org/abs/2511.10250)
*Yongji Zhang,Siqi Li,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: 构建首个带细粒度子分与扣分注释的空中滑雪AQA数据集，提出JudgeMind模型：分阶段评分、阶段感知特征增强与融合、基于知识的扣分先验解码器，提升可解释性与可靠性并取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有AQA方法可解释性和可靠性不足，且缺乏细粒度扣分与子分注释的数据集，限制了模型性能与裁判规则一致性的学习。

Method: 将视频划分为若干阶段，对每阶段独立评分；设计阶段感知特征增强与融合模块以突出关键区域并应对视角变化；基于知识的等级感知解码器将可能的扣分项作为先验来预测更准确分数。

Result: 在新构建的空中滑雪AQA数据集上，JudgeMind达到SOTA性能，并在可解释性和稳健性方面显著优于现有方法。

Conclusion: JudgeMind通过模拟裁判评分心态、分阶段评分与引入扣分先验，显著提升了空中滑雪AQA的性能与可靠性，且新数据集填补了细粒度注释空白。

Abstract: Action Quality Assessment (AQA) aims to evaluate and score sports actions, which has attracted widespread interest in recent years. Existing AQA methods primarily predict scores based on features extracted from the entire video, resulting in limited interpretability and reliability. Meanwhile, existing AQA datasets also lack fine-grained annotations for action scores, especially for deduction items and sub-score annotations. In this paper, we construct the first AQA dataset containing fine-grained sub-score and deduction annotations for aerial skiing, which will be released as a new benchmark. For the technical challenges, we propose a novel AQA method, named JudgeMind, which significantly enhances performance and reliability by simulating the judgment and scoring mindset of professional referees. Our method segments the input action video into different stages and scores each stage to enhance accuracy. Then, we propose a stage-aware feature enhancement and fusion module to boost the perception of stage-specific key regions and enhance the robustness to visual changes caused by frequent camera viewpoints switching. In addition, we propose a knowledge-based grade-aware decoder to incorporate possible deduction items as prior knowledge to predict more accurate and reliable scores. Experimental results demonstrate that our method achieves state-of-the-art performance.

</details>


### [72] [Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis](https://arxiv.org/abs/2511.10254)
*Jiulong Wu,Yucheng Shen,Lingyong Yan,Haixin Sun,Deguo Xia,Jizhou Huang,Min Cao*

Main category: cs.CV

TL;DR: 提出Facial-R1：1) 指令微调建立基本情感推理能力；2) 基于情感与AU标签的强化训练将推理与识别对齐；3) 数据合成循环扩充训练集。构建FEA-20K（~20K样本），在多个基准上达SOTA并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前将VLM用于细粒度面部情感分析时，存在推理幻觉和情感识别与推理不对齐问题，需设计对齐策略与可扩展数据生成以提升准确性与解释性。

Method: 三阶段：1）指令式微调（instruction fine-tuning）；2）以情感和AU标签为奖励的强化训练（reinforcement training）；3）基于前两阶段模型的迭代数据合成扩展训练集。另有构建FEA-20K数据集与广泛基准评估。

Result: Facial-R1提出了一个三阶段对齐框架，通过最小监督解决VLM在面部情感分析中的幻觉推理和识别-推理不一致两大问题；并构建了FEA-20K基准数据集。

Conclusion: 方法有效缓解了VLM的幻觉和推理-识别错位，通过最小监督和数据合成实现可扩展性；在多项基准上表现优异，且解释性更强。

Abstract: Facial Emotion Analysis (FEA) extends traditional facial emotion recognition by incorporating explainable, fine-grained reasoning. The task integrates three subtasks: emotion recognition, facial Action Unit (AU) recognition, and AU-based emotion reasoning to model affective states jointly. While recent approaches leverage Vision-Language Models (VLMs) and achieve promising results, they face two critical limitations: (1) hallucinated reasoning, where VLMs generate plausible but inaccurate explanations due to insufficient emotion-specific knowledge; and (2) misalignment between emotion reasoning and recognition, caused by fragmented connections between observed facial features and final labels. We propose Facial-R1, a three-stage alignment framework that effectively addresses both challenges with minimal supervision. First, we employ instruction fine-tuning to establish basic emotional reasoning capability. Second, we introduce reinforcement training guided by emotion and AU labels as reward signals, which explicitly aligns the generated reasoning process with the predicted emotion. Third, we design a data synthesis pipeline that iteratively leverages the prior stages to expand the training dataset, enabling scalable self-improvement of the model. Built upon this framework, we introduce FEA-20K, a benchmark dataset comprising 17,737 training and 1,688 test samples with fine-grained emotion analysis annotations. Extensive experiments across eight standard benchmarks demonstrate that Facial-R1 achieves state-of-the-art performance in FEA, with strong generalization and robust interpretability.

</details>


### [73] [H3Former: Hypergraph-based Semantic-Aware Aggregation via Hyperbolic Hierarchical Contrastive Loss for Fine-Grained Visual Classification](https://arxiv.org/abs/2511.10260)
*Yongji Zhang,Siqi Li,Kuiyang Huang,Yue Gao,Yu Jiang*

Main category: cs.CV

TL;DR: H3Former builds token-to-region aggregation via hypergraph-based SAAM and hyperbolic hierarchical contrastive loss HHCL to better capture fine-grained cues, improving FGVC performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: FGVC is hard due to subtle inter-class differences and large intra-class variations; existing methods miss comprehensive discriminative cues and add category-agnostic redundancy; need better aggregation of local representations with region-level modeling and leveraging high-order semantic relations.

Method: Use SAAM to build a weighted hypergraph among multi-scale tokens, apply hypergraph convolution to aggregate to region-level representations; train with HHCL in hyperbolic embedding space to enforce hierarchical intra/inter-class constraints; combined into transformer-based H3Former framework.

Result: Proposed H3Former: token-to-region framework with Semantic-Aware Aggregation Module (SAAM) using hypergraph convolution over dynamically weighted hypergraph to aggregate tokens into region-level representations; introduced Hyperbolic Hierarchical Contrastive Loss (HHCL) to enforce hierarchical semantic constraints in hyperbolic space; achieves state-of-the-art on four FGVC benchmarks.

Conclusion: H3Former effectively captures high-order semantic relations and hierarchical category structure, reducing redundancy and improving discriminability for FGVC; validated by experiments on four benchmarks.

Abstract: Fine-Grained Visual Classification (FGVC) remains a challenging task due to subtle inter-class differences and large intra-class variations. Existing approaches typically rely on feature-selection mechanisms or region-proposal strategies to localize discriminative regions for semantic analysis. However, these methods often fail to capture discriminative cues comprehensively while introducing substantial category-agnostic redundancy. To address these limitations, we propose H3Former, a novel token-to-region framework that leverages high-order semantic relations to aggregate local fine-grained representations with structured region-level modeling. Specifically, we propose the Semantic-Aware Aggregation Module (SAAM), which exploits multi-scale contextual cues to dynamically construct a weighted hypergraph among tokens. By applying hypergraph convolution, SAAM captures high-order semantic dependencies and progressively aggregates token features into compact region-level representations. Furthermore, we introduce the Hyperbolic Hierarchical Contrastive Loss (HHCL), which enforces hierarchical semantic constraints in a non-Euclidean embedding space. The HHCL enhances inter-class separability and intra-class consistency while preserving the intrinsic hierarchical relationships among fine-grained categories. Comprehensive experiments conducted on four standard FGVC benchmarks validate the superiority of our H3Former framework.

</details>


### [74] [PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning](https://arxiv.org/abs/2511.10279)
*Yanbei Jiang,Chao Lei,Yihao Ding,Krista Ehinger,Jey Han Lau*

Main category: cs.CV

TL;DR: PROPA generates dense intermediate-step rewards via MCTS+GRPO with interleaved SFT and trains a Process Reward Model to guide inference, boosting visual reasoning performance significantly


<details>
  <summary>Details</summary>
Motivation: VLMs fail in complex multi-step visual reasoning; need dense process-level rewards without step annotations and stable optimization

Method: PROPA: integrate MCTS with GRPO and interleaved SFT

Result: PROPA outperforms SFT and RLVR baselines across 7 benchmarks and 4 VLMs, up to 17% in-domain and 21% out-of-domain gains

Conclusion: PROPA effectively optimizes process-level reasoning without human step labels, achieving strong reasoning and generalization across tasks and models

Abstract: Despite significant progress, Vision-Language Models (VLMs) still struggle with complex visual reasoning, where multi-step dependencies cause early errors to cascade through the reasoning chain. Existing post-training paradigms are limited: Supervised Fine-Tuning (SFT) relies on costly step-level annotations, while Reinforcement Learning with Verifiable Rewards (RLVR) methods like GRPO provide only sparse, outcome-level feedback, hindering stable optimization. We introduce PROPA (Process-level Reasoning Optimization with interleaved Policy Alignment), a novel framework that integrates Monte Carlo Tree Search (MCTS) with GRPO to generate dense, process-level rewards and optimize reasoning at each intermediate step without human annotations. To overcome the cold-start problem, PROPA interleaves GRPO updates with SFT, enabling the model to learn from both successful and failed reasoning trajectories. A Process Reward Model (PRM) is further trained to guide inference-time search, aligning the test-time search with the training signal. Across seven benchmarks and four VLM backbones, PROPA consistently outperforms both SFT- and RLVR-based baselines. It achieves up to 17.0% gains on in-domain tasks and 21.0% gains on out-of-domain tasks compared to existing state-of-the-art, establishing a strong reasoning and generalization capability for visual reasoning tasks. The code isavailable at: https://github.com/YanbeiJiang/PROPA.

</details>


### [75] [Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models](https://arxiv.org/abs/2511.10292)
*Zhengtao Zou,Ya Gao,Jiarui Guan,Bin Li,Pekka Marttinen*

Main category: cs.CV

TL;DR: 提出RUDDER，通过一次标准前向传播提取残差更新方向（CARD）并用贝叶斯自适应门按标记注入校正信号，低开销减轻LVLM物体幻觉，性能接近最先进方法且延迟可忽略。


<details>
  <summary>Details</summary>
Motivation: 目标是解决现有推理时减少幻觉的方法在效果与计算成本之间的矛盾，提供一种既有效又低延迟的方案，便于在实际、延迟敏感的应用中部署LVLM。

Method: (1) 单次标准前向传播中，从自注意力层的残差更新中提取Contextual Activation Residual Direction（CARD）向量，作为视觉证据；(2) 设计贝叶斯启发的自适应门控，基于模型输出与视觉上下文的一致性逐标记调节注入CARD的强度，形成校正信号，最后在解码阶段引导生成以减少幻觉。

Result: RUDDER提出了一种在推理时低开销地减少大型视觉-语言模型（LVLMs）物体幻觉的方法。该方法核心为从单次标准前向传播中提取的上下文激活残差方向（CARD）向量，作为每个样本的视觉证据矢量，以及一个贝叶斯启发的自适应门控机制，按标记注入校正信号，信号强度根据模型与视觉上下文的偏差调整。实验表明在POPE和CHAIR等基准上，RUDDER在不显著增加延迟的情况下可实现与最先进方法相当的性能。

Conclusion: RUDDER在保持低计算开销（单次前向传播）和极小额外延迟的前提下，有效降低LVLM的物体幻觉，实验证明其在关键基准上的表现可与现有高开销方法相媲美，适合延迟敏感的现实部署。

Abstract: Large Vision-Language Models (LVLMs) often suffer from object hallucination, generating text inconsistent with visual inputs, which can critically undermine their reliability. Existing inference-time interventions to mitigate this issue present a challenging trade-off: while methods that steer internal states or adjust output logits can be effective, they often incur substantial computational overhead, typically requiring extra forward passes. This efficiency bottleneck can limit their practicality for real-world, latency-sensitive deployments. In this work, we aim to address this trade-off with Residual-Update Directed DEcoding Regulation (RUDDER), a low-overhead framework that steers LVLMs towards visually-grounded generation. RUDDER is built on two key innovations: (1) Contextual Activation Residual Direction (CARD) vector, a per-sample visual evidence vector extracted from the residual update of a self-attention layer during a single, standard forward pass. (2) A Bayesian-inspired adaptive gate that performs token-wise injection, applying a corrective signal whose strength is conditioned on the model's deviation from the visual context. Extensive experiments on key hallucination benchmarks, including POPE and CHAIR, indicate that RUDDER achieves performance comparable to state-of-the-art methods while introducing negligible computational latency, validating RUDDER as a pragmatic and effective approach for improving LVLMs' reliability without a significant compromise on efficiency.

</details>


### [76] [Generalizable Slum Detection from Satellite Imagery with Mixture-of-Experts](https://arxiv.org/abs/2511.10300)
*Sumin Lee,Sungwon Park,Jeasurk Yang,Jihee Kim,Meeyoung Cha*

Main category: cs.CV

TL;DR: 构建跨洲百万级卫星数据并提出GRAM：通过Mixture-of-Experts+预测一致性伪标签在测试时自适应，实现无监督跨区域棚户区分割，特别适用于低资源城市。


<details>
  <summary>Details</summary>
Motivation: 棚户区形态差异大，导致跨区域模型泛化差；获得目标区标注成本高，因此需一种无需目标标签即可在新城市中准确分割棚户区的可扩展方法。

Method: 1) 构建包含12个城市、四大洲、百万级高分辨率影像的训练集；2) 训练Mixture-of-Experts模型：共享主干提取通用特征，多专家分支捕捉区域特异性；3) 测试时两阶段自适应：利用专家预测一致性筛选可靠伪标签并微调模型以适应目标区；4) 在低资源城市上验证，比较SOTA基线。

Result: 提出一个大规模高分辨率卫星影像数据集和两阶段测试时自适应框架GRAM，用于无目标区标注的棚户区分割；使用Mixture-of-Experts结合共享主干学习通用和区域特异特征；在自适应阶段通过专家间预测一致性过滤伪标签以提高泛化能力；在多城市百万级数据训练下，方法在资源稀缺区域（如非洲城市）优于现有基线。

Conclusion: GRAM能在无目标标注条件下，通过专家集合和伪标签一致性机制，显著提升对未见区域棚户区分割的泛化能力，适用于全球尺度无监督棚户区识别与城市贫困估计。

Abstract: Satellite-based slum segmentation holds significant promise in generating global estimates of urban poverty. However, the morphological heterogeneity of informal settlements presents a major challenge, hindering the ability of models trained on specific regions to generalize effectively to unseen locations. To address this, we introduce a large-scale high-resolution dataset and propose GRAM (Generalized Region-Aware Mixture-of-Experts), a two-phase test-time adaptation framework that enables robust slum segmentation without requiring labeled data from target regions. We compile a million-scale satellite imagery dataset from 12 cities across four continents for source training. Using this dataset, the model employs a Mixture-of-Experts architecture to capture region-specific slum characteristics while learning universal features through a shared backbone. During adaptation, prediction consistency across experts filters out unreliable pseudo-labels, allowing the model to generalize effectively to previously unseen regions. GRAM outperforms state-of-the-art baselines in low-resource settings such as African cities, offering a scalable and label-efficient solution for global slum mapping and data-driven urban planning.

</details>


### [77] [Rethinking Visual Information Processing in Multimodal LLMs](https://arxiv.org/abs/2511.10301)
*Dongwan Kim,Viresh Ranjan,Takashi Nagata,Arnab Dhua,Amit Kumar K C*

Main category: cs.CV

TL;DR: Introduce LLaViT: modify LLM to be vision encoder with modality-specific QKV, bidirectional visual attention, and multi-scale visual features, achieving stronger vision-language performance than LLaVA


<details>
  <summary>Details</summary>
Motivation: Address mismatch between text and vision modalities in LLaVA by letting LLM act as vision encoder

Method: Three modifications: (1) separate QKV projections for vision modality, (2) enable bidirectional attention for visual tokens, (3) incorporate global and local visual representations

Result: LLaViT modifies LLM to serve as vision encoder via separate QKV for vision, bidirectional attention on visual tokens, and global+local visual representations; outperforms LLaVA and larger models on benchmarks

Conclusion: LLaViT provides an effective method to integrate visual features into LLMs, improving performance across benchmarks and surpassing larger models

Abstract: Despite the remarkable success of the LLaVA architecture for vision-language tasks, its design inherently struggles to effectively integrate visual features due to the inherent mismatch between text and vision modalities. We tackle this issue from a novel perspective in which the LLM not only serves as a language model but also a powerful vision encoder. To this end, we present LLaViT - Large Language Models as extended Vision Transformers - which enables the LLM to simultaneously function as a vision encoder through three key modifications: (1) learning separate QKV projections for vision modality, (2) enabling bidirectional attention on visual tokens, and (3) incorporating both global and local visual representations. Through extensive controlled experiments on a wide range of LLMs, we demonstrate that LLaViT significantly outperforms the baseline LLaVA method on a multitude of benchmarks, even surpassing models with double its parameter count, establishing a more effective approach to vision-language modeling.

</details>


### [78] [Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection](https://arxiv.org/abs/2511.10308)
*Patrick Feifel,Benedikt Franke,Frank Bonarens,Frank Köster,Arne Raulf,Friedhelm Schwenker*

Main category: cs.CV

TL;DR: 提出基于分割的八类检测错误与新评估指标，实现更细粒度、安全相关的模型比较，并在CityPersons-reasonable上无额外数据达SOTA


<details>
  <summary>Details</summary>
Motivation: Improve realism and safety-awareness in evaluating pedestrian detectors by addressing weaknesses in current benchmark metrics and leveraging segmentation to categorize detection errors

Method: Paper analysis of pedestrian detection metrics and segmentation-based error categorization

Result: Proposed eight error categories and new metrics; used metrics to compare backbones for simplified APD; achieved SOTA on CityPersons-reasonable without extra data using a simple architecture

Conclusion: New error-aware metrics provide more realistic and safety-critical evaluation of pedestrian detectors; segmentation-based analysis enables automated error categorization and better backbone comparison

Abstract: Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.

</details>


### [79] [CLIP4VI-ReID: Learning Modality-shared Representations via CLIP Semantic Bridge for Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.10309)
*Xiaomei Yang,Xizhan Gao,Sijie Niu,Fa Zhu,Guang Feng,Xiaofeng Qu,David Camacho*

Main category: cs.CV

TL;DR: 提出CLIP4VI-ReID：利用CLIP驱动的跨模态共享表征学习网络，包含文本语义生成(TSG)、红外特征嵌入(IFE)与高层语义对齐(HSA)，通过仅为可见图像生成文本语义、用文本修正红外特征并精炼高层语义，实现可见-红外的间接对齐，提高判别性并在VI-ReID数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 可见图像与红外图像在物理外观上差异显著，直接对齐困难。利用CLIP的多模态语义能力，通过为可见图像生成文本并以文本为桥梁，能在语义级别实现可见与红外的间接对齐，提升VI-ReID性能。

Method: 1) TSG：仅为可见图像生成文本语义以缓解可见与红外在物理特性上的巨大差异，建立可见-文本的初步对齐。
2) IFE：利用生成的文本语义校正红外图像的特征嵌入，将ID相关语义注入共享图像编码器，实现红外特征的语义增强并间接实现可见-红外对齐。
3) HSA：在高层语义空间进一步对齐细化，使文本语义仅保留ID相关信息，提高跨模态对齐精度和表示判别性。

Result: 在若干常用VI-ReID数据集上，CLIP4VI-ReID在指标上优于其他最新方法，实验结果验证了其在跨模态表征和识别任务上的有效性。

Conclusion: CLIP4VI-ReID通过文本作为桥梁并结合IFE和HSA，有效增强了共享编码器对红外模态的适应性和判别能力，从而在多个VI-ReID基准上取得了优于现有方法的性能。

Abstract: This paper proposes a novel CLIP-driven modality-shared representation learning network named CLIP4VI-ReID for VI-ReID task, which consists of Text Semantic Generation (TSG), Infrared Feature Embedding (IFE), and High-level Semantic Alignment (HSA). Specifically, considering the huge gap in the physical characteristics between natural images and infrared images, the TSG is designed to generate text semantics only for visible images, thereby enabling preliminary visible-text modality alignment. Then, the IFE is proposed to rectify the feature embeddings of infrared images using the generated text semantics. This process injects id-related semantics into the shared image encoder, enhancing its adaptability to the infrared modality. Besides, with text serving as a bridge, it enables indirect visible-infrared modality alignment. Finally, the HSA is established to refine the high-level semantic alignment. This process ensures that the fine-tuned text semantics only contain id-related information, thereby achieving more accurate cross-modal alignment and enhancing the discriminability of the learned modal-shared representations. Extensive experimental results demonstrate that the proposed CLIP4VI-ReID achieves superior performance than other state-of-the-art methods on some widely used VI-ReID datasets.

</details>


### [80] [Depth-Consistent 3D Gaussian Splatting via Physical Defocus Modeling and Multi-View Geometric Supervision](https://arxiv.org/abs/2511.10316)
*Yu Deng,Baozhu Zhao,Junyan Su,Xiaohan Zhang,Qi Liu*

Main category: cs.CV

TL;DR: 将物理散焦成像与多视图几何约束融合进3D Gaussian Splatting，通过景深损失与LoFTR匹配的深度一致性优化，显著提升城市场景下的深度保真与重建质量（Waymo数据集上PSNR+0.8 dB）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在近场结构退化与远场深度不准确之间存在监督冲突，难以同时兼顾两者。引入成像物理（散焦）信息和多视图几何约束以提供协调一致的监督，提升3D Gaussian Splatting在复杂深度分层场景下的表现。

Method: 方法由两部分组成：1) 景深监督：使用可恢复尺度的单目深度估计器生成深度先验，利用散焦卷积合成物理准确的散焦图像，通过景深损失强制几何一致性；2) 多视图一致性监督：基于LoFTR的半稠密特征匹配获得可靠匹配点，利用最小二乘优化减小跨视图几何误差并强制深度一致性。

Result: 在Waymo Open Dataset上相比最先进方法提升约0.8 dB PSNR，并在远近场深度保真度和结构完整性上均有改善。

Conclusion: 该文提出将景深(Defocus)监督与多视图一致性监督结合到3D Gaussian Splatting中，解决远近距离深度估计不一致问题，从而在城市场景中提升深度保真度与重建质量。

Abstract: Three-dimensional reconstruction in scenes with extreme depth variations remains challenging due to inconsistent supervisory signals between near-field and far-field regions. Existing methods fail to simultaneously address inaccurate depth estimation in distant areas and structural degradation in close-range regions. This paper proposes a novel computational framework that integrates depth-of-field supervision and multi-view consistency supervision to advance 3D Gaussian Splatting. Our approach comprises two core components: (1) Depth-of-field Supervision employs a scale-recovered monocular depth estimator (e.g., Metric3D) to generate depth priors, leverages defocus convolution to synthesize physically accurate defocused images, and enforces geometric consistency through a novel depth-of-field loss, thereby enhancing depth fidelity in both far-field and near-field regions; (2) Multi-View Consistency Supervision employing LoFTR-based semi-dense feature matching to minimize cross-view geometric errors and enforce depth consistency via least squares optimization of reliable matched points. By unifying defocus physics with multi-view geometric constraints, our method achieves superior depth fidelity, demonstrating a 0.8 dB PSNR improvement over the state-of-the-art method on the Waymo Open Dataset. This framework bridges physical imaging principles and learning-based depth regularization, offering a scalable solution for complex depth stratification in urban environments.

</details>


### [81] [Learning to Tell Apart: Weakly Supervised Video Anomaly Detection via Disentangled Semantic Alignment](https://arxiv.org/abs/2511.10334)
*Wenti Yin,Huaxin Zhang,Xiang Wang,Yuqing Lu,Yicheng Zhang,Bingquan Gong,Jialong Zuo,Li Yu,Changxin Gao,Nong Sang*

Main category: cs.CV

TL;DR: DSANet separates normal and abnormal features via prototype-guided reconstruction and decoupled contrastive alignment, improving anomaly localization and fine-grained classification; outperforms prior methods on two benchmarks.


<details>
  <summary>Details</summary>
Motivation: Weakly-supervised video anomaly detection methods using MIL and multimodal models (e.g., CLIP) often focus on most salient segments, neglect diverse normal patterns and causing category confusion.

Method: Coarse-grained: self-guided normality modeling branch reconstructs features under normal prototypes. Fine-grained: use frame-level anomaly scores to split video into event/background components and apply visual-language contrastive learning for class-discriminative features.

Result: Proposed DSANet that disentangles abnormal and normal features at coarse and fine-grained levels, with a self-guided normality modeling branch and a decoupled contrastive semantic alignment; achieves SOTA on XD-Violence and UCF-Crime.

Conclusion: Disentangling normal and abnormal cues at multiple granularities enhances detection and classification; DSANet effectively models normal prototypes and employs visual-language contrastive learning for better discriminability.

Abstract: Recent advancements in weakly-supervised video anomaly detection have achieved remarkable performance by applying the multiple instance learning paradigm based on multimodal foundation models such as CLIP to highlight anomalous instances and classify categories. However, their objectives may tend to detect the most salient response segments, while neglecting to mine diverse normal patterns separated from anomalies, and are prone to category confusion due to similar appearance, leading to unsatisfactory fine-grained classification results. Therefore, we propose a novel Disentangled Semantic Alignment Network (DSANet) to explicitly separate abnormal and normal features from coarse-grained and fine-grained aspects, enhancing the distinguishability. Specifically, at the coarse-grained level, we introduce a self-guided normality modeling branch that reconstructs input video features under the guidance of learned normal prototypes, encouraging the model to exploit normality cues inherent in the video, thereby improving the temporal separation of normal patterns and anomalous events. At the fine-grained level, we present a decoupled contrastive semantic alignment mechanism, which first temporally decomposes each video into event-centric and background-centric components using frame-level anomaly scores and then applies visual-language contrastive learning to enhance class-discriminative representations. Comprehensive experiments on two standard benchmarks, namely XD-Violence and UCF-Crime, demonstrate that DSANet outperforms existing state-of-the-art methods.

</details>


### [82] [FOUND: Fourier-based von Mises Distribution for Robust Single Domain Generalization in Object Detection](https://arxiv.org/abs/2511.10352)
*Mengzhu Wang,Changyuan Deng,Shanshan Wang,Nan Yin,Long Lan,Liang Yang*

Main category: cs.CV

TL;DR: 提出将vMF方向性建模与傅里叶域扰动整合进CLIP引导的单域泛化目标检测，增强域不变语义与频域鲁棒性，在天气驾驶基准上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP基语义增强方法忽视了特征分布的结构性（方向性）与频域特性，这些因素对跨域鲁棒性至关重要。通过显式建模方向性分布与频域扰动，可补足现有方法的不足。

Method: 在CLIP语义增强管线中，使用vMF分布对目标表示的方向性特征建模以捕捉域不变语义结构；同时在频域对特征的幅值与相位进行扰动以模拟域间变化，从而提高特征多样性与稳健性。

Result: 在多种天气驾驶基准上进行的大量实验表明，该方法在总体性能上优于现有最先进方法，证明了vMF建模与频域增强对SDG目标检测的有效性。

Conclusion: 本文提出了一种融合vMF分布与傅里叶变换的CLIP引导单域泛化目标检测框架，通过对方向性特征建模与频域扰动增强，实现了更强的域间结构一致性与鲁棒性。

Abstract: Single Domain Generalization (SDG) for object detection aims to train a model on a single source domain that can generalize effectively to unseen target domains. While recent methods like CLIP-based semantic augmentation have shown promise, they often overlook the underlying structure of feature distributions and frequency-domain characteristics that are critical for robustness. In this paper, we propose a novel framework that enhances SDG object detection by integrating the von Mises-Fisher (vMF) distribution and Fourier transformation into a CLIP-guided pipeline. Specifically, we model the directional features of object representations using vMF to better capture domain-invariant semantic structures in the embedding space. Additionally, we introduce a Fourier-based augmentation strategy that perturbs amplitude and phase components to simulate domain shifts in the frequency domain, further improving feature robustness. Our method not only preserves the semantic alignment benefits of CLIP but also enriches feature diversity and structural consistency across domains. Extensive experiments on the diverse weather-driving benchmark demonstrate that our approach outperforms the existing state-of-the-art method.

</details>


### [83] [DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile](https://arxiv.org/abs/2511.10367)
*Thales Bezerra,Emanoel Thyago,Kelvin Cunha,Rodrigo Abreu,Fábio Papais,Francisco Mauro,Natália Lopes,Érico Medeiros,Jéssica Guido,Shirley Cruz,Paulo Borba,Tsang Ing Ren*

Main category: cs.CV

TL;DR: DermAI is a phone app that does on-device image quality checks and local model fine-tuning; public models don't generalize well, local data improves accuracy, highlighting need for diverse standardized clinical datasets


<details>
  <summary>Details</summary>
Motivation: Address limited adoption due to biased datasets, poor image quality, and lack of validation; enable real-time, standardized data capture during consultations

Method: Lightweight smartphone app with on-device QA and local adaptation

Result: Public-trained models failed to generalize; fine-tuning with local DermAI data improved performance; DermAI enables diverse dataset collection and improved local model performance

Conclusion: Standardized, diverse, clinically aligned data collection and on-device/local adaptation are critical for reliable AI dermatology; DermAI facilitates this process.

Abstract: AI-based dermatology adoption remains limited by biased datasets, variable image quality, and limited validation. We introduce DermAI, a lightweight, smartphone-based application that enables real-time capture, annotation, and classification of skin lesions during routine consultations. Unlike prior dermoscopy-focused tools, DermAI performs on-device quality checks, and local model adaptation. The DermAI clinical dataset, encompasses a wide range of skin tones, ethinicity and source devices. In preliminary experiments, models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance. These results highlight the importance of standardized, diverse data collection aligned with healthcare needs and oriented to machine learning development.

</details>


### [84] [SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation](https://arxiv.org/abs/2511.10370)
*Kai-Hendrik Cohrs,Zuzanna Osika,Maria Gonzalez-Calabuig,Vishal Nedungadi,Ruben Cartuyvels,Steffen Knoblauch,Joppe Massant,Shruti Nath,Patrick Ebel,Vasileios Sitokonstantinou*

Main category: cs.CV

TL;DR: SHRUG-FM 通过融合输入/嵌入空间的 OOD 检测与预测不确定性，提高 GFMs 在欠代表环境下的可靠性，并揭示失败主要集中在特定地理条件，为更安全可解释的真实部署铺路。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型在预训练数据欠代表的地理/环境条件下表现不稳定，需一种机制在推理阶段识别潜在失败并提供可解释性，特别是在气候敏感或决策关键的遥感任务中。

Method: 提出 SHRUG-FM 框架，融合三类互补信号：输入空间的 OOD 检测（如像素/影像统计）、嵌入空间的 OOD 检测（模型特征表达的异常检测）以及任务特定的预测不确定性（如概率/置信度或置信区间），并将这些信号用于对烧痕分割结果进行可靠性评分与筛选。

Result: 在烧痕分割任务上验证：OOD 分数与在特定环境条件下的性能下降相关；基于不确定性的标记能剔除大量低性能预测；将失败与 HydroATLAS 土地覆盖属性关联表明失败分布并非随机，而集中在低海拔区域和大河流域等在预训练数据中欠代表的地理区域。

Conclusion: SHRUG-FM 能提高地理空间基础模型在未充分预训练环境下的可靠性，通过结合输入空间 OOD 检测、嵌入空间 OOD 检测和任务特定的不确定性评估，能有效识别并剔除低质量预测，从而在气候敏感应用中实现更安全、可解释的部署。

Abstract: Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.

</details>


### [85] [MSGNav: Unleashing the Power of Multi-modal 3D Scene Graph for Zero-Shot Embodied Navigation](https://arxiv.org/abs/2511.10376)
*Xun Huang,Shijia Zhao,Yunxiang Wang,Xin Lu,Wanfa Zhang,Rongsheng Qu,Weixin Li,Yunhong Wang,Chenglu Wen*

Main category: cs.CV

TL;DR: 用图像作为场景图的边构建多模态3D场景图M3DSG，并基于此提出MSGNav，通过关键子图选择、词汇自适应、闭环推理与可视性视点决策等模块，实现零样本开放词汇导航并在多个数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实世界部署需求开放词汇泛化和低训练开销，促使使用零样本方法。但现有零样本三维场景图方法习惯将视觉信息压缩为文本关系，导致高构建成本、不可逆的视觉证据丢失以及词汇受限，因此需保留视觉线索并支持动态词汇扩展。

Method: 用图结构表示3D场景，但将边从文本关系替换为动态分配的图像（M3DSG），在此基础上设计MSGNav：Key Subgraph Selection用于在大场景图中高效抽取相关子图；Adaptive Vocabulary Update允许系统在线扩展开放词汇；Closed-Loop Reasoning进行基于视觉证据的循环推理以提高定位与导航决策；Visibility-based Viewpoint Decision解决最后一英里视点选择问题。

Result: MSGNav在标准数据集GOAT-Bench和HM3D-OVON上达到了领先性能，证明多模态场景图与提出的模块设计能显著提升零样本导航的准确性与泛化能力；并且作者将开源代码以供社区复现。

Conclusion: 本文提出了M3DSG（一种多模态三维场景图）及其上构建的零样本导航系统MSGNav，有效保留视觉信息并支持开放词汇泛化，解决了现有文本化关系导致的信息丢失与高构建成本问题。通过关键子图选择、自适应词汇更新、闭环推理和基于可视性的视点决策等模块，MSGNav在GOAT-Bench和HM3D-OVON上取得了SOTA性能。

Abstract: Embodied navigation is a fundamental capability for robotic agents operating. Real-world deployment requires open vocabulary generalization and low training overhead, motivating zero-shot methods rather than task-specific RL training. However, existing zero-shot methods that build explicit 3D scene graphs often compress rich visual observations into text-only relations, leading to high construction cost, irreversible loss of visual evidence, and constrained vocabularies. To address these limitations, we introduce the Multi-modal 3D Scene Graph (M3DSG), which preserves visual cues by replacing textual relational edges with dynamically assigned images. Built on M3DSG, we propose MSGNav, a zero-shot navigation system that includes a Key Subgraph Selection module for efficient reasoning, an Adaptive Vocabulary Update module for open vocabulary support, and a Closed-Loop Reasoning module for accurate exploration reasoning. Additionally, we further identify the last-mile problem in zero-shot navigation - determining the feasible target location with a suitable final viewpoint, and propose a Visibility-based Viewpoint Decision module to explicitly resolve it. Comprehensive experimental results demonstrate that MSGNav achieves state-of-the-art performance on GOAT-Bench and HM3D-OVON datasets. The open-source code will be publicly available.

</details>


### [86] [Fragile by Design: On the Limits of Adversarial Defenses in Personalized Generation](https://arxiv.org/abs/2511.10382)
*Zhen Chen,Yi Zhang,Xiangyu Yin,Chengxuan Qin,Xingyu Zhao,Xiaowei Huang,Wenjie Ruan*

Main category: cs.CV

TL;DR: 现有Anti-DreamBooth类防护对可见伪影与对抗净化高度脆弱，容易被滤波或净化还原，需更不可察觉且稳健的防护。


<details>
  <summary>Details</summary>
Motivation: 尽管有方法通过注入扰动防止模型记忆用户身份，本文关注这些方法在现实中易被检测与被简单滤波或净化移除，导致假安全感，需更严谨评估和更强鲁棒防护。

Method: 提出AntiDB_Purify评估框架，构建包含传统图像滤波（模糊、中值、锐化等）和对抗净化（基于去噪/重构的神经网络）的一系列净化攻击，系统化测试现有防护在个性化模型中的效果恢复情况。

Result: 本文分析了个性化生成（如DreamBooth）防护机制在对抗面部身份泄露时的局限性，提出了针对现有方法的评估框架AntiDB_Purify，并验证了传统滤波与对抗净化对防护失效的影响。

Conclusion: 当前防护方法在实际净化威胁下失效，不能保障用户身份隐私；研究应转向隐蔽性更高且对净化鲁棒的对抗扰动或其他防护策略。

Abstract: Personalized AI applications such as DreamBooth enable the generation of customized content from user images, but also raise significant privacy concerns, particularly the risk of facial identity leakage. Recent defense mechanisms like Anti-DreamBooth attempt to mitigate this risk by injecting adversarial perturbations into user photos to prevent successful personalization. However, we identify two critical yet overlooked limitations of these methods. First, the adversarial examples often exhibit perceptible artifacts such as conspicuous patterns or stripes, making them easily detectable as manipulated content. Second, the perturbations are highly fragile, as even a simple, non-learned filter can effectively remove them, thereby restoring the model's ability to memorize and reproduce user identity. To investigate this vulnerability, we propose a novel evaluation framework, AntiDB_Purify, to systematically evaluate existing defenses under realistic purification threats, including both traditional image filters and adversarial purification. Results reveal that none of the current methods maintains their protective effectiveness under such threats. These findings highlight that current defenses offer a false sense of security and underscore the urgent need for more imperceptible and robust protections to safeguard user identity in personalized generation.

</details>


### [87] [SAMIRO: Spatial Attention Mutual Information Regularization with a Pre-trained Model as Oracle for Lane Detection](https://arxiv.org/abs/2511.10385)
*Hyunjong Lee,Jangho Lee,Jaekoo Lee*

Main category: cs.CV

TL;DR: SAMIRO：一种利用预训练Oracle、空间注意力与互信息正则化的可插拔模块，可在多种车道检测模型和数据集上提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 实时复杂环境（背景干扰、光照变化、遮挡）和标注成本高导致数据驱动方法受限，需要利用周边车道与物体的上下文和全局信息并减少对大量标注数据的依赖。

Method: 提出了一种可插拔的模块SAMIRO，利用预训练模型的知识迁移并保留领域无关的空间信息；通过空间注意力和互信息正则化促使主模型与Oracle在空间表征上对齐。将该模块整合到多种现有车道检测方法中进行实验验证。

Result: 在CULane、Tusimple、LLAMAS等基准上，SAMIRO作为插件模块在不同模型和数据集上均带来稳定性能提升。

Conclusion: 作者提出SAMIRO，通过使用预训练模型作为Oracle并结合空间注意力与互信息正则化，实现跨模型和跨数据集的车道检测性能提升。

Abstract: Lane detection is an important topic in the future mobility solutions. Real-world environmental challenges such as background clutter, varying illumination, and occlusions pose significant obstacles to effective lane detection, particularly when relying on data-driven approaches that require substantial effort and cost for data collection and annotation. To address these issues, lane detection methods must leverage contextual and global information from surrounding lanes and objects. In this paper, we propose a Spatial Attention Mutual Information Regularization with a pre-trained model as an Oracle, called SAMIRO. SAMIRO enhances lane detection performance by transferring knowledge from a pretrained model while preserving domain-agnostic spatial information. Leveraging SAMIRO's plug-and-play characteristic, we integrate it into various state-of-the-art lane detection approaches and conduct extensive experiments on major benchmarks such as CULane, Tusimple, and LLAMAS. The results demonstrate that SAMIRO consistently improves performance across different models and datasets. The code will be made available upon publication.

</details>


### [88] [Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery](https://arxiv.org/abs/2511.10387)
*Prince Mensah,Pelumi Victor Aderinto,Ibrahim Salihu Yusuf,Arnu Pretorius*

Main category: cs.CV

TL;DR: 提出将PROSAIL作为可微物理解码器嵌入Transformer-VAE，纯模拟训练即可在真实Sentinel-2数据上准确检索LAI和CCC，免去现场标注与校准，提升大尺度植被参数反演的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统混合方法依赖真实卫星影像进行自监督或校准，获取标注和现场数据代价高昂；目标是开发一种只用模拟数据就能在真实影像上准确检索植被生物物理量的廉价可推广方法。

Method: 构建一个Transformer编码器-变分自编码器框架，采用PROSAIL作为可微分物理解码器；模型只在模拟产生的光谱数据上训练，使潜在变量在重构光谱的同时受物理模型约束，从而对应真实的叶片与冠层属性。

Result: 在FRM4Veg和BelSAR实地数据集上，模型对LAI和CCC的估计误差与使用真实Sentinel-2训练的最先进模型相当；模型无需现场标签或对真实影像校准，展示了良好的泛化能力与物理一致性。

Conclusion: 该论文提出的物理约束Transformer-VAE能仅用模拟数据逆解PROSAIL辐射传输模型，估计植被冠层参数，且在真实Sentinel-2影像上对LAI和CCC的检索性能可与需真实影像训练的最先进方法相当，证明了物理模型与深度网络融合的可行性。

Abstract: Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.

</details>


### [89] [MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns](https://arxiv.org/abs/2511.10390)
*Jiarui Zhang,Yuliang Liu,Zijun Wu,Guosheng Pang,Zhili Ye,Yupei Zhong,Junteng Ma,Tao Wei,Haiyang Xu,Weikai Chen,Zeen Wang,Qiangjun Ji,Fanxi Zhou,Qi Zhang,Yuanrui Hu,Jiahao Liu,Zhang Li,Ziyang Zhang,Qiang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: 提出MonkeyOCR v1.5，两阶段解析：大规模多模态模型做布局与阅读顺序；局部识别文本/公式/表格；针对复杂表格用渲染比对的强化学习提升结构准确；引入图像解耦表格解析和类型引导表格合并。实验在OmniDocBench v1.5上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现实文档常有复杂布局、多层表格、嵌入图像/公式及跨页结构，传统OCR难以同时保证结构和语序一致且减少错误传播。

Method: 两阶段管线：1) 大多模态模型联合预测布局与阅读顺序；2) 局部识别模块对检测区域内的文本、公式与表格进行高保真恢复。表格方面引入基于渲染的一致性强化学习（render-and-compare）评估识别质量，以及Image-Decoupled Table Parsing和Type-Guided Table Merging两模块处理嵌图表格和跨页/列表格重构。

Result: 在OmniDocBench v1.5上，MonkeyOCR v1.5优于PPOCR-VL和MinerU 2.5，在视觉复杂场景表现鲁棒且结构识别准确性显著提升。

Conclusion: MonkeyOCR v1.5通过联合多模态布局推断、局部高保真识别及基于渲染比对的强化学习，有效解决复杂布局和跨页/嵌图表格问题，在多个基准上达到SOTA。

Abstract: Document parsing is a core task in document intelligence, supporting applications such as information extraction, retrieval-augmented generation, and automated document analysis. However, real-world documents often feature complex layouts with multi-level tables, embedded images or formulas, and cross-page structures, which remain challenging for existing OCR systems. We introduce MonkeyOCR v1.5, a unified vision-language framework that enhances both layout understanding and content recognition through a two-stage parsing pipeline. The first stage employs a large multimodal model to jointly predict document layout and reading order, leveraging visual information to ensure structural and sequential consistency. The second stage performs localized recognition of text, formulas, and tables within detected regions, maintaining high visual fidelity while reducing error propagation. To address complex table structures, we propose a visual consistency-based reinforcement learning scheme that evaluates recognition quality via render-and-compare alignment, improving structural accuracy without manual annotations. Additionally, two specialized modules, Image-Decoupled Table Parsing and Type-Guided Table Merging, are introduced to enable reliable parsing of tables containing embedded images and reconstruction of tables crossing pages or columns. Comprehensive experiments on OmniDocBench v1.5 demonstrate that MonkeyOCR v1.5 achieves state-of-the-art performance, outperforming PPOCR-VL and MinerU 2.5 while showing exceptional robustness in visually complex document scenarios.

</details>


### [90] [GrounDiff: Diffusion-Based Ground Surface Generation from Digital Surface Models](https://arxiv.org/abs/2511.10391)
*Oussema Dhaouadi,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: Introduce GrounDiff, a diffusion-based iterative denoising framework with gated confidence-guided generation and Prior-Guided Stitching for scalable high-res DTM from DSM, achieving large improvements over SOTA on multiple benchmarks and a variant for road reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing DTM extraction methods have limitations: traditional filters need manual tuning; learning methods require complex architectures and post-processing. Need scalable, selective, high-quality ground filtering.

Method: Diffusion-based DTM generation from DSM

Result: GrounDiff outperforms SOTA deep learning methods on DSM-to-DTM tasks, reducing RMSE up to 93% on ALS2DTM and 47% on USGS; achieves up to 81% lower distance error on road reconstruction (GeRoD) and maintains smoothness. PrioStitch enables scalable high-res outputs. GrounDiff+ yields smoother surfaces for roads.

Conclusion: Diffusion models with confidence-guided gating and global priors can effectively and scalably translate DSMs to high-quality DTMs, outperforming prior methods across datasets and tasks.

Abstract: Digital Terrain Models (DTMs) represent the bare-earth elevation and are important in numerous geospatial applications. Such data models cannot be directly measured by sensors and are typically generated from Digital Surface Models (DSMs) derived from LiDAR or photogrammetry. Traditional filtering approaches rely on manually tuned parameters, while learning-based methods require well-designed architectures, often combined with post-processing. To address these challenges, we introduce Ground Diffusion (GrounDiff), the first diffusion-based framework that iteratively removes non-ground structures by formulating the problem as a denoising task. We incorporate a gated design with confidence-guided generation that enables selective filtering. To increase scalability, we further propose Prior-Guided Stitching (PrioStitch), which employs a downsampled global prior automatically generated using GrounDiff to guide local high-resolution predictions. We evaluate our method on the DSM-to-DTM translation task across diverse datasets, showing that GrounDiff consistently outperforms deep learning-based state-of-the-art methods, reducing RMSE by up to 93% on ALS2DTM and up to 47% on USGS benchmarks. In the task of road reconstruction, which requires both high precision and smoothness, our method achieves up to 81% lower distance error compared to specialized techniques on the GeRoD benchmark, while maintaining competitive surface smoothness using only DSM inputs, without task-specific optimization. Our variant for road reconstruction, GrounDiff+, is specifically designed to produce even smoother surfaces, further surpassing state-of-the-art methods. The project page is available at https://deepscenario.github.io/GrounDiff/.

</details>


### [91] [LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components](https://arxiv.org/abs/2511.10394)
*Yaru Li,Yanxue Wang,Meng Li,Xinming Li,Jianbo Feng*

Main category: cs.CV

TL;DR: 本文提出YOLOMS+轻量KV映射+领域微调LLM的融合框架，将检测结果转为语义丰富的文本，实现可解释的风电机组故障诊断与维护建议，检测和报告准确率分别为90.6%和89%。


<details>
  <summary>Details</summary>
Motivation: 传统故障检测多为视觉识别、输出结构化但语义不可解释，难以支持维护决策，需将视觉结果转化为可理解的文本推理。

Method: 提出YOLOMS检测器（多尺度检测+滑窗裁剪）用于增强故障特征提取；设计轻量级KV映射模块将检测结果转换为富含定性与定量属性的文本结构；对领域微调的LLM进行语义推理生成可解释分析与维护建议。

Result: 在真实数据集上，框架实现了90.6%的故障检测精度，并生成平均89%准确率的维护报告，证明提高了解释性并能为运维提供实际决策支持。

Conclusion: 本研究将目标检测与大语言模型结合，实现了风电机组故障的可解释诊断与维护建议，提升了诊断结果的语义可读性与决策支持能力。

Abstract: The health condition of wind turbine (WT) components is crucial for ensuring stable and reliable operation. However, existing fault detection methods are largely limited to visual recognition, producing structured outputs that lack semantic interpretability and fail to support maintenance decision-making. To address these limitations, this study proposes an integrated framework that combines YOLOMS with a large language model (LLM) for intelligent fault analysis and diagnosis. Specifically, YOLOMS employs multi-scale detection and sliding-window cropping to enhance fault feature extraction, while a lightweight key-value (KV) mapping module bridges the gap between visual outputs and textual inputs. This module converts YOLOMS detection results into structured textual representations enriched with both qualitative and quantitative attributes. A domain-tuned LLM then performs semantic reasoning to generate interpretable fault analyses and maintenance recommendations. Experiments on real-world datasets demonstrate that the proposed framework achieves a fault detection accuracy of 90.6\% and generates maintenance reports with an average accuracy of 89\%, thereby improving the interpretability of diagnostic results and providing practical decision support for the operation and maintenance of wind turbines.

</details>


### [92] [3DFETUS: Standardizing Fetal Facial Planes in 3D Ultrasound](https://arxiv.org/abs/2511.10412)
*Alomar Antonia,Rubio Ricardo,Albaiges Gerard,Salort-Benejam Laura,Caminal Julia,Prat Maria,Rueda Carolina,Cortes Berta,Piella Gemma,Sukno Federico*

Main category: cs.CV

TL;DR: 本文提出两项方法：GT++利用标注解剖学地标从3D超声体积中稳健估计标准面；3DFETUS为深度学习模型，用于自动化并标准化在3D胎儿超声体积中定位这些面。该方法在定量和临床评估中优于现有方法，平均平移误差4.13 mm、旋转误差7.93度。


<details>
  <summary>Details</summary>
Motivation: 常规胎儿超声获取标准面受胎儿运动、取向多样性和操作者经验影响，导致一致性差、耗时长和诊断偏差。为面部评估提供自动、标准化的解决方案。

Method: GT++基于注释的解剖地标，从三维体积推断标准面的位置与姿态；3DFETUS为端到端深度学习网络，用于在3D超声中自动检测这些地标并输出对应标准面。两者通过定量指标（平移和旋转误差）和临床专家盲评比较现有方法。

Result: 在3D超声数据集上，所提方法平均平移误差4.13 mm、平均旋转误差7.93°，在统计检验下显著优于其他先进方法；临床专家评审亦证明改进具有实践意义。

Conclusion: GT++与3DFETUS能显著提高3D胎儿超声中面位估计的准确性和一致性，减少主观性和检查时间，临床专家评估支持其实用性。

Abstract: Acquiring standard facial planes during routine fetal ultrasound (US) examinations is often challenging due to fetal movement, variability in orientation, and operator-dependent expertise. These factors contribute to inconsistencies, increased examination time, and potential diagnostic bias.
  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.
  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 4.13 mm and a mean rotation error of 7.93 degrees per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.

</details>


### [93] [RodEpil: A Video Dataset of Laboratory Rodents for Seizure Detection and Benchmark Evaluation](https://arxiv.org/abs/2511.10431)
*Daniele Perlo,Vladimir Despotovic,Selma Boudissa,Sang-Yoon Kim,Petr Nazarov,Yanrong Zhang,Max Wintermark,Olivier Keunen*

Main category: cs.CV

TL;DR: 作者构建并公开了一个包含顶视与侧视短片的啮齿类癫痫发作视频数据集（RodEpil），以及TimeSformer基线，严格的受试者分离验证下达成97%平均F1。


<details>
  <summary>Details</summary>
Motivation: 提供用于自动检测啮齿类动物癫痫样发作的视频数据集，支持非侵入式视频监测在前临床癫痫研究中的可重复研究。

Method: 采集10秒顶视与侧视视频片段，按片段标注为正常或发作；数据预处理与筛选后形成13,053个样本。采用TimeSformer变换器视频分类器进行基线实验，采用严格的受试者级五折交叉验证评估。

Result: 发布了RodEpil数据集：19只动物的短视频片段，总计13,053个样本（10,101负样本，2,952正样本）；并提供了基于TimeSformer的视频分类基线代码，使用严格的受试者分层五折交叉验证，平均F1=97%。数据与代码公开（DOI:10.5281/zenodo.17601357）。

Conclusion: RodEpil为非侵入式、基于视频的啮齿类癫痫发作检测研究提供了标准化、公开的数据和基线，便于方法比较与可重复性工作。

Abstract: We introduce a curated video dataset of laboratory rodents for automatic detection of convulsive events. The dataset contains short (10~s) top-down and side-view video clips of individual rodents, labeled at clip level as normal activity or seizure. It includes 10,101 negative samples and 2,952 positive samples collected from 19 subjects. We describe the data curation, annotation protocol and preprocessing pipeline, and report baseline experiments using a transformer-based video classifier (TimeSformer). Experiments employ five-fold cross-validation with strict subject-wise partitioning to prevent data leakage (no subject appears in more than one fold). Results show that the TimeSformer architecture enables discrimination between seizure and normal activity with an average F1-score of 97%. The dataset and baseline code are publicly released to support reproducible research on non-invasive, video-based monitoring in preclinical epilepsy research. RodEpil Dataset access - DOI: 10.5281/zenodo.17601357

</details>


### [94] [Histology-informed tiling of whole tissue sections improves the interpretability and predictability of cancer relapse and genetic alterations](https://arxiv.org/abs/2511.10432)
*Willem Bonnaffé,Yang Hu,Andrea Chatrian,Mengran Fan,Stefano Malacrino,Sandy Figiel,CRUK ICGC Prostate Group,Srinivasa R. Rao,Richard Colling,Richard J. Bryant,Freddie C. Hamdy,Dan J. Woodcock,Ian G. Mills,Clare Verrill,Jens Rittscher*

Main category: cs.CV

TL;DR: HIT uses gland segmentation to produce biologically meaningful patches for MIL, improving accuracy (+10% AUC for certain CNV predictions), interpretability (15 gland phenotypes linked to outcomes), and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Improve MIL and phenotyping in digital pathology by using biologically meaningful patches (glands) instead of grid tiles, addressing irrelevant info and interpretability issues.

Method: Semantic segmentation to extract glands from WSIs, use extracted glands as patches in MIL and phenotyping; trained on 137 ProMPT samples; evaluated on ICGC-C and TCGA-PRAD; metrics: Dice for segmentation, AUC for MIL CNV detection, clustering for gland phenotypes.

Result: HIT segments glands from WSIs, achieves gland-level Dice 0.83±0.17 on ProMPT, extracts ~380k glands from 760 WSIs, improves MIL AUCs by 10% for CNV detection in EMT and MYC genes, identifies 15 gland clusters associated with relapse, mutations, high Gleason, and reduces computation by focusing on structures.

Conclusion: HIT enhances MIL prediction accuracy and interpretability by using gland-based patches, demonstrating strong segmentation performance and biological relevance tied to clinical outcomes and mutations.

Abstract: Histopathologists establish cancer grade by assessing histological structures, such as glands in prostate cancer. Yet, digital pathology pipelines often rely on grid-based tiling that ignores tissue architecture. This introduces irrelevant information and limits interpretability. We introduce histology-informed tiling (HIT), which uses semantic segmentation to extract glands from whole slide images (WSIs) as biologically meaningful input patches for multiple-instance learning (MIL) and phenotyping. Trained on 137 samples from the ProMPT cohort, HIT achieved a gland-level Dice score of 0.83 +/- 0.17. By extracting 380,000 glands from 760 WSIs across ICGC-C and TCGA-PRAD cohorts, HIT improved MIL models AUCs by 10% for detecting copy number variation (CNVs) in genes related to epithelial-mesenchymal transitions (EMT) and MYC, and revealed 15 gland clusters, several of which were associated with cancer relapse, oncogenic mutations, and high Gleason. Therefore, HIT improved the accuracy and interpretability of MIL predictions, while streamlining computations by focussing on biologically meaningful structures during feature extraction.

</details>


### [95] [OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data](https://arxiv.org/abs/2511.10461)
*Simon Donike,Cesar Aybar,Julio Contreras,Luis Gómez-Chova*

Main category: cs.CV

TL;DR: OpenSR-SRGAN是一个面向遥感单图像超分的开源模块化框架，通过配置文件驱动SRGAN模型组件与训练流程，简化实验、复现和部署


<details>
  <summary>Details</summary>
Motivation: 降低使用GAN超分在遥感领域的门槛，提供可配置、可复现、可扩展的SRGAN实现以便在多光谱卫星数据上应用和比较

Method: Open-source framework implementation and configuration-driven workflow for SRGAN-style models in remote sensing

Result: 提供了统一实现、现成配置、默认训练设置、日志/验证/大场景推理钩子，支持不同架构、放大倍数和波段组合的快速切换

Conclusion: 把GAN超分作为配置驱动工作流，降低实验门槛，便于比较和在多源地球观测数据上部署超分方案。

Abstract: We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.

</details>


### [96] [Utility of Pancreas Surface Lobularity as a CT Biomarker for Opportunistic Screening of Type 2 Diabetes](https://arxiv.org/abs/2511.10484)
*Tejas Sudharshan Mathai,Anisa V. Prasad,Xinya Wang,Praveen T. S. Balamuralikrishna,Yan Zhuang,Abhinav Suri,Jianfei Liu,Perry J. Pickhardt,Ronald M. Summers*

Main category: cs.CV

TL;DR: 提出全自动CT胰腺及腹部结构分割方法，自动计算胰腺表面叶化度(PSL)等影像生物标志物，PSL在糖尿病组显著升高；基于CT生物标志物的多变量模型预测T2DM达0.90 AUC。


<details>
  <summary>Details</summary>
Motivation: 早期检测T2DM可在形态学和脂肪沉积上发现胰腺变化，研究PSL在T2DM中的作用能够提高机会性筛查效率，并推动基于影像的早期诊断工具发展。

Method: 使用四个深度学习分割模型在内部584例患者CT上自动分割胰腺并提取体积、脂肪含量、PSL等指标；比较模型Dice和ASSD并统计各指标与T2DM的相关性；用多变量模型（基于CT生物标志物）预测T2DM并报告AUC、敏感性、特异性。

Result: Automated CT-based pancreas segmentation and biomarker extraction for T2DM screening

Conclusion: PSL可作为T2DM的辅助筛查指标；PancAP模型在胰腺分割上表现最佳；基于影像生物标志物的预测模型具有高特异性和良好AUC，提示可用于机会性筛查。

Abstract: Type 2 Diabetes Mellitus (T2DM) is a chronic metabolic disease that affects millions of people worldwide. Early detection is crucial as it can alter pancreas function through morphological changes and increased deposition of ectopic fat, eventually leading to organ damage. While studies have shown an association between T2DM and pancreas volume and fat content, the role of increased pancreatic surface lobularity (PSL) in patients with T2DM has not been fully investigated. In this pilot work, we propose a fully automated approach to delineate the pancreas and other abdominal structures, derive CT imaging biomarkers, and opportunistically screen for T2DM. Four deep learning-based models were used to segment the pancreas in an internal dataset of 584 patients (297 males, 437 non-diabetic, age: 45$\pm$15 years). PSL was automatically detected and it was higher for diabetic patients (p=0.01) at 4.26 $\pm$ 8.32 compared to 3.19 $\pm$ 3.62 for non-diabetic patients. The PancAP model achieved the highest Dice score of 0.79 $\pm$ 0.17 and lowest ASSD error of 1.94 $\pm$ 2.63 mm (p$<$0.05). For predicting T2DM, a multivariate model trained with CT biomarkers attained 0.90 AUC, 66.7\% sensitivity, and 91.9\% specificity. Our results suggest that PSL is useful for T2DM screening and could potentially help predict the early onset of T2DM.

</details>


### [97] [SPOT: Sparsification with Attention Dynamics via Token Relevance in Vision Transformers](https://arxiv.org/abs/2511.10488)
*Oded Schlesinger,Amirhossein Farzam,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: SPOT detects and removes redundant tokens early in ViTs using compact attention-like predictors based on embeddings and attention dynamics, achieving ≈40% compute savings without harming accuracy; plug-and-play and adaptable.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers (ViT) have high computational cost due to quadratic scaling with token count; need early detection and removal of redundant tokens to reduce computation.

Method: Design lightweight, input-specific predictors that use token embeddings, interactions, and cross-layer attention dynamics to infer token importance and guide early token pruning; integrate into ViTs to selectively remove tokens before attention computation, with training to balance efficiency and accuracy.

Result: SPOT framework uses token embeddings, interactions, and attention dynamics across layers with lightweight predictors to detect redundant tokens early, enabling up to 40% efficiency gains while maintaining or improving accuracy; supports multiple ViT architectures and performance-resource trade-offs.

Conclusion: SPOT provides a context-aware, interpretable token relevance detection mechanism that enables significant computational savings in ViTs via early token sparsification, maintaining or boosting accuracy and adaptable to various architectures.

Abstract: While Vision Transformers (ViT) have demonstrated remarkable performance across diverse tasks, their computational demands are substantial, scaling quadratically with the number of processed tokens. Compact attention representations, reflecting token interaction distributions, can guide early detection and reduction of less salient tokens prior to attention computation. Motivated by this, we present SParsification with attentiOn dynamics via Token relevance (SPOT), a framework for early detection of redundant tokens within ViTs that leverages token embeddings, interactions, and attention dynamics across layers to infer token importance, resulting in a more context-aware and interpretable relevance detection process. SPOT informs token sparsification and facilitates the elimination of such tokens, improving computational efficiency without sacrificing performance. SPOT employs computationally lightweight predictors that can be plugged into various ViT architectures and learn to derive effective input-specific token prioritization across layers. Its versatile design supports a range of performance levels adaptable to varying resource constraints. Empirical evaluations demonstrate significant efficiency gains of up to 40% compared to standard ViTs, while maintaining or even improving accuracy. Code and models are available at https://github.com/odedsc/SPOT .

</details>


### [98] [Learnable Total Variation with Lambda Mapping for Low-Dose CT Denoising](https://arxiv.org/abs/2511.10500)
*Yusuf Talha Basak,Mehmet Ozan Unal,Metin Ertas,Isa Yildirim*

Main category: cs.CV

TL;DR: 提出LTV：端到端学习的像素级可变TV正则化，通过LambdaNet预测正则化地图，实验证明在低剂量CT上优于传统TV和FBP+U-Net，提升PSNR/SSIM并具可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统TV依赖全局超参数lambda，难以兼顾降噪与边缘保留；需要一种能在不同区域自适应调节正则化强度的方法以提升低剂量CT重建质量。

Method: 构建一个将TV求解器展开并与Lambda Mapping Network结合的流水线，LambdaNet预测每像素的正则化参数图，整个系统端到端训练以获得空间自适应的平滑效果。

Result: 在DeepLesion数据集和基于LoDoPaB-CT的现实噪声模型下，LTV相比经典TV和FBP+U-Net平均提升约2.9 dB PSNR和6% SSIM，且提供更可解释的结果。

Conclusion: 本文提出了可学习的全变分（LTV）框架，通过联合未展开的TV求解器和预测逐像素正则化映射的LambdaNet，实现端到端训练，从而在重建和正则化之间协同优化。

Abstract: Although Total Variation (TV) performs well in noise reduction and edge preservation on images, its dependence on the lambda parameter limits its efficiency and makes it difficult to use effectively. In this study, we present a Learnable Total Variation (LTV) framework that couples an unrolled TV solver with a data-driven Lambda Mapping Network (LambdaNet) predicting a per-pixel regularization map. The pipeline is trained end-to-end so that reconstruction and regularization are optimized jointly, yielding spatially adaptive smoothing: strong in homogeneous regions, relaxed near anatomical boundaries. Experiments on the DeepLesion dataset, using a realistic noise model adapted from the LoDoPaB-CT methodology, show consistent gains over classical TV and FBP+U-Net: +2.9 dB PSNR and +6% SSIM on average. LTV provides an interpretable alternative to black-box CNNs and a basis for 3D and data-consistency-driven reconstruction. Our codes are available at: https://github.com/itu-biai/deep_tv_for_ldct

</details>


### [99] [SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation](https://arxiv.org/abs/2511.10518)
*Wei Li,Renshan Zhang,Rui Shao,Zhijian Fang,Kaiwen Zhou,Zhuotao Tian,Liqiang Nie*

Main category: cs.CV

TL;DR: SemanticVLA通过语义驱动的双视觉裁剪（ID-Pruner与SA-Pruner）、跨模态层次融合（SH-Fuser）与语义条件动作耦合器（SA-Coupler），在LIBERO等基准上同时提升成功率与计算效率，达到新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLA模型在部署时的两大问题：感知冗余导致效率低下，以及指令与视觉对齐浅层化导致语义落地困难。

Method: 1) SD-Pruner包含ID-Pruner（基于SigLIP提取全局指令线索与局部语义锚点）与SA-Pruner（基于DINOv2将几何信息聚合为任务自适应token）；2) SH-Fuser将稠密patch与稀疏token在两个视觉表征间分层融合以形成一致表示；3) SA-Coupler以语义条件替代传统observation-to-DoF映射，实现更高效可解释的动作生成。

Result: SemanticVLA提出了一套针对机器人操作的视觉-语言-动作框架，通过语义对齐的稀疏化与增强来提高效率与语义落地性。

Conclusion: SemanticVLA在保留语义信息的前提下有效减少感知冗余，并通过层次融合与语义条件动作建模增强了视觉-语言到动作的映射，使得在仿真与真实环境中都显著提升性能与效率。

Abstract: Vision-Language-Action (VLA) models have advanced in robotic manipulation, yet practical deployment remains hindered by two key limitations: 1) perceptual redundancy, where irrelevant visual inputs are processed inefficiently, and 2) superficial instruction-vision alignment, which hampers semantic grounding of actions. In this paper, we propose SemanticVLA, a novel VLA framework that performs Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation. Specifically: 1) To sparsify redundant perception while preserving semantic alignment, Semantic-guided Dual Visual Pruner (SD-Pruner) performs: Instruction-driven Pruner (ID-Pruner) extracts global action cues and local semantic anchors in SigLIP; Spatial-aggregation Pruner (SA-Pruner) compacts geometry-rich features into task-adaptive tokens in DINOv2. 2) To exploit sparsified features and integrate semantics with spatial geometry, Semantic-complementary Hierarchical Fuser (SH-Fuser) fuses dense patches and sparse tokens across SigLIP and DINOv2 for coherent representation. 3) To enhance the transformation from perception to action, Semantic-conditioned Action Coupler (SA-Coupler) replaces the conventional observation-to-DoF approach, yielding more efficient and interpretable behavior modeling for manipulation tasks. Extensive experiments on simulation and real-world tasks show that SemanticVLA sets a new SOTA in both performance and efficiency. SemanticVLA surpasses OpenVLA on LIBERO benchmark by 21.1% in success rate, while reducing training cost and inference latency by 3.0-fold and 2.7-fold.SemanticVLA is open-sourced and publicly available at https://github.com/JiuTian-VL/SemanticVLA

</details>


### [100] [Dynamic Avatar-Scene Rendering from Human-centric Context](https://arxiv.org/abs/2511.10539)
*Wenqing Wang,Haosen Yang,Josef Kittler,Xiatian Zhu*

Main category: cs.CV

TL;DR: StM提出用共享变换函数在分别建模的人体与场景间做信息映射，兼顾效率与一致性，显著提升单目视频的人体与场景联合重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有4D神经渲染方法要么整体建模动态场景，要么将场景与背景分开建模并引入参数化人体先验，但这些做法要么忽视组件间不同的运动特征导致重建不完整，要么分开建模导致信息孤岛，边界处出现不一致与伪影。

Method: 对人体与场景分别建模，每个高斯属性共享一个变换函数以进行信息映射，避免成对交互的昂贵计算；通过该映射在空间和视觉上对齐两部分，保证边界一致性。

Result: 在单目视频数据集上的大量实验表明，StM在视觉质量与渲染精度上显著优于现有方法，尤其在人与场景交互复杂的边界处提升明显。

Conclusion: 提出Separate-then-Map（StM）策略，通过在分别建模的人体和场景之间引入信息映射机制，实现统一与高效的结合，从而改善与环境交互时的人体重建。

Abstract: Reconstructing dynamic humans interacting with real-world environments from monocular videos is an important and challenging task. Despite considerable progress in 4D neural rendering, existing approaches either model dynamic scenes holistically or model scenes and backgrounds separately aim to introduce parametric human priors. However, these approaches either neglect distinct motion characteristics of various components in scene especially human, leading to incomplete reconstructions, or ignore the information exchange between the separately modeled components, resulting in spatial inconsistencies and visual artifacts at human-scene boundaries. To address this, we propose {\bf Separate-then-Map} (StM) strategy that introduces a dedicated information mapping mechanism to bridge separately defined and optimized models. Our method employs a shared transformation function for each Gaussian attribute to unify separately modeled components, enhancing computational efficiency by avoiding exhaustive pairwise interactions while ensuring spatial and visual coherence between humans and their surroundings. Extensive experiments on monocular video datasets demonstrate that StM significantly outperforms existing state-of-the-art methods in both visual quality and rendering accuracy, particularly at challenging human-scene interaction boundaries.

</details>


### [101] [Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation](https://arxiv.org/abs/2511.10547)
*Isabela Albuquerque,Ira Ktena,Olivia Wiles,Ivana Kajić,Amal Rannen-Triki,Cristina Vasconcelos,Aida Nematzadeh*

Main category: cs.CV

TL;DR: 提出一种针对T2I模型多样性的系统评估框架：细化的人类评估模板、概念+因素的提示集、基于二项检验的比较方法，以及图像嵌入的系统比较，能对模型多样性进行排序并定位问题领域。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型尽管生成质量提升，但在输出多样性方面仍显不足，需要更系统和细粒度的评估方法来揭示模型在不同概念和变化因素上的表现差异。

Method: 设计了细化的人类评估模板、整理覆盖多种概念及其变化因素的提示词集，并提出基于二项检验的人类注释比较方法；同时比较了多种图像嵌入用于多样性测量。

Result: 构建了一个新的多样性评估流程和模板，能够对模型进行排序并识别其在特定概念/因素上的薄弱点；并给出了不同图像嵌入在衡量多样性时的比较结论。

Conclusion: 该论文提出了一个系统性评估文本到图像（T2I）模型多样性的框架，通过对概念及其变化因素的逐一评估，为多样性衡量提供了更精细和可比较的方法。

Abstract: Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.
  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.

</details>


### [102] [A Style is Worth One Code: Unlocking Code-to-Style Image Generation with Discrete Style Space](https://arxiv.org/abs/2511.10555)
*Huijie Liu,Shuhao Cui,Haoxiang Cao,Shuai Ma,Kai Wu,Guoliang Kang*

Main category: cs.CV

TL;DR: One-line summary


<details>
  <summary>Details</summary>
Motivation: The abstract claims novel task code-to-style image generation; need to explain why it's important and gaps in existing work

Method: Describe CoTyle pipeline: discrete style codebook from images, condition T2I diffusion model, autoregressive style generator to produce novel embeddings, inference mapping numerical code to embedding then to image

Result: Explain experiments show effectiveness, diversity, simplicity, reproducibility

Conclusion: Concluding statement about impact and limitations

Abstract: Innovative visual stylization is a cornerstone of artistic creation, yet generating novel and consistent visual styles remains a significant challenge. Existing generative approaches typically rely on lengthy textual prompts, reference images, or parameter-efficient fine-tuning to guide style-aware image generation, but often struggle with style consistency, limited creativity, and complex style representations. In this paper, we affirm that a style is worth one numerical code by introducing the novel task, code-to-style image generation, which produces images with novel, consistent visual styles conditioned solely on a numerical style code. To date, this field has only been primarily explored by the industry (e.g., Midjourney), with no open-source research from the academic community. To fill this gap, we propose CoTyle, the first open-source method for this task. Specifically, we first train a discrete style codebook from a collection of images to extract style embeddings. These embeddings serve as conditions for a text-to-image diffusion model (T2I-DM) to generate stylistic images. Subsequently, we train an autoregressive style generator on the discrete style embeddings to model their distribution, allowing the synthesis of novel style embeddings. During inference, a numerical style code is mapped to a unique style embedding by the style generator, and this embedding guides the T2I-DM to generate images in the corresponding style. Unlike existing methods, our method offers unparalleled simplicity and diversity, unlocking a vast space of reproducible styles from minimal input. Extensive experiments validate that CoTyle effectively turns a numerical code into a style controller, demonstrating a style is worth one code.

</details>


### [103] [OmniVGGT: Omni-Modality Driven Visual Geometry Grounded](https://arxiv.org/abs/2511.10560)
*Haosong Peng,Hao Li,Yalun Dai,Yushi Lan,Yihang Luo,Tianyu Qi,Zhengshen Zhang,Yufeng Zhan,Junfei Zhang,Wenchao Xu,Ziwei Liu*

Main category: cs.CV

TL;DR: 提出OmniVGGT，通过GeoAdapter（零初始化卷积）和随机多模态融合，稳定地将深度、内外参等几何信息注入空间基模型，训练/推理均支持任意模态组合，性能和效率优异，推动VLA任务表现。


<details>
  <summary>Details</summary>
Motivation: 当前3D基模型多只用RGB，忽视易得的几何信息（相机内外参、深度等）。利用这些模态可提升空间理解与下游任务性能，但需避免破坏预训练表示或增加过多开销。

Method: 设计GeoAdapter：用零初始化卷积逐步注入深度与相机参数；提出随机多模态融合训练策略：每个样本随机采样模态子集以避免过拟合并支持任意测试模态组合。保持与VGGT相近的推理速度。

Result: OmniVGGT introduces GeoAdapter and stochastic multimodal fusion to incorporate geometric cues into 3D foundation models, improving performance across tasks and maintaining efficiency.

Conclusion: OmniVGGT能在不破坏原始表示空间的前提下利用任意辅助几何模态，带来稳健且高效的性能提升，且在RGB-only设置下亦达SOTA；对VLA和机器人任务有实际价值。

Abstract: General 3D foundation models have started to lead the trend of unifying diverse vision tasks, yet most assume RGB-only inputs and ignore readily available geometric cues (e.g., camera intrinsics, poses, and depth maps). To address this issue, we introduce OmniVGGT, a novel framework that can effectively benefit from an arbitrary number of auxiliary geometric modalities during both training and inference. In our framework, a GeoAdapter is proposed to encode depth and camera intrinsics/extrinsics into a spatial foundation model. It employs zero-initialized convolutions to progressively inject geometric information without disrupting the foundation model's representation space. This design ensures stable optimization with negligible overhead, maintaining inference speed comparable to VGGT even with multiple additional inputs. Additionally, a stochastic multimodal fusion regimen is proposed, which randomly samples modality subsets per instance during training. This enables an arbitrary number of modality inputs during testing and promotes learning robust spatial representations instead of overfitting to auxiliary cues. Comprehensive experiments on monocular/multi-view depth estimation, multi-view stereo, and camera pose estimation demonstrate that OmniVGGT outperforms prior methods with auxiliary inputs and achieves state-of-the-art results even with RGB-only input. To further highlight its practical utility, we integrated OmniVGGT into vision-language-action (VLA) models. The enhanced VLA model by OmniVGGT not only outperforms the vanilla point-cloud-based baseline on mainstream benchmarks, but also effectively leverages accessible auxiliary inputs to achieve consistent gains on robotic tasks.

</details>


### [104] [From 2D to 3D Without Extra Baggage: Data-Efficient Cancer Detection in Digital Breast Tomosynthesis](https://arxiv.org/abs/2511.10597)
*Yen Nhi Truong Vu,Dan Guo,Sripad Joshi,Harshit Kumar,Jason Su,Thomas Paul Matthews*

Main category: cs.CV

TL;DR: 提出M&M-3D：在不增加参数的前提下从2D FFDM模型直接迁移权重，实现可学习的3D推理，显著提升DBT的定位与分类性能，低数据下效果尤为突出。


<details>
  <summary>Details</summary>
Motivation: 受限于标注DBT数据稀缺，传统2D迁移丢失体积信息，3D模型又需大量数据；目标是在不增加参数和训练数据需求的前提下，利用FFDM预训练权重实现有效3D推理以提升DBT检测性能。

Method: 基于已有M&M（FFDM）架构，通过构建‘恶性引导’的3D特征并在不增加参数的情况下将这些3D特征与切片级信息反复混合，实现可学习的3D推理；通过修改M&M中的操作实现参数共享与权重直接迁移。

Result: M&M-3D enables parameter-free 3D reasoning by reusing FFDM weights, constructs malignancy-guided 3D features, mixes them with slice-level information repeatedly, and modifies M&M operations without adding parameters; leads to significant gains in localization (11-54%) and classification (3-10%), outperforms complex 3D variants in low-data regime and matches in high-data, and improves on BCS-DBT benchmark by 4% classification and 10% localization.

Conclusion: M&M-3D在保持与FFDM架构参数一致的情况下实现有效3D推理，显著提高了DBT肿瘤定位与分类性能，尤其在数据不足场景下优于现有方法，并在BCS-DBT基准上取得领先。

Abstract: Digital Breast Tomosynthesis (DBT) enhances finding visibility for breast cancer detection by providing volumetric information that reduces the impact of overlapping tissues; however, limited annotated data has constrained the development of deep learning models for DBT. To address data scarcity, existing methods attempt to reuse 2D full-field digital mammography (FFDM) models by either flattening DBT volumes or processing slices individually, thus discarding volumetric information. Alternatively, 3D reasoning approaches introduce complex architectures that require more DBT training data. Tackling these drawbacks, we propose M&M-3D, an architecture that enables learnable 3D reasoning while remaining parameter-free relative to its FFDM counterpart, M&M. M&M-3D constructs malignancy-guided 3D features, and 3D reasoning is learned through repeatedly mixing these 3D features with slice-level information. This is achieved by modifying operations in M&M without adding parameters, thus enabling direct weight transfer from FFDM. Extensive experiments show that M&M-3D surpasses 2D projection and 3D slice-based methods by 11-54% for localization and 3-10% for classification. Additionally, M&M-3D outperforms complex 3D reasoning variants by 20-47% for localization and 2-10% for classification in the low-data regime, while matching their performance in high-data regime. On the popular BCS-DBT benchmark, M&M-3D outperforms previous top baseline by 4% for classification and 10% for localization.

</details>


### [105] [Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping](https://arxiv.org/abs/2511.10604)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Kaylee Xiao,Motasem Alkayid,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出MSOM：基于超像素的OBIA-Mamba+全局-局部双分支CNN与多任务损失，提升Sentinel-2 LULC分类的精度与细节。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2 LULC分类受空间异质性、上下文信息缺失和光谱混淆影响，需在保持细粒度细节的同时利用全局语义信息并降低计算开销。

Method: 设计OBIA-Mamba以超像素降低计算冗余；构建GLocal双分支CNN-Mamba架构联合建模局部细节与全局上下文；采用多任务优化框架和双损失平衡局部精度与全局一致性。

Result: 在阿尔伯塔Sentinel-2影像上的对比实验证明MSOM在分类精度和细节表现上优于若干最先进方法。

Conclusion: MSOM通过将超像素作为Mamba令牌并结合全局-局部双分支CNN和多任务损失，在Sentinel-2 LULC分类中同时提升了局部精细度与全局一致性，实验表明在阿尔伯塔数据集上优于若干先进方法。

Abstract: Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.

</details>


### [106] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: SmolVLM2 variants evaluated for blind/low-vision accessibility using new context and navigation frameworks, prompt strategies, and mobile deployment with precision trade-offs


<details>
  <summary>Details</summary>
Motivation: Assess how model size and quantization affect BLV-oriented video descriptions and real-world mobile usability

Method: Analyze SmolVLM2 eval & deployment

Result: Introduced two BLV-focused evaluation frameworks; compared 500M vs 2.2B SmolVLM2 on AVCaps and Charades; tested 4 prompt strategies; measured FP32 vs INT8 performance on smartphone

Conclusion: Larger models and careful prompt design improve BLV-accessible descriptions, but quantized smaller models enable feasible on-device deployment with trade-offs in detail and accuracy.

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [107] [One Small Step in Latent, One Giant Leap for Pixels: Fast Latent Upscale Adapter for Your Diffusion Models](https://arxiv.org/abs/2511.10629)
*Aleksandr Razin,Danil Kazantsev,Ilya Makarov*

Main category: cs.CV

TL;DR: LUA: lightweight latent-space upscaler for generator latents that enables efficient 2x/4x SR with much lower time and good perceptual quality


<details>
  <summary>Details</summary>
Motivation: address scaling issues of diffusion models for high-resolution synthesis by doing SR in latent space

Method: analyze and summarize

Result: LUA matches native high-res fidelity with much lower decoding/upscaling time, generalizes across VAEs

Conclusion: LUA provides practical efficient path to scalable high-fidelity image synthesis without changing base model or extra diffusion stages

Abstract: Diffusion models struggle to scale beyond their training resolutions, as direct high-resolution sampling is slow and costly, while post-hoc image super-resolution (ISR) introduces artifacts and additional latency by operating after decoding. We present the Latent Upscaler Adapter (LUA), a lightweight module that performs super-resolution directly on the generator's latent code before the final VAE decoding step. LUA integrates as a drop-in component, requiring no modifications to the base model or additional diffusion stages, and enables high-resolution synthesis through a single feed-forward pass in latent space. A shared Swin-style backbone with scale-specific pixel-shuffle heads supports 2x and 4x factors and remains compatible with image-space SR baselines, achieving comparable perceptual quality with nearly 3x lower decoding and upscaling time (adding only +0.42 s for 1024 px generation from 512 px, compared to 1.87 s for pixel-space SR using the same SwinIR architecture). Furthermore, LUA shows strong generalization across the latent spaces of different VAEs, making it easy to deploy without retraining from scratch for each new decoder. Extensive experiments demonstrate that LUA closely matches the fidelity of native high-resolution generation while offering a practical and efficient path to scalable, high-fidelity image synthesis in modern diffusion pipelines.

</details>


### [108] [Depth Anything 3: Recovering the Visual Space from Any Views](https://arxiv.org/abs/2511.10647)
*Haotong Lin,Sili Chen,Junhao Liew,Donny Y. Chen,Zhenyu Li,Guang Shi,Jiashi Feng,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出了Depth Anything 3 (DA3)，用单一Transformer骨干和单一深度射线预测目标，通过师生训练在无/有位姿输入下从任意数量视图预测一致的几何，性能优于DA2和现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 追求最小建模假设：简化模型结构与目标，验证是否能在复杂几何任务上用最基础组件达到或超越专门化方法的性能。

Method: 使用单一非专化的Transformer（如vanilla DINO编码器）作为骨干，采用单一深度射线预测目标，结合师生（teacher-student）训练范式，支持有/无相机位姿输入的多视角几何预测。

Result: 在新建的视觉几何基准上，DA3在相机位姿准确度上平均优于VGGT 44.3%，在几何准确度上优25.1%，并在单目深度估计上超过DA2；与DA2在细节和泛化能力相当。

Conclusion: DA3在多个视觉几何任务上都显著优于先前方法（VGGT与DA2），以简单模型与单一目标实现更好泛化和细节恢复，且仅用公开学术数据训练。

Abstract: We present Depth Anything 3 (DA3), a model that predicts spatially consistent geometry from an arbitrary number of visual inputs, with or without known camera poses. In pursuit of minimal modeling, DA3 yields two key insights: a single plain transformer (e.g., vanilla DINO encoder) is sufficient as a backbone without architectural specialization, and a singular depth-ray prediction target obviates the need for complex multi-task learning. Through our teacher-student training paradigm, the model achieves a level of detail and generalization on par with Depth Anything 2 (DA2). We establish a new visual geometry benchmark covering camera pose estimation, any-view geometry and visual rendering. On this benchmark, DA3 sets a new state-of-the-art across all tasks, surpassing prior SOTA VGGT by an average of 44.3% in camera pose accuracy and 25.1% in geometric accuracy. Moreover, it outperforms DA2 in monocular depth estimation. All models are trained exclusively on public academic datasets.

</details>


### [109] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: SCS reduces learning from unfaithful chains by creating perturbed/resampled trajectory ensembles and using their agreement as a consistency-weighted reward, boosting multimodal RL accuracy with minimal cost.


<details>
  <summary>Details</summary>
Motivation: Outcome-reward RL in multimodal LLMs treats correct final answers equivalently whether derived faithfully or by guess, causing learning on unfaithful trajectories.

Method: For each question, apply small visual perturbations and perform repeated truncation and resampling on an initial trajectory to generate multiple trajectories; compute a differentiable consistency score from their agreement and use it to down-weight unreliable traces during policy updates; integrate SCS into RLOO, GRPO, REINFORCE++.

Result: Self-Consistency Sampling (SCS) introduces visual perturbations and repeated truncation/resampling to produce multiple trajectories; their agreement creates a differentiable consistency score used to down-weight unreliable trajectories, improving accuracy up to 7.7% across benchmarks with little extra compute and works across models.

Conclusion: SCS is a simple, general method to mitigate reward learning on spurious trajectories in outcome-reward RL for MLLMs, yielding substantial accuracy gains across models and benchmarks.

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [110] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出将反应式功能与复杂空间数据管理和并发语义相结合的分布式基于actor的框架扩展，通过引入“移动actor”抽象（包含感知、移动和空间查询能力）来简化反应式移动对象应用的开发；基于此提出M-AODBs平台并实现系统Dolphin（基于Microsoft Orleans），在多机实验中展示了可扩展性和近实时反应延迟。


<details>
  <summary>Details</summary>
Motivation: 现有移动对象研究主要关注时空数据管理，缺乏原生反应式行为支持，导致开发者需自行实现复杂且难以兼顾性能与一致性的方案；因此需要一种低延迟、可扩展且提供并发语义的解决方案。

Method: 提出移动actor抽象，定义M-AODBs平台，并在Orleans上实现Dolphin；在多机环境下用现实场景进行实验评估性能（可扩展性与延迟）。

Result: 实验显示Dolphin在多机部署下具有可扩展性并能提供近实时反应延迟，证明了移动actor及M-AODBs的可行性。

Conclusion: 移动actor抽象及其在M-AODBs/Dolphin中的实现能降低开发复杂性，满足低延迟和可扩展性的需求，适用于现实的反应式移动对象场景。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [111] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: CityVerse unifies multi-source urban data and standardized task taxonomy to enable systematic, reproducible evaluation of LLMs on urban computing tasks, providing APIs, tasks, and visualization with 38M+ records and 43 tasks across four capability levels.


<details>
  <summary>Details</summary>
Motivation: The paper identifies challenges in evaluating LLMs for urban computing due to fragmented data sources and inconsistent task definitions, which hinder fair and reproducible assessment.

Method: They integrate multi-source datasets into coordinate-based APIs, define a capability-based task taxonomy with Task APIs for 43 tasks across four cognitive levels, and implement an interactive frontend with dynamic simulation and visualization; then evaluate mainstream LLMs on representative tasks to demonstrate reproducibility and assessment capabilities.

Result: They build CityVerse, a unified platform offering coordinate-based Data APIs covering 10 urban data categories with 38M+ records, Task APIs for 43 tasks organized into a four-level cognitive hierarchy, and an interactive visualization frontend for real-time retrieval and simulation; validated with evaluations on mainstream LLMs.

Conclusion: CityVerse offers a reusable, standardized foundation that enables fair, reproducible, and systematic evaluation of LLMs in urban computing, facilitating future research and multi-task model development.

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>
