<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 97]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [UI-Venus-1.5 Technical Report](https://arxiv.org/abs/2602.09082)
*Veuns-Team,:,Changlong Gao,Zhangxuan Gu,Yulin Liu,Xinyu Qiu,Shuheng Shen,Yue Wen,Tianyu Xia,Zhenyu Xu,Zhengwen Zeng,Beitong Zhou,Xingran Zhou,Weizhi Chen,Sunhao Dai,Jingya Dou,Yichen Gong,Yuan Guo,Zhenlin Guo,Feng Li,Qian Li,Jinzhen Lin,Yuqi Zhou,Linchao Zhu,Liang Chen,Zhenyu Guo,Changhua Meng,Weiqiang Wang*

Main category: cs.CV

TL;DR: UI-Venus-1.5 通过大规模中间训练、在线 RL 与模型合并，推出统一的 GUI Agent 并在多项基准与真实中文移动应用中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 弥合通用性与任务性能之间的矛盾，打造一个既能广泛适应多种 GUI 场景又能在复杂长时序任务中保持稳健表现的端到端代理。

Method: 三项关键方法：1) 中间训练（Mid-Training）使用 100 亿 tokens、30+ 数据集以构建 GUI 语义基础；2) 在线强化学习（Online RL）采用完整轨迹回放以对齐长期导航目标；3) 通过模型合并（Model Merging）将多个领域模型（grounding、web、mobile）合成为单一检查点。模型家族包括 2B、8B 密集模型与 30B-A3B 专家混合模型。

Result: 在多个基准上创下新 SOTA：ScreenSpot-Pro 69.6%、VenusBench-GD 75.0%、AndroidWorld 77.6%，并在大量中文移动应用中展示了稳健的导航与指令执行能力。

Conclusion: UI-Venus-1.5 是一款面向真实场景的统一端到端 GUI Agent，结合大规模中间训练、在线强化学习和模型合并三大技术改进，实现了在多项 GUI 基准上的显著性能提升。

Abstract: GUI agents have emerged as a powerful paradigm for automating interactions in digital environments, yet achieving both broad generality and consistently strong task performance remains challenging.In this report, we present UI-Venus-1.5, a unified, end-to-end GUI Agent designed for robust real-world applications.The proposed model family comprises two dense variants (2B and 8B) and one mixture-of-experts variant (30B-A3B) to meet various downstream application scenarios.Compared to our previous version, UI-Venus-1.5 introduces three key technical advances: (1) a comprehensive Mid-Training stage leveraging 10 billion tokens across 30+ datasets to establish foundational GUI semantics; (2) Online Reinforcement Learning with full-trajectory rollouts, aligning training objectives with long-horizon, dynamic navigation in large-scale environments; and (3) a single unified GUI Agent constructed via Model Merging, which synthesizes domain-specific models (grounding, web, and mobile) into one cohesive checkpoint. Extensive evaluations demonstrate that UI-Venus-1.5 establishes new state-of-the-art performance on benchmarks such as ScreenSpot-Pro (69.6%), VenusBench-GD (75.0%), and AndroidWorld (77.6%), significantly outperforming previous strong baselines. In addition, UI-Venus-1.5 demonstrates robust navigation capabilities across a variety of Chinese mobile apps, effectively executing user instructions in real-world scenarios. Code: https://github.com/inclusionAI/UI-Venus; Model: https://huggingface.co/collections/inclusionAI/ui-venus

</details>


### [2] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Agent Banana用结构化记忆和图像层分解解决了高分辨率多轮指令编辑的过度修改和一致性问题，并在4K基准上取得最佳多轮一致性与背景保真。


<details>
  <summary>Details</summary>
Motivation: 现实专业编辑流程中存在三大问题：编辑器过度修改、模型多为单轮导致多轮一致性差以及评估分辨率偏低与实际4K工作流不符。为此需要面向高分辨率、对象感知和多轮可验的编辑系统与基准。

Method: 构建了一个由规划器和执行器组成的分层agent架构。规划器基于对话历史与目标进行分层决策；Context Folding将长交互历史压缩为结构化记忆以支持长周期控制；Image Layer Decomposition将图像分解为局部图层以进行局部化编辑，从而保护非目标区域并支持原生分辨率输出。

Result: 在新构建的HDD-Bench（原生4K、对话式、可步骤验证）上，Agent Banana在多轮一致性和背景保真上优于现有方法（例如IC 0.871、SSIM-OM 0.84、LPIPS-OM 0.12），在指令遵循上保持竞争力，并在标准单轮编辑基准上表现良好。

Conclusion: Agent Banana提出了层次化的规划-执行器框架，通过Context Folding和Image Layer Decomposition两大机制，实现了在原生4K分辨率下的高保真、面向对象的多轮指令图像编辑，能够显著改善多轮一致性和背景保真，且在单轮编辑上仍有竞争力。

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

</details>


### [3] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 提出一个无需训练的方法SemanticMoments，通过在预训练语义特征上计算时间高阶矩来编码运动语义，在新建的SimMotion基准上显著优于RGB、flow与文本监督方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示过度依赖静态外观和场景上下文，难以捕捉运动动态；而光流等传统运动输入缺乏语义锚定，二者均不能很好地支持基于语义的运动检索。引入SimMotion基准来暴露这一偏差。

Method: 提出SemanticMoments：一种无需训练的方法，对预训练语义模型的特征沿时间维度计算高阶矩统计量，以表示视频运动语义，并在SimMotion基准上进行评估。

Result: 在SimMotion合成与人工标注的真实数据集上，SemanticMoments持续优于现有方法，证明通过在语义特征空间中使用时间统计量可以更好地分离运动与外观并改进检索性能。

Conclusion: SemanticMoments通过在语义特征空间上计算时间统计量（高阶矩）能有效捕捉运动语义，优于传统RGB、光流和文本监督方法，表明语义时间统计量是可扩展且符合感知的运动理解基础。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [4] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 构建并评估了一个可审计的模块化流水线与注释语料，用于从新闻视频中提取人名，达到可用的精度/召回并避免生成式系统常见的幻觉问题，同时提供完整可追溯性。


<details>
  <summary>Details</summary>
Motivation: 新闻视频图形布局多变，手工索引不可行；需要既可靠又可审计的自动化提取人名方法以满足新闻与分析场景的可追溯性要求。

Method: 构建多样化注释帧语料，使用图形元素检测器（定位mAP@0.5=95.8%），结合确定性OCR与规则/后处理模块；与生成式多模态系统对比以评估准确性与可追溯性差异。

Result: 检测器表现强健（mAP@0.5=95.8%）；流水线精度79.9%、召回74.4%、F1=77.08%；生成式系统F1=84.18%但缺乏数据来源透明性；59%用户在快节奏广播中难以识别屏幕名。

Conclusion: 提出了一个可解释、模块化的流水线，在可查证性和鲁棒性间取得平衡，为新闻视频中的人名提取建立了基线。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [5] [Decoding Future Risk: Deep Learning Analysis of Tubular Adenoma Whole-Slide Images](https://arxiv.org/abs/2602.09155)
*Ahmed Rahu,Brian Shula,Brandon Combs,Aqsa Sultana,Surendra P. Singh,Vijayan K. Asari,Derrick Forchetti*

Main category: cs.CV

TL;DR: 本文通过CNN分析低级别管状腺瘤WSI，发现可从细微组织学特征中预测长期CRC风险，可能用于改进个体化随访策略。


<details>
  <summary>Details</summary>
Motivation: 尽管筛查减少了CRC发生率，但部分被判定为低风险的腺瘤患者仍会在随访期发展为CRC。传统组织学评估可能遗漏微小但重要的形态学或细胞学改变，迫切需要客观、可扩展的方法来识别这些高危个体，以优化随访和预防策略。

Method: 研究使用经验证的CNN架构对WSI进行切片与预处理（颜色归一化、分块、过滤无组织区域），基于病例随访数据将低级别管状腺瘤标注为“进展为CRC”和“未进展”两类，采用交叉验证评估模型并使用可解释性方法（如Grad-CAM、特征重要性映射）定位预测相关的图像区域。

Result: 模型在独立测试集中表现出显著区分能力（如AUC、敏感度、特异度均优于随机），可解释性分析提示预测信号集中在腺体不规则性、基底膜模糊、上皮细胞核密度与形态异质性等区域。外部验证与多中心数据增强了泛化性证据。

Conclusion: 该研究表明利用卷积神经网络（CNN）分析低级别腺瘤的病理全切片图像（WSI），可识别出传统组织学评估难以捕捉的细微形态学特征，从而对患者未来发生结直肠癌（CRC）的风险具有预测价值。

Abstract: Colorectal cancer (CRC) remains a significant cause of cancer-related mortality, despite the widespread implementation of prophylactic initiatives aimed at detecting and removing precancerous polyps. Although screening effectively reduces incidence, a notable portion of patients initially diagnosed with low-grade adenomatous polyps will still develop CRC later in life, even without the presence of known high-risk syndromes. Identifying which low-risk patients are at higher risk of progression is a critical unmet need for tailored surveillance and preventative therapeutic strategies. Traditional histological assessment of adenomas, while fundamental, may not fully capture subtle architectural or cytological features indicative of malignant potential. Advancements in digital pathology and machine learning provide an opportunity to analyze whole-slide images (WSIs) comprehensively and objectively. This study investigates whether machine learning algorithms, specifically convolutional neural networks (CNNs), can detect subtle histological features in WSIs of low-grade tubular adenomas that are predictive of a patient's long-term risk of developing colorectal cancer.

</details>


### [6] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 利用ASQL Conditioner在推理时生成软场景图条件，引入属性-尺寸-数量-位置信息以指导扩散模型，实现零样本、灵活且语义一致的复杂提示图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理包含多对象、属性与空间关系的复杂提示时，常出现语义偏差和结构不连贯。已有基于预定义布局的做法限制了组合灵活性与多样性，因此需要一种在保持自由度的同时提供结构性引导的方法。

Method: 提出ASQL Conditioner：使用轻量语言模型从文本提示中提取属性、尺寸、数量和位置等场景图要素，转换为软视觉条件（非预定义布局）。在推理阶段通过优化这些视觉条件以指导扩散模型生成（inference-time optimization），实现零样本场景图条件化，无需额外训练或刚性布局约束。

Result: 方法在保持文本-图像一致性方面表现提升，生成图像在语义完整性、对象关系与布局连贯性上优于使用刚性布局的基线，同时保持多样性与轻量性（无需额外训练）。具体定量指标和用户研究结果文中应有支持（摘要未给出具体数值）。

Conclusion: 本文提出了一种基于场景图的零样本条件化机制，通过在推理时生成软视觉引导（而非刚性布局），提升文本到图像生成的组合能力和语义一致性。ASQL Conditioner（属性-尺寸-数量-位置）由轻量语言模型生成可微视觉条件，并通过推理时优化引导扩散模型生成，从而在保持文本-图像对齐的同时支持更轻量、连贯且多样的图像合成。

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [7] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 使用RGB-D视觉与轻量级CNN-RNN，在步态的地面到上台阶过渡中可在50–150ms预测窗内实时、准确地预测AP方向的COP（误差~24–29 mm）与TOI（误差~18–21 ms），对辅助设备的预测性控制具有潜在价值。


<details>
  <summary>Details</summary>
Motivation: 现有环境感知多用于辅助系统的环境分类，但在动态变化环境中提前预测足部如何接触环境（以支持预测性控制）研究不足。

Method: 在八名受试者右胫部佩戴RGB-D相机并穿着带传感器鞋垫的情况下，采集行走到上台阶的过渡数据；使用CNN提取视觉特征并结合RNN进行时序预测，连续预测0–250ms预测窗内的AP方向COP和TOI。

Result: 在150/100/50ms预测窗下，COP MAE分别为29.42/26.82/23.72 mm，TOI MAE分别为21.14/20.08/17.73 ms。托干速度对误差影响不显著，较快的脚尖摆动速度可提升COP预测准确性；更前位的着地点降低COP预测准确性；模型可在普通笔记本或边缘设备上以60 FPS运行。

Conclusion: 视觉预测脚步接触点（COP）与接触时间（TOI）在台阶跨越过渡任务中是可行的，使用轻量级CNN-RNN模型在250ms预测窗口内能达到可接受的误差。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [8] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 提出VLM-UQBench和扰动流水线，展示当前UQ方法难以进行模态化、细粒度不确定性定位，亟需更好的UQ设计以保障VLM可靠部署。


<details>
  <summary>Details</summary>
Motivation: 确保视觉-语言模型在实际应用中的安全性与可靠性，需要将不确定性定位到来源（图像、文本或对齐问题），以便采取针对性缓解措施。

Method: 构建包含600个VizWiz样本的基准数据集，划分为clean、image-specific、text-specific和cross-modal四类子集；设计8种视觉、5种文本和3种跨模态扰动的可扩展扰动流水线；提出两个度量指标（评估UQ分数对扰动敏感性及其与幻觉的相关性），在四个VLM和三个数据集上评估多种UQ方法。

Result: 发现现有UQ方法存在模态专门化、对基础VLM依赖大、不确定性与幻觉存在并发但UQ分数信号弱且不一致，以及在捕捉细粒度实例级模糊性方面失败等问题。

Conclusion: VLM-UQBench揭示了现有不确定性量化方法在模态识别与跨模态不确定性定位方面的明显不足，表明当前UQ实践无法满足部署VLM时对细粒度、模态感知不确定性的需求。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [9] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: IR-SIS将微调的SAM3、视觉-语言模型和代理式自适应细化结合，支持自然语言临床交互，显著提升外科图像分割的准确性与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有外科分割方法通常限定在预定义类别，且仅输出一次性预测，无法在术中根据图像复杂性或临床反馈自适应改进；此外缺乏自然语言交互机制使临床专家难以直接参与并引导分割改进。

Method: 基于微调后的SAM3作为初始分割器，结合视觉语言模型（VLM）进行器械检测与分割质量评估；引入一个智能代理工作流，根据评估结果自适应选择不同的细化策略（如点/框提示、更多推理步骤或定制化后处理）；并支持通过自然语言的临床反馈作为环路输入，实现人机交互的迭代细化。

Result: 在EndoVis2017与EndoVis2018构建的多粒度语言标注数据集上进行了实验，模型在域内与跨域（OOD）测试中均取得了SOTA性能；引入临床自然语言交互后，分割结果进一步提升，体现了系统的实用性与鲁棒性。

Conclusion: 该论文提出了一种可接受自然语言描述、具备迭代自我完善能力的外科图像分割系统IR-SIS，填补了现有方法在类别限制、一次性预测和缺乏临床可交互机制方面的空白。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [10] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 池化文本嵌入常规用法价值有限，但作为训练免费且简单的引导，可为扩散Transformer带来显著且广泛的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前方法越来越倾向于仅用注意力抛弃调制，作者质疑是否调制仍有必要及其潜在优势，寻找简便且通用的改进方式。

Method: 对比分析了常规将池化嵌入用于调制与仅依赖注意力的做法；提出将池化嵌入作为训练免费且轻量的引导信号的方法，并在多个扩散模型与任务上评估其效果。

Result: 常规调制贡献有限，但作为引导的池化嵌入能带来显著改进，在文本到图像/视频生成及图像编辑等任务中均取得性能提升，且实现成本低。

Conclusion: 传统的池化文本嵌入（modulation）在扩散Transformer的常规用法中贡献有限，注意力机制通常已足够传递提示信息。但如果将池化嵌入重新作为“引导”使用，可以在无需训练、实现简单且开销极小的条件下显著提升性能，实现可控的属性偏移，适用于多种扩散模型与任务（文本到图像/视频生成、图像编辑）。

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [11] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: X-Mark 用条件 U-Net 在胸片显著区生成样本特异性低频扰动，结合多目标训练与拉普拉斯正则化，实现尺度不变且对黑盒所有权验证有效，CheXpert 实验显示优秀性能（WSR 100%、降低假阳性）。


<details>
  <summary>Details</summary>
Motivation: 现有面向自然图像的数据集所有权验证方法在医学影像上表现欠佳：固定尺度的静态水印难以适应高分辨率、动态缩放以及细微解剖结构的视觉限制，同时需维持医学诊断质量与避免可见伪影。

Method: 使用条件 U-Net 在每个样本显著区域生成唯一扰动；训练目标包含多组件（保证水印有效性、对动态缩放鲁棒性、保持诊断质量与视觉可区分性）；引入拉普拉斯正则化以罚高频扰动，从而实现尺度不变性；所有权验证采用黑盒检测模型特征行为。

Result: 在 CheXpert 上大量实验表明 X-Mark 有效：WSR 达到 100%，在 Ind-M 场景下减少 12% 误判概率，并对潜在自适应攻击表现出抗性。

Conclusion: X-Mark 提出了一种针对胸片的样本特异性干净标签水印方法，有效保护数据集版权，同时兼顾诊断质量与对抗鲁棒性。

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [12] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: 提出一种迁移学习驱动的深度多模态伤口评估方法，融合图像与临床变量，能同时预测伤口变量与愈合轨迹，从而用于住院风险预测和早期复杂性检测。


<details>
  <summary>Details</summary>
Motivation: 住院是伤口护理成本高的主要因素，早期发现可能恶化的伤口能降低住院率与医疗成本；现有工作多关注特定伤口类型的愈合轨迹，缺少通用且能同时利用图像与变量的预测方法。

Method: 基于迁移学习，构建多模态网络，输入为伤口图像与临床伤口变量，先用预训练视觉模型提取图像特征，再与结构化变量融合，通过分类或时间序列回归预测愈合轨迹与住院风险。

Result: 主要贡献为提出可从伤口图像预测伤口变量并联合预测愈合轨迹的迁移学习评估方案，能够早期识别复杂伤口并减少临床诊断时间（文中宣称提高了预测性能与诊断效率，具体指标文摘未提供）。

Conclusion: 该论文提出了一个基于深度多模态方法的伤口住院风险预测模型，能结合伤口变量与伤口图像来更自信地预测患者住院风险。

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [13] [GAFR-Net: A Graph Attention and Fuzzy-Rule Network for Interpretable Breast Cancer Image Classification](https://arxiv.org/abs/2602.09318)
*Lin-Guo Gao,Suxing Liu*

Main category: cs.CV

TL;DR: 提出GAFRNet：结合图注意力与可微分模糊规则的弱监督病理图像分类方法，提升性能并提供透明决策规则。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在标注稀缺时性能下降且缺乏可解释性，限制了临床应用；因此需要在弱监督下既鲁棒又可解释的模型。

Method: 构建基于相似性的样本图，使用多头图注意力捕捉样本间复杂关系，同时引入可微分模糊规则模块编码节点度、聚类系数和标签一致性等拓扑特征，形成显式IF-THEN规则。

Result: 在BreakHis、Mini-DDSM和ICIAR2018三个数据集的多放大倍数和分类任务上，GAFRNet优于多个最新方法，显示了更好的泛化性与实用性。

Conclusion: GAFRNet在弱监督条件下通过图注意力与可微分模糊规则融合，有效提高了组织学图像分类的准确性并增强了可解释性。

Abstract: Accurate classification of breast cancer histopathology images is pivotal for early oncological diagnosis and therapeutic intervention.However, conventional deep learning architectures often encounter performance degradation under limited annotations and suffer from a "blackbox" nature, hindering their clinical integration. To mitigate these limitations, we propose GAFRNet, a robust and interpretable Graph Attention and FuzzyRule Network specifically engineered for histopathology image classification with scarce supervision. GAFRNet constructs a similarity-driven graph representation to model intersample relationships and employs a multihead graph attention mechanism to capture complex relational features across heterogeneous tissue structures.Concurrently, a differentiable fuzzy-rule module encodes intrinsic topological descriptorsincluding node degree, clustering coefficient, and label consistencyinto explicit, human-understandable diagnostic logic. This design establishes transparent "IF-THEN" mappings that mimic the heuristic deduction process of medical experts, providing clear reasoning behind each prediction without relying on post-hoc attribution methods. Extensive evaluations on three benchmark datasets (BreakHis, Mini-DDSM, and ICIAR2018) demonstrate that GAFR-Net consistently outperforms various state-of-the-art methods across multiple magnifications and classification tasks. These results validate the superior generalization and practical utility of GAFR-Net as a reliable decision-support tool for weakly supervised medical image analysis.

</details>


### [14] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: 经过约300次基于多中心膀胱癌数据集的实验，发现ConvNeXt适合分布内分类，ViT在校准与分布外可解释性上更好，但无单一模型兼顾所有需求。


<details>
  <summary>Details</summary>
Motivation: 医学影像中异常区域通常只占图像一小部分，常见的在自然图像上表现优异的深度模型在此类任务上的适应性和可解释性尚不清楚，因此需要系统比较不同模型的分类性能、校准性与可解释性。

Method: 在公开多中心膀胱癌数据集上进行了约300次实验，评估内容包括：标准分类性能比较、模型校准分析（未指明具体校准度量但提及校准效果）、以及使用GradCAM++进行可解释性评估，同时加入测试时增强（TTA）来提升可解释性。比较的模型包括ConvNeXt系列、Swin Transformer、ViT及其变体等。

Result: 实验结果显示：ConvNeXt系列在分类准确率上对训练分布的样本较为有效（例如约60%准确率），但泛化能力有限；ViT系列在模型校准方面优于ConvNeXt与Swin Transformer；没有模型能同时兼顾高精度和通用的可解释性，ConvNeXt适合分布内样本，而ViT及其变体更适合解释分布外样本。测试时增强能改善可解释性表现。

Conclusion: 本文比较了13种深度模型（4个CNN，8个Transformer）在膀胱癌影像分类上的表现，发现没有单一模型适用于所有情况：ConvNeXt系列在分布内样本上表现较好但泛化受限，ViT及其变体在校准和处理分布外样本的可解释性方面更优。

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [15] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: 提出Kyrtos方法：通过聚类识别曲线线段中点，解析方向/趋势等特征，构建带属性图并生成自然语言描述，能高精度重构多函数图表并转换为SPN表示。


<details>
  <summary>Details</summary>
Motivation: 技术文档中大量图表包含重要知识，整体理解依赖于对图中不同模态（例如图形、表格、文本）及其关联的精确分析，自动识别含曲线图表能丰富文档语义并支持更复杂的功能表示。

Method: 识别阶段采用基于聚类的方法识别线段的中点以分割构成曲线的线段；分析阶段对提取的线段解析方向、趋势等行为特征，将线段关系转换为带属性的图并生成自然语言句子，最后转为SPN图。

Result: 通过结构相似度评估，广泛实验表明Kyrtos在多函数曲线图的识别与分析上具有较高的准确性。

Conclusion: Kyrtos方法能够从技术文档的图表图像中自动识别并分析含曲线的图表，提取线段中点并重构曲线结构，进一步生成带属性的图和自然语言描述，最终支持转换为随机Petri网以表示图表内部功能。

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [16] [Impact of domain adaptation in deep learning for medical image classifications](https://arxiv.org/abs/2602.09355)
*Yihang Wu,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 在四个医学影像数据集上系统比较了10种模型的域适应方法，发现DA通常能小幅到中幅提升性能、鲁棒性、可解释性与校准性，但在联邦学习场景效果有限。


<details>
  <summary>Details</summary>
Motivation: 解决源域有标签但目标域标签不足的问题，通过对齐不同域的数据至共享特征空间，使在源域学到的知识能迁移到目标域，从而提升目标域性能与可靠性。

Method: 使用10种深度学习模型（包括ResNet34等）模拟常见DA技术，在四个医学影像数据集上进行实验，测试情形涵盖多模态、噪声干扰、联邦学习、解释性分析（gradcam++）与分类器校准（ECE）。

Result: 在脑肿瘤数据集上，使用DA的ResNet34将性能提升约4.7%；对抗高斯噪声时，DA提高约3%准确率；在皮肤癌分类的联邦学习中，DA仅带来约0.3%的增益；使用DA的模型在gradcam++可解释性上更具临床价值；多模态数据集上DA使ECE降低约2%。

Conclusion: 该论文结论是：在多种医学影像数据集上引入域适应（DA）能普遍提升深度学习模型性能、鲁棒性、可解释性及校准性，尽管在联邦学习场景中增益有限。

Abstract: Domain adaptation (DA) is a quickly expanding area in machine learning that involves adjusting a model trained in one domain to perform well in another domain. While there have been notable progressions, the fundamental concept of numerous DA methodologies has persisted: aligning the data from various domains into a shared feature space. In this space, knowledge acquired from labeled source data can improve the model training on target data that lacks sufficient labels. In this study, we demonstrate the use of 10 deep learning models to simulate common DA techniques and explore their application in four medical image datasets. We have considered various situations such as multi-modality, noisy data, federated learning (FL), interpretability analysis, and classifier calibration. The experimental results indicate that using DA with ResNet34 in a brain tumor (BT) data set results in an enhancement of 4.7\% in model performance. Similarly, the use of DA can reduce the impact of Gaussian noise, as it provides $\sim 3\%$ accuracy increase using ResNet34 on a BT dataset. Furthermore, simply introducing DA into FL framework shows limited potential (e.g., $\sim 0.3\%$ increase in performance) for skin cancer classification. In addition, the DA method can improve the interpretability of the models using the gradcam++ technique, which offers clinical values. Calibration analysis also demonstrates that using DA provides a lower expected calibration error (ECE) value $\sim 2\%$ compared to CNN alone on a multi-modality dataset.

</details>


### [17] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: 提出可微分双向协同半监督框架（DBiSL），实现回归与分割任务的在线双向交互，结合有监督、一致性、伪标签和不确定性模块，在两个基准上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注成本高、像素级标注稀缺，半监督学习通过利用未标注数据缓解数据匮乏。当前双任务协同方法只支持单向（通常由回归指导分割），无法在线实现分割到回归的反向转换，限制了跨任务互助的潜力。

Method: 构建一个可微分的双向协同学习体系，融合四个关键模块：有监督学习、基于一致性的正则化、伪标签监督以及不确定性估计。核心是设计可导的任务间转换机制，使分割与回归能够在线双向交互，并通过不确定性来权衡伪标签与一致性损失。

Result: 在两个基准数据集上的实验显示所提方法优于现有半监督和双任务方法，取得了最先进的性能（文章宣称）。此外，提出的统一框架为双任务驱动的SSL和更广泛的多任务视觉应用提供了新的架构基础。

Conclusion: 本文提出的DBiSL框架通过可微分的双向协同学习，实现了回归与分割任务之间在线的双向交互，从而充分利用跨任务互补监督，提升了半监督医学图像分割性能。实验表明在两个基准数据集上达到了最先进的结果。

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [18] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: 单切片医学图像到三维的零样本重建受平面数据的深度歧义限制，SAM3D在保持拓扑结构上表现最好，但总体体积恢复不可靠，需多视角输入以提高临床可用性。


<details>
  <summary>Details</summary>
Motivation: 临床对三维解剖的需求高但体积成像成本与等待时间大，探讨能否用单张二维医疗影像通过预训练图像到三维模型重建可靠三维结构。

Method: 构建受控零样本基准，比较五种最新的图像到三维模型（SAM3D、Hunyuan3D-2.1、Direct3D、Hi3DGen、TripoSG），在六个医学数据集和两个自然数据集上使用体素重叠和点云距离等指标评估单切片到三维重建性能。

Result: 体素重叠指标在医学数据集上普遍为中等，显示深度恢复失败；全局点云/拓扑距离指标则能更好区分方法，SAM3D在拓扑相似性上表现最佳，其他模型更容易过度简化重建。

Conclusion: 单切片重建在医学影像中存在显著深度歧义，当前图像到三维的基础模型难以可靠恢复体积信息。

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [19] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: 提出K-Sort Eval，用自适应后验校正和动态匹配，使VLM评估更贴近人类偏好且更高效，实验证明其与人工评价一致且显著节省对局次数。


<details>
  <summary>Details</summary>
Motivation: 人工众包评价（如Arena平台）成本高、耗时且难以扩展；直接用VLM替代人工会受限于其幻觉与偏见，导致与人类偏好不一致；需要一种既可靠又高效的自动化评估方法。

Method: 构建K-Sort Arena的高质量人类投票数据集；在评估新模型时采用(K+1)-选对战并由VLM给出排序；使用基于一致性的自适应贝叶斯后验校正以修正VLM输出；设计不确定性与多样性平衡的动态匹配策略以选择最有效的比较对局。

Result: 在大量实验中，K-Sort Eval的评估结果与K-Sort Arena的人类评级高度一致，通常评估仅需少于90次模型对局，表现出更高的效率与可靠性。

Conclusion: K-Sort Eval通过结合后验校正与动态匹配，实现了在视觉语言模型（VLM）上对生成模型进行高效且与人类偏好对齐的评估。

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [20] [LARV: Data-Free Layer-wise Adaptive Rescaling Veneer for Model Merging](https://arxiv.org/abs/2602.09413)
*Xinyu Wang,Ke Deng,Fei Dou,Jinbo Bi,Jin Lu*

Main category: cs.CV

TL;DR: 提出LARV——一种无训练、无数据、与合并器无关的层自适应缩放外壳，通过在各层对任务向量做分层缩放抑制浅层干扰、放大深层信号，能显著提升多种任务向量合并方法的性能且开销极小。


<details>
  <summary>Details</summary>
Motivation: 现有任务向量合并方法对所有层采取近似统一的处理，但大规模视觉Transformer存在显著层间异质性：浅层对干扰敏感，而深层编码稳定的任务特定特征。因此需要层感知的缩放以改善合并质量。

Method: LARV是一个训练免费且无数据的外壳（veneer），在合并前为每个任务向量的各层分配缩放系数，可插到任意现有任务向量合并方法（如TIES、TSV-M、Iso-C/CTS）之前。通过计算简单的无数据层代理并用轻量规则（如分层两/三级固定缩放或连续映射）转成尺度，抑制浅层、放大深层，且不需重训练或改动基合并器。

Result: 在FusionBench上的ViT系列模型（ViT-B/32、ViT-B/16、ViT-L/14）和多任务设置（8/14/20任务），LARV稳定提升所有基线合并方法。例如Iso-C+LARV在ViT-B/32上达85.9%、ViT-B/16上达89.2%、ViT-L/14上达92.6%。层分析和鲁棒性测试表明LARV有效抑制浅层干扰并略微放大深层稳定特征。

Conclusion: 本文提出的LARV通过对每个任务向量在不同层应用自适应缩放，从而改善了任务向量合并的效果，尤其能抑制浅层干扰、放大全局稳定的深层特征，使合并更鲁棒。

Abstract: Model merging aims to combine multiple fine-tuned models into a single multi-task model without access to training data. Existing task-vector merging methods such as TIES, TSV-M, and Iso-C/CTS differ in their aggregation rules but treat all layers nearly uniformly. This assumption overlooks the strong layer-wise heterogeneity in large vision transformers, where shallow layers are sensitive to interference while deeper layers encode stable task-specific features. We introduce LARV, a training-free, data-free, merger-agnostic Layer-wise Adaptive Rescaling Veneer that plugs into any task-vector merger and assigns a per-layer scale to each task vector before aggregation, and show it consistently boosts diverse merging rules. LARV adaptively suppresses shallow-layer interference and amplifies deeper-layer alignment using a simple deterministic schedule, requiring no retraining or modification to existing mergers. To our knowledge, this is the first work to perform layer-aware scaling for task-vector merging. LARV computes simple data-free layer proxies and turns them into scales through a lightweight rule; we study several instantiations within one framework (e.g., tiered two/three-level scaling with fixed values, or continuous mappings) and show that tiered choices offer the best robustness, while continuous mappings remain an ablation. LARV is orthogonal to the base merger and adds negligible cost. On FusionBench with Vision Transformers, LARV consistently improves all task-vector baselines across 8/14/20-task settings; for example, Iso-C + LARV reaches 85.9% on ViT-B/32, 89.2% on ViT-B/16, and 92.6% on ViT-L/14. Layerwise analysis and corruption tests further indicate that LARV suppresses shallow-layer interference while modestly amplifying deeper, task-stable features, turning model merging into a robust, layer-aware procedure rather than a uniform one.

</details>


### [21] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: 提出算子层面的统一理论，证明块结构非线性逆问题在分块Lipschitz、局部可辨识和次高斯噪声下的稳定性和集中性结果，并以Gaussian Splatting为例给出显式常数，揭示分辨率—稳定性权衡。


<details>
  <summary>Details</summary>
Motivation: 现代成像与可微分渲染中的高维非线性逆问题参数通常具有块结构（例如场景分块、分辨率层次或分量化模型），理解在噪声和有限分辨率下参数可恢复性的算子级本质限制对于设计算法和评估性能是关键。因此希望一个与具体重建算法无关、能给出明确常数和分辨率依赖的理论框架。

Method: 构建块状参数的算子范式，结合块级Lipschitz条件和局部可辨识性来推导确定性稳定性不等式；通过控制最小二乘失配的梯度和Hölder/Lipschitz性质建立全局Lipschitz边界；利用次高斯噪声下的浓缩不等式（如Bernstein或Talagrand类型工具）导出非渐近的高概率参数误差界，且分析明确依赖于正向算子的算子范数和分辨率相关参数。

Result: 得到：1) 在统一假设下的确定性稳定不等式和最小二乘失配的全局Lipschitz常数表达；2) 次高斯噪声情形下的非渐近浓缩界，进而给出高概率的参数估计误差界，这些界仅由正向算子的性质决定；3) 对Gaussian Splatting渲染算子的验证与显式常数计算，展示分辨率与模型复杂度的不可避免权衡，说明提高分辨率会放大参数估计难度。

Conclusion: 本文建立了一个基于算子理论的统一框架，用于分析具有块结构参数的非线性逆问题的稳定性和统计收敛性，证明了在分块Lipschitz几何、局部可辨识性和次高斯噪声假设下可得确定性稳定不等式、最小二乘失配函数的全局Lipschitz界以及非渐近浓缩估计，从而得到与具体重建算法无关且与正向算子本征相关的高概率参数估计误差界；并以Gaussian Splatting渲染算子为例验证假设并导出显式常数，揭示了分辨率与模型复杂度之间的基本稳定性—分辨率权衡。

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [22] [Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification](https://arxiv.org/abs/2602.09425)
*Yiqiao Li,Bo Shang,Jie Wei*

Main category: cs.CV

TL;DR: 无需微调的VLM+深度图生成管线，能将稀疏路侧LiDAR点云转为图像代理，实现少样本细粒度卡车分类并能用作冷启动，减少人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于LiDAR的细粒度车辆分类依赖监督深度学习并需大量手工标注，难以扩展；而VLMs具备少样本泛化能力，但点云与图像之间的模态差异阻碍了其在路侧LiDAR上的应用，因此需要一种无须微调即可将点云映射到VLM可处理形式的方法。

Method: 提出了一个深度感知图像生成管线：噪声去除、时空配准、朝向校正、形态学操作和各向异性平滑，将稀疏、遮挡严重的LiDAR扫描转换为可被VLM处理的深度图，并在不微调模型参数的前提下用VLM进行少样本分类。此外，利用VLM生成标签来引导轻量监督模型作为冷启动方案。

Result: 在包含20类车辆的真实数据集上，使用每类仅16-30个样本时仍取得了具有竞争力的分类精度；在超低样本(k<4)时观察到“语义锚”效应（文本引导有正则化作用），但在样本较多时会因语义不匹配降低准确率；作为冷启动，VLM生成的标签帮助训练的轻量监督模型在特定驳运类别（20ft/40ft/53ft货柜）上达到超过75%的正确分类率。

Conclusion: 该论文提出了一个无需参数微调的框架，将现成的视觉-语言模型（VLMs）用于道路边缘LiDAR的细粒度卡车分类，通过将稀疏3D点云转化为深度编码的2D视觉代理来弥合模态差异，从而在少样本场景下仍能达到有竞争力的分类精度。

Abstract: Fine-grained truck classification is critical for intelligent transportation systems (ITS), yet current LiDAR-based methods face scalability challenges due to their reliance on supervised deep learning and labor-intensive manual annotation. Vision-Language Models (VLMs) offer promising few-shot generalization, but their application to roadside LiDAR is limited by a modality gap between sparse 3D point clouds and dense 2D imagery. We propose a framework that bridges this gap by adapting off-the-shelf VLMs for fine-grained truck classification without parameter fine-tuning. Our new depth-aware image generation pipeline applies noise removal, spatial and temporal registration, orientation rectification, morphological operations, and anisotropic smoothing to transform sparse, occluded LiDAR scans into depth-encoded 2D visual proxies. Validated on a real-world dataset of 20 vehicle classes, our approach achieves competitive classification accuracy with as few as 16-30 examples per class, offering a scalable alternative to data-intensive supervised baselines. We further observe a "Semantic Anchor" effect: text-based guidance regularizes performance in ultra-low-shot regimes $k < 4$, but degrades accuracy in more-shot settings due to semantic mismatch. Furthermore, we demonstrate the efficacy of this framework as a Cold Start strategy, using VLM-generated labels to bootstrap lightweight supervised models. Notably, the few-shot VLM-based model achieves over correct classification rate of 75 percent for specific drayage categories (20ft, 40ft, and 53ft containers) entirely without the costly training or fine-tuning, significantly reducing the intensive demands of initial manual labeling, thus achieving a method of practical use in ITS applications.

</details>


### [23] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: SceneReVis 通过“诊断-行动”循环和 SceneChain-12k 数据集，结合 SFT 到 Agentic RL 的两阶段训练，显著减少三维场景生成中的空间冲突并提升目标导向性能。


<details>
  <summary>Details</summary>
Motivation: 现有一次性（one-pass）三维场景生成方法缺乏深思熟虑的推理，容易出现空间冲突（如物体重叠或碰撞），需要一种可迭代、具备多模态验证能力的方法以提高生成场景的空间一致性和目标完成度。

Method: 提出 SceneReVis 框架，利用多模态反馈在生成过程中循环执行“诊断（检测空间冲突）—采取行动（修正）”的步骤；构建大规模数据集 SceneChain-12k，通过逆向工程管线生成因果建构轨迹以支持步骤式学习；设计两阶段训练流程：先监督微调（SFT），再通过智能体式强化学习（Agentic RL）使模型演化为主动的空间规划器。

Result: 大规模实验表明 SceneReVis 在高保真生成和目标导向优化上达到或超越现有最先进水平，并在长尾域（rare cases）中展现出稳健的泛化能力。

Conclusion: SceneReVis 提出了一种基于视觉的自反省（self-reflection）框架，通过迭代的“诊断-行动”循环显式定位与解决空间冲突，从而减少碰撞等空间幻觉问题，显著提升了三维场景合成的保真度与目标导向优化能力。

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [24] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: Fine-T2I：一个包含600万+对、高质量、开放许可的 T2I 微调数据集，通过合成与专业摄影图像并严格筛选，能显著提升模型的生成质量与指令遵从性。


<details>
  <summary>Details</summary>
Motivation: 当前公开可用的微调数据集存在分辨率低、文本-图像对齐差或多样性不足的问题，导致开源微调模型性能落后于企业级模型。需要一个大规模且高质量的开源数据集来改进微调效果。

Method: 收集合成图像（来自强大的现代生成模型）与专业摄影师的真实图像，按照 10 个任务组合、32 个提示类别、11 种视觉风格和 5 种提示模板构建数据样本；对文本-图像对进行严格过滤（文本-图像对齐、视觉保真度、提示质量），剔除超过95%的初始候选。

Result: 最终数据集包含超过600万对文本-图像，约2TB，覆盖多任务与多风格；在多种预训练扩散与自回归模型上微调均带来生成质量与指令遵从性的稳定提升（经人工评估、视觉对比和自动指标验证）。

Conclusion: Fine-T2I 提供了高质量、规模化且开放的 T2I 微调数据集，有望缩小开源模型与企业级模型之间的数据差距。

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [25] [A Scoping Review of Deep Learning for Urban Visual Pollution and Proposal of a Real-Time Monitoring Framework with a Visual Pollution Index](https://arxiv.org/abs/2602.09446)
*Mohammad Masudur Rahman,Md. Rashedur Rahman,Ashraful Islam,Saadia B Alam,M Ashraful Amin*

Main category: cs.CV

TL;DR: 基于PRISMA-ScR的综述（26篇），当前UVP深度学习研究分散、数据与评估不统一，建议建立统一分类、跨城市基准数据集、通用检测模型和视觉污染指数以实现可持续的城市视觉污染管理。


<details>
  <summary>Details</summary>
Motivation: 城市视觉污染影响市容、景观质量与居民福祉，但现有研究在检测、分类与综合管理应用上分散且缺乏统一标准；因此需要梳理现状并提出可操作的框架以促进标准化与可持续管理。

Method: 按PRISMA-ScR流程，从七个数据库（Scopus、Web of Science、IEEE Xplore、ACM DL、ScienceDirect、SpringerNatureLink、Wiley）系统检索并筛选出26篇相关论文；对模型架构（YOLO、Faster R-CNN、EfficientDet等）、数据集、应用系统与地理分布进行归纳分析；提出了一个包含视觉污染指数的监测与管理框架。

Result: 发现研究主要集中在特定污染物类别，采用主流目标检测模型的变体；现有数据集地域性强、缺乏统一分类体系；实时系统较少且存在地理偏倚；提出了包含污染物分类、跨城基准数据集、通用模型与视觉污染指数的一体化管理框架。

Conclusion: 该综述确认基于深度学习的城市视觉污染（UVP）检测研究已有一定进展，但仍分散且存在显著不足，尤其在数据集标准化、跨区域泛化与实时应用集成方面。

Abstract: Urban Visual Pollution (UVP) has emerged as a critical concern, yet research on automatic detection and application remains fragmented. This scoping review maps the existing deep learning-based approaches for detecting, classifying, and designing a comprehensive application framework for visual pollution management. Following the PRISMA-ScR guidelines, seven academic databases (Scopus, Web of Science, IEEE Xplore, ACM DL, ScienceDirect, SpringerNatureLink, and Wiley) were systematically searched and reviewed, and 26 articles were found. Most research focuses on specific pollutant categories and employs variations of YOLO, Faster R-CNN, and EfficientDet architectures. Although several datasets exist, they are limited to specific areas and lack standardized taxonomies. Few studies integrate detection into real-time application systems, yet they tend to be geographically skewed. We proposed a framework for monitoring visual pollution that integrates a visual pollution index to assess the severity of visual pollution for a certain area. This review highlights the need for a unified UVP management system that incorporates pollutant taxonomy, a cross-city benchmark dataset, a generalized deep learning model, and an assessment index that supports sustainable urban aesthetics and enhances the well-being of urban dwellers.

</details>


### [26] [Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing](https://arxiv.org/abs/2602.09449)
*Yan Luo,Henry Huang,Todd Y. Zhou,Mengyu Wang*

Main category: cs.CV

TL;DR: 提出Look-Ahead与Look-Back两种训练免费潜在轨迹平滑方法，通过在潜变量空间进行前瞻/回顾平滑，减少误差累积，显著提升流匹配确定性扩散生成质量，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 修改速度场v会在整个生成过程中传播、累积误差；而直接调整潜变量z能被预训练速度网络自我纠正，因此提出训练免费地在潜变量轨迹上进行平滑以减少误差累积并提升图像生成质量。

Method: 提出两种基于未来/过去速度v和潜变量z信息的轨迹调整方法：Look-Ahead（前瞻）通过用受曲率门控的权重平均当前与下一步潜变量来平滑轨迹；Look-Back（回顾）用指数移动平均（带衰减）对潜变量进行平滑。此外对比了修改速度场与修改潜变量的方法，并证明后者更易被预训练速度网络纠正。

Result: 在COCO17、CUB-200、Flickr30K等多个数据集上的大量实验及综合评估指标显示，所提的训练免费轨迹平滑方法显著优于多种最新方法，生成表现提升明显。

Conclusion: 本文提出两种训练免费（training-free）的潜在轨迹平滑方法，通过在潜在空间直接调整生成路径来减少误差累积，从而提升基于流匹配的确定性扩散模型的生成质量。

Abstract: Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.

</details>


### [27] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 预训练VLM内隐具备伪影检测能力；ArtifactLens通过in-context learning与文本指令优化等轻量脚手架，用仅数百张标注样本实现SOTA且更具泛化性。


<details>
  <summary>Details</summary>
Motivation: 现代图像生成器生成逼真图像，只有局部伪影（如畸变手、变形物体）揭示合成来源。现有检测器需在大量标注数据上微调，昂贵且难以应对新生成器/新伪影类型。希望用更少的标签快速适应并保持高效检测。

Method: 提出ArtifactLens系统：一个多组件架构，结合in-context learning、文本指令优化（text instruction optimization）以及若干改进模块，从而利用仅数百张每类标注样本对预训练VLM进行轻量化适应。

Result: ArtifactLens在五个人体伪影基准上取得SOTA（首次跨多数据集评估），所需标注数据量比现有方法少几个数量级；方法还能推广到物体形态、动物解剖及实体交互等伪影类型，并可用于AIGC检测任务。

Conclusion: 本文结论是：预训练的视觉语言模型（VLMs）本身已编码了检测图像合成伪影的能力，通过适当的“脚手架”（scaffolding）与少量标注样本即可解锁并实现高性能检测。

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [28] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: FD-DB通过低频可解释编辑+高频残差双分支和两阶段训练，在不牺牲结构稳定性的前提下实现合成到真实外观迁移，提升了下游分割效果。


<details>
  <summary>Details</summary>
Motivation: 合成数据和真实数据在外观与成像上差异大，现有无配对风格迁移方法常在真实感和结构保持之间权衡，需方法在保持几何语义结构的同时提高真实域一致性。

Method: 将外观迁移分为可解释的低频编辑分支（预测白平衡、曝光、对比度、饱和度、模糊、胶片颗粒等参数）和高频残差补偿的自由分支，并用门控融合在频率约束下合并；采用两阶段训练先稳定编辑分支再释放残差分支。

Result: 在YCB-V数据集上，FD-DB提升了真实域外观一致性，显著提高了下游语义分割性能，同时保持几何与语义结构。

Conclusion: 作者提出FD-DB，通过频率解耦的双分支设计在外观迁移中兼顾真实感与结构稳定性。

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [29] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: WeakSupCon：一种将袋级标签融入对比学习的弱监督表征学习方法，提升了WSI MIL任务的补丁特征质量与下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法多侧重于特征聚合，往往直接使用预训练的冻结patch特征，但在MIL场景下用于预训练的表征学习（尤其是利用袋标签的弱监督信号）尚被忽视。

Method: 在对比学习框架中利用滑动窗口或随机采样得到的补丁构成的bag-level标签信息，设计弱监督对比损失以拉开不同标签的补丁表示距离，而不需要实例级伪标签。训练时直接在patch特征上进行对比学习，保持与现有MIL流水线兼容。

Result: 在三个数据集上的实验表明，WeakSupCon生成的图像补丁特征在下游MIL性能上优于自监督对比学习方法。

Conclusion: 本文提出了WeakSupCon，一种将袋标签引入对比表征学习的方法，改善了用于MIL的补丁特征表示，从而提升下游MIL任务性能。

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [30] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: Align-TI通过对齐视觉-指令与令牌间动态生成概率，改进了多模态模型的知识蒸馏，显著提升小模型性能，甚至超越更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏多以静态下一个令牌对齐为主，忽略了多模态模型中重要的令牌间动态交互（包括视觉-指令交互与响应内令牌交互），这些交互承载了多模态理解与连贯生成的能力。

Method: 提出两个对齐模块：1) IVA（Instruction-Visual Alignment）：通过在显著视觉区域上对齐师模型和学生模型的视觉表示，使学生学习指令相关的视觉信息提取能力；2) TPA（Token-to-Token Probability Alignment）：对齐序列生成过程中逐步的令牌间转移概率，捕捉教师模型的动态生成逻辑。训练过程中将这两项损失与常规蒸馏目标结合。

Result: 在大量实验中，Align-TI相比Vanilla KD取得2.6%相对提升；蒸馏出2B参数模型（Align-TI-2B）在评测上比更大的LLaVA-1.5-7B高7.0%，证明方法在参数效率与性能上均优越。

Conclusion: 本文提出Align-TI，一种基于令牌交互的知识蒸馏框架，能够在压缩多模态大模型时保留关键信息提取与生成逻辑，从而在精度上显著超越现有蒸馏方法。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [31] [OSI: One-step Inversion Excels in Extracting Diffusion Watermarks](https://arxiv.org/abs/2602.09494)
*Yuwei Chen,Zhenliang He,Jia Tang,Meina Kan,Shiguang Shan*

Main category: cs.CV

TL;DR: OSI把水印提取变为符号分类，单步完成，速度快20x、精度更高、容量翻倍，并对多种设置泛化良好。


<details>
  <summary>Details</summary>
Motivation: 现有训练无关的方法（如Gaussian Shading）将水印嵌入扩散初始噪声，但提取需要耗时的多步扩散反演以恢复精确初始噪声，计算开销大且慢。故需一种更快且准确的单步提取方法。

Method: 将水印从初始噪声的精确回归问题转化为可学习的符号（正负）分类问题；以扩散模型骨干作为初始化网络，构建合成噪声-图像对并用符号分类损失进行微调，从而在单步前向推理中直接预测水印符号而无需多步反演。

Result: OSI在速度上比多步扩散反演快约20倍，提取精度更高，水印有效载荷翻倍；在不同调度器、扩散骨干和密码方案上的广泛实验显示一致性的改进与良好泛化性。

Conclusion: 本文提出了一种名为One-step Inversion（OSI）的单步水印提取方法，显著提升了从扩散模型生成图像中提取Gaussian Shading风格水印的速度与准确性。OSI将水印提取问题重构为可学习的符号分类任务，通过从扩散模型骨干初始化并在合成噪声——图像对上以符号分类目标微调，能在一步内完成提取。实验证明OSI比多步扩散逆向方法快20倍、提取准确率更高且水印容量翻倍，且对不同调度器、扩散骨干和加密方案具有良好泛化性。

Abstract: Watermarking is an important mechanism for provenance and copyright protection of diffusion-generated images. Training-free methods, exemplified by Gaussian Shading, embed watermarks into the initial noise of diffusion models with negligible impact on the quality of generated images. However, extracting this type of watermark typically requires multi-step diffusion inversion to obtain precise initial noise, which is computationally expensive and time-consuming. To address this issue, we propose One-step Inversion (OSI), a significantly faster and more accurate method for extracting Gaussian Shading style watermarks. OSI reformulates watermark extraction as a learnable sign classification problem, which eliminates the need for precise regression of the initial noise. Then, we initialize the OSI model from the diffusion backbone and finetune it on synthesized noise-image pairs with a sign classification objective. In this manner, the OSI model is able to accomplish the watermark extraction efficiently in only one step. Our OSI substantially outperforms the multi-step diffusion inversion method: it is 20x faster, achieves higher extraction accuracy, and doubles the watermark payload capacity. Extensive experiments across diverse schedulers, diffusion backbones, and cryptographic schemes consistently show improvements, demonstrating the generality of our OSI framework.

</details>


### [32] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出ECL，通过表征与分类器对齐的几何平衡处理类不平衡问题，实验证明提升了长尾数据上的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督对比学习在长尾/不平衡数据上性能受限，原因在于未对齐类均值/原型与分类器且原型仅作为单样本导致批内贡献不均。

Method: ECL包含两部分：1) 表征几何平衡，促使类内特征塌缩、类均值均匀分布（正则单纯形），并平衡类均值特征与类原型的贡献；2) 分类器-类中心几何平衡，通过对齐分类器权重与类原型。

Result: 在CIFAR-10-LT、ImageNet-LT、ISIC 2019及新构建的LCCT数据集上的实验表明，ECL在不平衡分类任务上优于现有原型/监督对比学习的SOTA方法。

Conclusion: 本文提出ECL，在不平衡数据下通过特征、类均值与分类器权重三者几何对齐实现更好的泛化，实验显示优于现有SOTA监督对比学习方法。

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [33] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS利用扩散反向启动时刻和噪声注入的自适应策略，使预训练扩散先验主导恢复，从而在未知和严重退化下实现稳健的深度超分辨。


<details>
  <summary>Details</summary>
Motivation: 传统深度超分辨方法直接回归深度值，面对未知或严重的退化常产生伪影。作者观察到高斯平滑会在前向过程中缩小分布差异并趋向各向同性高斯先验，借此设计鲁棒的逆向恢复策略。

Method: 基于扩散模型理论，AdaDS估计细化不确定性以自适应决定反向扩散的起始时刻，并注入特定强度的噪声将中间样本放置到目标后验的高概率区域，从而让预训练扩散生成先验主导恢复。

Result: 在真实和合成基准测试上，AdaDS在零样本泛化和多种退化鲁棒性方面优于现有最先进方法。

Conclusion: AdaDS通过利用高斯平滑的收缩特性，在扩散模型的反向过程中自适应选择起始时间步并注入定制噪声，使生成先验主导恢复过程，从而在面对各种未知或严重退化时仍能稳健地恢复高分辨率深度图。

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [34] [Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems](https://arxiv.org/abs/2602.09515)
*Mas Nurul Achmadiah,Afaroj Ahamad,Chi-Chia Sun,Wen-Kai Kuo*

Main category: cs.CV

TL;DR: 在边缘设备上，使用帧差+轻量分类器的方法能显著提升IoT快速移动目标检测的准确率、能效和时延，MobileNet为最佳选择，端到端方法在高速目标场景表现不足。


<details>
  <summary>Details</summary>
Motivation: IoT应用对能效和实时性要求高，端到端深度检测方法在资源受限场景中代价大、延迟高，且对快速移动目标表现不佳，故引入帧差法以降低计算量并提高速度。

Method: 采用帧差法进行快速运动区域提取，随后在边缘设备上运行轻量分类器（包括MobileNet、YOLOX及其他模型）进行识别；在三种加速硬件（AMD Alveo M U50、Jetson Orin Nano、Hailo-8T M）上评估性能。

Result: 实验显示，与端到端方法相比，平均准确率提升约28.314%，能效提升约3.6倍，平均延迟降低约39.305%；MobileNet在帧差流程中表现出高准确率、低延迟、能效高，YOLOX准确率最低但延迟也最低；对快速移动的列车和飞机检测准确率较低。

Conclusion: 本文提出的基于帧差法配合轻量分类器的IoT快速移动目标检测方法，在能耗与延迟上明显优于端到端方法，适用于资源受限的边缘设备。

Abstract: This paper presents an Internet of Things (IoT) application that utilizes an AI classifier for fast-object detection using the frame difference method. This method, with its shorter duration, is the most efficient and suitable for fast-object detection in IoT systems, which require energy-efficient applications compared to end-to-end methods. We have implemented this technique on three edge devices: AMD AlveoT M U50, Jetson Orin Nano, and Hailo-8T M AI Accelerator, and four models with artificial neural networks and transformer models. We examined various classes, including birds, cars, trains, and airplanes. Using the frame difference method, the MobileNet model consistently has high accuracy, low latency, and is highly energy-efficient. YOLOX consistently shows the lowest accuracy, lowest latency, and lowest efficiency. The experimental results show that the proposed algorithm has improved the average accuracy gain by 28.314%, the average efficiency gain by 3.6 times, and the average latency reduction by 39.305% compared to the end-to-end method. Of all these classes, the faster objects are trains and airplanes. Experiments show that the accuracy percentage for trains and airplanes is lower than other categories. So, in tasks that require fast detection and accurate results, end-to-end methods can be a disaster because they cannot handle fast object detection. To improve computational efficiency, we designed our proposed method as a lightweight detection algorithm. It is well suited for applications in IoT systems, especially those that require fast-moving object detection and higher accuracy.

</details>


### [35] [A Universal Action Space for General Behavior Analysis](https://arxiv.org/abs/2602.09518)
*Hung-Shuo Chang,Yue-Cheng Yang,Yu-Hsi Chen,Wei-Hsin Chen,Chien-Yao Wang,James C. Liao,Chien-Chang Chen,Hen-Hsen Huang,Hong-Yuan Mark Liao*

Main category: cs.CV

TL;DR: 基于大量有人类动作数据构建“通用动作空间”，利用深度学习表征将其迁移用于哺乳动物与黑猩猩行为分析，代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统行为分析依赖手工特征和基于轨迹的稀疏特征点，鲁棒性和泛化能力差。ImageNet和深度学习的成功提示通过大规模、有标签的人类动作数据学习通用动作表征，可能提升跨物种行为理解与分类性能。

Method: 作者收集并整合多个已有的人类动作标注数据集以构建UAS，使用深度神经网络在这些数据上学习动作表征，并将训练得到的UAS模型迁移或应用到哺乳动物和黑猩猩行为数据集上进行分析与分类。代码已在GitHub开源。

Result: 论文声称通过构建的UAS能够有效用于哺乳动物和黑猩猩行为数据集的分析与分类，且作者已公开源码以便复现和扩展。具体的定量性能、基线比较和消融实验细节在摘要中未给出。

Conclusion: 本文提出构建一个基于现有有人类动作标注数据集的大规模“通用动作空间”（Universal Action Space, UAS），并将其用于分析和分类哺乳动物与黑猩猩行为。作者主张通过借助ImageNet引导下的深度学习表征，能将动作识别从传统低级特征方法转向高层语义表示，从而提高跨物种行为分析的泛化能力。

Abstract: Analyzing animal and human behavior has long been a challenging task in computer vision. Early approaches from the 1970s to the 1990s relied on hand-crafted edge detection, segmentation, and low-level features such as color, shape, and texture to locate objects and infer their identities-an inherently ill-posed problem. Behavior analysis in this era typically proceeded by tracking identified objects over time and modeling their trajectories using sparse feature points, which further limited robustness and generalization. A major shift occurred with the introduction of ImageNet by Deng and Li in 2010, which enabled large-scale visual recognition through deep neural networks and effectively served as a comprehensive visual dictionary. This development allowed object recognition to move beyond complex low-level processing toward learned high-level representations. In this work, we follow this paradigm to build a large-scale Universal Action Space (UAS) using existing labeled human-action datasets. We then use this UAS as the foundation for analyzing and categorizing mammalian and chimpanzee behavior datasets. The source code is released on GitHub at https://github.com/franktpmvu/Universal-Action-Space.

</details>


### [36] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: 提出一种基于视觉-文本相似性的训练无关注意力重加权和束搜索注入策略，有效提升任务相关视觉token注意力，显著降低LVLMs的幻觉而不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型存在视觉注意力不足导致的幻觉问题；以往增强注意力的方法会同时放大与任务无关的视觉token注意力，反而带来负面影响。因此希望有选择性地增强任务相关token的注意力以降低幻觉。

Method: 提取视觉-文本交叉注意力子矩阵以表示视觉-文本相关性，基于这些子矩阵构造重加权矩阵来重新分配注意力；在束搜索解码时注入视觉注意力值以优先选择具有更高视觉关注度的候选答案。该方法为训练无关（training-free）的干预策略。

Result: 通过在多种主流LVLMs上进行广泛实验，方法显著减少了幻觉现象，同时保持甚至不损失生成内容的准确性与连贯性。

Conclusion: 本文提出了一种无训练的注意力干预算法，通过利用视觉-文本相似性重加权注意力矩阵并在束搜索中注入视觉注意力值，从而提升任务相关视觉token的注意力，减少LVLMs的幻觉，同时保持生成内容的准确性与连贯性。

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [37] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: 论文提出利用多模型弱标注+一致性融合+专家注入合成百万级细胞图像描述，并对Qwen3-VL-4B多阶段微调，得到面向宫颈细胞学的高性能视觉-语言模型Singpath-VL。


<details>
  <summary>Details</summary>
Motivation: 目前MLLM在组织病理学进展显著，但在细胞病理学（尤其宫颈细胞学）应用不足，主要原因是缺乏大规模高质量标注数据；因此通过合成数据和专用微调填补该空白。

Method: 构建三阶段数据合成流水线：使用多个通用MLLM作为弱标注器、通过一致性融合和专家知识注入提高描述质量，生成高保真细胞形态描述；随后采用多阶段策略微调Qwen3-VL-4B，训练出专用的细胞学MLLM。

Result: Singpath-VL在细粒度形态感知和细胞级诊断分类任务上表现优越，并计划开源部分合成数据集和基准。

Conclusion: 该论文提出了Singpath-VL，一种针对宫颈细胞学的视觉-语言大模型，通过合成百万级图像-描述数据集并对Qwen3-VL-4B进行多阶段微调，提升细胞形态感知和细胞级诊断分类性能。

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [38] [HLGFA: High-Low Resolution Guided Feature Alignment for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.09524)
*Han Zhou,Yuxuan Gao,Yinchao Du,Xuezhe Zheng*

Main category: cs.CV

TL;DR: 提出HLGFA，通过高低分辨率特征对齐和噪声感知增强，实现了在MVTec AD上接近98%的像素和图像级AUROC的无监督工业缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统基于像素重建的方法在复杂纹理或无标签设置下易产生不可控的伪差，特征级对齐通过学习表示一致性可更稳定地建模正常性；利用高低分辨率互补信息能更好区分结构性与细节性异常。

Method: 采用共享冻结骨干网络分别对双分辨率输入提取多层特征；将高分辨率表示分解为结构先验和细节先验，利用条件调制与门控残差校正来引导并精炼低分辨率特征，从而实现跨分辨率对齐；引入噪声感知数据增强以抑制工业环境中无害干扰。

Result: 在MVTec AD上实现97.9%像素级AUROC和97.5%图像级AUROC，显著优于代表性重建和特征方法，且在标准基准上表现稳健。

Conclusion: HLGFA通过学习高低分辨率表示间的跨分辨率特征一致性，有效建模正常样本，从而在无监督工业缺陷检测中实现高精度异常定位，优于像素重建和传统特征方法。

Abstract: Unsupervised industrial anomaly detection (UAD) is essential for modern manufacturing inspection, where defect samples are scarce and reliable detection is required. In this paper, we propose HLGFA, a high-low resolution guided feature alignment framework that learns normality by modeling cross-resolution feature consistency between high-resolution and low-resolution representations of normal samples, instead of relying on pixel-level reconstruction. Dual-resolution inputs are processed by a shared frozen backbone to extract multi-level features, and high-resolution representations are decomposed into structure and detail priors to guide the refinement of low-resolution features through conditional modulation and gated residual correction. During inference, anomalies are naturally identified as regions where cross-resolution alignment breaks down. In addition, a noise-aware data augmentation strategy is introduced to suppress nuisance-induced responses commonly observed in industrial environments. Extensive experiments on standard benchmarks demonstrate the effectiveness of HLGFA, achieving 97.9% pixel-level AUROC and 97.5% image-level AUROC on the MVTec AD dataset, outperforming representative reconstruction-based and feature-based methods.

</details>


### [39] [SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 提出SchröMind：通过求解Schrödinger桥在token层面将幻觉激活映射为真实激活，从而显著减少多模态大模型的幻觉问题，轻量训练并保持原性能，在POPE和MME上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在理解图像方面表现良好，但在生成准确令牌序列时容易产生幻觉，尤其在医疗等高风险领域；需要一种方法在不破坏模型现有能力下校正生成过程。

Method: 将幻觉与真实激活视为两种概率分布，通过求解Schrödinger桥问题建立逐token映射，使用轻量级训练保持原模型能力并引入最小计算开销。

Result: 在POPE和MME基准测试上，SchröMind实现了最先进的性能，同时仅带来极小的计算开销。

Conclusion: SchröMind通过最小化从幻觉激活到真实激活的传输代价，有效减少MLLMs在生成文本时的幻觉现象。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [40] [SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection](https://arxiv.org/abs/2602.09529)
*Emad Gholibeigi,Abbas Koochari,Azadeh ZamaniFar*

Main category: cs.CV

TL;DR: SCA-Net 通过多尺度差异金字塔、形状感知与高分辨率增强、以及多级注意力与动态损失与分阶段训练，显著提升小目标与整体变化检测性能，同时大幅缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习变化检测对小目标敏感性低、计算开销大，需一种既高效又能捕捉多尺度与细节变化的模型以满足城市管理与灾害评估等实际应用需求。

Method: 提出 Difference Pyramid Block（多尺度差异提取）、Adaptive Multi-scale Processing（包含 Shape-aware 与 High-resolution Enhancement 块）与多级注意力模块（PPM 与 CSAGate），并采用动态复合损失函数及四阶段训练策略；在 LEVIR-CD 与 LEVIR-MCI 上进行综合评估与对比实验。

Result: 在 LEVIR-MCI 上 mIoU 提升 2.64%，小建筑 IoU 提升 57.9%，训练时间缩短 61%，并在 LEVIR-CD 与其他基线方法上表现优越。

Conclusion: SCA-Net 在 Change-Agent 基础上通过多尺度差异分析、形状感知与高分辨率增强、以及多层注意力机制，实现了对建筑与道路微小变化的高精度检测，且通过动态复合损失与四阶段训练显著提升收敛速度与稳定性。

Abstract: Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>


### [41] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: 提出通过视觉-语言驱动的失真先验与差分精炼、混合专家加权融合的BIQA框架DR.Experts，显著提升了对细微失真的感知和评分对齐。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA方法学习统一浅层特征与质量分之间的关系，难以捕捉细微失真线索，缺乏可靠的失真先验导致与主观感知不一致。

Method: 采用降解感知的视觉-语言模型提取失真特定先验，使用失真-显著性差分模块从语义注意力中区分并精炼先验，最后用动态失真加权模块（类专家混合）融合失真先验、语义与桥接表示并按感知影响加权输出质量评分。

Result: 在五个BIQA基准上进行广泛实验，DR.Experts在泛化性和数据效率方面均优于现有方法。

Conclusion: 本文提出DR.Experts，通过显式引入失真先验改善无参考图像质量评估，从而更好对齐人类主观判断。

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [42] [RAD: Retrieval-Augmented Monocular Metric Depth Estimation for Underrepresented Classes](https://arxiv.org/abs/2602.09532)
*Michael Baltaxe,Dan Levi,Sagie Benaim*

Main category: cs.CV

TL;DR: RAD通过不确定性感知检索+双流编码+匹配交叉注意力，将检索到的RGB-D上下文作为几何代理，显著提升欠代表类的单目深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 动机是单目深度估计在复杂场景中对欠代表类别（如稀有物体或细节结构）的估计不准确，且多视角几何信息难得；通过检索语义相似且带深度的样本作为几何代理，可以近似多视角立体的结构优势，提升欠代表类的精度。

Method: 方法包括：1) 不确定性感知的检索模块，定位输入图像中低置信度区域并检索语义相似的RGB-D样本；2) 双流网络分别编码输入与检索上下文；3) 匹配交叉注意力融合模块，通过可靠的点对应仅传递几何信息。

Result: 在NYU Depth v2、KITTI和Cityscapes上，RAD在欠代表类上显著优于最先进方法：NYU减少相对绝对误差29.2%，KITTI减少13.3%，Cityscapes减少7.2%，同时在常规域内基准上保持竞争力。

Conclusion: 该文提出RAD，一种检索增强的单目深度估计框架，通过引入检索到的RGB-D邻居作为几何代理改善欠代表类的深度预测，结论是RAD在多个数据集上显著提升了欠代表类的性能，同时维持常规基准表现。

Abstract: Monocular Metric Depth Estimation (MMDE) is essential for physically intelligent systems, yet accurate depth estimation for underrepresented classes in complex scenes remains a persistent challenge. To address this, we propose RAD, a retrieval-augmented framework that approximates the benefits of multi-view stereo by utilizing retrieved neighbors as structural geometric proxies. Our method first employs an uncertainty-aware retrieval mechanism to identify low-confidence regions in the input and retrieve RGB-D context samples containing semantically similar content. We then process both the input and retrieved context via a dual-stream network and fuse them using a matched cross-attention module, which transfers geometric information only at reliable point correspondences. Evaluations on NYU Depth v2, KITTI, and Cityscapes demonstrate that RAD significantly outperforms state-of-the-art baselines on underrepresented classes, reducing relative absolute error by 29.2% on NYU Depth v2, 13.3% on KITTI, and 7.2% on Cityscapes, while maintaining competitive performance on standard in-domain benchmarks.

</details>


### [43] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: AUHead通过ALMs生成AUs并用AU条件的可控扩散模型合成视频，实现了对细粒度情绪的可控、逼真说话人脸视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉细腻情绪表达方面不足，缺少对面部微表情（AUs）的精细控制，导致情感真实度和表达可控性受限。

Method: 第一阶段利用大规模音频-语言模型(ALMs)的链式思维（先预测情绪再预测AUs）和时空AU标记化，从原始语音中生成AU序列；第二阶段设计AU驱动的可控扩散模型，将AU序列映射为结构化2D面部表示并在cross-attention中建模AU-视觉交互，同时在推理时引入AU解耦引导以平衡AU质量与身份一致性。

Result: 在基准数据集上，AUHead在情感真实感、唇同步准确性和视觉一致性方面表现优异，显著超越现有技术。

Conclusion: 该论文提出了一种两阶段方法AUHead，通过将细粒度情绪控制（面部动作单元AUs）从音频中解耦并用于可控扩散生成，实现更真实的说话人脸视频合成。

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [44] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 提出Scalpel：在推理时基于GMM与熵正则化最优传输修正注意力激活，动态干预以抑制LVLM幻觉，轻量无训练开销，实验表现领先。


<details>
  <summary>Details</summary>
Motivation: LVLM因LLM的强先验和跨模态注意力错配，常生成与视觉内容不一致（幻觉）的输出，需在不增加计算或训练的前提下进行有效纠正。

Method: Scalpel使用高斯混合模型建模可信与幻觉注意力的多峰分布，并用熵正则化最优传输（Schrödinger桥）精确映射高斯分量，在推理阶段根据分量归属与映射关系动态调整注意力激活的强度与方向。

Result: 在多数据集和基准上，Scalpel显著降低幻觉并超越既有方法，达到或接近最新性能；并且无需额外训练、数据或多步解码，适用于多模型和多数据场景。

Conclusion: Scalpel通过在推理时预测并调整Transformer各头的可信注意力方向，有效降低了LVLM的视觉幻觉问题。

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [45] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: 将NTK与视觉-语言对齐结合，用正词锚定构建多模态亲和矩阵并通过正则化扩散自适应融合提示，显著提升谱聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现有谱聚类多为单模态，未利用视觉-语言预训练模型中的跨模态信息。借助VL预训练模型可在谱聚类中引入语义层面的约束以提升聚类质量。

Method: 提出Neural Tangent Kernel Spectral Clustering：通过将NTK锚定于与图像语义接近的积极名词（正词），将图像间相似度建模为视觉接近性与语义重叠的耦合；此外设计正则化的亲和力扩散机制，自适应集合由不同提示词生成的亲和矩阵。

Result: 在16个包含传统、大规模、细粒度和域移位的数据集上，方法稳定并显著超越最先进方法，提升幅度较大。

Conclusion: 本文提出将神经切线核（NTK）与视觉-语言预训练模型的跨模态对齐相结合，用于多模态谱聚类，从而增强簇内连通性并抑制簇间噪声连接，最终实现更明显的块对角结构。实验表明在16个基准上显著优于现有方法。

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [46] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: 作者构建了MieDB-100k，一个通过专家模型、规则合成与人工检验构建的大规模高质量医学图像编辑数据集，按三类编辑任务组织，能显著提升模型在医学图像编辑任务上的效果与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像编辑中高质量数据的匮乏限制了多模态生成模型在医疗场景中的适配与性能，现有数据集在多样性和临床语义覆盖上存在缺陷。

Method: 作者设计了一个数据整理管道，结合模态专用的专家模型与基于规则的数据合成方法生成候选样本，随后通过严格的人工审查确保临床一致性，并将编辑任务分为感知（Perception）、修改（Modification）和变换（Transformation）三类以综合评估理解与生成能力。

Result: 在大量实验中，使用MieDB-100k训练的模型在性能和泛化能力上优于公开和专有模型，展示出更强的编辑效果和临床保真度。

Conclusion: 该论文提出了一个名为MieDB-100k的大规模高质量多模态医学图像编辑数据集，旨在解决现有数据集在多样性、医学理解和质量与可扩展性之间的权衡上的不足。

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [47] [Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures](https://arxiv.org/abs/2602.09600)
*Yuxi Wang,Wenqi Ouyang,Tianyi Wei,Yi Dong,Zhiqi Shen,Xingang Pan*

Main category: cs.CV

TL;DR: Hand2World通过3D手网格的遮挡不变条件、每像素Plücker射线相机嵌入和扩散到因果生成的蒸馏，解决了单图像下自由空间手势驱动的第一人称长时交互视频生成的关键挑战，显著提升了视觉质量与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 目标是在增强现实与具身AI场景中，从单张图像和自由空间手势生成低延迟、几何一致且稳定的第一人称交互视频，解决训练数据以接触为主而预测时为自由空间手势的分布偏移、单目视图中手与相机运动的二义性以及任意时长生成需求。

Method: 方法包括：1) 基于投影的三维手网格构建遮挡不变的手部条件表示，使能从场景上下文推断可见性而非在控制信号中编码遮挡；2) 引入每像素Plücker射线嵌入显式相机几何以解耦手运动与相机运动，稳定视角变化并防止背景漂移；3) 开发全自动单目标注流水线并将双向扩散模型蒸馏为因果生成器，支持任意长度合成。整个系统为自回归生成流程，结合3D手部投影与光线几何实现交互一致性。

Result: 在三个第一人称交互基准上，Hand2World在感知质量和3D一致性上显著优于基线，同时支持相机控制和长时序交互生成。

Conclusion: 本文提出Hand2World，一个用于单张场景图像下从自由空间手势生成第一人称交互视频的自回归框架，能生成长时序、几何一致且对用户输入低延迟响应的逼真动态图像。

Abstract: Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation.

</details>


### [48] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: Tele-Omni 是一个将多模态指令解析与扩散视频生成解耦的统一框架，支持文本/图像/视频等指令，能在多种视频生成与编辑任务中提供灵活控制并保持高质量时序与画面一致性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散类视频生成方法多为特定任务设计、主要依赖文本指令，难以统一处理多模态输入、上下文参考及多样的视频生成与编辑场景；且许多视频编辑方法依赖为单一操作精心设计的流水线，限制了可扩展性与可组合性。

Method: 方法上，Tele-Omni 解耦了指令解析与视频合成：使用预训练的多模态大语言模型（MLLM）将异构输入解析成结构化生成/编辑意图；基于扩散模型的视频生成器以这些结构化信号为条件进行合成；引入任务感知的数据处理管线，把多模态输入统一为结构化指令格式并保留任务约束，从而实现跨任务联合训练与推理。

Result: 实验表明 Tele-Omni 在多个视频相关任务上取得了具有竞争力的性能，同时实现了灵活的多模态控制、较强的时间一致性和视觉一致性，支持多任务与多场景的视频生成与编辑。

Conclusion: Tele-Omni 提出了一种统一的多模态视频生成与编辑框架，能接受文本、图像和参考视频等多种指令输入，利用预训练多模态大模型解析指令并生成结构化意图，再由基于扩散的生成器进行高质量视频合成。通过任务感知的数据处理管线统一多样输入格式并保持任务特有约束，框架支持多任务联合训练，从而在文本到视频、图像到视频、首末帧生成、上下文生成与编辑等多种视频任务上表现出灵活控制与良好时序/视觉一致性。

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [49] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: 提出基于注意力引导的动态水印（AGMark），通过实时识别视觉关键证据并结合熵与权重密度自适应分配受保护词元，提升视觉语义保真且保持高检测率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉无关水印引入视觉无关词元，破坏视觉定位；现有视觉专用方法依赖一次性关键权重估计、忽视权重分布密度，无法适应生成过程中视觉依赖的动态变化，且可能引入低质量长尾词元。

Method: 基于解码每步的注意力权重动态识别与视觉相关的语义证据，并结合上下文一致性线索；再基于不确定性（token熵）与证据权重密度共同决定受保护词元比例，实现自适应词汇划分以避免无关词元。

Result: AGMark在生成质量和视觉语义保真性上显著优于传统方法，尤其在生成后期增益明显；检测准确率至少99.36% AUC，抗攻击能力至少88.61% AUC，且不牺牲推理效率。

Conclusion: AGMark在保证视觉保真度的同时，有效嵌入可检测信号，优于现有水印方法。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [50] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: LELA利用LLM与多模态captioning，无需训练即可实现细粒度仇恨视频定位，在两个基准上显著优于现有训练免费方法，并具备良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视频仇恨检测要么依赖大量人工标注的监督训练，要么缺乏时间维度的精细定位能力。为降低标注成本且提升可解释性，提出一种训练免费但能做细粒度时间定位的跨模态方案。

Method: 将视频分解为图像、语音、OCR、音乐与视频上下文五种模态，采用模态专属captioning生成文本描述；利用多阶段提示（prompting）借助大语言模型对每帧进行仇恨性评分，最后通过组合匹配机制融合多模态信息以提高定位精度。该流程无需训练端到端模型，属于推理时基于LLM的规则/提示驱动方法。

Result: 在HateMM与MultiHateClip两个基准上，LELA在训练免费方法中取得明显领先的性能。作者还提供了大量消融实验与定性可视化，论证了多模态划分、多阶段提示与组合匹配对性能的贡献。

Conclusion: 本文提出的LELA是一种无需训练、基于大语言模型的仇恨视频定位框架，通过多模态描述与多阶段提示策略实现对视频帧的细粒度仇恨评分，并引入组合匹配机制增强跨模态推理。实验证明在HateMM与MultiHateClip上，LELA显著优于现有训练免费基线，且通过消融与可视化展示了方法的有效性和可解释性。

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [51] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: 作者构建了视频驱动的3D可供性数据集VIDA，并提出VideoAfford，通过融合大语言模型、潜在动作编码器与空间感知损失，从视频中学习动态交互先验，实现更精确的3D可供性定位和强泛化。


<details>
  <summary>Details</summary>
Motivation: 克服仅依赖静态语言或图像的研究在动态交互提示（时序、因果）方面的局限，利用视频提供的动态信息来增强3D可供性定位。

Method: 构建大规模视频-点云数据集VIDA；提出VideoAfford框架，结合多模态大语言模型、潜在动作编码器和空间感知损失，用于从HOI视频中提取动态交互先验并进行细粒度3D可供性分割。

Result: 在VIDA上大量实验表明，VideoAfford在可供性定位任务上显著优于已有方法，并具有良好的开放世界泛化与可供性推理能力。

Conclusion: 该论文提出了基于视频的数据集和模型，显著提升了3D可供性定位的时序与空间理解能力。

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [52] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: Time2General通过时空记忆解码与掩码时序一致性损失，解决了域移与采样率变化导致的帧间闪烁问题，显著提升跨域视频语义分割的准确性与稳定性。


<details>
  <summary>Details</summary>
Motivation: 在只在单一带标签驾驶域上训练并直接在未见域上部署的场景中，域差异与视频帧采样变化破坏了基于对应的传播与固定步长的时序聚合，导致标注稳定区域出现严重闪烁，影响实用性。

Method: Time2General包含(1) Spatio-Temporal Memory Decoder：将多帧上下文聚合成时空记忆并基于稳定查询解码出逐帧一致的分割，无需显式对应传播；(2) Stability Queries：用于提取跨帧稳定特征以维持时序一致性；(3) Masked Temporal Consistency Loss：通过对不同时间步长的预测差异进行掩码正则化并随机化训练步长，提高对采样率变化的鲁棒性。

Result: 在多个驾驶基准上进行的大量实验表明，Time2General在跨域准确率和时序稳定性上相比先前的DGSS和VSS方法有显著提升，推理速度可达约18 FPS，并将在审稿结束后开源代码。

Conclusion: 提出的Time2General通过时空记忆解码器和稳定查询，有效提高了跨域视频语义分割的准确性与时序一致性，显著抑制了因域移与采样率变化导致的帧间闪烁，在多数据集上优于现有DGSS和VSS基线。

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [53] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: 提出TreeCUA，用树结构和多智能体协作高效扩展GUI自动化轨迹，并通过TreeCUA-DPO基于树信息提升GUI规划，实验与OOD验证效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有工作侧重GUI grounding而非更关键的GUI规划，且CUA的探索呈树状结构，早期功能入口更常被访问，故用树结构组织大规模轨迹能降低数据成本并简化GUI规划的数据扩展。

Method: 构建多智能体协作框架负责探索、动作验证、轨迹总结与质量评估；引入树状拓扑存储与回放重复节点；设计自适应探索算法平衡深度与广度；引入世界知识引导与全局记忆回溯避免低质量生成；基于树节点信息提出TreeCUA-DPO用于参考相邻分支提升规划。

Result: TreeCUA和TreeCUA-DPO在实验中带来显著改善，且OOD测试展示了强泛化性。数据与代码将在GitHub公开。

Conclusion: TreeCUA提出用树结构组织GUI探索轨迹，通过多智能体协作、动作验证、轨迹总结与质量评估生成高质量可扩展的GUI轨迹，并在此基础上用DPO增强规划能力，实验显示显著提升并具备良好泛化能力。

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [54] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: 论文提出半监督分割+patch分类的多任务框架，解决多模态MRI下肝脏分割与纤维化分期，且在挑战独立测试上验证，开源代码可复现。


<details>
  <summary>Details</summary>
Motivation: 临床上肝纤维化的准确分期依赖于精确的肝脏分割与对多模态MRI特征的鲁棒处理，但受限于标注数据稀缺和模态间域移，传统方法性能受限。

Method: 分两阶段：LiSeg阶段采用半监督学习结合分割与配准，利用有标注与无标注数据，缓解多通道多模态MRI的域移问题；LiFS阶段采用基于patch的分类方法，实现纤维化分期并可视化分类结果。模型输入为三通道(T1,T2,DWI)或七通道(T1,T2,DWI,GED1-GED4)MRI。代码开源于GitHub。

Result: 作者的方法在CARE Liver 2025 Track 4挑战的独立测试集上提交结果（包含ID与OOD），宣称能有效处理多模态输入、标注稀缺与域移问题；具体定量指标需查看挑战官方评测，代码已公开以便复现。

Conclusion: 该论文提出了一个用于肝脏分割和纤维化分期的多任务深度学习框架，能够有效利用多参数MRI、多模态数据及有限标注样本，改善域间差异的影响，并在挑战组织者提供的独立测试集（包含ID与OOD案例）上进行了验证。

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [55] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: 提出GenSeg-R1与GenSeg-R1-G：通过VLM生成结构化空间提示并由SAM 2分割，采用GRPO微调Qwen3-VL，显著提升指称图像分割性能，且支持无目标检测和直接优化掩码质量。


<details>
  <summary>Details</summary>
Motivation: 动机是将复杂的视觉语言推理与高质量分割解耦，通过让VLM专注于生成结构化空间提示而不是直接输出掩码／像素级预测，从而利用强大的Promptable分割器（SAM 2）的能力，提高指称分割的准确性和鲁棒性，并减少对昂贵的像素级标注或推理链标注的依赖。

Method: 方法包括：1) 使用VLM接收图像与自然语言查询，输出每个被指称实例的结构化空间提示（一个边界框和两个内部关键点）；2) 将这些提示输入冻结的Promptable分割器（SAM 2）以得到掩码；3) 在GenSeg-R1框架中对Qwen3-VL（4B/8B）进行微调，采用无监督推理链注释的Group Relative Policy Optimization（GRPO）；4) 提出GenSeg-R1-G变体，在训练时在回报中引入SAM 2以直接优化掩码质量并支持无目标（no-target）检测。

Result: 主要结果包括：在RefCOCOg验证集上，GenSeg-R1-8B达成0.7127 cIoU和0.7382 mIoU，分别较Qwen3-VL Instruct基线提升15.3和21.9点，并较Seg-Zero-7B提升3.3 cIoU；在GRefCOCO验证集，GenSeg-R1-G实现76.69%目标mIoU和82.40%对无目标提示的准确检测；在ReasonSeg测试集，GenSeg-R1-4B达68.40% mIoU，较Seg-Zero-7B和Seg-R1-7B分别提升7.0和10.7点。

Conclusion: 该论文提出了一个解耦的“先推理后分割”流水线，将视觉-语言模型（VLM）用于结构化空间提示生成，并由冻结的可提示分割器（SAM 2）生成高质量掩码，从而在细粒度指称图像分割任务上取得显著性能提升。

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [56] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Stroke3D 用 Sk-VAE+Sk-DiT 在文本和 2D 笔画条件下先生成骨骼，再用 TextuRig 数据增强与 SKA-DPO 优化生成高质量可绑定纹理网格，实现从草图和文本到动画就绪 3D 资产的一体化流程。


<details>
  <summary>Details</summary>
Motivation: 现有 3D 生成方法难以输出可动画网格，传统绑定方法缺乏细粒度骨骼结构控制；用户希望用简单的 2D 草图和文本快速生成可直接动画的 3D 资产。

Method: 两阶段流水线：1) 可控骨骼生成：构建 Skeletal Graph VAE (Sk-VAE) 编码骨骼图结构并在潜在空间中由 Skeletal Graph DiT (Sk-DiT) 生成骨骼嵌入，条件化于文本与二维笔画，最终由 VAE 解码重建高质量 3D 骨骼；2) 增强网格合成：通过 TextuRig（从 Objaverse-XL 筛选的带纹理与绑定的网格及文本描述）扩充骨骼到网格的训练数据，并采用基于骨骼-网格对齐评分的偏好优化策略 SKA-DPO 提升几何一致性与质量。

Result: Stroke3D 能根据用户草图与文本生成合理的骨骼并合成高质量带纹理的可绑定网格，是首个支持以 2D 草图为条件生成带骨骼 3D 网格的工作；大量实验显示其在骨骼合理性与网格质量方面表现良好。

Conclusion: Stroke3D 提出了一种从用户输入的二维草图和文本直接生成可绑定骨骼的 3D 网格的新框架，解决了生成可动画几何体与精细骨骼控制的挑战。

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [57] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir,Tawsif Tashwar Dipto,Mehedi Ahamed,Sabbir Ahmed,Md Hasanul Kabir*

Main category: cs.CV

TL;DR: 系统性基准显示，经过替代梯度训练的轻量级SNN（尤其是SqueezeNet变体）在边缘场景中可实现接近CNN的精度同时大幅降低能耗；结构化剪枝进一步改善了效率与性能平衡。


<details>
  <summary>Details</summary>
Motivation: 现有工作多集中于大规模SNN设计，缺乏对轻量级CNN到SNN转换管线的系统性研究，而边缘设备对低功耗和紧凑模型有强烈需求，因此需要评估轻量级SNN在准确率与能效间的权衡。

Method: 在统一设置下，将ShuffleNet、SqueezeNet、MnasNet、MixNet等轻量级CNN转换为以LIF神经元为激活并用替代梯度下降训练的SNN；评估指标包括准确率、F1、参数量、计算复杂度和能耗；对表现最优的SqueezeNet进行结构化剪枝，去除冗余模块，得到SNN-SqueezeNet-P。

Result: SNN能比CNN实现最多15.7倍的能效提升；SqueezeNet的SNN变体表现最好；通过结构化剪枝得到的SNN-SqueezeNet-P在CIFAR-10上提高6%准确率并减少19%参数，相较CNN-SqueezeNet仅低1%准确率但能耗降低88.1%。

Conclusion: 轻量级SNN通过从紧凑CNN架构转换并使用LIF神经元与替代梯度训练，能在保持竞争性精度的同时显著降低能耗，成为边缘部署的实用低功耗替代方案。

Abstract: Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

</details>


### [58] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: 本文将裂纹检测建模为带深度生成先验与Mumford--Shah变分能量的反问题，通过图像分解实现像素级裂纹检测，兼顾艺术品纹理建模与结构化裂纹表示。


<details>
  <summary>Details</summary>
Motivation: 自动检测数字化油画中的裂纹对评估劣化和指导修复至关重要，但由于场景复杂且裂纹与刷痕、头发等相似，检测具有挑战性，因此需要结合先验知识的稳健方法。

Method: 采用混合方法：使用深度生成模型（作为强先验）来建模无裂纹画作；使用Mumford--Shah型变分能量和裂纹先验来表示裂缝结构；通过联合优化反问题以分解图像并生成裂纹像素级图谱。

Result: 联合优化方法能够在像素级上定位裂纹，利用深度生成先验提升对艺术纹理的建模，并通过变分方法捕捉裂缝结构，从而提高检测的准确性与鲁棒性。

Conclusion: 提出了将裂纹检测视为反问题，通过分解观测图像为无裂纹画作和裂纹分量，结合深度生成模型作为艺术品先验与Mumford--Shah型变分泛函及裂纹先验进行联合优化，从而得到像素级裂纹定位图。

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [59] [Toward Fine-Grained Facial Control in 3D Talking Head Generation](https://arxiv.org/abs/2602.09736)
*Shaoyang Xie,Xiaofeng Cong,Baosheng Yu,Zhipeng Gui,Jie Gui,Yuan Yan Tang,James Tin-Yau Kwok*

Main category: cs.CV

TL;DR: FG-3DGS通过频率感知的面部区域解耦与高频后处理对齐，结合3D高斯点渲染，实现了更精准的唇同步和更稳定的细粒度面部动作，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D Gaussian Splatting在实时高保真渲染上表现优异，但在细粒度面部运动控制（如精准唇动与面部抖动）方面存在不足，容易产生令人不适的“uncanny valley”问题，需要提高时间一致性与局部动作精确度。

Method: 使用频率感知解耦：低频区域（面颊、鼻、额头）由标准MLP联合建模，高频区域（眼睛、嘴）由专门网络并由面部区域掩码指导建模；网络输出为高斯位移（Gaussian deltas），作用于静态高斯生成帧特定的头部表现；渲染采用基于帧的相机参数的光栅化器；加入基于大规模音视频预训练模型的高频后渲染精细对齐模块以提升唇同步。

Result: 在常用的说话头生成数据集上，FG-3DGS在主观与客观指标（如唇同步精度、视觉保真度、时间一致性）上均优于近期最先进方法。

Conclusion: 该论文提出了FG-3DGS，通过频率感知的面部区域解耦和高频后处理对齐，解决了3D高斯点渲染在细粒度面部动作控制上的不足，在主流数据集上实现了更准确的唇动同步和更高保真度，优于现有方法。

Abstract: Audio-driven talking head generation is a core component of digital avatars, and 3D Gaussian Splatting has shown strong performance in real-time rendering of high-fidelity talking heads. However, achieving precise control over fine-grained facial movements remains a significant challenge, particularly due to lip-synchronization inaccuracies and facial jitter, both of which can contribute to the uncanny valley effect. To address these challenges, we propose Fine-Grained 3D Gaussian Splatting (FG-3DGS), a novel framework that enables temporally consistent and high-fidelity talking head generation. Our method introduces a frequency-aware disentanglement strategy to explicitly model facial regions based on their motion characteristics. Low-frequency regions, such as the cheeks, nose, and forehead, are jointly modeled using a standard MLP, while high-frequency regions, including the eyes and mouth, are captured separately using a dedicated network guided by facial area masks. The predicted motion dynamics, represented as Gaussian deltas, are applied to the static Gaussians to generate the final head frames, which are rendered via a rasterizer using frame-specific camera parameters. Additionally, a high-frequency-refined post-rendering alignment mechanism, learned from large-scale audio-video pairs by a pretrained model, is incorporated to enhance per-frame generation and achieve more accurate lip synchronization. Extensive experiments on widely used datasets for talking head generation demonstrate that our method outperforms recent state-of-the-art approaches in producing high-fidelity, lip-synced talking head videos.

</details>


### [60] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 本文提出CAV视觉系统参考架构，识别并评估针对各攻击面的攻击向量及其对CIA的影响，给出提高鲁棒性与安全性的对策建议。


<details>
  <summary>Details</summary>
Motivation: 随着Level-5自动驾驶的目标，CAV对视觉感知的依赖日益增加；视觉系统的漏洞可能导致严重安全事故，因此需要系统性识别攻击面与攻击向量，为防护设计提供理论基础。

Method: 通过分析关键传感器（摄像头、激光雷达、雷达）与视觉组件（检测、分割、跟踪、感知融合）构建参考架构，识别每层攻击面并分类攻击向量（物理遮挡/欺骗、数据注入、模型中毒、传输劫持、侧信道与供应链攻击），对每种向量评估对CIA的影响并提出缓解措施。

Result: 构建了CAVVS参考架构并映射出主要攻击面，列举并评估了多类攻击向量对CIA的影响，提出了包括多模态传感器融合、鲁棒学习、认证与加密、异常检测与可追溯性在内的防护建议和未来研究方向。

Conclusion: 本文得出结论：CAV视觉系统（CAVVS）在对抗性攻击和传感器/软件故障下存在显著脆弱性，需通过分层防护、传感器融合、鲁棒模型和实时完整性监测来提升保密性、完整性与可用性（CIA）。

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [61] [Self-Supervised Learning as Discrete Communication](https://arxiv.org/abs/2602.09764)
*Kawtar Zaher,Ilyass Moummad,Olivier Buisson,Alexis Joly*

Main category: cs.CV

TL;DR: 把视觉SSL当作二进制通信，学生预测教师的多标签二进制消息并用编码率正则化与投影头重置促进结构化离散表征，实验显示性能与可解释性均提升。


<details>
  <summary>Details</summary>
Motivation: 传统SSL通过对齐连续视图学习表征，但缺乏对信息在表征维度上结构化的控制；将通信容量限制为二进制固定通道可以强制学习更稠密、可解释的语义码。

Method: 构建教师-学生框架，教师通过投影头生成多标签二进制消息，学生用元素级二分类交叉熵对齐预测；加入编码率正则化（coding-rate regularization）约束信道利用率，并周期性重置投影头以促使嵌入对多重离散编码保持可预测。

Result: 在图像分类、检索和密集视觉预测任务上，相较连续对齐基线表现稳步提升；在领域迁移/自监督适配下也更鲁棒。对学到的二进制码分析表明其形成紧凑且信息丰富的离散语言，能捕捉可跨类复用的语义因子。

Conclusion: 该论文提出将视觉自监督学习重构为教师-学生间通过固定容量二进制信道进行离散通信，学生预测教师生成的多标签二进制消息，从而引导得到结构化、语义可解释的离散表征。

Abstract: Most self-supervised learning (SSL) methods learn continuous visual representations by aligning different views of the same input, offering limited control over how information is structured across representation dimensions. In this work, we frame visual self-supervised learning as a discrete communication process between a teacher and a student network, where semantic information is transmitted through a fixed-capacity binary channel. Rather than aligning continuous features, the student predicts multi-label binary messages produced by the teacher. Discrete agreement is enforced through an element-wise binary cross-entropy objective, while a coding-rate regularization term encourages effective utilization of the constrained channel, promoting structured representations. We further show that periodically reinitializing the projection head strengthens this effect by encouraging embeddings that remain predictive across multiple discrete encodings. Extensive experiments demonstrate consistent improvements over continuous agreement baselines on image classification, retrieval, and dense visual prediction tasks, as well as under domain shift through self-supervised adaptation. Beyond backbone representations, we analyze the learned binary codes and show that they form a compact and informative discrete language, capturing semantic factors reusable across classes.

</details>


### [62] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: 多模态训练数据严重偏向富裕国家（美英加），低收入地区被低估，且这影响生成模型的地理覆盖与多样性。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型生成的图像在地理代表性上存在不足，亟需理解训练样本来源的地理分布以评估偏差并推动更公平的模型设计。

Method: 使用LLM从图文配对的英文和多语言标题中提取位置信息，将大规模多模态数据集（Re-LAION、DataComp1B、Conceptual Captions）的样本映射到国家并基于20类常见实体进行统计分析；还分析了非英语子集和使用Stable Diffusion v1.3生成的国家特定图像与真实图像的覆盖差异。

Result: 美、英、加占48.0%的样本；南美和非洲仅占1.8%和3.8%；国家GDP与数据代表性高度相关（ρ=0.82）；非英语子集倾向于语言主要使用国；更高表示性并不保证更大视觉/语义多样性；生成模型覆盖度远低于真实图像。

Conclusion: 训练数据在地理分布上严重不均衡，主要集中在美英加等富裕国家，南美和非洲被严重低估，且高代表性不等于更高的视觉或语义多样性。

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [63] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出SciFlow-Bench：从像素输出评估科研图示生成的结构正确性，通过把生成图像逆解析为结构并进行比较，发现当前模型在复杂拓扑下结构恢复困难。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型生成的科研图示常在视觉上合理但结构上错误，且现有基准要么侧重图像或主观指标、要么评估中间符号表示，缺乏针对最终像素图的结构化评估。

Method: 从真实科研PDF中构建数据集，每个框架图配备规范化的真值图结构；以黑箱方式生成图片并通过闭环回路协议（将生成图像逆解析为结构图）进行比较；使用层次化多代理系统来协调规划、感知与结构推理以实现逆解析。

Result: 实验表明保持结构正确性仍具挑战，尤其是拓扑复杂的图示更易出错，强调需要结构感知的评估方法。

Conclusion: 本文提出SciFlow-Bench，针对科研图示生成的结构优先评估基准，强调通过结构可恢复性而非仅视觉相似性来评估模型。

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [64] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: CompSplat是一种压缩感知的NVS训练框架，通过帧加权与自适应裁剪提高在长序列和重压缩视频上的几何一致性与渲染/位姿性能。


<details>
  <summary>Details</summary>
Motivation: 现实视频存在长序列、非规则相机轨迹、未知位姿与有损压缩，这些因素导致位姿漂移、特征错位和几何失真，现有方法往往只关注长序列或无位姿场景，缺乏对多样压缩模式下的统一处理。

Method: 提出Compression-aware训练框架，包括压缩感知的帧权重分配和自适应裁剪（pruning）策略，针对每帧估计压缩程度并据此调整训练损失与数据使用，从而抑制受损帧对整体几何重建的负面影响。

Result: 在多个具有挑战性的基准集（包括Tanks and Temples、Free、Hike）上，CompSplat在严重压缩条件下显著优于近期大多数NVS方法，在渲染质量和位姿精度上达到了SOTA。

Conclusion: CompSplat通过显式建模逐帧压缩特性，有效缓解了长序列视频中因摄像机轨迹不规则和位姿未知引起的帧间不一致与几何累积误差，尤其在重压缩场景下显著提高渲染质量与位姿精度。

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [65] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: TL;DR：论文发现内部知识不稳定导致LVLM幻觉，提出无需训练的层级知识稳定性评分并用于解码（SAKED），能在多场景下显著降低幻觉。


<details>
  <summary>Details</summary>
Motivation: 动机：观察到人类在不确定或犹豫时更易出错，作者猜测模型内部知识的不稳定性（如注意力头激活、层间表示及令牌解码时的聚焦）会增加LVLM的幻觉风险，因而进行深入分析以找到缓解策略。

Method: 方法：作者通过对注意力头、模型层与解码令牌三个维度的广泛实证分析识别出三类幻觉模式（视觉激活漂移、跨层知识波动、邻近输出令牌间的视觉聚焦分散）。基于此，提出SAKED，在解码时计算层级知识稳定性评分（KSS），并通过对比最稳定与最不稳定层来抑制噪声、动态选择可靠知识；方法为训练无关、可插拔。

Result: 结果：实验表明SAKED在多种模型、任务与数据集上的幻觉缓解表现优于现有方法，证实利用层级知识稳定性进行动态解码可有效降低幻觉发生率，同时无需额外训练或结构改动。

Conclusion: 论文结论：模型内部知识不稳定是导致大型视觉-语言模型（LVLMs）产生幻觉的关键原因。通过引入层级知识稳定性评分（KSS）并在解码阶段对稳定与不稳定层进行对比，提出的SAKED方法能够抑制解码噪声并动态利用最可靠的内部知识，从而显著减少幻觉产生。该方法无需训练且易于集成，在多模型、多任务与多基准测试上均达到最先进的幻觉缓解效果。

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [66] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: ARK是一个面向知识域与推理技能的多模态检索基准，使用困难负样本评估23种检索器，发现细粒度视觉与空间推理为主要瓶颈，重排/重写能改善但仍有较大提升余地。


<details>
  <summary>Details</summary>
Motivation: 现有多模态检索基准侧重日常图像的语义匹配，缺乏对专业知识与复杂推理能力的诊断，难以衡量模型在现实专业场景（如医学、工程等）和需要多步推理的任务中的表现。

Method: 构建包含五大知识域（17个子类）与六类推理技能的基准ARK，覆盖16种异构视觉数据类型；设计包含单模与多模查询与候选项的检索任务；为避免捷径匹配，大量查询配备针对性的困难负样本，要求多步推理；在基准上评测23种代表性文本与多模检索器，并测试重排与重写等改进策略。

Result: 评测显示知识密集型与推理密集型检索存在明显差距；细粒度视觉和空间推理是持续挑战；简单方法如重排和重写能带来一致但有限的提升，整体仍有较大提升空间。

Conclusion: ARK补全了现有多模态检索基准在专业知识与复杂推理诊断上的空白，展示出检索模型在知识密集型与推理密集型任务之间存在显著差距，特别是细粒度视觉与空间推理是主要瓶颈。

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [67] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: Kelix 是一个采用高容量视觉离散化的全离散自回归多模态模型，解决了离散视觉令牌信息瓶颈，在理解能力上接近连续特征的 VLM。


<details>
  <summary>Details</summary>
Motivation: 现有自回归多模态模型仍依赖文本离散+视觉连续的混合接口，导致模型偏向理解且无法充分利用非文本自监督数据；而已有离散视觉令牌容量有限，信息丢失严重，理解能力落后于连续特征模型。

Method: 作者设计并训练了一种高容量的视觉离散化器（可能包括改进的向量量化或层次编码机制），并将其与自回归语言模型整合，进行联合自监督的下一标记预测训练以实现统一的生成与理解。

Result: Kelix 在多模态理解任务上显著缩小了与连续特征 VLMs 的差距，证明高容量离散视觉表示可以在自回归框架下同时支持强理解与生成能力。

Conclusion: Kelix 提出了一种完全离散的自回归多模态模型，通过改进视觉离散化方法，在理解能力上接近或达到连续特征的 VLM。

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [68] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: Reason-IAD通过知识检索和熵驱动的潜在迭代推理，结合动态视觉注入，提升工业异常检测的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有在通用域预训练的多模态大模型难以捕捉工业领域的类别特定异常细粒度模式，导致检测准确性和可解释性不足。

Method: 引入检索增强知识模块将类别文本描述整合到输入中；设计可优化的潜在think令牌，在紧凑潜在空间中进行迭代推理；基于熵的奖励函数鼓励置信且稳定的预测；动态视觉注入策略选择性将信息量最大的图像patch加入潜在序列以聚焦关键区域。

Result: 大量实验显示Reason-IAD在工业异常检测上持续超越最先进方法，具体指标未在摘要给出，但作者声称显著提升并将公开代码。

Conclusion: Reason-IAD通过结合类别特定知识检索与基于熵的潜在推理机制，改善了MLLMs在工业缺陷检测中的准确性与可解释性，从而实现更稳健的异常检测表现。

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [69] [Code2World: A GUI World Model via Renderable Code Generation](https://arxiv.org/abs/2602.09856)
*Yuhao Zheng,Li'an Zhong,Yi Wang,Rui Dai,Kaikui Liu,Xiangxiang Chu,Linyuan Lv,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

TL;DR: 通过生成可渲染代码并结合视觉反馈的RL训练，Code2World在UI预测和导航任务上显著优于现有方法，并发布了80K+高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 现有文本或像素方法难以同时兼顾高视觉保真与细粒度结构可控，因而提出通过代码生成来模拟界面变化。

Method: 构建AndroidCode数据集（将GUI轨迹翻译为高保真HTML并用视觉反馈修正），对VLM先进行SFT格式对齐，再用Render-Aware RL基于渲染结果作为奖励强化视觉语义保真与动作一致性。

Result: 在多个实验中，Code2World-8B在下一步UI预测上表现最佳，可与GPT-5和Gemini-3-Pro-Image媲美；在下游AndroidWorld导航任务上，能灵活提升成功率，例如提升Gemini-2.5-Flash+9.5%。数据集包含80K+高质量屏幕-动作对。

Conclusion: 本文提出Code2World，通过生成可渲染代码来预测下一步GUI状态，在视觉保真与结构可控间取得平衡，提升了界面预测与导航性能。

Abstract: Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World.

</details>


### [70] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: 提出 Free-GVC：一种无训练的生成式视频压缩方法，将视频编码为由扩散先验引导的潜在轨迹并在 GOP 级别逐步压缩；包含自适应质量控制和组间对齐模块以提升感知质量和时间一致性，在超低码率下显著优于 DCVC-RT（DISTS BD-Rate 降幅 93.29%）。


<details>
  <summary>Details</summary>
Motivation: 现有生成式视频压缩方法在极低码率下未能充分利用时间相关性，导致明显的闪烁和时间一致性差。作者希望在无需重新训练模型的条件下，利用视频扩散先验更好地压缩潜在轨迹，并通过在线自适应控制和组间对齐缓解闪烁问题，从而在超低码率下实现更好的主观视觉效果。

Method: Free-GVC 在 GOP 级别将视频分段并映射到潜在空间，沿着扩散模型的反向扩散（生成）轨迹对潜在表示进行逐步压缩；引入自适应质量控制模块在线估计率-感知替代模型以确定每个 GOP 的最佳扩散步骤数；组间对齐模块通过帧重叠和潜在融合在相邻 GOP 之间建立对齐，减少闪烁并提升连贯性。整个框架无需针对视频数据进行额外训练，依赖预训练的视频扩散先验进行感知导向的重建。

Result: Free-GVC 相较于最新神经编解码器 DCVC-RT 在 DISTS 指标上平均实现 93.29% 的 BD-Rate 降幅；另外作者通过用户研究证明在超低码率下具有更优的主观质量和时间一致性。

Conclusion: Free-GVC 提出了一种无训炼（training-free）的生成式视频压缩框架，将视频编码重新表述为由视频扩散先验引导的潜在轨迹压缩。该方法在 GOP 级别上工作，通过对视频段在潜在空间中的压缩并沿扩散轨迹逐步压缩来实现压缩性能。作者引入了自适应质量控制模块来动态构建在线率-感知替代模型，以预测每个 GOP 的最优扩散步数；并提出了组间对齐模块，通过帧重叠和潜在融合减少闪烁、提高时间一致性。实验证明在 DISTS 指标上相较 DCVC-RT 平均可实现 93.29% 的 BD-Rate 降幅，并通过用户研究验证在超低码率下的主观质量和时间一致性提升。

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [71] [BabyMamba-HAR: Lightweight Selective State Space Models for Efficient Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.09872)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 提出两种轻量SSM架构用于TinyML HAR，通过通道独立或早期融合stem、权重绑定双向扫描和门控时间注意力，达成高精度同时大幅降低MACs与参数量，在多数据集上验证并给出设计原则。


<details>
  <summary>Details</summary>
Motivation: 在TinyML设备上进行HAR面临内存与计算约束，同时需兼顾对不同传感器配置的鲁棒性。传统注意力机制复杂度高（平方级），而选择态空间模型能提供线性时间序列处理，适合TinyML但其设计空间尚未被充分探索。

Method: 提出名为BabyMamba-HAR的框架，包括（1）CI-BabyMamba-HAR：通道独立stem，使用共享权重但独立变换以抑制通道间噪声传播；（2）Crossover-BiDir-BabyMamba-HAR：早期融合stem，实现与通道数无关的计算复杂度。两者均采用权重绑定的双向扫描和轻量级门控时间注意力池化。使用轻量化设计以减少参数和MACs。

Result: 在八个基准数据集上评估，Crossover-BiDir-BabyMamba-HAR取得平均宏F1 86.52%，约27K参数、2.21M MACs，性能与TinyHAR（86.16%）相当，但在高通道数据集上计算量减少11倍。消融实验显示：双向扫描最多带来8.42% F1提升，门控时间注意力相较均值池化最多带来8.94%提升。

Conclusion: 本文提出了两种面向资源受限人体活动识别（HAR）的轻量级选择态空间模型（SSM）架构，并证明了其在多数据集上的有效性与高效性。

Abstract: Human activity recognition (HAR) on wearable and mobile devices is constrained by memory footprint and computational budget, yet competitive accuracy must be maintained across heterogeneous sensor configurations. Selective state space models (SSMs) offer linear time sequence processing with input dependent gating, presenting a compelling alternative to quadratic complexity attention mechanisms. However, the design space for deploying SSMs in the TinyML regime remains largely unexplored. In this paper, BabyMamba-HAR is introduced, a framework comprising two novel lightweight Mamba inspired architectures optimized for resource constrained HAR: (1) CI-BabyMamba-HAR, using a channel independent stem that processes each sensor channel through shared weight, but instance independent transformations to prevent cross channel noise propagation, and (2) Crossover-BiDir-BabyMamba-HAR, using an early fusion stem that achieves channel count independent computational complexity. Both variants incorporate weight tied bidirectional scanning and lightweight temporal attention pooling. Through evaluation across eight diverse benchmarks, it is demonstrated that Crossover-BiDir-BabyMamba-HAR achieves 86.52% average macro F1-score with approximately 27K parameters and 2.21M MACs, matching TinyHAR (86.16%) while requiring 11x fewer MACs on high channel datasets. Systematic ablation studies reveal that bidirectional scanning contributes up to 8.42% F1-score improvement, and gated temporal attention provides up to 8.94% F1-score gain over mean pooling. These findings establish practical design principles for deploying selective state space models as efficient TinyML backbones for HAR.

</details>


### [72] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了一种从单视角RGBD出发、通过跨视角与跨模态融合生成几何一致的任意视角RGBD并组装为4D世界模型的方法；结合测试时动作优化与残差逆动力学将生成的未来转化为可执行动作，在多个数据集上提升了场景预测与机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作要么只做基于图像的预测，要么只在部分3D几何上推理，无法预测完整的4D场景动态，且将预测转为动作的逆动力学存在多解性问题。

Method: 设计了跨视角与跨模态的特征融合模块以强制RGB与深度的一致性和几何对齐；采用想象-投影-融合流程，从单视角RGBD生成多视角预测并反投影组装成完整的随时间变化的3D结构；为动作生成引入了测试时动作优化（通过生成模型反向传播以在潜在空间中搜索最佳轨迹）和残差逆动力学模型把轨迹先验转化为可执行动作。

Result: 在三个数据集上，提出的方法在4D场景生成和下游操控任务上均表现优秀；消融研究验证了关键设计（如跨模态融合、测试时动作优化与残差逆动力学）的有效性。

Conclusion: 该论文提出了一种新型的具身4D世界模型，实现了几何一致的任意视角RGBD生成，并将生成结果用于机器人操作，提升了4D场景生成与下游操作性能。

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [73] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: AdaTSQ针对DiTs的时间动态特性，提出时间步动态比特分配与Fisher引导的时间校准，有效提升了PTQ后的生成质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法直接应用于DiTs效果不佳，原因在于这些方法忽视了扩散过程中固有的时间动态特性。为提高量化后模型在生成任务中的质量并减小计算/内存开销，提出针对时间敏感性的定制化PTQ方法。

Method: 提出两大关键技术：1）基于Pareto感知的时间步长动态比特宽度分配，将量化策略搜索建模为受约束的路径搜索问题，并使用束搜索（beam search）结合端到端重建误差在不同时间步为不同层动态分配比特位；2）基于Fisher信息的时间校准机制，利用时间Fisher信息优先选择高度敏感时间步的校准数据，并与基于Hessian的权重优化无缝结合。

Result: 在四种先进DiTs（例如Flux-Dev, Flux-Schnell, Z-Image, Wan2.1）上的大量实验表明，AdaTSQ在量化后生成质量与效率（例如重建误差、FID等指标）方面显著优于SVDQuant和ViDiT-Q等最先进方法。代码将在GitHub开源。

Conclusion: AdaTSQ通过利用扩散Transformer（DiTs）的时间敏感性，在后训练量化（PTQ）上实现了效率与生成质量的更优权衡，从而改善了在边缘设备上的部署表现。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [74] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: SARS是一个模块化的形状与外观感知3D重建流水线，从单图提取面部与身体语义特征并扩展3DMM以建模年龄、性别与细节，提升面部语义保真度与可控性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人像重建方法多集中于全局结构/几何，忽视年龄、性别、皱纹等语义面部特征，导致重建结果在表征高层特征时不足。为实现可控、多样化且语义一致的重建，需要在3DMM框架中显式引入这些高层语义信息。

Method: 提出模块化流水线：从单幅图像提取人脸与身体语义信息（身份、表情、年龄、性别、关键点等），基于3DMM融合形状与外观参数，使用身份与表情blendshapes与基础面网格结合，可能引入面部语义约束与自适应参数调优实现对高层特征的建模与控制。

Result: 提出的SARS系统能够从单张图像重建更具语义一致性的全身3D模型，较传统仅基于全局几何的方法在面部细节与语义属性（如年龄、性别、表情）保真度上有所提升；模块化设计支持扩展与参数可控，但具体量化性能、基准对比与消融研究需在完整论文中给出。

Conclusion: 本文提出了一种形状与外观感知的3D重建系统SARS，旨在从单幅图像重建全身3D模型，并同时考虑高层语义特征如年龄、性别与面部细节，弥补以往仅关注全局几何的不足。

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [75] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jürgen Weitz,Marius Distler,Sören Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: 作者构建并公开了一个包含1270段立体腹腔镜训练视频的带注释数据集（LASANA），每段包含聚合技能评分与错误二分类标签，并提供预定义划分与深度学习基线，为视频化手术技能评估研究提供基准。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的手术技能自动评估受限于标注视频数据集规模与多样性不足，因此需要一个更大且具有真实训练场景多样性的公开数据集以促进模型开发、比较和标准化评估。

Method: 收集并整理1270段立体（stereo）腹腔镜训练任务视频，涵盖四种基础训练任务；每段视频由三位独立评估者给出结构化技能评分并进行聚合，同时标注任务相关二分类错误标签；提供预定义的数据划分并使用深度学习模型给出基线实验结果。

Result: 发布了LASANA数据集（1270段立体视频，四种任务，聚合技能评分和错误标签），提供任务专用数据划分，并给出一个深度学习基线结果，展示数据集对自动评估与错误识别任务的可用性。

Conclusion: 该论文提出了一个大规模的腹腔镜技能评估数据集（LASANA），填补了现有带注释视频数据集规模小、样本不足的问题。数据集包含丰富的技能评分与错误标签，适用于视频化技能评估与错误识别研究。

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [76] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: 将法线估计重定义为阴影序列估计，利用图像到视频生成模型预测多光照阴影序列，再通过OLS求解法线，并用合成数据集MultiShade训练，显著提升了单目法线估计的几何对齐性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目法线估计直接预测法线图会导致重建表面与几何细节不对齐，原因是几何差异只在细微色彩变化中体现，模型难以区分。阴影序列对几何更敏感，因此可以更好地恢复细节。

Method: 提出RoSE方法：利用图像到视频的生成模型预测目标物体在多光照下的阴影序列；再通过最小二乘（OLS）将阴影序列反推法线。训练时构建了合成数据集MultiShade，包含多种形状、材质与光照。

Result: 在真实基准数据集上，RoSE在目标物体单目法线估计任务上取得了SOTA性能。

Conclusion: 本文提出通过估计光照序列（shading sequence）替代直接回归法线图，从而缓解3D对齐问题。

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [77] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: GeoFormer 为开源、多源（Sentinel-1/2 + DEM）Swin Transformer 框架，能在严格空间独立评测下有效估计建筑高度与轮廓，并在 54 城市上显著优于 CNN 基线，代码与全球产品已公布。


<details>
  <summary>Details</summary>
Motivation: 高质量三维城市数据对气候建模、灾害评估和城市规划非常重要，但现有数据受限于专有传感器或跨城市泛化能力差，因而提出基于开源卫星与 DEM 的可推广方法。

Method: 基于 Swin Transformer（GeoFormer），融合光学（Sentinel-2）、SAR（Sentinel-1）和 DEM 输入，使用 geo-blocked 划分保证训练/测试的严格空间独立性，进行联合回归/分割任务以输出建筑高度（BH）与建筑轮廓（BF）。

Result: 在 54 个多样化城市评测中，GeoFormer 达到 BH RMSE=3.19 m、BF RMSE=0.05，分别较最强 CNN 基线提升 7.5% 和 15.3%，并能在跨大陆迁移中维持 BH RMSE <3.5 m。消融显示 DEM 对高度估计不可或缺，光学信息优于 SAR，但多源融合效果最佳。所有代码、模型权重及全球产品均已公开。

Conclusion: GeoFormer 提出了一种开源的 Swin Transformer 框架，能够在 100 m 网格上仅用 Sentinel-1/2 影像和开源 DEM 数据联合估计建筑高度与建筑轮廓，且在多城市数据集上优于最强 CNN 基线。

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [78] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: 提出一种结合形变场可信度与非平衡最优传输的病灶配对方法，能无监督识别匹配、新生、消失、合并与分裂，明显优于距离-only基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于几何接近性的二分匹配在病灶出现/消失或体积变化显著时表现不佳，且难以应对合并与分裂情形，因此需要一种能融入注册信息并允许质量不守恒的稳健配对方法。

Method: 设计了一种注册感知的运输代价，结合：(i) 大小归一化的几何距离，(ii) 来自形变场雅可比的局部配准可信度先验，(iii) 可选的补丁级外观一致性。使用UOT处理不等质量（病灶体量）并通过相对剪枝稀疏化运输计划以得到一对一匹配及新生/消失/合并/分裂事件，无需重训练或启发式规则。

Result: 在纵向CT数据上，与仅基于距离的基线方法相比，本方法在边缘检测精确率与召回率、病灶状态召回以及病灶图组件F1分数上均有一致提升。

Conclusion: 本文提出的基于非平衡最优传输（UOT）的配对方法能更好地处理纵向CT中病灶的出现、消失、合并和分裂，从而提高病灶对应及演变评估的准确性。

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [79] [VersaViT: Enhancing MLLM Vision Backbones via Task-Guided Optimization](https://arxiv.org/abs/2602.09934)
*Yikun Liu,Yuan Liu,Shangzhe Di,Haicheng Wang,Zhongyin Zhao,Le Tian,Xiao Zhou,Jie Zhou,Jiangchao Yao,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 识别并改进MLLMs视觉编码器在密集表示上的不足，提出VersaViT多任务后训练框架及轻量任务头与多粒度监督，从而获得可用于语言推理与像素级任务的通用视觉Transformer。


<details>
  <summary>Details</summary>
Motivation: 问题来源于观察到MLLMs视觉编码器虽然在高层语义对齐上优秀，但其密集特征表示不足，导致无法胜任传统视觉任务。目标是构建一个既能支持语言介导推理又能进行像素级理解的通用视觉主干。

Method: 提出VersaViT，一种基于视觉Transformer的通用主干。方法核心是一个多任务协同后训练框架：使用轻量级任务头连接到编码器，在多粒度（可能包括像素级、区域级、全局语义）监督下同时训练多个经典视觉任务，从而改进密集特征表示。

Result: 通过在多个下游任务（语义分割、深度估计等）上的大量实验，VersaViT在密集预测任务上显著提升性能，证明了该方法能产出同时适用于语言推理和像素级理解的通用视觉主干。

Conclusion: 该论文认为现有多模态大模型（MLLMs）中的视觉编码器在高层语义对齐方面表现出色，但在密集特征表示上存在不足，影响了它们在传统视觉任务（如语义分割、深度估计）上的表现。作者提出了VersaViT，通过多任务协同后训练框架和轻量任务头及多粒度监督来优化视觉主干，从而提升其在像素级任务和语言推理两方面的能力。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable success in visual-language understanding, demonstrating superior high-level semantic alignment within their vision encoders. An important question thus arises: Can these encoders serve as versatile vision backbones, capable of reliably performing classic vision-centric tasks as well? To address the question, we make the following contributions: (i) we identify that the vision encoders within MLLMs exhibit deficiencies in their dense feature representations, as evidenced by their suboptimal performance on dense prediction tasks (e.g., semantic segmentation, depth estimation); (ii) we propose VersaViT, a well-rounded vision transformer that instantiates a novel multi-task framework for collaborative post-training. This framework facilitates the optimization of the vision backbone via lightweight task heads with multi-granularity supervision; (iii) extensive experiments across various downstream tasks demonstrate the effectiveness of our method, yielding a versatile vision backbone suited for both language-mediated reasoning and pixel-level understanding.

</details>


### [80] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krauß,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: HAC结合Transformer的拓扑先验与CNN的残差精化，并辅以物理感知自监督预训练，在膀胱内镜血管分割上解决伪影与标注稀缺问题，显著提升连通性与精确度，利于术中导航。


<details>
  <summary>Details</summary>
Motivation: 膀胱为空心且变形，缺乏稳定定位标志；内镜下血管可作为患者特异的“血管指纹”用于手术导航，但自动分割受限于标注稀少、光照/气泡等伪影、组织褶皱误检以及连续变形等领域特定挑战。

Method: 提出Hybrid Attention-Convolution (HAC)架构：Transformer模块学习去除短支和末梢枝的优化标注以建模血管全局拓扑先验，CNN分支生成残差精细化图以恢复细小血管细节；并采用物理感知的自监督预训练，通过基于临床的增强在无标签内镜数据上预训练以弥补标注稀缺。

Result: 在BlaVeS内镜视频帧数据集上，HAC达成准确率0.94、精确率0.61、clDice 0.66，优于现有医学分割方法，并能有效抑制由于黏膜褶皱导致的假阳性，从而提供临床导航所需的结构稳定性。

Conclusion: 该论文提出的HAC模型能同时利用Transformer的全局拓扑先验与CNN的细节恢复能力，从而在膀胱内镜血管分割任务中取得更稳健的结构连通性和较高的精度。

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [81] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: 论文在烘焙品检测上提出弱监督+伪标签训练流程，使用OWLv2/Grounding DINO与SAM生成伪标签，训练YOLOv11，在标注稀缺下仍能在非理想部署条件超越全监督基线。


<details>
  <summary>Details</summary>
Motivation: 德国烘焙品种类繁多且保质期短，人工监测剩余产品成本高且不易扩展，缺乏大量有标注数据，现有开域检测器在特定任务上表现不足，因此需要低监督成本且可部署的检测方案。

Method: 使用两条训练流程：1) 利用开域检测器OWLv2和Grounding DINO的定位能力与图像级标签结合，进行弱监督训练；2) 利用Segment Anything 2对视频帧做伪标签传播并微调模型以提高视角鲁棒性。模型选择YOLOv11并在这两种流程上训练。

Result: 仅用图像级监督时，模型达到mAP 0.91；用伪标签微调在非理想部署条件下提升19.3%；结合两种流程后，在非理想条件下的表现优于全监督基线。

Conclusion: 该论文提出了在烘焙行业中用弱监督和伪标签增强训练来自动检测烘焙品的方法，并在非理想部署条件下超过了全监督基线，证明了弱监督结合伪标签对稀缺标注场景的有效性。

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [82] [Coupled Inference in Diffusion Models for Semantic Decomposition](https://arxiv.org/abs/2602.09983)
*Calvin Yeung,Ali Zakeri,Zhuowen Zou,Mohsen Imani*

Main category: cs.CV

TL;DR: 将语义分解建为扩散模型中的耦合逆问题，通过重构引导项和新迭代采样法，实现比共振器网络更好的分解效果，并涵盖注意力共振器为特例。


<details>
  <summary>Details</summary>
Motivation: 许多视觉场景可由潜在因子组合构成，识别、推理与编辑需要解开这些组合（分解问题）。已有基于绑定操作的表征（如共振器网络）用于分解；鉴于霍普菲尔德网络与扩散模型的相似性，作者希望将耦合机制引入扩散模型以改进分解能力。

Method: 将语义分解视为逆问题，通过在多个扩散过程间加入基于重构误差的引导项来耦合它们；同时提出一种新的迭代采样策略以提升性能；理论上将注意力型共振器网络归约为该框架的一种特殊形式。

Result: 在一系列合成语义分解任务上，耦合推理框架的性能优于传统共振器网络；实验还验证了提出的迭代采样方案的有效性，并理论上联系了注意力型共振器与所提框架。

Conclusion: 本文提出了基于扩散模型的耦合推理框架，用于语义分解问题，证明了注意力型共振器网络是其特例，并在合成任务上优于共振器网络。

Abstract: Many visual scenes can be described as compositions of latent factors. Effective recognition, reasoning, and editing often require not only forming such compositional representations, but also solving the decomposition problem. One popular choice for constructing these representations is through the binding operation. Resonator networks, which can be understood as coupled Hopfield networks, were proposed as a way to perform decomposition on such bound representations. Recent works have shown notable similarities between Hopfield networks and diffusion models. Motivated by these observations, we introduce a framework for semantic decomposition using coupled inference in diffusion models. Our method frames semantic decomposition as an inverse problem and couples the diffusion processes using a reconstruction-driven guidance term that encourages the composition of factor estimates to match the bound vector. We also introduce a novel iterative sampling scheme that improves the performance of our model. Finally, we show that attention-based resonator networks are a special case of our framework. Empirically, we demonstrate that our coupled inference framework outperforms resonator networks across a range of synthetic semantic decomposition tasks.

</details>


### [83] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 缩略图分类在泛化性和计算效率上优于MIL，适合规模化质控；MIL在内部精度最高但成本高且泛化较差。


<details>
  <summary>Details</summary>
Motivation: 保持切片染色标签(metadata)的准确性对临床档案质量和计算病理学数据集完整性至关重要，需要可扩展且鲁棒的自动染色识别方案以用于常规视觉质控。

Method: 对比两种方法：1) Multi-Instance Learning (MIL)，使用整张切片的图像补丁进行训练与推理；2) 轻量级缩略图方法，基于整张切片的缩小图像进行分类评估。使用内部含16类（后合并为14类）数据集和外部TCGA数据集进行评估，指标为宏F1、加权F1与吞吐率（slides/s）。

Result: 在内部测试集上，MIL取得最佳表现（宏F1: 0.941/0.969），缩略图略逊（0.897/0.953）。在外部TCGA上，缩略图泛化更好（加权F1: 0.843 vs. 0.807）。缩略图方法推理速度远高于MIL（5.635 vs. 0.018 slides/s），提升两个数量级。

Conclusion: 缩略图方法在多项评价中表现出高效与鲁棒，适合日常质控部署；MIL在内部数据上略优但计算开销大，对外部数据泛化不如缩略图。

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [84] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: 通过归纳并扩展3DGS的工程与算法优化（数值稳定性、高斯截断、梯度近似等），Faster-GS 实现了高达5×的训练加速且保持重建质量，且可推广到4D非刚性场景。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS领域存在实现层面的改进与算法性修改混杂、性能与精度权衡问题，导致结果难以公平比较和复现。研究旨在整合并系统化这些策略，提供一个高效且保持质量的基线。

Method: 系统化整合先前有效且通用的策略，加入新的数值稳定性改进、高斯截断处理、梯度近似优化等实现细节，并在实现层面进行严格工程优化，从而构建 Faster-GS。对算法在各种基准上进行评估，并将优化方法扩展到4D高斯重建用于非刚性场景。

Result: Faster-GS 在多项基准上实现最高约5倍的训练加速，同时保持或接近原有视觉重建质量，并成功应用于4D非刚性重建，显著降低资源与时间成本。

Conclusion: 本文提出 Faster-GS，通过整合并改进现有3D Gaussian Splatting（3DGS）优化策略，并引入若干新优化，使训练速度显著提升同时保持视觉质量。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [85] [Perception with Guarantees: Certified Pose Estimation via Reachability Analysis](https://arxiv.org/abs/2602.10032)
*Tobias Ladner,Yasser Shoukry,Matthias Althoff*

Main category: cs.CV

TL;DR: 提出一种基于摄像头与已知目标几何的带形式化安全证书的3D位姿估计方法，结合可达性分析与神经网络验证，能在合成与真实场景中高效准确地定位并给出最坏情况保证。


<details>
  <summary>Details</summary>
Motivation: 安全关键系统需要在最坏情况（worst-case）下也能保证操作安全，粗略的位姿估计或不可信的外部服务（如GPS）不能提供这样的保证。因此需要一种只依赖可观测几何和摄像头输入、并能给出形式化最坏情况界限的位姿估计方法。

Method: 方法结合可达性分析与神经网络形式化验证：首先用深度网络或几何方法从图像提取位姿相关信息；然后通过可达性分析将输入不确定性传播到位姿空间；最后使用神经网络验证工具对网络在输入扰动范围内的输出进行上下界证明，从而得到位姿的形式化界限。

Result: 实验在合成与真实数据集上验证了方法的有效性：该方法在效率与精度上均能满足实际需求，并能输出带证书的位姿区间，证明在给定输入不确定性下位姿在该区间内。

Conclusion: 本文提出了一种仅依靠单张相机图像和已知目标几何信息的3D位姿估计方法，并为估计结果给出形式化的安全保证，适用于安全关键的赛博物理系统。

Abstract: Agents in cyber-physical systems are increasingly entrusted with safety-critical tasks. Ensuring safety of these agents often requires localizing the pose for subsequent actions. Pose estimates can, e.g., be obtained from various combinations of lidar sensors, cameras, and external services such as GPS. Crucially, in safety-critical domains, a rough estimate is insufficient to formally determine safety, i.e., guaranteeing safety even in the worst-case scenario, and external services might additionally not be trustworthy. We address this problem by presenting a certified pose estimation in 3D solely from a camera image and a well-known target geometry. This is realized by formally bounding the pose, which is computed by leveraging recent results from reachability analysis and formal neural network verification. Our experiments demonstrate that our approach efficiently and accurately localizes agents in both synthetic and real-world experiments.

</details>


### [86] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: 提出Fake-HR1：首个自适应是否推理的大规模混合推理模型，采用HFT+HGRPO两阶段训练，在保持甚至提升检测与推理能力的同时显著节省资源与延迟。


<details>
  <summary>Details</summary>
Motivation: 长链式思维（CoT）能提升生成图像检测效果，但过长推理代价高且对明显伪造样本冗余，因此需要自适应决定是否进行推理以优化效率。

Method: 提出两阶段训练：先进行混合微调（HFT）进行冷启动，再用混合推理分组策略优化（HGRPO）的在线强化学习隐式学习何时启用推理与选择推理模式。

Result: Fake-HR1在不同查询上能够自适应选择是否进行推理，超越现有大模型在推理能力与生成检测性能上，并显著提高响应效率。

Conclusion: Fake-HR1提出了一个基于混合推理的自适应检测模型，通过在推理是否必要时做选择，有效降低了资源开销并提升检测性能。

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [87] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: 去颅骨的T1脑MRI仍可通过标准预处理与图像相似性方法被精确匹配，提示共享此类数据存在实质性再识别风险，应促使数据共享政策和隐私评估更为谨慎。


<details>
  <summary>Details</summary>
Motivation: 监管框架要求在共享MRI前去除潜在可识别信息，但脑实质可能保留足够独特的特征导致不同数据库间的个体链接，从而带来隐私风险；此前研究依赖训练型或高计算成本方法，本研究希望展示更简单方法的有效性以推动政策制定。

Method: 使用常规预处理流程（配准、强度归一化等）对去颅骨T1加权影像进行标准化，然后计算图像相似性指标以进行配对匹配；实验在不同时间间隔、扫描仪类型、空间分辨率和采集协议下评估方法的匹配准确率。

Result: 在多种条件（跨时间、跨设备、分辨率和协议差异）下，使用标准预处理加图像相似性计算可以实现接近完美的样本匹配准确率，表明去颅骨并不能充分保障个体匿名化。

Conclusion: 本研究表明，即使在去颅骨（skull-stripped）后，T1加权脑MRI仍可通过简单的预处理与图像相似性计算实现高精度的个体匹配，从而存在再识别风险。

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [88] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: 提出用于实例分割的保形预测方法，按像素返回自适应置信掩码集合，具有理论覆盖保证并在多个任务上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有实例分割模型虽然平均性能高，但缺乏不确定性量化与校准机制，无法保证预测掩码与真实掩码接近，特别是像素级查询下需要能量化预测不确定性。

Method: 算法在给定图像和像素查询时，输出一组候选实例掩码，利用校准集估计阈值以保证至少一个掩码与真实实例的IoU超过目标阈值；提供了渐近和有限样本的理论保证，并与基线方法（Learn Then Test、Conformal Risk Control及形态学膨胀）对比。

Result: 在农业田块分割、细胞分割和车辆检测三个应用场景中，算法根据查询难度产生可变大小的预测集，实验显示达到了目标覆盖率并优于现有基线方法。

Conclusion: 该论文提出了一种基于保形预测（conformal prediction）的自适应置信集算法，用于实例分割中对像素级查询提供具有理论覆盖率保证的多预测掩码集合。

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [89] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: 提出的STA将多帧上下文整合进transformer注意力，简单高效地提升视频语义分割的时序一致性与精度，适用于多种transformer架构。


<details>
  <summary>Details</summary>
Motivation: 现有模型通常逐帧处理视频，未利用时间一致性，导致在动态场景下稳定性和精度不足。

Method: 在标准的自注意力机制上扩展，处理时空特征序列；保持计算效率并仅对现有架构做最小修改，使其适配轻量级和大规模transformer模型。

Result: 在Cityscapes和BDD100k上，STA在时序一致性指标上提升9.20个百分点，在mIoU上最高提升1.76个百分点，相较单帧基线有明显改善。

Conclusion: 作者提出了一个将时序信息引入transformer注意力模块的Spatio-Temporal Attention (STA)，用于视频语义分割，能够提升时序一致性和像素分割精度。

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [90] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim通过注意力驱动的视觉状态空间和块注意力模块，端到端输出三类掩码，实现源-目标联合定位并在基准上领先，同时发布新数据集CMFD-Anything。


<details>
  <summary>Details</summary>
Motivation: 仅检测目标（被篡改区域）可能导致误导性解读（例如在抗议图像中插入重复暴力场景），因此需要同时定位源与目标以恢复上下文和防止错误解读。

Method: 引入归一化注意力图以发现内部相似性，结合基于区域的块注意力模块以区分被操纵区域，输出三类掩码（原始、来源、目标），并在统一端到端可训练架构下支持拼接与拷贝移动检测。

Result: 在标准基准上取得了最先进的性能，并发布了新的CMFD-Anything数据集以弥补现有复制粘贴篡改数据集的不足。

Conclusion: Forensim提出了一种基于注意力的视觉状态空间框架，可同时定位图像中的源区域和被篡改（目标）区域，从而比仅依赖伪影线索的方法更全面地识别拼接与复制粘贴篡改。

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [91] [4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094)
*Yihang Luo,Shangchen Zhou,Yushi Lan,Xingang Pan,Chen Change Loy*

Main category: cs.CV

TL;DR: 4RC用Transformer一次编码全视频的时空潜变量，结合条件解码器和最小因子化4D表示，实现了高效、稠密且联合的4D重建，并在多任务评测中表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将运动与几何解耦或只产生稀疏轨迹/双视图场景流，无法提供完整的密集4D表示；因此提出一个能联合表示几何与运动并能高效查询任意时间点的统一框架。

Method: 采用Transformer作为骨干，将整段视频编码为紧凑的时空潜变量，并通过条件解码器在任意时间点查询3D几何与运动信息；对每视图的4D属性进行最小因子化表示，分解为基底几何与时间相关的相对运动。

Result: 在大量实验中，4RC在多项4D重建任务上优于先前和并行方法，证明了其在准确性和效率上的优势。

Conclusion: 4RC通过一次编码、多次查询的统一前向框架，实现了从单目视频到稠密4D场景几何与运动的联合重建，显著优于现有方法。

Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>


### [92] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: 作者发现因果扩散模型的时间推理可与多步去噪分离，提出SCD：一次性因果编码器+轻量逐帧扩散解码器，既提高效率又保持或提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 观察到现有因果扩散模型在所有层、每个去噪步骤和整个上下文上反复进行因果注意，导致计算冗余；希望将时间因果推理与多步去噪解耦以提高效率。

Method: 通过系统性探测自回归视频扩散器，发现早期层在多个去噪步骤中产生高度相似的特征，深层则表现为稀疏的跨帧注意力和主要进行帧内渲染。基于此，设计SCD：使用因果Transformer编码器负责每帧一次性的时间关系建模，使用轻量扩散解码器负责逐步的帧内渲染。

Result: 在合成与真实数据集的预训练与后训练任务上，SCD显著提高吞吐率和单帧延迟，同时在生成质量上匹配或优于强基线。

Conclusion: 提出SCD架构，将一次性帧间因果推理与多步逐帧渲染分离，以减少冗余计算并提升吞吐与延迟，同时保持或提高生成质量。

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [93] [VideoWorld 2: Learning Transferable Knowledge from Real-world Videos](https://arxiv.org/abs/2602.10102)
*Zhongwei Ren,Yunchao Wei,Xiao Yu,Guixun Luo,Yao Zhao,Bingyi Kang,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: VideoWorld 2通过预训练视频扩散模型与动态增强隐空间动力学模型解耦视觉与动作，实现从无标签真实视频中学习可迁移动态知识，显著提升复杂长时任务与机器人操作的性能。


<details>
  <summary>Details</summary>
Motivation: 希望直接从原始真实世界视频中学习可迁移的世界动力学知识，以便在新环境和机器人任务中复用而无需标注数据。

Method: 引入dynamic-enhanced Latent Dynamics Model (dLDM)，使用预训练视频扩散模型负责视觉外观建模，dLDM在隐藏空间学习紧凑且与任务相关的动力学编码，随后对这些编码进行自回归建模以学习策略并支持长时推理。

Result: 在手工制作任务上，VideoWorld 2相比先前方法在任务成功率上最多提升70%，且能生成连贯的长时执行视频；在机器人领域，从Open-X数据集学习的知识显著提升了CALVIN任务的操作性能。

Conclusion: 该论文提出VideoWorld 2，通过将视觉外观建模与动作动力学解耦，实现从真实无标签视频中学习可迁移的动态知识，能显著提升任务成功率并生成连贯长时执行视频，同时在机器人操作任务中提高性能。

Abstract: Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.

</details>


### [94] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出SeqΔ-REPA通过对齐潜动作与视频编码器的时序特征差分，构建可迁移的动作语义；在Olaf-World管线上实现从被动视频预训练，显著提升零-shot迁移与样本效率。


<details>
  <summary>Details</summary>
Motivation: 无监督视频中学习到的潜在动作往往与场景特定信息纠缠，缺乏跨上下文共享的语义坐标系；现有目标仅在单个片段内工作，无法在不同上下文间对齐动作语义。

Method: 在冻结的自监督视频编码器的时间特征差分上锚定集成的潜在动作，设计序列级控制-效果对齐目标（SeqΔ-REPA）；基于此构建Olaf-World管线，用大规模被动视频预训练动作条件化的视频世界模型。

Result: 在大量实验中，该方法学得更具结构性的动作潜在空间，在零样本动作迁移和以更少数据适应新控制接口方面均优于最先进基线。

Conclusion: 本文提出SeqΔ-REPA，通过对齐序列级控制效果来学习可迁移的动作潜变量，从而解决无监督视频中动作标签稀缺导致的上下文不可迁移问题。

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [95] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 构建大规模多视角对象数据集和新评估基准，并提出基于辅助未标注视图与视觉-几何双流编码的I2V生成框架ConsID-Gen，显著改进多视图一致性与身份保真度。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法在单视图观测下容易产生外观漂移与几何失真，且跨模态对齐薄弱，导致生成视频难以保持细粒度物体身份与多视角一致性。论文从数据和模型两端改进以解决这些问题。

Method: 作者首先构建了大规模对象中心数据集ConsIDVid并设计了ConsIDVid-Bench评估框架，提出了基于多视图一致性度量的新评估指标；在模型上引入ConsID-Gen：在首帧基础上增加未姿态标注的辅助视图，采用双流视觉-几何编码器融合语义与结构信息，并通过文本-视觉连接器为Diffusion Transformer提供统一条件。

Result: 在ConsIDVid-Bench上，ConsID-Gen在多项指标上均优于现有领先视频生成模型（如Wan2.1与HunyuanVideo），在身份保真度与时间一致性方面表现最佳，能更稳健地处理真实场景下的挑战。

Conclusion: 该论文提出了一个针对Image-to-Video生成中保持物体身份一致性和多视角几何稳定性的问题的系统性解决方案，通过数据与模型双管齐下，显著提升了生成视频的身份保真度和时序一致性。

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [96] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Möller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: IQARS用量子退火求解经过二值化的局部二次非凸子问题以替代凸松弛，保留旋转流形几何并利用量子隧穿与并行性，在小规模问题上比Shonan等经典方法有约12%精度提升，但受制于当前退火硬件。


<details>
  <summary>Details</summary>
Motivation: 经典MRA方法（如L1-IRLS与Shonan）在高噪声场景下精度下降或易陷入局部极小值，且凸松弛不能充分保留旋转流形的几何结构。提出基于量子退火的新范式以期克服这些限制，提升噪声鲁棒性与全局一致性恢复性能。

Method: 将MRA分解为局部二次非凸子问题，随后对每个子问题进行二值化以适配量子退火器（如D-Wave），通过迭代更新局部解来逐步逼近全局一致的绝对旋转。该方法避免了对凸松弛的依赖，保持非欧氏旋转流形几何，并借助量子隧穿跳出局部极小值，同时利用量子并行性加速搜索。

Result: 在合成与真实数据集上的经验评估显示，尽管现有量子退火硬件规模与性能有限，IQARS在D-Wave上实现了约12%的平均精度提升（相较于Shonan），并展示了在高噪声条件下更好的全局一致性恢复能力，但规模可扩展性、二值化误差与硬件噪声是主要局限。

Conclusion: IQARS提出了将多旋转平均(MRA)重写为一系列二次非凸子问题并在量子退火器上求解的新方法，旨在保留旋转流形的几何特性并利用量子退火的隧穿与并行优势。实验表明，在当前退火器规模受限的情况下，IQARS在若干数据集上相较于最佳经典方法Shonan可实现约12%的精度提升，但受限于硬件规模和问题二值化带来的近似误差，应用范围与性能仍有限。

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


### [97] [SAGE: Scalable Agentic 3D Scene Generation for Embodied AI](https://arxiv.org/abs/2602.10116)
*Hongchi Xia,Xuan Li,Zhaoshuo Li,Qianli Ma,Jiashu Xu,Ming-Yu Liu,Yin Cui,Tsung-Yi Lin,Wei-Chiu Ma,Shenlong Wang,Shuran Song,Fangyin Wei*

Main category: cs.CV

TL;DR: SAGE是一个自动化的场景生成代理，通过多生成器与多评价器的闭环自我修正，生成仿真就绪且物理合理的大规模3D环境，支持高质量策略训练与泛化。


<details>
  <summary>Details</summary>
Motivation: 真实环境数据收集代价高且存在安全风险，需要可扩展、真实且可用于仿真的三维环境来加速可体现智能体的训练与评估。

Method: 框架结合布局生成器与物体组合生成器，并配备语义、视觉与物理稳定性评价器；通过自适应工具选择和迭代推理循环，持续自我修正场景直到满足用户指定任务和物理有效性。

Result: 生成的环境现实、 多样并可直接部署到现代模拟器；用这些合成数据训练的策略展现出随数据规模增长的性能提升，并能泛化到未见物体和布局。

Conclusion: SAGE提出了一个能自动生成仿真就绪三维场景的agentic框架，通过多生成器和多评价器的闭环迭代优化，提升语义合理性、视觉真实度和物理稳定性，生成的数据可直接用于训练策略并在未见环境中泛化。

Abstract: Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., "pick up a bowl and place it on the table"), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [98] [Scaling GraphLLM with Bilevel-Optimized Sparse Querying](https://arxiv.org/abs/2602.09038)
*Yangzhe Peng,Haiquan Qiu,Quanming Yao,Kun He*

Main category: cs.DB

TL;DR: BOSQ通过双层优化的自适应稀疏查询，仅在高收益情况下调用LLM，显著降低计算成本并保持甚至提升节点级任务性能。


<details>
  <summary>Details</summary>
Motivation: 直接为所有节点调用LLM生成解释性特征成本高昂（时间与金钱），阻碍实际应用；需要一种方法在保持或提升任务性能的前提下大幅减少LLM查询次数。

Method: 设计了自适应稀疏查询策略（Bilevel-Optimized Sparse Querying），上层控制何时以及对哪些节点触发LLM调用以最大化性能增益，下层可能用于评估或学习节点的收益估计；通过减少冗余或低收益的查询显著降低计算开销。

Result: 在六个真实文本属性图数据集上、涵盖两类节点级任务的广泛实验表明，BOSQ相比现有GraphLLM方法在查询效率上实现数量级加速，同时在性能上保持相当或更好表现。

Conclusion: 本文提出BOSQ框架，通过双层优化的稀疏查询策略有选择性地调用LLM以生成解释性特征，从而在文本属性图的节点级任务上在性能与计算成本之间取得良好权衡。

Abstract: LLMs have recently shown strong potential in enhancing node-level tasks on text-attributed graphs (TAGs) by providing explanation features. However, their practical use is severely limited by the high computational and monetary cost of repeated LLM queries. To illustrate, naively generating explanations for all nodes on a medium-sized benchmark like Photo (48k nodes) using a representative method (e.g., TAPE) would consume days of processing time. In this paper, we propose Bilevel-Optimized Sparse Querying (BOSQ), a general framework that selectively leverages LLM-derived explanation features to enhance performance on node-level tasks on TAGs. We design an adaptive sparse querying strategy that selectively decides when to invoke LLMs, avoiding redundant or low-gain queries and significantly reducing computation overhead. Extensive experiments on six real-world TAG datasets involving two types of node-level tasks demonstrate that BOSQ achieves orders of magnitude speedups over existing GraphLLM methods while consistently delivering on-par or superior performance.

</details>


### [99] [Efficient Distance Pruning for Process Suffix Comparison in Prescriptive Process Monitoring](https://arxiv.org/abs/2602.09039)
*Sarra Madad*

Main category: cs.DB

TL;DR: 利用优化基点和三角不等式对后缀距离建立界限并剪枝，既保证精确性又显著加速了处方式流程监控中的后缀检索。


<details>
  <summary>Details</summary>
Motivation: 传统处方式流程监控需要对进行中的案例与日志中大量后缀进行相似性比较以预测并建议改进动作，然而后缀比较的规模随日志增长迅速膨胀，导致计算成本难以承受，需要更高效且精确的检索方法。

Method: 选取并优化一组基点（pivots），对日志中每个后缀计算与基点的距离，利用这些到基点的距离及三角不等式推导出上下界以剪除不可能成为最近邻的后缀；该过程可并行化且保持结果与穷尽比较完全一致。

Result: 通过度量基点剪枝，实验或分析表明运行时间大幅下降，剪枝不影响检索到的后缀集合（与穷尽比较一致），且算法天然支持并行，实现了可扩展的后缀比较加速。

Conclusion: 本文提出利用三角不等式的度量基剪枝方法，能在不损失检索精确性的前提下显著减少后缀比较的计算量，从而加速面向处方的流程监控并提升可扩展性。

Abstract: Prescriptive process monitoring seeks to recommend actions that improve process outcomes by analyzing possible continuations of ongoing cases. A key obstacle is the heavy computational cost of large-scale suffix comparisons, which grows rapidly with log size. We propose an efficient retrieval method exploiting the triangle inequality: distances to a set of optimized pivots define bounds that prune redundant comparisons. This substantially reduces runtime and is fully parallelizable. Crucially, pruning is exact: the retrieved suffixes are identical to those from exhaustive comparison, thereby preserving accuracy. These results show that metric-based pruning can accelerate suffix comparison and support scalable prescriptive systems.

</details>


### [100] [SciDataCopilot: An Agentic Data Preparation Framework for AGI-driven Scientific Discovery](https://arxiv.org/abs/2602.09132)
*Jiyong Rao,Yicheng Qiu,Jiahui Zhang,Juntao Deng,Shangquan Sun,Fenghua Ling,Hao Chen,Nanqing Dong,Zhangyang Gao,Siqi Sun,Yuqiang Li,Dongzhan Zhou,Guangyu Wang,Lijun Wu,Conghui He,Xuhong Wang,Jing Shao,Xiang Liu,Yu Zhu,Mianxin Liu,Qihao Zheng,Yinghui Zhang,Jiamin Wu,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Bo Zhang,Wanli Ouyang,Runkai Zhao,Chunfeng Song,Lei Bai,Chi Zhang*

Main category: cs.DB

TL;DR: 本文把“AI-Ready”推广到科学数据，提出SciDataCopilot自治框架，实现端到端数据就绪化，显著加速和标准化实验数据处理，促进AGI4S与实验现实对接。


<details>
  <summary>Details</summary>
Motivation: 当前AI4S以大规模文本为主，原始实验数据的异质性与缺乏语义对齐成为限制闭环科学发现的关键瓶颈，亟需将数据就绪性提升为核心操作原语。

Method: 提出了科学AI-Ready数据的形式化定义，并设计了SciDataCopilot自治代理框架，包含数据摄取、科学意图解析和多模态整合的端到端流水线，强调数据可复用性与可迁移性。

Result: 在三个异构科学领域的大规模评估表明，SciDataCopilot在数据准备方面比人工流程最高可达30倍加速，同时提升了效率、可扩展性与一致性。

Conclusion: SciDataCopilot通过将“科学AI-Ready数据”作为核心范式，桥接文本驱动AI系统与原始实验数据之间的鸿沟，从而推动实验驱动的科学通用智能发展。

Abstract: The current landscape of AI for Science (AI4S) is predominantly anchored in large-scale textual corpora, where generative AI systems excel at hypothesis generation, literature search, and multi-modal reasoning. However, a critical bottleneck for accelerating closed-loop scientific discovery remains the utilization of raw experimental data. Characterized by extreme heterogeneity, high specificity, and deep domain expertise requirements, raw data possess neither direct semantic alignment with linguistic representations nor structural homogeneity suitable for a unified embedding space. The disconnect prevents the emerging class of Artificial General Intelligence for Science (AGI4S) from effectively interfacing with the physical reality of experimentation. In this work, we extend the text-centric AI-Ready concept to Scientific AI-Ready data paradigm, explicitly formalizing how scientific data is specified, structured, and composed within a computational workflow. To operationalize this idea, we propose SciDataCopilot, an autonomous agentic framework designed to handle data ingestion, scientific intent parsing, and multi-modal integration in a end-to-end manner. By positioning data readiness as a core operational primitive, the framework provides a principled foundation for reusable, transferable systems, enabling the transition toward experiment-driven scientific general intelligence. Extensive evaluations across three heterogeneous scientific domains show that SciDataCopilot improves efficiency, scalability, and consistency over manual pipelines, with up to 30$\times$ speedup in data preparation.

</details>


### [101] [Predictive Query Language: A Domain-Specific Language for Predictive Modeling on Relational Databases](https://arxiv.org/abs/2602.09572)
*Vid Kocijan,Jinu Sunil,Jan Eric Lenssen,Viman Deb,Xinwei Xe,Federco Reyes Gomez,Matthias Fey,Jure Leskovec*

Main category: cs.DB

TL;DR: PQL 用 SQL 风格的声明式查询统一定义预测任务，自动生成训练标签，支持多种 ML 任务并在实际平台中成功应用，具备小规模低延迟和大规模实现。


<details>
  <summary>Details</summary>
Motivation: 当前在关系数据库上训练 ML 模型需要手动从多表提取训练样本与标签，过程繁琐、易错且耗时，亟需一种可复用、自动化的方式来定义和生成训练数据。

Method: 将预测任务抽象为单条声明式查询，自动计算训练标签，支持回归、分类、时间序列预测和推荐等多种任务；实现了两套系统：一套面向小规模低延迟应用，另一套可扩展到大规模数据库。

Result: PQL 已集成到预测 AI 平台并成功用于多种实际用例（金融欺诈、商品推荐、工作负载预测等），展示了语言的多样性与实用性；两种实现表明可以覆盖从低延迟到大规模的场景。

Conclusion: PQL 提供了一种基于 SQL 的声明式语言，使得在关系数据库上定义预测任务（预测实体与标签）自动化，减少人工特征/标签提取工作，提升开发效率与可靠性。

Abstract: The purpose of predictive modeling on relational data is to predict future or missing values in a relational database, for example, future purchases of a user, risk of readmission of the patient, or the likelihood that a financial transaction is fraudulent. Typically powered by machine learning methods, predictive models are used in recommendations, financial fraud detection, supply chain optimization, and other systems, providing billions of predictions every day. However, training a machine learning model requires manual work to extract the required training examples - prediction entities and target labels - from the database, which is slow, laborious, and prone to mistakes. Here, we present the Predictive Query Language (PQL), a SQL-inspired declarative language for defining predictive tasks on relational databases. PQL allows specifying a predictive task in a single declarative query, enabling the automatic computation training labels for a large variety of machine learning tasks, such as regression, classification, time-series forecasting, and recommender systems. PQL is already successfully integrated and used in a collection of use cases as part of a predictive AI platform. The versatility of the language can be demonstrated through its many ongoing use cases, including financial fraud, item recommendations, and workload prediction. We demonstrate its versatile design through two implementations; one for small-scale, low-latency use and one that can handle large-scale databases.

</details>


### [102] [Optimal Bounds-Only Pruning for Spatial AkNN Joins](https://arxiv.org/abs/2602.10027)
*Dominik Winecki*

Main category: cs.DB

TL;DR: 针对仅有分区界限统计的数据，提出三界限近邻测试以实现更积极的剪枝，从而在不建索引情况下高效执行精确欧氏AkNN连接。


<details>
  <summary>Details</summary>
Motivation: 数据仓库常对大表进行分区并保存行组统计以加速查询而不维护空间索引。现有裁剪方法对仅有界限信息的空间数据过于保守，未能充分利用方向性语义，导致无法在连接早期跳过不必要的分区。论文旨在利用分区统计的方向性信息提高AkNN连接的效率。

Method: 提出了“三界限近邻测试”（three-bound proximity test），该测试使用来自分区行组统计的三个界限来判断某一分区内的所有点是否在另一个（可能被遮蔽的）分区中都有更近的邻居。基于这一测试，加入策略将连接评估局部化到少数分区并实现早期剪枝。

Result: 通过理论分析证明了三界限测试的最优性与高效性，并通过实验（隐含）展示了在分区化、基于界限的数据上，相较于现有方法可更早跳过分区、降低加载和计算成本，从而加速精确AkNN连接。

Conclusion: 该论文提出了一种针对基于界限（bounds-only）空间数据的精确欧氏AkNN连接的裁剪测试，能够利用分区级别的行组统计信息，在不建索引的情况下提前排除不必要的分区，从而减少数据加载和计算开销。作者证明了算法的最优性和有效性。

Abstract: We propose a bounds-only pruning test for exact Euclidean AkNN joins on partitioned spatial datasets. Data warehouses commonly partition large tables and store row group statistics for them to accelerate searches and joins, rather than maintaining indexes. AkNN joins can benefit from such statistics by constructing bounds and localizing join evaluations to a few partitions before loading them to build spatial indexes. Existing pruning methods are overly conservative for bounds-only spatial data because they do not fully capture its directional semantics, thereby missing opportunities to skip unneeded partitions at the earliest stages of a join. We propose a three-bound proximity test to determine whether all points within a partition have a closer neighbor in one partition than in another, potentially occluded partition. We show that our algorithm is both optimal and efficient.

</details>
