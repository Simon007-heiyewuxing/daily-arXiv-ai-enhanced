<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 63]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LoRA-Edge: Tensor-Train-Assisted LoRA for Practical CNN Fine-Tuning on Edge Devices](https://arxiv.org/abs/2511.03765)
*Hyunseok Kwak,Kyeongwon Lee,Jae-Jin Lee,Woojoo Lee*

Main category: cs.CV

TL;DR: LoRA-Edge：在边缘设备上用 TT-SVD 辅助的 LoRA 进行卷积层参数高效微调，保留卷积结构并将更新融合回密集核，极大降低可训练参数（≤1.49%）且保持接近全量微调的性能，适合 HAR 等边缘应用。


<details>
  <summary>Details</summary>
Motivation: 边缘设备在内存、计算和能耗方面受限，但应用如 HAR 需对域迁移进行在线或设备端微调；全量微调资源开销太大，因此需要参数高效且结构保留的微调方法。

Method: LoRA-Edge 基于 LoRA 的低秩适配，使用 TT-SVD 将预训练卷积层分解为张量列车结构，仅零初始化并训练输出端的核心（保持辅助路径初始不活跃），训练完成后将更新融合回原始密集卷积核以维持推理效率。

Result: 在多个 HAR 数据集和 CNN 骨干网络上，LoRA-Edge 在仅更新至多1.49%参数的情况下，能将精度保持在距全量微调不超过4.7%的范围内，优于同等预算下的其他 PEFT 基线；在 Jetson Orin Nano 上，TT-SVD 初始化和选择性核训练能使收敛到目标 F1 的速度加快1.4-3.8倍。

Conclusion: LoRA-Edge 提供了一种在边缘设备上进行卷积神经网络（CNN）参数高效微调的可行方案，通过结合 TT-SVD 分解、选择性更新输出端核以及训练后融回密集核的策略，在保持推理成本不变的前提下，显著减少可训练参数并在多数 HAR 数据集上几乎达到全量微调的性能。

Abstract: On-device fine-tuning of CNNs is essential to withstand domain shift in edge
applications such as Human Activity Recognition (HAR), yet full fine-tuning is
infeasible under strict memory, compute, and energy budgets. We present
LoRA-Edge, a parameter-efficient fine-tuning (PEFT) method that builds on
Low-Rank Adaptation (LoRA) with tensor-train assistance. LoRA-Edge (i) applies
Tensor-Train Singular Value Decomposition (TT-SVD) to pre-trained convolutional
layers, (ii) selectively updates only the output-side core with
zero-initialization to keep the auxiliary path inactive at the start, and (iii)
fuses the update back into dense kernels, leaving inference cost unchanged.
This design preserves convolutional structure and reduces the number of
trainable parameters by up to two orders of magnitude compared to full
fine-tuning. Across diverse HAR datasets and CNN backbones, LoRA-Edge achieves
accuracy within 4.7% of full fine-tuning while updating at most 1.49% of
parameters, consistently outperforming prior parameter-efficient baselines
under similar budgets. On a Jetson Orin Nano, TT-SVD initialization and
selective-core training yield 1.4-3.8x faster convergence to target F1.
LoRA-Edge thus makes structure-aligned, parameter-efficient on-device CNN
adaptation practical for edge platforms.

</details>


### [2] [SILVI: Simple Interface for Labeling Video Interactions](https://arxiv.org/abs/2511.03819)
*Ozan Kanbertay,Richard Vogg,Elif Karakoc,Peter M. Kappeler,Claudia Fichtel,Alexander S. Ecker*

Main category: cs.CV

TL;DR: SILVI是一款将行为与交互标注与个体定位结合的开源视频标注工具，便于生成训练计算机视觉模型的结构化数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦单体动作检测，缺乏交互检测和标注工具，且现有开源工具要么只支持行为标注要么只支持个体定位。

Method: 在视频中集成行为标注与个体定位功能，生成适合训练与验证计算机视觉模型的结构化输出；提供软件、文档与下载地址。

Result: 开发并发布SILVI，支持在视频中直接标注行为与交互，促进精细行为分析和动态场景图提取，对动物与人类交互分析均有潜在应用。

Conclusion: 提出了SILVI一个开源视频标注软件，结合行为标注与个体定位以支持交互注释，填补现有工具的功能空白。

Abstract: Computer vision methods are increasingly used for the automated analysis of
large volumes of video data collected through camera traps, drones, or direct
observations of animals in the wild. While recent advances have focused
primarily on detecting individual actions, much less work has addressed the
detection and annotation of interactions -- a crucial aspect for understanding
social and individualized animal behavior. Existing open-source annotation
tools support either behavioral labeling without localization of individuals,
or localization without the capacity to capture interactions. To bridge this
gap, we present SILVI, an open-source labeling software that integrates both
functionalities. SILVI enables researchers to annotate behaviors and
interactions directly within video data, generating structured outputs suitable
for training and validating computer vision models. By linking behavioral
ecology with computer vision, SILVI facilitates the development of automated
approaches for fine-grained behavioral analyses. Although developed primarily
in the context of animal behavior, SILVI could be useful more broadly to
annotate human interactions in other videos that require extracting dynamic
scene graphs. The software, along with documentation and download instructions,
is available at: https://gitlab.gwdg.de/kanbertay/interaction-labelling-app.

</details>


### [3] [Noise Injection: Improving Out-of-Distribution Generalization for Limited Size Datasets](https://arxiv.org/abs/2511.03855)
*Duong Mai,Lawrence Hall*

Main category: cs.CV

TL;DR: 对CXR模型训练时加入高斯、斑点、泊松、椒盐四类噪声，可显著提升对新临床来源的泛化能力，将ID-OOD性能差距大幅降低。


<details>
  <summary>Details</summary>
Motivation: DL模型在不同设备/临床来源的胸片上泛化性差，模型利用源数据的伪特征（shortcut）导致在新分布上性能下降，研究旨在寻找简单有效的方法提升鲁棒性。

Method: 在训练阶段对输入CXR图像应用四种基础噪声（高斯、斑点/乘性、泊松、椒盐），并在十个随机种子下评估模型在ID与OOD数据集上的AUC、F1、准确率、召回率和特异性。

Result: 噪声注入训练在多个关键指标上显著缩小ID与OOD性能差距（AUC/F1/accuracy/recall/specificity），并公开了代码仓库。

Conclusion: 噪声注入能有效减少COVID-19胸片分类模型的ID与OOD表现差距，实验证明平均差距从0.10-0.20降至0.01-0.06。

Abstract: Deep learned (DL) models for image recognition have been shown to fail to
generalize to data from different devices, populations, etc. COVID-19 detection
from Chest X-rays (CXRs), in particular, has been shown to fail to generalize
to out-of-distribution (OOD) data from new clinical sources not covered in the
training set. This occurs because models learn to exploit shortcuts -
source-specific artifacts that do not translate to new distributions - rather
than reasonable biomarkers to maximize performance on in-distribution (ID)
data. Rendering the models more robust to distribution shifts, our study
investigates the use of fundamental noise injection techniques (Gaussian,
Speckle, Poisson, and Salt and Pepper) during training. Our empirical results
demonstrate that this technique can significantly reduce the performance gap
between ID and OOD evaluation from 0.10-0.20 to 0.01-0.06, based on results
averaged over ten random seeds across key metrics such as AUC, F1, accuracy,
recall and specificity. Our source code is publicly available at
https://github.com/Duongmai127/Noisy-ood

</details>


### [4] [Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures](https://arxiv.org/abs/2511.03882)
*Florence Klitzner,Blanca Inigo,Benjamin D. Killeen,Lalithkumar Seenivasan,Michelle Song,Axel Krieger,Mathias Unberath*

Main category: cs.CV

TL;DR: 在高保真仿真与模仿学习下，可用双平面X线影像实现开放式视觉控制的经皮套管插入，表现有前景但受限于入点精度与闭环反馈频率，需更强先验与领域知识改进。


<details>
  <summary>Details</summary>
Motivation: 探究基于模仿学习的视频机器人控制方法是否适用于复杂的X线引导脊柱手术（双平面），并评估其在套管插入任务中的能力与局限。

Method: 构建高逼真度的仿真沙盒生成双平面X线序列与正确轨迹数据，基于此数据训练模仿学习策略用于规划与开环控制，仅依赖视觉信息迭代对准套管。

Result: 策略在仿真环境首次尝试成功率为68.5%，能在不同椎体水平维持安全的椎体内轨迹，能泛化到复杂解剖（含骨折），对初始位置变化鲁棒，在真实双平面X线图像上rollout也能产生合理轨迹，但入点精度不足。

Conclusion: 论文表明，在仿真环境下通过模仿学习从双平面X线影像直接学习经皮套管插入控制策略是可行的，但仍存在精度和闭环控制频率方面的局限。

Abstract: Imitation learning-based robot control policies are enjoying renewed interest
in video-based robotics. However, it remains unclear whether this approach
applies to X-ray-guided procedures, such as spine instrumentation. This is
because interpretation of multi-view X-rays is complex. We examine
opportunities and challenges for imitation policy learning in bi-plane-guided
cannula insertion. We develop an in silico sandbox for scalable, automated
simulation of X-ray-guided spine procedures with a high degree of realism. We
curate a dataset of correct trajectories and corresponding bi-planar X-ray
sequences that emulate the stepwise alignment of providers. We then train
imitation learning policies for planning and open-loop control that iteratively
align a cannula solely based on visual information. This precisely controlled
setup offers insights into limitations and capabilities of this method. Our
policy succeeded on the first attempt in 68.5% of cases, maintaining safe
intra-pedicular trajectories across diverse vertebral levels. The policy
generalized to complex anatomy, including fractures, and remained robust to
varied initializations. Rollouts on real bi-planar X-rays further suggest that
the model can produce plausible trajectories, despite training exclusively in
simulation. While these preliminary results are promising, we also identify
limitations, especially in entry point precision. Full closed-look control will
require additional considerations around how to provide sufficiently frequent
feedback. With more robust priors and domain knowledge, such models may provide
a foundation for future efforts toward lightweight and CT-free robotic
intra-operative spinal navigation.

</details>


### [5] [Desert Waste Detection and Classification Using Data-Based and Model-Based Enhanced YOLOv12 DL Model](https://arxiv.org/abs/2511.03888)
*Abdulmumin Sa'ad,Sulaimon Oyeniyi Adebayo,Abdul Jabbar Siddiqui*

Main category: cs.CV

TL;DR: 提出一种剪枝后的轻量YOLOv12+SAT并结合专用数据增强的方法，针对沙漠无人机垃圾检测实现了高精度、低延迟和小模型体积，验证了数据中心+模型中心增强的有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是全球垃圾问题加剧，传统人工收集在偏远/恶劣环境（如沙漠）效率低且危险；现有自动化检测研究多集中于城市与可回收物，忽略有机/危险垃圾与特殊地形，因此需要面向沙漠无人机的实时、轻量且鲁棒的检测系统。

Method: 方法包括：基于YOLOv12的模型结构轻量化与剪枝，集成Self-Adversarial Training以提升鲁棒性，设计针对沙漠场景和无人机视角的专用数据增强（如亮度、沙尘扰动、尺度和旋转变换），在DroneTrashNet数据集上训练与评估，并与其他轻量级YOLO变体进行基准比较。

Result: 在DroneTrashNet上，模型在精确率、召回率和mAP上均显著提升，同时模型尺寸小、延迟低，适合部署于资源受限的无人机，与其它轻量YOLO变体相比取得了更优的精度-效率平衡。

Conclusion: 该论文提出了一种针对沙漠环境无人机实时垃圾检测的轻量化YOLOv12改进框架，通过模型剪枝、引入自我对抗训练(SAT)和专用数据增强策略，实现了在资源受限平台上的高精度低延迟部署。

Abstract: The global waste crisis is escalating, with solid waste generation expected
to increase by 70% by 2050. Traditional waste collection methods, particularly
in remote or harsh environments like deserts, are labor-intensive, inefficient,
and often hazardous. Recent advances in computer vision and deep learning have
opened the door to automated waste detection systems, yet most research focuses
on urban environments and recyclable materials, overlooking organic and
hazardous waste and underexplored terrains such as deserts. In this work, we
propose an enhanced real-time object detection framework based on a pruned,
lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT)
and specialized data augmentation strategies. Using the DroneTrashNet dataset,
we demonstrate significant improvements in precision, recall, and mean average
precision (mAP), while achieving low latency and compact model size suitable
for deployment on resource-constrained aerial drones. Benchmarking our model
against state-of-the-art lightweight YOLO variants further highlights its
optimal balance of accuracy and efficiency. Our results validate the
effectiveness of combining data-centric and model-centric enhancements for
robust, real-time waste detection in desert environments.

</details>


### [6] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine,Majid Ben Yakhlef,Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 将同类图像按3x1合成为复合输入（CoImg），建立平衡Co-OCTDL数据集，使用VGG16对比实验显示显著提升（Acc99.6%、F1 0.995、AUC0.9996），适用于小样本/不平衡医学影像任务。


<details>
  <summary>Details</summary>
Motivation: 小样本、类别不平衡和图像质量差会导致深度学习模型高误报；增加每个训练样本的有效信息密度与类内变异可提升判别能力。

Method: 提出Class-Based Image Composition：将同类别多张高分辨率OCT切片按3x1布局合成为单个训练样本（CoImg），并构建平衡数据集Co-OCTDL；使用相同VGG16架构与超参数在原始与复合数据上对比评估。

Result: 在构建的Co-OCTDL上，VGG16达到99.6%准确率，F1=0.995，AUC=0.9996，误报率显著降低，优于在原始数据集上的基线模型。

Conclusion: 该方法通过将同类图像融合为复合输入图像（CoImg），在小样本与类别不平衡情形下能显著提高模型性能，减少误报率，证明在OCT视网膜数据集上效果优异。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high
false predictions rates with deep learning models. This paper introduces
Class-Based Image Composition, an approach that allows us to reformulate
training inputs through a fusion of multiple images of the same class into
combined visual composites, named Composite Input Images (CoImg). That enhances
the intra-class variance and improves the valuable information density per
training sample and increases the ability of the model to distinguish between
subtle disease patterns. Our method was evaluated on the Optical Coherence
Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et
al., 2024), which contains 2,064 high-resolution optical coherence tomography
(OCT) scans of the human retina, representing seven distinct diseases with a
significant class imbalance. We constructed a perfectly class-balanced version
of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout
composite image. To assess the effectiveness of this new representation, we
conducted a comparative analysis between the original dataset and its variant
using a VGG16 model. A fair comparison was ensured by utilizing the identical
model architecture and hyperparameters for all experiments. The proposed
approach markedly improved diagnostic results.The enhanced Dataset achieved
near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared
to a baseline model trained on raw dataset. The false prediction rate was also
significantly lower, this demonstrates that the method can producehigh-quality
predictions even for weak datasets affected by class imbalance or small sample
size.

</details>


### [7] [I Detect What I Don't Know: Incremental Anomaly Learning with Stochastic Weight Averaging-Gaussian for Oracle-Free Medical Imaging](https://arxiv.org/abs/2511.03912)
*Nand Kumar Yadav,Rodrigue Rizk,William CW Chen,KC Santosh*

Main category: cs.CV

TL;DR: 提出一种基于冻结骨干+轻量适配器、核心集k-NN评分和不确定性门控的无监督增量正常样本扩充框架，在不依赖异常标签的前提下显著提升医疗影像异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 医疗影像中异常样本稀缺且专家标注昂贵，传统有监督或基于重构的方法在真实场景中受限。作者希望提出一种高效、实用且不依赖异常标签的增量学习框架，在不引入漂移和错误纳入的前提下扩充正常样本库。

Method: 方法包括：1) 冻结的预训练视觉骨干网络+小型卷积适配器进行快速域适配；2) 提取嵌入并存储为紧凑的核心集用于高效k-NN异常评分；3) 双重概率门控机制——基于z分数距离阈值和SWAG估计的元不确定性阈值，确保只有符合安全界限的样本才被并入正常内存；4) 交替进行适配器更新与准入判断，实现增量扩展。

Result: 在多个医疗影像数据集上显著提升异常检测性能：COVID-CXR上ROC-AUC从0.9489提升到0.9982（F1从0.8048到0.9746）；Pneumonia CXR上ROC-AUC从0.6834到0.8968；Brain MRI ND-5上ROC-AUC从0.6041到0.7269，PR-AUC从0.7539到0.8211。

Conclusion: 该论文提出了一种无监督、无需异常标签的增量正常样本扩充框架，通过小规模验证正常种子样本、轻量适配器更新和基于不确定性的样本准入控制来不断细化“正常性”定义，避免了生成重构和回放缓冲区等方法的缺陷。

Abstract: Unknown anomaly detection in medical imaging remains a fundamental challenge
due to the scarcity of labeled anomalies and the high cost of expert
supervision. We introduce an unsupervised, oracle-free framework that
incrementally expands a trusted set of normal samples without any anomaly
labels. Starting from a small, verified seed of normal images, our method
alternates between lightweight adapter updates and uncertainty-gated sample
admission. A frozen pretrained vision backbone is augmented with tiny
convolutional adapters, ensuring rapid domain adaptation with negligible
computational overhead. Extracted embeddings are stored in a compact coreset
enabling efficient k-nearest neighbor anomaly (k-NN) scoring. Safety during
incremental expansion is enforced by dual probabilistic gates, a sample is
admitted into the normal memory only if its distance to the existing coreset
lies within a calibrated z-score threshold, and its SWAG-based epistemic
uncertainty remains below a seed-calibrated bound. This mechanism prevents
drift and false inclusions without relying on generative reconstruction or
replay buffers. Empirically, our system steadily refines the notion of
normality as unlabeled data arrive, producing substantial gains over baselines.
On COVID-CXR, ROC-AUC improves from 0.9489 to 0.9982 (F1: 0.8048 to 0.9746); on
Pneumonia CXR, ROC-AUC rises from 0.6834 to 0.8968; and on Brain MRI ND-5,
ROC-AUC increases from 0.6041 to 0.7269 and PR-AUC from 0.7539 to 0.8211. These
results highlight the effectiveness and efficiency of the proposed framework
for real-world, label-scarce medical imaging applications.

</details>


### [8] [Adaptive Temporal Refinement: Continuous Depth Allocation and Distance Regression for Efficient Action Localization](https://arxiv.org/abs/2511.03943)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 论文通过BDR（带符号距离回归）和ATR（可微的连续深度选择）两项方法，提高了时序动作定位的边界精度并实现按需计算分配，在精度和计算效率间取得实质性改进，且易于集成和训练成本可控。


<details>
  <summary>Details</summary>
Motivation: 现有时序动作定位方法对所有边界采用统一计算，但边界难度差异大，导致资源浪费或精度不足。需要一种既能提高边界定位精度又能按需分配计算的机制。

Method: BDR将边界检测任务从分类改为回归预测每帧到最近动作边界的带符号距离，以获得更尖锐的边界峰值；ATR通过引入连续深度因子τ∈[0,1]实现按样本动态计算深度，并通过可微分机制进行端到端训练，避免了强化学习的复杂性。同时使用知识蒸馏来降低训练成本并保留性能。BDR可通过少量代码集成到现有方法。

Result: BDR在多种架构上能带来1.8–3.1% mAP@0.7的稳定提升并使边界峰值更尖锐43%；ATR在THUMOS14上以162G FLOPs达到56.5% mAP@0.7，相较于均匀处理的198G FLOPs/53.6% mAP@0.7提升2.9%且节省18%计算。对短动作、边界异质性大的情况提升更明显（短动作上提高4.2%）。通过知识蒸馏，轻量学生模型以基线计算成本保留了99%性能。多数据集上进行了严格统计检验验证。

Conclusion: 该论文提出了两项互补贡献：Boundary Distance Regression（BDR）通过有符号距离回归替代分类实现更精确的边界定位，和Adaptive Temporal Refinement（ATR）通过可微的连续深度选择按需分配计算，从而在减少计算的同时提高定位精度。

Abstract: Temporal action localization requires precise boundary detection; however,
current methods apply uniform computation despite significant variations in
difficulty across boundaries. We present two complementary contributions.
First, Boundary Distance Regression (BDR) provides information-theoretically
optimal localization through signed-distance regression rather than
classification, achieving 43\% sharper boundary peaks. BDR retrofits to
existing methods with approximately 50 lines of code, yielding consistent 1.8
to 3.1\% mAP@0.7 improvements across diverse architectures. Second, Adaptive
Temporal Refinement (ATR) allocates computation via continuous depth selection
$\tau \in [0,1]$, enabling end-to-end differentiable optimization without
reinforcement learning. On THUMOS14, ATR achieves 56.5\% mAP@0.7 at 162G FLOPs,
compared to 53.6\% at 198G for uniform processing, providing a 2.9\%
improvement with 18\% less compute. Gains scale with boundary heterogeneity,
showing 4.2\% improvement on short actions. Training cost is mitigated via
knowledge distillation, with lightweight students retaining 99\% performance at
baseline cost. Results are validated across four benchmarks with rigorous
statistical testing.

</details>


### [9] [Improving Multi-View Reconstruction via Texture-Guided Gaussian-Mesh Joint Optimization](https://arxiv.org/abs/2511.03950)
*Zhejia Cai,Puhua Jiang,Shiwei Mao,Hongkun Cao,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出Gaussian引导的网格可微渲染框架，统一优化网格几何与顶点颜色，兼顾几何精度与渲染真实感，便于下游编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在多视角立体几何精度和新视角光线合成真实感之间二选一，且常将几何与外观分开优化，导致得到的模型难以用于下游编辑（如重光照、形变）。因此需要一个能同时优化几何与外观、兼顾两者的统一方法。

Method: 引入Gaussian-guided mesh differentiable rendering，将高斯表示与可微网格渲染结合，在多视角图像的光度一致性约束下同时优化网格顶点位置与面结构，并利用法线图和深度图作为几何正则化项；优化目标包含光度重投影损失与几何正则化损失，从而实现联合几何-外观优化。

Result: 在合成与实景数据集上，方法能生成高质量几何与可直接编辑的纹理化网格，实验表明在几何重建精度和新视角渲染真实感上均优于或接近多种对比方法；并展示了重光照与形变等下游编辑应用。

Conclusion: 该论文提出了一个统一优化几何和外观的框架，通过高斯引导的网格可微渲染同时优化顶点位置、面片和顶点颜色，克服了以往方法在几何精度与光度真实感之间的权衡。

Abstract: Reconstructing real-world objects from multi-view images is essential for
applications in 3D editing, AR/VR, and digital content creation. Existing
methods typically prioritize either geometric accuracy (Multi-View Stereo) or
photorealistic rendering (Novel View Synthesis), often decoupling geometry and
appearance optimization, which hinders downstream editing tasks. This paper
advocates an unified treatment on geometry and appearance optimization for
seamless Gaussian-mesh joint optimization. More specifically, we propose a
novel framework that simultaneously optimizes mesh geometry (vertex positions
and faces) and vertex colors via Gaussian-guided mesh differentiable rendering,
leveraging photometric consistency from input images and geometric
regularization from normal and depth maps. The obtained high-quality 3D
reconstruction can be further exploit in down-stream editing tasks, such as
relighting and shape deformation. The code will be publicly available upon
acceptance.

</details>


### [10] [A Linear Fractional Transformation Model and Calibration Method for Light Field Camera](https://arxiv.org/abs/2511.03962)
*Zhong Chen,Changfeng Chen*

Main category: cs.CV

TL;DR: 论文提出以线性分式变换α解耦主镜头与MLA的光场相机内参模型，结合最小二乘解析解与非线性精细化，并给出原始图像特征检测方法，在真实与仿真数据上验证并加速了光场图像模拟。


<details>
  <summary>Details</summary>
Motivation: 内参准确标定是基于光场相机进行三维重建的前提，但主镜头与微透镜阵列的耦合使得标定复杂且不稳定，因此引入α参数以实现解耦，从而简化建模与优化。

Method: 先建立包含LFT参数α的光场相机成像模型，推导出线性的最小二乘解析解作为初值，然后采用非线性优化（可能为Levenberg-Marquardt或梯度下降）对参数进行精细化；同时提出了从原始光场图像检测特征的具体方法。

Result: 在物理与模拟数据上的实验验证了方法的有效性；基于该模型的原始光场图像模拟速度提高，有利于生成训练数据以支持数据驱动的深度学习方法；作者提供了代码以便复现。

Conclusion: 该论文提出了一个用于光场相机内参标定的线性分式变换(LFT)参数α，有效地将主镜头与微透镜阵列(MLA)解耦；通过解析最小二乘解与非线性精细化相结合的流程，提升了标定精度。

Abstract: Accurate calibration of internal parameters is a crucial yet challenging
prerequisite for 3D reconstruction using light field cameras. In this paper, we
propose a linear fractional transformation(LFT) parameter $\alpha$ to decoupled
the main lens and micro lens array (MLA). The proposed method includes an
analytical solution based on least squares, followed by nonlinear refinement.
The method for detecting features from the raw images is also introduced.
Experimental results on both physical and simulated data have verified the
performance of proposed method. Based on proposed model, the simulation of raw
light field images becomes faster, which is crucial for data-driven deep
learning methods. The corresponding code can be obtained from the author's
website.

</details>


### [11] [Room Envelopes: A Synthetic Dataset for Indoor Layout Reconstruction from Images](https://arxiv.org/abs/2511.03970)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 提出Room Envelopes数据集，提供可见与结构两类点图，支持单目网络同时预测可见表面与去除装饰后的布局表面，提升场景完整重建能力。


<details>
  <summary>Details</summary>
Motivation: 现有重建方法只能恢复可见表面，遮挡面丢失；场景结构（平面、重复、简单）本应易于预测，因此用更廉价的方法恢复完整场景布局具有实际价值。

Method: 构建合成数据集：每张RGB图像配套两个点图（可见表面与去除装饰后首个结构表面）。训练单目几何估计器进行两类点图预测，实现直接监督。

Result: 引入Room Envelopes数据集并通过实验表明，使用该监督可让模型同时预测可见表面与结构布局表面，从而获得对场景范围及物体形状位置的理解。

Conclusion: 本论文提出了用于场景结构（墙、地板、天花板）重建的合成数据集Room Envelopes，并展示了如何用单目前馈网络分别监督可见表面和结构布局表面的预测，从而恢复场景范围与物体形状位置。

Abstract: Modern scene reconstruction methods are able to accurately recover 3D
surfaces that are visible in one or more images. However, this leads to
incomplete reconstructions, missing all occluded surfaces. While much progress
has been made on reconstructing entire objects given partial observations using
generative models, the structural elements of a scene, like the walls, floors
and ceilings, have received less attention. We argue that these scene elements
should be relatively easy to predict, since they are typically planar,
repetitive and simple, and so less costly approaches may be suitable. In this
work, we present a synthetic dataset -- Room Envelopes -- that facilitates
progress on this task by providing a set of RGB images and two associated
pointmaps for each image: one capturing the visible surface and one capturing
the first surface once fittings and fixtures are removed, that is, the
structural layout. As we show, this enables direct supervision for feed-forward
monocular geometry estimators that predict both the first visible surface and
the first layout surface. This confers an understanding of the scene's extent,
as well as the shape and location of its objects.

</details>


### [12] [Simple 3D Pose Features Support Human and Machine Social Scene Understanding](https://arxiv.org/abs/2511.03988)
*Wenshuo Qin,Leyla Isik*

Main category: cs.CV

TL;DR: TL;DR: 人类依赖显式的3D体态（尤其是人脸位置与朝向）来判断社交互动。简单的3D社交体态特征既能解释人类判断，又能提升并预测现有视觉模型的性能。


<details>
  <summary>Details</summary>
Motivation: 动机是探究为何当前AI视觉模型在社交互动识别上远不及人类，假设差异来自于人类利用的显式3D视觉空间位置信息，而大多数AI模型缺乏或未表征这些信息。

Method: 方法上，作者使用最先进的姿态估计与深度估计算法从短视频中提取3D关节点位置，比较这些3D关节数据与多种现成视觉模型（包括其层级嵌入）在预测人类社交互动判断上的性能；并构建了一个压缩的3D社交体态特征集（主要为面部位置与朝向）以测试其解释力和与视觉模型的结合效果。

Result: 结果显示：1) 3D关节位置信息显著优于多数现有视觉模型；2) 极小的3D社交体态特征（面部位置與朝向）几乎匹配完整3D关节集的预测能力；3) 将这些3D特征与现成视觉模型的嵌入结合可提升模型预测；4) 模型内部对3D社交体态特征的表征程度能预测其匹配人类判断的能力。

Conclusion: 论文结论是：人类在识别人际互动时主要依赖显式的3D体态信息，尤其是人脸的3D位置与朝向；这些3D社交体态特征比大多数现有视觉模型的抽取特征更能预测人类的社交判断，并能显著提升现有模型的表现。

Abstract: Humans can quickly and effortlessly extract a variety of information about
others' social interactions from visual input, ranging from visuospatial cues
like whether two people are facing each other to higher-level information. Yet,
the computations supporting these abilities remain poorly understood, and
social interaction recognition continues to challenge even the most advanced AI
vision systems. Here, we hypothesized that humans rely on 3D visuospatial pose
information to make social interaction judgments, which is absent in most AI
vision models. To test this, we combined state-of-the-art pose and depth
estimation algorithms to extract 3D joint positions of people in short video
clips depicting everyday human actions and compared their ability to predict
human social interaction judgments with current AI vision models. Strikingly,
3D joint positions outperformed most current AI vision models, revealing that
key social information is available in explicit body position but not in the
learned features of most vision models, including even the layer-wise
embeddings of the pose models used to extract joint positions. To uncover the
critical pose features humans use to make social judgments, we derived a
compact set of 3D social pose features describing only the 3D position and
direction of faces in the videos. We found that these minimal descriptors
matched the predictive strength of the full set of 3D joints and significantly
improved the performance of off-the-shelf AI vision models when combined with
their embeddings. Moreover, the degree to which 3D social pose features were
represented in each off-the-shelf AI vision model predicted the model's ability
to match human social judgments. Together, our findings provide strong evidence
that human social scene understanding relies on explicit representations of 3D
pose and can be supported by simple, structured visuospatial primitives.

</details>


### [13] [CaRF: Enhancing Multi-View Consistency in Referring 3D Gaussian Splatting Segmentation](https://arxiv.org/abs/2511.03992)
*Yuwen Tao,Kanglei Zhou,Xin Tan,Yuan Xie*

Main category: cs.CV

TL;DR: CaRF通过在高斯场内引入相机感知编码与成对视角监督，解决了2D伪监督带来的跨视图不一致，显著提升了指代表达到3D区域的分割性能与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D渲染伪监督和视图特定特征学习的方法在跨视图一致性上存在缺陷，导致对语言表达的3D定位不稳定，需要一个直接在3D空间并显式建模相机几何与视图依赖性的解决方案。

Method: 提出Camera Aware Referring Field (CaRF)，包括Gaussian Field Camera Encoding (GFCE)将相机几何信息注入高斯-文本交互以建模视图依赖变化，以及In Training Paired View Supervision (ITPVS)在训练期间对齐校准视角下每个高斯体元的logits以避免单视图过拟合。整个框架在3D高斯空间中可微，直接优化多视图一致性。

Result: 在Ref LERF、LERF OVS和3D OVS三套基准上，CaRF在mIoU上分别较最先进方法提升平均16.8%、4.3%和2.0%；整体表现证明了更可靠和视图一致的3D场景理解能力。

Conclusion: CaRF通过在3D高斯场中直接做跨视图一致的语言-几何对齐，有效提升了指代3D高斯渲染分割的性能，尤其在多视角一致性和几何推理上优于现有方法。

Abstract: Referring 3D Gaussian Splatting Segmentation (R3DGS) aims to interpret
free-form language expressions and localize the corresponding 3D regions in
Gaussian fields. While recent advances have introduced cross-modal alignment
between language and 3D geometry, existing pipelines still struggle with
cross-view consistency due to their reliance on 2D rendered pseudo supervision
and view specific feature learning. In this work, we present Camera Aware
Referring Field (CaRF), a fully differentiable framework that operates directly
in the 3D Gaussian space and achieves multi view consistency. Specifically,
CaRF introduces Gaussian Field Camera Encoding (GFCE), which incorporates
camera geometry into Gaussian text interactions to explicitly model view
dependent variations and enhance geometric reasoning. Building on this, In
Training Paired View Supervision (ITPVS) is proposed to align per Gaussian
logits across calibrated views during training, effectively mitigating single
view overfitting and exposing inter view discrepancies for optimization.
Extensive experiments on three representative benchmarks demonstrate that CaRF
achieves average improvements of 16.8%, 4.3%, and 2.0% in mIoU over state of
the art methods on the Ref LERF, LERF OVS, and 3D OVS datasets, respectively.
Moreover, this work promotes more reliable and view consistent 3D scene
understanding, with potential benefits for embodied AI, AR/VR interaction, and
autonomous perception.

</details>


### [14] [PhysCorr: Dual-Reward DPO for Physics-Constrained Text-to-Video Generation with Automated Preference Selection](https://arxiv.org/abs/2511.03997)
*Peiyao Wang,Weining Wang,Qi Li*

Main category: cs.CV

TL;DR: 引入面向物理一致性的奖励模型与直接偏好优化，显著改善文本到视频生成的物理合理性，且方法可扩展到多种模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成虽在感知质量上进步显著，但常常违反物理规律，导致物体动力学和交互不合理，阻碍在机器人和仿真等领域的应用，因此需要提升物理一致性。

Method: 提出PhysicsRM双维度奖励模型（评估物体内部稳定性与物体间交互）并构建PhyDPO优化管线，利用对比反馈与物理感知重加权进行直接偏好优化，可与多种生成骨干无缝集成。

Result: 在多个基准上，PhysCorr在保持视觉逼真性与语义对齐的同时，显著提升了物理真实感。

Conclusion: 该论文提出了PhysCorr框架，通过物理一致性奖励模型和直接偏好优化提升文本到视频生成的物理合理性。

Abstract: Recent advances in text-to-video generation have achieved impressive
perceptual quality, yet generated content often violates fundamental principles
of physical plausibility - manifesting as implausible object dynamics,
incoherent interactions, and unrealistic motion patterns. Such failures hinder
the deployment of video generation models in embodied AI, robotics, and
simulation-intensive domains. To bridge this gap, we propose PhysCorr, a
unified framework for modeling, evaluating, and optimizing physical consistency
in video generation. Specifically, we introduce PhysicsRM, the first
dual-dimensional reward model that quantifies both intra-object stability and
inter-object interactions. On this foundation, we develop PhyDPO, a novel
direct preference optimization pipeline that leverages contrastive feedback and
physics-aware reweighting to guide generation toward physically coherent
outputs. Our approach is model-agnostic and scalable, enabling seamless
integration into a wide range of video diffusion and transformer-based
backbones. Extensive experiments across multiple benchmarks demonstrate that
PhysCorr achieves significant improvements in physical realism while preserving
visual fidelity and semantic alignment. This work takes a critical step toward
physically grounded and trustworthy video generation.

</details>


### [15] [GNN-MoE: Context-Aware Patch Routing using GNNs for Parameter-Efficient Domain Generalization](https://arxiv.org/abs/2511.04008)
*Mahmoud Soliman,Omar Abdelaziz,Ahmed Radwan,Anand,Mohamed Shehata*

Main category: cs.CV

TL;DR: 用GNN对ViT补丁建图并基于图路由的MoE进行参数高效微调，提升域泛化性能且参数开销小。


<details>
  <summary>Details</summary>
Motivation: 传统对预训练ViT的微调整体昂贵且可能损害泛化性能，现有MoE多采用基于token的路由忽视补丁间关系。通过引入图结构上的上下文路由，有望在保证参数高效的同时增强对域外数据的鲁棒性。

Method: 在PEFT框架下构建Mixture-of-Experts，使用高效Kronecker adapter作为专家模块；引入基于补丁间关系的GNN路由器（GCN、GAT、SAGE）替代传统的基于token的路由器，实现按补丁动态分配到不同专家；整个方法只微调少量参数以保持高效性。

Result: 在若干域泛化基准（论文声称）达到SOTA或与SOTA相当，并显著降低可训练参数量，验证了GNN路由相较于token路由在适应域移位时更具优势。

Conclusion: 该论文提出GNN-MoE，在ViT微调中通过图神经网络路由对补丁之间关系进行建模，以实现参数高效且对域外泛化鲁棒的专家分配。实验显示在DG基准上取得领先或有竞争力的结果，证明图上下文路由在轻量级泛化适配中的有效性。

Abstract: Domain generalization (DG) seeks robust Vision Transformer (ViT) performance
on unseen domains. Efficiently adapting pretrained ViTs for DG is challenging;
standard fine-tuning is costly and can impair generalization. We propose
GNN-MoE, enhancing Parameter-Efficient Fine-Tuning (PEFT) for DG with a
Mixture-of-Experts (MoE) framework using efficient Kronecker adapters. Instead
of token-based routing, a novel Graph Neural Network (GNN) router (GCN, GAT,
SAGE) operates on inter-patch graphs to dynamically assign patches to
specialized experts. This context-aware GNN routing leverages inter-patch
relationships for better adaptation to domain shifts. GNN-MoE achieves
state-of-the-art or competitive DG benchmark performance with high parameter
efficiency, highlighting the utility of graph-based contextual routing for
robust, lightweight DG.

</details>


### [16] [MedDChest: A Content-Aware Multimodal Foundational Vision Model for Thoracic Imaging](https://arxiv.org/abs/2511.04016)
*Mahmoud Soliman,Islam Osman,Mohamed S. Shehata,Rasika Rajapakshe*

Main category: cs.CV

TL;DR: 针对胸部影像从头训练的ViT（MedDChest）配合解剖引导的随机裁剪增强，在超过120万张多模态医学图像上预训练，从而在多项诊断任务上显著超越ImageNet预训练模型，权重将开源。


<details>
  <summary>Details</summary>
Motivation: 现有常用做法是微调在自然图像（ImageNet）上预训练的backbone，但自然图像与医学影像存在显著域差异，导致性能受限；因此需要在域内进行大规模预训练并设计适合医学影像的增强策略。

Method: 从头在一个由10个公开数据源汇编、包含超过120万张多模态影像（胸部X光与CT）的数据集上训练ViT模型，提出Guided Random Resized Crops（基于解剖感兴趣区的采样策略）作为核心增强方法，并在多种下游任务上微调验证效果。

Result: 实验表明，MedDChest在多个胸部诊断下游任务上显著优于强基线（公开可用的ImageNet预训练模型），证明域内大规模预训练与Guided Random Resized Crops的有效性；且模型权重将开源以便社区使用。

Conclusion: 作者证明了在胸部医学影像上进行大规模、域内预训练并结合领域特定的数据增强（Guided Random Resized Crops）能显著优于ImageNet预训练模型，从而提升下游诊断任务的表现。

Abstract: The performance of vision models in medical imaging is often hindered by the
prevailing paradigm of fine-tuning backbones pre-trained on out-of-domain
natural images. To address this fundamental domain gap, we propose MedDChest, a
new foundational Vision Transformer (ViT) model optimized specifically for
thoracic imaging. We pre-trained MedDChest from scratch on a massive, curated,
multimodal dataset of over 1.2 million images, encompassing different
modalities including Chest X-ray and Computed Tomography (CT) compiled from 10
public sources. A core technical contribution of our work is Guided Random
Resized Crops, a novel content-aware data augmentation strategy that biases
sampling towards anatomically relevant regions, overcoming the inefficiency of
standard cropping techniques on medical scans. We validate our model's
effectiveness by fine-tuning it on a diverse set of downstream diagnostic
tasks. Comprehensive experiments empirically demonstrate that MedDChest
significantly outperforms strong, publicly available ImageNet-pretrained
models. By establishing the superiority of large-scale, in-domain pre-training
combined with domain-specific data augmentation, MedDChest provides a powerful
and robust feature extractor that serves as a significantly better starting
point for a wide array of thoracic diagnostic tasks. The model weights will be
made publicly available to foster future research and applications.

</details>


### [17] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出无需提取等值面的高保真稀疏体素表示 Faithful Contouring 及其双模自编码器，支持 2048+ 分辨率，显著提升表示与重建精度，兼具编辑与纹理灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的体素化方法依赖封闭化或渲染优化，导致几何保真度下降，无法高效保留尖锐边与内部拓扑结构，因此需要一种无需等值面提取且高保真的表示。

Method: 提出 Faithful Contouring 表示和一个双模自编码器。该表示避免了等值面提取和场函数转换，通过稀疏体素化直接编码网格几何，支持高分辨率（2048+）；自编码器有两种模式以实现可扩展的细节保留重建。

Result: 在表示任务上误差达到 1e-5 量级；在网格重建上，比强基线在 Chamfer Distance 上降低 93%，F-score 提升 35%，并在效率与灵活性上优于现有方法。

Conclusion: Faithful Contouring 提出了一种稀疏体素化表示，能够在不需转换为场函数或提取等值面情况下，保持高保真网格表示，支持 2048+ 分辨率并保留尖锐边缘和内部结构，适用于纹理、编辑和细节保留的重建。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [18] [A Hybrid Deep Learning Model for Robust Biometric Authentication from Low-Frame-Rate PPG Signals](https://arxiv.org/abs/2511.04037)
*Arfina Rahman,Mahesh Banavar*

Main category: cs.CV

TL;DR: 基于低帧率指尖视频的PPG生物认证：用PCA等预处理+CWT得到时频图，提出CVT-ConvMixer-LSTM混合模型，在46人数据上达98%准确率，适合移动/嵌入式应用。


<details>
  <summary>Details</summary>
Motivation: PPG具备非侵入、活体检测和低成本可穿戴设备适配性，但受运动伪影、光照变化和个体差异影响，需设计鲁棒且轻量的特征提取与分类方法以用于实际移动/嵌入式认证。

Method: 将原始PPG信号经过基线漂移去除、PCA抑制运动伪影、带通滤波、傅里叶重采样和幅值归一化等预处理；将1D PPG信号转为通过连续小波变换（CWT）得到的二维时频标度图；提出混合深度模型CVT-ConvMixer-LSTM，融合CVT和ConvMixer提取空间特征，并用LSTM捕捉时序特征。

Result: 在CFIHSR数据集（46名，被试，14 Hz采样）上，经过上述预处理和CWT表示，所提CVT-ConvMixer-LSTM模型达到98%认证准确率，显示出对噪声和被试间变异的鲁棒性。

Conclusion: 本文提出了一种基于低帧率指尖视频的轻量级PPG生物认证框架，实验证明在46名被试上的认证准确率达98%，表明方法对噪声和个体差异具有鲁棒性，适合移动和嵌入式场景。

Abstract: Photoplethysmography (PPG) signals, which measure changes in blood volume in
the skin using light, have recently gained attention in biometric
authentication because of their non-invasive acquisition, inherent liveness
detection, and suitability for low-cost wearable devices. However, PPG signal
quality is challenged by motion artifacts, illumination changes, and
inter-subject physiological variability, making robust feature extraction and
classification crucial. This study proposes a lightweight and cost-effective
biometric authentication framework based on PPG signals extracted from
low-frame-rate fingertip videos. The CFIHSR dataset, comprising PPG recordings
from 46 subjects at a sampling rate of 14 Hz, is employed for evaluation. The
raw PPG signals undergo a standard preprocessing pipeline involving baseline
drift removal, motion artifact suppression using Principal Component Analysis
(PCA), bandpass filtering, Fourier-based resampling, and amplitude
normalization. To generate robust representations, each one-dimensional PPG
segment is converted into a two-dimensional time-frequency scalogram via the
Continuous Wavelet Transform (CWT), effectively capturing transient
cardiovascular dynamics. We developed a hybrid deep learning model, termed
CVT-ConvMixer-LSTM, by combining spatial features from the Convolutional Vision
Transformer (CVT) and ConvMixer branches with temporal features from a Long
Short-Term Memory network (LSTM). The experimental results on 46 subjects
demonstrate an authentication accuracy of 98%, validating the robustness of the
model to noise and variability between subjects. Due to its efficiency,
scalability, and inherent liveness detection capability, the proposed system is
well-suited for real-world mobile and embedded biometric security applications.

</details>


### [19] [Unveiling Deep Semantic Uncertainty Perception for Language-Anchored Multi-modal Vision-Brain Alignment](https://arxiv.org/abs/2511.04078)
*Zehui Feng,Chenqi Zhang,Mingru Wang,Minuo Wei,Shiwei Cheng,Cuntai Guan,Ting Han*

Main category: cs.CV

TL;DR: Bratrix利用语言锚定的层级语义解耦与不确定性感知模块，改进视觉-脑对齐，显著提升EEG/MEG/fMRI检索与重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将脑信号与视觉表征对齐，但视觉表征难以覆盖潜在语义维度且可解释性和鲁棒性不足；需借助语言语义增强对齐并处理神经信号噪声与个体差异。

Method: 提出端到端框架Bratrix，解耦视觉刺激为层级视觉与语言语义成分，构建共享潜在空间映射视觉与脑表征，并设计不确定性感知模块进行权重调整；引入可学习的语言锚定语义矩阵并采用先单模态预训练再多模态微调的二阶段训练策略。

Result: 在EEG、MEG、fMRI基准上，Bratrix在检索、重建与图像描述任务上均优于现有方法；在200类EEG检索任务上提升约14.3%。模型与代码已公开。

Conclusion: Bratrix通过语言锚定的多模态对齐显著提升了视觉-脑信号语义对齐的精度和鲁棒性。

Abstract: Unveiling visual semantics from neural signals such as EEG, MEG, and fMRI
remains a fundamental challenge due to subject variability and the entangled
nature of visual features. Existing approaches primarily align neural activity
directly with visual embeddings, but visual-only representations often fail to
capture latent semantic dimensions, limiting interpretability and deep
robustness. To address these limitations, we propose Bratrix, the first
end-to-end framework to achieve multimodal Language-Anchored Vision-Brain
alignment. Bratrix decouples visual stimuli into hierarchical visual and
linguistic semantic components, and projects both visual and brain
representations into a shared latent space, enabling the formation of aligned
visual-language and brain-language embeddings. To emulate human-like perceptual
reliability and handle noisy neural signals, Bratrix incorporates a novel
uncertainty perception module that applies uncertainty-aware weighting during
alignment. By leveraging learnable language-anchored semantic matrices to
enhance cross-modal correlations and employing a two-stage training strategy of
single-modality pretraining followed by multimodal fine-tuning, Bratrix-M
improves alignment precision. Extensive experiments on EEG, MEG, and fMRI
benchmarks demonstrate that Bratrix improves retrieval, reconstruction, and
captioning performance compared to state-of-the-art methods, specifically
surpassing 14.3% in 200-way EEG retrieval task. Code and model are available.

</details>


### [20] [Adversarial and Score-Based CT Denoising: CycleGAN vs Noise2Score](https://arxiv.org/abs/2511.04083)
*Abu Hanif Muhammad Syarubany*

Main category: cs.CV

TL;DR: 在无配对/自监督 CT 降噪场景下，CycleGAN（U-Net 骨干）给出最佳重建质量，Noise2Score 是无配对情况下的稳健可行替代。


<details>
  <summary>Details</summary>
Motivation: 目标是在无配对或无清洁配对的条件下提升 CT 图像质量，评估在数据受限情形下哪种自监督或无配对方法更有效。

Method: 比较两种训练数据高效的范式：基于 CycleGAN 的残差翻译器（使用标准 U-Net 骨干，lambda_cycle=30, lambda_iden=2, ngf=ndf=64，并进行长训练调度）和基于分数匹配的 Noise2Score 自监督去噪器。采用统一评估协议与配置扫描选择最可靠的 CycleGAN 设置，并对两方法在 PSNR、SSIM 与排行榜评分上进行对比。

Result: 所选 CycleGAN 将输入噪声从 34.66 dB / 0.9234 SSIM 提升到 38.913 dB / 0.971 SSIM，估计分数 1.9441，未见集合（Kaggle 排行榜）得分 1.9343。Noise2Score 在绝对 PSNR/SSIM 上略逊一筹，但在非常嘈杂输入上取得显著增益。

Conclusion: CycleGAN 在无配对自监督 CT 降噪中提供了最高的图像质量，而 Noise2Score 在没有干净配对时作为稳健替代方案表现有竞争力。

Abstract: We study CT image denoising in the unpaired and self-supervised regimes by
evaluating two strong, training-data-efficient paradigms: a CycleGAN-based
residual translator and a Noise2Score (N2S) score-matching denoiser. Under a
common evaluation protocol, a configuration sweep identifies a simple standard
U-Net backbone within CycleGAN (lambda_cycle = 30, lambda_iden = 2, ngf = ndf =
64) as the most reliable setting; we then train it to convergence with a longer
schedule. The selected CycleGAN improves the noisy input from 34.66 dB / 0.9234
SSIM to 38.913 dB / 0.971 SSIM and attains an estimated score of 1.9441 and an
unseen-set (Kaggle leaderboard) score of 1.9343. Noise2Score, while slightly
behind in absolute PSNR / SSIM, achieves large gains over very noisy inputs,
highlighting its utility when clean pairs are unavailable. Overall, CycleGAN
offers the strongest final image quality, whereas Noise2Score provides a robust
pair-free alternative with competitive performance. Source code is available at
https://github.com/hanifsyarubany/CT-Scan-Image-Denoising-using-CycleGAN-and-Noise2Score.

</details>


### [21] [When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation](https://arxiv.org/abs/2511.04084)
*Nishchal Sapkota,Haoyan Shi,Yejia Zhang,Xianshi Ma,Bofang Zheng,Danny Z. Chen*

Main category: cs.CV

TL;DR: 通过在Swin Transformer中引入有理函数的GR-KANs，UKAST实现了更表达性和数据高效的医学图像分割，在多项基准上超越现有方法，尤其适合数据稀缺场景。


<details>
  <summary>Details</summary>
Motivation: 传统CNN擅长局部特征但难以建模长程依赖；Transformer能捕捉全局上下文但数据需求大且计算昂贵。目标是结合两者优点，提升数据稀缺情形下的分割性能。

Method: 提出一种U-Net结构的架构（UKAST），在Swin Transformer编码器中替换或增强为Group Rational KANs (GR-KANs) 和有理基函数，借鉴Kolmogorov-Arnold Transformer (KAT) 的设计，以提高表示能力并降低对大量训练数据的依赖。

Result: 在四个不同的2D和3D医学图像分割数据集上，UKAST在各项指标上超越了CNN和Transformer基线，特别是在样本稀缺情况下表现更优；模型FLOPs下降，参数量仅略增。

Conclusion: UKAST通过在Swin Transformer编码器中集成基于有理函数的Kolmogorov-Arnold Networks (KANs)，在保持参数量几乎不变的情况下提升了表达能力和数据效率，降低了FLOPs并在四个2D/3D医学图像分割基准上取得SOTA表现。

Abstract: Medical image segmentation is critical for accurate diagnostics and treatment
planning, but remains challenging due to complex anatomical structures and
limited annotated training data. CNN-based segmentation methods excel at local
feature extraction, but struggle with modeling long-range dependencies.
Transformers, on the other hand, capture global context more effectively, but
are inherently data-hungry and computationally expensive. In this work, we
introduce UKAST, a U-Net like architecture that integrates rational-function
based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By
leveraging rational base functions and Group Rational KANs (GR-KANs) from the
Kolmogorov-Arnold Transformer (KAT), our architecture addresses the
inefficiencies of vanilla spline-based KANs, yielding a more expressive and
data-efficient framework with reduced FLOPs and only a very small increase in
parameter count compared to SwinUNETR. UKAST achieves state-of-the-art
performance on four diverse 2D and 3D medical image segmentation benchmarks,
consistently surpassing both CNN- and Transformer-based baselines. Notably, it
attains superior accuracy in data-scarce settings, alleviating the data-hungry
limitations of standard Vision Transformers. These results show the potential
of KAN-enhanced Transformers to advance data-efficient medical image
segmentation. Code is available at: https://github.com/nsapkota417/UKAST

</details>


### [22] [SpatialLock: Precise Spatial Control in Text-to-Image Synthesis](https://arxiv.org/abs/2511.04112)
*Biao Liu,Yuanzhi Liang*

Main category: cs.CV

TL;DR: SpatialLock通过注入位置信息并结合感知监督，实现了对生成图像中对象位置的高精度控制（IOU>0.9），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有T2I方法未能充分利用显式位置信息，导致对目标空间布局理解不足，难以实现精准的目标定位；因此需要一种方法将定位信号与生成网络紧密耦合以提升控制能力。

Method: 提出两大模块：Position-Engaged Injection (PoI) —— 在注意力层中直接注入位置编码/grounding信息以强化模型对空间信息的建模；Position-Guided Learning (PoG) —— 使用感知级别的监督（如检测器或掩码对齐损失）来进一步精细化目标位置。两者联合在生成过程中约束目标的空间分布。

Result: 实验表明，SpatialLock在多个数据集上实现了新的定位精度SOTA，IOU得分超过0.9，并提升了生成图像的视觉质量。

Conclusion: 该论文提出了SpatialLock，通过引入位置信息注入和位置引导学习来增强文本到图像生成模型对目标空间布局的掌控，从而实现高度精确的目标定位。

Abstract: Text-to-Image (T2I) synthesis has made significant advancements in recent
years, driving applications such as generating datasets automatically. However,
precise control over object localization in generated images remains a
challenge. Existing methods fail to fully utilize positional information,
leading to an inadequate understanding of object spatial layouts. To address
this issue, we propose SpatialLock, a novel framework that leverages perception
signals and grounding information to jointly control the generation of spatial
locations. SpatialLock incorporates two components: Position-Engaged Injection
(PoI) and Position-Guided Learning (PoG). PoI directly integrates spatial
information through an attention layer, encouraging the model to learn the
grounding information effectively. PoG employs perception-based supervision to
further refine object localization. Together, these components enable the model
to generate objects with precise spatial arrangements and improve the visual
quality of the generated images. Experiments show that SpatialLock sets a new
state-of-the-art for precise object positioning, achieving IOU scores above 0.9
across multiple datasets.

</details>


### [23] [Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration](https://arxiv.org/abs/2511.04117)
*Yunghee Lee,Byeonghyun Pak,Junwha Hong,Hoseong Kim*

Main category: cs.CV

TL;DR: 提出训练免费、多速率的Tortoise and Hare Guidance，通过对引导分支进行粗粒度积分并配合误差感知步长采样与尺度调度，实现显著加速（NFE↓≈30%）且保持高保真图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统CFG求解器在数值求解上未能利用附加引导项的冗余性，导致加速受限。观察到噪声估计与引导项对数值误差敏感性不同，提出利用该差异以减少计算量并加速采样。

Method: 在不训练模型的前提下，提出多速率积分策略：噪声估计（主分支）在原始细时间步上积分（tortoise），而附加引导项在粗时间步上积分（hare）；并设计了基于误差界的步长采样器和指导尺度调度器以稳定大幅外推。通过误差界分析与实验验证，比较了不同NFE下的生成质量。

Result: THG在不改动模型的情况下将NFE最多降低30%，生成质量几乎不降（ImageReward变化≤0.032），在相同计算预算下超越其他CFG基础的无训练加速方法，并开源了代码。

Conclusion: THG通过将CFG ODE重构为多速率系统，证明了额外引导项对数值误差更鲁棒，从而允许在粗步长上近似计算引导分支以减少计算开销，在保持生成质量的同时显著降低采样函数评估次数。

Abstract: In this paper, we propose Tortoise and Hare Guidance (THG), a training-free
strategy that accelerates diffusion sampling while maintaining high-fidelity
generation. We demonstrate that the noise estimate and the additional guidance
term exhibit markedly different sensitivity to numerical error by reformulating
the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our
error-bound analysis shows that the additional guidance branch is more robust
to approximation, revealing substantial redundancy that conventional solvers
fail to exploit. Building on this insight, THG significantly reduces the
computation of the additional guidance: the noise estimate is integrated with
the tortoise equation on the original, fine-grained timestep grid, while the
additional guidance is integrated with the hare equation only on a coarse grid.
We also introduce (i) an error-bound-aware timestep sampler that adaptively
selects step sizes and (ii) a guidance-scale scheduler that stabilizes large
extrapolation spans. THG reduces the number of function evaluations (NFE) by up
to 30% with virtually no loss in generation fidelity ($\Delta$ImageReward
$\leq$ 0.032) and outperforms state-of-the-art CFG-based training-free
accelerators under identical computation budgets. Our findings highlight the
potential of multirate formulations for diffusion solvers, paving the way for
real-time high-quality image synthesis without any model retraining. The source
code is available at https://github.com/yhlee-add/THG.

</details>


### [24] [Text to Sketch Generation with Multi-Styles](https://arxiv.org/abs/2511.04123)
*Tengjie Li,Shikui Tu,Lei Xu*

Main category: cs.CV

TL;DR: 提出M3S，一种训练免费扩散框架，通过线性平滑参考特征与风格-内容引导实现精确可控的单/多风格草图生成，显著降低内容泄露并提升质量。


<details>
  <summary>Details</summary>
Motivation: 现有草图生成方法缺乏对风格的精确控制，且在参考与目标结构差异大时易发生内容泄露，需一种训练免费且能精确对齐风格的方案。

Method: 在扩散模型中将参考特征作为线性平滑的辅助信息引入，配合风格-内容引导机制；通过联合AdaIN模块融合多参考草图特征以实现多风格生成。

Result: 在大量实验中证明方法能生成高质量草图，风格对齐准确，并在风格控制灵活性上有提升。

Conclusion: 提出了一种基于扩散模型且无训练的新框架，通过文本提示和参考风格草图实现显式风格引导，减少内容泄露并提升合成质量，支持多风格可控生成。

Abstract: Recent advances in vision-language models have facilitated progress in sketch
generation. However, existing specialized methods primarily focus on generic
synthesis and lack mechanisms for precise control over sketch styles. In this
work, we propose a training-free framework based on diffusion models that
enables explicit style guidance via textual prompts and referenced style
sketches. Unlike previous style transfer methods that overwrite key and value
matrices in self-attention, we incorporate the reference features as auxiliary
information with linear smoothing and leverage a style-content guidance
mechanism. This design effectively reduces content leakage from reference
sketches and enhances synthesis quality, especially in cases with low
structural similarity between reference and target sketches. Furthermore, we
extend our framework to support controllable multi-style generation by
integrating features from multiple reference sketches, coordinated via a joint
AdaIN module. Extensive experiments demonstrate that our approach achieves
high-quality sketch generation with accurate style alignment and improved
flexibility in style control. The official implementation of M3S is available
at https://github.com/CMACH508/M3S.

</details>


### [25] [Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)](https://arxiv.org/abs/2511.04126)
*Venkata Manikanta Desu,Syed Fawaz Ali*

Main category: cs.CV

TL;DR: 提出一个基于YOLOv8、定制YOLOv5和ResNet50的实时网球分析管线，能在多种场景下稳健检测与跟踪，输出可视化视频和详细分析指标，助力教练与转播。


<details>
  <summary>Details</summary>
Motivation: 为教练、转播和球员提供一种自动化工具，实时生成比赛数据与战术洞察，减少人工标注成本并提升分析效率。

Method: 集成多种深度学习模型：使用YOLOv8检测球员、自定义训练的YOLOv5跟踪球，基于ResNet50的架构检测球场关键点；系统将检测结果结合追踪算法与速度/位置计算生成运动与击球分析。

Result: 实验结果显示在不同场地条件和比赛场景下表现稳健，能够输出标注视频与详细的运动学与技术指标，如球速、击球准确率与球员反应时间。

Conclusion: 该论文提出了一个实时网球比赛自动分析的完整流程，能检测并跟踪球员、网球与球场关键点，输出可视化视频与详细性能指标，适用于教练、转播与运动员分析。

Abstract: This study presents a complete pipeline for automated tennis match analysis.
Our framework integrates multiple deep learning models to detect and track
players and the tennis ball in real time, while also identifying court
keypoints for spatial reference. Using YOLOv8 for player detection, a
custom-trained YOLOv5 model for ball tracking, and a ResNet50-based
architecture for court keypoint detection, our system provides detailed
analytics including player movement patterns, ball speed, shot accuracy, and
player reaction times. The experimental results demonstrate robust performance
in varying court conditions and match scenarios. The model outputs an annotated
video along with detailed performance metrics, enabling coaches, broadcasters,
and players to gain actionable insights into the dynamics of the game.

</details>


### [26] [DMSORT: An efficient parallel maritime multi-object tracking architecture for unmanned vessel platforms](https://arxiv.org/abs/2511.04128)
*Shengyu Tang,Zeyuan Lu,Jiazhi Dong,Changdong Yu,Xiaoyu Wang,Yaohui Lyu,Weihao Xia*

Main category: cs.CV

TL;DR: DMSORT通过并行的检测/ReID与运动补偿分支及特征融合，有效分离平台运动并补偿卡尔曼滤波，提升海事场景MOT的速度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 海事环境导致相机抖动、视觉退化，传统MOT难以区分平台运动与目标真实运动，影响跟踪稳定性和ID一致性。需设计高效鲁棒的方法应对抖动、遮挡和漂移。

Method: 框架由并行两分支组成：检测+ReID分支使用RCDN和轻量Transformer外观提取器（Li-TAE）；运动分支通过估计仿射/射影变换分离平台运动与目标运动，并在卡尔曼滤波中进行补偿；最后用聚类优化的特征融合模块结合外观与运动信息进行关联。

Result: 在Singapore Maritime Dataset上实验，DMSORT在保持高身份一致性和对抖动/遮挡鲁棒性的同时，取得了SOTA性能，并成为ReID-based MOT方法中运行最快的框架之一。

Conclusion: 本文提出DMSORT，一种双分支并行跟踪框架，结合检测+ReID与动态相机运动估计，通过投影变换在卡尔曼滤波中补偿平台运动，提升海事MOT的稳定性与身份一致性。

Abstract: Accurate perception of the marine environment through robust multi-object
tracking (MOT) is essential for ensuring safe vessel navigation and effective
maritime surveillance. However, the complicated maritime environment often
causes camera motion and subsequent visual degradation, posing significant
challenges to MOT. To address this challenge, we propose an efficient
Dual-branch Maritime SORT (DMSORT) method for maritime MOT. The core of the
framework is a parallel tracker with affine compensation, which incorporates an
object detection and re-identification (ReID) branch, along with a dedicated
branch for dynamic camera motion estimation. Specifically, a Reversible
Columnar Detection Network (RCDN) is integrated into the detection module to
leverage multi-level visual features for robust object detection. Furthermore,
a lightweight Transformer-based appearance extractor (Li-TAE) is designed to
capture global contextual information and generate robust appearance features.
Another branch decouples platform-induced and target-intrinsic motion by
constructing a projective transformation, applying platform-motion compensation
within the Kalman filter, and thereby stabilizing true object trajectories.
Finally, a clustering-optimized feature fusion module effectively combines
motion and appearance cues to ensure identity consistency under noise,
occlusion, and drift. Extensive evaluations on the Singapore Maritime Dataset
demonstrate that DMSORT achieves state-of-the-art performance. Notably, DMSORT
attains the fastest runtime among existing ReID-based MOT frameworks while
maintaining high identity consistency and robustness to jitter and occlusion.
Code is available at:
https://github.com/BiscuitsLzy/DMSORT-An-efficient-parallel-maritime-multi-object-tracking-architecture-.

</details>


### [27] [Learning from Online Videos at Inference Time for Computer-Use Agents](https://arxiv.org/abs/2511.04137)
*Yujian Liu,Ze Wang,Hao Chen,Ximeng Sun,Xiaodong Yu,Jialian Wu,Jiang Liu,Emad Barsoum,Zicheng Liu,Shiyu Chang*

Main category: cs.CV

TL;DR: 提出一种从在线视频提取并动态选择短轨迹作为推理时的上下文演示的方法，结合VLM进行动作理解与分割，实验证明能显著提升计算机操作代理的表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动化代理在需要特定应用、平台和多步工作流的领域仍落后于人类；人类通过观看、检索与模仿视频教程解决这一问题，论文旨在赋能代理在推理时从在线视频中学习并获得类似能力。

Method: 使用视觉语言模型（VLM）推断界面操作，将视频分割为短的动作子序列，给每个子序列分配文本目标；在推理时通过两阶段选择机制动态选择单条轨迹作为上下文指导，并进行动作过滤与视觉信息融合。

Result: 在两个常用基准上的实验显示该框架持续优于强基线和只使用文本/转录的方案；分析表明轨迹分割与选择、动作过滤和视觉信息对性能提升重要。

Conclusion: 本文提出了在推理时从在线视频提取可操作演示来改进计算机操作代理的方法，并证明在两个基准上优于只用文本教程或转录的变体。

Abstract: Computer-use agents can operate computers and automate laborious tasks, but
despite recent rapid progress, they still lag behind human users, especially
when tasks require domain-specific procedural knowledge about particular
applications, platforms, and multi-step workflows. Humans can bridge this gap
by watching video tutorials: we search, skim, and selectively imitate short
segments that match our current subgoal. In this paper, we study how to enable
computer-use agents to learn from online videos at inference time effectively.
We propose a framework that retrieves and filters tutorial videos, converts
them into structured demonstration trajectories, and dynamically selects
trajectories as in-context guidance during execution. Particularly, using a
VLM, we infer UI actions, segment videos into short subsequences of actions,
and assign each subsequence a textual objective. At inference time, a two-stage
selection mechanism dynamically chooses a single trajectory to add in context
at each step, focusing the agent on the most helpful local guidance for its
next decision. Experiments on two widely used benchmarks show that our
framework consistently outperforms strong base agents and variants that use
only textual tutorials or transcripts. Analyses highlight the importance of
trajectory segmentation and selection, action filtering, and visual
information, suggesting that abundant online videos can be systematically
distilled into actionable guidance that improves computer-use agents at
inference time. Our code is available at
https://github.com/UCSB-NLP-Chang/video_demo.

</details>


### [28] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 提出ORB基准与基于Phi-3.5-Vision的轻量旋转分类器，达到高精度分类并显著提升OCR性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中拍摄或扫描文档常因相机基线方向错误导致页面旋转，从而影响OCR等下游任务，需一个鲁棒且轻量的旋转校正方案与评测基准。

Method: 构建了OCR-Rotation-Bench（包含ORB-En和ORB-Indic），并基于Phi-3.5-Vision的视觉编码器，采用动态图像裁剪与微调，训练4类旋转分类器作为独立模块部署。

Result: 在ORB-En与ORB-Indic上分别达到约96%和92%的旋转分类准确率；在模拟真实场景下，旋转校正模块使闭源OCR性能提升最多14%，开源模型提升可达4倍。

Conclusion: 本文提出了一个面向OCR旋转鲁棒性的基准与轻量级分类模块，实验表明在识别图像旋转方向上能达到高精度，并显著提升下游OCR性能。

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [29] [Systematic Evaluation of Preprocessing Techniques for Accurate Image Registration in Digital Pathology](https://arxiv.org/abs/2511.04171)
*Fatemehzahra Darzi,Rodrigo Escobar Diaz Guerrero,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 对20对样本比较多种颜色变换对H&E与非线性多模态图像配准的影响，使用VALIS配准并以rTRE评估；CycleGAN预处理效果最佳，显著降低配准误差。


<details>
  <summary>Details</summary>
Motivation: 探究不同颜色转换预处理对H&E染色与非线性多模态图像配准效果的影响，以提升数字病理中跨模态图像对齐的准确性，支持后续生物标志物分析与组织重建。

Method: 对20对组织样本分别进行多种预处理（CycleGAN、Macenko、Reinhard、Vahadane 颜色变换；图像反转；对比度调整；强度归一化；去噪），然后使用VALIS先刚性再分两步非刚性（低/高分辨率）配准，评价指标为rTRE，报告MMrTRE与AMrTRE，并做基于十个手动关键点的定制评估；分别在原始与反转的多模态情形下独立配准比较。

Result: 在两种情形（原图/反转）下，CycleGAN颜色变换均取得最低的配准误差，其他传统颜色标准化方法（Macenko、Reinhard、Vahadane）误差更高；总体表明先进行颜色变换可改善跨模态配准。

Conclusion: 应用颜色变换（尤其是CycleGAN）在配准前能显著降低不同模态病理图像间的配准误差，从而提高配准可靠性。

Abstract: Image registration refers to the process of spatially aligning two or more
images by mapping them into a common coordinate system, so that corresponding
anatomical or tissue structures are matched across images. In digital
pathology, registration enables direct comparison and integration of
information from different stains or imaging modalities, sup-porting
applications such as biomarker analysis and tissue reconstruction. Accurate
registration of images from different modalities is an essential step in
digital pathology. In this study, we investigated how various color
transformation techniques affect image registration between hematoxylin and
eosin (H&E) stained images and non-linear multimodal images. We used a dataset
of 20 tissue sample pairs, with each pair undergoing several preprocessing
steps, including different color transformation (CycleGAN, Macenko, Reinhard,
Vahadane), inversion, contrast adjustment, intensity normalization, and
denoising. All images were registered using the VALIS registration method,
which first applies rigid registration and then performs non-rigid registration
in two steps on both low and high-resolution images. Registration performance
was evaluated using the relative Target Registration Error (rTRE). We reported
the median of median rTRE values (MMrTRE) and the average of median rTRE values
(AMrTRE) for each method. In addition, we performed a custom point-based
evaluation using ten manually selected key points. Registration was done
separately for two scenarios, using either the original or inverted multimodal
images. In both scenarios, CycleGAN color transformation achieved the lowest
registration errors, while the other methods showed higher errors. These
findings show that applying color transformation before registration improves
alignment between images from different modalities and supports more reliable
analysis in digital pathology.

</details>


### [30] [Covariance Descriptors Meet General Vision Encoders: Riemannian Deep Learning for Medical Image Classification](https://arxiv.org/abs/2511.04190)
*Josef Mayr,Anna Reithmeir,Maxime Di Folco,Julia A. Schnabel*

Main category: cs.CV

TL;DR: 短评：用DINOv2/MedSAM提取特征并构建协方差描述子，GVE特征优于手工特征；将协方差描述子输入SPDNet（以DINOv2效果最佳）能在MedMNSIT上达到或超过最先进的分类性能，表明此方向在医疗影像分析中有显著潜力。


<details>
  <summary>Details</summary>
Motivation: 动机：协方差描述子能有效捕捉特征的二阶统计信息，尚未在医疗影像中得到充分利用；希望探索将预训练GVE的强大表征能力与协方差描述子结合，提升医疗影像分类性能，并验证专门处理SPD矩阵的网络（SPDNet）的有效性。

Method: 方法：从两种预训练GVE（DINOv2和MedSAM）提取图像特征，构建基于这些特征的协方差描述子；同时构造基于手工特征的协方差描述子作为对照；在MedMNSIT基准的11个二分类和多分类数据集上，比较不同描述子与分类器的性能，重点评估SPDNet在处理SPD矩阵描述子时的效果。

Result: 结果：1）基于GVE特征的协方差描述子在11个MedMNSIT数据集上普遍优于基于手工特征的描述子；2）在这些描述子上使用SPDNet进一步提升性能，尤其是结合DINOv2特征时，SPDNet超过了现有的最先进方法；3）总体证明了GVE+协方差描述子+SPDNet的组合在医疗影像分类上的优势。

Conclusion: 论文结论：将预训练通用视觉编码器（GVE）提取的特征用于构建协方差描述子，并与手工特征进行比较，结果表明GVE特征生成的协方差描述子在医疗图像分类任务中表现更好；将这些协方差描述子输入专门处理SPD矩阵的网络SPDNet，尤其是与DINOv2特征结合时，优于现有方法，显示出将协方差描述子与强大预训练编码器结合的潜力。

Abstract: Covariance descriptors capture second-order statistics of image features.
They have shown strong performance in general computer vision tasks, but remain
underexplored in medical imaging. We investigate their effectiveness for both
conventional and learning-based medical image classification, with a particular
focus on SPDNet, a classification network specifically designed for symmetric
positive definite (SPD) matrices. We propose constructing covariance
descriptors from features extracted by pre-trained general vision encoders
(GVEs) and comparing them with handcrafted descriptors. Two GVEs - DINOv2 and
MedSAM - are evaluated across eleven binary and multi-class datasets from the
MedMNSIT benchmark. Our results show that covariance descriptors derived from
GVE features consistently outperform those derived from handcrafted features.
Moreover, SPDNet yields superior performance to state-of-the-art methods when
combined with DINOv2 features. Our findings highlight the potential of
combining covariance descriptors with powerful pretrained vision encoders for
medical image analysis.

</details>


### [31] [AStF: Motion Style Transfer via Adaptive Statistics Fusor](https://arxiv.org/abs/2511.04192)
*Hanmo Chen,Chenghao Xu,Jiexi Yan,Cheng Deng*

Main category: cs.CV

TL;DR: 论文通过引入偏度与峰度，设计AStF（含SDM与HOS-Attn）并配合MCR判别器，实现更全面的时空统计建模，从而在动作风格迁移上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于均值和方差的图像风格迁移方法难以捕捉动作数据的复杂动态模式与时空一致性，因此需要引入更高阶的统计量来更全面地描述运动风格。

Method: 提出Adaptive Statistics Fusor (AStF)，包括Style Disentanglement Module (SDM)和High-Order Multi-Statistics Attention (HOS-Attn)，并与Motion Consistency Regularization (MCR)判别器联合训练。

Result: 实验表明AStF在动作风格迁移上比现有最先进方法表现更好，证明高阶统计量有助于提升风格表达与一致性。

Conclusion: 该论文提出通过引入高阶统计量（偏度与峰度）来增强动作风格迁移的表达能力，使模型更全面地刻画动态风格的时空统计特性，从而在风格迁移任务上优于现有方法。

Abstract: Human motion style transfer allows characters to appear less rigidity and
more realism with specific style. Traditional arbitrary image style transfer
typically process mean and variance which is proved effective. Meanwhile,
similar methods have been adapted for motion style transfer. However, due to
the fundamental differences between images and motion, relying on mean and
variance is insufficient to fully capture the complex dynamic patterns and
spatiotemporal coherence properties of motion data. Building upon this, our key
insight is to bring two more coefficient, skewness and kurtosis, into the
analysis of motion style. Specifically, we propose a novel Adaptive Statistics
Fusor (AStF) which consists of Style Disentanglement Module (SDM) and
High-Order Multi-Statistics Attention (HOS-Attn). We trained our AStF in
conjunction with a Motion Consistency Regularization (MCR) discriminator.
Experimental results show that, by providing a more comprehensive model of the
spatiotemporal statistical patterns inherent in dynamic styles, our proposed
AStF shows proficiency superiority in motion style transfers over
state-of-the-arts. Our code and model are available at
https://github.com/CHMimilanlan/AStF.

</details>


### [32] [MedSapiens: Taking a Pose to Rethink Medical Imaging Landmark Detection](https://arxiv.org/abs/2511.04255)
*Marawan Elbatel,Anbang Wang,Keyuan Liu,Kaouther Mouheb,Enrique Almar-Munoz,Lizhuo Lin,Yanqi Yang,Karim Lekadir,Xiaomeng Li*

Main category: cs.CV

TL;DR: 将人体姿态预训练的视觉基础模型复用到医学标志点检测，通过多数据集预训练构建MedSapiens，带来显著性能提升并在少量标注下仍具优势。


<details>
  <summary>Details</summary>
Motivation: 动机是观察到现有标志点检测多依赖领域专用模型，而人体中心的基础模型已在空间姿态定位上有强先验，尚未被充分用于医学影像的解剖标志点检测。作者希望验证这种迁移是否能带来性能提升。

Method: 方法为多数据集预训练，将Sapiens模型进行域适配以构建MedSapiens；在标准化评测上与通用模型和专用模型比较，并在少量标注下进行微调评估。

Result: 结果显示MedSapiens在平均成功检测率（SDR）上比通用模型最高提升5.26%，比专用模型最高提升21.81%；在少样本设置下也比few-shot SOTA提升2.69%。代码与权重公开。

Conclusion: 本论文结论是：无需新架构，通过将以人体姿态估计为中心的大规模视觉基础模型（Sapiens）适配到医学图像中，能显著提升解剖学标志点检测性能，提出的MedSapiens在多数据集上实现了新的SOTA。

Abstract: This paper does not introduce a novel architecture; instead, it revisits a
fundamental yet overlooked baseline: adapting human-centric foundation models
for anatomical landmark detection in medical imaging. While landmark detection
has traditionally relied on domain-specific models, the emergence of
large-scale pre-trained vision models presents new opportunities. In this
study, we investigate the adaptation of Sapiens, a human-centric foundation
model designed for pose estimation, to medical imaging through multi-dataset
pretraining, establishing a new state of the art across multiple datasets. Our
proposed model, MedSapiens, demonstrates that human-centric foundation models,
inherently optimized for spatial pose localization, provide strong priors for
anatomical landmark detection, yet this potential has remained largely
untapped. We benchmark MedSapiens against existing state-of-the-art models,
achieving up to 5.26% improvement over generalist models and up to 21.81%
improvement over specialist models in the average success detection rate (SDR).
To further assess MedSapiens adaptability to novel downstream tasks with few
annotations, we evaluate its performance in limited-data settings, achieving
2.69% improvement over the few-shot state of the art in SDR. Code and model
weights are available at https://github.com/xmed-lab/MedSapiens .

</details>


### [33] [Proto-LeakNet: Towards Signal-Leak Aware Attribution in Synthetic Human Face Imagery](https://arxiv.org/abs/2511.04260)
*Claudio Giusti,Luca Guarnera,Sebastiano Battiato*

Main category: cs.CV

TL;DR: Proto-LeakNet在扩散模型潜在域模拟部分前向扩散并用时间注意力与原型化嵌入实现解释性来源归属和开放集检测，在仅用封闭数据训练下达成98.13% Macro AUC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散与深度伪造模型会在潜在表示中留下持久的统计性信号泄露，利用这些隐含信号可用于图像来源归属与伪造鉴定，尤其需要兼顾对未见生成器的检测能力与解释性。

Method: 在扩散潜在空间再模拟部分前向扩散以揭示残留的生成器痕迹，使用时间注意力编码聚合多步潜在特征，采用带权重的原型头构建嵌入空间，同时结合密度估计进行开放集评估，训练仅使用封闭集数据。

Result: 在仅用封闭集训练的条件下，Proto-LeakNet在基准上实现Macro AUC 98.13%，在抗后处理鲁棒性、已知与未知生成器可分性方面均优于现有方法，展示了潜在域信号泄露建模在AI图像取证中的有效性与可解释性。

Conclusion: 本文提出的Proto-LeakNet通过在扩散模型的潜在域中模拟部分前向扩散并结合时间注意力编码器与原型化分类头，能有效捕捉生成器特有的信号泄露并实现可解释的来源归属与开放集检测。

Abstract: The growing sophistication of synthetic image and deepfake generation models
has turned source attribution and authenticity verification into a critical
challenge for modern computer vision systems. Recent studies suggest that
diffusion pipelines unintentionally imprint persistent statistical traces,
known as signal leaks, within their outputs, particularly in latent
representations. Building on this observation, we propose Proto-LeakNet, a
signal-leak-aware and interpretable attribution framework that integrates
closed-set classification with a density-based open-set evaluation on the
learned embeddings, enabling analysis of unseen generators without retraining.
Operating in the latent domain of diffusion models, our method re-simulates
partial forward diffusion to expose residual generator-specific cues. A
temporal attention encoder aggregates multi-step latent features, while a
feature-weighted prototype head structures the embedding space and enables
transparent attribution. Trained solely on closed data and achieving a Macro
AUC of 98.13%, Proto-LeakNet learns a latent geometry that remains robust under
post-processing, surpassing state-of-the-art methods, and achieves strong
separability between known and unseen generators. These results demonstrate
that modeling signal-leak bias in latent space enables reliable and
interpretable AI-image and deepfake forensics. The code for the whole work will
be available upon submission.

</details>


### [34] [DINOv2 Driven Gait Representation Learning for Video-Based Visible-Infrared Person Re-identification](https://arxiv.org/abs/2511.04281)
*Yujie Yang,Shuang Li,Jun Ye,Neng Dong,Fan Li,Huafeng Li*

Main category: cs.CV

TL;DR: 提出DinoGRL，结合DINOv2语义先验的步态学习与多粒度步态-外观双向增强，有效提升可见光-红外视频行人再识别性能，实验验证优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模态不变的外观特征，忽视了同样模态不变且富含时序信息的步态特征，导致难以建模跨模态视频匹配所需的时空一致性。

Method: 方法包括两个关键模块：SASGL利用DINOv2的语义先验生成并增强人体轮廓表示，联合ReID目标训练以获取语义富化且任务自适应的步态特征；PBMGE在多尺度空间粒度上实现步态与外观流的双向交互，逐步细化特征并增强全局表示。

Result: 在HITSZ-VCM和BUPT两个数据集上进行的大量实验表明，DinoGRL显著优于现有最先进方法，在跨模态视频检索任务中取得更高的检索准确率。

Conclusion: 本文提出DinoGRL框架，通过引入DINOv2视觉先验学习步态特征，与外观特征互补，提高可见光-红外视频再识别性能。

Abstract: Video-based Visible-Infrared person re-identification (VVI-ReID) aims to
retrieve the same pedestrian across visible and infrared modalities from video
sequences. Existing methods tend to exploit modality-invariant visual features
but largely overlook gait features, which are not only modality-invariant but
also rich in temporal dynamics, thus limiting their ability to model the
spatiotemporal consistency essential for cross-modal video matching. To address
these challenges, we propose a DINOv2-Driven Gait Representation Learning
(DinoGRL) framework that leverages the rich visual priors of DINOv2 to learn
gait features complementary to appearance cues, facilitating robust
sequence-level representations for cross-modal retrieval. Specifically, we
introduce a Semantic-Aware Silhouette and Gait Learning (SASGL) model, which
generates and enhances silhouette representations with general-purpose semantic
priors from DINOv2 and jointly optimizes them with the ReID objective to
achieve semantically enriched and task-adaptive gait feature learning.
Furthermore, we develop a Progressive Bidirectional Multi-Granularity
Enhancement (PBMGE) module, which progressively refines feature representations
by enabling bidirectional interactions between gait and appearance streams
across multiple spatial granularities, fully leveraging their complementarity
to enhance global representations with rich local details and produce highly
discriminative features. Extensive experiments on HITSZ-VCM and BUPT datasets
demonstrate the superiority of our approach, significantly outperforming
existing state-of-the-art methods.

</details>


### [35] [FastGS: Training 3D Gaussian Splatting in 100 Seconds](https://arxiv.org/abs/2511.04283)
*Shiwei Ren,Tianci Wen,Yongchun Fang,Biao Lu*

Main category: cs.CV

TL;DR: FastGS通过多视图一致性的稠密化与剪枝策略动态管理高斯数目，显著加速3D Gaussian splatting训练（2–15×），同时保持可比的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有的3D Gaussian splatting加速方法未能在训练过程中合理调控高斯数量，导致计算冗余和训练时间增加，需设计一种能根据几何与视图一致性动态调节高斯数目的方法来提升效率。

Method: 提出了一个通用的加速框架FastGS，基于多视图一致性评估每个高斯的重要性，并通过稠密化（在关键区域增加高斯）和剪枝（移除不重要高斯）策略动态调整高斯集合，从而在训练中保持必要的表示能力同时降低计算量。

Result: 在多个数据集（Mip-NeRF 360、Tanks & Temples、Deep Blending）上，FastGS在训练速度上显著优于现有方法：在Mip-NeRF 360上相较DashGaussian达到约3.32×加速并保持相当渲染质量；在Deep Blending上相比原始3DGS达到约15.45×加速；并在多种任务中展现2–7×的通用加速效果。

Conclusion: FastGS通过基于多视图一致性的稠密化与剪枝策略，去掉了对预算机制的依赖，有效减少训练过程中冗余高斯，提升训练速度并保持渲染质量。

Abstract: The dominant 3D Gaussian splatting (3DGS) acceleration methods fail to
properly regulate the number of Gaussians during training, causing redundant
computational time overhead. In this paper, we propose FastGS, a novel, simple,
and general acceleration framework that fully considers the importance of each
Gaussian based on multi-view consistency, efficiently solving the trade-off
between training time and rendering quality. We innovatively design a
densification and pruning strategy based on multi-view consistency, dispensing
with the budgeting mechanism. Extensive experiments on Mip-NeRF 360, Tanks &
Temples, and Deep Blending datasets demonstrate that our method significantly
outperforms the state-of-the-art methods in training speed, achieving a
3.32$\times$ training acceleration and comparable rendering quality compared
with DashGaussian on the Mip-NeRF 360 dataset and a 15.45$\times$ acceleration
compared with vanilla 3DGS on the Deep Blending dataset. We demonstrate that
FastGS exhibits strong generality, delivering 2-7$\times$ training acceleration
across various tasks, including dynamic scene reconstruction, surface
reconstruction, sparse-view reconstruction, large-scale reconstruction, and
simultaneous localization and mapping. The project page is available at
https://fastgs.github.io/

</details>


### [36] [Vision Foundation Models in Agriculture: Toward Domain-Specific Adaptation for Weed Herbicide Trials Assessment](https://arxiv.org/abs/2511.04288)
*Leire Benito-Del-Valle,Artzai Picón,Daniel Mugica,Manuel Ramos,Eva Portillo,Javier Romero,Carlos Javier Jimenez,Ramón Navarra-Mestre*

Main category: cs.CV

TL;DR: 用大规模农业自监督预训练提升视觉基础模型在除草试验中对物种、药害和分割的性能，显著改善域泛化并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 动机是现有通用视觉基础模型在农业场景中对细粒度物种与药害类型的区分能力受限，且手工标注昂贵，因而需要一个在农业图像上进行微调或预训练的领域特定模型以提高准确性与注释效率。

Method: 方法是在大规模精心构建的农业图像数据集上，采用自监督学习对通用视觉模型进行领域特定预训练，学习富含可迁移性的表征；随后通过下游任务（物种识别、药害分类和分割）评估，并与通用模型对比，同时做域移位和注释效率分析。

Result: 结果显示：领域特定模型在种类识别F1从0.91提升到0.94，药害分类从0.26提升到0.33；在未见条件下（新地点/不同时间）种类识别从0.56提升到0.66，药害分类从0.17提升到0.27；在无人机域移位中种类识别从0.49提升到0.60。分割任务在低注释量下也显著受益，未见条件下以80%更少标注仍能获得比通用模型高5.4%的F1。

Conclusion: 本文结论是：相较于通用视觉基础模型，针对农业领域（除草试验）进行领域特定自监督预训练能显著提高物种识别、药害分类和分割任务的性能，并在域移位和未见条件下表现更好，同时大幅降低标注需求。

Abstract: Herbicide field trials require accurate identification of plant species and
assessment of herbicide-induced damage across diverse environments. While
general-purpose vision foundation models have shown promising results in
complex visual domains, their performance can be limited in agriculture, where
fine-grained distinctions between species and damage types are critical.
  In this work, we adapt a general-purpose vision foundation model to herbicide
trial characterization. Trained using a self-supervised learning approach on a
large, curated agricultural dataset, the model learns rich and transferable
representations optimized for herbicide trials images.
  Our domain-specific model significantly outperforms the best general-purpose
foundation model in both species identification (F1 score improvement from 0.91
to 0.94) and damage classification (from 0.26 to 0.33). Under unseen conditions
(new locations and other time), it achieves even greater gains (species
identification from 0.56 to 0.66; damage classification from 0.17 to 0.27). In
domain-shift scenarios, such as drone imagery, it maintains strong performance
(species classification from 0.49 to 0.60).
  Additionally, we show that domain-specific pretraining enhances segmentation
accuracy, particularly in low-annotation regimes. An annotation-efficiency
analysis reveals that, under unseen conditions, the domain-specific model
achieves 5.4% higher F1 score than the general-purpose model, while using 80%
fewer labeled samples.
  These results demonstrate the generalization capabilities of domain-specific
foundation models and their potential to significantly reduce manual annotation
efforts, offering a scalable and automated solution for herbicide trial
analysis.

</details>


### [37] [Deep learning-based object detection of offshore platforms on Sentinel-1 Imagery and the impact of synthetic training data](https://arxiv.org/abs/2511.04304)
*Robin Spanier,Thorsten Hoeser,Claudia Kuenzer*

Main category: cs.CV

TL;DR: 通过在Sentinel-1影像上混合合成与真实数据训练YOLOv10，作者实现了更高的海上平台检测F1（0.85→0.90）并验证了模型在不同海域的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 海上基础设施迅速扩张，需要可扩展监测系统；真实数据中某些类别、形态和尺寸样本稀缺，导致检测模型性能受限，故探索合成数据能否弥补样本不平衡并提升泛化能力。

Method: 使用2023年第四季度来自里海、南中国海、几内亚湾和巴西海岸的Sentinel-1卫星影像，结合合成数据对YOLOv10进行训练；采用区域留出（region-holdout）评估：在墨西哥湾、北海和波斯湾三个未见过的区域测试模型，以检验地理泛化能力。

Result: 在三个未见区域共检测到3529个海上平台（北海411、墨西哥湾1519、波斯湾1593）；模型F1从0.85提升到0.90（加入合成数据后）；证明了合成数据在改善类别不平衡和提升整体性能方面的有效性。

Conclusion: 通过将合成数据与真实Sentinel-1影像结合训练YOLOv10模型，可以显著提高海上基础设施（尤其是海上平台）检测的性能，并提升模型在不同地理区域间的迁移能力。

Abstract: The recent and ongoing expansion of marine infrastructure, including offshore
wind farms, oil and gas platforms, artificial islands, and aquaculture
facilities, highlights the need for effective monitoring systems. The
development of robust models for offshore infrastructure detection relies on
comprehensive, balanced datasets, but falls short when samples are scarce,
particularly for underrepresented object classes, shapes, and sizes. By
training deep learning-based YOLOv10 object detection models with a combination
of synthetic and real Sentinel-1 satellite imagery acquired in the fourth
quarter of 2023 from four regions (Caspian Sea, South China Sea, Gulf of
Guinea, and Coast of Brazil), this study investigates the use of synthetic
training data to enhance model performance. We evaluated this approach by
applying the model to detect offshore platforms in three unseen regions (Gulf
of Mexico, North Sea, Persian Gulf) and thereby assess geographic
transferability. This region-holdout evaluation demonstrated that the model
generalises beyond the training areas. In total, 3,529 offshore platforms were
detected, including 411 in the North Sea, 1,519 in the Gulf of Mexico, and
1,593 in the Persian Gulf. The model achieved an F1 score of 0.85, which
improved to 0.90 upon incorporating synthetic data. We analysed how synthetic
data enhances the representation of unbalanced classes and overall model
performance, taking a first step toward globally transferable detection of
offshore infrastructure. This study underscores the importance of balanced
datasets and highlights synthetic data generation as an effective strategy to
address common challenges in remote sensing, demonstrating the potential of
deep learning for scalable, global offshore infrastructure monitoring.

</details>


### [38] [RISE-T2V: Rephrasing and Injecting Semantics with LLM for Expansive Text-to-Video Generation](https://arxiv.org/abs/2511.04317)
*Xiangjun Zhang,Litong Gong,Yinglin Zheng,Yansong Liu,Wentao Jiang,Mingyi Xu,Biao Wang,Tiezheng Ge,Ming Zeng*

Main category: cs.CV

TL;DR: RISE-T2V通过将提示改写与语义提取一体化并引入Rephrasing Adapter，利用LLM隐藏态条件化视频扩散模型，显著改善简短提示下的文本-视频语义对齐与生成质量，且兼容多种模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型依赖预训练文本编码器但对简短或非精心设计的提示语理解不足，且无法在线改写提示以更好对齐用户意图，限制了可扩展性与易用性。

Method: 提出Rephrasing Adapter模块，将LLM在下一个token预测时的隐藏态作为条件注入到视频扩散模型，允许模型隐式将简短提示改写为更全面的语义表示；框架兼容多种预训练LLM与视频扩散架构。

Result: 在多种视频扩散模型与LLM组合上，RISE-T2V显著提升了在简短提示下生成的视频质量与语义对齐能力，实验与可视化结果证明其通用性和效果提升。

Conclusion: RISE-T2V通过将提示改写与语义特征提取合并为一步，并引入Rephrasing Adapter，使预训练LLM与视频扩散模型在生成视频时更好地对齐用户意图，从而提升了短提示下的生成质量与可用性。

Abstract: Most text-to-video(T2V) diffusion models depend on pre-trained text encoders
for semantic alignment, yet they often fail to maintain video quality when
provided with concise prompts rather than well-designed ones. The primary issue
lies in their limited textual semantics understanding. Moreover, these text
encoders cannot rephrase prompts online to better align with user intentions,
which limits both the scalability and usability of the models, To address these
challenges, we introduce RISE-T2V, which uniquely integrates the processes of
prompt rephrasing and semantic feature extraction into a single and seamless
step instead of two separate steps. RISE-T2V is universal and can be applied to
various pre-trained LLMs and video diffusion models(VDMs), significantly
enhancing their capabilities for T2V tasks. We propose an innovative module
called the Rephrasing Adapter, enabling diffusion models to utilize text hidden
states during the next token prediction of the LLM as a condition for video
generation. By employing a Rephrasing Adapter, the video generation model can
implicitly rephrase basic prompts into more comprehensive representations that
better match the user's intent. Furthermore, we leverage the powerful
capabilities of LLMs to enable video generation models to accomplish a broader
range of T2V tasks. Extensive experiments demonstrate that RISE-T2V is a
versatile framework applicable to different video diffusion model
architectures, significantly enhancing the ability of T2V models to generate
high-quality videos that align with user intent. Visual results are available
on the webpage at https://rise-t2v.github.io.

</details>


### [39] [Submanifold Sparse Convolutional Networks for Automated 3D Segmentation of Kidneys and Kidney Tumours in Computed Tomography](https://arxiv.org/abs/2511.04334)
*Saúl Alonso-Monsalve,Leigh H. Whitehead,Adam Aurisano,Lorena Escudero Sanchez*

Main category: cs.CV

TL;DR: 提出体素稀疏化结合子流形稀疏卷积的两阶段原生3D分割方法，在保持或超越精度的同时大幅降低计算资源需求，适合高分辨率医学图像肿瘤分割。


<details>
  <summary>Details</summary>
Motivation: 3D扫描体素量巨大，传统密集3D CNN需对图像下采样或使用patch，导致丢失细节或增加计算负担；目标是实现高分辨率原生3D分割同时降低GPU内存与时间开销。

Method: 先对原始3D体积进行体素稀疏化以去除大量无关背景体素，再使用子流形稀疏卷积网络在稀疏体素上进行原生3D卷积操作；两阶段设计允许保留高分辨率输入而避免全体素密集计算。

Result: 在KiTS23上得到：肾脏+肿块Dice 95.8%，肿瘤+囊肿85.7%，肿瘤80.3%；在CPU及多种GPU上较等效密集模型推理时间减少最多60%，显存使用减少最多75%。

Conclusion: 该论文提出了一种两阶段方法（体素稀疏化 + 子流形稀疏卷积网络）用于高分辨率3D医学图像肿瘤分割，在KiTS23肾癌CT数据集上取得了与挑战赛优胜者可比的高Dice分数，同时在推理时间和显存占用上显著优于等效密集架构。

Abstract: The accurate delineation of tumours in radiological images like Computed
Tomography is a very specialised and time-consuming task, and currently a
bottleneck preventing quantitative analyses to be performed routinely in the
clinical setting. For this reason, developing methods for the automated
segmentation of tumours in medical imaging is of the utmost importance and has
driven significant efforts in recent years. However, challenges regarding the
impracticality of 3D scans, given the large amount of voxels to be analysed,
usually requires the downsampling of such images or using patches thereof when
applying traditional convolutional neural networks. To overcome this problem,
in this paper we propose a new methodology that uses, divided into two stages,
voxel sparsification and submanifold sparse convolutional networks. This method
allows segmentations to be performed with high-resolution inputs and a native
3D model architecture, obtaining state-of-the-art accuracies while
significantly reducing the computational resources needed in terms of GPU
memory and time. We studied the deployment of this methodology in the context
of Computed Tomography images of renal cancer patients from the KiTS23
challenge, and our method achieved results competitive with the challenge
winners, with Dice similarity coefficients of 95.8% for kidneys + masses, 85.7%
for tumours + cysts, and 80.3% for tumours alone. Crucially, our method also
offers significant computational improvements, achieving up to a 60% reduction
in inference time and up to a 75\% reduction in VRAM usage compared to an
equivalent dense architecture, across both CPU and various GPU cards tested.

</details>


### [40] [Comparative Study of CNN Architectures for Binary Classification of Horses and Motorcycles in the VOC 2008 Dataset](https://arxiv.org/abs/2511.04344)
*Muhammad Annas Shaikh,Hamza Zaman,Arbaz Asif*

Main category: cs.CV

TL;DR: 评估九种网络在VOC2008马/摩托二分类上的表现，发现ConvNeXt-Tiny最佳，数据增强显著缓解少数类不平衡问题，深层模型受益明显。


<details>
  <summary>Details</summary>
Motivation: 解决VOC 2008中马与摩托车类别之间的严重类别不平衡，寻找在不平衡数据条件下表现稳健的网络架构并量化数据增强策略的影响。

Method: 比较了九种卷积神经网络（例如ResNet-50、ConvNeXt-Tiny、DenseNet-121、ViT等）在不平衡二分类任务上的性能，采用少数类增强技术缓解类别不平衡，并通过多项性能指标（如Average Precision）进行评估。

Result: ConvNeXt-Tiny在马检测上达到了95.53% AP，在摩托车检测上为89.12% AP；不同架构间性能差异显著，数据增强对少数类检测提升显著，深层模型受益更大。

Conclusion: 本研究表明在VOC 2008数据集上，ConvNeXt-Tiny在马与摩托车二分类中表现最佳，同时数据增强对少数类（马或摩托车）有显著提升效果，尤其利于较深网络。

Abstract: This paper presents a comprehensive evaluation of nine convolutional neural
network architectures for binary classification of horses and motorcycles in
the VOC 2008 dataset. We address the significant class imbalance problem by
implementing minority-class augmentation techniques. Our experiments compare
modern architectures including ResNet-50, ConvNeXt-Tiny, DenseNet-121, and
Vision Transformer across multiple performance metrics. Results demonstrate
substantial performance variations, with ConvNeXt-Tiny achieving the highest
Average Precision (AP) of 95.53% for horse detection and 89.12% for motorcycle
detection. We observe that data augmentation significantly improves minority
class detection, particularly benefiting deeper architectures. This study
provides insights into architecture selection for imbalanced binary
classification tasks and quantifies the impact of data augmentation strategies
in mitigating class imbalance issues in object detection.

</details>


### [41] [Evaluating the Impact of Weather-Induced Sensor Occlusion on BEVFusion for 3D Object Detection](https://arxiv.org/abs/2511.04347)
*Sanjay Kumar,Tim Brophy,Eoin Martino Grua,Ganesh Sistu,Valentina Donzella,Ciaran Eising*

Main category: cs.CV

TL;DR: Occlusions hurt 3D detection; LiDAR is more critical for BEVFusion—fusion degrades much more when LiDAR is occluded than when camera is occluded; need occlusion-aware evaluation and robust fusion methods.


<details>
  <summary>Details</summary>
Motivation: Study how sensor occlusions (fog, haze, obstructions) affect BEV-based 3D detection and fusion robustness, an underexplored area.

Method: Used BEVFusion on nuScenes, applied controlled synthetic occlusions to camera and LiDAR, evaluated mAP and NDS for camera-only, LiDAR-only, and fused models across occlusion levels.

Result: Camera-only: moderate occlusion causes 41.3% mAP drop (35.6%→20.9%). LiDAR-only: heavy occlusion causes 47.3% mAP drop (64.7%→34.1%), harming long-range detection. Fusion: occluding camera drops mAP 4.1% (68.5%→65.7%); occluding LiDAR drops 26.8% (to 50.1%), indicating stronger reliance on LiDAR.

Conclusion: Occlusions significantly degrade 3D detection, with camera more sensitive at moderate occlusion and LiDAR suffering under heavy occlusion; fused systems rely more on LiDAR so LiDAR occlusion harms fusion more.

Abstract: Accurate 3D object detection is essential for automated vehicles to navigate
safely in complex real-world environments. Bird's Eye View (BEV)
representations, which project multi-sensor data into a top-down spatial
format, have emerged as a powerful approach for robust perception. Although
BEV-based fusion architectures have demonstrated strong performance through
multimodal integration, the effects of sensor occlusions, caused by
environmental conditions such as fog, haze, or physical obstructions, on 3D
detection accuracy remain underexplored. In this work, we investigate the
impact of occlusions on both camera and Light Detection and Ranging (LiDAR)
outputs using the BEVFusion architecture, evaluated on the nuScenes dataset.
Detection performance is measured using mean Average Precision (mAP) and the
nuScenes Detection Score (NDS). Our results show that moderate camera
occlusions lead to a 41.3% drop in mAP (from 35.6% to 20.9%) when detection is
based only on the camera. On the other hand, LiDAR sharply drops in performance
only under heavy occlusion, with mAP falling by 47.3% (from 64.7% to 34.1%),
with a severe impact on long-range detection. In fused settings, the effect
depends on which sensor is occluded: occluding the camera leads to a minor 4.1%
drop (from 68.5% to 65.7%), while occluding LiDAR results in a larger 26.8%
drop (to 50.1%), revealing the model's stronger reliance on LiDAR for the task
of 3D object detection. Our results highlight the need for future research into
occlusion-aware evaluation methods and improved sensor fusion techniques that
can maintain detection accuracy in the presence of partial sensor failure or
degradation due to adverse environmental conditions.

</details>


### [42] [A MATLAB tutorial on deep feature extraction combined with chemometrics for analytical applications](https://arxiv.org/abs/2511.04349)
*Puneet Mishra,Martijntje Vollebregt,Yizhou Ma,Maria Font-i-Furnols*

Main category: cs.CV

TL;DR: 论文是一个实用教程：用开源预训练深度学习模型在MATLAB中提取成像数据的深度特征并示范如何与光谱等数据融合，旨在降低分析化学领域采用深度学习的门槛。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在图像处理上能力强大，但分析化学领域对其采纳受限，主要因为缺乏结构化的逐步实施指南；因此作者提供了可运行的教程以降低应用门槛。

Method: 不训练新模型，而是利用现有开源深度学习模型（预训练网络）对多尺度图像特征进行提取，通过MATLAB示例代码演示不同成像模态的数据预处理、特征提取和与谱学数据的融合流程。

Result: 提供了详尽的MATLAB代码示例，覆盖多种常见成像模态的处理步骤及示范如何提取深度特征并与其他数据源结合，但不包含模型训练部分，强调用户需在自身数据上运行示例。

Conclusion: 本文为分析化学领域提供了一个面向实践的教程，旨在指导研究者使用现成的开源深度学习模型从成像数据中提取空间特征，并将其与光谱等其他数据源整合，用以探索性和预测性分析。

Abstract: Background In analytical chemistry, spatial information about materials is
commonly captured through imaging techniques, such as traditional color cameras
or with advanced hyperspectral cameras and microscopes. However, efficiently
extracting and analyzing this spatial information for exploratory and
predictive purposes remains a challenge, especially when using traditional
chemometric methods. Recent advances in deep learning and artificial
intelligence have significantly enhanced image processing capabilities,
enabling the extraction of multiscale deep features that are otherwise
challenging to capture with conventional image processing techniques. Despite
the wide availability of open-source deep learning models, adoption in
analytical chemistry remains limited because of the absence of structured,
step-by-step guidance for implementing these models.
  Results This tutorial aims to bridge this gap by providing a step-by-step
guide for applying deep learning approaches to extract spatial information from
imaging data and integrating it with other data sources, such as spectral
information. Importantly, the focus of this work is not on training deep
learning models for image processing but on using existing open source models
to extract deep features from imaging data.
  Significance The tutorial provides MATLAB code tutorial demonstrations,
showcasing the processing of imaging data from various imaging modalities
commonly encountered in analytical chemistry. Readers must run the tutorial
steps on their own datasets using the codes presented in this tutorial.

</details>


### [43] [Multi-Task Learning for Visually Grounded Reasoning in Gastrointestinal VQA](https://arxiv.org/abs/2511.04384)
*Itbaan Safwan,Muhammad Annas Shaikh,Muhammad Haaris,Ramail Khan,Muhammad Atif Tahir*

Main category: cs.CV

TL;DR: 通过LoRA微调的Florence-2在联合训练VQA、解释生成和视觉定位三项任务上，利用Kvasir-VQA-x1和合成解释及文本-区域对数据，实现更准确且可解释的医学VQA，优于单任务基线。


<details>
  <summary>Details</summary>
Motivation: 医学图像问答需要准确回答、可解释性和可靠的视觉定位，单任务模型难以同时满足这三方面，故提出通过多任务联合学习提升模型在医学VQA中的性能与可解释性。

Method: 使用LoRA对Florence-2进行微调，构建联合训练管道，包括VQA任务、合成增强的解释生成任务和文本-区域对的视觉定位任务；整合Kvasir-VQA-x1数据集、合成解释数据和文本到分割掩码的配对数据；在联合损失下训练模型以实现跨任务共享视觉和文本表示。

Result: 在多项评估中，提出的方法在答案准确率和视觉定位指标上显著优于单任务基线，表明多任务、基于视觉定位的训练能够提升医学VQA系统的性能与解释性。

Conclusion: 本文提出了一种基于LoRA微调的Florence-2模型的多任务框架，能同时处理医学VQA、解释生成和视觉定位，实验证明多任务训练在答案准确性和视觉定位上优于单任务基线。

Abstract: We present a multi-task framework for the MediaEval Medico 2025 challenge,
leveraging a LoRA-tuned Florence-2 model for simultaneous visual question
answering (VQA), explanation generation, and visual grounding. The proposed
system integrates three curated datasets: (1) Kvasir-VQA-x1 for question-answer
learning, (2) a synthetically enriched explanation dataset offering structured
medical reasoning, and (3) text-to-region pairs linking visual features with
segmentation masks. This multi-task setup enables the model to jointly learn
visual grounding, reasoning, and interpretation, producing responses that are
both accurate and interpretable. Extensive evaluation demonstrates that our
approach substantially improves over single-task baselines in both answer
accuracy and visual localization, highlighting the effectiveness of grounded
multi-task learning for medical VQA applications.

</details>


### [44] [BoRe-Depth: Self-supervised Monocular Depth Estimation with Boundary Refinement for Embedded Systems](https://arxiv.org/abs/2511.04388)
*Chang Liu,Juan Li,Sheng Zhang,Chang Liu,Jie Li,Xu Zhang*

Main category: cs.CV

TL;DR: BoRe-Depth：8.7M参数的轻量单目深度估计模型，通过EFAF模块和语义编码融合提升边界与识别，在Jetson Orin上能实时运行并在多个数据集上优于已有轻量方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计在嵌入式设备上性能差、物体边界模糊，需要轻量且高边界质量的模型。

Method: 提出了EFAF（增强特征自适应融合模块）用于自适应融合深度特征以增强边界细节；在编码器中融合语义知识以提升物体识别与边界感知；将模型部署到NVIDIA Jetson Orin验证实时性（50.7 FPS）。

Result: 模型参数量仅8.7M，在多个数据集上显著优于先前轻量模型，并在Jetson Orin上达到50.7 FPS，边界质量明显提升；附有详细消融研究。

Conclusion: BoRe-Depth在嵌入式系统上以小参数量实现了高效、边界清晰的单目深度估计，性能优于现有轻量化方法。

Abstract: Depth estimation is one of the key technologies for realizing 3D perception
in unmanned systems. Monocular depth estimation has been widely researched
because of its low-cost advantage, but the existing methods face the challenges
of poor depth estimation performance and blurred object boundaries on embedded
systems. In this paper, we propose a novel monocular depth estimation model,
BoRe-Depth, which contains only 8.7M parameters. It can accurately estimate
depth maps on embedded systems and significantly improves boundary quality.
Firstly, we design an Enhanced Feature Adaptive Fusion Module (EFAF) which
adaptively fuses depth features to enhance boundary detail representation.
Secondly, we integrate semantic knowledge into the encoder to improve the
object recognition and boundary perception capabilities. Finally, BoRe-Depth is
deployed on NVIDIA Jetson Orin, and runs efficiently at 50.7 FPS. We
demonstrate that the proposed model significantly outperforms previous
lightweight models on multiple challenging datasets, and we provide detailed
ablation studies for the proposed methods. The code is available at
https://github.com/liangxiansheng093/BoRe-Depth.

</details>


### [45] [DORAEMON: A Unified Library for Visual Object Modeling and Representation Learning at Scale](https://arxiv.org/abs/2511.04394)
*Ke Du,Yimin Peng,Chao Gao,Fan Zhou,Siqiao Xue*

Main category: cs.CV

TL;DR: DORAEMON：一个基于PyTorch、YAML驱动的开源平台，统一视觉分类、检索与度量学习流程，集成大量预训练模型与训练组件，支持可复现配方与一键部署，便于快速实验与落地。


<details>
  <summary>Details</summary>
Motivation: 当前视觉识别与表征学习在不同任务与尺度上工具链分散，复现实验与部署成本高，该项目通过整合模型、数据集和训练技巧来降低试验门槛并加速研究成果向实际应用的转化。

Method: 基于PyTorch，采用YAML驱动的单一工作流，集成了超过1000个timm兼容的预训练骨干网络、模块化损失函数、数据增强和分布式训练工具，并提供一键导出ONNX和HuggingFace的功能。

Result: 提供可复现的训练配方，在ImageNet-1K、MS-Celeb-1M和Stanford online products上达到或超过参考结果；实现多任务（分类、检索、度量学习）的统一支持；码库公开。

Conclusion: 该论文（项目）旨在构建一个统一、开源且可复现的视觉对象建模与表征学习平台，兼顾研究与部署需求。

Abstract: DORAEMON is an open-source PyTorch library that unifies visual object
modeling and representation learning across diverse scales. A single
YAML-driven workflow covers classification, retrieval and metric learning; more
than 1000 pretrained backbones are exposed through a timm-compatible interface,
together with modular losses, augmentations and distributed-training utilities.
Reproducible recipes match or exceed reference results on ImageNet-1K,
MS-Celeb-1M and Stanford online products, while one-command export to ONNX or
HuggingFace bridges research and deployment. By consolidating datasets, models,
and training techniques into one platform, DORAEMON offers a scalable
foundation for rapid experimentation in visual recognition and representation
learning, enabling efficient transfer of research advances to real-world
applications. The repository is available at https://github.com/wuji3/DORAEMON.

</details>


### [46] [HideAndSeg: an AI-based tool with automated prompting for octopus segmentation in natural habitats](https://arxiv.org/abs/2511.04426)
*Alan de Aguiar,Michaella Pereira Andrade,Charles Morphy D. Santos,João Paulo Gois*

Main category: cs.CV

TL;DR: HideAndSeg用少量人工点提示和SAM2生成初始掩码训练YOLOv11，随后自动化生成提示并引入两个无监督指标来评估与改进分割，在真实海洋环境下对章鱼分割表现出更高鲁棒性和遮挡恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中章鱼具备伪装、快速皮肤纹理与颜色变化、非刚体变形及频繁遮挡，并且缺乏大规模标注数据，导致传统分割方法难以应用；因此需要一种最小监督且能在无标注或少量标注下工作的分割工具。

Method: 用户初始提供点位，SAM2生成初始掩码作为YOLOv11训练数据；训练后使用YOLO检测器自动生成边界框并作为对SAM2的提示，实现全自动流水线；引入两种无监督评价指标DICE_t和NC_t用于量化时间一致性和组件变化并指导掩码精炼。

Result: HideAndSeg在所测试野外视频上表现令人满意：相比手动点提示方法减少了分割噪声，能够在完全遮挡后重新识别并分割章鱼；提供了任务的量化基线并展示对实际行为学研究的适用性。

Conclusion: HideAndSeg提出了一种结合SAM2与定制YOLOv11检测器的最小监督视频分割工具，能在无大规模标注数据下对野生章鱼进行鲁棒分割，降低人工干预并在遮挡恢复场景中优于手动点提示方法。

Abstract: Analyzing octopuses in their natural habitats is challenging due to their
camouflage capability, rapid changes in skin texture and color, non-rigid body
deformations, and frequent occlusions, all of which are compounded by variable
underwater lighting and turbidity. Addressing the lack of large-scale annotated
datasets, this paper introduces HideAndSeg, a novel, minimally supervised
AI-based tool for segmenting videos of octopuses. It establishes a quantitative
baseline for this task. HideAndSeg integrates SAM2 with a custom-trained
YOLOv11 object detector. First, the user provides point coordinates to generate
the initial segmentation masks with SAM2. These masks serve as training data
for the YOLO model. After that, our approach fully automates the pipeline by
providing a bounding box prompt to SAM2, eliminating the need for further
manual intervention. We introduce two unsupervised metrics - temporal
consistency $DICE_t$ and new component count $NC_t$ - to quantitatively
evaluate segmentation quality and guide mask refinement in the absence of
ground-truth data, i.e., real-world information that serves to train, validate,
and test AI models. Results show that HideAndSeg achieves satisfactory
performance, reducing segmentation noise compared to the manually prompted
approach. Our method can re-identify and segment the octopus even after periods
of complete occlusion in natural environments, a scenario in which the manually
prompted model fails. By reducing the need for manual analysis in real-world
scenarios, this work provides a practical tool that paves the way for more
efficient behavioral studies of wild cephalopods.

</details>


### [47] [Solving Convex Partition Visual Jigsaw Puzzles](https://arxiv.org/abs/2511.04450)
*Yaniv Ohayon,Ofir Itzhak Shahar,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 该工作突破只处理正方形拼图的局限，针对凸多边形拼图提出结合几何与图像相容性的贪心求解器并提供首个基准数据集与实验评估。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注正方形拼图，限制了实用性；凸分割拼图在现实应用中更常见且具有挑战性，因而需要新的方法与基准来推进研究。

Method: 结合几何兼容性（多边形边界/形状匹配）与图像兼容性（拼接处图像相似度），设计了一个贪心拼接策略以迭代地将最匹配的片段合并，使用若干性能指标评估效果并与基线比较。

Result: 提出的贪心方法在新构建的凸分割拼图基准上表现出有效性，并通过多项性能度量展示了方法能成功重建拼图，提供了实验数据作为后续研究的参考。

Conclusion: 本文扩展了自动拼图求解的研究范围，从传统的正方形拼图转向处理更通用的凸分割（Convex Partitions）多边形拼图，提出并验证了一种基于几何和图像相容性的贪心求解器，并构建了首个相关基准数据集。

Abstract: Jigsaw puzzle solving requires the rearrangement of unordered pieces into
their original pose in order to reconstruct a coherent whole, often an image,
and is known to be an intractable problem. While the possible impact of
automatic puzzle solvers can be disruptive in various application domains, most
of the literature has focused on developing solvers for square jigsaw puzzles,
severely limiting their practical use. In this work, we significantly expand
the types of puzzles handled computationally, focusing on what is known as
Convex Partitions, a major subset of polygonal puzzles whose pieces are convex.
We utilize both geometrical and pictorial compatibilities, introduce a greedy
solver, and report several performance measures next to the first benchmark
dataset of such puzzles.

</details>


### [48] [V-Thinker: Interactive Thinking with Images](https://arxiv.org/abs/2511.04460)
*Runqi Qiao,Qiuna Tan,Minghan Yang,Guanting Dong,Peiqing Yang,Shiqiang Lang,Enhui Wan,Xiaowan Wang,Yida Xu,Lan Yang,Chong Sun,Chen Li,Honggang Zhang*

Main category: cs.CV

TL;DR: 提出V-Thinker：结合数据演化飞轮与视觉渐进培训并用两阶段强化学习训练的图像交互式思维系统，显著提升LMM的交互式视觉推理能力，并构建VTBench进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前大多LMM在视觉推理方面受限于有限的视觉工具空间与任务专用工作流，难以实现细粒度图像交互式思维与长程推理。研究动机是构建通用且可进化的数据与训练策略，使模型能在交互式视觉思维任务上表现更好。

Method: 方法包括两个核心模块：1) 数据演化飞轮（Data Evolution Flywheel），自动合成、进化并验证交互式推理数据集，优化多样性、质量与难度；2) 视觉渐进训练课程（Visual Progressive Training Curriculum），先通过点级监督进行感知对齐，再通过两阶段强化学习将交互式推理能力整合进模型。此外提出VTBench作为专家验证的基准测试。

Result: 通过大规模实验，V-Thinker在通用和交互式推理场景下均显著优于强基线LMM模型，验证了方法的有效性，并提供了推进图像交互式推理应用的深入见解。

Conclusion: 该论文提出V-Thinker，一个通过端到端强化学习实现的通用多模态推理助手，旨在实现图像交互式思维与长程推理的深度融合。

Abstract: Empowering Large Multimodal Models (LMMs) to deeply integrate image
interaction with long-horizon reasoning capabilities remains a long-standing
challenge in this field. Recent advances in vision-centric reasoning explore a
promising "Thinking with Images" paradigm for LMMs, marking a shift from
image-assisted reasoning to image-interactive thinking. While this milestone
enables models to focus on fine-grained image regions, progress remains
constrained by limited visual tool spaces and task-specific workflow designs.
To bridge this gap, we present V-Thinker, a general-purpose multimodal
reasoning assistant that enables interactive, vision-centric thinking through
end-to-end reinforcement learning. V-Thinker comprises two key components: (1)
a Data Evolution Flywheel that automatically synthesizes, evolves, and verifies
interactive reasoning datasets across three dimensions-diversity, quality, and
difficulty; and (2) a Visual Progressive Training Curriculum that first aligns
perception via point-level supervision, then integrates interactive reasoning
through a two-stage reinforcement learning framework. Furthermore, we introduce
VTBench, an expert-verified benchmark targeting vision-centric interactive
reasoning tasks. Extensive experiments demonstrate that V-Thinker consistently
outperforms strong LMM-based baselines in both general and interactive
reasoning scenarios, providing valuable insights for advancing
image-interactive reasoning applications.

</details>


### [49] [Landslide Hazard Mapping with Geospatial Foundation Models: Geographical Generalizability, Data Scarcity, and Band Adaptability](https://arxiv.org/abs/2511.04474)
*Wenwen Li,Sizhe Wang,Hyunho Lee,Chenyan Lu,Sujit Roy,Rahul Ramachandran,Chia-Yu Hsu*

Main category: cs.CV

TL;DR: 研究构建并验证了基于传感器-标注-域三轴分析的GeoFM评估范式，证明Prithvi-EO-2.0在滑坡制图上比传统模型更泛化和稳健，但计算开销和训练数据可用性仍是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 滑坡灾害频发且地理/传感器差异大，传统深度学习模型在跨传感器、跨区域及样本稀缺场景下泛化性差，需探索更具通用性与鲁棒性的地理空间基础模型。

Method: 提出一个以传感器、标注、域三轴为核心的分析框架，基于全球预训练、自监督学习与可适配微调，对Prithvi-EO-2.0进行系统评估，并与U-Net系列、Segformer、SwinV2-B、TerraMind、SatMAE等模型比较。

Result: Prithvi-EO-2.0在多个数据集与地理区域的试验中持续优于任务特定CNN和ViT以及其他GeoFMs；对光谱差异和标注稀缺展现出较好鲁棒性，且跨域泛化性能更稳健。

Conclusion: GeoFMs（以Prithvi-EO-2.0为例）在滑坡制图任务上表现出更强的泛化能力与稳健性，是对传统任务特定模型的重要补充，但仍受计算成本和可复用标注数据不足限制。

Abstract: Landslides cause severe damage to lives, infrastructure, and the environment,
making accurate and timely mapping essential for disaster preparedness and
response. However, conventional deep learning models often struggle when
applied across different sensors, regions, or under conditions of limited
training data. To address these challenges, we present a three-axis analytical
framework of sensor, label, and domain for adapting geospatial foundation
models (GeoFMs), focusing on Prithvi-EO-2.0 for landslide mapping. Through a
series of experiments, we show that it consistently outperforms task-specific
CNNs (U-Net, U-Net++), vision transformers (Segformer, SwinV2-B), and other
GeoFMs (TerraMind, SatMAE). The model, built on global pretraining,
self-supervision, and adaptable fine-tuning, proved resilient to spectral
variation, maintained accuracy under label scarcity, and generalized more
reliably across diverse datasets and geographic settings. Alongside these
strengths, we also highlight remaining challenges such as computational cost
and the limited availability of reusable AI-ready training data for landslide
research. Overall, our study positions GeoFMs as a step toward more robust and
scalable approaches for landslide risk reduction and environmental monitoring.

</details>


### [50] [THEval. Evaluation Framework for Talking Head Video Generation](https://arxiv.org/abs/2511.04520)
*Nabyl Quignon,Baptiste Chopin,Yaohui Wang,Antitza Dantcheva*

Main category: cs.CV

TL;DR: 提出面向口型人像生成的8项指标评测框架（质量、自然性、同步性三维度），并在85k视频、17模型上验证，发现唇同步已普遍良好，但表达与无伪影细节仍需加强；公开数据与排行榜以推动进步。


<details>
  <summary>Details</summary>
Motivation: 当前口型人像生成评估指标不足，主要依赖有限的通用视频质量、唇同步指标及人工主观测试，无法全面、高效且与人类感知一致地衡量生成视频品质，因此需要一个更细粒度且可扩展的评测框架。

Method: 从效率与与人类偏好对齐出发，筛选并设计了8个可计算指标，聚焦头部、口部、眉毛细粒度动态分析和面部质量评估；构建了一个新的真实数据集以减少训练数据偏差；在85,000个由17种最新模型生成的视频上进行大规模实验，并计划公开代码、数据集与排行榜以持续更新评测基准。

Result: 提出的评测框架在大规模实验中揭示：多数方法在唇同步指标上表现优异，但在生成表情丰富性和细节无伪影上存在明显短板；新数据集和基准为后续模型改进提供了量化评估手段。

Conclusion: 本文提出了一个针对口型人像（talking head）生成评估的新框架，包含8项指标，覆盖质量、自然性与同步性三大维度；基于大规模生成样本发现现有方法在唇同步表现良好，但在表情丰富性和无伪影细节方面仍有欠缺。

Abstract: Video generation has achieved remarkable progress, with generated videos
increasingly resembling real ones. However, the rapid advance in generation has
outpaced the development of adequate evaluation metrics. Currently, the
assessment of talking head generation primarily relies on limited metrics,
evaluating general video quality, lip synchronization, and on conducting user
studies. Motivated by this, we propose a new evaluation framework comprising 8
metrics related to three dimensions (i) quality, (ii) naturalness, and (iii)
synchronization. In selecting the metrics, we place emphasis on efficiency, as
well as alignment with human preferences. Based on this considerations, we
streamline to analyze fine-grained dynamics of head, mouth, and eyebrows, as
well as face quality. Our extensive experiments on 85,000 videos generated by
17 state-of-the-art models suggest that while many algorithms excel in lip
synchronization, they face challenges with generating expressiveness and
artifact-free details. These videos were generated based on a novel real
dataset, that we have curated, in order to mitigate bias of training data. Our
proposed benchmark framework is aimed at evaluating the improvement of
generative methods. Original code, dataset and leaderboards will be publicly
released and regularly updated with new methods, in order to reflect progress
in the field.

</details>


### [51] [Learning from Single Timestamps: Complexity Estimation in Laparoscopic Cholecystectomy](https://arxiv.org/abs/2511.04525)
*Dimitrios Anastasiou,Santiago Barbarisi,Lucy Culshaw,Jayna Patel,Evangelos B. Mazomenos,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 提出一种弱时序监督的STC-Net，能在完整LC视频中自动定位关键时刻并进行PGS分级，在大规模私有数据集上比基线提升显著。


<details>
  <summary>Details</summary>
Motivation: 在LC中，炎症程度与手术复杂度相关，PGS是临床验证的分级体系，但现有工作多局限于静态图像或手工剪辑的视频，缺乏对完整手术视频的自动化分级方法。该工作旨在实现可扩展的自动PGS评估，适用于真实、未裁剪的视频。

Method: STC-Net由三个模块组成：时序定位模块检测关键时间点；窗口提议模块在检测点周围生成短时视频片段；分级模块对提议窗口进行PGS评分。提出了结合硬定位与软定位目标的损失函数，并引入背景感知的分级监督。

Result: 在包含1859个LC视频的私有数据集上，STC-Net达到了62.11%的准确率和61.42%的F1分数，较非定位基线在两项指标上均超过10%的提升。

Conclusion: 该研究提出的STC-Net能在未经人工裁剪的完整腹腔镜胆囊切除术视频上，基于单时间点的弱时序监督，实现Parkland分级的炎症严重度评估，性能显著优于非定位基线方法。

Abstract: Purpose: Accurate assessment of surgical complexity is essential in
Laparoscopic Cholecystectomy (LC), where severe inflammation is associated with
longer operative times and increased risk of postoperative complications. The
Parkland Grading Scale (PGS) provides a clinically validated framework for
stratifying inflammation severity; however, its automation in surgical videos
remains largely unexplored, particularly in realistic scenarios where complete
videos must be analyzed without prior manual curation. Methods: In this work,
we introduce STC-Net, a novel framework for SingleTimestamp-based Complexity
estimation in LC via the PGS, designed to operate under weak temporal
supervision. Unlike prior methods limited to static images or manually trimmed
clips, STC-Net operates directly on full videos. It jointly performs temporal
localization and grading through a localization, window proposal, and grading
module. We introduce a novel loss formulation combining hard and soft
localization objectives and background-aware grading supervision. Results:
Evaluated on a private dataset of 1,859 LC videos, STC-Net achieves an accuracy
of 62.11% and an F1-score of 61.42%, outperforming non-localized baselines by
over 10% in both metrics and highlighting the effectiveness of weak supervision
for surgical complexity assessment. Conclusion: STC-Net demonstrates a scalable
and effective approach for automated PGS-based surgical complexity estimation
from full LC videos, making it promising for post-operative analysis and
surgical training.

</details>


### [52] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 作者提出‘Thinking with Video’并通过VideoThinkBench验证视频生成模型（Sora-2）可作为统一的多模态推理模型，表现优异且受自洽性与in-context learning增益。


<details>
  <summary>Details</summary>
Motivation: 解决图像只捕捉瞬时信息、文本与视觉作为分离模态限制统一多模态理解与生成的问题，通过视频引入时间维度并桥接视觉与文本推理。

Method: 提出‘Thinking with Video’范式，构建VideoThinkBench（包含视觉中心任务与文本中心任务），用Sora-2进行评估并分析其能力来源，测试自洽性与上下文学习的影响。

Result: 在视觉中心任务上Sora-2与SOTA VLMs相当并在若干任务上超越；在文本中心任务上在MATH达92%准确率、MMMU达75.53%；自洽性与上下文学习能进一步提升性能。

Conclusion: 视频生成模型如Sora-2能将视觉和文本推理统一到时间序列框架中，展现出强大的多模态推理能力，支持“Thinking with Video”作为新的推理范式。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


### [53] [UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction](https://arxiv.org/abs/2511.04595)
*Chen Shi,Shaoshuai Shi,Xiaoyang Lyu,Chunyang Liu,Kehua Sheng,Bo Zhang,Li Jiang*

Main category: cs.CV

TL;DR: 提出UniSplat：在3D潜变量骨架内做统一时空融合，配合双分支解码与静态记忆，实现稀疏非重叠视角及动态场景下的高质量流式三维重建与视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有前馈式（feed-forward）三维重建方法在自动驾驶场景中面临稀疏且不重叠摄像头视角与复杂动态场景的联合挑战，导致跨视角一致性差、时间整合困难及视点外渲染质量不佳，因此需要一种统一的时空融合表征来提升鲁棒性与完整性。

Method: 构建基于预训练基础模型的3D潜变量骨架（latent scaffold），在该骨架内直接进行高效的时空融合以实现跨视角与跨时间帧的一致对齐；采用双分支解码器，一支基于点锚（point-anchored）精细化生成动态感知高斯组件，另一支基于体素生成以补全细节；维护静态高斯的持久记忆以支持流式场景补全。

Result: 在真实世界数据集上的大量实验表明，UniSplat在新视图合成任务上达到或超越最先进水平，并在原始相机覆盖范围外仍能提供鲁棒且高质量的渲染结果，支持流式场景补全能力。

Conclusion: UniSplat通过在3D潜变量骨架上统一时空融合，有效提升了动态场景的重建与视图合成能力，能在稀疏且不重叠摄像头视角以及复杂动态场景中实现鲁棒且高质量的渲染，特别在超出原始相机覆盖范围的视点上仍表现出色。

Abstract: Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,
yet existing methods struggle with the joint challenges of sparse,
non-overlapping camera views and complex scene dynamics. We present UniSplat, a
general feed-forward framework that learns robust dynamic scene reconstruction
through unified latent spatio-temporal fusion. UniSplat constructs a 3D latent
scaffold, a structured representation that captures geometric and semantic
scene context by leveraging pretrained foundation models. To effectively
integrate information across spatial views and temporal frames, we introduce an
efficient fusion mechanism that operates directly within the 3D scaffold,
enabling consistent spatio-temporal alignment. To ensure complete and detailed
reconstructions, we design a dual-branch decoder that generates dynamic-aware
Gaussians from the fused scaffold by combining point-anchored refinement with
voxel-based generation, and maintain a persistent memory of static Gaussians to
enable streaming scene completion beyond current camera coverage. Extensive
experiments on real-world datasets demonstrate that UniSplat achieves
state-of-the-art performance in novel view synthesis, while providing robust
and high-quality renderings even for viewpoints outside the original camera
coverage.

</details>


### [54] [PixCLIP: Achieving Fine-grained Visual Language Understanding via Any-granularity Pixel-Text Alignment Learning](https://arxiv.org/abs/2511.04601)
*Yicheng Xiao,Yu Chen,Haoxuan Ma,Jiale Hong,Caorui Li,Lingxiang Wu,Haiyun Guo,Jinqiao Wang*

Main category: cs.CV

TL;DR: PixCLIP通过自动生成像素级长文本数据集并用LLM替换CLIP文本编码器，提出三分支对齐框架，实现对长文本和视觉提示的细粒度像素-文本对齐，达成SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP受文本编码器长度限制难以处理长文本，且多数工作只侧重提升视觉细粒度处理。结合视觉prompt与长文本能互补提升细粒度对齐能力。

Method: 构建自动标注流水线生成像素级本地化长文本描述，形成近150万样本的LongGRIT数据集；将CLIP的文本编码器替换为LLM，提出三分支像素-文本对齐学习框架，支持任意粒度的区域与文本对齐；训练时接收视觉提示与长文本输入联合优化。

Result: 在像素级任务与长文本对齐场景上取得SOTA性能，展示出更强的像素交互能力和长文本理解能力。

Conclusion: 该工作提出PixCLIP，通过同时增强视觉提示和长文本处理能力，实现更细粒度的像素-文本对齐，展示了在像素级交互和长文本理解上的性能突破。

Abstract: While the Contrastive Language-Image Pretraining(CLIP) model has achieved
remarkable success in a variety of downstream vison language understanding
tasks, enhancing its capability for fine-grained image-text alignment remains
an active research focus. To this end, most existing works adopt the strategy
of explicitly increasing the granularity of visual information processing,
e.g., incorporating visual prompts to guide the model focus on specific local
regions within the image. Meanwhile, researches on Multimodal Large Language
Models(MLLMs) have demonstrated that training with long and detailed textual
descriptions can effectively improve the model's fine-grained vision-language
alignment. However, the inherent token length limitation of CLIP's text encoder
fundamentally limits CLIP to process more granular textual information embedded
in long text sequences. To synergistically leverage the advantages of enhancing
both visual and textual content processing granularity, we propose PixCLIP, a
novel framework designed to concurrently accommodate visual prompt inputs and
process lengthy textual descriptions. Specifically, we first establish an
automated annotation pipeline capable of generating pixel-level localized,
long-form textual descriptions for images. Utilizing this pipeline, we
construct LongGRIT, a high-quality dataset comprising nearly 1.5 million
samples. Secondly, we replace CLIP's original text encoder with the LLM and
propose a three-branch pixel-text alignment learning framework, facilitating
fine-grained alignment between image regions and corresponding textual
descriptions at arbitrary granularity. Experiments demonstrate that PixCLIP
showcases breakthroughs in pixel-level interaction and handling long-form
texts, achieving state-of-the-art performance.

</details>


### [55] [Building Trust in Virtual Immunohistochemistry: Automated Assessment of Image Quality](https://arxiv.org/abs/2511.04615)
*Tushar Kataria,Shikha Dubey,Mary Bronner,Jolanta Jedrzkiewicz,Ben J. Brintz,Shireen Y. Elhabian,Beatrice S. Knudsen*

Main category: cs.CV

TL;DR: 作者提出了一种基于颜色去卷积与像素级指标的自动化框架，用于评估虚拟IHC的染色准确性，结果表明应优先使用染色准确性指标与WSI级基准来替代传统图像保真度评估。


<details>
  <summary>Details</summary>
Motivation: 现有质控指标（如FID、PSNR、SSIM）评估的是图像保真度而非染色准确性，无法保证虚拟IHC在病理学标注上的可靠性，需建立与临床可用性相关的自动化评估方法。

Method: 利用颜色去卷积生成棕色(即IHC阳性)像素掩码，比较真实与虚拟IHC掩码，计算Dice、IoU和Hausdorff距离等染色准确性指标；在16种配对和未配对图像翻译模型上进行基于patch和WSI的比较分析。

Result: 发现传统保真度指标与染色准确性及病理学家评估相关性差；配对模型（如PyramidPix2Pix和AdaptiveNCE）在染色准确性上表现最好，未配对的扩散模型和GAN模型较差；在WSI上性能下降明显，patch评估难以反映真实表现。

Conclusion: 该论文提出了一个基于染色准确性的自动评估框架，用于评估从H&E图像生成的虚拟IHC染色图像，揭示传统图像保真度指标无法反映IHC阳性像素标签的准确性。

Abstract: Deep learning models can generate virtual immunohistochemistry (IHC) stains
from hematoxylin and eosin (H&E) images, offering a scalable and low-cost
alternative to laboratory IHC. However, reliable evaluation of image quality
remains a challenge as current texture- and distribution-based metrics quantify
image fidelity rather than the accuracy of IHC staining. Here, we introduce an
automated and accuracy grounded framework to determine image quality across
sixteen paired or unpaired image translation models. Using color deconvolution,
we generate masks of pixels stained brown (i.e., IHC-positive) as predicted by
each virtual IHC model. We use the segmented masks of real and virtual IHC to
compute stain accuracy metrics (Dice, IoU, Hausdorff distance) that directly
quantify correct pixel - level labeling without needing expert manual
annotations. Our results demonstrate that conventional image fidelity metrics,
including Frechet Inception Distance (FID), peak signal-to-noise ratio (PSNR),
and structural similarity (SSIM), correlate poorly with stain accuracy and
pathologist assessment. Paired models such as PyramidPix2Pix and AdaptiveNCE
achieve the highest stain accuracy, whereas unpaired diffusion- and GAN-based
models are less reliable in providing accurate IHC positive pixel labels.
Moreover, whole-slide images (WSI) reveal performance declines that are
invisible in patch-based evaluations, emphasizing the need for WSI-level
benchmarks. Together, this framework defines a reproducible approach for
assessing the quality of virtual IHC models, a critical step to accelerate
translation towards routine use by pathologists.

</details>


### [56] [NovisVQ: A Streaming Convolutional Neural Network for No-Reference Opinion-Unaware Frame Quality Assessment](https://arxiv.org/abs/2511.04628)
*Kylie Cancilla,Alexander Moore,Amar Saini,Carmen Carrano*

Main category: cs.CV

TL;DR: 构建了一个基于时间卷积的、无参考且无主观标签的流式VQA模型，通过合成降级在DAVIS上训练以预测FR指标，在多种降级下优于图像基线并胜过BRISQUE。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法受限于需参考原视频或依赖昂贵的人工主观标签，且大多数无主观标签的方法仅基于图像忽略时间信息，无法满足视频目标检测等对时间上下文敏感的应用。

Method: 使用合成退化的数据在DAVIS上训练一个具备时间建模能力的卷积架构，采用流式处理以保留时间上下文，直接从降质视频预测FR指标；与图像级基线和BRISQUE进行对比评估。

Result: 流式时间模型在多种降级条件下优于图像级基线，并在与FR指标的相关性上超过BRISQUE，证明了时间建模和无参考、无主观标签方法在可扩展VQA中的有效性。

Conclusion: 提出了一个无需参考且无需人工意见的流式视频质量评估（VQA）模型，通过对DAVIS数据集进行合成退化并训练时间卷积网络来预测全参考（FR）指标（LPIPS、PSNR、SSIM），在推理时不需要参考视频。

Abstract: Video quality assessment (VQA) is vital for computer vision tasks, but
existing approaches face major limitations: full-reference (FR) metrics require
clean reference videos, and most no-reference (NR) models depend on training on
costly human opinion labels. Moreover, most opinion-unaware NR methods are
image-based, ignoring temporal context critical for video object detection. In
this work, we present a scalable, streaming-based VQA model that is both
no-reference and opinion-unaware. Our model leverages synthetic degradations of
the DAVIS dataset, training a temporal-aware convolutional architecture to
predict FR metrics (LPIPS , PSNR, SSIM) directly from degraded video, without
references at inference. We show that our streaming approach outperforms our
own image-based baseline by generalizing across diverse degradations,
underscoring the value of temporal modeling for scalable VQA in real-world
vision systems. Additionally, we demonstrate that our model achieves higher
correlation with full-reference metrics compared to BRISQUE, a widely-used
opinion-aware image quality assessment baseline, validating the effectiveness
of our temporal, opinion-unaware approach.

</details>


### [57] [Polarization-resolved imaging improves eye tracking](https://arxiv.org/abs/2511.04652)
*Mantas Žurauskas,Tom Bu,Sanaz Alali,Beyza Kalkanli,Derek Shi,Fernando Alamos,Gauresh Pandit,Christopher Mei,Ali Behrooz,Ramin Mirjalili,Dave Stronks,Alexander Fix,Dmitri Model*

Main category: cs.CV

TL;DR: Using a polarization-filter-array NIR camera with polarized illumination, CNN models exploit polarization contrast from sclera/cornea to reduce gaze error by ~10–16% vs intensity-only, showing PET is a simple, robust modality for wearable eye tracking.


<details>
  <summary>Details</summary>
Motivation: To add a robust contrast mechanism (polarization) to eye tracking that can reveal additional ocular features and improve gaze-estimation robustness for wearable devices.

Method: Use a polarization-filter-array (PFA) camera with linearly polarized NIR illumination to capture polarization-resolved images; train CNN-based gaze-estimation models on PET data and compare to capacity-matched intensity-only baselines across varied conditions and a large cohort.

Result: Across 346 participants, PET-trained CNN models reduced median 95th-percentile absolute gaze error by 10–16% relative to intensity baselines under nominal and challenging conditions (eyelid occlusion, eye-relief changes, pupil-size variation).

Conclusion: Polarization-enabled eye tracking (PET) improves gaze estimation by leveraging polarization-resolved near-infrared imaging to reveal features on sclera and cornea not visible in intensity-only images, producing measurable accuracy gains.

Abstract: Polarization-resolved near-infrared imaging adds a useful optical contrast
mechanism to eye tracking by measuring the polarization state of light
reflected by ocular tissues in addition to its intensity. In this paper we
demonstrate how this contrast can be used to enable eye tracking. Specifically,
we demonstrate that a polarization-enabled eye tracking (PET) system composed
of a polarization--filter--array camera paired with a linearly polarized
near-infrared illuminator can reveal trackable features across the sclera and
gaze-informative patterns on the cornea, largely absent in intensity-only
images. Across a cohort of 346 participants, convolutional neural network based
machine learning models trained on data from PET reduced the median
95th-percentile absolute gaze error by 10--16\% relative to capacity-matched
intensity baselines under nominal conditions and in the presence of eyelid
occlusions, eye-relief changes, and pupil-size variation. These results link
light--tissue polarization effects to practical gains in human--computer
interaction and position PET as a simple, robust sensing modality for future
wearable devices.

</details>


### [58] [Benchmark Designers Should "Train on the Test Set" to Expose Exploitable Non-Visual Shortcuts](https://arxiv.org/abs/2511.04655)
*Ellis Brown,Jihan Yang,Shusheng Yang,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 论文提出了TsT+IBP框架：用在测试集文本上训练的大模型检测语言/偏差捷径，然后迭代剔除高偏样本，得到更能考察视觉理解的去偏基准。


<details>
  <summary>Details</summary>
Motivation: 观察到MLLM在许多多模态基准上可以通过语言线索或表面模式取得高分，从而不能反映真实视觉理解能力，因而需要一种系统化的诊断与去偏流程来提高基准的诊断力和可靠性。

Method: 提出Test-set Stress-Test (TsT) 和 Iterative Bias Pruning (IBP) 两步法：先用在测试集文本输入上微调的大语言模型进行k折交叉验证来评估样本偏置分数s(x)，并用基于手工特征的随机森林做快速可解释诊断；再按偏置分数迭代过滤高偏样本以得到去偏后的基准。

Result: 在对VSI-Bench、CV-Bench、MMMU和VideoMME四个基准的分析中发现普遍的非视觉偏置；并通过构建VSI-Bench-Debiased展示了去偏后非视觉可解率下降、视觉盲模型与完整模型之间差距扩大，表明基准更能测量视觉理解。

Conclusion: 该论文提出了对多模态大模型（MLLM）基准测试进行稳健性诊断与去偏的方法，结论是许多现有视觉中心基准易被语言信号和数据偏差所“作弊”，需要通过有针对性的测试与筛除高偏样本来提升基准可信度。

Abstract: Robust benchmarks are crucial for evaluating Multimodal Large Language Models
(MLLMs). Yet we find that models can ace many multimodal benchmarks without
strong visual understanding, instead exploiting biases, linguistic priors, and
superficial patterns. This is especially problematic for vision-centric
benchmarks that are meant to require visual inputs. We adopt a diagnostic
principle for benchmark design: if a benchmark can be gamed, it will be.
Designers should therefore try to ``game'' their own benchmarks first, using
diagnostic and debiasing procedures to systematically identify and mitigate
non-visual biases. Effective diagnosis requires directly ``training on the test
set'' -- probing the released test set for its intrinsic, exploitable patterns.
  We operationalize this standard with two components. First, we diagnose
benchmark susceptibility using a ``Test-set Stress-Test'' (TsT) methodology.
Our primary diagnostic tool involves fine-tuning a powerful Large Language
Model via $k$-fold cross-validation on exclusively the non-visual, textual
inputs of the test set to reveal shortcut performance and assign each sample a
bias score $s(x)$. We complement this with a lightweight Random Forest-based
diagnostic operating on hand-crafted features for fast, interpretable auditing.
Second, we debias benchmarks by filtering high-bias samples using an
``Iterative Bias Pruning'' (IBP) procedure. Applying this framework to four
benchmarks -- VSI-Bench, CV-Bench, MMMU, and VideoMME -- we uncover pervasive
non-visual biases. As a case study, we apply our full framework to create
VSI-Bench-Debiased, demonstrating reduced non-visual solvability and a wider
vision-blind performance gap than the original.

</details>


### [59] [SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding](https://arxiv.org/abs/2511.04668)
*Ellis Brown,Arijit Ray,Ranjay Krishna,Ross Girshick,Rob Fergus,Saining Xie*

Main category: cs.CV

TL;DR: 利用3D模拟器生成有针对性的三类空间问题训练数据（度量、视角、追踪），少量数据即可显著提升视频LLM在真实世界时空推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 真实世界视频数据难以获取多样且精确的空间标注，限制模型学习时空推理能力，故使用模拟器生成可控且标注丰富的数据以弥补这一瓶颈。

Method: 构建SIMS-V框架，从3D模拟器提取精确空间标注，系统生成各类空间问题并对问题类型、比例和规模进行消融实验；选出对迁移最有效的三类问题（度量测量、视角相关推理、时间追踪），并用这些生成的数据微调视频LLM。

Result: 7B参数的视频LLM在仅25K模拟样本微调后，超过了72B基线模型，并在真实世界空间推理基准上与专有模型表现相近，同时保持对一般视频理解任务的性能。

Conclusion: SIMS-V通过利用3D模拟器生成带空间信息的视频训练数据，显著提升多模态语言模型的时空推理能力，并能以较小数据量实现向真实世界迁移。

Abstract: Despite impressive high-level video comprehension, multimodal language models
struggle with spatial reasoning across time and space. While current spatial
training approaches rely on real-world video data, obtaining diverse footage
with precise spatial annotations remains a bottleneck. To alleviate this
bottleneck, we present SIMS-V -- a systematic data-generation framework that
leverages the privileged information of 3D simulators to create spatially-rich
video training data for multimodal language models. Using this framework, we
investigate which properties of simulated data drive effective real-world
transfer through systematic ablations of question types, mixes, and scales. We
identify a minimal set of three question categories (metric measurement,
perspective-dependent reasoning, and temporal tracking) that prove most
effective for developing transferable spatial intelligence, outperforming
comprehensive coverage despite using fewer question types. These insights
enable highly efficient training: our 7B-parameter video LLM fine-tuned on just
25K simulated examples outperforms the larger 72B baseline and achieves
competitive performance with proprietary models on rigorous real-world spatial
reasoning benchmarks. Our approach demonstrates robust generalization,
maintaining performance on general video understanding while showing
substantial improvements on embodied and real-world spatial tasks.

</details>


### [60] [Cambrian-S: Towards Spatial Supersensing in Video](https://arxiv.org/abs/2511.04670)
*Shusheng Yang,Jihan Yang,Pinzhi Huang,Ellis Brown,Zihao Yang,Yue Yu,Shengbang Tong,Zihan Zheng,Yifan Xu,Muhan Wang,Daohan Lu,Rob Fergus,Yann LeCun,Li Fei-Fei,Saining Xie*

Main category: cs.CV

TL;DR: 作者提出‘空间超感知’四阶段框架并发布VSI-SUPER基准（VSR与VSC）与VSI-590K数据集；大规模训练提升有限基线表现但不足以解决长时序空间理解；提出并验证基于下一潜在帧预测和惊奇驱动记忆的预测感知方法，在挑战性基准上带来明显改进。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型和基准主要侧重视觉-语言语义标注，无法充分评估和推动空间、长期和世界建模能力。为实现真正的多模态智能，需要模型具有跨时序的记忆、隐含三维空间推理和能预测整理经验的内在世界模型，因此需要专门的任务、数据与方法来驱动研究。

Method: 提出空间超感知四阶段框架；构建VSI-SUPER基准（VSR和VSC）以考察长时序视频理解且抗暴力扩展上下文；收集大规模数据集VSI-590K并训练Cambrian-S以测试数据扩展效应；实现并评估自监督的下一潜在帧预测器，利用预测误差（惊奇）驱动记忆写入与事件分割作为预测感知的证明性实现。

Result: 在VSI-Bench上，借由构建VSI-590K并训练Cambrian-S获得约+30%绝对提升且未牺牲通用能力；但在更具挑战性的VSI-SUPER上表现仍有限。引入的预测感知方法在VSI-SUPER上显著超越主要专有基线，表明预测驱动的记忆与事件分割有助于空间超感知。

Conclusion: 本文提出将多模态智能研究从反应式任务驱动和盲目扩展上下文，转向“超感知（supersensing）”范式，强调空间超感知的四个阶段，并指出现有基准覆盖有限。作者构建了VSI-SUPER基准（包含VSR和VSC子任务）、构建了大规模数据集VSI-590K并训练Cambrian-S，取得在VSI-Bench上显著提升，但在VSI-SUPER上仍有明显不足；提出预测感知（predictive sensing）并通过下一时刻潜在帧预测结合惊奇驱动记忆与事件分割的思路取得更好性能。

Abstract: We argue that progress in true multimodal intelligence calls for a shift from
reactive, task-driven systems and brute-force long context towards a broader
paradigm of supersensing. We frame spatial supersensing as four stages beyond
linguistic-only understanding: semantic perception (naming what is seen),
streaming event cognition (maintaining memory across continuous experiences),
implicit 3D spatial cognition (inferring the world behind pixels), and
predictive world modeling (creating internal models that filter and organize
information). Current benchmarks largely test only the early stages, offering
narrow coverage of spatial cognition and rarely challenging models in ways that
require true world modeling. To drive progress in spatial supersensing, we
present VSI-SUPER, a two-part benchmark: VSR (long-horizon visual spatial
recall) and VSC (continual visual spatial counting). These tasks require
arbitrarily long video inputs yet are resistant to brute-force context
expansion. We then test data scaling limits by curating VSI-590K and training
Cambrian-S, achieving +30% absolute improvement on VSI-Bench without
sacrificing general capabilities. Yet performance on VSI-SUPER remains limited,
indicating that scale alone is insufficient for spatial supersensing. We
propose predictive sensing as a path forward, presenting a proof-of-concept in
which a self-supervised next-latent-frame predictor leverages surprise
(prediction error) to drive memory and event segmentation. On VSI-SUPER, this
approach substantially outperforms leading proprietary baselines, showing that
spatial supersensing requires models that not only see but also anticipate,
select, and organize experience.

</details>


### [61] [InfinityStar: Unified Spacetime AutoRegressive Modeling for Visual Generation](https://arxiv.org/abs/2511.04675)
*Jinlai Liu,Jian Han,Bin Yan,Hui Wu,Fengda Zhu,Xing Wang,Yi Jiang,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: InfinityStar是一种纯离散的时空自回归生成框架，统一建模视频与图像生成任务，在质量（VBench 83.74）和速度（生成5s 720p视频约10x更快）上取得显著优势，首次实现离散自回归生成工业级720p视频并开源代码与模型。


<details>
  <summary>Details</summary>
Motivation: 借鉴自回归在视觉和语言领域的成功，作者希望构建一个高效、统一且可扩展的生成模型，解决高分辨率和长时视频生成的效率与质量问题。

Method: 采用纯离散的自回归建模，统一处理空间和时间信息，通过时空自回归生成高分辨率图像和长时视频，未依赖扩散模型或连续表示，并通过优化生成流程以提升速度。

Result: 在VBench上得分83.74，显著优于其他自回归方法，并超越部分扩散模型（如HunyuanVideo）；生成5秒720p视频速度约为扩散方法的10倍；首次实现可产出工业级720p视频的离散自回归视频生成器。

Conclusion: 该论文提出了InfinityStar，一种统一的时空自回归框架，能够在单一离散模型中同时建模空间和时间依赖，支持文本到图像/视频、图像到视频及长时交互视频合成。

Abstract: We introduce InfinityStar, a unified spacetime autoregressive framework for
high-resolution image and dynamic video synthesis. Building on the recent
success of autoregressive modeling in both vision and language, our purely
discrete approach jointly captures spatial and temporal dependencies within a
single architecture. This unified design naturally supports a variety of
generation tasks such as text-to-image, text-to-video, image-to-video, and long
interactive video synthesis via straightforward temporal autoregression.
Extensive experiments demonstrate that InfinityStar scores 83.74 on VBench,
outperforming all autoregressive models by large margins, even surpassing some
diffusion competitors like HunyuanVideo. Without extra optimizations, our model
generates a 5s, 720p video approximately 10x faster than leading
diffusion-based methods. To our knowledge, InfinityStar is the first discrete
autoregressive video generator capable of producing industrial level 720p
videos. We release all code and models to foster further research in efficient,
high-quality video generation.

</details>


### [62] [Tracking and Understanding Object Transformations](https://arxiv.org/abs/2511.04678)
*Yihong Sun,Xinyu Yang,Jennifer J. Sun,Bharath Hariharan*

Main category: cs.CV

TL;DR: 提出Track Any State任务和VOST-TAS数据集；提出零-shot TubeletGraph，通过补救遗漏轨迹并构建状态图实现变换下更鲁棒的跟踪与语义化的状态推理。


<details>
  <summary>Details</summary>
Motivation: Existing trackers lose targets after object transformations due to appearance changes; need to track and describe state changes.

Method: TubeletGraph identifies overlooked tracks, integrates them using semantic and proximity priors, and builds state graphs to describe transformations; zero-shot, temporal grounding and semantic reasoning components.

Result: State-of-the-art tracking under transformations, better understanding of object transformations, effective temporal grounding and semantic reasoning; benchmark and code released.

Conclusion: The paper introduces Track Any State and VOST-TAS and proposes TubeletGraph, demonstrating improved tracking through transformations and state reasoning compared to baselines.

Abstract: Real-world objects frequently undergo state transformations. From an apple
being cut into pieces to a butterfly emerging from its cocoon, tracking through
these changes is important for understanding real-world objects and dynamics.
However, existing methods often lose track of the target object after
transformation, due to significant changes in object appearance. To address
this limitation, we introduce the task of Track Any State: tracking objects
through transformations while detecting and describing state changes,
accompanied by a new benchmark dataset, VOST-TAS. To tackle this problem, we
present TubeletGraph, a zero-shot system that recovers missing objects after
transformation and maps out how object states are evolving over time.
TubeletGraph first identifies potentially overlooked tracks, and determines
whether they should be integrated based on semantic and proximity priors. Then,
it reasons about the added tracks and generates a state graph describing each
observed transformation. TubeletGraph achieves state-of-the-art tracking
performance under transformations, while demonstrating deeper understanding of
object transformations and promising capabilities in temporal grounding and
semantic reasoning for complex object transformations. Code, additional
results, and the benchmark dataset are available at
https://tubelet-graph.github.io.

</details>


### [63] [Carousel: A High-Resolution Dataset for Multi-Target Automatic Image Cropping](https://arxiv.org/abs/2511.04680)
*Rafe Loya,Andrew Hamara,Benjamin Estell,Benjamin Kilpatrick,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 提出生成多张美学裁剪的任务，发布277图像数据集，并证明用图像分区+单裁剪模型能有效生成多样化裁剪。


<details>
  <summary>Details</summary>
Motivation: 现代社交媒体常需在同一张图片上生成多张不同且有美感的裁剪（如轮播展示），但现有工作多聚焦于生成单一最优裁剪，缺乏多样化裁剪方案。

Method: 作者收集并发布了277张与任务相关的图像和人工标注，提出在单裁剪模型前加入图像分区（partitioning）预处理，以生成多且互补的裁剪；并对若干单裁剪模型在该预处理下进行评估。

Result: 他们构建并公开了包含277张图片与人工标注的数据集，实验证明将图像分区作为预处理能让单裁剪模型产生多样化且具有视觉吸引力的裁剪，适用于社交媒体情境。

Conclusion: 该论文提出并解决了生成多张美学吸引力裁剪（multi-crop）的问题，通过构建数据集并评估现有单裁剪模型结合图像分割预处理的效果，展示了该思路在社交媒体场景下的适用性。

Abstract: Automatic image cropping is a method for maximizing the human-perceived
quality of cropped regions in photographs. Although several works have proposed
techniques for producing singular crops, little work has addressed the problem
of producing multiple, distinct crops with aesthetic appeal. In this paper, we
motivate the problem with a discussion on modern social media applications,
introduce a dataset of 277 relevant images and human labels, and evaluate the
efficacy of several single-crop models with an image partitioning algorithm as
a pre-processing step. The dataset is available at
https://github.com/RafeLoya/carousel.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [64] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li,Weiyan Wang,Ruiyuan Li,Chao Chen,Xianlei Long,Linjiang Zheng,Quanqing Xu,Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon是针对浮点时间序列的GPU无损压缩框架，通过异步流水线、精确的float-to-int转换和自适应稀疏位平面编码取得了更高的压缩率与吞吐。


<details>
  <summary>Details</summary>
Motivation: IoT和HPC产生海量浮点时间序列数据，要求无损且高吞吐的压缩，GPU并行性提供潜力，但面临数据移动、精度转换与异常值影响三方面挑战。

Method: 提出轻量级异步流水线隐藏CPU-GPU I/O延迟；设计带理论保证的精确高速浮点到整数转换；提出自适应稀疏位平面无损编码以缓解异常值导致的稀疏性下降。

Result: 在12个数据集上，压缩率较最先进CPU方法提高9.1%，压缩/解压吞吐分别比最快GPU竞争方法高2.43倍和2.4倍。

Conclusion: Falcon成功解决了GPU上面向浮点时间序列数据的无损压缩中遇到的三大挑战，提供了高压缩率与高吞吐的综合性能提升。

Abstract: Domains such as IoT (Internet of Things) and HPC (High Performance Computing)
generate a torrential influx of floating-point time-series data. Compressing
these data while preserving their absolute fidelity is critical, and leveraging
the massive parallelism of modern GPUs offers a path to unprecedented
throughput. Nevertheless, designing such a high-performance GPU-based lossless
compressor faces three key challenges: 1) heterogeneous data movement
bottlenecks, 2) precision-preserving conversion complexity, and 3)
anomaly-induced sparsity degradation. To address these challenges, this paper
proposes Falcon, a GPU-based Floating-point Adaptive Lossless COmpressioN
framework. Specifically, Falcon first introduces a lightweight asynchronous
pipeline, which hides the I/O latency during the data transmission between the
CPU and GPU. Then, we propose an accurate and fast float-to-integer
transformation method with theoretical guarantees, which eliminates the errors
caused by floating-point arithmetic. Moreover, we devise an adaptive sparse
bit-plane lossless encoding strategy, which reduces the sparsity caused by
outliers. Extensive experiments on 12 diverse datasets show that our
compression ratio improves by 9.1% over the most advanced CPU-based method,
with compression throughput 2.43X higher and decompression throughput 2.4X
higher than the fastest GPU-based competitors, respectively.

</details>


### [65] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao,Daniel E. Lucani*

Main category: cs.DB

TL;DR: EntroGD通过样本凝练+熵引导的位选择，将GD位选择复杂度从O(nd^2)降为O(nd)，在多数据集上显著加速配置与分析且保持压缩与分析性能。


<details>
  <summary>Details</summary>
Motivation: 现有GD算法（如GreedyGD）在高维数据上因逐位迭代选择基与偏差导致计算复杂度和训练时间急剧增加，降低样本数量会牺牲性能，因此需要一种在高维下仍能高效且保持压缩与分析性能的方法。

Method: EntroGD先对原始数据生成凝练样本以保留分析保真度，然后基于信息熵对位进行选择以最大化压缩效率；该位选择过程避免了GreedyGD的迭代逐位检查，从而实现线性复杂度。

Result: 在18个不同数据集上，EntroGD在压缩率上与GD基线和通用压缩器相当；配置时间相比GreedyGD最多减少53.5×；在凝练样本上进行聚类可比原始数据加速最多31.6×，且聚类准确度损失可忽略。

Conclusion: EntroGD通过熵引导的位选择和样本凝练，将广义重复数据删除（GD）的位选择复杂度从O(nd^2)降为O(nd)，在保证与GD和通用压缩器相当的压缩率下，大幅减少配置时间并加速基于压缩数据的分析。

Abstract: Generalized Deduplication (GD) enables lossless compression with direct
analytics on compressed data by dividing data into \emph{bases} and
\emph{deviations} and performing dictionary encoding on the former. However, GD
algorithms face scalability challenges for high-dimensional data. For example,
the GreedyGD algorithm relies on an iterative bit-selection process across
$d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to
select bits to be used as bases and deviations. Although the $n$ data rows can
be reduced during training at the expense of performance, highly dimensional
data still experiences a marked loss in performance. This paper introduces
EntroGD, an entropy-guided GD framework that reduces complexity of the
bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step
process. First, it generates condensed samples to preserve analytic fidelity.
Second, it applies entropy-guided bit selection to maximize compression
efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD
achieves compression performance comparable to GD-based and universal
compressors, while reducing configuration time by up to 53.5$\times$ over
GreedyGD and accelerating clustering by up to 31.6$\times$ over the original
data with negligible accuracy loss by performing analytics on the condensed
samples, which are much fewer than original samples. Thus, EntroGD provides an
efficient and scalable solution to performing analytics directly on compressed
data.

</details>
