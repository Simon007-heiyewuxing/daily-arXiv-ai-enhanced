<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 161]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility](https://arxiv.org/abs/2601.17027)
*Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu*

Main category: cs.CV

TL;DR: 研究系统评估科学图像合成，提出逻辑驱动ImgCoder与基准SciGenBench，发现像素模型存在系统性错误，程序化合成更精确且可用于提升LMM推理


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型在科学场景中视觉可信但逻辑错误的问题，提升多模态科学推理能力通过高保真合成数据

Method: 分析方法

Result: 提出ImgCoder方法，比较像素生成和程序化生成，并构建SciGenBench进行评估

Conclusion: 程序化合成在科学精确度上优于纯像素生成，且用高保真合成数据微调LMM可提升多模态推理

Abstract: While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit "understand - plan - code" workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.

</details>


### [2] [Data-Efficient Meningioma Segmentation via Implicit Spatiotemporal Mixing and Sim2Real Semantic Injection](https://arxiv.org/abs/2601.17031)
*Yunhao Xu,Fuquan Zong,Yexuan Xing,Chulong Zhang,Guang Yang,Shilong Yang,Xiaokun Liang,Juan Yu*

Main category: cs.CV

TL;DR: Dual-augmentation: interpolate INR-based deformation fields + transplant lesion textures into healthy scans to boost segmentation from few annotations.


<details>
  <summary>Details</summary>
Motivation: Limited labeled medical images and complex lesion variability demand augmentation strategies that generate realistic anatomical and pathological diversity from few anchors to improve segmentation performance.

Method: 1) Use Implicit Neural Representations to learn continuous velocity fields for anatomy; perform linear mixing on integrated deformation fields to synthesize new anatomically plausible variants. 2) Sim2Real lesion injection: transplant lesion textures into healthy anatomical images to create high-fidelity simulated pathology. Integrate both augmentations in training pipelines for models like nnU-Net.

Result: Proposes a dual-augmentation framework combining spatial manifold expansion via INR-modeled continuous velocity fields and Sim2Real lesion texture injection to improve data efficiency in medical image segmentation.

Conclusion: The framework enhances data efficiency and robustness of segmentation models (nnU-Net, U-Mamba) by generating anatomically plausible deformations and high-fidelity lesion augmentations, improving performance with limited annotations.

Abstract: The performance of medical image segmentation is increasingly defined by the efficiency of data utilization rather than merely the volume of raw data. Accurate segmentation, particularly for complex pathologies like meningiomas, demands that models fully exploit the latent information within limited high-quality annotations. To maximize the value of existing datasets, we propose a novel dual-augmentation framework that synergistically integrates spatial manifold expansion and semantic object injection. Specifically, we leverage Implicit Neural Representations (INR) to model continuous velocity fields. Unlike previous methods, we perform linear mixing on the integrated deformation fields, enabling the efficient generation of anatomically plausible variations by interpolating within the deformation space. This approach allows for the extensive exploration of structural diversity from a small set of anchors. Furthermore, we introduce a Sim2Real lesion injection module. This module constructs a high-fidelity simulation domain by transplanting lesion textures into healthy anatomical backgrounds, effectively bridging the gap between synthetic augmentation and real-world pathology. Comprehensive experiments on a hybrid dataset demonstrate that our framework significantly enhances the data efficiency and robustness of state-of-the-art models, including nnU-Net and U-Mamba, offering a potent strategy for high-performance medical image analysis with limited annotation budgets.

</details>


### [3] [Diagnosis Support of Sickle Cell Anemia by Classifying Red Blood Cell Shape in Peripheral Blood Images](https://arxiv.org/abs/2601.17032)
*Wilkie Delgado-Font,Miriela Escobedo-Nicot,Manuel González-Hidalgo,Silena Herold-Garcia,Antoni Jaume-i-Capó,Arnau Mir*

Main category: cs.CV

TL;DR: 提出基于Chan-Vese分割+CSF/ESF形状分析的自动化红细胞分类方法，对临床样本验证效果优（F1: 正常0.97，延长0.95），可用于镰状细胞贫血诊断支持。


<details>
  <summary>Details</summary>
Motivation: 人工显微镜观察耗时且主观性强，临床上需要自动化、客观、快速的RBC形态学分析以辅助镰状细胞贫血等变形相关疾病的诊断和监测。

Method: 1) 使用Chan-Vese主动轮廓模型从外周血涂片图像中分割目标细胞；2) 采用圆形因子（CSF）和椭圆形因子（ESF）等基本形状描述子对每个分割对象进行形态学分析以分类为正常、延长或其他变形；3) 对簇状部分遮挡的细胞应用椭圆拟合调整以改善形状估计；4) 在临床采集的血片图像上进行验证，并与若干现有方法比较性能。

Result: 在真实临床数据集上该方法对正常细胞与延长细胞的F-measure分别为0.97和0.95，整体多类性能优于一些先进方法，表明该方法在临床诊断支持中具有实用性。

Conclusion: 该文提出了一种基于外周血涂片图像的自动化RBC差异计数方法，使用Chan-Vese主动轮廓分割和基于形状描述子的分类（CSF和ESF），并对部分重叠细胞做椭圆拟合调整。对临床样本验证显示对正常和延长（镰状）细胞的F-measure分别达到0.97和0.95，整体性能优于若干先进方法，适用于镰状细胞贫血的诊断支持。

Abstract: Red blood cell (RBC) deformation is the consequence of several diseases, including sickle cell anemia, which causes recurring episodes of pain and severe pronounced anemia. Monitoring patients with these diseases involves the observation of peripheral blood samples under a microscope, a time-consuming procedure. Moreover, a specialist is required to perform this technique, and owing to the subjective nature of the observation of isolated RBCs, the error rate is high. In this paper, we propose an automated method for differentially enumerating RBCs that uses peripheral blood smear image analysis. In this method, the objects of interest in the image are segmented using a Chan-Vese active contour model. An analysis is then performed to classify the RBCs, also called erythrocytes, as normal or elongated or having other deformations, using the basic shape analysis descriptors: circular shape factor (CSF) and elliptical shape factor (ESF). To analyze cells that become partially occluded in a cluster during sample preparation, an elliptical adjustment is performed to allow the analysis of erythrocytes with discoidal and elongated shapes. The images of patient blood samples used in the study were acquired by a clinical laboratory specialist in the Special Hematology Department of the ``Dr. Juan Bruno Zayas'' General Hospital in Santiago de Cuba. A comparison of the results obtained by the proposed method in our experiments with those obtained by some state-of-the-art methods showed that the proposed method is superior for the diagnosis of sickle cell anemia. This superiority is achieved for evidenced by the obtained F-measure value (0.97 for normal cells and 0.95 for elongated ones) and several overall multiclass performance measures. The results achieved by the proposed method are suitable for the purpose of clinical treatment and diagnostic support of sickle cell anemia.

</details>


### [4] [AMVICC: A Novel Benchmark for Cross-Modal Failure Mode Profiling for VLMs and IGMs](https://arxiv.org/abs/2601.17037)
*Aahana Basappa,Pranay Goel,Anusri Karra,Anish Karra,Asa Gilmore,Kevin Zhu*

Main category: cs.CV

TL;DR: 提出AMVICC基准，比较图像到文本与文本到图像模型在视觉推理上的失败模式，发现共享与特有失败，并指出IGM在操控视觉细节上的不足。


<details>
  <summary>Details</summary>
Motivation: 填补对图像生成与视觉理解模型在基础视觉推理能力比较与失败模式分析的研究空白，助力跨模态对齐研究。

Method: 基于MMVP问题集，设计显式与隐式提示，构建AMVICC；对11个多模态LLM与3个图像生成模型在9类任务上进行评测并统计失败模式，分析模态与模型间的共性与差异。

Result: 该论文构建了跨模态视觉推理基准AMVICC，对11个MLLM和3个IGM在9类视觉推理任务上的失败模式进行了系统分析。

Conclusion: 研究发现许多失败模式在模型与模态间共享，但也存在模型/模态特有的缺陷，IGM对显式提示下的细粒度视觉属性控制能力最弱。

Abstract: We investigated visual reasoning limitations of both multimodal large language models (MLLMs) and image generation models (IGMs) by creating a novel benchmark to systematically compare failure modes across image-to-text and text-to-image tasks, enabling cross-modal evaluation of visual understanding. Despite rapid growth in machine learning, vision language models (VLMs) still fail to understand or generate basic visual concepts such as object orientation, quantity, or spatial relationships, which highlighted gaps in elementary visual reasoning. By adapting MMVP benchmark questions into explicit and implicit prompts, we create \textit{AMVICC}, a novel benchmark for profiling failure modes across various modalities. After testing 11 MLLMs and 3 IGMs in nine categories of visual reasoning, our results show that failure modes are often shared between models and modalities, but certain failures are model-specific and modality-specific, and this can potentially be attributed to various factors. IGMs consistently struggled to manipulate specific visual components in response to prompts, especially in explicit prompts, suggesting poor control over fine-grained visual attributes. Our findings apply most directly to the evaluation of existing state-of-the-art models on structured visual reasoning tasks. This work lays the foundation for future cross-modal alignment studies, offering a framework to probe whether generation and interpretation failures stem from shared limitations to guide future improvements in unified vision-language modeling.

</details>


### [5] [Hybrid Deep Feature Extraction and ML for Construction and Demolition Debris Classification](https://arxiv.org/abs/2601.17038)
*Obai Alashram,Nejad Alagha,Mahmoud AlKakuri,Zeeshan Swaveel,Abigail Copiaco*

Main category: cs.CV

TL;DR: 使用预训练Xception提取特征，结合简单分类器（线性SVM、kNN、Bagged Trees），在自建1800张均衡真实施工场景图片数据集上，达到最高99.5%准确率和宏F1，优于端到端深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 建筑拆除与施工产生大量废弃物，需高效自动化分类以实现可持续管理与资源回收；现有方法在现场条件下鲁棒性与部署成本上存在不足，因此探索轻量、稳健的混合视觉方案。

Method: 采集来自阿联酋工地的1800张四类（瓷砖、混凝土、垃圾、木材）高质量图像；使用预训练Xception网络提取深层特征；对提取的特征训练多种经典机器学习分类器（SVM、kNN、Bagged Trees、LDA、Logistic Regression）；比较各模型性能并分析部署可行性。

Result: Xception特征配合线性SVM、kNN和Bagged Trees表现最佳，实验在平衡数据集上达到最高99.5%准确率与宏F1，显示混合管线在精度与现场部署成本上的优势，并提出与机器人和现场自动化集成的未来方向。

Conclusion: This paper shows a practical, high-accuracy hybrid pipeline for classifying C&D debris by combining Xception deep features with simple ML classifiers, achieving up to 99.5% accuracy and strong macro-F1, outperforming more complex end-to-end DL models and enabling field-deployable solutions.

Abstract: The construction industry produces significant volumes of debris, making effective sorting and classification critical for sustainable waste management and resource recovery. This study presents a hybrid vision-based pipeline that integrates deep feature extraction with classical machine learning (ML) classifiers for automated construction and demolition (C\&D) debris classification. A novel dataset comprising 1,800 balanced, high-quality images representing four material categories, Ceramic/Tile, Concrete, Trash/Waste, and Wood was collected from real construction sites in the UAE, capturing diverse real-world conditions. Deep features were extracted using a pre-trained Xception network, and multiple ML classifiers, including SVM, kNN, Bagged Trees, LDA, and Logistic Regression, were systematically evaluated. The results demonstrate that hybrid pipelines using Xception features with simple classifiers such as Linear SVM, kNN, and Bagged Trees achieve state-of-the-art performance, with up to 99.5\% accuracy and macro-F1 scores, surpassing more complex or end-to-end deep learning approaches. The analysis highlights the operational benefits of this approach for robust, field-deployable debris identification and provides pathways for future integration with robotics and onsite automation systems.

</details>


### [6] [MANGO: A Global Single-Date Paired Dataset for Mangrove Segmentation](https://arxiv.org/abs/2601.17039)
*Junhyuk Heo,Beomkyu Choi,Hyunjin Shin,Darongsae Kwon*

Main category: cs.CV

TL;DR: 作者提出MANGO全球芒果（红树林）数据集：42,703对2020年单日Sentinel-2影像与红树林掩膜，涵盖124国，并用目标检测驱动方法选择最佳单日观测，提供按国家不相交划分的分割基准。


<details>
  <summary>Details</summary>
Motivation: 现有红树林数据集常为年地图、缺少每个影像-掩膜的单日配对、或受限于特定地区/不可公开，限制深度学习模型在全球尺度的泛化与监测能力。

Method: 收集2020年内可用的Sentinel-2影像，针对红树林区域检索所有影像，使用基于目标检测的像素级坐标参考方法，从中选择与年红树林掩膜一致的最优单日观测，生成42,703幅图像-掩膜配对，覆盖124国。此外设计国家不相交的数据划分并评测多种语义分割架构。

Result: 构建了大规模全球红树林单日影像-掩膜数据集（42,703对，124国），并在国家不相交测试集上对多种分割模型给出基准性能，展示数据集能支持可扩展、可靠的全球红树林监测研究。

Conclusion: MANGO填补了现有数据集在单日配对、全球覆盖和公开可获取性方面的空白，为红树林自动检测与监测提供了标准化基准和实践基础，推动全球尺度深度学习方法的发展。

Abstract: Mangroves are critical for climate-change mitigation, requiring reliable monitoring for effective conservation. While deep learning has emerged as a powerful tool for mangrove detection, its progress is hindered by the limitations of existing datasets. In particular, many resources provide only annual map products without curated single-date image-mask pairs, limited to specific regions rather than global coverage, or remain inaccessible to the public. To address these challenges, we introduce MANGO, a large-scale global dataset comprising 42,703 labeled image-mask pairs across 124 countries. To construct this dataset, we retrieve all available Sentinel-2 imagery within the year 2020 for mangrove regions and select the best single-date observations that align with the mangrove annual mask. This selection is performed using a target detection-driven approach that leverages pixel-wise coordinate references to ensure adaptive and representative image-mask pairings. We also provide a benchmark across diverse semantic segmentation architectures under a country-disjoint split, establishing a foundation for scalable and reliable global mangrove monitoring.

</details>


### [7] [FP-THD: Full page transcription of historical documents](https://arxiv.org/abs/2601.17040)
*H Neji,J Nogueras-Iso,J Lacasta,MÁ Latre,FJ García-Marco*

Main category: cs.CV

TL;DR: 结合布局分析与掩码自编码器的文本识别流水线，先分割行再识别，能保留历史文献中特殊字符并对手写、印刷及多语言文本表现良好。


<details>
  <summary>Details</summary>
Motivation: 确保转录过程中保留历史文献中具有意义的特殊字符与符号，同时提高不同文本类型的识别性能。

Method: Extend a line-recognition OCR with layout analysis (text-line segmentation + MAE-based OCR).

Result: Pipeline extracts text lines via layout analysis, then applies masked autoencoder OCR to recognize text; evaluated on multiple datasets; effectively handles handwritten, printed, and multilingual texts.

Conclusion: The proposed pipeline preserves historical characters/symbols and yields efficient digitization of Latin documents from 15th-16th centuries; masked autoencoder shows robust performance across text types.

Abstract: The transcription of historical documents written in Latin in XV and XVI centuries has special challenges as it must maintain the characters and special symbols that have distinct meanings to ensure that historical texts retain their original style and significance. This work proposes a pipeline for the transcription of historical documents preserving these special features. We propose to extend an existing text line recognition method with a layout analysis model. We analyze historical text images using a layout analysis model to extract text lines, which are then processed by an OCR model to generate a fully digitized page. We showed that our pipeline facilitates the processing of the page and produces an efficient result. We evaluated our approach on multiple datasets and demonstrate that the masked autoencoder effectively processes different types of text, including handwritten, printed and multi-language.

</details>


### [8] [Arabic Sign Language Recognition using Multimodal Approach](https://arxiv.org/abs/2601.17041)
*Ghadeer Alanazi,Abir Benabid*

Main category: cs.CV

TL;DR: Combining Leap Motion and fine-tuned VGG16 improved ArSL recognition to 78% on 18-word set; promising but preliminary.


<details>
  <summary>Details</summary>
Motivation: Single-sensor ArSL systems (Leap Motion or RGB) suffer tracking and 3D movement recognition limitations; multimodal fusion may capture complementary spatial/temporal features.

Method: Parallel subnetworks: dense NN for Leap Motion with dropout and L2 regularization; image subnetwork using fine-tuned VGG16 with augmentation; concatenated features fed to fully connected fusion and SoftMax classification.

Result: Multimodal fusion of Leap Motion and RGB for Arabic Sign Language (ArSL) recognition achieved 78% accuracy on 18-word custom dataset; 13 words correctly recognized.

Conclusion: Multimodal approach shows promise over single-sensor methods but limited by dataset size, class imbalance, and potential model/feature fusion suboptimality; further optimization and larger, balanced datasets needed.

Abstract: Arabic Sign Language (ArSL) is an essential communication method for individuals in the Deaf and Hard-of-Hearing community. However, existing recognition systems face significant challenges due to their reliance on single sensor approaches like Leap Motion or RGB cameras. These systems struggle with limitations such as inadequate tracking of complex hand orientations and imprecise recognition of 3D hand movements. This research paper aims to investigate the potential of a multimodal approach that combines Leap Motion and RGB camera data to explore the feasibility of recognition of ArSL. The system architecture includes two parallel subnetworks: a custom dense neural network for Leap Motion data, incorporating dropout and L2 regularization, and an image subnetwork based on a fine-tuned VGG16 model enhanced with data augmentation techniques. Feature representations from both modalities are concatenated in a fusion model and passed through fully connected layers, with final classification performed via SoftMax activation to analyze spatial and temporal features of hand gestures. The system was evaluated on a custom dataset comprising 18 ArSL words, of which 13 were correctly recognized, yielding an overall accuracy of 78%. These results offer preliminary insights into the viability of multimodal fusion for sign language recognition and highlight areas for further optimization and dataset expansion.

</details>


### [9] [Interpretable and Sparse Linear Attention with Decoupled Membership-Subspace Modeling via MCR2 Objective](https://arxiv.org/abs/2601.17042)
*Tianyuan Liu,Libin Hou,Linyuan Wang,Bin Yan*

Main category: cs.CV

TL;DR: 通过解耦MCR2中的成员矩阵与子空间并对优化过程做梯度展开，提出了可解释的稀疏线性注意力DMSA，替换ToST得到的DMST在ImageNet上显著提升性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于MCR2的白盒Transformer因会员矩阵与子空间耦合，在错误的token投影下会产生冗余编码，影响效率与可解释性。为提高编码效率与可解释性，需在结构上解耦两者并导出更稀疏且可解释的注意力机制。

Method: 作者先在MCR2目标中解除成员矩阵与子空间U的函数耦合，改为直接从输入学习成员矩阵，再从全空间S导出稀疏子空间；随后对该优化问题进行梯度展开（unrolled gradient descent），得到一种可解释的稀疏线性注意力算子DMSA，并将其替换进Token Statistics Transformer (ToST)形成DMST进行实验验证。

Result: 在ImageNet-1K上，DMST比ToST在top-1准确率上提升1.08%-1.45%；同时显示了更快的编码缩减速率、更高的计算效率与更强的可解释性。

Conclusion: 该论文提出将MCR2中的“成员矩阵”和“子空间矩阵U”解耦，从而消除错误投影导致的冗余编码，进而通过对优化目标的梯度展开推导出可解释的稀疏线性注意力算子DMSA，并将其替换到ToST中得到DMST，在ImageNet-1K上提高了1.08%-1.45% top-1准确率，同时在编码收敛速度、计算效率和可解释性方面优于现有方法。

Abstract: Maximal Coding Rate Reduction (MCR2)-driven white-box transformer, grounded in structured representation learning, unifies interpretability and efficiency, providing a reliable white-box solution for visual modeling. However, in existing designs, tight coupling between "membership matrix" and "subspace matrix U" in MCR2 causes redundant coding under incorrect token projection. To this end, we decouple the functional relationship between the "membership matrix" and "subspaces U" in the MCR2 objective and derive an interpretable sparse linear attention operator from unrolled gradient descent of the optimized objective. Specifically, we propose to directly learn the membership matrix from inputs and subsequently derive sparse subspaces from the fullspace S. Consequently, gradient unrolling of the optimized MCR2 objective yields an interpretable sparse linear attention operator: Decoupled Membership-Subspace Attention (DMSA). Experimental results on visual tasks show that simply replacing the attention module in Token Statistics Transformer (ToST) with DMSA (we refer to as DMST) not only achieves a faster coding reduction rate but also outperforms ToST by 1.08%-1.45% in top-1 accuracy on the ImageNet-1K dataset. Compared with vanilla Transformer architectures, DMST exhibits significantly higher computational efficiency and interpretability.

</details>


### [10] [Atomic Depth Estimation From Noisy Electron Microscopy Data Via Deep Learning](https://arxiv.org/abs/2601.17046)
*Matan Leibovich,Mai Tan,Adria Marcos-Morales,Sreyas Mohan,Peter A. Crozier,Carlos Fernandez-Granda*

Main category: cs.CV

TL;DR: 将原子柱深度估计问题转化为语义分割，使用训练好的卷积神经网络对受噪声影响的TEM图像进行像素级深度分割；在CeO2纳米颗粒的模拟与真实TEM数据上验证，结果显示估计准确、经过校准并对噪声鲁棒。


<details>
  <summary>Details</summary>
Motivation: 传统基于TEM的深度估计在强噪声下性能欠佳，且原子级深度恢复难以直接从单幅图像提取。将深度估计视为语义分割可利用深度学习的像素级分类能力，提高噪声环境下的深度恢复精度与鲁棒性。

Method: 将深度范围离散化为多个类别，构建像素级深度标签；使用模拟数据（含合成噪声）训练深度卷积神经网络以预测每个像素的深度类别；对网络输出进行校准以获得置信度良好的概率估计；在模拟和真实CeO2 TEM图像上评估性能。

Result: 在模拟数据与真实TEM数据上，方法能准确区分不同深度层的原子柱位置，产生的深度估计与真实值高度一致；校准后的概率估计反映出良好的置信度；在不同噪声水平下仍保持鲁棒性。

Conclusion: 将单幅TEM图像的深度估计改为语义分割并用深度学习训练可在噪声环境下实现准确且经过校准的原子级深度恢复，为实验TEM分析提供可行的三维信息提取途径。

Abstract: We present a novel approach for extracting 3D atomic-level information from transmission electron microscopy (TEM) images affected by significant noise. The approach is based on formulating depth estimation as a semantic segmentation problem. We address the resulting segmentation problem by training a deep convolutional neural network to generate pixel-wise depth segmentation maps using simulated data corrupted by synthetic noise. The proposed method was applied to estimate the depth of atomic columns in CeO2 nanoparticles from simulated images and real-world TEM data. Our experiments show that the resulting depth estimates are accurate, calibrated and robust to noise.

</details>


### [11] [A Contrastive Pre-trained Foundation Model for Deciphering Imaging Noisomics across Modalities](https://arxiv.org/abs/2601.17047)
*Yuanjie Gu,Yiqun Wang,Chaohui Yu,Ang Xuan,Fan Wang,Zhi Lu,Biqin Dong*

Main category: cs.CV

TL;DR: Noisomics提出通过合成噪声基因组与对比学习训练的CoP模型，把噪声从干扰转为信息。作者声称用100个样本实现了强零-shot泛化，显著超越大规模监督基线，能在多种成像任务中无校准地解码设备特性并优化成像策略。


<details>
  <summary>Details</summary>
Motivation: 现代成像传感器将物理信号与复杂算法性伪影混合，使得噪声表征高度依赖数据与设备；现有方法依赖大量有监督数据且将噪声视为需消除的干扰，作者提出将噪声视作携带设备与成像过程信息的“多参数指纹”，从而通过解码噪声实现诊断与优化。

Method: 构建合成噪声基因组以覆盖多参数噪声变异，基于流形假设使用对比学习框架训练CoP，以将语义信号与随机噪声分解开来。训练流程可能包括合成-真实混合数据、对比损失设计以及低样本量的预训练和微调策略。评估通过与监督基线（10万样本训练）比较，以及在12个域外数据集上的零-shot测试来验证泛化性。

Result: 作者报告CoP在仅100个训练样本下超过了使用10万监督样本训练的基线，宣称实现了约三个数量级的数据和计算成本降低；在12个异域数据集上的零-shot测试中，平均估计误差降低63.8%，决定系数提升85.1%；并在消费摄影和深度显微镜两个任务上展示实际应用效果。

Conclusion: CoP（Contrastive Pre-trained Foundation Model）提出了一种将噪声从需抑制的干扰转变为可解码的信息资源的方法，称为“Noisomics”。作者声称通过对噪声流形的对比学习，CoP能用极少的数据（仅100个样本）达到或超过传统监督方法（10万样本）的性能，并在多域数据集上展示出强大的零-shot泛化能力和显著的估计误差降低。论文强调其在消费摄影和深层组织显微成像中的实用性，并主张通过噪声解析实现无设备校准的精确成像诊断。

Abstract: Characterizing imaging noise is notoriously data-intensive and device-dependent, as modern sensors entangle physical signals with complex algorithmic artifacts. Current paradigms struggle to disentangle these factors without massive supervised datasets, often reducing noise to mere interference rather than an information resource. Here, we introduce "Noisomics", a framework shifting the focus from suppression to systematic noise decoding via the Contrastive Pre-trained (CoP) Foundation Model. By leveraging the manifold hypothesis and synthetic noise genome, CoP employs contrastive learning to disentangle semantic signals from stochastic perturbations. Crucially, CoP breaks traditional deep learning scaling laws, achieving superior performance with only 100 training samples, outperforming supervised baselines trained on 100,000 samples, thereby reducing data and computational dependency by three orders of magnitude. Extensive benchmarking across 12 diverse out-of-domain datasets confirms its robust zero-shot generalization, demonstrating a 63.8% reduction in estimation error and an 85.1% improvement in the coefficient of determination compared to the conventional training strategy. We demonstrate CoP's utility across scales: from deciphering non-linear hardware-noise interplay in consumer photography to optimizing photon-efficient protocols for deep-tissue microscopy. By decoding noise as a multi-parametric footprint, our work redefines stochastic degradation as a vital information resource, empowering precise imaging diagnostics without prior device calibration.

</details>


### [12] [SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis](https://arxiv.org/abs/2601.17048)
*Jing Jie Tan,Rupert Schreiner,Matthias Hausladen,Ali Asgharzade,Simon Edler,Julian Bartsch,Michael Bachmann,Andreas Schels,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: 提出SiMiC：带注意力的CNN用于SEM图像中硅微结构的多类分类与尺寸预测，构建专用数据集，性能优于传统图像处理且保持一定可解释性，数据与代码公开。


<details>
  <summary>Details</summary>
Motivation: 传统SEM基于手工测量和经典图像处理，耗时且不一致；需要自动化且可扩展的方法来高精度表征微尺度硅结构并将几何形貌与场发射性能关联。

Method: 构建硅基场发射尖端的标注SEM图像数据集，设计带注意力模块的卷积神经网络用于多类微结构分类和连续维度（如尺寸、顶角曲率）预测；训练时可能使用数据增强、损失函数组合（分类与回归）、可解释性工具（注意力可视化、Grad-CAM等）。

Result: SiMiC在分类和尺寸/曲率预测任务上显著优于传统影像处理方法，提供更高的一致性和自动化水平，同时通过注意力图提高模型可解释性，为将几何特征与场发射行为关联的后续研究奠定基础。

Conclusion: SiMiC提出了一种基于注意力机制的卷积神经网络用于硅微结构（特别是场发射器尖端）的SEM图像表征，能高效提取形貌特征并减少人工干预，为将几何参数与场发射性能关联提供基础。

Abstract: Accurate characterization of silicon microstructures is essential for advancing microscale fabrication, quality control, and device performance. Traditional analysis using Scanning Electron Microscopy (SEM) often requires labor-intensive, manual evaluation of feature geometry, limiting throughput and reproducibility. In this study, we propose SiMiC: Context-Aware Silicon Microstructure Characterization Using Attention-Based Convolutional Neural Networks for Field-Emission Tip Analysis. By leveraging deep learning, our approach efficiently extracts morphological features-such as size, shape, and apex curvature-from SEM images, significantly reducing human intervention while improving measurement consistency. A specialized dataset of silicon-based field-emitter tips was developed, and a customized CNN architecture incorporating attention mechanisms was trained for multi-class microstructure classification and dimensional prediction. Comparative analysis with classical image processing techniques demonstrates that SiMiC achieves high accuracy while maintaining interpretability. The proposed framework establishes a foundation for data-driven microstructure analysis directly linked to field-emission performance, opening avenues for correlating emitter geometry with emission behavior and guiding the design of optimized cold-cathode and SEM electron sources. The related dataset and algorithm repository that could serve as a baseline in this area can be found at https://research.jingjietan.com/?q=SIMIC

</details>


### [13] [Summary of the Unusual Activity Recognition Challenge for Developmental Disability Support](https://arxiv.org/abs/2601.17049)
*Christina Garcia,Nhat Tan Le,Taihei Fujioka,Umang Dobhal,Milyun Ni'ma Shoumi,Thanh Nha Nguyen,Sozo Inoue*

Main category: cs.CV

TL;DR: 该文章概述了ISAS 2025举办的“识别未见：基于姿态数据的异常行为识别”挑战赛，参赛队伍需用骨架关键点区分正常与异常行为，数据具有类别不平衡与时间不规则性，采用LOSO评估，40支队伍参赛，评估以宏平均F1为主，结果显示在低维噪声数据上识别罕见突发动作困难，需更好地建模时间与上下文信息。


<details>
  <summary>Details</summary>
Motivation: 在为有发育障碍个体提供设施时，需要非侵入式、隐私保护的自动异常行为识别方法，基于视频的姿态估计（骨架关键点）可减少隐私问题，但数据低维且含噪，且异常行为稀少，迫切需要评估现有方法的有效性并推动新方法发展。

Method: 组织一次挑战赛：构建反映真实世界不平衡和时间不规则性的姿态数据集（来自模拟场景的视频），任务为依据骨架关键点检测正常/异常行为；采用LOSO策略保证主体无关泛化；评估指标为宏平均F1以处理类别不平衡；吸引40支队伍提交多种方法（传统机器学习和深度学习）。

Result: 参赛方法总体表现有限，特别是在检测罕见、突发的异常行为时准确率低；一些采用时序建模（如RNN、Transformer）和上下文增强的方法表现更好，但仍受噪声与低维特征限制。宏平均F1分数显示问题具有挑战性。

Conclusion: 基于姿态的异常行为识别在隐私敏感场景有潜力，但当前方法受限于数据噪声、低维信息与样本不平衡。未来研究应着重时序与上下文信息融合、数据增强和不平衡学习策略，以推动在医疗与行为监测中负责任的AI应用。

Abstract: This paper presents an overview of the Recognize the Unseen: Unusual Behavior Recognition from Pose Data Challenge, hosted at ISAS 2025. The challenge aims to address the critical need for automated recognition of unusual behaviors in facilities for individuals with developmental disabilities using non-invasive pose estimation data. Participating teams were tasked with distinguishing between normal and unusual activities based on skeleton keypoints extracted from video recordings of simulated scenarios. The dataset reflects real-world imbalance and temporal irregularities in behavior, and the evaluation adopted a Leave-One-Subject-Out (LOSO) strategy to ensure subject-agnostic generalization. The challenge attracted broad participation from 40 teams applying diverse approaches ranging from classical machine learning to deep learning architectures. Submissions were assessed primarily using macro-averaged F1 scores to account for class imbalance. The results highlight the difficulty of modeling rare, abrupt actions in noisy, low-dimensional data, and emphasize the importance of capturing both temporal and contextual nuances in behavior modeling. Insights from this challenge may contribute to future developments in socially responsible AI applications for healthcare and behavior monitoring.

</details>


### [14] [Single-Pixel Vision-Language Model for Intrinsic Privacy-Preserving Behavioral Intelligence](https://arxiv.org/abs/2601.17050)
*Hongjun An,Yiliang Song,Jiawei Shao,Zhe Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 提出SP-VLM，利用单像素传感实现隐私保护的环境监控，并通过视觉-语言融合在低维观测上恢复行为语义，达成在不侵犯身份隐私的前提下检测异常与理解活动。


<details>
  <summary>Details</summary>
Motivation: 传统监控在更衣室、洗手间等场所受限，需一种既能及时发现威胁事件又不侵犯个人隐私的监控方法。单像素传感提供固有的低维隐私保护特性，配合视觉-语言模型可实现语义层面的监控。

Method: 引入单像素传感器获取低维时间序列数据，证明在低采样率下人脸识别等身份恢复不可行；设计视觉-语言融合模块在单像素观测上学习行为语义并应用于异常检测、计数和活动理解任务；通过实验证明存在一个采样率区间能兼顾隐私保护与行为智能。

Result: 实验表明：1) 在低于关键采样率时，主流人脸识别模型无法恢复身份；2) SP-VLM能在严重退化的单像素观测下实现鲁棒的异常检测、人数估计和活动理解；3) 确定了一个实用的采样率范围，在该范围内行为信息出现而身份信息仍被抑制。

Conclusion: 本论文提出的单像素视觉-语言模型（SP-VLM）展示了在隐私敏感环境中进行安全监控的可行方案，通过单像素模态结合视觉-语言推理，在保护身份隐私的同时实现行为语义理解。

Abstract: Adverse social interactions, such as bullying, harassment, and other illicit activities, pose significant threats to individual well-being and public safety, leaving profound impacts on physical and mental health. However, these critical events frequently occur in privacy-sensitive environments like restrooms, and changing rooms, where conventional surveillance is prohibited or severely restricted by stringent privacy regulations and ethical concerns. Here, we propose the Single-Pixel Vision-Language Model (SP-VLM), a novel framework that reimagines secure environmental monitoring. It achieves intrinsic privacy-by-design by capturing human dynamics through inherently low-dimensional single-pixel modalities and inferring complex behavioral patterns via seamless vision-language integration. Building on this framework, we demonstrate that single-pixel sensing intrinsically suppresses identity recoverability, rendering state-of-the-art face recognition systems ineffective below a critical sampling rate. We further show that SP-VLM can nonetheless extract meaningful behavioral semantics, enabling robust anomaly detection, people counting, and activity understanding from severely degraded single-pixel observations. Combining these findings, we identify a practical sampling-rate regime in which behavioral intelligence emerges while personal identity remains strongly protected. Together, these results point to a human-rights-aligned pathway for safety monitoring that can support timely intervention without normalizing intrusive surveillance in privacy-sensitive spaces.

</details>


### [15] [Synthetic Data Guided Feature Selection for Robust Activity Recognition in Older Adults](https://arxiv.org/abs/2601.17053)
*Shuhao Que,Dieuwke van Dartel,Ilse Heeringa,Han Hegeman,Miriam Vollenbroek-Hutten,Ying Wang*

Main category: cs.CV

TL;DR: 构建针对80岁以上老年人的人体活动识别（HAR）系统，使用腰部与大腿加速度计，在模拟日常环境下收集75分钟数据，结合合成数据训练的特征干预模型（FIM）实现对行走、站立、坐下、躺下和姿势转换的高精度识别，尤其提升了临床重要的姿势转换检测。


<details>
  <summary>Details</summary>
Motivation: 目前市售可穿戴设备的HAR算法多以中年人为训练数据，无法可靠识别步态缓慢且变化大的高龄人群活动；在髋部骨折康复中，需要连续量化活动以减缓功能下降，但临床中很少进行此类监测。

Method: 招募24名>80岁健康老年人，佩戴下背与大腿前侧两个加速度计，在模拟自由生活条件下完成75分钟的日常活动（行走、站立、坐、躺和姿势转换）。使用留一人交叉验证评估模型稳健性，并引入合成数据以提升跨参与者泛化。提出基于特征干预的模型（FIM），并与不使用合成数据的对照模型比较。

Result: FIM在合成数据引导下，对各活动的平均F1分别为：行走0.896、站立0.927、坐下0.997、躺下0.937、姿势转换0.816。与未使用合成数据模型相比，FIM在姿势转换（临床高度相关）识别上显著提升。

Conclusion: 初步结果表明在高龄人群中实现稳健活动识别是可行的；需在髋部骨折患者中进一步验证以评估临床实用性。

Abstract: Physical activity during hip fracture rehabilitation is essential for mitigating long-term functional decline in geriatric patients. However, it is rarely quantified in clinical practice. Existing continuous monitoring systems with commercially available wearable activity trackers are typically developed in middle-aged adults and therefore perform unreliably in older adults with slower and more variable gait patterns. This study aimed to develop a robust human activity recognition (HAR) system to improve continuous physical activity recognition in the context of hip fracture rehabilitation. 24 healthy older adults aged over 80 years were included to perform activities of daily living (walking, standing, sitting, lying down, and postural transfers) under simulated free-living conditions for 75 minutes while wearing two accelerometers positioned on the lower back and anterior upper thigh. Model robustness was evaluated using leave-one-subject-out cross-validation. The synthetic data demonstrated potential to improve generalization across participants. The resulting feature intervention model (FIM), aided by synthetic data guidance, achieved reliable activity recognition with mean F1-scores of 0.896 for walking, 0.927 for standing, 0.997 for sitting, 0.937 for lying down, and 0.816 for postural transfers. Compared with a control condition model without synthetic data, the FIM significantly improved the postural transfer detection, i.e., an activity class of high clinical relevance that is often overlooked in existing HAR literature. In conclusion, these preliminary results demonstrate the feasibility of robust activity recognition in older adults. Further validation in hip fracture patient populations is required to assess the clinical utility of the proposed monitoring system.

</details>


### [16] [Ego4OOD: Rethinking Egocentric Video Domain Generalization via Covariate Shift Scoring](https://arxiv.org/abs/2601.17056)
*Zahra Vaseqi,James Clark*

Main category: cs.CV

TL;DR: 引入Ego4OOD基准，提出协变量漂移度量和one-vs-all二元训练策略，轻量模型在域泛化任务上效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有自视角域泛化基准混淆协变量漂移与概念漂移，难以评估模型泛化能力；需一个侧重协变量多样性并减少概念漂移的基准。

Method: 构建Ego4OOD（8个地理域，语义一致的瞬间级动作类），提出聚类基的协变量漂移指标；采用one-vs-all二元化多类识别，将每类作为独立二分类，以降低类别间干扰；使用轻量两层全连接网络作为分类器进行评测。

Result: Ego4OOD是一个基于Ego4D的数据集，针对可度量的协变量多样性并减少概念漂移，包含8个地理域，提出基于聚类的协变量漂移度量和one-vs-all二元训练目标，并用轻量两层全连接网络在Argo1M和Ego4OOD上取得竞争性结果。

Conclusion: 通过降低类间干扰并量化协变量漂移，one-vs-all二元化方法在协变量分布变化下表现稳定，且轻量模型可匹敌复杂方法；同时强调需要受控的基准和定量域表征。

Abstract: Egocentric video action recognition under domain shifts remains challenging due to large intra-class spatio-temporal variability, long-tailed feature distributions, and strong correlations between actions and environments. Existing benchmarks for egocentric domain generalization often conflate covariate shifts with concept shifts, making it difficult to reliably evaluate a model's ability to generalize across input distributions. To address this limitation, we introduce Ego4OOD, a domain generalization benchmark derived from Ego4D that emphasizes measurable covariate diversity while reducing concept shift through semantically coherent, moment-level action categories. Ego4OOD spans eight geographically distinct domains and is accompanied by a clustering-based covariate shift metric that provides a quantitative proxy for domain difficulty. We further leverage a one-vs-all binary training objective that decomposes multi-class action recognition into independent binary classification tasks. This formulation is particularly well-suited for covariate shift by reducing interference between visually similar classes under feature distribution shift. Using this formulation, we show that a lightweight two-layer fully connected network achieves performance competitive with state-of-the-art egocentric domain generalization methods on both Argo1M and Ego4OOD, despite using fewer parameters and no additional modalities. Our empirical analysis demonstrates a clear relationship between measured covariate shift and recognition performance, highlighting the importance of controlled benchmarks and quantitative domain characterization for studying out-of-distribution generalization in egocentric video.

</details>


### [17] [A Computer Vision Pipeline for Iterative Bullet Hole Tracking in Rifle Zeroing](https://arxiv.org/abs/2601.17062)
*Robert M. Belcher,Brendan C. Degryse,Leonard R. Kosta,Christopher J. Lowrance*

Main category: cs.CV

TL;DR: 提出一个结合YOLOv8、IoU追踪、去除式数据增强与ORB透视校正的系统，实现从射击线图像自动检测并按射次跟踪子弹孔，检测mAP=97.0%、迭代分配准确率=88.8%。


<details>
  <summary>Details</summary>
Motivation: 传统瞄准调零依赖人工现场检查子弹孔，受限于射场安全流程并伴随人为误判风险，需一种能在射击线直接、自动区分多次射击子弹孔的方案以提升效率和安全。

Method: 1) 使用ORB特征进行透视校正与目标朝向标准化的预处理；2) 采用YOLOv8进行小目标（子弹孔）检测；3) 在检测后用IoU分析对连续帧中的子弹孔进行匹配与按射次区分；4) 提出去除式数据增强（通过随机移除目标模拟连续射击造成的子弹孔演变）来弥补缺乏标注序列数据的问题。

Result: 在测试集上实现子弹孔检测mAP=97.0%，并在连续图像的射次分配任务上达到88.8%的准确率，展示了该方法在射击零点调整任务中的实用性与可推广性。

Conclusion: 该论文提出了针对步枪瞄准调零过程的端到端计算机视觉系统，实现了从射击线拍摄图片中自动检测子弹孔并按射击次序跟踪。通过将YOLOv8用于小目标检测并结合IoU进行序列间的目标区分，配合去除式数据增强和ORB透视校正预处理，提高了模型在有限数据下的鲁棒性和准确性。结果显示检测mAP为97.0%，迭代分配准确率为88.8%，表明该方法在实战应用中能显著减少人工检查工作量并提升安全性。

Abstract: Adjusting rifle sights, a process commonly called "zeroing," requires shooters to identify and differentiate bullet holes from multiple firing iterations. Traditionally, this process demands physical inspection, introducing delays due to range safety protocols and increasing the risk of human error. We present an end-to-end computer vision system for automated bullet hole detection and iteration-based tracking directly from images taken at the firing line. Our approach combines YOLOv8 for accurate small-object detection with Intersection over Union (IoU) analysis to differentiate bullet holes across sequential images. To address the scarcity of labeled sequential data, we propose a novel data augmentation technique that removes rather than adds objects to simulate realistic firing sequences. Additionally, we introduce a preprocessing pipeline that standardizes target orientation using ORB-based perspective correction, improving model accuracy. Our system achieves 97.0% mean average precision on bullet hole detection and 88.8% accuracy in assigning bullet holes to the correct firing iteration. While designed for rifle zeroing, this framework offers broader applicability in domains requiring the temporal differentiation of visually similar objects.

</details>


### [18] [A Mechanistic View on Video Generation as World Models: State and Dynamics](https://arxiv.org/abs/2601.17067)
*Luozhou Wang,Zhifei Chen,Yihua Du,Dongyu Yan,Wenhang Ge,Guibao Shen,Xinli Xu,Leyi Wu,Man Chen,Tianshuo Xu,Peiran Ren,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了一个以状态为中心的视频世界模型分类法，区分隐式/显式状态构建与知识/架构驱动的动力学建模，主张采用功能性评测并强调持久性与因果性的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视频生成在物理一致性上展现出能力，但与以状态为核心的经典世界模型理论存在差距，亟需一种将“无状态”视频模型与“有状态”世界模型理论连接的分类框架。

Method: 提出基于两轴（状态构建、动力学建模）的分类法。状态构建分为隐式（上下文管理）与显式（潜变量压缩）；动力学建模通过知识整合（如物理先验）和架构重构（如记忆模块）来分析。并提出评估从视觉质量转向测试物理持续性与因果推理的功能性基准。

Result: 提出分类框架与研究路线图，识别出增强持久性（数据驱动记忆、压缩保真）和提升因果性（潜变量解耦、推理先验整合）作为未来关键方向。

Conclusion: 本文提出了基于“状态构建”和“动力学建模”的视频世界模型分类法，指出需要从视觉逼真度评估转向功能性评测，并提出持久性和因果性两大研究前沿。

Abstract: Large-scale video generation models have demonstrated emergent physical coherence, positioning them as potential world models. However, a gap remains between contemporary "stateless" video architectures and classic state-centric world model theories. This work bridges this gap by proposing a novel taxonomy centered on two pillars: State Construction and Dynamics Modeling. We categorize state construction into implicit paradigms (context management) and explicit paradigms (latent compression), while dynamics modeling is analyzed through knowledge integration and architectural reformulation. Furthermore, we advocate for a transition in evaluation from visual fidelity to functional benchmarks, testing physical persistence and causal reasoning. We conclude by identifying two critical frontiers: enhancing persistence via data-driven memory and compressed fidelity, and advancing causality through latent factor decoupling and reasoning-prior integration. By addressing these challenges, the field can evolve from generating visually plausible videos to building robust, general-purpose world simulators.

</details>


### [19] [Superpixel-Based Image Segmentation Using Squared 2-Wasserstein Distances](https://arxiv.org/abs/2601.17071)
*Jisui Huang,Andreas Alpers,Ke Chen,Na Lei*

Main category: cs.CV

TL;DR: 提出了一种用于存在强非齐次性的图像分割方法：先通过线性最小二乘指派将像素聚成超像素（可视为离散最优传输问题特例），再用平方2-瓦瑟斯坦距离将超像素按经验分布贪婪合并为对象分割。相较于基于均值颜色的合并策略，利用分布式OT距离在两级聚类上统一表述，提高了在困难影像上的分割精度并保持高效性。


<details>
  <summary>Details</summary>
Motivation: 传统超像素合并通常只比较均值颜色，难以处理强光照和纹理非齐次造成的内部分布差异，导致分割错误。提出将像素到超像素以及超像素到对象的合并都用分布间距离（OT）度量，以更好地捕获局部色彩/纹理分布信息，从而提高鲁棒性。

Method: 两级聚类框架：1) 像素到超像素：将像素分配到超像素中心，形式化为线性最小二乘指派问题，可看作离散OT的特例，求解快速且保持局部一致性；2) 超像素到对象：计算每个超像素的经验分布，并使用平方2-瓦瑟斯坦距离作为合并度量，采用贪婪合并策略逐步合并最相似的超像素簇，直到满足停止条件（如数量或距离阈值）。算法强调分布性距离替代均值距离以处理非齐次性。

Result: 数值实验显示，与基线方法相比，在具有强非齐次性（如光照不均、纹理复杂）的图像上分割精度提高，且计算效率仍然很高（方法在超像素生成和瓦瑟斯坦距离计算上有可优化的实现）。

Conclusion: 通过将离散OT观点引入两级聚类——从像素到超像素再到对象—并用平方2-瓦瑟斯坦距离做超像素合并，提出的方法在保持效率的同时提升了对非齐次图像的分割性能，为基于分布的图像聚类提供了统一且有效的框架。

Abstract: We present an efficient method for image segmentation in the presence of strong inhomogeneities. The approach can be interpreted as a two-level clustering procedure: pixels are first grouped into superpixels via a linear least-squares assignment problem, which can be viewed as a special case of a discrete optimal transport (OT) problem, and these superpixels are subsequently greedily merged into object-level segments using the squared 2-Wasserstein distance between their empirical distributions. In contrast to conventional superpixel merging strategies based on mean-color distances, our framework employs a distributional OT distance, yielding a mathematically unified formulation across both clustering levels. Numerical experiments demonstrate that this perspective leads to improved segmentation accuracy on challenging images while retaining high computational efficiency.

</details>


### [20] [GlassesGB: Controllable 2D GAN-Based Eyewear Personalization for 3D Gaussian Blendshapes Head Avatars](https://arxiv.org/abs/2601.17088)
*Rui-Yang Ju,Jen-Shiun Chiang*

Main category: cs.CV

TL;DR: GlassesGB把2D可定制眼镜生成与3D Gaussian Blendshapes头部表示结合，支持在3D头部头像上进行个性化、实时的眼镜试戴，弥合了GlassesGAN与3D渲染之间的差距。


<details>
  <summary>Details</summary>
Motivation: 动机是现有VTON方法局限于预定义眼镜模板，缺乏细粒度的用户驱动定制；而GlassesGAN只能生成2D图像，无法直接用于3D头部头像和VR试戴，故需将2D生成能力扩展到3D头像渲染。

Method: 方法上，作者在GlassesGAN的2D图像生成基础上引入3D Gaussian Blendshapes来构建头部与眼镜的几何表示，通过映射/投影机制将2D生成的眼镜形状与纹理映射到3D头模型上，并在渲染管线中融合以支持VR场景下的实时交互与视角变化。

Result: 提出的GlassesGB能够为3D头部头像生成个性化、可交互的眼镜模型，并实现与头部几何一致的渲染，提升VR虚拟试戴的可定制性与真实感。作者公开了实现代码。

Conclusion: 本文提出了GlassesGB，将2D可定制眼镜生成（借鉴GlassesGAN）与3D高斯基形(blendshapes)头部重建相结合，实现可交互的3D虚拟试戴与个性化眼镜设计。

Abstract: Virtual try-on systems allow users to interactively try different products within VR scenarios. However, most existing VTON methods operate only on predefined eyewear templates and lack support for fine-grained, user-driven customization. While GlassesGAN enables personalized 2D eyewear design, its capability remains limited to 2D image generation. Motivated by the success of 3D Gaussian Blendshapes in head reconstruction, we integrate these two techniques and propose GlassesGB, a framework that supports customizable eyewear generation for 3D head avatars. GlassesGB effectively bridges 2D generative customization with 3D head avatar rendering, addressing the challenge in achieving personalized eyewear design for VR applications. The implementation code is available at https://ruiyangju.github.io/GlassesGB.

</details>


### [21] [GRASP: Guided Region-Aware Sparse Prompting for Adapting MLLMs to Remote Sensing](https://arxiv.org/abs/2601.17089)
*Qigan Sun,Chaoning Zhang,Jianwei Zhang,Xudong Wang,Jiehui Xie,Pengcheng Zheng,Haoyu Wang,Sungyoung Lee,Chi-lok Andy Tai,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GRASP通过区域感知的稀疏软提示和问题引导融合，使MLLM更专注于遥感图像中的相关目标，从而在RSVQA任务中以极少新增参数取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 遥感图像存在大尺度变化、目标稀疏分布和复杂的区域语义，直接应用现有微调方法会导致对背景噪声过拟合或忽视目标细节，因此需要一种能聚焦相关区域并保持参数高效的微调策略。

Method: 提出空间结构化软提示（与冻结的视觉token网格中的空间块对应）与问题引导的稀疏融合模块。先将视觉token网格划分为空间块，给每个块分配可学习的软提示；在推理时根据问题对这些块进行稀疏加权聚合，生成紧凑的全局提示输入语言模型。

Result: 在多个遥感视觉问答（RSVQA）基准上，GRASP在参数效率高的前提下取得了与现有微调和提示方法具有竞争力的性能，表明其在过滤无关背景和增强目标敏感性方面有效。

Conclusion: GRASP通过在视觉token网格上引入与空间块关联的结构化软提示，并结合问题引导的稀疏融合机制，有效聚焦目标区域并过滤背景噪声，从而提升了在遥感VQA任务中的性能，同时保持参数高效性。

Abstract: In recent years, Multimodal Large Language Models (MLLMs) have made significant progress in visual question answering tasks. However, directly applying existing fine-tuning methods to remote sensing (RS) images often leads to issues such as overfitting on background noise or neglecting target details. This is primarily due to the large-scale variations, sparse target distributions, and complex regional semantic features inherent in RS images. These challenges limit the effectiveness of MLLMs in RS tasks. To address these challenges, we propose a parameter-efficient fine-tuning (PEFT) strategy called Guided Region-Aware Sparse Prompting (GRASP). GRASP introduces spatially structured soft prompts associated with spatial blocks extracted from a frozen visual token grid. Through a question-guided sparse fusion mechanism, GRASP dynamically aggregates task-specific context into a compact global prompt, enabling the model to focus on relevant regions while filtering out background noise. Extensive experiments on multiple RSVQA benchmarks show that GRASP achieves competitive performance compared to existing fine-tuning and prompt-based methods while maintaining high parameter efficiency.

</details>


### [22] [LoD Sketch Extraction from Architectural Models Using Generative AI: Dataset Construction for Multi-Level Architectural Design Generation](https://arxiv.org/abs/2601.17095)
*Xusheng Du,Athiwat Kongkaeo,Ye Zhang,Haoran Xie*

Main category: cs.CV

TL;DR: 提出一种结合计算机视觉与生成式AI的自动LoD草图提取框架，能逐级简化高细节建筑模型并保持几何一致性，为生成式建筑建模提供高质量多LoD数据。


<details>
  <summary>Details</summary>
Motivation: 传统LoD建模依赖人工操作，耗时且易生几何不一致；生成式AI受限于缺乏高质量的成对LoD训练数据，因此需要自动化生成几何一致的多LoD训练样本的技术。

Method: 框架结合计算机视觉与生成式AI，采用自上而下的逐步简化流程：从高细节模型（LoD3）开始，依序通过语义抽象和体积化处理生成LoD2与LoD1草图。评价使用结构相似性（SSIM）和归一化Hausdorff距离衡量不同LoD间的几何一致性。

Result: 方法在LoD3->LoD2和LoD2->LoD1的转换上分别取得SSIM 0.7319和0.7532；对应归一化Hausdorff距离为图像对角线的25.1%和61.0%，表明在抽象过程中保持了全局结构同时控制了几何偏差。

Conclusion: 本文提出的自动LoD草图提取框架能有效生成几何一致且语义上分层的多级细节（LoD）表示，为AI驱动的建筑多层次建模提供了可靠数据与技术支持。

Abstract: For architectural design, representation across multiple Levels of Details (LoD) is essential for achieving a smooth transition from conceptual massing to detailed modeling. However, traditional LoD modeling processes rely on manual operations that are time-consuming, labor-intensive, and prone to geometric inconsistencies. While the rapid advancement of generative artificial intelligence (AI) has opened new possibilities for generating multi-level architectural models from sketch inputs, its application remains limited by the lack of high-quality paired LoD training data. To address this issue, we propose an automatic LoD sketch extraction framework using generative AI models, which progressively simplifies high-detail architectural models to automatically generate geometrically consistent and hierarchically coherent multi-LoD representations. The proposed framework integrates computer vision techniques with generative AI methods to establish a progressive extraction pipeline that transitions from detailed representations to volumetric abstractions. Experimental results demonstrate that the method maintains strong geometric consistency across LoD levels, achieving SSIM values of 0.7319 and 0.7532 for the transitions from LoD3 to LoD2 and from LoD2 to LoD1, respectively, with corresponding normalized Hausdorff distances of 25.1% and 61.0% of the image diagonal, reflecting controlled geometric deviation during abstraction. These results verify that the proposed framework effectively preserves global structure while achieving progressive semantic simplification across different LoD levels, providing reliable data and technical support for AI-driven multi-level architectural generation and hierarchical modeling.

</details>


### [23] [Performance uncertainty in medical image analysis: a large-scale investigation of confidence intervals](https://arxiv.org/abs/2601.17103)
*Pascaline André,Charles Heitz,Evangelia Christodoulou,Annika Reinke,Carole H. Sudre,Michela Antonelli,Patrick Godau,M. Jorge Cardoso,Antoine Gilson,Sophie Tezenas du Montcel,Gaël Varoquaux,Lena Maier-Hein,Olivier Colliot*

Main category: cs.CV

TL;DR: 大规模实证研究显示，样本量、性能指标、聚合策略与任务类型共同决定置信区间在医学影像AI中的可靠性与精度，不同CI方法适用性存在差异，需据此制定报告指南。


<details>
  <summary>Details</summary>
Motivation: 医学影像AI临床转化要求对模型性能不确定性有可靠量化，但社区对各种CI方法及其在特定设置下的表现缺乏系统认识，本研究旨在填补该知识空白，指导后续报告规范制定。

Method: 在24个分割与分类任务上，针对每类任务训练并使用19个模型，评估多种常用性能指标、不同聚合策略（如micro与macro）与多种常用CI估计方法，统计评估各CI方法的覆盖率（可靠性）与宽度（精度），并分析这些指标与样本量、指标类型、聚合策略及问题类型之间的关系。

Result: 主要发现包括：1) 不同研究参数下，获得可靠CI所需样本量从几十到数千不等；2) 性能指标的选择强烈影响CI行为；3) 聚合策略（如macro比micro）显著影响CI的可靠性，通常需要更多样本；4) 问题类型（分割 vs 分类）会调制上述效应；5) 各CI方法在不同用例中可靠性与精度不一。

Conclusion: 本研究系统评估了多种置信区间（CI）方法在医学影像AI中对分割与分类任务性能不确定性量化的表现，指出样本量、性能指标、聚合策略与任务类型均显著影响CI的可靠性与精度，并发现不同CI方法在不同情境下表现差异较大。

Abstract: Performance uncertainty quantification is essential for reliable validation and eventual clinical translation of medical imaging artificial intelligence (AI). Confidence intervals (CIs) play a central role in this process by indicating how precise a reported performance estimate is. Yet, due to the limited amount of work examining CI behavior in medical imaging, the community remains largely unaware of how many diverse CI methods exist and how they behave in specific settings. The purpose of this study is to close this gap. To this end, we conducted a large-scale empirical analysis across a total of 24 segmentation and classification tasks, using 19 trained models per task group, a broad spectrum of commonly used performance metrics, multiple aggregation strategies, and several widely adopted CI methods. Reliability (coverage) and precision (width) of each CI method were estimated across all settings to characterize their dependence on study characteristics. Our analysis revealed five principal findings: 1) the sample size required for reliable CIs varies from a few dozens to several thousands of cases depending on study parameters; 2) CI behavior is strongly affected by the choice of performance metric; 3) aggregation strategy substantially influences the reliability of CIs, e.g. they require more observations for macro than for micro; 4) the machine learning problem (segmentation versus classification) modulates these effects; 5) different CI methods are not equally reliable and precise depending on the use case. These results form key components for the development of future guidelines on reporting performance uncertainty in medical imaging AI.

</details>


### [24] [StealthMark: Harmless and Stealthy Ownership Verification for Medical Segmentation via Uncertainty-Guided Backdoors](https://arxiv.org/abs/2601.17107)
*Qinkai Yu,Chong Zhang,Gaojie Jin,Tianjin Huang,Wei Zhou,Wenhui Li,Xiaobo Jin,Bo Huang,Yitian Zhao,Guang Yang,Gregory Y. H. Lip,Yalin Zheng,Aline Villavicencio,Yanda Meng*

Main category: cs.CV

TL;DR: StealthMark 是一种对医疗分割模型隐蔽且无害的黑盒所有权验证方法，通过修改不确定性并用 LIME 提取解释，触发时产生二维码水印，能高效识别所有权且对性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注成本高且受隐私伦理限制，导致训练的医疗分割模型成为重要且需要保护的资产；现有模型保护方法多关注分类与生成任务，分割模型保护尚不充分，故提出专门针对分割模型的隐蔽无害验证方法。

Method: 在模型推理过程中通过微调不确定性（但不改变分割结果）嵌入触发条件；使用模型不可知的解释器（LIME）从输出中提取特征重要性图；将预先设计的二维码作为水印模板，并在特定触发输入下使解释图匹配该二维码，从而进行黑盒所有权验证。

Result: 在四个医疗影像数据集和五种主流分割模型（包括 SAM）上广泛实验，StealthMark 在维持模型性能（Dice与AUC下降<1%）的同时，实现了高验证成功率（如在 SAM 上 ASR>95%），显著优于基于后门的水印方法。

Conclusion: StealthMark 提出了一种在黑盒条件下验证医疗分割模型所有权的新方法，通过在不改变最终分割输出的前提下微调模型不确定性并利用模型无关的解释方法（如 LIME）提取特征归因，触发时显示可识别的二维码水印，达到高验证成功率且对模型性能影响极小。

Abstract: Annotating medical data for training AI models is often costly and limited due to the shortage of specialists with relevant clinical expertise. This challenge is further compounded by privacy and ethical concerns associated with sensitive patient information. As a result, well-trained medical segmentation models on private datasets constitute valuable intellectual property requiring robust protection mechanisms. Existing model protection techniques primarily focus on classification and generative tasks, while segmentation models-crucial to medical image analysis-remain largely underexplored. In this paper, we propose a novel, stealthy, and harmless method, StealthMark, for verifying the ownership of medical segmentation models under black-box conditions. Our approach subtly modulates model uncertainty without altering the final segmentation outputs, thereby preserving the model's performance. To enable ownership verification, we incorporate model-agnostic explanation methods, e.g. LIME, to extract feature attributions from the model outputs. Under specific triggering conditions, these explanations reveal a distinct and verifiable watermark. We further design the watermark as a QR code to facilitate robust and recognizable ownership claims. We conducted extensive experiments across four medical imaging datasets and five mainstream segmentation models. The results demonstrate the effectiveness, stealthiness, and harmlessness of our method on the original model's segmentation performance. For example, when applied to the SAM model, StealthMark consistently achieved ASR above 95% across various datasets while maintaining less than a 1% drop in Dice and AUC scores, significantly outperforming backdoor-based watermarking methods and highlighting its strong potential for practical deployment. Our implementation code is made available at: https://github.com/Qinkaiyu/StealthMark.

</details>


### [25] [iFSQ: Improving FSQ for Image Generation with 1 Line of Code](https://arxiv.org/abs/2601.17124)
*Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan*

Main category: cs.CV

TL;DR: iFSQ: a one-line fix to FSQ that enforces uniform prior and prevents activation collapse, enabling fair comparisons between discrete and continuous image generative models; finds ~4 bits/dim optimal and shows ARs learn faster early while diffusion models reach better ultimate quality; also provides LlamaGen-REPA.


<details>
  <summary>Details</summary>
Motivation: Bridge the methodological gap between discrete-token AR models and continuous-latent diffusion models caused by VQ-VAE vs VAE distinctions, enabling unified modeling and fair benchmarking; address FSQ's activation collapse due to equal-interval quantization causing trade-offs between fidelity and efficiency.

Method: Replace FSQ activation with a distribution-matching mapping enforcing uniform prior; mathematical proof of optimal bin utilization and reconstruction precision; use iFSQ to quantify bits-per-dimension trade-offs and benchmark AR vs diffusion under equal reconstruction constraints; adapt Representation Alignment (REPA) to AR models to create LlamaGen-REPA.

Result: iFSQ mathematically guarantees optimal bin utilization and reconstruction precision with minimal code change. Empirically, optimal representation is ~4 bits/dim; AR models show faster early convergence but diffusion models achieve higher final performance; LlamaGen-REPA demonstrates improved AR modeling when using REPA adaptation.

Conclusion: This paper presents iFSQ, an improved Finite Scalar Quantization method that replaces FSQ's activation with a distribution-matching mapping to enforce a uniform prior, resolving activation collapse and balancing reconstruction fidelity and information efficiency. It uses iFSQ as a controlled benchmark to compare AR and diffusion models, finding optimal discrete-continuous trade-off around 4 bits/dim, ARs converge faster early while diffusion models reach higher ceilings, and adapts REPA to AR yielding LlamaGen-REPA.

Abstract: The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at https://github.com/Tencent-Hunyuan/iFSQ

</details>


### [26] [Scaling medical imaging report generation with multimodal reinforcement learning](https://arxiv.org/abs/2601.17151)
*Qianchu Liu,Sheng Zhang,Guanghui Qin,Yu Gu,Ying Jin,Sam Preston,Yanbo Xu,Sid Kiblawi,Wen-wai Yim,Tim Ossowski,Tristan Naumann,Mu Wei,Hoifung Poon*

Main category: cs.CV

TL;DR: 使用强化学习直接优化下游评估指标的通用医疗报告生成框架UniRG，在胸片报告任务上显著优于监督微调，ReXrank上实现新SOTA并具更好泛化。


<details>
  <summary>Details</summary>
Motivation: 前沿大模型在纯文本推理表现强劲，但在多模态理解和推理（如医学影像报告生成）存在明显差距；监督微调易过拟合表层样式，不能保证跨机构泛化，需要直接优化下游实际评估指标的方案。

Method: 提出通用框架UniRG：以强化学习为核心，利用评价指标作为奖励信号来训练模型（例如UniRG-CXR用于胸片），以避免对模板式样板过拟合，替代或补充监督微调。训练使用公开的CXR数据，并在权威基准ReXrank上评估。

Result: 在胸片报告生成任务及严格评估场景下，UniRG-CXR在ReXrank基准上达到新的整体SOTA，较先前方法有大幅提升，并展示更稳健的跨机构与临床实践泛化能力。

Conclusion: UniRG通过将强化学习作为统一机制，直接优化面向实际应用的评估指标，从而在医疗影像报告生成上较监督微调实现显著提升，并保持在不同机构与临床实践间的稳健泛化。

Abstract: Frontier models have demonstrated remarkable capabilities in understanding and reasoning with natural-language text, but they still exhibit major competency gaps in multimodal understanding and reasoning especially in high-value verticals such as biomedicine. Medical imaging report generation is a prominent example. Supervised fine-tuning can substantially improve performance, but they are prone to overfitting to superficial boilerplate patterns. In this paper, we introduce Universal Report Generation (UniRG) as a general framework for medical imaging report generation. By leveraging reinforcement learning as a unifying mechanism to directly optimize for evaluation metrics designed for end applications, UniRG can significantly improve upon supervised fine-tuning and attain durable generalization across diverse institutions and clinical practices. We trained UniRG-CXR on publicly available chest X-ray (CXR) data and conducted a thorough evaluation in CXR report generation with rigorous evaluation scenarios. On the authoritative ReXrank benchmark, UniRG-CXR sets new overall SOTA, outperforming prior state of the art by a wide margin.

</details>


### [27] [LGDWT-GS: Local and Global Discrete Wavelet-Regularized 3D Gaussian Splatting for Sparse-View Scene Reconstruction](https://arxiv.org/abs/2601.17185)
*Shima Salehi,Atharva Agashe,Andrew J. McFarland,Joshua Peeples*

Main category: cs.CV

TL;DR: 提出结合全局与局部频域正则化的少样本3DGS重建方法，并伴随四波段多光谱植物数据集与评测套件，实验证明在稀疏视角下可获得更稳定、清晰且光谱一致的重建。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在稀疏视角场景中常出现几何不稳定与细节丢失的问题；另外，缺乏针对多光谱植物场景的公开数据与统一少样本重建评测协议，限制了方法比较与进步。

Method: 方法结合全局与局部频域正则化：全局频率正则化用于抑制低频漂移与整体几何不稳定，局部频率正则化用于保留高频纹理细节；该正则化被整合进3D Gaussian Splatting训练流程中以稳定稀疏视角下的优化。

Result: 在新构建的四波段多光谱温室数据集及标准数据集上，所提方法在细节清晰度、几何稳定性和光谱一致性方面均优于基线方法。作者同时发布数据集与代码以促进社区复现与扩展。

Conclusion: 本文提出了在稀疏视角下对3D Gaussian Splatting模型进行频域正则化的少样本3D重建方法，显著提升了几何稳定性与细节保留，并伴随公开的多光谱温室数据集与评测套件，验证了方法在空间与光谱一致性上的优势。

Abstract: We propose a new method for few-shot 3D reconstruction that integrates global and local frequency regularization to stabilize geometry and preserve fine details under sparse-view conditions, addressing a key limitation of existing 3D Gaussian Splatting (3DGS) models. We also introduce a new multispectral greenhouse dataset containing four spectral bands captured from diverse plant species under controlled conditions. Alongside the dataset, we release an open-source benchmarking package that defines standardized few-shot reconstruction protocols for evaluating 3DGS-based methods. Experiments on our multispectral dataset, as well as standard benchmarks, demonstrate that the proposed method achieves sharper, more stable, and spectrally consistent reconstructions than existing baselines. The dataset and code for this work are publicly available

</details>


### [28] [Decoding Psychological States Through Movement: Inferring Human Kinesic Functions with Application to Built Environments](https://arxiv.org/abs/2601.17194)
*Cheyu Lin,Katherine A. Flanigan,Sirajum Munir*

Main category: cs.CV

TL;DR: DUET是一个隐私保护的双人肢体运动数据集和基于迁移学习的kinesics功能识别框架，能够更一致地测量建成环境中与社会资本相关的互动，但现有动作识别方法在此任务上仍存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 现有民用与建成环境研究缺乏一致且隐私保护的方法来衡量在公共空间中发生的、与社会资本有关的双人互动行为；不同研究采用不一致的“互动”定义，限制了理论验证与干预评估。

Method: 构建包含12类双人互动、覆盖五类kinesic功能的DUET数据集，采集四种传感模态和三种建成环境。基于隐私保护的骨骼关键点建立表示，评估六种开源人体行为识别模型作为基线，并提出一个迁移学习架构直接从骨骼运动推断交际功能，无需手工动作到功能映射。

Result: 展示了DUET的基准困难度：现有动作识别模型在识别以功能为导向的双人交际行为时表现受限；迁移学习框架显示功能间有结构化聚类并且表征质量与分类性能高度相关，同时能在不同受试者与环境间泛化。

Conclusion: DUET提供了一种隐私保护的骨骼运动数据集和识别框架，用于将Ekman和Friesen的肢体语言功能学（kinesics）映射到对社会资本相关行为有意义的交互功能，从而填补了内建环境研究中对“互动”一致测量的缺失。

Abstract: Social infrastructure and other built environments are increasingly expected to support well-being and community resilience by enabling social interaction. Yet in civil and built-environment research, there is no consistent and privacy-preserving way to represent and measure socially meaningful interaction in these spaces, leaving studies to operationalize "interaction" differently across contexts and limiting practitioners' ability to evaluate whether design interventions are changing the forms of interaction that social capital theory predicts should matter. To address this field-level and methodological gap, we introduce the Dyadic User Engagement DataseT (DUET) dataset and an embedded kinesics recognition framework that operationalize Ekman and Friesen's kinesics taxonomy as a function-level interaction vocabulary aligned with social capital-relevant behaviors (e.g., reciprocity and attention coordination). DUET captures 12 dyadic interactions spanning all five kinesic functions-emblems, illustrators, affect displays, adaptors, and regulators-across four sensing modalities and three built-environment contexts, enabling privacy-preserving analysis of communicative intent through movement. Benchmarking six open-source, state-of-the-art human activity recognition models quantifies the difficulty of communicative-function recognition on DUET and highlights the limitations of ubiquitous monadic, action-level recognition when extended to dyadic, socially grounded interaction measurement. Building on DUET, our recognition framework infers communicative function directly from privacy-preserving skeletal motion without handcrafted action-to-function dictionaries; using a transfer-learning architecture, it reveals structured clustering of kinesic functions and a strong association between representation quality and classification performance while generalizing across subjects and contexts.

</details>


### [29] [Structural Complexity of Brain MRI reveals age-associated patterns](https://arxiv.org/abs/2601.17211)
*Anzhe Cheng,Italo Ivo Lima Dias Pinto,Paul Bogdan*

Main category: cs.CV

TL;DR: 将结构复杂性扩展到3D MRI，采用滑动窗口粗粒化提升粗尺度稳定性，发现随年龄系统性下降并可用于预测生物年龄。


<details>
  <summary>Details</summary>
Motivation: 扩展已有的结构复杂性（多尺度信息理论）方法到三维体数据，以更稳健地刻画MRI的多尺度组织并提升粗尺度估计的稳定性与生物学可解释性。

Method: 提出并实现了一种滑动窗口粗粒化方案，用以替代传统的块状粗粒化，减少在粗尺度上因采样不足造成的不稳定；通过多尺度计算相邻分辨率间的信息损失来量化复杂性；在大规模结构MRI数据集上验证并用于预测生物学年龄。

Result: 滑动窗口方法在粗尺度上提供更平滑和稳健的复杂性估计；发现结构复杂性随年龄下降、粗尺度效应最显著；基于该指标可用于预测生物学年龄。

Conclusion: 该论文证明了结构复杂性分析可扩展到三维信号（尤其是脑MRI），并且在随年龄增长的群体中，结构复杂性在多尺度上（尤其是粗尺度）系统性下降。

Abstract: We adapt structural complexity analysis to three-dimensional signals, with an emphasis on brain magnetic resonance imaging (MRI). This framework captures the multiscale organization of volumetric data by coarse-graining the signal at progressively larger spatial scales and quantifying the information lost between successive resolutions. While the traditional block-based approach can become unstable at coarse resolutions due to limited sampling, we introduce a sliding-window coarse-graining scheme that provides smoother estimates and improved robustness at large scales. Using this refined method, we analyze large structural MRI datasets spanning mid- to late adulthood and find that structural complexity decreases systematically with age, with the strongest effects emerging at coarser scales. These findings highlight structural complexity as a reliable signal processing tool for multiscale analysis of 3D imaging data, while also demonstrating its utility in predicting biological age from brain MRI.

</details>


### [30] [Spatiotemporal Semantic V2X Framework for Cooperative Collision Prediction](https://arxiv.org/abs/2601.17216)
*Murat Arda Onsu,Poonam Lohan,Burak Kantarci,Aisha Syed,Matthew Andrews,Sean Kennedy*

Main category: cs.CV

TL;DR: Use V-JEPA to send low-dim semantic embeddings from RSUs to vehicles for real-time collision prediction; improves F1 by 10% and cuts bandwidth by 10,000x vs raw video.


<details>
  <summary>Details</summary>
Motivation: Reduce bandwidth/latency issues of transmitting raw video in ITS by sending only task-relevant semantic embeddings for timely collision prediction.

Method: RSU cameras use V-JEPA to predict embeddings of future frames; embeddings sent over V2X; vehicles use a lightweight attentive probe+classifier to decode embeddings and predict collisions. Evaluation via urban digital twin with varied traffic scenarios.

Result: 10% absolute F1-score improvement and four orders of magnitude reduction in transmission requirements compared to raw video; validates semantic V2X for cooperative collision prediction.

Conclusion: This paper proposes a semantic V2X framework using V-JEPA to generate spatiotemporal semantic embeddings at RSUs and transmit them to vehicles for collision prediction, achieving substantial communication savings and improved F1-score.

Abstract: Intelligent Transportation Systems (ITS) demand real-time collision prediction to ensure road safety and reduce accident severity. Conventional approaches rely on transmitting raw video or high-dimensional sensory data from roadside units (RSUs) to vehicles, which is impractical under vehicular communication bandwidth and latency constraints. In this work, we propose a semantic V2X framework in which RSU-mounted cameras generate spatiotemporal semantic embeddings of future frames using the Video Joint Embedding Predictive Architecture (V-JEPA). To evaluate the system, we construct a digital twin of an urban traffic environment enabling the generation of d verse traffic scenarios with both safe and collision events. These embeddings of the future frame, extracted from V-JEPA, capture task-relevant traffic dynamics and are transmitted via V2X links to vehicles, where a lightweight attentive probe and classifier decode them to predict imminent collisions. By transmitting only semantic embeddings instead of raw frames, the proposed system significantly reduces communication overhead while maintaining predictive accuracy. Experimental results demonstrate that the framework with an appropriate processing method achieves a 10% F1-score improvement for collision prediction while reducing transmission requirements by four orders of magnitude compared to raw video. This validates the potential of semantic V2X communication to enable cooperative, real-time collision prediction in ITS.

</details>


### [31] [Semi-Supervised Domain Adaptation with Latent Diffusion for Pathology Image Classification](https://arxiv.org/abs/2601.17228)
*Tengyue Zhang,Ruiwen Ding,Luoting Zhuang,Yuxiao Wu,Erika F. Rodriguez,William Hsu*

Main category: cs.CV

TL;DR: 利用以基础模型特征和队列条件的潜在扩散模型生成目标感知且形态保持的合成图像，作为半监督域适应的增强数据，在肺腺癌预后任务上显著改善目标域泛化绩效。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法利用目标域未标注数据，要么依赖图像到图像翻译导致组织结构失真，进而影响模型性能；因此需要一种能利用未标注目标域数据且保持组织形态的合成数据生成策略以改善域间迁移。

Method: 在源和目标域未标注数据上训练潜在扩散模型；通过以基础模型特征、队列身份和组织处理方法为条件，生成保留源域组织结构同时引入目标域外观的合成图像；将生成的目标感知合成图像与源域真实有标签图像联合用于训练下游分类器，并在目标域上评估。

Result: 在肺腺癌预后任务上，所提方法在目标队列独立测试集上显著提升性能：加权F1由0.611提升至0.706，macro F1由0.641提升至0.716，同时未损害源队列性能。

Conclusion: 该工作提出了一种基于潜在扩散模型的半监督域适应框架，用于生成保持组织形态且具有目标域外观的合成图像，从而提升计算病理学模型在目标队列上的泛化能力。

Abstract: Deep learning models in computational pathology often fail to generalize across cohorts and institutions due to domain shift. Existing approaches either fail to leverage unlabeled data from the target domain or rely on image-to-image translation, which can distort tissue structures and compromise model accuracy. In this work, we propose a semi-supervised domain adaptation (SSDA) framework that utilizes a latent diffusion model trained on unlabeled data from both the source and target domains to generate morphology-preserving and target-aware synthetic images. By conditioning the diffusion model on foundation model features, cohort identity, and tissue preparation method, we preserve tissue structure in the source domain while introducing target-domain appearance characteristics. The target-aware synthetic images, combined with real, labeled images from the source cohort, are subsequently used to train a downstream classifier, which is then tested on the target cohort. The effectiveness of the proposed SSDA framework is demonstrated on the task of lung adenocarcinoma prognostication. The proposed augmentation yielded substantially better performance on the held-out test set from the target cohort, without degrading source-cohort performance. The approach improved the weighted F1 score on the target-cohort held-out test set from 0.611 to 0.706 and the macro F1 score from 0.641 to 0.716. Our results demonstrate that target-aware diffusion-based synthetic data augmentation provides a promising and effective approach for improving domain generalization in computational pathology.

</details>


### [32] [C-RADIOv4 (Tech Report)](https://arxiv.org/abs/2601.17237)
*Mike Ranzinger,Greg Heinrich,Collin McCarthy,Jan Kautz,Andrew Tao,Bryan Catanzaro,Pavlo Molchanov*

Main category: cs.CV

TL;DR: C-RADIOv4通过SigLIP2、DINOv3和SAM3的多教师蒸馏，提供两个中大型参数变体，提升了下游性能、任意分辨率支持和高分辨率效率，并具有宽松许可。


<details>
  <summary>Details</summary>
Motivation: 为构建一个统一的学生模型，汇聚不同教师模型的优势，使模型在多个视觉任务上同时表现良好并提升高分辨率效率。

Method: 采用多教师（SigLIP2、DINOv3、SAM3）聚合蒸馏，训练两个参数规模的变体（412M和631M），并改进了任意分辨率支持与ViTDet选项。

Result: 在核心评测指标和新能力（来自SAM3）上均得到改进；保持相同计算复杂度的同时提升下游任务表现；提供了任意分辨率支持与高效的ViTDet选项，并采用宽松许可。

Conclusion: C-RADIOv4通过多教师蒸馏保持并增强多教师模型的能力，在相同计算复杂度下对下游任务有显著提升，并恢复了ViTDet以提升高分辨率效率，同时采用更宽松的许可。

Abstract: By leveraging multi-teacher distillation, agglomerative vision backbones provide a unified student model that retains and improves the distinct capabilities of multiple teachers. In this tech report, we describe the most recent release of the C-RADIO family of models, C-RADIOv4, which builds upon AM-RADIO/RADIOv2.5 in design, offering strong improvements on key downstream tasks at the same computational complexity. We release -SO400M (412M params), and -H (631M) model variants, both trained with an updated set of teachers: SigLIP2, DINOv3, and SAM3. In addition to improvements on core metrics and new capabilities from imitating SAM3, the C-RADIOv4 model family further improves any-resolution support, brings back the ViTDet option for drastically enhanced efficiency at high-resolution, and comes with a permissive license.

</details>


### [33] [Multi-stage Bridge Inspection System: Integrating Foundation Models with Location Anonymization](https://arxiv.org/abs/2601.17254)
*Takato Yasuno*

Main category: cs.CV

TL;DR: 提出一套兼顾损伤检测与区域隐私保护的桥梁检测流水线：用SAM3分割钢筋腐蚀、DBSCAN补全、检测并模糊告示牌、四种OCR预处理、GPU优化到1.7s/图，基于开源工具实现。


<details>
  <summary>Details</summary>
Motivation: 日本要求每五年进行一次视觉巡检，现场照片常包含能泄露地区信息的施工标识。需要在不引发公众恐慌的前提下，既保护区域隐私又准确提取损伤特征，为维护决策提供可视化指标。

Method: 1) 使用SAM3对钢筋腐蚀与混凝土裂缝进行分割；2) 用DBSCAN聚类对分割结果进行自动连通补全以修复漏检；3) 检测施工告示牌并对其区域应用高斯模糊以保护隐私；4) 提出四种图像预处理策略以提升pytesseract的OCR准确率；5) 采用GPU优化（PyTorch）与OpenCV流水线实现约1.7秒/图的推理速度。

Result: 系统基于SAM3、PyTorch、OpenCV、pytesseract与scikit-learn构建，能够有效检测钢筋暴露与腐蚀、保护地区信息并加速OCR识别，实现高效的桥梁巡检流程。

Conclusion: 该论文提出了一个开源的桥梁损伤检测系统，结合SAM3进行钢筋腐蚀检测、DBSCAN补全漏检区域，并对施工告示牌进行高斯模糊以保护区域隐私，同时通过四种预处理方法提升OCR识别率，并在GPU加速下实现每图约1.7秒的处理速度。

Abstract: In Japan, civil infrastructure condition monitoring is mandated through visual inspection every five years. Field-captured damage images frequently contain concrete cracks and rebar exposure, often accompanied by construction signs revealing regional information. To enable safe infrastructure use without causing public anxiety, it is essential to protect regional information while accurately extracting damage features and visualizing key indicators for repair decision-making. This paper presents an open-source bridge damage detection system with regional privacy protection capabilities. We employ Segment Anything Model (SAM) 3 for rebar corrosion detection and utilize DBSCAN for automatic completion of missed regions. Construction sign regions are detected and protected through Gaussian blur. Four preprocessing methods improve OCR accuracy, and GPU optimization enables 1.7-second processing per image. The technology stack includes SAM3, PyTorch, OpenCV, pytesseract, and scikit-learn, achieving efficient bridge inspection with regional information protection.

</details>


### [34] [FineVAU: A Novel Human-Aligned Benchmark for Fine-Grained Video Anomaly Understanding](https://arxiv.org/abs/2601.17258)
*João Pereira,Vasco Lopes,João Neves,David Semedo*

Main category: cs.CV

TL;DR: FineVAU通过三要素评估、FVScore指标与FineW3数据集，提供了更精细且人对齐的视频异常理解基准，揭示了LVLM在空间与时间推理方面的局限。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法（n-gram指标和LLM评价）要么无法衡量自由形式、视觉依赖的回答，要么侧重语言质量而忽视事实视觉相关性；因此需要一个更细粒度且与人类感知对齐的评估基准来衡量LVLM在视频异常理解上的能力。

Method: 把VAU任务形式化为三要素（事件、参与实体、位置），设计FVScore来自动检测LVLM回答中是否包含关键视觉元素；同时通过结构化、完全自动化流程，将现有人工标注扩展生成包含细粒度视觉信息的FineW3数据集；在基准上进行人类评估与大量定量实验。

Result: 提出的FVScore在人类评估中表现出更好的对齐性；FineW3提供大量带细粒度视觉信息的样本；实验展示LVLM在识别需空间和细粒度时间推理的异常事件上表现不足，但在静态信息和明显视觉线索事件上表现较好。

Conclusion: 该论文提出FineVAU，一个针对视频异常理解的新基准，通过精细化三要素（What/Who/Where）评估LVLM的视觉理解能力，并引入人对齐的FVScore指标与自动构建的FineW3数据集。实验证明FVScore更贴合人工评价，且揭示了LVLM在空间与细粒度时间理解上的明显局限。

Abstract: Video Anomaly Understanding (VAU) is a novel task focused on describing unusual occurrences in videos. Despite growing interest, the evaluation of VAU remains an open challenge. Existing benchmarks rely on n-gram-based metrics (e.g., BLEU, ROUGE-L) or LLM-based evaluation. The first fails to capture the rich, free-form, and visually grounded nature of LVLM responses, while the latter focuses on assessing language quality over factual relevance, often resulting in subjective judgments that are misaligned with human perception. In this work, we address this issue by proposing FineVAU, a new benchmark for VAU that shifts the focus towards rich, fine-grained and domain-specific understanding of anomalous videos. We formulate VAU as a three-fold problem, with the goal of comprehensively understanding key descriptive elements of anomalies in video: events (What), participating entities (Who) and location (Where). Our benchmark introduces a) FVScore, a novel, human-aligned evaluation metric that assesses the presence of critical visual elements in LVLM answers, providing interpretable, fine-grained feedback; and b) FineW3, a novel, comprehensive dataset curated through a structured and fully automatic procedure that augments existing human annotations with high quality, fine-grained visual information. Human evaluation reveals that our proposed metric has a superior alignment with human perception of anomalies in comparison to current approaches. Detailed experiments on FineVAU unveil critical limitations in LVLM's ability to perceive anomalous events that require spatial and fine-grained temporal understanding, despite strong performance on coarse grain, static information, and events with strong visual cues.

</details>


### [35] [Inference-Time Loss-Guided Colour Preservation in Diffusion Sampling](https://arxiv.org/abs/2601.17259)
*Angad Singh Ahuja,Aarush Ram Anandh*

Main category: cs.CV

TL;DR: 本文提出一种在推理阶段、无需额外训练的区域约束颜色保持方法，用于稳定扩散图像生成中的精确颜色控制。方法结合ROI掩膜修复、背景潜变量重置和基于复合损失（CIE Lab与线性RGB）的潜变量梯度引导，并使用CVaR风格和软最大化惩罚来控制像素误差尾部。通过延迟启动门和时间相关调度稳定去噪步骤的引导。实验证明，仅控制均值会产生局部失真，而该分布感知目标可更好地保证目标区域的颜色一致性，可无缝集成至Stable Diffusion的inpainting流程。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型中对用户指定颜色目标的精确匹配问题，尤其在设计类工作流程中，现有方法常在区域内产生局部明显的颜色偏差。希望在不重新训练模型的前提下，在推理过程中实现可控、局部的颜色保持。

Method: 在推理阶段通过三部分协同实现颜色约束：1) 基于ROI的inpainting用于空间选择性，以限定受控区域；2) 背景潜变量重置（background-latent re-imposition）以防止ROI外的颜色漂移；3) 在潜变量空间进行梯度引导（latent nudging），使用在CIE Lab与线性RGB上的复合损失。该损失不仅控制ROI的平均颜色，还通过CVaR样式和软最大化惩罚控制像素误差分布的尾部，同时采用延迟启动门和时间相关的调度以稳定去噪过程中的引导。

Result: 与仅控制均值的基线相比，作者的方法在满足平均颜色约束的同时显著减少了感知上显著的局部颜色失配和极端像素错误，提升了目标区域颜色一致性和可用性。方法可直接集成到标准Stable Diffusion inpainting流程且无需再训练。

Conclusion: 提出的推理时区域约束颜色保持方法在不改变模型权重的情况下，提供了一种切实可行的机制来实现目标区域的颜色精确性，尤其适用于设计导向的应用场景，并通过分布敏感损失解决了仅基于均值约束产生的局部失真问题。

Abstract: Precise color control remains a persistent failure mode in text-to-image diffusion systems, particularly in design-oriented workflows where outputs must satisfy explicit, user-specified color targets. We present an inference-time, region-constrained color preservation method that steers a pretrained diffusion model without any additional training. Our approach combines (i) ROI-based inpainting for spatial selectivity, (ii) background-latent re-imposition to prevent color drift outside the ROI, and (iii) latent nudging via gradient guidance using a composite loss defined in CIE Lab and linear RGB. The loss is constructed to control not only the mean ROI color but also the tail of the pixelwise error distribution through CVaR-style and soft-maximum penalties, with a late-start gate and a time-dependent schedule to stabilize guidance across denoising steps. We show that mean-only baselines can satisfy average color constraints while producing perceptually salient local failures, motivating our distribution-aware objective. The resulting method provides a practical, training-free mechanism for targeted color adherence that can be integrated into standard Stable Diffusion inpainting pipelines.

</details>


### [36] [Cross360: 360° Monocular Depth Estimation via Cross Projections Across Scales](https://arxiv.org/abs/2601.17271)
*Kun Huang,Fang-Lue Zhang,Neil Dodgson*

Main category: cs.CV

TL;DR: 提出Cross360，一种基于跨注意力（cross-attention）的360°深度估计架构，结合无畸变切线贴片（tangent patches）与等距柱状图（equirectangular）特征，通过Cross Projection Feature Alignment模块使局部贴片感知全局语境，并用Progressive Feature Aggregation with Attention逐步融合多尺度特征，在多数基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在保持球面图像全局连续性和避免投影畸变之间取得平衡。多投影方法的局部贴片特征缺乏全局感知，且在贴片边界处特征提取存在不一致，影响深度估计的全局一致性和精度。

Method: 提出Cross360架构：
- 使用无畸变的切线投影贴片与等距柱状图并行提取局部与全局特征；
- Cross Projection Feature Alignment模块：通过跨注意力将切线贴片特征与等距柱状图的360°视野对齐，使每个切线贴片具备全局上下文感知；
- Progressive Feature Aggregation with Attention模块：逐步融合多尺度特征并利用注意力机制精炼特征，用于最终深度回归。

Result: 在多数基准数据集上（特别是提供完整360°图像的场景），Cross360在深度估计精度和全局一致性方面显著优于现有方法，实验结果和消融分析证明了跨投影对齐与逐步注意力聚合的有效性。

Conclusion: Cross360通过跨注意力对齐局部无畸变贴片与全局等距特征，并采用逐步注意力聚合，多方面改善了360°深度估计的全局一致性与精度，具有实际推广价值；代码已开源。

Abstract: 360° depth estimation is a challenging research problem due to the difficulty of finding a representation that both preserves global continuity and avoids distortion in spherical images. Existing methods attempt to leverage complementary information from multiple projections, but struggle with balancing global and local consistency. Their local patch features have limited global perception, and the combined global representation does not address discrepancies in feature extraction at the boundaries between patches. To address these issues, we propose Cross360, a novel cross-attention-based architecture integrating local and global information using less-distorted tangent patches along with equirectangular features. Our Cross Projection Feature Alignment module employs cross-attention to align local tangent projection features with the equirectangular projection's 360° field of view, ensuring each tangent projection patch is aware of the global context. Additionally, our Progressive Feature Aggregation with Attention module refines multi-scaled features progressively, enhancing depth estimation accuracy. Cross360 significantly outperforms existing methods across most benchmark datasets, especially those in which the entire 360° image is available, demonstrating its effectiveness in accurate and globally consistent depth estimation. The code and model are available at https://github.com/huangkun101230/Cross360.

</details>


### [37] [Fluxamba: Topology-Aware Anisotropic State Space Models for Geological Lineament Segmentation in Multi-Source Remote Sensing](https://arxiv.org/abs/2601.17288)
*Jin Bai,Huiyao Zhang,Qi Wen,Shengyang Li,Xiaolin Tian,Atta ur Rahman*

Main category: cs.CV

TL;DR: 提出Fluxamba及其SFB（含ASG和PMF）、HSR与HFFU，以拓扑感知的各向异性信息流和多尺度对齐提升地质线状特征分割，实现高精度、低成本、实时可部署的解决方案。


<details>
  <summary>Details</summary>
Motivation: 提出用于精确分割地质线状特征（如行星线性体和地面裂缝），需要捕捉跨复杂各向异性拓扑的长程依赖，但现有State Space Models（SSMs）受限于轴对齐扫描轨迹与弯曲目标拓扑不匹配，导致上下文断裂和特征损耗。

Method: 提出Fluxamba架构，核心为Structure Flux Block(SFB)，通过Anisotropic Structural Gate(ASG)与Prior-Modulated Flow(PMF)整合来实现各向异性信息流，解耦特征方向与空间位置，按目标内在几何动态门控上下文聚合。此外引入Hierarchical Spatial Regulator(HSR)进行多尺度语义对齐，和High-Fidelity Focus Unit(HFFU)提升低对比环境下的信噪比。

Result: 在多个地质基准（LROC-Lineament、LineaMapper、GeoCrack）上进行了大量实验，Fluxamba取得新的最先进成绩。特别是在LROC-Lineament上达成F1 89.22%和mIoU 89.87%。模型参数仅3.4M，6.3G FLOPs，推理速度>24 FPS，比重量级基线在计算成本上最多降低两个数量级。

Conclusion: Fluxamba通过拓扑感知的特征整流与多尺度噪声抑制，在分割精度与部署可行性之间建立新的帕累托前沿，特别适合资源受限的实时机载/行星探测应用。

Abstract: The precise segmentation of geological linear features, spanning from planetary lineaments to terrestrial fractures, demands capturing long-range dependencies across complex anisotropic topologies. Although State Space Models (SSMs) offer near-linear computational complexity, their dependence on rigid, axis-aligned scanning trajectories induces a fundamental topological mismatch with curvilinear targets, resulting in fragmented context and feature erosion. To bridge this gap, we propose Fluxamba, a lightweight architecture that introduces a topology-aware feature rectification framework. Central to our design is the Structural Flux Block (SFB), which orchestrates an anisotropic information flux by integrating an Anisotropic Structural Gate (ASG) with a Prior-Modulated Flow (PMF). This mechanism decouples feature orientation from spatial location, dynamically gating context aggregation along the target's intrinsic geometry rather than rigid paths. Furthermore, to mitigate serialization-induced noise in low-contrast environments, we incorporate a Hierarchical Spatial Regulator (HSR) for multi-scale semantic alignment and a High-Fidelity Focus Unit (HFFU) to explicitly maximize the signal-to-noise ratio of faint features. Extensive experiments on diverse geological benchmarks (LROC-Lineament, LineaMapper, and GeoCrack) demonstrate that Fluxamba establishes a new state-of-the-art. Notably, on the challenging LROC-Lineament dataset, it achieves an F1-score of 89.22% and mIoU of 89.87%. Achieving a real-time inference speed of over 24 FPS with only 3.4M parameters and 6.3G FLOPs, Fluxamba reduces computational costs by up to two orders of magnitude compared to heavy-weight baselines, thereby establishing a new Pareto frontier between segmentation fidelity and onboard deployment feasibility.

</details>


### [38] [Dynamic Meta-Ensemble Framework for Efficient and Accurate Deep Learning in Plant Leaf Disease Detection on Resource-Constrained Edge Devices](https://arxiv.org/abs/2601.17290)
*Weloday Fikadu Moges,Jianmei Su,Amin Waqas*

Main category: cs.CV

TL;DR: 提出DMEF（动态元集成框架），在边缘设备上结合三个轻量CNN（MobileNetV2、NASNetMobile、InceptionV3）通过自适应权重在准确率与复杂度间权衡，实现高精度低延迟的植物病害诊断。


<details>
  <summary>Details</summary>
Motivation: 在IoT/移动/嵌入式等资源受限设备上部署深度学习的挑战：计算资源和能耗限制影响模型实用性，需要在保持高准确率的同时控制模型复杂度和推理延迟。

Method: 设计动态元集成框架DMEF，基于三个轻量级CNN构建元集成器。引入自适应加权机制，训练过程中迭代更新各模型权重以优化DeltaAcc（准确率提升）与模型大小的折中。权重偏向高性能且复杂度低的模型。实现目标函数可能包含精度增益与复杂度惩罚项，且保证推理延迟低于75ms和参数量小于1M。

Result: 在土豆和玉米的基准病害数据集上，DMEF分别达到99.53%和96.61%的分类精度，较单模型和静态集成分别提高约2.1%和6.3%。同时满足推理延迟<75ms和参数<1M的资源约束。

Conclusion: DMEF在边缘农业监测场景表现出高准确率与低资源占用的良好平衡，证明了将高精度AI应用于实地作物病害管理的可行性。对此方向有重要实践意义。

Abstract: Deploying deep learning models for plant disease detection on edge devices such as IoT sensors, smartphones, and embedded systems is severely constrained by limited computational resources and energy budgets. To address this challenge, we introduce a novel Dynamic Meta-Ensemble Framework (DMEF) for high-accuracy plant disease diagnosis under resource constraints. DMEF employs an adaptive weighting mechanism that dynamically combines the predictions of three lightweight convolutional neural networks (MobileNetV2, NASNetMobile, and InceptionV3) by optimizing a trade-off between accuracy improvements (DeltaAcc) and computational efficiency (model size). During training, the ensemble weights are updated iteratively, favoring models exhibiting high performance and low complexity. Extensive experiments on benchmark datasets for potato and maize diseases demonstrate state-of-the-art classification accuracies of 99.53% and 96.61%, respectively, surpassing standalone models and static ensembles by 2.1% and 6.3%. With computationally efficient inference latency (<75ms) and a compact footprint (<1 million parameters), DMEF shows strong potential for edge-based agricultural monitoring, suggesting viability for scalable crop disease management. This bridges the gap between high-accuracy AI and practical field applications.

</details>


### [39] [ClinNet: Evidential Ordinal Regression with Bilateral Asymmetry and Prototype Memory for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17315)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: 提出ClinNet：结合双侧不对称编码、诊断记忆库与基于NIG分布的证据序数头，将KOA分级作为可不确定性的序数回归任务，显著提升评分准确性与可信度。


<details>
  <summary>Details</summary>
Motivation: KOA影像分级存在等级间微小差异、专家标注不确定性及疾病的序数进展特性，传统确定性多类分类忽略了这些关键问题，导致性能与可信度不足。

Method: ClinNet包含三部分：(1) Bilateral Asymmetry Encoder显式建模膝关节内外侧结构差异；(2) Diagnostic Memory Bank维护类原型以稳定特征；(3) Evidential Ordinal Head基于Normal-Inverse-Gamma分布同时估计连续KL分数与认识不确定性，形成证据型序数回归框架。

Result: 在大量实验中，ClinNet达成QWK=0.892与Accuracy=0.768，统计学上优于最先进基线（p<0.001）；并验证不确定性能有效提示分布外样本与潜在误诊，有助安全部署。

Conclusion: ClinNet通过将KOA分级建模为证据序数回归，有效捕捉了病变的连续性与标注不确定性，并通过BAE、诊断记忆库与基于NIG的证据序数头实现性能与可信度提升。实验证明ClinNet在QWK与准确率上超越现有最先进方法，并且不确定性估计能够识别分布外样本与潜在误诊。

Abstract: Knee osteoarthritis (KOA) grading based on radiographic images is a critical yet challenging task due to subtle inter-grade differences, annotation uncertainty, and the inherently ordinal nature of disease progression. Conventional deep learning approaches typically formulate this problem as deterministic multi-class classification, ignoring both the continuous progression of degeneration and the uncertainty in expert annotations. In this work, we propose ClinNet, a novel trustworthy framework that addresses KOA grading as an evidential ordinal regression problem. The proposed method integrates three key components: (1) a Bilateral Asymmetry Encoder (BAE) that explicitly models medial-lateral structural discrepancies; (2) a Diagnostic Memory Bank that maintains class-wise prototypes to stabilize feature representations; and (3) an Evidential Ordinal Head based on the Normal-Inverse-Gamma (NIG) distribution to jointly estimate continuous KL grades and epistemic uncertainty. Extensive experiments demonstrate that ClinNet achieves a Quadratic Weighted Kappa of 0.892 and Accuracy of 0.768, statistically outperforming state-of-the-art baselines (p < 0.001). Crucially, we demonstrate that the model's uncertainty estimates successfully flag out-of-distribution samples and potential misdiagnoses, paving the way for safe clinical deployment.

</details>


### [40] [SkyReels-V3 Technique Report](https://arxiv.org/abs/2601.17323)
*Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-V3：统一的diffusion-Transformer视频生成框架，支持参考图像到视频、视频延展与音频驱动生成；通过数据增强和混合训练提高参考保真与时序一致性，宣称接近闭源领先系统表现。


<details>
  <summary>Details</summary>
Motivation: 作者旨在构建一个能在多模态上下文中进行条件视频生成的统一模型，覆盖参考图像生成、视频延展与音频指导生成三种实用场景，提升参考依从性、时空一致性与音画同步，接近或超越闭源系统。

Method: 采用diffusion Transformer为主干，设计了交叉帧配对、图像编辑与语义重写的数据处理流水线；训练策略包括图像视频混合训练与多分辨率联合优化；视频延展结合大规模视频理解以支持单镜头续写与多镜头切换；说话人头像通过首末帧插入和关键帧重构实现音画同步。

Result: SkyReels-V3提出了一种统一的多模态上下文学习框架，在单一架构中支持图像到视频、视频延展和音频驱动视频生成三大范式。通过交叉帧配对、图像编辑与语义重写的数据处理策略，结合图像视频混合训练与多分辨率联合优化，提升了对参考图像的依从性、时间一致性与叙事连贯性。同时引入大规模视频理解以增强时空一致性，及通过首末帧插入和关键帧重构优化说话人头像的音画同步。作者宣称在多项关键指标上达到或接近最先进水平。

Conclusion: SkyReels-V3在方法上将多模态条件生成任务统一到一个diffusion Transformer框架，并通过数据处理与训练策略改进参考遵循与时序稳定性；若实验证实，其可成为强大的开放式视频生成基线，但需更多细节（网络结构、训练数据规模与评测细节）来全面评估其泛化与安全风险。

Abstract: Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized.
  Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: https://github.com/SkyworkAI/SkyReels-V3.

</details>


### [41] [SymbolSight: Minimizing Inter-Symbol Interference for Reading with Prosthetic Vision](https://arxiv.org/abs/2601.17326)
*Jasmine Lesner,Michael Beyeler*

Main category: cs.CV

TL;DR: They create SymbolSight to optimize letter-symbol mappings for prosthetic vision, using SPV and bigram-informed optimization, and show simulated reductions in temporal letter confusion (median 22x) across languages, suggesting redesigned glyphs could improve reading with retinal prostheses.


<details>
  <summary>Details</summary>
Motivation: Retinal prostheses restore limited visual perception but suffer from low spatial resolution and temporal persistence, causing difficulty in reading due to afterimage interference between sequentially presented letters. The paper aims to see if redesigning symbols can reduce temporal confusion without hardware improvements.

Method: They introduce SymbolSight, a computational framework that (1) uses simulated prosthetic vision (SPV) and a neural proxy observer to estimate pairwise confusability between symbol patterns, (2) optimizes symbol-to-letter assignments using language-specific bigram statistics to minimize confusion among frequently adjacent letters, producing heterogeneous symbol sets.

Result: Across simulations for Arabic, Bulgarian, and English, optimized symbol sets reduced predicted confusion by a median factor of 22 compared to native alphabets, indicating substantial improvements in mitigating temporal interference in sequential letter presentation.

Conclusion: Standard typography is poorly matched to serial, low-bandwidth prosthetic vision. Computational modeling like SymbolSight can efficiently search visual encoding designs to find candidates that may improve reading performance for prosthetic users, warranting psychophysical and clinical follow-up.

Abstract: Retinal prostheses restore limited visual perception, but low spatial resolution and temporal persistence make reading difficult. In sequential letter presentation, the afterimage of one symbol can interfere with perception of the next, leading to systematic recognition errors. Rather than relying on future hardware improvements, we investigate whether optimizing the visual symbols themselves can mitigate this temporal interference. We present SymbolSight, a computational framework that selects symbol-to-letter mappings to minimize confusion among frequently adjacent letters. Using simulated prosthetic vision (SPV) and a neural proxy observer, we estimate pairwise symbol confusability and optimize assignments using language-specific bigram statistics. Across simulations in Arabic, Bulgarian, and English, the resulting heterogeneous symbol sets reduced predicted confusion by a median factor of 22 relative to native alphabets. These results suggest that standard typography is poorly matched to serial, low-bandwidth prosthetic vision and demonstrate how computational modeling can efficiently narrow the design space of visual encodings to generate high-potential candidates for future psychophysical and clinical evaluation.

</details>


### [42] [Learning with Geometric Priors in U-Net Variants for Polyp Segmentation](https://arxiv.org/abs/2601.17331)
*Fabian Vazquez,Jose A. Nuñez,Diego Adame,Alissen Moreno,Augustin Zhan,Huimin Li,Jinghao Yang,Haoteng Tang,Bin Fu,Pengfei Gu*

Main category: cs.CV

TL;DR: 提出一种几何先验引导模块（GPM），将深度信息注入U-Net类网络以提升息肉分割，在五个数据集上显著改进。


<details>
  <summary>Details</summary>
Motivation: 当前U-Net变体难以在低对比度或复杂背景内捕捉几何与结构线索，影响息肉分割精度与鲁棒性，因此引入显式几何先验以增强特征表达。

Method: 先在模拟ColonDepth数据集上微调Visual Geometry Grounded Transformer (VGGT)以预测内镜图像深度图。设计GPM模块：将深度图编码为几何先验并注入编码器特征，通过空间与通道注意力机制融合并精炼局部与全局信息。GPM为即插即用，可集成到多种U-Net变体。

Result: 在五个公共息肉分割数据集上，对三个强基线模型均带来持续性能提升，证明几何先验对分割任务的有效性。代码与生成的深度图已开源。

Conclusion: 利用微调后的深度估计为U-Net类分割网络提供显式几何约束，可增强模型在低对比与复杂内镜场景下的分割能力，且GPM具有良好可迁移性与实用性。

Abstract: Accurate and robust polyp segmentation is essential for early colorectal cancer detection and for computer-aided diagnosis. While convolutional neural network-, Transformer-, and Mamba-based U-Net variants have achieved strong performance, they still struggle to capture geometric and structural cues, especially in low-contrast or cluttered colonoscopy scenes. To address this challenge, we propose a novel Geometric Prior-guided Module (GPM) that injects explicit geometric priors into U-Net-based architectures for polyp segmentation. Specifically, we fine-tune the Visual Geometry Grounded Transformer (VGGT) on a simulated ColonDepth dataset to estimate depth maps of polyp images tailored to the endoscopic domain. These depth maps are then processed by GPM to encode geometric priors into the encoder's feature maps, where they are further refined using spatial and channel attention mechanisms that emphasize both local spatial and global channel information. GPM is plug-and-play and can be seamlessly integrated into diverse U-Net variants. Extensive experiments on five public polyp segmentation datasets demonstrate consistent gains over three strong baselines. Code and the generated depth maps are available at: https://github.com/fvazqu/GPM-PolypSeg

</details>


### [43] [AGE-Net: Spectral--Spatial Fusion and Anatomical Graph Reasoning with Evidential Ordinal Regression for Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.17336)
*Xiaoyang Li,Runni Zhou*

Main category: cs.CV

TL;DR: AGE-Net: ConvNeXt + SSF + AGR + DFR + NIG evidential head + ordinal constraint achieves state-of-the-art KL grading with strong uncertainty modeling.


<details>
  <summary>Details</summary>
Motivation: KL grading is hard due to subtle changes, long-range anatomy dependencies, and boundary ambiguity; need modeling of spatial-spectral features, anatomical context, and predictive uncertainty with ordinal labels.

Method: Use ConvNeXt backbone; SSF fuses spectral and spatial features; AGR builds graph over anatomical landmarks for relational reasoning; DFR refines predictions differentially; NIG evidential regression head captures uncertainty; pairwise ordinal ranking enforces label order. Experimental evaluation on knee KL dataset with three seeds reports QWK and MSE; ablations and analyses of uncertainty, robustness, explainability included.

Result: AGE-Net integrates spectral-spatial fusion, anatomical graph reasoning, and differential refinement with a ConvNeXt backbone, using NIG evidential regression and ordinal ranking for KL grading.

Conclusion: AGE-Net substantially improves KL grading performance (QWK 0.9017, MSE 0.2349) over baselines, offering better uncertainty estimation and interpretability; ablations confirm each component's contribution.

Abstract: Automated Kellgren--Lawrence (KL) grading from knee radiographs is challenging due to subtle structural changes, long-range anatomical dependencies, and ambiguity near grade boundaries. We propose AGE-Net, a ConvNeXt-based framework that integrates Spectral--Spatial Fusion (SSF), Anatomical Graph Reasoning (AGR), and Differential Refinement (DFR). To capture predictive uncertainty and preserve label ordinality, AGE-Net employs a Normal-Inverse-Gamma (NIG) evidential regression head and a pairwise ordinal ranking constraint. On a knee KL dataset, AGE-Net achieves a quadratic weighted kappa (QWK) of 0.9017 +/- 0.0045 and a mean squared error (MSE) of 0.2349 +/- 0.0028 over three random seeds, outperforming strong CNN baselines and showing consistent gains in ablation studies. We further outline evaluations of uncertainty quality, robustness, and explainability, with additional experimental figures to be included in the full manuscript.

</details>


### [44] [TEXTS-Diff: TEXTS-Aware Diffusion Model for Real-World Text Image Super-Resolution](https://arxiv.org/abs/2601.17340)
*Haodong He,Xin Zhan,Yancheng Bai,Rui Lan,Lei Sun,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 作者构建了包含丰富中英文本的Real-Texts数据集，并提出TEXTS-Diff扩散模型，通过结合抽象语义与具体文本区域信息，有效提升真实场景文本超分的背景与文字恢复，达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中文本样本稀缺且多为孤立文本，导致超分模型在文字区域表现差、背景重建质量受限，需要更丰富的真实文本数据及专门针对文本区域的生成方法。

Method: 构建大规模真实场景中中英双语文本数据集Real-Texts；提出TEXTS-Aware Diffusion Model（TEXTS-Diff），结合抽象概念信息理解文本元素与具体文本区域增强文字细节，采用扩散生成框架同时优化背景与文本恢复。

Result: 在多个评价指标上（包括视觉质量和文本可读性评估）优于现有方法，展示出更好的泛化能力与复杂场景下的文本恢复准确性。

Conclusion: 本文提出了Real-Texts数据集和TEXTS-Diff模型，显著提升了真实场景文本图像超分辨率的背景与文本恢复质量，减少文字区域失真与幻觉，且在多项指标上达到SOTA。

Abstract: Real-world text image super-resolution aims to restore overall visual quality and text legibility in images suffering from diverse degradations and text distortions. However, the scarcity of text image data in existing datasets results in poor performance on text regions. In addition, datasets consisting of isolated text samples limit the quality of background reconstruction. To address these limitations, we construct Real-Texts, a large-scale, high-quality dataset collected from real-world images, which covers diverse scenarios and contains natural text instances in both Chinese and English. Additionally, we propose the TEXTS-Aware Diffusion Model (TEXTS-Diff) to achieve high-quality generation in both background and textual regions. This approach leverages abstract concepts to improve the understanding of textual elements within visual scenes and concrete text regions to enhance textual details. It mitigates distortions and hallucination artifacts commonly observed in text regions, while preserving high-quality visual scene fidelity. Extensive experiments demonstrate that our method achieves state-of-the-art performance across multiple evaluation metrics, exhibiting superior generalization ability and text restoration accuracy in complex scenarios. All the code, model, and dataset will be released.

</details>


### [45] [STARS: Shared-specific Translation and Alignment for missing-modality Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2601.17342)
*Tong Wang,Xiaodong Zhang,Guanzhou Chen,Jiaqi Wang,Chenxi Liu,Xiaoliang Tan,Wenchao Guo,Xuyang Li,Xuanrui Wang,Zifan Wang*

Main category: cs.CV

TL;DR: 提出STARS：通过不对称双向翻译+stop-gradient与基于类别平衡的像素级语义对齐，解决遥感多模态缺失问题，提升分割性能并缓解特征坍缩与类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在遥感多模态分析中，不同模态（光学、SAR、DSM）融合能显著提升地表语义理解。但实际应用中常遇到模态缺失问题，导致传统多模态融合模型性能下降。现有缺模态方法存在特征坍缩和恢复特征过于泛化的问题，难以兼顾稳健性与类别区分性。

Method: 提出STARS框架，包含两大设计：1）不对称对齐机制：采用双向翻译并结合stop-gradient，避免特征坍缩并降低对超参数敏感性；2）像素级语义采样对齐（PSA）：通过类别平衡的像素采样与跨模态语义对齐损失，缓解类别不平衡导致的对齐失败，提升少数类识别。

Result: 在不完整模态输入下，STARS在语义分割任务中显著优于现有方法，表现为更高的整体和少数类指标，同时对超参数更鲁棒，特征表达更区分化。

Conclusion: STARS通过不对称对齐和像素级语义采样对齐有效缓解了模态缺失带来的性能下降，增强了多模态遥感语义分割在实际场景下的稳健性和少数类识别能力。

Abstract: Multimodal remote sensing technology significantly enhances the understanding of surface semantics by integrating heterogeneous data such as optical images, Synthetic Aperture Radar (SAR), and Digital Surface Models (DSM). However, in practical applications, the missing of modality data (e.g., optical or DSM) is a common and severe challenge, which leads to performance decline in traditional multimodal fusion models. Existing methods for addressing missing modalities still face limitations, including feature collapse and overly generalized recovered features. To address these issues, we propose \textbf{STARS} (\textbf{S}hared-specific \textbf{T}ranslation and \textbf{A}lignment for missing-modality \textbf{R}emote \textbf{S}ensing), a robust semantic segmentation framework for incomplete multimodal inputs. STARS is built on two key designs. First, we introduce an asymmetric alignment mechanism with bidirectional translation and stop-gradient, which effectively prevents feature collapse and reduces sensitivity to hyperparameters. Second, we propose a Pixel-level Semantic sampling Alignment (PSA) strategy that combines class-balanced pixel sampling with cross-modality semantic alignment loss, to mitigate alignment failures caused by severe class imbalance and improve minority-class recognition.

</details>


### [46] [Revisiting Lightweight Low-Light Image Enhancement: From a YUV Color Space Perspective](https://arxiv.org/abs/2601.17349)
*Hailong Yan,Shice Liu,Xiangtao Zhang,Lujian Yao,Fengxiang Yang,Jinwei Chen,Bo Li*

Main category: cs.CV

TL;DR: 提出基于YUV频域分析的轻量低照度增强范式：Y通道用双流全局-局部注意力恢复低频，UV通道用Y引导的局部频率注意力抑高频噪声，最后引导交互融合，达到更好质量与更小模型。


<details>
  <summary>Details</summary>
Motivation: 现有轻量化低照度增强方法通过分离策略（如Retinex或YUV变换）简化模型，但忽略了各通道的退化差异及通道间交互，限制了性能。作者通过频域分析发现通道特性差异，提出了面向通道特性设计的轻量化架构以提升表现。

Method: 方法包括三部分：1) 对色彩空间进行频域分析并验证YUV优越性；2) 对Y通道使用双流全局-局部注意力模块(Dual-Stream Global-Local Attention)以恢复低频信息；对UV通道使用受Y引导的局部感知频率注意力模块(Y-guided Local-Aware Frequency Attention)以抑制高频噪声；3) 使用Guided Interaction模块进行最终特征融合。

Result: 在多个基准数据集上，模型在主观视觉效果和若干客观指标上超越现有方法，同时参数量显著更少，达成新的state-of-the-art。

Conclusion: 该论文提出了一种基于YUV色域的轻量低照度图像增强方法，通过频域分析识别Y通道主要丢失低频信息、UV通道受高频噪声污染的特点，设计了针对性的通道恢复模块，最终在多个基准上以更少的参数实现了更优的视觉质量。

Abstract: In the current era of mobile internet, Lightweight Low-Light Image Enhancement (L3IE) is critical for mobile devices, which faces a persistent trade-off between visual quality and model compactness. While recent methods employ disentangling strategies to simplify lightweight architectural design, such as Retinex theory and YUV color space transformations, their performance is fundamentally limited by overlooking channel-specific degradation patterns and cross-channel interactions. To address this gap, we perform a frequency-domain analysis that confirms the superiority of the YUV color space for L3IE. We identify a key insight: the Y channel primarily loses low-frequency content, while the UV channels are corrupted by high-frequency noise. Leveraging this finding, we propose a novel YUV-based paradigm that strategically restores channels using a Dual-Stream Global-Local Attention module for the Y channel, a Y-guided Local-Aware Frequency Attention module for the UV channels, and a Guided Interaction module for final feature fusion. Extensive experiments validate that our model establishes a new state-of-the-art on multiple benchmarks, delivering superior visual quality with a significantly lower parameter count.

</details>


### [47] [NeRF-MIR: Towards High-Quality Restoration of Masked Images with Neural Radiance Fields](https://arxiv.org/abs/2601.17350)
*Xianliang Huang,Zhizhou Zhong,Shuhang Chen,Yi Xu,Juhong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: NeRF-MIR通过基于patch的熵引导光线采样、逐步迭代自训练恢复和动态加权损失，有效提升了基于NeRF的遮挡图像恢复性能，并构建了相应数据集支持实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF在输入图像受损（遮挡、遮罩）时性能下降，随机发射光线难以学习细节纹理，且缺乏专门用于NeRF的遮挡恢复方法与数据集。

Method: 提出PERE（基于patch的熵测量用于光线发射分配）、PIRE（逐步迭代自训练恢复机制）以及动态加权损失函数；并构建三套遮挡数据集进行评估。

Result: 在构建的三种遮挡数据集及真实捕获数据上，NeRF-MIR在恢复质量（视觉主观效果和定量指标）上均优于现有方法，证明了PERE和PIRE的有效性。

Conclusion: NeRF-MIR有效提升了在缺损/遮挡图像条件下基于NeRF的3D场景重建与遮挡区域恢复能力，并在构建的数据集与真实数据上优于对比方法。

Abstract: Neural Radiance Fields (NeRF) have demonstrated remarkable performance in novel view synthesis. However, there is much improvement room on restoring 3D scenes based on NeRF from corrupted images, which are common in natural scene captures and can significantly impact the effectiveness of NeRF. This paper introduces NeRF-MIR, a novel neural rendering approach specifically proposed for the restoration of masked images, demonstrating the potential of NeRF in this domain. Recognizing that randomly emitting rays to pixels in NeRF may not effectively learn intricate image textures, we propose a \textbf{P}atch-based \textbf{E}ntropy for \textbf{R}ay \textbf{E}mitting (\textbf{PERE}) strategy to distribute emitted rays properly. This enables NeRF-MIR to fuse comprehensive information from images of different views. Additionally, we introduce a \textbf{P}rogressively \textbf{I}terative \textbf{RE}storation (\textbf{PIRE}) mechanism to restore the masked regions in a self-training process. Furthermore, we design a dynamically-weighted loss function that automatically recalibrates the loss weights for masked regions. As existing datasets do not support NeRF-based masked image restoration, we construct three masked datasets to simulate corrupted scenarios. Extensive experiments on real data and constructed datasets demonstrate the superiority of NeRF-MIR over its counterparts in masked image restoration.

</details>


### [48] [HyDeMiC: A Deep Learning-based Mineral Classifier using Hyperspectral Data](https://arxiv.org/abs/2601.17352)
*M. L. Mamud,Piyoosh Jaysaval,Frederick D Day-Lewis,M. K. Mudunuru*

Main category: cs.CV

TL;DR: 提出HyDeMiC卷积神经网络用于含噪HSI矿物分类，使用USGS光谱生成训练数据并在1-10%噪声合成数据上测试，MCC在低噪声接近1，在中等噪声也表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在高维HSI数据中对环境噪声和传感器限制的脆弱性，提升矿物分类在真实噪声条件下的鲁棒性

Method: Call to analyze abstract

Result: Generated CNN model HyDeMiC, used USGS library spectra convolved with sensor response, tested on synthetic noisy datasets 1%,2%,5%,10%, evaluated with MCC

Conclusion: HyDeMiC achieves near-perfect accuracy on clean/low-noise and strong performance under moderate noise, showing robustness for real-world HSI mineral classification

Abstract: Hyperspectral imaging (HSI) has emerged as a powerful remote sensing tool for mineral exploration, capitalizing on unique spectral signatures of minerals. However, traditional classification methods such as discriminant analysis, logistic regression, and support vector machines often struggle with environmental noise in data, sensor limitations, and the computational complexity of analyzing high-dimensional HSI data. This study presents HyDeMiC (Hyperspectral Deep Learning-based Mineral Classifier), a convolutional neural network (CNN) model designed for robust mineral classification under noisy data. To train HyDeMiC, laboratory-measured hyperspectral data for 115 minerals spanning various mineral groups were used from the United States Geological Survey (USGS) library. The training dataset was generated by convolving reference mineral spectra with an HSI sensor response function. These datasets contained three copper-bearing minerals, Cuprite, Malachite, and Chalcopyrite, used as case studies for performance demonstration. The trained CNN model was evaluated on several synthetic 2D hyperspectral datasets with noise levels of 1%, 2%, 5%, and 10%. Our noisy data analysis aims to replicate realistic field conditions. The HyDeMiC's performance was assessed using the Matthews Correlation Coefficient (MCC), providing a comprehensive measure across different noise regimes. Results demonstrate that HyDeMiC achieved near-perfect classification accuracy (MCC = 1.00) on clean and low-noise datasets and maintained strong performance under moderate noise conditions. These findings emphasize HyDeMiC's robustness in the presence of moderate noise, highlighting its potential for real-world applications in hyperspectral imaging, where noise is often a significant challenge.

</details>


### [49] [PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling](https://arxiv.org/abs/2601.17354)
*Wenzhi Guo,Guangchi Fang,Shu Yang,Bing Wang*

Main category: cs.CV

TL;DR: PocketGS enables efficient on-device 3D Gaussian Splatting via geometry priors (G), local surface statistics injection (I), and memory-efficient backprop unrolling (T), yielding high-quality reconstructions within mobile constraints.


<details>
  <summary>Details</summary>
Motivation: Enable high-fidelity 3DGS training on mobile devices constrained by minute-scale training budgets and limited peak memory.

Method: Analyze 3DGS (PocketGS) operators and results

Result: PocketGS introduces three operators (G, I, T) enabling on-device training of 3D Gaussian Splatting under strict time/memory limits, achieving better or comparable fidelity to workstation baselines.

Conclusion: PocketGS resolves training/memory/fidelity trade-offs and enables practical mobile capture-to-render pipeline.

Abstract: Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.

</details>


### [50] [UCAD: Uncertainty-guided Contour-aware Displacement for semi-supervised medical image segmentation](https://arxiv.org/abs/2601.17366)
*Chengbo Ding,Fenghe Tang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: UCAD：利用超像素和不确定性引导的轮廓感知位移与动态加权一致性损失，改善半监督医学图像分割边界与一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的位移策略仅在矩形区域操作，忽略了解剖结构，导致边界失真和语义不一致，需一种能保持解剖边界的位移方法并引入不确定性指导以增强一致性学习。

Method: 1) 使用超像素生成与解剖边界对齐的区域；2) 基于模型预测不确定性选择性地对困难/高不确定性区域进行位移增强一致性学习；3) 设计动态不确定性加权一致性损失以自适应稳定训练并给予未标注区域适当正则化；4) 在多数据集上与SOTA对比并提供代码。

Result: UCAD提出了一种基于不确定性引导的轮廓感知位移框架，利用超像素生成与解剖结构对齐的区域，并选择性地对高不确定性区域进行位移，以增强一致性学习。提出动态不确定性加权一致性损失来稳定训练并对未标注区域进行有效正则化。实验表明在少量标注情况下优于现有方法，并公开了代码。

Conclusion: 通过结合超像素的解剖对齐分割区域与基于不确定性的选择性位移策略，UCAD在保护边界语义的同时提升了半监督分割的一致性学习，显著提高了分割精度。

Abstract: Existing displacement strategies in semi-supervised segmentation only operate on rectangular regions, ignoring anatomical structures and resulting in boundary distortions and semantic inconsistency. To address these issues, we propose UCAD, an Uncertainty-Guided Contour-Aware Displacement framework for semi-supervised medical image segmentation that preserves contour-aware semantics while enhancing consistency learning. Our UCAD leverages superpixels to generate anatomically coherent regions aligned with anatomy boundaries, and an uncertainty-guided selection mechanism to selectively displace challenging regions for better consistency learning. We further propose a dynamic uncertainty-weighted consistency loss, which adaptively stabilizes training and effectively regularizes the model on unlabeled regions. Extensive experiments demonstrate that UCAD consistently outperforms state-of-the-art semi-supervised segmentation methods, achieving superior segmentation accuracy under limited annotation. The code is available at:https://github.com/dcb937/UCAD.

</details>


### [51] [Physical Prompt Injection Attacks on Large Vision-Language Models](https://arxiv.org/abs/2601.17383)
*Chen Ling,Kai Hu,Hangcheng Liu,Xingshuo Han,Tianwei Zhang,Changhai Ou*

Main category: cs.CV

TL;DR: Introduces PPIA: place malicious text-like prompts on physical objects to hijack LVLMs; black-box, high success, robust in real settings


<details>
  <summary>Details</summary>
Motivation: Existing prompt injection defenses assume access to channels or query knowledge; need attacks that work in real-world deployments without such access

Method: Physical Prompt Injection Attack (PPIA): offline selection of recognizable typographic prompts; environment-aware placement guided by spatiotemporal attention

Result: Evaluated on 10 LVLMs in simulation and real-world across VQA, planning, navigation; success up to 98%; robust to distance, viewpoint, illumination

Conclusion: PPIA is a practical, black-box, query-agnostic physical attack that can reliably influence LVLM behavior without model access

Abstract: Large Vision-Language Models (LVLMs) are increasingly deployed in real-world intelligent systems for perception and reasoning in open physical environments. While LVLMs are known to be vulnerable to prompt injection attacks, existing methods either require access to input channels or depend on knowledge of user queries, assumptions that rarely hold in practical deployments. We propose the first Physical Prompt Injection Attack (PPIA), a black-box, query-agnostic attack that embeds malicious typographic instructions into physical objects perceivable by the LVLM. PPIA requires no access to the model, its inputs, or internal pipeline, and operates solely through visual observation. It combines offline selection of highly recognizable and semantically effective visual prompts with strategic environment-aware placement guided by spatiotemporal attention, ensuring that the injected prompts are both perceivable and influential on model behavior. We evaluate PPIA across 10 state-of-the-art LVLMs in both simulated and real-world settings on tasks including visual question answering, planning, and navigation, PPIA achieves attack success rates up to 98%, with strong robustness under varying physical conditions such as distance, viewpoint, and illumination. Our code is publicly available at https://github.com/2023cghacker/Physical-Prompt-Injection-Attack.

</details>


### [52] [ONRW: Optimizing inversion noise for high-quality and robust watermark](https://arxiv.org/abs/2601.17388)
*Xuan Ding,Xiu Yan,Chuanlong Xie,Yao Zhu*

Main category: cs.CV

TL;DR: 提出基于扩散模型的水印策略：通过null-text逆向优化得到inversion noise并在潜在空间优化，结合自注意力约束与伪掩码，最后用扩散模型迭代去噪生成高质量且鲁棒的带水印图像，在COCO上比stable signature平均提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水印方法虽然能在不显著损害图像质量下隐藏水印，但在图像传输过程中遇到各种腐败（如噪声、模糊、压缩等）时鲁棒性不足，限制了其实用性。论文旨在提升水印在图像腐败下的稳定性与视觉质量。

Method: 方法包括：1) 使用null-text优化将干净图像转化为inversion noise；2) 在潜在空间对inversion noise进行优化（加入自注意力约束和伪掩码策略以保护语义）；3) 使用扩散模型的迭代去噪过程生成高质量带水印图像，该过程同时起到净化和鲁棒性增强作用；4) 在多种图像变换下进行对比实验并与stable signature等方法比较。

Result: 实验结果显示，该方法在COCO数据集上对12种图像变换的平均性能比stable signature高约10%，在多种图像腐败下均表现出更好的鲁棒性与视觉保持。代码已公开。

Conclusion: 该论文提出了一种基于扩散模型的高质量、鲁棒图像水印框架，通过对图像进行null-text逆向优化得到inversion noise，并在潜在空间优化后通过扩散模型的迭代去噪生成带水印图像。迭代去噪作为强大的净化机制，提高了水印在图像传输过程中的鲁棒性，同时保持图像视觉质量。作者还引入了自注意力约束与伪掩码策略以避免逆向噪声优化破坏图像语义。大量实验表明在多种图像腐败下性能优越，在COCO数据集的12种变换上平均超越stable signature方法约10%。

Abstract: Watermarking methods have always been effective means of protecting intellectual property, yet they face significant challenges. Although existing deep learning-based watermarking systems can hide watermarks in images with minimal impact on image quality, they often lack robustness when encountering image corruptions during transmission, which undermines their practical application value. To this end, we propose a high-quality and robust watermark framework based on the diffusion model. Our method first converts the clean image into inversion noise through a null-text optimization process, and after optimizing the inversion noise in the latent space, it produces a high-quality watermarked image through an iterative denoising process of the diffusion model. The iterative denoising process serves as a powerful purification mechanism, ensuring both the visual quality of the watermarked image and enhancing the robustness of the watermark against various corruptions. To prevent the optimizing of inversion noise from distorting the original semantics of the image, we specifically introduced self-attention constraints and pseudo-mask strategies. Extensive experimental results demonstrate the superior performance of our method against various image corruptions. In particular, our method outperforms the stable signature method by an average of 10\% across 12 different image transformations on COCO datasets. Our codes are available at https://github.com/920927/ONRW.

</details>


### [53] [SMV-EAR: Bring Spatiotemporal Multi-View Representation Learning into Efficient Event-Based Action Recognition](https://arxiv.org/abs/2601.17391)
*Rui Fan,Weidong Hao*

Main category: cs.CV

TL;DR: 提出平移不变稠密事件表示、双分支动态融合与时间扭曲增强，显著提升EAR性能并降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的时空多视图表示学习（SMVRL）通过沿空间轴投影事件，但存在空间分箱平移敏感和早期拼接融合简单的问题，限制了EAR性能。作者希望通过改进表示和融合策略，并加入增强，解决这些缺陷。

Method: 方法包括：1) 将稀疏的H-W-T事件转为具有平移不变性的稠密表示；2) 设计双分支动态融合架构，对来自不同视图的运动特征进行样本级互补建模；3) 引入仿生时间扭曲增强，用于模拟动作速度变化。

Result: 在HARDVS、DailyDVS-200和THU-EACT-50-CHL三个数据集上，分别比现有SMVRL方法提高了Top-1准确率+7.0%、+10.7%和+10.2%，同时参数减少约30.1%，计算量降低约35.7%。

Conclusion: 该论文提出了针对事件相机动作识别（EAR）的改进框架，通过平移不变的稠密事件转换、双分支动态融合网络和生物启发的时间扭曲增强，显著提升了识别性能并降低了模型复杂度。

Abstract: Event cameras action recognition (EAR) offers compelling privacy-protecting and efficiency advantages, where temporal motion dynamics is of great importance. Existing spatiotemporal multi-view representation learning (SMVRL) methods for event-based object recognition (EOR) offer promising solutions by projecting H-W-T events along spatial axis H and W, yet are limited by its translation-variant spatial binning representation and naive early concatenation fusion architecture. This paper reexamines the key SMVRL design stages for EAR and propose: (i) a principled spatiotemporal multi-view representation through translation-invariant dense conversion of sparse events, (ii) a dual-branch, dynamic fusion architecture that models sample-wise complementarity between motion features from different views, and (iii) a bio-inspired temporal warping augmentation that mimics speed variability of real-world human actions. On three challenging EAR datasets of HARDVS, DailyDVS-200 and THU-EACT-50-CHL, we show +7.0%, +10.7%, and +10.2% Top-1 accuracy gains over existing SMVRL EOR method with surprising 30.1% reduced parameters and 35.7% lower computations, establishing our framework as a novel and powerful EAR paradigm.

</details>


### [54] [ReLE: A Scalable System and Structured Benchmark for Diagnosing Capability Anisotropy in Chinese LLMs](https://arxiv.org/abs/2601.17399)
*Rui Fang,Jian Li,Wei Chen,Bin Hu,Ying-Cong Chen,Xin Tang,Liang Diao*

Main category: cs.CV

TL;DR: ReLE通过符号-语义混合评分和方差感知调度，实现了对模型能力非均匀性的高效、稳健实时诊断，显著降低计算成本并揭示现代模型更偏向专业化而非通用性。


<details>
  <summary>Details</summary>
Motivation: 现有静态排行榜存在饱和和掩盖能力间权衡的问题，且全面评估成本高昂；需要一个高频、结构化的诊断工具来测量模型在不同领域与能力维度上的非均匀表现。

Method: 引入符号-基于语义的混合评分机制以消除嵌入相似性在推理任务中的误判，并设计基于Neyman分配并带噪声修正的动态方差感知调度器以大幅降低计算成本。

Result: 使用ReLE对304个模型在一个207,843样本的领域×能力矩阵上评估，动态调度将计算成本降低70%，排名相关性保持ρ=0.96；发现聚合排名对权重敏感，ReLE的排名稳定幅度(RSA)为11.4，明显高于传统基准的≈5.0。

Conclusion: ReLE是一个用于诊断模型“能力各向异性”的可扩展在线评估系统，展示了对大规模模型组的高效、稳健评估能力。

Abstract: Large Language Models (LLMs) have achieved rapid progress in Chinese language understanding, yet accurately evaluating their capabilities remains challenged by benchmark saturation and prohibitive computational costs. While static leaderboards provide snapshot rankings, they often mask the structural trade-offs between capabilities. In this work, we present ReLE (Robust Efficient Live Evaluation), a scalable system designed to diagnose Capability Anisotropy, the non-uniformity of model performance across domains. Using ReLE, we evaluate 304 models (189 commercial, 115 open-source) across a Domain $\times$ Capability orthogonal matrix comprising 207,843 samples. We introduce two methodological contributions to address current evaluation pitfalls: (1) A Symbolic-Grounded Hybrid Scoring Mechanism that eliminates embedding-based false positives in reasoning tasks; (2) A Dynamic Variance-Aware Scheduler based on Neyman allocation with noise correction, which reduces compute costs by 70\% compared to full-pass evaluations while maintaining a ranking correlation of $ρ=0.96$. Our analysis reveals that aggregate rankings are highly sensitive to weighting schemes: models exhibit a Rank Stability Amplitude (RSA) of 11.4 in ReLE versus $\sim$5.0 in traditional benchmarks, confirming that modern models are highly specialized rather than generally superior. We position ReLE not as a replacement for comprehensive static benchmarks, but as a high-frequency diagnostic monitor for the evolving model landscape.

</details>


### [55] [HAAF: Hierarchical Adaptation and Alignment of Foundation Models for Few-Shot Pathology Anomaly Detection](https://arxiv.org/abs/2601.17405)
*Chunze Yang,Wenjie Zhao,Yue Tang,Junbo Lu,Jiusong Ge,Qidong Liu,Zeyu Gao,Chen Li*

Main category: cs.CV

TL;DR: 为解决V-L模型在病理细粒度ROI检测中的粒度不匹配，HAAF通过视觉先注入文本再回导视觉的交叉层对齐与双分支推理，有效提升少样本检测表现。


<details>
  <summary>Details</summary>
Motivation: 病理诊断依赖ROI内的纹理级细粒度线索，但现有V-L模型和适配方法在跨模态对齐上存在粒度错配，无法准确定位并表征微小异常。

Method: 提出分层适配与对齐框架（HAAF），核心为交叉层尺度对齐（CLSA）：先将视觉特征注入文本提示生成内容自适应描述符，再用这些描述符对视觉编码器进行空间引导；并设计双分支推理将语义分数与几何原型融合以增强少样本稳定性。

Result: 在四个基准数据集上，HAAF显著优于现有最先进方法，并在低资源场景下与领域特定骨干（如CONCH）协同获得更强性能。

Conclusion: HAAF有效缓解了视觉与文本在病理细粒度异常检测中的粒度不匹配，显著提升少样本条件下的检测性能，并能与领域特定骨干网络协同放大效果。

Abstract: Precision pathology relies on detecting fine-grained morphological abnormalities within specific Regions of Interest (ROIs), as these local, texture-rich cues - rather than global slide contexts - drive expert diagnostic reasoning. While Vision-Language (V-L) models promise data efficiency by leveraging semantic priors, adapting them faces a critical Granularity Mismatch, where generic representations fail to resolve such subtle defects. Current adaptation methods often treat modalities as independent streams, failing to ground semantic prompts in ROI-specific visual contexts. To bridge this gap, we propose the Hierarchical Adaptation and Alignment Framework (HAAF). At its core is a novel Cross-Level Scaled Alignment (CLSA) mechanism that enforces a sequential calibration order: visual features first inject context into text prompts to generate content-adaptive descriptors, which then spatially guide the visual encoder to spotlight anomalies. Additionally, a dual-branch inference strategy integrates semantic scores with geometric prototypes to ensure stability in few-shot settings. Experiments on four benchmarks show HAAF significantly outperforms state-of-the-art methods and effectively scales with domain-specific backbones (e.g., CONCH) in low-resource scenarios.

</details>


### [56] [Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity](https://arxiv.org/abs/2601.17408)
*Harsharaj Pathak,Vineeth N Balasubramanian*

Main category: cs.CV

TL;DR: 通过邻域签名与单一相似/不相似损失，本文在无源域数据下改进了领域自适应，VisDA上表现最佳，其他数据集也很有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统SFDA方法依赖邻域一致性，但会因为误导性邻居信息而产生错误。通过学习更可靠的邻域表征（签名），可以缓解噪声邻居对自适应的负面影响。

Method: 基于邻域一致性的思想，定义每个样本的邻域签名以降低噪声邻居的影响；构造一个单一的损失函数，该损失同时促进预测相似样本的靠拢和不同样本的分离，从而学习更有判别性的簇结构以适应目标域。

Result: 在挑战性较高的VisDA数据集上，该方法领先于现有方法，在其他基准数据集上也取得了有竞争力的结果，表明单一损失配合邻域签名能有效提升SFDA效果。

Conclusion: 本文提出通过邻域签名(neighborhood signature)学习更具信息性的簇，并用单一损失项在目标域优化样本预测的相似性与差异性，从而在无源域数据的前提下实现领域自适应。该方法在VisDA数据集上优于现有方法，并在其他基准上表现具有竞争力。

Abstract: Source-Free Domain Adaptation (SFDA) is an emerging area of research that aims to adapt a model trained on a labeled source domain to an unlabeled target domain without accessing the source data. Most of the successful methods in this area rely on the concept of neighborhood consistency but are prone to errors due to misleading neighborhood information. In this paper, we explore this approach from the point of view of learning more informative clusters and mitigating the effect of noisy neighbors using a concept called neighborhood signature, and demonstrate that adaptation can be achieved using just a single loss term tailored to optimize the similarity and dissimilarity of predictions of samples in the target domain. In particular, our proposed method outperforms existing methods in the challenging VisDA dataset while also yielding competitive results on other benchmark datasets.

</details>


### [57] [Cloud-Enabled IoT System for Real-Time Environmental Monitoring and Remote Device Control Using Firebase](https://arxiv.org/abs/2601.17414)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 本文实现了一个基于ESP32与Firebase的低成本云端环境监测与远程控制系统，具备实时同步、低延迟控制和数据持久化，适合智能家居与小规模工业监测应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统监测系统在实时数据访问、远程可控性和云集成方面的不足，降低复杂服务器部署门槛，使开发者能以低成本实现云端同步监测与控制。

Method: 使用ESP32通过Wi-Fi将DHT22和HC-SR04采集的数据实时推送到Google Firebase Realtime Database，同时从数据库读取LED控制命令；通过实验测量数据传输成功率、控制延迟和数据持久化能力。

Result: 实验表明数据传输成功率99.2%，控制延迟<1.5秒，且实现了历史数据持久化，整体实现成本32.5美元。系统在实时性、可扩展性和易用性上表现良好。

Conclusion: 该文提出了一个基于Firebase的云端物联网系统，利用ESP32读取DHT22和HC-SR04传感器数据，并可远程控制两颗LED。系统低成本且易实现，适合入门级和小规模部署。

Abstract: The proliferation of Internet of Things (IoT) devices has created unprecedented opportunities for remote monitoring and control applications across various domains. Traditional monitoring systems often suffer from limitations in real-time data accessibility, remote controllability, and cloud integration. This paper presents a cloud-enabled IoT system that leverages Google's Firebase Realtime Database for synchronized environmental monitoring and device control. The system utilizes an ESP32 microcontroller to interface with a DHT22 temperature/humidity sensor and an HC-SR04 ultrasonic distance sensor, while enabling remote control of two LED indicators through a cloud-based interface. Real-time sensor data is transmitted to Firebase, providing a synchronized platform accessible from multiple devices simultaneously. Experimental results demonstrate reliable data transmission with 99.2\% success rate, real-time control latency under 1.5 seconds, and persistent data storage for historical analysis. The system architecture offers a scalable framework for various IoT applications, from smart home automation to industrial monitoring, with a total implementation cost of \$32.50. The integration of Firebase provides robust cloud capabilities without requiring complex server infrastructure, making advanced IoT applications accessible to developers and researchers with limited resources.

</details>


### [58] [CoT-Seg: Rethinking Segmentation with Chain-of-Thought Reasoning and Self-Correction](https://arxiv.org/abs/2601.17420)
*Shiu-hong Kao,Chak Ho Huang,Huaiqian Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: CoT-Seg是一个训练免费框架，利用GPT-4o的内在推理能力通过链式思维分解任务并自我修正分割结果，支持检索增强，并在新的ReasonSeg-Hard数据集上展现出对复杂查询和域外图像的改进性能。


<details>
  <summary>Details</summary>
Motivation: 现有分割方法在处理复杂查询和域外图像时表现不佳。受人类“逐步思考”启发，希望构建一个能逐步推理、查阅信息、自我评估并迭代优化结果的系统，以提高鲁棒性和在模糊/复杂情形下的可靠性。

Method: 方法包括：1) 使用预训练MLLM解析复杂提示，将查询分解为多步元指令；2) 从图像中提取细粒度语义并识别目标对象；3) 生成初始分割掩码；4) 自我评估模块基于原始查询与推理轨迹识别不一致之处并迭代修正掩码；5) 可选的检索增强模块用于在输入信息不足时访问外部知识。该框架无需微调，依赖模型内在推理能力。

Result: 提出了新的数据集ReasonSeg-Hard用于评估困难案例。实验证明，将连锁思维与自我修正结合在无训练框架下能显著提升分割的可靠性与稳健性，尤其在模糊、隐含提示或易出错情形下表现更好。

Conclusion: 该论文提出了一个无需训练的推理分割框架CoT-Seg，通过将连锁思维(chain-of-thought)与自我修正(self-correction)结合，利用预训练多模态大模型（GPT-4o）分解查询并逐步生成与修正分割掩码。

Abstract: Existing works of reasoning segmentation often fall short in complex cases, particularly when addressing complicated queries and out-of-domain images. Inspired by the chain-of-thought reasoning, where harder problems require longer thinking steps/time, this paper aims to explore a system that can think step-by-step, look up information if needed, generate results, self-evaluate its own results, and refine the results, in the same way humans approach harder questions. We introduce CoT-Seg, a training-free framework that rethinks reasoning segmentation by combining chain-of-thought reasoning with self-correction. Instead of fine-tuning, CoT-Seg leverages the inherent reasoning ability of pre-trained MLLMs (GPT-4o) to decompose queries into meta-instructions, extract fine-grained semantics from images, and identify target objects even under implicit or complex prompts. Moreover, CoT-Seg incorporates a self-correction stage: the model evaluates its own segmentation against the original query and reasoning trace, identifies mismatches, and iteratively refines the mask. This tight integration of reasoning and correction significantly improves reliability and robustness, especially in ambiguous or error-prone cases. Furthermore, our CoT-Seg framework allows easy incorporation of retrieval-augmented reasoning, enabling the system to access external knowledge when the input lacks sufficient information. To showcase CoT-Seg's ability to handle very challenging cases ,we introduce a new dataset ReasonSeg-Hard. Our results highlight that combining chain-of-thought reasoning, self-correction, offers a powerful paradigm for vision-language integration driven segmentation.

</details>


### [59] [Coronary Artery Segmentation and Vessel-Type Classification in X-Ray Angiography](https://arxiv.org/abs/2601.17429)
*Mehdi Yousefzadeh,Siavash Shirzadeh Barough,Ashkan Fakharifar,Yashar Tayyarazad,Narges Eghbali,Mohaddeseh Mozaffari,Hoda Taeb,Negar Sadat Rafiee Tabatabaee,Parsa Esfahanian,Ghazaleh Sadeghi Gohar,Amineh Safavirad,Saeideh Mazloomzadeh,Ehsan khalilipur,Armin Elahifar,Majid Maleki*

Main category: cs.CV

TL;DR: 每图像学习调参可提升经典滤波器，FPN与合并标签监督在高分辨率下能显著提升分割精度和外部迁移性，轻微微调可显著恢复性能。


<details>
  <summary>Details</summary>
Motivation: 临床上XCA是冠状动脉疾病评估金标准，但因低对比度、运动、遮挡、导管干扰等因素使常规数据的血管分割困难，限制了定量分析与基于血管类型的下游测量，亟需鲁棒且可迁移的分割与标注方法。

Method: 从670个cine序列中选取峰值显影帧，进行超分辨率与增强，比较经典Meijering、Frangi、Sato滤波器（使用每图像oracle调参、全局平均设置、及基于SVR的每图像参数预测）和深度学习基线（U-Net、FPN、Swin Transformer），并训练两种监督策略（仅冠状动脉与冠状动脉+导管合并）；第二阶段进行血管类型（LAD、LCX、RCA）标注；在DCA1数据集上进行外部验证，评估轻度域内微调效果。

Result: SVR每图像调参使经典滤波器Dice提升（例如Frangi由0.741增至0.759）；FPN在深度模型中表现最佳（仅冠状动脉监督Dice=0.914±0.007，合并标签提高到0.931±0.006）；在严格外部测试DCA1上，Dice下降到0.798（仅冠状动脉）和0.814（合并），轻度域内微调可恢复到约0.882；血管类型标注准确率分别为RCA 98.5%、LAD 95.4%、LCX 96.2%。

Conclusion: 本文提出并评估了多种血管分割方法在X射线冠状动脉造影（XCA）影像上的性能，指出通过每图像参数预测可以提升经典滤波器的分割表现；高分辨率FPN和合并冠状动脉+导管的标签监督在域外迁移和稳定性上表现更好。

Abstract: X-ray coronary angiography (XCA) is the clinical reference standard for assessing coronary artery disease, yet quantitative analysis is limited by the difficulty of robust vessel segmentation in routine data. Low contrast, motion, foreshortening, overlap, and catheter confounding degrade segmentation and contribute to domain shift across centers. Reliable segmentation, together with vessel-type labeling, enables vessel-specific coronary analytics and downstream measurements that depend on anatomical localization. From 670 cine sequences (407 subjects), we select a best frame near peak opacification using a low-intensity histogram criterion and apply joint super-resolution and enhancement. We benchmark classical Meijering, Frangi, and Sato vesselness filters under per-image oracle tuning, a single global mean setting, and per-image parameter prediction via Support Vector Regression (SVR). Neural baselines include U-Net, FPN, and a Swin Transformer, trained with coronary-only and merged coronary+catheter supervision. A second stage assigns vessel identity (LAD, LCX, RCA). External evaluation uses the public DCA1 cohort. SVR per-image tuning improves Dice over global means for all classical filters (e.g., Frangi: 0.759 vs. 0.741). Among deep models, FPN attains 0.914+/-0.007 Dice (coronary-only), and merged coronary+catheter labels further improve to 0.931+/-0.006. On DCA1 as a strict external test, Dice drops to 0.798 (coronary-only) and 0.814 (merged), while light in-domain fine-tuning recovers to 0.881+/-0.014 and 0.882+/-0.015. Vessel-type labeling achieves 98.5% accuracy (Dice 0.844) for RCA, 95.4% (0.786) for LAD, and 96.2% (0.794) for LCX. Learned per-image tuning strengthens classical pipelines, while high-resolution FPN models and merged-label supervision improve stability and external transfer with modest adaptation.

</details>


### [60] [ReflexSplit: Single Image Reflection Separation via Layer Fusion-Separation](https://arxiv.org/abs/2601.17468)
*Chia-Ming Lee,Yu-Fan Lin,Jing-Hui Jung,Yu-Jou Hsiao,Chih-Chung Hsu,Yu-Lun Liu*

Main category: cs.CV

TL;DR: ReflexSplit通过跨尺度门控融合、交替的融合-分离模块和课程化训练，有效解决单图像反射分离中深层混淆问题，带来更好的感知质量与泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非线性混合下，尤其是深层解码器中由于隐式融合机制和多尺度协调不足，导致传输层与反射层混淆，影响分离效果和视觉质量。

Method: 设计了双流编码器-解码器结构：1) 跨尺度门控融合（CrGF）在不同层次自适应聚合语义先验、纹理细节和解码器上下文；2) 层级融合-分离模块（LFSB）交替执行共享结构的融合和层特异性的差分分离，并借鉴差分Transformer通过跨流减法实现注意力抵消；3) 课程化训练采用深度相关初始化和分阶段热启动加强差分分离能力。

Result: 在合成和真实场景基准上进行大量实验，ReflexSplit在主观感知质量和客观指标上均优于现有方法，表现出更强的鲁棒性和泛化能力。

Conclusion: 本文提出了ReflexSplit双流框架，针对非线性混合下的单图像反射分离问题，通过跨尺度门控融合、层级融合-分离模块和课程化训练三项创新，显著减少了传输层与反射层在深层解码器中的混淆，提升了视觉质量与泛化能力。

Abstract: Single Image Reflection Separation (SIRS) disentangles mixed images into transmission and reflection layers. Existing methods suffer from transmission-reflection confusion under nonlinear mixing, particularly in deep decoder layers, due to implicit fusion mechanisms and inadequate multi-scale coordination. We propose ReflexSplit, a dual-stream framework with three key innovations. (1) Cross-scale Gated Fusion (CrGF) adaptively aggregates semantic priors, texture details, and decoder context across hierarchical depths, stabilizing gradient flow and maintaining feature consistency. (2) Layer Fusion-Separation Blocks (LFSB) alternate between fusion for shared structure extraction and differential separation for layer-specific disentanglement. Inspired by Differential Transformer, we extend attention cancellation to dual-stream separation via cross-stream subtraction. (3) Curriculum training progressively strengthens differential separation through depth-dependent initialization and epoch-wise warmup. Extensive experiments on synthetic and real-world benchmarks demonstrate state-of-the-art performance with superior perceptual quality and robust generalization. Our code is available at https://github.com/wuw2135/ReflexSplit.

</details>


### [61] [PhaSR: Generalized Image Shadow Removal with Physically Aligned Priors](https://arxiv.org/abs/2601.17470)
*Chia-Ming Lee,Yu-Fan Lin,Yu-Jou Hsiao,Jing-Hui Jung,Yu-Lun Liu,Chih-Chung Hsu*

Main category: cs.CV

TL;DR: 提出PhaSR：通过物理归一化和几何-语义注意对齐实现鲁棒的去影，适用于单光源与多光源场景，性能优且复杂度低。


<details>
  <summary>Details</summary>
Motivation: 在复杂照明（尤其多光源/环境光）下，传统基于物理先验的方法常因先验不匹配或模态冲突而失效，需鲁棒地分离照明与反射率。

Method: 方法包括：1) PAN：使用Gray-world归一化、对数域Retinex分解和动态范围重组合的闭式照明校正以抑制色偏；2) GSRA：将深度几何与DINO-v2语义嵌入通过扩展的差分注意力进行跨模态对齐，解决模态冲突并保持语义一致性。

Result: 实验显示PhaSR在去影任务中取得有竞争力的性能，复杂度较低，并能推广到多光源环境，补足传统方法在环境照明下的不足。

Conclusion: PhaSR提出通过双层先验对齐（物理对齐归一化PAN和几何-语义整流注意GSRA）来改善不同照明条件下的去影效果，在单光源和多光源环境均表现稳健。

Abstract: Shadow removal under diverse lighting conditions requires disentangling illumination from intrinsic reflectance, a challenge compounded when physical priors are not properly aligned. We propose PhaSR (Physically Aligned Shadow Removal), addressing this through dual-level prior alignment to enable robust performance from single-light shadows to multi-source ambient lighting. First, Physically Aligned Normalization (PAN) performs closed-form illumination correction via Gray-world normalization, log-domain Retinex decomposition, and dynamic range recombination, suppressing chromatic bias. Second, Geometric-Semantic Rectification Attention (GSRA) extends differential attention to cross-modal alignment, harmonizing depth-derived geometry with DINO-v2 semantic embeddings to resolve modal conflicts under varying illumination. Experiments show competitive performance in shadow removal with lower complexity and generalization to ambient lighting where traditional methods fail under multi-source illumination. Our source code is available at https://github.com/ming053l/PhaSR.

</details>


### [62] [BMDS-Net: A Bayesian Multi-Modal Deep Supervision Network for Robust Brain Tumor Segmentation](https://arxiv.org/abs/2601.17504)
*Yan Zhou,Zhen Huang,Yingqiu Li,Yue Ouyang,Suncheng Xiang,Zehua Wang*

Main category: cs.CV

TL;DR: 针对临床中常见的模态缺失与置信度需求，BMDS-Net通过MMCF、DDS和贝叶斯微调提升鲁棒性与可解释性，实验显示在BraTS2021上更稳定、边界更精确并提供不确定性图。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在理想化数据上Dice高但在缺失模态和置信度校准方面存在安全风险，需提升临床可用性。

Method: 提出零初始化多模态上下文融合(MMCF)与残差门控深度解码监督(DDS)构建确定性骨干，并用内存高效贝叶斯微调将网络转为概率预测，产生体素级不确定性图。

Result: 在BraTS2021上，BMDS-Net在保持竞争性精度的同时，在缺模场景下表现出更高稳定性并显著降低Hausdorff距离，且能输出不确定性提示。

Conclusion: BMDS-Net强调临床鲁棒性与可置信度，兼顾准确性与稳定性。

Abstract: Accurate brain tumor segmentation from multi-modal magnetic resonance imaging (MRI) is a prerequisite for precise radiotherapy planning and surgical navigation. While recent Transformer-based models such as Swin UNETR have achieved impressive benchmark performance, their clinical utility is often compromised by two critical issues: sensitivity to missing modalities (common in clinical practice) and a lack of confidence calibration. Merely chasing higher Dice scores on idealized data fails to meet the safety requirements of real-world medical deployment. In this work, we propose BMDS-Net, a unified framework that prioritizes clinical robustness and trustworthiness over simple metric maximization. Our contribution is three-fold. First, we construct a robust deterministic backbone by integrating a Zero-Init Multimodal Contextual Fusion (MMCF) module and a Residual-Gated Deep Decoder Supervision (DDS) mechanism, enabling stable feature learning and precise boundary delineation with significantly reduced Hausdorff Distance, even under modality corruption. Second, and most importantly, we introduce a memory-efficient Bayesian fine-tuning strategy that transforms the network into a probabilistic predictor, providing voxel-wise uncertainty maps to highlight potential errors for clinicians. Third, comprehensive experiments on the BraTS 2021 dataset demonstrate that BMDS-Net not only maintains competitive accuracy but, more importantly, exhibits superior stability in missing-modality scenarios where baseline models fail. The source code is publicly available at https://github.com/RyanZhou168/BMDS-Net.

</details>


### [63] [FMIR, a foundation model-based Image Registration Framework for Robust Image Registration](https://arxiv.org/abs/2601.17529)
*Fengting Zhang,Yue He,Qinghao Liu,Yaonan Wang,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 提出FMIR：用基础模型编码器+通用配准头+通道正则化，在单数据集训练下既达域内SOTA又提升跨域稳健性，展示了低资源下构建泛化医学影像基础模型的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习配准模型在速度上虽有优势，但受限于小规模医学数据，泛化能力差，难以直接应用于临床。希望通过引入基础模型提取稳健解剖特征来提升跨域泛化。

Method: 使用一个基于基础模型的特征编码器提取解剖结构特征，结合通用配准头（registration head）进行配准；在训练过程中加入通道正则化策略以增强特征的鲁棒性；仅使用单一数据集进行训练并评估域内与域外表现。

Result: 在仅用一套训练数据的情况下，FMIR在域内达到SOTA性能，同时在多个域外测试集上表现出稳健的配准性能，表明所提方法能提高泛化能力。

Conclusion: FMIR通过将基础模型特征编码器与通用配准头结合，并在单一数据集上使用通道正则化训练，能够在保持域内SOTA性能的同时显著提升跨域配准的稳健性，证明了用有限资源构建可泛化医学影像基础模型的可行性。

Abstract: Deep learning has revolutionized medical image registration by achieving unprecedented speeds, yet its clinical application is hindered by a limited ability to generalize beyond the training domain, a critical weakness given the typically small scale of medical datasets. In this paper, we introduce FMIR, a foundation model-based registration framework that overcomes this limitation.Combining a foundation model-based feature encoder for extracting anatomical structures with a general registration head, and trained with a channel regularization strategy on just a single dataset, FMIR achieves state-of-the-art(SOTA) in-domain performance while maintaining robust registration on out-of-domain images.Our approach demonstrates a viable path toward building generalizable medical imaging foundation models with limited resources. The code is available at https://github.com/Monday0328/FMIR.git.

</details>


### [64] [Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries](https://arxiv.org/abs/2601.17535)
*Kevin Robbins,Xiaotong Liu,Yu Wu,Le Sun,Grady McPeak,Abby Stylianou,Robert Pless*

Main category: cs.CV

TL;DR: 通过为任务生成合成图像并将其与文本评估结合，作者能更准确地在无标签情形下预测VLM（如CLIP）的零样本性能，并向用户展示用于判断的图像示例。


<details>
  <summary>Details</summary>
Motivation: VLM在跨域适应性差异大，非专业用户难以判断某个VLM在自己任务上的效果；需要一种无需标签就能预测VLM任务表现的方法，并为用户提供直观示例。

Method: 在现有文本比对（text-only）评分基础上，引入生成模型（如图像生成器）创建与任务相关的合成图像，使用这些图像与VLM进行推断以估计零样本准确率；并将图像结果与文本评分结合以提高预测质量。

Result: 在标准CLIP基准数据集上实验表明，加入合成图像的评估方法相比仅用文本的基线显著提高了对零样本准确率的预测质量，同时还能展示用于评估的示例图像。

Conclusion: 作者提出通过生成合成图像来改进仅基于文本的VLM零样本性能预测方法，从而更准确评估模型在特定任务域的表现并为用户提供直观反馈。

Abstract: Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.

</details>


### [65] [OTI: A Model-free and Visually Interpretable Measure of Image Attackability](https://arxiv.org/abs/2601.17536)
*Jiaming Liang,Haowei Liu,Chi-Man Pun*

Main category: cs.CV

TL;DR: 提出模型无关且可视化的图像可攻击性度量OTI，通过语义对象的纹理强度评估攻击易感性，理论联系决策边界与中高频扰动特征，实验验证有效且高效。


<details>
  <summary>Details</summary>
Motivation: 现有可攻击性度量依赖模型（梯度或最小扰动）且缺乏视觉可解释性，很多实际场景无法获得任务模型或需要直观解释，因此需要一种无模型且可视化的量化方法。

Method: 定义并计算语义对象的纹理强度作为OTI，理论上从决策边界与对抗扰动的中高频特性说明纹理强度与易攻击性相关；在实践中通过语义分割/对象提取与频域或滤波器操作得到纹理强度指标，进而评估可攻击性，并在多组实验中对比验证其有效性与效率。

Result: 提出了一种新的图像可攻击性度量OTI（Object Texture Intensity），无须依赖模型且具有可视化解释性，通过测量图像语义对象的纹理强度评估其被对抗扰动攻击的易感性。

Conclusion: OTI能在不依赖具体任务模型的情况下有效预测图像的可攻击性，并且计算高效、具有视觉可解释性，能为主动学习、对抗训练和攻击增强等应用提供帮助。

Abstract: Despite the tremendous success of neural networks, benign images can be corrupted by adversarial perturbations to deceive these models. Intriguingly, images differ in their attackability. Specifically, given an attack configuration, some images are easily corrupted, whereas others are more resistant. Evaluating image attackability has important applications in active learning, adversarial training, and attack enhancement. This prompts a growing interest in developing attackability measures. However, existing methods are scarce and suffer from two major limitations: (1) They rely on a model proxy to provide prior knowledge (e.g., gradients or minimal perturbation) to extract model-dependent image features. Unfortunately, in practice, many task-specific models are not readily accessible. (2) Extracted features characterizing image attackability lack visual interpretability, obscuring their direct relationship with the images. To address these, we propose a novel Object Texture Intensity (OTI), a model-free and visually interpretable measure of image attackability, which measures image attackability as the texture intensity of the image's semantic object. Theoretically, we describe the principles of OTI from the perspectives of decision boundaries as well as the mid- and high-frequency characteristics of adversarial perturbations. Comprehensive experiments demonstrate that OTI is effective and computationally efficient. In addition, our OTI provides the adversarial machine learning community with a visual understanding of attackability.

</details>


### [66] [Saliency Driven Imagery Preprocessing for Efficient Compression -- Industrial Paper](https://arxiv.org/abs/2601.17555)
*Justin Downes,Sam Saltwick,Anthony Chen*

Main category: cs.CV

TL;DR: 用显著性图分级，并对不同等级应用不同平滑强度，再用常规有损编码，实现大幅卫星图像内的可变速率压缩，节省存储带宽同时保留关键区域细节。


<details>
  <summary>Details</summary>
Motivation: 卫星每天采集大量高分辨率图像，但许多下游任务只关心图像中的小区域。通过显著性信息引导编码，可在保存关键区域细节的同时降低整体存储和传输成本。

Method: 先生成显著性图并量化为若干等级，然后对每个等级应用对应大小的平滑卷积核对像素进行预处理，最后将处理后的图像输入传统有损压缩编码器，以实现单图内的可变压缩率。

Result: 提出了一种基于显著性图驱动的卫星图像预处理方法，通过对不同显著性等级应用可变大小平滑核，结合传统有损压缩编码标准，实现单幅大图内的可变速率压缩。

Conclusion: 显著性引导的预处理能在保持重要区域信息的同时降低不重要区域的数据率，从而提升下游编码与存储效率，适用于关注小区域的遥感任务。

Abstract: The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.

</details>


### [67] [Sponge Tool Attack: Stealthy Denial-of-Efficiency against Tool-Augmented Agentic Reasoning](https://arxiv.org/abs/2601.17566)
*Qi Li,Xinchao Wang*

Main category: cs.CV

TL;DR: STA是一种仅通过提示重写、在查询级访问下破坏agentic推理效率且隐蔽的攻击方法，实验展示其在多模型多工具多任务上的有效性。


<details>
  <summary>Details</summary>
Motivation: 在将大型语言模型与外部工具结合形成agentic推理时，工具调用流程可能成为新的攻击面；现有研究较少关注对工具调用过程的恶意操纵风险。

Method: 作者设计了一个迭代的多智能体协作框架，包含显式的重写策略控制，用以生成与原始提示语义高度一致但更“吸水”的重写提示，攻击流程不修改底层模型或工具，仅改写提示。

Result: 在6种模型（开源与闭源API）、12种工具、4个agentic框架和13个数据集（跨5个领域）上广泛实验表明，STA能有效延长推理路径并显著增加计算负担，同时保持任务语义与用户意图的一致性，攻击具有普适性与隐蔽性。

Conclusion: 本文揭示并验证了针对基于工具的agentic推理的脆弱性，提出的Sponge Tool Attack（STA）能在仅有查询级访问的约束下，通过重写输入提示使推理轨迹变得冗长且复杂，从而增加计算开销且难以察觉。

Abstract: Enabling large language models (LLMs) to solve complex reasoning tasks is a key step toward artificial general intelligence. Recent work augments LLMs with external tools to enable agentic reasoning, achieving high utility and efficiency in a plug-and-play manner. However, the inherent vulnerabilities of such methods to malicious manipulation of the tool-calling process remain largely unexplored. In this work, we identify a tool-specific attack surface and propose Sponge Tool Attack (STA), which disrupts agentic reasoning solely by rewriting the input prompt under a strict query-only access assumption. Without any modification on the underlying model or the external tools, STA converts originally concise and efficient reasoning trajectories into unnecessarily verbose and convoluted ones before arriving at the final answer. This results in substantial computational overhead while remaining stealthy by preserving the original task semantics and user intent. To achieve this, we design STA as an iterative, multi-agent collaborative framework with explicit rewritten policy control, and generates benign-looking prompt rewrites from the original one with high semantic fidelity. Extensive experiments across 6 models (including both open-source models and closed-source APIs), 12 tools, 4 agentic frameworks, and 13 datasets spanning 5 domains validate the effectiveness of STA.

</details>


### [68] [Stylizing ViT: Anatomy-Preserving Instance Style Transfer for Domain Generalization](https://arxiv.org/abs/2601.17586)
*Sebastian Doerrich,Francesco Di Salvo,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 提出Stylizing ViT，用共享注意力块实现自注意力保持结构与跨注意力做风格迁移，增强数据/测试时增强，提升多任务准确率（训练+13%，测试+17%），生成图像无明显伪影。


<details>
  <summary>Details</summary>
Motivation: Medical image models fail to generalize across domains/demographics due to heterogeneity and scarce data. Existing augmentations or style-based methods either lack diversity or introduce artifacts; need an augmentation that preserves anatomy while providing diverse, artifact-free styles.

Method: They introduce Stylizing ViT, a Vision Transformer encoder using weight-shared attention blocks for self- and cross-attention to enable style transfer while preserving anatomy.

Result: On three classification tasks (histopathology and dermatology), augmentation with Stylizing ViT improved robustness up to +13% accuracy over state-of-the-art; test-time augmentation yielded up to +17% improvement. Generated images were perceptually convincing without artifacts.

Conclusion: Stylizing ViT provides diverse, artifact-free style augmentation improving domain generalization and test-time robustness for medical image classification, outperforming prior stylistic augmentation methods.

Abstract: Deep learning models in medical image analysis often struggle with generalizability across domains and demographic groups due to data heterogeneity and scarcity. Traditional augmentation improves robustness, but fails under substantial domain shifts. Recent advances in stylistic augmentation enhance domain generalization by varying image styles but fall short in terms of style diversity or by introducing artifacts into the generated images. To address these limitations, we propose Stylizing ViT, a novel Vision Transformer encoder that utilizes weight-shared attention blocks for both self- and cross-attention. This design allows the same attention block to maintain anatomical consistency through self-attention while performing style transfer via cross-attention. We assess the effectiveness of our method for domain generalization by employing it for data augmentation on three distinct image classification tasks in the context of histopathology and dermatology. Results demonstrate an improved robustness (up to +13% accuracy) over the state of the art while generating perceptually convincing images without artifacts. Additionally, we show that Stylizing ViT is effective beyond training, achieving a 17% performance improvement during inference when used for test-time augmentation. The source code is available at https://github.com/sdoerrich97/stylizing-vit .

</details>


### [69] [SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation](https://arxiv.org/abs/2601.17657)
*Taewan Cho,Taeryang Kim,Andrew Jaeyong Choi*

Main category: cs.CV

TL;DR: SPACE-CLIP提出了一个绕开文本提示、从冻结的CLIP视觉编码器直接提取几何信息的双路径解码器（语义+结构），通过层次融合在KITTI上显著提升深度估计性能，并可作为空间感知模块集成于未来的VLA系统。


<details>
  <summary>Details</summary>
Motivation: CLIP在语义理解上表现出色但难以感知几何结构；现有通过文本提示查询CLIP的方法间接且低效，故设计直接从视觉编码器中提取几何信息的新架构。

Method: 提出双路径解码器：语义路径使用FiLM基于全局上下文动态调制高层特征，结构路径从早期层提取细粒度空间细节；两路径层次融合以合成语义与精确几何信息。完全绕过CLIP文本编码器，不依赖文本提示。

Result: 在KITTI基准上，SPACE-CLIP显著优于此前基于CLIP的方法；消融实验表明双路径的协同融合对性能至关重要。

Conclusion: SPACE-CLIP能从冻结的CLIP视觉编码器中直接解锁并解释潜在几何知识，显著提升基于CLIP的深度估计性能，尤其在KITTI基准上表现优越；双通道路径（语义与结构）及其层次融合为关键因素。

Abstract: Contrastive Language-Image Pre-training (CLIP) has accomplished extraordinary success for semantic understanding but inherently struggles to perceive geometric structure. Existing methods attempt to bridge this gap by querying CLIP with textual prompts, a process that is often indirect and inefficient. This paper introduces a fundamentally different approach using a dual-pathway decoder. We present SPACE-CLIP, an architecture that unlocks and interprets latent geometric knowledge directly from a frozen CLIP vision encoder, completely bypassing the text encoder and its associated textual prompts. A semantic pathway interprets high-level features, dynamically conditioned on global context using feature-wise linear modulation (FiLM). In addition, a structural pathway extracts fine-grained spatial details from early layers. These complementary streams are hierarchically fused, enabling a robust synthesis of semantic context and precise geometry. Extensive experiments on the KITTI benchmark show that SPACE-CLIP dramatically outperforms previous CLIP-based methods. Our ablation studies validate that the synergistic fusion of our dual pathways is critical to this success. SPACE-CLIP offers a new, efficient, and architecturally elegant blueprint for repurposing large-scale vision models. The proposed method is not just a standalone depth estimator, but a readily integrable spatial perception module for the next generation of embodied AI systems, such as vision-language-action (VLA) models. Our model is available at https://github.com/taewan2002/space-clip

</details>


### [70] [Training-Free Text-to-Image Compositional Food Generation via Prompt Grafting](https://arxiv.org/abs/2601.17666)
*Xinyue Pan,Yuhao Chen,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出训练无关的Prompt Grafting，通过先构建布局再接入目标提示，在采样中加入布局指导，有效控制多食物图像的纠缠问题并提高目标物出现率。


<details>
  <summary>Details</summary>
Motivation: 生成多食物餐盘时常见对象纠缠（相邻食物融合），传统扩散模型难以保持食物边界清晰，影响膳食评估和配方可视化等应用，因此需要一种能够控制食物分离与混合的生成方法。

Method: PG为无训练方法，分两阶段运行：先用布局提示（layout prompt）在图像中建立独立区域，在布局形成稳定后再将目标语句（target prompt）接入（graft），并在采样过程中引入隐式布局引导以维持区域边界。

Result: 在两个食物数据集上的实验显示，PG在目标对象出现率上显著提升，并定性展示了通过编辑布局可以控制食物分离或混合的能力。

Conclusion: 该论文提出了Prompt Grafting（PG）框架，有效缓解了文本到图像扩散模型在生成多食物图像时的对象纠缠问题，通过在采样阶段结合显式的布局提示和隐式的布局引导，实现对食物分离/混合的可控性。

Abstract: Real-world meal images often contain multiple food items, making reliable compositional food image generation important for applications such as image-based dietary assessment, where multi-food data augmentation is needed, and recipe visualization. However, modern text-to-image diffusion models struggle to generate accurate multi-food images due to object entanglement, where adjacent foods (e.g., rice and soup) fuse together because many foods do not have clear boundaries. To address this challenge, we introduce Prompt Grafting (PG), a training-free framework that combines explicit spatial cues in text with implicit layout guidance during sampling. PG runs a two-stage process where a layout prompt first establishes distinct regions and the target prompt is grafted once layout formation stabilizes. The framework enables food entanglement control: users can specify which food items should remain separated or be intentionally mixed by editing the arrangement of layouts. Across two food datasets, our method significantly improves the presence of target objects and provides qualitative evidence of controllable separation.

</details>


### [71] [Uni-RS: A Spatially Faithful Unified Understanding and Generation Model for Remote Sensing](https://arxiv.org/abs/2601.17673)
*Weiyu Zhang,Yuan Hu,Yong Li,Yu Liu*

Main category: cs.CV

TL;DR: Uni-RS通过布局规划、空间查询监督和布局变换，解决遥感多模态模型的空间不对称问题，提升生成任务的空间忠实性且不损害理解能力。


<details>
  <summary>Details</summary>
Motivation: 观察到现有统一遥感多模态模型在理解（如定位）上能准确把握物体位置，但在生成任务（如文本到图像）中无法忠实复现这些空间关系，存在明显的空间反转诅咒，因此需要显式建模几何信息以缩小理解与生成间的差距。

Method: 引入三大模块：(1) Spatial-Layout Planning：将文本指令转换为显式的空间布局规划，解耦几何规划与视觉合成；(2) Spatial-Aware Query Supervision：对可学习查询施加基于指令中空间关系的监督，引导其关注具体空间语义；(3) Image-Caption Spatial Layout Variation：对图像-描述对进行一致的几何变换以增强模型对空间变换的鲁棒性。

Result: 在多个基准上广泛实验表明，Uni-RS显著提升了文本到图像生成的空间忠实度，同时在图像描述、视觉定位和VQA等多模态理解任务上保持强劲表现。

Conclusion: 提出的Uni-RS有效缓解了遥感多模态模型在理解与生成之间的空间不对称问题，通过显式的布局规划、空间感知查询监督和布局变换增强，使模型在文本到图像生成时更能忠实表达空间关系，同时对理解任务保持较好性能。

Abstract: Unified remote sensing multimodal models exhibit a pronounced spatial reversal curse: Although they can accurately recognize and describe object locations in images, they often fail to faithfully execute the same spatial relations during text-to-image generation, where such relations constitute core semantic information in remote sensing. Motivated by this observation, we propose Uni-RS, the first unified multimodal model tailored for remote sensing, to explicitly address the spatial asymmetry between understanding and generation. Specifically, we first introduce explicit Spatial-Layout Planning to transform textual instructions into spatial layout plans, decoupling geometric planning from visual synthesis. We then impose Spatial-Aware Query Supervision to bias learnable queries toward spatial relations explicitly specified in the instruction. Finally, we develop Image-Caption Spatial Layout Variation to expose the model to systematic geometry-consistent spatial transformations. Extensive experiments across multiple benchmarks show that our approach substantially improves spatial faithfulness in text-to-image generation, while maintaining strong performance on multimodal understanding tasks like image captioning, visual grounding, and VQA tasks.

</details>


### [72] [StyleDecoupler: Generalizable Artistic Style Disentanglement](https://arxiv.org/abs/2601.17697)
*Zexi Jia,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: StyleDecoupler extracts pure artistic style from frozen vision-language models by minimizing mutual information with content-only uni-modal embeddings, plus a large WeART benchmark; outperforms prior methods in style retrieval.


<details>
  <summary>Details</summary>
Motivation: The paper aims to disentangle artistic style from semantic content in visual representations to improve style retrieval and analysis.

Method: Use uni-modal (content-focused) representations as references and minimize mutual information with multi-modal embeddings to extract style-only features; operates without fine-tuning as a plug-and-play module on frozen models.

Result: A plug-and-play module, StyleDecoupler, that isolates style features via mutual information minimization using uni-modal references; introduces WeART dataset; achieves SOTA on style retrieval and enables downstream applications.

Conclusion: StyleDecoupler effectively decouples style from content, improving style retrieval and enabling analyses; WeART provides a comprehensive benchmark for artistic style tasks.

Abstract: Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.

</details>


### [73] [An AI-enabled tool for quantifying overlapping red blood cell sickling dynamics in microfluidic assays](https://arxiv.org/abs/2601.17703)
*Nikhil Kadivar,Guansheng Li,Jianlu Zheng,John M. Higgins,Ming Dao,George Em Karniadakis,Mengjia Xu*

Main category: cs.CV

TL;DR: 提出结合AI标注、nnU-Net分割与分水岭重叠分解的自动化管线，在稀少标注与高密度重叠条件下高效量化镰刀红细胞时序形态变化，显著提升实验通量并支持药物评价。


<details>
  <summary>Details</summary>
Motivation: 在高密度和重叠细胞环境下，手工标注稀缺且传统方法难以准确识别细胞形态转变，迫切需要自动化、可扩展且对标注需求低的分析平台来研究镰刀细胞动力学及药物效应。

Method: 使用Roboflow平台进行AI辅助标注，训练nnU-Net进行分割；利用分水岭算法处理重叠细胞并进行实例计数；通过网络预测随时间变化的镰刀细胞比例。

Result: 即使在标注数据有限的情况下，框架仍实现了高分割性能，能够通过密集细胞悬液提高实验通量超过2倍，捕捉药物诱导的镰刀化动态并区分不同的形态演化力学生物学特征。

Conclusion: 该工作提出了一个自动化深度学习框架，可在时序显微镜图像中对不同密度下红细胞（RBC）形态转变进行量化，从而提高实验通量并揭示药物依赖的镰刀化行为及力学生物学特征。

Abstract: Understanding sickle cell dynamics requires accurate identification of morphological transitions under diverse biophysical conditions, particularly in densely packed and overlapping cell populations. Here, we present an automated deep learning framework that integrates AI-assisted annotation, segmentation, classification, and instance counting to quantify red blood cell (RBC) populations across varying density regimes in time-lapse microscopy data. Experimental images were annotated using the Roboflow platform to generate labeled dataset for training an nnU-Net segmentation model. The trained network enables prediction of the temporal evolution of the sickle cell fraction, while a watershed algorithm resolves overlapping cells to enhance quantification accuracy. Despite requiring only a limited amount of labeled data for training, the framework achieves high segmentation performance, effectively addressing challenges associated with scarce manual annotations and cell overlap. By quantitatively tracking dynamic changes in RBC morphology, this approach can more than double the experimental throughput via densely packed cell suspensions, capture drug-dependent sickling behavior, and reveal distinct mechanobiological signatures of cellular morphological evolution. Overall, this AI-driven framework establishes a scalable and reproducible computational platform for investigating cellular biomechanics and assessing therapeutic efficacy in microphysiological systems.

</details>


### [74] [Advancing Structured Priors for Sparse-Voxel Surface Reconstruction](https://arxiv.org/abs/2601.17720)
*Ting-Hsun Chi,Chu-Rong Chen,Chi-Tun Hsu,Hsuan-Ting Lin,Sheng-Yu Huang,Cheng Sun,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 本文提出将3D Gaussian Splatting与稀疏体素光栅化结合：用基于高斯点的初始化放置体素并按细节层次配置，另外引入基于射线的精细深度监督，以提高几何精度和细节恢复，同时保持快速收敛。


<details>
  <summary>Details</summary>
Motivation: 两种显式表示各有优缺点：高斯点收敛快但参数化限制细节，稀疏体素几何清晰但初始化通常为均匀稠密网格导致收敛慢并浪费结构信息。结合二者可发挥各自优势。

Method: 首先用3D Gaussian Splatting生成的点云估计场景结构，再在这些可信位置初始化稀疏体素并设置不同分辨率的体素大小作为先验；随后对稀疏体素进行连续不透明度字段的优化，同时加入将多视角信息转换为逐射线深度约束的精细深度监督（避免在边缘处模糊），最终通过每场景优化得到高质量表面。

Result: 在标准数据集上，方法在几何误差（深度/点到面距离等）和细节重建上领先，同时表面更完整，并且保留了快速收敛特性。

Conclusion: 结合高斯点驱动的体素初始化和射线级深度正则化，能在标准基准上比先前方法取得更高的几何准确性、更多细节与更完整的表面，且收敛速度快。

Abstract: Reconstructing accurate surfaces with radiance fields has progressed rapidly, yet two promising explicit representations, 3D Gaussian Splatting and sparse-voxel rasterization, exhibit complementary strengths and weaknesses. 3D Gaussian Splatting converges quickly and carries useful geometric priors, but surface fidelity is limited by its point-like parameterization. Sparse-voxel rasterization provides continuous opacity fields and crisp geometry, but its typical uniform dense-grid initialization slows convergence and underutilizes scene structure. We combine the advantages of both by introducing a voxel initialization method that places voxels at plausible locations and with appropriate levels of detail, yielding a strong starting point for per-scene optimization. To further enhance depth consistency without blurring edges, we propose refined depth geometry supervision that converts multi-view cues into direct per-ray depth regularization. Experiments on standard benchmarks demonstrate improvements over prior methods in geometric accuracy, better fine-structure recovery, and more complete surfaces, while maintaining fast convergence.

</details>


### [75] [Implicit Neural Representation-Based Continuous Single Image Super Resolution: An Empirical Study](https://arxiv.org/abs/2601.17723)
*Tayyab Nasir,Daochang Liu,Ajmal Mian*

Main category: cs.CV

TL;DR: 本文建立统一基准和代码库，对INR-based ASSR方法及训练配方做严格实证对比，发现复杂模型增益有限、训练配置与目标设计决定性能，并提出一种改进损失与缩放律验证。


<details>
  <summary>Details</summary>
Motivation: 缺乏对现有INR ASSR方法的系统性实验比较与训练配方分析，导致难以判断方法真实增益及研究方向；因此需要严格的对比基准、复现性代码及对训练/目标设计的深入研究。

Method: 作者搭建统一评估框架和代码库，汇总多种现有INR ASSR方法，在统一训练配方下比较性能，测试不同训练配置（如数据多样性、模型复杂度、优化策略）并提出一种新损失函数用于惩罚强度变化同时保留边缘/纹理细节。

Result: 实验表明：(1) 新的复杂INR方法相较早期方法仅有边际提升；(2) 模型性能高度依赖训练配置；(3) 所提损失可提升纹理保真度；(4) INR ASSR遵循缩放律，增加模型容量和数据多样性可获得可预测的性能提升。

Conclusion: 该文系统性评估了基于隐式神经表示（INR）的任意尺度图像超分（ASSR）方法，得出复杂模型提升有限、训练配置和目标函数对性能影响显著、并验证了缩放律适用性的结论。

Abstract: Implicit neural representation (INR) has become the standard approach for arbitrary-scale image super-resolution (ASSR). To date, no empirical study has systematically examined the effectiveness of existing methods, nor investigated the effects of different training recipes, such as scaling laws, objective design, and optimization strategies. A rigorous empirical analysis is essential not only for benchmarking performance and revealing true gains but also for establishing the current state of ASSR, identifying saturation limits, and highlighting promising directions. We fill this gap by comparing existing techniques across diverse settings and presenting aggregated performance results on multiple image quality metrics. We contribute a unified framework and code repository to facilitate reproducible comparisons. Furthermore, we investigate the impact of carefully controlled training configurations on perceptual image quality and examine a new loss function that penalizes intensity variations while preserving edges, textures, and finer details during training. We conclude the following key insights that have been previously overlooked: (1) Recent, more complex INR methods provide only marginal improvements over earlier methods. (2) Model performance is strongly correlated to training configurations, a factor overlooked in prior works. (3) The proposed loss enhances texture fidelity across architectures, emphasizing the role of objective design for targeted perceptual gains. (4) Scaling laws apply to INR-based ASSR, confirming predictable gains with increased model complexity and data diversity.

</details>


### [76] [Flatten The Complex: Joint B-Rep Generation via Compositional $k$-Cell Particles](https://arxiv.org/abs/2601.17733)
*Junran Lu,Yuanqi Li,Hengji Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 将B-Rep重新表征为共享潜变量的可组合k-单元粒子集合，并用多模态流匹配生成，统一拓扑与几何生成，提升有效性、可编辑性与应用扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统B-Rep生成方法依赖级联序列处理不同阶k-单元，无法充分利用单元间的共享和邻接关系，导致上下文感知不足与错误传播，因而需要一种打破刚性层次、增强几何耦合和编辑性的表示。

Method: 将B-Rep表征为粒子集合，每个拓扑实体由多个粒子组成，邻接单元在共享界面上拥有相同潜变量；采用多模态流匹配框架合成粒子集合，支持无条件生成和条件生成（如单视图或点云重建）；此外表示可用于局部修补并能直接合成非流形结构。

Result: 实验表明该方法在生成高保真CAD模型方面具备更好的有效性和可编辑性，优于现有最先进方法；同时支持局部in-painting及非流形结构合成。

Conclusion: 该论文提出了一种将B-Rep重构为可组合的k-单元粒子集的新范式，通过在单元界面共享潜在表示实现几何耦合，从而统一处理顶点、边和面，实现拓扑与几何的联合生成，改善了以往层级化方法在上下文感知和错误恢复方面的不足。

Abstract: Boundary Representation (B-Rep) is the widely adopted standard
  in Computer-Aided Design (CAD) and manufacturing. However, generative modeling of B-Reps remains a formidable challenge due to their inherent heterogeneity as geometric cell complexes, which entangles topology with geometry across cells of varying orders (i.e., $k$-cells such as vertices, edges, faces). Previous methods typically rely on cascaded sequences to handle this hierarchy, which fails to fully exploit the geometric relationships between cells, such as adjacency and sharing, limiting context awareness and error recovery. To fill this gap, we introduce a novel paradigm that reformulates B-Reps into sets of compositional $k$-cell particles. Our approach encodes each topological entity as a composition of particles, where adjacent cells share identical latents at their interfaces, thereby promoting geometric coupling along shared boundaries. By decoupling the rigid hierarchy, our representation unifies vertices, edges, and faces, enabling the joint generation of topology and geometry with global context awareness.
  We synthesize these particle sets using a multi-modal flow matching framework to handle unconditional generation as well as precise conditional tasks, such as 3D reconstruction from single-view or point cloud. Furthermore, the explicit and localized nature of our representation naturally extends to downstream tasks like local in-painting and enables the direct synthesis of non-manifold structures (e.g., wireframes). Extensive experiments demonstrate that our method produces high-fidelity CAD models with superior validity and editability compared to state-of-the-art methods.

</details>


### [77] [The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation](https://arxiv.org/abs/2601.17737)
*Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus*

Main category: cs.CV

TL;DR: 作者提出ScripterAgent+DirectorAgent的代理化流程与ScriptBench数据集，通过脚本驱动的跨场景连续生成显著提高对话到电影化视频的忠实度与长时一致性，同时揭示视觉华丽性与脚本一致性的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型无法从高层次对话生成长时、连贯且忠实于剧本的电影式视频，存在语义鸿沟，需通过脚本中介来桥接创意与镜头执行。

Method: 构建ScripterAgent将粗略对话翻译为可执行脚本，创建ScriptBench数据集进行训练；再用DirectorAgent基于跨场景连续生成策略调度现有视频模型生成长时视频；引入CriticAgent与VSA度量评估。

Result: 在AI CriticAgent和VSA指标评估下，框架在脚本忠实度和时间保真度上对比基线模型有显著提升，并发现SOTA模型在视觉效果与脚本遵从性之间存在权衡。

Conclusion: 该论文提出了一个端到端的代理框架，将对话转为电影化脚本并生成长片段连贯视频，显著改善了脚本忠实度与时间一致性。

Abstract: Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap'' between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.

</details>


### [78] [Learning Sewing Patterns via Latent Flow Matching of Implicit Fields](https://arxiv.org/abs/2601.17740)
*Cong Cao,Ren Li,Corentin Dumery,Hao Li*

Main category: cs.CV

TL;DR: 用SDF+unsigned DF的隐式表示编码裁片，结合潜在流匹配与缝合预测，实现更准确的纸样生成、估计与编辑。


<details>
  <summary>Details</summary>
Motivation: 传统自动化纸样生成难以准确建模多样的裁片几何形状与缝合关系，故引入更强的隐式表征与生成模型以提高表达能力与下游任务性能。

Method: 将每个裁片用有符号距离场（SDF）表示边界、无符号距离场表示缝端点，并将两者编码到连续潜在空间以支持可微分网格化；使用潜在流匹配（latent flow matching）模型学习裁片组合分布；采用缝合预测模块从提取的边缘段恢复缝合关系。

Result: 方法能准确建模与生成复杂结构的裁片组合，在从图像估计纸样、纸样补全与重合身（refitting）等任务上优于现有方法，支持数字时装设计应用。

Conclusion: 本文提出了基于隐式表示的裁片（panel）建模方法，以提高缝制纸样的表达与生成能力。

Abstract: Sewing patterns define the structural foundation of garments and are essential for applications such as fashion design, fabrication, and physical simulation. Despite progress in automated pattern generation, accurately modeling sewing patterns remains difficult due to the broad variability in panel geometry and seam arrangements. In this work, we introduce a sewing pattern modeling method based on an implicit representation. We represent each panel using a signed distance field that defines its boundary and an unsigned distance field that identifies seam endpoints, and encode these fields into a continuous latent space that enables differentiable meshing. A latent flow matching model learns distributions over panel combinations in this representation, and a stitching prediction module recovers seam relations from extracted edge segments. This formulation allows accurate modeling and generation of sewing patterns with complex structures. We further show that it can be used to estimate sewing patterns from images with improved accuracy relative to existing approaches, and supports applications such as pattern completion and refitting, providing a practical tool for digital fashion design.

</details>


### [79] [Frequency-aware Neural Representation for Videos](https://arxiv.org/abs/2601.17741)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: FaNeRV通过频率解耦、多分辨率监督与动态高频注入，有效提升INR视频压缩的细节保留与率失真表现。


<details>
  <summary>Details</summary>
Motivation: 现有INR偏向低频，导致重建模糊、率失真性能不佳，需显式处理不同频率成分以保留细节同时保持压缩效率。

Method: 提出多项技术：1) 多分辨率（分阶段）监督策略，逐步从全局到细节引导网络学习；2) 动态高频注入机制，自适应强调难重建区域的高频分量；3) 频率分解网络模块，对不同频段进行专门特征建模。

Result: 在标准数据集上的大量实验表明，FaNeRV在PSNR/SSIM等重建质量指标上明显优于最先进的INR方法，并在RD曲线上接近或超过部分传统视频编码器。

Conclusion: FaNeRV通过频率解耦和多分辨率监督，以及动态高频注入和频率分解网络模块，有效缓解了INR的频谱偏置，提升高频细节重建，从而在多个基准上优于现有INR方法，并在率失真上与传统编码器竞争。

Abstract: Implicit Neural Representations (INRs) have emerged as a promising paradigm for video compression. However, existing INR-based frameworks typically suffer from inherent spectral bias, which favors low-frequency components and leads to over-smoothed reconstructions and suboptimal rate-distortion performance. In this paper, we propose FaNeRV, a Frequency-aware Neural Representation for videos, which explicitly decouples low- and high-frequency components to enable efficient and faithful video reconstruction. FaNeRV introduces a multi-resolution supervision strategy that guides the network to progressively capture global structures and fine-grained textures through staged supervision . To further enhance high-frequency reconstruction, we propose a dynamic high-frequency injection mechanism that adaptively emphasizes challenging regions. In addition, we design a frequency-decomposed network module to improve feature modeling across different spectral bands. Extensive experiments on standard benchmarks demonstrate that FaNeRV significantly outperforms state-of-the-art INR methods and achieves competitive rate-distortion performance against traditional codecs.

</details>


### [80] [Video Compression with Hierarchical Temporal Neural Representation](https://arxiv.org/abs/2601.17743)
*Jun Zhu,Xinfeng Zhang,Lv Tang,Junhao Jiang,Gai Zhang,Jia Wang*

Main category: cs.CV

TL;DR: TeNeRV通过引入Inter-Frame Feature Fusion和GoP-Adaptive Modulation，在INR视频模型中同时建模短/长期依赖，显著优于先前方法的率失真表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式神经表示（INR）的视频压缩方法通常将时间作为独立输入，难以有效建模复杂的时序依赖，导致重建质量和压缩效率受限。

Method: 方法包含两部分：1) Inter-Frame Feature Fusion (IFF)：从相邻帧聚合特征，加强局部时序一致性并捕捉细粒度运动信息；2) GoP-Adaptive Modulation (GAM)：将视频按Group-of-Pictures划分，学习组特定先验并通过调制网络参数使表示在不同GoP间自适应。整体以分层方式整合短期与长期依赖。

Result: 广泛实验表明，TeNeRV在率-失真（rate-distortion）指标上稳定优于现有INR方法，验证了IFF与GAM对视频表示与压缩的有效性。

Conclusion: 本文提出了 TeNeRV，一种分层时序神经视频表示，通过邻帧特征融合和GOP自适应调制捕获短期与长期时序依赖，从而提升了 INR 基于方法的视频压缩性能。

Abstract: Video compression has recently benefited from implicit neural representations (INRs), which model videos as continuous functions. INRs offer compact storage and flexible reconstruction, providing a promising alternative to traditional codecs. However, most existing INR-based methods treat the temporal dimension as an independent input, limiting their ability to capture complex temporal dependencies. To address this, we propose a Hierarchical Temporal Neural Representation for Videos, TeNeRV. TeNeRV integrates short- and long-term dependencies through two key components. First, an Inter-Frame Feature Fusion (IFF) module aggregates features from adjacent frames, enforcing local temporal coherence and capturing fine-grained motion. Second, a GoP-Adaptive Modulation (GAM) mechanism partitions videos into Groups-of-Pictures and learns group-specific priors. The mechanism modulates network parameters, enabling adaptive representations across different GoPs. Extensive experiments demonstrate that TeNeRV consistently outperforms existing INR-based methods in rate-distortion performance, validating the effectiveness of our proposed approach.

</details>


### [81] [Bridging Supervision Gaps: A Unified Framework for Remote Sensing Change Detection](https://arxiv.org/abs/2601.17747)
*Kaixuan Jiang,Chen Wu,Zhenghui Zhao,Chengxi Han*

Main category: cs.CV

TL;DR: 提出UniCD统一框架，通过共享编码器和三分支协同学习处理三种监督方式，在弱监督和无监督任务上显著提升性能（在LEVIR-CD上分别提升12.72%和12.37%）。


<details>
  <summary>Details</summary>
Motivation: 现实场景中像素级标注昂贵，需适应多样注释可用性，现有模型难以统一处理不同监督形式，故提出统一框架以共享特征并协同学习不同监督信号

Method: UniCD采用共享编码器与三监督分支（监督分支引入STAM用于时空融合，弱监督分支采用CRR引导模型从粗激活到一致可分的变化表示，无监督分支提出SPCI将无监督转为受控弱监督路径优化），通过多分支协同学习实现异构监督的深度耦合。

Result: See fields filled below

Conclusion: UniCD provides a unified architecture effectively handling supervised, weakly-supervised, and unsupervised change detection, improving performance especially in weak/unsupervised settings and offering a flexible framework for varied annotation availability.

Abstract: Change detection (CD) aims to identify surface changes from multi-temporal remote sensing imagery. In real-world scenarios, Pixel-level change labels are expensive to acquire, and existing models struggle to adapt to scenarios with diverse annotation availability. To tackle this challenge, we propose a unified change detection framework (UniCD), which collaboratively handles supervised, weakly-supervised, and unsupervised tasks through a coupled architecture. UniCD eliminates architectural barriers through a shared encoder and multi-branch collaborative learning mechanism, achieving deep coupling of heterogeneous supervision signals. Specifically, UniCD consists of three supervision-specific branches. In the supervision branch, UniCD introduces the spatial-temporal awareness module (STAM), achieving efficient synergistic fusion of bi-temporal features. In the weakly-supervised branch, we construct change representation regularization (CRR), which steers model convergence from coarse-grained activations toward coherent and separable change modeling. In the unsupervised branch, we propose semantic prior-driven change inference (SPCI), which transforms unsupervised tasks into controlled weakly-supervised path optimization. Experiments on mainstream datasets demonstrate that UniCD achieves optimal performance across three tasks. It exhibits significant accuracy improvements in weakly and unsupervised scenarios, surpassing current state-of-the-art by 12.72% and 12.37% on LEVIR-CD, respectively.

</details>


### [82] [MV-S2V: Multi-View Subject-Consistent Video Generation](https://arxiv.org/abs/2601.17756)
*Ziyang Song,Xinyu Gong,Bangya Liu,Zelin Zhao*

Main category: cs.CV

TL;DR: 提出MV-S2V任务，利用合成+真实数据训练并引入TS-RoPE以区分视角与主体，实现更强的3D主体一致性的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有S2V方法通常仅使用单视角参考，导致生成可被分解为S2I+I2V流程，未能充分利用多视角信息以保证3D一致性。提出MV-S2V以实现从多视角参考生成具备3D一致性的主体驱动视频。

Method: 方法包括：1) 构建合成数据生成管道以扩充训练数据，并辅以小规模真实采集数据；2) 在条件编码中引入Temporally Shifted RoPE (TS-RoPE)，用于区分不同主体和同一主体的不同视角参考，从而减少跨主体/跨视角混淆。

Result: 模型在多视角参考下，生成的视频在3D主体一致性和视觉质量上均优于基线，证明了方法在主控视频生成上的有效性。

Conclusion: 该工作提出了多视角Subject-to-Video (MV-S2V)任务，通过多视角参考图像来提升生成视频在3D层面的主体一致性，克服了单视角S2V方法的局限。

Abstract: Existing Subject-to-Video Generation (S2V) methods have achieved high-fidelity and subject-consistent video generation, yet remain constrained to single-view subject references. This limitation renders the S2V task reducible to an S2I + I2V pipeline, failing to exploit the full potential of video subject control. In this work, we propose and address the challenging Multi-View S2V (MV-S2V) task, which synthesizes videos from multiple reference views to enforce 3D-level subject consistency. Regarding the scarcity of training data, we first develop a synthetic data curation pipeline to generate highly customized synthetic data, complemented by a small-scale real-world captured dataset to boost the training of MV-S2V. Another key issue lies in the potential confusion between cross-subject and cross-view references in conditional generation. To overcome this, we further introduce Temporally Shifted RoPE (TS-RoPE) to distinguish between different subjects and distinct views of the same subject in reference conditioning. Our framework achieves superior 3D subject consistency w.r.t. multi-view reference images and high-quality visual outputs, establishing a new meaningful direction for subject-driven video generation. Our project page is available at <a href="https://szy-young.github.io/mv-s2v">this URL</a>

</details>


### [83] [Agreement-Driven Multi-View 3D Reconstruction for Live Cattle Weight Estimation](https://arxiv.org/abs/2601.17791)
*Rabin Dulal,Wenfeng Jia,Lihong Zheng,Jane Quinn*

Main category: cs.CV

TL;DR: 用多视角RGB+SAM 3D一致性融合生成高质量点云，结合经典集成回归在小样本下可实现精确、实用的无接触牛体重估计，重建质量比模型复杂度更关键。


<details>
  <summary>Details</summary>
Motivation: 传统称重或体况评分需人工操作、干扰动物并影响生产效率，故研究低成本、无接触且易部署的3D重建体重估计方法以减少人工干预并提高养殖管理效率。

Method: 采用多视角RGB图像生成每头牛的单一3D点云，使用SAM 3D的一致性引导融合方法进行点云重建，然后比较经典集成回归模型与深度学习模型在小样本条件下的表现。

Result: 实验表明SAM 3D与多视角一致性融合在3D重建上优于其他方法，且经典集成模型在实用场景中表现最稳定（R^2=0.69±0.10，MAPE=2.22±0.56%），适合农场部署。

Conclusion: 本文提出并验证了一种基于多视角RGB图像和SAM 3D一致性引导融合的无接触牛体重估计方案，证明提高3D重建质量优于增加模型复杂度，对农场低数据场景具有实用性。

Abstract: Accurate cattle live weight estimation is vital for livestock management, welfare, and productivity. Traditional methods, such as manual weighing using a walk-over weighing system or proximate measurements using body condition scoring, involve manual handling of stock and can impact productivity from both a stock and economic perspective. To address these issues, this study investigated a cost-effective, non-contact method for live weight calculation in cattle using 3D reconstruction. The proposed pipeline utilized multi-view RGB images with SAM 3D-based agreement-guided fusion, followed by ensemble regression. Our approach generates a single 3D point cloud per animal and compares classical ensemble models with deep learning models under low-data conditions. Results show that SAM 3D with multi-view agreement fusion outperforms other 3D generation methods, while classical ensemble models provide the most consistent performance for practical farm scenarios (R$^2$ = 0.69 $\pm$ 0.10, MAPE = 2.22 $\pm$ 0.56 \%), making this practical for on-farm implementation. These findings demonstrate that improving reconstruction quality is more critical than increasing model complexity for scalable deployment on farms where producing a large volume of 3D data is challenging.

</details>


### [84] [ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning](https://arxiv.org/abs/2601.17818)
*Wen Luo,Peng Chen,Xiaotao Huang,LiQun Huang*

Main category: cs.CV

TL;DR: ViTCoP提出视觉与文本语义协同剪枝，通过视觉端冗余过滤与LLM内分层协同剪枝（以K-V L2范数为重要性度量）来保留重要且多样的视觉token，达到更高精度与更低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有token剪枝方法要么在视觉编码器阶段过早丢失重要视觉信息，要么在LLM阶段造成已选token间信息冗余；需要一种既能保留关键信息又能保证信息多样性的协同剪枝策略。

Method: 方法包含两阶段：1) 在视觉编码器中进行冗余过滤以初步降低token数量；2) 在LLM内部基于分层结构逐步协同剪枝，利用K-V向量L2范数作为token重要性度量以兼容FlashAttention等加速技术。

Result: 在多种大型视-语模型和图像/视频理解任务上，ViTCoP在性能上优于现有方法，同时显著降低推理延迟和GPU内存占用，且在极端高剪枝率下优势更加明显。

Conclusion: ViTCoP通过在视觉编码器中先过滤冗余，再在LLM中基于层级特性进行逐步协同剪枝，有效保留关键且信息多样的视觉token，从而在大视-语模型上实现更高的效能与更低的推理开销。

Abstract: Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.

</details>


### [85] [VAE-REPA: Variational Autoencoder Representation Alignment for Efficient Diffusion Training](https://arxiv.org/abs/2601.17830)
*Mengmeng Wang,Dengyang Jiang,Liuzhuozheng Li,Yucheng Lin,Guojiang Shen,Xiangjie Kong,Yong Liu,Guang Dai,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出将扩散变换器中间特征与预训练VAE特征对齐的轻量内在引导方法，显著加速训练收敛并提升生成质量，代价仅为微小的计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于去噪的扩散变换器虽生成性能强，但训练收敛慢。现有加速方法依赖外部编码器(REPA)或双模型(SRA)，带来额外计算开销。作者希望通过利用VAE内在的视觉先验信息，设计一种无需外部依赖的轻量级加速方案。

Method: 使用开箱即用的预训练VAE的潜在重建特征，添加轻量投影层将扩散变换器的中间潜在特征映射到VAE特征空间，通过特征对齐损失进行监督，训练时无需额外外部模型或双模型结构。

Result: 在多项实验中，\name在生成质量和训练收敛速度上均优于原始扩散变换器，并与最先进的加速方法相当或更好。方法仅增加约4% GFLOPs，且不需要额外外部引导模型。

Conclusion: 该文提出了一种名为\namex的轻量级内在引导框架，通过对齐扩散变换器中间潜在特征与预训练VAE特征，加速训练收敛并提升生成质量。实验证明该方法在不依赖外部表示编码器或双模型维护的情况下，匹配或优于现有加速方法，且仅增加约4% GFLOPs。

Abstract: Denoising-based diffusion transformers, despite their strong generation performance, suffer from inefficient training convergence. Existing methods addressing this issue, such as REPA (relying on external representation encoders) or SRA (requiring dual-model setups), inevitably incur heavy computational overhead during training due to external dependencies. To tackle these challenges, this paper proposes \textbf{\namex}, a lightweight intrinsic guidance framework for efficient diffusion training. \name leverages off-the-shelf pre-trained Variational Autoencoder (VAE) features: their reconstruction property ensures inherent encoding of visual priors like rich texture details, structural patterns, and basic semantic information. Specifically, \name aligns the intermediate latent features of diffusion transformers with VAE features via a lightweight projection layer, supervised by a feature alignment loss. This design accelerates training without extra representation encoders or dual-model maintenance, resulting in a simple yet effective pipeline. Extensive experiments demonstrate that \name improves both generation quality and training convergence speed compared to vanilla diffusion transformers, matches or outperforms state-of-the-art acceleration methods, and incurs merely 4\% extra GFLOPs with zero additional cost for external guidance models.

</details>


### [86] [Geometry-Grounded Gaussian Splatting](https://arxiv.org/abs/2601.17835)
*Baowen Zhang,Chenxing Jiang,Heng Li,Shaojie Shen,Ping Tan*

Main category: cs.CV

TL;DR: 把高斯片元形式化为随机实心体，利用体渲染深度直接提取几何，提升多视图一致性和抗漂浮物能力，在公开数据集上表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有从高斯片元提取几何的方法参数化或近似不足，导致多视图不一致和对浮游体敏感，需一个更有理论基础的表示来直接处理几何。

Method: 理论推导高斯片元为随机实心体，利用其体积特性进行基于深度的渲染以提取几何，随后进行细化并评估于公开数据集。

Result: They derive Gaussian primitives as stochastic solids and use volumetric rendering for depth to extract geometry, improving multi-view consistency and floater robustness; achieves SOTA among GS-based methods.

Conclusion: 将高斯片元视作具有体积的随机实心体，基于此直接渲染高质量深度并提取精细几何，可有效改进形状重建的多视图一致性与对浮游体的鲁棒性。

Abstract: Gaussian Splatting (GS) has demonstrated impressive quality and efficiency in novel view synthesis. However, shape extraction from Gaussian primitives remains an open problem. Due to inadequate geometry parameterization and approximation, existing shape reconstruction methods suffer from poor multi-view consistency and are sensitive to floaters. In this paper, we present a rigorous theoretical derivation that establishes Gaussian primitives as a specific type of stochastic solids. This theoretical framework provides a principled foundation for Geometry-Grounded Gaussian Splatting by enabling the direct treatment of Gaussian primitives as explicit geometric representations. Using the volumetric nature of stochastic solids, our method efficiently renders high-quality depth maps for fine-grained geometry extraction. Experiments show that our method achieves the best shape reconstruction results among all Gaussian Splatting-based methods on public datasets.

</details>


### [87] [SynMind: Reducing Semantic Hallucination in fMRI-Based Image Reconstruction](https://arxiv.org/abs/2601.17857)
*Lan Yang,Minghan Yang,Ke Li,Honggang Zhang,Kaiyue Pang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 通过将fMRI解码为多粒度文本语义并与视觉先验联合，SynMind在语义一致性和重建质量上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于fMRI的图像重建虽在视觉质量上进步显著，但常出现语义错位（对象替换或幻觉），作者认为这是因为依赖纠缠的视觉嵌入，缺乏显式语义身份信息。

Method: 使用可落地的视觉语言模型（VLM）生成合成的人类式多粒度文本表示，训练一个文本对齐模块将fMRI映射到这些语义描述，再将语义编码与视觉先验一起作为条件输入到预训练扩散模型（SynMind）进行图像重建。

Result: SynMind在大多数定量指标上优于最先进方法，使用更小的Stable Diffusion 1.4和单个消费级GPU即可超过基于SDXL的方法；大规模人类评估和神经可视化分析也验证了其语义一致性和更广泛的语义相关脑区参与。

Conclusion: 该论文提出通过将fMRI信号解析为多粒度文本语义描述，并结合视觉先验来引导扩散模型，从而显著改善重建图像的语义一致性。

Abstract: Recent advances in fMRI-based image reconstruction have achieved remarkable photo-realistic fidelity. Yet, a persistent limitation remains: while reconstructed images often appear naturalistic and holistically similar to the target stimuli, they frequently suffer from severe semantic misalignment -- salient objects are often replaced or hallucinated despite high visual quality. In this work, we address this limitation by rethinking the role of explicit semantic interpretation in fMRI decoding. We argue that existing methods rely too heavily on entangled visual embeddings which prioritize low-level appearance cues -- such as texture and global gist -- over explicit semantic identity. To overcome this, we parse fMRI signals into rich, sentence-level semantic descriptions that mirror the hierarchical and compositional nature of human visual understanding. We achieve this by leveraging grounded VLMs to generate synthetic, human-like, multi-granularity textual representations that capture object identities and spatial organization. Built upon this foundation, we propose SynMind, a framework that integrates these explicit semantic encodings with visual priors to condition a pretrained diffusion model. Extensive experiments demonstrate that SynMind outperforms state-of-the-art methods across most quantitative metrics. Notably, by offloading semantic reasoning to our text-alignment module, SynMind surpasses competing methods based on SDXL while using the much smaller Stable Diffusion 1.4 and a single consumer GPU. Large-scale human evaluations further confirm that SynMind produces reconstructions more consistent with human visual perception. Neurovisualization analyses reveal that SynMind engages broader and more semantically relevant brain regions, mitigating the over-reliance on high-level visual areas.

</details>


### [88] [Domain Generalization with Quantum Enhancement for Medical Image Classification: A Lightweight Approach for Cross-Center Deployment](https://arxiv.org/abs/2601.17862)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出基于MobileNetV2的轻量级量子增强域泛化框架，结合图像扰动、域对抗训练和参数化量子电路，并配合测试时自适应；在模拟多中心数据上提升了跨域AUC与敏感性，降低域间性能差异。


<details>
  <summary>Details</summary>
Motivation: 医学图像AI模型在单中心或单设备训练时表现优异，但在跨中心部署时由于域移导致性能下降，限制了临床泛化性；且获取真实多中心标注数据代价高，因此需要在无真实多中心标注数据情况下提升模型泛化。

Method: 构建基于MobileNetV2的域不变编码器，通过三部分优化：1) 使用亮度、对比度、锐化和噪声扰动模拟多域成像差异；2) 采用带梯度反转的域对抗训练以抑制域判别性特征；3) 引入轻量级量子特征增强层，使用参数化量子电路进行非线性特征映射与纠缠建模；并在推理时加入测试时自适应策略。

Result: 在模拟的多中心医学影像数据集上，所提方法在未见域上显著优于无域泛化或无量子增强的基线模型，表现为域内性能方差减小、AUC和敏感性提升。

Conclusion: 该论文提出了一种轻量级的域泛化框架，结合量子增强协同学习，旨在提高医学图像AI在未见目标域的泛化能力，结论认为该方法在模拟多中心数据上优于基线模型，降低了域间性能方差，并提升了AUC与敏感性。

Abstract: Medical image artificial intelligence models often achieve strong performance in single-center or single-device settings, yet their effectiveness frequently deteriorates in real-world cross-center deployment due to domain shift, limiting clinical generalizability. To address this challenge, we propose a lightweight domain generalization framework with quantum-enhanced collaborative learning, enabling robust generalization to unseen target domains without relying on real multi-center labeled data. Specifically, a MobileNetV2-based domain-invariant encoder is constructed and optimized through three key components: (1) multi-domain imaging shift simulation using brightness, contrast, sharpening, and noise perturbations to emulate heterogeneous acquisition conditions; (2) domain-adversarial training with gradient reversal to suppress domain-discriminative features; and (3) a lightweight quantum feature enhancement layer that applies parameterized quantum circuits for nonlinear feature mapping and entanglement modeling. In addition, a test-time adaptation strategy is employed during inference to further alleviate distribution shifts. Experiments on simulated multi-center medical imaging datasets demonstrate that the proposed method significantly outperforms baseline models without domain generalization or quantum enhancement on unseen domains, achieving reduced domain-specific performance variance and improved AUC and sensitivity. These results highlight the clinical potential of quantum-enhanced domain generalization under constrained computational resources and provide a feasible paradigm for hybrid quantum--classical medical imaging systems.

</details>


### [89] [MV-SAM: Multi-view Promptable Segmentation using Pointmap Guidance](https://arxiv.org/abs/2601.17866)
*Yoonwoo Jeong,Cheng Sun,Yu-Chiang Frank Wang,Minsu Cho,Jaesung Choe*

Main category: cs.CV

TL;DR: MV-SAM 用从未配姿图像重建的 pointmaps 将 SAM 的 2D 嵌入提升到 3D，利用 transformer 与 3D 提示交叉注意实现多视图 3D 一致的可提示分割，无需显式 3D 网络或 per-scene 优化，性能优于 SAM2-Video、与优化基线可比。


<details>
  <summary>Details</summary>
Motivation: 当前 promptable segmentation 在多视图/视频场景中缺乏 3D 感知，导致跨视图不一致，现有解决方案依赖代价高昂的 per-scene 优化来强制 3D 一致性。MV-SAM 旨在通过利用从未配姿图像恢复的 3D 点（pointmaps）来内置 3D 一致性，改进跨视图分割一致性且降低计算开销。

Method: 在 SA-1B 数据集上训练；利用 pointmaps 的像素-点一一对应，将预训练编码器的 2D 图像嵌入映射到 3D 点嵌入，结合 3D 位置嵌入并由 transformer 解码，通过与 3D 提示的交叉注意实现 2D 交互与 3D 几何的对齐。该设计无需显式的 3D 网络或 3D 标注，也不依赖 per-scene 优化。

Result: 在 NVOS、SPIn-NeRF、ScanNet++、uCo3D 和 DL3DV 等基准上，MV-SAM 超越了 SAM2-Video，并在无需 per-scene 优化的情况下与需要优化的基线方法达到可比性能；在 SA-1B 上训练后具备良好领域泛化能力。

Conclusion: MV-SAM 提出了一种利用 pointmaps（由视觉几何模型从未配姿的图像重建的 3D 点）将图像和提示提升到三维空间的多视图分割框架，从而在无需显式三维网络或标注三维数据的情况下实现 3D 一致性。该方法通过将 SAM 的图像嵌入提升为 3D 点嵌入，并由一个使用与 3D 提示嵌入进行交叉注意的 transformer 解码器来生成掩码。

Abstract: Promptable segmentation has emerged as a powerful paradigm in computer vision, enabling users to guide models in parsing complex scenes with prompts such as clicks, boxes, or textual cues. Recent advances, exemplified by the Segment Anything Model (SAM), have extended this paradigm to videos and multi-view images. However, the lack of 3D awareness often leads to inconsistent results, necessitating costly per-scene optimization to enforce 3D consistency. In this work, we introduce MV-SAM, a framework for multi-view segmentation that achieves 3D consistency using pointmaps -- 3D points reconstructed from unposed images by recent visual geometry models. Leveraging the pixel-point one-to-one correspondence of pointmaps, MV-SAM lifts images and prompts into 3D space, eliminating the need for explicit 3D networks or annotated 3D data. Specifically, MV-SAM extends SAM by lifting image embeddings from its pretrained encoder into 3D point embeddings, which are decoded by a transformer using cross-attention with 3D prompt embeddings. This design aligns 2D interactions with 3D geometry, enabling the model to implicitly learn consistent masks across views through 3D positional embeddings. Trained on the SA-1B dataset, our method generalizes well across domains, outperforming SAM2-Video and achieving comparable performance with per-scene optimization baselines on NVOS, SPIn-NeRF, ScanNet++, uCo3D, and DL3DV benchmarks. Code will be released.

</details>


### [90] [VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://arxiv.org/abs/2601.17868)
*Zhihao He,Tieyuan Chen,Kangyu Wang,Ziran Qin,Yang Shao,Chaofan Gan,Shijie Li,Zuxuan Wu,Weiyao Lin*

Main category: cs.CV

TL;DR: VidLaDA用扩散语言模型+双向注意力提高视频理解能力，配合MARS-Cache实现显著推理加速（>12x）而不牺牲精度。


<details>
  <summary>Details</summary>
Motivation: 自回归视频LLM因因果掩码导致的只能单向建模，限制了全局时空信息的高效融合与理解，且扩散解码虽然可双向建模但推理开销高，需设计加速机制。

Method: 提出基于扩散语言模型的Video LLM（VidLaDA），使用双向注意力捕捉双向依赖；为解决扩散解码在大量视频token上的推理瓶颈，引入MARS-Cache，通过异步视觉缓存刷新与逐帧分块注意力相结合，利用锚点token保持全局连通性并裁剪冗余。

Result: 在大量实验中，VidLaDA优于其他扩散基线并可与Qwen2.5-VL、LLaVA-Video等自回归SOTA模型相媲美；引入MARS-Cache后推理速度提升超过12倍，且在推理准确性上几乎无损失。

Conclusion: VidLaDA通过将扩散语言模型与双向注意力相结合，有效克服了自回归视频LLM的因果掩码偏差，在理解效率和全局时空建模上取得提升。结合MARS-Cache后，推理速度显著提升 (>12x)，同时保持推理准确率，与现有自回归强基线性能相当或更优。

Abstract: Standard Autoregressive Video LLMs inevitably suffer from causal masking biases that hinder global spatiotemporal modeling, leading to suboptimal understanding efficiency. We propose VidLaDA, a Video LLM based on Diffusion Language Model utilizing bidirectional attention to capture bidirectional dependencies. To further tackle the inference bottleneck of diffusion decoding on massive video tokens, we introduce MARS-Cache. This framework accelerates inference by combining asynchronous visual cache refreshing with frame-wise chunk attention, effectively pruning redundancy while preserving global connectivity via anchor tokens. Extensive experiments show VidLaDA outperforms diffusion baselines and rivals state-of-the-art autoregressive models (e.g., Qwen2.5-VL and LLaVA-Video), with MARS-Cache delivering over 12x speedup without compromising reasoning accuracy. Code and checkpoints are open-sourced at https://github.com/ziHoHe/VidLaDA.

</details>


### [91] [Quran-MD: A Fine-Grained Multilingual Multimodal Dataset of the Quran](https://arxiv.org/abs/2601.17880)
*Muhammad Umar Salman,Mohammad Areeb Qazi,Mohammed Talha Alam*

Main category: cs.CV

TL;DR: Quran MD 提供节与词级的阿拉伯文、英文翻译、音标与32名诵读者的音频对齐，适用于ASR、TTS、风格迁移和教学等多模态研究与应用。


<details>
  <summary>Details</summary>
Motivation: 填补现有古兰经资源在跨模态、多诵读者、多粒度对齐方面的空白，支持ASR、TTS、朗诵风格分析和教学应用。

Method: 收集每节经文的阿拉伯原文、英文翻译、音标转写；从32名诵读者处采集节级音频；对每个单词进行逐词对齐，提供阿拉伯词形、英文翻译、音标以及对齐的单词级音频。

Result: 构建了包含节级与词级文本-音频对齐的多诵读者数据集，覆盖32名诵读者，支持多项下游任务并已公开发布于 Hugging Face。

Conclusion: Quran MD 是一个多模态、细粒度的古兰经数据集，兼顾文本文本、音频和语言学标注，对研究和应用具有重要价值。

Abstract: We present Quran MD, a comprehensive multimodal dataset of the Quran that integrates textual, linguistic, and audio dimensions at the verse and word levels. For each verse (ayah), the dataset provides its original Arabic text, English translation, and phonetic transliteration. To capture the rich oral tradition of Quranic recitation, we include verse-level audio from 32 distinct reciters, reflecting diverse recitation styles and dialectical nuances. At the word level, each token is paired with its corresponding Arabic script, English translation, transliteration, and an aligned audio recording, allowing fine-grained analysis of pronunciation, phonology, and semantic context. This dataset supports various applications, including natural language processing, speech recognition, text-to-speech synthesis, linguistic analysis, and digital Islamic studies. Bridging text and audio modalities across multiple reciters, this dataset provides a unique resource to advance computational approaches to Quranic recitation and study. Beyond enabling tasks such as ASR, tajweed detection, and Quranic TTS, it lays the foundation for multimodal embeddings, semantic retrieval, style transfer, and personalized tutoring systems that can support both research and community applications. The dataset is available at https://huggingface.co/datasets/Buraaq/quran-audio-text-dataset

</details>


### [92] [PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation](https://arxiv.org/abs/2601.17885)
*Qingyu Fan,Zhaoxiang Li,Yi Lu,Wang Chen,Qiu Shen,Xiao-xiao Long,Yinghao Cai,Tao Lu,Shuo Wang,Xun Cao*

Main category: cs.CV

TL;DR: 提出PEAfowl：通过深度分布预测+可微3D提升+局部跨视图聚合和Perceiver式文本readout，以及训练时深度蒸馏，增强多视图VLA在双手操控中的几何一致性与指令对齐，显著提升模拟与真实环境表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在多视角信息融合时采用视角不可知的token拼接，导致弱3D一致性；且语言作为全局条件注入，造成粗糙的指令落地。作者希望通过引入几何感知的感知前端与更细粒度的文本感知读取，提高在拥挤场景下的稳健性和泛化能力。

Method: 方法包括：1) 对视觉token预测深度分布并进行可微分3D提升；2) 聚合局部跨视图邻居以获得几何约束的跨视图一致表征；3) 用Perceiver式文本感知readout替代全局语言条件，实现逐步证据累积的指令接地；4) 在训练阶段从预训练深度教师蒸馏深度分布，利用商品深度数据的不完全信息而不增加推理开销。

Result: 在RoboTwin 2.0的领域随机化设置下，PEAfowl较最强基线成功率提升23.0个百分点；真实机器人实验显示可靠的sim-to-real迁移以及深度蒸馏的一致提升效果。

Conclusion: PEAfowl通过引入每token深度分布、可微三维提升与局部跨视图聚合，以及采用Perceiver风格的文本感知读取器和训练时深度蒸馏，有效提升了多视角VLA策略在双手操控任务中的几何一致性与指令定位能力，从而显著提高了模拟与真实环境下的成功率。

Abstract: Bimanual manipulation in cluttered scenes requires policies that remain stable under occlusions, viewpoint and scene variations. Existing vision-language-action models often fail to generalize because (i) multi-view features are fused via view-agnostic token concatenation, yielding weak 3D-consistent spatial understanding, and (ii) language is injected as global conditioning, resulting in coarse instruction grounding.
  In this paper, we introduce PEAfowl, a perception-enhanced multi-view VLA policy for bimanual manipulation. For spatial reasoning, PEAfowl predicts per-token depth distributions, performs differentiable 3D lifting, and aggregates local cross-view neighbors to form geometrically grounded, cross-view consistent representations. For instruction grounding, we propose to replace global conditioning with a Perceiver-style text-aware readout over frozen CLIP visual features, enabling iterative evidence accumulation. To overcome noisy and incomplete commodity depth without adding inference overhead, we apply training-only depth distillation from a pretrained depth teacher to supervise the depth-distribution head, providing perception front-end with geometry-aware priors.
  On RoboTwin 2.0 under domain-randomized setting, PEAfowl improves the strongest baseline by 23.0 pp in success rate, and real-robot experiments further demonstrate reliable sim-to-real transfer and consistent improvements from depth distillation.
  Project website: https://peafowlvla.github.io/.

</details>


### [93] [Masked Depth Modeling for Spatial Perception](https://arxiv.org/abs/2601.17895)
*Bin Tan,Changjiang Sun,Xiage Qin,Hanat Adai,Zelin Fu,Tianxiang Zhou,Han Zhang,Yinghao Xu,Xing Zhu,Yujun Shen,Nan Xue*

Main category: cs.CV

TL;DR: 提出利用视觉上下文和遮 mask 深度建模的深度补全模型LingBot-Depth，配合自动数据整备，宣称超越顶级RGB-D传感器并提供3M对数据与模型。


<details>
  <summary>Details</summary>
Motivation: 针对RGB-D相机在高光、无纹理表面等场景下深度测量不可靠的问题，提出把传感器误差视为掩码信号并用视觉线索去补全，从而提高实用场景下的深度可用性。

Method: 核心方法为masked depth modeling：将传感器稀疏/不准确深度视为带掩码的信号，利用RGB上下文与网络推断被掩盖区域的度量深度；并搭配自动化数据清洗/筛选与合成数据扩充来规模化训练。

Result: LingBot-Depth提出了一种基于遮挡/掩码深度建模(masked depth modeling)的深度补全方法，并结合自动数据整理管道进行大规模训练；作者声称在深度精度和像素覆盖率上优于顶级RGB-D相机，并发布了代码、权重及3M RGB-深度对数据集。

Conclusion: LingBot-Depth通过把深度传感器误差视为“被掩盖的”信号，利用视觉信息恢复完整深度，实验显示在精度、覆盖率和下游任务表现上有优势；公开了代码和大规模训练数据，有利于社区复现与应用。

Abstract: Spatial visual perception is a fundamental requirement in physical-world applications like autonomous driving and robotic manipulation, driven by the need to interact with 3D environments. Capturing pixel-aligned metric depth using RGB-D cameras would be the most viable way, yet it usually faces obstacles posed by hardware limitations and challenging imaging conditions, especially in the presence of specular or texture-less surfaces. In this work, we argue that the inaccuracies from depth sensors can be viewed as "masked" signals that inherently reflect underlying geometric ambiguities. Building on this motivation, we present LingBot-Depth, a depth completion model which leverages visual context to refine depth maps through masked depth modeling and incorporates an automated data curation pipeline for scalable training. It is encouraging to see that our model outperforms top-tier RGB-D cameras in terms of both depth precision and pixel coverage. Experimental results on a range of downstream tasks further suggest that LingBot-Depth offers an aligned latent representation across RGB and depth modalities. We release the code, checkpoint, and 3M RGB-depth pairs (including 2M real data and 1M simulated data) to the community of spatial perception.

</details>


### [94] [Revisiting 3D Reconstruction Kernels as Low-Pass Filters](https://arxiv.org/abs/2601.17900)
*Shengjun Zhang,Min Chen,Yibo Wei,Mingyu Dong,Yueqi Duan*

Main category: cs.CV

TL;DR: 从信号处理角度，使用Jinc与调制Jinc核以实现更理想的低通滤波，减少频谱重叠，从而提升3D重建与渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决既有三维重建核函数无法完全隔离基带频谱与高频分量重叠的问题，提升渲染与重建的频域保真度。

Method: 分析离散采样导致的周期性频谱扩展问题，比较了现有核函数（高斯、指数、Student's t）的低通特性及其不足，提出采用Jinc核实现理想低通（在截止频率处幅值为零），并为加速空间收敛设计调制后的Jinc变体以提高空间域衰减，从而在频域保真与空间效率之间权衡。

Result: 引入Jinc和调制核后，在渲染性能和重建效果上优于传统核函数，实验结果证明其在减少频谱混叠、提升重建质量方面有效。

Conclusion: 本文从信号处理视角重审三维重建，认为离散采样引入的周期性频谱扩展是核心挑战。提出使用Jinc核（在截止频率处幅值瞬时降为零）作为理想低通滤波器以避免频谱重叠，并为解决Jinc在空间域衰减慢的问题，引入调制核在空间效率与频域保真之间取得平衡。实验验证了所提方法的有效性。

Abstract: 3D reconstruction is to recover 3D signals from the sampled discrete 2D pixels, with the goal to converge continuous 3D spaces. In this paper, we revisit 3D reconstruction from the perspective of signal processing, identifying the periodic spectral extension induced by discrete sampling as the fundamental challenge. Previous 3D reconstruction kernels, such as Gaussians, Exponential functions, and Student's t distributions, serve as the low pass filters to isolate the baseband spectrum. However, their unideal low-pass property results in the overlap of high-frequency components with low-frequency components in the discrete-time signal's spectrum. To this end, we introduce Jinc kernel with an instantaneous drop to zero magnitude exactly at the cutoff frequency, which is corresponding to the ideal low pass filters. As Jinc kernel suffers from low decay speed in the spatial domain, we further propose modulated kernels to strick an effective balance, and achieves superior rendering performance by reconciling spatial efficiency and frequency-domain fidelity. Experimental results have demonstrated the effectiveness of our Jinc and modulated kernels.

</details>


### [95] [Feature-Space Generative Models for One-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.17905)
*Jack Foster,Kirill Paramonov,Mete Ozay,Umberto Michieli*

Main category: cs.CV

TL;DR: Gen1S：对嵌入空间做原型残差建模，使用VAE/扩散模型学习基类残差多模态分布，作为先验生成残差增强1-shot新类识别，效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 1-shot FSCIL极度缺乏新类样本，直接泛化困难。假设基类与新类的嵌入结构相似，通过建模基类残差的多模态分布可以提供有益的结构先验，改善新类识别。

Method: 将样本嵌入减去对应类别原型得到残差表征，在基类残差上训练VAE或扩散模型，作为结构先验生成多样残差以增强新类样本；在推理时将生成残差与新样本原型结合用于分类。

Result: 提出的Gen1S方法在多个基准和不同骨干网络上均优于现有最先进方法，显著提升了新类识别准确率。

Conclusion: 该论文在1-shot FSCIL情景下，通过学习基于基类残差分布的生成模型，提升了新类识别性能。

Abstract: Few-shot class-incremental learning (FSCIL) is a paradigm where a model, initially trained on a dataset of base classes, must adapt to an expanding problem space by recognizing novel classes with limited data. We focus on the challenging FSCIL setup where a model receives only a single sample (1-shot) for each novel class and no further training or model alterations are allowed after the base training phase. This makes generalization to novel classes particularly difficult. We propose a novel approach predicated on the hypothesis that base and novel class embeddings have structural similarity. We map the original embedding space into a residual space by subtracting the class prototype (i.e., the average class embedding) of input samples. Then, we leverage generative modeling with VAE or diffusion models to learn the multi-modal distribution of residuals over the base classes, and we use this as a valuable structural prior to improve recognition of novel classes. Our approach, Gen1S, consistently improves novel class recognition over the state of the art across multiple benchmarks and backbone architectures.

</details>


### [96] [Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models](https://arxiv.org/abs/2601.17918)
*Dain Kim,Jiwoo Lee,Jaehoon Yun,Yong Hoe Koo,Qingyu Chen,Hyunjae Kim,Jaewoo Kang*

Main category: cs.CV

TL;DR: 系统比较九种DPO变体在两款医疗LVLM上的效果，发现收益不稳且常忽视视觉误判；提出针对视觉误读的偏好构建策略，VQA上提升3.6%，并开源全部资源。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO在通用领域表现良好，但其在医疗高风险场景的有效性与可靠性尚无系统评估，且缺乏解决视觉误读的机制。本文旨在填补这一空白，为医疗LVLM的安全部署提供实证基础与改进方向。

Method: 比较分析九种DPO配方（包含不同偏好构建与约束策略）在两个医疗LVLM（LLaVA-Med与HuatuoGPT-Vision）上的表现；通过任务级评估（VQA等）与故障模式分析定位视觉误读错误；并设计专门的偏好构建策略对常见视觉误读进行惩罚或纠正，最后在验证集上测量改进。

Result: 发现当前DPO方法相较于监督微调并不总是稳健提升，效果在任务类型与模型结构间差异显著；常见视觉误读未被充分纠正。提出的偏好构建策略能显著降低视觉误读并在VQA上带来3.6%绝对收益；并公开全部训练数据与代码。

Conclusion: 本文系统评估了多种DPO变体在医疗大视觉-语言模型（LVLMs）中的表现，发现现有DPO方法在医疗场景中收益不稳定且常忽略视觉误读问题。提出的基于优先级构建的偏好策略针对视觉误读进行校正，作为概念验证在VQA任务上比最强DPO基线提升3.6%。同时发布完整框架与数据供后续研究使用。

Abstract: Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.

</details>


### [97] [RemEdit: Efficient Diffusion Editing with Riemannian Geometry](https://arxiv.org/abs/2601.17927)
*Eashan Adhikarla,Brian D. Davison*

Main category: cs.CV

TL;DR: 提出RemEdit：通过黎曼流形导航+目标感知提示增强的编辑控制，结合任务特定注意力剪枝实现加速，达成高保真与实时性兼顾。


<details>
  <summary>Details</summary>
Motivation: 目标是在保持高语义编辑保真度的同时显著加速基于扩散模型的图像编辑推理，使其达到实时或近实时水平，从而在实际应用中更具可用性。

Method: 方法包括：1) 在潜在空间构建黎曼流形，使用mamba模块学习流形结构并计算测地线实现平滑语义编辑；2) 采用双SLERP混合与视觉-语言模型生成的目标感知提示增强以精细控制编辑；3) 引入任务特定的注意力剪枝，训练轻量剪枝头保留对编辑重要的token以实现加速而不损害语义。

Result: RemEdit在编辑质量上超过之前的最先进编辑框架，并在50%注意力剪枝下仍维持实时性能，证明方法在速度与保真之间取得良好平衡。相关代码已开源。

Conclusion: 该论文提出了名为RemEdit的编辑框架，通过在潜在空间上以黎曼流形方式导航并结合任务特定的注意力剪枝，解决了语义保真度与推理速度之间的权衡，实验显示在50%剪枝下仍能保持实时性能并优于现有方法。

Abstract: Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold's structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.

</details>


### [98] [From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images](https://arxiv.org/abs/2601.17934)
*Vi Vu,Thanh-Huy Nguyen,Tien-Thinh Nguyen,Ba-Thinh Lam,Hoang-Thien Nguyen,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出SC-SAM：U-Net向SAM提供点提示与伪标签，SAM反过来监督U-Net，形成双向协同的半监督框架，在医学分割任务上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: SAM等通用基础模型具有很强的泛化能力，但在医学域受域差、标注稀缺和PEFT无法充分利用未标注数据的限制，传统U-Net在半监督医学任务中表现优异，可作为专家协助通用模型适配。

Method: 提出specialist-generalist框架：U-Net作为专家生成点提示和伪标签以引导SAM（通过PEFT适配），同时SAM作为通用监督者正则化U-Net，构成双向协同训练循环以利用未标注数据。实验在两个医学分割数据集上进行比较，展示与其他半监督SAM变体及MedSAM的对比结果，代码开源。

Result: 在前列腺MRI和息肉分割任务上取得SOTA结果，优于其他半监督SAM变体与MedSAM，表明专家—通用模型协同能实现高标签效率的医学分割。

Conclusion: 本文提出SC-SAM，通过U-Net与SAM互为师生的协同训练，实现半监督医学图像分割时双向利用未标注数据，显著提升了在前列腺MRI和息肉分割基准上的性能，并超越了现有半监督SAM方法和专用医学基础模型。

Abstract: Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.

</details>


### [99] [DTC: A Deformable Transposed Convolution Module for Medical Image Segmentation](https://arxiv.org/abs/2601.17939)
*Chengkun Sun,Jinqian Pan,Renjie Liang,Zhengkang Fan,Xin Miao,Jiang Bian,Jie Xu*

Main category: cs.CV

TL;DR: 提出DTC：对转置卷积引入可变形采样位置，提升UNet类分割模型的上采样质量与细节恢复，适用于2D/3D医学图像。


<details>
  <summary>Details</summary>
Motivation: 传统上采样方法（转置卷积、线性插值）在固定位置采样，难以捕捉预定义采样点之外的结构信息，容易导致伪影或细节丢失。受到可变形卷积启发，提出学习采样位置以改善上采样质量。

Method: 在传统转置卷积的基础上引入可变形思想，学习采样偏移以生成动态采样位置，适用于2D和3D数据。将DTC替换或嵌入现有分割模型的上采样模块，进行端到端训练。

Result: 在多个数据集上验证，DTC在2D（ISIC18、BUSI）和3D（BTCV15）任务中均能提升分割性能，增强解码器对细节和结构的恢复能力。

Conclusion: 该论文提出了一种用于UNet类网络的可学习上采样方法——可变形转置卷积（DTC），通过学习动态采样坐标改善高分辨率特征图重建，从而提升解码器细节恢复能力。

Abstract: In medical image segmentation, particularly in UNet-like architectures, upsampling is primarily used to transform smaller feature maps into larger ones, enabling feature fusion between encoder and decoder features and supporting multi-scale prediction. Conventional upsampling methods, such as transposed convolution and linear interpolation, operate on fixed positions: transposed convolution applies kernel elements to predetermined pixel or voxel locations, while linear interpolation assigns values based on fixed coordinates in the original feature map. These fixed-position approaches may fail to capture structural information beyond predefined sampling positions and can lead to artifacts or loss of detail. Inspired by deformable convolutions, we propose a novel upsampling method, Deformable Transposed Convolution (DTC), which learns dynamic coordinates (i.e., sampling positions) to generate high-resolution feature maps for both 2D and 3D medical image segmentation tasks. Experiments on 3D (e.g., BTCV15) and 2D datasets (e.g., ISIC18, BUSI) demonstrate that DTC can be effectively integrated into existing medical image segmentation models, consistently improving the decoder's feature reconstruction and detail recovery capability.

</details>


### [100] [FlowMorph: Physics-Consistent Self-Supervision for Label-Free Single-Cell Mechanics in Microfluidic Videos](https://arxiv.org/abs/2601.17947)
*Bora Yimenicioglu,Vishal Manikanden*

Main category: cs.CV

TL;DR: FlowMorph是一个无监督且遵循流体—弹性物理的RBC轮廓跟踪与力学代理学习框架，能高保真重建细胞轮廓、改善面积与壁面守恒，并用单一标量k有效区分动力学模式与预测弹性模量，表现稳健。


<details>
  <summary>Details</summary>
Motivation: 动机是现有微流控RBC力学测量依赖有监督分割或人工特征，且很少将控制形状演化的层流Stokes流物理纳入模型；因此需设计一个无标签、可解释且符合物理规律的高通量变形性表征方法。

Method: 方法上，FlowMorph用低维参数化轮廓表示细胞，通过可微的“胶囊随流”模型推进边界点，该模型将层流平流和曲率正则化的弹性松弛结合；用仅由自动提取的轮廓和光流构建的损失（轮廓重叠、胞内流动一致性、面积守恒、壁面约束和时间平滑）进行自监督优化；无需监督分割或手工kymograph。

Result: 在四个公开数据集上，FlowMorph在含速度场的物理信息丰富视频上达成平均轮廓IoU 0.905，较纯数据驱动基线在面积守恒与壁面违例显著改进。在约1.5×10^5个居中序列上，标量k用于区分tank-treading与flipping动力学的AUC为0.863。仅用200个RT-DC事件标定出的单调映射E=g(k)在600个测试细胞上预测表观杨氏模量的平均绝对误差为0.118 MPa，并能在通道几何、光学和帧率变化下平滑退化。

Conclusion: FlowMorph提出了一种物理一致的自监督框架，能够从无标签的亮场微流控视频中为每个红细胞学习一个标量力学代理k，从而实现高通量、保留流体力学信息的变形性表征。

Abstract: Mechanical properties of red blood cells (RBCs) are promising biomarkers for hematologic and systemic disease, motivating microfluidic assays that probe deformability at throughputs of $10^3$--$10^6$ cells per experiment. However, existing pipelines rely on supervised segmentation or hand-crafted kymographs and rarely encode the laminar Stokes-flow physics that governs RBC shape evolution. We introduce FlowMorph, a physics-consistent self-supervised framework that learns a label-free scalar mechanics proxy $k$ for each tracked RBC from short brightfield microfluidic videos. FlowMorph models each cell by a low-dimensional parametric contour, advances boundary points through a differentiable ''capsule-in-flow'' combining laminar advection and curvature-regularized elastic relaxation, and optimizes a loss coupling silhouette overlap, intra-cellular flow agreement, area conservation, wall constraints, and temporal smoothness, using only automatically derived silhouettes and optical flow.
  Across four public RBC microfluidic datasets, FlowMorph achieves a mean silhouette IoU of $0.905$ on physics-rich videos with provided velocity fields and markedly improves area conservation and wall violations over purely data-driven baselines. On $\sim 1.5\times 10^5$ centered sequences, the scalar $k$ alone separates tank-treading from flipping dynamics with an AUC of $0.863$. Using only $200$ real-time deformability cytometry (RT-DC) events for calibration, a monotone map $E=g(k)$ predicts apparent Young's modulus with a mean absolute error of $0.118$\,MPa on $600$ held-out cells and degrades gracefully under shifts in channel geometry, optics, and frame rate.

</details>


### [101] [UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders](https://arxiv.org/abs/2601.17950)
*Matthew Walmer,Saksham Suri,Anirud Aggarwal,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: UPLiFT通过提出局部注意力的迭代上采样器（Local Attender），证明迭代方法可以在更低推理成本下达到或超过基于交叉注意力的像素级特征上采样性能，并在生成式任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有的任务无关特征上采样方法希望以远低于重训练backbone的成本，从低分辨率特征高效生成高分辨率特征。尽管近期方法转向基于交叉注意力的设计，但这些方法可能继承backbone的计算扩展问题。作者怀疑迭代方法仍有潜力并希望通过改进实现更高效的性能。

Method: 提出UPLiFT架构并设计了Local Attender算子——一种全局替代的局部注意力池化形式，使迭代上采样在计算和内存上更高效且保持特征稳定。通过实验比较UPLiFT与基于交叉注意力的上采样器和早期迭代方法，展示了其在推理成本和任务性能（包括生成式任务）上的优势。

Result: UPLiFT在保持或超过现有交叉注意力上采样器性能的同时，显著降低了推理成本；Local Attender使特征在上采样过程中稳定；在VAE特征上采样生成任务中，UPLiFT取得了与最先进的Coupled Flow Matching模型可比的结果。

Conclusion: UPLiFT证明了迭代上采样方法在效率和性能上均可与基于交叉注意力的方法竞争，并在某些情况下优于后者。通过引入Local Attender算子，UPLiFT在上采样过程中保持特征稳定性，实现了更低推理成本下的最先进性能，同时在生成式下游任务（如VAE特征上采样）中也表现出与Coupled Flow Matching模型相当的性能。总体而言，UPLiFT提供了一种通用且高效的像素级致密特征变换方法。

Abstract: The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features.

</details>


### [102] [Domain-Expert-Guided Hybrid Mixture-of-Experts for Medical AI: Integrating Data-Driven Learning with Clinical Priors](https://arxiv.org/abs/2601.17977)
*Jinchen Gu,Nan Zhao,Lei Qiu,Lu Zhang*

Main category: cs.CV

TL;DR: DKGH-MoE将数据驱动MoE与眼动引导MoE统一，通过门控融合实现性能与可解释性提升，适配小样本医学场景。


<details>
  <summary>Details</summary>
Motivation: 医学领域数据稀缺且难以捕获专家的诊断启发（如注视模式），将数据驱动学习与专家知识结合可弥补单一方法的不足，提升鲁棒性与临床相关性。

Method: 设计双分支MoE结构：一侧为数据驱动MoE从原始影像学习新颖特征，另一侧为领域专家引导MoE利用临床先验（如眼动注视热图）强化诊断相关区域；通过门控机制融合两类专家输出为下游预测，模块为可插拔且可解释。

Result: 在含眼动数据的医学影像任务上，DKGH-MoE在准确率和可解释性指标上均优于仅数据驱动的MoE或单一基线网络，且保持计算开销适中。

Conclusion: 提出的DKGH-MoE通过结合数据驱动和领域专家引导的专家子网络，提高了医学影像任务的性能和可解释性，适用于小样本领域并兼容多种主模型。

Abstract: Mixture-of-Experts (MoE) models increase representational capacity with modest computational cost, but their effectiveness in specialized domains such as medicine is limited by small datasets. In contrast, clinical practice offers rich expert knowledge, such as physician gaze patterns and diagnostic heuristics, that models cannot reliably learn from limited data. Combining data-driven experts, which capture novel patterns, with domain-expert-guided experts, which encode accumulated clinical insights, provides complementary strengths for robust and clinically meaningful learning. To this end, we propose Domain-Knowledge-Guided Hybrid MoE (DKGH-MoE), a plug-and-play and interpretable module that unifies data-driven learning with domain expertise. DKGH-MoE integrates a data-driven MoE to extract novel features from raw imaging data, and a domain-expert-guided MoE incorporates clinical priors, specifically clinician eye-gaze cues, to emphasize regions of high diagnostic relevance. By integrating domain expert insights with data-driven features, DKGH-MoE improves both performance and interpretability.

</details>


### [103] [MorphXAI: An Explainable Framework for Morphological Analysis of Parasites in Blood Smear Images](https://arxiv.org/abs/2601.18001)
*Aqsa Yousaf,Sint Sint Win,Megan Coffee,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: MorphXAI 是一个将形态学监督与寄生虫检测结合的可解释AI框架，使用临床注释数据集实现寄生虫定位与细粒度形态学属性预测，提升检测效果并提供对临床有用的解释。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽能自动检测寄生虫，但缺乏可解释性；可视化热图或注意力图无法反映临床诊断所依赖的具体形态学特征，因此需要一个能提供细粒度形态学解释的框架。

Method: MorphXAI 将形态学监督直接集成到检测模型中，输出包括位置信息和形态标签（如形状、曲率、可见点数、鞭毛存在与发育阶段），并使用临床注释的数据集进行训练与评估。

Result: 在包含三种寄生虫（利什曼、布氏锥虫与克氏锥虫）的临床注释数据集上，MorphXAI 不仅在检测任务上超越基线模型，还能输出结构化的形态学解释，证明其对临床可解释性与性能均有提升。

Conclusion: MorphXAI 提出了一种同时进行寄生虫检测与精细形态学分析的可解释框架，通过在预测流程中引入形态学监督，模型能够定位寄生虫并表征临床相关属性，从而改善检测性能并提供结构化、生物学有意义的解释。

Abstract: Parasitic infections remain a pressing global health challenge, particularly in low-resource settings where diagnosis still depends on labor-intensive manual inspection of blood smears and the availability of expert domain knowledge. While deep learning models have shown strong performance in automating parasite detection, their clinical usefulness is constrained by limited interpretability. Existing explainability methods are largely restricted to visual heatmaps or attention maps, which highlight regions of interest but fail to capture the morphological traits that clinicians rely on for diagnosis. In this work, we present MorphXAI, an explainable framework that unifies parasite detection with fine-grained morphological analysis. MorphXAI integrates morphological supervision directly into the prediction pipeline, enabling the model to localize parasites while simultaneously characterizing clinically relevant attributes such as shape, curvature, visible dot count, flagellum presence, and developmental stage. To support this task, we curate a clinician-annotated dataset of three parasite species (Leishmania, Trypanosoma brucei, and Trypanosoma cruzi) with detailed morphological labels, establishing a new benchmark for interpretable parasite analysis. Experimental results show that MorphXAI not only improves detection performance over the baseline but also provides structured, biologically meaningful explanations.

</details>


### [104] [Strip-Fusion: Spatiotemporal Fusion for Multispectral Pedestrian Detection](https://arxiv.org/abs/2601.18008)
*Asiegbu Miracle Kanu-Asiegbu,Nitin Jotwani,Xiaoxiao Du*

Main category: cs.CV

TL;DR: 提出了Strip-Fusion，一种针对多光谱（可见光+热成像）行人检测的时空融合网络，兼顾对齐鲁棒性、光照变化与遮挡问题；引入时间自适应卷积与KL散度模态不平衡损失，并提出后处理算法；在KAIST和CVC-14基准上取得竞争性或更好结果，尤其在重遮挡和图像未对齐情形显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱行人检测大多侧重空间融合、忽略时间信息；且RGB与热图像可能存在未对齐问题，行人检测受光照变化与遮挡影响严重。需要一种同时利用时序信息并对模态不对齐鲁棒的方法。

Method: 提出Strip-Fusion：1) 时空融合网络结构，使用时间自适应卷积（temporally adaptive convolutions）动态加权时空特征以捕捉运动与上下文；2) 设计基于KL散度的新损失以缓解可见光与热模态间的不平衡，训练中引导特征向更有信息的模态靠拢；3) 开发新的后处理算法以减少误检。网络对输入未对齐具鲁棒性并能处理恶劣光照与遮挡。

Result: 在KAIST和CVC-14数据集上进行大量实验，结果显示与现有方法相比具有竞争力。在重遮挡与模态未对齐的挑战性情形下，取得了显著改进；并通过消融验证了时间自适应卷积、KL损失和后处理的有效性。

Conclusion: Strip-Fusion通过引入时序信息与模态不平衡的显式约束，在多光谱行人检测中提升了鲁棒性和性能，尤其在未对齐与严苛场景下表现更好，值得在实际机器人感知应用中推广。

Abstract: Pedestrian detection is a critical task in robot perception. Multispectral modalities (visible light and thermal) can boost pedestrian detection performance by providing complementary visual information. Several gaps remain with multispectral pedestrian detection methods. First, existing approaches primarily focus on spatial fusion and often neglect temporal information. Second, RGB and thermal image pairs in multispectral benchmarks may not always be perfectly aligned. Pedestrians are also challenging to detect due to varying lighting conditions, occlusion, etc. This work proposes Strip-Fusion, a spatial-temporal fusion network that is robust to misalignment in input images, as well as varying lighting conditions and heavy occlusions. The Strip-Fusion pipeline integrates temporally adaptive convolutions to dynamically weigh spatial-temporal features, enabling our model to better capture pedestrian motion and context over time. A novel Kullback-Leibler divergence loss was designed to mitigate modality imbalance between visible and thermal inputs, guiding feature alignment toward the more informative modality during training. Furthermore, a novel post-processing algorithm was developed to reduce false positives. Extensive experimental results show that our method performs competitively for both the KAIST and the CVC-14 benchmarks. We also observed significant improvements compared to previous state-of-the-art on challenging conditions such as heavy occlusion and misalignment.

</details>


### [105] [Leveraging Persistence Image to Enhance Robustness and Performance in Curvilinear Structure Segmentation](https://arxiv.org/abs/2601.18045)
*Zhuangzhi Gao,Feixiang Zhou,He Zhao,Xiuju Chen,Xiaoxin Li,Qinkai Yu,Yitian Zhao,Alena Shantsila,Gregory Y. H. Lip,Eduard Shantsila,Yalin Zheng*

Main category: cs.CV

TL;DR: 提出可学习的持久性图像回归模块并将其在网络结构中融合，从而在无需重度手工拓扑损失的情况下提升曲线类医学图像分割的拓扑一致性与鲁棒性，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 传统通过基于PD的拓扑度量难以直接嵌入端到端训练，且现有多依赖手工设计的拓扑损失，泛化性差且计算开销大；因此希望获得可微且计算友好的拓扑表示并将其融入模型结构以提升分割的拓扑一致性与鲁棒性。

Method: 设计了PIs-Regressor用于学习PI表示，Topology SegNet在下采样和上采样阶段融合所学拓扑特征，整个框架替换或补充传统的基于手工拓扑损失的方法，将拓扑特征作为可训练模块嵌入网络。

Result: 在三个曲线结构分割基准上，本文方法在像素级准确性与拓扑保真度上达到或超过现有最优，能更好应对过曝、模糊等医学图像难例，且设计可与其他拓扑方法结合进一步提升性能。

Conclusion: 该论文提出了将持久性图（PD）的可微分表示——持久性图像（PI）——直接从数据中学习的模块（PIs-Regressor），并在网络架构中融合拓扑特征（Topology SegNet），从而将拓扑信息作为网络的一部分而非辅助损失，以提高曲线状结构分割的鲁棒性与拓扑一致性。

Abstract: Segmenting curvilinear structures in medical images is essential for analyzing morphological patterns in clinical applications. Integrating topological properties, such as connectivity, improves segmentation accuracy and consistency. However, extracting and embedding such properties - especially from Persistence Diagrams (PD) - is challenging due to their non-differentiability and computational cost. Existing approaches mostly encode topology through handcrafted loss functions, which generalize poorly across tasks. In this paper, we propose PIs-Regressor, a simple yet effective module that learns persistence image (PI) - finite, differentiable representations of topological features - directly from data. Together with Topology SegNet, which fuses these features in both downsampling and upsampling stages, our framework integrates topology into the network architecture itself rather than auxiliary losses. Unlike existing methods that depend heavily on handcrafted loss functions, our approach directly incorporates topological information into the network structure, leading to more robust segmentation. Our design is flexible and can be seamlessly combined with other topology-based methods to further enhance segmentation performance. Experimental results show that integrating topological features enhances model robustness, effectively handling challenges like overexposure and blurring in medical imaging. Our approach on three curvilinear benchmarks demonstrate state-of-the-art performance in both pixel-level accuracy and topological fidelity.

</details>


### [106] [Semi-Supervised Hyperspectral Image Classification with Edge-Aware Superpixel Label Propagation and Adaptive Pseudo-Labeling](https://arxiv.org/abs/2601.18049)
*Yunfei Qiu,Qiqiong Ma,Tianhua Lv,Li Fang,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 本文提出EASLP+ DREPL（含DHP和ATSC）的半监督HSI分类框架，通过边缘感知的超像素标签传播与历史融合伪标签策略，提升了伪标签稳定性与边界鲁棒性，在四个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动机是解决半监督HSI分类中标注样本稀少导致的边界标签扩散和伪标签不稳定问题，通过引入空间边缘先验和时间域历史信息来提升伪标签的可靠性与分类鲁棒性。

Method: 主要方法包括：1) Edge-Aware Superpixel Label Propagation (EASLP)：在超像素标签传播中加入边缘强度惩罚和邻域纠错策略，减少边界区域的标签渗流并增强鲁棒性；2) Dynamic History-Fused Prediction (DHP)：维护历史预测并与当前预测按动态权重融合，以平滑伪标签波动，提升时间一致性；3) Adaptive Tripartite Sample Categorization (ATSC)：基于置信度和一致性将样本划分为易、模糊和难三类，分层利用以提升伪标签质量和学习效率；4) Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL)：由DHP和ATSC组成，实现时间域和样本域的伪标签稳定性强化，并与EASLP协同优化时空一致性。

Result: 在四个基准数据集上的评估表明，所提出的方法在保持或提升分类精度的同时，能更稳健地处理边界与噪声样本，展示了更好的时空一致性和泛化能力。

Conclusion: 该论文提出了一个结合空间先验和动态学习机制的半监督高光谱图像分类框架，以缓解超像素分割造成的标签扩散和伪标签不稳定性问题。通过时空一致性优化，方法在基准数据集上取得了稳健的分类性能提升。

Abstract: Significant progress has been made in semi-supervised hyperspectral image (HSI) classification regarding feature extraction and classification performance. However, due to high annotation costs and limited sample availability, semi-supervised learning still faces challenges such as boundary label diffusion and pseudo-label instability. To address these issues, this paper proposes a novel semi-supervised hyperspectral classification framework integrating spatial prior information with a dynamic learning mechanism. First, we design an Edge-Aware Superpixel Label Propagation (EASLP) module. By integrating edge intensity penalty with neighborhood correction strategy, it mitigates label diffusion from superpixel segmentation while enhancing classification robustness in boundary regions. Second, we introduce a Dynamic History-Fused Prediction (DHP) method. By maintaining historical predictions and dynamically weighting them with current results, DHP smoothens pseudo-label fluctuations and improves temporal consistency and noise resistance. Concurrently, incorporating condifence and consistency measures, the Adaptive Tripartite Sample Categorization (ATSC) strategy implements hierarchical utilization of easy, ambiguous, and hard samples, leading to enhanced pseudo-label quality and learning efficiency. The Dynamic Reliability-Enhanced Pseudo-Label Framework (DREPL), composed of DHP and ATSC, strengthens pseudo-label stability across temporal and sample domains. Through synergizes operation with EASLP, it achieves spatio-temporal consistency optimization. Evaluations on four benchmark datasets demonstrate its capability to maintain superior classification performance.

</details>


### [107] [Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification](https://arxiv.org/abs/2601.18088)
*Jianshu Chao,Tianhua Lv,Qiqiong Ma,Yunfei Qiu,Li Fang,Huifang Shen,Wei Yao*

Main category: cs.CV

TL;DR: 提出S2Former与频域约束的自监督预训练，结合DAFT蒸馏微调，实现无需源标签、少样本下的高光谱跨域迁移，实验显示效果稳健。


<details>
  <summary>Details</summary>
Motivation: 在跨域高光谱表示学习中，缺乏无需源域标注且能在目标域少量样本下高效适应的方法；现有方法受分布偏移影响，泛化能力差。

Method: 提出自监督跨域迁移框架：预训练阶段引入S2Former模块（双分支空间-光谱Transformer，双向交叉注意力，空间分支通过随机掩码增强结构感知，光谱分支捕捉细粒度差异），并加入频域约束（rFFT与高频幅值损失）保持频域一致性；微调阶段提出DAFT蒸馏机制（教师-学生结构，对齐语义演进轨迹）实现低标注条件下的鲁棒迁移。

Result: 在四个高光谱数据集上验证，方法在分类性能上表现稳定，并在跨域适应性上优于对比方法，尤其在资源受限（少标注）条件下表现突出。

Conclusion: 所提自监督跨域迁移框架能在无源域标签和少量目标样本条件下学习可迁移的光谱-空间联合表示，兼顾细节识别与结构一致性，提升跨域泛化能力。

Abstract: Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.

</details>


### [108] [Text-Pass Filter: An Efficient Scene Text Detector](https://arxiv.org/abs/2601.18098)
*Chuang Yang,Haozhao Ma,Xu Han,Yuan Yuan,Qi Wang*

Main category: cs.CV

TL;DR: 提出TPF：模拟带通滤波器为每个文本构建特征-滤波器对，直接分割整段文本，能分离粘连文本并适配实时检测；引入REU增强特征一致性和FPU强化前景先验，提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于收缩-扩展的检测方法在收缩过程中丢失文本边缘视觉特征并使前景与背景差异混淆，限制了文本特征识别能力，需一种能直接分割整段文本并自然分离粘连文本的方法。

Method: 提出Text-Pass Filter (TPF)：通过模拟带通滤波器概念，为每个文本构建唯一的特征-滤波器对，滤波器在推理时通过“通过”匹配文本的特征并“阻断”其他特征，从而独立提取每个文本。为增强对长宽比大的带状文本的识别，设计Reinforcement Ensemble Unit (REU)以提升同一文本特征一致性并扩大感受野；引入Foreground Prior Unit (FPU)帮助区分前景与背景，提升特征-滤波器配对质量。

Result: 实验表明REU和FPU有效，TPF在任意形状文本检测上优于现有收缩-扩展策略，能自然分离粘连文本并适合实时应用。

Conclusion: 本文提出了Text-Pass Filter (TPF)用于任意形状文本检测，通过模拟带通滤波器为每个文本构建对应的特征-滤波器对，实现对整段文本的直接分割，从而避免了收缩操作带来的边缘信息丢失和前景背景混淆问题。TPF还能自然分离粘连文本，适合实时检测。为了解决长宽比大的带状文本识别完整性问题，设计了强化集成单元（REU）以增强相同文本的特征一致性并扩大滤波器的识别范围；同时引入前景先验单元（FPU）以提升前背景区分，改善特征-滤波器对质量。实验验证了REU和FPU的有效性并展示了TPF优越性。

Abstract: To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.

</details>


### [109] [Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs](https://arxiv.org/abs/2601.18099)
*Akbar Saadat*

Main category: cs.CV

TL;DR: 提出无需训练的解析—离散化前向框架，通过邻域相似性从多解中选择单解，实现实时高精度高斯模糊估计，MAE<1.7%、强度误差<2%。


<details>
  <summary>Details</summary>
Motivation: 提出一个零训练（zero training）前向计算框架，用于在实时应用中实现高斯模糊模型，从更清晰图像计算并估计较模糊图像的高斯核参数。

Method: 基于模糊图像的解析表达式离散化，从清晰图像生成模糊图像候选并选择最佳匹配；在解析表达式导致多解的像素位置利用邻域相似性度量筛选为单一解；可处理给定两幅图像互为部分模糊的情形。

Result: 在真实图像上实验，合成模糊值估计的平均绝对误差低于1.7%；将提取的散焦滤波器应用到较少模糊的图像时，实际模糊图像强度与估计值的差异小于2%。

Conclusion: 所提出的零训练前向计算框架能在无需训练的条件下实时估计和匹配高斯模糊参数，且在合成与真实图像测试中表现出高准确性，适用于两幅图像互为部分模糊的情况。

Abstract: Following the earlier verification for Gaussian model in \cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\%$, obtained by applying the extracted defocus filters to less blurred images.

</details>


### [110] [Spatial-Conditioned Reasoning in Long-Egocentric Videos](https://arxiv.org/abs/2601.18100)
*James Tribble,Hao Wang,Si-En Hong,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 作者构建了Sanpo-D数据集，评估多种VLM在长时程第一人称视频的空间问题上表现，发现将深度与RGB融合可提升行人与障碍物检测等安全关键任务，但可能牺牲部分通用准确性。


<details>
  <summary>Details</summary>
Motivation: 长时程第一人称视频中视角漂移与缺乏持续几何上下文，导致VLM在空间推理上表现有限；研究者希望在不改动模型架构的情况下，探讨输入级显式空间信号（如深度）对VLM视频理解的影响。

Method: 1) 对Google Sanpo数据集进行细粒度重标注，得到Sanpo-D；2) 构建导航导向的空间查询集；3) 在多种VLM上进行基线评估；4) 将深度图与RGB帧在输入层融合，比较有无深度时的性能差异。

Result: 实验表明：深度感知与空间对齐的表示在安全关键任务（如行人检测、障碍物识别）上显著提升；同时也观察到模型在通用任务准确率与空间专业化之间存在权衡。

Conclusion: 该论文表明在不改模型结构与推理流程的前提下，引入显式空间信号（如深度）能在某些导航相关任务上提升视觉-语言模型（VLM）的空间推理能力，但也存在与通用性能的权衡。

Abstract: Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.

</details>


### [111] [LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment](https://arxiv.org/abs/2601.18118)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出LungCRCT：通过图自编码器、距离相关性解耦和熵驱动重建学习潜在因果表示，既能做因果干预分析又在LDCT恶性肿瘤分类上达到93.91% AUC，且下游模型轻量高效。


<details>
  <summary>Details</summary>
Motivation: 传统CNN/ViT模型虽然在影像分类上表现优秀，但缺乏可解释的因果结构和干预分析能力，限制了向治疗分析或因果模拟扩展的能力；因此需要在影像表示中挖掘物理因果机制的潜在因子。

Method: 构建图自编码器进行因果图结构学习，利用距离相关性（distance correlation）进行因果因子解耦，并结合熵指标驱动的图像重建精炼以优化潜变量表达。下游使用轻量分类器在潜在因果表示上进行恶性肿瘤分类。

Result: 在公开/私有LDCT任务上，LungCRCT在恶性肿瘤分类达到了AUC=93.91%，同时实现了因果干预分析能力并生成更轻量的下游模型。

Conclusion: LungCRCT提出了一种基于潜在因果表示学习的肺癌分析框架，通过图自编码器的因果发现、距离相关性解耦和熵驱动的图像重建精炼，实现了因果干预分析能力和高效肿瘤分类性能。

Abstract: Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.

</details>


### [112] [Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection](https://arxiv.org/abs/2601.18135)
*Jiahao Lyu,Minghua Zhao,Xuewen Huang,Yifei Chen,Shuangli Du,Jing Hu,Cheng Shi,Zhiyong Lv*

Main category: cs.CV

TL;DR: FoGA: 2M-param Unet-based VAD using gated skip fusion and forward consistency loss for immediate+forward predictions, achieving SOTA accuracy and 155 FPS on edge-like settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing VAD methods for deployment on resource-limited edge devices and improve anomaly detection by leveraging longer-term temporal information instead of single-frame prediction only.

Method: Propose FoGA: a lightweight (~2M params) Unet-based model that extracts features from consecutive frames to produce immediate and forward predictions; introduce a gated context aggregation module in skip connections to fuse encoder and decoder features dynamically; jointly optimize with a forward consistency loss and use a hybrid anomaly measurement integrating errors from immediate and forward predictions.

Result: FoGA substantially outperforms state-of-the-art competing methods, runs up to 155 FPS, and shows effectiveness through extensive experiments.

Conclusion: FoGA offers an excellent trade-off between detection performance and efficiency, making it suitable for edge deployment for real-time VAD by combining lightweight architecture, gated feature fusion, and forward consistency learning.

Abstract: As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.

</details>


### [113] [Agentic Very Long Video Understanding](https://arxiv.org/abs/2601.18157)
*Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim*

Main category: cs.CV

TL;DR: EGAgent用实体场景图和多模态检索工具实现对长时段第一视角视频的跨模态、多跳推理，显著提升纵向视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 全天候可穿戴设备带来持续、长期的第一视角视频数据，现有模型上下文窗口有限且缺乏多跳组合推理能力，难以完成跨天的纵向理解任务。

Method: 构建实体场景图以表示随时间演化的对象、人物与场景关系；设计带有规划能力的代理，配备场景图搜索、视觉与音频混合检索工具以支持多跳推理。

Result: 在EgoLifeQA上取得SOTA（57.5%），在Video-MME（Long）上表现竞争（74.1%）。

Conclusion: 提出了基于实体场景图的代理框架EGAgent，通过结构化搜索与跨模态检索改进长期视角视频理解。

Abstract: The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.

</details>


### [114] [TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration](https://arxiv.org/abs/2601.18168)
*Zehua Liu,Shihao Zou,Jincai Huang,Yanfang Zhang,Chao Tong,Weixin Si*

Main category: cs.CV

TL;DR: 该文提出SA-PnP与基于时间的扩散细化（TempDiffReg）的粗到细2D-3D血管配准方法，在临床数据上显著优于现有方法，能增强TACE手术导航的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: TACE手术中血管导航复杂、解剖变异显著，精确稳健的2D-3D血管配准对导管定位和靶向治疗至关重要；现有方法在精度或解剖合理性上存在不足，因此需要一种能处理时间序列信息和局部形变的配准策略。

Method: 提出了两阶段方法：1) 全局对齐模块SA-PnP，用于在二维血管影像与三维血管结构间建立对应关系；2) 局部细化模块TempDiffReg，一种利用时间上下文的扩散模型，迭代执行血管形变以捕捉复杂的解剖变化和局部结构细节。

Result: 在23例患者、626对多帧样本的评估中，方法在MSE（0.63 mm）和MAE（0.51 mm）上分别比最优现有方法降低66.7%和17.7%，在精度与解剖合理性上均优于SOTA，具有辅助临床手术的潜力。

Conclusion: 所提方法通过粗到细的配准策略在TACE术中实现了更精确且解剖上合理的2D-3D血管配准，能够降低注册误差并提升临床可用性。

Abstract: Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\% lower MSE and 17.7\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}

</details>


### [115] [YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection](https://arxiv.org/abs/2601.18172)
*Lin Huang,Yujuan Tan,Weisheng Li,Shitai Shan,Liu Liu,Bo Liu,Linlin Shen,Jing Yu,Yue Niu*

Main category: cs.CV

TL;DR: 提出YOLO-DS，通过双统计协同算子(DSO)建模通道均值与峰均差，配合轻量门控模块DSG与MSG，提高YOLO在MS-COCO上的AP 1.1%~1.7%。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO缺乏对共享通道内异构目标响应的显式建模，限制性能提升。作者希望通过显式分解与选择增强判别能力。

Method: 提出DSO同时建模通道均值和峰值-均值差，解耦目标特征；基于DSO设计两种门控：DSG用于自适应通道选择，MSG用于深度方向加权。模块轻量，插入YOLO骨干或颈部以提升特征表示。

Result: 在MS-COCO上五个规模(N,S,M,L,X)均超越YOLOv8，AP提升1.1%~1.7%，推理延迟仅微增。通过可视化、消融和对比实验证明方法有效。

Conclusion: YOLO-DS通过双统计特征解耦与轻量门控，有效提升对异构目标的区分能力，在精度与效率间取得更好平衡，适合在实际检测场景中替代YOLOv8。

Abstract: One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.

</details>


### [116] [\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation](https://arxiv.org/abs/2601.18188)
*Weiye Zhu,Zekai Zhang,Xiangchen Wang,Hewei Pan,Teng Wang,Tiantian Geng,Rongtao Xu,Feng Zheng*

Main category: cs.CV

TL;DR: NaVIDA通过学习动作—视觉因果关系和分层动作分块并在推断时自适应控制执行长度，提升了VLN在准确性、稳健性和实操性上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLN方法主要依赖反应式状态-动作映射，未显式建模动作如何因果地改变后续视觉观测，导致无法预测动作引起的视觉变化，从而出现行为不稳定、泛化差和轨迹误差累积。

Method: 提出NaVIDA框架：在策略学习中加入基于动作的视觉动力学和自适应执行。训练时用基于片段的逆动力学监督学习视觉变化与动作的因果关系；引入分层概率动作分块（HPAC）将轨迹组织为多步片段，提供判别性和更长范围的视觉变化提示；推断时用熵引导机制自适应地设定动作片段执行时长以抑制误差累积并稳定行为。

Result: 在多个基准上，NaVIDA在参数更少（3B vs 8B）的情况下优于最先进方法；并通过真实机器人评测验证了方法的可行性与有效性。

Conclusion: 通过将逆动力学监督与分层动作分块及熵驱动自适应执行相结合，NaVIDA能显著提升VLN代理的稳健性和泛化能力，减少轨迹误差累积，且具备实际部署潜力。

Abstract: Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.

</details>


### [117] [Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval](https://arxiv.org/abs/2601.18190)
*Yifan Li,Shiying Wang,Jianqiang Huang*

Main category: cs.CV

TL;DR: 提出MPS-CLIP：LLM提取关键词+SamGeo生成子视角，G^2A适配器实现参数高效全局信息建模，MPR聚合多视角表示，混合损失训练，在RSITR任务上显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有VLP方法在遥感影像文本检索中多依赖粗粒度全局对齐，忽视遥感图像中丰富的多尺度局部语义；且全量微调计算开销大且易遗忘。

Method: 利用LLM提取语义关键词，借助SamGeo生成语义子视角；引入Gated Global Attention (G^2A)适配器以低成本捕获全局上下文；通过Multi-Perspective Representation (MPR)聚合局部提示；使用多视角对比损失与加权三元组损失的混合目标进行训练。

Result: 在RSICD和RSITMD数据集上分别取得35.18%和48.40%的mR，显著优于全量微调基线和近期方法。

Conclusion: MPS-CLIP通过关键字引导的多视角细粒度对齐和参数高效适配在遥感图像文本检索上取得了显著提升，证明了在冻结主干的情况下引入局部语义视角与轻量化适配器是有效的。

Abstract: Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.

</details>


### [118] [MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models](https://arxiv.org/abs/2601.18192)
*Tian-Yi Zhou,Xuan-Hao Liu,Bao-Liang Lu,Wei-Long Zheng*

Main category: cs.CV

TL;DR: MindCine通过多模态联合学习和借助预训练大规模EEG模型，在有限数据上实现了更好的EEG到视频重建。


<details>
  <summary>Details</summary>
Motivation: 解决EEG到视频重建中仅对齐文本模态导致的信息缺失/过拟合和数据稀缺导致的训练难以收敛问题。

Method: 在训练阶段引入多模态联合学习（超越文本），利用预训练大规模EEG模型用于解码语义信息，并设计带因果注意力的Seq2Seq模型用于解码感知信息。

Result: 提出一种名为MindCine的框架，实现从脑电（EEG）信号到视频的高保真重建，针对现有方法的单一模态（仅对齐文本）和数据稀缺问题提出解决方案。

Conclusion: 多模态联合学习（不仅限于文本）和利用预训练大规模EEG模型结合Seq2Seq带因果注意力的设计，可在有限数据下显著提升EEG到视频的重建质量，优于现有最先进方法。

Abstract: Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.

</details>


### [119] [QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding](https://arxiv.org/abs/2601.18195)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Kaiwei Zhang,Jun Jia,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: QualiRAG: a training-free RAG framework that decomposes queries to generate four auxiliary knowledge sources for evidence-grounded visual quality understanding, improving LMM performance without finetuning.


<details>
  <summary>Details</summary>
Motivation: This paper aims to enable interpretable visual quality assessment without task-specific training by leveraging latent knowledge in large multimodal models, avoiding annotated datasets and finetuning.

Method: Propose QualiRAG, a training-free retrieval-augmented-generation framework that dynamically generates auxiliary knowledge via question decomposition into four knowledge sources (visual metadata, subject localization, global quality summaries, local quality descriptions) and performs relevance-aware retrieval for reasoning.

Result: QualiRAG substantially improves over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks and is competitive on visual quality comparison tasks, demonstrating robust assessment without training.

Conclusion: QualiRAG shows that dynamic auxiliary knowledge generation combined with LMMs enables interpretable, training-free visual quality assessment, reducing annotation needs and dataset biases.

Abstract: Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \textit{fine-grained spatiotemporal perception} and \textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \textbf{QualiRAG}, a \textit{training-free} \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration \textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \textit{visual metadata}, \textit{subject localization}, \textit{global quality summaries}, and \textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.

</details>


### [120] [HomoFM: Deep Homography Estimation with Flow Matching](https://arxiv.org/abs/2601.18222)
*Mengfan He,Liangzheng Sun,Chunyu Li,Ziyang Meng*

Main category: cs.CV

TL;DR: HomoFM用流匹配学习点级速度场来求单应性，并用梯度反转层增强域泛化，提升了精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将单应性估计视为直接回归或迭代细化，难以捕获复杂几何变换及跨域泛化；为此引入生成建模中的流匹配技术，以更加连续、可控的方式建模变换流程，同时通过域适应提升稳健性。

Method: 将单应性估计问题转换为速度场学习问题，构建条件流轨迹来逐步变换点坐标；在特征提取骨干中集成梯度反转层实现域不变表示；使用神经网络学习点级速度场并通过流推理获得最终单应性变换。

Result: 在多项标准基准上，HomoFM在估计精度和鲁棒性方面优于现有最先进方法。公开了代码和数据资源。

Conclusion: 该工作提出将流匹配（flow matching）方法引入单应性估计，通过学习连续点位速度场，将噪声分布推向配准坐标，从而实现高精度变换恢复，并通过梯度反转层实现域自适应，提高鲁棒性。

Abstract: Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.

</details>


### [121] [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228)
*Sahil Naik,Soham Bagayatkar,Pavankumar Singh*

Main category: cs.CV

TL;DR: 提出一个基于EfficientNetB2的轻量FER方案，通过多项训练技巧解决噪声标注与类不平衡，达68.78%测试准确率，参数远少于VGG16，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，面部表情识别面临低质量图像、光照与姿态变化、背景干扰、小类间差异、标注噪声与类别严重不平衡等挑战，而大规模CNN虽然准确但计算与内存开销大，不利于实时或边缘应用，因此需要轻量高效且稳健的模型。

Method: 使用EfficientNetB2骨干，先进行warm-up阶段再微调；采用AdamW优化器和解耦权重衰减；标签平滑(eps=0.06)处理噪声标注；对类不平衡采用剪切类权重；加入dropout、混合精度训练及丰富的实时数据增强；训练时采用分层的87.5%/12.5%训练-验证划分，保留官方测试集评估。

Result: 在FER-2013（48x48灰度图）上测试准确率为68.78%，训练稳定，泛化能力强；模型参数显著减少（约为VGG16的1/10），并提供了每类指标和学习动态分析，证明适于实时和边缘场景。

Conclusion: 该工作提出了一个基于EfficientNetB2的轻量级面部表情识别（FER）管道，通过两阶段预热与微调训练策略以及一系列训练技巧（AdamW、解耦权重衰减、标签平滑、裁剪类权重、dropout、混合精度训练和实时数据增强）在FER-2013数据集上获得了68.78%的测试准确率，参数量比VGG16基线少近10倍，适合实时和边缘部署。

Abstract: Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.

</details>


### [122] [V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering](https://arxiv.org/abs/2601.18240)
*Mengyuan Jin,Zehui Liao,Yong Xia*

Main category: cs.CV

TL;DR: 提出V-Loop：通过从主问答对提取语义单元、基于答案生成验证问题并强制视觉注意力一致性，构建视觉逻辑回路，无需训练即可高效检测医学VQA中的幻觉，实验表明优于现有内省方法并能与不确定性方法互补。


<details>
  <summary>Details</summary>
Motivation: 现有基于不确定性的内省方法间接估计预测不确定性，无法直接验证特定答案是否与图像事实一致，存在在医学高风险场景下的幻觉风险。因此需要一种直接、可解释且无需额外训练的幻觉检测方法。

Method: V-Loop 的流程包括：1) MLLM 对图像-问题产生初始答案；2) 从主QA对中提取语义单元（问题单元与答案单元）；3) 基于答案单元生成验证问题以重新询问问题单元（反向查询）；4) 强制两次回答在视觉注意力上保持一致以保证基于相同图像证据；5) 若验证答案与预期语义一致则闭环，否则判为幻觉。该方法不需要额外训练且可与不确定性方法组合。

Result: 在多个医学VQA基准和不同 MLLMs 上的大量实验表明，V-Loop 在幻觉检测性能上持续超越现有内省方法，效率高并能进一步提升不确定性方法的表现。

Conclusion: V-Loop 是一个训练-free、即插即用的医学VQA幻觉检测框架，通过构建双向推理的视觉逻辑回路来验证答案的事实性，依靠重查询和视觉注意力一致性判断答案是否被视觉证据支持，从而有效检测幻觉并提升不确定性方法性能。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.

</details>


### [123] [Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation](https://arxiv.org/abs/2601.18242)
*Zerui Kang,Yishen Lim,Zhouyou Gu,Seung-Woo Ko,Tony Q. S. Quek,Jihong Park*

Main category: cs.CV

TL;DR: 利用VLM提供语义材料先验和信息性放置策略，可显著加速并提升可微光追中的多材料射频参数估计，达到高准确率和测量效率。


<details>
  <summary>Details</summary>
Motivation: 在6G电磁数字孪生中，精确的射频材料参数对于仿真和预测至关重要，但现有基于梯度的逆RT对初始化敏感且在测量受限时代价高昂。

Method: 通过VLM从场景图像解析材料类别，并映射到ITU-R材料表得到电导率先验；VLM还选择信息量大的发/收发器布局；以这些先验初始化，可微RT中基于梯度的细化利用接收信号强度测量进行参数估计。

Result: 在NVIDIA Sionna的室内场景实验中，与均匀/随机初始化或随机放置基线相比，收敛速度提升2-4×，最终参数误差降低10-100×，在仅几个接收器下实现低于0.1%的平均相对误差；复杂度分析显示每次迭代时间随材料数量和测量设置近线性扩展，VLM指引的布局减少了所需测量量，消融实验证明增加RT深度和射线数进一步提高精度且无明显每次迭代开销。

Conclusion: VLM指引的语义先验能显著加速并稳定基于可微光线追踪的多材料射频参数估计，在受限测量下仍能达到高精度恢复。

Abstract: Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.

</details>


### [124] [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250)
*Kang Yu,Dingyu Wang,Zimu Yuan,Nan Zhou,Jiajun Liu,Jiaxin Liu,Shanggui Liu,Yaoyan Zheng,Huishu Yuan,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: OrthoFoundation通过在120万膝部X光/MRI上自监督预训练，显著提升骨科影像AI的性能、标注效率与跨解剖泛化，推动通用肌骨基础模型发展。


<details>
  <summary>Details</summary>
Motivation: 当前骨科AI方法依赖大量有标签数据、任务专用且缺乏跨模态/跨部位泛化；缺乏大规模公开肌骨数据集限制了基础模型发展。

Method: 收集了120万张膝关节X光与MRI无标签图像，采用Dinov3骨干网络，并通过自监督对比学习预训练以学习稳健的放射学表征。

Result: 在14个下游任务中实现SOTA，X光骨关节炎诊断准确率优异，MRI结构损伤检测排名第一；在仅用50%标注数据时即可匹配有监督基线，并展现出跨髋、肩、踝的出色泛化能力。

Conclusion: OrthoFoundation显著提升了肌肉骨骼影像的泛化能力与标注效率，证明了自监督多模态基础模型在骨科影像学中的可行性与优势。

Abstract: Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.

</details>


### [125] [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252)
*Chao Wang,Xuanying Li,Cheng Dai,Jinglei Feng,Yuxiang Luo,Yuqi Ouyang,Hao Qin*

Main category: cs.CV

TL;DR: Co-PLNet passes early detections through a Point-Line Prompt Encoder to create spatial prompts, then uses a Cross-Guidance Line Decoder with sparse attention to refine predictions, improving accuracy, robustness, and speed on wireframe parsing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Separate prediction of lines and junctions leads to mismatches; a collaborative approach using spatial prompts can enforce consistency and improve performance

Method: Point-Line collaborative network with PLP-Encoder and CGL-Decoder

Result: Improved accuracy and robustness in junction and line detection on Wireframe and YorkUrban; favorable real-time efficiency

Conclusion: Exchanging spatial cues via spatial prompts and cross-guidance attention enforces consistency and efficiency, outperforming separate prediction baselines

Abstract: Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.

</details>


### [126] [Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images](https://arxiv.org/abs/2601.18260)
*Eytan Kats,Kai Geissler,Daniel Mensing,Jochen G. Hirsch,Stefan Heldman,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: 本文用合成的MRI-深度图数据训练卷积网络，从单张体表深度图预测多器官三维位置与形状，实现了无需表面重建的自动病人定位，展示了在放射学自动化定位中的潜力。


<details>
  <summary>Details</summary>
Motivation: 提高扫描流程效率与患者流转率，通过自动化病人定位减少人为干预和错误。利用RGB-D相机获取体表深度信息，推断内部器官位置以指导定位。

Method: 提出一个基于学习的端到端框架，输入单幅2D体表深度图，使用统一卷积神经网络直接预测多器官的三维位置与形状；用大规模全身MRI数据合成深度图与相应的解剖分割作为训练对；无需显式重建表面。

Result: 在合成数据上对骨骼和软组织等多种解剖结构实现了准确定位；实验显示该方法可从单视深度图恢复内部器官的空间分布，证明了深度传感器在放射学工作流中用于自动化定位的可行性。

Conclusion: 通过将RGB-D相机与学习模型结合，可在无需复杂建模的情况下实现对内部器官的有效估计，具有简化扫描流程、提高患者体验和工作效率的潜力。

Abstract: Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.

</details>


### [127] [Revisiting Aerial Scene Classification on the AID Benchmark](https://arxiv.org/abs/2601.18263)
*Subhajeet Das,Susmita Ghosh,Abhiroop Chatterjee*

Main category: cs.CV

TL;DR: 本文综述航拍图像分类方法并提出Aerial-Y-Net：一种结合空间注意力与多尺度特征融合的CNN，在AID数据集上取得91.72%准确率，表现优于若干基线。


<details>
  <summary>Details</summary>
Motivation: 航拍图像场景具有异质性（建筑、森林、山地、空地等），使得鲁棒分类模型的开发具有挑战性；需要引入 attention 与多尺度融合以更好地捕捉复杂场景特征。

Method: 论文回顾了从手工特征（如SIFT、LBP）到传统CNN（如VGG、GoogLeNet）及深度混合网络的多种方法；在此基础上提出Aerial-Y-Net，结合空间注意力模块与多尺度特征融合机制；在AID数据集上进行训练与评估，并与基线架构比较性能。

Result: Aerial-Y-Net在AID数据集上达到91.72%准确率，优于若干基线架构（文中未列出具体基线数值），表明空间注意力与多尺度融合有助于提升航拍图像分类性能。

Conclusion: 本文提出并评估了一种名为Aerial-Y-Net的空间注意力增强、多尺度特征融合的卷积神经网络，用于提高航拍图像场景分类性能，并在AID数据集上取得了91.72%的准确率，优于若干基线模型。

Abstract: Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.

</details>


### [128] [Contextual Range-View Projection for 3D LiDAR Point Clouds](https://arxiv.org/abs/2601.18301)
*Seyedali Mousavi,Seyedhamidreza Mousavi,Masoud Daneshtalab*

Main category: cs.CV

TL;DR: 通过将实例中心距离和类别权重融入range-view投影的点选择规则，CAP和CWAP分别提高了实例保留率和目标类别性能，从而提升了点云语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统的depth-first选择策略在range-view投影中简单但忽视语义和物体结构，导致边界噪声和背景点被保留而重要的实例内部点丢失，影响分割性能。

Method: 提出两种机制：Centerness-Aware Projection (CAP)：根据点到实例中心的距离调整深度，使靠近中心的点更容易被保留；Class-Weighted-Aware Projection (CWAP)：基于用户设定的类别权重对不同类别点优先级进行调整。两者都是在投影前修改深度值以影响像素选择。

Result: 在SemanticKITTI上评估表明，CAP在保留实例点方面效果更好，最高可带来3.1% mIoU提升；CWAP可提升目标类别的性能且对其他类别影响很小。

Conclusion: 本文通过在range-view投影中引入上下文信息（实例中心和类别权重），改进了传统基于深度的点选策略，从而缓解多对一冲突导致的信息丢失问题。

Abstract: Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \textit{Centerness-Aware Projection (CAP)} and \textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes

</details>


### [129] [SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis](https://arxiv.org/abs/2601.18305)
*Xuan Wang,Siyuan Su,Quantong Fu,Yongxiang Hu,Yangfan Zhou*

Main category: cs.CV

TL;DR: 通过SwipeGen合成以多维度建模人类滑动，并据此训练GUISwiper及首个滑动评估基准，显著提升GUI代理的滑动执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理在滑动交互上采用过于简化的策略，导致无法准确复制人类行为，成为任务完成的瓶颈，需要更精细的滑动建模与数据支持来提升执行能力。

Method: 提出自动合成管线SwipeGen，通过在GUI环境中的探索生成符合多维度人类滑动特性的交互数据；基于这些合成数据训练并设计GUISwiper代理以改进滑动执行策略；构建并发布首个滑动执行能力评估基准用于量化比较。

Result: GUISwiper在新构建的基准上达到69.07%的滑动执行准确率，相较于现有视觉语言模型基线提高214%。

Conclusion: 本文提出通过分解人类滑动手势的多个维度并合成数据来提升GUI代理的滑动执行能力，最终构建基准并提出GUISwiper，显著提高滑动执行准确率。

Abstract: With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.

</details>


### [130] [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330)
*Muhammad Ali Shah,Muhammad Mansoor Alam,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出了一种高效的Densely Swin Hybrid (EDSH) 框架，用于脑肿瘤MRI分析，结合DenseNet和Swin Transformer分别学习局部纹理和全局上下文，在两个肿瘤感知实验设置（BFS与DFE/DR）下提升分类与检测性能，报告在40260张图像的数据集上达到98.50%的准确率与召回率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉细粒度纹理与长程上下文信息，致使对不同类型脑肿瘤（弥散性浸润性胶质瘤与边界清晰的脑膜瘤/垂体瘤）诊断敏感性不足。作者旨在构建一个能够同时学习局部和全局表征的混合模型以改善诊断性能。

Method: 提出EDSH框架，包括兩種实验设置：1) Boosted Feature Space (BFS)：独立定制的DenseNet与Swint分支学习互补的局部与全局表征，维度对齐后融合并增强，用于提高对弥散性胶质瘤的检测；2) 分层DenseNet-Swint架构（DFE与DR）：DenseNet作为干CNN干（stem）提取局部结构特征，Swint通过任务对齐的patch embedding和shifted-window self-attention建模全局形态，两者通过双残差连接（DR）和深层特征提取（DFE）抑制脑膜瘤和垂体瘤的假阴性。模型对DenseNet与Swint都做了针对MRI特性的定制。

Result: 在严格划分的40260张MRI图像（四类肿瘤）测试集上，EDSH优于单独的CNN、ViT与混合模型，达成98.50%的准确率与召回率。

Conclusion: 通过在架构设计上同时兼顾细粒度纹理与长程上下文，EDSH在脑肿瘤分类任务上显著提升性能，尤其在处理不同病理形态（弥散性与局限性肿瘤）时更具鲁棒性。

Abstract: This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.

</details>


### [131] [PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction](https://arxiv.org/abs/2601.18336)
*Isaac Deutsch,Nicolas Moënne-Loccoz,Gavriel State,Zan Gojcic*

Main category: cs.CV

TL;DR: 本文提出PPISP模块和对应控制器，模拟真实相机的自动曝光和自动白平衡，通过物理可解释的参数预测新视点的ISP，改善多视角重建的光度不一致性问题并达到SoTA。


<details>
  <summary>Details</summary>
Motivation: 多视角3D重建对光度不一致性（来自镜头特性和ISP差异）敏感，现有方法（每帧潜变量或仿射颜色校正）缺乏物理依据，无法在新视点上泛化，影响评价公平性和重建质量。

Method: 提出PPISP模块，基于物理可解释的变换将相机内在（如光学畸变、传感器响应）与拍摄相关（如曝光、白平衡、伽马校正）效应解耦。设计PPISP控制器，在输入视图上训练以预测新视点的ISP参数，类似相机的自动曝光/白平衡；允许使用元数据辅助。

Result: 在标准多视角重建基准上，PPISP在定量评估中达到了最先进（SoTA）性能，同时提供直观可控的参数和在无真实图像时对新视点进行合理ISP预测的能力。源代码开源。

Conclusion: PPISP通过物理可解释的ISP变换对相机内在与拍摄相关效应进行解耦，提升了多视角三维重建在视图一致性任务的泛化能力，并在标准基准上实现了最先进性能。

Abstract: Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp

</details>


### [132] [Beyond Rigid: Benchmarking Non-Rigid Video Editing](https://arxiv.org/abs/2601.18340)
*Bingzheng Qu,Kehai Chen,Xuefeng Bai,Jun Yu,Min Zhang*

Main category: cs.CV

TL;DR: 提出NRVBench数据集与NRVE-Acc评估指标，并设计训练自由的VM-Edit方法，解决非刚性视频编辑中的物理一致性和时序稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动视频编辑在处理非刚性变形时常出现物理畸变和时间闪烁，缺乏专门评测和方法来衡量与保持物理一致性与时序一致性。

Method: 构建包含6类物理基准的视频数据集（180段），配合2340条细粒度指令和360道多选题；提出基于视觉语言模型的NRVE-Acc评估指标；提出无训练基线VM-Edit，采用双区域去噪机制实现结构感知控制。

Result: 实验表明现有方法在物理合理性上仍有不足，NRVE-Acc能更好捕捉复杂动态下的表现，VM-Edit在保持结构与动态变形之间取得平衡并在多项度量上表现优异。

Conclusion: 该论文提出了针对非刚性视频编辑的首个专门基准NRVBench，并提出评价指标和基线方法，有助于推动物理感知的视频编辑研究。

Abstract: Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.

</details>


### [133] [Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception](https://arxiv.org/abs/2601.18346)
*Sijing Wu,Yunhao Li,Zicheng Zhang,Qi Jia,Xinyue Li,Huiyu Duan,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出Q-Bench-Portrait：首个面向人像图像质量感知的基准（2765条三元组），评估20开源+5闭源MLLMs，发现现有模型对人像质量感知能力有限，与人类仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在低级视觉基准上表现出色，但主要针对通用图像，人像图像具有独特结构与感知属性，亟需专门基准来评估模型在人像质量感知与评估方面的能力。

Method: 构建了包含2765个图像-问-答三元组的数据集，设计了全局与局部两层次的题目，包含单选、多选、判断与开放式问题，并评估了20个开源与5个闭源MLLMs的性能。

Result: 实验结果表明现有模型在部分人像感知任务上有一定能力但整体表现有限且不够精确，与人类判断存在明显差距，提示需要进一步专门化或改进模型。

Conclusion: 该论文提出了首个面向人像图像质量感知的全面性基准Q-Bench-Portrait，覆盖多样图像来源、全面质量维度和多种题型，填补了现有多模态大模型在人像域评估的空白。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.

</details>


### [134] [OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI](https://arxiv.org/abs/2601.18368)
*Caterina Fuster-Barceló,Claudia Castrillón,Laura Rodrigo-Muñoz,Victor Manuel Vega-Suárez,Nicolás Pérez-Fernández,Gorka Bastarrika,Arrate Muñoz-Barrutia*

Main category: cs.CV

TL;DR: OREHAS is an automatic MRI pipeline that accurately measures inner ear ELR from standard sequences using minimal annotated data, outperforming clinical software and enabling consistent volumetric EH assessment.


<details>
  <summary>Details</summary>
Motivation: Quantify endolymphatic hydrops (EH) volumetrically from standard MRI automatically to remove manual intervention and operator dependence.

Method: OREHAS pipeline combining slice classification, inner ear localization, and sequence-specific segmentation trained with few annotated slices per patient to compute per-ear ELR from whole MRI volumes.

Result: Achieved Dice 0.90 (SPACE-MRC) and 0.75 (REAL-IR); external validation VSI 74.3% vs syngo.via 42.5%; vestibular measures consistent while endolymphatic volumes smaller and more realistic across 19 patients.

Conclusion: OREHAS enables reliable, reproducible EH quantification from standard MRI with limited supervision, reducing operator dependence and supporting large-scale studies and recalibration of diagnostic thresholds.

Abstract: We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.
  Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.
  These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.

</details>


### [135] [Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues](https://arxiv.org/abs/2601.18372)
*Christos Petrou,Harris Partaourides,Athanasios Balomenos,Yannis Kopsinis,Sotirios Chatzis*

Main category: cs.CV

TL;DR: Combine UniSal saliency and HMD motion; fuse features; use TSMixer/LSTM to predict gaze; outperforms simple baselines on EHTask and real hardware, enabling better VR interaction without eye trackers.


<details>
  <summary>Details</summary>
Motivation: Gaze prediction is important in VR to reduce latency and enable foveated rendering, but direct eye tracking is unavailable due to hardware/privacy limits. Combining HMD motion and visual saliency can predict gaze without eye trackers.

Method: Use UniSal lightweight saliency encoder to extract visual features from video frames; fuse with HMD motion data; process through time-series prediction module. Evaluate TSMixer and LSTM architectures. Forecast future gaze directions.

Result: On EHTask dataset and commercial VR hardware deployment, approach outperforms baselines (Center-of-HMD, Mean Gaze) consistently.

Conclusion: Predictive gaze modeling using HMD motion and visual saliency effectively reduces perceptual lag and improves interaction in VR without eye tracking.

Abstract: Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.

</details>


### [136] [Estimation of geometric transformation matrices using grid-shaped pilot signals](https://arxiv.org/abs/2601.18385)
*Rinka Kawano,Masaki Kawamura*

Main category: cs.CV

TL;DR: 本文通过在图像中嵌入横纵编码不同的网格先导信号，利用Radon变换分析网格失真来估计几何变换，实现裁剪下的同步与水印恢复，模拟实验显示准确性高。


<details>
  <summary>Details</summary>
Motivation: 针对现有水印在几何失真尤其是裁剪情况下难以同步的问题，提出一种能估计几何变换以恢复嵌入位置的方法，提升对裁剪等攻击的鲁棒性。

Method: 在原图中嵌入格状先导信号，横、纵格线赋予不同编码。对被攻击（缩放、旋转、错切、裁剪等）的图像，先对图像做Radon变换以检测格线的角度和间隔，通过分析这些失真参数并结合横纵差异确定格子方向，反推出变换矩阵以实现同步。

Result: 仿真结果表明，所提方法在单一及复合几何攻击下均能以较低误差准确估计变换矩阵，从而实现裁剪后同步与水印提取。

Conclusion: 本文提出利用网格状先导信号和Radon变换估计被裁剪后的嵌入图像的几何变换矩阵，从而实现同步与水印提取，实验表明对各类几何攻击（各向异性缩放、旋转、错切、裁剪及组合攻击）均能低误差估计变换矩阵。

Abstract: Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.

</details>


### [137] [ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks](https://arxiv.org/abs/2601.18386)
*Gabriel Lee Jun Rong,Christos Korgialas,Dion Jia Xu Ho,Pai Chet Ng,Xiaoxiao Miao,Konstantinos N. Plataniotis*

Main category: cs.CV

TL;DR: 提出ARMOR：一个用VLM指导的多智能体系统，协调CW、JSMA和STA攻击原语，LLM实时调参和融合输出，提升跨模型迁移与攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有自动化攻击套件往往为静态的固定序列，缺乏语义感知与实时策略调整能力，导致对不同图像和目标模型的泛化和成功率有限。需要一种能够在语义层面自适应地组合不同对抗原语的机制。

Method: ARMOR构建多智能体体系：三个并行攻击代理分别负责CW、JSMA和STA。由VLM评估图像语义与感知影响，LLM在闭环中实时调节每个代理的参数并决定融合策略。代理通过共享“Mixing Desk”交换中间扰动并生成混合输出；对于白盒目标，系统通过置信度与SSIM评分选择最佳单一或混合攻击。

Result: 在标准基准上，ARMOR在跨架构迁移攻击上表现更好，能在盲测时输出混合扰动欺骗目标，在白盒场景下基于置信度与SSIM可靠地选择最有效攻击或其融合，整体提高了攻击成功率与感知质量。

Conclusion: ARMOR通过将CW、JSMA、STA三种对抗原语在VLM引导的多智能体框架下编排，并利用LLM进行实时自适应重参数化，实现了在语义水平上针对图像特定脆弱性的攻击优化，从而提升了跨架构迁移性和攻击成功率。

Abstract: Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.

</details>


### [138] [Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space](https://arxiv.org/abs/2601.18392)
*Moritz Rempe,Lukas T. Rotkopf,Marco Schlimbach,Helmut Becker,Fabian Hörst,Johannes Haubold,Philipp Dammann,Kevin Kröninger,Jens Kleesiek*

Main category: cs.CV

TL;DR: kViT: complex-valued Vision Transformer for direct k-space MRI classification with radial patching; matches image-domain performance, much lower VRAM (up to 68x), robust to high acceleration.


<details>
  <summary>Details</summary>
Motivation: Existing DL MRI uses magnitude images, losing phase and needing expensive reconstructions; standard CNNs are local and unsuitable for k-space global nature. Need model to operate directly on raw k-space, preserving phase and efficiency.

Method: Propose kViT: a complex-valued Vision Transformer operating on k-space. Introduce radial k-space patching respecting spectral energy distribution. Train and evaluate on fastMRI and in-house datasets, compare to image-domain baselines.

Result: kViT achieves classification performance competitive with ResNet, EfficientNet, ViT. Shows superior robustness to high acceleration factors and reduces VRAM consumption during training up to 68x versus standard methods.

Conclusion: Direct k-space complex-valued transformer with radial patching enables resource-efficient, phase-preserving MRI classification, offering robustness to undersampling and substantial computational savings, paving way for on-scanner AI.

Abstract: Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.

</details>


### [139] [Larger than memory image processing](https://arxiv.org/abs/2601.18407)
*Jon Sporring,David Stansby*

Main category: cs.CV

TL;DR: 提出一种面向超大体积图像的切片流式处理框架和DSL，利用扫掠执行、窗口化与重叠感知分块在有限内存下实现近线性磁盘扫描，显著降低I/O开销并优化吞吐。


<details>
  <summary>Details</summary>
Motivation: 面向拍字节级和百TB级图像数据，传统全量内存加载不可行且性能受I/O限制。作者主张通过流式切片处理减少重复磁盘访问，提升极大数据集上的图像分析可行性与效率。

Method: 将分析结构化为基于切片的多通道流式扫描，形式化为扫掠执行、窗口化操作与重叠感知切片（overlap-aware tiling）。引入DSL以在编译/运行时分析管道，自动选择窗口大小、阶段融合、流的分叉合并（tee/zip）、以及在受限RAM机器上的通道调度，从而保证顺序读写、最小化冗余访问并保持可预测内存占用。

Result: 理论与工程实现上表明，相比基于3D块的访问模式，切片流式架构在邻域运算下能显著减少磁盘重复访问（避免对每个3D块多次访问），并在分割与形态学预/后处理场景中实现显著吞吐提升和近线性I/O扫描。

Conclusion: 该论文提出了一套面向千兆字节至拍字节规模图像（如1.4 PB电子显微镜体数据和150 TB人体器官图谱）的流式分块分析方法，强调I/O为性能瓶颈，通过以切片为单位的流式处理、窗口化与重叠感知切片策略以及扫掠（sweep）执行模型，最小化磁盘访问并实现近线性I/O扫描，提供DSL自动分析与调度以在内存受限环境中获得高吞吐和可预测内存占用。

Abstract: This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.

</details>


### [140] [Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings](https://arxiv.org/abs/2601.18414)
*Aura Loredana Dan*

Main category: cs.CV

TL;DR: 比较MobileNet、EfficientNet与VGG16在儿童绘画情感分类上的表现：EfficientNet在精度与效率间取得最佳平衡，MobileNet最适合移动部署，VGG16表示能力强但计算代价高；迁移学习对小样本有效，但需关注标注噪声与类不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 动机是：早期识别自闭谱系儿童的情绪状态有助于及早干预，但传统评估方法主观、侵入性或难以规模化。儿童绘画作为一种非侵入、自然的表达方式，提供了潜在的情感线索；通过比较不同轻量及深层卷积网络，旨在为移动端或实时情感识别系统选择合适模型并探索实用的研究方向。

Method: 方法包括：收集儿童绘画数据集，由心理学专家进行情感标签标注；采用迁移学习微调三种预训练卷积神经网络（MobileNet、EfficientNet、VGG16）；在统一实验设置下比较性能指标（准确率、精确率、召回率、F1、混淆矩阵）、鲁棒性测试（噪声、遮挡、风格变换）及计算效率（参数量、推理时间、内存占用）；使用交叉验证与统计显著性检验确保结果可靠。

Result: 结果显示：1) EfficientNet在多数情感类别上取得最高或接近最高的F1分数，且参数量和延迟优于VGG16；2) MobileNet在延迟和模型大小上最优，适合嵌入式/移动部署，但在细粒度情感判别上略逊于EfficientNet；3) VGG16在部分复杂样式或高噪声样本上表现稳健但推理成本高；4) 所有模型对标注不一致和少数类表现存在困难，需更多数据增强和类不平衡处理；5) 迁移学习显著提升小数据集上的性能。

Conclusion: 该论文结论为：在儿童绘画情感识别任务中，不同深度学习架构在准确率、鲁棒性与计算效率之间存在显著权衡。MobileNet在移动与实时场景下具备较低的计算成本与可接受的性能；EfficientNet在精度与效率之间表现最佳；VGG16虽能取得高表示能力但计算开销大，不利于资源受限环境。总体上，基于迁移学习的方案对小样本标注数据有效，但仍需针对数据偏差与标注噪声进行改进。

Abstract: Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.

</details>


### [141] [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448)
*Lloyd Austin Courtenay*

Main category: cs.CV

TL;DR: 本文研究了GPA在形态测量学中作为预处理对机器学习模型的污染效应，提出了一个将测试样本对齐到训练集的新方法以消除统计依赖，并通过2D/3D模拟及解析推导揭示了样本量与坐标数之间的RMSE标度规律，强调空间自相关在预测中的重要性。


<details>
  <summary>Details</summary>
Motivation: 标准流程先对所有样本做GPA再划分训练/测试集，会引入跨样本统计依赖，污染机器学习评估。需定量描述和修正这一偏差并给出实用预处理建议。

Method: 使用受控2D和3D模拟，改变样本量、地标密度和全ometr y模式，比较常规GPA与提出的“将测试样本对齐到训练集”方法在不同回归模型（线性与卷积）下的表现；对Procrustes切向空间的自由度进行解析推导以解释RMSE标度。

Result: 模拟显示存在一条稳健的“对角线”关系（样本量 vs 地标空间）反映在各向同性变异下RMSE的标度，斜率可由Procrustes自由度解析得到；忽略地标空间自相关会显著降低线性与卷积回归模型的性能；提出的实时对齐方法能消除GPA引入的依赖性，改善模型评估可靠性。

Conclusion: 在GMM与机器学习结合时需谨慎预处理，推荐在训练-测试分割后将测试样本对齐至训练集以避免数据污染，并注意Procrustes形状空间的统计约束及地标之间的空间自相关对模型性能的影响。

Abstract: Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust "diagonal" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.

</details>


### [142] [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451)
*Xuanmeng Sha,Liyun Zhang,Tomohiro Mashita,Naoya Chiba,Yuki Uranishi*

Main category: cs.CV

TL;DR: 提出3DGesPolicy，通过将整体姿势生成视为连续轨迹控制问题，用扩散策略学习帧间整体动作变化，并通过GAP模块融合语音与音素，实现语义一致且空间稳定的全身协同共语动作生成，在BEAT2数据集上效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有分段或逐帧回归方法导致全身动作语义不连贯和空间上无意义抖动，需要一种能学习帧间整体运动模式并约束运动轨迹真实性的生成方法，同时需更精细地对齐语音语义与身体与面部表情。

Method: 提出3DGesPolicy：把帧到帧的变化建模为统一的整体动作，将生成问题视为连续轨迹控制并采用来自机器人学的扩散策略（diffusion policy）进行学习，保证运动沿现实运动流形连续；同时设计Gesture-Audio-Phoneme (GAP) 融合模块，深度整合语音、音素和手势信号，提升语义和时序对齐。

Result: 在BEAT2数据集上，定量与定性实验表明3DGesPolicy在自然度、表达性和与语音的对齐性方面均优于多种最新方法，生成的全身-面部协同动作更平滑一致且语义相关性更高。

Conclusion: 将共语动作生成重构为基于动作的连续轨迹控制并结合GAP多模态融合，可以显著改善全身协同姿势的连贯性和语义一致性，为更自然的虚拟人和人机交互提供新的方法路径。

Abstract: Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.

</details>


### [143] [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464)
*Wenbin Wei,Suyuan Yao,Cheng Huang,Xiangyu Gao*

Main category: cs.CV

TL;DR: 提出一种面向筛查到随访闭环的多模态青光眼AI系统，兼顾性能、稳健性与公平性，实验显示在预测准确率、减少种族漏诊差异和提前风险预警方面均表现出色。


<details>
  <summary>Details</summary>
Motivation: 早期诊断和长期随访对预防青光眼导致的不可逆失明至关重要，但现有筛查依赖单一检测或松散关联的检查，主观性和碎片化护理严重，且在真实世界中影像设备和专家资源有限，导致一致性和公平性不足。

Method: 提出Fair-Eye Net，一种多模态AI系统，采用双流异构融合结构，整合眼底照片、OCT结构指标、视野功能指标和人口统计特征；引入不确定性感知的分层门控策略用于选择性预测和安全转诊，同时在训练中加入公平性约束以减少弱势群体的漏诊；通过多任务学习保证临床可靠性。

Result: 在实验中，系统达成AUC 0.912（特异性96.7%），减少种族间漏诊差异73.4%（从12.31%降至3.28%），保持稳定的跨域性能，并能实现提前3–12个月的风险预警（灵敏度92%，特异性88%）。

Conclusion: Fair-Eye Net将公平性作为主要优化目标，并通过多任务学习提升临床可靠性，提供了可再现的路径以实现临床转化和大规模部署，有望推动全球眼健康公平性提升。

Abstract: Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.

</details>


### [144] [DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment](https://arxiv.org/abs/2601.18493)
*Sara Tehrani,Yonghao Xu,Leif Haglund,Amanda Berg,Michael Felsberg*

Main category: cs.CV

TL;DR: DisasterInsight: 112K building-centric multimodal benchmark for disaster imagery tasks; DI-Chat: LoRA-fine-tuned VLM improves damage/disaster classification and report generation, but building-function classification is still difficult.


<details>
  <summary>Details</summary>
Motivation: Current remote sensing VLM benchmarks use coarse labels and image-level tasks, not reflecting functional understanding and instruction robustness needed in real humanitarian workflows; thus a more realistic, instruction-diverse benchmark is needed.

Method: Restructure xBD into ~112K building-centered instances; design instruction-diverse tasks (function, damage, disaster-type, counting, structured reports); fine-tune existing VLM backbones with LoRA on disaster-specific instructions to create DI-Chat; evaluate multiple SOTA generic and remote-sensing VLMs across tasks to measure gaps and improvements.

Result: This paper introduces DisasterInsight, a multimodal benchmark for disaster analysis using satellite imagery restructured from xBD into ~112K building-centered instances with instruction-diverse evaluation across tasks (building-function classification, damage-level and disaster-type classification, counting, structured report generation). It also proposes DI-Chat, a domain-adapted VLM fine-tuned with LoRA, and reports experiments showing remaining gaps, especially in damage understanding and report generation; DI-Chat improves classification and report quality but building-function classification remains hard.

Conclusion: DisasterInsight fills a gap by providing a unified, instruction-rich benchmark for grounded multimodal reasoning in disaster imagery; DI-Chat demonstrates domain adaptation benefits but highlights persistent challenges, guiding future research.

Abstract: Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.

</details>


### [145] [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532)
*Devon Levy,Bar Assayag,Laura Gaspar,Ilan Shimshoni,Bella Specktor-Fadida*

Main category: cs.CV

TL;DR: 结合foundation-model嵌入的自动聚类cold-start加上不确定性+空间多样性的主动学习，在低数据下能显著提升分割性能并具有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 手工标注分割代价高且耗时，主动学习可减少标注量，但传统冷启动多采用随机或简单多样性采样，难以兼顾代表性与多样性。利用foundation-model的强大表征能力结合聚类，可更高效地构建代表性初始集，进而提升后续AL效率。

Method: 方法包括两阶段：1) Cold-start阶段：使用预训练基础模型提取图像特征，基于这些嵌入进行自动聚类（自动确定簇数），按簇比例抽样以构建初始训练集；2) 主动学习阶段：基于不确定性度量（如熵）对候选样本排序，同时引入空间多样性约束以避免选择高度冗余样本。可视化嵌入以增强可解释性。

Result: 在三个数据集（CheXmask/X-ray、Montgomery/X-ray、SynthStrip/MRI）上评估：cold-start相较随机显著提升Dice并降低Hausdorff距离；在CheXmask上cold-start Dice从0.918提升到0.929，Hausdorff从32.41降到27.66mm；AL结合熵与多样性在CheXmask上将Dice提升至0.939并将Hausdorff降至19.16mm；Montgomery上cold-start Dice由0.928提升到0.950，Hausdorff由14.22降至9.38mm；SynthStrip上cold-start对Dice影响小但降低Hausdorff，AL使Dice从0.816增至0.826并将Hausdorff从7.76降至6.38mm。总体在低数据情形下优于基线。

Conclusion: 提出的方法通过结合foundation-model嵌入与聚类用于cold-start采样，并在后续的主动学习中融合不确定性与空间多样性，引导样本选择，从而在低数据场景下稳定提升分割性能。

Abstract: Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.

</details>


### [146] [GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning](https://arxiv.org/abs/2601.18543)
*Kaixun Jiang,Yuzheng Wang,Junjie Zhou,Pandeng Li,Zhihang Liu,Chen-Wei Xie,Zhaoyu Chen,Yun Zheng,Wenqiang Zhang*

Main category: cs.CV

TL;DR: GenAgent treats generators as invokable tools and uses agentic multimodal reasoning plus supervised fine-tuning and RL to boost image generation quality and generalization.


<details>
  <summary>Details</summary>
Motivation: Unify visual understanding and generation while avoiding expensive training and trade-offs between understanding and generation.

Method: Agentic framework decoupling understanding (handled by multimodal model) and generation (invoking image generators as tools); two-stage training: supervised fine-tuning on tool invocation and reflection data, then agentic RL with pointwise and pairwise rewards and trajectory resampling.

Result: Significant improvements on GenEval++ (+23.6%) and WISE (+14%) for base generator FLUX.1-dev; shows cross-tool generalization, test-time scaling, and task-adaptive reasoning.

Conclusion: GenAgent enables autonomous multi-turn agentic interactions to iteratively refine generated images, improving generation quality and adaptability without retraining generators.

Abstract: We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\%) and WISE (+14\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \href{https://github.com/deep-kaixun/GenAgent}{this url}.

</details>


### [147] [REMAC: Reference-Based Martian Asymmetrical Image Compression](https://arxiv.org/abs/2601.18547)
*Qing Ding,Mai Xu,Shengxi Li,Xin Deng,Xin Zou*

Main category: cs.CV

TL;DR: 为适配火星影像压缩需求，REMAC 把繁重计算留给地面解码端，利用参考图像和多尺度深解码器及特征回收，显著降低编码端复杂度并提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有学习型图像压缩方法未考虑火星上极度受限的计算资源，且未利用火星影像间的强互相似性。作者基于纹理、颜色及语义层面的实证分析提出改进方法。

Method: REMAC 包括参考引导的熵模型、ref-decoder（深层多尺度架构以扩大感受野）以及潜变量特征回收机制。编码端简化操作以适应火星受限计算资源，解码端利用参考图像和更大感受野建模长程空间依赖。

Result: 在实验中，REMAC 将编码器复杂度比最先进方法降低约43.51%，同时在 BD-PSNR 上提升约0.2664 dB。

Conclusion: 该文提出了一种面向火星影像的参考式非对称图像压缩方法（REMAC），通过将计算复杂度从编码端转移到资源丰富的解码端，并利用影像之间的强互相似性提升压缩性能。

Abstract: To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \textit{intra-} and \textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.

</details>


### [148] [Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray](https://arxiv.org/abs/2601.18555)
*Roberto Di Via,Vito Paolo Pastore,Francesca Odone,Siôn Glyn-Jones,Irina Voiculescu*

Main category: cs.CV

TL;DR: 在89例配对MRI/X光的前瞻性验证中，基于热图回归的关键点检测在MRI上实现了与X光等效的cam型FAI定位与诊断精度，支持将自动化FAI评估并入常规MRI流程。


<details>
  <summary>Details</summary>
Motivation: 评估能否在MRI上复现X光片中用于髋关节撞击(FAI)筛查的角度测量，以实现跨模态临床等效，从而在MRI中进行3D体积评估并融入常规流程。

Method: 使用配对队列验证研究(89例患者，配对MRI与X光)，采用标准热图回归(heatmap regression)网络进行关键点/标志点检测，在MRI冠状面切片上定位用于cam型撞击的关键点并与X光结果对比，评估定位与诊断精度。

Result: 在配对数据上，MRI的关键点检测与X光具有等效的定位精度和诊断性能，证明在冠状位3D MRI卷上可实现临床可行的FAI评估。

Conclusion: MRI可替代或补充X光用于cam型FAI的角度测量与定位，支持将自动化FAI评估整合进常规MRI工作流，并为后续的体积分析与更多标志点放置奠定基础。

Abstract: Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions

</details>


### [149] [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 提出SDA-QEC：轻量扩散生成用于少数类补样，量子特征层嵌入MobileNetV2提升判别，冠状动脉造影分类上显著优于传统CNNs，达到高准确率和均衡敏感/特异性


<details>
  <summary>Details</summary>
Motivation: 解决医疗影像中严重类别不平衡导致模型偏向多数类、少数类召回率低和临床误诊风险，提升少样本高风险场景下的诊断可靠性

Method: Analyze methodology: lightweight diffusion augmentor + quantum feature layer in MobileNetV2; training on coronary angiography images with class imbalance; evaluation vs classical CNNs

Result: SDA-QEC yields 98.33% accuracy, 98.78% AUC, 98.33% F1, sensitivity and specificity both 98.33%

Conclusion: Integration of generative augmentation and quantum-enhanced feature mapping effectively addresses class imbalance in small-sample medical imaging, improving balanced diagnostic performance and offering promising path for clinical AI systems

Abstract: In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.

</details>


### [150] [AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging](https://arxiv.org/abs/2601.18560)
*Li Fang,Tianyu Li,Yanghong Lin,Shudong Zhou,Wei Yao*

Main category: cs.CV

TL;DR: 提出一种轻量级非深度少样本像素级标签传播方法：通过秩约束聚类选择锚点，构建锚-像素亲和并生成top-k稀疏图，基于闭式解进行两阶段传播，适用于卫星边缘计算下受损/错位/噪声高的高光谱影像。


<details>
  <summary>Details</summary>
Motivation: 缓解卫星下行带宽瓶颈与卫星平台计算资源受限，支持在轨快速决策；同时应对传感器故障与扫描误差导致的像元退化，减少对空间结构和深度模型的依赖。

Method: Lightweight, non-deep few-shot pixel-wise label propagation with rank-constrained anchor clustering

Result: Efficient two-stage pixel-wise label propagation: stage1 anchor-pixel affinity propagation; stage2 closed-form solution on top-k pruned sparse graph; rank-constrained graph clustering for anchor selection. Validated for degraded/misaligned/mixed-noise hyperspectral images onboard satellites; reduces need for spatial structure and deep models, suitable for resource-limited satellite edge computing.

Conclusion: The method enables autonomous, lightweight hyperspectral classification onboard satellites under resource and data degradation constraints, achieving efficient, robust labeling using spectral-only features and closed-form propagation for low computation.

Abstract: As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.

</details>


### [151] [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 无额外训练或外部验证器的自我精炼视频采样，通过将预训练生成器作为去噪自编码器并引入不确定性感知的选择性迭代精炼，显著提升视频的运动一致性与物理真实感。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成器在复杂物理动力学和细粒度运动上表现欠佳，且现有依赖外部验证器或数据增强的解决方案计算成本高且效果有限，因而希望在不增加训练成本的情况下提升生成视频的物理真实感和运动一致性。

Method: 将预训练视频生成器视为去噪自动编码器，在推理阶段进行多步迭代的自我精炼（inner-loop refinement）；引入基于自一致性的的不确定性度量，在高不确定区域执行更多精炼，低不确定区域减少操作；无需额外训练或外部评价器，保持模型权重不变。

Result: 在多个先进视频生成器上实验证明，该方法在运动连贯性和物理对齐上显著提升，相较默认采样器和基于引导的采样器，人类偏好超过70%。

Conclusion: 本文提出了一种在推理时无外部验证器或额外训练的自我精炼视频采样方法，通过将预训练视频生成器解释为去噪自编码器，进行迭代内环精炼，并结合不确定性感知的选择性精炼策略以避免过度精炼伪影，从而改善运动一致性和物理对齐。

Abstract: Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.

</details>


### [152] [GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization](https://arxiv.org/abs/2601.18585)
*Chenxi Liu,Selena Ling,Alec Jacobson*

Main category: cs.CV

TL;DR: GimmBO uses a two-stage Preferential Bayesian Optimization to efficiently explore high-dimensional adapter-merge weight spaces for diffusion image generation, outperforming baselines in studies.


<details>
  <summary>Details</summary>
Motivation: Customize diffusion-based image generation by merging fine-tuned adapters to create diverse visuals; manual slider tuning is inefficient for exploring large adapter weight spaces.

Method: Propose GimmBO: an interactive system using Preferential Bayesian Optimization with a two-stage BO backend that accounts for sparsity and constrained weight ranges for efficient sampling and convergence in high-dimensional merge weight spaces.

Result: Evaluations with simulated users and a user study show improved convergence, high success rates, and consistent gains over standard BO and line-search baselines.

Conclusion: GimmBO enables scalable, efficient, and flexible exploration of adapter merges for image generation, addressing limitations of manual tuning and improving user outcomes.

Abstract: Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.

</details>


### [153] [AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment](https://arxiv.org/abs/2601.18589)
*KV Karthikeya,Ashok Kumar Das,Shantanu Pal,Vivekananda Bhat K,Arun Sekar Rajasekaran*

Main category: cs.CV

TL;DR: 提出AGSP-DSA框架，通过双图构建、谱图滤波和多尺度GCN进行跨模态关系建模与节点嵌入，结合语义注意力实现动态模态对齐。在CMU-MOSEI、AVE和MM-IMDB三项基准上表现优异，报告若干高性能指标并在缺模态场景下展现鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法在处理异构模态间复杂关系、噪声抑制与缺失模态时存在不足，需一种能动态权衡模态贡献、增强有信息信号并同时建模模态内外关系的鲁棒框架。

Method: 1) 双图构建：分别学习模态内部（intra-modal）和模态间（inter-modal）关系；2) 谱图滤波：在图谱域弱化噪声、增强重要信号；3) 多尺度GCN：提取不同尺度的节点嵌入；4) 语义感知注意力：根据上下文动态分配各模态权重，实现语义对齐与融合。

Result: 在CMU-MOSEI上取得95.3%准确率、0.936 F1与0.924 mAP，较MM-GNN在准确率上提升2.6%；在AVE上达93.4%准确率与0.911 F1；在MM-IMDB得91.8%准确率与0.886 F1，且在缺模态设置下仍表现稳健。

Conclusion: AGSP-DSA通过双图与谱滤波结合多尺度GCN与动态语义注意力，有效提升多模态融合性能并增强对缺失模态和噪声的鲁棒性，适用于情感分析、事件识别与多媒体分类等任务。

Abstract: In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.

</details>


### [154] [EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery](https://arxiv.org/abs/2601.18597)
*Yu Xia,Chang Liu,Tianqi Xiang,Zhigang Tu*

Main category: cs.CV

TL;DR: EFSI-DETR通过DyFusNet（频率-空域动态融合）、ESFC（高效语义集中）与FFR（细粒度保留）三大模块提升无人机小目标检测精度并保持实时性，在VisDrone上实现显著AP提升并达到188 FPS。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中小目标多、尺寸小、细节弱，现有方法未充分利用频率信息且依赖静态卷积，导致浅层细节与深层语义无法兼顾，制约小目标检测性能和速度。

Method: 提出DyFusNet用于动态统一处理频域和空域特征以增强多尺度融合能力；设计ESFC用于在低计算开销下提取深层语义特征；引入FFR策略在融合中保留浅层细粒度特征。整体框架为DETR类别的检测器改进，重点在特征提取与融合模块。

Result: 在VisDrone上AP提升1.6%，AP_s提升5.8%；在CODrone也取得优异结果，单卡RTX4090下实现188 FPS的实时推理速度，达到SOTA性能与高效率。

Conclusion: 本文提出的EFSI-DETR通过频域与空域联合动态融合以及高效语义集中器解决了无人机影像中小目标检测的表征不足和多尺度融合低效问题，实验在VisDrone和CODrone上证明了其在精度和实时性上的优势。

Abstract: Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \textbf{1.6}\% and \textbf{5.8}\% in AP and AP$_{s}$ on VisDrone, while obtaining \textbf{188} FPS inference speed on a single RTX 4090 GPU.

</details>


### [155] [Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures](https://arxiv.org/abs/2601.18619)
*Jorge Quesada,Ghassan AlRegib*

Main category: cs.CV

TL;DR: Introduce small-window cropping in SSL pretraining to improve segmentation of small/sparse objects; shows notable gains in seismic and neuroimaging tasks, limited impact on large-scale structures.


<details>
  <summary>Details</summary>
Motivation: Standard SSL pipelines bias toward large, homogeneous regions and underperform on small, sparse or locally irregular objects; need to adapt augmentations to target object scale.

Method: They add small-window cropping to the SSL augmentation pipeline to focus on fine-scale structures during pretraining.

Result: Consistent improvements under label constraints: up to 13% accuracy gain for seismic fault segmentation and 5% for cell delineation; little benefit for large-scale features.

Conclusion: SSL should be designed with awareness of object size and sparsity; scale-aware augmentations like small-window cropping improve representation for small/sparse objects.

Abstract: Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.

</details>


### [156] [Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation](https://arxiv.org/abs/2601.18623)
*Zihao Wang,Yuzhou Chen,Shaogang Ren*

Main category: cs.CV

TL;DR: 通过在扩散反向过程中预测空间可变混合场并注入目标一致恢复项，将全局线性域传递替换为局部残差修正；理论上给出连续时间解与一阶采样器，实验证明提升保真度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态图像翻译方法依赖单一全局线性域传递，导致采样器进入流形外高成本区域，增加纠正负担并引发语义漂移（称为固定步长域传递问题）。因此需要将域位移的空间和时间变化融入生成过程中以实现更稳健的翻译。

Method: 在连续时间扩散框架中引入可预测的空间可变混合场和显式恢复项，形成带有域移位动力学的生成过程；推导出精确连续解并构造保持边缘一致性的一阶实用采样器；在每个反向步骤进行in-step guidance以约束更新在流形上。

Result: 在医学影像、遥感和电致发光语义映射等任务上，所提出框架在结构保真度和语义一致性方面均优于标准扩散方法，并且在更少的去噪步骤内收敛。

Conclusion: 本文提出将域间位移动力学内嵌到扩散生成过程中，通过在每个反向步骤预测空间可变的混合场并注入目标一致的恢复项，使大步长更新保持在流形上，从而减少语义漂移和纠正负担。该方法将模型的任务从全局对齐转为局部残差修正，并推导出连续时间精确解形式及一阶采样器，实验表明在多个跨模态翻译任务中提升结构保真度与语义一致性并加速收敛。

Abstract: Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.

</details>


### [157] [CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search](https://arxiv.org/abs/2601.18625)
*Zequn Xie*

Main category: cs.CV

TL;DR: CONQUER: training-time optimal transport alignment and inference-time query enhancement for TBPS; outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Address cross-modal discrepancy and ambiguous/incomplete user queries in text-based person search to enable robust real-world deployment.

Method: Two-stage approach: training with multi-granularity encoding, complementary pair mining, context-guided optimal transport matching; inference with plug-and-play anchor selection and attribute-driven query enrichment.

Result: Improved cross-modal alignment and query refinement via a two-stage framework named CONQUER.

Conclusion: CONQUER achieves better Rank-1 accuracy and mAP across CUHK-PEDES, ICFG-PEDES, and RSTPReid, and improves cross-domain and incomplete-query robustness.

Abstract: Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.

</details>


### [158] [Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting](https://arxiv.org/abs/2601.18633)
*Tong Shi,Melonie de Almeida,Daniela Ivanova,Nicolas Pugeault,Paul Henderson*

Main category: cs.CV

TL;DR: Splat-Portrait uses Gaussian splatting to reconstruct 3D head from single image and synthesize lip motion from audio, trained without 3D labels, achieving better quality


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior 3D talking head methods that used heuristics and produced inaccurate 3D reconstructions; aim to improve realism and novel view synthesis using Gaussian splatting and data-driven lip motion synthesis

Method: Read abstract of Splat-Portrait; identify method, results, conclusion, tldr, motivation

Result: Proposes Gaussian-splatting-based method for talking head generation; learns disentangled static 3D Gaussians and 2D background; generates lip motion from audio without motion priors; trained with 2D reconstruction and score-distillation losses; outperforms prior work in visual quality and novel view synthesis

Conclusion: Splat-Portrait enables realistic talking head generation and novel view synthesis without 3D supervision or landmark priors, improving visual realism over prior methods

Abstract: Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.

</details>


### [159] [Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge](https://arxiv.org/abs/2601.18698)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: 该论文提出Geo-Attraction Landmark Probing (GAP)框架和GEOATTRACTION-500基准，评估文本到视频模型对全球旅游景点的地理知识。通过结构与关键点对齐、视觉-语言判断等指标并以人工评估验证，发现Sora 2在不同地区、发展水平和文化群体间的地理视觉知识较为均衡，仅与景点流行度弱相关。


<details>
  <summary>Details</summary>
Motivation: 研究文本到视频模型是否编码了地理上公平的视觉知识，尤其在合成世界各地旅游景点时是否存在偏差。

Method: 提出GAP框架，构建包含500个分布广泛景点的GEOATTRACTION-500基准；采用多种互补指标评估：全局结构对齐、基于关键点的细粒度对齐、以及视觉-语言模型判断，并用人工评估进行验证。

Result: 对Sora 2进行评测，发现模型在不同地区、发展程度和文化分组间表现较为均匀，地理偏差弱，景点流行度对表现有轻微影响。

Conclusion: 当前文本到视频模型在地理视觉知识上比预期更均衡，显示其在全球部署的潜力，但需要持续评估以应对未来系统演进。

Abstract: Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.

</details>


### [160] [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714)
*Judith Vilella-Cantos,Mauro Martini,Marcello Chiaberge,Mónica Ballesta,David Valiente*

Main category: cs.CV

TL;DR: 提出轻量级的MinkUNeXt-VINE，利用预处理与Matryoshka多损失学习，实现对稀疏低成本LiDAR的高效地点识别，在两套长期葡萄园数据集上优于现有方法，并提供了详尽消融研究和开源代码。


<details>
  <summary>Details</summary>
Motivation: 农业环境结构无序、缺乏显著地标，使移动机器人地点识别困难；希望在低成本传感器和实时约束下提高识别性能。

Method: 提出轻量级深度学习网络（MinkUNeXt-VINE），结合预处理步骤与多损失（Matryoshka 表征学习）训练策略，输出低维表示以适应实时和低资源场景；使用稀疏 LiDAR 数据并在两个长期葡萄园数据集上评估，包含详尽消融实验。

Result: 在两个长时序葡萄园数据集（不同 LiDAR 传感器）上，MinkUNeXt-VINE 在准确率和鲁棒性上均超越现有方法，特别在低分辨率/低成本传感器输入下表现良好；提供开源代码以便复现。

Conclusion: MinkUNeXt-VINE 在葡萄园环境中通过预处理和 Matryoshka Representation Learning 多损失方法，在低成本稀疏 LiDAR 输入下实现了较高的定位性能，优于现有方法。

Abstract: Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.

</details>


### [161] [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739)
*Ignacio Antequera-Sánchez,Juan Luis Suárez-Díaz,Rosana Montes,Francisco Herrera*

Main category: cs.CV

TL;DR: 提出SeNeDiF-OOD：基于语义嵌套二叉融合的层次化OOD检测，能更好地处理从低级噪声到高级语义漂移的异质OOD，实验证明在MonuMAI应用上显著优于单阶段方法。


<details>
  <summary>Details</summary>
Motivation: 单阶段检测器难以同时覆盖从低级输入扰动到高级语义偏移的多样化OOD问题；因此提出层次化、语义感知的融合框架以匹配OOD的多尺度本质。

Method: 将OOD检测分解为多层二元融合节点，每一层对应特定语义抽象水平，通过对层间决策边界的集成（融合）来逐步筛选和判别不同类型的OOD样本。

Result: 在MonuMAI建筑风格识别的开放环境实验中，SeNeDiF-OOD在各种OOD类型（非纪念性图片、未知风格、对抗样本）上均显著优于传统基线，并在保持原有分类性能的前提下提升了检出率和过滤能力。

Conclusion: SeNeDiF-OOD通过语义嵌套二叉融合的层次结构，有效提升了对异构OOD样本（包括低级损坏、语义漂移及对抗样本）的检测性能，在保持内部分布识别能力的同时显著优于传统单阶段检测基线。

Abstract: Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [162] [Context Lake: A System Class Defined by Decision Coherence](https://arxiv.org/abs/2601.17019)
*Xiaowei Jiang*

Main category: cs.DB

TL;DR: 为确保多个AI代理在共享资源上做出不可逆决策时不发生冲突，必须在决策时刻评估相互作用决策的现实一致性。现有系统无法组合以满足此要求；作者提出并形式化了新系统类“Context Lake”的三项必要条件，奠定了AI代理协作的理论基础。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理成为主要数据消费者并持续执行不可逆决策，传统为人类分析设计的数据系统在并发、不可逆行为下成为正确性瓶颈；当多个代理在共享资源上操作时，决策在和解前会相互影响，因此需要在决策时刻提供一致的现实表征以避免冲突。

Method: 主要通过理论分析与证明（提出定律、证明组合不可能性定理），并从不可能性结论推导出新系统类的三项设计要求，随后形式化架构约束与可接受条件，作为立场论文进行系统性论述，而非通过实验验证。

Result: 提出决策一致性法则与组合不可能性定理，证明现有系统无法满足决策一致性需求；提出并定义Context Lake作为必要的新系统类及其三项核心要求，形式化了正确性所需的架构和条件，为未来系统设计提供理论基础。

Conclusion: 该论文提出“决策一致性法则”，强调在AI代理对共享资源做出不可逆决策时，必须在决策时刻基于一致的现实表征来评估相互作用的决策，否则事后保证无法避免冲突。论文证明了现有系统类无法满足该要求，并通过“组合不可能性定理”表明独立推进的系统无法在保持原系统属性下组合以提供决策一致性。由此提出了必需的新系统类“环境湖（Context Lake）”，并规定三项必要条件：语义操作为原生能力、对所有决策相关状态提供事务性一致性、以及在负载下界定陈旧度和退化的操作边界。论文形式化了为集体代理系统正确性所需的架构不变量、强制边界和可接受条件，阐明了现有架构的不足并为大规模AI代理协同提出系统保证要求。

Abstract: AI agents are increasingly the primary consumers of data, operating continuously to make concurrent, irreversible decisions. Traditional data systems designed for human analysis cycles become correctness bottlenecks under this operating regime. When multiple agents operate over shared resources, their actions interact before reconciliation is possible. Correctness guarantees that apply after the decision window therefore fail to prevent conflicts. We introduce the Decision Coherence Law: for agents that take irreversible actions whose effects interact, correctness requires that interacting decisions be evaluated against a coherent representation of reality at the moment they are made. We show that no existing system class satisfies this requirement and prove through the Composition Impossibility Theorem that independently advancing systems cannot be composed to provide Decision Coherence while preserving their native system classes. From this impossibility result, we derive Context Lake as a necessary system class with three requirements: (1) semantic operations as native capabilities, (2) transactional consistency over all decision-relevant state, and (3) operational envelopes bounding staleness and degradation under load. We formalize the architectural invariants, enforcement boundaries, and admissibility conditions required for correctness in collective agent systems. This position paper establishes the theoretical foundation for Context Lakes, identifies why existing architectures fail, and specifies what systems must guarantee for AI agents to operate constructively at scale.

</details>


### [163] [Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs](https://arxiv.org/abs/2601.17058)
*Wei Zhou,Jun Zhou,Haoyu Wang,Zhenghao Li,Qikang He,Shaokun Han,Guoliang Li,Xuanhe Zhou,Yeye He,Chunwei Liu,Zirui Tang,Bin Wang,Shen Tang,Kai Zuo,Yuyu Luo,Zhenzhe Zheng,Conghui He,Jingren Zhou,Fan Wu*

Main category: cs.DB

TL;DR: 综述指出LLM正推动数据准备从规则化流水线向提示驱动、代理化工作流转变，带来语义理解与泛化优势，但需解决成本、幻觉与评估不足等关键挑战，并朝可扩展、可靠与可评估的方向发展。


<details>
  <summary>Details</summary>
Motivation: 随着对可直接应用数据需求的增长、LLM技术能力提升及支持灵活代理构建的基础设施兴起，LLM增强的数据准备方法正成为重要范式，值得对其现状、优劣与研究空白进行系统评估。

Method: 通过检索并系统分析数百篇近年文献，构建以任务为中心的分类体系（数据清洗、数据集成、数据增强），并对每类任务下代表性技术进行机制解析与比较，同时汇总常用数据集与评估指标，最后提出未来研究方向与挑战。

Result: 论文归纳了LLM在数据准备领域的优点（如更强的语义理解与泛化能力、灵活的提示驱动工作流）与缺点（如扩展成本高、持续出现的幻觉问题、评估与方法脱节），并提供了任务层面的技术细分、常用数据集与评价指标清单，以及面向可扩展系统、可靠代理设计与健全评估的路线图。

Conclusion: 本文系统综述指出，基于大模型（LLM）的数据准备方法正在从基于规则和模型特定的流水线，向基于提示、上下文感知及具代理能力的工作流转变，并在数据清洗、融合与增强三大任务上显示出显著优势，但也面临成本、幻觉与评估不足等限制。

Abstract: Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation.
  By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.

</details>


### [164] [Vidformer: Drop-in Declarative Optimization for Rendering Video-Native Query Results](https://arxiv.org/abs/2601.17221)
*Dominik Winecki,Arnab Nandi*

Main category: cs.DB

TL;DR: Vidformer通过声明式化与按需并行渲染，将视频渲染瓶颈从数秒/分/小时缩短到0.25–0.5秒首帧播放，整体渲染加速2–3倍，支持亚秒交互与LLM对话式查询。


<details>
  <summary>Details</summary>
Motivation: 传统基于OpenCV/Python的后处理脚本在渲染阶段成为视频原生查询的瓶颈，尤其是短片仍包含大量高分辨率帧，导致用户需等待很长时间才能看到第一帧。

Method: 将现有可视化脚本自动转换为声明式管线，进行图式优化与并行执行，结合Video on Demand的按需分段即时渲染与传输。

Result: 在多种标注工作负载上，整体渲染时间减少2-3倍，首帧播放时间降至0.25–0.5秒，表明首次播放延迟与片长解耦，且支持基于LLM的交互式视频查询。

Conclusion: Vidformer通过将视频可视化代码提升为声明式表示并进行透明优化、并行化与按需分段渲染，显著降低了渲染时间并将首帧播放延迟降至亚秒级，从而使视频原生查询变得可交互。

Abstract: When interactively exploring video data, video-native querying involves consuming query results as videos, including steps such as compilation of extracted video clips or data overlays. These video-native queries are bottlenecked by rendering, not the execution of the underlying queries. This rendering is currently performed using post-processing scripts that are often slow. This step poses a critical point of friction in interactive video data workloads: even short clips contain thousands of high-definition frames; conventional OpenCV/Python scripts must decode -> transform -> encode the entire data stream before a single pixel appears, leaving users waiting for many seconds, minutes, or hours.
  To address these issues, we present Vidformer, a drop-in rendering accelerator for video-native querying which, (i) transparently lifts existing visualization code into a declarative representation, (ii) transparently optimizes and parallelizes rendering, and (iii) instantly serves videos through a Video on Demand protocol with just-in-time segment rendering. We demonstrate that Vidformer cuts full-render time by 2-3x across diverse annotation workloads, and, more critically, drops time-to-playback to 0.25-0.5s. This represents a 400x improvement that decouples clip length from first-frame playback latency, and unlocks the ability to perform interactive video-native querying with sub-second latencies. Furthermore, we show how our approach enables interactive video-native LLM-based conversational querying as well.

</details>


### [165] [Constant-time Connectivity and 2-Edge Connectivity Querying in Dynamic Graphs](https://arxiv.org/abs/2601.17285)
*Lantian Xu,Junhua Zhang,Dong Wen,Lu Qin,Ying Zhang,Xuemin Lin*

Main category: cs.DB

TL;DR: 提出一种在完全动态无向图中维护连通性查询的新方法，通过同时维护生成树和并查集树，实现常数查询时间，并在边插入/删除上比D-tree有理论和实践改进；扩展到2-边连通性，实测在大数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实动态图应用中边频繁更新，要高效处理连通性查询。现有方法D-tree通过为每个连通分量维护生成树并用启发式降低树深度，但在插入/删除性能和理论复杂度上仍有改进空间。

Method: 提出同时维护生成树与并查集树（disjoint-set tree）的框架：生成树用于结构维护，disjoint-set提供常数时间的连通性查询。结合两者优势设计边插入与删除的算法，改进理论运行时间，并将算法扩展到维护2-边连通性。

Result: 理论上实现了查询O(1)并在插入/删除复杂度上优于D-tree；在真实大规模数据集上的实验显示算法在查询和更新上均有显著性能提升。

Conclusion: 通过双树并用的方法，可以在完全动态图中高效维护连通性和2-边连通性，兼顾常数查询时间与更优的更新开销，实验证明其可在大规模图上显著提升性能。

Abstract: Connectivity query processing is a fundamental problem in graph processing. Given an undirected graph and two query vertices, the problem aims to identify whether they are connected via a path. Given frequent edge updates in real graph applications, in this paper, we study connectivity query processing in fully dynamic graphs, where edges are frequently inserted or deleted. A recent solution, called D-tree, maintains a spanning tree for each connected component and applies several heuristics to reduce the depth of the tree. To improve efficiency, we propose a new spanning-tree-based solution by maintaining a disjoint-set tree simultaneously. By combining the advantages of two trees, we achieve the constant query time complexity and also significantly improve the theoretical running time in both edge insertion and edge deletion. In addition, we extend our connectivity maintenance algorithms to maintain 2-edge connectivity. Our performance studies on real large datasets show considerable improvement of our algorithms.

</details>


### [166] [UTune: Towards Uncertainty-Aware Online Index Tuning](https://arxiv.org/abs/2601.18199)
*Chenning Wu,Sifan Chen,Wentao Wu,Yinan Jing,Zhenying He,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: UTune uses uncertainty quantification and an uncertainty-weighted ε-greedy strategy to make learned index benefit estimators work better in online tuning with limited feedback and workload drift.


<details>
  <summary>Details</summary>
Motivation: Existing learned index benefit estimators struggle in online tuning due to scarce execution feedback and evolving unseen queries, harming generalization.

Method: UTune quantifies model uncertainty at operator level and integrates uncertainty into index selection via an uncertainty-weighted ε-greedy search for configuration enumeration.

Result: UTune introduces uncertainty-aware operator-level learned models to improve online index tuning performance.

Conclusion: UTune improves workload execution time and reduces index exploration overhead, enabling faster convergence under stable workloads.

Abstract: There have been a flurry of recent proposals on learned benefit estimators for index tuning. Although these learned estimators show promising improvement over what-if query optimizer calls in terms of the accuracy of estimated index benefit, they face significant limitations when applied to online index tuning, an arguably more common and more challenging scenario in real-world applications. There are two major challenges for learned index benefit estimators in online tuning: (1) limited amount of query execution feedback that can be used to train the models, and (2) constant coming of new unseen queries due to workload drifts. The combination of the two hinders the generalization capability of existing learned index benefit estimators. To overcome these challenges, we present UTune, an uncertainty-aware online index tuning framework that employs operator-level learned models with improved generalization over unseen queries. At the core of UTune is an uncertainty quantification mechanism that characterizes the inherent uncertainty of the operator-level learned models given limited online execution feedback. We further integrate uncertainty information into index selection and configuration enumeration, the key component of any index tuner, by developing a new variant of the classic $ε$-greedy search strategy with uncertainty-weighted index benefits. Experimental evaluation shows that UTune not only significantly improves the workload execution time compared to state-of-the-art online index tuners but also reduces the index exploration overhead, resulting in faster convergence when the workload is relatively stable.

</details>
