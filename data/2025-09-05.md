<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: GFP用轻量级的目标生成网络动态生成跨时空层次的多样化监督信号，通过约束优化保持特征多样性并避免坍缩，实现比传统掩码骨骼方法快6.2×的训练速度并在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MAE类骨骼自监督方法仅重建低层坐标，造成计算冗余且语义表示有限，需探索高层语义特征的预测以提高效率和表达能力。

Method: 引入一个轻量级的目标生成网络，动态生成多层次的监督特征；设计受约束的优化目标以保证特征多样性并防止模型坍缩；在标准数据集上进行对比实验验证效率和性能优势。

Result: 提出了一种通用特征预测（GFP）框架，用于掩码骨骼建模，将低层重建替换为高层特征预测，显著提高训练效率并提升语义表示能力。

Conclusion: GFP通过预测高层特征（从局部运动到全局语义）替代坐标重建，减少计算冗余，提升表示质量，实现更高效的自监督骨骼动作识别。

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Dimitri Androutsos,Susan Done,April Khademi*

Main category: cs.CV

TL;DR: 提出将有丝分裂检测建模为像素级分割，结合域泛化模块和师生生成的伪掩码，并用多尺度分类器实现检测与不典型有丝分裂分类，初步结果良好（F1=0.7660，BA=0.8414）。


<details>
  <summary>Details</summary>
Motivation: 减轻病理学家计数有丝分裂的工作量与主观差异，通过AI实现自动检测，但需应对域偏移与严重类别不平衡问题。

Method: 基于UNet的像素级分割骨干，融合对比表征学习与域对抗训练的域泛化模块；采用师生策略生成像素级伪掩码（包括有丝分裂、难负例和正常细胞核）；分类采用多尺度CNN，利用分割特征并在多任务框架下训练。

Result: 在初步测试集上，检测（Track 1）F1=0.7660，分类（Track 2）平衡准确率=0.8414，验证了方法在鲁棒性和性能上的有效性。

Conclusion: 本文提出的基于分割的师生模型在处理有域偏移和类不平衡的有丝分裂检测问题上表现良好，统一了检测与分类任务，提高了鲁棒性与判别能力。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [3] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

TL;DR: 提出GMBM：训练阶段显式学习偏见方向（ABIL），微调阶段通过梯度抑制剔除这些方向，配合SBA指标，在多偏见和分布偏移设置下显著降低偏见并提升最差组性能。


<details>
  <summary>Details</summary>
Motivation: 现实图像中常常存在多重重叠的偏见（纹理、水印、性别化妆、场景对象组合等），单独缓解某一偏见会导致或加剧其他偏见，因此需要端到端、多偏见的整体解决方案且仅在训练时需分组标签，在测试时最小化偏见。

Method: 两阶段方法：1) ABIL：为每个属性训练独立编码器并将其与主骨干集成，使分类器显式识别已知捷径；2) Gradient Suppression Fine Tuning：通过抑制骨干网络梯度中与偏见方向相关的分量，剪除这些偏见方向，最终得到单一模型。并提出了SBA作为新的测试时偏见度量。

Result: 在FB-CMNIST、CelebA和COCO数据集上验证：提升最差组准确率，减半多属性偏见放大，SBA达到新的低值，且在偏见复杂性和分布偏移加剧时仍保持效果。

Conclusion: GMBM提出了一个两阶段的轻量级框架，通过在训练阶段学习并识别已知偏见方向（ABIL），在微调阶段通过梯度抑制去除这些偏见方向，从而在测试时获得一个不依赖这些捷径的紧凑模型。实验证明在多偏见复杂且分布偏移的设置下，GMBM能显著提升最差组准确率、减半多属性偏见放大，并在SBA指标上取得新的最低值。

Abstract: Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [4] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

TL;DR: 作者通过消融实验找出关键组件（仿射增强、深度监督），据此设计2M参数的轻量级U-Net，在精度不降低的前提下显著减小模型并加速推理，适合实时心脏超声分割应用。


<details>
  <summary>Details</summary>
Motivation: 动机是现有由nnU-Net配置的模型虽然性能优异，但模型体积大、推理慢，限制了实时临床应用，因此寻找更轻量且速度更快的替代方案，同时保持分割精度。

Method: 方法包括逐步消融实验，评估数据增强方案、网络结构修改、损失函数和后处理技术的影响；基于发现设计并训练一个约2M参数的轻量级U-Net；在公开CAMUS数据集和内部数据集上进行对比实验和交叉数据集评估。

Result: 结果显示：在CAMUS（N=500）上，轻量级U-Net在LV/MYO/LA的Dice分别为0.93/0.85/0.89，与nnU-Net的0.93/0.86/0.89无显著差异（p>0.05）；模型参数从33M降至2M，推理速度从5.40ms提高到1.35ms每帧，体积小16倍、速度快4倍；在内部数据集（N=311）上也验证了类似的泛化能力。

Conclusion: 该论文结论是：通过系统的消融研究，简化并保留nnU-Net中最有效的组件（如仿射数据增强和深度监督），可以设计出参数更少、推理更快且性能与nnU-Net统计上等效的轻量级U-Net用于超声左心室分割。

Abstract: Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [5] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

TL;DR: 本文提出改进的treeX算法，为TLS/PLS与ULS分别提供参数预设，实现无监督的树实例分割。相较原算法，改进后运行更快，尤其在地面扫描数据上检测F1提升显著；对ULS也能达到0.58的F1。方法可作为轻量级替代或半自动标注工具，开源实现于pointtree包。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法虽有效但需大量标注数据与计算资源。为资源受限场景提供一种无需人工标注、计算开销小的替代方法。

Method: 基于聚类的树干检测结合区域生长的树冠分割，无监督流程，并为不同采集方式提供两套参数预设（地面与无人机）。实现优化以降低运行时间并提高准确率，提供Python开源实现。

Result: 在六个公开数据集上评估，对比六种开源方法。地面数据实例检测F1较原算法提高0.11到0.49；ULS达到0.58 F1（原算法失败）；整体与近期开源方法（包括深度学习）精度相当。

Conclusion: 改进后的treeX在地面激光扫描数据上显著提升实例检测性能，并能在无人机激光扫描场景中成功工作；其资源效率使其成为深度学习方法的实用替代或标注辅助工具。

Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [6] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

TL;DR: Introduce Reg3D: geometry-aware supervision as both input and target; reconstructive tasks to learn spatial reasoning; strong gains on 3D benchmarks.


<details>
  <summary>Details</summary>
Motivation: Text-only supervision lacks geometric constraints; need to learn robust 3D spatial representations by reconstructing geometry.

Method: Reconstructive Geometry Instruction Tuning

Result: Dual-supervision with object-level and frame-level reconstruction in a dual-encoder architecture yields substantial improvements on ScanQA, Scan2Cap, ScanRefer, SQA3D.

Conclusion: Reg3D establishes a new training paradigm for spatially aware multimodal models by enforcing geometric consistency through reconstructive supervision.

Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [7] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: 提出首个全量化V2X多智能体中间融合系统，通过统一端到端量化同时压缩模型与通信表示，兼顾精度与部署效率，实测显著降延迟、增精度并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有V2X协作感知研究侧重准确率提升，忽视了效率、延迟与可部署性，且普遍使用全精度模型导致计算与传输开销过大，不适合资源受限的实时部署。

Method: 设计并实现了统一的端到端量化策略，覆盖神经网络模型与通信消息表示；将模型和中间特征均量化以节省计算与传输；在多模态、多智能体的中间融合架构上验证量化方案并评估系统级指标（延迟、内存、带宽）。

Result: QuantV2X在低位宽下保持与全精度系统相近的准确性，同时在部署导向指标上将系统级延迟降低约3.2倍，在mAP30上相较全精度基线提高9.5，且能在严格内存预算下支持更大模型。

Conclusion: 本文提出了QuantV2X，一种首个针对多智能体V2X协作感知的全量化系统，实现了在低位宽下接近全精度的精度，同时显著降低延迟和带宽占用，具备实用部署潜力。

Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [8] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

TL;DR: 在叶脉图像的植物分类任务中，EfficientNetB0优于ResNet50与MobileNetV2；ResNet50过拟合，MobileNetV2适合轻量化实时场景，EfficientNetB0兼顾准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 叶脉形态是具有较高分类学价值的关键形态学特征；研究旨在评估不同轻量与高性能卷积架构在叶脉图像自动物种识别任务中的效果，以支持可扩展的植物分类工具开发。

Method: 在 Swedish Leaf Dataset（15个物种，75张/物种，总计1125张图）上比较三种深度卷积网络（ResNet50、MobileNetV2、EfficientNetB0）；使用标准训练/测试流程与性能指标（准确率、精确率、召回、F1），记录训练与测试阶段结果以评估拟合与泛化能力。

Result: ResNet50：训练准确率94.11%，测试准确率88.45%，F1=87.82%，存在过拟合；MobileNetV2：测试准确率93.34%，F1=93.23%，泛化性好，适合实时轻量应用；EfficientNetB0：测试准确率94.67%，精确率/召回/F1均>94.6%，表现最佳且稳健。

Conclusion: EfficientNetB0 在基于叶脉的植物物种分类中表现最佳，表现出较高的测试准确率与F1，适合构建可扩展、精确的自动化分类工具；ResNet50 存在过拟合风险，MobileNetV2 适合轻量化实时应用。

Abstract: This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [9] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

TL;DR: 提出LayoutGKN：通过把跨图节点交互推迟到联合嵌入末端并使用可微图核，实现在相似或更好的匹配精度下大幅提升推理速度，适用于楼层平面图比对任务。


<details>
  <summary>Details</summary>
Motivation: 楼层平面图作为表达空间关系的图结构，在检索、聚类与可视化等任务中需要高效且准确的图相似度计算；现有最成功的方法依赖代价高昂的跨图节点交互导致推理慢，需更高效的替代方案。

Method: 通过延后跨图节点级交互至联合嵌入架构的末端，使用可微图核在最终学习到的节点嵌入上直接计算距离，从而避免昂贵的中间跨图交互。

Result: 在多个数据集上，LayoutGKN的相似度计算性能与图匹配网络相当或更好，同时推理速度显著加快。代码与数据已开源。

Conclusion: LayoutGKN在保持或提升图匹配精度的同时，大幅减少推理时间，是对现有图匹配网络的有效高效替代。

Abstract: Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [10] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD用SVD微调奇异值，0.04%参数成本实现对CLIP的高效且可解释的领域适配，在多组数据集的少样本分类中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有自适应方法依赖提示工程或新增模块，可能降低适配质量、破坏模型稳定性并损害预训练知识，故需一种保留预训练参数结构且参数高效的适配方式。

Method: 提出在CLIP内部参数空间上应用SVD分解，仅调整奇异值来重标定基向量，从而实现多模态且参数高效的微调（仅0.04%参数），无须增加提示词或适配器模块。

Result: 在11个自然图像和10个生物医学数据集的少样本分类上，CLIP-SVD在准确率和泛化能力上均优于现有方法；并通过基于自然语言的分析提升了适配过程的可解释性。

Conclusion: CLIP-SVD通过只微调矩阵的奇异值，在不引入额外模块的情况下实现对CLIP的高效领域自适应，兼顾了参数高效性和泛化能力，并在自然与生物医学数据集的少样本分类任务上取得了SOTA表现。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [11] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

TL;DR: 为在边缘设备上提升病害细粒特征识别，论文提出DeepMAD轻量骨干与形状-纹理解耦注意力STAM，实验证明在CCMT上实现高效且高精度的作物病害诊断。


<details>
  <summary>Details</summary>
Motivation: 现有轻量化网络多沿用用于通用目标识别的注意力机制，难以捕捉病害图像中的不规则病斑形状与复杂纹理，限制了在边缘设备上高精度作物病害诊断的实现。

Method: 采用两步方法：1) 使用训练自由的神经架构搜索DeepMAD生成参数与计算量受限的轻量骨干；2) 设计STAM，将注意力分为形状分支（基于DCNv4的可变形卷积增强形状感知）和纹理分支（利用Gabor滤波器组捕获频域纹理特征），并将其集成到轻量骨干中进行训练与评估。

Result: 在公开的CCMT植物病害数据集上，STA-Net仅有401K参数、51.1M FLOPs，但实现了89.00%准确率与88.96% F1分数。消融实验表明STAM相比基线与常见注意力模块能显著提升性能。

Conclusion: 该论文提出了面向边缘设备的作物病害诊断轻量化网络STA-Net，通过训练无关的神经结构搜索（DeepMAD）设计高效骨干，并提出形状-纹理解耦注意力模块（STAM），显著提升病变细粒征识别能力和诊断性能。

Abstract: Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [12] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

TL;DR: 文章引入DeepCamo数据集并提出SLENet（含GAE、LGB、MSSD三模块），显著提升水下伪装目标检测性能并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下伪装目标检测在海洋生态学中重要但研究不足，受光学畸变和水体浑浊影响以及目标与背景高度相似，传统COD方法难以适应；因此需要专门的数据集和设计针对性网络来提升检测性能。

Method: 提出了三大关键模块：Gamma-Asymmetric Enhancement (GAE)用于改善多尺度特征的增强，Localization Guidance Branch (LGB)生成富含全局语义的位置信息图，以及Multi-Scale Supervised Decoder (MSSD)在位置信息引导下融合多尺度监督以输出更精确的检测结果。并在DeepCamo上对现有SOTA COD模型进行了基准测试以发现问题并驱动架构设计。

Result: 在DeepCamo和三个现有的COD数据集上，SLENet在主要评价指标（如mIoU、F-measure等）上显著超过对比SOTA方法，表明其在处理水下图像伪装和整体COD任务上均具有优势。

Conclusion: 该论文提出了针对水下伪装目标检测（UCOD）的数据集DeepCamo与新模型SLENet，旨在解决水下成像模糊、水体浑浊及海洋生物复杂外观带来的检测难题。结果显示SLENet在DeepCamo和其他三个COD基准数据集上优于现有SOTA方法，具有较好泛化能力。

Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [13] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

TL;DR: 在扩散模型训练中加入基于视频连续帧的时间一致性正则化，无需改动架构即可显著加快收敛、降低FID并提升多样性，原因为其有效减少了梯度方差。


<details>
  <summary>Details</summary>
Motivation: 静态图像采样的训练范式固有地缺失时间信息，导致收敛慢、分布覆盖不足和泛化能力差。利用视频的时间连续性可以补充这些信息缺口。

Method: 在标准扩散训练管线中插入一种基于连续视频帧的时间一致性正则化（无需修改模型架构），通过在训练中利用相邻帧的时间相关性来减小梯度方差并鼓励模型捕捉时间变化。

Result: 在HandCo数据集上，方法使收敛速度加快超过2倍，训练和验证集上的FID均降低，并提升了生成多样性，实验和优化分析表明正则化降低了梯度方差，促进了快速收敛。

Conclusion: 该论文提出了一种简单且高效的训练策略，通过利用视频帧的时间归纳偏置来改进基于扩散模型的图像生成训练，提升收敛速度、分布覆盖和泛化能力。

Abstract: Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [14] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MedVista3D 通过局部-全局多尺度对齐与语义增强的报告一致性处理，解决了 3D 医学影像中精细检测与整体理解及报告语言多样性的问题，取得了多项任务的领先表现。


<details>
  <summary>Details</summary>
Motivation: 放射诊断中存在误读、注意力盲点和沟通失败，尤其在三维影像中，需同时具备精确的局部检测、整体体积推理以及语义一致的报告生成。

Method: 在多尺度下进行局部（slice/patch 级）与全局（体积级）图像-文本对齐，使用语言模型重写放大报告一致性，并构建 Radiology Semantic Matching Bank 以进行语义感知对齐。

Result: MedVista3D 在零样本疾病分类、报告检索、医学视觉问答上达到 SOTA，并能良好迁移到器官分割和预后预测任务。

Conclusion: MedVista3D 提出了一种面向 3D CT 的多尺度语义增强视觉-语言预训练框架，通过局部和全局图像-文本对齐以及对报告语义的一致性处理，实现了精细定位检测与整体推理的统一。

Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [15] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: CaPL通过属性解缠与视觉颗粒化结合因果推断，增强了CLIP文本提示的细粒度辨识能力，在多项数据集上取得了领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的提示学习在细粒度数据上表现有限，需要捕捉类间细微差异以提升辨别能力。

Method: 提出两个模块：属性解缠模块（用Brownian Bridge扩散模型将视觉特征分解为非个体化属性和个体化属性）和颗粒学习模块（在两种因果策略下整合这些属性构建视觉颗粒，指导文本提示学习）。

Result: 在15个数据集上进行广泛实验证明CaPL在细粒度数据集上尤其显著地超过了最新提示学习方法。

Conclusion: CaPL通过视觉颗粒化和因果推断提升CLIP在细粒度识别上的提示词学习能力，性能显著优于现有方法。

Abstract: Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [16] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 利用事件相机提出EGTM，通过事件-帧协同的像素级temporal lucky fusion，高效且小模型地完成湍流恢复，并提供首个真实事件驱动湍流数据集，显著降低计算开销且略升恢复质量。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的TM方法依赖多帧寻找“幸运”无畸变块，需要高容量网络学习粗粒度的湍流时序动力学，受限于相机帧率，导致计算与存储低效；事件相机以微秒级时间分辨率和稀疏异步成像有潜力高效捕捉湍流引起的快速变化，因此可根本性缓解上述瓶颈。

Method: 提出EGTM框架：首先揭示事件流与湍流畸变的逆时空分布相关性（event-lucky insight），然后从嘈杂的湍流事件中提取像素级可靠无湍流指导，并用于时间域的lucky fusion；同时搭建真实数据采集系统，构建事件-帧联合的湍流数据集用于训练与评估。

Result: 在作者构建的真实事件驱动湍流数据集上，EGTM在模型大小、推理延迟和模型复杂度上分别比SOTA方法优越约710倍、214倍和224倍，同时在PSNR上提升约0.94 dB、SSIM提升约0.08，实现了更高效且等同或更好的重建质量。

Conclusion: 本文提出将事件相机引入湍流消除任务，通过“event-lucky insight”利用事件流的时空分布提取无湍流指导，从而进行像素级的temporal lucky fusion，并构建了首个真实事件驱动湍流数据集，实验显示在模型体积、推理延时和复杂度上大幅优于现有SOTA，同时在恢复质量上略有提升。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [17] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: 本文提出FocusMamba，通过事件引导的自适应多模态稀疏化与跨模态聚焦融合，在保留关键信息的同时大幅降低计算代价，提升RGB-Event检测任务的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-Event方法对图像背景与事件数据的非事件区域一视同仁地处理，导致计算开销大且性能受限；现有稀疏化方法采用固定阈值/数量选择令复杂度可变样本难以兼顾信息保留与效率。

Method: 设计Event-Guided Multimodal Sparsification (EGMS)来根据事件相机感知的场景变化自适应筛除低信息区域；在此基础上提出Cross-Modality Focus Fusion (CMFF)模块，用于跨模态捕获并融合互补特征。

Result: 在DSEC-Det与PKU-DAVIS-SOD数据集上，FocusMamba在精度与效率两方面均优于现有方法。

Conclusion: FocusMamba通过基于事件的自适应多模态稀疏化与跨模态聚焦融合，有效降低计算冗余并提升检测性能。

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [18] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

TL;DR: 提出SalientFusion，通过显著性背景去除、深度辅助角色区分与提示-视觉对齐去偏，提高组合式零样本食物识别性能，在新基准和通用数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 食物类别多变且新菜品不断出现，传统监督学习难以覆盖未见类别，因此需要零样本学习能力；此外食物图像常含复杂背景且同一属性（如配料）语义歧义和主/配菜角色混淆，影响模型对组成属性的正确推理。

Method: 方法由两部分组成：1) SalientFormer：一种上下文感知模块，利用注意力机制去除背景冗余并融合深度（可能为深度估计或深度图）特征来区分主菜与配菜的角色；2) DebiasAT：一种去偏模块，通过将可学习的/设计的文本提示与视觉特征对齐来减弱单一属性导致的语义偏差。整体框架结合视觉-文本对齐和显著性增强以提升组合式零样本泛化能力。

Result: 在作者构建的两个新基准CZSFood-90和CZSFood-164上，SalientFusion优于现有CZSL和ZSFL方法，报告了更高的Harmonic Mean或类似综合指标；在通用CZSL数据集上也达到了最先进性能。代码已开源。

Conclusion: 该论文提出了用于组合式零样本食物识别（CZSFR）的SalientFusion方法，通过去除背景冗余、利用深度特征解决主菜/配菜角色混淆、以及通过对齐文本提示与视觉特征来降低属性语义偏差，从而提升对未见食物类别的识别性能。实验在作者提出的CZSFood-90和CZSFood-164基准以及通用CZSL数据集上达到或超过现有方法的最先进结果。

Abstract: Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [19] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 该综述首次从生成流程角度系统梳理人类运动视频生成领域，覆盖输入、运动规划、视频生成、细化与输出五个阶段，评述200+篇工作并讨论大语言模型的潜在作用，为数字人研究提供全面资源与未来方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述多聚焦具体方法或子领域，缺乏对‘生成流程’的整体性梳理，且未充分评估大语言模型在运动视频生成中的应用潜力。作者希望通过系统化的阶段划分与跨模态综述，填补该空白，帮助研究者理解整个生成链路并识别未来研究方向与挑战。

Method: 作者将人类运动视频生成过程拆分为五个阶段：输入（多模态源如文本、音频、图像、动作条件等）、运动规划（动作表示与时间/空间规划）、运动视频生成（基于图像到视频、条件生成模型、扩散模型等技术）、细化（超分辨率、帧间一致性、动作细节增强）和输出（渲染、合成与评估指标）。论文并按视觉、文本、音频模态逐一综述了约十多个子任务与相应方法，比较并总结了各类方法的优缺点与适用场景，同时汇总了实现资源与开源模型。

Result: 提供了覆盖200+篇论文的全面综述，明确了五个关键生成阶段及十余个子任务，列举并分析了重要里程碑工作与方法趋势。论文强调大语言模型在规划与多模态理解方面的潜力，并提供了模型列表与代码仓库链接，成为研究和应用的参考资料。

Conclusion: 该综述论文系统性整合了人类运动视频生成领域的研究，覆盖输入、运动规划、视频生成、细化和输出五个关键阶段，并首次讨论了大语言模型在该领域的潜力。论文通过跨视觉、文本、音频三类模态，评述了200+篇相关工作，梳理了重要里程碑，为数字人应用的进一步发展提供了有价值的资源与未来研究方向建议。

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [20] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

TL;DR: 提出OccTENS（基于TENS的生成式占据模型）与TensFormer及位姿聚合策略，解决效率、时序退化与可控性问题，提升占据质量与推理速度。


<details>
  <summary>Details</summary>
Motivation: 目标是解决现有基于自回归方法在占据世界模型中存在的效率低、长时序退化与可控性差的问题，同时保持对细粒度3D几何与动态演化的高保真建模。

Method: 方法上，作者设计了TensFormer结构，结合空间尺度分解和时间场景预测来管理时序因果与空间关系；并提出整体位姿聚合策略，将占据表示与自车运动统一序列建模以增强位姿可控性。

Result: 实验证明OccTENS在占据质量和推理速度上均优于现有最先进方法，达成更高质量长时序预测且推理更快。

Conclusion: OccTENS提出了一种基于TENS（temporal next-scale prediction）任务的生成式占据世界模型，通过将时间序列建模分解为空间的逐级生成与场景的逐帧预测，实现了高效、可控且高保真度的长时序占据图生成。

Abstract: In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [21] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 通过视-语伪标注与像素对比蒸馏功能与空间信息，提出弱监督方法实现跨类稠密功能对应，且在合成/真实数据上优于基线。


<details>
  <summary>Details</summary>
Motivation: 跨类别匹配困难且结构外观差异大，但物体的功能性部件在形状和外观上具有相似性，功能信息能指导建立更有意义的对应。

Method: 利用视觉-语言模型对多视角图像进行伪标注以提取功能性部件，结合像素级稠密对比学习蒸馏空间和功能信息，构建新的模型用于预测稠密功能对应。

Result: 在合成与真实数据集的基准测试中，新方法优于基线（自监督表征与已接地的视觉-语言模型），显示在功能性对应任务上的优势。

Conclusion: 我们提出基于功能的稠密对应定义，并通过弱监督学习结合视觉-语言模型伪标签和像素级对比学习，训练能在跨类场景下建立功能性稠密对应的模型。

Abstract: Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [22] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: 提出了一种在线少样本学习框架Attn-Adapter，包含Memory Attn-Adapter和Local-Global Attn-Adapter两部分，分别增强类别和图像嵌入，实现无需重训练即可从少量样本动态适配，提升跨类和跨数据集泛化。


<details>
  <summary>Details</summary>
Motivation: 为解决基于提示学习的离线微调计算昂贵且易过拟合的问题，提出一种在线、轻量且高效的少样本适配方法。

Method: 设计了双注意力机制：Memory Attn-Adapter利用支持样本细化类别嵌入；Local-Global Attn-Adapter融合局部与全局特征增强图像表示，且在在线场景下运行无需更新CLIP参数。

Result: Attn-Adapter在少样本场景中通过在线适配提升CLIP性能，减少离线微调和过拟合风险

Conclusion: Attn-Adapter在保持推理效率和可扩展性的同时，显著提高了CLIP在少样本下的性能，优于现有最先进方法。

Abstract: Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [23] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 提出SPECS：对CLIP进行特异性增强的参考无关相似性度量，兼顾与人工评价的一致性和计算效率，适合长图像描述的迭代评估。


<details>
  <summary>Details</summary>
Motivation: 现有评测指标两极化：传统n-gram指标计算高效但无法反映语义正确性；基于表征相似性的RS方法虽能捕捉语义但过去计算代价高且目前与人工评价相关性不佳；基于大型语言模型的度量相关性好但成本高，不利于迭代开发。研究动机为在保持效率的前提下提升与人工评价的相关性，尤其针对长描述中的细节正确性。

Method: 在CLIP的表示相似性框架上，设计并训练一个强调“特异性”的判别目标：鼓励模型捕捉并放大正确细节的表示差异，抑制和惩罚对错误细节或模糊描述的高相似度评分。SPECS作为参考无关（reference-free）的RS指标，通过该目标调整后用于对长图像描述进行评分。

Result: 实验显示SPECS在与人工评价的相关性上能与开源LLM度量相当，同时在计算效率上显著优于LLM方法，适合用于训练/验证过程中的迭代评估。作者提供了代码实现。

Conclusion: 该论文提出了一种针对长图像描述的参考无关度量SPECS，通过在CLIP基础上引入以特异性为导向的新目标，增强对正确细节的奖励并对错误细节进行惩罚，从而提高与人工评价的一致性。作者声称SPECS在与人类判断的相关性上能匹配开源LLM度量，同时在计算效率上占优，使其适合在模型开发迭代过程中使用。

Abstract: As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [24] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

TL;DR: ChexGen是一个在96万胸片-报告数据上预训练的潜在扩散Transformer生成模型，支持文本/掩码/边界框引导合成胸片。合成质量高，能用于数据增广与预训练，提升小样本下游任务性能并改善公平性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像标注昂贵且数据多样性不足，限制可靠医疗AI模型的开发；因此希望利用生成式基础模型合成多样、可控的医学影像以缓解数据稀缺并提升模型公平性。

Method: 基于潜在扩散Transformer架构进行训练，模型支持文本、掩码和边界框引导的合成。使用96万对胸片-报告对的大规模数据集进行预训练，并通过专家评估与定量指标验证合成质量。演示了在少量标签数据下通过数据增广和监督预训练提升分类、检测、分割任务性能。

Result: 模型在专家评估和量化指标上显示出准确的胸片合成能力；通过生成数据进行数据增广和预训练，在下游分类、检测、分割任务中提高了性能，并能生成多样患者队列以检测和缓解人口统计偏差，改善模型公平性。

Conclusion: ChexGen展示了生成式视觉-语言基础模型在胸部X光影像合成方面的可行性，能生成高质量、可控的放射影像并在数据增广、预训练和公平性方面提升下游任务表现。

Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>


### [25] [LMVC: An End-to-End Learned Multiview Video Coding Framework](https://arxiv.org/abs/2509.03922)
*Xihua Sheng,Yingwen Zhang,Long Xu,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出一种端到端学习的多视角视频编码方法，通过利用独立视图的运动与内容特征来提升相关视图的压缩，显著优于传统MV-HEVC。


<details>
  <summary>Details</summary>
Motivation: 解决多视角视频在体积视频重建中带来的巨大存储与传输开销，通过端到端学习的多视角视频编码提升压缩效率，同时兼顾随机访问和向后兼容性。

Method: 基于独立视图的已解码运动特征进行视间运动向量预测并用视间运动熵模型建模；基于已解码内容特征进行无视差的视间上下文预测并用视间上下文熵模型建模，整体为端到端学习框架。

Result: 提出LMVC框架，包含基于特征的视间运动矢量预测、视间运动熵模型、无视差的视间上下文预测模块和视间上下文熵模型，在实验中显著优于MV-HEVC参考软件。

Conclusion: LMVC有效利用视间运动与内容相关性，提升多视角视频压缩效率并保持随机访问和向后兼容，能作为该领域的强基线。

Abstract: Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.

</details>


### [26] [TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes](https://arxiv.org/abs/2509.03938)
*Minghui Zhang,Yaoyu Liu,Junyang Wu,Xin You,Hanxiao Zhang,Junjun He,Yun Gu*

Main category: cs.CV

TL;DR: TopoSculpt通过整体建模、TIB约束与分阶段持久同调细化，从全局角度有效修复3D管状解剖结构的拓扑与几何缺陷，实验显示大幅改善连通性与分支检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素重叠的评价无法反映拓扑正确性，且补充的拓扑损失或持久同调方法多为局部/patch级别，无法保证全局拓扑保持或在推理时修复几何错误。为此需要一种全局、可在推理阶段纠错的拓扑细化框架。

Method: TopoSculpt采用全区建模以获得完整空间上下文；引入Topological Integrity Betti (TIB)约束，联合强制Betti数先验与全局结构完整性；并设计从粗到细的课程化细化流程，利用持久同调在不同尺度逐步纠正拓扑错误。

Result: 在肺气道与Circle of Willis数据集上取得显著提升：例如气道数据集的β0错误从69.00降至3.40，CoW数据集从1.65降至0.30，树长检测与分支检测率均提高近10%。

Conclusion: 本文提出的TopoSculpt通过整体建模、Betti数先验与全局完整性约束（TIB），以及基于持久同调的分阶段细化，有效修正3D管状解剖结构的拓扑与几何错误，在气道和脑动脉数据集上显著降低了连通分量误差并提升了树长和分支检测率。

Abstract: Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.

</details>


### [27] [Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture](https://arxiv.org/abs/2509.03950)
*Alvaro Aranibar Roque,Helga Sebastian*

Main category: cs.CV

TL;DR: 提出基于EfficientNet-B4编码器的U-Net分割管道，在公开数据集上表现良好，能辅助放射科医生检测气胸。


<details>
  <summary>Details</summary>
Motivation: 提高胸片中气胸检测和定位的自动化准确性，辅助放射科医生，尤其是小而隐匿的气胸病例。

Method: 使用U-Net架构，EfficientNet-B4作为编码器；在SIIM-ACR数据上进行训练，采用数据增强和二元交叉熵+Dice混合损失函数；在PTX-498独立数据集上评估IoU和Dice指标。

Result: 在SIIM-ACR数据集训练并在独立PTX-498数据集上测试，U-Net结合EfficientNet-B4编码器、数据增强和交叉熵+Dice混合损失，取得IoU 0.7008和Dice 0.8241，显示能准确定位气胸区域。

Conclusion: 该深度学习分割模型在独立测试集上表现稳定，有潜力作为放射科工作流程中的辅助工具，但仍需在更多临床场景和潜在泛化性上进一步验证。

Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.

</details>


### [28] [ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection](https://arxiv.org/abs/2509.03951)
*Zhu Wenjie,Zhang Yabin,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: ANTS uses multimodal LLMs to describe likely-OOD images and reason about visually similar ID classes to create adaptive negative textual spaces, improving both far- and near-OOD detection without training; achieves SOTA and is zero-shot.


<details>
  <summary>Details</summary>
Motivation: Existing NL-based OOD methods lack image-level OOD understanding and suffer false negatives, harming near-OOD detection; leveraging MLLMs can describe negative images and reason about visually-similar ID classes to generate better negative labels.

Method: Use multimodal LLMs to generate adaptive negative textual space for OOD detection

Result: Proposed ANTS constructs expressive negative sentences for far-OOD and visually-similar negative labels for near-OOD, with an adaptive weighted scoring to balance both. On ImageNet, reduces FPR95 by 4.2%, training-free and zero-shot.

Conclusion: Shaping adaptive negative textual spaces with MLLMs reduces false negatives and enhances OOD detection across near/far scenarios via descriptive negative sentences, visually-aligned negative labels, and adaptive weighting—scalable and training-free.

Abstract: The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.

</details>


### [29] [Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection](https://arxiv.org/abs/2509.03961)
*Yijun Zhou,Yikui Zhai,Zilu Ying,Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Xiaolin Tian,Xudong Jia,Hongsheng Zhang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: MMChange通过IFR、TDE与ITFF三大模块将图像与VLM生成的文本深度融合，从而实现更准确且鲁棒的遥感变化检测，实验证明优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 单纯依赖影像模态在特征表示、变化语义建模及对光照和噪声的鲁棒性方面存在局限，故引入文本模态补充语义信息以增强性能与泛化能力。

Method: 引入IFR模块对图像特征进行关键区域突出与噪声抑制；利用VLM为双时相影像生成语义描述，随后用TDE模块捕捉细粒度语义变化；设计ITFF模块实现深层图文特征融合，从而联合训练实现多模态变化检测。

Result: 在LEVIRCD、WHUCD、SYSUCD三数据集上，MMChange在多项指标上均优于现有最先进方法，证明了多模态方案在遥感变化检测中的有效性与稳健性。

Conclusion: 该文提出将图像与文本多模态结合用于遥感变化检测，通过图像特征精炼、文本差异增强及跨模态融合提升检测性能与鲁棒性。

Abstract: Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.

</details>


### [30] [SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2509.03973)
*Yu Bai,Zitong Yu,Haowen Tian,Xijing Wang,Shuo Yan,Lin Wang,Honglin Li,Xitong Ling,Bo Zhang,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: cs.CV

TL;DR: SAC-MIL用坐标位置编码和MLP式SAC块在保持线性复杂度和易部署性的同时建模WSI内实例的空间相关性，改善了WSI分类效果并在多个数据集上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是提升基于MIL的WSI分类性能，正确建模实例之间的空间关系并保持可扩展性和部署便利，克服现有Transformer方法在长序列计算和部署上的限制。

Method: 方法包括两个关键模块：1) 位置编码模块：基于切片内实例坐标而非序列索引编码空间关系，支持训练/测试序列长度外推；2) SAC块：基于MLP的全实例相关计算，时间复杂度为O(N)，避免了Transformer需要自定义CUDA核的问题，易于部署。

Result: 在CAMELYON-16、TCGA-LUNG和TCGA-BRAC三个数据集上实现了state-of-the-art性能；代码将在论文接受后发布。

Conclusion: 本文提出的SAC-MIL通过位置编码和SAC模块实现WSI（整片切片）分类时的实例间空间相关建模，解决了位置表示和可变序列长度的问题，并以线性复杂度实现全实例相关计算，取得了若干数据集上的最好结果。

Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.

</details>


### [31] [Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training](https://arxiv.org/abs/2509.03975)
*Daniel Sobotka,Alexander Herold,Matthias Perkonigg,Lucian Beer,Nina Bastati,Alina Sablatnig,Ahmed Ba-Ssalamah,Georg Langs*

Main category: cs.CV

TL;DR: Train segmentation model on native MRI using auxiliary contrast-enhanced images available only during training to boost performance and reduce annotation needs.


<details>
  <summary>Details</summary>
Motivation: Vessel segmentation in native (non-contrast) liver MRI is challenging and annotated data are scarce; contrast-enhanced scans provide clearer vessel information but are not always acquired at inference.

Method: Multi-task learning framework leveraging paired native and contrast-enhanced MRI; train with both annotated and unannotated pairs, using auxiliary modality only at training time.

Result: Auxiliary contrast-enhanced data improved segmentation accuracy, especially with few annotations; benefit validated on brain tumor segmentation task as well.

Conclusion: Using auxiliary contrast-enhanced MRI during training improves vessel segmentation on non-contrast liver MRI and reduces need for annotations.

Abstract: Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.

</details>


### [32] [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
*Mohamed Insaf Ismithdeen,Muhammad Uzair Khattak,Salman Khan*

Main category: cs.CV

TL;DR: Promptception: 61 prompt types to evaluate LMM prompt sensitivity in MCQA; proprietary models more prompt-sensitive; leads to proposed principles for fairer, robust evaluation.


<details>
  <summary>Details</summary>
Motivation: Minor prompt variations cause large accuracy swings, undermining transparent and fair evaluation of LMMs; need systematic assessment of prompt sensitivity.

Method: Design 61 prompt types across 15 categories and 6 supercategories; evaluate 10 LMMs (open-source to GPT-4o/Gemini 1.5 Pro) on 3 benchmarks (MMStar, MMMU-Pro, MVBench); measure accuracy variations and sensitivity patterns.

Result: Found up to 15% accuracy deviations due to prompt phrasing; proprietary models show higher sensitivity to phrasing, open-source models more stable but struggle with nuanced phrasing. Proposed Prompting Principles for both model types to improve robustness and fairness.

Conclusion: Prompt formulation significantly affects LMM performance in MCQA; proprietary models are more sensitive while open-source models are steadier but weaker on nuance. Promptception provides comprehensive evaluation framework and guiding principles for robust evaluation.

Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.

</details>


### [33] [SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation](https://arxiv.org/abs/2509.03999)
*Han Huang,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen*

Main category: cs.CV

TL;DR: 提出基于高度切片的多模态3D占据表示（SliceSemOcc），引入全局/局部切片融合与高度感知SEAttention3D，显著提升mIoU、特别是小物体类别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理体素特征时忽视高度轴信息；传统的SENet通道注意力对所有高度层赋予相同权重，无法强调不同高度的特征，限制了对竖直方向语义变化的建模能力。

Method: 提出SliceSemOcc框架：沿高度轴提取全局和局部垂直切片特征，并用全局-局部融合模块整合细粒度空间与全局上下文；提出SEAttention3D模块，通过保持高度分辨率的池化并对每个高度层分配动态通道注意力。

Result: 在nuScenes-SurroundOcc和nuScenes-OpenOccupancy数据集上取得明显mIoU提升，尤其在多数小物体类别上表现突出；消融实验验证了各模块的有效性。

Conclusion: 该论文通过引入高度切片和高度敏感通道注意力，有效提升了3D语义占据预测的性能，尤其对小目标类别增益显著。

Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.

</details>


### [34] [Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding](https://arxiv.org/abs/2509.04009)
*Solha Kang,Esla Timothy Anzaku,Wesley De Neve,Arnout Van Messem,Joris Vankerschaver,Francois Rameau,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文提出并验证了一种面向视觉Transformer的虚假相关检测方法，揭示训练方式影响模型依赖，并公开受污染的ImageNet图像与医疗案例分析。


<details>
  <summary>Details</summary>
Motivation: 神经网络可能利用数据中非预期、统计上相关但语义上不合理的模式来完成预测，导致模型不可靠、不可泛化，尤其在关键领域（如医学）风险更高，因此识别并规避这类虚假相关对提升模型可信性和安全性至关重要。

Method: 针对视觉Transformer设计检测流程，利用模型在不同区域或特征上的关注/激活信息（attention/feature attribution）来识别与类别预测强相关但语义上不合理的图像区域或信号；在有监督和自监督模型上统一评估，并结合统计检验与可视化手段确认候选虚假信号；整理并公开易受虚假信号影响的ImageNet图像清单；在医疗图像分类任务中复现并分析虚假信号对决策的影响。

Result: 提出方法能在视觉Transformer上成功检测出多种虚假相关信号（如颜色、文本、水印、小物体等），并发现自监督/监督训练会显著改变模型对这些信号的依赖程度；公开了含明显虚假信号的ImageNet类别与图像清单；案例研究显示在乳腺肿块分类中类似问题会导致模型过度依赖无关标记，从而威胁临床可靠性。

Conclusion: 本文提出了一种用于检测视觉Transformer中虚假相关（spurious correlations）的方法，并在ImageNet上进行大规模实验，验证了方法有效性。研究还发现训练方式（监督学习 vs 自监督）对模型依赖虚假相关的程度有显著影响，并列举了ImageNet中含有明显虚假信号的类别与图像，最后通过侵袭性乳腺肿块分类案例研究，将方法应用于真实医疗场景。

Abstract: Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.

</details>


### [35] [Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning](https://arxiv.org/abs/2509.04023)
*Shiku Kaito,Shinnosuke Matsuo,Daiki Suehiro,Ryoma Bise*

Main category: cs.CV

TL;DR: 本文提出LML问题并用Counting Network+MPEM解决，通过提高袋内多数类比例来改善实例分类，实验和消融验证了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中仅能获得袋级多数类标签，目标是利用多数标签训练出实例级分类器，如病理图像分割、投票预测、情感分析和环境监测。

Method: 设计Counting Network以统计每类实例数并预测袋级多数类标签；提出Majority Proportion Enhancement Module(MPEM)通过移除袋内少数类实例来提高多数类比例；在四个数据集上与常规MIL方法比较并进行消融实验。

Result: 在四个数据集上，提出的方法优于传统MIL方法；消融研究证明Counting Network和MPEM各自有效；分析显示高多数类比例的袋更利于学习。

Conclusion: 提出了学习多数标签（LML）的新型多类多实例学习问题，并提出Counting Network和MPEM模块，有效提升多数类比例与性能，实验证明优于传统MIL方法。

Abstract: The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.

</details>


### [36] [Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"](https://arxiv.org/abs/2509.04043)
*Yuchen Zhu,Longxiang Yin,Kai Zhao*

Main category: cs.CV

TL;DR: 提出基于国产飞腾CPU与寒武纪MLU的异构加速方案，结合YOLOv5s+DeepSORT实现无人机监控系统，达到50-100 ms延迟和>98.5%识别率。


<details>
  <summary>Details</summary>
Motivation: 传统摄像头系统在动态场景中因深度特征提取能力不足及计算架构效率瓶颈导致超过200 ms的响应延迟，无法满足复杂场景的实时性要求。

Method: 硬件上采用飞腾FT-2000/4 CPU与MLU220加速卡的协同计算，多卡并行以增强算力；软件上结合轻量化YOLOv5s检测网络与DeepSORT级联跟踪算法，构建“检测-跟踪-反馈”闭环控制流程。

Result: 在1920×1080视频流处理下，系统单帧综合处理延迟稳定在50-100 ms，目标识别准确率超过98.5%，兼具低时延与高精度。

Conclusion: 该文提出并实现了一种基于飞腾处理器和寒武纪加速卡的异构计算架构，用于无人机目标追踪与凝视系统，显著降低了视频监控场景下的响应延迟，实现毫秒级响应能力。

Abstract: In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.

</details>


### [37] [A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification](https://arxiv.org/abs/2509.04050)
*Quang-Huy Che,Le-Chuong Nguyen,Gia-Nghia Tran,Dinh-Duy Phan,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: 通过KWF无监督聚合邻居生成多视图特征，实现无需微调的高效重排序，显著提升行人重识别Rank@1和mAP，尤其在MSMT17和Occluded-DukeMTMC上效果突出。


<details>
  <summary>Details</summary>
Motivation: 单视图特征易受视角、姿态和遮挡等影响导致视图偏差，利用邻居的多视图特征能缓解视图偏差并提高排序鲁棒性且避免额外标注或模型微调。

Method: 提出K-nearest Weighted Fusion (KWF)：对初始检索结果的前M候选，基于无监督选取的K个邻居特征按设计的权重策略加权融合生成多视图特征，再基于这些多视图特征进行重排序。探索多种权重选择策略以确定最优方案。

Result: 在Market1501、MSMT17和Occluded-DukeMTMC上验证，重排序对初始Top-M候选效果明显提升；在MSMT17和Occluded-DukeMTMC的Rank@1分别提高9.8%和22.0%，同时计算效率优于其他方法。

Conclusion: 本文提出通过KNN加权聚合邻居特征生成多视图特征的重排序方法，可在不需微调和额外标注的前提下提升行人重识别性能，特别在MSMT17和Occluded-DukeMTMC上Rank@1获得显著提升，且计算效率高。

Abstract: In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.

</details>


### [38] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 结合BiT（语义注入与校准）与CATS（按类别的时间图传播），抑制伪标签噪声放大，取得LLP与UnAV-100上的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么强化时间建模架构，要么生成更丰富的伪标签，但都存在将噪声伪标签作为可靠监督或让注意力无差别扩散噪声的问题，导致训练过程中初始错误被放大。本文动机是融合两类方法的优点并设计机制抑制噪声放大。

Method: 提出的框架先用BiT模块在音频与视觉特征间进行语义注入与动态校准，以定位并净化更清晰、更丰富的语义线索；随后用CATS模块进行语义传播与连接，按类别感知地在时间维度上传播精确语义信息，避免噪声扩散。

Result: 在LLP与UnAV-100两个基准数据集上的多个关键指标上均达到了或超过当前SOTA性能，验证了所提方法在抑制伪标签噪声、提升时序定位与分类准确性方面的有效性。

Conclusion: 本论文通过结合Bi-Directional Text Fusion (BiT)与Category-Aware Temporal Graph (CATS)模块，有效缓解了弱监督音视视频解析中伪标签噪声的放大问题，从而在LLP和UnAV-100数据集上取得了SOTA性能。

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [39] [TriLiteNet: Lightweight Model for Multi-Task Visual Perception](https://arxiv.org/abs/2509.04092)
*Quang-Huy Che,Duc-Khai Lam*

Main category: cs.CV

TL;DR: TriLiteNet为ADAS提供了一个参数极少、计算量低且能同时完成三项关键感知任务的可部署多任务模型。


<details>
  <summary>Details</summary>
Motivation: 为满足ADAS对实时性和低计算资源的需求，设计一个在嵌入式设备上可运行的高效多任务感知模型。

Method: 提出轻量级多任务网络TriLiteNet（包括base和tiny两个配置），在单一模型中同时处理车辆检测、可行驶区域和车道线分割，并在参数量和GFLOPs上进行优化以降低延迟与能耗。

Result: 在BDD100k数据集上，TriLiteNet_base具有85.6%车辆检测召回、92.4%可行驶区域mIoU、82.3%车道线分割准确率；参数量仅2.35M、计算量7.72 GFLOPs，并提供0.14M参数的tiny版本；在嵌入式设备上延迟低、功耗可控。

Conclusion: TriLiteNet在多任务全景驾驶感知中实现了在低计算成本下的可竞争性能，适合嵌入式ADAS部署。

Abstract: Efficient perception models are essential for Advanced Driver Assistance
Systems (ADAS), as these applications require rapid processing and response to
ensure safety and effectiveness in real-world environments. To address the
real-time execution needs of such perception models, this study introduces the
TriLiteNet model. This model can simultaneously manage multiple tasks related
to panoramic driving perception. TriLiteNet is designed to optimize performance
while maintaining low computational costs. Experimental results on the BDD100k
dataset demonstrate that the model achieves competitive performance across
three key tasks: vehicle detection, drivable area segmentation, and lane line
segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of
85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for
drivable area segmentation, and an Acc of 82.3% for lane line segmentation with
only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed
model includes a tiny configuration with just 0.14M parameters, which provides
a multi-task solution with minimal computational demand. Evaluated for latency
and power consumption on embedded devices, TriLiteNet in both configurations
shows low latency and reasonable power during inference. By balancing
performance, computational efficiency, and scalability, TriLiteNet offers a
practical and deployable solution for real-world autonomous driving
applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

</details>


### [40] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是面向行人检测与过街意图分析的事件相机数据集，融合CARLA合成与v2e转换的JAAD真实数据，提供事件帧、RGB和原始事件文件；基线SNN实验显示sim-to-real差距，鼓励域自适应与多模态融合研究。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机低延迟、高动态范围和运动鲁棒性的优势，促进在复杂天气与光照条件下的行人检测与过街意图预测研究，同时弥补缺乏此类领域特定事件数据集的空白。

Method: 数据集由两部分组成：1) 在CARLA中生成各种天气/光照下的受控“靠近-过街”场景的合成事件流；2) 使用v2e将JAAD行车记录视频转换为事件流以保留自然行为与背景。每序列包含RGB帧、33ms累积的事件帧、逐帧过街标签，并提供AEDAT原始文件与AVI DVS视频与元数据。作者用SpikingJelly实现基线SNN以验证数据集可用性。

Result: 发布了包含合成与真实来源的多模态DVS-PedX数据集、相应原始事件文件与标签，并基于SpikingJelly提供了基线SNN结果，实验揭示了从仿真到真实的性能下降，提示需研究领域自适应与多模态方法。

Conclusion: 作者构建了一个针对行人检测与过街意图分析的事件相机数据集DVS-PedX，包含合成（CARLA）与真实（JAAD→v2e）两类事件流，提供配对RGB、事件帧、原始事件文件及标签，便于重处理。基准SNN实验显示存在sim-to-real差距，强调领域自适应与多模态融合的必要性。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [41] [TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering](https://arxiv.org/abs/2509.04123)
*Ayan Banerjee,Josep Lladós,Umapada Pal,Anjan Dutta*

Main category: cs.CV

TL;DR: TaleDiffusion提出通过LLM生成逐帧描述、基于box的有界注意力掩码、身份一致自注意力和区域感知交叉注意力，并用CLIPSeg做对话气泡分配，从而解决多角色跨帧一致性和对话渲染问题，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多角色跨帧一致性和对话渲染上表现较差，导致画面伪影和故事性不连贯，需一种能保证角色一致性和准确对话分配的新框架。

Method: 先用预训练大模型通过in-context learning生成逐帧描述、角色细节和对话；采用基于box的有界注意力掩码控制角色交互并减少伪影；引入身份一致的自注意力保证跨帧角色一致性；使用区域感知交叉注意力精确放置对象；最后通过CLIPSeg进行对话气泡检测并后处理将对话分配给角色。

Result: 在多项实验中，TaleDiffusion在角色一致性、噪声抑制和对话渲染指标上均优于现有方法，生成的故事连贯且对话位置与角色匹配度更高。

Conclusion: TaleDiffusion在多角色故事生成中显著改善了角色一致性、降低伪影并提高对话渲染准确性，适用于需跨帧一致交互的文本到故事可视化任务。

Abstract: Text-to-story visualization is challenging due to the need for consistent
interaction among multiple characters across frames. Existing methods struggle
with character consistency, leading to artifact generation and inaccurate
dialogue rendering, which results in disjointed storytelling. In response, we
introduce TaleDiffusion, a novel framework for generating multi-character
stories with an iterative process, maintaining character consistency, and
accurate dialogue assignment via postprocessing. Given a story, we use a
pre-trained LLM to generate per-frame descriptions, character details, and
dialogues via in-context learning, followed by a bounded attention-based
per-box mask technique to control character interactions and minimize
artifacts. We then apply an identity-consistent self-attention mechanism to
ensure character consistency across frames and region-aware cross-attention for
precise object placement. Dialogues are also rendered as bubbles and assigned
to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion
outperforms existing methods in consistency, noise reduction, and dialogue
rendering.

</details>


### [42] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: MEPG 用位置-风格感知的 LLM 解读提示并结合多专家跨区域路由生成图像，显著提升复杂提示下的布局控制和风格多样性，同时具备可扩展的专家模块和交互式编辑功能。


<details>
  <summary>Details</summary>
Motivation: 现有文本-图像扩散模型在处理复杂多目标提示、精确布局控制和多风格生成时表现不足，缺乏对位置和风格的联合建模与可扩展的专家协同机制。

Method: 框架包含两个核心模块：1) PSA 模块：通过监督微调的 LLM 将输入提示分解为精确的空间坐标和编码风格的语义指令；2) MED 模块：通过跨区域的专家路由实现生成，在局部与全局区域间动态选择专家（如现实感专家、风格化专家），并用基于注意力的门控机制激活对应专家。

Result: 实验表明，MEPG 在相同主干模型下，比基线在图像质量和风格多样性上有显著提升；系统还支持实时布局编辑和按区域风格选择。

Conclusion: MEPG 提出了一种将位置-风格感知的大语言模型与可插拔的空间-语义专家模块结合的框架，以解决文本到图像扩散模型在复杂多项提示和风格多样性方面的不足。

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.

</details>


### [43] [Revisiting Simple Baselines for In-The-Wild Deepfake Detection](https://arxiv.org/abs/2509.04150)
*Orlando Castaneda,Kevin So-Tang,Kshitij Gurung*

Main category: cs.CV

TL;DR: 在真实场景基准Deepfake-Eval-2024上，简单的微调预训练视觉模型经适当调参可将准确率提升到81%，接近商业水平，强调超参数对泛化性能的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测多在受控数据集上评估，实用性不足。Deepfake-Eval-2024提供了真实场景基准，初始基线表现较差，亟需验证和改进开源方法以提升可用性。

Method: 复现并改进Ojha等人的方法：对预训练视觉骨干（如ViT/ResNet）进行微调，搜索并优化关键超参数（学习率、权重衰减、数据增强、训练步数、冻结策略等），在Deepfake-Eval-2024数据集上训练并评估模型。

Result: 通过更合理的超参数选择，基线模型在Deepfake-Eval-2024上准确率从原报告的~63%提高到81%，超出原报告18%，与领先商业检测器（82%）相当。同时分析了精度、计算成本和可解释性之间的权衡。

Conclusion: 该论文指出通过更好调参，使用预训练视觉骨干的简单基线方法可以显著提升in-the-wild深度伪造检测性能，达到与商业检测器相当的准确率。

Abstract: The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.

</details>


### [44] [YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components](https://arxiv.org/abs/2509.04156)
*Serhii Svystun,Pavlo Radiuk,Oleksandr Melnychenko,Oleg Savenko,Anatoliy Sachenko*

Main category: cs.CV

TL;DR: 通过将YOLOv8与专用热成像YOLO集成并用边界框融合，利用可见光与热通道互补性，检测性能（mAP和F1）优于单模型。


<details>
  <summary>Details</summary>
Motivation: 无人机搭载高分辨率多光谱传感器可对风电设备进行高效巡检，但单一光谱或单模型方法在检测热异常或细微的视觉缺陷时存在局限，故尝试通过多模型多模态融合提升检测性能。

Method: 提出将通用YOLOv8模型与专门的热成像YOLO模型并行训练与预测，采用精细化的边界框融合策略（基于置信度加权与IoU阈值的NMS改进）合并检测结果，从而利用可见光和热通道的互补信息。

Result: 在数据集与实验中，融合模型达到了mAP@.5=0.93和F1=0.90，优于单一YOLOv8（mAP@.5=0.91），表明融合策略在检测视觉和热成分上的缺陷具有更高的召回与精确率。

Conclusion: 结合多模态热成像与可见光数据、并通过YOLO系列模型的集成与边界框融合算法，能显著提升风电场关键设备缺陷检测的准确率与鲁棒性。

Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.

</details>


### [45] [VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision](https://arxiv.org/abs/2509.04180)
*Safouane El Ghazouali,Umberto Michelucci*

Main category: cs.CV

TL;DR: VisioFirm是一个集成CLIP、Grounding DINO、Ultralytics和SAM的开源浏览器端半自动标注工具，通过低置信候选、CLIP聚类与IoU抑制冗余、WebGPU加速分割，实现高召回初始标注并将人工工作量显著降低（可达90%）。


<details>
  <summary>Details</summary>
Motivation: 现有标注工具需大量人工操作，难以在大规模数据集上扩展，作者希望通过将基础模型与交互界面结合，降低人工投入并提升标注效率与可达性。

Method: 系统架构将预训练检测器与CLIP检索和零样本定位模型组合，采用低阈值生成初始候选，再用CLIP向量聚类和IoU图去重，提供交互式编辑（bbox、obb、多边形）及在浏览器端通过WebGPU调用SAM进行细粒度分割，支持多种导出格式并实现本地缓存离线运行。

Result: 在COCO类数据集和多样化数据集基准测试中，VisioFirm宣称可将人工标注工作量最多降低约90%，初始预测多数正确且允许用户高效修正，支持多种格式导出并能离线运行。

Conclusion: VisioFirm提出了一种将多种大模型（CLIP、Grounding DINO、Ultralytics、Segment Anything）整合到浏览器端标注流程的混合半自动化工具，通过低置信度召回、CLIP聚类与IoU图抑制冗余检测、以及WebGPU加速的即时分割，显著减少人工标注工作量并保持高准确性。

Abstract: AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.

</details>


### [46] [DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval](https://arxiv.org/abs/2509.04193)
*Ruohong Yang,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 提出DUDE：借助文本-图像生成模型进行风格与语义解耦，并通过渐进式互邻对齐实现无监督跨域图像检索的性能提升，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接对整张图像特征进行对齐，导致被域特有风格干扰，检索时对象语义信息被混淆，影响跨域检索性能。为此需要将语义对象与域风格分离以获得更稳健的检索特征。

Method: 利用文本到图像生成模型对图像特征进行风格-语义解耦，提取与类别相关的对象特征；其次在特征空间中采用从域内到域间的渐进式互为邻居对齐策略，保证解耦后对象特征的可靠对齐；训练与检索过程中无需标签。

Result: 在三个基准数据集、覆盖13个域的设置下，DUDE在无监督跨域图像检索任务上达到了最新的最优性能，实验包括消融研究和对比方法验证。

Conclusion: DUDE通过将对象特征与领域风格分离并进行渐进式跨域邻居对齐，有效缓解了领域差异，在无监督跨域图像检索任务上显著提升性能。

Abstract: Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of
the same category across diverse domains without relying on annotations.
Existing UCIR methods, which align cross-domain features for the entire image,
often struggle with the domain gap, as the object features critical for
retrieval are frequently entangled with domain-specific styles. To address this
challenge, we propose DUDE, a novel UCIR method building upon feature
disentanglement. In brief, DUDE leverages a text-to-image generative model to
disentangle object features from domain-specific styles, thus facilitating
semantical image retrieval. To further achieve reliable alignment of the
disentangled object features, DUDE aligns mutual neighbors from within domains
to across domains in a progressive manner. Extensive experiments demonstrate
that DUDE achieves state-of-the-art performance across three benchmark datasets
over 13 domains. The code will be released.

</details>


### [47] [Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding](https://arxiv.org/abs/2509.04243)
*Wanfu Wang,Qipeng Huang,Guangquan Xue,Xiaobo Liang,Juntao Li*

Main category: cs.CV

TL;DR: 提出LASER，通过蒙特卡罗与IoU联合的自进化多步感知框架，提高VLM对GUI任务的定位精度与鲁棒性，刷新7B模型基准成绩。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在GUI场景中对合适图像区域的推理不足，尤其面对高分辨率输入与多元素交互时难以精确定位，需增强模型的主动感知与多步检测能力。

Method: 结合蒙特卡罗质量估计与基于IoU的区域质量评估，构建兼顾准确性与多样性的偏好数据，并根据任务复杂度自适应分配推理步数，实现多步放大搜索与精确坐标回归。

Result: 在ScreenSpot Pro和ScreenSpot-v2上取得持续提升；在GTA1-7B微调后于ScreenSpot-Pro上达55.7分，成为7B规模模型的新的SoTA。

Conclusion: LASER能通过自进化、多步感知和坐标预测显著提升VLM在GUI定位任务的表现，尤其在高分辨率与复杂场景下有效。

Abstract: Vision Language Models (VLMs) have recently achieved significant progress in
bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model
introduced a zoom-in search strategy that effectively elicits active perception
capabilities in VLMs, improving downstream task performance. However, enabling
VLMs to reason effectively over appropriate image regions remains a core
challenge in GUI grounding, particularly under high-resolution inputs and
complex multi-element visual interactions. In this work, we propose LASER, a
self-evolving framework that progressively endows VLMs with multi-step
perception capabilities, enabling precise coordinate prediction. Specifically,
our approach integrate Monte Carlo quality estimation with
Intersection-over-Union (IoU)-based region quality evaluation to jointly
encourage both accuracy and diversity in constructing high-quality preference
data. This combination explicitly guides the model to focus on
instruction-relevant key regions while adaptively allocating reasoning steps
based on task complexity. Comprehensive experiments on the ScreenSpot Pro and
ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating
the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER
achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new
state-of-the-art (SoTA) among 7B-scale models.

</details>


### [48] [Differential Morphological Profile Neural Networks for Semantic Segmentation](https://arxiv.org/abs/2509.04268)
*David Huangal,J. Alex Hurt*

Main category: cs.CV

TL;DR: 把DMP作为单独编码流与RGB融合，比直接把DMP当作输入通道更能提升遥感语义分割性能，能在多个指标上超越不使用DMP的模型。


<details>
  <summary>Details</summary>
Motivation: 现有分割网络多为地面视角图像设计，未专门处理遥感影像的极端尺度变化、前后景不平衡和大图像尺寸等问题；DMP能提取多尺度形状信息，有望改善遥感语义分割性能。

Method: 在三种最先进的卷积与Transformer分割架构中集成DMP，采用两种集成方式：直接输入（修改输入stem以接受DMP通道）和混合双流（分别编码RGB和DMP后融合）。在iSAID上评估多种DMP差分算子与结构元素形状以选择最优配置。

Result: 实验表明：非DMP模型通常优于直接输入DMP的变体；但混合双流DMP模型稳健地优于直接输入方式，并且可以在mIoU、F1和Recall指标上超越无DMP基线，显示DMP在补充形状信息上有实际价值。

Conclusion: 将差分形态学谱（DMP）特征融合到语义分割网络中能在遥感高空影像场景提供有益的形状信息，混合双流架构比直接把DMP作为额外输入更有效，且在iSAID数据集上能在mIoU、F1、Recall上超过非DMP基线。

Abstract: Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.

</details>


### [49] [TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models](https://arxiv.org/abs/2509.04269)
*Yuxin Gong,Se-in Jang,Wei Shao,Yi Su,Kuang Gong*

Main category: cs.CV

TL;DR: 提出一种以血浆p-tau217文本提示和结构MRI为条件的3D扩散模型，能在ADNI数据上合成逼真的tau PET，用于数据增强和疾病进展模拟。


<details>
  <summary>Details</summary>
Motivation: tau PET昂贵且稀缺，而结构性MRI和血浆生物标志物非侵入、易获得，二者可互补提供关于脑解剖与病程的信息，用以合成有临床意义的tau PET图像。

Method: 构建多模态条件的3D扩散生成框架：文本提示来自血浆p-tau217数值，MRI作为空间解剖约束，模型在ADNI的AV1451 tau PET数据上进行训练和评估。

Result: 在ADNI数据集上实验表明该方法可生成现实且具有临床意义的3D tau PET，覆盖不同疾病阶段；可用于数据增强、非侵入性替代可视化和在不同血浆生物标志物水平下的疾病进程仿真。

Conclusion: 该论文提出了一种基于文本引导的3D扩散模型，用于从结构性MRI和血浆p-tau217测量合成3D tau PET影像，旨在缓解tau PET获取困难并支持数据增强与疾病进程模拟。

Abstract: Accurate quantification of tau pathology via tau positron emission tomography
(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).
However, the high cost and limited availability of tau PET restrict its
widespread use. In contrast, structural magnetic resonance imaging (MRI) and
plasma-based biomarkers provide non-invasive and widely available complementary
information related to brain anatomy and disease progression. In this work, we
propose a text-guided 3D diffusion model for 3D tau PET image synthesis,
leveraging multimodal conditions from both structural MRI and plasma
measurement. Specifically, the textual prompt is from the plasma p-tau217
measurement, which is a key indicator of AD progression, while MRI provides
anatomical structure constraints. The proposed framework is trained and
evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that
our approach can generate realistic, clinically meaningful 3D tau PET across a
range of disease stages. The proposed framework can help perform tau PET data
augmentation under different settings, provide a non-invasive, cost-effective
alternative for visualizing tau pathology, and support the simulation of
disease progression under varying plasma biomarker levels and cognitive
conditions.

</details>


### [50] [Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2509.04273)
*Junying Meng,Gangxuan Zhou,Jun Liu,Weihong Guo*

Main category: cs.CV

TL;DR: 提出一种将显式/隐式体积先验和Threshold Dynamics空间正则化融入半监督分割的框架，通过回归预测体积并用图像/数据集尺度Wasserstein损失进行约束，实验证明在多个医学影像数据集上性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有半监督医学图像分割方法在特征提取和利用数据集先验信息方面不足，作者希望通过引入显式和隐式体积先验及空间正则化来提供更有效的引导，从而提高分割性能。

Method: 方法包括：1) 使用回归网络预测每个无标签图像的目标区域体积分布，并通过图像尺度的Wasserstein距离约束，确保分割结果的类别比例与回归预测一致；2) 设计基于弱隐式体积先验的数据集尺度Wasserstein距离损失，约束无标签数据的体积分布与有标签数据相似；3) 将Threshold Dynamics空间正则化与显式体积先验结合进主干分割网络。

Result: 在ACDC 2017、PROMISE12和大腿肌肉MR数据集上进行实验，结果显示所提方法优于基线方法，证明了体积先验与空间正则化在半监督分割中的有效性。

Conclusion: 该论文提出将体积先验和Threshold Dynamics空间正则化从变分模型融入半监督医学图像分割网络中，以提升无标签图像分割的结果稳定性和准确性。

Abstract: Despite signi cant progress in semi-supervised medical image segmentation,
most existing segmentation networks overlook e ective methodological guidance
for feature extraction and important prior information from
  datasets. In this paper, we develop a semi-supervised medical image
segmentation framework that e ectively integrates spatial regularization
methods and volume priors. Speci cally, our approach integrates a strong
explicit volume prior at the image scale and Threshold Dynamics spatial
regularization, both derived from variational models, into the backbone
segmentation network. The target region volumes for each unlabeled image are
estimated by a regression network, which e ectively regularizes the backbone
segmentation network through an image-scale Wasserstein distance constraint,
ensuring that the class ratios in the segmentation results for each unlabeled
image match those predicted by the regression network. Additionally, we design
a dataset-scale Wasserstein distance loss function based on a weak implicit
volume prior, which enforces that the volume distribution predicted for the
unlabeled dataset is similar to that of labeled dataset. Experimental results
on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset
show the superiority of the proposed method.

</details>


### [51] [PAOLI: Pose-free Articulated Object Learning from Sparse-view Images](https://arxiv.org/abs/2509.04276)
*Jianning Deng,Kartic Subr,Hakan Bilen*

Main category: cs.CV

TL;DR: 一种无需相机位姿、仅需稀疏视图（最低4视图）即可学习关节化物体表示的自监督方法：先稀疏视图重建、再学习形变场建立跨位姿对应，渐进解耦静动部件并联合优化几何/外观/运动学，实验表明在弱监督下效果优秀。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集多视角或相机位姿标签，在实际采集场景中难以获得；作者希望在更弱的输入假设下（少量视图、无位姿）仍能学习出高质量的关节化物体表示。

Method: 先对每个位姿使用稀疏视图单独重建形状（利用稀疏视图3D重建技术），然后学习形变场以建立跨位姿的稠密对应；采用渐进式解耦将静态与可动部件分离，联合优化几何、外观与运动学并通过自监督损失约束跨视图与跨位姿一致性。

Result: 在标准基准和真实世界示例上，该方法在输入假设显著更弱的情况下仍能生成准确、细致的关节化物体表示，优于现有需强监督的方法。

Conclusion: 该论文提出了一种自监督框架，可从稀疏视图、无相机位姿监督的图像中学习关节化物体表示，实现了在极弱监督下的准确建模。

Abstract: We present a novel self-supervised framework for learning articulated object
representations from sparse-view, unposed images. Unlike prior methods that
require dense multi-view observations and ground-truth camera poses, our
approach operates with as few as four views per articulation and no camera
supervision. To address the inherent challenges, we first reconstruct each
articulation independently using recent advances in sparse-view 3D
reconstruction, then learn a deformation field that establishes dense
correspondences across poses. A progressive disentanglement strategy further
separates static from moving parts, enabling robust separation of camera and
object motion. Finally, we jointly optimize geometry, appearance, and
kinematics with a self-supervised loss that enforces cross-view and cross-pose
consistency. Experiments on the standard benchmark and real-world examples
demonstrate that our method produces accurate and detailed articulated object
representations under significantly weaker input assumptions than existing
approaches.

</details>


### [52] [Noisy Label Refinement with Semantically Reliable Synthetic Images](https://arxiv.org/abs/2509.04298)
*Yingxuan Li,Jiafeng Mao,Yusuke Matsui*

Main category: cs.CV

TL;DR: 用文本生成的合成图像作为可靠参考来发现并修正语义错标样本，作为预处理与现有噪声鲁棒训练方法结合，可在强语义噪声下大幅提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 语义噪声（visually similar categories经常被错标）对监督学习伤害更大且传统噪声建模方法难以有效区分细粒度相似类别。现代文本到图像生成器能产出高质量、语义准确的图像，提供可信的参考标签信息，因此可用于辅助纠正真实数据的标签错误。

Method: 核心方法是用生成模型合成带有可靠标签的图像，作为“参考样本”（reliable references），通过比较特征相似性和/或模型预测一致性来检测原始训练集中可能被误标的样本，并对其进行纠正或剔除。该方法与现有噪声鲁棒训练技术是正交的，可在先检测/修正标签后与任何噪声鲁棒训练方法结合。

Result: 在多种基准和噪声条件下进行广泛实验：当70%语义噪声时，结合该方法后与最先进噪声鲁棒方法一起在CIFAR-10上提升约30%，CIFAR-100提升约11%；在含有真实噪声的ImageNet-100上提升约24%。总体显著提高了分类准确率，尤其在语义标签噪声极重的情况下效果突出。

Conclusion: 本论文提出利用高质量文本到图像合成图像作为参考点来识别并纠正含有语义噪声的图像分类数据集中的错误标注，从而显著提升分类器在高语义噪声场景下的性能。

Abstract: Semantic noise in image classification datasets, where visually similar
categories are frequently mislabeled, poses a significant challenge to
conventional supervised learning approaches. In this paper, we explore the
potential of using synthetic images generated by advanced text-to-image models
to address this issue. Although these high-quality synthetic images come with
reliable labels, their direct application in training is limited by domain gaps
and diversity constraints. Unlike conventional approaches, we propose a novel
method that leverages synthetic images as reliable reference points to identify
and correct mislabeled samples in noisy datasets. Extensive experiments across
multiple benchmark datasets show that our approach significantly improves
classification accuracy under various noise conditions, especially in
challenging scenarios with semantic label noise. Additionally, since our method
is orthogonal to existing noise-robust learning techniques, when combined with
state-of-the-art noise-robust training methods, it achieves superior
performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100
under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise
conditions.

</details>


### [53] [Efficient Odd-One-Out Anomaly Detection](https://arxiv.org/abs/2509.04326)
*Silvio Chito,Paolo Rabino,Tatiana Tommasi*

Main category: cs.CV

TL;DR: 提出一种基于DINO的高效odd-one-out检测方法，参数与训练时间显著下降，性能仍具竞争力，并评估了多模态LLM基线，揭示其在该任务上的不足。


<details>
  <summary>Details</summary>
Motivation: odd-one-out任务需要跨视图的空间推理与关系推理，且需在效率（参数与训练时间）方面做出改进，以便实用化。

Method: 基于DINO的模型设计，通过结构或训练策略优化把参数减少约三分之一，并使训练时间缩短约三倍；还比较了多模态大型语言模型作为基线。

Result: 模型在保持竞争性性能的同时，参数减少约33%，训练时间加速约3x；并展示了多模态LLM在结构化视觉推理任务上的局限性。

Conclusion: 该论文提出了一种高效的odd-one-out异常检测模型，在参数量和训练时间上显著优于当前最先进方法，同时保持相当的性能。

Abstract: The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/

</details>


### [54] [GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization](https://arxiv.org/abs/2509.04334)
*Pengyue Jia,Yingyi Zhang,Xiangyu Zhao,Yixuan Li*

Main category: cs.CV

TL;DR: GeoArena通过开放上传图片和基于人类两两判断的评测，提供了更真实、隐私友好且能防止测试集泄漏的LVLM图像地理定位基准，并已通过实测数据生成模型排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有图像地理定位评测存在两个主要问题：模型预训练时可能泄漏测试集导致评价失真；以及采用精确地理坐标作为指标既忽视推理过程又涉及隐私问题。需要一种更真实、更人本且防止数据泄漏的评测方法。

Method: 搭建在线平台，用户上传图片并收集来自不同LVLM的地理定位输出；通过两两比较的人类投票判断哪个模型输出更符合人类预期；平台运行两个月，收集了数千条投票记录并据此建立排行榜。

Result: 平台运行两个月收集到大量人类投票数据，基于这些数据进行了详细分析，并给出了不同LVLM在图像地理定位任务上的排行榜，展示了模型性能差异与人类判断的一致性。

Conclusion: 本文提出GeoArena，一个用于评估大视觉-语言模型（LVLM）图像地理定位能力的开放平台，通过开放用户上传的真实世界图片和基于两两人类判断的评测方式，避免了测试集数据泄漏并减轻了对精确坐标的依赖。

Abstract: Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.

</details>


### [55] [From Editor to Dense Geometry Estimator](https://arxiv.org/abs/2509.04338)
*JiYuan Wang,Chunyu Lin,Lei Sun,Rongying Liu,Lang Nie,Mingxing Li,Kang Liao,Xiangxiang Chu,Yao Zhao*

Main category: cs.CV

TL;DR: 将DiT编辑器改造为确定性稠密几何估计器（FE2E），通过consistent velocity目标、对数量化和一次性联合深度-法线估计，带来显著零-shot性能提升，且优于大量数据训练的基线。


<details>
  <summary>Details</summary>
Motivation: 尽管利用T2I生成模型的视觉先验已用于稠密预测，但稠密预测本质为图像到图像任务，因此基于图像编辑的模型可能更适合作为微调基础。作者因此系统比较编辑器与生成器在微调稠密几何估计时的表现。

Method: 提出FE2E框架：基于Diffusion Transformer (DiT)的高级编辑模型改造用于密集几何预测。关键改动包括将原编辑器的flow matching loss重写为“consistent velocity”训练目标；采用对数量化解决BFloat16数值精度冲突；利用DiT的全局注意力在一次前向传递中联合估计深度与法线。

Result: 在不扩充训练数据的条件下，FE2E在多数据集上实现零-shot单目深度与法线估计显著提升。尤其在ETH3D上提升超过35%，并超越了使用100倍数据训练的DepthAnything系列模型。

Conclusion: 编辑器（image editing models）作为预训练视觉先验在稠密几何预测任务上优于文本到图像生成器（T2I generators），因其内在结构先验使微调更稳定并能精炼固有特征，从而获得更高性能。

Abstract: Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by ``refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the ``consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100$\times$ data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.

</details>


### [56] [MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition](https://arxiv.org/abs/2509.04344)
*Feng-Qi Cui,Zhen Lin,Xinlong Rao,Anyang Tong,Shiyao Li,Fei Wang,Changlin Chen,Bin Liu*

Main category: cs.CV

TL;DR: MICACL通过GEIIM、WIAN和MCCL联合解决DFER中的时空建模与长尾问题，在公开数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有DFER方法在长尾类别分布和时空特征建模上存在偏差和能力不足，导致模型归纳偏差和对小类鲁棒性差；因此需要一种能够同时建模实例间时空依赖并对长尾进行平衡优化的方法。

Method: 设计了GEIIM模块以自适应邻接矩阵和多尺度卷积捕捉相邻实例的复杂时空依赖；提出WIAN动态加权聚合实例特征；引入MCCL在多尺度上进行类别感知对比学习以平衡主/次要类别训练。

Result: 在DFEW和FERV39k等真实场景数据集上，MICACL达到了SOTA表现，且在小类识别、鲁棒性和泛化性上优于对比方法（论文声称的实验结果）。

Conclusion: 本文提出的MICACL框架通过多实例学习、图增强实例交互和类别感知对比学习，能有效缓解长尾分布与时空建模复杂性问题，实验显示在DFEW和FERV39k上具有更好性能、鲁棒性与泛化能力。

Abstract: Dynamic facial expression recognition (DFER) faces significant challenges due
to long-tailed category distributions and complexity of spatio-temporal feature
modeling. While existing deep learning-based methods have improved DFER
performance, they often fail to address these issues, resulting in severe model
induction bias. To overcome these limitations, we propose a novel
multi-instance learning framework called MICACL, which integrates
spatio-temporal dependency modeling and long-tailed contrastive learning
optimization. Specifically, we design the Graph-Enhanced Instance Interaction
Module (GEIIM) to capture intricate spatio-temporal between adjacent instances
relationships through adaptive adjacency matrices and multiscale convolutions.
To enhance instance-level feature aggregation, we develop the Weighted Instance
Aggregation Network (WIAN), which dynamically assigns weights based on instance
importance. Furthermore, we introduce a Multiscale Category-aware Contrastive
Learning (MCCL) strategy to balance training between major and minor
categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and
FERV39k) demonstrate that MICACL achieves state-of-the-art performance with
superior robustness and generalization.

</details>


### [57] [Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage](https://arxiv.org/abs/2509.04370)
*Dor Cohen,Inga Efrosman,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: 基于单目SLAM的帧聚类与多帧拼接构建事件全景图，为佩戴式摄像机视频提供快速的视觉摘要。


<details>
  <summary>Details</summary>
Motivation: 长时间的佩戴式摄像机录像难以在时间关键的场景中查看，需一种简洁且易于快速理解的视觉摘要来提高态势感知和决策效率。

Method: 论文使用单目SLAM估计相机轨迹和场景空间布局；沿轨迹对相机位姿进行聚类以识别关键观点；从每个簇中选择具有代表性的帧；采用多帧拼接方法将这些帧融合为空间一致的全景图。

Result: 实验表明，生成的全景图能在复杂环境中快速传达空间信息，便于快速理解和事件回顾；具体定量结果论文摘要中未给出。

Conclusion: 该论文提出了一种将执法/应急人员佩戴式相机视频转换为全景图的计算机视觉流程，旨在为事件回顾和决策提供快速的视觉摘要。

Abstract: First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.

</details>


### [58] [AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search](https://arxiv.org/abs/2509.04376)
*Hao Ju,Hu Zhang,Zhedong Zheng*

Main category: cs.CV

TL;DR: 提出AnomalyLMM，通过粗到细整合LMM与训练-free适配方法，提升文本驱动的人体异常检索性能并提供可解释对齐，在PAB基准上取得小幅提升。


<details>
  <summary>Details</summary>
Motivation: 解决文本驱动的人体异常检索问题，面对细粒度跨模态对齐与稀疏异常样本的挑战，探索大多模态模型在此任务中的潜力并弥合生成性知识与判别性检索的差距。

Method: 设计粗到细管线：先用LMM进行大范围生成性理解，再通过masked cross-modal prompting聚焦细节；引入behavioral saliency prediction来强调异常动作区域；最后用knowledge-aware re-ranking基于世界知识调整检索结果，实现零样本适配。

Result: 提出AnomalyLMM框架：粗到细(coarse-to-fine)管线整合LMMs，并给出无训练适配策略（masked cross-modal prompting、behavioral saliency prediction、knowledge-aware re-ranking），在PAB数据集上使Recall@1提升约+0.96%，并展示可解释的文本-行为对齐。

Conclusion: AnomalyLMM证明大多模态模型可用于细粒度异常检索；采用训练-free提示与重排序策略能在稀缺样本条件下有效对齐文本与视觉行为，为未来研究提供起点。

Abstract: With growing public safety demands, text-based person anomaly search has
emerged as a critical task, aiming to retrieve individuals with abnormal
behaviors via natural language descriptions. Unlike conventional person search,
this task presents two unique challenges: (1) fine-grained cross-modal
alignment between textual anomalies and visual behaviors, and (2) anomaly
recognition under sparse real-world samples. While Large Multi-modal Models
(LMMs) excel in multi-modal understanding, their potential for fine-grained
anomaly retrieval remains underexplored, hindered by: (1) a domain gap between
generative knowledge and discriminative retrieval, and (2) the absence of
efficient adaptation strategies for deployment. In this work, we propose
AnomalyLMM, the first framework that harnesses LMMs for text-based person
anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline
integrating LMMs to bridge generative world knowledge with retrieval-centric
anomaly detection; (2) A training-free adaptation cookbook featuring masked
cross-modal prompting, behavioral saliency prediction, and knowledge-aware
re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study
to explore LMMs for this task, we conduct a rigorous evaluation on the PAB
dataset, the only publicly available benchmark for text-based person anomaly
search, with its curated real-world anomalies covering diverse scenarios (e.g.,
falling, collision, and being hit). Experiments show the effectiveness of the
proposed method, surpassing the competitive baseline by +0.96% Recall@1
accuracy. Notably, our method reveals interpretable alignment between textual
anomalies and visual behaviors, validated via qualitative analysis. Our code
and models will be released for future research.

</details>


### [59] [Aesthetic Image Captioning with Saliency Enhanced MLLMs](https://arxiv.org/abs/2509.04378)
*Yilin Tao,Jiashui Huang,Huaze Xu,Ling Shao*

Main category: cs.CV

TL;DR: 作者提出将审美显著性模块与ViT结合并融入MLLM（ASE-MLLM），以专门增强模型对审美内容的关注，从而在AIC任务上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有AIC研究多侧重于审美评分预测，且基于MLLM的AIC大多通过微调整体模型，缺乏对审美内容的专门适配，导致生成描述无法充分关注审美要素。

Method: 提出ASE-MLLM框架，包括Image Aesthetic Saliency Module（IASM）用于高效提取图像审美显著性特征，以及IAS-ViT编码器用于通过交叉注意力将审美显著性与原始图像特征融合，进而与MLLM联合训练。

Result: 在多项主流AIC基准上，ASE-MLLM显著优于传统方法和通用MLLM，取得了SOTA性能。

Conclusion: 该论文提出了将审美显著性显式融合到多模态大模型中，以提升美学图像描述（AIC）性能，实验显示在主流数据集上实现了SOTA。

Abstract: Aesthetic Image Captioning (AIC) aims to generate textual descriptions of
image aesthetics, becoming a key research direction in the field of
computational aesthetics. In recent years, pretrained Multimodal Large Language
Models (MLLMs) have advanced rapidly, leading to a significant increase in
image aesthetics research that integrates both visual and textual modalities.
However, most existing studies on image aesthetics primarily focus on
predicting aesthetic ratings and have shown limited application in AIC.
Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods
without specifically adapting MLLMs to focus on target aesthetic content. To
address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal
Large Language Model (ASE-MLLM), an end-to-end framework that explicitly
incorporates aesthetic saliency into MLLMs. Within this framework, we introduce
the Image Aesthetic Saliency Module (IASM), which efficiently and effectively
extracts aesthetic saliency features from images. Additionally, we design
IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency
features with original image features via a cross-attention mechanism. To the
best of our knowledge, ASE-MLLM is the first framework to integrate image
aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments
demonstrated that our approach significantly outperformed traditional methods
and generic MLLMs on current mainstream AIC benchmarks, achieving
state-of-the-art (SOTA) performance.

</details>


### [60] [SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer](https://arxiv.org/abs/2509.04379)
*Jimin Xu,Bosheng Qin,Tao Jin,Zhou Zhao,Zhenhui Ye,Jun Yu,Fei Wu*

Main category: cs.CV

TL;DR: 引入2D扩散模型先验，通过跨视图注意力与实例级一致性传递，实现更语义化且结构清晰的3D场景风格迁移，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法难以从风格参考图提取高层语义，且生成结果结构不清晰、对象实例分离差。作者希望借助2D扩散模型的语义先验提升风格语义保真与实例级一致性。

Method: 两阶段流程：1) 使用预训练的2D扩散模型生成风格化的关键视图；2) 将这些风格化视图转移到3D表示。提出两项设计：跨视图风格对齐——在UNet最后上采样块插入跨视图注意力，实现多视图特征交互；实例级风格迁移——利用实例间一致性将风格从关键视图传到3D表示。

Result: 定性与定量实验表明，该管线在从前视到360°复杂场景的多种环境下均显著优于现有方法，生成更具风格保真、结构清晰和实例分离的3D风格化结果。

Conclusion: 该论文提出了一种将2D扩散模型先验融入3D风格迁移的新框架，通过生成风格化关键视图并将其一致性转移到3D表示上，提升了风格语义迁移能力与实例级结构清晰度。

Abstract: Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.

</details>


### [61] [Learning neural representations for X-ray ptychography reconstruction with unknown probes](https://arxiv.org/abs/2509.04402)
*Tingyou Li,Zixin Xu,Zirui Gao,Hanfei Yan,Xiaojing Huang,Jizhou Li*

Main category: cs.CV

TL;DR: PtyINR用隐式神经表示联合自监督恢复样品与探针，提升未知探针与低信噪比条件下X射线拼图的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统迭代方法和深度学习在未知探针或低剂量/高速采集的低信号条件下重建效果欠佳，限制了X射线拼图术的应用潜力。

Method: 将样品和探针均用连续隐式神经表示（INR）参数化，采用自监督优化直接拟合物理前向模型（ptychography）从衍射强度到像素相位与幅值，端到端联合重建，无需探针预表征。

Result: 在模拟与真实实验数据上，PtyINR在重建质量上优于基线方法，且在低信号场景下表现出显著鲁棒性，表明其具有广泛的适用性和物理约束下的泛化能力。

Conclusion: PtyINR能在未知探测光束条件下，从原始衍射图直接恢复样品与探针，克服低信噪比场景下的重建困难。

Abstract: X-ray ptychography provides exceptional nanoscale resolution and is widely
applied in materials science, biology, and nanotechnology. However, its full
potential is constrained by the critical challenge of accurately reconstructing
images when the illuminating probe is unknown. Conventional iterative methods
and deep learning approaches are often suboptimal, particularly under the
low-signal conditions inherent to low-dose and high-speed experiments. These
limitations compromise reconstruction fidelity and restrict the broader
adoption of the technique. In this work, we introduce the Ptychographic
Implicit Neural Representation (PtyINR), a self-supervised framework that
simultaneously addresses the object and probe recovery problem. By
parameterizing both as continuous neural representations, PtyINR performs
end-to-end reconstruction directly from raw diffraction patterns without
requiring any pre-characterization of the probe. Extensive evaluations
demonstrate that PtyINR achieves superior reconstruction quality on both
simulated and experimental data, with remarkable robustness under challenging
low-signal conditions. Furthermore, PtyINR offers a generalizable,
physics-informed framework for addressing probe-dependent inverse problems,
making it applicable to a wide range of computational microscopy problems.

</details>


### [62] [Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios](https://arxiv.org/abs/2509.04403)
*Jingen Qu,Lijun Li,Bo Zhang,Yichen Yan,Jing Shao*

Main category: cs.CV

TL;DR: 提出图像为中心的自适应RMS数据构建方法并生成35k样本，配套以微调的安全判别模型作为统一评估指标，实验证明可扩展且有效。


<details>
  <summary>Details</summary>
Motivation: 现有以风险为导向的数据构建方法无法覆盖日益复杂的真实世界多模态安全场景，且缺乏统一评估指标导致效果难以证明。

Method: 先从图像出发，自动生成与图像配对的文本描述和指导回复，构成包含35k图文及指导回复的数据集；并通过微调安全判别模型，作为统一的安全评估指标，对其他数据集进行评估。

Result: 构建了35k图文与指导回复的RMS数据集；提出并验证了基于微调安全判别模型的标准化评估指标；在多项任务上的大量实验验证了该管线的可扩展性和有效性。

Conclusion: 该论文提出了一种基于图像的自适应构建方法，自动生成包含文本及安全指导回复的真实世界多模态安全（RMS）数据集，并提出了一个统一的评估指标（使用微调的安全判别模型在其他数据集上评估）。实验表明方法可扩展且有效。

Abstract: Multimodal large language models (MLLMs) are rapidly evolving, presenting
increasingly complex safety challenges. However, current dataset construction
methods, which are risk-oriented, fail to cover the growing complexity of
real-world multimodal safety scenarios (RMS). And due to the lack of a unified
evaluation metric, their overall effectiveness remains unproven. This paper
introduces a novel image-oriented self-adaptive dataset construction method for
RMS, which starts with images and end constructing paired text and guidance
responses. Using the image-oriented method, we automatically generate an RMS
dataset comprising 35k image-text pairs with guidance responses. Additionally,
we introduce a standardized safety dataset evaluation metric: fine-tuning a
safety judge model and evaluating its capabilities on other safety
datasets.Extensive experiments on various tasks demonstrate the effectiveness
of the proposed image-oriented pipeline. The results confirm the scalability
and effectiveness of the image-oriented approach, offering a new perspective
for the construction of real-world multimodal safety datasets.

</details>


### [63] [Few-step Flow for 3D Generation via Marginal-Data Transport Distillation](https://arxiv.org/abs/2509.04406)
*Zanwei Zhou,Taoran Yi,Jiemin Fang,Chen Yang,Lingxi Xie,Xinggang Wang,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: 提出MDT-dist，通过速度匹配与速度蒸馏将3D流模型蒸馏为少步采样学生模型，实现极大加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的3D生成需大量采样步骤；尽管一致性模型在2D扩散模型中实现了少步推理，但在复杂的3D任务上未被充分探索。作者旨在将少步蒸馏方法扩展到3D流模型，加速推理同时保持质量。

Method: MDT-dist基于“边际数据输运”（Marginal-Data Transport）的蒸馏目标。由于直接积分速度场不可行，作者提出了两种可优化的等价目标：速度匹配（VM）和速度蒸馏（VD）。VM使学生模型匹配教师模型的速度场，但会引入有偏梯度；VD利用学得的速度场进行概率密度蒸馏以增强优化过程。

Result: 在TRELLIS框架上，MDT-dist将每个流变换器的采样步数从25降至1或2，在A800上分别达成0.68s（1步x2）和0.94s（2步x2）延迟，速度提升9.0x和6.5x，并在视觉和几何质量上优于现有CM蒸馏方法。

Conclusion: 该论文提出了MDT-dist，一种用于将预训练3D流模型蒸馏为少步采样学生模型的新框架，能将TRELLIS的每个流变换器的采样步数从25降到1或2，同时保持高视觉与几何保真度，获得显著加速。

Abstract: Flow-based 3D generation models typically require dozens of sampling steps
during inference. Though few-step distillation methods, particularly
Consistency Models (CMs), have achieved substantial advancements in
accelerating 2D diffusion models, they remain under-explored for more complex
3D generation tasks. In this study, we propose a novel framework, MDT-dist, for
few-step 3D flow distillation. Our approach is built upon a primary objective:
distilling the pretrained model to learn the Marginal-Data Transport. Directly
learning this objective needs to integrate the velocity fields, while this
integral is intractable to be implemented. Therefore, we propose two
optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),
to equivalently convert the optimization target from the transport level to the
velocity and the distribution level respectively. Velocity Matching (VM) learns
to stably match the velocity fields between the student and the teacher, but
inevitably provides biased gradient estimates. Velocity Distillation (VD)
further enhances the optimization process by leveraging the learned velocity
fields to perform probability density distillation. When evaluated on the
pioneer 3D generation framework TRELLIS, our method reduces sampling steps of
each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s
(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high
visual and geometric fidelity. Extensive experiments demonstrate that our
method significantly outperforms existing CM distillation methods, and enables
TRELLIS to achieve superior performance in few-step 3D generation.

</details>


### [64] [Durian: Dual Reference-guided Portrait Animation with Attribute Transfer](https://arxiv.org/abs/2509.04434)
*Hyunsoo Cha,Byungjun Kim,Hanbyul Joo*

Main category: cs.CV

TL;DR: Durian通过双参考网络与自重构训练策略，实现了无监督、零样本的人像属性迁移动画，兼具空间一致性、多属性组合与鲁棒性，达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 解决在零样本下将参考图像的面部属性稳定一致地迁移到目标人像动画的视频生成问题，尤其关注跨帧的空间一致性和多属性组合能力。

Method: 提出双参考网络，在扩散模型去噪过程中同时注入人像和属性图像的空间特征；训练采用自重构范式，从同一视频采样两帧作为属性与目标，并用遮罩与关键点引导的图像生成进行掩码扩展训练；同时进行空间与外观层面的增强以提高对齐鲁棒性。

Result: 在人像动画与属性迁移任务上达到最先进性能；双参考设计还支持在一次生成过程中组合多属性而无需额外训练。

Conclusion: Durian能以零样本方式将参考图像的面部属性转移到目标人像视频上，且在空间一致性和高保真度上表现优异。

Abstract: We present Durian, the first method for generating portrait animation videos
with facial attribute transfer from a given reference image to a target
portrait in a zero-shot manner. To enable high-fidelity and spatially
consistent attribute transfer across frames, we introduce dual reference
networks that inject spatial features from both the portrait and attribute
images into the denoising process of a diffusion model. We train the model
using a self-reconstruction formulation, where two frames are sampled from the
same portrait video: one is treated as the attribute reference and the other as
the target portrait, and the remaining frames are reconstructed conditioned on
these inputs and their corresponding masks. To support the transfer of
attributes with varying spatial extent, we propose a mask expansion strategy
using keypoint-conditioned image generation for training. In addition, we
further augment the attribute and portrait images with spatial and
appearance-level transformations to improve robustness to positional
misalignment between them. These strategies allow the model to effectively
generalize across diverse attributes and in-the-wild reference combinations,
despite being trained without explicit triplet supervision. Durian achieves
state-of-the-art performance on portrait animation with attribute transfer, and
notably, its dual reference design enables multi-attribute composition in a
single generation pass without additional training.

</details>


### [65] [From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform](https://arxiv.org/abs/2509.04437)
*Benjamin El-Zein,Dominik Eckert,Andreas Fieselmann,Christopher Syben,Ludwig Ritschl,Steffen Kappler,Sebastian Stober*

Main category: cs.CV

TL;DR: 结合可微分霍夫变换的深度学习框架，通过线性几何约束增强X光准直阴影分割，能在遮挡和散射噪声下稳定重建多边形形状的ROI，性能优异。


<details>
  <summary>Details</summary>
Motivation: 准直器在X光图像中形成多边形阴影，但散射辐射会模糊边缘，传统基于像素的分割难以可靠检测边界。引入几何先验（直线/多边形）能提高检测鲁棒性。

Method: 方法由两个分支组成：一个基于可微分霍夫变换的线检测网络，用于检测准直边界并推断ROI中心；另一个是常规的分割网络。推理阶段结合两者的输出，利用线约束生成精炼的分割掩码。

Result: 在真实X射线测试集上取得了良好效果，重建的准直区域中位Hausdorff距离为4.3–5.0mm，且方法不局限于固定边数（尽管实际应用最多四条边）。

Conclusion: 该论文提出了一种将几何约束（多边形/直线边界）融入深度学习分割框架的方法，用于检测X射线图片中的准直器阴影，从而提高ROI分割的鲁棒性和准确性。

Abstract: Collimation in X-ray imaging restricts exposure to the region-of-interest
(ROI) and minimizes the radiation dose applied to the patient. The detection of
collimator shadows is an essential image-based preprocessing step in digital
radiography posing a challenge when edges get obscured by scattered X-ray
radiation. Regardless, the prior knowledge that collimation forms
polygonal-shaped shadows is evident. For this reason, we introduce a deep
learning-based segmentation that is inherently constrained to its geometry. We
achieve this by incorporating a differentiable Hough transform-based network to
detect the collimation borders and enhance its capability to extract the
information about the ROI center. During inference, we combine the information
of both tasks to enable the generation of refined, line-constrained
segmentation masks. We demonstrate robust reconstruction of collimated regions
achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real
Xray images. While this application involves at most four shadow borders, our
method is not fundamentally limited by a specific number of edges.

</details>


### [66] [The Telephone Game: Evaluating Semantic Drift in Unified Models](https://arxiv.org/abs/2509.04438)
*Sabbir Mollah,Rohit Gupta,Sirnam Swetha,Qingyang Liu,Ahnaf Munir,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出UCF-UM循环评估框架与三项指标，创建ND400基准，在七个模型上验证，发现单次评估无法揭示跨模态语义漂移，需要循环一致性评估作为补充。


<details>
  <summary>Details</summary>
Motivation: 现有评估仅考察单次I2T或T2I表现，无法反映模型在模态循环中的语义保持能力；需要新的评估以检测理解能力与生成能力之间的一致性和语义漂移。

Method: 提出交替执行I2T与T2I的多代生成评估协议，定义三类指标：Mean Cumulative Drift (MCD)、Semantic Drift Rate (SDR)、Multi-Generation GenEval (MGG)，并构建ND400基准用于跨数据集泛化测试；在七个近期模型上进行实验评估。

Result: UCF-UM揭示了显著的跨模态稳定性差异：部分模型（如BAGEL）在多次往返中能维持语义一致性，而其他模型（如Vila-u）尽管单次成绩优秀，但多代循环后语义迅速漂移。

Conclusion: 该论文提出了用于统一视觉-语言模型（同时支持I2T与T2I）的循环一致性评估框架UCF-UM，强调了衡量跨模态语义稳定性的必要性，并通过新基准ND400及多模型对比展示了不同模型在多次模态往返后语义漂移的差异。

Abstract: Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models

</details>


### [67] [One Flight Over the Gap: A Survey from Perspective to Panoramic Vision](https://arxiv.org/abs/2509.04444)
*Xin Lin,Xian Ge,Dizhe Zhang,Zhaoliang Wan,Xianshun Wang,Xiangtai Li,Wenjie Jiang,Bo Du,Dacheng Tao,Ming-Hsuan Yang,Lu Qi*

Main category: cs.CV

TL;DR: 本文综述了360°全景视觉，分析透视图到全景图域适配的三大挑战，归纳跨任务策略并将研究分为四类，指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全景图提供360°视角，对虚拟现实、自动驾驶等应用重要，但其与传统透视图在几何与采样等方面存在显著差异，直接迁移透视方法效果有限，因而需要系统性综述与方法分类，帮助推进透视到全景的域适配研究。

Method: 回顾性综述：重访全景成像管线与投影方法以建立结构差异的先验，总结三大适配挑战，覆盖300+篇论文与20+任务，进行跨方法分析与跨任务比较并分类全景视觉任务为四类，同时讨论开放问题与未来方向。

Result: 总结出三大挑战（极点几何畸变、ERP非均匀采样、周期边界连续性），归纳了多种应对策略（投影变换、卷积适配、采样修正、边界处理等），将全景视觉任务划分为视觉质量增强与评估、视觉理解、多模态理解与视觉生成四大类，提出数据、模型与应用层面的未来研究方向。

Conclusion: 该综述系统性地回顾了全景视觉（ODIs）领域，特别聚焦于从透视图到全景图的域适配问题，指出了全景图与透视图在投影几何、采样分布和边界连续性上的差异，并总结了三大适配挑战与四类研究方向，为未来研究指明数据、模型与应用方向。

Abstract: Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama

</details>


### [68] [Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2509.04446)
*Kiymet Akdemir,Jing Shi,Kushal Kafle,Brian Price,Pinar Yanardag*

Main category: cs.CV

TL;DR: Plot'n Polish通过分层控制与基于扩散模型的零样本编辑方法，实现了可控且一致的故事视觉化生成，支持创作者在多帧故事中进行细粒度修改与迭代。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在创意领域的广泛应用，创作者需要在生成后进行可控的细化与编辑，同时保持多帧之间的叙事和视觉一致性，现有方法难以同时兼顾细粒度控制与多帧一致性。

Method: 结合分层控制策略，通过在不同抽象层次（全局故事布局、场景帧一致性、局部细节编辑）上施加约束与引导，实现对生成过程的多尺度调节；利用基于扩散模型的编辑操作（如条件采样、区域修补、文本到图像重新渲染）以支持零样本情形下的精细修改，同时通过跨帧特征对齐或记忆模块保持叙事与视觉一致性。

Result: 框架在定性与定量评估中展示了在多帧故事可视化任务中更强的一致性与编辑能力，能在不同编辑粒度上（从整体布局到局部细节）保持视觉与叙事一致性，并提高用户对创作流程的可控性与效率。

Conclusion: 该论文提出了Plot'n Polish，一个零样本框架，旨在在故事可视化中实现一致性生成并提供细粒度控制与后期修改能力。

Abstract: Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.

</details>


### [69] [TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection](https://arxiv.org/abs/2509.04448)
*Zehong Yan,Peng Qi,Wynne Hsu,Mong Li Lee*

Main category: cs.CV

TL;DR: 提出TRUST-VL和198K样本的TRUST-Instruct进行联合训练，通过Question-Aware Visual Amplifier提升多模态错误信息检测的泛化与可解释性，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 多模态错误信息形式多样（文本、图像、跨模态），现有方法通常针对单一失真类型，难以泛化。作者认为不同失真类型存在共享的推理能力，通过联合训练可促进知识共享并提升泛化能力。

Method: 提出统一的视觉-语言模型TRUST-VL，包含Question-Aware Visual Amplifier模块用于提取任务相关视觉特征；并基于大规模198K条指令式数据集TRUST-Instruct进行联合训练，数据包含结构化的推理链以模拟人类事实核查流程。

Result: 在多个内域基准和零样本测试上，TRUST-VL达到了SOTA性能，同时提供强泛化能力与可解释性。

Conclusion: TRUST-VL通过联合训练多种失真类型并引入Question-Aware Visual Amplifier，有效提升了多模态错误信息检测的泛化性与可解释性，实验证明其在内域及零样本评估上均达到了最先进水平。

Abstract: Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.

</details>


### [70] [Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview](https://arxiv.org/abs/2509.04450)
*Jun-Kun Chen,Aayush Bansal,Minh Phuoc Vo,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 提出VFR：一种自回归分段生成结合前缀条件与360°锚点视频的长虚拟试衣视频生成方法，解决局部平滑与全局一致性问题，能生成分钟级连贯视频。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成/试衣方法通常受限于短时建模、显存与训练数据长度，无法生成任意长的连贯虚拟试衣视频；VFR旨在提供可扩展到任意长度的生成框架，兼顾局部平滑与全局一致性。

Method: 将长视频生成建模为自回归的分段生成流程；通过使用前缀视频条件保证相邻段的局部平滑；引入“锚点视频”（360度全身外观视频）来维护跨段的全局时间一致性，从而在无需大量长视频训练数据的情况下生成分钟级虚拟试穿视频。

Result: 宣称在各种运动场景下能生成分钟级别的虚拟试穿视频，兼具局部平滑与全球时间一致性，是首个能生成长虚拟试穿视频的工作。

Conclusion: 本论文提出了虚拟试衣长视频生成的新范式VFR，有望解决短视频方法无法扩展到任意长度的问题，但仍存在若干实验和方法细节缺失与可复现性挑战。

Abstract: We introduce the Virtual Fitting Room (VFR), a novel video generative model
that produces arbitrarily long virtual try-on videos. Our VFR models long video
generation tasks as an auto-regressive, segment-by-segment generation process,
eliminating the need for resource-intensive generation and lengthy video data,
while providing the flexibility to generate videos of arbitrary length. The key
challenges of this task are twofold: ensuring local smoothness between adjacent
segments and maintaining global temporal consistency across different segments.
To address these challenges, we propose our VFR framework, which ensures
smoothness through a prefix video condition and enforces consistency with the
anchor video -- a 360-degree video that comprehensively captures the human's
wholebody appearance. Our VFR generates minute-scale virtual try-on videos with
both local smoothness and global temporal consistency under various motions,
making it a pioneering work in long virtual try-on video generation.

</details>
