<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.DB](#cs.DB) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文首次对3D Gaussian Splatting的IP保护进行系统综述，提出自下而上的分析框架，归纳保护范式与威胁，并给出六条未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在实时场景合成中的快速发展及其商业化价值增长，显式参数化结构带来知识产权泄露与非法复制风险，亟需系统研究该领域的IP保护方法与挑战。

Method: 通过系统化调研与分析，作者提出一个自下而上的框架：首先解析高斯基元及其可被利用的扰动机制；其次分类总结被动（如水印、指纹）与主动（如模型层级的保护）两大保护范式；最后评估新兴生成式AI对保护方案的鲁棒性威胁并指出现有评估的不足。

Result: 构建了首个3DGS IP保护综述框架，识别并总结了现有方法的机制与范式，揭示鲁棒性评估中的空白，提出六个未来研究方向，提供了针对可信IP保护的路线图。

Conclusion: 本文综述了3D Gaussian Splatting (3DGS) 的知识产权保护研究现状，指出现有工作分散、缺乏统一框架，并提出自下而上的分析框架，涵盖高斯扰动机制、被动与主动保护范式以及在生成式AI时代面临的鲁棒性威胁，最后提出六条未来研究方向。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

</details>


### [2] [TruKAN: Towards More Efficient Kolmogorov-Arnold Networks Using Truncated Power Functions](https://arxiv.org/abs/2602.03879)
*Ali Bayeh,Samira Sadaoui,Malek Mouhoub*

Main category: cs.CV

TL;DR: TruKAN用截断幂基替代B样条，实现更快更准且更可解释的KAN变体，并在EfficientNet-V2框架上显示出优于现有KAN方法的性能。


<details>
  <summary>Details</summary>
Motivation: 缓解KAN在计算效率与遵循Kolmogorov-Arnold网络原则间的权衡，通过更简单、更可解释且高效的基函数改进实际应用表现。

Method: 基于k阶样条理论用截断幂函数与多项式项组合构建TruKAN层；支持共享或独立结点；将TruKAN集成进基于EfficientNet-V2的框架，并与MLP/KAN/SineKAN变体在不同规模架构上比较，训练采用混合优化并研究层归一化策略。

Result: 在计算效率、内存占用和准确性上，TruKAN优于其他KAN模型，在复杂图像任务上表现更稳健；共享与独立结点设置及层归一化对性能有显著影响。

Conclusion: TruKAN在保持KAN表达能力的同时，通过将B样条基替换为截断幂函数族，提升了训练速度、准确性和可解释性，适用于复杂视觉任务且优于其他KAN变体。

Abstract: To address the trade-off between computational efficiency and adherence to Kolmogorov-Arnold Network (KAN) principles, we propose TruKAN, a new architecture based on the KAN structure and learnable activation functions. TruKAN replaces the B-spline basis in KAN with a family of truncated power functions derived from k-order spline theory. This change maintains the KAN's expressiveness while enhancing accuracy and training time. Each TruKAN layer combines a truncated power term with a polynomial term and employs either shared or individual knots. TruKAN exhibits greater interpretability than other KAN variants due to its simplified basis functions and knot configurations. By prioritizing interpretable basis functions, TruKAN aims to balance approximation efficacy with transparency. We develop the TruKAN model and integrate it into an advanced EfficientNet-V2-based framework, which is then evaluated on computer vision benchmark datasets. To ensure a fair comparison, we develop various models: MLP-, KAN-, SineKAN and TruKAN-based EfficientNet frameworks and assess their training time and accuracy across small and deep architectures. The training phase uses hybrid optimization to improve convergence stability. Additionally, we investigate layer normalization techniques for all the models and assess the impact of shared versus individual knots in TruKAN. Overall, TruKAN outperforms other KAN models in terms of accuracy, computational efficiency and memory usage on the complex vision task, demonstrating advantages beyond the limited settings explored in prior KAN studies.

</details>


### [3] [DiGAN: Diffusion-Guided Attention Network for Early Alzheimer's Disease Detection](https://arxiv.org/abs/2602.03881)
*Maxx Richard Rahman,Mostafa Hammouda,Wolfgang Maass*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Early diagnosis of Alzheimer's disease (AD) remains a major challenge due to the subtle and temporally irregular progression of structural brain changes in the prodromal stages. Existing deep learning approaches require large longitudinal datasets and often fail to model the temporal continuity and modality irregularities inherent in real-world clinical data. To address these limitations, we propose the Diffusion-Guided Attention Network (DiGAN), which integrates latent diffusion modelling with an attention-guided convolutional network. The diffusion model synthesizes realistic longitudinal neuroimaging trajectories from limited training data, enriching temporal context and improving robustness to unevenly spaced visits. The attention-convolutional layer then captures discriminative structural--temporal patterns that distinguish cognitively normal subjects from those with mild cognitive impairment and subjective cognitive decline. Experiments on synthetic and ADNI datasets demonstrate that DiGAN outperforms existing state-of-the-art baselines, showing its potential for early-stage AD detection.

</details>


### [4] [PriorProbe: Recovering Individual-Level Priors for Personalizing Neural Networks in Facial Expression Recognition](https://arxiv.org/abs/2602.03882)
*Haijiang Yan,Nick Chater,Adam Sanborn*

Main category: cs.CV

TL;DR: 提出基于MCMC with People的PriorProbe用于恢复个体先验并个性化神经网络，在面部表情识别任务上显著提高对模糊刺激的个体预测性能。


<details>
  <summary>Details</summary>
Motivation: 个体认知先验可用于个性化神经网络，但现有先验获取方法要么无法唯一识别个体先验，要么会引入系统性偏差。为此需要一种既能精确恢复个体先验又能避免偏差的新方法。

Method: 提出PriorProbe：运用MCMC with People的交互式采样机制，通过让参与者参与马尔科夫链的状态转换来推断其细粒度先验。将恢复的个体先验以某种方式（未在摘要详述）融入到现有的深度神经网络（用于面部表情识别），并在包含模糊刺激的数据上对比表现，评估其对个体化预测的贡献。

Result: PriorProbe能够恢复细粒度的个体先验，并将其整合到先进神经网络中，从而在模糊刺激的个体分类任务上显著提升预测性能，优于网络自身以及其他先验来源，同时保持对真实标签的推断不受破坏。

Conclusion: 该论文提出了PriorProbe，一种基于MCMC with People的方法，用于精确恢复个体层面的先验分布，并将其整合进神经网络以实现个性化预测。实验证明在面部表情识别任务上，PriorProbe恢复的先验能显著提升模型在模糊刺激上的个体分类预测性能，同时不破坏模型对真实标签的推断。

Abstract: Incorporating individual-level cognitive priors offers an important route to personalizing neural networks, yet accurately eliciting such priors remains challenging: existing methods either fail to uniquely identify them or introduce systematic biases. Here, we introduce PriorProbe, a novel elicitation approach grounded in Markov Chain Monte Carlo with People that recovers fine-grained, individual-specific priors. Focusing on a facial expression recognition task, we apply PriorProbe to individual participants and test whether integrating the recovered priors with a state-of-the-art neural network improves its ability to predict an individual's classification on ambiguous stimuli. The PriorProbe-derived priors yield substantial performance gains, outperforming both the neural network alone and alternative sources of priors, while preserving the network's inference on ground-truth labels. Together, these results demonstrate that PriorProbe provides a general and interpretable framework for personalizing deep neural networks.

</details>


### [5] [Explainable Computer Vision Framework for Automated Pore Detection and Criticality Assessment in Additive Manufacturing](https://arxiv.org/abs/2602.03883)
*Akshansh Mishra,Rakesh Morisetty*

Main category: cs.CV

TL;DR: 解释性计算机视觉框架用于三维断层数据的孔隙检测与关键性评估：通过阈值分割得到500个孔，构建相互作用网络并用机器学习预测关键性，SHAP显示与边界距离（归一化表面距离）对预测具有压倒性影响，孔径影响很小，几何形状影响可忽略。


<details>
  <summary>Details</summary>
Motivation: 内含孔隙是增材制造组件中的关键缺陷，降低结构性能并阻碍工业应用；现有自动检测缺乏可解释性，工程师难以理解关键性预测的物理基础，因此需要一个透明且可解释的孔隙评估方法。

Method: 重建连续灰度切片为体积数据；基于强度阈值与连通组件分析分割孔隙；提取尺寸、长宽比、extent、与样本边界的相对位置等几何特征；用百分位欧氏距离阈值构建孔隙相互作用网络；训练机器学习模型预测孔隙关键性；用SHAP解释每个特征对预测的贡献。

Result: The paper presents an explainable computer vision framework to detect pores and assess their criticality in 3D tomographic volumes of additively manufactured parts. It reconstructs grayscale slices into volumes, segments pores via intensity thresholding and connected components, extracts geometric and positional features for 500 pores, builds an interaction network using percentile-based distance thresholds (24,950 edges), trains ML models to predict pore criticality, and applies SHAP to interpret feature importance.

Conclusion: 归一化表面距离是预测孔隙关键性的主导因素，表明边界邻近孔隙更可能促成失效；孔径和几何特征对模型预测贡献有限。该框架提升了缺陷评估的可解释性，有助于工艺优化与质量控制。

Abstract: Internal porosity remains a critical defect mode in additively manufactured components, compromising structural performance and limiting industrial adoption. Automated defect detection methods exist but lack interpretability, preventing engineers from understanding the physical basis of criticality predictions. This study presents an explainable computer vision framework for pore detection and criticality assessment in three-dimensional tomographic volumes. Sequential grayscale slices were reconstructed into volumetric datasets, and intensity-based thresholding with connected component analysis identified 500 individual pores. Each pore was characterized using geometric descriptors including size, aspect ratio, extent, and spatial position relative to the specimen boundary. A pore interaction network was constructed using percentile-based Euclidean distance criteria, yielding 24,950 inter-pore connections. Machine learning models predicted pore criticality scores from extracted features, and SHAP analysis quantified individual feature contributions. Results demonstrate that normalized surface distance dominates model predictions, contributing more than an order of magnitude greater importance than all other descriptors. Pore size provides minimal influence, while geometric parameters show negligible impact. The strong inverse relationship between surface proximity and criticality reveals boundary-driven failure mechanisms. This interpretable framework enables transparent defect assessment and provides actionable insights for process optimization and quality control in additive manufacturing.

</details>


### [6] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 提出4DPC^2hat：首个面向动态点云的MLLM，包含4DPC^2hat-200K数据集、Mamba时序模型与失败感知自举训练，显著提升动作与时序理解。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要处理静态点云，缺乏对动态点云序列的理解，受限于大型跨模态数据集的缺失与时空运动建模的困难，因此需要专门针对4D（时空）点云的模型与数据。

Method: 提出了三方面方法：1) 构建4DPC^2hat-200K数据集（含拓扑一致的4D点云构建与两级文本描述），2) 引入Mamba增强的时序推理MLLM以捕捉长程依赖与动态模式，3) 设计失败感知自举训练策略，迭代发现模型弱点并生成针对性QA监督。

Result: 构建了包含44K动态对象序列、700K点云帧、200K问答对的大规模数据集；实验显示在动作识别与时间关系推理上明显优于现有模型，证明方法有效。

Conclusion: 4DPC^2hat成功构建了面向动态点云理解的首个跨模态大模型，通过大规模数据集与新的模型与训练策略，显著提升了动作理解与时序推理能力。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

</details>


### [7] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 本工作提出无须真值的Ref-AVS掩码质量评估任务与基准，并设计基于MLLM的MQ-Auditor来定量与定性评估掩码质量，实验验证其有效性并能辅助分割改进。


<details>
  <summary>Details</summary>
Motivation: 现有Ref-AVS只关注生成分割掩码，缺乏在推理时无需真值即可判断掩码质量的机制，无法提供可解释的质量诊断与纠错建议。

Method: 构建包含多种几何与语义误差模式的基准数据集MQ-RAVSBench；设计MQ-Auditor，利用MLLM显式融合视频、音频、文本与掩码信息，预测IoU、错误类型并给出质量控制建议；在多种开源与商用模型上进行对比实验并整合到现有Ref-AVS系统做下游验证。

Result: MQ-Auditor在MQ-RAVSBench上在IoU预测、错误分类与决策建议等任务上均优于强基线；可检测并定位分割失败，提升下游分割系统的错误控制能力。

Conclusion: 提出了MQA-RefAVS任务与基准MQ-RAVSBench，以及基于多模态大模型的评估器MQ-Auditor；实验显示其优于其他MLLM并能帮助下游Ref-AVS系统提升鲁棒性。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [8] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: GPAIR通过高斯核连续化和解析声场表达式，结合GPU可微Triton算子，实现了对大规模3D PACT迭代重建的亚秒级超快速加速，推动近实时临床应用。


<details>
  <summary>Details</summary>
Motivation: 传统IR在3D PACT中计算量巨大、重建耗时长，限制了其在大尺度、临床场景下的实用性，因而需要一种极大加速的重建方法。

Method: GPAIR将传统的离散空间网格替换为连续各向同性高斯核表示，推导出声波的解析闭式表达，并基于GPU加速的可微分Triton算子实现高效前向与反向投影。

Result: 在动物实验中，GPAIR对包含840万体素的三维目标实现了亚秒级重建，计算速度较传统IR提高数个数量级，且保持重建质量。

Conclusion: GPAIR能显著加速3D PACT的迭代重建，将毫秒到秒级重建时间从传统小时级缩短，实现近实时大规模三维成像，从而推动临床可用性。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

</details>


### [9] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 利用ViT（尤其DINOv3）与适当降维聚类，可无标注地高精度实现物种级图像聚类，并能发现种内生态学差异，显著降低人工标注负担。


<details>
  <summary>Details</summary>
Motivation: 传统的动物图片人工标注耗时且限制监测规模，探究是否可利用强大的视觉基础模型直接将未标注的大量图片聚类至物种级，以扩展生物多样性监测的规模与效率，并评估何时、为何及如何成功或失败。

Method: 构建基于五种ViT模型的嵌入提取管线，结合五种降维方法（如t-SNE等）与四种聚类算法（两监督两无监督），在60个物种（30哺乳类、30鸟类），每物种随机抽取200张已验证图片的设置下进行系统性基准评估；并对长尾分布与过度聚类用于提取种内变异进行额外试验，同时提供开放工具包。

Result: 使用DINOv3嵌入+t-SNE+监督层次聚类达成V-measure 0.958的近完美物种级聚类；无监督管线也达0.943并只将1.14%图片作为异常值需要专家审核；方法对长尾种群分布鲁棒，且通过过度聚类可可靠地提取年龄、性别与毛色等种内差异；并发布了开源基准工具和方法建议。

Conclusion: ViT基础模型（尤其是DINOv3）在无需人工标注的情况下，能将大量动物相片高效聚类到物种级别，达到接近完美的效果；无监督方法也能取得接近性能并只小幅需人工审查；故可显著缓解生态学中标注瓶颈，并能进一步发现种内生态学变异。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [10] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: NH-Fair通过统一协议评估视觉和视觉-语言模型的公平性，指出良好调参的ERM强基线、复合数据增强的实用性，以及LVLM仍有子群差距，强调需调参意识的可复现评估。


<details>
  <summary>Details</summary>
Motivation: 现实数据训练的模型会继承并放大对特定社会群体的偏见，现有去偏方法难以比较且评估不一致，因而需要统一、公平且可复现的基准来进行伤害意识的公平性评估。

Method: 构建NH-Fair基准，统一数据集、评价指标和训练协议；进行系统性的ERM超参数调优研究；在有监督和零-shot场景下比较各种去偏方法、数据增强策略及LVLM与视觉模型的表现；分析缩放效应与架构/训练选择的相对影响。

Result: 发现充分调参的ERM基线常常与去偏方法不相上下；复合数据增强方法稳定带来公平性改善且不牺牲效用；LVLM虽提升平均准确率但仍存在子组差异，模型规模带来的改善通常不及架构或训练协议选择。

Conclusion: 该论文提出了NH-Fair基准，强调在统一数据、度量和训练协议下对视觉模型和大视觉-语言模型（LVLMs）进行公平性评估，并得出若干实践性结论，如精心调参的ERM基线难以被多数去偏方法稳定超越，且复合数据增强在保持性能的同时能显著缩小群体差距。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [11] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: HY3D-Bench提供了250k净化3D对象、结构化部件分解与125k合成样本，构成开放数据生态，缓解数据瓶颈并促进3D生成研究与应用。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成与表示学习受限于数据处理瓶颈和长尾类别分布不均，亟需统一的高质量训练资源来推动模型训练与应用落地。

Method: 通过构建严格的数据处理管线从大规模仓库中提取并净化250k高保真3D对象（包含水密网格和多视图渲染），引入结构化的部件级分解，以及运行可扩展的AIGC合成流程生成125k合成资产。

Result: 产出250k训练就绪高保真对象和125k合成资产，并通过训练Hunyuan3D-2.1-Small验证其有效性，展示对3D感知、机器人与数字内容创作的推动作用。

Conclusion: HY3D-Bench建立了一个统一且高质量的3D数据生态系统，通过数据清洗、部分级分解和合成资产扩充，缓解了3D生成领域的数据瓶颈并促进下游任务发展。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [12] [Entropy-Aware Structural Alignment for Zero-Shot Handwritten Chinese Character Recognition](https://arxiv.org/abs/2602.03913)
*Qiuming Luo,Tao Zeng,Feng Li,Heming Liu,Rui Mao,Chang Kong*

Main category: cs.CV

TL;DR: 提出熵感知结构对齐网络，通过熵先验、双视角偏旁树和Top-K融合，显著提升零样本HCCR的性能与数据效率。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法将字符视为平坦偏旁序列、忽视层次拓扑和不同组件信息密度不均的问题，提出了基于信息论的结构对齐方法以更好地捕捉判别性组件。

Method: 方法包括信息熵先验用于动态调节位置嵌入、双视角偏旁树提取多粒度结构特征、以及Top-K语义特征融合用于解码阶段的特征级共识校正。

Result: 在零样本设置下，方法在多个实验上取得新的最先进水平，显著优于现有基于CLIP的基线，并展示出极佳的数据效率和在少量支持样本下的快速适应性。

Conclusion: 本文提出了一种熵感知结构对齐网络，通过信息论建模弥合视觉-语义差距，在零样本手写汉字识别任务中实现了性能提升。

Abstract: Zero-shot Handwritten Chinese Character Recognition (HCCR) aims to recognize unseen characters by leveraging radical-based semantic compositions. However, existing approaches often treat characters as flat radical sequences, neglecting the hierarchical topology and the uneven information density of different components. To address these limitations, we propose an Entropy-Aware Structural Alignment Network that bridges the visual-semantic gap through information-theoretic modeling. First, we introduce an Information Entropy Prior to dynamically modulate positional embeddings via multiplicative interaction, acting as a saliency detector that prioritizes discriminative roots over ubiquitous components. Second, we construct a Dual-View Radical Tree to extract multi-granularity structural features, which are integrated via an adaptive Sigmoid-based gating network to encode both global layout and local spatial roles. Finally, a Top-K Semantic Feature Fusion mechanism is devised to augment the decoding process by utilizing the centroid of semantic neighbors, effectively rectifying visual ambiguities through feature-level consensus. Extensive experiments demonstrate that our method establishes new state-of-the-art performance, significantly outperforming existing CLIP-based baselines in the challenging zero-shot setting. Furthermore, the framework exhibits exceptional data efficiency, demonstrating rapid adaptability with minimal support samples.

</details>


### [13] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 为科学与PDE图像设计的Phaedra tokenizer，基于形状-增益量化与POD，能更好保留物理与频谱特性，提升重建与OOD泛化表现。


<details>
  <summary>Details</summary>
Motivation: 现有tokenizer为视觉感知优化，难以保留科学图像所需的大动态范围和物理/谱信息，导致在PDE相关任务上表现欠佳。

Method: 分析并比较了现有多种图像tokenizer在物理和频谱空间对PDE属性保真性的表现；提出Phaedra，结合形状-增益量化（shape-gain quantization）与正交分解（proper orthogonal decomposition）思想进行token化与重构。

Result: 在多种PDE数据集上，Phaedra在重建精度上稳定优于现有方法，并在三类OOD任务（已知PDE但条件不同、未知PDE、真实地球观测与气象数据）上表现出强泛化能力。

Conclusion: Phaedra是一种针对科学图像与PDE数据的专用tokenizer，能更好保留物理与频谱信息，从而提升重建精度与泛化能力。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [14] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: SpatiaLab提出了一个面向现实场景的空间推理基准（1400问答，6大类30子类），评测显示当前VLM在空间理解方面显著落后于人类，指明了改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖合成或LLM生成的环境，任务设计受限、缺乏真实场景的视觉噪声和多样空间关系，无法充分评估VLM在现实世界中的空间推理能力，因此需要一个更全面的评测基准。

Method: 构建一个包含1400个视觉问答对的现实场景基准，覆盖6大类（相对位置、深度与遮挡、方向、尺寸与尺度、空间导航、三维几何）和30个子任务，支持多项选择和开放式评估，并在多种领先的开源与闭源VLM上进行评测。

Result: 在多项选择设置下，最佳模型InternVL3.5-72B达54.93%准确率，人类为87.57%；开放式设置下所有模型普遍下降10-25%，GPT-5-mini最高为40.93%，人类为64.93%。这些评测结果表明模型在多类空间任务上存在明显不足。

Conclusion: SpatiaLab揭示当前视觉-语言模型在现实场景下的空间推理能力仍显不足，尤其在复杂空间关系、深度与遮挡、导航和三维几何理解方面存在明显差距，相较于人类表现仍有显著差距。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [15] [Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers](https://arxiv.org/abs/2602.03918)
*Peihao Xiang,Kaida Wu,Ou Bai*

Main category: cs.CV

TL;DR: 通过测量预训练权重的信息熵，Gardener能在无数据、一次性条件下准确识别并剪掉冗余Transformer块，实现极高压缩率且保留下游性能。


<details>
  <summary>Details</summary>
Motivation: 大型masked自监督视觉Transformer模型在部署和迁移学习中受限于资源，需识别哪些块对下游性能不重要以实现高效压缩而无需昂贵的逐块评估或大量数据。

Method: 提出Gardener方法：计算每个Transformer块预训练权重的信息熵，基于熵值排序并一次性移除低重要性块来剪枝模型，无需访问任何数据或反向传播迭代。

Result: 在VideoMAE-B上、跨多个剪枝比率与视频识别下游任务中，Gardener以极低计算开销匹配或优于现有无数据剪枝基线，接近灵敏度基准；即使剪掉高达91.7%块，仍能保持有竞争力的迁移性能，表明块级冗余显著且信息论指标有效。

Conclusion: 作者证明了在masked self-supervised视觉变换器中，不同块的重要性存在冗余且可以无数据估计，基于权重信息熵的度量与逐块移除并微调得到的灵敏度高度相关，从而实现有效的数据无关、一次性块级剪枝。

Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.

</details>


### [16] [TiCLS : Tightly Coupled Language Text Spotter](https://arxiv.org/abs/2602.04030)
*Leeje Jang,Yijun Lin,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: TiCLS将字符级预训练语言模型显式地融入端到端文本检测与识别，通过语言解码器与视觉特征融合，显著改善对短、片段化及模糊场景文本的识别，达到了ICDAR2015和Total-Text上的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索，且只隐式建模字符间局部依赖，忽视外部语言知识；已有整合尝试要么仅使用语言建模目标没有借助外部知识，要么直接采用预训练模型但与场景文本的词级粒度不匹配。作者认为利用字符级PLM能更好对齐场景文本的细粒度需求，提升对短或片段化文本的识别能力。

Method: 设计端到端文本检测与识别框架TiCLS：视觉编码器提取图像特征，字符级预训练语言模型被用作语言解码器初始化，构建一个融合模块将视觉和语言特征结合用于逐字符预测；训练目标包含检测和识别损失，语言解码器通过微调适配场景文本的词粒度。

Result: 在ICDAR2015和Total-Text数据集上，TiCLS在端到端文本识别任务上达到了新的SOTA。消融实验表明：使用字符级PLM初始化的语言解码器比随机初始化或词级PLM更能提升识别准确率；融合模块能有效结合视觉与语言信息，尤其提升对模糊和片段化文本的识别。

Conclusion: 本文提出的TiCLS通过引入字符级预训练语言模型（PLM）作为语言解码器初始化，并与视觉特征融合，实现了对场景文本中短、片段化或视觉歧义实例的鲁棒识别，实验在ICDAR2015和Total-Text上取得了最先进性能，表明外部语言知识能显著提升文本检测与识别。

Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.

</details>


### [17] [AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting](https://arxiv.org/abs/2602.04043)
*Joanna Kaleta,Bartosz Świrta,Kacper Kania,Przemysław Spurek,Marek Kowalski*

Main category: cs.CV

TL;DR: AnyStyle：一个可插拔的多模态（文本/图像）条件前向3D重建与风格化框架，实现无姿态、零-shot风格迁移，提升风格可控性且不损几何质量。


<details>
  <summary>Details</summary>
Motivation: 当前无姿态前向3D重建在快速生成高质量场景上有优势，但对外观风格的可控性和多样化支持不足，现有方法多依赖图像级条件限制灵活性，因此需要一个支持文本与图像任意输入的风格化机制。

Method: 构建模块化的风格化架构，对现有前向3D重建网络仅做最小改动，加入多模态（文本与图像）条件分支，将风格信息注入到外观预测路径；训练采用无姿态图像集合监督，零样式化时使用预训练的多模态编码器（如CLIP或类似模型）进行条件表示。

Result: 在多个基准和用户研究中，AnyStyle在风格可控性与视觉风格一致性上优于现有前向风格化方法，同时保持或略优于基线的几何重建质量。用户研究显示参与者更偏好AnyStyle生成的风格效果。

Conclusion: AnyStyle提出了一种基于多模态条件的前向（feed-forward）无姿态3D重建与风格化框架，能够实现文本和图像的零样式迁移，并能嵌入现有3DGS式重建主干，保持几何质量同时提升样式可控性。

Abstract: The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.

</details>


### [18] [A Parameterizable Convolution Accelerator for Embedded Deep Learning Applications](https://arxiv.org/abs/2602.04044)
*Panagiotis Mousouliotis,Georgios Keramidas*

Main category: cs.CV

TL;DR: 提出了基于HLS的可参数化CNN加速器HW/SW协同设计方法，能在多约束下优化FPGA实现，优于非参数化设计并具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统FPGA上实现的CNN加速器通常仅以最大化GOPS为目标，忽视嵌入式应用中对延迟、功耗、面积和成本的多约束需求。为此需要一种能够快速调优并在多约束下找到合适设计点的方法。

Method: 作者使用HLS工具实现可参数化的CNN加速器架构，使设计参数（如并行度、缓存大小、数据流复用策略等）易于调整。通过在硬件平台上对不同参数配置进行探索和验证，结合软件层的调度/量化策略，进行多目标优化以权衡GOPS、延迟、功耗和资源占用。

Result: 实验表明，基于HLS的参数化设计方法在多约束优化下优于非参数化设计方法，能够在保持或提升计算性能的同时更好地满足延迟、功耗和资源使用的要求，且方法可扩展至其他深度学习应用。

Conclusion: 该论文提出了一个基于硬件-软件协同设计的方法，通过使用高层次综合（HLS）工具对CNN加速器进行参数化描述，在满足延迟、功耗、面积和成本等嵌入式约束下，提升设计优化效率并获得优于非参数化设计的性能。

Abstract: Convolutional neural network (CNN) accelerators implemented on Field-Programmable Gate Arrays (FPGAs) are typically designed with a primary focus on maximizing performance, often measured in giga-operations per second (GOPS). However, real-life embedded deep learning (DL) applications impose multiple constraints related to latency, power consumption, area, and cost. This work presents a hardware-software (HW/SW) co-design methodology in which a CNN accelerator is described using high-level synthesis (HLS) tools that ease the parameterization of the design, facilitating more effective optimizations across multiple design constraints. Our experimental results demonstrate that the proposed design methodology is able to outperform non-parameterized design approaches, and it can be easily extended to other types of DL applications.

</details>


### [19] [Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs](https://arxiv.org/abs/2602.04046)
*Shikha Dubey,Patricia Raciti,Kristopher Standish,Albert Juan Ramon,Erik Ames Burlingame*

Main category: cs.CV

TL;DR: 提出一个基于降采样组织掩码和变形场的快速无监督WSI配准质量评估方法，在多标记与多专家验证中与人工评估高度相关，适合大规模实时质控。


<details>
  <summary>Details</summary>
Motivation: WSI配准对多模态集成分子分析重要，但缺乏地面真值使得大规模、可靠、低成本的配准质量评估困难。传统基于标注或强度相似度的方法耗时、主观或计算复杂。

Method: 对配准后的WSI对计算两类指标：1) 掩码基（降采样组织分割/掩码）度量全局结构对应；2) 变形场基（从配准变换获取）度量局部平滑性、连续性和变换真实性。两类指标联合用于无监督评分，支持实时评估并节省计算资源。

Result: 在多种IHC标记和多专家人工评估上，自动化指标与人工评分呈高相关性，证明该框架在无GT情况下能提供高保真、实时的配准质量控制，适用于大规模数字病理。

Conclusion: 提出了一种快速、无监督的WSI配准质量评估框架，结合降采样的组织掩码和变形场指标，在缺乏地面真值时能可靠评估H&E与IHC配对图像的配准质量。

Abstract: High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.

</details>


### [20] [Artifact Removal and Image Restoration in AFM:A Structured Mask-Guided Directional Inpainting Approach](https://arxiv.org/abs/2602.04051)
*Juntao Zhang,Angona Biswas,Jaydeep Rade,Charchit Shukla,Juan Ren,Anwesha Sarkar,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CV

TL;DR: 提出一套轻量、自动化、几何感知的AFM图像伪影检测与修复流程，包含分类、掩码分割与方向性插值修补，能在保留表面细节的同时高效去伪。


<details>
  <summary>Details</summary>
Motivation: AFM图像常受环境噪声、扫描误差和探针-样品相互作用导致的伪影影响，影响纳米尺度结构的准确表征，因而需要一个自动化且几何感知的修复方法。

Method: 方法包括三阶段：1) 使用分类模型判断图像是否含伪影；2) 对含伪影的图像，采用为AFM数据定制的轻量级语义分割网络生成精确的伪影掩码，并基于掩码的结构方向自适应扩展；3) 采用方向邻域插值进行修补以保持三维表面连续性，随后局部高斯平滑实现无缝恢复。系统集成GUI支持实时参数调整与批处理。

Result: 实验证明该框架能有效去除伪影并保留纳米级结构细节，提供了稳健的几何感知解决方案，支持实时交互与批量处理。

Conclusion: 该论文提出了一个轻量级、完全自动化的AFM图像伪影检测与修复框架，能够在去除伪影的同时保持纳米级结构细节，从而提高AFM数据的解析精度。

Abstract: Atomic Force Microscopy (AFM) enables high-resolution surface imaging at the nanoscale, yet the output is often degraded by artifacts introduced by environmental noise, scanning imperfections, and tip-sample interactions. To address this challenge, a lightweight and fully automated framework for artifact detection and restoration in AFM image analysis is presented. The pipeline begins with a classification model that determines whether an AFM image contains artifacts. If necessary, a lightweight semantic segmentation network, custom-designed and trained on AFM data, is applied to generate precise artifact masks. These masks are adaptively expanded based on their structural orientation and then inpainted using a directional neighbor-based interpolation strategy to preserve 3D surface continuity. A localized Gaussian smoothing operation is then applied for seamless restoration. The system is integrated into a user-friendly GUI that supports real-time parameter adjustments and batch processing. Experimental results demonstrate the effective artifact removal while preserving nanoscale structural details, providing a robust, geometry-aware solution for high-fidelity AFM data interpretation.

</details>


### [21] [Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal](https://arxiv.org/abs/2602.04053)
*Rio Aguina-Kang,Kevin James Blackburn-Matzen,Thibault Groueix,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出SeeingThroughClutter：用VLM协同、逐个移除前景物体并分别拟合3D的迭代管线，实现单张图像中对遮挡和杂乱场景下结构化三维重建的无训练或少训练鲁棒方法，在3D-Front和ADE20K上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖语义分割和深度估计等中间任务，在复杂、遮挡场景中表现有限。通过逐个移除前景物体并分别建模，可以将复杂重建问题拆解为一系列更易处理的子问题，从而提升鲁棒性。

Method: 使用视觉语言模型（VLM）作为协调器，按序检测、分割并移除图像中的前景物体，然后对每个移除的物体进行3D拟合，迭代处理使得后续物体的分割更干净；整个流程不依赖任务特定训练，借助基础模型能力完成各模块。

Result: 在3D-Front和ADE20K数据集上展示了最先进的鲁棒性，尤其在高度遮挡和杂乱场景中表现优越。

Conclusion: 该工作提出了一种无需特定任务训练、基于迭代移除前景物体的单张图像到结构化三维重建的方法，通过把复杂场景分解为一系列更简单的子任务，从而在遮挡和杂乱环境下获得稳健的分割与重建结果。

Abstract: We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/

</details>


### [22] [iSight: Towards expert-AI co-assessment for improved immunohistochemistry staining interpretation](https://arxiv.org/abs/2602.04063)
*Jacob S. Leiby,Jialu Yao,Pan Lu,George Hu,Anna Davidian,Shunsuke Koga,Olivia Leung,Pravin Patel,Isabella Tondi Resta,Rebecca Rojansky,Derek Sung,Eric Yang,Paul J. Zhang,Emma Lundberg,Dokyoon Kim,Serena Yeung-Levy,James Zou,Thomas Montine,Jeffrey Nirschl,Zhi Huang*

Main category: cs.CV

TL;DR: 作者发布 HPA10M 数据集并提出多任务学习模型 iSight，用于 IHC 染色评估，显著优于现有基础模型且在病理学家协助下提高诊断一致性。


<details>
  <summary>Details</summary>
Motivation: IHC 是病理诊断和分型中常用的蛋白表达检测手段，但存在领域特异性差异，限制了现有基于 H&E 的 AI 模型在 IHC 上的泛化。作者构建大规模标注数据集并提出专门的多任务模型以提升自动化 IHC 评估性能。

Method: 构建大规模 IHC 数据集，采用多任务学习框架将图像视觉特征与组织元数据通过 token 级注意力融合，同时输出染色位置、强度、数量、组织类型和恶性状态；在多个数据集上评估并与 PLIP、CONCH 等微调模型比较，进行校准分析与病理学家用户研究。

Result: 构建 HPA10M（1,049.5 万张 IHC 图像及元数据），训练多任务模型 iSight，实现对染色位置、强度、数量、组织类型及恶性状态的预测；模型在多项指标上超过微调基础模型并具有良好校准性；在病理学家用户研究中提升了评估准确率与组间一致性。

Conclusion: HPA10M 与 iSight 为自动化 IHC 评估奠定基础，能在临床工作流中提高一致性与可靠性，促进专家—AI 协作。

Abstract: Immunohistochemistry (IHC) provides information on protein expression in tissue sections and is commonly used to support pathology diagnosis and disease triage. While AI models for H\&E-stained slides show promise, their applicability to IHC is limited due to domain-specific variations. Here we introduce HPA10M, a dataset that contains 10,495,672 IHC images from the Human Protein Atlas with comprehensive metadata included, and encompasses 45 normal tissue types and 20 major cancer types. Based on HPA10M, we trained iSight, a multi-task learning framework for automated IHC staining assessment. iSight combines visual features from whole-slide images with tissue metadata through a token-level attention mechanism, simultaneously predicting staining intensity, location, quantity, tissue type, and malignancy status. On held-out data, iSight achieved 85.5\% accuracy for location, 76.6\% for intensity, and 75.7\% for quantity, outperforming fine-tuned foundation models (PLIP, CONCH) by 2.5--10.2\%. In addition, iSight demonstrates well-calibrated predictions with expected calibration errors of 0.0150-0.0408. Furthermore, in a user study with eight pathologists evaluating 200 images from two datasets, iSight outperformed initial pathologist assessments on the held-out HPA dataset (79\% vs 68\% for location, 70\% vs 57\% for intensity, 68\% vs 52\% for quantity). Inter-pathologist agreement also improved after AI assistance in both held-out HPA (Cohen's $κ$ increased from 0.63 to 0.70) and Stanford TMAD datasets (from 0.74 to 0.76), suggesting expert--AI co-assessment can improve IHC interpretation. This work establishes a foundation for AI systems that can improve IHC diagnostic accuracy and highlights the potential for integrating iSight into clinical workflows to enhance the consistency and reliability of IHC assessment.

</details>


### [23] [VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding](https://arxiv.org/abs/2602.04094)
*Junbo Zou,Ziheng Huang,Shengjie Zhang,Liwen Zhang,Weining Shen*

Main category: cs.CV

TL;DR: VideoBrain通过CLIP与Uniform双代理协同和行为感知奖励，学习自适应帧采样策略，在减少帧数的同时显著提升长视频VLM的理解性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解要求在计算受限下抓取分散在数千帧的信息，固定或一次性抽取关键帧容易丢失信息且无法纠错，因此需要自适应的、可恢复的采样策略。

Method: 提出双代理架构：CLIP代理用于语义检索以寻找分布式关键信息，Uniform代理用于在区间内进行密集时间采样；VLM直接感知帧并判断信息是否充分；引入行为感知奖励和数据分类管线以避免代理被滥用。

Result: 在四个长视频基准上，VideoBrain相比基线在性能上提升3.5%–9.0%，同时使用的帧数减少30%–40%，并在短视频数据集上表现出良好的跨数据集泛化能力。

Conclusion: 本文提出了VideoBrain，通过学习的采样策略让VLM在长视频中自适应获取视觉信息，从而在减少帧数的同时提升理解性能。

Abstract: Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.

</details>


### [24] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 本文提出DMS2F-HAD：双分支Mamba模型+动态门控融合，能高效捕捉空间/光谱信息，在14个数据集上达98.78%平均AUC，推理速度显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法要么无法有效捕捉长程光谱依赖（CNN），要么计算开销高（Transformer），因此需要兼顾长程依赖建模与高效性的模型。

Method: 提出双分支Mamba模型，分别学习空间和光谱特征，使用线性时间建模及动态门控融合机制整合分支特征。

Result: 在14个基准数据集上平均AUC为98.78%，推理速度比可比深度学习方法快4.6倍，展示了良好泛化性与可扩展性。

Conclusion: DMS2F-HAD在多数据集上表现优异，兼顾精度与效率，具有较好实用潜力。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [25] [SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy](https://arxiv.org/abs/2602.04108)
*O. Leon Barbed,José M. M. Montiel,Pascal Fua,Ana C. Murillo*

Main category: cs.CV

TL;DR: 针对内镜视频提出SuperPoint-E和Tracking Adaptation监督，提升特征密度与判别性，显著增强SfM重建质量，优于SuperPoint与COLMAP。


<details>
  <summary>Details</summary>
Motivation: 内镜视频中光照、纹理稀少和场景重复性高导致传统局部特征在SfM中表现差，进而影响三维重建的稠密度与连续性，因此需要针对性地提升特征检测与描述性能。

Method: 在SuperPoint框架基础上引入Tracking Adaptation监督，调整检测器以在内镜场景中更密集且稳定地触发关键点，同时训练判别性更强的描述子，减少对引导匹配的依赖。通过在真实内镜录像上的广泛实验选择最佳配置并评估特征质量。

Result: SuperPoint-E在内镜数据上比原始SuperPoint和COLMAP管线表现更好：检测更密集、检测精度更高、描述子更具判别力，生成的3D重建更稠密，覆盖更多且更长的视频片段，指导匹配步骤几乎可省略。

Conclusion: 本文提出了SuperPoint-E，通过Tracking Adaptation监督策略，专门提升内镜视频中局部特征的检测与描述，显著改进了SfM重建效果。

Abstract: In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos. We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy. Extensive experimentation on real endoscopy recordings studies our approach's most suitable configuration and evaluates SuperPoint-E feature quality. The comparison with other baselines also shows that our 3D reconstructions are denser and cover more and longer video segments because our detector fires more densely and our features are more likely to survive (i.e. higher detection precision). In addition, our descriptor is more discriminative, making the guided matching step almost redundant. The presented approach brings significant improvements in the 3D reconstructions obtained, via SfM on endoscopy videos, compared to the original SuperPoint and the gold standard SfM COLMAP pipeline.

</details>


### [26] [JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models](https://arxiv.org/abs/2602.04142)
*Hiroshi Sasaki*

Main category: cs.CV

TL;DR: JSynFlow是一个用LLM合成的日文流程图视觉QA数据集，包含任务描述、DSL渲染的流程图图像与QA对，能显著提升VLM在流程图理解任务上的表现，数据公开可用。


<details>
  <summary>Details</summary>
Motivation: 手工构建大规模流程图图像与文本标注成本高昂，且缺乏日语领域的大规模标注数据，故采用合成方法以降低成本并快速扩展数据规模。

Method: 利用LLMs生成面向不同业务岗位的任务描述、基于领域专用语言(DSL)渲染流程图图像，并生成对应的QA对；构建大规模合成数据集用于微调VLM。

Result: 在论文中通过实验展示，使用JSynFlow进行微调后，VLM在流程图QA任务上性能显著提升；数据集已在Hugging Face公开发布。

Conclusion: JSynFlow通过使用大语言模型自动合成日文流程图视觉QA数据集，能有效提升VLM在流程图理解与QA任务上的表现。

Abstract: Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.

</details>


### [27] [Context Determines Optimal Architecture in Materials Segmentation](https://arxiv.org/abs/2602.04154)
*Mingjian Lu,Pawan K. Tripathi,Mark Shteyn,Debargha Ganguly,Roger H. French,Vipin Chaudhary,Yinghui Wu*

Main category: cs.CV

TL;DR: 提出跨模态材料图像分割评估框架，系统比较多种架构并加入OOD检测与反事实解释，指导在不同成像条件下选择合适模型并评估其可信度。


<details>
  <summary>Details</summary>
Motivation: 现有分割架构通常仅在单一成像模态上基准测试，导致研究者难以为具体材料表征任务选择合适模型，也难判断模型在新样本上的可靠性。

Method: 构建跨模态评估管线：选择七个材料显微图像数据集，比较UNet、DeepLabv3+等六种编码器-解码器组合的分割性能；引入OOD检测与反事实解释模块来评估模型在新样本上的信任度与解释性；统计分析架构在不同模态、噪声级别和难度场景下的性能差异。

Result: 实验证明UNet在高对比度2D成像（如某些光学或SEM图像）上表现最佳，而在最困难的样本（噪声、低对比度或复杂拓扑）中DeepLabv3+更优。此外，OOD检测与反事实解释为模型部署提供了可操作的可靠性与可解释性反馈。

Conclusion: 本文提出了一个跨模态材料图像分割评估框架，通过对不同显微成像模态（SEM, AFM, XCT, 光学显微镜）和多个数据集上六种经典编码器-解码器组合的系统评估，发现最佳网络结构随成像场景系统性变化，填补了现实部署中缺乏针对特定成像条件选择模型及评估可靠性的空白。

Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.

</details>


### [28] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出ISCS：在扩散采样阶段对切片噪声一致性进行控制，能无缝插入现有2D扩散到3D重建流程，减少切片间不连续性并提升3D医学影像质量，且不需额外训练或计算开销。


<details>
  <summary>Details</summary>
Motivation: 3D医疗图像重建中直接训练3D扩散模型受数据稀缺和训练计算开销限制，常用的以2D扩散先验重建堆叠切片方法会因扩散采样的随机性产生明显的切片间不连续性，现有通过轴向连续性正则化的方法存在超参数敏感和过平滑问题。

Method: 提出了Inter-Slice Consistent Stochasticity (ISCS)策略，在扩散采样阶段对各切片的随机噪声分量进行一致性控制，使得不同切片采样轨迹对齐，从而减少切片间不连续性。该方法为即插即用，无需新增损失项或优化步骤，可直接用于任何使用2D训练扩散模型的3D重建流水线。

Result: 在多项医疗影像重建任务上，ISCS在不增加计算代价的情况下显著改善了重建的切片连续性与整体影像质量，验证了通过控制跨切片随机性以提升3D一致性的有效性。

Conclusion: 通过控制扩散采样过程中的跨切片随机性，可在不改变模型训练或增加计算成本的前提下显著提升基于2D扩散模型的3D医疗图像重建一致性与质量。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [29] [Point2Insert: Video Object Insertion via Sparse Point Guidance](https://arxiv.org/abs/2602.04167)
*Yu Zhou,Xiaoyan Yang,Bojia Zi,Lihan Zhang,Ruijie Sun,Weishi Zheng,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出只需少量正负稀疏点的Point2Insert，通过两阶段训练与掩码模型蒸馏，实现高效精确的视频物体插入，性能优于多种更大模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码导向需要繁琐标注且指令式方法难以精确放置，因而提出只需稀疏正负点的方案以减少标注负担并实现精细空间控制。

Method: 两阶段训练：阶段1在稀疏点或二值掩码条件下训练插入模型；阶段2用由物体移除模型合成的配对视频进一步训练以适配视频插入；并通过掩码引导的教师模型对点引导模型进行蒸馏。

Result: 在大量实验中，Point2Insert持续优于强基线，并且超越参数量大10倍的模型，表明稀疏点提示与蒸馏策略有效。

Conclusion: Point2Insert通过稀疏点提示实现视频中用户友好的物体插入，兼顾易用性与精确定位，优于现有掩码或指令方法。

Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.

</details>


### [30] [Partial Ring Scan: Revisiting Scan Order in Vision State Space Models](https://arxiv.org/abs/2602.04170)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin li,Ming-Ching Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: 提出基于同心环的扫描与部分通道过滤的PRISMamba，使视觉SSM在准确性、效率和旋转鲁棒性上显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉SSM需要把2D图像按预定扫描顺序展开为1D序列，扫描顺序改变空间邻接性和对象连续性，导致性能下降并在几何变换（如旋转）下更脆弱，作者希望设计一种对扫描顺序更鲁棒且高效的序列化方法。

Method: 提出Partial RIng Scan Mamba (PRISMamba)：1) 将图像划分为同心环并在环内进行顺序无关的聚合；2) 用多个短的径向SSM在环间传播上下文；3) 通过部分通道过滤把信息丰富的通道送入循环环路径，其余通道走轻量残差支路以提高效率。

Result: 在ImageNet-1K上，PRISMamba达成84.5% Top-1，3.9G FLOPs，A100上吞吐3054 img/s，优于VMamba且Flops更低；在旋转变换下性能保持稳定，而固定路径扫描下降1~2%。

Conclusion: PRISMamba通过重新设计图像序列化路径（同心环划分与环内无序聚合）、短径向SSM传播和部分通道过滤，显著提升了视觉SSM在准确性、效率与旋转鲁棒性方面的表现。

Abstract: State Space Models (SSMs) have emerged as efficient alternatives to attention for vision tasks, offering lineartime sequence processing with competitive accuracy. Vision SSMs, however, require serializing 2D images into 1D token sequences along a predefined scan order, a factor often overlooked. We show that scan order critically affects performance by altering spatial adjacency, fracturing object continuity, and amplifying degradation under geometric transformations such as rotation. We present Partial RIng Scan Mamba (PRISMamba), a rotation-robust traversal that partitions an image into concentric rings, performs order-agnostic aggregation within each ring, and propagates context across rings through a set of short radial SSMs. Efficiency is further improved via partial channel filtering, which routes only the most informative channels through the recurrent ring pathway while keeping the rest on a lightweight residual branch. On ImageNet-1K, PRISMamba achieves 84.5% Top-1 with 3.9G FLOPs and 3,054 img/s on A100, outperforming VMamba in both accuracy and throughput while requiring fewer FLOPs. It also maintains performance under rotation, whereas fixed-path scans drop by 1~2%. These results highlight scan-order design, together with channel filtering, as a crucial, underexplored factor for accuracy, efficiency, and rotation robustness in Vision SSMs. Code will be released upon acceptance.

</details>


### [31] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 通过2D全息时空表示与FFT频域门控，HoloEv-Net在效率与准确率上同时突破，适合边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 解决现有EAR方法中体素表示的计算冗余、多分支结构冗余以及频谱信息未被充分利用的问题，提高识别精度与计算效率以适配边缘部署。

Method: 提出Compact Holographic Spatiotemporal Representation(CHSR)，将水平空间信息嵌入时间-高度(T-H)视图以替代稠密体素；设计Global Spectral Gating(GSG)模块，利用FFT在频域进行全局token混合。

Result: HoloEv-Net-Base在THU-EACT-50-CHL、HARDVS和DailyDVS-200上分别超越现有方法10.29%、1.71%和6.25%；HoloEv-Net-Small在参数、FLOPs和延迟上大幅降低（参数减5.4倍，FLOPs减300倍，延迟减2.4倍），同时保持竞争力的准确率。

Conclusion: 该论文提出了HoloEv-Net，通过紧凑的二维全息时空表示和频域全局门控模块，显著提升了事件相机动作识别的性能与效率。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [32] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 将自然语言乘客指令接入OpenEMMA，在真实doScenes数据上实现指令驱动轨迹预测基线，显著提高鲁棒性并可小幅改善精度，开放代码与提示以促进复现。


<details>
  <summary>Details</summary>
Motivation: 现有指令驱动规划多依赖仿真或受限命令词表，难以推广到真实世界；doScenes首次提供自由形式、有指称关系的真实指令与运动真值配对，促使研究指令条件下的规划。

Method: 将doScenes中的自然语言指令作为乘客式提示接入OpenEMMA的视觉-语言接口，输入前摄像头图像和自车状态，输出10步速度—曲率轨迹；在849个标注场景上以ADE评估模型表现，并分析不同提示措辞对预测轨迹的影响。

Result: 指令条件显著减少极端失败，平均ADE减少98.7%；在去除异常值后，良好措辞的提示仍能将ADE进一步改善最多5.1%；并公开了评估提示与脚本以便复现。

Conclusion: 本文将开放源码的多模态大模型驾驶框架OpenEMMA扩展为支持自由形式乘客指令的条件轨迹预测，并在真实世界数据集doScenes上建立了可复现基线，证明指令能显著提高鲁棒性并在去除异常值后提升轨迹对齐。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [33] [DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding](https://arxiv.org/abs/2602.04188)
*Ning Zhang,Zhengyu Li,Kwong Weng Loh,Mingxi Xu,Qi Wang,Zhengyu Wen,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: DiMo通过离散掩码迭代细化、RVQ和GRPO，在单模型内统一T2M、M2T和M2M，提供质量-延迟可控的强生成与双向理解能力。


<details>
  <summary>Details</summary>
Motivation: 弥补现有以文本到动作为主的掩码建模生成方法的单向性，提出能同时支持生成与理解的统一框架，并提供推理时质量-延迟的可调机制。

Method: 使用残差向量量化(RVQ)提高动作令牌重建保真度，采用掩码建模的离散扩散式迭代解码替代自回归，及引入Group Relative Policy Optimization(GRPO)增强对齐与可控性。

Result: 在HumanML3D和KIT-ML数据集上取得了较高的动作质量，并在双向理解任务上表现有竞争力，同时展示了无文本动作补全、文本引导动作预测与动作描述纠错等能力。

Conclusion: DiMo通过离散扩散风格的掩码令牌迭代细化，成功统一了文本到动作、动作到文本和无文本动作到动作任务，在单一模型下实现双向理解与生成。

Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

</details>


### [34] [Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution](https://arxiv.org/abs/2602.04193)
*Hyeonjae Kim,Dongjin Kim,Eugene Jin,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 利用流匹配在潜在退化空间从单张HR合成逼真多样的LR图像，构建大规模真实感SR训练集，从而显著提升在真实降质场景下的超分辨性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超分辨方法在合成退化（如bicubic）上表现优异，但在真实世界复杂非线性退化（噪声、模糊、压缩）下效果差。真实LR-HR成对数据采集昂贵且局限于特定放大倍数，因此需要一种单张HR即可生成逼真多样LR以扩展训练数据的方法。

Method: 提出了通过在潜在退化空间中应用流匹配技术来从单张HR图像生成LR图像。具体包括：1）构建潜在退化表示并学习其分布；2）使用流匹配（flow matching）方法在该潜在空间中生成多样且连续的退化样本；3）将生成的退化映射回像素域以得到具有噪声、模糊和压缩伪影等复杂退化的LR图像；4）基于合成数据训练传统与任意尺度SR模型并进行评估。

Result: 合成的LR图像在量化指标和视觉质量上与真实退化分布匹配良好；用这些合成数据训练的超分辨模型（包括传统固定尺度和任意尺度模型）在多种真实降质测试集上均有显著PSNR/SSIM及视觉改善。

Conclusion: 本文提出了一种基于流匹配（flow matching）的潜在退化空间建模方法，从单张高分辨率图像合成逼真的低分辨率图像，能够生成未见退化程度的真实伪影，从而用于大规模真实感超分辨率训练数据集构建。实验证明合成的低分辨率图像在定量和定性上接近真实退化，且用这些数据训练的传统和任意放缩比的超分辨率模型均显著提升重建效果。

Abstract: While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.

</details>


### [35] [VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents](https://arxiv.org/abs/2602.04202)
*Feng Wang,Yichun Shi,Ceyuan Yang,Qiushan Guo,Jingxiang Sun,Alan Yuille,Peng Wang*

Main category: cs.CV

TL;DR: 通过关键帧+残差token的解耦token化，VTok以更短的序列实现更好的视频理解与生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言系统常用简单的帧采样进行token化，导致序列冗长且难以捕捉连续帧间的冗余和运动变化。作者希望设计一种更紧凑且能保持时间一致性的token化方法，兼顾理解与生成任务。

Method: 提出将视频的空间和时间表示解耦：保留单个关键帧的空间token，其他每帧只编码为相对于关键帧的残差token，显著减少token总量（由帧数×每帧token数降低为两者之和）。

Result: VTok在多个视频理解和文本到视频生成基准上优于使用朴素token化的基线：例如TV-Align基准上提升3.4%准确率，VBench上提升1.9%分数，并生成更连贯的运动与更强的文本引导性，同时每视频的token序列更短。

Conclusion: VTok通过保留关键帧的完整空间特征并将后续帧编码为残差token，实现了紧凑且有表达力的视频token化，从而提升了理解与生成任务的性能与效率。

Abstract: This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.

</details>


### [36] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: 本文提出AGMA：通过从数据提取行为模式并蒸馏为场景自适应的高斯混合先验，解决先验错配问题，显著提升轨迹预测的准确性和多样性，并在多个基准数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法的先验（固定或学得）无法充分覆盖可能的未来行为分布，导致预测精度和多样性受限；理论上证明预测误差被先验质量下界约束，因此优质先验是关键瓶颈。

Method: AGMA先从训练数据中提取多样化的行为模式（形成锚点或簇），然后将这些模式蒸馏为对场景自适应的全局高斯混合先验，在推理时以此作为条件生成未来轨迹预测。

Result: 在ETH-UCY、Stanford Drone和JRDB数据集上，AGMA实现了最新的最优性能，实验结果证明高质量先验对轨迹预测至关重要。

Conclusion: AGMA通过构建自适应高斯混合先验，有效缓解了先验错配问题，从而提升了轨迹预测的准确性和多样性。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [37] [Adaptive 1D Video Diffusion Autoencoder](https://arxiv.org/abs/2602.04220)
*Yao Teng,Minxuan Lin,Xian Liu,Shuai Wang,Xiao Yang,Xihui Liu*

Main category: cs.CV

TL;DR: 提出 One-DVA：一个基于 Transformer 的自适应一维编码 + 扩散解码的视频自编码器，支持可变长度压缩并在重建与生成上优于或可比传统 3D-CNN VAE。


<details>
  <summary>Details</summary>
Motivation: 现有视频自编码器存在三大局限：固定速率压缩导致在简单视频上浪费令牌；基于 CNN 的结构不利于可变长度潜空间建模；确定性解码器难以从高度压缩的潜变量中恢复合适细节。

Method: 采用 query-based 视觉 Transformer 提取时空特征生成一维可变长度潜变量，使用可变长度 dropout 动态调整潜变量长度，解码器为基于像素空间的扩散 Transformer，将潜变量作为条件进行视频重建。训练采用两阶段策略，第一阶段训练自编码器以匹配重建指标，第二阶段正则化潜变量分布并微调解码器以减少生成时伪影。

Result: One-DVA 在相同压缩率下达到与 3D-CNN VAE 可比的重建表现，且凭借自适应压缩能力能实现更高压缩比。通过对潜变量分布的正则化和解码器微调，改进了下游潜空间生成质量并减轻了生成伪影。

Conclusion: One-DVA 提出了一种基于 Transformer 的自适应一维编码与扩散解码的视频自编码器框架，能够在相同压缩率下达到与 3D-CNN VAE 可比的重建性能，并支持可变长度自适应压缩以获得更高压缩比。

Abstract: Recent video generation models largely rely on video autoencoders that compress pixel-space videos into latent representations. However, existing video autoencoders suffer from three major limitations: (1) fixed-rate compression that wastes tokens on simple videos, (2) inflexible CNN architectures that prevent variable-length latent modeling, and (3) deterministic decoders that struggle to recover appropriate details from compressed latents. To address these issues, we propose One-Dimensional Diffusion Video Autoencoder (One-DVA), a transformer-based framework for adaptive 1D encoding and diffusion-based decoding. The encoder employs query-based vision transformers to extract spatiotemporal features and produce latent representations, while a variable-length dropout mechanism dynamically adjusts the latent length. The decoder is a pixel-space diffusion transformer that reconstructs videos with the latents as input conditions. With a two-stage training strategy, One-DVA achieves performance comparable to 3D-CNN VAEs on reconstruction metrics at identical compression ratios. More importantly, it supports adaptive compression and thus can achieve higher compression ratios. To better support downstream latent generation, we further regularize the One-DVA latent distribution for generative modeling and fine-tune its decoder to mitigate artifacts caused by the generation process.

</details>


### [38] [An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation](https://arxiv.org/abs/2602.04227)
*Hanuman Verma,Kiho Im,Pranabesh Maji,Akshansh Gupta*

Main category: cs.CV

TL;DR: 将直觉模糊逻辑融入UNet的输入表示（隶属/非隶属/犹豫）能够更有效应对部分容积效应，提高MRI脑图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统UNet对部分容积效应引起的组织模糊性和边界不确定性处理不足，需引入不确定性建模以提升分割鲁棒性与准确性。

Method: 在UNet的输入处理阶段引入直觉模糊逻辑，将原始像素映射为隶属度、非隶属度和犹豫度三分量，并在网络中利用这些成分进行特征学习与融合，最终输出分割结果。

Result: 在IBSR数据集上进行验证，使用准确率、Dice系数和IoU评估，实验结果表明IF-UNet在分割质量上优于基础UNet，特别是在处理组织边界和模糊区域时表现更好。

Conclusion: IF-UNet通过将直觉模糊逻辑与UNet结合，能够更好地处理部分容积效应和边界不确定性，从而提升MRI脑图像分割的质量。

Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.

</details>


### [39] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出通过原型引导的稀疏Transformer解码器和去噪训练策略，显著提升了相机驱动的实时稀疏3D占据预测的速度与精度。


<details>
  <summary>Details</summary>
Motivation: 从摄像头实时得到高精度3D占据预测对自动驾驶至关重要。稀疏3D表示解决了编码瓶颈，但使解码器面临如何从稀疏且非均匀分布的体素特征中高效聚合信息的问题，现有的密集注意力计算代价高昂。

Method: 设计了稀疏原型选择机制：每个查询自适应选取若干最显著的体素特征作为原型；在聚合阶段仅聚合这些原型以减少计算量。同时引入去噪范式，使用真值掩码在多层解码器中稳定查询—原型关联。

Result: 提出的SPOT-Occ在速度上显著优于先前方法，并在精度上有所提升，论文并开源代码。

Conclusion: 本文提出了基于原型的稀疏Transformer解码器（SPOT-Occ），通过两阶段的原型引导特征选择与聚合，替代繁重的密集注意力，实现了高效且精确的稀疏3D占据预测。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [40] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: ACIL把主动学习引入类增量学习，通过不确定性与多样性准则选择外样本标注并保留，从而在降低标注成本的同时缓解灾难性遗忘，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统类增量学习假设每个回合的所有样本均被标注，这在注释成本和实际可行性上存在问题；同时，主动学习可在大量未标注数据中挑选信息量大的样本以降低人类标注成本。将主动学习与类增量学习结合可以同时降低注释开销并减少遗忘。

Method: 提出基于不确定性与多样性联合准则的样本选择策略：在每个学习回合中，从未标注数据中选取既能提高模型性能又能代表数据分布的示例作为外样本（exemplars）进行标注，并将这些样本在下一回合保留用于训练。

Result: 在若干视觉数据集上进行广泛实证分析，结果显示ACIL在注释成本显著降低的同时，能在防止模型遗忘方面优于或可比于相关基线方法。

Conclusion: 该论文提出了一种在类增量学习情境下结合主动学习的框架（ACIL），旨在通过选取高不确定性与高多样性的样本进行标注，从而减少注释成本并缓解灾难性遗忘。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [41] [Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery](https://arxiv.org/abs/2602.04257)
*Jiaxin Cen,Xudong Mao,Guanghui Yue,Wei Zhou,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 提出一个深度引导的三段式框架（多尺度融合、深度标定估计、运动-深度对齐精炼），显著改善单目视频人体网格恢复的尺度一致性和时序稳定性，尤其在遮挡严重场景表现突出。


<details>
  <summary>Details</summary>
Motivation: 单目视频人体网格恢复受限于深度与尺度不确定性，单纯依赖RGB特征和时序平滑难以解决深度排序、尺度漂移和遮挡下的抖动问题，因此需要将深度几何先验与时序运动信息相结合以获得度量一致且时序稳定的重建。

Method: 方法由三部分组成：1) 深度引导的多尺度融合模块（Depth-Guided Multi-Scale Fusion），通过置信度感知门控自适应融合几何先验与RGB多尺度特征；2) D-MAPS估计器（Depth-guided Metric-Aware Pose and Shape），利用深度标定的骨骼统计信息进行尺度一致的初始姿态与形状估计；3) MoDAR（Motion-Depth Aligned Refinement）模块，通过运动动力学与几何线索之间的跨模态注意力实现时序一致性的精炼。

Result: 在三个具有挑战性的基准上，方法在空间精度和抗遮挡性方面显著优于现有方法，同时保持计算效率。结果包括在深度鲁棒性、尺度一致性以及时序平滑性指标上的提升。

Conclusion: 本论文提出的深度引导框架通过结合几何先验与RGB特征，并在估计与优化阶段引入深度校准统计与跨模态时序对齐，有效缓解了单目视频人体网格恢复中的尺度漂移、深度歧义和遮挡导致的不稳定性。

Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.

</details>


### [42] [Decoupled Hierarchical Distillation for Multimodal Emotion Recognition](https://arxiv.org/abs/2602.04260)
*Yong Li,Yuanzhi Wang,Yi Ding,Shiqing Zhang,Ke Lu,Cuntai Guan*

Main category: cs.CV

TL;DR: DHMD通过解耦特征与粗/细粒度分层蒸馏，有效改善跨模态对齐与知识转移，在两大数据集上显著提升MER性能并提供可解释性视觉证据。


<details>
  <summary>Details</summary>
Motivation: 现有MER方法受限于模态异质性和不同模态对情感识别贡献不均，直接融合可能引入噪声或丢失互补信息，需要一种能区分同质与异质信息并自适应传递有用知识的框架。

Method: 方法包括：1) 使用自回归机制将每个模态特征分解为模态-无关（同质）与模态-独有（异质）成分；2) 在每个解耦后的特征空间中通过图蒸馏单元（GD-Unit）进行粗粒度知识蒸馏，动态图用于自适应模态间蒸馏；3) 通过跨模态字典匹配进行细粒度蒸馏，对齐语义粒度以生成更具判别性的表示。

Result: 在CMU-MOSI/CMU-MOSEI上分别在ACC7、ACC2和F1上取得相对提升（1.3%/2.4%、1.3%/1.9%、1.9%/1.8%），同时可视化显示图边与字典激活在模态无关/模态独有空间呈现有意义分布。

Conclusion: 本文提出的DHMD通过将每个模态特征解耦为模态无关与模态独有部分，并采用分层知识蒸馏策略，有效缓解了多模态异质性与模态间贡献差异问题，实验在CMU-MOSI/CMU-MOSEI上优于现有方法。

Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\%/2.4\% (ACC$_7$), 1.3\%/1.9\% (ACC$_2$) and 1.9\%/1.8\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.

</details>


### [43] [KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing](https://arxiv.org/abs/2602.04268)
*Siyu Jiang,Feiyang Chen,Xiaojin Zhang,Kun He*

Main category: cs.CV

TL;DR: KVSmooth是一种训练免费、推理时对KV缓存执行注意力熵引导的EMA平滑方法，有效减少MLLM的视觉幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: MLLM在长序列解码中常发生语义漂移，生成与视觉输入不一致的内容（幻觉）；需要一种高效、无需重训练且能在推理时减少幻觉的方法。

Method: 在推理时对KV-Cache中的keys和values应用指数移动平均(EMA)，并通过每个token注意力分布的熵来度量其“sink degree”，据此自适应调整平滑强度，从而抑制语义漂移。

Result: 在实验中显著降低了幻觉指标（CHAIR_S由41.8降至18.2），同时改善整体F1从77.5提升到79.2，在精度和召回率上均取得提升。

Conclusion: KVSmooth有效通过对KV缓存的键和值进行基于注意力熵的自适应指数移动平均平滑，在推理阶段无需训练或修改模型即可显著降低多模态大模型的幻觉现象并提升整体性能。

Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.
  To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.
  Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.

</details>


### [44] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: SkeletonGaussian：骨架驱动的可编辑4D动态3D Gaussian生成，结合LBS与六平面细化，实现高质量且可编辑的单目视频到动态3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有4D生成方法通常使用隐式变形场表示运动，导致可控性与可编辑性差。通过将运动显式分解为骨架驱动的刚体部分与补充的非刚体部分，可以实现直观的运动编辑和更好的物理可解释性。

Method: 从单目视频中提取鲁棒骨架，使用线性混合蒙皮（LBS）对骨架驱动的刚体运动进行建模，然后通过基于六平面的细化模块对非刚体形变进行建模。整体表示采用可渲染的动态3D Gaussian场，并在训练中联合优化骨架参数、LBS权重和六平面细化网络。

Result: 在多个单目视频数据集上，SkeletonGaussian在生成质量（例如重建误差和观测一致性）上超过了基线方法，同时支持骨骼变形、关键帧编辑等直观的动作编辑操作。消融实验显示层次化分解与六平面细化均显著提升性能。

Conclusion: SkeletonGaussian提出了一种可编辑的动态3D Gaussian生成框架，通过骨架驱动的层次化关节表示将运动分解为稀疏刚体运动和细粒度非刚体运动，从而提高了解释性与编辑性，并在质量与可编辑性上优于现有方法。

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [45] [Light Up Your Face: A Physically Consistent Dataset and Diffusion Model for Face Fill-Light Enhancement](https://arxiv.org/abs/2602.04300)
*Jue Gong,Zihan Zhou,Jingkai Wang,Xiaohong Liu,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出LYF-160K数据集和基于物理条件的单步扩散模型FiLitDiff（配合PALP），实现可控、高保真且低成本的人脸补光，能更好地保持背景照明。


<details>
  <summary>Details</summary>
Motivation: 现有人脸重光方法通常调整整体照明，可能抑制输入照明或改变整场景，造成前景-背景不一致，不符合实际补光需求。为实现可控且物理一致的补光，需要大规模带条件标签的成对数据和将物理参数融入生成模型的方案。

Method: 构建LightYourFace-160K数据集：使用物理一致渲染器在人脸区域注入由6个可解耦因子控制的盘状补光，生成160K对输入-输出图像；预训练物理感知光照提示（PALP），将6维参数嵌入为条件token并用平面光重建辅助任务；基于预训练扩散骨干，训练一阶段的填光扩散模型FiLitDiff，作为高效单步模型以物理约束的光照编码为条件。

Result: 在保留背景照明的同时，方法在感知质量和部分全参考指标上表现良好；在保留输入背景照明方面优于比较方法。作者将发布数据集与模型代码。

Conclusion: 本文提出针对人脸补光增强（FFE）的新方法与大规模数据集，能在保持原始场景照明与背景不变的情况下为欠曝人脸添加虚拟补光。

Abstract: Face fill-light enhancement (FFE) brightens underexposed faces by adding virtual fill light while keeping the original scene illumination and background unchanged. Most face relighting methods aim to reshape overall lighting, which can suppress the input illumination or modify the entire scene, leading to foreground-background inconsistency and mismatching practical FFE needs. To support scalable learning, we introduce LightYourFace-160K (LYF-160K), a large-scale paired dataset built with a physically consistent renderer that injects a disk-shaped area fill light controlled by six disentangled factors, producing 160K before-and-after pairs. We first pretrain a physics-aware lighting prompt (PALP) that embeds the 6D parameters into conditioning tokens, using an auxiliary planar-light reconstruction objective. Building on a pretrained diffusion backbone, we then train a fill-light diffusion (FiLitDiff), an efficient one-step model conditioned on physically grounded lighting codes, enabling controllable and high-fidelity fill lighting at low computational cost. Experiments on held-out paired sets demonstrate strong perceptual quality and competitive full-reference metrics, while better preserving background illumination. The dataset and model will be at https://github.com/gobunu/Light-Up-Your-Face.

</details>


### [46] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: 提出基于层敏感性的VAQ和训练-free推理方法LASER，按任务动态选择注意力层以增强视觉定向，显著提升多种VQA任务性能。


<details>
  <summary>Details</summary>
Motivation: 固定的视觉token预算和统一预训练分辨率会丢失细节并导致模型过度依赖语言先验，静态的注意力增强方法依赖经验层次在复杂推理任务上不鲁棒。

Method: 作者通过层级敏感性分析证明视觉定向是动态的，提出VAQ指标衡量某层对特定查询的注意力敏感性；基于VAQ，在推理时自适应选择合适层进行关注和解码，无需额外训练。

Result: 在多个VQA基准上，LASER在不同复杂度任务中显著提高了VQA准确率，验证了层自适应选择比静态“魔法层”更有效。

Conclusion: 本文提出LASER，通过动态选择模型层来改进视觉定位和问答，显著提升复杂VQA任务表现。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [47] [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/abs/2602.04317)
*Zihan Lou,Jinlong Fan,Sihan Ma,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: JOintGS 通过前景-背景分离和联合优化摄像机、姿态与 3D Gaussian，在不精确摄参与姿态初始情况下实现更鲁棒高质量、实时的可动画人体重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于 3D Gaussians 的 splatting 方法（如 3DGS）在渲染质量和实时性上优秀，但对精确的摄像机标定和姿态标注依赖强，在真实场景和粗略初始化下性能下降。作者希望通过联合优化来减少对精确初值的依赖，提升鲁棒性与质量。

Method: 引入显式前景-背景分离、多视一致性约束来锚定摄像机估计，并通过相互强化的优化流程同时精化摄像机、姿态和 3D Gaussians；增加时间动态模块以捕捉姿态依赖的细节形变，并用残差颜色场建模光照变化，保持实时渲染。

Result: 在 NeuMan 和 EMDB 数据集上进行广泛实验，NeuMan 上相对于 SOTA 方法提升约 2.1 dB PSNR，同时保持实时渲染；对噪声初始化表现出更强鲁棒性。

Conclusion: JOintGS 提出通过前景-背景分离联合优化摄像机外参、人体姿态和 3D 高斯表示，从粗略初始化精炼到高保真可动画化人体重建，提升了在野外场景下的鲁棒性和重建质量。

Abstract: Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.

</details>


### [48] [Multiview Self-Representation Learning across Heterogeneous Views](https://arxiv.org/abs/2602.04328)
*Jie Chen,Zhu Wang,Chuanbin Liu,Xi Peng*

Main category: cs.CV

TL;DR: 提出MSRL，利用自表示信息传递与分配概率一致性在冻结的预训练骨干上堆叠线性模型，学习跨异构视图的表示不变性，并在多数据集上取得优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 不同预训练模型因任务和结构差异导致同一样本的特征分布不一致，如何在完全无监督的迁移设置下从多种预训练模型产生的大规模无标签视觉数据中学习到跨模型不变的表示是一个挑战。

Method: 对每个预训练骨干网络冻结并在其上堆叠单个线性模型；利用自表示学习构建信息传递机制，使各视图输出可相互重构和聚合；引入分配概率分布一致性方案，利用不同视图间互补信息引导自表示学习，从而在各线性模型间强制表示不变性；理论分析了信息传递、概率一致性和增量视图的性质，并在多数据集上做大量对比实验。

Result: MSRL在多个视觉基准数据集上的广泛实验中稳定优于多种最新方法，证明了自表示与分配概率一致性结合在跨视图表示不变学习中的有效性。

Conclusion: 该文提出的多视角自表示学习（MSRL）有效地从异构预训练模型生成的多视图无标签特征中学习不变表示，通过信息传递机制和概率分配一致性约束实现视图间的特征聚合和互补信息利用。实验表明在多个视觉基准上优于现有方法。

Abstract: Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches.

</details>


### [49] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: CoFT通过双提示与双模型协作、两阶段训练和负提示正则化，实现了对VLM的稳健无监督适配，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有利用未标注数据的自训练方法依赖置信过滤和伪标签，易受不可靠置信度、确认偏差和低置信样本未被有效利用的影响，亟需一种更稳健且自动化的无监督适配方法。

Method: 提出CoFT框架：双提示学习（正/负文本提示）显式建模样本相关的伪标签洁净度；负提示对轻量视觉适配模块起正则化作用；两阶段训练先在高置信样本上进行参数高效微调，再用协同过滤的伪标签进行全量微调。CoFT+在此基础上加入迭代微调、动量对比学习和大模型生成提示以进一步提升性能。

Result: 在多个数据集和任务上，CoFT和CoFT+持续优于现有无监督自训练方法，并能超过一些少样本有监督基线，证明了双提示、协作过滤和两阶段训练的有效性和鲁棒性。

Conclusion: CoFT通过双模型、跨模态协作的无监督适配机制，有效提升了大型视觉-语言模型在下游任务上的表现，减少了对人工阈值和噪声假设的依赖，并在实验中显著优于现有无监督方法和部分少样本有监督基线。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [50] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 通过在CLIP文本端引入正/负双提示，正提示增强判别性，负提示显式建模预测正确性概率，为主动学习提供更可靠的不确定性信号，从而在有限标注预算下更高效地适配CLIP。


<details>
  <summary>Details</summary>
Motivation: 在有限标注预算下，主动学习需要从大量未标注数据中挑选最有信息量的样本。现有方法往往基于熵或表示聚类估计不确定性，缺乏从模型自身角度对不确定性的显式建模；此外，直接微调整个模型代价高且在少样本下表现欠佳，因此需要一种轻量且能提供可信不确定性信号的适配方法。

Method: 在CLIP的文本分支中引入两个可学习的提示：正提示通过与轻量调优的视觉嵌入配合，提高文本嵌入的判别能力，从而提升分类可靠性；负提示以反向方式训练，显式估计预测标签正确的概率，作为不确定性度量用于主动样本选择。整个框架可与多种微调范式结合，保持轻量化调优。

Result: 在多个数据集和不同微调设置下进行广泛实验，结果显示所提方法在相同标注预算下稳定优于基线主动学习策略，证明了负提示作为不确定性度量的有效性以及双提示策略提升分类可靠性的能力。

Conclusion: 本文提出了一种基于双提示（dual-prompt）调优的鲁棒不确定性建模框架，用于在有限标注预算下对预训练视觉-文本模型（CLIP）进行主动学习适配。通过在文本分支中引入正提示和负提示，分别增强任务特定文本嵌入的判别性并显式建模预测正确性的概率，从而为样本选择提供更可靠的不确定性信号。实验证明在不同微调范式下，本方法在相同标注预算下优于现有主动学习方法。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [51] [Finding NeMO: A Geometry-Aware Representation of Template Views for Few-Shot Perception](https://arxiv.org/abs/2602.04343)
*Sebastian Jung,Leonard Klüpfel,Rudolph Triebel,Maximilian Durner*

Main category: cs.CV

TL;DR: NeMO通过把物体信息外包到一个可复用的神经记忆表示，并用单个网络处理多项感知任务，实现了对未见物体的少样本快速感知（检测、分割、6DoF位姿），在BOP基准上达到了优异性能。


<details>
  <summary>Details</summary>
Motivation: 希望实现对训练时未见的新物体的高效感知——包括检测、分割和6DoF位姿估计——同时降低重新训练和相机标定等开销，提升物体快速上手与多任务扩展性。

Method: 采用编码器从少量RGB模板视图生成稀疏对象点云与学习得到的UDF（包含语义与几何信息），并用解码器结合查询图像生成密集预测（检测、分割、位姿等）。整个系统不依赖摄像机特定参数，支持单网络多任务输出。

Result: 在BOP基准的多个数据集和任务上取得了竞争性甚至SOTA的结果，展示了少样本模板输入即可实现对新物体的多任务感知能力，并且能在无需重训练的情况下直接部署。

Conclusion: NeMO提出了一种用于少样本、未见物体的对象感知的新型表征，通过将对象信息存储在可重用的NeMO中并用单网络完成检测、分割和6DoF位姿估计，实现了无需针对目标数据重训练或摄像机参数的快速上手和多任务处理。

Abstract: We present Neural Memory Object (NeMO), a novel object-centric representation that can be used to detect, segment and estimate the 6DoF pose of objects unseen during training using RGB images. Our method consists of an encoder that requires only a few RGB template views depicting an object to generate a sparse object-like point cloud using a learned UDF containing semantic and geometric information. Next, a decoder takes the object encoding together with a query image to generate a variety of dense predictions. Through extensive experiments, we show that our method can be used for few-shot object perception without requiring any camera-specific parameters or retraining on target data. Our proposed concept of outsourcing object information in a NeMO and using a single network for multiple perception tasks enhances interaction with novel objects, improving scalability and efficiency by enabling quick object onboarding without retraining or extensive pre-processing. We report competitive and state-of-the-art results on various datasets and perception tasks of the BOP benchmark, demonstrating the versatility of our approach. https://github.com/DLR-RM/nemo

</details>


### [52] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: VecSet-Edit 基于 VecSet LRM，通过令牌空间分析与三种令牌级策略，实现在网格上高保真、免 3D 掩码的局部编辑并保持几何与纹理细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦于 Gaussian Splatting 或多视图图像，直接对三维网格进行高保真编辑尚不充分；体素方法像 VoxHammer 分辨率有限且需人工 3D 掩码，故需新方案。

Method: 在分析 VecSet 令牌的空间属性后，提出三大技术：Mask-guided Token Seeding（基于2D条件种子化目标令牌）、Attention-aligned Token Gating（基于注意力对令牌进行门控以精确定位）、Drift-aware Token Pruning（在扩散去噪过程中剔除几何离群令牌），以及 Detail-preserving Texture Baking 用于纹理保持。

Result: 提出 VecSet-Edit 管道，通过对令牌空间结构的利用和新的令牌操作策略，实现了无需 3D 掩码、只用 2D 条件即可在网格上精确编辑，同时保留几何细节和纹理；项目已开源。

Conclusion: VecSet-Edit 成功将高保真 VecSet LRM 用于直接网格编辑，解决了体素方法的分辨率限制与掩码负担，能在仅用二维图像条件下实现精确区域定位并保持几何与材质细节。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [53] [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/abs/2602.04356)
*Jaehyun Kwak,Nam Cao,Boryeong Cho,Segyu Lee,Sumyeong Ahn,Se-Young Yun*

Main category: cs.CV

TL;DR: SAGA通过分阶段注意力引导，在高注意力区域集中扰动，显著提高了在受限扰动预算下对LVLMs的攻击效率与成功率。


<details>
  <summary>Details</summary>
Motivation: 随机裁剪等基于输入变换的攻击虽然显示局部扰动有效，但随机性导致扰动预算利用低效。为更高效地使用有限每像素扰动预算，需有策略地选择攻击区域。

Method: 基于两点观察：（i）区域注意力得分与对抗损失敏感性正相关，（ii）攻击高注意力区域会诱导注意力向后续显著区域重新分配。SAGA采用分阶段策略，在每一阶段利用注意力图选择高注意力区域并优先施加扰动，逐步集中扰动，使得在受限LP预算下生成高度不可察觉的对抗样本。

Result: 在十个LVLMs上，SAGA在保持高度不可察觉的同时，始终实现最先进的攻击成功率。

Conclusion: 该文提出基于注意力的分阶段攻击方法SAGA，通过聚焦高注意力区域来更高效地分配有限的扰动预算，实现了对多个LVLMs的强攻击效果。

Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.

</details>


### [54] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: SparVAR通过训练免费地预测并复用稀疏跨尺度注意力模式与局部块稀疏核，实现在不跳过高分辨率尺度的前提下显著加速VAR注意力计算，并保留高频图像细节。


<details>
  <summary>Details</summary>
Motivation: 主流VAR在每一步对所有历史尺度令牌进行注意，随着分辨率增长注意力计算随之呈四次方增长，导致高延迟；已有加速通过跳过高分辨率尺度但牺牲图像细节。需要在不跳尺度的前提下加速VAR。

Method: 利用三个VAR注意力的性质：注意力汇聚、跨尺度激活相似性和显著的局部性。方法包括从稀疏决策尺度动态预测高分辨率尺度的稀疏注意力模式；通过索引映射构建尺度自相似稀疏注意力；提出跨尺度局部稀疏注意力并实现块级稀疏高效核，从而实现高效稀疏注意力计算。

Result: SparseVAR在8B模型生成1024×1024图像时，将生成时间降至约1s；相比FlashAttention的VAR基线实现1.57×加速并几乎保留所有高频细节；与尺度跳过策略结合可达2.28×加速，且保持竞争性的生成质量。

Conclusion: SparVAR通过稀疏化高分辨率尺度的自回归注意力，在不跳过最终尺度的情况下大幅降低计算和延迟，保留高频细节并实现显著加速。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [55] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: 提出UltraSeg极致轻量分割网络（0.108M/0.13M参数），在单CPU核心实现90FPS，保留>94% U-Net Dice得分，适用于资源受限的结肠镜实时息肉检测与更广泛的微创外科视觉场景。


<details>
  <summary>Details</summary>
Motivation: 当前高精度分割模型依赖GPU，无法在基层医院、移动内镜或胶囊机器人等资源受限场景部署，促使作者探索极致参数压缩下的CPU原生实时分割方案。

Method: 通过联合优化编码器-解码器宽度、引入受限膨胀卷积扩大感受野，以及集成跨层轻量融合模块来构建UltraSeg-108K和UltraSeg-130K两种模型，并针对单中心与多中心多模态数据分别优化。

Result: 在七个公开数据集上，UltraSeg在参数量仅为31M U-Net的0.4%条件下仍能保留>94%的Dice分数，UltraSeg-108K和UltraSeg-130K分别适配单中心与多中心应用，并在单CPU核心上达到90FPS。

Conclusion: 该论文通过在极限压缩（<0.3M参数）下设计轻量级分割网络UltraSeg系列，实现了在单CPU核心上实时（90 FPS）且高精度的结直肠息肉分割，证明了在资源受限环境中可行性。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [56] [Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2602.04405)
*Yixin Zhu,Long Lv,Pingping Zhang,Xuehu Liu,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出ISFM，利用MSE、MFF和ISF在空间与频率域交互融合，多尺度自适应结合频率信息，在六个数据集上显著提升多模态图像融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有空间-频率融合方法多为串/并联且缺乏交互，未充分利用频率信息对空间特征的引导作用。

Method: 提出模块化框架：模态专用提取器（MSE，用于捕捉长程依赖，线性复杂度）、多尺度频率融合（MFF，自适应整合低高频）、交互式空间-频率融合（ISF，频率引导空间特征跨模态交互）。

Result: 在六个MMIF数据集上进行大量实验，ISFM在定量和定性评估上均优于其他先进方法，并提供了开源代码。

Conclusion: ISFM通过交互式空间-频率融合有效提升多模态图像融合性能，优于现有方法。

Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.

</details>


### [57] [LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration](https://arxiv.org/abs/2602.04406)
*Jue Gong,Zihan Zhou,Jingkai Wang,Shu Li,Libo Liu,Jianliang Lan,Yulun Zhang*

Main category: cs.CV

TL;DR: LCUDiff通过将4通道潜在扩散模型扩展到16通道，结合通道分裂蒸馏、先验保持适配和解码器路由器，解决VAE带来的保真度瓶颈，在人体恢复任务中实现更高细节保留和更少伪影，且保持单步高效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在人体恢复任务中受限于VAE的瓶颈，导致细节丢失和保真度不足，作者希望通过扩展潜在通道并设计特定适配与路由机制来提升恢复质量。

Method: 方法包括：1) 通道分裂蒸馏（CSD）在微调VAE时保持前四通道与预训练先验对齐，同时将额外通道用于编码高频细节；2) 先验保持适配（PPA）用于平滑4通道扩散骨干与16通道潜在空间之间的不匹配；3) 解码器路由器（DeR）基于恢复质量评分对样本进行解码器路由，以提升多样条件下的视觉质量。

Result: 在合成与真实数据集上的实验表明，LCUDiff在轻度退化下比现有方法在保真度和伪影控制上更优，同时保持一步恢复的效率。

Conclusion: 本文提出LCUDiff，通过将预训练4通道潜在扩散模型升级到16通道以提高人像恢复（尤其人体恢复）的保真度，并在保持一步效率的同时减少伪影。

Abstract: Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.

</details>


### [58] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: Med-MMFL：首个面向医疗多模态联邦学习的全面基准，覆盖10种模态、4类任务、3类联邦场景并评测6种SOTA方法，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前医疗多模态联邦学习缺乏标准化和全面的基准，现有工作多集中于单模态或双模态且任务单一，阻碍了系统性比较与方法发展。

Method: 构建包含2至4模态、覆盖10种医疗模态（文本、病理图像、心电图、X光、放射学报告、多序列MRI等）和多种联邦场景（天然联邦、合成IID、合成非IID）的数据集与任务；评估了6种代表性联邦学习算法，涵盖不同聚合策略、损失形式和正则化技巧；任务包括分割、分类、模态对齐（检索）和视觉问答，并开放完整实现与数据处理/分区管线。

Result: 提供了广泛的基准测试结果，展示了不同算法在多模态、多任务和不同联邦分布下的性能差异，强调了场景异质性对算法表现的影响，并通过开源代码支持可重复性。

Conclusion: 本文提出了首个面向医疗领域的多模态联邦学习基准Med-MMFL，填补了现有基准在模态和任务覆盖上的不足，推动MMFL的标准化评估。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [59] [TrajVG: 3D Trajectory-Coupled Visual Geometry Learning](https://arxiv.org/abs/2602.04439)
*Xingyu Miao,Weiguang Zhao,Tao Lu,Linning Yu,Mulin Yu,Yang Long,Jiangmiao Pang,Junting Dong*

Main category: cs.CV

TL;DR: TrajVG通过在相机坐标系显式预测稀疏3D轨迹并与局部点图和相对位姿耦合，通过双向一致性和静态锚点位姿约束（可自监督化）解决多物体运动视频中的跨帧对齐问题，显著提升重建与跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有前馈多帧3D重建在存在物体运动的视频中表现下降：全局参考在多运动场景下模糊，局部点图高度依赖相对位姿易漂移，导致跨帧错配和重复结构。需要显式建模跨帧对应以稳健处理动态。

Method: 提出基于相机坐标3D轨迹的显式跨帧对应预测，耦合（1）稀疏轨迹、（2）每帧局部点图、（3）相对相机位姿，设计几何一致性目标：双向轨迹-点图一致性（带受控梯度流）和由静态轨迹锚点驱动的位姿一致性，用于抑制动态区域梯度。并将耦合约束重写为只需伪2D跟踪的自监督目标，支持混合监督训练以扩展到野外视频。

Result: 在3D跟踪、位姿估计、点图重建和视频深度任务上，TrajVG均优于当前前馈性能基线，且通过混合有/无监督训练适配了标注稀缺的野外视频场景。

Conclusion: TrajVG通过在相机坐标系下显式预测稀疏3D轨迹并与每帧局部点图和相对位姿耦合，能有效解决多物体运动视频中跨帧对应模糊和点图漂移问题，从而提升视频级3D重建、跟踪和深度估计性能。

Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.

</details>


### [60] [SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking](https://arxiv.org/abs/2602.04441)
*Weiguang Zhao,Haoran Xu,Xingyu Miao,Qin Zhao,Rui Zhang,Kaizhu Huang,Ning Gao,Peizhou Cao,Mingze Sun,Mulin Yu,Tao Lu,Linning Xu,Junting Dong,Jiangmiao Pang*

Main category: cs.CV

TL;DR: SynthVerse：一个覆盖动画、具身操作、导航与关节物体等新域的大规模合成点追踪数据集，显著提升跨域泛化并用于构建更全面的评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在多样性和轨迹注释质量上不足，限制了点追踪算法在复杂真实世界场景中的泛化性能，需引入更丰富高质量的数据。

Method: 构建包含动画片风格、具身操作、场景导航与关节物体等新域的大规模合成数据集，并设计高质量的动态运动与交互场景；建立多域点追踪基准用于系统评估。

Result: 在广泛实验中，使用SynthVerse训练的模型在不同域和更严苛设置下表现出稳定改进，并暴露了现有追踪器在多样性场景中的不足。

Conclusion: SynthVerse通过引入大规模、多样化的合成数据集，有效缓解了现有点追踪数据不足的问题，提升了模型在跨域和复杂场景下的泛化能力。

Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.

</details>


### [61] [Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search](https://arxiv.org/abs/2602.04454)
*Tianming Liang,Qirui Du,Jian-Fang Hu,Haichao Jiang,Zicheng Lin,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: Seg-ReSearch通过交替的外部检索与推理并配合分层奖励，解决了多模态大模型知识瓶颈，使分割系统能处理需要外部知识的开放世界查询，并在新基准OK-VOS和其他测试上显著优于已有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的分割依赖冻结的多模态大模型知识，无法应对实时或领域专有信息，限制了在开放世界场景中的应用。

Method: Seg-ReSearch在分割流程中实现推理与外部搜索的交替执行；设计了分层奖励（包含初始引导与渐进激励）来缓解训练中的稀疏回报与僵化监督冲突；构建OK-VOS基准用于评价需要外部知识的视频目标分割。

Result: 在新构建的OK-VOS及两个人工推理分割基准上，Seg-ReSearch相比最先进方法有显著提升（论文宣称为大幅度提升）。代码与数据计划公开。

Conclusion: 本文提出Seg-ReSearch，通过在分割过程中引入外部检索与交互式推理，突破了多模态大模型内部知识的限制，使系统能处理动态、开放世界的查询。采用分层奖励设计以平衡稀疏信号与逐步监督，实现有效训练。

Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.

</details>


### [62] [Temporal Slowness in Central Vision Drives Semantic Object Learning](https://arxiv.org/abs/2602.04462)
*Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch*

Main category: cs.CV

TL;DR: 在Ego4D类人视觉流上，结合中央视野模拟与时间缓慢性自监督学习能更好地形成语义对象表征：中央视野强调前景特征，时间缓慢性（尤其在注视微动时）扩展语义信息。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何在最小监督下从自我视角视频中学习语义对象表征，重点考察中央高分辨率视野和视觉输入随时间平滑性的影响，模拟眼睛注视与微动对学习的贡献。

Method: 作者使用Ego4D数据集模拟五个月的人类视觉体验，使用先进的注视预测模型生成注视坐标，并按注视位置裁剪中央视野图像。基于这些裁剪，训练一个时间对比的自监督学习模型（time-contrastive SSL），并分析不同设置（有无中央视野裁剪、考虑/不考虑时间缓慢性、不同眼动阶段）对表征的影响。

Result: 实验表明：1) 中央视野裁剪提升了对前景物体特征的表征质量；2) 时间缓慢性学习（特别是在注视微动期间）促使模型捕捉更广泛的语义信息；3) 两者结合效果最佳，支持人类视觉系统通过高分辨率注视区域和时间平滑性来学习语义表示的假设。

Conclusion: 作者结论是：结合中央视野（高分辨率注视区）和时间缓慢性学习（slowness）有助于从类人视觉流中形成语义对象表征；中央视野强化前景对象特征提取，而时间缓慢性，尤其在注视期微动阶段，帮助编码更广泛的语义信息。

Abstract: Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.

</details>


### [63] [SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening](https://arxiv.org/abs/2602.04473)
*Junjie Li,Congyang Ou,Haokui Zhang,Guoting Wei,Shengqin Jiang,Ying Li,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出在潜在空间做扩散的传感器不可知全色锐化方法：单通道VAE编码多波段MS、交互控制注入物理与影像信息、中心层跨谱注意力，提升精度、速度和跨传感器鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的全色锐化多数在像素空间进行、为不同传感器训练独立模型，导致推理延迟高且缺乏跨传感器泛化能力。作者欲通过潜在空间扩散与传感器不可知的编码机制提升效率及通用性，同时保证光谱保真。

Method: 1) 训练一个按波段共享的单通道VAE将HRMS编码到紧凑潜在表示，支持不同波段数的MS；2) 在扩散骨干中通过单向和双向交互控制结构注入光谱物理属性及PAN/MS信息；3) 在扩散模型中间层加入轻量跨谱注意力模块以增强光谱一致性。

Result: 在GaoFen-2、QuickBird和WorldView-3数据集上，SALAD-Pan在评估指标上优于现有扩散方法，达到2-3倍推理加速，并显示出显著的零样本（跨传感器）泛化能力。

Conclusion: SALAD-Pan通过在潜在空间进行扩散、采用带通道共享的单通道VAE、引入光谱物理约束与交互控制结构以及中心层跨谱注意力模块，有效提升了全色锐化的精度、速度和跨传感器鲁棒性。

Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.

</details>


### [64] [Vision-aligned Latent Reasoning for Multi-modal Large Language Model](https://arxiv.org/abs/2602.04476)
*Byungwoo Jeon,Yoonwoo Jeong,Hyunseok Lee,Minsu Cho,Jinwoo Shin*

Main category: cs.CV

TL;DR: 提出VaLR（Vision-aligned Latent Reasoning），在每一步Chain of Thought推理前动态生成与视觉对齐的潜在tokens，通过使多模态LLM中间嵌入与视觉编码器对齐以保留视觉信息，从而改善长链推理中视觉信息稀释问题。实验证明在多项需要长上下文理解或精确视觉感知的基准上优于现有方法，并呈现测试时可扩展性，在VSI-Bench上从33.0%提升到52.9%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态LLM在需要大量多步推理的任务中性能受限，原因是长上下文生成过程中视觉信息逐步被稀释，无法充分利用测试时更大规模的推理能力。VaLR旨在通过在潜在空间保留视觉信息来解决这一问题。

Method: 在每次Chain of Thought推理步骤前生成与图像感知对齐的潜在tokens（vision-aligned latent tokens），训练目标是使MLLM的中间嵌入与视觉编码器的嵌入对齐，从而在潜在空间保留视觉知识，指导后续推理步骤基于感知线索进行。

Result: 在多个需要长上下文理解或精确视觉感知的基准上优于现有方法，且呈现测试时缩放收益。VSI-Bench上从33.0%提高到52.9%，比Qwen2.5-VL高19.9个百分点。

Conclusion: VaLR能有效缓解多模态LLM在多步长上下文推理中视觉信息被稀释的问题，显著提升多步视觉推理性能并带来测试时规模扩展的收益。

Abstract: Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.

</details>


### [65] [S-MUSt3R: Sliding Multi-view 3D Reconstruction](https://arxiv.org/abs/2602.04517)
*Leonid Antsfeld,Boris Chidlovskii,Yohann Cabon,Vincent Leroy,Jerome Revaud*

Main category: cs.CV

TL;DR: 通过序列分段+分段对齐+轻量回环优化，无需重训练MUSt3R，即可实现可扩展的单目长序列三维重建，效果接近传统复杂方法，并能直接输出度量空间重建。


<details>
  <summary>Details</summary>
Motivation: 当前基于基础模型（foundation models）的单目三维重建在小规模或稀疏图像上表现优越，但受限于显存与计算资源，难以直接扩展到长时序RGB流或大尺度场景。为利用MUSt3R强大的单目重建能力且克服规模限制，需设计轻量且高效的流水线实现长序列处理。

Method: 将长视频序列切分为多个子段，分别用已有的MUSt3R基础模型独立重建每段三维结构；随后通过段间对齐（可能基于相对位姿或特征匹配）将局部重建拼接到统一坐标系；最后引入轻量级回环闭合优化修正累计误差，保证全局一致性。整个流程无需对MUSt3R模型重新训练，主要在后处理尺度实现可扩展性。

Result: 在TUM、7-Scenes及专有机器人导航数据集上，S-MUSt3R能在长RGB序列上稳定运行，生成准确且一致的三维重建，轨迹与重建质量与更复杂架构的传统方法相当，同时保持更低的实现复杂度且直接输出度量空间结果。

Conclusion: S-MUSt3R通过序列分段、分段对齐与轻量回环优化，在不重新训练基础模型的前提下，实现了可扩展的单目三维构建，达到与复杂传统方法相当的轨迹与重建性能，且能直接在度量空间给出预测，适用于长RGB序列与真实机器人导航场景。

Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.

</details>


### [66] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 作者提供了新的数据集与半监督语义分割方法（类别自适应阈值+原型库），解决注释匮乏、类不平衡和特征退化问题，在八个城市上验证并展现出优异的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 城市快速扩张导致低中收入国家大量非正规住区出现，但大规模制图受限于注释匮乏以及遥感影像中正规与非正规结构的光谱相似性和标注噪声。研究旨在构建数据集并提出鲁棒的半监督方法以应对类不平衡和特征退化问题。

Method: 提出了Class-Aware Adaptive Thresholding（类别感知自适应阈值）动态调整置信阈值防止少数类被抑制；引入Prototype Bank（原型库）保持语义一致性，将预测锚定到历史学习到的高保真特征表征上；在多个城市和基准上进行半监督训练与评估。

Result: 构建了包含拉合尔、卡拉奇和孟买在内的综合数据集（总计1869 km^2），并在另外五个已建立基准上的八个城市验证方法。所提方法在半监督基线之上取得显著提升，特别是在域转移情形下，使用仅10%源标签训练的模型在未见地理区域上达到0.461 mIoU，超越了全监督模型的零样本泛化。

Conclusion: 该论文在具有标签稀缺和数据质量差异的大范围城市非正规住区映射中取得进展，通过构建新数据集并提出新的半监督语义分割框架，提高了跨域泛化性和少样本学习性能。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [67] [OmniRad: A Radiological Foundation Model for Multi-Task Medical Image Analysis](https://arxiv.org/abs/2602.04547)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto*

Main category: cs.CV

TL;DR: OmniRad是一个基于1.2M医学影像自监督预训练的放射学基础模型，强调表征重用，能在冻结或微调下改善分类与分割表现，尤其在冻结表示用于密集预测时效果显著。


<details>
  <summary>Details</summary>
Motivation: 希望构建一个通用的放射学基础模型，通过表征重用与跨任务可迁移性改善在不同影像模态与下游任务上的表现，减少为每个任务训练模型的代价。

Method: 采用基于放射学设计原则的自监督学习，在1.2百万医学影像上预训练编码器；评估使用冻结骨干+轻量适配器与端到端微调两种下游适配模式，覆盖分类与分割任务。

Result: 在MedMNISTv2上分类F1提升最多2.05%；在六个MedSegBench数据集上冻结表示下平均Dice获得提升；质性分析显示特征聚类与模态分离更清晰。

Conclusion: OmniRad展示了在大规模自监督预训练下提高放射学表征质量与跨任务迁移能力的潜力，尤其在冻结骨干网情况下对密集预测任务有显著提升。

Abstract: Radiological analysis increasingly benefits from pretrained visual representations that can support heterogeneous downstream tasks across imaging modalities. In this work, we introduce OmniRad, a self-supervised radiological foundation model pretrained on 1.2 million medical images, designed with radiology-inspired principles emphasizing representation reuse and cross-task transferability. We evaluate the pretrained encoder under multiple downstream adaptation regimes, including lightweight task-specific adapters with a frozen backbone as well as full end-to-end fine-tuning for classification, allowing us to assess both representation quality and task-specific performance. OmniRad is evaluated on a broad suite of public benchmarks spanning classification and segmentation across multiple modalities. On the MedMNISTv2 collection, OmniRad improves classification F1 by up to 2.05% over competing foundation models. For dense prediction, OmniRad attains mean Dice score improvements across six MedSegBench datasets when using frozen representations. Qualitative analyses and latent-space visualizations suggest improved feature clustering and modality-related separation.

</details>


### [68] [Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models](https://arxiv.org/abs/2602.04549)
*Cem Eteke,Enzo Tartaglione*

Main category: cs.CV

TL;DR: NiFi利用伪影感知的一步扩散蒸馏，能在极低存储（0.1 MB）下恢复3DGS渲染，达到SOTA感知质量并实现约1000×压缩率提升。


<details>
  <summary>Details</summary>
Motivation: 3DGS虽实现了实时新视角渲染，但使用稀疏高斯导致存储开销大，限制在沉浸式通信等场景的应用。现有压缩方法在低码率下产生严重伪影，需一种能在极低比特率下恢复视觉质量的方案。

Method: 提出基于扩散模型的一步蒸馏恢复流程（artifact-aware, diffusion-based one-step distillation），针对3DGS压缩引入的伪影进行感知驱动的重建，以在超低比特率下恢复高质量渲染。

Result: 在极端压缩下（至0.1 MB），NiFi在感知质量指标上优于现有方法，实现约1000倍的率改善而保持可比的感知表现。代码将在论文接收后开源。

Conclusion: 本文提出了NiFi，通过面向伪影的扩散一阶蒸馏对极端压缩后的3D Gaussian Splatting(3DGS)进行恢复，在极低比特率（最低0.1 MB）下达到最先进的感知质量，显著提升压缩率（约1000倍）同时保持可视质量。

Abstract: 3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.

</details>


### [69] [Understanding Degradation with Vision Language Model](https://arxiv.org/abs/2602.04565)
*Guanzhou Lan,Chenyi Liao,Yuqi Yang,Qianli Ma,Zhigang Wang,Dong Wang,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 提出将退化理解建模为分层自回归结构化预测，提出DU-VLM并配合强化学习与大规模数据集DU-110k，有效提升参数化退化理解与零样本图像恢复能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM能进行定性描述但难以理解图像退化的参数化物理机制，需同时估计退化类型、参数键及连续物理值，克服跨空间子任务整合困难。

Method: 证明不同子任务可统一为自回归预测；设计并训练DU-VLM，结合监督微调与基于结构化奖励的强化学习；将模型用于零样本控制预训练扩散模型；构建并使用DU-110k数据集进行训练与评估。

Result: 在大规模实验中，DU-VLM在准确性、鲁棒性上显著优于通用基线，并能泛化到未见分布；DU-VLM还能在不微调生成骨干的情况下指导扩散模型实现高质量修复；发布了110k对带物理标注的数据集DU-110k。

Conclusion: 本文将图像退化理解定义为一个分层结构化的预测任务，提出将不同子任务统一为自回归下一个token预测范式，并通过值空间量化网格界定误差界限。基于此提出DU-VLM模型，并展示其可作为无微调的扩散模型控制器实现高保真图像恢复，同时构建DU-110k数据集。

Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.

</details>


### [70] [PEPR: Privileged Event-based Predictive Regularization for Domain Generalization](https://arxiv.org/abs/2602.04583)
*Gabriele Magrini,Federico Becattini,Niccolò Biondi,Pietro Pala*

Main category: cs.CV

TL;DR: 利用训练时可用的事件相机作为特权信息，通过在共享潜空间中让RGB预测事件潜特征（PEPR）来蒸馏鲁棒性，从而得到在域转移下更稳健的单模态RGB模型，且优于直接特征对齐基线。


<details>
  <summary>Details</summary>
Motivation: 视觉模型在域变化（如昼夜）下性能严重下降。事件相机提供稀疏但更域不变的信号，可在训练时作为特权信息用来提升单模态RGB模型的鲁棒性，但直接对齐会损失语义细节，因此需要新的蒸馏策略。

Method: 在LUPI范式下利用训练阶段可用的事件相机作为特权信息，避免直接特征对齐，构建共享潜空间并训练RGB编码器预测事件模态的潜在表示（PEPR），以蒸馏事件模态的鲁棒性同时保持语义密度。

Result: 经在目标检测和语义分割任务上的实验，单模态RGB模型在昼夜及其它域移位下稳健性显著提升，优于基于跨模态对齐的方法。

Conclusion: 提出的PEPR方法能在不牺牲RGB语义信息的情况下，通过在共享潜空间预测事件模态特征来增强RGB模型对域变化的鲁棒性，适用于目标检测和语义分割，并优于对齐基线。

Abstract: Deep neural networks for visual perception are highly susceptible to domain shift, which poses a critical challenge for real-world deployment under conditions that differ from the training data. To address this domain generalization challenge, we propose a cross-modal framework under the learning using privileged information (LUPI) paradigm for training a robust, single-modality RGB model. We leverage event cameras as a source of privileged information, available only during training. The two modalities exhibit complementary characteristics: the RGB stream is semantically dense but domain-dependent, whereas the event stream is sparse yet more domain-invariant. Direct feature alignment between them is therefore suboptimal, as it forces the RGB encoder to mimic the sparse event representation, thereby losing semantic detail. To overcome this, we introduce Privileged Event-based Predictive Regularization (PEPR), which reframes LUPI as a predictive problem in a shared latent space. Instead of enforcing direct cross-modal alignment, we train the RGB encoder with PEPR to predict event-based latent features, distilling robustness without sacrificing semantic richness. The resulting standalone RGB model consistently improves robustness to day-to-night and other domain shifts, outperforming alignment-based baselines across object detection and semantic segmentation.

</details>


### [71] [SalFormer360: a transformer-based saliency estimation model for 360-degree videos](https://arxiv.org/abs/2602.04584)
*Mahmoud Z. A. Wahba,Francesco Barbato,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 提出了结合SegFormer编码器、自定义Transformer解码器和Viewing Center Bias的SalFormer360模型，用于360度视频显著性估计，在三大数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 360度视频中用户注意力分布与传统2D图像不同，精确估计显著性对于视口预测和沉浸式内容优化非常重要，因此需要专门的模型来提高预测准确性。

Method: 采用了SegFormer编码器（原用于2D分割）微调以适应360度内容，设计了一个定制的Transformer解码器用于显著性图生成，并融入Viewing Center Bias来模拟360度场景下的注意力分布。

Result: 在三大基准数据集（Sport360、PVS-HM、VR-EyeTracking）上，SalFormer360在Pearson相关系数上分别比前人提高8.4%、2.5%和18.6%，体现了显著的性能提升。

Conclusion: SalFormer360在360度视频显著性估计任务中表现出色，基于SegFormer的编码器和自定义解码器结合Transfomer架构，并加入Viewing Center Bias提高了对用户注意力的适配性。实验显示在三个基准数据集上显著优于现有方法。

Abstract: Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.

</details>


### [72] [ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry](https://arxiv.org/abs/2602.04585)
*Marcin Możejko,Dawid Uchal,Krzysztof Gogolewski,Piotr Kupidura,Szymon Łukasik,Jakub Giezgała,Tomasz Nocoń,Kacper Pietrzyk,Robert Pieniuta,Mateusz Sulimowicz,Michal Orzyłowski,Tomasz Siłkowski,Karol Zagródka,Eike Staub,Ewa Szczurek*

Main category: cs.CV

TL;DR: ImmuVis 通过标记自适应超卷积和大规模自监督预训练，提供了对可变标记集泛化且高效的 IMC 基础模型，性能优越且能产生校准不确定性。


<details>
  <summary>Details</summary>
Motivation: IMC 的多通道标记集合在不同研究中变化，违背了标准视觉骨干对固定通道空间的假设，需要一种能处理可变标记集且高效的模型。

Method: 引入标记自适应超卷积：从学习的标记嵌入生成卷积核；在最大规模数据集 IMC17M 上进行自监督掩码重建预训练；使用异方差似然目标以获得校准的不确定性估计。

Result: 在虚拟染色和下游分类任务上优于 SOTA 基线和消融实验，计算成本低于基于变压器的替代方法，并且是唯一通过异方差似然提供校准不确定性的方法。

Conclusion: ImmuVis 提出了一种适用于成像质谱细胞术（IMC）的高效卷积基础模型，通过标记自适应超卷积解决了多通道标记集不固定的问题，从而实现了单一模型对任意标记子集的泛化，无需重新训练。

Abstract: We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.

</details>


### [73] [A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction](https://arxiv.org/abs/2602.04624)
*Raúl Jiménez Cruz,César Torres-Huitzil,Marco Franceschetti,Ronny Seiger,Luciano García-Bañuelos,Barbara Weber*

Main category: cs.CV

TL;DR: 公开了一个11,884张、含五类多边形分割标注的模拟采血训练臂图像数据集，经过面部匿名化与SSIM去冗余，导出为兼容YOLOv8的分割格式，并划分为训练/验证/测试集，适用于医学训练自动化与流程分析研究。


<details>
  <summary>Details</summary>
Motivation: 推动医疗程序自动化与教学辅助的发展，填补公开可用的、针对静脉采血工具与步骤标注的视觉数据集空白，使研究者能开发工具检测、步骤识别、流程分析与合规检查等应用，从而提升培训效果与患者安全。

Method: 从高分辨率受控条件视频中抽帧，先对视频应用自动面部匿名化，再使用SSIM度量去除相似帧以减少冗余，随后人工或半自动地为每帧绘制五类多边形分割标注（注射器、橡皮筋/止血带、消毒擦拭巾、手套、训练臂），并导出为与YOLOv8等兼容的分割标签格式。数据按照70/15/15划分为训练/验证/测试集。

Result: 生成了11,884张带多边形分割标注的图像数据集，涵盖五类医学相关对象，公开托管于Zenodo，配套分割标签可直接用于现代目标检测与分割模型训练与评估。数据集划分与预处理步骤提高了可用性与隐私性。

Conclusion: 该数据集为静脉采血（穿刺）模拟训练臂提供了规模适中的带注释图像，适用于医学训练与人-物交互的计算机视觉研究。数据质量控制措施（高分辨率视频来源、SSIM去冗余、面部匿名化）提高了数据的可靠性和隐私合规性，分割格式与主流检测框架兼容，便于复用。

Abstract: This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.

</details>


### [74] [PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective](https://arxiv.org/abs/2602.04657)
*Haokui Zhang,Congyang Ou,Dawei Yan,Peng Wang,Qingsen Yan,Ying Li,Rong Xiao,Chunhua Shen*

Main category: cs.CV

TL;DR: PIO-FVLM是一种训练自由、面向推理目标的视觉token压缩方法，基于层局部代理损失计算token梯度显著性并用NMS选择token，在大幅降低token数量的同时保持高性能并显著加速与降费。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法多依赖视觉token间或视觉-文本相似性启发式规则，存在压缩性能与实际部署限制。作者从保持推理输出不变性的目标出发，直接选择对最终输出重要的token，以提升压缩效果和部署友好性。

Method: 方法包括：1) 设计层局部代理损失(layer-local proxy loss)以从当前层粗略约束到最终输出，计算token级梯度显著性；2) 根据梯度显著性对视觉token重排，并采用非极大值抑制(NMS)原则选择最有价值的token；3) 方法训练自由，兼容FlashAttention，可作为编码器外方法单独部署，也可与编码器压缩方法结合。

Result: 在LLaVA-Next-7B上，PIO-FVLM只保留11.1%的视觉token即可保持97.2%的原始性能，获得2.67× prefill加速、2.11×推理加速、6.22×更低FLOPs和6.05×更小KV Cache开销。

Conclusion: 本文提出了一种基于推理目标的视觉token压缩方法PIO-FVLM，通过保持输出不变性来选择重要token，从而在不训练的情况下显著加速VLM推理并降低资源开销。

Abstract: Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.

</details>


### [75] [AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation](https://arxiv.org/abs/2602.04672)
*Jin-Chuan Shi,Binhong Ye,Tao Liu,Junzhe He,Yangjinhui Xu,Xiaoyang Liu,Zeju Li,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: AGILE通过VLM辅助的物体生成、无需SfM的锚定跟踪和接触感知优化，实现从单目视频到可仿真手-物交互资产的稳健生成，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有单目重建受神经渲染导致几何碎片化和对SfM初始化的依赖而在真实场景（遮挡、多视不足）下失败，亟需生成式与更鲁棒的位姿跟踪与物理约束以得到可用于机器人/VR的仿真级资产。

Method: 方法包括：(1) VLM引导的生成模型合成完整且有高质量纹理的闭合物体网格，克服遮挡；(2) 用基础模型在交互起始帧初始化物体位姿，并通过与生成资产的视觉相似性进行锚定与跟踪，绕过SfM；(3) 接触感知优化融合语义、几何与交互稳定性约束，提升物理合理性并生成可仿真资产。

Result: 在HO3D、DexYCB及野外视频上，AGILE在全局几何精度上超过基线方法，并在具有挑战性的序列（强遮挡、稀视角）上显示出显著鲁棒性。生成的资产通过真实到仿真的重定向验证，适用于机器人任务。

Conclusion: AGILE提出通过生成完整物体网格并结合鲁棒跟踪和接触优化，实现从单目视频到可仿真手-物交互重建的稳健流程，解决了神经渲染碎片化几何与SfM脆弱性问题。

Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.

</details>


### [76] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 提出DRMOT任务与DRSet数据集，设计DRTrack融合RGB、深度与语言实现3D感知的指代多目标追踪，实验验证其在空间语义理解与鲁棒追踪上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有RMOT仅依赖2D RGB，难以处理包含复杂空间语义的描述（如“离摄像头最近的人”）与遮挡下的身份保持，缺乏显式3D信息指导。

Method: 构建包含RGB、深度与语言输入的追踪框架DRTrack，融合多模态（RGB-D-L）进行目标定位与轨迹关联，并借助大型多模态语言模型（MLLM）指导深度感知。

Result: 在自建DRSet数据集上进行大量实验，结果显示DRTrack在空间语义定位和在遮挡下的身份保持方面优于仅用RGB的方法。

Conclusion: 提出RGBD Referring Multi-Object Tracking（DRMOT）新任务，并构建DRSet数据集，提出DRTrack框架，实验证明有效。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [77] [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/abs/2602.04699)
*Samet Hicsonmez,Jose Sosa,Dan Pineau,Inder Pal Singh,Arunkumar Rathinam,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 利用VLM自动生成伪标签并通过教师-学生蒸馏训练轻量模型，实现注释免费且在航天目标分割上显著优于直接零样本VLM推理（AP提升可达10%）。


<details>
  <summary>Details</summary>
Motivation: 航天图像标注昂贵且困难（低能见度、光照变化、目标与背景混合），需要无需大量人工标注即可实现航天器与轨道目标的检测与分割方法。

Method: 先用预训练VLM对少量未标注真实图像自动生成伪标签；然后采用教师-学生标签蒸馏，用伪标签训练轻量化的学生模型以提升鲁棒性并降低噪声影响；最终在多个航天分割数据集上评估。

Result: 在SPARK-2024、SPEED+、TANGO等分割数据集上，所提方法比直接用VLM零样本推理在AP上最多提升约10个百分点，表现稳健且代码开源。

Conclusion: 本工作提出了一种基于视觉语言模型（VLM）的无标注航天目标检测与分割流程，通过利用VLM自动生成伪标签并在教师-学生蒸馏框架中训练轻量模型，在无人工标注情况下显著提升了分割性能。

Abstract: Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.

</details>


### [78] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 提出SAR-RAG：把向量检索的图像示例作为多模态LLM的外部记忆以增强SAR自动目标识别，实验显示分类与回归性能提升。


<details>
  <summary>Details</summary>
Motivation: SAR影像中车辆目标外观相似、辨识困难，利用历史带标签示例进行检索辅助可提升识别性能。

Method: 构建MLLM与向量数据库相连，使用语义嵌入进行相似图像检索，并以检索到的示例作为上下文辅助ATR分类与尺寸回归。

Result: 在检索指标、类别分类准确率以及车辆尺寸回归误差上，相较于仅用MLLM基线，加入SAR-RAG均有提升。

Conclusion: SAR-RAG将检索增强生成与多模态大语言模型结合，作为附加记忆库，可改进SAR图像中的目标识别准确率。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [79] [How to rewrite the stars: Mapping your orchard over time through constellations of fruits](https://arxiv.org/abs/2602.04722)
*Gonçalo P. Matos,Carlos Santiago,João P. Costeira,Ricardo L. Saldanha,Ernesto M. Morgado*

Main category: cs.CV

TL;DR: 通过为稀疏3D质心点云设计描述子并匹配星座，本文实现了跨时段果实匹配、果园建图与6DoF定位，解决了视觉果实跟踪中的非刚性、遮挡和特征匮乏难题。


<details>
  <summary>Details</summary>
Motivation: 人工用卡尺或树干计量果实尺寸既费力又无法扩展；现有视觉方法未能可靠匹配不同日期视频中完全相同的果实，且常依赖固定起始相机位姿或附加传感器（GPS）。因此需要一种鲁棒且不依赖额外信息的匹配方法。

Method: 构建稀疏3D点云（果实质心），设计一种用于非常稀疏点云的描述子，通过匹配点云星座而非单个果实来应对非刚性形变、遮挡与缺乏显著视觉特征的问题；可进一步由匹配结果建立果园全局地图并进行相机位姿估计。

Result: 实验表明该方法能成功在不同时间的视频间匹配果实，构建果园地图，并实现基于该地图的6DoF相机定位，为机器人自主导航与选择性采摘提供支持。

Conclusion: 本文提出基于3D质心星座的匹配范式，成功解决了不同时间视频中果实体匹配的问题，能用于果实生长跟踪、果园地图构建及相机6DoF定位。

Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.

</details>


### [80] [Mitigating Long-Tail Bias via Prompt-Controlled Diffusion Augmentation](https://arxiv.org/abs/2602.04749)
*Buddhi Wijenayake,Nichula Wasalathilake,Roshan Godaliyadda,Vijitha Herath,Parakrama Ekanayake,Vishal M. Patel*

Main category: cs.CV

TL;DR: 两阶段流程：A阶段生成按类比例和域条件控制的语义布局；B阶段用Stable Diffusion+ControlNet将布局转为域一致的光照真实图像，混合合成数据提升少数类与城/乡泛化


<details>
  <summary>Details</summary>
Motivation: LoveDA中存在强烈长尾像素不平衡且城/乡域在外观与类频率上显著不同，需要可控数据合成以补偿少数类并改善跨域泛化

Method: 双阶段方法：A阶段使用域感知、掩码比例条件的离散扩散模型生成满足用户指定类比例并保留共现结构的布局；B阶段用Stable Diffusion结合ControlNet把布局翻译成光照真实、域一致图像；将合成对与真实数据混合用于训练分割网络

Result: 提出一种可控的扩增框架，通过生成带标签的合成像对来缓解遥感影像语义分割中的长尾像素不平衡问题

Conclusion: 可控合成增强在LoveDA数据集上对少数类和城/乡域泛化均有显著提升，是缓解长尾偏差的实用方法，代码与模型开源

Abstract: Semantic segmentation of high-resolution remote-sensing imagery is critical for urban mapping and land-cover monitoring, yet training data typically exhibits severe long-tailed pixel imbalance. In the dataset LoveDA, this challenge is compounded by an explicit Urban/Rural split with distinct appearance and inconsistent class-frequency statistics across domains. We present a prompt-controlled diffusion augmentation framework that synthesizes paired label--image samples with explicit control of both domain and semantic composition. Stage~A uses a domain-aware, masked ratio-conditioned discrete diffusion model to generate layouts that satisfy user-specified class-ratio targets while respecting learned co-occurrence structure. Stage~B translates layouts into photorealistic, domain-consistent images using Stable Diffusion with ControlNet guidance. Mixing the resulting ratio and domain-controlled synthetic pairs with real data yields consistent improvements across multiple segmentation backbones, with gains concentrated on minority classes and improved Urban and Rural generalization, demonstrating controllable augmentation as a practical mechanism to mitigate long-tail bias in remote-sensing segmentation. Source codes, pretrained models, and synthetic datasets are available at \href{https://github.com/Buddhi19/SyntheticGen.git}{Github}

</details>


### [81] [Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention](https://arxiv.org/abs/2602.04789)
*Chengtao Lv,Yumeng Shi,Yushi Huang,Ruihao Gong,Shen Ren,Wenya Wang*

Main category: cs.CV

TL;DR: 为AR视频生成提出Light Forcing：按块渐进分配稀疏性并采用帧/块两层稀疏注意力，提升生成质量并显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法直接用于自回归（AR）视频生成会导致性能下降，原因包括对块生成的孤立处理和未充分利用历史有用上下文，需设计专门适配AR生成的稀疏策略。

Method: 提出Chunk-Aware Growth机制按块估计贡献并渐进增加稀疏性，使当前块继承先前知识；提出Hierarchical Sparse Attention在帧级和块级两层进行粗到细的注意力掩码选择，以捕获历史和局部信息。

Result: 在VBench上质量提升（例如84.5）并取得1.2–1.3×端到端加速；结合FP8量化与LightVAE可达2.3×加速，在RTX 5090上实现19.7 FPS。

Conclusion: 本文提出的Light Forcing是首个针对自回归视频生成模型的稀疏注意力方案，通过分块感知增长与分层稀疏注意力两大机制，在保持生成质量的同时显著提升推理效率。

Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.

</details>


### [82] [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/abs/2602.04802)
*Qing'an Liu,Juntong Feng,Yuhao Wang,Xinzhe Han,Yujie Cheng,Yue Zhu,Haiwen Diao,Yunzhi Zhuge,Huchuan Lu*

Main category: cs.CV

TL;DR: VISTA-Bench揭示并量化了VLM在处理图像中的文本时相比纯文本存在的性能下降，且对渲染敏感，提醒需提升模型对视觉化语言的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM基准主要使用纯文本查询，但现实中语言常以图像内文本出现，需检验模型对视觉化文本请求的处理能力与纯文本是否一致。

Method: 构建对照测试集：在受控渲染条件下，将相同语义以纯文本和视觉化文本两种形式提出问题，覆盖多模态感知、推理到单模态理解任务；对20+代表性VLM进行广泛评估，分析随渲染难度变化的性能趋势。

Result: 发现显著的模态差距：在纯文本上表现良好的模型在视觉化文本输入时通常显著退化；渲染难度越高、退化越明显，说明模型对渲染变化敏感。VISTA-Bench为诊断该问题并推动跨文本和像素统一语言表示提供了原则性评估框架。

Conclusion: 本文提出VISTA-Bench，系统评估视觉化文本（visualized text）理解在视觉-语言模型中的表现差距。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.

</details>


### [83] [X2HDR: HDR Image Generation in a Perceptually Uniform Space](https://arxiv.org/abs/2602.04814)
*Ronghuan Wu,Wanchao Su,Kede Ma,Jing Liao,Rafał K. Mantiuk*

Main category: cs.CV

TL;DR: 将HDR转换为感知均匀编码后，只微调去噪器（冻结VAE）并使用低秩适配，可高效地把LDR预训练扩散模型扩展到高质量HDR生成。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模HDR训练数据导致现有图像生成器主要输出LDR。直接在线性RGB上适配会引起显著退化，因此需要一种能利用现有LDR预训练模型且适配HDR统计差异的简单高效方法。

Method: 作者先指出线性RGB HDR与sRGB LDR在强度和颜色统计上的差异会使预训练VAE重建失败；通过将HDR转换为感知均匀编码后，发现LDR预训练的VAE能以与LDR数据相当的保真度进行重建。基于此，他们固定VAE，只在感知均匀空间中对扩散模型的去噪器进行低秩适配（LoRA）微调，从而在计算上高效地支持文本到HDR合成和单图RAW到HDR重建。

Result: 实验证明，在感知均匀编码下进行的适配在感知保真度、文本-图像一致性和有效动态范围上均优于现有技术；VAE在PU21编码下对HDR输入的重建与对LDR输入相当，而线性RGB输入会导致严重降解。

Conclusion: 本文提出了一种在不从头训练的情况下，将已有的低动态范围（LDR）预训练扩散模型扩展到高动态范围（HDR）生成的方法，并证明了通过将HDR数据转换为感知均匀编码（如PU21或PQ）可以有效弥合与sRGB编码LDR图像之间的统计差异，从而实现高保真HDR合成。

Abstract: High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.

</details>


### [84] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 提出一种参数极少（~32k）的状态空间-ConvNeXt混合模型，通过 SCAB 与 FNOClassifier 提升多尺度特征与泛化，在 WSI 上对管状腺瘤分类达到了≈97%准确率与0.9767 F1，且优于更大模型。


<details>
  <summary>Details</summary>
Motivation: 传统病理学对低级别不典型增生的判读主观且有限，数字病理与深度学习可发掘微妙形态学特征以改进风险分层。

Method: 结合浅层 ConvNeXt 特征提取器与并行 vision mamba（用于长短程依赖建模），引入 Spatial and Channel Attention Bridge（SCAB）进行多尺度增强，并使用 Fixed Non-Negative Orthogonal Classifier（FNOClassifier）来大幅减少参数并提升泛化。

Result: 在经分层的低级别管状腺瘤数据集上，XtraLight-MedMamba 在约32,000参数下取得 97.18% 准确率和 0.9767 F1 分数，优于复杂度更高的 transformer 与常规模型。

Conclusion: XtraLight-MedMamba 是一种超轻量级的基于状态空间的深度学习框架，能在全片图（WSI）上对肿瘤性管状腺瘤进行高精度分类。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


### [85] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 该工作基于公开数据集训练并比较四个CNN模型用于指甲疾病分类，InceptionV3效果最佳（95.57%），并通过对抗训练与SHAP解释增强了鲁棒性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 指甲疾病在各年龄层尤其是老年人中常见，但因视觉差异细微常被忽视，早期检测和准确诊断对发现潜在健康问题很重要，因此需要自动化、可靠的诊断工具。

Method: 将图像统一缩放到224x224像素，比较训练InceptionV3、DenseNet201、EfficientNetV2和ResNet50四种预训练CNN模型，评估分类性能；在训练过程中加入对抗训练以提升鲁棒性；使用SHAP可视化重要特征以辅助解释。

Result: 在3835张224x224像素的图像上比较四种CNN模型，InceptionV3达到95.57%准确率，DenseNet201为94.79%；引入对抗训练提升了模型在噪声或难例上的鲁棒性；SHAP可视化提高了决策透明度。

Conclusion: 该论文提出了基于深度学习的指甲疾病自动分类系统，使用公开数据集（3835张图像，6类），并比较了四个CNN模型，最终InceptionV3表现最佳（准确率95.57%），采用对抗训练增强模型鲁棒性，并用SHAP解释模型决策。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [86] [LitS: A novel Neighborhood Descriptor for Point Clouds](https://arxiv.org/abs/2602.04838)
*Jonatan B. Bastos,Francisco F. Rivera,Oscar G. Lorenzo,David L. Vilariño,José C. Cabaleiro,Alberto M. Esmorís,Tomás F. Pena*

Main category: cs.CV

TL;DR: LitS通过在单位圆上为每个方向统计锥状邻居计数，形成分段常数描述子，具有两种变体和两个可调参数，能鲁棒地描述点云局部几何并用于比较点间结构差异。


<details>
  <summary>Details</summary>
Motivation: 当前点云分析需要鲁棒且信息丰富的局部描述子，以处理可变密度、噪声及捕捉方向性邻域分布，从而支持分类、配准、重建等任务。

Method: 在每个点建立局部参考系，在单位圆上构造分段常数函数（方向域），每个方向对应锥形邻域内邻居计数；提供常规与累积两种变体，含两个可调参数（角分辨率与锥角或邻域尺度）。通过比较近邻点的LitS变化获取全局结构信息。

Result: LitS可在2D/3D点云上表示方向性邻域分布，适应不同密度与噪声，能够在点间比较中揭示几何变化；文中展示了两种变体的有效性和参数敏感性（假设实验结果支持鲁棒性与表达性）。

Conclusion: LitS是一种新颖且灵活的邻域描述子，能有效捕捉点云局部几何信息，并通过方向性统计支持全局结构推断。

Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.

</details>


### [87] [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/abs/2602.04864)
*Soumya Jahagirdar,Walid Bousselham,Anna Kukleva,Hilde Kuehne*

Main category: cs.CV

TL;DR: Mask-LLaVA通过掩码对象+全局+局部多层次视觉表示，训练时用全部tokens，推理时可裁减掩码对象tokens，实现少token高效推理且性能损失小。


<details>
  <summary>Details</summary>
Motivation: 当前自回归VLM依赖大量视觉tokens导致推理计算开销大，需一种在保持性能前提下减少tokens的方法。

Method: 在训练阶段同时使用掩码对象tokens、全局tokens和局部patch tokens；在推理阶段可动态降低（尤其是删除部分）掩码对象tokens数量而无需重训练。

Result: 在标准基准上，Mask-LLaVA使用更少视觉tokens下，性能与现有高效token方法相当，并能与原始LLaVA基线相近。分析表明多层次特征结合能实现用更少tokens高效学习并在测试时动态选择tokens以维持良好表现。

Conclusion: 本文提出Mask-LLaVA，通过融合掩码目标表示、全局token与局部patch token，构建紧凑且信息丰富的视觉表示，从而减少自回归VLM所需的视觉tokens数量，并保持性能。

Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.

</details>


### [88] [Laminating Representation Autoencoders for Efficient Diffusion](https://arxiv.org/abs/2602.04873)
*Ramón Calvo-González,François Fleuret*

Main category: cs.CV

TL;DR: FlatDINO用VAE把DINOv2补丁特征压缩到32个连续token，显著减少计算并保持高质量（ImageNet gFID 1.80），节省推理和训练FLOPs。


<details>
  <summary>Details</summary>
Motivation: DINOv2等自监督encoder产生的密集补丁网格冗余高，导致在特征空间上运行的扩散模型计算开销大。通过压缩表示，希望在不损失生成质量的前提下降低计算成本。

Method: 设计并训练一个变分自编码器（FlatDINO），将DINOv2补丁表示压缩为仅32个连续token；在这些低维潜空间上训练DiT-XL扩散模型，并使用classifier-free guidance评估生成质量。

Result: 在ImageNet 256×256上，DiT-XL在FlatDINO潜空间训练并使用classifier-free guidance时达到了gFID 1.80；与在未压缩DINOv2特征上训练相比，前向推理FLOPs降低约8倍，训练步FLOPs最高降低约4.5倍。

Conclusion: 本文提出了FlatDINO，一种将DINOv2的密集补丁特征压缩为一维连续token序列的VAE，从而显著降低序列长度与维度并加速基于特征的扩散模型。

Abstract: Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.

</details>


### [89] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder通过统一的双向表示和多视角监督更新，实现了首个闭环混合生成模拟器，能从单张图像生成长时程、动作条件的物理一致且视觉连贯的4D场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法将物理状态与视觉表征解耦，导致生成修正无法更新底层物理，难以实现长期、交互驱动的4D场景生成；因此需要一种能在视觉与物理间建立可逆映射并能在闭环中持续自我修正的方案。

Method: 引入统一的双向表示（将物理状态与视觉原语建立可互映的联系）和稳健的更新机制（从多视角收集监督以消除优化歧义），构成闭环系统，使生成修正能影响后续物理演化；系统为混合生成模拟器，支持动作条件下的长时程4D生成。

Result: 实验证明从单张图像出发，PerpetualWonder能模拟复杂的多步交互，执行长时程动作并保持物理合理性与视觉一致性，优于现有解耦方法。

Conclusion: PerpetualWonder提出了一种首创的闭环混合生成模拟器，能从单张图像生成长期、动作条件的4D场景，并通过统一表示连接物理状态与视觉原语，从而在生成细化中同时更新动力学与外观，保证物理合理性与视觉一致性。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


### [90] [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/abs/2602.04877)
*Zihang Lai,Eldar Insafutdinov,Edgar Sucar,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 用warp+Transformer替代cost volume，得到高效且效果领先的密集点跟踪与光流统一模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于cost volume的跟踪器在空间分辨率上有平方级复杂度，限制了扩展性与效率；受光流领域warp方法的启发，作者希望设计一种更高效且可扩展的密集点跟踪架构。

Method: 提出名为Method的跟踪器（文中命名为\method），核心是用warp替代cost volume：基于当前跟踪估计将目标帧特征warp到查询帧，迭代细化；同时采用Transformer用于在所有track之间进行联合时空推理以建立长程对应关系，而不计算特征相关性矩阵。

Result: 在TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP等密集点跟踪基准上达到或超过最先进性能；在光流任务（Sintel、KITTI、Spring）上也表现优异，有时超越专业光流方法，显示warp方法的通用性。

Conclusion: 本文提出了一种基于warp的密集点跟踪器，避免了传统代价体（cost volume）的二次复杂度，通过迭代性地将目标帧特征按当前估计反向warp到查询帧并结合Transformer进行联合时空推理，从而建立长程对应关系。该方法在多个密集点跟踪和光流基准上达到或超越现有最优性能，表明warp架构能统一密集点跟踪与光流估计。

Abstract: Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [91] [StraTyper: Automated Semantic Type Discovery and Multi-Type Annotation for Dataset Collections](https://arxiv.org/abs/2602.04004)
*Christos Koutras,Juliana Freire*

Main category: cs.DB

TL;DR: StraTyper无需预定义标签，通过聚类+受控LM生成+级联发现实现集合级别类型发现与多类型注释，准确、低成本且改善下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有列类型注释方法需预先定义标签集并假设列为单一类型，无法适应领域特定数据与多类型列，且商业LLM成本高且输出不稳定。

Method: 依赖列聚类、受控类型生成与级联迭代的策略化调用LLM，在集合级别发现语义类型并对每列执行多类型注释，同时通过簇内共享与阈值控制减少调用次数与冗余。

Result: 在真实基准上（人工与LLM辅助评估），StraTyper在数值与非数值列上均能发现准确类型、处理多类型列、比商业LLM节省显著成本，并提升连接发现和模式匹配等下游任务性能。

Conclusion: StraTyper通过无标签的LM驱动类型发现与多类型注释，解决了现有CTA方法在领域覆盖性、单一类型假设和高昂LLM成本上的局限。

Abstract: Understanding dataset semantics is crucial for effective search, discovery, and integration pipelines. To this end, column type annotation (CTA) methods associate columns of tabular datasets with semantic types that accurately describe their contents, using pre-trained deep learning models or Large Language Models (LLMs). However, existing approaches require users to specify a closed set of semantic types either at training or inference time, hindering their application to domain-specific datasets where pre-defined labels often lack adequate coverage and specificity. Furthermore, real-world datasets frequently contain columns with values belonging to multiple semantic types, violating the single-type assumption of existing CTA methods. While proprietary LLMs have shown effectiveness for CTA, they incur high monetary costs and produce inconsistent outputs for similar columns, leading to type redundancy that negatively affects downstream applications. To address these challenges, we introduce StraTyper, a cost-effective method for column type discovery (CTD) and multi-type annotation (CMTA) in dataset collections. StraTyper eliminates the need for pre-defined semantic labels by systematically employing LLMs to discovery types tailored to the dataset collection at hand. Through strategic column clustering, controlled type generation, and iterative cascading discovery, StraTyper balances type precision with annotation coverage while minimizing LLM costs. Our experimental evaluation-both manual and LLM-assisted-on real-world benchmarks demonstrates that StraTyper discovers accurate types for both numerical and non-numerical data, achieves substantial cost savings compared to commercial LLMs, and effectively handles multi-typed columns. We further show that StraTyper's annotations improve downstream tasks, including join discovery and schema matching, outperforming LLM-only baselines.

</details>


### [92] [PluRel: Synthetic Data unlocks Scaling Laws for Relational Foundation Models](https://arxiv.org/abs/2602.04029)
*Vignesh Kothapalli,Rishabh Ranjan,Valter Hudovernik,Vijay Prakash Dwivedi,Johannes Hoffart,Carlos Guestrin,Jure Leskovec*

Main category: cs.DB

TL;DR: PluRel 是一个分阶段的多表合成数据库生成框架（建模 schema、有向/二分图和条件因果特征生成），能生成大量多样合成数据库。基于这些合成数据训练的关系型基础模型展示出随数据规模的幂律扩展性、对真实数据库的泛化提升，以及作为后续真实数据微调的良好起点。


<details>
  <summary>Details</summary>
Motivation: 现实中用于训练关系型基础模型的多表数据库稀缺且常受隐私限制，现有合成表格数据方法难以同时保留多表模式结构与主-外键连通性。为了解决这一数据可用性瓶颈，需要一种能够从头生成具备合理模式、连通性和表内统计特征的多表合成数据库的方法，以便扩展 RFM 的预训练数据规模并研究其扩展性。

Method: PluRel 将合成过程拆分为三步：用有向图建模模式(schema)；用二分图模拟表间主-外键连接关系；在表内通过条件因果机制(conditioned causal mechanisms)模拟特征分布。该设计空间在每一阶段都支持多样性选择，并保持计算开销低。基于这些组件，作者生成大量不同的合成数据库用于 RFM 预训练与评估，并分析性能随数据规模的变化规律。

Result: 作者基于 PluRel 的实验表明：1) RFM 的预训练损失相对于合成数据库数量和预训练 tokens 呈现幂律下降；2) 增加合成数据库的数量能改善模型在真实数据库上的泛化性能；3) 在合成数据上预训练得到的模型可作为在真实数据上继续微调的强基础模型。此外 PluRel 本身实现轻量且能生成多样数据库。

Conclusion: PluRel 提出了一种从头合成多表关系型数据库的模块化框架，能够在保留模式、主外键连通性和表内特征分布的同时生成多样且轻量的合成数据。基于 PluRel 生成的数据进行 RFM 预训练，作者观察到预训练损失随合成数据库数量和总预训练 tokens 呈幂律缩放；增加合成数据库数量能提升对真实数据库的泛化；合成预训练能作为在真实数据库上继续训练的良好基础模型。整体结论是：合成数据扩展（scaling）为关系型基础模型的预训练提供了可行且有效的范式。

Abstract: Relational Foundation Models (RFMs) facilitate data-driven decision-making by learning from complex multi-table databases. However, the diverse relational databases needed to train such models are rarely public due to privacy constraints. While there are methods to generate synthetic tabular data of arbitrary size, incorporating schema structure and primary--foreign key connectivity for multi-table generation remains challenging. Here we introduce PluRel, a framework to synthesize multi-tabular relational databases from scratch. In a step-by-step fashion, PluRel models (1) schemas with directed graphs, (2) inter-table primary-foreign key connectivity with bipartite graphs, and, (3) feature distributions in tables via conditional causal mechanisms. The design space across these stages supports the synthesis of a wide range of diverse databases, while being computationally lightweight. Using PluRel, we observe for the first time that (1) RFM pretraining loss exhibits power-law scaling with the number of synthetic databases and total pretraining tokens, (2) scaling the number of synthetic databases improves generalization to real databases, and (3) synthetic pretraining yields strong base models for continued pretraining on real databases. Overall, our framework and results position synthetic data scaling as a promising paradigm for RFMs.

</details>


### [93] [Piece of CAKE: Adaptive Execution Engines via Microsecond-Scale Learning](https://arxiv.org/abs/2602.04181)
*Zijie Zhao,Ryan Marcus*

Main category: cs.DB

TL;DR: CAKE用反事实多臂老虎机和低延迟决策树，在morsel级别快速选择最优算子，实现显著延迟提升（最高2x）。


<details>
  <summary>Details</summary>
Motivation: 低级数据库算子有多种物理实现，性能依赖数据分布；静态启发式或最坏情况默认策略常错失性能优化机会，需自适应、低延迟的内核选择方法。

Method: 使用上下文多臂老虎机学习针对每个morsel的内核选择；在训练阶段有选择地并行运行多个内核以获取反事实完整反馈；将学习到的策略编译为低延迟的regret trees以实现微秒级决策时间。

Result: 在实验中，CAKE相比最先进的静态启发式方法将端到端工作负载延迟最高减少约2倍。

Conclusion: 本文提出CAKE系统，通过微秒级上下文多臂老虎机在数据morsel粒度选择最佳物理算子实现，利用反事实反馈和低延迟决策树降低延迟。

Abstract: Low-level database operators often admit multiple physical implementations ("kernels") that are semantically equivalent but have vastly different performance characteristics depending on the input data distribution. Existing database systems typically rely on static heuristics or worst-case optimal defaults to select these kernels, often missing significant performance opportunities. In this work, we propose CAKE (Counterfactual Adaptive Kernel Execution), a system that learns to select the optimal kernel for each data "morsel" using a microsecond-scale contextual multi-armed bandit. CAKE circumvents the high latency of traditional reinforcement learning by exploiting the cheapness of counterfactuals -- selectively running multiple kernels to obtain full feedback -- and compiling policies into low-latency regret trees. Experimentally, we show that CAKE can reduce end-to-end workload latency by up to 2x compared to state-of-the-art static heuristics.

</details>


### [94] [LatentTune: Efficient Tuning of High Dimensional Database Parameters via Latent Representation Learning](https://arxiv.org/abs/2602.04190)
*Sein Kwon,Youngwan Jo,Seungyeon Choi,Jieun Lee,Huijun Jin,Sanghyun Park*

Main category: cs.DB

TL;DR: LatentTune通过数据增强、全参数潜在空间和外部指标融合，能在更短数据生成时间下对目标工作负载进行精确调参，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习调参方法生成训练数据耗时长、通常仅优化参数子集且依赖相似工作负载信息而非目标工作负载信息，限制了优化效果与适用性。

Method: LatentTune采用数据增强减少训练数据生成时间，构建将所有数据库参数压缩为潜在表示的潜在空间以支持全参数优化，并在潜在空间中融合外部性能指标以针对目标工作负载进行精细调优。

Result: 在四个工作负载上对MySQL和RocksDB的实验显示，LatentTune比基线模型表现更好：RocksDB最高提升达1332%，MySQL取得最高11.82%吞吐量提升和46.01%延迟减少。

Conclusion: LatentTune通过数据增强、潜在空间压缩全参数和外部指标融合，实现了更高效和精确的数据库参数调优，在MySQL和RocksDB上显著优于基线方法。

Abstract: As data volumes continue to grow, optimizing database performance has become increasingly critical, making the implementation of effective tuning methods essential. Among various approaches, database parameter tuning has proven to be a highly effective means of enhancing performance. Recent studies have shown that machine learning techniques can successfully optimize database parameters, leading to significant performance improvements. However, existing methods still face several limitations. First, they require substantial time to generate large training datasets. Second, to cope with the challenges of highdimensional optimization, they typically optimize only a subset of parameters rather than the full configuration space. Third, they often rely on information from similar workloads instead of directly leveraging information from the target workload. To address these limitations, we propose LatentTune, a novel approach that differs fundamentally from traditional methods. To reduce the time required for data generation, LatentTune incorporates a data augmentation strategy. Furthermore, it constructs a latent space that compresses information from all database parameters, enabling the optimization of the full configuration space. In addition, LatentTune integrates external metric information into the latent space, allowing for precise tuning tailored to the actual target workload. Experimental results demonstrate that LatentTune outperforms baseline models across four workloads on MySQL and RocksDB, achieving up to 1332% improvement for RocksDB and 11.82% throughput gain with 46.01% latency reduction for MySQL.

</details>


### [95] [Data Agents: Levels, State of the Art, and Open Problems](https://arxiv.org/abs/2602.04261)
*Yuyu Luo,Guoliang Li,Ju Fan,Nan Tang*

Main category: cs.DB

TL;DR: 提出L0–L5数据代理分层分类，回顾当前系统，定位Proto-L3，并展望实现主动与生成性代理的研究路线。


<details>
  <summary>Details</summary>
Motivation: 当前“数据代理”一词定义模糊，混淆了简单响应式助手与理想中完全自治的数据科学家，导致能力界限与责任难以判断。需要统一层级化框架以便用户、开发者与监管者理解与比较。

Method: 通过定义L0至L5五个自治等级，分析各等级之间的关键演进差异；回顾代表性L0–L2系统；识别Proto-L3系统并讨论向L4/L5的研究挑战。

Result: 给出L0–L5分类、生命周期与层级视角；汇总L0–L2现有系统示例；指出Proto-L3的发展方向并列出面向L4/L5的关键研究问题（如主动性、生成性与责任归属）。

Conclusion: 本文提出了首个分层的数据代理分类（L0–L5），并基于此构建生命周期与层级驱动视角，为理解与开发数据代理提供实践图谱与未来研究路线图。

Abstract: Data agents are an emerging paradigm that leverages large language models (LLMs) and tool-using agents to automate data management, preparation, and analysis tasks. However, the term "data agent" is currently used inconsistently, conflating simple query responsive assistants with aspirational fully autonomous "data scientists". This ambiguity blurs capability boundaries and accountability, making it difficult for users, system builders, and regulators to reason about what a "data agent" can and cannot do.
  In this tutorial, we propose the first hierarchical taxonomy of data agents from Level 0 (L0, no autonomy) to Level 5 (L5, full autonomy). Building on this taxonomy, we will introduce a lifecycleand level-driven view of data agents. We will (1) present the L0-L5 taxonomy and the key evolutionary leaps that separate simple assistants from truly autonomous data agents, (2) review representative L0-L2 systems across data management, preparation, and analysis, (3) highlight emerging Proto-L3 systems that strive to autonomously orchestrate end-to-end data workflows to tackle diverse and comprehensive data-related tasks under supervision, and (4) discuss forward-looking research challenges towards proactive (L4) and generative (L5) data agents. We aim to offer both a practical map of today's systems and a research roadmap for the next decade of data-agent development.

</details>


### [96] [Identifying knowledge gaps in biodiversity data and their determinants at the regional level](https://arxiv.org/abs/2602.04314)
*Didier Alard,Anaïs Guéry*

Main category: cs.DB

TL;DR: 开放数据库中知识空白由可达性与地方治理驱动，无脊椎动物受影响更甚，建议针对弱采样区域与分类群重新分配资金或在分析时校正空间抽样偏差。


<details>
  <summary>Details</summary>
Motivation: 为了理解开放获取物种出现数据中空间与分类群知识空白的决定因素，进而为数据使用者提供校正空间抽样偏差（SSP）的依据，并为资助分配与监测策略提供建议。

Method: 使用完整性（completeness）和无知（ignorance）两种指标评估知识空白，针对8个分类群（5个脊椎动物组和3个无脊椎动物组），在全区和三个子区层面分析，考察可达性、生态吸引力、农业压力与治理等变量对知识空白的影响。

Result: 发现无脊椎动物知识空白显著高于脊椎动物；可达性变量（如道路可达性、人类活动）比生态吸引力更能解释知识空白；农业压力对无脊椎动物影响更大；不同子区的结果显示地方治理与资金分配改变了知识分布模式。

Conclusion: 本研究表明在法国最大行政区的开放生物多样性数据库中，知识空白主要由可达性相关因素驱动，而非生态吸引力，且无脊椎动物比有脊椎动物的知识空白更大；地方生物多样性治理（资金与政策）显著影响知识分布。

Abstract: Biodiversity open-access databases are valuable resources in the structuring and accessibility of species occurrence data. By compiling different data sources, they reveal the uneven spatial distribution of knowledge, with areas or taxonomic groups better prospected than others. Understanding the determinants of spatial and taxonomic knowledge gaps helps in informing the use of open-access data. Here, we identified knowledge gaps' determinants within a French regional biodiversity database, in the largest administrative region in France. Knowledge gaps were assessed using two metrics, completeness and ignorance scores, for 8 taxonomic groups covering five vertebrates and three invertebrates groups. The data was analyzed for the entire region, but also at the level of the three former sub-regions, to identify the potential drivers that may account for knowledge gaps' determinants. Our findings show that invertebrates were characterized by higher knowledge gaps than vertebrates. Overall, knowledge gaps are influenced by variables related to sites' accessibility rather than ecological appeal across both metrics. All groups shared similar determinants of gaps, except for the impact of agricultural pressure which is found to be more significant for invertebrates than vertebrates. Ultimately, our study emphasizes the impact of biodiversity governance, through local funding and regional political decisions, on knowledge distribution in open-access databases. We recommend limiting these biases by redirecting biodiversity funding towards under-sampled taxonomic groups and under-prospected areas. When not possible, users of data extracted from these databases should correct for spatial-sampling biases (SSP) using knowledge gaps' maps in order to get a more accurate understanding of species occurrence.

</details>


### [97] [The Stretto Execution Engine for LLM-Augmented Data Systems](https://arxiv.org/abs/2602.04430)
*Gabriele Sanmartino,Matthias Urban,Paolo Papotti,Carsten Binnig*

Main category: cs.DB

TL;DR: Stretto用约束优化+KV缓存构建可调物理操作符光谱，联合选择实现与误差分配，提供端到端质量保证并在运行时—精度上超越现有系统。


<details>
  <summary>Details</summary>
Motivation: LLM算子在结构化和非结构化数据上支持语义查询，但其运行时与精度之间存在不可避免的权衡；需要一个能够提供端到端质量保证且高效导航该权衡的执行引擎。

Method: 将查询规划视为约束优化，使用梯度优化器联合选择操作符实现与错误预算分配；引入KV缓存机制实现可调的物理操作符，形成连续的运行时—精度折衷；整体执行引擎在执行时按优化结果配置管道并监控质量保证。

Result: 实验表明Stretto在满足质量保证的同时，性能优于最先进系统，并能一致地达到预设的质量约束。

Conclusion: Stretto通过将查询规划建模为带约束的优化问题，并使用基于梯度的优化器联合选择操作符实现与误差分配，同时利用KV缓存将离散设计空间扩展为连续的运行时—精度折衷谱，能够在满足端到端质量保证的前提下高效权衡运行时与精度，从而优于现有系统并稳定满足质量约束。

Abstract: LLM-augmented data systems enable semantic querying over structured and unstructured data, but executing queries with LLM-powered operators introduces a fundamental runtime--accuracy trade-off. In this paper, we present Stretto, a new execution engine that provides end-to-end query guarantees while efficiently navigating this trade-off in a holistic manner. For this, Stretto formulates query planning as a constrained optimization problem and uses a gradient-based optimizer to jointly select operator implementations and allocate error budgets across pipelines. Moreover, to enable fine-grained execution choices, Stretto introduces a novel idea on how KV-caching can be used to realize a spectrum of different physical operators that transform a sparse design space into a dense continuum of runtime--accuracy trade-offs. Experiments show that Stretto outperforms state-of-the-art systems while consistently meeting quality guarantees.

</details>
