<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 71]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices](https://arxiv.org/abs/2510.23775)
*Aryan Mathur,Asaduddin Ahmed,Pushti Amit Vasoya,Simeon Kandan Sonar,Yasir Z,Madesh Kuppusamy*

Main category: cs.CV

TL;DR: 本文提出一种将轻量卷积网络与视觉语言模型结合的可解释真实性检测系统，针对32×32图像实现高准确率（96.5%）与边缘设备实时推理（175ms），并提供伪影定位与文本解释。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成图像的真实性验证难题，提供既高效又可解释的检测系统，适用于低分辨率图像和边缘设备部署。

Method: 使用轻量卷积分类器“Faster-Than-Lies”进行快速分类；基于自编码器的重建误差生成伪影定位热图；调用Qwen2-VL-7B对热图与图像联合输入以生成自然语言解释；在扩展CiFAKE数据集（含对抗扰动）上训练与评价，并在8核CPU上优化推理时间至175ms。

Result: 提出结合轻量卷积分类器“Faster-Than-Lies”和视觉语言模型Qwen2-VL-7B的可解释图像真实性检测系统，在32×32图像上实现分类、定位与解释；在扩展的CiFAKE数据集（含对抗扰动）上达成96.5%准确率，CPU推理175ms；利用自编码器重建误差图生成伪影定位热图，并将70种视觉伪影归纳为8类语义组，生成可解释文本说明。

Conclusion: 结合视觉与语言推理可在低分辨率图像上实现可解释的真实性检测，具有可部署性并能扩展到取证、工业检测和社交平台审核等领域，但需要注意对抗鲁棒性、数据偏差与可解释性评估的局限。

Abstract: The increasing realism of AI-generated imagery poses challenges for verifying
visual authenticity. We present an explainable image authenticity detection
system that combines a lightweight convolutional classifier
("Faster-Than-Lies") with a Vision-Language Model (Qwen2-VL-7B) to classify,
localize, and explain artifacts in 32x32 images. Our model achieves 96.5%
accuracy on the extended CiFAKE dataset augmented with adversarial
perturbations and maintains an inference time of 175ms on 8-core CPUs, enabling
deployment on local or edge devices. Using autoencoder-based reconstruction
error maps, we generate artifact localization heatmaps, which enhance
interpretability for both humans and the VLM. We further categorize 70 visual
artifact types into eight semantic groups and demonstrate explainable text
generation for each detected anomaly. This work highlights the feasibility of
combining visual and linguistic reasoning for interpretable authenticity
detection in low-resolution imagery and outlines potential cross-domain
applications in forensics, industrial inspection, and social media moderation.

</details>


### [2] [CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting](https://arxiv.org/abs/2510.23785)
*Md Tanvir Hossain,Akif Islam,Mohd Ruhul Ameen*

Main category: cs.CV

TL;DR: CountFormer replaces CounTR encoder with DINOv2 and fuses positional embeddings, decoding to density maps, improving structural and dense-counting performance and approaching exemplar-free general counting


<details>
  <summary>Details</summary>
Motivation: Enable class-agnostic counting by learning repetition and structural coherence; address failures on complex shapes, symmetry, overlap by richer features from foundation model

Method: Transformer-based enhancement of CounTR using DINOv2 and positional fusion

Result: Comparable to SOTA on FSC-147, superior on structurally intricate or densely packed scenes; density-map decoding via conv decoder; positional embedding preserves geometry

Conclusion: Integrating self-supervised foundation models (DINOv2) and positional fusion in transformer-based counters enhances structural perception and counting accuracy, moving toward general class-agnostic counting

Abstract: Humans can effortlessly count diverse objects by perceiving visual repetition
and structural relationships rather than relying on class identity. However,
most existing counting models fail to replicate this ability; they often
miscount when objects exhibit complex shapes, internal symmetry, or overlapping
components. In this work, we introduce CountFormer, a transformer-based
framework that learns to recognize repetition and structural coherence for
class-agnostic object counting. Built upon the CounTR architecture, our model
replaces its visual encoder with the self-supervised foundation model DINOv2,
which produces richer and spatially consistent feature representations. We
further incorporate positional embedding fusion to preserve geometric
relationships before decoding these features into density maps through a
lightweight convolutional decoder. Evaluated on the FSC-147 dataset, our model
achieves performance comparable to current state-of-the-art methods while
demonstrating superior accuracy on structurally intricate or densely packed
scenes. Our findings indicate that integrating foundation models such as DINOv2
enables counting systems to approach human-like structural perception,
advancing toward a truly general and exemplar-free counting paradigm.

</details>


### [3] [A geometric and deep learning reproducible pipeline for monitoring floating anthropogenic debris in urban rivers using in situ cameras](https://arxiv.org/abs/2510.23798)
*Gauthier Grimmer,Romain Wenger,Clément Flint,Germain Forestier,Gilles Rixhon,Valentin Chardon*

Main category: cs.CV

TL;DR: 提出基于固定水面摄像头的自动监测框架：用深度学习连续检测漂浮垃圾，并通过相机几何模型估算物体真实尺寸；强调数据集构建及避免时间/泄漏偏差的重要性。


<details>
  <summary>Details</summary>
Motivation: 河流中漂浮的人工垃圾对生态、航行与游憩造成严重影响，现有监测方法成本高或覆盖不足，故提出一种低成本、自动化且可连续量化的监测方案。

Method: 使用多种目标检测深度学习模型对不同环境条件下的漂浮垃圾进行训练与测试，检验不同学习配置及时间泄漏的影响；并基于相机内参与外参建立投影几何模型，再用回归修正提升尺寸估计精度。

Result: This paper presents a practical framework for monitoring floating anthropogenic debris in rivers using fixed in-situ cameras, combining deep learning detection with geometric size estimation.

Conclusion: 基于固定摄像头的深度学习检测结合投影几何和回归修正能实现低成本、稳健的河道漂浮垃圾自动监测，但数据集负样本、时间泄漏要谨慎处理。

Abstract: The proliferation of floating anthropogenic debris in rivers has emerged as a
pressing environmental concern, exerting a detrimental influence on
biodiversity, water quality, and human activities such as navigation and
recreation. The present study proposes a novel methodological framework for the
monitoring the aforementioned waste, utilising fixed, in-situ cameras. This
study provides two key contributions: (i) the continuous quantification and
monitoring of floating debris using deep learning and (ii) the identification
of the most suitable deep learning model in terms of accuracy and inference
speed under complex environmental conditions. These models are tested in a
range of environmental conditions and learning configurations, including
experiments on biases related to data leakage. Furthermore, a geometric model
is implemented to estimate the actual size of detected objects from a 2D image.
This model takes advantage of both intrinsic and extrinsic characteristics of
the camera. The findings of this study underscore the significance of the
dataset constitution protocol, particularly with respect to the integration of
negative images and the consideration of temporal leakage. In conclusion, the
feasibility of metric object estimation using projective geometry coupled with
regression corrections is demonstrated. This approach paves the way for the
development of robust, low-cost, automated monitoring systems for urban aquatic
environments.

</details>


### [4] [RareFlow: Physics-Aware Flow-Matching for Cross-Sensor Super-Resolution of Rare-Earth Features](https://arxiv.org/abs/2510.23816)
*Forouzan Fallah,Wenwen Li,Chia-Yu Hsu,Hyunho Lee,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出RareFlow：结合Gated ControlNet、文本提示与物理一致性损失，并用随机前向传播估计不确定性，以提高遥感SR在分布外场景的真实性与可控性，实验显示显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有遥感SR方法在分布外（如罕见地貌和多传感器差异）下常产生视觉上合理但物理上不准确的结果，需一种能保持物理一致性并能识别不熟悉输入的鲁棒SR方法。

Method: 双条件网络架构（Gated ControlNet + 文本提示）结合多分量物理一致性损失（光谱、辐射与传感器特性约束），并采用随机前向传播估计不确定性；在多传感器卫星影像基准上训练与评估。

Result: RareFlow 提出了一种面向遥感超分辨率（SR）的物理感知框架，旨在提高在分布外（OOD）条件下的鲁棒性。核心为双条件架构：Gated ControlNet 保持低分辨率输入的几何细节，文本提示提供语义指导；复合损失函数约束光谱与辐射一致性以符合传感器特性；使用随机前向传播量化预测不确定性以检测不熟悉样本并减少虚假特征。实验在多传感器卫星影像基准上显示，在专家盲评中接近真实影像的保真度，FID 等感知指标有明显提升（FID 约下降40%）。

Conclusion: RareFlow 在处理传感器多样性和稀有地貌时能生成更物理一致且更少虚假的超分结果，且可通过输出方差识别不熟悉输入，适用于数据稀缺的科学场景。

Abstract: Super-resolution (SR) for remote sensing imagery often fails under
out-of-distribution (OOD) conditions, such as rare geomorphic features captured
by diverse sensors, producing visually plausible but physically inaccurate
results. We present RareFlow, a physics-aware SR framework designed for OOD
robustness. RareFlow's core is a dual-conditioning architecture. A Gated
ControlNet preserves fine-grained geometric fidelity from the low-resolution
input, while textual prompts provide semantic guidance for synthesizing complex
features. To ensure physically sound outputs, we introduce a multifaceted loss
function that enforces both spectral and radiometric consistency with sensor
properties. Furthermore, the framework quantifies its own predictive
uncertainty by employing a stochastic forward pass approach; the resulting
output variance directly identifies unfamiliar inputs, mitigating feature
hallucination. We validate RareFlow on a new, curated benchmark of multi-sensor
satellite imagery. In blind evaluations, geophysical experts rated our model's
outputs as approaching the fidelity of ground truth imagery, significantly
outperforming state-of-the-art baselines. This qualitative superiority is
corroborated by quantitative gains in perceptual metrics, including a nearly
40\% reduction in FID. RareFlow provides a robust framework for high-fidelity
synthesis in data-scarce scientific domains and offers a new paradigm for
controlled generation under severe domain shift.

</details>


### [5] [TRELLISWorld: Training-Free World Generation from Object Generators](https://arxiv.org/abs/2510.23880)
*Hanke Chen,Yuan Liu,Minchen Li*

Main category: cs.CV

TL;DR: 无需重训练，利用文本到3D对象扩散模型作为瓦片生成器，重叠区域独立生成并加权平均融合，支持大尺度、可编辑、语义可控的360°文本驱动3D场景合成。


<details>
  <summary>Details</summary>
Motivation: 训练-free、多物体、全景可视的3D场景生成需求；现有方法受限于单物体、需特定训练或无法360度查看。

Method: 把场景分割为重叠的3D瓦片，使用已训练的文本到3D对象扩散模型独立对每个瓦片进行采样生成，最后通过加权平均对重叠区域的体素/特征进行融合，采用最小启发式策略来保证语义一致性并支持局部编辑与可扩展布局。

Result: 提出一种无训练的3D场景合成方法：把通用文本到3D对象的扩散模型当作模块化瓦片生成器，将场景生成改写为多瓦片去噪问题，独立生成重叠3D区域并通过加权平均无缝混合，实现大尺度一致场景的可扩展合成、保留局部语义控制。

Conclusion: 该方法提供了一个简单有效的基础，用于通用语言驱动的3D场景构建：无需场景级数据或重训，兼具多样布局、高效生成和灵活编辑能力，但可能受限于对象级先验的表征能力和瓦片混合的细节一致性。

Abstract: Text-driven 3D scene generation holds promise for a wide range of
applications, from virtual prototyping to AR/VR and simulation. However,
existing methods are often constrained to single-object generation, require
domain-specific training, or lack support for full 360-degree viewability. In
this work, we present a training-free approach to 3D scene synthesis by
repurposing general-purpose text-to-3D object diffusion models as modular tile
generators. We reformulate scene generation as a multi-tile denoising problem,
where overlapping 3D regions are independently generated and seamlessly blended
via weighted averaging. This enables scalable synthesis of large, coherent
scenes while preserving local semantic control. Our method eliminates the need
for scene-level datasets or retraining, relies on minimal heuristics, and
inherits the generalization capabilities of object-level priors. We demonstrate
that our approach supports diverse scene layouts, efficient generation, and
flexible editing, establishing a simple yet powerful foundation for
general-purpose, language-driven 3D scene construction.

</details>


### [6] [Improving Visual Discriminability of CLIP for Training-Free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2510.23894)
*Jinxin Zhou,Jiachen Jiang,Zhihui Zhu*

Main category: cs.CV

TL;DR: 该论文提出LHT-CLIP，一个训练免费的方法，通过在层、头、标记三个层面恢复CLIP的视觉可区分性以提升语义分割性能。主要发现：末层强化图文对齐但牺牲视觉可区分性；部分注意力头长期保留可区分性；异常标记有稀疏一致的激活。基于此，提出语义-空间重加权、选择性头增强和异常标记替换三项技术，实现无需训练或额外网络即可在多基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是解决CLIP从图像级预训练到像素级分割的错配问题：模型后层偏向图文全局对齐，降低对局部像素差异的敏感性，从而影响密集预测。作者希望在不额外训练或引入新模型的前提下，挖掘CLIP内部仍存在的视觉区分信号，通过层、头、标记级别的处理恢复像素级判别力。

Method: 方法包括三步：1) 语义-空间重加权：针对不同层和空间位置调整特征权重以强化语义与空间信息；2) 选择性头增强：识别并放大那些在多数据集上表现出强视觉可区分性的注意力头；3) 异常标记替换：检测并用正常标记或插值替换激活稀疏且一致的异常标记，减少噪声对分割的干扰。全部方法均无训练，直接基于CLIP特征重组与加权。

Result: 在8个语义分割基准上，LHT-CLIP在零样本、弱监督和通用分割设置中均显著优于先前无训练方法，达到或超越当前SOTA，展示出在不同分辨率、场景与类集合下的稳健性与实用性。

Conclusion: LHT-CLIP能在不额外训练或依赖辅助模型的情况下，通过层/头/标记级别的重构恢复CLIP的像素级视觉判别能力，大幅提升零样本和弱监督语义分割表现，并在8个常用基准上达到或超过现有方法。

Abstract: Extending CLIP models to semantic segmentation remains challenging due to the
misalignment between their image-level pre-training objectives and the
pixel-level visual understanding required for dense prediction. While prior
efforts have achieved encouraging results by reorganizing the final layer and
features, they often inherit the global alignment bias of preceding layers,
leading to suboptimal segmentation performance. In this work, we propose
LHT-CLIP, a novel training-free framework that systematically exploits the
visual discriminability of CLIP across layer, head, and token levels. Through
comprehensive analysis, we reveal three key insights: (i) the final layers
primarily strengthen image-text alignment with sacrifice of visual
discriminability (e.g., last 3 layers in ViT-B/16 and 8 layers in ViT-L/14),
partly due to the emergence of anomalous tokens; (ii) a subset of attention
heads (e.g., 10 out of 144 in ViT-B/16) display consistently strong visual
discriminability across datasets; (iii) abnormal tokens display sparse and
consistent activation pattern compared to normal tokens. Based on these
findings, we propose three complementary techniques: semantic-spatial
reweighting, selective head enhancement, and abnormal token replacement to
effectively restore visual discriminability and improve segmentation
performance without any additional training, auxiliary pre-trained networks, or
extensive hyperparameter tuning. Extensive experiments on 8 common semantic
segmentation benchmarks demonstrate that LHT-CLIP achieves state-of-the-art
performance across diverse scenarios, highlighting its effectiveness and
practicality for real-world deployment.

</details>


### [7] [DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning](https://arxiv.org/abs/2510.23907)
*Eddison Pham,Prisha Priyadarshini,Adrian Maliackel,Kanishk Bandi,Cristian Meo,Kevin Zhu*

Main category: cs.CV

TL;DR: DynaStride automatically creates temporally coherent, informative scene-level captions for instructional videos using adaptive sampling and multimodal reasoning, outperforming strong baselines on standard metrics.


<details>
  <summary>Details</summary>
Motivation: Scene-level captioning in instructional videos needs understanding of visual cues and temporal structure to support procedural learning; current captions often lack coherence and quality, harming educational value.

Method: Adaptive frame sampling, multimodal windowing, multimodal chain-of-thought to produce multiple action-object pairs, refinement and fusion via dynamic stride window selection algorithm balancing temporal context and redundancy; uses YouCookII scene annotations for evaluation.

Result: DynaStride pipeline generates coherent, scene-level captions without manual scene segmentation by using adaptive frame sampling, multimodal windowing, multimodal chain-of-thought to produce action-object pairs, and dynamic stride window selection to fuse them; improves metrics over VLLaMA3 and GPT-4o.

Conclusion: DynaStride yields more temporally coherent and informative instructional captions by integrating visual semantics and temporal reasoning via adaptive sampling and dynamic stride fusion.

Abstract: Scene-level captioning in instructional videos can enhance learning by
requiring an understanding of both visual cues and temporal structure. By
aligning visual cues with textual guidance, this understanding supports
procedural learning and multimodal reasoning, providing a richer context for
skill acquisition. However, captions that fail to capture this structure may
lack coherence and quality, which can create confusion and undermine the
video's educational intent. To address this gap, we introduce DynaStride, a
pipeline to generate coherent, scene-level captions without requiring manual
scene segmentation. Using the YouCookII dataset's scene annotations, DynaStride
performs adaptive frame sampling and multimodal windowing to capture key
transitions within each scene. It then employs a multimodal chain-of-thought
process to produce multiple action-object pairs, which are refined and fused
using a dynamic stride window selection algorithm that adaptively balances
temporal context and redundancy. The final scene-level caption integrates
visual semantics and temporal reasoning in a single instructional caption.
Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,
demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) and
semantic similarity measures (BERTScore, CLIPScore). Qualitative analyses
further show that DynaStride produces captions that are more temporally
coherent and informative, suggesting a promising direction for improving
AI-powered instructional content generation.

</details>


### [8] [TurboPortrait3D: Single-step diffusion-based fast portrait novel-view synthesis](https://arxiv.org/abs/2510.23929)
*Emily Kim,Julieta Martinez,Timur Bagautdinov,Jessica Hodgins*

Main category: cs.CV

TL;DR: TurboPortrait3D：基于单步图像扩散对图像到3D头像结果进行多视角一致性细化，采用合成预训练+真实微调，实现在单张正面图输入下的高质量、低延迟人像新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有图像到3D模型易出现伪影、细节不足且难以保持人物身份；图像扩散模型图像质量高但计算开销大且缺乏3D一致性。结合两者以提升质量、保留3D感并保持低延迟。

Method: 先使用前馈的图像到头像方法从单个正面图像生成初始3D表示及对应噪渲染，然后将这些有噪渲染输入一个单步扩散模型（条件于输入图像），该模型在多视图一致性约束下训练以生成精细化渲染。训练策略包括先在大量合成多视图数据上预训练，再在高质量真实图像上微调。

Result: 在定性和定量评估上均优于当前人像新视角合成最先进方法，同时保持时间效率低延迟。

Conclusion: 该论文提出TurboPortrait3D，通过在图像到3D头像流水线后引入单步图像扩散模型对噪渲染进行多视角一致性细化，实现低延迟、高质量的人像新视角合成。

Abstract: We introduce TurboPortrait3D: a method for low-latency novel-view synthesis
of human portraits. Our approach builds on the observation that existing
image-to-3D models for portrait generation, while capable of producing
renderable 3D representations, are prone to visual artifacts, often lack of
detail, and tend to fail at fully preserving the identity of the subject. On
the other hand, image diffusion models excel at generating high-quality images,
but besides being computationally expensive, are not grounded in 3D and thus
are not directly capable of producing multi-view consistent outputs. In this
work, we demonstrate that image-space diffusion models can be used to
significantly enhance the quality of existing image-to-avatar methods, while
maintaining 3D-awareness and running with low-latency. Our method takes a
single frontal image of a subject as input, and applies a feedforward
image-to-avatar generation pipeline to obtain an initial 3D representation and
corresponding noisy renders. These noisy renders are then fed to a single-step
diffusion model which is conditioned on input image(s), and is specifically
trained to refine the renders in a multi-view consistent way. Moreover, we
introduce a novel effective training strategy that includes pre-training on a
large corpus of synthetic multi-view data, followed by fine-tuning on
high-quality real images. We demonstrate that our approach both qualitatively
and quantitatively outperforms current state-of-the-art for portrait novel-view
synthesis, while being efficient in time.

</details>


### [9] [PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors](https://arxiv.org/abs/2510.23930)
*Xirui Jin,Renbiao Jin,Boying Li,Danping Zou,Wenxian Yu*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Three-dimensional Gaussian Splatting (3DGS) has recently emerged as an
efficient representation for novel-view synthesis, achieving impressive visual
quality. However, in scenes dominated by large and low-texture regions, common
in indoor environments, the photometric loss used to optimize 3DGS yields
ambiguous geometry and fails to recover high-fidelity 3D surfaces. To overcome
this limitation, we introduce PlanarGS, a 3DGS-based framework tailored for
indoor scene reconstruction. Specifically, we design a pipeline for
Language-Prompted Planar Priors (LP3) that employs a pretrained vision-language
segmentation model and refines its region proposals via cross-view fusion and
inspection with geometric priors. 3D Gaussians in our framework are optimized
with two additional terms: a planar prior supervision term that enforces planar
consistency, and a geometric prior supervision term that steers the Gaussians
toward the depth and normal cues. We have conducted extensive experiments on
standard indoor benchmarks. The results show that PlanarGS reconstructs
accurate and detailed 3D surfaces, consistently outperforming state-of-the-art
methods by a large margin. Project page: https://planargs.github.io

</details>


### [10] [Adaptive Training of INRs via Pruning and Densification](https://arxiv.org/abs/2510.23943)
*Diana Aldana,João Paulo Lima,Daniel Csillag,Daniel Perazzo,Haoan Feng,Luiz Velho,Tiago Novello*

Main category: cs.CV

TL;DR: AIRe通过循环执行神经元剪枝（信息迁移+结构化剪枝）和输入频率密集化，实现更小且性能更好的INR模型，用于图像与SDF重建。


<details>
  <summary>Details</summary>
Motivation: 动机是解决为隐式神经表示选择合适的输入频率和架构时的挑战，避免依赖启发式和大量超参数优化，同时控制参数冗余以提升高频细节建模能力。

Method: 方法包括两个交替阶段：1) 剪枝阶段：识别对输出贡献小的神经元，施加有针对性的权重衰减以将其信息迁移到剩余神经元，然后进行结构化剪枝；2) 密集化阶段：在信号欠拟合的频谱区域增加输入的正弦编码频率，从而扩展表示基函数。

Result: 在图像和隐式表面（SDFs）实验中，AIRe在减小模型参数量的同时保持或提升了重建质量，证明了剪枝与频率密集化的互补性。

Conclusion: AIRe通过在训练过程中自适应地裁剪神经元和在输入频谱上密集化编码频率，实现了在保持或提升重建质量的同时减少模型规模，从而改善网络大小与重建质量之间的权衡。

Abstract: Encoding input coordinates with sinusoidal functions into multilayer
perceptrons (MLPs) has proven effective for implicit neural representations
(INRs) of low-dimensional signals, enabling the modeling of high-frequency
details. However, selecting appropriate input frequencies and architectures
while managing parameter redundancy remains an open challenge, often addressed
through heuristics and heavy hyperparameter optimization schemes. In this
paper, we introduce AIRe ($\textbf{A}$daptive $\textbf{I}$mplicit neural
$\textbf{Re}$presentation), an adaptive training scheme that refines the INR
architecture over the course of optimization. Our method uses a neuron pruning
mechanism to avoid redundancy and input frequency densification to improve
representation capacity, leading to an improved trade-off between network size
and reconstruction quality. For pruning, we first identify less-contributory
neurons and apply a targeted weight decay to transfer their information to the
remaining neurons, followed by structured pruning. Next, the densification
stage adds input frequencies to spectrum regions where the signal underfits,
expanding the representational basis. Through experiments on images and SDFs,
we show that AIRe reduces model size while preserving, or even improving,
reconstruction quality. Code and pretrained models will be released for public
use.

</details>


### [11] [Neural USD: An object-centric framework for iterative editing and control](https://arxiv.org/abs/2510.23956)
*Alejandro Escontrela,Shrinu Kushagra,Sjoerd van Steenkiste,Yulia Rubanova,Aleksander Holynski,Kelsey Allen,Kevin Murphy,Thomas Kipf*

Main category: cs.CV

TL;DR: 本文提出Neural USD（神经通用场景描述符），将场景和对象以结构化、层次化方式表示，支持对单个对象的外观、几何与姿态进行可控、逐步编辑，并通过微调实现控制信号的相互解耦，减少全局意外变化。


<details>
  <summary>Details</summary>
Motivation: 当前可控生成模型在对图像中单个对象进行精确编辑时常导致意外的全局变化，需一种支持对象级、可迭代编辑且与模型无强依赖的表征方法。

Method: 借鉴计算机图形学中的USD标准，构建层次化场景与对象表示；引入针对控制信号的微调策略以实现外观、几何与姿态的解耦；评估多种设计选项并展示其在逐步编辑与增量工作流中的效果。

Result: 在若干设计选择的评估中，Neural USD 展示了对单个对象的精确编辑能力、控制信号的解耦效果以及支持迭代／增量编辑的实用性。更多细节见项目页。

Conclusion: Neural USD 能在生成模型中实现精确、可迭代的对象级编辑，提供结构化场景表示与解耦的控制信号，从而支持增量化工作流并减少对模型的特殊限制。

Abstract: Amazing progress has been made in controllable generative modeling,
especially over the last few years. However, some challenges remain. One of
them is precise and iterative object editing. In many of the current methods,
trying to edit the generated image (for example, changing the color of a
particular object in the scene or changing the background while keeping other
elements unchanged) by changing the conditioning signals often leads to
unintended global changes in the scene. In this work, we take the first steps
to address the above challenges. Taking inspiration from the Universal Scene
Descriptor (USD) standard developed in the computer graphics community, we
introduce the "Neural Universal Scene Descriptor" or Neural USD. In this
framework, we represent scenes and objects in a structured, hierarchical
manner. This accommodates diverse signals, minimizes model-specific
constraints, and enables per-object control over appearance, geometry, and
pose. We further apply a fine-tuning approach which ensures that the above
control signals are disentangled from one another. We evaluate several design
considerations for our framework, demonstrating how Neural USD enables
iterative and incremental workflows. More information at:
https://escontrela.me/neural_usd .

</details>


### [12] [SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability](https://arxiv.org/abs/2510.23960)
*Peiyang Xu,Minzhou Pan,Zhaorun Chen,Shuang Yang,Chaowei Xiao,Bo Li*

Main category: cs.CV

TL;DR: 提出SafeVision，一种结合类人推理的图像安全防护系统，通过数据收集/生成、策略遵循训练管道和定制损失，支持推理时动态对齐策略，无需重训练。并构建高质量数据集VisionHarm（T和C子集）。实验显示在VisionHarm上超越GPT-4o且速度更快。


<details>
  <summary>Details</summary>
Motivation: 传统图像防护模型依赖预定义类别与纯特征学习，缺乏语义推理能力且难以适应新兴威胁，需要昂贵的重训练；因此需要一个可解释、可动态更新且具类人推理能力的系统。

Method: 提出数据收集与生成框架、策略遵循训练管道、定制损失函数以及多样化问答生成与训练策略；模型在推理阶段通过动态对齐机制适配不断演进的安全策略，免去重训练。

Result: 在VisionHarm-T上比GPT-4o高出8.6%，在VisionHarm-C上高出15.5%，并且推理速度超过对手16倍以上；在多个基准上实现最先进表现。

Conclusion: SafeVision提供了可解释、可动态适配的图像安全防护解决方案，在多个基准上取得领先性能并显著提升推理速度，解决了传统图像防护模型的泛化和策略更新问题。

Abstract: With the rapid proliferation of digital media, the need for efficient and
transparent safeguards against unsafe content is more critical than ever.
Traditional image guardrail models, constrained by predefined categories, often
misclassify content due to their pure feature-based learning without semantic
reasoning. Moreover, these models struggle to adapt to emerging threats,
requiring costly retraining for new threats. To address these limitations, we
introduce SafeVision, a novel image guardrail that integrates human-like
reasoning to enhance adaptability and transparency. Our approach incorporates
an effective data collection and generation framework, a policy-following
training pipeline, and a customized loss function. We also propose a diverse QA
generation and training strategy to enhance learning effectiveness. SafeVision
dynamically aligns with evolving safety policies at inference time, eliminating
the need for retraining while ensuring precise risk assessments and
explanations. Recognizing the limitations of existing unsafe image benchmarks,
which either lack granularity or cover limited risks, we introduce VisionHarm,
a high-quality dataset comprising two subsets: VisionHarm Third-party
(VisionHarm-T) and VisionHarm Comprehensive(VisionHarm-C), spanning diverse
harmful categories. Through extensive experiments, we show that SafeVision
achieves state-of-the-art performance on different benchmarks. SafeVision
outperforms GPT-4o by 8.6% on VisionHarm-T and by 15.5% on VisionHarm-C, while
being over 16x faster. SafeVision sets a comprehensive, policy-following, and
explainable image guardrail with dynamic adaptation to emerging threats.

</details>


### [13] [Reasoning Visual Language Model for Chest X-Ray Analysis](https://arxiv.org/abs/2510.23968)
*Andriy Myronenko,Dong Yang,Baris Turkbey,Mariam Aboian,Sena Azamat,Esra Akcicek,Hongxu Yin,Pavlo Molchanov,Marc Edgar,Yufan He,Pengfei Guo,Yucheng Tang,Daguang Xu*

Main category: cs.CV

TL;DR: 提出一种将Chain-of-Thought推理引入胸片解读的VLM框架，通过SFT+RL训练使模型生成可验证、符合放射科工作流程的分步推理，提高可解释性并保持分类性能；发布模型NV-Reason-CXR-3B。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在医学影像中多为“黑盒”输出，无法满足临床对透明、可审计分步推理的需求；希望学习放射科医生的思维过程以支持质量保证和更安全的人机协作。

Method: 高保真视觉编码器 + 两阶段训练：先进行符合推理风格的监督微调（SFT），再用基于可验证异常列表的强化学习（RL）优化；输出包含步骤化推理、不确定性和鉴别诊断。

Result: The paper introduces a VLM framework that generates chain-of-thought (CoT) style reasoning for chest X-ray interpretation, improving interpretability and maintaining competitive classification performance. It presents a two-stage training: reasoning-style supervised fine-tuning (SFT) and RL with verifiable rewards tied to X-ray abnormalities. The model outputs systematic radiologist-like reasoning, uncertainty, and differential diagnoses, and shows benefits in out-of-distribution tests and a radiologist reader study. NV-Reason-CXR-3B is released.

Conclusion: 方法在可解释性、审计性和临床协作上带来明显改进，同时在多标签分类任务保持竞争力，读者研究表明完整推理轨迹提高了放射科医生信心并缩短报告时间。

Abstract: Vision-language models (VLMs) have shown strong promise for medical image
analysis, but most remain opaque, offering predictions without the transparent,
stepwise reasoning clinicians rely on. We present a framework that brings
chain-of-thought (CoT) reasoning to chest X-ray interpretation. Inspired by
reasoning-first training paradigms, our approach is designed to learn how
experts reason, not just what they conclude, by aligning intermediate steps
with observable image evidence and radiology workflow. Beyond accuracy, the
explicit reasoning traces support clinical auditability: they reveal why a
conclusion was reached, which alternatives were considered, and where
uncertainty remains, enabling quality assurance, error analysis, and safer
human-AI collaboration.
  Our model couples high-fidelity visual encoding with a two-stage training
recipe: a reasoning-style supervised fine-tuning (SFT) followed by
reinforcement learning (RL) that uses verifiable rewards over a list of X-ray
abnormalities. The model outputs reasoning that mirrors radiologists systematic
thought process, uncertainty, and differential diagnosis. In
out-of-distribution evaluation, the approach achieves competitive multi-label
classification while improving interpretability. In a reader study with expert
radiologists, full reasoning traces increased confidence, supported error
auditing, and reduced time to finalize reports. We release code and the model
NV-Reason-CXR-3B to support community progress toward trustworthy, explainable
AI in chest radiography and other medical imaging tasks where reasoning quality
is as critical as prediction quality.

</details>


### [14] [Efficient Cost-and-Quality Controllable Arbitrary-scale Super-resolution with Fourier Constraints](https://arxiv.org/abs/2510.23978)
*Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 针对逐一预测傅里叶分量的低效及性能下降问题，本文通过联合预测多个傅里叶分量来提升超分辨率质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有逐一预测频域分量的方法在长序列预测时会产生误差累积且推理速度慢，限制了任意尺度超分辨率的实用性，因此需要一种既高效又能保持或提升重建质量的预测策略。

Method: 用一种能够同时输出多个傅里叶分量的网络结构替代传统的循环神经网络逐步预测机制，可能通过并行分支或多头输出实现联合预测，并在训练中引入适当的损失函数来约束频域重建质量。

Result: 提出了一种在任意放大倍数超分辨率任务中同时控制成本和质量（CQ）的改进方法。该方法针对现有基于循环神经网络逐一预测傅里叶分量导致的性能下降和效率低下的问题，提出了多分量联合预测策略，从而在提高重建质量的同时提升计算效率。

Conclusion: 联合预测多个傅里叶分量能够缓解独立预测带来的误差累积和推理瓶颈，从而在任意尺度超分辨率中实现更好的质量-成本权衡。

Abstract: Cost-and-Quality (CQ) controllability in arbitrary-scale super-resolution is
crucial. Existing methods predict Fourier components one by one using a
recurrent neural network. However, this approach leads to performance
degradation and inefficiency due to independent prediction. This paper proposes
predicting multiple components jointly to improve both quality and efficiency.

</details>


### [15] [TeleEgo: Benchmarking Egocentric AI Assistants in the Wild](https://arxiv.org/abs/2510.23981)
*Jiaqi Yan,Ruilong Ren,Jingren Liu,Shuning Xu,Ling Wang,Yiheng Wang,Yun Wang,Long Zhang,Xiangyu Chen,Changzhi Sun,Jixiang Luo,Dell Zhang,Hao Sun,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: TeleEgo 是一个面向真实日常场景的长时、流式、全模态评测基准，包含大量同步第一视角数据、人工精炼的标注、12个子任务与3,291条QA，并提出实时准确率和记忆持久时间两项指标来评估AI助手的实时性与长期记忆能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准通常单项评估能力、缺乏真实流式场景或只支持短期任务，无法充分衡量面向真实应用的自我中心（egocentric）AI助手在多模态、实时响应和长期记忆保持方面的综合能力，因此需要一个更现实、更全面的评测套件。

Method: 构建包含每位参与者超过14小时同步的第一视角视频、音频与文本数据，并通过人工精炼得到高质量视觉叙述和语音转录；在统一全局时间线上对齐数据；设计12个诊断子任务并收集3,291条人工验证问答，所有任务在严格流式设置下评估。

Result: 提供包含四大领域（工作与学习、生活与日常、社交活动、外出与文化）的大规模数据集；提出两项关键指标（实时准确率与记忆持久时间）以联合衡量正确性、时间响应性与长期记忆保持；并通过3,291条QA在流式设置下对模型进行严苛评估，促进实用助手的发展。

Conclusion: TeleEgo 提供了一个长时、流式、全模态的基准，能同时考察记忆、理解和跨记忆推理三项能力，弥补现有基准在真实流式场景与长期记忆评估上的不足。

Abstract: Egocentric AI assistants in real-world settings must process multi-modal
inputs (video, audio, text), respond in real time, and retain evolving
long-term memory. However, existing benchmarks typically evaluate these
abilities in isolation, lack realistic streaming scenarios, or support only
short-term tasks. We introduce \textbf{TeleEgo}, a long-duration, streaming,
omni-modal benchmark for evaluating egocentric AI assistants in realistic daily
contexts. The dataset features over 14 hours per participant of synchronized
egocentric video, audio, and text across four domains: work \& study, lifestyle
\& routines, social activities, and outings \& culture. All data is aligned on
a unified global timeline and includes high-quality visual narrations and
speech transcripts, curated through human refinement.TeleEgo defines 12
diagnostic subtasks across three core capabilities: Memory (recalling past
events), Understanding (interpreting the current moment), and Cross-Memory
Reasoning (linking distant events). It contains 3,291 human-verified QA items
spanning multiple question formats (single-choice, binary, multi-choice, and
open-ended), evaluated strictly in a streaming setting. We propose two key
metrics -- Real-Time Accuracy and Memory Persistence Time -- to jointly assess
correctness, temporal responsiveness, and long-term retention. TeleEgo provides
a realistic and comprehensive evaluation to advance the development of
practical AI assistants.

</details>


### [16] [AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization](https://arxiv.org/abs/2510.24000)
*Heethanjan Kanagalingam,Thenukan Pathmanathan,Mokeeshan Vathanakumar,Tharmakulasingam Mukunthan*

Main category: cs.CV

TL;DR: 提出一种名为AdvBlur的方法，通过在训练集中加入对抗生成的模糊图像并使用双重损失函数管理域泛化，提升糖尿病视网膜病变（DR）分类模型对未知分布变化（如设备、拍摄条件和低质量图像）的鲁棒性。实验在多个数据集、不同相机类型、低质量图像和数据规模下验证了方法有效性，并通过消融研究确认模糊图像和损失函数的贡献，取得接近或超越现有域泛化DR模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有DL模型在不同采集设备、拍摄条件和人群分布下泛化能力不足，导致对未知外部数据表现下降。通过合成模糊样本并在训练过程中显式优化域不变表示，可提高模型对分布变化的鲁棒性，从而提升临床推广价值。

Method: 方法包括：1) 使用对抗策略生成模糊化的基金影像（adversarial blur）以模拟真实世界的拍摄退化；2) 将这些模糊图像扩充到训练集以覆盖更多分布；3) 设计双重损失函数（可能包含分类损失与域对抗/对比损失）以同时优化分类性能和域不变特征学习；4) 在多个公开眼底数据集上进行训练与跨数据集测试，并加入关于相机类型、低质量图像和数据规模的消融实验。

Result: 在多数据集和未见外部测试集上，AdvBlur相比基线及部分最先进的域泛化方法表现更优或相当，消融实验显示对抗模糊样本和双重损失各自对性能提升有显著贡献。

Conclusion: AdvBlur通过融入对抗模糊图像和双重损失，有效缓解了因采集设备、成像条件和人群差异导致的分布偏移问题，实验证明在未见外部数据集上的泛化性能优于或接近现有最先进的域泛化方法。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, yet
early and accurate detection can significantly improve treatment outcomes.
While numerous Deep learning (DL) models have been developed to predict DR from
fundus images, many face challenges in maintaining robustness due to
distributional variations caused by differences in acquisition devices,
demographic disparities, and imaging conditions. This paper addresses this
critical limitation by proposing a novel DR classification approach, a method
called AdvBlur. Our method integrates adversarial blurred images into the
dataset and employs a dual-loss function framework to address domain
generalization. This approach effectively mitigates the impact of unseen
distributional variations, as evidenced by comprehensive evaluations across
multiple datasets. Additionally, we conduct extensive experiments to explore
the effects of factors such as camera type, low-quality images, and dataset
size. Furthermore, we perform ablation studies on blurred images and the loss
function to ensure the validity of our choices. The experimental results
demonstrate the effectiveness of our proposed method, achieving competitive
performance compared to state-of-the-art domain generalization DR models on
unseen external datasets.

</details>


### [17] [Towards the Automatic Segmentation, Modeling and Meshing of the Aortic Vessel Tree from Multicenter Acquisitions: An Overview of the SEG.A. 2023 Segmentation of the Aorta Challenge](https://arxiv.org/abs/2510.24009)
*Yuan Jin,Antonio Pepe,Gian Marco Melito,Yuxuan Chen,Yunsu Byeon,Hyeseong Kim,Kyungwon Kim,Doohyun Park,Euijoon Choi,Dosik Hwang,Andriy Myronenko,Dong Yang,Yufan He,Daguang Xu,Ayman El-Ghotni,Mohamed Nabil,Hossam El-Kady,Ahmed Ayyad,Amr Nasr,Marek Wodzinski,Henning Müller,Hyeongyu Kim,Yejee Shin,Abbas Khan,Muhammad Asad,Alexander Zolotarev,Caroline Roney,Anthony Mathur,Martin Benning,Gregory Slabaugh,Theodoros Panagiotis Vagenas,Konstantinos Georgas,George K. Matsopoulos,Jihan Zhang,Zhen Zhang,Liqin Huang,Christian Mayer,Heinrich Mächler,Jan Egger*

Main category: cs.CV

TL;DR: 发布了大型多机构CTA AVT分割基准数据集并举办竞赛，3D U-Net主导，模型融合和定制后处理显著提升效果，建立了新的性能基线。


<details>
  <summary>Details</summary>
Motivation: 缺乏共享高质量AVT分割数据阻碍领域发展，发布大规模公开数据集并通过竞赛评价算法以推动研究与临床应用落地。

Method: 参赛方法以深度学习为主，尤其3D U-Net及其变体；关键包括数据增强、类别平衡损失、精细化的后处理（连通组件过滤、形态学操作、投影修正等）；一些团队还基于表面重建进行可选网格生成。

Result: SEG.A challenge built a large multi-institutional CTA dataset for aortic vessel tree segmentation and benchmarked algorithms; top methods were deep learning 3D U-Nets; ensembles improved performance; customized post-processing and training data crucial; provided lasting resource and benchmark.

Conclusion: 竞赛促成了算法进步，3D U-Net架构为主流，集成方法优于单模型，数据质量和后处理对性能影响大，数据集将推动未来临床可转化工具的发展。

Abstract: The automated analysis of the aortic vessel tree (AVT) from computed
tomography angiography (CTA) holds immense clinical potential, but its
development has been impeded by a lack of shared, high-quality data. We
launched the SEG.A. challenge to catalyze progress in this field by introducing
a large, publicly available, multi-institutional dataset for AVT segmentation.
The challenge benchmarked automated algorithms on a hidden test set, with
subsequent optional tasks in surface meshing for computational simulations. Our
findings reveal a clear convergence on deep learning methodologies, with 3D
U-Net architectures dominating the top submissions. A key result was that an
ensemble of the highest-ranking algorithms significantly outperformed
individual models, highlighting the benefits of model fusion. Performance was
strongly linked to algorithmic design, particularly the use of customized
post-processing steps, and the characteristics of the training data. This
initiative not only establishes a new performance benchmark but also provides a
lasting resource to drive future innovation toward robust, clinically
translatable tools.

</details>


### [18] [Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks](https://arxiv.org/abs/2510.24010)
*Mirali Purohit,Bimal Gajera,Vatsal Malaviya,Irish Mehta,Kunal Kasodekar,Jacob Adler,Steven Lu,Umaa Rebbapragada,Hannah Kerner*

Main category: cs.CV

TL;DR: Mars-Bench是第一个面向火星影像的标准化基准，包含20个数据集与多种任务，基线实验显示火星专用的基础模型可能比通用模型更优。


<details>
  <summary>Details</summary>
Motivation: 火星科学缺乏标准化基准与评测框架，限制了基础模型在火星任务上的发展；通过构建Mars-Bench来填补这一空白，促进系统性评估与模型比较。

Method: 收集并标准化来自轨道和地面影像的数据集，整理为20个任务，实施基线实验以比较在不同预训练源（自然图像、地球遥感、视觉-语言模型）上微调的模型性能，公开数据与代码。

Result: Mars-Bench提出了用于火星科学的首个标准化基准，覆盖轨道和地面影像，共包含20个数据集，任务包括分类、分割、目标检测，聚焦环形坑、圆锥、巨石和霜等地质特征。提供了标准化数据集和基线评估，比较了在自然图像、地球卫星数据和先进视觉-语言模型上预训练的模型。分析结果表明，针对火星域适配的基础模型可能优于通用域模型，推动领域适配预训练的研究。

Conclusion: 建立标准化评估框架对推动火星机器学习研究至关重要；初步结果支持开发火星域适配的基础模型，并提供了开源数据与基线代码供社区使用。

Abstract: Foundation models have enabled rapid progress across many specialized domains
by leveraging large-scale pre-training on unlabeled data, demonstrating strong
generalization to a variety of downstream tasks. While such models have gained
significant attention in fields like Earth Observation, their application to
Mars science remains limited. A key enabler of progress in other domains has
been the availability of standardized benchmarks that support systematic
evaluation. In contrast, Mars science lacks such benchmarks and standardized
evaluation frameworks, which have limited progress toward developing foundation
models for Martian tasks. To address this gap, we introduce Mars-Bench, the
first benchmark designed to systematically evaluate models across a broad range
of Mars-related tasks using both orbital and surface imagery. Mars-Bench
comprises 20 datasets spanning classification, segmentation, and object
detection, focused on key geologic features such as craters, cones, boulders,
and frost. We provide standardized, ready-to-use datasets and baseline
evaluations using models pre-trained on natural images, Earth satellite data,
and state-of-the-art vision-language models. Results from all analyses suggest
that Mars-specific foundation models may offer advantages over general-domain
counterparts, motivating further exploration of domain-adapted pre-training.
Mars-Bench aims to establish a standardized foundation for developing and
comparing machine learning models for Mars science. Our data, models, and code
are available at: https://mars-bench.github.io/.

</details>


### [19] [AutoPrompt: Automated Red-Teaming of Text-to-Image Models via LLM-Driven Adversarial Prompts](https://arxiv.org/abs/2510.24034)
*Yufan Liu,Wanqian Zhang,Huashan Chen,Lin Wang,Xiaojun Jia,Zheng Lin,Weiping Wang*

Main category: cs.CV

TL;DR: APT uses LLMs to craft readable adversarial suffixes through iterative optimization and fine-tuning, adding perplexity scoring and banned-token penalties to evade filters, yielding effective and transferable red-teaming prompts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to evaluate and exploit vulnerabilities in text-to-image models by creating adversarial prompts that can bypass safety filters while remaining human-readable, addressing limitations of white-box and semantically meaningless prior methods.

Method: Alternating optimization and LLM fine-tuning loop for suffix generation; auxiliary LLM perplexity scoring to enforce human-readability; banned-token penalties to avoid blacklist words; evaluation across models and APIs demonstrating transferability and effectiveness.

Result: Developed APT, a black-box framework using LLMs to generate adversarial suffixes via an alternating optimization-finetuning pipeline and dual-evasion strategies, achieving effective, human-readable, filter-resistant prompts with strong zero-shot transferability, exposing vulnerabilities including in commercial APIs.

Conclusion: APT provides a practical black-box technique to generate human-readable, filter-resistant adversarial prompts that reveal safety flaws in T2I models and can adapt to unseen prompts and commercial services.

Abstract: Despite rapid advancements in text-to-image (T2I) models, their safety
mechanisms are vulnerable to adversarial prompts, which maliciously generate
unsafe images. Current red-teaming methods for proactively assessing such
vulnerabilities usually require white-box access to T2I models, and rely on
inefficient per-prompt optimization, as well as inevitably generate
semantically meaningless prompts easily blocked by filters. In this paper, we
propose APT (AutoPrompT), a black-box framework that leverages large language
models (LLMs) to automatically generate human-readable adversarial suffixes for
benign prompts. We first introduce an alternating optimization-finetuning
pipeline between adversarial suffix optimization and fine-tuning the LLM
utilizing the optimized suffix. Furthermore, we integrates a dual-evasion
strategy in optimization phase, enabling the bypass of both perplexity-based
filter and blacklist word filter: (1) we constrain the LLM generating
human-readable prompts through an auxiliary LLM perplexity scoring, which
starkly contrasts with prior token-level gibberish, and (2) we also introduce
banned-token penalties to suppress the explicit generation of banned-tokens in
blacklist. Extensive experiments demonstrate the excellent red-teaming
performance of our human-readable, filter-resistant adversarial prompts, as
well as superior zero-shot transferability which enables instant adaptation to
unseen prompts and exposes critical vulnerabilities even in commercial APIs
(e.g., Leonardo.Ai.).

</details>


### [20] [ResNet: Enabling Deep Convolutional Neural Networks through Residual Learning](https://arxiv.org/abs/2510.24036)
*Xingyu Liu,Kun Ming Goh*

Main category: cs.CV

TL;DR: ResNet通过残差连接解决梯度消失，使得训练更深网络成为可能；在CIFAR-10上ResNet-18准确率89.9%，优于传统深度CNN的84.1%。


<details>
  <summary>Details</summary>
Motivation: 解决深层卷积网络训练困难（梯度消失），实现更深网络以提升视觉任务性能。

Method: 采用残差块（skip connection），在网络中加入恒等映射的快捷连接以让梯度直接传递；在CIFAR-10上实现ResNet-18并与等深传统CNN对比。

Result: ResNet在CIFAR-10上的实验结果优于传统深度CNN，提升精度并加速收敛。

Conclusion: 残差连接显著改善训练稳定性和收敛速度，使更深网络可行且性能更好。

Abstract: Convolutional Neural Networks (CNNs) has revolutionized computer vision, but
training very deep networks has been challenging due to the vanishing gradient
problem. This paper explores Residual Networks (ResNet), introduced by He et
al. (2015), which overcomes this limitation by using skip connections. ResNet
enables the training of networks with hundreds of layers by allowing gradients
to flow directly through shortcut connections that bypass intermediate layers.
In our implementation on the CIFAR-10 dataset, ResNet-18 achieves 89.9%
accuracy compared to 84.1% for a traditional deep CNN of similar depth, while
also converging faster and training more stably.

</details>


### [21] [Kernelized Sparse Fine-Tuning with Bi-level Parameter Competition for Vision Models](https://arxiv.org/abs/2510.24037)
*Shufan Shen,Junshu Sun,Shuhui Wang,Qingming Huang*

Main category: cs.CV

TL;DR: SNELLA是一种一阶段、端到端的稀疏参数高效微调方法：通过低秩可学习矩阵合并（引入非线性核）实现选择性更新，并用自适应双层稀疏分配定位重要参数，达到更高精度和显著降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏微调方法为两阶段：先用梯度定位相关参数再稀疏更新，这忽略了微调过程中参数调整同时导致优化器需保存所有权重致内存开销大；需要一种能在更新阶段同时定位和稀疏更新、并降低内存占用的方法。

Method: 提出一阶段方法SNELLA：用两个可学习的低秩矩阵合并成稀疏矩阵并加回权重实现选择性更新，扩展低秩分解引入非线性核函数以提高合并矩阵秩并防止更新间依赖；同时设计自适应双层稀疏分配机制，端到端根据重要性分数在层间层内分配稀疏预算。

Result: 在分类、分割、生成任务及不同预训练视觉模型上进行评估，SNELLA在FGVC上Top-1准确率较SPT-LoRA提升1.8%（91.9% vs 90.1%），并在86M到632M参数规模的模型上实现31.1%~39.9%的内存削减，整体达到SOTA性能与低内存占用的折中。

Conclusion: SNELLA通过将稀疏更新与低秩可学习矩阵合并并引入非线性核函数、以及自适应双层稀疏分配，在内存受限下实现端到端的一阶段参数高效微调，能更有效定位并更新任务相关参数，获得SOTA性能且显著降低内存占用。

Abstract: Parameter-efficient fine-tuning (PEFT) aims to adapt pre-trained vision
models to downstream tasks. Among PEFT paradigms, sparse tuning achieves
remarkable performance by adjusting only the weights most relevant to
downstream tasks, rather than densely tuning the entire weight matrix. Current
methods follow a two-stage paradigm. First, it locates task-relevant weights by
gradient information, which overlooks the parameter adjustments during
fine-tuning and limits the performance. Second, it updates only the located
weights by applying a sparse mask to the gradient of the weight matrix, which
results in high memory usage due to the storage of all weight matrices in the
optimizer. In this paper, we propose a one-stage method named SNELLA to
overcome the above limitations. For memory usage, SNELLA selectively updates
the weight matrix by adding it to another sparse matrix that is merged by two
low-rank learnable matrices. We extend the low-rank decomposition by
introducing nonlinear kernel functions, thereby increasing the rank of the
resulting merged matrix to prevent the interdependency among weight updates,
enabling better adaptation to downstream tasks. For locating task-relevant
weights, we propose an adaptive bi-level sparsity allocation mechanism that
encourages weights to compete across and inside layers based on their
importance scores in an end-to-end manner. Extensive experiments are conducted
on classification, segmentation, and generation tasks using different
pre-trained vision models. The results show that SNELLA achieves SOTA
performance with low memory usage. Notably, SNELLA obtains 1.8% (91.9% v.s.
90.1%) higher Top-1 accuracy on the FGVC benchmark compared to SPT-LoRA.
Compared to previous methods, SNELLA achieves a memory reduction of 31.1%-39.9%
across models with parameter scales from 86M to 632M. Our source codes are
available at https://github.com/ssfgunner/SNELL.

</details>


### [22] [Enhancing CLIP Robustness via Cross-Modality Alignment](https://arxiv.org/abs/2510.24038)
*Xingyu Zhu,Beier Zhu,Shuo Wang,Kesen Zhao,Hanwang Zhang*

Main category: cs.CV

TL;DR: COLA为无训练的最优传输框架，通过子空间投影和多视角OT对齐修复对抗下的图文特征不对齐，显著提升CLIP等VLM的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 观察到CLIP等VLM在特征空间中图像与文本特征存在较大距离，对抗扰动会放大这种不对齐，导致性能下降，因此需要在特征空间显式恢复跨模态对齐。

Method: 先将对抗图像嵌入投影到由类别文本特征张成的子空间，以滤除非语义性噪声；再将图像与文本视为多个增强视角组成的离散分布，结合子空间投影的成本函数，用最优传输（OT）细化其局部结构对齐。该方法无训练且可兼容已微调的模型。

Result: 在14个零样本分类基准上进行评估，在PGD对抗攻击下，COLA在ImageNet及其变体上平均提升约6.7%的准确率，同时在干净样本上保持高精度。

Conclusion: COLA通过子空间投影和基于最优传输的局部结构对齐，有效缓解了VLM在对抗扰动下的模态不对齐问题，从而提升在对抗攻击下的零样本分类鲁棒性。

Abstract: Vision-language models (VLMs) such as CLIP demonstrate strong generalization
in zero-shot classification but remain highly vulnerable to adversarial
perturbations. Existing methods primarily focus on adversarial fine-tuning or
prompt optimization; they often overlook the gaps in CLIP's encoded features,
which is shown as the text and image features lie far apart from each other.
This misalignment is significantly amplified under adversarial perturbations,
leading to severe degradation in classification performance. To address this
problem, we propose Cross-modality Alignment, dubbed COLA, an optimal
transport-based framework that explicitly addresses adversarial misalignment by
restoring both global image-text alignment and local structural consistency in
the feature space. (1) COLA first projects adversarial image embeddings onto a
subspace spanned by class text features, effectively filtering out non-semantic
distortions while preserving discriminative information. (2) It then models
images and texts as discrete distributions over multiple augmented views and
refines their alignment via OT, with the subspace projection seamlessly
integrated into the cost computation. This design ensures stable cross-modal
alignment even under adversarial conditions. COLA is training-free and
compatible with existing fine-tuned models. Extensive evaluations across 14
zero-shot classification benchmarks demonstrate the effectiveness of COLA,
especially with an average improvement of 6.7% on ImageNet and its variants
under PGD adversarial attacks, while maintaining high accuracy on clean
samples.

</details>


### [23] [Beyond Objects: Contextual Synthetic Data Generation for Fine-Grained Classification](https://arxiv.org/abs/2510.24078)
*William Yang,Xindi Wu,Zhiwei Deng,Esin Tureci,Olga Russakovsky*

Main category: cs.CV

TL;DR: BOB通过显式条件化与边缘化类无关属性，减轻微调引起的过拟合和多样性丧失，从而为低样本精细分类生成更有效的合成训练数据，显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 直接用少量真实样本微调T2I模型会造成过拟合与样本多样性下降，影响用于精细分类的合成训练集的有效性。BOB旨在在保持生成质量的同时避免这些负面影响。

Method: 从少量真实样本中提取场景背景、物体姿态等类无关属性；在T2I模型微调阶段将这些属性作为条件输入；在合成阶段对这些属性进行边缘化以恢复多样性，减少对类别相关特征的错误关联。

Result: 在多种T2I模型、骨干网络与数据集上的大量实验中，BOB在低样本精细分类任务上取得最先进的性能；例如在Aircraft数据集上将CLIP分类器在5张真实图+100张合成图下的准确率从50.0%提高到57.4%，在18/24个设置中优于先前方法。

Conclusion: BOB通过在微调时显式条件化类无关属性并在生成时对其边缘化，成功减轻了过拟合并保留了T2I模型的生成先验，从而在低样本精细分类任务中显著提升了合成数据增强的效果。

Abstract: Text-to-image (T2I) models are increasingly used for synthetic dataset
generation, but generating effective synthetic training data for classification
remains challenging. Fine-tuning a T2I model with a few real examples can help
improve the quality of synthetic training data; however, it may also cause
overfitting and reduce diversity in the generated samples. We propose a
fine-tuning strategy BOB (BeyondOBjects) to mitigate these concerns for
fine-grained classification. Given a small set of real examples, we first
extract class-agnostic attributes such as scene background and object pose. We
then explicitly condition on these attributes during fine-tuning of the T2I
model and marginalize them out during generation. This design mitigates
overfitting, preserves the T2I model's generative prior, reduces estimation
errors, and further minimizes unintended inter-class associations. Extensive
experiments across multiple T2I models, backbones, and datasets show that our
method achieves state-of-the-art performance in low-shot fine-grained
classification when augmented with synthetic data. Concretely, BOB outperforms
DataDream by 7.4% on the Aircraft dataset (from 50.0% to 57.4% when fine-tuning
a CLIP classifier with five real images augmented with 100 synthetic images).
In three of the four benchmarks, fine-tuning downstream models with 5 real
images augmented with BOB achieves better performance than fine-tuning with 10
real images. Collectively, BOB outperforms prior art in 18 of 24 experimental
settings, with 2+% accuracy improvements in 14 of these settings.

</details>


### [24] [OmniText: A Training-Free Generalist for Controllable Text-Image Manipulation](https://arxiv.org/abs/2510.24093)
*Agus Gunawan,Samuel Teodoro,Yun Chen,Soo Ye Kim,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: 提出OmniText——一个训练免费、基于注意力机制操作的通用文本图像编辑框架，通过自注意力逆运算去除文本、重分配交叉注意力减少幻觉，并在潜空间优化中引入注意力相关损失实现内容与风格可控；并构建OmniText-Bench用于评估，多任务上达SOTA或可比性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本图像编辑方法在文本去除、样式可控性和重复字母生成方面存在局限，限制了其在广泛TIM任务中的适用性。研究旨在设计一个无需训练的通用方法来解决这些问题，实现文本移除、样式可控的插入与编辑等多样任务。

Method: 分析并利用自注意力与交叉注意力的性质：1) 通过自注意力逆运算抑制模型对周围文本的关注以实现文本去除；2) 通过重分配交叉注意力来减少文本重复与幻觉；3) 在潜在优化框架中设计交叉注意力内容损失（提升文本内容准确性）与自注意力风格损失（实现样式控制）；此外构建了OmniText-Bench评测数据集用于多样TIM任务评估。

Result: OmniText在包含移除、重缩放、重定位、插入与编辑等多种任务的OmniText-Bench数据集上表现优异，相较于现有文本修复/插画方法取得了更好的或接近的指标与视觉效果，成为首个能覆盖多种TIM任务的通用方法。

Conclusion: OmniText提出了一种无需训练的通用文本图像操作（TIM）方法，通过利用自注意力逆运算实现文本去除、通过重分配交叉注意力降低文本幻觉，并在潜空间优化中引入交叉注意力内容损失和自注意力风格损失以实现可控文本渲染与样式定制。该方法在多个任务和指标上达到了最先进或可比拟的性能。

Abstract: Recent advancements in diffusion-based text synthesis have demonstrated
significant performance in inserting and editing text within images via
inpainting. However, despite the potential of text inpainting methods, three
key limitations hinder their applicability to broader Text Image Manipulation
(TIM) tasks: (i) the inability to remove text, (ii) the lack of control over
the style of rendered text, and (iii) a tendency to generate duplicated
letters. To address these challenges, we propose OmniText, a training-free
generalist capable of performing a wide range of TIM tasks. Specifically, we
investigate two key properties of cross- and self-attention mechanisms to
enable text removal and to provide control over both text styles and content.
Our findings reveal that text removal can be achieved by applying
self-attention inversion, which mitigates the model's tendency to focus on
surrounding text, thus reducing text hallucinations. Additionally, we
redistribute cross-attention, as increasing the probability of certain text
tokens reduces text hallucination. For controllable inpainting, we introduce
novel loss functions in a latent optimization framework: a cross-attention
content loss to improve text rendering accuracy and a self-attention style loss
to facilitate style customization. Furthermore, we present OmniText-Bench, a
benchmark dataset for evaluating diverse TIM tasks. It includes input images,
target text with masks, and style references, covering diverse applications
such as text removal, rescaling, repositioning, and insertion and editing with
various styles. Our OmniText framework is the first generalist method capable
of performing diverse TIM tasks. It achieves state-of-the-art performance
across multiple tasks and metrics compared to other text inpainting methods and
is comparable with specialist methods.

</details>


### [25] [Enhancing Pre-trained Representation Classifiability can Boost its Interpretability](https://arxiv.org/abs/2510.24105)
*Shufan Shen,Zhaobo Qi,Junshu Sun,Qingming Huang,Qi Tian,Shuhui Wang*

Main category: cs.CV

TL;DR: They introduce IIS to measure how much of a pre-trained visual representation is interpretable (vs. lost); discover interpretability and classifiability are positively correlated, enabling joint improvement via interpretability-focused fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Pre-trained visual models prioritize classifiability but emerging applications require interpretability; unclear whether high interpretability and classifiability can co-exist, so they seek to quantify interpretability and explore its relation with classifiability.

Method: They define interpretability as ratio of interpretable semantics captured by interpretations and measure information loss when mapping representations to these semantics; propose IIS metric; evaluate across representations with varying classifiability; fine-tune representations to maximize interpretability and test classification accuracy from interpretations.

Result: IIS effectively quantifies interpretability; surprising positive correlation found: higher classifiability implies more interpretable semantics. Fine-tuning to maximize interpretability improves classifiability, and classifiers built from interpretations suffer less accuracy loss after such improvements.

Conclusion: The paper proposes Inherent Interpretability Score (IIS) to quantify interpretability of pre-trained visual representations via measuring information loss when mapping to interpretable semantics, and finds a positive correlation between interpretability and classifiability, enabling joint improvements.

Abstract: The visual representation of a pre-trained model prioritizes the
classifiability on downstream tasks, while the widespread applications for
pre-trained visual models have posed new requirements for representation
interpretability. However, it remains unclear whether the pre-trained
representations can achieve high interpretability and classifiability
simultaneously. To answer this question, we quantify the representation
interpretability by leveraging its correlation with the ratio of interpretable
semantics within the representations. Given the pre-trained representations,
only the interpretable semantics can be captured by interpretations, whereas
the uninterpretable part leads to information loss. Based on this fact, we
propose the Inherent Interpretability Score (IIS) that evaluates the
information loss, measures the ratio of interpretable semantics, and quantifies
the representation interpretability. In the evaluation of the representation
interpretability with different classifiability, we surprisingly discover that
the interpretability and classifiability are positively correlated, i.e.,
representations with higher classifiability provide more interpretable
semantics that can be captured in the interpretations. This observation further
supports two benefits to the pre-trained representations. First, the
classifiability of representations can be further improved by fine-tuning with
interpretability maximization. Second, with the classifiability improvement for
the representations, we obtain predictions based on their interpretations with
less accuracy degradation. The discovered positive correlation and
corresponding applications show that practitioners can unify the improvements
in interpretability and classifiability for pre-trained vision models. Codes
are available at https://github.com/ssfgunner/IIS.

</details>


### [26] [UHKD: A Unified Framework for Heterogeneous Knowledge Distillation via Frequency-Domain Representations](https://arxiv.org/abs/2510.24116)
*Fengming Yu,Haiwei Pan,Kejia Zhang,Jian Guan,Haiying Jiang*

Main category: cs.CV

TL;DR: UHKD 利用傅里叶变换将教师中间特征转为频域表示，并通过可学习对齐模块与学生特征进行多层匹配，结合 MSE 与 KL 损失，在异构模型蒸馏中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有 KD 多针对同构模型，异构模型间因结构差异导致中间表征语义不一致，限制了中间层知识的利用；希望通过频域表示统一语义差异以便跨架构迁移。

Method: 对教师中间特征进行傅里叶变换，使用 Feature Transformation Module 提取紧凑的频域表示；学生端用可学习的 Feature Alignment Module 将其特征投影并进行多层对齐；训练目标结合中间特征的均方误差和 logits 的 KL 散度。

Result: 在 CIFAR-100 和 ImageNet-1K 上相较最新方法分别提升约 5.59% 和 0.83%，证明在异构蒸馏场景下有效。

Conclusion: UHKD 提出通过频域特征跨架构蒸馏，能够缓解异构模型间的表征差异，从而提升学生模型性能。

Abstract: Knowledge distillation (KD) is an effective model compression technique that
transfers knowledge from a high-performance teacher to a lightweight student,
reducing cost while maintaining accuracy. In visual applications, where
large-scale image models are widely used, KD enables efficient deployment.
However, architectural diversity introduces semantic discrepancies that hinder
the use of intermediate representations. Most existing KD methods are designed
for homogeneous models and degrade in heterogeneous scenarios, especially when
intermediate features are involved. Prior studies mainly focus on the logits
space, making limited use of the semantic information in intermediate layers.
To address this limitation, Unified Heterogeneous Knowledge Distillation (UHKD)
is proposed as a framework that leverages intermediate features in the
frequency domain for cross-architecture transfer. Fourier transform is applied
to capture global feature information, alleviating representational
discrepancies between heterogeneous teacher-student pairs. A Feature
Transformation Module (FTM) produces compact frequency-domain representations
of teacher features, while a learnable Feature Alignment Module (FAM) projects
student features and aligns them via multi-level matching. Training is guided
by a joint objective combining mean squared error on intermediate features with
Kullback-Leibler divergence on logits. Experiments on CIFAR-100 and ImageNet-1K
demonstrate gains of 5.59% and 0.83% over the latest method, highlighting UHKD
as an effective approach for unifying heterogeneous representations and
enabling efficient utilization of visual knowledge

</details>


### [27] [DogMo: A Large-Scale Multi-View RGB-D Dataset for 4D Canine Motion Recovery](https://arxiv.org/abs/2510.24117)
*Zan Wang,Siyu Chen,Luya Mo,Xinfeng Gao,Yuxin Shen,Lebin Ding,Wei Liang*

Main category: cs.CV

TL;DR: DogMo: 1.2k multi-view RGB-D canine motion sequences (10 dogs). Benchmarks for RGB/RGB-D and mono/multi-view. Method: three-stage instance-specific SMAL optimization (coarse alignment, dense correspondence supervision, temporal regularization) to recover body shape and pose.


<details>
  <summary>Details</summary>
Motivation: Existing dog motion datasets lack multi-view, real 3D data, scale, and breed/motion diversity. DogMo aims to provide large-scale, diverse multi-view RGB-D data and benchmarks to enable systematic evaluation and improved motion recovery methods for dogs.

Method: Three-stage optimization: 1) Coarse alignment of SMAL to sequences; 2) Dense correspondence supervision using RGB-D and multi-view cues to refine pose and shape; 3) Temporal regularization to ensure smooth, consistent motion across frames.

Result: Provides DogMo dataset and three-stage SMAL fitting pipeline for dog motion recovery; establishes four benchmark settings for evaluation across monocular/multi-view and RGB/RGB-D.

Conclusion: DogMo fills gaps in scale, diversity, and true 3D multi-view data for dog motion recovery; combined with the proposed optimization pipeline, it enables accurate instance-specific SMAL fits and standardized benchmarks to advance research.

Abstract: We present DogMo, a large-scale multi-view RGB-D video dataset capturing
diverse canine movements for the task of motion recovery from images. DogMo
comprises 1.2k motion sequences collected from 10 unique dogs, offering rich
variation in both motion and breed. It addresses key limitations of existing
dog motion datasets, including the lack of multi-view and real 3D data, as well
as limited scale and diversity. Leveraging DogMo, we establish four motion
recovery benchmark settings that support systematic evaluation across monocular
and multi-view, RGB and RGB-D inputs. To facilitate accurate motion recovery,
we further introduce a three-stage, instance-specific optimization pipeline
that fits the SMAL model to the motion sequences. Our method progressively
refines body shape and pose through coarse alignment, dense correspondence
supervision, and temporal regularization. Our dataset and method provide a
principled foundation for advancing research in dog motion recovery and open up
new directions at the intersection of computer vision, computer graphics, and
animal behavior modeling.

</details>


### [28] [ETC: training-free diffusion models acceleration with Error-aware Trend Consistency](https://arxiv.org/abs/2510.24129)
*Jiajian Xie,Hubery Yin,Chen Li,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: ETC predicts stable denoising trends from past steps and applies model-specific error tolerance thresholds to safely reuse outputs, speeding sampling with minimal consistency loss.


<details>
  <summary>Details</summary>
Motivation: Existing training-free acceleration methods for diffusion reuse model outputs but ignore denoising trends and lack error control, causing deviations and inconsistent generations; ETC addresses these gaps to enable faster, controlled sampling.

Method: 1) Consistent trend predictor: projects historical denoising patterns into future directions and distributes them across steps. 2) Model-specific error tolerance search: finds corrective thresholds separating volatile semantic planning from stable quality refinement to control reuse errors.

Result: The paper proposes a framework named Error-aware Trend Consistency (ETC) to accelerate diffusion model sampling by reusing model outputs while controlling error and preserving consistency.

Conclusion: ETC enables multi-step reuse of model outputs without significant trajectory deviation by combining trend prediction and error-tolerance thresholds, achieving substantial acceleration with little SSIM degradation.

Abstract: Diffusion models have achieved remarkable generative quality but remain
bottlenecked by costly iterative sampling. Recent training-free methods
accelerate diffusion process by reusing model outputs. However, these methods
ignore denoising trends and lack error control for model-specific tolerance,
leading to trajectory deviations under multi-step reuse and exacerbating
inconsistencies in the generated results. To address these issues, we introduce
Error-aware Trend Consistency (ETC), a framework that (1) introduces a
consistent trend predictor that leverages the smooth continuity of diffusion
trajectories, projecting historical denoising patterns into stable future
directions and progressively distributing them across multiple approximation
steps to achieve acceleration without deviating; (2) proposes a model-specific
error tolerance search mechanism that derives corrective thresholds by
identifying transition points from volatile semantic planning to stable quality
refinement. Experiments show that ETC achieves a 2.65x acceleration over FLUX
with negligible (-0.074 SSIM score) degradation of consistency.

</details>


### [29] [Compositional Image Synthesis with Inference-Time Scaling](https://arxiv.org/abs/2510.24133)
*Minsuk Ji,Sanghyeok Lee,Namhyuk Ahn*

Main category: cs.CV

TL;DR: 训练免费，LLM生成布局+对象中心VLM迭代重排行，提升布局和场景对齐。


<details>
  <summary>Details</summary>
Motivation: 现代文本-图像模型逼真但构图能力差，常错位、计数错误或属性/空间关系不一致，需要一种无需训练即可提升布局忠实度的方案。

Method: LLM从文本提示生成显式布局（对象位置/属性）；在图像生成阶段注入该布局；生成多候选图像；使用对象中心VLM对候选进行评分并重排，迭代选择最符合提示的结果（self-refinement）。

Result: Improves compositionality in text-to-image generation by using LLMs to create layouts and a VLM-based reranking self-refinement loop; training-free; preserves aesthetics.

Conclusion: 通过显式布局引导和迭代自我改进，在不额外训练的情况下显著增强文本-图像模型的构图一致性与提示对齐。

Abstract: Despite their impressive realism, modern text-to-image models still struggle
with compositionality, often failing to render accurate object counts,
attributes, and spatial relations. To address this challenge, we present a
training-free framework that combines an object-centric approach with
self-refinement to improve layout faithfulness while preserving aesthetic
quality. Specifically, we leverage large language models (LLMs) to synthesize
explicit layouts from input prompts, and we inject these layouts into the image
generation process, where a object-centric vision-language model (VLM) judge
reranks multiple candidates to select the most prompt-aligned outcome
iteratively. By unifying explicit layout-grounding with self-refine-based
inference-time scaling, our framework achieves stronger scene alignment with
prompts compared to recent text-to-image models. The code are available at
https://github.com/gcl-inha/ReFocus.

</details>


### [30] [VC4VG: Optimizing Video Captions for Text-to-Video Generation](https://arxiv.org/abs/2510.24134)
*Yang Du,Zhuoran Lin,Kaiqiang Song,Biao Wang,Zhicheng Zheng,Tiezheng Ge,Bo Zheng,Qin Jin*

Main category: cs.CV

TL;DR: 提出面向T2V的字幕优化框架VC4VG，设计多维必要性刻画并构建VC4VG-Bench指标，实验证明改进字幕能提升视频生成。


<details>
  <summary>Details</summary>
Motivation: Improve training captions for text-to-video generation by designing captions tailored for T2V needs and providing evaluation benchmarks.

Method: 从T2V角度分解字幕要素为多个维度，提出字幕设计方法并构建多维必要性分级评测基准，基于此对T2V模型进行细化训练验证相关性。

Result: VC4VG framework for caption optimization, VC4VG-Bench benchmark, experiments show better captions improve T2V fine-tuning performance; code released.

Conclusion: 针对T2V需求优化字幕质量能显著提高生成效果，VC4VG及其基准工具可推动相关研究。

Abstract: Recent advances in text-to-video (T2V) generation highlight the critical role
of high-quality video-text pairs in training models capable of producing
coherent and instruction-aligned videos. However, strategies for optimizing
video captions specifically for T2V training remain underexplored. In this
paper, we introduce VC4VG (Video Captioning for Video Generation), a
comprehensive caption optimization framework tailored to the needs of T2V
models.We begin by analyzing caption content from a T2V perspective,
decomposing the essential elements required for video reconstruction into
multiple dimensions, and proposing a principled caption design methodology. To
support evaluation, we construct VC4VG-Bench, a new benchmark featuring
fine-grained, multi-dimensional, and necessity-graded metrics aligned with
T2V-specific requirements.Extensive T2V fine-tuning experiments demonstrate a
strong correlation between improved caption quality and video generation
performance, validating the effectiveness of our approach. We release all
benchmark tools and code at https://github.com/qyr0403/VC4VG to support further
research.

</details>


### [31] [Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning](https://arxiv.org/abs/2510.24152)
*Aodi Wu,Xubo Luo*

Main category: cs.CV

TL;DR: 本文提出面向自动驾驶场景理解的系统化VLM提示工程框架，包含问题路由、任务专属提示、可视组装和任务级推理配置，在Qwen2.5-VL-72B上在干净和腐败数据上均取得约71-73%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: RoboSense Challenge要求VLM在自动驾驶场景下跨感知、预测、规划与腐败检测任务表现可靠，故需设计能处理多样问题类型、保持空间一致性并抗腐败的结构化提示与视觉预处理方案。

Method: 四个核心组件：1) Mixture-of-Prompts路由器对问题分类并分发至任务专家提示；2) 任务专属提示嵌入坐标系、空间推理规则、角色扮演、CoT/ToT与少样本示例；3) 可视组装模块根据问题需求合成多视图图像、目标裁剪、标记与自适应历史帧；4) 为每个任务调优推理参数（温度、top-p、消息角色）。在Qwen2.5-VL-72B上实现并评估。

Result: 在Qwen2.5-VL-72B上，Phase-1（干净数据）平均准确率70.87%，Phase-2（腐败数据）平均准确率72.85%，证明方法在噪声/腐败条件下仍稳健；代码与提示已开源。

Conclusion: 结构化提示与空间制图显著提升了VLM在感知、预测、规划与腐败检测等自动驾驶任务中的表现；提出的方法在两个阶段均表现稳健，证明了提示路由、任务专属提示和视觉组装的重要性。

Abstract: This technical report presents our solution for the RoboSense Challenge at
IROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving
scene understanding across perception, prediction, planning, and corruption
detection tasks. We propose a systematic framework built on four core
components. First, a Mixture-of-Prompts router classifies questions and
dispatches them to task-specific expert prompts, eliminating interference
across diverse question types. Second, task-specific prompts embed explicit
coordinate systems, spatial reasoning rules, role-playing,
Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to
each task. Third, a visual assembly module composes multi-view images with
object crops, magenta markers, and adaptive historical frames based on question
requirements. Fourth, we configure model inference parameters (temperature,
top-p, message roles) per task to optimize output quality. Implemented on
Qwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean
data) and 72.85% on Phase-2 (corrupted data), demonstrating that structured
prompting and spatial grounding substantially enhance VLM performance on
safety-critical autonomous driving tasks. Code and prompt are available at
https://github.com/wuaodi/UCAS-CSU-phase2.

</details>


### [32] [Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2](https://arxiv.org/abs/2510.24195)
*Ziqi Zhou,Yifan Hu,Yufei Song,Zijing Li,Shengshan Hu,Leo Yu Zhang,Dezhong Yao,Long Zheng,Hai Jin*

Main category: cs.CV

TL;DR: 本文研究了图像分割基础模型SAM2对抗攻击的脆弱性，提出了首个面向SAM2的跨提示（cross-prompt）通用对抗扰动UAP-SAM2。方法通过目标扫描策略减少提示依赖，并构建双重语义偏离目标函数：在当前帧扰乱语义并破坏相邻帧之间的语义一致性，从而提升跨提示和跨帧的攻击转移性。大量实验证明在六个数据集和两类分割任务上显著优于现有SOTA攻击。


<details>
  <summary>Details</summary>
Motivation: 尽管SAM2在视频分割上泛化能力强，但其鲁棒性未知，且现有针对SAM的攻击能否迁移到SAM2存在性能差距，主要受提示方向性和帧间语义纠缠影响，因此需要专门设计能处理提示与时序语义的攻击方法。

Method: 提出目标扫描策略将帧划分为k个区域并随机分配提示以降低提示依赖；构建双重语义偏离（dual semantic deviation）损失，包括在当前帧上扭曲语义和破坏相邻帧间的语义一致性；基于此优化通用对抗扰动（UAP）。

Result: 在六个数据集和两种分割任务上进行的大量实验表明，UAP-SAM2在成功率和破坏效果上大幅优于现有SOTA攻击，验证了方法的有效性。

Conclusion: UAP-SAM2能有效攻击SAM2，在跨提示和跨帧条件下表现出更强的转移性和破坏性，明显超过现有SOTA方法，表明SAM2也存在显著的对抗脆弱性。

Abstract: Recent studies reveal the vulnerability of the image segmentation foundation
model SAM to adversarial examples. Its successor, SAM2, has attracted
significant attention due to its strong generalization capability in video
segmentation. However, its robustness remains unexplored, and it is unclear
whether existing attacks on SAM can be directly transferred to SAM2. In this
paper, we first analyze the performance gap of existing attacks between SAM and
SAM2 and highlight two key challenges arising from their architectural
differences: directional guidance from the prompt and semantic entanglement
across consecutive frames. To address these issues, we propose UAP-SAM2, the
first cross-prompt universal adversarial attack against SAM2 driven by dual
semantic deviation. For cross-prompt transferability, we begin by designing a
target-scanning strategy that divides each frame into k regions, each randomly
assigned a prompt, to reduce prompt dependency during optimization. For
effectiveness, we design a dual semantic deviation framework that optimizes a
UAP by distorting the semantics within the current frame and disrupting the
semantic consistency across consecutive frames. Extensive experiments on six
datasets across two segmentation tasks demonstrate the effectiveness of the
proposed method for SAM2. The comparative results show that UAP-SAM2
significantly outperforms state-of-the-art (SOTA) attacks by a large margin.

</details>


### [33] [CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation](https://arxiv.org/abs/2510.24202)
*Anshul Kaushal,Kunal Jangid,Vinod K. Kurmi*

Main category: cs.CV

TL;DR: 提出CLFSeg：在编码器-解码器中集成模糊-卷积模块以提升边界处的不确定性处理，结合BCE+Dice损失，提升息肉与心脏分割性能并保证计算效率，优于现有SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在泛化、鲁棒性和不确定性处理上不足，尤其在边界和小目标上表现欠佳，需提高精确度与稳定性以满足临床诊断需求。

Method: 提出Fuzzy-Convolutional模块，将卷积特征与模糊逻辑运算结合以减少边界噪声与不确定性，集成于编码器-解码器网络并以BCE+Dice联合损失训练。进行了大量消融与可视化实验验证。

Result: The paper proposes CLFSeg, an encoder-decoder segmentation framework that integrates a Fuzzy-Convolutional (FC) module combining convolutional layers and fuzzy logic to enhance local/global feature extraction and reduce uncertainty at boundaries, improving robustness and efficiency. It uses a combined BCE with Dice loss to address class imbalance and small/boundary regions. The model outperforms SOTA on four datasets (three polyp datasets and ACDC cardiac dataset) and is efficient for real-world use.

Conclusion: CLFSeg通过融合模糊逻辑与卷积，改善了分割模型在不确定边界和小目标处的表现，在四个公开数据集上达到了更高的精度和鲁棒性，同时保持计算效率，具备临床应用潜力。

Abstract: Accurate polyp and cardiac segmentation for early detection and treatment is
essential for the diagnosis and treatment planning of cancer-like diseases.
Traditional convolutional neural network (CNN) based models have represented
limited generalizability, robustness, and inability to handle uncertainty,
which affects the segmentation performance. To solve these problems, this paper
introduces CLFSeg, an encoder-decoder based framework that aggregates the
Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy
logic. This module enhances the segmentation performance by identifying local
and global features while minimizing the uncertainty, noise, and ambiguity in
boundary regions, ensuring computing efficiency. In order to handle class
imbalance problem while focusing on the areas of interest with tiny and
boundary regions, binary cross-entropy (BCE) with dice loss is incorporated.
Our proposed model exhibits exceptional performance on four publicly available
datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC.
Extensive experiments and visual studies show CLFSeg surpasses the existing
SOTA performance and focuses on relevant regions of interest in anatomical
structures. The proposed CLFSeg improves performance while ensuring computing
efficiency, which makes it a potential solution for real-world medical
diagnostic scenarios. Project page is available at
https://visdomlab.github.io/CLFSeg/

</details>


### [34] [MC-SJD : Maximal Coupling Speculative Jacobi Decoding for Autoregressive Visual Generation Acceleration](https://arxiv.org/abs/2510.24211)
*Junhyuk So,Hyunho Kook,Chaeyeon Jang,Eunhyeok Park*

Main category: cs.CV

TL;DR: MC-SJD: single-line, training-free coupling change to SJD that stabilizes draft tokens and yields large, lossless parallel decoding speedups for images (~4.2x) and videos (~13.3x).


<details>
  <summary>Details</summary>
Motivation: AR visual generation is slow due to per-token sequential sampling; prior SJD accelerates but suffers low acceptance due to token instability from independent draft sampling.

Method: Speculative Jacobi Decoding (SJD) extended by Monte Carlo coupling

Result: Introduce MC-SJD, a coupling-based information-theoretic modification that maximizes probability of identical draft tokens across iterations, preserving lossless decoding and requires one-line change; achieves up to ~4.2x image and ~13.3x video speedups over AR decoding without quality loss.

Conclusion: Coupling draft token sampling across iterations greatly raises acceptance rates and parallelism of SJD, enabling substantial, quality-preserving acceleration of AR visual generation with minimal implementation changes.

Abstract: While autoregressive (AR) modeling has recently emerged as a new paradigm in
visual generation, its practical adoption is severely constrained by the slow
inference speed of per-token generation, which often requires thousands of
steps to produce a single sample. To address this challenge, we propose MC-SJD,
a training-free, lossless parallel decoding framework designed to accelerate AR
visual generation by extending the recently introduced Speculative Jacobi
Decoding (SJD). Although SJD shows strong potential for accelerating AR
generation, we demonstrate that token instability across iterations
significantly reduces the acceptance rate, a limitation that primarily arises
from the independent sampling process used during draft token generation. To
overcome this, we introduce MC-SJD, an information-theoretic approach based on
coupling, which substantially accelerates standard SJD by maximizing the
probability of sampling identical draft tokens across consecutive iterations,
all while preserving its lossless property. Remarkably, this method requires
only a single-line modification to the existing algorithm, yet achieves
substantial performance gains, delivering up to a ~4.2x acceleration in image
generation and ~13.3x acceleration in video generation compared to standard AR
decoding, without any degradation in output quality.

</details>


### [35] [Beyond Inference Intervention: Identity-Decoupled Diffusion for Face Anonymization](https://arxiv.org/abs/2510.24213)
*Haoxin Yang,Yihong Lin,Jingdan Kang,Xuemiao Xu,Yue Li,Cheng Xu,Shengfeng He*

Main category: cs.CV

TL;DR: Training-time disentangled latent diffusion for face anonymization enabling direct anonymization by sampling identity vectors, avoiding inference-time optimization and improving quality and utility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to avoid inference-time interventions for face anonymization which cause distribution shifts and entangle attributes; instead they propose a training-centric approach to disentangle identity and non-identity attributes in latent space.

Method: analysis of method

Result: ID2Face (ID^2Face) designs a conditional diffusion model with identity-masked learning, an Identity-Decoupled Latent Recomposer using an Identity VAE, bidirectional latent alignment, an Identity-Guided Latent Harmonizer with soft-gating, recomposition-based reconstruction loss, and Orthogonal Identity Mapping; at inference they sample random identity vectors for anonymization; experiments show improvements in visual quality, identity suppression, and utility preservation.

Conclusion: ID^2Face provides a structured latent space separating identity and non-identity, enabling controllable anonymization and outperforming prior methods.

Abstract: Face anonymization aims to conceal identity information while preserving
non-identity attributes. Mainstream diffusion models rely on inference-time
interventions such as negative guidance or energy-based optimization, which are
applied post-training to suppress identity features. These interventions often
introduce distribution shifts and entangle identity with non-identity
attributes, degrading visual fidelity and data utility. To address this, we
propose \textbf{ID\textsuperscript{2}Face}, a training-centric anonymization
framework that removes the need for inference-time optimization. The rationale
of our method is to learn a structured latent space where identity and
non-identity information are explicitly disentangled, enabling direct and
controllable anonymization at inference. To this end, we design a conditional
diffusion model with an identity-masked learning scheme. An Identity-Decoupled
Latent Recomposer uses an Identity Variational Autoencoder to model identity
features, while non-identity attributes are extracted from same-identity pairs
and aligned through bidirectional latent alignment. An Identity-Guided Latent
Harmonizer then fuses these representations via soft-gating conditioned on
noisy feature prediction. The model is trained with a recomposition-based
reconstruction loss to enforce disentanglement. At inference, anonymization is
achieved by sampling a random identity vector from the learned identity space.
To further suppress identity leakage, we introduce an Orthogonal Identity
Mapping strategy that enforces orthogonality between sampled and source
identity vectors. Experiments demonstrate that ID\textsuperscript{2}Face
outperforms existing methods in visual quality, identity suppression, and
utility preservation.

</details>


### [36] [SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs](https://arxiv.org/abs/2510.24214)
*Jinhong Deng,Wen Li,Joey Tianyi Zhou,Yang He*

Main category: cs.CV

TL;DR: SCOPE prunes visual tokens by combining saliency and coverage: compute coverage via token relationships, define coverage gain for unselected tokens, integrate saliency into gain to score tokens, and iteratively pick highest-scoring tokens; outperforms prior methods on LLaVA variants.


<details>
  <summary>Details</summary>
Motivation: Many visual tokens in MLLMs are redundant, and selecting only high-attention tokens leads to semantic gaps; need to consider both importance (saliency) and semantic completeness (coverage) when pruning tokens.

Method: Compute set-coverage for selected tokens using token relationships; for each unselected token compute token-coverage gain (how much new coverage it adds); integrate saliency score into coverage gain to form SCOPE score; iteratively select highest SCOPE-scored token until budget.

Result: The paper introduces SCOPE, a visual token pruning strategy for MLLMs that jointly models saliency and coverage to preserve semantic completeness, using a set-coverage measure and token-coverage gain integrated into a SCOPE score for iterative token selection.

Conclusion: SCOPE achieves better performance than previous visual token pruning methods by preserving semantic completeness through joint saliency-coverage modeling and iterative selection based on SCOPE score.

Abstract: Multimodal Large Language Models (MLLMs) typically process a large number of
visual tokens, leading to considerable computational overhead, even though many
of these tokens are redundant. Existing visual token pruning methods primarily
focus on selecting the most salient tokens based on attention scores, resulting
in the semantic incompleteness of the selected tokens. In this paper, we
propose a novel visual token pruning strategy, called
\textbf{S}aliency-\textbf{C}overage \textbf{O}riented token \textbf{P}runing
for \textbf{E}fficient MLLMs (SCOPE), to jointly model both the saliency and
coverage of the selected visual tokens to better preserve semantic
completeness. Specifically, we introduce a set-coverage for a given set of
selected tokens, computed based on the token relationships. We then define a
token-coverage gain for each unselected token, quantifying how much additional
coverage would be obtained by including it. By integrating the saliency score
into the token-coverage gain, we propose our SCOPE score and iteratively select
the token with the highest SCOPE score. We conduct extensive experiments on
multiple vision-language understanding benchmarks using the LLaVA-1.5 and
LLaVA-Next models. Experimental results demonstrate that our method
consistently outperforms prior approaches. Our code is available at
\href{https://github.com/kinredon/SCOPE}{https://github.com/kinredon/SCOPE}.

</details>


### [37] [Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation](https://arxiv.org/abs/2510.24231)
*Waseem Shariff,Timothy Hanley,Maciej Stec,Hossein Javidnia,Peter Corcoran*

Main category: cs.CV

TL;DR: 该论文构建了首个基于事件相机的微扫视数据集，通过Blender仿真眼球运动并用v2e转为事件流，覆盖0.5–2.0度、0.25–2.25ms的七类位移；用Spiking-VGG系列与新增光流增强模型Spiking-VGG16Flow在SpikingJelly上进行评估，均取得约90%准确率，证明脉冲神经网络在细微运动识别中的可行性，并将开源数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有微扫视研究多依赖高成本眼动仪或帧级分析，受采样率与时延限制。事件相机以高时间分辨率与低延迟提供捕获细微、快速眼动的新途径，有望扩大可测规模并提升对小幅运动的检测能力。

Method: 在Blender中渲染高保真眼球场景并按设定角位移生成七类微扫视序列；用v2e将帧级渲染转成事件流（保留时间动态，持续0.25–2.25ms）；基于SpikingJelly实现Spiking-VGG11/13/16，并提出包含光流模块的Spiking-VGG16Flow；训练与评估使用事件数据分类角位移。

Result: 构建并公开首个事件型微扫视数据集（七类，0.5–2.0°，0.25–2.25ms）；Spiking-VGG系列与Spiking-VGG16Flow在该数据集上平均准确率约90%，分类与事件数量或持续时长无显著相关性。

Conclusion: 基于事件相机的微扫视数据集与Spiking-VGG16Flow模型能稳定分类微小角位移（0.5–2.0°），实现约90%准确率，展示了事件感知+脉冲神经网络在精细运动识别的潜力并提供基准供后续研究。

Abstract: Microsaccades are small, involuntary eye movements vital for visual
perception and neural processing. Traditional microsaccade studies typically
use eye trackers or frame-based analysis, which, while precise, are costly and
limited in scalability and temporal resolution. Event-based sensing offers a
high-speed, low-latency alternative by capturing fine-grained spatiotemporal
changes efficiently. This work introduces a pioneering event-based microsaccade
dataset to support research on small eye movement dynamics in cognitive
computing. Using Blender, we render high-fidelity eye movement scenarios and
simulate microsaccades with angular displacements from 0.5 to 2.0 degrees,
divided into seven distinct classes. These are converted to event streams using
v2e, preserving the natural temporal dynamics of microsaccades, with durations
ranging from 0.25 ms to 2.25 ms. We evaluate the dataset using Spiking-VGG11,
Spiking-VGG13, and Spiking-VGG16, and propose Spiking-VGG16Flow, an
optical-flow-enhanced variant implemented in SpikingJelly. The models achieve
around 90 percent average accuracy, successfully classifying microsaccades by
angular displacement, independent of event count or duration. These results
demonstrate the potential of spiking neural networks for fine motion
recognition and establish a benchmark for event-based vision research. The
dataset, code, and trained models will be publicly available at
https://waseemshariff126.github.io/microsaccades/ .

</details>


### [38] [Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy](https://arxiv.org/abs/2510.24232)
*Qing Zhao,Weijian Deng,Pengxu Wei,ZiYi Dong,Hannan Lu,Xiangyang Ji,Liang Lin*

Main category: cs.CV

TL;DR: Propose LROD that enforces Lipschitz continuity alignment between restoration and detection; implement as LR-YOLO, improving robustness in adverse conditions


<details>
  <summary>Details</summary>
Motivation: Existing cascade of restoration then detection suffers from functional mismatch; restoration smooth, detector discontinuous => instability

Method: Lipschitz-regularized YOLO for adverse conditions

Result: LR-YOLO improves stability, optimization, and accuracy on haze and low-light benchmarks

Conclusion: Integrating restoration into detector with Lipschitz regularization harmonizes transformations, reduces noise amplification and training instability, yielding better detection performance

Abstract: To improve detection robustness in adverse conditions (e.g., haze and low
light), image restoration is commonly applied as a pre-processing step to
enhance image quality for the detector. However, the functional mismatch
between restoration and detection networks can introduce instability and hinder
effective integration -- an issue that remains underexplored. We revisit this
limitation through the lens of Lipschitz continuity, analyzing the functional
differences between restoration and detection networks in both the input space
and the parameter space. Our analysis shows that restoration networks perform
smooth, continuous transformations, while object detectors operate with
discontinuous decision boundaries, making them highly sensitive to minor
perturbations. This mismatch introduces instability in traditional cascade
frameworks, where even imperceptible noise from restoration is amplified during
detection, disrupting gradient flow and hindering optimization. To address
this, we propose Lipschitz-regularized object detection (LROD), a simple yet
effective framework that integrates image restoration directly into the
detector's feature learning, harmonizing the Lipschitz continuity of both tasks
during training. We implement this framework as Lipschitz-regularized YOLO
(LR-YOLO), extending seamlessly to existing YOLO detectors. Extensive
experiments on haze and low-light benchmarks demonstrate that LR-YOLO
consistently improves detection stability, optimization smoothness, and overall
accuracy.

</details>


### [39] [DeshadowMamba: Deshadowing as 1D Sequential Similarity](https://arxiv.org/abs/2510.24260)
*Zhaotong Yang,Yi Chen,Yanying Li,Shengfeng He,Yangyang Xu,Junyu Dong,Jian Yang,Yong Du*

Main category: cs.CV

TL;DR: 将序列模型Mamba改造用于图像去阴影，通过CrossGate注入阴影感知相似性，和ColorShift对比正则化抑制颜色污染，实现结构与色彩保真，性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有注意力模型固定注意模式易混入无关区域的光照信息，导致结构失真和颜色不一致，因而探索序列建模的定向状态空间以更好控制上下文传播。

Method: 用Mamba的方向性状态转移传播全局上下文；CrossGate在输入门中按方向选择性融合与阴影相关的上下文；ColorShift通过构造有信息的负样本及基于全局颜色统计的对比损失约束色彩恢复。

Result: Paper proposes DeshadowMamba: uses Mamba state space model with CrossGate and ColorShift regularization for shadow removal

Conclusion: 引入CrossGate和ColorShift后，Mamba能更好保持结构连续性与色彩一致性，提升去阴影视觉和量化指标。

Abstract: Recent deep models for image shadow removal often rely on attention-based
architectures to capture long-range dependencies. However, their fixed
attention patterns tend to mix illumination cues from irrelevant regions,
leading to distorted structures and inconsistent colors. In this work, we
revisit shadow removal from a sequence modeling perspective and explore the use
of Mamba, a selective state space model that propagates global context through
directional state transitions. These transitions yield an efficient global
receptive field while preserving positional continuity. Despite its potential,
directly applying Mamba to image data is suboptimal, since it lacks awareness
of shadow-non-shadow semantics and remains susceptible to color interference
from nearby regions. To address these limitations, we propose CrossGate, a
directional modulation mechanism that injects shadow-aware similarity into
Mamba's input gate, allowing selective integration of relevant context along
transition axes. To further ensure appearance fidelity, we introduce ColorShift
regularization, a contrastive learning objective driven by global color
statistics. By synthesizing structured informative negatives, it guides the
model to suppress color contamination and achieve robust color restoration.
Together, these components adapt sequence modeling to the structural integrity
and chromatic consistency required for shadow removal. Extensive experiments on
public benchmarks demonstrate that DeshadowMamba achieves state-of-the-art
visual quality and strong quantitative performance.

</details>


### [40] [UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation](https://arxiv.org/abs/2510.24262)
*Jiyu Guo,Shuo Yang,Yiming Huang,Yancheng Long,Xiaobo Xia,Xiu Su,Bo Zhao,Zeke Xie,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出UtilGen：通过下游任务反馈实行生成器和样本级双重优化，按样本分配权重以生成对下游任务更有用的合成数据，平均提升3.87%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的数据增强多关注视觉保真度与多样性，忽视下游任务对训练数据的特定需求；不同任务和架构对数据的需求差异大，因此需要一个以任务效用为中心的生成数据优化框架。

Method: 引入权重分配网络评估每个合成样本的下游任务效用；在模型级通过对生成模型进行任务特定微调，以及在实例级通过调整prompt嵌入和初始噪声等生成策略，进行双层迭代优化来最大化样本效用。

Result: UtilGen proposes a utility-driven data augmentation framework using generative models to produce task-specific synthetic data, guided by downstream task feedback. It introduces a weight allocation network for per-sample utility estimation and uses dual-level optimization: model-level (fine-tune generator) and instance-level (adjust prompts/noise). Experiments on 8 benchmarks show average +3.87% accuracy over SOTA, with analysis showing generated data is more task-relevant and influential.

Conclusion: 从视觉质量转向任务效用的范式能显著提升数据增强效果；UtilGen通过评估与迭代优化生成过程，产生更具任务相关性和影响力的样本，从而提高下游任务性能。

Abstract: Data augmentation using generative models has emerged as a powerful paradigm
for enhancing performance in computer vision tasks. However, most existing
augmentation approaches primarily focus on optimizing intrinsic data attributes
-- such as fidelity and diversity -- to generate visually high-quality
synthetic data, while often neglecting task-specific requirements. Yet, it is
essential for data generators to account for the needs of downstream tasks, as
training data requirements can vary significantly across different tasks and
network architectures. To address these limitations, we propose UtilGen, a
novel utility-centric data augmentation framework that adaptively optimizes the
data generation process to produce task-specific, high-utility training data
via downstream task feedback. Specifically, we first introduce a weight
allocation network to evaluate the task-specific utility of each synthetic
sample. Guided by these evaluations, UtilGen iteratively refines the data
generation process using a dual-level optimization strategy to maximize the
synthetic data utility: (1) model-level optimization tailors the generative
model to the downstream task, and (2) instance-level optimization adjusts
generation policies -- such as prompt embeddings and initial noise -- at each
generation round. Extensive experiments on eight benchmark datasets of varying
complexity and granularity demonstrate that UtilGen consistently achieves
superior performance, with an average accuracy improvement of 3.87% over
previous SOTA. Further analysis of data influence and distribution reveals that
UtilGen produces more impactful and task-relevant synthetic data, validating
the effectiveness of the paradigm shift from visual characteristics-centric to
task utility-centric data augmentation.

</details>


### [41] [Training-free Source Attribution of AI-generated Images via Resynthesis](https://arxiv.org/abs/2510.24278)
*Pietro Bongini,Valentina Molinari,Andrea Costanzo,Benedetta Tondi,Mauro Barni*

Main category: cs.CV

TL;DR: 提出无需训练的重合成一拍归属方法并构建人脸合成归属数据集；在少样本条件下优于现有方法，数据集可作为有价值基准。


<details>
  <summary>Details</summary>
Motivation: 合成图像来源归属在小样本甚至零样本条件下仍难以解决，现有少样本方法性能受限；因此提出一个无需训练的重合成策略并构建新基准数据集来推动相关研究。

Method: 首先为待测图像生成描述性提示(prompt)，然后用候选生成模型分别基于该提示重合成图像；在合适的特征空间中比较原图与各重合成图像的相似度，将图像归属到与原图最相近的模型。

Result: 在包含商用与开源文本到图像生成器人脸图像的新数据集上，重合成方法在少量样本情形下优于现有最先进的少样本方法和其他基线，显示出该数据集具有挑战性且适合作为未来方法评估基准。

Conclusion: 本文提出了一种基于重合成的无训练一拍(one-shot)合成图像来源归属方法，针对样本稀缺下的归属问题表现优越。

Abstract: Synthetic image source attribution is a challenging task, especially in data
scarcity conditions requiring few-shot or zero-shot classification
capabilities. We present a new training-free one-shot attribution method based
on image resynthesis. A prompt describing the image under analysis is
generated, then it is used to resynthesize the image with all the candidate
sources. The image is attributed to the model which produced the resynthesis
closest to the original image in a proper feature space. We also introduce a
new dataset for synthetic image attribution consisting of face images from
commercial and open-source text-to-image generators. The dataset provides a
challenging attribution framework, useful for developing new attribution models
and testing their capabilities on different generative architectures. The
dataset structure allows to test approaches based on resynthesis and to compare
them to few-shot methods. Results from state-of-the-art few-shot approaches and
other baselines show that the proposed resynthesis method outperforms existing
techniques when only a few samples are available for training or fine-tuning.
The experiments also demonstrate that the new dataset is a challenging one and
represents a valuable benchmark for developing and evaluating future few-shot
and zero-shot methods.

</details>


### [42] [ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model](https://arxiv.org/abs/2510.24285)
*Juntian Zhang,Song Jin,Chuanqi Cheng,Yuhan Liu,Yankai Lin,Xun Zhang,Yufei Zhang,Fei Jiang,Guojun Yin,Wei Lin,Rui Yan*

Main category: cs.CV

TL;DR: 提出ViPER：通过两阶段自我引导（自我批评+自我预测）和图像/实例重建的闭环训练，自生成数据提升VLM细粒度感知，Qwen-Viper在多项基准上稳定提升


<details>
  <summary>Details</summary>
Motivation: Improve fine-grained visual perception of VLMs despite scarce labeled data and limitations of SFT/RFT by structuring perception as progressive coarse-to-fine task and enabling self-generated data loop

Method: Two-stage coarse-to-fine self-bootstrapping via self-critiquing and self-prediction

Result: ViPER uses image- and instance-level reconstruction with two-stage RL to iteratively self-improve, producing Qwen-Viper models with avg +1.7% on seven benchmarks and up to +6.0% on fine-grained tasks

Conclusion: ViPER证明了生成与理解的互惠关系，为更自主强大的视觉语言模型提供可行路径

Abstract: The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

</details>


### [43] [Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning](https://arxiv.org/abs/2510.24321)
*Ivica Dimitrovski,Vlatko Spasev,Ivan Kitanovski*

Main category: cs.CV

TL;DR: Evaluate multiple prompt-learning strategies for few-shot remote sensing; prompt tuning improves accuracy and generalization over CLIP baselines, especially with semantic regularization


<details>
  <summary>Details</summary>
Motivation: CLIP-like models underperform on remote sensing due to domain gaps and lack of task-specific semantics; need lightweight adaptation for few-shot scenarios

Method: Prompt learning for few-shot remote sensing scene classification

Result: Prompt learning methods consistently outperform zero-shot hand-crafted CLIP and linear probe on frozen CLIP; Self-Regulating Constraints prompt yields best cross-domain robustness

Conclusion: Prompt learning is an efficient, scalable way to adapt vision-language models to remote sensing, reducing domain gap and enabling robust few-shot scene classification

Abstract: Remote sensing applications increasingly rely on deep learning for scene
classification. However, their performance is often constrained by the scarcity
of labeled data and the high cost of annotation across diverse geographic and
sensor domains. While recent vision-language models like CLIP have shown
promise by learning transferable representations at scale by aligning visual
and textual modalities, their direct application to remote sensing remains
suboptimal due to significant domain gaps and the need for task-specific
semantic adaptation. To address this critical challenge, we systematically
explore prompt learning as a lightweight and efficient adaptation strategy for
few-shot remote sensing image scene classification. We evaluate several
representative methods, including Context Optimization, Conditional Context
Optimization, Multi-modal Prompt Learning, and Prompting with Self-Regulating
Constraints. These approaches reflect complementary design philosophies: from
static context optimization to conditional prompts for enhanced generalization,
multi-modal prompts for joint vision-language adaptation, and semantically
regularized prompts for stable learning without forgetting. We benchmark these
prompt-learning methods against two standard baselines: zero-shot CLIP with
hand-crafted prompts and a linear probe trained on frozen CLIP features.
Through extensive experiments on multiple benchmark remote sensing datasets,
including cross-dataset generalization tests, we demonstrate that prompt
learning consistently outperforms both baselines in few-shot scenarios.
Notably, Prompting with Self-Regulating Constraints achieves the most robust
cross-domain performance. Our findings underscore prompt learning as a scalable
and efficient solution for bridging the domain gap in satellite and aerial
imagery, providing a strong foundation for future research in this field.

</details>


### [44] [Adaptive Knowledge Transferring with Switching Dual-Student Framework for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2510.24366)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Ba-Thinh Lam,Vi Vu,Bach X. Nguyen,Jianhua Xing,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: 提出切换双学生和损失感知EMA，通过选择性协作和动态教师更新提升伪标签质量，显著改进半监督3D医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统教师-学生框架在半监督医学图像分割中表现良好，但受到教师与学生之间强相关性和不可靠知识传递的限制，导致伪标签噪声和错误强化，影响学习效果。论文旨在通过架构与更新策略设计降低这种不良相关性、提高伪标签可信度并提升分割性能。

Method: 设计了一个包含两个学生网络的切换机制，在每次迭代中基于可靠性选择表现更可靠的学生与另一学生协作，避免错误信息相互放大；同时引入损失感知的EMA更新策略，根据学生的损失动态调整教师参数的更新权重，从而使教师更可靠地吸收有用信息并生成更高质量伪标签。

Result: 在多个3D医学图像分割数据集上进行广泛评估，方法优于现有最先进的半监督方法，在有限标注条件下显著提升分割准确率（文中给出具体定量对比，表明稳健的改进）。

Conclusion: 该论文提出的切换双学生（switching Dual-Student）架构和损失感知指数移动平均（Loss-Aware EMA）策略有效缓解了教师-学生框架中知识传递的相关性问题与误差强化问题，提高了伪标签质量和半监督3D医学图像分割性能。

Abstract: Teacher-student frameworks have emerged as a leading approach in
semi-supervised medical image segmentation, demonstrating strong performance
across various tasks. However, the learning effects are still limited by the
strong correlation and unreliable knowledge transfer process between teacher
and student networks. To overcome this limitation, we introduce a novel
switching Dual-Student architecture that strategically selects the most
reliable student at each iteration to enhance dual-student collaboration and
prevent error reinforcement. We also introduce a strategy of Loss-Aware
Exponential Moving Average to dynamically ensure that the teacher absorbs
meaningful information from students, improving the quality of pseudo-labels.
Our plug-and-play framework is extensively evaluated on 3D medical image
segmentation datasets, where it outperforms state-of-the-art semi-supervised
methods, demonstrating its effectiveness in improving segmentation accuracy
under limited supervision.

</details>


### [45] [Decoupling What to Count and Where to See for Referring Expression Counting](https://arxiv.org/abs/2510.24374)
*Yuda Zou,Zijian Zhang,Yongchao Xu*

Main category: cs.CV

TL;DR: W2-Net用双查询机制（w2c + w2s）和SSM匹配策略，改善了细粒度属性识别与计数，显著降低REC任务的计数误差并提升定位F1。


<details>
  <summary>Details</summary>
Motivation: 现有REC数据集的标注点通常位于类别代表性区域，导致模型更多学习到类级特征而忽略属性相关区域，影响子类级计数准确性，因此需要一种机制明确学习“在哪看”以补充“看什么”。

Method: 提出双查询架构：标准的what-to-count (w2c)查询负责目标定位；新增where-to-see (w2s)查询引导模型提取属性相关区域特征；并引入Subclass Separable Matching(SSM)在标签分配阶段加入排斥力以增强子类可分性。

Result: W2-Net提出了一种将“what to count”和“where to see”解耦的双查询框架，弥补了以往标注点偏向类别代表性区域导致属性判别弱的不足。通过引入w2s查询，模型能关注属性相关的视觉区域；并设计了Subclass Separable Matching(SSM)来增强子类间的可分性。实验在REC-8K上表现显著优于现有方法。

Conclusion: W2-Net通过显式分离目标定位与属性感知两个子任务，实现了更精确的子类计数与定位，验证了w2s查询和SSM在REC任务中的有效性。

Abstract: Referring Expression Counting (REC) extends class-level object counting to
the fine-grained subclass-level, aiming to enumerate objects matching a textual
expression that specifies both the class and distinguishing attribute. A
fundamental challenge, however, has been overlooked: annotation points are
typically placed on class-representative locations (e.g., heads), forcing
models to focus on class-level features while neglecting attribute information
from other visual regions (e.g., legs for "walking"). To address this, we
propose W2-Net, a novel framework that explicitly decouples the problem into
"what to count" and "where to see" via a dual-query mechanism. Specifically,
alongside the standard what-to-count (w2c) queries that localize the object, we
introduce dedicated where-to-see (w2s) queries. The w2s queries are guided to
seek and extract features from attribute-specific visual regions, enabling
precise subclass discrimination. Furthermore, we introduce Subclass Separable
Matching (SSM), a novel matching strategy that incorporates a repulsive force
to enhance inter-subclass separability during label assignment. W2-Net
significantly outperforms the state-of-the-art on the REC-8K dataset, reducing
counting error by 22.5% (validation) and 18.0% (test), and improving
localization F1 by 7% and 8%, respectively. Code will be available.

</details>


### [46] [Stroke Lesion Segmentation in Clinical Workflows: A Modular, Lightweight, and Deployment-Ready Tool](https://arxiv.org/abs/2510.24378)
*Yann Kerverdo,Florent Leray,Youwan Mahé,Stéphanie Leplaideur,Francesca Galassi*

Main category: cs.CV

TL;DR: StrokeSeg decouples preprocessing (Anima/BIDS), Float16 ONNX inference, and postprocessing into a lightweight, cross-platform tool matching original accuracy while halving model size


<details>
  <summary>Details</summary>
Motivation: Make high-performing stroke lesion segmentation models usable in clinic by removing heavy dependencies and monolithic design

Method: Modular deployment + quantised ONNX inference

Result: Equivalent segmentation performance to original PyTorch (Dice diff < 1e-3); model size reduced ~50% with Float16 ONNX

Conclusion: Research-grade segmentation pipelines can be converted into portable, clinician-friendly applications without loss of performance using modular design and model quantisation.

Abstract: Deep learning frameworks such as nnU-Net achieve state-of-the-art performance
in brain lesion segmentation but remain difficult to deploy clinically due to
heavy dependencies and monolithic design. We introduce \textit{StrokeSeg}, a
modular and lightweight framework that translates research-grade stroke lesion
segmentation models into deployable applications. Preprocessing, inference, and
postprocessing are decoupled: preprocessing relies on the Anima toolbox with
BIDS-compliant outputs, and inference uses ONNX Runtime with \texttt{Float16}
quantisation, reducing model size by about 50\%. \textit{StrokeSeg} provides
both graphical and command-line interfaces and is distributed as Python scripts
and as a standalone Windows executable. On a held-out set of 300 sub-acute and
chronic stroke subjects, segmentation performance was equivalent to the
original PyTorch pipeline (Dice difference $<10^{-3}$), demonstrating that
high-performing research pipelines can be transformed into portable, clinically
usable tools.

</details>


### [47] [A Luminance-Aware Multi-Scale Network for Polarization Image Fusion with a Multi-Scene Dataset](https://arxiv.org/abs/2510.24379)
*Zhuangfan Huang,Xiaosong Li,Gao Wang,Tao Ye,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: 提出MLSN，通过亮度分支的多尺度权重注入与亮度增强模块，结合全局-局部自注意力进行极化图像融合，并提供1000对MSP数据集，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Integrate complementary information from different polarized images under complex luminance by injecting luminance into features and correcting fused output.

Method: Luminance-aware multi-scale network for polarization image fusion

Result: Proposed MLSN with brightness-branch spatial weight, global-local windowed self-attention, Brightness-Enhancement module; introduced MSP dataset of 1000 image pairs; outperforms SOTA on MSP, PIF, GAND with notable metric improvements.

Conclusion: MLSN有效解决极化图像亮度差异，提升融合质量与纹理/结构恢复，且新数据集推动该领域研究。

Abstract: Polarization image fusion combines S0 and DOLP images to reveal surface
roughness and material properties through complementary texture features, which
has important applications in camouflage recognition, tissue pathology
analysis, surface defect detection and other fields. To intergrate
coL-Splementary information from different polarized images in complex
luminance environment, we propose a luminance-aware multi-scale network (MLSN).
In the encoder stage, we propose a multi-scale spatial weight matrix through a
brightness-branch , which dynamically weighted inject the luminance into the
feature maps, solving the problem of inherent contrast difference in polarized
images. The global-local feature fusion mechanism is designed at the bottleneck
layer to perform windowed self-attention computation, to balance the global
context and local details through residual linking in the feature dimension
restructuring stage. In the decoder stage, to further improve the adaptability
to complex lighting, we propose a Brightness-Enhancement module, establishing
the mapping relationship between luminance distribution and texture features,
realizing the nonlinear luminance correction of the fusion result. We also
present MSP, an 1000 pairs of polarized images that covers 17 types of indoor
and outdoor complex lighting scenes. MSP provides four-direction polarization
raw maps, solving the scarcity of high-quality datasets in polarization image
fusion. Extensive experiment on MSP, PIF and GAND datasets verify that the
proposed MLSN outperms the state-of-the-art methods in subjective and objective
evaluations, and the MS-SSIM and SD metircs are higher than the average values
of other methods by 8.57%, 60.64%, 10.26%, 63.53%, 22.21%, and 54.31%,
respectively. The source code and dataset is avalable at
https://github.com/1hzf/MLS-UNet.

</details>


### [48] [When are radiology reports useful for training medical image classifiers?](https://arxiv.org/abs/2510.24385)
*Herman Bergström,Zhongqi Yue,Fredrik D. Johansson*

Main category: cs.CV

TL;DR: 系统性评估放射科报告在预训练与微调阶段对医学影像分类的影响：当标签在文本中被充分表征时，报告有助于预训练；若关联弱，显式对齐预训练可能有害；微调使用报告在若干场景中能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像常伴随放射科报告，含丰富专家注释，但临床使用图像时报告并不总可用。研究旨在弄清何时以及如何在训练阶段利用这些“特权”文本以提升仅基于图像的分类模型性能，同时填补先前研究在任务范围和评估维度上的空白。

Method: 设计了系统化实验：在不同任务（诊断 vs 预后）、不同训练数据规模下比较多种利用报告的方法，包括仅在预训练阶段使用报告（如图像-文本对齐）、在微调阶段提供报告（如联合输入或辅助任务），并以若干基准数据集与评估指标进行定量分析。

Result: 这篇论文研究了在医学影像任务中，如何在训练阶段利用放射科报告（文本）来提升仅基于图像的分类性能。作者系统性地比较了在预训练和微调两个阶段利用报告的不同方法，并评估了诊断类与预后类任务（例如12个月再入院）以及不同训练集规模下的效果。主要发现包括：在标签与文本高度相关的任务中，利用报告进行预训练能带来收益，但在标签与文本关联较弱的任务中，基于显式图像-文本对齐的预训练反而可能有害；在一些设置中，微调阶段引入报告能带来显著提升，甚至超过预训练方法的影响。论文给出了何时以及如何利用“特权”文本数据以训练医学图像分类器的实用建议，并指出了现有研究的空白。

Conclusion: 报告可在训练不同阶段提供收益，但其效用取决于标签与文本的关联性；微调阶段使用报告在某些情况下比预训练更重要；需谨慎使用显式图像-文本对齐预训练以免在弱相关任务中损害性能。

Abstract: Medical images used to train machine learning models are often accompanied by
radiology reports containing rich expert annotations. However, relying on these
reports as inputs for clinical prediction requires the timely manual work of a
trained radiologist. This raises a natural question: when can radiology reports
be leveraged during training to improve image-only classification? Prior works
are limited to evaluating pre-trained image representations by fine-tuning them
to predict diagnostic labels, often extracted from reports, ignoring tasks with
labels that are weakly associated with the text. To address this gap, we
conduct a systematic study of how radiology reports can be used during both
pre-training and fine-tuning, across diagnostic and prognostic tasks (e.g.,
12-month readmission), and under varying training set sizes. Our findings
reveal that: (1) Leveraging reports during pre-training is beneficial for
downstream classification tasks where the label is well-represented in the
text; however, pre-training through explicit image-text alignment can be
detrimental in settings where it's not; (2) Fine-tuning with reports can lead
to significant improvements and even have a larger impact than the pre-training
method in certain settings. These results provide actionable insights into when
and how to leverage privileged text data to train medical image classifiers
while highlighting gaps in current research.

</details>


### [49] [Unsupervised Detection of Post-Stroke Brain Abnormalities](https://arxiv.org/abs/2510.24398)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

TL;DR: Flow-based unsupervised model REFLECT detects both lesions and non-lesional post-stroke abnormalities; training on healthy controls improves detection and modelling of normal variability.


<details>
  <summary>Details</summary>
Motivation: Detect post-stroke MRI abnormalities including focal lesions and secondary non-lesional changes (atrophy, ventricular enlargement) which supervised methods poorly capture.

Method: Trained REFLECT on lesion-free slices from stroke patients (ATLAS) and on healthy controls (IXI); evaluated using dual-expert central-slice annotations and FROC for anomaly maps, assessed object-level performance and Dice for lesion segmentation.

Result: REFLECT, a flow-based generative model, was evaluated for unsupervised detection; models trained on healthy controls (IXI) outperformed those trained on stroke lesion-free slices (ATLAS) in lesion segmentation (Dice 0.37 vs 0.27) and sensitivity to non-lesional abnormalities (FROC 0.62 vs 0.43).

Conclusion: Training on fully healthy anatomy yields better modelling of normal variability and more reliable detection of structural abnormalities in post-stroke MRI than training on lesion-free patient slices.

Abstract: Post-stroke MRI not only delineates focal lesions but also reveals secondary
structural changes, such as atrophy and ventricular enlargement. These
abnormalities, increasingly recognised as imaging biomarkers of recovery and
outcome, remain poorly captured by supervised segmentation methods. We evaluate
REFLECT, a flow-based generative model, for unsupervised detection of both
focal and non-lesional abnormalities in post-stroke patients. Using dual-expert
central-slice annotations on ATLAS data, performance was assessed at the object
level with Free-Response ROC analysis for anomaly maps. Two models were trained
on lesion-free slices from stroke patients (ATLAS) and on healthy controls
(IXI) to test the effect of training data. On ATLAS test subjects, the
IXI-trained model achieved higher lesion segmentation (Dice = 0.37 vs 0.27) and
improved sensitivity to non-lesional abnormalities (FROC = 0.62 vs 0.43).
Training on fully healthy anatomy improves the modelling of normal variability,
enabling broader and more reliable detection of structural abnormalities.

</details>


### [50] [GenTrack: A New Generation of Multi-Object Tracking](https://arxiv.org/abs/2510.24399)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 提出名为GenTrack的多目标跟踪方法，结合随机（PSO）与确定性策略，利用社交交互与综合观测模型来提升ID一致性和鲁棒性，提供三种变体与开源实现，并在基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决未知且随时间变化目标数、弱/噪声检测器、非线性运动及遮挡导致的ID切换和轨迹丢失问题，通过混合随机与确定性方法及社交约束提高鲁棒性和一致性。

Method: 结合粒子群优化（PSO）引导随机粒子向目标分布模态收敛，并与确定性跟踪机制混合；提出若干适应度函数、引入社交交互得分用于粒子增强与轨迹更新；构建包括位置、外观、检测置信度、轨迹惩罚和社交分数的状态-观测模型；实现三种变体：Basic、PSO、PSO-Social。

Result: 在标准MOT基准和真实场景测试中，GenTrack及其变体在ID切换、丢失率和整体性能上优于多项现有最先进跟踪器，且开源代码保证了公平对比与可复现性。

Conclusion: GenTrack通过融合PSO和社交信息，改进了弱检测与遮挡条件下的目标维护与ID一致性，实验表明在标准基准和实际场景上优于现有方法，同时提供开源实现便于复现。

Abstract: This paper introduces a novel multi-object tracking (MOT) method, dubbed
GenTrack, whose main contributions include: a hybrid tracking approach
employing both stochastic and deterministic manners to robustly handle unknown
and time-varying numbers of targets, particularly in maintaining target
identity (ID) consistency and managing nonlinear dynamics, leveraging particle
swarm optimization (PSO) with some proposed fitness measures to guide
stochastic particles toward their target distribution modes, enabling effective
tracking even with weak and noisy object detectors, integration of social
interactions among targets to enhance PSO-guided particles as well as improve
continuous updates of both strong (matched) and weak (unmatched) tracks,
thereby reducing ID switches and track loss, especially during occlusions, a
GenTrack-based redefined visual MOT baseline incorporating a comprehensive
state and observation model based on space consistency, appearance, detection
confidence, track penalties, and social scores for systematic and efficient
target updates, and the first-ever publicly available source-code reference
implementation with minimal dependencies, featuring three variants, including
GenTrack Basic, PSO, and PSO-Social, facilitating flexible reimplementation.
Experimental results have shown that GenTrack provides superior performance on
standard benchmarks and real-world scenarios compared to state-of-the-art
trackers, with integrated implementations of baselines for fair comparison.
Potential directions for future work are also discussed. The source-code
reference implementations of both the proposed method and compared-trackers are
provided on GitHub: https://github.com/SDU-VelKoTek/GenTrack

</details>


### [51] [A Hybrid Approach for Visual Multi-Object Tracking](https://arxiv.org/abs/2510.24410)
*Toan Van Nguyen,Rasmus G. K. Christiansen,Dirk Kraft,Leon Bodenhagen*

Main category: cs.CV

TL;DR: 提出一种融合随机粒子滤波+PSO和确定性关联的多目标跟踪器，通过运动、外观和交互适应粒子采样与目标保持，并引入身份保护的平滑更新与速度回归，适用于离线与实时视频；实验优于SOTA，代码开源。


<details>
  <summary>Details</summary>
Motivation: Ensure robust multi-object tracking under nonlinear dynamics, non-Gaussian noise, unknown and varying target numbers, and appearance changes/occlusions.

Method: Combine particle filter with PSO using fitness that blends motion consistency, appearance similarity, and social cues; deterministic association with cost matrix (spatial consistency, detection confidence, track penalties); smooth identity-preserving state update for occlusions and interactions; velocity regression from past states to seed particles.

Result: A hybrid tracker combining particle filter with PSO-guided particles, deterministic data association using a novel cost matrix, smooth identity-preserving state updates, and velocity regression for trend-seed velocities, achieving superior performance vs SOTA; code on GitHub.

Conclusion: Hybrid stochastic-deterministic framework improves identity consistency for challenging MOT scenarios with nonlinear dynamics and occlusions; velocity regression and PSO-enhanced particles are key, and deterministic association plus smooth update protects weak tracks.

Abstract: This paper proposes a visual multi-object tracking method that jointly
employs stochastic and deterministic mechanisms to ensure identifier
consistency for unknown and time-varying target numbers under nonlinear
dynamics. A stochastic particle filter addresses nonlinear dynamics and
non-Gaussian noise, with support from particle swarm optimization (PSO) to
guide particles toward state distribution modes and mitigate divergence through
proposed fitness measures incorporating motion consistency, appearance
similarity, and social-interaction cues with neighboring targets. Deterministic
association further enforces identifier consistency via a proposed cost matrix
incorporating spatial consistency between particles and current detections,
detection confidences, and track penalties. Subsequently, a novel scheme is
proposed for the smooth updating of target states while preserving their
identities, particularly for weak tracks during interactions with other targets
and prolonged occlusions. Moreover, velocity regression over past states
provides trend-seed velocities, enhancing particle sampling and state updates.
The proposed tracker is designed to operate flexibly for both pre-recorded
videos and camera live streams, where future frames are unavailable.
Experimental results confirm superior performance compared to state-of-the-art
trackers. The source-code reference implementations of both the proposed method
and compared-trackers are provided on GitHub:
https://github.com/SDU-VelKoTek/GenTrack2

</details>


### [52] [50 Years of Water Body Monitoring: The Case of Qaraaoun Reservoir, Lebanon](https://arxiv.org/abs/2510.24413)
*Ali Ahmad Faour,Nabil Amacha,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 本文提出一种无传感器方法，结合Sentinel-2/Landsat遥感影像、新的水体分割指数和支持向量回归（SVR），实现Qaraaoun水库表面积和蓄水量的近实时估算。水体分割与实地岸线吻合度超过95%，SVR经网格搜索优化后容量误差低于1.5%，R^2>0.98，方法具成本效益且可推广。


<details>
  <summary>Details</summary>
Motivation: 由于Qaraaoun水库传感器常故障且维护能力有限，需一种无需地面传感器、成本低且可持续的蓄水监测方法。

Method: 使用Sentinel-2和Landsat影像，提出新的水体分割指数进行水面提取；基于水面与水深-体积测深数据构建数据集，训练并通过GridSearchCV调参优化的支持向量回归模型，仅用遥感提取的水面面积预测水库蓄水量。

Result: 水体分割与岸线实测吻合度>95%；优化后的SVR在全库容误差<1.5%，R^2>0.98；方法可复制并产生50年时序数据，适用于气候与环境研究。

Conclusion: 所提方法在无地面传感器情况下，能高精度、实时估算Qaraaoun水库蓄水量，具有鲁棒性、经济性和可复制性，可用于构建长时间序列以支持气候和环境研究。

Abstract: The sustainable management of the Qaraaoun Reservoir, the largest surface
water body in Lebanon located in the Bekaa Plain, depends on reliable
monitoring of its storage volume despite frequent sensor malfunctions and
limited maintenance capacity. This study introduces a sensor-free approach that
integrates open-source satellite imagery, advanced water-extent segmentation,
and machine learning to estimate the reservoir surface area and volume in near
real time. Sentinel-2 and Landsat images are processed, where surface water is
delineated using a newly proposed water segmentation index. A machine learning
model based on Support Vector Regression (SVR) is trained on a curated dataset
that includes water surface area, water level, and water volume calculations
using a reservoir bathymetry survey. The model is then able to estimate
reservoir volume relying solely on surface area extracted from satellite
imagery, without the need for ground measurements. Water segmentation using the
proposed index aligns with ground truth for more than 95 percent of the
shoreline. Hyperparameter tuning with GridSearchCV yields an optimized SVR
performance with error under 1.5 percent of full reservoir capacity and
coefficients of determination exceeding 0.98. These results demonstrate the
robustness and cost-effectiveness of the method, offering a practical solution
for continuous, sensor-independent monitoring of reservoir storage. The
proposed methodology can be replicated for other water bodies, and the
resulting 50 years of time-series data is valuable for research on climate
change and environmental patterns.

</details>


### [53] [XAI Evaluation Framework for Semantic Segmentation](https://arxiv.org/abs/2510.24414)
*Reem Hammoud,Abdul karim Gizzini,Ali J. Ghandour*

Main category: cs.CV

TL;DR: 为语义分割设计的系统性XAI评估框架，采用像素级策略和专门度量，利用CAM类方法仿真验证，证明了评估方法的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI评估多集中于分类任务，缺少针对语义分割的系统性评估方法；语义分割在空间布局与上下文依赖上更复杂，需要像素级和上下文感知的评估手段以保证解释性方法在高风险应用中的可信度。

Method: 基于像素级评估策略，设计专门的评价指标来同时捕捉空间一致性、上下文依赖与类别相关性；将多种改编的CAM方法应用于语义分割并在仿真设置中评估其输出与真实像素标签的对齐程度，进而量化解释方法的性能。

Result: 提出了一种面向语义分割的可解释性评估框架，结合像素级评估与度量，考虑空间与上下文复杂性；使用基于CAM的XAI方法进行仿真验证，结果显示方法高效、鲁棒、可靠，有助于提升语义分割模型的透明性与可信度。

Conclusion: 所提出的评估框架能够细粒度地衡量解释性方法在语义分割任务中的表现，兼顾空间与上下文信息，实验结果支持其在效率、鲁棒性和可靠性方面的优势，有助于开发更透明与负责任的分割模型。

Abstract: Ensuring transparency and trust in artificial intelligence (AI) models is
essential, particularly as they are increasingly applied in safety-critical and
high-stakes domains. Explainable AI (XAI) has emerged as a promising approach
to address this challenge, yet the rigorous evaluation of XAI methods remains
crucial for optimizing the trade-offs between model complexity, predictive
performance, and interpretability. While extensive progress has been achieved
in evaluating XAI techniques for classification tasks, evaluation strategies
tailored to semantic segmentation remain relatively underexplored. This work
introduces a comprehensive and systematic evaluation framework specifically
designed for assessing XAI in semantic segmentation, explicitly accounting for
both spatial and contextual task complexities. The framework employs
pixel-level evaluation strategies and carefully designed metrics to provide
fine-grained interpretability insights. Simulation results using recently
adapted class activation mapping (CAM)-based XAI schemes demonstrate the
efficiency, robustness, and reliability of the proposed methodology. These
findings contribute to advancing transparent, trustworthy, and accountable
semantic segmentation models.

</details>


### [54] [Deeply-Conditioned Image Compression via Self-Generated Priors](https://arxiv.org/abs/2510.24437)
*Zhineng Zhao,Zhihai He,Zikun Zhou,Siwei Ma,Yaowei Wang*

Main category: cs.CV

TL;DR: Introduce DCIC-sgp: self-generated structural priors deeply condition compression pipeline to separate structure and texture, reducing geometric artifacts and improving RD by ~15% over VTM-12.1.


<details>
  <summary>Details</summary>
Motivation: Current learned image compression methods struggle to model complex correlations in natural images, mixing global structure and local texture in a single representation, causing geometric deformation at low bitrates.

Method: Functional decomposition: first encode a self-generated prior capturing structural backbone; use prior to modulate whole compression pipeline (especially analysis transform) so main representation focuses on residual high-entropy details; hierarchical dependency-driven design.

Result: Proposes DCIC-sgp: encode a self-generated prior representing structural backbone, use it to deeply condition the entire pipeline, especially analysis transform, enabling representation of residual details; achieves disentanglement and reduces geometric deformation; reports BD-rate reductions of ~14–15% vs VTM-12.1 on Kodak, CLIC, Tecnick.

Conclusion: Deep conditioning via self-generated priors effectively disentangles global structure from local details, mitigating deformation at low bitrates and improving rate-distortion performance significantly.

Abstract: Learned image compression (LIC) has shown great promise for achieving high
rate-distortion performance. However, current LIC methods are often limited in
their capability to model the complex correlation structures inherent in
natural images, particularly the entanglement of invariant global structures
with transient local textures within a single monolithic representation. This
limitation precipitates severe geometric deformation at low bitrates. To
address this, we introduce a framework predicated on functional decomposition,
which we term Deeply-Conditioned Image Compression via self-generated priors
(DCIC-sgp). Our central idea is to first encode a potent, self-generated prior
to encapsulate the image's structural backbone. This prior is subsequently
utilized not as mere side-information, but to holistically modulate the entire
compression pipeline. This deep conditioning, most critically of the analysis
transform, liberates it to dedicate its representational capacity to the
residual, high-entropy details. This hierarchical, dependency-driven approach
achieves an effective disentanglement of information streams. Our extensive
experiments validate this assertion; visual analysis demonstrates that our
method substantially mitigates the geometric deformation artifacts that plague
conventional codecs at low bitrates. Quantitatively, our framework establishes
highly competitive performance, achieving significant BD-rate reductions of
14.4%, 15.7%, and 15.1% against the VVC test model VTM-12.1 on the Kodak, CLIC,
and Tecnick datasets.

</details>


### [55] [Rethinking Visual Intelligence: Insights from Video Pretraining](https://arxiv.org/abs/2510.24448)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: 该论文提出将视频扩散模型（VDM）作为视觉基础模型的替代或补充，认为视频预训练为结构与动态建模提供有利归纳偏置，从而在少样本、多任务的视觉理解和推理上比仅基于语言的模型更高效。作者通过在多项任务上对比预训练的LLM与VDM（配备轻量适配器），发现VDM在数据效率和若干视觉任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模语言预训练带来强大的泛化能力，但在视觉领域的组合性理解、样本效率和通用问题解决上仍存在不足；视频预训练可能通过学习结构和动态信息提供更好的归纳偏置，弥补这些不足。

Method: 在受控评价框架下，将预训练的语言模型和视频扩散模型分别配备轻量适配器（adapters），并在其各自的自然模态上进行微调与少样本测试，评估在ARC-AGI、ConceptARC、视觉游戏、路径规划和细胞自动机等基准上的表现。

Result: 实验表明，VDM在多个基准任务上比等价的LLM展示出更高的数据效率，支持将视频预训练作为发展视觉基础模型的有前景方向。

Conclusion: 视频预训练的VDM在多种视觉推理和规划任务中显示出比语言模型更高的数据效率，说明视频数据所带来的时空归纳偏置有助于构建通用的视觉基础模型。

Abstract: Large language models (LLMs) have demonstrated that large-scale pretraining
enables systems to adapt rapidly to new problems with little supervision in the
language domain. This success, however, has not translated as effectively to
the visual domain, where models, including LLMs, continue to struggle with
compositional understanding, sample efficiency, and general-purpose
problem-solving. We investigate Video Diffusion Models (VDMs) as a promising
direction for bridging this gap. Pretraining on spatiotemporal data endows
these models with strong inductive biases for structure and dynamics, which we
hypothesize can support broad task adaptability. To test this, we design a
controlled evaluation in which both a pretrained LLM and a pretrained VDM are
equipped with lightweight adapters and presented with tasks in their natural
modalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,
route planning, and cellular automata, VDMs demonstrate higher data efficiency
than their language counterparts. Taken together, our results indicate that
video pretraining offers inductive biases that support progress toward visual
foundation models.

</details>


### [56] [A Critical Study towards the Detection of Parkinsons Disease using ML Technologies](https://arxiv.org/abs/2510.24456)
*Vivek Chetia,Abdul Taher Khan,Rahish Gogoi,David Kapsian Khual,Purnendu Bikash,Sajal Saha*

Main category: cs.CV

TL;DR: 论文用SSD和Faster R-CNN做茶叶病虫害检测，并用Mask R-CNN分割计算受损面积；Faster R-CNN效果稍好，但总体mAP较低，分割评估指标缺失。


<details>
  <summary>Details</summary>
Motivation: 自动化识别茶叶三类常见病虫害（红锈病、Helopeltis和红蜘蛛螨）并量化叶片受损面积，以辅助农业监测和防治决策。

Method: 使用两种目标检测模型（SSD MobileNet V2、Faster R-CNN ResNet50 V1）进行病虫害类别识别，并使用Mask R-CNN进行实例分割以估算叶片受损面积；对模型在IOU 0.50:0.95范围内的精度、召回率和mAP进行评估。

Result: Faster R-CNN ResNet50 V1在检测任务上表现优于SSD MobileNet V2（mAP分别约25% vs 20.9%，Precision分别0.252 vs 0.209，Recall分别0.044 vs 0.02）；同时实现了Mask R-CNN的病变区域计算方法，但未给出该部分的量化指标。

Conclusion: 该论文提出基于深度学习的茶叶病虫害检测与病变区域分割方法，评估了SSD MobileNet V2、Faster R-CNN ResNet50 V1用于目标检测，并基于Mask R-CNN实现病变面积计算。

Abstract: The proposed solution is Deep Learning Technique that will be able classify
three types of tea leaves diseases from which two diseases are caused by the
pests and one due to pathogens (infectious organisms) and environmental
conditions and also show the area damaged by a disease in leaves. Namely Red
Rust, Helopeltis and Red spider mite respectively. In this paper we have
evaluated two models namely SSD MobileNet V2 and Faster R-CNN ResNet50 V1 for
the object detection. The SSD MobileNet V2 gave precision of 0.209 for IOU
range of 0.50:0.95 with recall of 0.02 on IOU 0.50:0.95 and final mAP of 20.9%.
While Faster R-CNN ResNet50 V1 has precision of 0.252 on IOU range of 0.50:0.95
and recall of 0.044 on IOU of 0.50:0.95 with a mAP of 25%, which is better than
SSD. Also used Mask R-CNN for Object Instance Segmentation where we have
implemented our custom method to calculate the damaged diseased portion of
leaves. Keywords: Tea Leaf Disease, Deep Learning, Red Rust, Helopeltis and Red
Spider Mite, SSD MobileNet V2, Faster R-CNN ResNet50 V1 and Mask RCNN.

</details>


### [57] [Kineo: Calibration-Free Metric Motion Capture From Sparse RGB Cameras](https://arxiv.org/abs/2510.24464)
*Charles Javerliat,Pierre Raimbaud,Guillaume Lavoué*

Main category: cs.CV

TL;DR: Kineo 提出了一种全自动、免标定的多视图人体动作捕捉管线，使用无同步、无标定的消费级 RGB 摄像机输入，仅依赖 2D 关键点检测，同时联合估计相机内外参数（含畸变）、3D 关键点与稠密场景点。关键贡献包括基于置信度的时空关键点采样、图全局优化以保持固定计算成本，以及用于评估重建可靠性的配对重投影一致性得分。实验证明在 EgoHumans 和 Human3.6M 上相比先前无标定方法在相机平移、角度与 W-MPJPE 上均有大幅提升，且在实际拍摄场景中效率高、代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前免标定多视图动作捕捉方法计算代价高且精度不足，限制了在非专业与野外场景的应用。研究动机是降低使用门槛、提高精度与效率，使普通消费级摄像机也能实现可靠的多视角人体动作捕捉。

Method: 使用现成的 2D 关键点检测器提取关键点；基于置信度的时空采样策略选取用于配准的关键点；构建图形式的全局优化同时估计相机内参（含 Brown-Conrady 畸变）、相机外参与 3D 关键点与稠密场景点；引入配对重投影一致性分数评估 3D 重建可靠性；设计使计算成本固定且独立于序列长度。

Result: 在 EgoHumans 与 Human3.6M 上与先前无标定方法相比，Kineo 在相机平移误差降低约 83-85%，角度误差降低 86-92%，W-MPJPE 降低 83-91%。在实际场景中运行高效（如在某配置下用 36 分钟处理 1 小时 20 分钟视频），并开源完整管线与评测代码。

Conclusion: Kineo 实现了高精度与高效率并存的免标定多视图人体动作捕捉，显著优于现有免标定方法，适用于非专业与野外采集场景，并提供可靠性度量以支持下游任务。

Abstract: Markerless multiview motion capture is often constrained by the need for
precise camera calibration, limiting accessibility for non-experts and
in-the-wild captures. Existing calibration-free approaches mitigate this
requirement but suffer from high computational cost and reduced reconstruction
accuracy.
  We present Kineo, a fully automatic, calibration-free pipeline for markerless
motion capture from videos captured by unsynchronized, uncalibrated,
consumer-grade RGB cameras. Kineo leverages 2D keypoints from off-the-shelf
detectors to simultaneously calibrate cameras, including Brown-Conrady
distortion coefficients, and reconstruct 3D keypoints and dense scene point
maps at metric scale. A confidence-driven spatio-temporal keypoint sampling
strategy, combined with graph-based global optimization, ensures robust
calibration at a fixed computational cost independent of sequence length. We
further introduce a pairwise reprojection consensus score to quantify 3D
reconstruction reliability for downstream tasks.
  Evaluations on EgoHumans and Human3.6M demonstrate substantial improvements
over prior calibration-free methods. Compared to previous state-of-the-art
approaches, Kineo reduces camera translation error by approximately 83-85%,
camera angular error by 86-92%, and world mean-per-joint error (W-MPJPE) by
83-91%.
  Kineo is also efficient in real-world scenarios, processing multi-view
sequences faster than their duration in specific configuration (e.g., 36min to
process 1h20min of footage). The full pipeline and evaluation code are openly
released to promote reproducibility and practical adoption at
https://liris-xr.github.io/kineo/.

</details>


### [58] [Decoupled MeanFlow: Turning Flow Models into Flow Maps for Accelerated Sampling](https://arxiv.org/abs/2510.24474)
*Kyungmin Lee,Sihyun Yu,Jinwoo Shin*

Main category: cs.CV

TL;DR: 提出Decoupled MeanFlow方法，将预训练流模型转换为流图模型，无需改动架构；通过在扩散变换器最后块上条件化后续时间步，结合训练技巧，实现1–4步高质量生成。在ImageNet上1步FID≈2.16/2.12（256/512），4步FID≈1.51/1.68，推理速度提升100x以上。


<details>
  <summary>Details</summary>
Motivation: 减少扩散/流式去噪模型的离散化误差，减少去噪步数以加速采样，同时保持高质量样本并兼容预训练模型。

Method: 在扩散变换器的最后若干块上对后续时间步进行条件化（即对解码器进行时间步依赖的调制），作为一种解码策略把流模型转换为流图模型；结合若干增强训练技巧（论文未具体列出细节），以支持1–4步采样。

Result: 在ImageNet 256x256与512x512上，1步FID分别为2.16与2.12；4步FID分别为1.51与1.68，几乎匹配流模型性能且推理速度提升超过100x。

Conclusion: Decoupled MeanFlow能在不改动架构下把流模型转为流图模型，显著提升少步采样质量与速度；训练先做流模型然后转换比从头训练流图更高效更有效。

Abstract: Denoising generative models, such as diffusion and flow-based models, produce
high-quality samples but require many denoising steps due to discretization
error. Flow maps, which estimate the average velocity between timesteps,
mitigate this error and enable faster sampling. However, their training
typically demands architectural changes that limit compatibility with
pretrained flow models. We introduce Decoupled MeanFlow, a simple decoding
strategy that converts flow models into flow map models without architectural
modifications. Our method conditions the final blocks of diffusion transformers
on the subsequent timestep, allowing pretrained flow models to be directly
repurposed as flow maps. Combined with enhanced training techniques, this
design enables high-quality generation in as few as 1 to 4 steps. Notably, we
find that training flow models and subsequently converting them is more
efficient and effective than training flow maps from scratch. On ImageNet
256x256 and 512x512, our models attain 1-step FID of 2.16 and 2.12,
respectively, surpassing prior art by a large margin. Furthermore, we achieve
FID of 1.51 and 1.68 when increasing the steps to 4, which nearly matches the
performance of flow models while delivering over 100x faster inference.

</details>


### [59] [Fast and accurate neural reflectance transformation imaging through knowledge distillation](https://arxiv.org/abs/2510.24486)
*Tinsae G. Dulecha,Leonardo Righetto,Ruggero Pintus,Enrico Gobbetti,Andrea Giachetti*

Main category: cs.CV

TL;DR: Use knowledge distillation to compress NeuralRTI into a small student network that runs much faster and uses less memory while keeping visual quality nearly unchanged.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of NeuralRTI for interactive relighting while keeping quality, enabling full-resolution rendering on limited hardware.

Method: Train a compact per-pixel MLP student using distillation losses (MSE on predicted coefficients and perceptual/image-space losses) with data augmentation and careful architecture choices; compare against baseline PTM/HSH/NeuralRTI on reconstruction and relighting metrics; measure inference latency and memory.

Result: Proposed DisK-NeuralRTI: knowledge distillation to train a compact 'student' network guided by a large pre-trained 'teacher' NeuralRTI model, achieving similar perceptual quality with much lower latency and memory; evaluated on two datasets with significant speedups and comparable PSNR/SSIM.

Conclusion: DisK-NeuralRTI effectively transfers the teacher's expressive reflectance modeling to a lightweight student, enabling real-time, full-resolution RTI relighting on limited hardware with minor perceptual differences.

Abstract: Reflectance Transformation Imaging (RTI) is very popular for its ability to
visually analyze surfaces by enhancing surface details through interactive
relighting, starting from only a few tens of photographs taken with a fixed
camera and variable illumination. Traditional methods like Polynomial Texture
Maps (PTM) and Hemispherical Harmonics (HSH) are compact and fast, but struggle
to accurately capture complex reflectance fields using few per-pixel
coefficients and fixed bases, leading to artifacts, especially in highly
reflective or shadowed areas. The NeuralRTI approach, which exploits a neural
autoencoder to learn a compact function that better approximates the local
reflectance as a function of light directions, has been shown to produce
superior quality at comparable storage cost. However, as it performs
interactive relighting with custom decoder networks with many parameters, the
rendering step is computationally expensive and not feasible at full resolution
for large images on limited hardware. Earlier attempts to reduce costs by
directly training smaller networks have failed to produce valid results. For
this reason, we propose to reduce its computational cost through a novel
solution based on Knowledge Distillation (DisK-NeuralRTI). ...

</details>


### [60] [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://arxiv.org/abs/2510.24514)
*Huanyu Zhang,Wenshan Wu,Chengzu Li,Ning Shang,Yan Xia,Yangyu Huang,Yifan Zhang,Li Dong,Zhang Zhang,Liang Wang,Tieniu Tan,Furu Wei*

Main category: cs.CV

TL;DR: Introduce Latent Sketchpad: adds autoregressive visual latent generation to MLLMs via a Context-Aware Vision Head and Sketch Decoder, enabling internal visual thinking and improved or comparable planning performance on MazePlanning


<details>
  <summary>Details</summary>
Motivation: Humans use sketching as visual thinking; MLLMs lack internal visual planning and imagination, so provide an internal visual scratchpad to support generative visual thought

Method: Latent Sketchpad integrates visual generation into autoregressive reasoning by adding a Context-Aware Vision Head and a pretrained Sketch Decoder

Result: Latent Sketchpad matches or exceeds backbone MLLMs on MazePlanning; generalizes across Gemma3 and Qwen2.5-VL; allows interleaving text reasoning with visual latent generation and renders sketches for interpretability

Conclusion: Extending MLLMs with internal visual scratchpad enables richer reasoning, interpretability, and cross-model generalization

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding,
they often struggle in complex scenarios that require visual planning and
imagination. Inspired by how humans use sketching as a form of visual thinking
to develop and communicate ideas, we introduce Latent Sketchpad, a framework
that equips MLLMs with an internal visual scratchpad. The internal visual
representations of MLLMs have traditionally been confined to perceptual
understanding. We repurpose them to support generative visual thought without
compromising reasoning ability. Building on frontier MLLMs, our approach
integrates visual generation directly into their native autoregressive
reasoning process. It allows the model to interleave textual reasoning with the
generation of visual latents. These latents guide the internal thought process
and can be translated into sketch images for interpretability. To realize this,
we introduce two components: a Context-Aware Vision Head autoregressively
produces visual representations, and a pretrained Sketch Decoder renders these
into human-interpretable images. We evaluate the framework on our new dataset
MazePlanning. Experiments across various MLLMs show that Latent Sketchpad
delivers comparable or even superior reasoning performance to their backbone.
It further generalizes across distinct frontier MLLMs, including Gemma3 and
Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our
framework opens new opportunities for richer human-computer interaction and
broader applications. More details and resources are available on our project
page: https://latent-sketchpad.github.io/.

</details>


### [61] [OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents](https://arxiv.org/abs/2510.24563)
*Hongrui Jia,Jitong Liao,Xi Zhang,Haiyang Xu,Tianbao Xie,Chaoya Jiang,Ming Yan,Si Liu,Wei Ye,Fei Huang*

Main category: cs.CV

TL;DR: 提出 OSWorld-MCP：第一个公平评估多模态代理在真实环境中通过 MCP 工具调用与 GUI 操作能力的基准。构建了 158 个经人工验证的工具并发现工具一般能提升成功率，但模型的工具调用率仍低，任务仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有评估侧重 GUI 交互，忽视了借助工具（如 MCP 协议）执行复杂任务的能力；将两类代理直接比较不公平，因此需要一个公平、全面的基准来评估工具调用与 GUI 操作并重的真实使用能力。

Method: 作者开发了一个自动化代码生成流水线以创建工具，并结合精选现有工具，随后对生成工具进行严格人工验证，最终形成 158 个工具。基于真实环境的 OSWorld-MCP benchmark，对多模态代理进行了大规模评估，统计任务成功率、工具调用率等指标。

Result: OSWorld-MCP 提出并构建了一个针对多模态代理在真实计算环境中使用工具调用（通过 MCP 协议）、GUI 操作和决策能力的公平基准。作者通过自动化代码生成与人工筛选相结合，构建并验证了 158 个高质量工具，覆盖 7 类常用应用。评估显示，引入 MCP 工具普遍提升任务成功率（例如 OpenAI o3 在 15 步时从 8.3% 提升到 20.4%；Claude 4 Sonnet 在 50 步时从 40.1% 提升到 43.3%），但顶尖模型的工具调用率仍低（仅 36.3%），表明工具使用能力仍有较大改进空间。基准环境、代码和数据已公开。

Conclusion: OSWorld-MCP 有效填补了评估工具调用能力的空白，通过构建高质量工具集和真实环境测试，证明 MCP 工具能提升任务成功率，但当前模型在工具调用上的表现不足，未来研究需聚焦提高工具使用判断与调用策略。

Abstract: With advances in decision-making and reasoning capabilities, multimodal
agents show strong potential in computer application scenarios. Past
evaluations have mainly assessed GUI interaction skills, while tool invocation
abilities, such as those enabled by the Model Context Protocol (MCP), have been
largely overlooked. Comparing agents with integrated tool invocation to those
evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP,
the first comprehensive and fair benchmark for assessing computer-use agents'
tool invocation, GUI operation, and decision-making abilities in a real-world
environment. We design a novel automated code-generation pipeline to create
tools and combine them with a curated selection from existing tools. Rigorous
manual validation yields 158 high-quality tools (covering 7 common
applications), each verified for correct functionality, practical
applicability, and versatility. Extensive evaluations of state-of-the-art
multimodal agents on OSWorld-MCP show that MCP tools generally improve task
success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1%
to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of
assessing tool invocation capabilities. However, even the strongest models have
relatively low tool invocation rates, Only 36.3%, indicating room for
improvement and highlighting the benchmark's challenge. By explicitly measuring
MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents
and sets a new standard for evaluating performance in complex, tool-assisted
environments. Our code, environment, and data are publicly available at
https://osworld-mcp.github.io.

</details>


### [62] [Physics-Inspired Gaussian Kolmogorov-Arnold Networks for X-ray Scatter Correction in Cone-Beam CT](https://arxiv.org/abs/2510.24579)
*Xu Jiang,Huiying Pan,Ligen Shi,Jianing Sun,Wenfeng Xu,Xing Zhao*

Main category: cs.CV

TL;DR: 提出一种结合物理先验（投影域散射的旋转对称性）与KAN网络的深度学习散射校正方法，使用高斯RBF建模点散射并嵌入网络，实现对CBCT散射伪影的有效校正，合成与真实数据上表现优越。


<details>
  <summary>Details</summary>
Motivation: CBCT易受散射影响导致CT值偏置和组织对比度降低，影响诊断。传统散射校正方法有限，故引入考虑物理先验的深度学习方法以提高散射建模和校正精度。

Method: 基于物理先验，首先在投影域假设点散射概率密度呈旋转对称，用高斯RBF对点散射函数建模；然后将该RBF表示嵌入KAN层以提供强大的非线性高维映射能力，网络学习散射特征并预测/校正投影数据的散射成分；最后重建经校正投影以得到去散射的三维图像。

Result: 在合成数据和真实扫描数据上，所提方法能有效去除散射伪影，恢复CT值与组织对比，且在多个定量指标上（例如误差、对比度恢复或峰值信噪比等）优于现有方法。

Conclusion: 该方法通过将物理先验（散射点概率密度的旋转对称性）与深度学习相结合，利用高斯径向基函数建模点散射函数并嵌入Kolmogorov-Arnold网络（KAN）层，实现对散射分布的高维非线性映射，从而有效纠正CBCT散射伪影。综合实验（合成与真实扫描）验证了方法在定量指标和重建图像质量上的优越性。

Abstract: Cone-beam CT (CBCT) employs a flat-panel detector to achieve
three-dimensional imaging with high spatial resolution. However, CBCT is
susceptible to scatter during data acquisition, which introduces CT value bias
and reduced tissue contrast in the reconstructed images, ultimately degrading
diagnostic accuracy. To address this issue, we propose a deep learning-based
scatter artifact correction method inspired by physical prior knowledge.
Leveraging the fact that the observed point scatter probability density
distribution exhibits rotational symmetry in the projection domain. The method
uses Gaussian Radial Basis Functions (RBF) to model the point scatter function
and embeds it into the Kolmogorov-Arnold Networks (KAN) layer, which provides
efficient nonlinear mapping capabilities for learning high-dimensional scatter
features. By incorporating the physical characteristics of the scattered photon
distribution together with the complex function mapping capacity of KAN, the
model improves its ability to accurately represent scatter. The effectiveness
of the method is validated through both synthetic and real-scan experiments.
Experimental results show that the model can effectively correct the scatter
artifacts in the reconstructed images and is superior to the current methods in
terms of quantitative metrics.

</details>


### [63] [A Dual-Branch CNN for Robust Detection of AI-Generated Facial Forgeries](https://arxiv.org/abs/2510.24640)
*Xin Zhang,Yuqi Song,Fei Zuo*

Main category: cs.CV

TL;DR: 提出双分支（RGB+频域）+通道注意力的卷积网络与FSC统一损失，在DiFF数据集上实现对多类人脸伪造的强检测性能，超过人类平均水平。


<details>
  <summary>Details</summary>
Motivation: 生成式AI能产生高度逼真的人脸伪造图像，威胁安全与信任，需构建通用且稳健的检测方法，特别需同时利用空间语义与频谱高频伪迹。

Method: 设计了RGB分支提取语义信息、频域分支捕捉高频伪造痕迹，使用通道注意力模块自适应融合异构特征；提出FSC Loss（focal loss + 监督对比损失 + 频率中心间隔损失）以增强类别可分性与鲁棒性；在DiFF基准上进行评估。

Result: 在包含文本到图像、图像到图像、换脸与人脸编辑四类伪造方法的DiFF基准上，该方法在各类别均表现出强性能，优于平均人类准确率，证明方法有效。

Conclusion: 该文提出了一种双分支卷积神经网络，通过融合空间（RGB）和频域特征并引入通道注意力与统一损失函数，实现对人脸伪造的鲁棒检测。

Abstract: The rapid advancement of generative AI has enabled the creation of highly
realistic forged facial images, posing significant threats to AI security,
digital media integrity, and public trust. Face forgery techniques, ranging
from face swapping and attribute editing to powerful diffusion-based image
synthesis, are increasingly being used for malicious purposes such as
misinformation, identity fraud, and defamation. This growing challenge
underscores the urgent need for robust and generalizable face forgery detection
methods as a critical component of AI security infrastructure. In this work, we
propose a novel dual-branch convolutional neural network for face forgery
detection that leverages complementary cues from both spatial and frequency
domains. The RGB branch captures semantic information, while the frequency
branch focuses on high-frequency artifacts that are difficult for generative
models to suppress. A channel attention module is introduced to adaptively fuse
these heterogeneous features, highlighting the most informative channels for
forgery discrimination. To guide the network's learning process, we design a
unified loss function, FSC Loss, that combines focal loss, supervised
contrastive loss, and a frequency center margin loss to enhance class
separability and robustness. We evaluate our model on the DiFF benchmark, which
includes forged images generated from four representative methods:
text-to-image, image-to-image, face swap, and face edit. Our method achieves
strong performance across all categories and outperforms average human
accuracy. These results demonstrate the model's effectiveness and its potential
contribution to safeguarding AI ecosystems against visual forgery attacks.

</details>


### [64] [Eye-Tracking, Mouse Tracking, Stimulus Tracking,and Decision-Making Datasets in Digital Pathology](https://arxiv.org/abs/2510.24653)
*Veronica Thai,Rui Li,Meng Ling,Shuning Jiang,Jeremy Wolfe,Raghu Machiraju,Yan Hu,Zaibo Li,Anil Parwani,Jian Chen*

Main category: cs.CV

TL;DR: PathoGaze1.0提供大规模、生态效度高的病理诊断行为数据（眼动+交互+决策），可用于研究诊断错误、培训与AI辅助系统；数据公开可复现。


<details>
  <summary>Details</summary>
Motivation: 填补病理学领域关于诊断过程行为数据的空白，解释病理诊断中的错误与不一致性，提高培训与AI支持的可能性。

Method: 通过应用级测试平台PTAH，在接近真实临床环境下记录19名病理学家阅读397张WSI时的眼动、鼠标、视窗和决策数据；所有实验预注册并公开数据与分析代码。

Result: 构建了PathoGaze1.0数据集，包含19名病理学家解释397张WSI时的18.69小时眼动、鼠标交互、视窗导航与诊断决策数据（EMSVD）；记录了171,909次注视、263,320次扫视和1,867,362次鼠标事件；数据与代码公开并且实验预注册。

Conclusion: 该数据集为理解和改进病理诊断提供了重要资源，能用于分析视觉搜索策略、决策过程并推动教育与AI辅助工具发展。

Abstract: Interpretation of giga-pixel whole-slide images (WSIs) is an important but
difficult task for pathologists. Their diagnostic accuracy is estimated to
average around 70%. Adding a second pathologist does not substantially improve
decision consistency. The field lacks adequate behavioral data to explain
diagnostic errors and inconsistencies. To fill in this gap, we present
PathoGaze1.0, a comprehensive behavioral dataset capturing the dynamic visual
search and decision-making processes of the full diagnostic workflow during
cancer diagnosis. The dataset comprises 18.69 hours of eye-tracking, mouse
interaction, stimulus tracking, viewport navigation, and diagnostic decision
data (EMSVD) collected from 19 pathologists interpreting 397 WSIs. The data
collection process emphasizes ecological validity through an
application-grounded testbed, called PTAH. In total, we recorded 171,909
fixations, 263,320 saccades, and 1,867,362 mouse interaction events. In
addition, such data could also be used to improve the training of both
pathologists and AI systems that might support human experts. All experiments
were preregistered at https://osf.io/hj9a7, and the complete dataset along with
analysis code is available at https://go.osu.edu/pathogaze.

</details>


### [65] [Group Relative Attention Guidance for Image Editing](https://arxiv.org/abs/2510.24657)
*Xuanpu Zhang,Xuesong Niu,Ruidong Chen,Dan Song,Jianhao Zeng,Penghui Du,Haoxiang Cao,Kai Wu,An-an Liu*

Main category: cs.CV

TL;DR: GRAG adjusts token-wise 'delta' (token minus layer bias) in MM-Attention to continuously control editing strength in DiT-based image editing; simple to integrate and outperforms CFG in smoothness and precision.


<details>
  <summary>Details</summary>
Motivation: Investigate MM-Attention in DiT and control editing degree in diffusion-in-Transformer image editing; propose mechanism to modulate editing intensity without tuning.

Method: Paper analysis

Result: Propose Group Relative Attention Guidance (GRAG) that reweights token deltas to control editing intensity; integrates with minimal code and offers smoother control than Classifier-Free Guidance; validated with experiments.

Conclusion: GRAG provides fine-grained, continuous control of editing intensity by reweighting content-specific deltas in attention; easy to implement and improves editing quality across frameworks.

Abstract: Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.

</details>


### [66] [SAGE: Structure-Aware Generative Video Transitions between Diverse Clips](https://arxiv.org/abs/2510.24667)
*Mia Kan,Yilin Liu,Niloy Mitra*

Main category: cs.CV

TL;DR: 提出了SAGE，一种零样本结构感知视频过渡方法，结合线描和光流引导与生成模型合成中间帧，能在语义差异大和时间间隔长的片段间生成连贯过渡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理语义差异大或时间差距长的片段时常产生伪影或结构混乱，需要一种内容感知且保持感知连贯性的过渡生成方法。

Method: 受艺术家工作流启发，提取并对齐轮廓线图和运动光流作为结构信息，结合生成模型进行引导合成，采用显著特征插值策略来保持结构连续性，并以零样本方式运行。

Result: 在与FILM、TVG、DiffMorpher、VACE、GI等方法的比较中，SAGE在量化指标和用户评价中均表现更优，能更稳健地生成多样片段间的视觉连贯过渡。

Conclusion: SAGE在定量指标和用户研究中均优于现有传统和生成基线，能够在无需微调的情况下生成结构和语义一致的平滑过渡。

Abstract: Video transitions aim to synthesize intermediate frames between two clips,
but naive approaches such as linear blending introduce artifacts that limit
professional use or break temporal coherence. Traditional techniques
(cross-fades, morphing, frame interpolation) and recent generative inbetweening
methods can produce high-quality plausible intermediates, but they struggle
with bridging diverse clips involving large temporal gaps or significant
semantic differences, leaving a gap for content-aware and visually coherent
transitions. We address this challenge by drawing on artistic workflows,
distilling strategies such as aligning silhouettes and interpolating salient
features to preserve structure and perceptual continuity. Building on this, we
propose SAGE (Structure-Aware Generative vidEo transitions) as a zeroshot
approach that combines structural guidance, provided via line maps and motion
flow, with generative synthesis, enabling smooth, semantically consistent
transitions without fine-tuning. Extensive experiments and comparison with
current alternatives, namely [FILM, TVG, DiffMorpher, VACE, GI], demonstrate
that SAGE outperforms both classical and generative baselines on quantitative
metrics and user studies for producing transitions between diverse clips. Code
to be released on acceptance.

</details>


### [67] [MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection](https://arxiv.org/abs/2510.24688)
*Yun Zhang,Zhaoliang Zheng,Johnson Liu,Zhiyu Huang,Zewei Zhou,Zonglin Meng,Tianhui Cai,Jiaqi Ma*

Main category: cs.CV

TL;DR: 提出MIC-BEV，一种基于Transformer和图增强的BEV框架，支持异构多摄像头并在合成与真实数据上表现优异且鲁棒性强


<details>
  <summary>Details</summary>
Motivation: Existing camera-based models struggle in infrastructure scenarios due to multi-view setups, heterogeneous camera parameters, degraded inputs, and varied road layouts

Method: Transformer-based BEV for infrastructure multi-camera 3D detection

Result: MIC-BEV supports variable camera numbers with heterogeneous intrinsics/extrinsics, robust to sensor degradation; introduces graph-enhanced fusion to integrate multi-view features into BEV; M2I synthetic dataset; state-of-the-art results on M2I and RoScenes

Conclusion: MIC-BEV有效应对基础设施感知挑战，具备实际部署潜力，伴随M2I数据集和代码公开

Abstract: Infrastructure-based perception plays a crucial role in intelligent
transportation systems, offering global situational awareness and enabling
cooperative autonomy. However, existing camera-based detection models often
underperform in such scenarios due to challenges such as multi-view
infrastructure setup, diverse camera configurations, degraded visual inputs,
and various road layouts. We introduce MIC-BEV, a Transformer-based
bird's-eye-view (BEV) perception framework for infrastructure-based
multi-camera 3D object detection. MIC-BEV flexibly supports a variable number
of cameras with heterogeneous intrinsic and extrinsic parameters and
demonstrates strong robustness under sensor degradation. The proposed
graph-enhanced fusion module in MIC-BEV integrates multi-view image features
into the BEV space by exploiting geometric relationships between cameras and
BEV cells alongside latent visual cues. To support training and evaluation, we
introduce M2I, a synthetic dataset for infrastructure-based object detection,
featuring diverse camera configurations, road layouts, and environmental
conditions. Extensive experiments on both M2I and the real-world dataset
RoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3D
object detection. It also remains robust under challenging conditions,
including extreme weather and sensor degradation. These results highlight the
potential of MIC-BEV for real-world deployment. The dataset and source code are
available at: https://github.com/HandsomeYun/MIC-BEV.

</details>


### [68] [Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?](https://arxiv.org/abs/2510.24709)
*Yihao Li,Saeed Salehi,Lyle Ungar,Konrad P. Kording*

Main category: cs.CV

TL;DR: 自监督ViT自发学到‘哪些patch属于同一对象’（IsSameObject），可高精度解码并影响注意力；监督训练则弱；该信号为低维可消融子空间，关乎下游表现。


<details>
  <summary>Details</summary>
Motivation: 探究ViT是否能自然出现对象绑定能力（将patchs归并为对象），以及这种能力是否依赖于预训练目标或仅由架构引起。

Method: 使用相似性探针在各层patch嵌入上解码IsSameObject，比较多种预训练（DINO、MAE、CLIP与ImageNet监督），分析低维子空间，进行消融实验并测量对注意力与下游性能的影响。

Result: ViTs在无监督预训练下能自发编码对象绑定（IsSameObject），在patch嵌入中可用相似性探针解码，准确率>90%。这种能力在DINO、MAE、CLIP等自监督模型中稳定出现，而在ImageNet监督模型中弱，表明是预训练目标驱动而非架构本身。IsSameObject位于对象特征之上的低维子空间，并能引导注意力。消融该信号会降低下游性能并与学习目标冲突，说明该能力服务于预训练目标。

Conclusion: ViTs并非缺乏对象绑定能力；特定自监督预训练会促使模型在嵌入中形成清晰的IsSameObject信号，这一符号式知识自然出现在连接主义系统中并支持注意力与下游任务。

Abstract: Object binding, the brain's ability to bind the many features that
collectively represent an object into a coherent whole, is central to human
cognition. It groups low-level perceptual features into high-level object
representations, stores those objects efficiently and compositionally in
memory, and supports human reasoning about individual object instances. While
prior work often imposes object-centric attention (e.g., Slot Attention)
explicitly to probe these benefits, it remains unclear whether this ability
naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they
could: recognizing which patches belong to the same object should be useful for
downstream prediction and thus guide attention. Motivated by the quadratic
nature of self-attention, we hypothesize that ViTs represent whether two
patches belong to the same object, a property we term IsSameObject. We decode
IsSameObject from patch embeddings across ViT layers using a similarity probe,
which reaches over 90% accuracy. Crucially, this object-binding capability
emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker
in ImageNet-supervised models, suggesting that binding is not a trivial
architectural artifact, but an ability acquired through specific pretraining
objectives. We further discover that IsSameObject is encoded in a
low-dimensional subspace on top of object features, and that this signal
actively guides attention. Ablating IsSameObject from model activations
degrades downstream performance and works against the learning objective,
implying that emergent object binding naturally serves the pretraining
objective. Our findings challenge the view that ViTs lack object binding and
highlight how symbolic knowledge of "which parts belong together" emerges
naturally in a connectionist system.

</details>


### [69] [Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance](https://arxiv.org/abs/2510.24711)
*Yujie Wei,Shiwei Zhang,Hangjie Yuan,Yujin Han,Zhekai Chen,Jiayu Wang,Difan Zou,Xihui Liu,Yingya Zhang,Yu Liu,Hongming Shan*

Main category: cs.CV

TL;DR: ProMoE通过条件+原型两步显式路由和路由对比损失，促进视觉MoE专家专门化，在ImageNet上超越现有DiT MoE方法。


<details>
  <summary>Details</summary>
Motivation: 视觉token与语言token在特性上差异显著，视觉token存在空间冗余与功能异质性，导致传统MoE难以在视觉扩散Transformer中实现专家专门化，需设计显式路由引导。

Method: 设计了条件路由将图像token按功能分为条件/无条件集合，随后用带可学习原型的原型路由基于语义内容细化条件token的分配；引入相似性基础的专家分配与显式语义引导，并提出路由对比损失以增强专家内部一致性与专家间多样性。

Result: 在ImageNet上、在Rectified Flow与DDPM训练目标下，ProMoE优于现有最先进方法；并验证了显式语义引导与路由对比损失的重要性。

Conclusion: 本文提出ProMoE，通过两步显式路由引导促进视觉MoE专家专门化，从而弥补了将MoE从语言应用到视觉扩散模型的差距。

Abstract: Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.

</details>


### [70] [Uniform Discrete Diffusion with Metric Path for Video Generation](https://arxiv.org/abs/2510.24717)
*Haoge Deng,Ting Pan,Fan Zhang,Yang Liu,Zhuoyan Luo,Yufeng Cui,Wenxuan Wang,Chunhua Shen,Shiguang Shan,Zhaoxiang Zhang,Xinlong Wang*

Main category: cs.CV

TL;DR: URSA 是一种改进的离散扩散视频生成框架，通过线性化度量路径和分辨率依赖时间步迁移，实现对离散时空 token 的高效迭代精修，显著缩小与连续方法的差距，支持高分辨率与长时长视频生成并统一插帧/图像到视频任务。


<details>
  <summary>Details</summary>
Motivation: 当前连续空间视频生成快速进展，而离散方法因误差累积和长时上下文不一致落后。研究动机是让离散生成模型克服这些限制，在高分辨率与长时长场景下与连续方法竞争，同时保持离散表示的优点（如可控性、紧凑性）。

Method: 将视频生成视为对离散时空 token 的迭代全局精修；提出 Linearized Metric Path（线性化度量路径）以改进相似性度量与噪声调度；引入 Resolution-dependent Timestep Shifting（分辨率依赖的时间步迁移）以在不同分辨率上高效扩展并减少推理步数；并采用异步时间向精调策略统一插帧与图像到视频等任务。

Result: 在多项挑战性视频与图像生成基准上，URSA 显著优于现有离散方法，并在生成质量与一致性上接近甚至匹配最先进的连续扩散方法；模型在高分辨率与长时长生成时推理步数更少且更高效；代码与模型开源。

Conclusion: URSA 将离散扩散模型与新的度量路径和分辨率依赖时间步迁移机制结合，通过迭代全局精修离散时空 token，显著提升了离散视频生成的质量与可扩展性，使其在高分辨率与长时长视频生成上接近连续扩散方法表现。

Abstract: Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA

</details>


### [71] [Generative View Stitching](https://arxiv.org/abs/2510.24718)
*Chonghyuk Song,Michal Stary,Boyuan Chen,George Kopanas,Vincent Sitzmann*

Main category: cs.CV

TL;DR: GVS enables parallel sampling for camera-guided video using Diffusion Forcing models and Omni Guidance to ensure collision-free, temporally consistent, loop-closing generations.


<details>
  <summary>Details</summary>
Motivation: Autoregressive video diffusion models can't use future conditioning, causing collisions with scenes in camera-guided generation with predefined trajectories.

Method: Extend diffusion stitching methods from robot planning to video; sample entire sequence in parallel; use models trained with Diffusion Forcing; introduce Omni Guidance conditioning on both past and future and loop-closing mechanism.

Result: Proposed Generative View Stitching (GVS) samples entire sequence in parallel using diffusion stitching methods adapted for video; compatible with off-the-shelf models trained with Diffusion Forcing; introduces Omni Guidance for conditioning on past and future and loop-closing; yields collision-free, consistent, loop-closing camera-guided videos.

Conclusion: GVS provides a sampling algorithm and Omni Guidance that together allow off-the-shelf diffusion video models to generate long, consistent, collision-free, loop-closing camera-guided videos.

Abstract: Autoregressive video diffusion models are capable of long rollouts that are
stable and consistent with history, but they are unable to guide the current
generation with conditioning from the future. In camera-guided video generation
with a predefined camera trajectory, this limitation leads to collisions with
the generated scene, after which autoregression quickly collapses. To address
this, we propose Generative View Stitching (GVS), which samples the entire
sequence in parallel such that the generated scene is faithful to every part of
the predefined camera trajectory. Our main contribution is a sampling algorithm
that extends prior work on diffusion stitching for robot planning to video
generation. While such stitching methods usually require a specially trained
model, GVS is compatible with any off-the-shelf video model trained with
Diffusion Forcing, a prevalent sequence diffusion framework that we show
already provides the affordances necessary for stitching. We then introduce
Omni Guidance, a technique that enhances the temporal consistency in stitching
by conditioning on both the past and future, and that enables our proposed
loop-closing mechanism for delivering long-range coherence. Overall, GVS
achieves camera-guided video generation that is stable, collision-free,
frame-to-frame consistent, and closes loops for a variety of predefined camera
paths, including Oscar Reutersv\"ard's Impossible Staircase. Results are best
viewed as videos at https://andrewsonga.github.io/gvs.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [72] [MSRANetV2: An Explainable Deep Learning Architecture for Multi-class Classification of Colorectal Histopathological Images](https://arxiv.org/abs/2510.24136)
*Ovi Sarkar,Md Shafiuzzaman,Md. Faysal Ahamed,Golam Mahmud,Muhammad E. H. Chowdhury*

Main category: eess.IV

TL;DR: 提出MSRANetV2用于结直肠组织分类，结合残差注意力和SE、多尺度融合，5折交叉验证在CRC-VAL-HE-7K与NCT-CRC-HE-100K上均实现≈99%分类性能并用Grad-CAM解释预测。


<details>
  <summary>Details</summary>
Motivation: 为提高结直肠癌病理图像分类的精度和可解释性，减少传统方法的主观性与耗时，通过深度学习构建鲁棒且可解释的自动化分类模型。

Method: 基于ResNet50V2骨干，加入残差注意力模块与SE块以增强通道和空间特征表示，通过通道对齐与上采样实现多尺度特征融合，使用5折分层交叉验证评估性能，并用Grad-CAM可视化关注区域。

Result: MSRANetV2在两份公开数据集上的多项指标均达到或接近0.99，表现非常优秀；方法上结合ResNet50V2主干、残差注意力和SE模块，多尺度特征融合与上采样用于增强鲁棒性；使用Grad-CAM提升可解释性。

Conclusion: MSRANetV2在公开CRC图像数据集上显示出高准确率与稳定性，且通过注意力和SE模块提升特征表达、通过Grad-CAM增强可解释性，具有临床辅助诊断潜力，但需注意泛化性与真实临床场景验证。

Abstract: Colorectal cancer (CRC) is a leading worldwide cause of cancer-related
mortality, and the role of prompt precise detection is of paramount interest in
improving patient outcomes. Conventional diagnostic methods such as colonoscopy
and histological examination routinely exhibit subjectivity, are extremely
time-consuming, and are susceptible to variation. Through the development of
digital pathology, deep learning algorithms have become a powerful approach in
enhancing diagnostic precision and efficiency. In our work, we proposed a
convolutional neural network architecture named MSRANetV2, specially optimized
for the classification of colorectal tissue images. The model employs a
ResNet50V2 backbone, extended with residual attention mechanisms and
squeeze-and-excitation (SE) blocks, to extract deep semantic and fine-grained
spatial features. With channel alignment and upsampling operations, MSRANetV2
effectively fuses multi-scale representations, thereby enhancing the robustness
of the classification. We evaluated our model on a five-fold stratified
cross-validation strategy on two publicly available datasets: CRC-VAL-HE-7K and
NCT-CRC-HE-100K. The proposed model achieved remarkable average Precision,
recall, F1-score, AUC, and test accuracy were 0.9884 plus-minus 0.0151, 0.9900
plus-minus 0.0151, 0.9900 plus-minus 0.0145, 0.9999 plus-minus 0.00006, and
0.9905 plus-minus 0.0025 on the 7K dataset. On the 100K dataset, they were
0.9904 plus-minus 0.0091, 0.9900 plus-minus 0.0071, 0.9900 plus-minus 0.0071,
0.9997 plus-minus 0.00016, and 0.9902 plus-minus 0.0006. Additionally, Grad-CAM
visualizations were incorporated to enhance model interpretability by
highlighting tissue areas that are medically relevant. These findings validate
that MSRANetV2 is a reliable, interpretable, and high-performing architectural
model for classifying CRC tissues.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [73] [Odyssey: An End-to-End System for Pareto-Optimal Serverless Query Processing](https://arxiv.org/abs/2510.24307)
*Shyam Jesalpura,Shengda Zhu,Amir Shaikhha,Antonio Barbalace,Boris Grot*

Main category: cs.DB

TL;DR: Odyssey为无服务器(FaaS)环境设计端到端数据分析管道，结合查询规划器、成本模型与执行引擎，自动生成并搜索服务器less查询计划，利用剪枝启发式和新搜索算法快速找到在成本与性能上呈帕累托最优的计划；评估显示可准确预测成本与延迟，并在成本或延迟上优于AWS Athena。


<details>
  <summary>Details</summary>
Motivation: 现有serverless数据分析工作多聚焦执行引擎，假设已有良好查询计划或依赖用户手工指定。但在serverless中，查询计划空间巨大且影响成本与性能，缺乏自动化的端到端规划与选择工具。

Method: 构建端到端系统：查询生成器、基于serverless特性的成本模型、执行引擎；对计划空间进行状态剪枝与提出新搜索算法，用以高效发现帕累托最优计划，并对计划进行评估以选取执行计划。

Result: Odyssey能准确预测金钱成本与延迟；在评测中相较于AWS Athena在成本或延迟上持续更优，能为复杂查询快速找到低延迟的帕累托最优计划。

Conclusion: Odyssey能在serverless环境中自动生成并挑选成本-性能平衡的查询执行计划，准确预测成本和延迟，实测优于AWS Athena，适用于复杂查询并在低延迟下运行。

Abstract: Running data analytics queries on serverless (FaaS) workers has been shown to
be cost- and performance-efficient for a variety of real-world scenarios,
including intermittent query arrival patterns, sudden load spikes and
management challenges that afflict managed VM clusters. Alas, existing
serverless data analytics works focus primarily on the serverless execution
engine and assume the existence of a "good" query execution plan or rely on
user guidance to construct such a plan. Meanwhile, even simple analytics
queries on serverless have a huge space of possible plans, with vast
differences in both performance and cost among plans.
  This paper introduces Odyssey, an end-to-end serverless-native data analytics
pipeline that integrates a query planner, cost model and execution engine.
Odyssey automatically generates and evaluates serverless query plans, utilizing
state space pruning heuristics and a novel search algorithm to identify
Pareto-optimal plans that balance cost and performance with low latency even
for complex queries. Our evaluations demonstrate that Odyssey accurately
predicts both monetary cost and latency, and consistently outperforms AWS
Athena on cost and/or latency.

</details>


### [74] [Evaluating Joinable Column Discovery Approaches for Context-Aware Search](https://arxiv.org/abs/2510.24599)
*Harsha Kokel,Aamod Khatiwada,Tejaswini Pedapati,Haritha Ananthakrishnan,Oktie Hassanzadeh,Horst Samulowitz,Kavitha Srinivas*

Main category: cs.DB

TL;DR: 对比并评估了多种基于语法和语义的列连接发现方法，在七个基准上分析六类判据及其组合，发现元数据与值语义对数据湖重要，基于大小的判据在关系型数据库更有效，集成方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有工作多关注语法重叠或语义相似，但缺乏对不同数据特性下各方法表现的系统比较以及多判据结合效果的理解。

Method: 在七个覆盖关系型数据库和数据湖的基准数据集上，对比语法（如交集、唯一值比）与语义（如预训练嵌入模型）方法，分析六个判据（唯一值、交集大小、连接大小、反向连接大小、值语义、元数据语义），并通过集成排序融合多判据评估性能。

Result: 实验表明：元数据与值语义在数据湖中最重要；大小相关判据在关系型数据库中更有区分力；不同预训练嵌入模型在语义连接上表现有差异；集成排序通常优于单一判据。

Conclusion: 不同数据情境下，各判据表现差异显著：数据湖依赖元数据与语义信息，关系型数据库更受大小类判据影响；将多种判据集成可显著提升发现效果。研究为方法选择提供了实用指南。

Abstract: Joinable Column Discovery is a critical challenge in automating enterprise
data analysis. While existing approaches focus on syntactic overlap and
semantic similarity, there remains limited understanding of which methods
perform best for different data characteristics and how multiple criteria
influence discovery effectiveness. We present a comprehensive experimental
evaluation of joinable column discovery methods across diverse scenarios. Our
study compares syntactic and semantic techniques on seven benchmarks covering
relational databases and data lakes. We analyze six key criteria -- unique
values, intersection size, join size, reverse join size, value semantics, and
metadata semantics -- and examine how combining them through ensemble ranking
affects performance. Our analysis reveals differences in method behavior across
data contexts and highlights the benefits of integrating multiple criteria for
robust join discovery. We provide empirical evidence on when each criterion
matters, compare pre-trained embedding models for semantic joins, and offer
practical guidelines for selecting suitable methods based on dataset
characteristics. Our findings show that metadata and value semantics are
crucial for data lakes, size-based criteria play a stronger role in relational
databases, and ensemble approaches consistently outperform single-criterion
methods.

</details>
