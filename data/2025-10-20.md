<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 87]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GAZE:Governance-Aware pre-annotation for Zero-shot World Model Environments](https://arxiv.org/abs/2510.14992)
*Leela Krishna,Mengyang Zhao,Saicharithreddy Pasula,Harshit Rajgarhia,Abhishek Mukherji*

Main category: cs.CV

TL;DR: GAZE是一套用于将长视频自动转为隐私感知、多模态、可验证训练标签的生产化流水线，显著降低人工开销并提升数据质量与一致性。


<details>
  <summary>Details</summary>
Motivation: 手工标注多模态大规模长视频既昂贵又慢，阻碍鲁棒世界模型的训练；需要一个自动化、高通量且兼顾隐私与治理的标注流水线。

Method: 将360度专有格式归一化为标准视图并分片并行处理；利用场景理解、目标跟踪、语音转录、PII/NSFW/未成年人检测等AI模型进行密集预注释；合并多模态信号为结构化输出并用于快速人工校验。

Result: 流水线在生产环境测试中每小时审查节省约19分钟，人为审查量通过自动跳过低显著性片段减少超过80%，同时提高标签密度与一致性并保留可追溯的元数据。

Conclusion: GAZE管道能显著提升长视频到多模态标注的效率与一致性，生成可直接用于世界模型训练的高质量、隐私合规数据集。

Abstract: Training robust world models requires large-scale, precisely labeled
multimodal datasets, a process historically bottlenecked by slow and expensive
manual annotation. We present a production-tested GAZE pipeline that automates
the conversion of raw, long-form video into rich, task-ready supervision for
world-model training. Our system (i) normalizes proprietary 360-degree formats
into standard views and shards them for parallel processing; (ii) applies a
suite of AI models (scene understanding, object tracking, audio transcription,
PII/NSFW/minor detection) for dense, multimodal pre-annotation; and (iii)
consolidates signals into a structured output specification for rapid human
validation.
  The GAZE workflow demonstrably yields efficiency gains (~19 minutes saved per
review hour) and reduces human review volume by >80% through conservative
auto-skipping of low-salience segments. By increasing label density and
consistency while integrating privacy safeguards and chain-of-custody metadata,
our method generates high-fidelity, privacy-aware datasets directly consumable
for learning cross-modal dynamics and action-conditioned prediction. We detail
our orchestration, model choices, and data dictionary to provide a scalable
blueprint for generating high-quality world model training data without
sacrificing throughput or governance.

</details>


### [2] [PC-UNet: An Enforcing Poisson Statistics U-Net for Positron Emission Tomography Denoising](https://arxiv.org/abs/2510.14995)
*Yang Shi,Jingchao Wang,Liangsi Lu,Mingxuan Huang,Ruixin He,Yifeng Xie,Hanqian Liu,Minzhe Guo,Yangyang Liang,Weipeng Zhang,Zimeng Li,Xuhang Chen*

Main category: cs.CV

TL;DR: 引入基于泊松统计的一致性损失（PVMC-Loss）的U-Net（PC-UNet），在低剂量PET去噪中提升了物理一致性与图像保真度，且统计无偏且鲁棒。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET会引入显著泊松噪声，现有去噪方法不能充分利用噪声的物理统计特性，导致重建图像出现失真和伪影，因此需要一种能把物理噪声模型融入学习过程的方法以提高重建质量。

Method: 构建基于U-Net的深度去噪网络（PC-UNet），并设计Poisson Variance and Mean Consistency Loss（PVMC-Loss），该损失基于泊松统计的矩条件（均值与方差一致性），并通过广义矩估计思想保证无偏性和梯度自适应。训练时保留数据的物理统计信息以约束重建结果。

Result: 在多个PET数据集上的实验表明，PC-UNet在物理一致性指标和图像质量指标上都优于对比方法，且对轻微数据分布不匹配具有鲁棒性。

Conclusion: 该论文提出的PC-UNet通过在损失函数中引入符合泊松噪声统计特性的PVMC-Loss，能在低剂量PET图像重建中提升物理一致性和图像保真度。

Abstract: Positron Emission Tomography (PET) is crucial in medicine, but its clinical
use is limited due to high signal-to-noise ratio doses increasing radiation
exposure. Lowering doses increases Poisson noise, which current denoising
methods fail to handle, causing distortions and artifacts. We propose a Poisson
Consistent U-Net (PC-UNet) model with a new Poisson Variance and Mean
Consistency Loss (PVMC-Loss) that incorporates physical data to improve image
fidelity. PVMC-Loss is statistically unbiased in variance and gradient
adaptation, acting as a Generalized Method of Moments implementation, offering
robustness to minor data mismatches. Tests on PET datasets show PC-UNet
improves physical consistency and image fidelity, proving its ability to
integrate physical information effectively.

</details>


### [3] [DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.15015)
*Mor Ventura,Michael Toker,Or Patashnik,Yonatan Belinkov,Roi Reichart*

Main category: cs.CV

TL;DR: 提出了通过动态重加权注意力图在推理时抑制语义泄露的DeLeaker，并通过SLIM数据集验证其在不损失图像质量下有效减少实体间语义混淆。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型易发生语义泄露（不同实体间语义特征意外迁移），且现有缓解方法依赖优化或外部输入，效率低或受限；需要轻量、实时且不引入额外资源的解决方案。

Method: 在扩散过程的每一步动态重加权注意力图，通过抑制实体间过度交互并强化各实体身份来减少语义混淆；设计了基于注意力控制的插入机制，无需额外外部信息或训练。

Result: 在新建的SLIM数据集（1,130个人工验证样本）和自动评估框架上，DeLeaker优于所有基线方法，即使基线使用外部信息；在降低泄露的同时不显著影响图像保真度或质量。

Conclusion: 本文提出DeLeaker，一种轻量级、无需优化的推理时注意力干预方法，有效缓解文本到图像生成模型的语义泄露问题，同时保持图像质量。

Abstract: Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable
to semantic leakage, the unintended transfer of semantically related features
between distinct entities. Existing mitigation strategies are often
optimization-based or dependent on external inputs. We introduce DeLeaker, a
lightweight, optimization-free inference-time approach that mitigates leakage
by directly intervening on the model's attention maps. Throughout the diffusion
process, DeLeaker dynamically reweights attention maps to suppress excessive
cross-entity interactions while strengthening the identity of each entity. To
support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages),
the first dataset dedicated to semantic leakage, comprising 1,130
human-verified samples spanning diverse scenarios, together with a novel
automatic evaluation framework. Experiments demonstrate that DeLeaker
consistently outperforms all baselines, even when they are provided with
external information, achieving effective leakage mitigation without
compromising fidelity or quality. These results underscore the value of
attention control and pave the way for more semantically precise T2I models.

</details>


### [4] [UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos](https://arxiv.org/abs/2510.15018)
*Mingxuan Liu,Honglin He,Elisa Ricci,Wayne Wu,Bolei Zhou*

Main category: cs.CV

TL;DR: 提出UrbanVerse：从城市旅游视频自动生成物理感知仿真场景及100k+资产库，显著提升城市导航策略的仿真与sim-to-real性能。


<details>
  <summary>Details</summary>
Motivation: 现有手工或程序生成的城市仿真场景要么缺乏可扩展性要么无法逼真反映现实复杂性，影响训练能在真实世界泛化的城市机器人代理。

Method: 构建了两个核心模块：UrbanVerse-100K（含100k+带语义与物理属性的城市3D资产库）和UrbanVerse-Gen（从众包城市旅游视频中自动提取场景布局并用检索到的3D资产生成公尺尺度物理仿真场景），并在IsaacSim中产出160个高质量场景与10个测试场景基准进行评估。

Result: 生成160个来自24国的高质量场景，UrbanVerse生成的场景在人类评估中逼真度与手工场景相当；在导航任务上，训练的策略在仿真中提升+6.3%成功率，零样本sim-to-real迁移提升+30.1%，完成300m真实任务仅两次干预。

Conclusion: UrbanVerse提供了一个以真实城市视频为源的高质量、可扩展的real-to-sim系统，实验证明其场景在语义和布局上接近人工设计场景，并能提升城市导航策略的模拟与实地泛化性能。

Abstract: Urban embodied AI agents, ranging from delivery robots to quadrupeds, are
increasingly populating our cities, navigating chaotic streets to provide
last-mile connectivity. Training such agents requires diverse, high-fidelity
urban environments to scale, yet existing human-crafted or procedurally
generated simulation scenes either lack scalability or fail to capture
real-world complexity. We introduce UrbanVerse, a data-driven real-to-sim
system that converts crowd-sourced city-tour videos into physics-aware,
interactive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a
repository of 100k+ annotated urban 3D assets with semantic and physical
attributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene
layouts from video and instantiates metric-scale 3D simulations using retrieved
assets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed
scenes from 24 countries, along with a curated benchmark of 10 artist-designed
test scenes. Experiments show that UrbanVerse scenes preserve real-world
semantics and layouts, achieving human-evaluated realism comparable to manually
crafted scenes. In urban navigation, policies trained in UrbanVerse exhibit
scaling power laws and strong generalization, improving success by +6.3% in
simulation and +30.1% in zero-shot sim-to-real transfer comparing to prior
methods, accomplishing a 300 m real-world mission with only two interventions.

</details>


### [5] [NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks](https://arxiv.org/abs/2510.15019)
*Junliang Ye,Shenghao Xie,Ruowen Zhao,Zhengyi Wang,Hongyu Yan,Wenqiang Zu,Lei Ma,Jun Zhu*

Main category: cs.CV

TL;DR: 提出训练-free的Nano3D，通过FlowEdit+TRELLIS和Voxel/Slat-Merge实现无掩码的精准一致3D编辑，并发布100k+数据集，显著提高编辑质量与可用数据规模。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖对多视图渲染的编辑后重建，导致伪影、效率低且难以保留未编辑区域的结构一致性；因此需要一个无需训练、能在3D层面直接保持局部精确编辑与整体连贯性的方案。

Method: 基于训练-free框架，Nano3D先在前视图渲染上利用FlowEdit做局部编辑，再通过TRELLIS进行多视图与体素级别的整合，最后采用Voxel-Merge和Slat-Merge两种区域感知合并策略保证编辑与未编辑区域的一致性。

Result: 实验表明Nano3D在3D一致性和视觉质量上优于现有方法；基于该框架构建了规模超过100,000对高质量编辑样本的Nano3D-Edit-100k数据集，为训练前馈式3D编辑模型提供了资源。

Conclusion: Nano3D在无需训练的情况下，通过将FlowEdit集成进TRELLIS并引入区域感知的合并策略（Voxel/Slat-Merge），实现了精准且一致的3D物体编辑，能更好地保留未编辑区域并提升3D一致性与视觉质量。

Abstract: 3D object editing is essential for interactive content creation in gaming,
animation, and robotics, yet current approaches remain inefficient,
inconsistent, and often fail to preserve unedited regions. Most methods rely on
editing multi-view renderings followed by reconstruction, which introduces
artifacts and limits practicality. To address these challenges, we propose
Nano3D, a training-free framework for precise and coherent 3D object editing
without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized
edits guided by front-view renderings, and further introduces region-aware
merging strategies, Voxel/Slat-Merge, which adaptively preserve structural
fidelity by ensuring consistency between edited and unedited areas. Experiments
demonstrate that Nano3D achieves superior 3D consistency and visual quality
compared with existing methods. Based on this framework, we construct the first
large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000
high-quality 3D editing pairs. This work addresses long-standing challenges in
both algorithm design and data availability, significantly improving the
generality and reliability of 3D editing, and laying the groundwork for the
development of feed-forward 3D editing models. Project
Page:https://jamesyjl.github.io/Nano3D

</details>


### [6] [Constantly Improving Image Models Need Constantly Improving Benchmarks](https://arxiv.org/abs/2510.15021)
*Jiaxin Ge,Grace Luo,Heekyung Lee,Nishant Malpani,Long Lian,XuDong Wang,Aleksander Holynski,Trevor Darrell,Sewon Min,David M. Chan*

Main category: cs.CV

TL;DR: ECHO用社交媒体上的真实提示和反馈构建大规模图像生成基准，更好捕捉社区实际用例，发现新任务并改进评测指标，从而更有效地区分模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成基准滞后于快速发展的产出，不能覆盖社区实际使用中新出现的复杂场景与创意用例，需要以真实使用证据构建更贴近用户的评测。

Method: 提出从社交媒体收集并筛选用户生成的图像生成提示与对应定性评价，构建包含3.1万+提示的数据集；基于这些提示设计评价任务与度量（颜色、身份、结构变化等），并用以评测GPT-4o Image Gen及替代模型。

Result: ECHO发现了传统基准中缺失的任务（如跨语言重绘产品标签、按指定总额生成收据），在区分最先进模型与替代品上表现更清晰，并为设计新的质量衡量指标提供了社区驱动的证据。

Conclusion: ECHO通过从社交媒体真实用户示例构建基准，填补了现有评测与实际使用场景之间的差距，能发现更具创意和复杂性的任务并更有效区分模型性能。

Abstract: Recent advances in image generation, often driven by proprietary systems like
GPT-4o Image Gen, regularly introduce new capabilities that reshape how users
interact with these models. Existing benchmarks often lag behind and fail to
capture these emerging use cases, leaving a gap between community perceptions
of progress and formal evaluation. To address this, we present ECHO, a
framework for constructing benchmarks directly from real-world evidence of
model use: social media posts that showcase novel prompts and qualitative user
judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset
of over 31,000 prompts curated from such posts. Our analysis shows that ECHO
(1) discovers creative and complex tasks absent from existing benchmarks, such
as re-rendering product labels across languages or generating receipts with
specified totals, (2) more clearly distinguishes state-of-the-art models from
alternatives, and (3) surfaces community feedback that we use to inform the
design of metrics for model quality (e.g., measuring observed shifts in color,
identity, and structure). Our website is at https://echo-bench.github.io.

</details>


### [7] [LoRAverse: A Submodular Framework to Retrieve Diverse Adapters for Diffusion Models](https://arxiv.org/abs/2510.15022)
*Mert Sonmezer,Matthew Zheng,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出基于子模函数的适配器选择方法，通过组合优化在海量LoRA中挑选相关且多样的子集，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前有超过10万的LoRA适配器可用，但它们缺乏结构化组织，导致用户难以筛选出最适合其需求的适配器；因此需要一种自动化方法在相关性与多样性之间取得平衡。

Method: 将适配器选择问题建模为组合优化问题，设计了子模函数以同时衡量相关性与多样性，采用启发式或近似算法（例如贪心算法）求解子模最大化，从而在有限预算下选出最佳适配器集合。

Result: 定量与定性实验表明，该方法能在多个领域生成具有更高多样性且相关性良好的输出，提升了适配器选择的有效性。

Conclusion: 本文提出了基于子模函数的组合优化框架，用于从海量LoRA适配器中选择最相关且多样的子集，解决了用户在海量模型中挑选与使用的困难。

Abstract: Low-rank Adaptation (LoRA) models have revolutionized the personalization of
pre-trained diffusion models by enabling fine-tuning through low-rank,
factorized weight matrices specifically optimized for attention layers. These
models facilitate the generation of highly customized content across a variety
of objects, individuals, and artistic styles without the need for extensive
retraining. Despite the availability of over 100K LoRA adapters on platforms
like Civit.ai, users often face challenges in navigating, selecting, and
effectively utilizing the most suitable adapters due to their sheer volume,
diversity, and lack of structured organization. This paper addresses the
problem of selecting the most relevant and diverse LoRA models from this vast
database by framing the task as a combinatorial optimization problem and
proposing a novel submodular framework. Our quantitative and qualitative
experiments demonstrate that our method generates diverse outputs across a wide
range of domains.

</details>


### [8] [MOBIUS: Big-to-Mobile Universal Instance Segmentation via Multi-modal Bottleneck Fusion and Calibrated Decoder Pruning](https://arxiv.org/abs/2510.15026)
*Mattia Segu,Marta Tintore Gazulla,Yongqin Xian,Luc Van Gool,Federico Tombari*

Main category: cs.CV

TL;DR: MOBIUS通过架构与训练上的高效设计，实现跨平台的Pareto最优实例分割模型，显著减小计算量并保持高性能，适合部署在从加速器到移动端的多类设备。


<details>
  <summary>Details</summary>
Motivation: 当前大型基础模型虽性能优异，但计算成本高，难以在边缘或移动设备上部署；需在性能与效率间取得Pareto最优的下放版本。

Method: 提出三项关键技术：瓶颈像素解码器用于高效多尺度多模态融合；语言引导的不确定性校准损失以实现自适应解码器剪枝；简化统一的训练策略。

Result: 在像素和变换器解码器FLOPs上分别最多降低约55%和75%，训练迭代次数降至原来的三分之一，同时保持SOTA级别的分割性能，并在高性能和移动平台上建立新的高效分割基准。

Conclusion: MOBIUS在不显著降低精度的前提下，通过架构优化和训练策略显著降低了计算开销，适合资源受限设备部署。

Abstract: Scaling up model size and training data has advanced foundation models for
instance-level perception, achieving state-of-the-art in-domain and zero-shot
performance across object detection and segmentation. However, their high
computational cost limits adoption on resource-constrained platforms. We first
examine the limitations of existing architectures in enabling efficient edge
deployment without compromising performance. We then introduce MOBIUS, a family
of foundation models for universal instance segmentation, designed for
Pareto-optimal downscaling to support deployment across devices ranging from
high-end accelerators to mobile hardware. To reduce training and inference
demands, we propose: (i) a bottleneck pixel decoder for efficient multi-scale
and multi-modal fusion, (ii) a language-guided uncertainty calibration loss for
adaptive decoder pruning, and (iii) a streamlined, unified training strategy.
Unlike efficient baselines that trade accuracy for reduced complexity, MOBIUS
reduces pixel and transformer decoder FLOPs by up to 55% and 75%, respectively,
while maintaining state-of-the-art performance in just a third of the training
iterations. MOBIUS establishes a new benchmark for efficient segmentation on
both high-performance computing platforms and mobile devices.

</details>


### [9] [Composition-Grounded Instruction Synthesis for Visual Reasoning](https://arxiv.org/abs/2510.15040)
*Xinyi Gu,Jiayuan Mao,Zhang-Wei Hong,Zhuoran Yu,Pengyuan Li,Dhiraj Joshi,Rogerio Feris,Zexue He*

Main category: cs.CV

TL;DR: COGS通过将少量种子问题分解为感知和推理因子并重组生成带中间步骤的合成问答，结合因子级强化学习，能用极少标注显著提升多模态模型在图表和网页等人工图像域的推理和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人工图像域（图表、渲染文档、网页）虽然丰富但缺乏大规模人工标注的推理数据，现有预训练多模态大模型在这些领域的推理能力受限，故提出数据高效的方法从少量种子问题扩展出可训练的合成推理数据。

Method: 将每个种子问题分解为原子感知与推理因子；用新图像系统性重组这些因子生成大量合成问答；每个合成问题附带子问题与中间答案，以用于基于因子级过程奖励的强化学习训练；在训练集上采用因子混合以增强跨数据集迁移。

Result: 在图表推理任务上，COGS显著提升了对未见问题的表现，尤其在推理密集与组合性问题上提升最大；使用因子级混合种子数据可提高不同数据集间的迁移能力；方法也成功扩展到网页等其他人工图像域。

Conclusion: 该论文提出了COGS，一种从少量种子问题合成大规模推理数据以提升多模态大模型在人工图像域（如图表、渲染文档、网页）上推理能力的方法；通过分解问题为感知与推理因子并重组生成带中间步骤的合成问答，从而支持基于因子级奖励的强化学习，显著提高了模型在复杂、组合性强问题上的泛化能力，并能跨数据集与域迁移。

Abstract: Pretrained multi-modal large language models (MLLMs) demonstrate strong
performance on diverse multimodal tasks, but remain limited in reasoning
capabilities for domains where annotations are difficult to collect. In this
work, we focus on artificial image domains such as charts, rendered documents,
and webpages, which are abundant in practice yet lack large-scale human
annotated reasoning datasets. We introduce COGS (COmposition-Grounded
instruction Synthesis), a data-efficient framework for equipping MLLMs with
advanced reasoning abilities from a small set of seed questions. The key idea
is to decompose each seed question into primitive perception and reasoning
factors, which can then be systematically recomposed with new images to
generate large collections of synthetic question-answer pairs. Each generated
question is paired with subquestions and intermediate answers, enabling
reinforcement learning with factor-level process rewards. Experiments on chart
reasoning show that COGS substantially improves performance on unseen
questions, with the largest gains on reasoning-heavy and compositional
questions. Moreover, training with a factor-level mixture of different seed
data yields better transfer across multiple datasets, suggesting that COGS
induces generalizable capabilities rather than dataset-specific overfitting. We
further demonstrate that the framework extends beyond charts to other domains
such as webpages.

</details>


### [10] [Generalized Dynamics Generation towards Scannable Physical World Model](https://arxiv.org/abs/2510.15041)
*Yichen Li,Zhiyi Li,Brandon Feng,Dinghuai Zhang,Antonio Torralba*

Main category: cs.CV

TL;DR: GDGen用势能最小化的思路，将方向性刚度扩展进弹性动力学并结合神经场，实现了对刚体、关节体和软体动力学的几何无关统一建模，能从运动数据推断物理属性，适用于交互式虚拟环境与机器人训练。


<details>
  <summary>Details</summary>
Motivation: 希望在可扫描环境中构建通用的具身智能体，需要一个能够同时处理多种物理行为（刚体、关节体、软体）且与几何无关的动力学表示与生成方法。

Method: 通过扩展弹性动力学引入方向性刚度，并使用专门网络学习扩展的材料属性，结合神经场（neural field）表示形变，GDGen在一个整体势能最小化框架下进行优化与推断。

Result: 实验证明GDGen能稳健地统一多种仿真范式，能从简单运动观测中推断物理属性，为构建互动虚拟环境和训练机器人提供了多功能基础。

Conclusion: GDGen提出了一种统一的势能视角来整合刚体、关节体和软体动力学，能在几何无关的表示下进行动力学生成与参数推断。

Abstract: Digital twin worlds with realistic interactive dynamics presents a new
opportunity to develop generalist embodied agents in scannable environments
with complex physical behaviors. To this end, we present GDGen (Generalized
Representation for Generalized Dynamics Generation), a framework that takes a
potential energy perspective to seamlessly integrate rigid body, articulated
body, and soft body dynamics into a unified, geometry-agnostic system. GDGen
operates from the governing principle that the potential energy for any stable
physical system should be low. This fresh perspective allows us to treat the
world as one holistic entity and infer underlying physical properties from
simple motion observations. We extend classic elastodynamics by introducing
directional stiffness to capture a broad spectrum of physical behaviors,
covering soft elastic, articulated, and rigid body systems. We propose a
specialized network to model the extended material property and employ a neural
field to represent deformation in a geometry-agnostic manner. Extensive
experiments demonstrate that GDGen robustly unifies diverse simulation
paradigms, offering a versatile foundation for creating interactive virtual
environments and training robotic agents in complex, dynamically rich
scenarios.

</details>


### [11] [Comprehensive language-image pre-training for 3D medical image understanding](https://arxiv.org/abs/2510.15042)
*Tassilo Wald,Ibrahim Ethem Hamamci,Yuan Gao,Sam Bond-Taylor,Harshita Sharma,Maximilian Ilse,Cynthia Lo,Olesya Melnichenko,Noel C. F. Codella,Maria Teodora Wetscherek,Klaus H. Maier-Hein,Panagiotis Korfiatis,Valentina Salvatelli,Javier Alvarez-Valle,Fernando Pérez-García*

Main category: cs.CV

TL;DR: COLIPRI利用报告生成与图像单模态预训练两种归纳偏置，扩大数据来源，提升3D医学影像视觉-语言模型在生成、分类与检索任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像中配对图像-文本数据稀缺限制了视觉语言预训练（VLP）模型的表现，影响检索、分类和报告生成等下游临床任务的能力。作者通过注入额外归纳偏置以扩大可用训练数据和改善模型学习。

Method: 引入两个主要的归纳偏置：1) 报告生成目标（将图像到文本的生成任务并入训练），2) 将视觉-语言预训练与视觉单独预训练配对，允许使用更多图像单模态数据。结合3D医学影像领域的最佳实践（如适配3D卷积/Transformer结构、数据增强和专用损失设计），构建COLIPRI编码器族。

Result: 通过融合图像单模态与图文配对数据并引入报告生成目标，COLIPRI在报告生成任务、分类探测（probing）与零样本分类上取得了最先进的结果，并在语义分割任务上表现出与最优方法相近的性能。

Conclusion: COLIPRI通过在3D医学影像视觉语言预训练中引入报告生成目标并结合仅视觉预训练，有效缓解了配对文本-图像数据稀缺的问题，从而利用更多图像和图文数据提升模型泛化能力。最终得到的模型在报告生成、分类探测和零样本分类任务上达到或领先于现有方法，并在语义分割上保持竞争力。

Abstract: Vision-language pre-training, i.e., aligning images with paired text, is a
powerful paradigm to create encoders that can be directly used for tasks such
as classification and retrieval, and for downstream tasks such as segmentation
and report generation. In the 3D medical image domain, these capabilities allow
vision-language encoders (VLEs) to support radiologists by retrieving patients
with similar abnormalities or predicting likelihoods of abnormality. While the
methodology holds promise, data availability limits the capabilities of current
3D VLEs.
  In this paper, we alleviate the lack of data by injecting additional
inductive biases: introducing a report generation objective and pairing
vision-language pre-training with vision-only pre-training. This allows us to
leverage both image-only and paired image-text 3D datasets, increasing the
total amount of data to which our model is exposed. Through these additional
inductive biases, paired with best practices of the 3D medical imaging domain,
we develop the Comprehensive Language-image Pre-training (COLIPRI) encoder
family. Our COLIPRI encoders achieve state-of-the-art performance in report
generation, classification probing, and zero-shot classification, and remain
competitive for semantic segmentation.

</details>


### [12] [Directional Reasoning Injection for Fine-Tuning MLLMs](https://arxiv.org/abs/2510.15050)
*Chao Huang,Zeliang Zhang,Jiang Liu,Ximeng Sun,Jialian Wu,Xiaodong Yu,Ze Wang,Chenliang Xu,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: 提出 DRIFT：通过在梯度空间注入预计算的推理先验，轻量且高效地把强推理能力迁移到多模态大模型，避免合并失败，显著提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 直接参数插值（模型合并）在跨模型家族间效果不稳定，有些模型会退化。寻找一种资源低、稳定且简单的方式，将强文本推理能力带入 MLLM，同时不破坏多模态对齐。

Method: 预先计算推理先验：计算推理增强模型与多模态模型参数的差；在多模态微调时，将该先验用于偏置梯度（方向性注入），从而在不改变标准监督微调流程的前提下转移推理能力。

Result: 在 MathVista、MathVerse 等多模态推理基准上，DRIFT 比直接合并和常规模型微调表现更好，并在成本显著更低的情况下匹配或超越训练密集型方法。

Conclusion: DRIFT 是一种轻量级方法，通过在梯度空间注入预计算的推理先验，将推理能力从增强推理的 LLM 转移到多模态模型，避免破坏多模态对齐，能在多模态推理任务上稳定提升性能。

Abstract: Multimodal large language models (MLLMs) are rapidly advancing, yet their
reasoning ability often lags behind that of strong text-only counterparts.
Existing methods to bridge this gap rely on supervised fine-tuning over
large-scale multimodal reasoning data or reinforcement learning, both of which
are resource-intensive. A promising alternative is model merging, which
interpolates parameters between reasoning-enhanced LLMs and multimodal
variants. However, our analysis shows that naive merging is not always a "free
lunch": its effectiveness varies drastically across model families, with some
(e.g., LLaVA, Idefics) benefiting while others (e.g., Qwen) suffer performance
degradation. To address this, we propose Directional Reasoning Injection for
Fine-Tuning (DRIFT) MLLMs, a lightweight method that transfers reasoning
knowledge in the gradient space, without destabilizing multimodal alignment.
DRIFT precomputes a reasoning prior as the parameter-space difference between
reasoning and multimodal variants, then uses it to bias gradients during
multimodal fine-tuning. This approach preserves the simplicity of standard
supervised fine-tuning pipelines while enabling efficient reasoning transfer.
Extensive experiments on multimodal reasoning benchmarks, including MathVista
and MathVerse, demonstrate that DRIFT consistently improves reasoning
performance over naive merging and supervised fine-tuning, while matching or
surpassing training-heavy methods at a fraction of the cost.

</details>


### [13] [A solution to generalized learning from small training sets found in everyday infant experiences](https://arxiv.org/abs/2510.15060)
*Frangil Ramirez,Elizabeth Clerkin,David J. Crandall,Linda B. Smith*

Main category: cs.CV

TL;DR: 婴儿日常视觉存在大量高度相似的重复经验（块状相似性），这种结构有助于从少量数据学习物体类别，且将此结构应用于机器学习可提升小样本泛化。


<details>
  <summary>Details</summary>
Motivation: 尽管婴儿能从有限经验中学会并泛化基本层级对象类别，但传统观点认为需要大规模多样化数据，作者提出应关注婴儿视觉经历的统计结构，尤其是重复单一对象实例的高相似度簇可能支持高效学习。

Method: 作者分析了14位7到11个月婴儿的自视角（egocentric）图像，量化图像之间在八类早期学习类别上的相似性结构；并在计算实验中通过构建具有相似‘块状’分布的训练集来测试机器学习模型的泛化性能。

Result: 实证分析显示婴儿视觉输入在各类别上呈现簇状相似分布；计算实验表明，模仿这种分布的训练数据能提高模型在小样本下对类别的泛化表现。

Conclusion: 论文结论是：婴儿日常视觉经验具有“块状相似性”结构（即大量高度相似的重复图像簇与少量多样化图像混合），这种自然的“块状性”可以帮助从有限样本中学习并推广物体类别；将这种结构模仿到机器学习中可以提高在小数据集上的泛化能力。

Abstract: Young children readily recognize and generalize visual objects labeled by
common nouns, suggesting that these basic level object categories may be given.
Yet if they are, how they arise remains unclear. We propose that the answer
lies in the statistics of infant daily life visual experiences. Whereas large
and diverse datasets typically support robust learning and generalization in
human and machine learning, infants achieve this generalization from limited
experiences. We suggest that the resolution of this apparent contradiction lies
in the visual diversity of daily life, repeated experiences with single object
instances. Analyzing egocentric images from 14 infants (aged 7 to 11 months) we
show that their everyday visual input exhibits a lumpy similarity structure,
with clusters of highly similar images interspersed with rarer, more variable
ones, across eight early-learned categories. Computational experiments show
that mimicking this structure in machines improves generalization from small
datasets in machine learning. The natural lumpiness of infant experience may
thus support early category learning and generalization and, more broadly,
offer principles for efficient learning across a variety of problems and kinds
of learners.

</details>


### [14] [SaLon3R: Structure-aware Long-term Generalizable 3D Reconstruction from Unposed Images](https://arxiv.org/abs/2510.15072)
*Jiaxin Guo,Tongfan Guan,Wenzhen Dong,Wenzhao Zheng,Wenting Wang,Yue Wang,Yeung Yam,Yun-Hui Liu*

Main category: cs.CV

TL;DR: 提出SaLon3R：通过显著性引导的高斯量化与3D点Transformer细化，在线压缩冗余并修正跨帧不一致，实现高效鲁棒的长期泛化3DGS重建。


<details>
  <summary>Details</summary>
Motivation: 现有泛化在线3DGS对每像素预测高斯并合并所有视图，導致长序列中大量冗余与几何不一致，影响效率与质量，需一种能在线压缩冗余并修正跨帧不一致的结构化方法。

Method: 先用3D重建骨干预测密集每像素高斯与显著性图；根据显著性优先将冗余高斯压缩为紧凑锚点原语；再用3D点Transformer学习三维结构先验，细化锚点属性与显著性并实现区域自适应高斯解码。全流程为单次前向，无需相机参数或测试时优化。

Result: 在超过50帧长序列上实现10+ FPS重建，去冗余率50%~90%，在新视图合成与深度估计任务上达到或超越现有最优方法，体现出更高效率、鲁棒性与泛化能力。

Conclusion: SaLon3R有效解决了长期序列3DGS冗余与几何不一致问题，通过锚点原语与可微显著性量化显著压缩冗余，并结合3D点Transformer细化属性与显著性，从而实现快速、在线、泛化的3D重建与渲染。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled generalizable,
on-the-fly reconstruction of sequential input views. However, existing methods
often predict per-pixel Gaussians and combine Gaussians from all views as the
scene representation, leading to substantial redundancies and geometric
inconsistencies in long-duration video sequences. To address this, we propose
SaLon3R, a novel framework for Structure-aware, Long-term 3DGS Reconstruction.
To our best knowledge, SaLon3R is the first online generalizable GS method
capable of reconstructing over 50 views in over 10 FPS, with 50% to 90%
redundancy removal. Our method introduces compact anchor primitives to
eliminate redundancy through differentiable saliency-aware Gaussian
quantization, coupled with a 3D Point Transformer that refines anchor
attributes and saliency to resolve cross-frame geometric and photometric
inconsistencies. Specifically, we first leverage a 3D reconstruction backbone
to predict dense per-pixel Gaussians and a saliency map encoding regional
geometric complexity. Redundant Gaussians are compressed into compact anchors
by prioritizing high-complexity regions. The 3D Point Transformer then learns
spatial structural priors in 3D space from training data to refine anchor
attributes and saliency, enabling regionally adaptive Gaussian decoding for
geometric fidelity. Without known camera parameters or test-time optimization,
our approach effectively resolves artifacts and prunes the redundant 3DGS in a
single feed-forward pass. Experiments on multiple datasets demonstrate our
state-of-the-art performance on both novel view synthesis and depth estimation,
demonstrating superior efficiency, robustness, and generalization ability for
long-term generalizable 3D reconstruction. Project Page:
https://wrld.github.io/SaLon3R/.

</details>


### [15] [TGT: Text-Grounded Trajectories for Locally Controlled Video Generation](https://arxiv.org/abs/2510.15104)
*Guofeng Zhang,Angtian Wang,Jacob Zhiyuan Fang,Liming Jiang,Haotian Yang,Bo Liu,Yiding Yang,Guang Chen,Longyin Wen,Alan Yuille,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出基于轨迹与局部文本配对的TGT框架，通过LACA与双重CFG以及大规模带注释视频数据，提升多对象场景下的视频生成可控性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频方法在视觉质量提升的同时，对场景中主体的构图与多对象精确控制能力有限，尤其在复杂与多对象场景中难以将轨迹与具体视觉实体一一对应。

Method: 提出Location-Aware Cross-Attention (LACA)融合轨迹与文本，并采用双重CFG分别调节局部与全局文本引导；构建数据处理管线，生成带局部描述的轨迹并标注200万高质量视频用于训练。

Result: 在视觉质量、文本对齐精度和运动可控性上均优于先前方法；能够用点轨迹作为直观的运动控制手柄，支持按轨迹—文本对控制外观与运动。

Conclusion: TGT通过将轨迹与局部文本描述配对，显著提升了视频生成中对主体构图和运动的可控性。

Abstract: Text-to-video generation has advanced rapidly in visual fidelity, whereas
standard methods still have limited ability to control the subject composition
of generated scenes. Prior work shows that adding localized text control
signals, such as bounding boxes or segmentation masks, can help. However, these
methods struggle in complex scenarios and degrade in multi-object settings,
offering limited precision and lacking a clear correspondence between
individual trajectories and visual entities as the number of controllable
objects increases. We introduce Text-Grounded Trajectories (TGT), a framework
that conditions video generation on trajectories paired with localized text
descriptions. We propose Location-Aware Cross-Attention (LACA) to integrate
these signals and adopt a dual-CFG scheme to separately modulate local and
global text guidance. In addition, we develop a data processing pipeline that
produces trajectories with localized descriptions of tracked entities, and we
annotate two million high quality video clips to train TGT. Together, these
components enable TGT to use point trajectories as intuitive motion handles,
pairing each trajectory with text to control both appearance and motion.
Extensive experiments show that TGT achieves higher visual quality, more
accurate text alignment, and improved motion controllability compared with
prior approaches. Website: https://textgroundedtraj.github.io.

</details>


### [16] [Deep generative priors for 3D brain analysis](https://arxiv.org/abs/2510.15119)
*Ana Lawry Aguila,Dina Zemlyanker,You Cheng,Sudeshna Das,Daniel C. Alexander,Oula Puonti,Annabel Sorby-Adams,W. Taylor Kimberly,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 将大规模脑MRI训练的score-based扩散模型作为贝叶斯先验，配合灵活正向模型，解决多种脑MRI逆问题并提升解剖学保真度，无需成对数据，达到了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 经典贝叶斯成像依赖的数学先验难以捕捉脑解剖结构的复杂性，而纯数据驱动模型虽强但缺少将成像物理知识融入的能力；因此希望将扩散生成模型的强大表征作为先验，结合成像物理以解决多种医疗成像逆问题且无需配对训练数据。

Method: 训练基于评分匹度（score-based）的扩散先验于大量多样脑MRI数据，并结合灵活的正向观测模型以表达不同成像任务；在推断时以贝叶斯逆问题框架利用扩散先验进行采样/优化，并可用于细化现有深学习方法的输出。

Result: 在异构临床与科研MRI数据上，方法在重建质量和解剖一致性上达到了或超过最先进水平，且无需配对训练集；还能提升现有深学习方法结果的解剖保真度。

Conclusion: 本文提出将扩散模型用作脑MRI问题的通用先验，能在多种成像逆问题（超分辨、偏场校正、缺损修补及其组合）中替代传统数学先验，提高解的解剖学保真度和鲁棒性。

Abstract: Diffusion models have recently emerged as powerful generative models in
medical imaging. However, it remains a major challenge to combine these
data-driven models with domain knowledge to guide brain imaging problems. In
neuroimaging, Bayesian inverse problems have long provided a successful
framework for inference tasks, where incorporating domain knowledge of the
imaging process enables robust performance without requiring extensive training
data. However, the anatomical modeling component of these approaches typically
relies on classical mathematical priors that often fail to capture the complex
structure of brain anatomy. In this work, we present the first general-purpose
application of diffusion models as priors for solving a wide range of medical
imaging inverse problems. Our approach leverages a score-based diffusion prior
trained extensively on diverse brain MRI data, paired with flexible forward
models that capture common image processing tasks such as super-resolution,
bias field correction, inpainting, and combinations thereof. We further
demonstrate how our framework can refine outputs from existing deep learning
methods to improve anatomical fidelity. Experiments on heterogeneous clinical
and research MRI data show that our method achieves state-of-the-art
performance producing consistent, high-quality solutions without requiring
paired training datasets. These results highlight the potential of diffusion
priors as versatile tools for brain MRI analysis.

</details>


### [17] [Fourier Transform Multiple Instance Learning for Whole Slide Image Classification](https://arxiv.org/abs/2510.15138)
*Anthony Bilic,Guangyu Sun,Ming Li,Md Sanzid Bin Hossain,Yu Tian,Wei Zhang,Laura Brattain,Dexter Hadley,Chen Chen*

Main category: cs.CV

TL;DR: 提出FFT-MIL：用FFT提取低频全局信息并通过FFT-Block处理后与空间补丁特征融合，能显著提升多种MIL方法在WSI分类任务上的表现。


<details>
  <summary>Details</summary>
Motivation: WSI过大导致传统基于补丁的MIL难以建模全局依赖，缺乏对粗尺度结构的感知影响诊断性能，故引入频域表征以提供紧凑的全局上下文。

Method: 使用FFT从WSI提取低频裁剪图像，经过包含卷积层和Min-Max归一化的模块化FFT-Block以降低频域数据方差，得到全局频率特征并通过轻量级融合策略与空间补丁特征集成，兼容多种MIL架构。

Result: 在BRACS、LUAD和IMP三个公开数据集上，将FFT-Block集成到六种先进MIL方法中，平均提高macro F1 3.51%和AUC 1.51%，表明频域学习在提升WSI分类的准确性与可扩展性方面稳定有效。

Conclusion: FFT-MIL通过引入频域分支有效补充了局部空间补丁的全局上下文，从而提升WSI分类性能。

Abstract: Whole Slide Image (WSI) classification relies on Multiple Instance Learning
(MIL) with spatial patch features, yet existing methods struggle to capture
global dependencies due to the immense size of WSIs and the local nature of
patch embeddings. This limitation hinders the modeling of coarse structures
essential for robust diagnostic prediction.
  We propose Fourier Transform Multiple Instance Learning (FFT-MIL), a
framework that augments MIL with a frequency-domain branch to provide compact
global context. Low-frequency crops are extracted from WSIs via the Fast
Fourier Transform and processed through a modular FFT-Block composed of
convolutional layers and Min-Max normalization to mitigate the high variance of
frequency data. The learned global frequency feature is fused with spatial
patch features through lightweight integration strategies, enabling
compatibility with diverse MIL architectures.
  FFT-MIL was evaluated across six state-of-the-art MIL methods on three public
datasets (BRACS, LUAD, and IMP). Integration of the FFT-Block improved macro F1
scores by an average of 3.51% and AUC by 1.51%, demonstrating consistent gains
across architectures and datasets. These results establish frequency-domain
learning as an effective and efficient mechanism for capturing global
dependencies in WSI classification, complementing spatial features and
advancing the scalability and accuracy of MIL-based computational pathology.

</details>


### [18] [XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models](https://arxiv.org/abs/2510.15148)
*Xingrui Wang,Jiang Liu,Chao Huang,Xiaodong Yu,Ze Wang,Ximeng Sun,Jialian Wu,Alan Yuille,Emad Barsoum,Zicheng Liu*

Main category: cs.CV

TL;DR: XModBench是一个大规模三模态基准，用于测量OLLM的跨模态一致性。实验表明现有模型在模态不变推理上还远未达标，存在显著模态差异和方向性不平衡。


<details>
  <summary>Details</summary>
Motivation: 现有评测偏重跨模态问答能力，无法衡量OLLM是否实现模态不变推理或是否存在模态特定偏差，需要一个系统诊断工具。

Method: 提出XModBench基准，包含60,828道三模态（文本、视觉、音频）多项选择题，覆盖五类任务和所有六种模态组合，用于诊断模态不变性、模态差异和方向性不平衡。

Result: Gemini 2.5 Pro在空间与时间推理上表现弱（<60%准确率），音频传达同一语义时性能显著下降，且以视觉作为上下文时一致性低于以文本作为上下文，显示模态差异和方向性不平衡。

Conclusion: 当前最强的OLLM在跨模态一致性上仍有明显不足，不能实现真正的模态不变推理。

Abstract: Omni-modal large language models (OLLMs) aim to unify audio, vision, and text
understanding within a single framework. While existing benchmarks primarily
evaluate general cross-modal question-answering ability, it remains unclear
whether OLLMs achieve modality-invariant reasoning or exhibit modality-specific
biases. We introduce XModBench, a large-scale tri-modal benchmark explicitly
designed to measure cross-modal consistency. XModBench comprises 60,828
multiple-choice questions spanning five task families and systematically covers
all six modality compositions in question-answer pairs, enabling fine-grained
diagnosis of an OLLM's modality-invariant reasoning, modality disparity, and
directional imbalance. Experiments show that even the strongest model, Gemini
2.5 Pro, (i) struggles with spatial and temporal reasoning, achieving less than
60% accuracy, (ii) reveals persistent modality disparities, with performance
dropping substantially when the same semantic content is conveyed through audio
rather than text, and (iii) shows systematic directional imbalance, exhibiting
lower consistency when vision serves as context compared to text. These
findings indicate that current OLLMs remain far from truly modality-invariant
reasoning and position XModBench as a fundamental diagnostic tool for
evaluating and improving cross-modal competence. All data and evaluation tools
will be available at https://xingruiwang.github.io/projects/XModBench/.

</details>


### [19] [Train a Unified Multimodal Data Quality Classifier with Synthetic Data](https://arxiv.org/abs/2510.15162)
*Weizhi Wang,Rongmei Lin,Shiyang Li,Colin Lockard,Ritesh Sarkhel,Sanket Lokegaonkar,Jingbo Shang,Xifeng Yan,Nasser Zalmout,Xian Li*

Main category: cs.CV

TL;DR: 提出半合成数据驱动的统一多模态数据质量分类器UniFilter，用于筛选高质量图文及图文交错文档数据，从而提升MLLMs的预训练与下游表现；并公开资源。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM持续预训练使用混合的图文与交错文档数据，但缺乏对图文交错文档的高质量数据过滤方法，作者希望构建一个统一的过滤器改善预训练数据质量。

Method: 通过半合成数据生成方法，利用原始图像合成对应文本并构建四个质量等级的样本-分数对，训练UniFilter；将其用于筛选DataComp和OBELICS数据集，得到高质量子集并用于MLLM预训练。

Result: 用UniFilter筛选的数据训练的MLLM在零样本推理、上下文学习以及视觉监督微调后的基准测试上都有显著改进，作者并公开了合成训练数据、模型checkpoint和OBELICS-HQ子集。

Conclusion: 本论文提出了UniFilter——一个用于筛选高质量图文和图文交错文档数据的统一多模态数据质量分类器，能有效提升后续MLLMs的预训练效果。

Abstract: The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

</details>


### [20] [Hyperparameter Optimization and Reproducibility in Deep Learning Model Training](https://arxiv.org/abs/2510.15164)
*Usman Afzaal,Ziyu Su,Usama Sajjad,Hao Lu,Mostafa Rezapour,Metin Nafi Gurcan,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 在组织病理学CLIP训练中，选择合适的RandomResizedCrop(0.7-0.8)、避免在分布式训练中做本地损失、并避免过低学习率(<5e-5)能显著提升可重复性；LC25000(Colon)是推荐的基准。


<details>
  <summary>Details</summary>
Motivation: 识别影响计算病理学基础模型训练可重复性的关键因素，包括软件随机性、硬件非确定性和不一致的超参数报告，以便提出改善可重复性的实用规则。

Method: 使用CLIP模型在QUILT-1M数据集上训练，并在PatchCamelyon、LC25000-Lung、LC25000-Colon三个下游数据集上系统性地改变超参数和增强策略以评估可重复性；比较不同RandomResizedCrop、学习率和分布式训练设置对性能稳定性的影响。

Result: 发现RandomResizedCrop在0.7-0.8范围性能最佳；分布式训练若不进行本地损失计算能提升稳定性；学习率低于5.0e-5会持续降低表现；LC25000(Colon)提供最稳定的可重复性结果。此外，报告强调可重复性需透明记录与合理配置并重，并给出实践规则。

Conclusion: 该论文结论是：在组织病理学基础模型训练中，可重复性受训练配置显著影响，特定数据增强（RandomResizedCrop在0.7-0.8）和分布式训练策略（不在本地计算损失）能提高稳定性，而过低学习率（<5.0e-5）会损害性能；LC25000（Colon）是最具可重复性的下游基准。

Abstract: Reproducibility remains a critical challenge in foundation model training for
histopathology, often hindered by software randomness, hardware
non-determinism, and inconsistent hyperparameter reporting. To investigate
these issues, we trained a CLIP model on the QUILT-1M dataset and
systematically evaluated the impact of different hyperparameter settings and
augmentation strategies across three downstream histopathology datasets
(PatchCamelyon, LC25000-Lung, and LC25000-Colon). Despite variability across
runs, we identified clear trends: RandomResizedCrop values of 0.7-0.8
outperformed more aggressive (0.6) or conservative (0.9) settings, distributed
training without local loss improved stability, and learning rates below 5.0e-5
consistently degraded performance across all datasets. The LC25000 (Colon)
dataset consistently provided the most reproducible benchmark. These findings
highlight that reproducibility in computational pathology depends not only on
transparent documentation but also on carefully chosen experimental
configurations, and we provide practical rules to guide future efforts in
developing reproducible foundation models for digital pathology.

</details>


### [21] [Salient Concept-Aware Generative Data Augmentation](https://arxiv.org/abs/2510.15194)
*Tianchen Zhao,Xuanbai Chen,Zhihua Li,Jun Fang,Dongsheng An,Xiang Xu,Zhuowen Tu,Yifan Xing*

Main category: cs.CV

TL;DR: 提出显著概念感知的图像嵌入引导个性化生成，减少不相关细节干扰，提升生成样本的判别性与多样性，从而改进增强效果并提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像+文本条件生成增强在保留关键信息与响应多样化文本提示之间存在冲突，因合成过程中表示被输入图像的非必要属性（如环境背景）纠缠，导致生成图像要么忠实但缺乏多样性，要么多样但丢失关键信息。

Method: 构建个性化图像生成框架：训练或使用一个显著概念感知的图像嵌入，将输入图像中与类别判别相关的特征突出，抑制环境等非必要属性。然后以该嵌入作为条件，与文本提示联合指导生成模型产生图像，保持类别判别性且允许受控变化。

Result: 在八个细粒度视觉数据集上优于现有增强方法；在常规设置和长尾设置下，平均分类准确率分别提升约0.73%和6.5%。

Conclusion: 该论文提出通过显著概念感知的图像嵌入模型，在生成式数据增强中抑制不相关视觉细节对合成过程的干扰，从而在保持图像细节和遵循文本提示之间取得更好平衡。

Abstract: Recent generative data augmentation methods conditioned on both image and
text prompts struggle to balance between fidelity and diversity, as it is
challenging to preserve essential image details while aligning with varied text
prompts. This challenge arises because representations in the synthesis process
often become entangled with non-essential input image attributes such as
environmental contexts, creating conflicts with text prompts intended to modify
these elements. To address this, we propose a personalized image generation
framework that uses a salient concept-aware image embedding model to reduce the
influence of irrelevant visual details during the synthesis process, thereby
maintaining intuitive alignment between image and text inputs. By generating
images that better preserve class-discriminative features with additional
controlled variations, our framework effectively enhances the diversity of
training datasets and thereby improves the robustness of downstream models. Our
approach demonstrates superior performance across eight fine-grained vision
datasets, outperforming state-of-the-art augmentation methods with averaged
classification accuracy improvements by 0.73% and 6.5% under conventional and
long-tail settings, respectively.

</details>


### [22] [CARDIUM: Congenital Anomaly Recognition with Diagnostic Images and Unified Medical records](https://arxiv.org/abs/2510.15208)
*Daniela Vega,Hannah V. Ceballos,Javier S. Vera,Santiago Rodriguez,Alejandra Perez,Angela Castillo,Maria Escobar,Dario Londoño,Luis A. Sarmiento,Camila I. Castro,Nadiezhda Rodriguez,Juan C. Briceño,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 作者发布了首个包含胎儿影像与母体临床信息的公开多模态CHD数据集CARDIUM，并提出基于交叉注意力的多模态Transformer，显著提升了产前CHD检测性能（F1≈79.8%），并将数据与代码开源。


<details>
  <summary>Details</summary>
Motivation: CHD病例稀少导致高质量诊断数据缺乏，现有研究缺少将影像与临床记录整合的公开资源，阻碍AI在产前CHD检测中的发展。

Method: 构建多模态数据集（胎儿超声、心脏回声图像与母体临床表格数据），并设计一种带交叉注意力的多模态Transformer用于融合图像与表格特征，比较单模态与多模态性能。

Result: 多模态模型在CARDIUM上比仅图像方法提升11%、比仅表格方法提升50%，在数据集上取得F1=79.8±4.8%。同时公开数据集与代码以促进后续研究。

Conclusion: 提出并公开了首个多模态胎儿先天性心脏病（CHD）数据集CARDIUM，包含超声与孕母临床记录，并提出跨注意力多模态Transformer模型，有效提升CHD检测性能。

Abstract: Prenatal diagnosis of Congenital Heart Diseases (CHDs) holds great potential
for Artificial Intelligence (AI)-driven solutions. However, collecting
high-quality diagnostic data remains difficult due to the rarity of these
conditions, resulting in imbalanced and low-quality datasets that hinder model
performance. Moreover, no public efforts have been made to integrate multiple
sources of information, such as imaging and clinical data, further limiting the
ability of AI models to support and enhance clinical decision-making. To
overcome these challenges, we introduce the Congenital Anomaly Recognition with
Diagnostic Images and Unified Medical records (CARDIUM) dataset, the first
publicly available multimodal dataset consolidating fetal ultrasound and
echocardiographic images along with maternal clinical records for prenatal CHD
detection. Furthermore, we propose a robust multimodal transformer architecture
that incorporates a cross-attention mechanism to fuse feature representations
from image and tabular data, improving CHD detection by 11% and 50% over image
and tabular single-modality approaches, respectively, and achieving an F1 score
of 79.8 $\pm$ 4.8% in the CARDIUM dataset. We will publicly release our dataset
and code to encourage further research on this unexplored field. Our dataset
and code are available at https://github.com/BCVUniandes/Cardium, and at the
project website https://bcv-uniandes.github.io/CardiumPage/

</details>


### [23] [The Face of Persuasion: Analyzing Bias and Generating Culture-Aware Ads](https://arxiv.org/abs/2510.15240)
*Aysan Aghazadeh,Adriana Kovashka*

Main category: cs.CV

TL;DR: 文本到图像广告生成可放大人口偏见且导致不同性別/種族的說服力差異，提示工程可部分定向國家風格但不能完全消除偏見。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型在商业广告中的应用增多，研究人员关注这些模型是否继承或放大社会偏见，可能导致针对特定群体的刻板化或不公平的广告投放效果，进而影响不同群体的受众体验与社会公正。

Method: 作者使用文本到图像生成模型（如GAN或Diffusion模型）生成包含不同性别/种族人物的广告图像，针对多种广告主题统计分析生成图像的人口分布，并设计对比实验让评估模型对仅在性别/种族上差异化的广告进行说服力评分。同时尝试通过提示工程或地理标记来定向生成特定国家风格的广告。

Result: 实验证明：1）不同广告主题下生成的图像在人口统计学分布上存在偏倚；2）同一广告在只改变性别/种族时，其被评为更具说服力的概率有显著差异，表明模型可能强化某些群体的视觉吸引力或信任感；3）基于提示的定向技术能够在一定程度上生成更符合特定国家风格的广告，但仍存在局限性。

Conclusion: 该论文表明文本到图像生成模型在广告制作中会放大并反映社会人口统计偏见，且不同性别/种族的呈现会影响模型判定的说服力。通过控制样本只改变性别/种族，作者发现相同广告内容在不同群体间的说服力评分存在显著差异。

Abstract: Text-to-image models are appealing for customizing visual advertisements and
targeting specific populations. We investigate this potential by examining the
demographic bias within ads for different ad topics, and the disparate level of
persuasiveness (judged by models) of ads that are identical except for
gender/race of the people portrayed. We also experiment with a technique to
target ads for specific countries. The code is available at
https://github.com/aysanaghazadeh/FaceOfPersuasion

</details>


### [24] [DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion](https://arxiv.org/abs/2510.15264)
*Weijie Wang,Jiagang Zhu,Zeyu Zhang,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Chaojun Ni,Haoxiao Wang,Guan Huang,Xinze Chen,Yukun Zhou,Wenkang Qin,Duochao Shi,Haoyun Li,Guanghong Jia,Jiwen Lu*

Main category: cs.CV

TL;DR: DriveGen3D 通过 FastDrive-DiT 与 FastRecon3D 的协同，首次实现了高分辨率、长时序、可控的驾驶视频生成与快速动态 3D 重建，兼顾质量、速度与参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么在长期时间生成上计算开销大，要么只做长视频但无 3D 表示，或仅限静态单场景重建；需要一个既能高效生成长时视频又能构建一致动态 3D 场景的统一方案。

Method: 框架由两部分组成：FastDrive-DiT（基于视频扩散的 Transformer，用于在文本和 BEV 布局引导下高分辨率、时间一致性的视频合成）和 FastRecon3D（前向重建模块，快速生成时序 3D Gaussian 表示以保证时空一致性）。二者协同实现实时长时视频（高达 424×800 分辨率、12 FPS）与动态三维场景生成。

Result: 系统在新视角合成上取得 SSIM=0.811、PSNR=22.84，并实现参数高效的实时长视频与动态 3D 场景生成。

Conclusion: DriveGen3D 提出了一个高效、可控的动态 3D 行驶场景生成框架，结合快速视频扩散和快速时空三维重建，弥补了现有方法在长期生成、3D 表示和时空一致性方面的不足。

Abstract: We present DriveGen3D, a novel framework for generating high-quality and
highly controllable dynamic 3D driving scenes that addresses critical
limitations in existing methodologies. Current approaches to driving scene
synthesis either suffer from prohibitive computational demands for extended
temporal generation, focus exclusively on prolonged video synthesis without 3D
representation, or restrict themselves to static single-scene reconstruction.
Our work bridges this methodological gap by integrating accelerated long-term
video generation with large-scale dynamic scene reconstruction through
multimodal conditional control. DriveGen3D introduces a unified pipeline
consisting of two specialized components: FastDrive-DiT, an efficient video
diffusion transformer for high-resolution, temporally coherent video synthesis
under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a
feed-forward reconstruction module that rapidly builds 3D Gaussian
representations across time, ensuring spatial-temporal consistency. Together,
these components enable real-time generation of extended driving videos (up to
$424\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM
of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining
parameter efficiency.

</details>


### [25] [CuSfM: CUDA-Accelerated Structure-from-Motion](https://arxiv.org/abs/2510.15271)
*Jingrui Yu,Jun Liu,Kefei Ren,Joydeep Biswas,Rurui Ye,Keqiang Wu,Chirag Majithia,Di Zeng*

Main category: cs.CV

TL;DR: cuSfM是一个基于CUDA的离线SfM系统，利用GPU并行化高精度特征提取与非冗余数据关联，实现更快且更精确的相机位姿估计与全局一致地图，实验优于COLMAP，并以PyCuSfM开源。


<details>
  <summary>Details</summary>
Motivation: 在自动导航、机器人感知和虚拟仿真中，稠密重建依赖高效且精确的相机位姿估计。现有方法（如COLMAP）在速度或精度上存在权衡，尤其当使用更精确的特征提取器时计算成本高。论文旨在利用GPU并行能力，在不牺牲精度的前提下提升处理速度，并生成全局一致且非冗余的数据关联以提高离线SfM的性能。

Method: 通过将计算密集型特征提取与匹配流程移植到GPU并行计算框架（CUDA），构建端到端的离线SfM流水线。核心包括并行化特征提取、去冗余的数据关联策略、基于这些高质量关联进行的全局位姿优化与地图生成，以及支持先验地图定位和外参精化的模块。实现上以Python包装器（PyCuSfM）对外提供接口并开源。

Result: 实验表明cuSfM在多种测试场景下相比COLMAP显著提高了精度和处理速度，同时保持高精度和全局一致性，满足离线SfM应用需求。作者并发布了开源实现PyCuSfM以便复现与推广。

Conclusion: 该论文提出了cuSfM，一种基于CUDA加速的离线Structure-from-Motion系统，通过GPU并行化使用高精度但计算密集的特征提取器，实现了全面且非冗余的数据关联，从而提升相机位姿估计精度和地图全局一致性。系统支持位姿优化、建图、先验地图定位和外参精化，专为离线处理场景设计以充分利用计算资源。作者公开了PyCuSfM实现以促进研究与应用。

Abstract: Efficient and accurate camera pose estimation forms the foundational
requirement for dense reconstruction in autonomous navigation, robotic
perception, and virtual simulation systems. This paper addresses the challenge
via cuSfM, a CUDA-accelerated offline Structure-from-Motion system that
leverages GPU parallelization to efficiently employ computationally intensive
yet highly accurate feature extractors, generating comprehensive and
non-redundant data associations for precise camera pose estimation and globally
consistent mapping. The system supports pose optimization, mapping, prior-map
localization, and extrinsic refinement. It is designed for offline processing,
where computational resources can be fully utilized to maximize accuracy.
Experimental results demonstrate that cuSfM achieves significantly improved
accuracy and processing speed compared to the widely used COLMAP method across
various testing scenarios, while maintaining the high precision and global
consistency essential for offline SfM applications. The system is released as
an open-source Python wrapper implementation, PyCuSfM, available at
https://github.com/nvidia-isaac/pyCuSFM, to facilitate research and
applications in computer vision and robotics.

</details>


### [26] [Post-Processing Methods for Improving Accuracy in MRI Inpainting](https://arxiv.org/abs/2510.15282)
*Nishad Kulkarni,Krithika Iyer,Austin Tapp,Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,María J. Ledesma-Carbayo,Syed Muhammad Anwar,Marius George Linguraru*

Main category: cs.CV

TL;DR: 针对肿瘤区域MRI修复，作者用模型集成+后处理+轻量U-Net增强构建流水线，弥补单模型瓶颈，提升解剖合理性与视觉效果，并提供可复现的Docker实现。


<details>
  <summary>Details</summary>
Motivation: 现有自动化MRI分析工具多为健康解剖优化，面对大病灶（如肿瘤）常失败；通过在病灶区域合成健康组织，可使通用工具可靠适用，推动临床部署。

Method: 系统评估现有顶尖inpainting模型，观察单模型性能饱和；提出模型集成（ensemble）并结合中值滤波、直方图匹配、像素平均等高效后处理；最后加入轻量U-Net对解剖结构进行细化。

Result: 综合评估显示所提流水线在解剖合理性和视觉保真度上优于单一模型，提高了准确性与稳健性；并提出资源友好的可访问实现（提供Docker镜像）。

Conclusion: 作者提出将多个现有MRI病灶修复（inpainting）模型与轻量后处理和U-Net增强相结合的流水线，以提高修复区域的解剖合理性与视觉逼真度，相较单一基线模型表现更好。

Abstract: Magnetic Resonance Imaging (MRI) is the primary imaging modality used in the
diagnosis, assessment, and treatment planning for brain pathologies. However,
most automated MRI analysis tools, such as segmentation and registration
pipelines, are optimized for healthy anatomies and often fail when confronted
with large lesions such as tumors. To overcome this, image inpainting
techniques aim to locally synthesize healthy brain tissues in tumor regions,
enabling the reliable application of general-purpose tools. In this work, we
systematically evaluate state-of-the-art inpainting models and observe a
saturation in their standalone performance. In response, we introduce a
methodology combining model ensembling with efficient post-processing
strategies such as median filtering, histogram matching, and pixel averaging.
Further anatomical refinement is achieved via a lightweight U-Net enhancement
stage. Comprehensive evaluation demonstrates that our proposed pipeline
improves the anatomical plausibility and visual fidelity of inpainted regions,
yielding higher accuracy and more robust outcomes than individual baseline
models. By combining established models with targeted post-processing, we
achieve improved and more accessible inpainting outcomes, supporting broader
clinical deployment and sustainable, resource-conscious research. Our 2025
BraTS inpainting docker is available at
https://hub.docker.com/layers/aparida12/brats2025/inpt.

</details>


### [27] [QCFace: Image Quality Control for boosting Face Representation & Recognition](https://arxiv.org/abs/2510.15289)
*Duc-Phuong Doan-Ngo,Thanh-Dang Diep,Thanh Nguyen-Duc,Thanh-Sach LE,Nam Thoai*

Main category: cs.CV

TL;DR: 提出QCFace：一种基于硬边界的损失，通过解耦特征方向与幅值，实现可识别性与身份表征分离，提升低质量人脸的识别性能并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前深度人脸识别方法中，可识别性只通过软边界约束部分编码，导致低质量/模糊人脸的表征质量和判别性不足；此外特征方向与幅值的梯度重叠导致优化不稳定与表征纠缠，影响泛化。需要一种能显式编码可识别性并与身份表征解耦的方法。

Method: 提出Quality Control Face (QCFace)，采用硬边界策略设计新的基于硬边界的损失函数；引入指导因子用于超球面规划，解耦可识别性与身份特征的方向和幅值，避免互相重叠的梯度干扰，从而稳定优化过程并输出可量化的可识别性编码。

Result: 大量实验证明QCFace能稳健且可量化地编码可识别性，并在验证和识别基准上达到或超过现有基于可识别性的损失方法的最先进性能。

Conclusion: QCFace通过引入硬边界（hard margin）策略，成功将可识别性（recognizability）与身份表征分离，并在损失函数层面以引导因子规划超球面，从而同时优化识别性能与明确的可识别性表示。该方法解决了现有方法中软边界限制及特征方向与幅值梯度重叠带来的训练干扰问题，提升了表征质量与判别性，特别是在低质量或模糊人脸上表现更优。

Abstract: Recognizability, a key perceptual factor in human face processing, strongly
affects the performance of face recognition (FR) systems in both verification
and identification tasks. Effectively using recognizability to enhance feature
representation remains challenging. In deep FR, the loss function plays a
crucial role in shaping how features are embedded. However, current methods
have two main drawbacks: (i) recognizability is only partially captured through
soft margin constraints, resulting in weaker quality representation and lower
discrimination, especially for low-quality or ambiguous faces; (ii) mutual
overlapping gradients between feature direction and magnitude introduce
undesirable interactions during optimization, causing instability and confusion
in hypersphere planning, which may result in poor generalization, and entangled
representations where recognizability and identity are not cleanly separated.
To address these issues, we introduce a hard margin strategy - Quality Control
Face (QCFace), which overcomes the mutual overlapping gradient problem and
enables the clear decoupling of recognizability from identity representation.
Based on this strategy, a novel hard-margin-based loss function employs a
guidance factor for hypersphere planning, simultaneously optimizing for
recognition ability and explicit recognizability representation. Extensive
experiments confirm that QCFace not only provides robust and quantifiable
recognizability encoding but also achieves state-of-the-art performance in both
verification and identification benchmarks compared to existing
recognizability-based losses.

</details>


### [28] [Hyperbolic Structured Classification for Robust Single Positive Multi-label Learning](https://arxiv.org/abs/2510.15296)
*Yiming Lin,Shang Wang,Junkai Zhou,Qiufeng Wang,Xiao-Bo Jin,Kaizhu Huang*

Main category: cs.CV

TL;DR: 将标签建模为双曲球，借助温度自适应判别与双势井正则化，同时支持包含/重叠/分离三类关系，有力提升SPMLL的表示与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过点或向量的距离隐式建模标签关系，无法显式区分包含、重叠与分离等多种语义关系；SPMLL中标签不完备标注使得学习复杂关系更困难，需更强的几何表征。

Method: 构建了首个基于双曲空间的球形分类器：将每个标签表示为超球（具有中心与半径），并设计了温度自适应的分类策略以及受物理双势井启发的正则项，推动球体形成有意义的包含、重叠与分离配置。

Result: 在MS-COCO、PASCAL VOC、NUS-WIDE、CUB-200-2011等四个基准上实验表明方法在性能上具有竞争力，并在可解释性上优于现有方法；统计分析显示学习到的嵌入与真实共现模式高度相关。

Conclusion: 该论文提出在超曲率几何空间中用球体表示标签，显著提高了在单一正标注多标签学习（SPMLL）问题下对标签关系的建模能力与可解释性。

Abstract: Single Positive Multi-Label Learning (SPMLL) addresses the challenging
scenario where each training sample is annotated with only one positive label
despite potentially belonging to multiple categories, making it difficult to
capture complex label relationships and hierarchical structures. While existing
methods implicitly model label relationships through distance-based similarity,
lacking explicit geometric definitions for different relationship types. To
address these limitations, we propose the first hyperbolic classification
framework for SPMLL that represents each label as a hyperbolic ball rather than
a point or vector, enabling rich inter-label relationship modeling through
geometric ball interactions. Our ball-based approach naturally captures
multiple relationship types simultaneously: inclusion for hierarchical
structures, overlap for co-occurrence patterns, and separation for semantic
independence. Further, we introduce two key component innovations: a
temperature-adaptive hyperbolic ball classifier and a physics-inspired
double-well regularization that guides balls toward meaningful configurations.
To validate our approach, extensive experiments on four benchmark datasets
(MS-COCO, PASCAL VOC, NUS-WIDE, CUB-200-2011) demonstrate competitive
performance with superior interpretability compared to existing methods.
Furthermore, statistical analysis reveals strong correlation between learned
embeddings and real-world co-occurrence patterns, establishing hyperbolic
geometry as a more robust paradigm for structured classification under
incomplete supervision.

</details>


### [29] [Latent Diffusion Model without Variational Autoencoder](https://arxiv.org/abs/2510.15301)
*Minglei Shi,Haolin Wang,Wenzhao Zheng,Ziyang Yuan,Xiaoshi Wu,Xintao Wang,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Replace VAE latents with fixed self-supervised features (DINO) plus a small residual branch; train diffusion on this space for faster, better, and more transferable image generation.


<details>
  <summary>Details</summary>
Motivation: VAEs produce latents lacking clear semantic separation and discriminative structure, hindering training efficiency, sampling speed, and transferability; self-supervised features offer better semantics for generative modeling.

Method: Use frozen DINO features as primary latent space, add a lightweight residual branch for high-frequency detail, and train diffusion model directly on this combined latent space (no VAE).

Result: SVG achieves faster diffusion training, few-step sampling, improved generative quality, and retains semantic/discriminative capabilities for downstream tasks compared to VAE-based latent diffusion.

Conclusion: Paper concludes that replacing VAE latents with semantically structured self-supervised features enables more efficient, faster, and higher-quality latent diffusion models; SVG preserves discriminative properties and supports downstream tasks.

Abstract: Recent progress in diffusion-based visual generation has largely relied on
latent diffusion models with variational autoencoders (VAEs). While effective
for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited
training efficiency, slow inference, and poor transferability to broader vision
tasks. These issues stem from a key limitation of VAE latent spaces: the lack
of clear semantic separation and strong discriminative structure. Our analysis
confirms that these properties are crucial not only for perception and
understanding tasks, but also for the stable and efficient training of latent
diffusion models. Motivated by this insight, we introduce SVG, a novel latent
diffusion model without variational autoencoders, which leverages
self-supervised representations for visual generation. SVG constructs a feature
space with clear semantic discriminability by leveraging frozen DINO features,
while a lightweight residual branch captures fine-grained details for
high-fidelity reconstruction. Diffusion models are trained directly on this
semantically structured latent space to facilitate more efficient learning. As
a result, SVG enables accelerated diffusion training, supports few-step
sampling, and improves generative quality. Experimental results further show
that SVG preserves the semantic and discriminative capabilities of the
underlying self-supervised representations, providing a principled pathway
toward task-general, high-quality visual representations.

</details>


### [30] [Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](https://arxiv.org/abs/2510.15304)
*Fei Wang,Li Shen,Liang Ding,Chao Xue,Ye Liu,Changxing Ding*

Main category: cs.CV

TL;DR: CoMe通过通道敏感度选择、拼接式层合并和层级蒸馏，改进了层级结构化剪枝，能在大幅压缩参数的同时较好保留模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有层级结构化剪枝方法忽视保留被剪掉部分的能力，且存在直接移除层导致性能下降、线性权重合并表现差以及缺乏有效恢复机制等问题。

Method: 提出渐进层剪枝框架、基于拼接的层融合技术和分层蒸馏的后训练恢复：利用通道敏感度度量进行细粒度通道选择，基于拼接的层合并将相邻层的重要通道融合，最后用剪枝过程中建立的层对应关系进行层级蒸馏知识迁移。

Result: 在七个基准上达到SOTA；在对LLaMA-2-7b剪枝30%参数时，保留了83%的平均精度。

Conclusion: 本文通过CoMe方法在结构化按层剪枝中保持被剪枝部分能力，实现了在剪枝30%参数时保留较高精度，表明其在压缩大模型方面有效。

Abstract: Large Language Models excel at natural language processing tasks, but their
massive size leads to high computational and storage demands. Recent works have
sought to reduce their model size through layer-wise structured pruning.
However, they tend to ignore retaining the capabilities in the pruned part. In
this work, we re-examine structured pruning paradigms and uncover several key
limitations: 1) notable performance degradation due to direct layer removal, 2)
incompetent linear weight layer aggregation, and 3) the lack of effective
post-training recovery mechanisms. To address these limitations, we propose
CoMe, including a progressive layer pruning framework with a
Concatenation-based Merging technology and a hierarchical distillation
post-training process. Specifically, we introduce a channel sensitivity metric
that utilizes activation intensity and weight norms for fine-grained channel
selection. Subsequently, we employ a concatenation-based layer merging method
to fuse the most critical channels across adjacent layers, enabling progressive
model size reduction. Finally, we propose a hierarchical distillation protocol
that leverages the correspondences between the original and pruned model layers
established during pruning, thereby enabling efficient knowledge transfer.
Experiments on seven benchmarks show that CoMe achieves state-of-the-art
performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model
retains 83% of its original average accuracy. Our code is available at
https://github.com/MPI-Lab/CoMe.

</details>


### [31] [Proto-Former: Unified Facial Landmark Detection by Prototype Transformer](https://arxiv.org/abs/2510.15338)
*Shengkai Hu,Haozhe Qi,Jun Wan,Jiaxing Huang,Lefei Zhang,Hang Sun,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出Proto-Former，通过原型感知的编码器-解码器和PA损失，实现跨数据集的统一面部关键点检测，解决专家寻址不稳定和梯度冲突问题，提升泛化与精度，并在基准数据集上超过SOTA。


<details>
  <summary>Details</summary>
Motivation: 不同数据集标注关键点数量不一致导致模型通常只能在单个数据集上训练，限制了泛化性和统一模型的建立，需一种能在多数据集上联合训练并处理不同关键点定义的方案。

Method: 框架由Adaptive Prototype-Aware Encoder (APAE)和Progressive Prototype-Aware Decoder (PPAD)组成。APAE自适应提取特征并学习原型表示，PPAD逐步精炼原型生成提示引导注意力。另引入Prototype-Aware (PA)损失以约束原型专家选择权重，缓解梯度冲突和专家寻址不稳定。

Result: 在多个基准数据集上进行大量实验，Proto-Former在性能上优于现有最先进方法（SOTA），并公开了代码。

Conclusion: 该文提出Proto-Former，一种可在多数据集上联合训练的统一面部关键点检测框架，通过学习与增强数据集特异的面部结构原型，提高泛化性和精度。实验显示优于当前SOTA方法。

Abstract: Recent advances in deep learning have significantly improved facial landmark
detection. However, existing facial landmark detection datasets often define
different numbers of landmarks, and most mainstream methods can only be trained
on a single dataset. This limits the model generalization to different datasets
and hinders the development of a unified model. To address this issue, we
propose Proto-Former, a unified, adaptive, end-to-end facial landmark detection
framework that explicitly enhances dataset-specific facial structural
representations (i.e., prototype). Proto-Former overcomes the limitations of
single-dataset training by enabling joint training across multiple datasets
within a unified architecture. Specifically, Proto-Former comprises two key
components: an Adaptive Prototype-Aware Encoder (APAE) that performs adaptive
feature extraction and learns prototype representations, and a Progressive
Prototype-Aware Decoder (PPAD) that refines these prototypes to generate
prompts that guide the model's attention to key facial regions. Furthermore, we
introduce a novel Prototype-Aware (PA) loss, which achieves optimal path
finding by constraining the selection weights of prototype experts. This loss
function effectively resolves the problem of prototype expert addressing
instability during multi-dataset training, alleviates gradient conflicts, and
enables the extraction of more accurate facial structure features. Extensive
experiments on widely used benchmark datasets demonstrate that our Proto-Former
achieves superior performance compared to existing state-of-the-art methods.
The code is publicly available at: https://github.com/Husk021118/Proto-Former.

</details>


### [32] [SHARE: Scene-Human Aligned Reconstruction](https://arxiv.org/abs/2510.15342)
*Joshua Li,Brendan Chharawala,Chang Shu,Xue Bin Peng,Pengcheng Xi*

Main category: cs.CV

TL;DR: SHARE用场景点图和关键帧对齐策略，通过保持关键/非关键帧根关节相对位置，一致地在单目视频中把人体准确放到3D场景中，提升重建精度和场景交互真实感。


<details>
  <summary>Details</summary>
Motivation: 现有单目人体重建方法在将人物精确放置到场景中存在困难，导致与环境交互不自然，影响游戏、AR/VR和机器人等应用的真实感和实用性。

Method: 在关键帧上同时估计人体网格、分割掩码和场景点图，迭代优化关键帧人体位置，使人体网格与从场景中提取的人体点图对齐；并在优化中保持非关键帧网格相对关键帧根关节位置的一致性，确保全序列一致性。

Result: 在公开数据集和网络视频上均优于现有方法，显著提高了3D人体放置精度并能同时重建周围场景。

Conclusion: SHARE通过利用场景几何来提高单目视频中人体在3D空间中的放置精度，从而改善人体运动重建的真实感与环境一致性。

Abstract: Animating realistic character interactions with the surrounding environment
is important for autonomous agents in gaming, AR/VR, and robotics. However,
current methods for human motion reconstruction struggle with accurately
placing humans in 3D space. We introduce Scene-Human Aligned REconstruction
(SHARE), a technique that leverages the scene geometry's inherent spatial cues
to accurately ground human motion reconstruction. Each reconstruction relies
solely on a monocular RGB video from a stationary camera. SHARE first estimates
a human mesh and segmentation mask for every frame, alongside a scene point map
at keyframes. It iteratively refines the human's positions at these keyframes
by comparing the human mesh against the human point map extracted from the
scene using the mask. Crucially, we also ensure that non-keyframe human meshes
remain consistent by preserving their relative root joint positions to keyframe
root joints during optimization. Our approach enables more accurate 3D human
placement while reconstructing the surrounding scene, facilitating use cases on
both curated datasets and in-the-wild web videos. Extensive experiments
demonstrate that SHARE outperforms existing methods.

</details>


### [33] [Cortical-SSM: A Deep State Space Model for EEG and ECoG Motor Imagery Decoding](https://arxiv.org/abs/2510.15371)
*Shuntaro Suzuki,Shunya Nagashima,Masayuki Hirata,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出的Cortical-SSM通过在时间-空间-频率域联合建模深度状态空间，显著提升EEG/ECoG运动意像分类性能并提供有意义的可视化解释。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法难以捕捉EEG/ECoG信号的细粒度依赖，且生理伪迹影响大，需一种能同时建模时序、空间与频率依赖的鲁棒方法。

Method: 提出Cortical-SSM，将深度状态空间模型扩展以在时间、空间和频率域内建模信号的耦合依赖；在结构上可能融入多尺度特征提取与跨域融合模块，并用于分类任务。

Result: 在两个大规模公开MI-EEG数据集（>50名受试者）和一个ALS患者的MI-ECoG临床数据集上，Cortical-SSM的分类性能均优于基线方法；可视化解释显示模型关注的区域具有神经生理学意义。

Conclusion: Cortical-SSM能够有效提升对运动意像( MI )的EEG和ECoG信号分类性能，并在三个基准数据集上优于基线方法，且可视化结果显示模型关注的脑区与神经生理学相关。

Abstract: Classification of electroencephalogram (EEG) and electrocorticogram (ECoG)
signals obtained during motor imagery (MI) has substantial application
potential, including for communication assistance and rehabilitation support
for patients with motor impairments. These signals remain inherently
susceptible to physiological artifacts (e.g., eye blinking, swallowing), which
pose persistent challenges. Although Transformer-based approaches for
classifying EEG and ECoG signals have been widely adopted, they often struggle
to capture fine-grained dependencies within them. To overcome these
limitations, we propose Cortical-SSM, a novel architecture that extends deep
state space models to capture integrated dependencies of EEG and ECoG signals
across temporal, spatial, and frequency domains. We validated our method across
three benchmarks: 1) two large-scale public MI EEG datasets containing more
than 50 subjects, and 2) a clinical MI ECoG dataset recorded from a patient
with amyotrophic lateral sclerosis. Our method outperformed baseline methods on
the three benchmarks. Furthermore, visual explanations derived from our model
indicate that it effectively captures neurophysiologically relevant regions of
both EEG and ECoG signals.

</details>


### [34] [Adaptive transfer learning for surgical tool presence detection in laparoscopic videos through gradual freezing fine-tuning](https://arxiv.org/abs/2510.15372)
*Ana Davila,Jacinto Colan,Yasuhisa Hasegawa*

Main category: cs.CV

TL;DR: 提出线性探测+渐进冻结的分阶段自适应微调，单次训练即可高效适配预训练CNN，提升外科工具检测性能并具跨手术领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 外科内镜影像标注稀缺，直接微调预训练深度网络易过拟合或低效，需一种既能利用预训练特征又能稳健适应外科领域的新方法。

Method: 方法包括两步：1) 线性探测阶段，只训练新加入的分类层以在预训练CNN（ResNet-50、DenseNet-121）上进行条件化；2) 渐进冻结阶段，逐步减少可微调的层数以动态调控适应程度，从而降低网络复杂度并提高效率，整个过程在单次训练循环内完成，无需多次迭代。

Result: 在Cholec80上取得mAP 96.4%，优于常规微调和现有方法；在CATARACTS上也验证了方法的泛化性，表明该策略适用于不同类型微创手术的视频工具检测。

Conclusion: 该论文提出的分阶段自适应微调方法（线性探测+渐进冻结）有效缓解了外科内窥镜工具检测中的数据匮乏问题，在Cholec80和CATARACTS数据集上均能提升检测性能，表现出良好的泛化能力，且训练流程简化、效率更高。

Abstract: Minimally invasive surgery can benefit significantly from automated surgical
tool detection, enabling advanced analysis and assistance. However, the limited
availability of annotated data in surgical settings poses a challenge for
training robust deep learning models. This paper introduces a novel staged
adaptive fine-tuning approach consisting of two steps: a linear probing stage
to condition additional classification layers on a pre-trained CNN-based
architecture and a gradual freezing stage to dynamically reduce the
fine-tunable layers, aiming to regulate adaptation to the surgical domain. This
strategy reduces network complexity and improves efficiency, requiring only a
single training loop and eliminating the need for multiple iterations. We
validated our method on the Cholec80 dataset, employing CNN architectures
(ResNet-50 and DenseNet-121) pre-trained on ImageNet for detecting surgical
tools in cholecystectomy endoscopic videos. Our results demonstrate that our
method improves detection performance compared to existing approaches and
established fine-tuning techniques, achieving a mean average precision (mAP) of
96.4%. To assess its broader applicability, the generalizability of the
fine-tuning strategy was further confirmed on the CATARACTS dataset, a distinct
domain of minimally invasive ophthalmic surgery. These findings suggest that
gradual freezing fine-tuning is a promising technique for improving tool
presence detection in diverse surgical procedures and may have broader
applications in general image classification tasks.

</details>


### [35] [FreqPDE: Rethinking Positional Depth Embedding for Multi-View 3D Object Detection Transformers](https://arxiv.org/abs/2510.15385)
*Haisheng Su,Junjie Zhang,Feixiang Song,Sanping Zhou,Wei Wu,Nanning Zheng,Junchi Yan*

Main category: cs.CV

TL;DR: 提出FreqPDE：通过FSPE提取频域分离特征、CSDP跨视图预测深度分布、PDE结合位置信息生成深度感知特征，并用混合深度监督，显著提升多视图3D检测性能（nuScenes验证）。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于深度预测并且需LiDAR监督，但受投影点稀疏与使用高层图像特征的影响，导致深度在目标边界和小物体上效果差，同时缺乏视图一致性与尺度不变性。为此提出FreqPDE提升深度质量与编码空间信息。

Method: 方法包含三部分：1) 频率感知空间金字塔编码器（FSPE），将高频边缘线索与低频语义在不同层级融合构建特征金字塔；2) 跨视图尺度不变深度预测器（CSDP），利用跨视图与高效通道注意力估计像素级深度分布；3) 位置深度编码器（PDE），将2D图像特征与3D位置信息结合为深度感知特征供Transformer解码器使用。并采用混合深度监督（metric + distribution）。

Result: 在nuScenes数据集上的广泛实验表明所提方法在3D检测任务中效果优于对比方法，显示出更精确的深度预测与更好的小目标与边界处理能力。

Conclusion: 本文提出的FreqPDE通过频率感知的位置深度嵌入显著提升了多视图2D图像到3D检测的性能，解决了深度预测在边界与小目标上的不连续性与不精确问题，并兼顾了视图间一致性与尺度不变性，实验在nuScenes上表现优越。

Abstract: Detecting 3D objects accurately from multi-view 2D images is a challenging
yet essential task in the field of autonomous driving. Current methods resort
to integrating depth prediction to recover the spatial information for object
query decoding, which necessitates explicit supervision from LiDAR points
during the training phase. However, the predicted depth quality is still
unsatisfactory such as depth discontinuity of object boundaries and
indistinction of small objects, which are mainly caused by the sparse
supervision of projected points and the use of high-level image features for
depth prediction. Besides, cross-view consistency and scale invariance are also
overlooked in previous methods. In this paper, we introduce Frequency-aware
Positional Depth Embedding (FreqPDE) to equip 2D image features with spatial
information for 3D detection transformer decoder, which can be obtained through
three main modules. Specifically, the Frequency-aware Spatial Pyramid Encoder
(FSPE) constructs a feature pyramid by combining high-frequency edge clues and
low-frequency semantics from different levels respectively. Then the Cross-view
Scale-invariant Depth Predictor (CSDP) estimates the pixel-level depth
distribution with cross-view and efficient channel attention mechanism.
Finally, the Positional Depth Encoder (PDE) combines the 2D image features and
3D position embeddings to generate the 3D depth-aware features for query
decoding. Additionally, hybrid depth supervision is adopted for complementary
depth learning from both metric and distribution aspects. Extensive experiments
conducted on the nuScenes dataset demonstrate the effectiveness and superiority
of our proposed method.

</details>


### [36] [PFGS: Pose-Fused 3D Gaussian Splatting for Complete Multi-Pose Object Reconstruction](https://arxiv.org/abs/2510.15386)
*Ting-Yu Yen,Yu-Sheng Chiu,Shih-Hsuan Hung,Peter Wonka,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: PFGS是一个姿态感知的3D Gaussian Splatting框架，通过全局与局部配准、背景辅助的相机位姿估计和利用大模型进行跨姿态对齐，能将多姿态图像融合为主姿态的完整高质量3D重建，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3DGS多假设物体为单一静态姿态，导致对被遮挡或自遮挡区域重建不完整；实际采集常有多姿态图像，需要方法能从多姿态数据中重建完整物体。

Method: 提出Pose-aware 3DGS（PFGS），通过迭代将辅姿态图像集合融合到主姿态的统一3DGS表示中；采用全局+局部配准策略；利用背景特征估计每个姿态的相机位姿，并用大模型做跨姿态配准，从而提高鲁棒性与效率并解决背景不一致问题。

Result: 在定性与定量评估中，PFGS较强基线表现更好，生成更加完整且保真度更高的3DGS重建结果。

Conclusion: PFGS能在多姿态图像数据下构建更完整、更高保真度的3D Gaussian Splatting模型，优于现有基线方法。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality,
real-time novel-view synthesis from multi-view images. However, most existing
methods assume the object is captured in a single, static pose, resulting in
incomplete reconstructions that miss occluded or self-occluded regions. We
introduce PFGS, a pose-aware 3DGS framework that addresses the practical
challenge of reconstructing complete objects from multi-pose image captures.
Given images of an object in one main pose and several auxiliary poses, PFGS
iteratively fuses each auxiliary set into a unified 3DGS representation of the
main pose. Our pose-aware fusion strategy combines global and local
registration to merge views effectively and refine the 3DGS model. While recent
advances in 3D foundation models have improved registration robustness and
efficiency, they remain limited by high memory demands and suboptimal accuracy.
PFGS overcomes these challenges by incorporating them more intelligently into
the registration process: it leverages background features for per-pose camera
pose estimation and employs foundation models for cross-pose registration. This
design captures the best of both approaches while resolving background
inconsistency issues. Experimental results demonstrate that PFGS consistently
outperforms strong baselines in both qualitative and quantitative evaluations,
producing more complete reconstructions and higher-fidelity 3DGS models.

</details>


### [37] [LILAC: Long-sequence Incremental Low-latency Arbitrary Motion Stylization via Streaming VAE-Diffusion with Causal Decoding](https://arxiv.org/abs/2510.15392)
*Peng Ren,Hai Yang*

Main category: cs.CV

TL;DR: LILAC通过潜在空间滑动窗口因果流式VAE-扩散与解码特征注入，实现无需未来帧的实时长序列任意动作风格化，兼顾质量与低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有流式方法在原始动作空间处理计算量大且时间稳定性差；而潜在空间VAE-扩散方法虽然风格化质量高但主要用于离线场景。需融合两者优点以实现长序列实时高质量风格化。

Method: 在潜在空间构建流式架构：使用VAE将动作编码到潜在空间，采用扩散模型进行风格化；引入滑动窗口的因果解码器以实现增量在线处理，并在窗口边界注入解码后的动作特征以平滑过渡。系统不依赖未来帧，也无需改动扩散模型架构。

Result: 在基准数据集上实验证明LILAC在风格化质量和响应性之间取得了良好平衡，支持长序列实时任意风格化，且无需未来帧或扩散模型改动。项目页面提供补充视频与示例。

Conclusion: 该论文提出了LILAC，一种面向实时长序列任意动作风格化的流式VAE-扩散框架，通过潜在空间的滑动窗口因果解码设计与解码后动作特征注入，实现无需未来帧或修改扩散模型即可保持高质量风格化与连续性。

Abstract: Generating long and stylized human motions in real time is critical for
applications that demand continuous and responsive character control. Despite
its importance, existing streaming approaches often operate directly in the raw
motion space, leading to substantial computational overhead and making it
difficult to maintain temporal stability. In contrast, latent-space
VAE-Diffusion-based frameworks alleviate these issues and achieve high-quality
stylization, but they are generally confined to offline processing. To bridge
this gap, LILAC (Long-sequence Incremental Low-latency Arbitrary Motion
Stylization via Streaming VAE-Diffusion with Causal Decoding) builds upon a
recent high-performing offline framework for arbitrary motion stylization and
extends it to an online setting through a latent-space streaming architecture
with a sliding-window causal design and the injection of decoded motion
features to ensure smooth motion transitions. This architecture enables
long-sequence real-time arbitrary stylization without relying on future frames
or modifying the diffusion model architecture, achieving a favorable balance
between stylization quality and responsiveness as demonstrated by experiments
on benchmark datasets. Supplementary video and examples are available at the
project page: https://pren1.github.io/lilac/

</details>


### [38] [MARIS: Marine Open-Vocabulary Instance Segmentation with Geometric Enhancement and Semantic Alignment](https://arxiv.org/abs/2510.15398)
*Bingyu Li,Feiyu Wang,Da Zhang,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.CV

TL;DR: 引入首个海洋开放词汇实例分割基准MARIS，并提出GPEM和SAIM两模块来应对海下视觉退化与语义错配，从而在未知类别识别上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有海洋实例分割多受限于封闭词汇，不能识别新颖海洋类别；同时，直接将开放词汇分割方法从自然图像转移到海下图像会遇到视觉退化（如颜色衰减）和语义不对齐问题，因此需要专门的数据集和方法。

Method: 提出统一框架包含两部分：1) GPEM利用稳定的部分级和结构线索（几何先验）来维持在颜色衰减等视觉退化下的对象一致性；2) SAIM将域特定先验注入语言嵌入以缓解语义歧义并增强未见类别识别。并在新建的MARIS基准上进行域内和跨域对比实验。

Result: 在MARIS上，所提框架在域内和跨域设置下均持续超越现有开放词汇基线，证明GPEM和SAIM对抵抗视觉退化和改善语义对齐有效。

Conclusion: 本文提出了用于海洋开放词汇实例分割的新数据集MARIS和一个统一框架，通过几何先验增强模块（GPEM）和语义对齐注入机制（SAIM）提升在海下环境中对未知类别的识别能力。实验表明该方法在MARIS上优于现有开放词汇基线，增强了对视觉退化和语义错配的鲁棒性。

Abstract: Most existing underwater instance segmentation approaches are constrained by
close-vocabulary prediction, limiting their ability to recognize novel marine
categories. To support evaluation, we introduce \textbf{MARIS}
(\underline{Mar}ine Open-Vocabulary \underline{I}nstance
\underline{S}egmentation), the first large-scale fine-grained benchmark for
underwater Open-Vocabulary (OV) segmentation, featuring a limited set of seen
categories and diverse unseen categories. Although OV segmentation has shown
promise on natural images, our analysis reveals that transfer to underwater
scenes suffers from severe visual degradation (e.g., color attenuation) and
semantic misalignment caused by lack underwater class definitions. To address
these issues, we propose a unified framework with two complementary components.
The Geometric Prior Enhancement Module (\textbf{GPEM}) leverages stable
part-level and structural cues to maintain object consistency under degraded
visual conditions. The Semantic Alignment Injection Mechanism (\textbf{SAIM})
enriches language embeddings with domain-specific priors, mitigating semantic
ambiguity and improving recognition of unseen categories. Experiments show that
our framework consistently outperforms existing OV baselines both In-Domain and
Cross-Domain setting on MARIS, establishing a strong foundation for future
underwater perception research.

</details>


### [39] [Robust High-Resolution Multi-Organ Diffusion MRI Using Synthetic-Data-Tuned Prompt Learning](https://arxiv.org/abs/2510.15400)
*Chen Qian,Haoyu Zhang,Junnan Ma,Liuhong Zhu,Qingrui Cai,Yu Wang,Ruibo Song,Lv Li,Lin Mei,Xianwang Jiang,Qin Xu,Boyu Jiang,Ran Tao,Chunmiao Chen,Shufang Chen,Dongyun Liang,Qiu Guo,Jianzhong Lin,Taishan Kang,Mengtian Lu,Liyuan Fu,Ruibin Huang,Huijuan Wan,Xu Huang,Jianhua Wang,Di Guo,Hai Zhong,Jianjun Zhou,Xiaobo Qu*

Main category: cs.CV

TL;DR: LoSP-Prompt 用 LoSP 物理建模与合成数据提示学习自动定参，实现在无导航信号和真实监督下、对多器官多模态 DWI 的高质量、可解释与扫描仪无关的多次采样重建。


<details>
  <summary>Details</summary>
Motivation: 多次采样 DWI 虽能带来更高空间分辨率与 SNR，但在全身肿瘤诊断中的临床应用受限于呼吸、蠕动等导致的相位伪影，以及多组织、多切片、多方向、多 b 值带来的复杂性，需要一种既可解释又通用的重建方法。

Method: 将跨次采样的相位变化建模为高阶局部平滑相位（LoSP），并将该模型整合到低秩 Hankel 矩阵重建中；通过在模拟生理运动的合成腹部 DWI 数据上训练的提示学习自动设置算法的秩参数，从而无需导航信号与真实标签数据。

Result: 在超过10000张临床图像（43 名受试者、5 个中心、4 种扫描仪）上验证：分辨率比单次采样 DWI 提高两倍，肝病灶对比度增强；单模型泛化到肝、肾、骶髂、骨盆、膝、脊髓、脑等七个解剖区；在图像质量、伪影抑制和降噪方面优于现有方法（11 位放射科医师的5分量表评分，p<0.05），部分序列评分达 4-5 分。

Conclusion: LoSP-Prompt 提出了一种结合物理建模与合成数据驱动提示学习的重建框架，有效解决了多切片多器官多方向多 b 值多次采样的相位伪影问题，提升了多次采样 DWI 的临床可用性。

Abstract: Clinical adoption of multi-shot diffusion-weighted magnetic resonance imaging
(multi-shot DWI) for body-wide tumor diagnostics is limited by severe
motion-induced phase artifacts from respiration, peristalsis, and so on,
compounded by multi-organ, multi-slice, multi-direction and multi-b-value
complexities. Here, we introduce a reconstruction framework, LoSP-Prompt, that
overcomes these challenges through physics-informed modeling and
synthetic-data-driven prompt learning. We model inter-shot phase variations as
a high-order Locally Smooth Phase (LoSP), integrated into a low-rank Hankel
matrix reconstruction. Crucially, the algorithm's rank parameter is
automatically set via prompt learning trained exclusively on synthetic
abdominal DWI data emulating physiological motion. Validated across 10,000+
clinical images (43 subjects, 4 scanner models, 5 centers), LoSP-Prompt: (1)
Achieved twice the spatial resolution of clinical single-shot DWI, enhancing
liver lesion conspicuity; (2) Generalized to seven diverse anatomical regions
(liver, kidney, sacroiliac, pelvis, knee, spinal cord, brain) with a single
model; (3) Outperformed state-of-the-art methods in image quality, artifact
suppression, and noise reduction (11 radiologists' evaluations on a 5-point
scale, $p<0.05$), achieving 4-5 points (excellent) on kidney DWI, 4 points
(good to excellent) on liver, sacroiliac and spinal cord DWI, and 3-4 points
(good) on knee and tumor brain. The approach eliminates navigator signals and
realistic data supervision, providing an interpretable, robust solution for
high-resolution multi-organ multi-shot DWI. Its scanner-agnostic performance
signifies transformative potential for precision oncology.

</details>


### [40] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models](https://arxiv.org/abs/2510.15430)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CV

TL;DR: LoD通过多模态安全概念向量和安全模式自编码器，从任务特定角度无监督识别未知越狱攻击，显著提升了检测泛化性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法要么依赖攻击专属参数，缺乏对未知攻击的泛化；要么依赖启发式原则，导致准确性和效率受限。论文通过将学习目标从攻击特定转到任务特定，期望提升对未知越狱攻击的检测能力与效率。

Method: LoD包含两个模块：1）多模态安全概念激活向量（MSCAV），用于学习安全导向的表示，结合视觉和语言特征提取安全相关概念激活；2）安全模式自编码器（SPAE），用于无监督地对输入编码并重建安全模式，基于重建误差或潜在表征进行攻击分类。

Result: 在多种未知攻击上，LoD在检测AUROC上持续优于基线方法，同时在效率上有所提升，代码开源。

Conclusion: 本文提出LoD，一个从攻击特定学习转向任务特定学习的检测框架，通过多模态安全概念激活向量和安全模式自编码器实现对未知越狱攻击的准确检测。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. To address
this, existing detection methods either learn attack-specific parameters, which
hinders generalization to unseen attacks, or rely on heuristically sound
principles, which limit accuracy and efficiency. To overcome these limitations,
we propose Learning to Detect (LoD), a general framework that accurately
detects unknown jailbreak attacks by shifting the focus from attack-specific
learning to task-specific learning. This framework includes a Multi-modal
Safety Concept Activation Vector module for safety-oriented representation
learning and a Safety Pattern Auto-Encoder module for unsupervised attack
classification. Extensive experiments show that our method achieves
consistently higher detection AUROC on diverse unknown attacks while improving
efficiency. The code is available at
https://anonymous.4open.science/r/Learning-to-Detect-51CB.

</details>


### [41] [Semantic4Safety: Causal Insights from Zero-shot Street View Imagery Segmentation for Urban Road Safety](https://arxiv.org/abs/2510.15434)
*Huan Chen,Ting Han,Siyu Chen,Zhihao Guo,Yiping Chen,Meiliu Wu*

Main category: cs.CV

TL;DR: Semantic4Safety 用零样本语义分割从街景图像提取可解释指标，结合XGBoost+SHAP做预测并用GPS+ATE做因果推断，发现场景复杂性、暴露与道路几何对事故有异质因果效应，可用于城市道路安全规划。


<details>
  <summary>Details</summary>
Motivation: 解决如何从街景图像构建与事故相关且可解释的街道级指标，以及如何量化这些指标对不同事故类型的因果影响。

Method: 使用零样本语义分割生成11个可解释的街景指标，结合路段类型与约3万条奥斯汀事故记录；训练XGBoost多分类器并用SHAP解释全局与局部特征贡献；随后用广义倾向评分(GPS)加权与平均处理效应(ATE)估计来控制混杂并量化因果效应。

Result: 模型表明场景复杂性、暴露与道路几何是主要预测因子；具体因果发现包括：较大可行驶面积与应急空间显著降低事故风险，而视觉开放性在某些事故类型下会提高风险；方法支持识别高风险路段并为定向干预提供证据。

Conclusion: Semantic4Safety 提出将零样本语义分割应用于街景图像以提取可解释街景指标，并结合路段类型进行事故因果影响分析，结论显示不同事故类型存在异质性因果效应：场景复杂性、暴露量与道路几何特征主导预测力；更大的可行驶面积与紧急空间能降低风险，而过度视觉开放性可能增加风险。

Abstract: Street-view imagery (SVI) offers a fine-grained lens on traffic risk, yet two
fundamental challenges persist: (1) how to construct street-level indicators
that capture accident-related features, and (2) how to quantify their causal
impacts across different accident types. To address these challenges, we
propose Semantic4Safety, a framework that applies zero-shot semantic
segmentation to SVIs to derive 11 interpretable streetscape indicators, and
integrates road type as contextual information to analyze approximately 30,000
accident records in Austin. Specifically, we train an eXtreme Gradient Boosting
(XGBoost) multi-class classifier and use Shapley Additive Explanations (SHAP)
to interpret both global and local feature contributions, and then apply
Generalized Propensity Score (GPS) weighting and Average Treatment Effect (ATE)
estimation to control confounding and quantify causal effects. Results uncover
heterogeneous, accident-type-specific causal patterns: features capturing scene
complexity, exposure, and roadway geometry dominate predictive power; larger
drivable area and emergency space reduce risk, whereas excessive visual
openness can increase it. By bridging predictive modeling with causal
inference, Semantic4Safety supports targeted interventions and high-risk
corridor diagnosis, offering a scalable, data-informed tool for urban road
safety planning.

</details>


### [42] [Rethinking Convergence in Deep Learning: The Predictive-Corrective Paradigm for Anatomy-Informed Brain MRI Segmentation](https://arxiv.org/abs/2510.15439)
*Feifei Zhang,Zhenhong Jia,Sensen Song,Fei Shi,Dayong Ren*

Main category: cs.CV

TL;DR: 通过先预测关注区域再修正残差，PCMambaNet在脑MRI分割上实现了1-5 epoch内收敛和SOTA精度，缓解了小样本和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 端到端深度学习在小样本医学影像上收敛慢、依赖大规模数据且易过拟合。本工作通过将任务分解为预测与修正两步并引入解剖先验来加速学习并降低样本需求。

Method: 提出Predictive Prior Module(PPM)利用左右双侧对称等解剖先验生成低成本粗预测（关注图），随后Corrective Residual Network(CRN)对残差进行精细建模，集中计算资源在不对称且病灶相关区域。整体构成PCMambaNet网络。

Result: 在高分辨率脑MRI分割上，PCMambaNet在仅1-5个epoch内达到或超过现有端到端模型的性能，实现了显著加速与更好泛化，展示了解剖先验与任务分解在数据稀缺场景中的优势。

Conclusion: PCMambaNet提出的Predictive-Corrective范式通过先预测粗略“关注图”再用残差网络精修，实现了快速收敛和高精度，特别适合数据稀缺的医学影像分割任务。

Abstract: Despite the remarkable success of the end-to-end paradigm in deep learning,
it often suffers from slow convergence and heavy reliance on large-scale
datasets, which fundamentally limits its efficiency and applicability in
data-scarce domains such as medical imaging. In this work, we introduce the
Predictive-Corrective (PC) paradigm, a framework that decouples the modeling
task to fundamentally accelerate learning. Building upon this paradigm, we
propose a novel network, termed PCMambaNet. PCMambaNet is composed of two
synergistic modules. First, the Predictive Prior Module (PPM) generates a
coarse approximation at low computational cost, thereby anchoring the search
space. Specifically, the PPM leverages anatomical knowledge-bilateral
symmetry-to predict a 'focus map' of diagnostically relevant asymmetric
regions. Next, the Corrective Residual Network (CRN) learns to model the
residual error, focusing the network's full capacity on refining these
challenging regions and delineating precise pathological boundaries. Extensive
experiments on high-resolution brain MRI segmentation demonstrate that
PCMambaNet achieves state-of-the-art accuracy while converging within only 1-5
epochs-a performance unattainable by conventional end-to-end models. This
dramatic acceleration highlights that by explicitly incorporating domain
knowledge to simplify the learning objective, PCMambaNet effectively mitigates
data inefficiency and overfitting.

</details>


### [43] [Select Less, Reason More: Prioritizing Evidence Purity for Video Reasoning](https://arxiv.org/abs/2510.15440)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang*

Main category: cs.CV

TL;DR: 针对长视频推理中信息稀释问题，EARL用强化学习主动选帧并做局部重采样，实现“少选多推理”，在多项基准上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 静态的均匀帧采样导致信息稀释，现有像素级主动交互的视频推理代理缺乏严格的奖励机制，无法在预采样帧之外补充时序信息，因此需要一种能优先选择高质量证据并动态补采的机制。

Method: 提出Evidence-Aware Reinforcement Learning (EARL)，将模型设为主动证据询问者；设计奖励机制以保证证据纯度；策略包括动态选择相关帧和对选中帧进行局部重采样以获取细粒度时序信息。

Result: 在五个长视频推理基准上取得开放源代码Video LLMs的新SOTA：7B模型在LongVideoBench上59.8%、MVBench上69.0%、VideoMME上64.9%，展示了证据纯度优先策略和局部重采样的有效性。

Conclusion: 该论文提出了一种基于证据优先的自适应视频理解框架（EARL），通过强化学习动态选择关键帧并在关键帧周边进行局部重采样，从而提高长视频推理中的信息密度与推理准确性。

Abstract: Long-form video reasoning remains a major challenge for Video Large Language
Models (Video LLMs), as static uniform frame sampling leads to information
dilution and obscures critical evidence. Furthermore, existing pixel-space
video reasoning agents, which are designed to actively interact with the video
to acquire new visual information, remain suboptimal due to their lack of
rigorous reward mechanisms to enforce evidence purity and their inability to
perform temporal information supplementation beyond pre-sampled frames. To
address this critical gap, we propose a novel evidence-prioritized adaptive
framework built upon our core philosophy: "Select Less, Reason More." Our core
contribution is the evidence-aware reinforcement learning (EARL) framework,
which transforms the model into an active interrogator of evidence. EARL is
precisely engineered to dynamically select the most relevant frames and,
crucially, to perform localized re-sampling around the selected key frames to
access fine-grained temporal detail. Extensive experiments on five demanding
video reasoning benchmarks demonstrate that our EARL-trained model achieves new
state-of-the-art among open-source Video LLMs, simultaneously learning an
effective and high-purity visual evidence selection policy. Impressively, our
7B model achieves 59.8% on LongVideoBench, 69.0% on MVBench and 64.9% on
VideoMME. These results highlight the importance of prioritizing evidence
purity and the effectiveness of our framework.

</details>


### [44] [MAVR-Net: Robust Multi-View Learning for MAV Action Recognition with Cross-View Attention](https://arxiv.org/abs/2510.15448)
*Nengbo Zhang,Hann Woei Ho*

Main category: cs.CV

TL;DR: MAVR-Net通过融合RGB、光流和分割掩码，并结合多尺度金字塔、跨视注意力与对齐损失，大幅提升了微型无人机动作识别性能，三个基准数据集准确率分别为97.8%、96.5%和92.8%。


<details>
  <summary>Details</summary>
Motivation: 单视角（仅RGB）方法难以捕捉MAV复杂的时空运动特性，导致动作区分性能受限，因此通过多视角模态融合来提高识别的鲁棒性与准确性。

Method: 使用ResNet编码器分别提取RGB、光流和分割掩码的特征，构建多尺度特征金字塔以保留时空细节；引入跨视注意力模块建模模态与尺度间的依赖，设计多视对齐损失以保证语义一致性并强化跨视特征表示。

Result: 在Short MAV、Medium MAV和Long MAV基准数据集上，MAVR-Net分别取得97.8%、96.5%和92.8%的准确率，显著优于现有方法。

Conclusion: 本文提出的MAVR-Net通过多视角融合（RGB、光流、分割掩码）、多尺度特征金字塔、跨视注意力模块和多视对齐损失，有效提升了微型无人机动作识别的鲁棒性与准确度，实验在三个数据集上分别达到了97.8%、96.5%和92.8%的准确率，结论可信且具有实用价值。

Abstract: Recognizing the motion of Micro Aerial Vehicles (MAVs) is crucial for
enabling cooperative perception and control in autonomous aerial swarms. Yet,
vision-based recognition models relying only on RGB data often fail to capture
the complex spatial temporal characteristics of MAV motion, which limits their
ability to distinguish different actions. To overcome this problem, this paper
presents MAVR-Net, a multi-view learning-based MAV action recognition
framework. Unlike traditional single-view methods, the proposed approach
combines three complementary types of data, including raw RGB frames, optical
flow, and segmentation masks, to improve the robustness and accuracy of MAV
motion recognition. Specifically, ResNet-based encoders are used to extract
discriminative features from each view, and a multi-scale feature pyramid is
adopted to preserve the spatiotemporal details of MAV motion patterns. To
enhance the interaction between different views, a cross-view attention module
is introduced to model the dependencies among various modalities and feature
scales. In addition, a multi-view alignment loss is designed to ensure semantic
consistency and strengthen cross-view feature representations. Experimental
results on benchmark MAV action datasets show that our method clearly
outperforms existing approaches, achieving 97.8\%, 96.5\%, and 92.8\% accuracy
on the Short MAV, Medium MAV, and Long MAV datasets, respectively.

</details>


### [45] [DPTrack:Directional Kernel-Guided Prompt Learning for Robust Nighttime Aerial Tracking](https://arxiv.org/abs/2510.15449)
*Zhiqiang Zhu,Xinbo Gao,Wen Lu,Jie Li,Zhaoyang Wang,Mingqian Ge*

Main category: cs.CV

TL;DR: DPTrack通过拓扑感知的方向核生成精细Prompt，在夜间航拍跟踪中显著提升定位与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Prompt Learning的夜间航拍跟踪器仅依赖空间定位监督，缺乏指向目标细粒度属性的线索，导致提示模糊，影响跟踪器聚焦目标特征的能力。

Method: 提出分层拓扑结构捕获模块以丰富目标特征，再用编码器将这些拓扑感知特征压缩为方向核(direction kernel)，最后利用基于通道-类别对应的核引导提示模块将方向核传播到搜索区特征并结合空间门控生成精确提示。

Result: 在多个基准数据集上的广泛评估表明，DPTrack在夜间场景下取得了优越的跟踪性能（论文宣称改进显著），并将开源代码。

Conclusion: DPTrack通过将目标的属性特征编码为方向核并结合通道-类别对应的提示传播模块，能在夜间航拍场景下生成更精确的Prompt，从而提升定位精度与鲁棒性。

Abstract: Existing nighttime aerial trackers based on prompt learning rely solely on
spatial localization supervision, which fails to provide fine-grained cues that
point to target features and inevitably produces vague prompts. This limitation
impairs the tracker's ability to accurately focus on the object features and
results in trackers still performing poorly. To address this issue, we propose
DPTrack, a prompt-based aerial tracker designed for nighttime scenarios by
encoding the given object's attribute features into the directional kernel
enriched with fine-grained cues to generate precise prompts. Specifically,
drawing inspiration from visual bionics, DPTrack first hierarchically captures
the object's topological structure, leveraging topological attributes to enrich
the feature representation. Subsequently, an encoder condenses these
topology-aware features into the directional kernel, which serves as the core
guidance signal that explicitly encapsulates the object's fine-grained
attribute cues. Finally, a kernel-guided prompt module built on
channel-category correspondence attributes propagates the kernel across the
features of the search region to pinpoint the positions of target features and
convert them into precise prompts, integrating spatial gating for robust
nighttime tracking. Extensive evaluations on established benchmarks demonstrate
DPTrack's superior performance. Our code will be available at
https://github.com/zzq-vipsl/DPTrack.

</details>


### [46] [Improving Micro-Expression Recognition with Phase-Aware Temporal Augmentation](https://arxiv.org/abs/2510.15466)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 通过将微表情序列按前后两阶段分别生成动态图进行时序增强，补充了运动多样性和互补时间线索，在多模型与两个基准数据集上显著提升识别表现，尤其对不平衡数据有效。


<details>
  <summary>Details</summary>
Motivation: 现有微表情识别受限于标注数据稀少与缺乏运动多样性，且多数工作只使用空间增强而忽视时间增强，无法充分利用微表情的细微时序信息。

Method: 将每个微表情序列按时间分解为两个阶段：onset-to-apex 和 apex-to-offset，为每个阶段分别生成动态图（DI），形成双阶段表示；将此时序增强与常见空间增强联合用于训练，适配多种深度架构（CNN、ViT、LEARNet等）。

Result: 在CASME-II和SAMM数据集上，使用六种网络结构进行大量实验，双阶段DI在准确率、unweighted F1和unweighted average recall上均有稳定提升；与空间增强结合时最多实现约10%相对提升，且方法简单、模型无关、对低资源场景友好。

Conclusion: 本文提出的双阶段动态图（Dual-phase DI）时序增强策略能有效提升微表情识别性能，尤其在数据稀缺和类别不平衡场景中具有显著作用。

Abstract: Micro-expressions (MEs) are brief, involuntary facial movements that reveal
genuine emotions, typically lasting less than half a second. Recognizing these
subtle expressions is critical for applications in psychology, security, and
behavioral analysis. Although deep learning has enabled significant advances in
micro-expression recognition (MER), its effectiveness is limited by the
scarcity of annotated ME datasets. This data limitation not only hinders
generalization but also restricts the diversity of motion patterns captured
during training. Existing MER studies predominantly rely on simple spatial
augmentations (e.g., flipping, rotation) and overlook temporal augmentation
strategies that can better exploit motion characteristics. To address this gap,
this paper proposes a phase-aware temporal augmentation method based on dynamic
image. Rather than encoding the entire expression as a single onset-to-offset
dynamic image (DI), our approach decomposes each expression sequence into two
motion phases: onset-to-apex and apex-to-offset. A separate DI is generated for
each phase, forming a Dual-phase DI augmentation strategy. These phase-specific
representations enrich motion diversity and introduce complementary temporal
cues that are crucial for recognizing subtle facial transitions. Extensive
experiments on CASME-II and SAMM datasets using six deep architectures,
including CNNs, Vision Transformer, and the lightweight LEARNet, demonstrate
consistent performance improvements in recognition accuracy, unweighted
F1-score, and unweighted average recall, which are crucial for addressing class
imbalance in MER. When combined with spatial augmentations, our method achieves
up to a 10\% relative improvement. The proposed augmentation is simple,
model-agnostic, and effective in low-resource settings, offering a promising
direction for robust and generalizable MER.

</details>


### [47] [MRASfM: Multi-Camera Reconstruction and Aggregation through Structure-from-Motion in Driving Scenes](https://arxiv.org/abs/2510.15467)
*Lingfeng Xuan,Chang Nie,Yiqing Xu,Zhe Liu,Yanzi Miao,Hesheng Wang*

Main category: cs.CV

TL;DR: 针对多摄像头驾驶场景的SfM问题，MRASfM通过利用摄像头间固定几何关系、道路平面过滤和将多摄像头作为BA单元等策略提升了位姿鲁棒性、去除道路异常点并加速优化，同时支持多场景聚合，在真实部署和公开数据集上表现优异（nuScenes APE=0.124）。


<details>
  <summary>Details</summary>
Motivation: 传统SfM在多摄像头驾驶场景中面临位姿不可靠、道路点云含大量异常点以及计算效率低等问题，且难以在多场景下进行高效聚合与重建。

Method: 在注册阶段利用多摄像头系统中固定的空间关系约束来提升相机位姿估计的可靠性；对道路面使用平面模型对三角化后的点云进行误差点剔除；在BA中将多摄像头组视为单一单位以减少优化变量提高效率；通过粗到细的场景关联与拼接模块实现多场景聚合。

Result: 在真实车辆上部署验证了方法的泛化性和在复杂场景下的鲁棒性；在公开大规模数据集上取得领先性能（nuScenes绝对位姿误差0.124）。

Conclusion: 本文提出的MRASfM针对多摄像头驾驶场景的SfM问题，显著提高了位姿估计鲁棒性、道路面重建质量与重建效率，并通过多场景聚合实现大范围重建，实验结果（nuScenes上0.124绝对位姿误差）证明了方法的有效性。

Abstract: Structure from Motion (SfM) estimates camera poses and reconstructs point
clouds, forming a foundation for various tasks. However, applying SfM to
driving scenes captured by multi-camera systems presents significant
difficulties, including unreliable pose estimation, excessive outliers in road
surface reconstruction, and low reconstruction efficiency. To address these
limitations, we propose a Multi-camera Reconstruction and Aggregation
Structure-from-Motion (MRASfM) framework specifically designed for driving
scenes. MRASfM enhances the reliability of camera pose estimation by leveraging
the fixed spatial relationships within the multi-camera system during the
registration process. To improve the quality of road surface reconstruction,
our framework employs a plane model to effectively remove erroneous points from
the triangulated road surface. Moreover, treating the multi-camera set as a
single unit in Bundle Adjustment (BA) helps reduce optimization variables to
boost efficiency. In addition, MRASfM achieves multi-scene aggregation through
scene association and assembly modules in a coarse-to-fine fashion. We deployed
multi-camera systems on actual vehicles to validate the generalizability of
MRASfM across various scenes and its robustness in challenging conditions
through real-world applications. Furthermore, large-scale validation results on
public datasets show the state-of-the-art performance of MRASfM, achieving
0.124 absolute pose error on the nuScenes dataset.

</details>


### [48] [MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval](https://arxiv.org/abs/2510.15470)
*Jinghao Huang,Yaxiong Chen,Ganchao Liu*

Main category: cs.CV

TL;DR: 针对航拍视频检索提出MSAM，通过多语义自适应挖掘和目标区域聚焦的跨模态融合，提升视频-文本匹配鲁棒性，在自建数据集上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 无人机视频数据量快速增长，航拍视角与地面视角差异显著，现有地面视角的跨模态检索方法难以有效建模航拍视频特性，因而需要专门的检索机制。

Method: 提出多语义自适应学习机制，包括自适应语义构建模块、分布驱动语义学习项和多样性语义项；利用细粒度词-帧交互和跨模态交互特征融合池化，重点匹配目标区域并抑制背景噪声。

Result: 在两个人工构建的无人机视频-文本数据集上，MSAM优于现有方法。论文将公开代码与数据集。

Conclusion: MSAM方法通过多语义自适应挖掘和跨模态交互特征融合池化，有效提高了无人机视频-文本检索性能，特别在面对航拍视角、结构同质性强和背景复杂的场景时表现优越。

Abstract: With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

</details>


### [49] [A Novel Combined Optical Flow Approach for Comprehensive Micro-Expression Recognition](https://arxiv.org/abs/2510.15471)
*Vu Tram Anh Khuong,Thi Bich Phuong Man,Luu Tu Nguyen,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出将onset-to-apex与apex-to-offset两个阶段的光流联合为Combined Optical Flow (COF)，以获得更丰富的时间动态信息，实验证明在CASMEII和SAMM数据集上性能优于单阶段光流方法。


<details>
  <summary>Details</summary>
Motivation: 现有多数基于光流的微表情识别方法仅关注onset-to-apex阶段，忽略apex-to-offset阶段，而后者也蕴含关键的时间变化信息；因此提出结合两阶段信息以获得更完整的运动描述。

Method: 在传统仅使用onset-to-apex光流的基础上，设计并计算COF：分别提取onset-to-apex和apex-to-offset两个阶段的光流场，然后进行融合（如直接叠加或加权融合）以得到联合光流特征，随后将融合特征输入微表情识别网络进行分类。

Result: 在CASMEII和SAMM数据集上的实验显示，COF相比仅使用单一阶段光流的方法在识别准确率上取得提升，验证了结合两个阶段光流能更有效地捕捉微表情动态。

Conclusion: 提出的Combined Optical Flow( COF) 通过结合onset-to-apex和apex-to-offset两阶段光流，能够更全面地捕捉微表情的时间动态，从而提升特征表征能力，有助于改进微表情识别性能。

Abstract: Facial micro-expressions are brief, involuntary facial movements that reveal
hidden emotions. Most Micro-Expression Recognition (MER) methods that rely on
optical flow typically focus on the onset-to-apex phase, neglecting the
apex-to-offset phase, which holds key temporal dynamics. This study introduces
a Combined Optical Flow (COF), integrating both phases to enhance feature
representation. COF provides a more comprehensive motion analysis, improving
MER performance. Experimental results on CASMEII and SAMM datasets show that
COF outperforms single optical flow-based methods, demonstrating its
effectiveness in capturing micro-expression dynamics.

</details>


### [50] [Iterative Motion Compensation for Canonical 3D Reconstruction from UAV Plant Images Captured in Windy Conditions](https://arxiv.org/abs/2510.15491)
*Andre Rochow,Jonas Marcic,Svetlana Seliunina,Sven Behnke*

Main category: cs.CV

TL;DR: 提出一种基于无人机采集与迭代光流驱动图像变形对齐的三维植物重建管线，有效减小叶片运动影响，提升现有重建方法的质量并能输出高分辨率网格，且将开源代码和数据集。


<details>
  <summary>Details</summary>
Motivation: 植物三维表型对于理解生长、产量预测与病害控制至关重要，但在无人机拍摄环境中，风和无人机筒流引起的叶片运动使得基于图像的三维重建难以获得高质量结果，因而需要一种能抑制叶片动态影响、提升重建精度的管线。

Method: 使用商用小型UAV通过自研Android应用自主采集植株图像，采集过程中布置ArUco标记；重建流程兼容多种现有三维重建方法，核心创新是迭代图像校正：先对现有重建结果从对应视角渲染中间图像，再用光流估计原始输入图像与渲染图像之间的运动场，将运动反向应用于输入图像以减小场景动态，重复若干次以得到规范化表征，最终生成高分辨率3D网格。

Result: 经过若干次迭代对齐后，管线能显著改善现有最先进重建方法的结果，获得更完整、更精细的3D网格；同时作者将公开重建管线源码与包含多株、多作物、多时间点的图像数据集，便于社区复现与比较。

Conclusion: 本文提出了一个用于单株农业植物高质量三维重建的管线，通过无人机采集图像并采用迭代基于光流的图像变形对齐来减少叶片运动带来的误差，从而显著提升现有三维重建方法的效果，并能输出高分辨率网格。作者将开源代码并提供包含多作物、多时间点的数据集。

Abstract: 3D phenotyping of plants plays a crucial role for understanding plant growth,
yield prediction, and disease control. We present a pipeline capable of
generating high-quality 3D reconstructions of individual agricultural plants.
To acquire data, a small commercially available UAV captures images of a
selected plant. Apart from placing ArUco markers, the entire image acquisition
process is fully autonomous, controlled by a self-developed Android application
running on the drone's controller. The reconstruction task is particularly
challenging due to environmental wind and downwash of the UAV. Our proposed
pipeline supports the integration of arbitrary state-of-the-art 3D
reconstruction methods. To mitigate errors caused by leaf motion during image
capture, we use an iterative method that gradually adjusts the input images
through deformation. Motion is estimated using optical flow between the
original input images and intermediate 3D reconstructions rendered from the
corresponding viewpoints. This alignment gradually reduces scene motion,
resulting in a canonical representation. After a few iterations, our pipeline
improves the reconstruction of state-of-the-art methods and enables the
extraction of high-resolution 3D meshes. We will publicly release the source
code of our reconstruction pipeline. Additionally, we provide a dataset
consisting of multiple plants from various crops, captured across different
points in time.

</details>


### [51] [Rethinking Efficient Hierarchical Mixing Architecture for Low-light RAW Image Enhancement](https://arxiv.org/abs/2510.15497)
*Xianmin Chen,Peiliang Huang,Longfei Han,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 本文提出HiMA，通过Transformer与Mamba的层次混合、LoDA局部分布自适应对齐和MPF多先验融合，实现高效且高质量的低光RAW图像增强，性能优于SOTA且参数更少。


<details>
  <summary>Details</summary>
Motivation: 现有低光RAW增强方法难以同时兼顾增强质量与计算效率；两阶段框架存在模糊与不一致问题，且对局部强光照变化处理不足。作者重新思考高效低光ISP架构以提升性能并降低参数。

Method: 提出Hierarchical Mixing Architecture (HiMA)，在大尺度用Transformer模块处理上下文信息，在小尺度用Mamba模块高效处理细节；引入Local Distribution Adjustment (LoDA)自适应对齐局部区域特征分布；设计Multi-prior Fusion (MPF)融合空间与频域先验，利用第一阶段去噪结果增强细节。

Result: 在多个公开数据集上进行大量实验，方法在性能上超过当前最先进方法，同时模型参数更少，验证了HiMA在低光RAW增强任务上的有效性与高效性。

Conclusion: 该论文提出HiMA框架，兼顾增强质量与效率，通过层次混合结构结合Transformer与Mamba模块，并引入LoDA和MPF模块以改善局部光照不均与细节恢复，实验表明优于现有方法并参数更少。

Abstract: Low-light RAW image enhancement remains a challenging task. Although numerous
deep learning based approaches have been proposed, they still suffer from
inherent limitations. A key challenge is how to simultaneously achieve strong
enhancement quality and high efficiency. In this paper, we rethink the
architecture for efficient low-light image signal processing (ISP) and
introduce a Hierarchical Mixing Architecture (HiMA). HiMA leverages the
complementary strengths of Transformer and Mamba modules to handle features at
large and small scales, respectively, thereby improving efficiency while
avoiding the ambiguities observed in prior two-stage frameworks. To further
address uneven illumination with strong local variations, we propose Local
Distribution Adjustment (LoDA), which adaptively aligns feature distributions
across different local regions. In addition, to fully exploit the denoised
outputs from the first stage, we design a Multi-prior Fusion (MPF) module that
integrates spatial and frequency-domain priors for detail enhancement.
Extensive experiments on multiple public datasets demonstrate that our method
outperforms state-of-the-art approaches, achieving superior performance with
fewer parameters. Code will be released at https://github.com/Cynicarlos/HiMA.

</details>


### [52] [Exploring Conditions for Diffusion models in Robotic Control](https://arxiv.org/abs/2510.15510)
*Heeseong Shin,Byeongho Heo,Dongyoon Han,Seungryong Kim,Taekyung Kim*

Main category: cs.CV

TL;DR: ORCA通过可学习的任务与视觉提示，利用未微调的扩散模型生成任务自适应视觉表征，从而在机器人控制任务中显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉表征虽能推动模仿学习，但通常在策略学习中保持冻结，缺乏任务相关性；直接使用文本条件在控制场景效果有限，原因在于训练数据分布与机器人环境存在域差距，需要考虑动态且细粒度的视觉信息作为条件。

Method: 提出ORCA方法：使用可学习的任务提示（task prompts）以适配控制任务中的长期/任务相关信息，和视觉提示（visual prompts）以捕捉帧级细节，两者作为条件输入到预训练的文本-图像扩散模型以生成动态、任务相关的视觉表征，且不修改扩散模型本体参数。

Result: 在多项机器人控制基准上，ORCA显著超越先前方法，达到或刷新最先进性能，表明通过学习的条件可以弥补域差距并生成更有利于控制的视觉表征。

Conclusion: 该工作提出在不微调扩散模型的前提下，通过引入可学习的任务提示和视觉提示，使文本到图像的扩散模型生成的表征对控制任务自适应，从而提升机器人模仿学习性能。

Abstract: While pre-trained visual representations have significantly advanced
imitation learning, they are often task-agnostic as they remain frozen during
policy learning. In this work, we explore leveraging pre-trained text-to-image
diffusion models to obtain task-adaptive visual representations for robotic
control, without fine-tuning the model itself. However, we find that naively
applying textual conditions - a successful strategy in other vision domains -
yields minimal or even negative gains in control tasks. We attribute this to
the domain gap between the diffusion model's training data and robotic control
environments, leading us to argue for conditions that consider the specific,
dynamic visual information required for control. To this end, we propose ORCA,
which introduces learnable task prompts that adapt to the control environment
and visual prompts that capture fine-grained, frame-specific details. Through
facilitating task-adaptive representations with our newly devised conditions,
our approach achieves state-of-the-art performance on various robotic control
benchmarks, significantly surpassing prior methods.

</details>


### [53] [Latent Feature Alignment: Discovering Biased and Interpretable Subpopulations in Face Recognition Models](https://arxiv.org/abs/2510.15520)
*Ignacio Serna*

Main category: cs.CV

TL;DR: LFA是一种无需属性标签的潜在方向基方法，用于发现和解释面部识别模型中的偏差子群体，相较k-means和最近邻在语义一致性与可解释性上更优。


<details>
  <summary>Details</summary>
Motivation: 现有的偏差评估依赖带标注的属性来构造子群体，标注昂贵且受限于预定义类别。提出LFA以实现无属性标签的子群体发现和解释，从而更实用地审计面部识别模型中的系统性偏差。

Method: LFA通过在模型的潜在表征中寻找方向（latent directions）来划分子群体：从潜在特征中挖掘语义方向并按该方向分组，比较传统聚类（如k-means）与最近邻的性能。评估在四个端到端识别模型（ArcFace、CosFace、ElasticFace、PartialFC）和两个数据集（RFW、CelebA）上进行，使用组内语义一致性和方向可解释性作为主要度量。

Result: 在四个模型与两个基准上，LFA在组内语义一致性度量上始终优于k-means与最近邻检索，并发现与人口统计和上下文属性高度对齐的可解释潜在方向，证明了该方法在识别和解释有偏子群体方面的有效性。

Conclusion: LFA是一种无需属性标签的表示审计方法，通过在潜在特征空间识别语义方向来发现和评估面部识别模型中的子群体偏差。实验表明，LFA在语义一致性和可解释性方面优于k-means和最近邻方法，并能揭示与年龄、族群和服饰等相关的可解释方向。

Abstract: Modern face recognition models achieve high overall accuracy but continue to
exhibit systematic biases that disproportionately affect certain
subpopulations. Conventional bias evaluation frameworks rely on labeled
attributes to form subpopulations, which are expensive to obtain and limited to
predefined categories. We introduce Latent Feature Alignment (LFA), an
attribute-label-free algorithm that uses latent directions to identify
subpopulations. This yields two main benefits over standard clustering: (i)
semantically coherent grouping, where faces sharing common attributes are
grouped together more reliably than by proximity-based methods, and (ii)
discovery of interpretable directions, which correspond to semantic attributes
such as age, ethnicity, or attire. Across four state-of-the-art recognition
models (ArcFace, CosFace, ElasticFace, PartialFC) and two benchmarks (RFW,
CelebA), LFA consistently outperforms k-means and nearest-neighbor search in
intra-group semantic coherence, while uncovering interpretable latent
directions aligned with demographic and contextual attributes. These results
position LFA as a practical method for representation auditing of face
recognition models, enabling practitioners to identify and interpret biased
subpopulations without predefined attribute annotations.

</details>


### [54] [Balanced Multi-Task Attention for Satellite Image Classification: A Systematic Approach to Achieving 97.23% Accuracy on EuroSAT Without Pre-Training](https://arxiv.org/abs/2510.15527)
*Aditya Vir*

Main category: cs.CV

TL;DR: 通过引入平衡多任务注意力（Coordinate Attention + SE）和进阶正则化策略，作者在无预训练条件下将EuroSAT精度提升至97.23%，验证了面向遥感任务的架构设计效能。


<details>
  <summary>Details</summary>
Motivation: 针对卫星遥感图像的特殊性（空间与光谱信息双重重要、类间混淆问题和小数据集易过拟合），作者系统性地设计网络架构以提升分类精度并避免依赖大规模预训练模型。

Method: 提出并逐步改进三种网络：基线、加入CBAM的中间版、以及最终的平衡多任务注意力网络。核心技术为将Coordinate Attention用于空间特征、Squeeze-Excitation用于光谱特征，并以可学习融合参数alpha联合两者；此外引入逐层DropBlock和类别平衡损失权重进行正则化。

Result: 最终12层模型在EuroSAT上达97.23%精度、Cohen's Kappa 0.9692；所有类别准确率均>94.46%，模型在不使用外部数据下仅比微调的ResNet-50低1.34%。可学习融合参数alpha收敛到约0.57，表明空间与光谱重要性接近。

Conclusion: 该论文通过三阶段体系化架构改进，在不依赖预训练模型的情况下，在EuroSAT数据集上实现了97.23%的测试精度，证明了设计专用轻量级网络并结合注意力机制与正则化策略可接近大型预训练网络性能。

Abstract: This work presents a systematic investigation of custom convolutional neural
network architectures for satellite land use classification, achieving 97.23%
test accuracy on the EuroSAT dataset without reliance on pre-trained models.
Through three progressive architectural iterations (baseline: 94.30%,
CBAM-enhanced: 95.98%, and balanced multi-task attention: 97.23%) we identify
and address specific failure modes in satellite imagery classification. Our
principal contribution is a novel balanced multi-task attention mechanism that
combines Coordinate Attention for spatial feature extraction with
Squeeze-Excitation blocks for spectral feature extraction, unified through a
learnable fusion parameter. Experimental results demonstrate that this
learnable parameter autonomously converges to alpha approximately 0.57,
indicating near-equal importance of spatial and spectral modalities for
satellite imagery. We employ progressive DropBlock regularization (5-20% by
network depth) and class-balanced loss weighting to address overfitting and
confusion pattern imbalance. The final 12-layer architecture achieves Cohen's
Kappa of 0.9692 with all classes exceeding 94.46% accuracy, demonstrating
confidence calibration with a 24.25% gap between correct and incorrect
predictions. Our approach achieves performance within 1.34% of fine-tuned
ResNet-50 (98.57%) while requiring no external data, validating the efficacy of
systematic architectural design for domain-specific applications. Complete
code, trained models, and evaluation scripts are publicly available.

</details>


### [55] [Diffusion Bridge Networks Simulate Clinical-grade PET from MRI for Dementia Diagnostics](https://arxiv.org/abs/2510.15556)
*Yitong Li,Ralph Buchert,Benita Schmitz-Koep,Timo Grimmer,Björn Ommer,Dennis M. Hedderich,Igor Yakushev,Christian Wachinger*

Main category: cs.CV

TL;DR: SiM2P用3D扩散桥从MRI与基本人口学信息生成模拟FDG-PET，显著提升了痴呆谱系三分类诊断性能与一致性，且可用少量本地数据快速部署，降低FDG-PET不可及带来的影响。


<details>
  <summary>Details</summary>
Motivation: FDG-PET在痴呆诊断中有明确价值，但受限于可及性高成本。希望通过用常规可得的MRI与辅助信息生成PET样图，使FDG-PET的诊断收益在资源受限环境中更可用。

Method: 使用3D扩散桥模型学习MRI与辅助人口学信息到FDG-PET图像的概率映射，生成模拟PET；进行了盲法临床阅片研究，由两名神经放射学和两名核医学医师对原始MRI与模拟PET进行评分，并评估诊断准确率、诊断确定性与评审间一致性；并开发了仅需约20例站点特异性数据和基本人口信息即可部署的本地化工作流程。

Result: SiM2P将三分类总体诊断准确率从75.0%提升到84.7%（p<0.05）；模拟PET获得更高的诊断确定性评分和更好的评审间一致性；并提出了易于本地部署的工作流程与开源代码。

Conclusion: 该论文提出了SiM2P，一种基于3D扩散桥（diffusion bridge）的生成框架，用MRI和病人辅助信息模拟诊断级FDG-PET图像，并能在分化阿尔茨海默病、行为变异型额颞叶痴呆和认知健康对照组中显著提高诊断准确率。

Abstract: Positron emission tomography (PET) with 18F-Fluorodeoxyglucose (FDG) is an
established tool in the diagnostic workup of patients with suspected dementing
disorders. However, compared to the routinely available magnetic resonance
imaging (MRI), FDG-PET remains significantly less accessible and substantially
more expensive. Here, we present SiM2P, a 3D diffusion bridge-based framework
that learns a probabilistic mapping from MRI and auxiliary patient information
to simulate FDG-PET images of diagnostic quality. In a blinded clinical reader
study, two neuroradiologists and two nuclear medicine physicians rated the
original MRI and SiM2P-simulated PET images of patients with Alzheimer's
disease, behavioral-variant frontotemporal dementia, and cognitively healthy
controls. SiM2P significantly improved the overall diagnostic accuracy of
differentiating between three groups from 75.0% to 84.7% (p<0.05). Notably, the
simulated PET images received higher diagnostic certainty ratings and achieved
superior interrater agreement compared to the MRI images. Finally, we developed
a practical workflow for local deployment of the SiM2P framework. It requires
as few as 20 site-specific cases and only basic demographic information. This
approach makes the established diagnostic benefits of FDG-PET imaging more
accessible to patients with suspected dementing disorders, potentially
improving early detection and differential diagnosis in resource-limited
settings. Our code is available at https://github.com/Yiiitong/SiM2P.

</details>


### [56] [ClapperText: A Benchmark for Text Recognition in Low-Resource Archival Documents](https://arxiv.org/abs/2510.15557)
*Tingyu Lin,Marco Peer,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: ClapperText 提供~98百帧与~95千词的拍板文本标注，强调退化与低资源挑战，适合少样本与档案OCR研究；基准测试显示微调能显著提升性能，数据与代码已开源。


<details>
  <summary>Details</summary>
Motivation: 档案视频中拍板文本包含结构化的元数据，但常受运动模糊、笔迹多样性、曝光波动和背景杂乱影响；现有OCR在此类退化、非标准形式下效果有限，因此需要真实、文化语境的低资源基准来推动鲁棒OCR与文献理解研究。

Method: 从127段二战时期档案视频中抽取有拍板（clapperboard）的片段，人工标注9813帧、94573个词级实例，包含转录、语义类别、文本类型、遮挡状态，并用4点多边形表示旋转边界框，提供整帧与裁剪词图像；对6个识别模型与7个检测模型在零-shot与微调条件下进行按视频的一致评估。

Result: 数据集中67%的文本为手写，1566个词部分遮挡；微调（仅18个视频训练集）相比零-shot显著提升性能，证明数据集对少样本学习有效。

Conclusion: ClapperText 是一个针对视觉退化与低资源场景下手写与印刷文本识别的基准数据集，适合少样本/迁移学习研究。

Abstract: This paper presents ClapperText, a benchmark dataset for handwritten and
printed text recognition in visually degraded and low-resource settings. The
dataset is derived from 127 World War II-era archival video segments containing
clapperboards that record structured production metadata such as date,
location, and camera-operator identity. ClapperText includes 9,813 annotated
frames and 94,573 word-level text instances, 67% of which are handwritten and
1,566 are partially occluded. Each instance includes transcription, semantic
category, text type, and occlusion status, with annotations available as
rotated bounding boxes represented as 4-point polygons to support spatially
precise OCR applications. Recognizing clapperboard text poses significant
challenges, including motion blur, handwriting variation, exposure
fluctuations, and cluttered backgrounds, mirroring broader challenges in
historical document analysis where structured content appears in degraded,
non-standard forms. We provide both full-frame annotations and cropped word
images to support downstream tasks. Using a consistent per-video evaluation
protocol, we benchmark six representative recognition and seven detection
models under zero-shot and fine-tuned conditions. Despite the small training
set (18 videos), fine-tuning leads to substantial performance gains,
highlighting ClapperText's suitability for few-shot learning scenarios. The
dataset offers a realistic and culturally grounded resource for advancing
robust OCR and document understanding in low-resource archival contexts. The
dataset and evaluation code are available at
https://github.com/linty5/ClapperText.

</details>


### [57] [Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation](https://arxiv.org/abs/2510.15564)
*Xiaoming Zhu,Xu Huang,Qinghongbing Xie,Zhi Deng,Junsheng Yu,Yirui Guan,Zhongyuan Liu,Lin Zhu,Qijun Zhao,Ligang Liu,Long Zeng*

Main category: cs.CV

TL;DR: 提出视觉引导的3D场景布局生成管线：构建资产库→图像生成并对齐资产→图像解析恢复3D布局→基于场景图优化，显著提升布局丰富性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的方法需要繁琐手动规则，深度生成模型难以生成丰富多样的内容，大型语言模型在处理复杂空间关系时不够稳健，因而提出视觉引导方法以提高质量、丰富性与空间一致性。

Method: 构建高质量资产库；用图像生成模型将提示扩展为图像并微调以对齐资产库；开发图像解析模块基于视觉语义与几何信息恢复3D布局；基于场景图和整体视觉语义对布局进行优化。

Result: 通过大量用户测试，算法在布局丰富性和质量方面显著优于现有方法；并提供了包含2,037个场景资产与147个3D场景布局的资产库和数据集。

Conclusion: 该论文提出了一个基于视觉引导的3D场景布局生成系统，通过资产库、图像生成与解析、以及基于场景图的布局优化，实现了更丰富和一致的3D场景布局。

Abstract: Generating artistic and coherent 3D scene layouts is crucial in digital
content creation. Traditional optimization-based methods are often constrained
by cumbersome manual rules, while deep generative models face challenges in
producing content with richness and diversity. Furthermore, approaches that
utilize large language models frequently lack robustness and fail to accurately
capture complex spatial relationships. To address these challenges, this paper
presents a novel vision-guided 3D layout generation system. We first construct
a high-quality asset library containing 2,037 scene assets and 147 3D scene
layouts. Subsequently, we employ an image generation model to expand prompt
representations into images, fine-tuning it to align with our asset library. We
then develop a robust image parsing module to recover the 3D layout of scenes
based on visual semantics and geometric information. Finally, we optimize the
scene layout using scene graphs and overall visual semantics to ensure logical
coherence and alignment with the images. Extensive user testing demonstrates
that our algorithm significantly outperforms existing methods in terms of
layout richness and quality. The code and dataset will be available at
https://github.com/HiHiAllen/Imaginarium.

</details>


### [58] [Unmasking Facial DeepFakes: A Robust Multiview Detection Framework for Natural Images](https://arxiv.org/abs/2510.15576)
*Sami Belguesmia,Mohand Saïd Allili,Assia Hamadene*

Main category: cs.CV

TL;DR: 提出一种融合全局、中层、局部和朝向编码器的多视角DeepFake检测框架，通过特征融合提升在姿态、遮挡和光照变化下的检测鲁棒性，实验验证优于单视角方法。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在姿态变化、遮挡和真实世界中的细微伪影检测上表现欠佳，需通过多尺度、多视角的信息聚合提高鲁棒性。

Method: 构建三种专用编码器：全局视角（边界不一致检测）、中层视角（纹理与颜色对齐分析）、局部视角（眼、鼻、口表情区域失真捕捉），并加入人脸朝向编码器以保证对不同姿态的鲁棒性；采用特征融合策略进行最终判别。

Result: 在若干具有挑战性的数据集上进行实验，结果显示该方法优于传统单视角方法，在复杂姿态与光照条件下检测性能显著提升。

Conclusion: 本论文提出的多视角架构通过融合全局、中层、局部及人脸朝向编码器，有效提升了在复杂姿态与遮挡下的DeepFake检测性能，结论可信且有一定创新性。

Abstract: DeepFake technology has advanced significantly in recent years, enabling the
creation of highly realistic synthetic face images. Existing DeepFake detection
methods often struggle with pose variations, occlusions, and artifacts that are
difficult to detect in real-world conditions. To address these challenges, we
propose a multi-view architecture that enhances DeepFake detection by analyzing
facial features at multiple levels. Our approach integrates three specialized
encoders, a global view encoder for detecting boundary inconsistencies, a
middle view encoder for analyzing texture and color alignment, and a local view
encoder for capturing distortions in expressive facial regions such as the
eyes, nose, and mouth, where DeepFake artifacts frequently occur. Additionally,
we incorporate a face orientation encoder, trained to classify face poses,
ensuring robust detection across various viewing angles. By fusing features
from these encoders, our model achieves superior performance in detecting
manipulated images, even under challenging pose and lighting
conditions.Experimental results on challenging datasets demonstrate the
effectiveness of our method, outperforming conventional single-view approaches

</details>


### [59] [Lightweight CycleGAN Models for Cross-Modality Image Transformation and Experimental Quality Assessment in Fluorescence Microscopy](https://arxiv.org/abs/2510.15579)
*Mohammad Soltaninezhad,Yashar Rouzbahani,Jhonatan Contreras,Rohan Chippalkatti,Daniel Kwaku Abankwa,Christian Eggeling,Thomas Bocklitz*

Main category: cs.CV

TL;DR: 论文用固定通道的超轻量CycleGAN实现从共聚焦到STED的模态转换，参数量从41.8M降到≈9k，训练更快、占用更少，还能作为实验质量诊断工具。


<details>
  <summary>Details</summary>
Motivation: 科学应用中对计算资源和环境影响的要求促使需要更轻量的深度学习模型；显微镜模态转换常缺乏配对数据，且需要快速、低内存的推断与训练；同时希望模型能辅助检测实验和标注问题。

Method: 将U-Net型生成器中的传统通道倍增策略替换为固定通道设计，使可训练参数从41.8M降至约9k；基于CycleGAN框架处理非配对数据；并将GAN输出用于实验质量诊断。

Result: 显著减小模型参数和内存占用，加速训练且性能优于原始更大模型；此外模型可作为诊断工具，发现光漂白、伪影或错误标注等问题。

Conclusion: 该论文提出了一种极轻量的CycleGAN用于荧光显微镜模态转换（共聚焦到超分辨STED/反卷积STED），在保持或提升性能的同时显著降低参数量与计算开销。

Abstract: Lightweight deep learning models offer substantial reductions in
computational cost and environmental impact, making them crucial for scientific
applications. We present a lightweight CycleGAN for modality transfer in
fluorescence microscopy (confocal to super-resolution STED/deconvolved STED),
addressing the common challenge of unpaired datasets. By replacing the
traditional channel-doubling strategy in the U-Net-based generator with a fixed
channel approach, we drastically reduce trainable parameters from 41.8 million
to approximately nine thousand, achieving superior performance with faster
training and lower memory usage. We also introduce the GAN as a diagnostic tool
for experimental and labeling quality. When trained on high-quality images, the
GAN learns the characteristics of optimal imaging; deviations between its
generated outputs and new experimental images can reveal issues such as
photobleaching, artifacts, or inaccurate labeling. This establishes the model
as a practical tool for validating experimental accuracy and image fidelity in
microscopy workflows.

</details>


### [60] [Standardization for improved Spatio-Temporal Image Fusion](https://arxiv.org/abs/2510.15589)
*Harkaitz Goyena,Peter M. Atkinson,Unai Pérez-Goya,M. Dolores Ugarte*

Main category: cs.CV

TL;DR: 为解决多源影像分辨率/光谱不匹配问题，作者提出传统上采样和ABSIS两种标准化方法；两者均能提高USTFIP融合精度，ABSIS效果最佳，光谱/空间精度分别最高提升约49%与78%。


<details>
  <summary>Details</summary>
Motivation: STIF方法需输入分辨率与光谱匹配的多源影像，实际应用中获取匹配影像困难，故提出标准化方法以便更好地使用现有影像进行融合。

Method: 比较传统的细分放大（upscaling）与新的ABSIS锐化方法；将标准化后的影像用于Unpaired Spatio Temporal Fusion of Image Patches (USTFIP)并评估精度提升。

Result: 两种标准化方法均显著提高USTFIP融合结果的准确性，ABSIS在光谱精度上最多提升49.46%，在空间精度上最多提升78.40%。

Conclusion: 提出两种标准化方法能显著提升STIF方法的性能，尤其是ABSIS锐化方法在光谱和空间精度上提升明显。

Abstract: Spatio-Temporal Image Fusion (STIF) methods usually require sets of images
with matching spatial and spectral resolutions captured by different sensors.
To facilitate the application of STIF methods, we propose and compare two
different standardization approaches. The first method is based on traditional
upscaling of the fine-resolution images. The second method is a sharpening
approach called Anomaly Based Satellite Image Standardization (ABSIS) that
blends the overall features found in the fine-resolution image series with the
distinctive attributes of a specific coarse-resolution image to produce images
that more closely resemble the outcome of aggregating the fine-resolution
images. Both methods produce a significant increase in accuracy of the Unpaired
Spatio Temporal Fusion of Image Patches (USTFIP) STIF method, with the
sharpening approach increasing the spectral and spatial accuracies of the fused
images by up to 49.46\% and 78.40\%, respectively.

</details>


### [61] [FlexiReID: Adaptive Mixture of Expert for Multi-Modal Person Re-Identification](https://arxiv.org/abs/2510.15595)
*Zhen Sun,Lei Tan,Yunhang Shen,Chengmao Cai,Xing Sun,Pingyang Dai,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: FlexiReID通过自适应MoE和跨模态查询融合实现对RGB/IR/素描/文本四模态的灵活检索，构建了CIRS-PEDES数据集，实验表明其性能及泛化性优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦于有限的跨模态设置，不能支持任意查询-检索模态组合，限制了实际部署。作者希望构建一个灵活且通用的框架以满足现实应用中不同模态组合的检索需求，并提供全面的数据集评估。

Method: 提出了自适应混合专家（adaptive Mixture-of-Experts）机制用于动态集成来自不同模态的特征，和跨模态查询融合模块用于在查询阶段融合多模态信息；训练上可能使用多任务损失和跨模态对齐策略以保持特征一致性；构建统一的数据集CIRS-PEDES以覆盖四种模态。

Result: 在所构建的CIRS-PEDES及其他基准上，FlexiReID在七种检索模式中取得了最先进的性能，并显示出在复杂情景下的较强泛化能力，证明其方法有效性。

Conclusion: 该论文提出了FlexiReID，一个支持四模态（RGB、红外、素描、文本）和七种检索模式的通用行人重识别框架，通过自适应MoE和跨模态查询融合模块实现灵活融合与增强特征提取，并构建了CIRS-PEDES数据集以评估多模态性能。

Abstract: Multimodal person re-identification (Re-ID) aims to match pedestrian images
across different modalities. However, most existing methods focus on limited
cross-modal settings and fail to support arbitrary query-retrieval
combinations, hindering practical deployment. We propose FlexiReID, a flexible
framework that supports seven retrieval modes across four modalities: rgb,
infrared, sketches, and text. FlexiReID introduces an adaptive
mixture-of-experts (MoE) mechanism to dynamically integrate diverse modality
features and a cross-modal query fusion module to enhance multimodal feature
extraction. To facilitate comprehensive evaluation, we construct CIRS-PEDES, a
unified dataset extending four popular Re-ID datasets to include all four
modalities. Extensive experiments demonstrate that FlexiReID achieves
state-of-the-art performance and offers strong generalization in complex
scenarios.

</details>


### [62] [Quantized FCA: Efficient Zero-Shot Texture Anomaly Detection](https://arxiv.org/abs/2510.15602)
*Andrei-Timotei Ardelean,Patrick Rückbeil,Tim Weyrich*

Main category: cs.CV

TL;DR: QFCA通过将FCA量化为基于直方图的比较并结合PCA预处理，实现在纹理异常检测与定位上的10x加速且几乎不损失精度，适合实时工业部署。


<details>
  <summary>Details</summary>
Motivation: 现有无监督纹理异常定位方法尽管效果好，但推理速度慢，不适合工业场景（如流水线监控）需要实时性的应用。

Method: 对FCA算法进行量化：将特征补丁的比较改为量化值直方图的比较；引入PCA作为特征预处理以增强正常与异常特征对比；整体实现优化以达到实时性能。

Result: 在多种基准上与现有方法比较，QFCA在保持相似甚至更好检测/定位性能的同时，实现约10倍运行时间加速；PCA预处理在复杂纹理上显著提高检测精度。

Conclusion: 提出了QFCA，一种对特征对应分析(FCA)进行量化的实时异常定位方法，在保持精度的前提下实现约10倍加速，并通过PCA预处理提升复杂纹理的检测精度。

Abstract: Zero-shot anomaly localization is a rising field in computer vision research,
with important progress in recent years. This work focuses on the problem of
detecting and localizing anomalies in textures, where anomalies can be defined
as the regions that deviate from the overall statistics, violating the
stationarity assumption. The main limitation of existing methods is their high
running time, making them impractical for deployment in real-world scenarios,
such as assembly line monitoring. We propose a real-time method, named QFCA,
which implements a quantized version of the feature correspondence analysis
(FCA) algorithm. By carefully adapting the patch statistics comparison to work
on histograms of quantized values, we obtain a 10x speedup with little to no
loss in accuracy. Moreover, we introduce a feature preprocessing step based on
principal component analysis, which enhances the contrast between normal and
anomalous features, improving the detection precision on complex textures. Our
method is thoroughly evaluated against prior art, comparing favorably with
existing methods. Project page:
https://reality.tf.fau.de/pub/ardelean2025quantized.html

</details>


### [63] [Lightweight Data-Free Denoising for Detail-Preserving Biomedical Image Restoration](https://arxiv.org/abs/2510.15611)
*Tomáš Chobola,Julia A. Schnabel,Tingying Peng*

Main category: cs.CV

TL;DR: N2D是一个极轻量的多阶段无监督去噪方法，利用噪声相关性扰动生成平滑中间结果并从噪声输入恢复细节，兼顾高质量重建与低算力需求，特别适用于数据稀缺的生物医学成像。


<details>
  <summary>Details</summary>
Motivation: 当前自监督去噪在实际应用受限于高计算和内存需求，需在推理速度与重建质量间权衡；此外生物医学成像中干净训练数据稀缺，需数据无关且高效的去噪方法。

Method: 基于Noise2Noise训练，设计多阶段管线：第一阶段通过扰动噪声空间相关性生成平滑中间结果，后续阶段利用原始噪声输入和中间结果迭代恢复细节，模型结构极其轻量以降低算力与内存需求。

Result: 在多项无数据/无干净参考的基准上，N2D在性能上优于现有无数据方法，同时计算资源消耗仅为其一小部分，适合生物医学成像场景并实现快速推理。

Conclusion: 提出了一种名为Noise2Detail(N2D)的多阶段无监督轻量级去噪模型，在Noise2Noise框架下不需要干净参考图像或噪声建模，推理阶段通过破坏噪声空间相关性得到中间平滑结构，再从噪声输入中恢复细节。

Abstract: Current self-supervised denoising techniques achieve impressive results, yet
their real-world application is frequently constrained by substantial
computational and memory demands, necessitating a compromise between inference
speed and reconstruction quality. In this paper, we present an
ultra-lightweight model that addresses this challenge, achieving both fast
denoising and high quality image restoration. Built upon the Noise2Noise
training framework-which removes the reliance on clean reference images or
explicit noise modeling-we introduce an innovative multistage denoising
pipeline named Noise2Detail (N2D). During inference, this approach disrupts the
spatial correlations of noise patterns to produce intermediate smooth
structures, which are subsequently refined to recapture fine details directly
from the noisy input. Extensive testing reveals that Noise2Detail surpasses
existing dataset-free techniques in performance, while requiring only a
fraction of the computational resources. This combination of efficiency, low
computational cost, and data-free approach make it a valuable tool for
biomedical imaging, overcoming the challenges of scarce clean training data-due
to rare and complex imaging modalities-while enabling fast inference for
practical use.

</details>


### [64] [Deep Learning Based Domain Adaptation Methods in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2510.15615)
*Shuchang Lyu,Qi Zhao,Zheng Zhou,Meng Li,You Zhou,Dingding Yao,Guangliang Cheng,Huiyu Zhou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 全面综述深度学习在遥感域适应的研究，建立系统 taxonomy，总结数据集与性能，指出挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 遥感数据分布差异大（分辨率、传感器模态、地理/环境差异等），影响模型迁移泛化，因而需要系统总结深度学习在遥感域适应方面的研究以指导未来工作。

Method: 通过文献调研汇总并构建分类体系，按任务类型、数据输入模式、监督范式与算法层次对方法进行组织和比较，汇总数据集与实验结果，提炼存在的问题与发展方向。

Result: 给出领域内算法的系统分类、常用数据集与方法对比，归纳出当前方法在跨分辨率、跨传感器、跨地区等任务上的表现与局限，并列出若干开放问题和潜在研究方向。

Conclusion: 该综述系统总结了深度学习在遥感领域域适应的最新进展，提出了全面的分类体系并梳理了任务、输入模态、监督范式与方法粒度等视角下的算法，评估了常用数据集与性能，指出了挑战与未来方向。

Abstract: Domain adaptation is a crucial and increasingly important task in remote
sensing, aiming to transfer knowledge from a source domain a differently
distributed target domain. It has broad applications across various real-world
applications, including remote sensing element interpretation, ecological
environment monitoring, and urban/rural planning. However, domain adaptation in
remote sensing poses significant challenges due to differences in data, such as
variations in ground sampling distance, imaging modes from various sensors,
geographical landscapes, and environmental conditions. In recent years, deep
learning has emerged as a powerful tool for feature representation and
cross-domain knowledge transfer, leading to widespread adoption in remote
sensing tasks. In this paper, we present a comprehensive survey of significant
advancements in deep learning based domain adaptation for remote sensing. We
first introduce the preliminary knowledge to clarify key concepts, mathematical
notations, and the taxonomy of methodologies. We then organize existing
algorithms from multiple perspectives, including task categorization, input
mode, supervision paradigm, and algorithmic granularity, providing readers with
a structured understanding of the field. Next, we review widely used datasets
and summarize the performance of state-of-the-art methods to provide an
overview of current progress. We also identify open challenges and potential
directions to guide future research in domain adaptation for remote sensing.
Compared to previous surveys, this work addresses a broader range of domain
adaptation tasks in remote sensing, rather than concentrating on a few
subfields. It also presents a systematic taxonomy, providing a more
comprehensive and organized understanding of the field. As a whole, this survey
can inspire the research community, foster understanding, and guide future work
in the field.

</details>


### [65] [Uncertainty-Aware Extreme Point Tracing for Weakly Supervised Ultrasound Image Segmentation](https://arxiv.org/abs/2510.15666)
*Lei Shi,Gang Li,Junxing Zhang*

Main category: cs.CV

TL;DR: 提出基于4个极点和SAM2的弱监督超声分割，结合FGEPM（含MC-dropout不确定性）、USC损失与盒子对齐，实验证明可与或优于全监督且标注成本更低。


<details>
  <summary>Details</summary>
Motivation: 像素级标注在医学图像分割中成本高昂且耗时，故希望通过极点等弱监督标注形式减少标注负担，同时保持或接近全监督分割性能。

Method: 方法包括：1）用四个极点生成的包围盒作为SAM2的提示，获取初始伪标签；2）提出增强的Feature-Guided Extreme Point Masking（FGEPM），结合Monte Carlo dropout估计不确定性，构建统一的梯度不确定性代价图用于边界追踪与伪标签精修；3）设计双分支不确定性感知尺度一致性（USC）损失和盒子对齐损失，保证空间一致性与边界对齐；4）在BUSI和UNS超声数据集上进行大量实验验证。

Result: 在BUSI和UNS两个公开超声数据集上，所提方法在若干指标上达到或超越全监督方法，并显著降低标注成本，证明了方法的有效性与实用性。

Conclusion: 该论文提出了一种基于四个极点标注的弱监督超声图像分割框架，利用SAM2生成初始伪标签，并通过增强的FGEPM算法和不确定性感知一致性损失以及盒子对齐损失逐步优化，最终在BUSI和UNS数据集上达到或超过全监督性能，同时大幅降低标注成本。

Abstract: Automatic medical image segmentation is a fundamental step in computer-aided
diagnosis, yet fully supervised approaches demand extensive pixel-level
annotations that are costly and time-consuming. To alleviate this burden, we
propose a weakly supervised segmentation framework that leverages only four
extreme points as annotation. Specifically, bounding boxes derived from the
extreme points are used as prompts for the Segment Anything Model 2 (SAM2) to
generate reliable initial pseudo labels. These pseudo labels are progressively
refined by an enhanced Feature-Guided Extreme Point Masking (FGEPM) algorithm,
which incorporates Monte Carlo dropout-based uncertainty estimation to
construct a unified gradient uncertainty cost map for boundary tracing.
Furthermore, a dual-branch Uncertainty-aware Scale Consistency (USC) loss and a
box alignment loss are introduced to ensure spatial consistency and precise
boundary alignment during training. Extensive experiments on two public
ultrasound datasets, BUSI and UNS, demonstrate that our method achieves
performance comparable to, and even surpassing fully supervised counterparts
while significantly reducing annotation cost. These results validate the
effectiveness and practicality of the proposed weakly supervised framework for
ultrasound image segmentation.

</details>


### [66] [Valeo Near-Field: a novel dataset for pedestrian intent detection](https://arxiv.org/abs/2510.15673)
*Antonyo Musabini,Rachid Benmokhtar,Jagdish Bhanushali,Victor Galizzi,Bertrand Luvison,Xavier Perrotton*

Main category: cs.CV

TL;DR: 新多模态近车行人数据集：鱼眼相机+激光雷达+超声波+动作捕捉3D姿态，同步标注并附嵌入式评测基准与基线结果，旨在推动近场行人检测、3D姿态与意图预测研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏近车近场、多模态且包含精确3D姿态与传感器同步的资源，无法充分支持行人意图预测与近距离安全决策算法的发展，因此构建该数据集以填补空白。

Method: 数据采集使用同步多传感器平台记录真实场景；通过动作捕捉系统获取精确3D关节位置，并与鱼眼相机帧同步；激光雷达用于提取行人3D位置；提供标注工具和基准评测代码，包括准确性、效率与嵌入式可扩展性指标；并训练自定义神经网络作为基线。

Result: 发布了部分数据集与评测套件，包含同步的鱼眼图像、激光雷达3D位置信息和动作捕捉3D关节标注；提供基线网络的性能（如检测、3D姿态估计和意图预测的准确率与嵌入式延迟/资源使用），并演示数据集在处理遮挡、动态场景和硬件受限情况下的有效性。

Conclusion: 该论文构建了面向近车场景行人意图识别的新型多模态数据集，包含鱼眼相机、激光雷达、超声波与基于动作捕捉的3D姿态，并提供同步标注和基于嵌入式系统的评测基准与基线结果，推动了近场智能车感知算法研究。

Abstract: This paper presents a novel dataset aimed at detecting pedestrians'
intentions as they approach an ego-vehicle. The dataset comprises synchronized
multi-modal data, including fisheye camera feeds, lidar laser scans, ultrasonic
sensor readings, and motion capture-based 3D body poses, collected across
diverse real-world scenarios. Key contributions include detailed annotations of
3D body joint positions synchronized with fisheye camera images, as well as
accurate 3D pedestrian positions extracted from lidar data, facilitating robust
benchmarking for perception algorithms. We release a portion of the dataset
along with a comprehensive benchmark suite, featuring evaluation metrics for
accuracy, efficiency, and scalability on embedded systems. By addressing
real-world challenges such as sensor occlusions, dynamic environments, and
hardware constraints, this dataset offers a unique resource for developing and
evaluating state-of-the-art algorithms in pedestrian detection, 3D pose
estimation and 4D trajectory and intention prediction. Additionally, we provide
baseline performance metrics using custom neural network architectures and
suggest future research directions to encourage the adoption and enhancement of
the dataset. This work aims to serve as a foundation for researchers seeking to
advance the capabilities of intelligent vehicles in near-field scenarios.

</details>


### [67] [Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI](https://arxiv.org/abs/2510.15684)
*Gerard Comas-Quiles,Carles Garcia-Cabrera,Julia Dietlmeier,Noel E. O'Connor,Ferran Marques*

Main category: cs.CV

TL;DR: 作者提出MViT-AE，一种仅用健康MRI训练的多模态Transformer自编码器，通过重建误差和SAM后处理实现无监督脑肿瘤分割，结果在多个指标上展现出临床可用性，尽管对小病灶或非强化病灶仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 在标注数据有限、昂贵或不一致的情况下，监督学习受限；无监督异常检测能在无标签或少标签的情形下实现病灶定位，提升神经影像学工作流程的可扩展性。

Method: 提出Multimodal Vision Transformer Autoencoder（MViT-AE），采用多模态早-晚融合策略融合多序列MRI信息，训练时只使用健康脑MRI；推断时通过重建误差图生成异常图，并使用Segment Anything Model（SAM）作为后处理以优化肿瘤轮廓。

Result: 在BraTS-GoAT 2025 Lighthouse数据集上，MViT-AE在测试集上的病灶级Dice为：Whole Tumor 0.437，Tumor Core 0.316，Enhancing Tumor 0.350；验证集上的异常检测率为89.4%。

Conclusion: 该文提出了一种基于Transformer的无监督异常检测框架（MViT-AE），在仅用健康脑MRI训练的条件下，通过重建误差图实现肿瘤检测与定位，显示了在标注稀缺场景下的可行性。

Abstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to
supervised learning for brain tumor segmentation in magnetic resonance imaging
(MRI), particularly when annotated datasets are limited, costly, or
inconsistent. In this work, we propose a novel Multimodal Vision Transformer
Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and
localize tumors via reconstruction-based error maps. This unsupervised paradigm
enables segmentation without reliance on manual labels, addressing a key
scalability bottleneck in neuroimaging workflows. Our method is evaluated in
the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors
such as gliomas, meningiomas, and pediatric brain tumors. To enhance
performance, we introduce a multimodal early-late fusion strategy that
leverages complementary information across multiple MRI sequences, and a
post-processing pipeline that integrates the Segment Anything Model (SAM) to
refine predicted tumor contours. Despite the known challenges of UAD,
particularly in detecting small or non-enhancing lesions, our method achieves
clinically meaningful tumor localization, with lesion-wise Dice Similarity
Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing
Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the
validation set. These findings highlight the potential of transformer-based
unsupervised models to serve as scalable, label-efficient tools for
neuro-oncological imaging.

</details>


### [68] [Unimedvl: Unifying Medical Multimodal Understanding And Generation Through Observation-Knowledge-Analysis](https://arxiv.org/abs/2510.15710)
*Junzhi Ning,Wei Li,Cheng Tang,Jiashi Lin,Chenglong Ma,Chaoyang Zhang,Jiyao Liu,Ying Chen,Shujian Gao,Lihao Liu,Yuandong Pu,Huihui Xu,Chenhui Gou,Ziyan Huang,Yi Xin,Qi Qin,Zhongying Deng,Diping Song,Bin Fu,Guang Yang,Yuanfeng Ji,Tianbin Li,Yanzhou Su,Jin Ye,Shixiang Tang,Ming Hu,Junjun He*

Main category: cs.CV

TL;DR: 本文通过OKA范式和UniMed-5M数据集，提出统一多模态医疗模型UniMedVL，首次在单一架构中同时实现医学图像理解与生成，并展示了双向知识共享带来的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI模型能力分离——图像理解模型不能生成视觉输出，图像生成模型不能给出文本解释，导致数据表示与任务整合受限，无法支持诊断场景中多模态输入与多样输出的需求。

Method: 构建多层框架：1) Observation层：构建UniMed-5M数据集（5.6M样本），将多种单模态数据重构为多模态对；2) Knowledge层：提出Progressive Curriculum Learning逐步引入医疗多模态知识；3) Analysis层：设计统一模型UniMedVL，单一架构同时处理图像理解与生成任务，支持双向知识共享。

Result: UniMedVL在五个医学图像理解基准上表现优异，在八种医学成像模态的生成质量上与专用模型相当；并证明生成任务可提升视觉理解特征，显示统一架构可促进跨任务性能提升。

Conclusion: 提出了统一多模态医疗模型UniMedVL，通过Observation-Knowledge-Analysis (OKA)范式整合图像理解与生成能力，弥补现有系统分离的短板，能够在理解与生成任务间进行双向知识共享。

Abstract: Medical diagnostic applications require models that can process multimodal
medical inputs (images, patient histories, lab results) and generate diverse
outputs including both textual reports and visual content (annotations,
segmentation masks, and images). Despite this need, existing medical AI systems
disrupt this unified process: medical image understanding models interpret
images but cannot generate visual outputs, while medical image generation
models synthesize images but cannot provide textual explanations. This leads to
gaps in data representation, feature integration, and task-level multimodal
capabilities. To this end, we propose a multi-level framework that draws
inspiration from diagnostic workflows through the
Observation-Knowledge-Analysis (OKA) paradigm. Specifically, at the observation
level, we construct UniMed-5M, a dataset comprising over 5.6M samples that
reformat diverse unimodal data into multimodal pairs for foundational
observation. At the knowledge level, we propose Progressive Curriculum Learning
that systematically introduces medical multimodal knowledge. At the analysis
level, we introduce UniMedVL, the first medical unified multimodal model for
the simultaneous analysis of image understanding and generation tasks within a
single architecture. UniMedVL achieves superior performance on five medical
image understanding benchmarks, while matching specialized models in generation
quality across eight medical imaging modalities. Crucially, our unified
architecture enables bidirectional knowledge sharing: generation tasks enhance
visual understanding features, demonstrating that integrating traditionally
separate capabilities within a single medical framework unlocks improvements
across diverse medical vision-language tasks. Code is available at
https://github.com/uni-medical/UniMedVL.

</details>


### [69] [DGME-T: Directional Grid Motion Encoding for Transformer-Based Historical Camera Movement Classification](https://arxiv.org/abs/2510.15725)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

TL;DR: 通过向Video Swin Transformer加入来自光流的可学习方向网格运动编码（DGME-T），在统一重构的基准上显著提升了现代与历史影片的摄像机运动分类性能，且中间微调能进一步提高跨域表现。


<details>
  <summary>Details</summary>
Motivation: 现代CMC模型在应用于老旧档案影片时性能下降，原因是噪声、缺帧和低对比度掩盖了运动线索，需要引入结构化运动先验以提高鲁棒性。

Method: 构建统一基准（将两个现代语料合并为四类，并将HISTORIAN重构为五类均衡类别），设计轻量级的方向网格运动编码（来自光流）作为可学习的后融合层集成到Video Swin Transformer中，并进行跨域和中间微调实验。

Result: 在现代剪辑上，DGME-T将top-1准确率从81.78%提升到86.14%，macro F1从82.08%提升到87.81%；在二战档案上准确率从83.43%提升到84.62%，macro F1从81.72%提升到82.63%。中间在现代数据上的微调使历史数据性能提升超五个百分点。

Conclusion: 本文提出的DGME-T通过在Video Swin Transformer后接可学习的归一化方向网格运动编码模块，有效增强了经典骨干在档案电影上的运动鲁棒性，验证表明在现代和历史片段上均显著提升了分类性能。

Abstract: Camera movement classification (CMC) models trained on contemporary,
high-quality footage often degrade when applied to archival film, where noise,
missing frames, and low contrast obscure motion cues. We bridge this gap by
assembling a unified benchmark that consolidates two modern corpora into four
canonical classes and restructures the HISTORIAN collection into five balanced
categories. Building on this benchmark, we introduce DGME-T, a lightweight
extension to the Video Swin Transformer that injects directional grid motion
encoding, derived from optical flow, via a learnable and normalised late-fusion
layer. DGME-T raises the backbone's top-1 accuracy from 81.78% to 86.14% and
its macro F1 from 82.08% to 87.81% on modern clips, while still improving the
demanding World-War-II footage from 83.43% to 84.62% accuracy and from 81.72%
to 82.63% macro F1. A cross-domain study further shows that an intermediate
fine-tuning stage on modern data increases historical performance by more than
five percentage points. These results demonstrate that structured motion priors
and transformer representations are complementary and that even a small,
carefully calibrated motion head can substantially enhance robustness in
degraded film analysis. Related resources are available at
https://github.com/linty5/DGME-T.

</details>


### [70] [Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset](https://arxiv.org/abs/2510.15742)
*Qingyan Bai,Qiuyu Wang,Hao Ouyang,Yue Yu,Hanlin Wang,Wen Wang,Ka Leong Cheng,Shuailei Ma,Yanhong Zeng,Zichen Liu,Yinghao Xu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: 提出Ditto框架，通过图像编辑器+上下文视频生成器的数据流水线、蒸馏模型+时间增强器降低成本，并由智能代理自动生成与筛选指令，构建了Ditto-1M（一百万示例）用于训练Editto，显著提升指令式视频编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令驱动视频编辑缺乏大规模高质量训练数据，限制了模型进步；需要一种可扩展且成本可控的数据生成与模型训练方案来推动该领域发展。

Method: 提出了一个数据生成流水线：用高级图像编辑器生成多样化图像编辑样本，再用上下文视频生成器扩展为视频编辑示例；采用蒸馏的高效模型架构并加入时间增强器以降低成本同时提升时序一致性；使用智能代理自动生成多样化编辑指令并过滤输出以保证质量。训练策略上，使用课程学习在Ditto-1M上训练Editto模型。

Result: 构建了包含一百万高保真视频编辑示例的Ditto-1M数据集，投入约12000 GPU-天；训练的Editto模型在指令跟随能力与编辑质量上优于现有方法，成为该任务的新基准。

Conclusion: 该论文提出了Ditto框架，通过综合图像编辑器的多样性与基于上下文的视频生成器，构建了大规模高质量的视频编辑训练数据集Ditto-1M，并训练了Editto模型，显著提升了指令驱动视频编辑效果，达成新的最先进水平。

Abstract: Instruction-based video editing promises to democratize content creation, yet
its progress is severely hampered by the scarcity of large-scale, high-quality
training data. We introduce Ditto, a holistic framework designed to tackle this
fundamental challenge. At its heart, Ditto features a novel data generation
pipeline that fuses the creative diversity of a leading image editor with an
in-context video generator, overcoming the limited scope of existing models. To
make this process viable, our framework resolves the prohibitive cost-quality
trade-off by employing an efficient, distilled model architecture augmented by
a temporal enhancer, which simultaneously reduces computational overhead and
improves temporal coherence. Finally, to achieve full scalability, this entire
pipeline is driven by an intelligent agent that crafts diverse instructions and
rigorously filters the output, ensuring quality control at scale. Using this
framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of
one million high-fidelity video editing examples. We trained our model, Editto,
on Ditto-1M with a curriculum learning strategy. The results demonstrate
superior instruction-following ability and establish a new state-of-the-art in
instruction-based video editing.

</details>


### [71] [SEGA: A Stepwise Evolution Paradigm for Content-Aware Layout Generation with Design Prior](https://arxiv.org/abs/2510.15749)
*Haoran Wang,Bo Zhao,Jinghui Wang,Hanzhang Wang,Huan Yang,Wei Ji,Hao Liu,Xinyan Xiao*

Main category: cs.CV

TL;DR: SEGA提出粗到细的分步演化布局生成，结合设计先验与新大规模数据集，显著提高复杂背景图下的布局质量与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有单步推理方法缺乏基于反馈的自我纠错机制，面对复杂元素布局时失败率显著上升，因此需要一种分步、可修正的生成策略。

Method: SEGA由两个模块组成：粗粒度模块先估计布局大致位置，细粒度模块基于粗规划进行逐步优化，同时引入布局设计原则作为先验以约束生成过程。

Result: 在包括作者新构建的GenPoster-100K在内的多项基准数据集上，SEGA取得了优于现有方法的性能，证明了分步演化与设计先验的有效性。

Conclusion: 本文提出SEGA，一种分步演化范式，通过粗到细的分层推理和设计原则先验，有效提升了面向背景图的布局生成性能，实验证明在多个数据集上达到了SOTA。

Abstract: In this paper, we study the content-aware layout generation problem, which
aims to automatically generate layouts that are harmonious with a given
background image. Existing methods usually deal with this task with a
single-step reasoning framework. The lack of a feedback-based self-correction
mechanism leads to their failure rates significantly increasing when faced with
complex element layout planning. To address this challenge, we introduce SEGA,
a novel Stepwise Evolution Paradigm for Content-Aware Layout Generation.
Inspired by the systematic mode of human thinking, SEGA employs a hierarchical
reasoning framework with a coarse-to-fine strategy: first, a coarse-level
module roughly estimates the layout planning results; then, another refining
module performs fine-level reasoning regarding the coarse planning results.
Furthermore, we incorporate layout design principles as prior knowledge into
the model to enhance its layout planning ability. Besides, we present
GenPoster-100K that is a new large-scale poster dataset with rich
meta-information annotation. The experiments demonstrate the effectiveness of
our approach by achieving the state-of-the-art results on multiple benchmark
datasets. Our project page is at: https://brucew91.github.io/SEGA.github.io/

</details>


### [72] [NDM: A Noise-driven Detection and Mitigation Framework against Implicit Sexual Intentions in Text-to-Image Generation](https://arxiv.org/abs/2510.15752)
*Yitong Sun,Yao Huang,Ruochen Zhang,Huanran Chen,Shouwei Ruan,Ranjie Duan,Xingxing Wei*

Main category: cs.CV

TL;DR: NDM通过早期预测噪声分离检测和噪声增强的自适应负向引导，在不微调模型的前提下高效检测并缓解隐式性提示引发的T2I不当内容，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 动机是当前T2I扩散模型在面对隐式性提示时容易产生不当内容，而现有检测方法多针对显式性提示且微调会损害生成质量，因此需要一种既能识别隐式性提示又能在不破坏模型能力下进行缓解的解决方案。

Method: 方法包括两部分：1) 利用扩散过程早期预测噪声的可分离性，设计噪声驱动的检测器来识别含有隐式性意图的提示词；2) 提出噪声增强的自适应负向引导，通过优化初始噪声并抑制高显著区域的注意力来增强性内容缓解效果。该方法不依赖模型微调，直接在采样噪声层面进行操作。

Result: 在自然与对抗性数据集上，NDM在检测与缓解隐式性内容方面均优于现有SOTA方法（如SLD、UCE、RECE等），在保证生成质量的同时实现更高的检测准确率与更有效的性内容抑制。作者已开源代码与资源。

Conclusion: 该论文提出了一种无需微调、基于噪声的检测与缓解框架NDM，能够在保留T2I扩散模型生成质量的同时，高效识别并抑制隐式性暗示引发的不当性内容。

Abstract: Despite the impressive generative capabilities of text-to-image (T2I)
diffusion models, they remain vulnerable to generating inappropriate content,
especially when confronted with implicit sexual prompts. Unlike explicit
harmful prompts, these subtle cues, often disguised as seemingly benign terms,
can unexpectedly trigger sexual content due to underlying model biases, raising
significant ethical concerns. However, existing detection methods are primarily
designed to identify explicit sexual content and therefore struggle to detect
these implicit cues. Fine-tuning approaches, while effective to some extent,
risk degrading the model's generative quality, creating an undesirable
trade-off. To address this, we propose NDM, the first noise-driven detection
and mitigation framework, which could detect and mitigate implicit malicious
intention in T2I generation while preserving the model's original generative
capabilities. Specifically, we introduce two key innovations: first, we
leverage the separability of early-stage predicted noise to develop a
noise-based detection method that could identify malicious content with high
accuracy and efficiency; second, we propose a noise-enhanced adaptive negative
guidance mechanism that could optimize the initial noise by suppressing the
prominent region's attention, thereby enhancing the effectiveness of adaptive
negative guidance for sexual mitigation. Experimentally, we validate NDM on
both natural and adversarial datasets, demonstrating its superior performance
over existing SOTA methods, including SLD, UCE, and RECE, etc. Code and
resources are available at https://github.com/lorraine021/NDM.

</details>


### [73] [Semantic segmentation with coarse annotations](https://arxiv.org/abs/2510.15756)
*Jort de Jong,Mike Holenderski*

Main category: cs.CV

TL;DR: 引入SLIC超像素正则化到FCN-16的上采样过程，在粗标注场景下能有效提升分割边界的对齐与召回。


<details>
  <summary>Details</summary>
Motivation: 在缺乏精细像素级标注或只具备粗标注时，分割模型难以获得准确的类边界，尤其边界处未标注或标注不精确。希望通过利用与分割目标无关的图像低级结构（超像素）来引导模型改善边界对齐。

Method: 在编码器-解码器架构（FCN-16）中引入基于SLIC超像素的上采样正则化，鼓励解码输出的分割块与基于像素颜色和位置的SLIC超像素一致。训练时在损失中加入该正则项以利用超像素信息，不依赖细粒度标注。

Result: 在SUIM、Cityscapes和PanNuke数据集上进行评估，结果显示在粗标注条件下，相较于现有方法，该方法显著提高了边界召回率（boundary recall），提升了边界对齐质量。

Conclusion: 该文提出一种基于超像素正则化的方法，用于提升在粗标注下语义分割模型的边界对齐能力，结论是该方法在多数据集上能显著提高边界召回率。

Abstract: Semantic segmentation is the task of classifying each pixel in an image.
Training a segmentation model achieves best results using annotated images,
where each pixel is annotated with the corresponding class. When obtaining fine
annotations is difficult or expensive, it may be possible to acquire coarse
annotations, e.g. by roughly annotating pixels in an images leaving some pixels
around the boundaries between classes unlabeled. Segmentation with coarse
annotations is difficult, in particular when the objective is to optimize the
alignment of boundaries between classes. This paper proposes a regularization
method for models with an encoder-decoder architecture with superpixel based
upsampling. It encourages the segmented pixels in the decoded image to be
SLIC-superpixels, which are based on pixel color and position, independent of
the segmentation annotation. The method is applied to FCN-16 fully
convolutional network architecture and evaluated on the SUIM, Cityscapes, and
PanNuke data sets. It is shown that the boundary recall improves significantly
compared to state-of-the-art models when trained on coarse annotations.

</details>


### [74] [QSilk: Micrograin Stabilization and Adaptive Quantile Clipping for Detail-Friendly Latent Diffusion](https://arxiv.org/abs/2510.15761)
*Denis Rychkovskiy*

Main category: cs.CV

TL;DR: QSilk是一种轻量、无需训练的稳定层，通过微限幅和自适应分位数裁剪提高潜在扩散模型的高频细节保真度并抑制激活尖峰，集成成本低，兼容多种骨干并与CFG/Rescale协同。


<details>
  <summary>Details</summary>
Motivation: 缓解潜在扩散生成中出现的极端激活值导致的纹理破坏和细节丢失，提升超高分辨率和低步骤数下的清晰度，同时保持轻量、无需训练的特点。

Method: 结合两个机制：每样本微限幅（micro clamp）温和限制极端值而不冲刷纹理；自适应分位数裁剪（AQClip），按区域自适应允许值区间，支持代理统计模式和基于注意力熵的模型置信度模式。集成到CADE 2.5渲染管线。

Result: 在SD/SDXL骨干上均表现出一致的定性改进，在低采样步数和超高分辨率下生成更清晰、更锐利的图像，与CFG/Rescale协同可略微提高引导强度而不产生伪影。

Conclusion: QSilk有效提升潜在扩散模型在高频细节上的保真度，同时抑制罕见激活峰值，并且无需训练或微调，开销极小，易于集成与控制。

Abstract: We present QSilk, a lightweight, always-on stabilization layer for latent
diffusion that improves high-frequency fidelity while suppressing rare
activation spikes. QSilk combines (i) a per-sample micro clamp that gently
limits extreme values without washing out texture, and (ii) Adaptive Quantile
Clip (AQClip), which adapts the allowed value corridor per region. AQClip can
operate in a proxy mode using local structure statistics or in an attention
entropy guided mode (model confidence). Integrated into the CADE 2.5 rendering
pipeline, QSilk yields cleaner, sharper results at low step counts and
ultra-high resolutions with negligible overhead. It requires no training or
fine-tuning and exposes minimal user controls. We report consistent qualitative
improvements across SD/SDXL backbones and show synergy with CFG/Rescale,
enabling slightly higher guidance without artifacts.

</details>


### [75] [Towards more holistic interpretability: A lightweight disentangled Concept Bottleneck Model](https://arxiv.org/abs/2510.15770)
*Gaoxiang Huang,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: 提出LDCBM，通过过滤器分组和联合概念监督在无区域标注下实现概念与视觉特征的解耦对齐，提升了可解释性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有CBM存在输入到概念的映射偏差和可控性差的问题，导致解释不可靠。作者希望通过将视觉证据直接接地到概念，从而增强解释的可靠性和模型的可控性。

Method: 在基于概念瓶颈的架构中，引入过滤器分组损失以鼓励同一概念相关的卷积滤波器产生相似响应，并通过联合概念监督使得每组滤波器与对应概念对齐；模型设计为轻量级，且不依赖区域级标注。

Result: 在三个不同数据集上的实验表明，LDCBM在概念预测准确率和最终分类准确率上均优于先前的CBM方法，证明了其在可解释性和分类性能上的改进。

Conclusion: 该论文提出了一种轻量级的解耦概念瓶颈模型（LDCBM），通过过滤器分组损失和联合概念监督，在无需区域注释下将视觉特征自动分组为语义成分，从而改善概念-视觉模式的对齐，提高了可解释性和分类性能。

Abstract: Concept Bottleneck Models (CBMs) enhance interpretability by predicting
human-understandable concepts as intermediate representations. However,
existing CBMs often suffer from input-to-concept mapping bias and limited
controllability, which restricts their practical value, directly damage the
responsibility of strategy from concept-based methods. We propose a lightweight
Disentangled Concept Bottleneck Model (LDCBM) that automatically groups visual
features into semantically meaningful components without region annotation. By
introducing a filter grouping loss and joint concept supervision, our method
improves the alignment between visual patterns and concepts, enabling more
transparent and robust decision-making. Notably, Experiments on three diverse
datasets demonstrate that LDCBM achieves higher concept and class accuracy,
outperforming previous CBMs in both interpretability and classification
performance. By grounding concepts in visual evidence, our method overcomes a
fundamental limitation of prior models and enhances the reliability of
interpretable AI.

</details>


### [76] [Controlling the image generation process with parametric activation functions](https://arxiv.org/abs/2510.15778)
*Ilia Pavlov*

Main category: cs.CV

TL;DR: 通过可参数化激活函数替换并允许用户调参，本文提出一种交互式方法来理解与控制StyleGAN2与BigGAN的输出。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型在质量和普及度上提升，需要可解释且可交互的工具帮助用户通过直接操作模型内部机制来理解模型行为，但这方面研究较少。

Method: 在已有生成网络（StyleGAN2、BigGAN）中，将固定的激活函数替换为参数化激活函数，并提供用户界面或接口让用户直接调整这些参数以观察输出变化，进而实现交互式实验与控制。

Result: 在FFHQ上的StyleGAN2和ImageNet上的BigGAN实验证明，该方法能通过调节激活函数参数控制生成图像的属性，展示了可控性和解释性的潜力。

Conclusion: 本文提出通过替换生成网络的激活函数为可参数化函数并允许用户设置参数来实现可交互的模型可控性，从而帮助用户理解和控制生成模型的输出。

Abstract: As image generative models continue to increase not only in their fidelity
but also in their ubiquity the development of tools that leverage direct
interaction with their internal mechanisms in an interpretable way has received
little attention In this work we introduce a system that allows users to
develop a better understanding of the model through interaction and
experimentation By giving users the ability to replace activation functions of
a generative network with parametric ones and a way to set the parameters of
these functions we introduce an alternative approach to control the networks
output We demonstrate the use of our method on StyleGAN2 and BigGAN networks
trained on FFHQ and ImageNet respectively.

</details>


### [77] [ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection](https://arxiv.org/abs/2510.15783)
*Haowei Zhu,Tianxiang Pan,Rui Qin,Jun-Hai Yong,Bin Wang*

Main category: cs.CV

TL;DR: ReCon在扩散模型采样中加入区域级反馈纠正与区域对齐交叉注意力，显著提升生成用于目标检测的数据的语义一致性与位置准确性，实验证明能稳定提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 生成式数据增强常依赖复杂后处理或大规模微调，且易出现内容-位置不匹配和语义泄露，限制了其在检测任务中的效果与可用性。

Method: 在结构可控的生成模型（扩散模型）中引入区域引导的纠正（region-guided rectification）——利用预训练感知模型对生成区域给予反馈并在采样过程中修正；并设计区域对齐交叉注意力（region-aligned cross-attention）以强化图像区域与文本提示之间的空间语义对齐。

Result: 通过在多数据集、多骨干和不同数据规模上的大量实验，ReCon在生成数据质量与可训练性方面均有显著提升，带来一致的性能增益。

Conclusion: 该论文提出ReCon，通过在扩散采样过程中整合区域引导的纠正机制以及区域对齐的交叉注意力，解决生成数据在语义对齐与位置一致性方面的问题，从而提升用于目标检测的数据增强效果。

Abstract: The scale and quality of datasets are crucial for training robust perception
models. However, obtaining large-scale annotated data is both costly and
time-consuming. Generative models have emerged as a powerful tool for data
augmentation by synthesizing samples that adhere to desired distributions.
However, current generative approaches often rely on complex post-processing or
extensive fine-tuning on massive datasets to achieve satisfactory results, and
they remain prone to content-position mismatches and semantic leakage. To
overcome these limitations, we introduce ReCon, a novel augmentation framework
that enhances the capacity of structure-controllable generative models for
object detection. ReCon integrates region-guided rectification into the
diffusion sampling process, using feedback from a pre-trained perception model
to rectify misgenerated regions within diffusion sampling process. We further
propose region-aligned cross-attention to enforce spatial-semantic alignment
between image regions and their textual cues, thereby improving both semantic
consistency and overall image fidelity. Extensive experiments demonstrate that
ReCon substantially improve the quality and trainability of generated data,
achieving consistent performance gains across various datasets, backbone
architectures, and data scales. Our code is available at
https://github.com/haoweiz23/ReCon .

</details>


### [78] [ERNet: Efficient Non-Rigid Registration Network for Point Sequences](https://arxiv.org/abs/2510.15800)
*Guangzhao He,Yuxi Xiao,Zhen Xu,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 提出ERNet，通过两阶段变形图预测与滑动窗口时间精化，实现更准确、鲁棒且快速的非刚性点云序列配准。


<details>
  <summary>Details</summary>
Motivation: 传统配准方法受非凸目标的局部极小值与长序列中误差累积影响，导致在噪声或部分观测下鲁棒性差和跟踪失败。需要一种高效、鲁棒且能利用时间信息的端到端方法。

Method: 两阶段流水线：第一阶段逐帧估计粗略图节点作为初始值；第二阶段在滑动窗口内对节点轨迹进行时间上精化，预测一系列变形图。模型为在大变形数据集上训练的前馈网络，能处理噪声与部分观测，并利用时间信息实现一致的序列配准。

Result: 在DeformingThings4D和D-FAUST数据集上优于现有最优方法，并在效率上相比先前最佳方案提升超过4倍。

Conclusion: ERNet通过可扩展的数据驱动前馈模型与两阶段变形图预测，有效提升了非刚性点云序列配准的准确性与鲁棒性，同时大幅提升效率。

Abstract: Registering an object shape to a sequence of point clouds undergoing
non-rigid deformation is a long-standing challenge. The key difficulties stem
from two factors: (i) the presence of local minima due to the non-convexity of
registration objectives, especially under noisy or partial inputs, which
hinders accurate and robust deformation estimation, and (ii) error accumulation
over long sequences, leading to tracking failures. To address these challenges,
we introduce to adopt a scalable data-driven approach and propose ERNet, an
efficient feed-forward model trained on large deformation datasets. It is
designed to handle noisy and partial inputs while effectively leveraging
temporal information for accurate and consistent sequential registration. The
key to our design is predicting a sequence of deformation graphs through a
two-stage pipeline, which first estimates frame-wise coarse graph nodes for
robust initialization, before refining their trajectories over time in a
sliding-window fashion. Extensive experiments show that our proposed approach
(i) outperforms previous state-of-the-art on both the DeformingThings4D and
D-FAUST datasets, and (ii) achieves more than 4x speedup compared to the
previous best, offering significant efficiency improvement.

</details>


### [79] [VISTA: A Test-Time Self-Improving Video Generation Agent](https://arxiv.org/abs/2510.15831)
*Do Xuan Long,Xingchen Wan,Hootan Nakhost,Chen-Yu Lee,Tomas Pfister,Sercan Ö. Arık*

Main category: cs.CV

TL;DR: VISTA是一个多智能体循环系统，通过计划分解、锦标赛筛选与多维度批评-重写提示的闭环改进文本到视频生成，显著提高质量与意图对齐。


<details>
  <summary>Details</summary>
Motivation: 解决现有测试时优化方法在视频多面性（时序、视觉、音频、语义）下效果不稳定的问题，使优化过程自治化并专门针对视频的多模态特性。

Method: 将用户想法分解为时间化计划，生成多条视频候选，通过成对锦标赛选择最佳视频，再由视觉、音频、语境三个专家代理进行批评，最后由推理代理综合反馈重写提示以进行下一轮生成。

Result: 在单场景与多场景生成任务中，VISTA对比最先进基线实现了高达60%的成对胜率；人工评估中，评审者在66.4%的比较中更偏好VISTA生成的视频。

Conclusion: VISTA通过迭代多智能体流程能显著提升文本到视频生成的质量和与用户意图的一致性。

Abstract: Despite rapid advances in text-to-video synthesis, generated video quality
remains critically dependent on precise user prompts. Existing test-time
optimization methods, successful in other domains, struggle with the
multi-faceted nature of video. In this work, we introduce VISTA (Video
Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously
improves video generation through refining prompts in an iterative loop. VISTA
first decomposes a user idea into a structured temporal plan. After generation,
the best video is identified through a robust pairwise tournament. This winning
video is then critiqued by a trio of specialized agents focusing on visual,
audio, and contextual fidelity. Finally, a reasoning agent synthesizes this
feedback to introspectively rewrite and enhance the prompt for the next
generation cycle. Experiments on single- and multi-scene video generation
scenarios show that while prior methods yield inconsistent gains, VISTA
consistently improves video quality and alignment with user intent, achieving
up to 60% pairwise win rate against state-of-the-art baselines. Human
evaluators concur, preferring VISTA outputs in 66.4% of comparisons.

</details>


### [80] [Neuro-Symbolic Spatial Reasoning in Segmentation](https://arxiv.org/abs/2510.15841)
*Jiayi Lin,Jiabo Huang,Shaogang Gong*

Main category: cs.CV

TL;DR: 通过在OVSS中引入神经-符号的一阶逻辑空间关系（伪类别+模糊逻辑松弛），RelateSeg在不增加模型参数的情况下显著提升多目标场景的分割一致性与mIoU。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉-语言模型的方法在将局部补丁与未见类别相关联时缺乏对场景中对象空间关系的理解，导致分割在多类别和复杂布局下一致性差。

Method: 提出RelateSeg框架，自动提取像素级空间关系并用伪类别编码为一阶逻辑公式；每个像素同时预测语义类别和空间伪类别；将逻辑约束通过模糊逻辑松弛形式融入深度网络，作为一个额外的损失项以端到端训练。

Result: RelateSeg在四个基准数据集上平均mIoU达到最先进表现，尤其在包含多类别的图像上改进明显；仅新增一个辅助损失，无额外参数开销。

Conclusion: 本文提出将神经-符号空间推理引入开放词汇语义分割（OVSS），通过显式的一阶逻辑空间关系约束提升对未见类别和多目标场景的分割一致性。

Abstract: Open-Vocabulary Semantic Segmentation (OVSS) assigns pixel-level labels from
an open set of categories, requiring generalization to unseen and unlabelled
objects. Using vision-language models (VLMs) to correlate local image patches
with potential unseen object categories suffers from a lack of understanding of
spatial relations of objects in a scene. To solve this problem, we introduce
neuro-symbolic (NeSy) spatial reasoning in OVSS. In contrast to contemporary
VLM correlation-based approaches, we propose Relational Segmentor (RelateSeg)
to impose explicit spatial relational constraints by first order logic (FOL)
formulated in a neural network architecture. This is the first attempt to
explore NeSy spatial reasoning in OVSS. Specifically, RelateSeg automatically
extracts spatial relations, e.g., <cat, to-right-of, person>, and encodes them
as first-order logic formulas using our proposed pseudo categories. Each pixel
learns to predict both a semantic category (e.g., "cat") and a spatial pseudo
category (e.g., "right of person") simultaneously, enforcing relational
constraints (e.g., a "cat" pixel must lie to the right of a "person"). Finally,
these logic constraints are formulated in a deep network architecture by fuzzy
logic relaxation, enabling end-to-end learning of spatial-relationally
consistent segmentation. RelateSeg achieves state-of-the-art performance in
terms of average mIoU across four benchmark datasets and particularly shows
clear advantages on images containing multiple categories, with the cost of
only introducing a single auxiliary loss function and no additional parameters,
validating the effectiveness of NeSy spatial reasoning in OVSS.

</details>


### [81] [3DPR: Single Image 3D Portrait Relight using Generative Priors](https://arxiv.org/abs/2510.15846)
*Pramod Rao,Abhimitra Meka,Xilong Zhou,Gereon Fox,Mallikarjun B R,Fangneng Zhan,Tim Weyrich,Bernd Bickel,Hanspeter Pfister,Wojciech Matusik,Thabo Beeler,Mohamed Elgharib,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 3DPR结合生成模型几何先验与光台OLAT反射先验，通过triplane反射网络在潜空间生成OLAT实现从单张人像到高保真重光照与新视角渲染，效果优于既往方法。


<details>
  <summary>Details</summary>
Motivation: 单图头部重光照与新视点渲染为高度欠定问题，显式分解几何、材质、光照受模型假设限制，故引入基于生成先验与光台数据的图像驱动重光照方法以捕捉高频反射特性。

Method: 使用预训练生成头模型（供几何先验）通过编码器反演将单张输入嵌入潜空间；构建并训练基于triplane的反射网络在该潜空间上生成OLAT图像，训练数据来自新采集的大规模4K多视角OLAT光场数据集；合成生成的OLAT并根据HDRI环境贴图进行环境光照重建。

Result: 在定量与定性评估中，3DPR在保持身份特征及再现镜面高光、自阴影、次表面散射等光照效应方面明显优于既有方法。作者还发布了包含139名受试者的高分辨率多视角OLAT数据集。

Conclusion: 本文提出3DPR，利用光场（OLAT）多视角数据与预训练生成头模型潜空间，实现从单张人像图像的高保真重光照与新视角渲染，优于先前方法。

Abstract: Rendering novel, relit views of a human head, given a monocular portrait
image as input, is an inherently underconstrained problem. The traditional
graphics solution is to explicitly decompose the input image into geometry,
material and lighting via differentiable rendering; but this is constrained by
the multiple assumptions and approximations of the underlying models and
parameterizations of these scene components. We propose 3DPR, an image-based
relighting model that leverages generative priors learnt from multi-view
One-Light-at-A-Time (OLAT) images captured in a light stage. We introduce a new
diverse and large-scale multi-view 4K OLAT dataset of 139 subjects to learn a
high-quality prior over the distribution of high-frequency face reflectance. We
leverage the latent space of a pre-trained generative head model that provides
a rich prior over face geometry learnt from in-the-wild image datasets. The
input portrait is first embedded in the latent manifold of such a model through
an encoder-based inversion process. Then a novel triplane-based reflectance
network trained on our lightstage data is used to synthesize high-fidelity OLAT
images to enable image-based relighting. Our reflectance network operates in
the latent space of the generative head model, crucially enabling a relatively
small number of lightstage images to train the reflectance model. Combining the
generated OLATs according to a given HDRI environment maps yields physically
accurate environmental relighting results. Through quantitative and qualitative
evaluations, we demonstrate that 3DPR outperforms previous methods,
particularly in preserving identity and in capturing lighting effects such as
specularities, self-shadows, and subsurface scattering. Project Page:
https://vcai.mpi-inf.mpg.de/projects/3dpr/

</details>


### [82] [Memory-SAM: Human-Prompt-Free Tongue Segmentation via Retrieval-to-Prompt](https://arxiv.org/abs/2510.15849)
*Joongwon Chae,Lihui Luo,Xi Yuan,Dongmei Yu,Zhenglin Chen,Lian Zhang,Peiwu Qin*

Main category: cs.CV

TL;DR: Memory-SAM通过检索-蒸馏提示驱动SAM2，提供无需训练与人工提示的高效舌面分割方案，在真实场景下表现优异并公开了代码。


<details>
  <summary>Details</summary>
Motivation: 受中医舌象分析对分割精度的高需求与标注数据稀缺的限制，提出一种数据高效且免人工参与的提示生成方法以提升SAM在舌面分割中的实用性。

Method: 使用DINOv3提取密集特征、利用FAISS进行最近邻检索，从检索到的示例与查询图像之间建立受掩码约束的对应关系，并将这些对应关系转换为前景/背景点提示，直接输入SAM2以生成分割，无需微调或手工点击。

Result: 在600张专家标注图像（300受控、300真实）上评估，混合测试集mIoU达0.9863，远超FCN（0.8188）和基于检测器的SAM框基线（0.1839）；在受控数据上由于标注上限效应差异不显著，但在真实世界场景下展现明显优势。

Conclusion: Memory-SAM通过检索相似样本并将相应关系蒸馏为点提示，引导SAM2实现无监督、免人工提示的舌面分割，能在混合和真实场景下显著超过传统分割器，尤其在真实世界条件下更鲁棒。

Abstract: Accurate tongue segmentation is crucial for reliable TCM analysis. Supervised
models require large annotated datasets, while SAM-family models remain
prompt-driven. We present Memory-SAM, a training-free, human-prompt-free
pipeline that automatically generates effective prompts from a small memory of
prior cases via dense DINOv3 features and FAISS retrieval. Given a query image,
mask-constrained correspondences to the retrieved exemplar are distilled into
foreground/background point prompts that guide SAM2 without manual clicks or
model fine-tuning. We evaluate on 600 expert-annotated images (300 controlled,
300 in-the-wild). On the mixed test split, Memory-SAM achieves mIoU 0.9863,
surpassing FCN (0.8188) and a detector-to-box SAM baseline (0.1839). On
controlled data, ceiling effects above 0.98 make small differences less
meaningful given annotation variability, while our method shows clear gains
under real-world conditions. Results indicate that retrieval-to-prompt enables
data-efficient, robust segmentation of irregular boundaries in tongue imaging.
The code is publicly available at https://github.com/jw-chae/memory-sam.

</details>


### [83] [BLIP3o-NEXT: Next Frontier of Native Image Generation](https://arxiv.org/abs/2510.15857)
*Jiuhai Chen,Le Xue,Zhiyang Xu,Xichen Pan,Shusheng Yang,Can Qin,An Yan,Honglu Zhou,Zeyuan Chen,Lifu Huang,Tianyi Zhou,Junnan Li,Silvio Savarese,Caiming Xiong,Ran Xu*

Main category: cs.CV

TL;DR: BLIP3o-NEXT把自回归模型的推理与扩散模型的细节渲染结合，辅以强化学习与数据优化，提升文本生成与图像编辑的效果。


<details>
  <summary>Details</summary>
Motivation: 推进原生图像生成（native image generation）能力，统一文本到图像与图像编辑任务，提升指令跟随与生成-参考图像一致性。

Method: 提出了Autoregressive + Diffusion混合架构：先用自回归模型生成离散图像token及隐藏态，再将隐藏态作为扩散模型的条件，生产高保真图像；并结合后训练、数据引擎与强化学习优化。

Result: 在多个文本到图像和图像编辑基准上，BLIP3o-NEXT优于现有模型，展示更好的连贯性与真实感。

Conclusion: BLIP3o-NEXT通过将自回归与扩散模型结合，取得了文本生成与图像编辑任务上的显著提升，表明混合架构在一致性与细节还原上具有优势。

Abstract: We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3
series that advances the next frontier of native image generation. BLIP3o-NEXT
unifies text-to-image generation and image editing within a single
architecture, demonstrating strong image generation and image editing
capabilities. In developing the state-of-the-art native image generation model,
we identify four key insights: (1) Most architectural choices yield comparable
performance; an architecture can be deemed effective provided it scales
efficiently and supports fast inference; (2) The successful application of
reinforcement learning can further push the frontier of native image
generation; (3) Image editing still remains a challenging task, yet instruction
following and the consistency between generated and reference images can be
significantly enhanced through post-training and data engine; (4) Data quality
and scale continue to be decisive factors that determine the upper bound of
model performance. Building upon these insights, BLIP3o-NEXT leverages an
Autoregressive + Diffusion architecture in which an autoregressive model first
generates discrete image tokens conditioned on multimodal inputs, whose hidden
states are then used as conditioning signals for a diffusion model to generate
high-fidelity images. This architecture integrates the reasoning strength and
instruction following of autoregressive models with the fine-detail rendering
ability of diffusion models, achieving a new level of coherence and realism.
Extensive evaluations of various text-to-image and image-editing benchmarks
show that BLIP3o-NEXT achieves superior performance over existing models.

</details>


### [84] [BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models](https://arxiv.org/abs/2510.15866)
*Kaushitha Silva,Mansitha Eashwara,Sanduni Ubayasiri,Ruwan Tennakoon,Damayanthi Herath*

Main category: cs.CV

TL;DR: BiomedXPro用进化策略和大语言模型生成多样、可解释的自然语言提示集合，提升医疗视觉-语言模型性能并增强预测的可验证性，特别适用于少样本场景。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法输出不可解释的潜在向量或单一文本提示，缺乏透明性且无法反映临床诊断所需的多维观察，限制了在高风险医疗场景中的可信度与可验证性。

Method: 提出一种基于进化策略的框架，利用大型语言模型同时作为生物医学知识提取器和自适应优化器，自动生成可解释的提示对（prompt pairs）的集合，并采用多样性驱动的选择机制优化提示组合。

Result: 在多个生物医学基准上，BiomedXPro优于最新提示微调方法，尤其在少样本设置中表现显著；发现的提示与统计显著的临床特征语义上高度一致，从而为模型预测提供可验证依据。

Conclusion: BiomedXPro通过生成多样且可解释的自然语言提示对医疗视觉-语言模型进行优化，从而提高诊断性能并增强可验证性和临床对齐性。

Abstract: The clinical adoption of biomedical vision-language models is hindered by
prompt optimization techniques that produce either uninterpretable latent
vectors or single textual prompts. This lack of transparency and failure to
capture the multi-faceted nature of clinical diagnosis, which relies on
integrating diverse observations, limits their trustworthiness in high-stakes
settings. To address this, we introduce BiomedXPro, an evolutionary framework
that leverages a large language model as both a biomedical knowledge extractor
and an adaptive optimizer to automatically generate a diverse ensemble of
interpretable, natural-language prompt pairs for disease diagnosis. Experiments
on multiple biomedical benchmarks show that BiomedXPro consistently outperforms
state-of-the-art prompt-tuning methods, particularly in data-scarce few-shot
settings. Furthermore, our analysis demonstrates a strong semantic alignment
between the discovered prompts and statistically significant clinical features,
grounding the model's performance in verifiable concepts. By producing a
diverse ensemble of interpretable prompts, BiomedXPro provides a verifiable
basis for model predictions, representing a critical step toward the
development of more trustworthy and clinically-aligned AI systems.

</details>


### [85] [LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal](https://arxiv.org/abs/2510.15868)
*Shr-Ruei Tsai,Wei-Cheng Chang,Jie-Ying Lee,Chih-Hai Su,Yu-Lun Liu*

Main category: cs.CV

TL;DR: LightsOut使用LoRA微调的扩散外延和多任务回归来重建帧外光源，作为通用预处理显著提升单张图像去耀斑方法的效果。


<details>
  <summary>Details</summary>
Motivation: 现有SIFR方法在帧外光源不完整或缺失时表现差；通过重建这些光源可改善去耀斑效果。

Method: 提出基于扩散模型的帧外光源外延(outpainting)框架，结合多任务回归模块预测光源相关属性，并采用LoRA对扩散模型微调以生成真实且物理一致的外延结果。

Result: 实验表明LightsOut在多种困难场景下均能稳定提升现有SIFR方法的性能，且不需对下游模型进行额外重训练，具备通用的插拔式预处理能力。

Conclusion: LightsOut通过重建帧外光源，提高了单张图像耀斑去除的效果；该方法作为一个前处理模块可无缝配合现有SIFR模型，提升性能。

Abstract: Lens flare significantly degrades image quality, impacting critical computer
vision tasks like object detection and autonomous driving. Recent Single Image
Flare Removal (SIFR) methods perform poorly when off-frame light sources are
incomplete or absent. We propose LightsOut, a diffusion-based outpainting
framework tailored to enhance SIFR by reconstructing off-frame light sources.
Our method leverages a multitask regression module and LoRA fine-tuned
diffusion model to ensure realistic and physically consistent outpainting
results. Comprehensive experiments demonstrate LightsOut consistently boosts
the performance of existing SIFR methods across challenging scenarios without
additional retraining, serving as a universally applicable plug-and-play
preprocessing solution. Project page: https://ray-1026.github.io/lightsout/

</details>


### [86] [Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery](https://arxiv.org/abs/2510.15869)
*Jie-Ying Lee,Yi-Ruei Liu,Shr-Ruei Tsai,Wei-Cheng Chang,Chung-Ho Wu,Jiewen Chan,Zhenjun Zhao,Chieh Hubert Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Skyfall-GS 通过结合卫星影像与扩散模型，并采用课程驱动迭代细化，实现了无需三维标注的街区级可探索写实三维场景生成。


<details>
  <summary>Details</summary>
Motivation: 缺乏大规模高质量真实三维扫描数据使得训练可泛化的生成模型困难，因此作者转而利用易得的卫星影像与强大的扩散模型来构建大规模三维城市场景。

Method: 利用卫星影像提供的粗略几何轮廓与开域扩散模型生成近景高质量外观，采用课程驱动的迭代细化策略逐步提升几何完整性与纹理质量，并实现实时沉浸式三维探索。

Result: 在大规模实验中，Skyfall-GS 在视角一致性几何和纹理真实感方面均优于现有最先进方法，并支持街区尺度的实时交互浏览。

Conclusion: Skyfall-GS 成功提出了一个无须昂贵三维标注即可在街区尺度生成可探索、高质量的三维城市场景的方法，结合卫星影像与扩散模型，能够在几何完整性与纹理写实性上超越现有方法。

Abstract: Synthesizing large-scale, explorable, and geometrically accurate 3D urban
scenes is a challenging yet valuable task in providing immersive and embodied
applications. The challenges lie in the lack of large-scale and high-quality
real-world 3D scans for training generalizable generative models. In this
paper, we take an alternative route to create large-scale 3D scenes by
synergizing the readily available satellite imagery that supplies realistic
coarse geometry and the open-domain diffusion model for creating high-quality
close-up appearances. We propose \textbf{Skyfall-GS}, the first city-block
scale 3D scene creation framework without costly 3D annotations, also featuring
real-time, immersive 3D exploration. We tailor a curriculum-driven iterative
refinement strategy to progressively enhance geometric completeness and
photorealistic textures. Extensive experiments demonstrate that Skyfall-GS
provides improved cross-view consistent geometry and more realistic textures
compared to state-of-the-art approaches. Project page:
https://skyfall-gs.jayinnn.dev/

</details>


### [87] [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://arxiv.org/abs/2510.15870)
*Hanrong Ye,Chao-Han Huck Yang,Arushi Goel,Wei Huang,Ligeng Zhu,Yuanhang Su,Sean Lin,An-Chieh Cheng,Zhen Wan,Jinchuan Tian,Yuming Lou,Dong Yang,Zhijian Liu,Yukang Chen,Ambrish Dantrey,Ehsan Jahangiri,Sreyan Ghosh,Daguang Xu,Ehsan Hosseini-Asl,Danial Mohseni Taheri,Vidya Murali,Sifei Liu,Jason Lu,Oluwatobi Olabiyi,Frank Wang,Rafael Valle,Bryan Catanzaro,Andrew Tao,Song Han,Jan Kautz,Hongxu Yin,Pavlo Molchanov*

Main category: cs.CV

TL;DR: OmniVinci通过架构与数据双重创新，实现了少量训练数据下的高性能全模态大模型，提升跨模态感知与推理能力。


<details>
  <summary>Details</summary>
Motivation: 模拟人类多模态感知能力，提升模型对视觉、音频及其时序关系的联合理解，从而推动通用感知与推理。

Method: 提出三项架构创新（OmniAlignNet、Temporal Embedding Grouping、Constrained Rotary Time Embedding），并配合一个生成24M单模态与全模态对话的数据整理与合成流程。

Result: 在DailyOmni、MMAR、Video-MME等数据集上分别超越Qwen2.5-Omni，且训练tokens仅为0.2T，显著降低训练成本；并展示在机器人、医疗AI和智能工厂等下游任务的优势。

Conclusion: OmniVinci提出了一种高效的全模态大模型设计，通过架构创新和大规模模态对齐的数据构建，在更少训练tokens下实现了优异的跨模态理解与推理能力。

Abstract: Advancing machine intelligence requires developing the ability to perceive
across multiple modalities, much as humans sense the world. We introduce
OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We
carefully study the design choices across model architecture and data curation.
For model architecture, we present three key innovations: (i) OmniAlignNet for
strengthening alignment between vision and audio embeddings in a shared
omni-modal latent space; (ii) Temporal Embedding Grouping for capturing
relative temporal alignment between vision and audio signals; and (iii)
Constrained Rotary Time Embedding for encoding absolute temporal information in
omni-modal embeddings. We introduce a curation and synthesis pipeline that
generates 24M single-modal and omni-modal conversations. We find that
modalities reinforce one another in both perception and reasoning. Our model,
OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal
understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while
using just 0.2T training tokens - a 6 times reduction compared to
Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream
applications spanning robotics, medical AI, and smart factory.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [88] [TKHist: Cardinality Estimation for Join Queries via Histograms with Dominant Attribute Correlation Finding](https://arxiv.org/abs/2510.15368)
*Renrui Li,Qingzhi Ma,Jiajie Xu,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: TKHist通过在直方图中编码bin内非均匀性并发现主导连接路径相关性，显著降低多表连接基数估计误差（误差方差降2-3个数量级），同时保持或降低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有多表连接基数估计方法虽然准确性提升，但带来更高的空间开销、延迟和与二元连接框架集成的复杂性，需要一种在不显著增加资源消耗的前提下保持高精度的解决方案；此外，属性独立性假设在多表连接下往往导致严重过估，需要识别和处理重要相关性。

Method: 提出TKHist直方图，保存bin级别的非均匀性信息以改善选择性估计；设计主导连接路径相关性发现算法来识别连接键与过滤谓词之间的关键相关性，从而在多表连接估计中校正属性独立性假设的误差。

Result: 在多个流行基准上，TKHist将估计误差方差降低了2-3个数量级，同时内存使用量与SOTA方法相当或更低，证明了其在精度与资源消耗之间的良好权衡。

Conclusion: TKHist在不显著增加内存开销和延迟的情况下，通过在直方图中放宽均匀性假设并记录bin内非均匀信息，实现了对无过滤谓词的多表连接查询的精确基数估计；同时，通过主导连接路径相关性发现算法识别并管理连接键与过滤谓词间的重要相关性，避免了独立性假设导致的严重过度估计。

Abstract: Cardinality estimation has long been crucial for cost-based database
optimizers in identifying optimal query execution plans, attracting significant
attention over the past decades. While recent advancements have significantly
improved the accuracy of multi-table join query estimations, these methods
introduce challenges such as higher space overhead, increased latency, and
greater complexity, especially when integrated with the binary join framework.
In this paper, we introduce a novel cardinality estimation method named TKHist,
which addresses these challenges by relaxing the uniformity assumption in
histograms. TKHist captures bin-wise non-uniformity information, enabling
accurate cardinality estimation for join queries without filter predicates.
Furthermore, we explore the attribute independent assumption, which can lead to
significant over-estimation rather than under-estimation in multi-table join
queries. To address this issue, we propose the dominating join path correlation
discovery algorithm to highlight and manage correlations between join keys and
filter predicates. Our extensive experiments on popular benchmarks demonstrate
that TKHist reduces error variance by 2-3 orders of magnitude compared to SOTA
methods, while maintaining comparable or lower memory usage.

</details>


### [89] [Optimizing Data Lakes' Queries](https://arxiv.org/abs/2510.15445)
*Gregory,Weintraub*

Main category: cs.DB

TL;DR: 提出并形式化“查询覆盖集合”概念，目标为为每个查询找出最小需访问文件子集，仅在该子集上执行查询，从而减少数据传输并提升云数据湖的查询性能。


<details>
  <summary>Details</summary>
Motivation: 动机是云数据湖中计算与存储分离导致每次查询都需将数据从存储传输到计算节点，造成性能下降和网络带宽高消耗，因此需要策略减少数据传输并提升查询效率。

Method: 作者建立了一个严格的理论框架，形式化问题和权衡，基于覆盖集合最小化目标设计方法，针对每个查询识别最小覆盖集合，并仅对该子集执行查询以提升性能。

Result: 结果表明，按最小查询覆盖集合执行查询能够显著减少需要传输的数据量，从而改善查询延迟和网络使用，实验（或理论分析）支持该方法的有效性。

Conclusion: 论文的结论是：通过定义并利用“查询覆盖集合”（每个查询所需访问的最小文件集），可以在分离计算与存储的云数据湖架构中显著提升查询性能并降低网络带宽消耗。

Abstract: Cloud data lakes provide a modern solution for managing large volumes of
data. The fundamental principle behind these systems is the separation of
compute and storage layers. In this architecture, inexpensive cloud storage is
utilized for data storage, while compute engines are employed to perform
analytics on this data in an "on-demand" mode. However, to execute any
calculations on the data, it must be transferred from the storage layer to the
compute layer over the network for each query. This transfer can negatively
impact calculation performance and requires significant network bandwidth. In
this thesis, we examine various strategies to enhance query performance within
a cloud data lake architecture. We begin by formalizing the problem and
proposing a straightforward yet robust theoretical framework that clearly
outlines the associated trade-offs. Central to our framework is the concept of
a "query coverage set," which is defined as the collection of files that need
to be accessed from storage to fulfill a specific query. Our objective is to
identify the minimal coverage set for each query and execute the query
exclusively on this subset of files. This approach enables us to significantly
improve query performance.

</details>
