<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 183]
- [cs.DB](#cs.DB) [Total: 14]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Generative human motion mimicking through feature extraction in denoising diffusion settings](https://arxiv.org/abs/2511.00011)
*Alexander Okupnik,Johannes Schneider,Kyriakos Flouris*

Main category: cs.CV

TL;DR: 提出首个仅基于单人MoCap和高层特征的交互式舞蹈生成模型，结合扩散、动作修补与风格迁移，能生成时间连贯、回应参考动作且具有创意偏离的舞蹈动作，定量评估显示结果既多样又真实。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推动了人机口头交互的发展，但缺乏具身性；舞蹈作为人类原始表达形式，可为这种互动补充具身维度，探索创意人机互动的可能性。

Method: 模型结合了两类扩散模型、动作修补（motion inpainting）和动作风格迁移（motion style transfer）思想，利用高层次特征而非低层人际交互数据，生成时间上连贯且对参考动作响应的运动表示。

Result: 通过量化评估生成样本与测试集（模拟人类表演者）特征分布的收敛性，证明生成动作既多样（在保持现实感的同时出现对人类伙伴的各种偏离），又现实可信，体现为创意舞蹈的初步成果。

Conclusion: 该论文成功提出了一个基于单人MoCap数据的交互式舞蹈生成模型，能够部分模仿并富有创造性地增强输入动作序列，展示了人机共舞中创意互动的可能性。

Abstract: Recent success with large language models has sparked a new wave of verbal
human-AI interaction. While such models support users in a variety of creative
tasks, they lack the embodied nature of human interaction. Dance, as a primal
form of human expression, is predestined to complement this experience. To
explore creative human-AI interaction exemplified by dance, we build an
interactive model based on motion capture (MoCap) data. It generates an
artificial other by partially mimicking and also "creatively" enhancing an
incoming sequence of movement data. It is the first model, which leverages
single-person motion data and high level features in order to do so and, thus,
it does not rely on low level human-human interaction data. It combines ideas
of two diffusion models, motion inpainting, and motion style transfer to
generate movement representations that are both temporally coherent and
responsive to a chosen movement reference. The success of the model is
demonstrated by quantitatively assessing the convergence of the feature
distribution of the generated samples and the test set which serves as
simulating the human performer. We show that our generations are first steps to
creative dancing with AI as they are both diverse showing various deviations
from the human partner while appearing realistic.

</details>


### [2] [Deep Learning Models for Coral Bleaching Classification in Multi-Condition Underwater Image Datasets](https://arxiv.org/abs/2511.00021)
*Julio Jerison E. Macrohon,Gordon Hung*

Main category: cs.CV

TL;DR: 本文构建并比较了ResNet、ViT与CNN三类模型用于多环境下珊瑚白化分类，经过调参后CNN以88%准确率领先，展示了自动化珊瑚监测的可行性与模型选择的实证对比。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁面临污染、海水酸化和海温异常等威胁，迫切需要高效的监测与保护手段，因而研究自动化珊瑚白化检测与分类方法。

Method: 收集包含健康与白化珊瑚的多环境（深海、沼泽、沿海）图像数据集，对ResNet、ViT和CNN进行基线测试并通过全面的超参数调优优化模型性能，最终选择最佳模型。

Result: 经过超参数调优后，自建CNN在测试集上达到最高88%准确率，优于ResNet与ViT，表明传统卷积网络在该任务上仍具优势。

Conclusion: 该研究提出了一种基于机器学习的珊瑚白化分类系统，并在多样化的全球数据集上比较了ResNet、ViT和自建CNN三种模型，最终CNN取得了88%准确率。

Abstract: Coral reefs support numerous marine organisms and are an important source of
coastal protection from storms and floods, representing a major part of marine
ecosystems. However coral reefs face increasing threats from pollution, ocean
acidification, and sea temperature anomalies, making efficient protection and
monitoring heavily urgent. Therefore, this study presents a novel
machine-learning-based coral bleaching classification system based on a diverse
global dataset with samples of healthy and bleached corals under varying
environmental conditions, including deep seas, marshes, and coastal zones. We
benchmarked and compared three state-of-the-art models: Residual Neural Network
(ResNet), Vision Transformer (ViT), and Convolutional Neural Network (CNN).
After comprehensive hyperparameter tuning, the CNN model achieved the highest
accuracy of 88%, outperforming existing benchmarks. Our findings offer
important insights into autonomous coral monitoring and present a comprehensive
analysis of the most widely used computer vision models.

</details>


### [3] [Automating Coral Reef Fish Family Identification on Video Transects Using a YOLOv8-Based Deep Learning Pipeline](https://arxiv.org/abs/2511.00022)
*Jules Gerard,Leandro Di Bella,Filip Huyghe,Marc Kochzius*

Main category: cs.CV

TL;DR: 使用YOLOv8在肯尼亚和坦桑尼亚视频上进行24科珊瑚礁鱼自动识别，最佳mAP@0.5=0.52，适合常见科，稀有/复杂科仍需改进。


<details>
  <summary>Details</summary>
Motivation: 减轻水下目视普查的人工负担，提升珊瑚礁鱼类监测的规模化与自动化能力，提供区域特定的基准测试。

Method: 基于YOLOv8的深度学习检测管线，对肯尼亚和坦桑尼亚的视频样带进行家族级标注与训练，使用含24个科的精心挑选数据集进行多配置测试与评估。

Result: 最佳模型在mAP@0.5上达到0.52，对丰度较高的科表现良好，但对稀有或形态复杂的科检测较弱。该工作为西印度洋自动化礁鱼监测提供了首个区域性基准。

Conclusion: 深度学习能在西印度洋珊瑚礁鱼类监测中作为传统目视调查的可扩展补充，但目前对稀有或形态复杂科的检测仍有限。

Abstract: Coral reef monitoring in the Western Indian Ocean is limited by the labor
demands of underwater visual censuses. This work evaluates a YOLOv8-based deep
learning pipeline for automating family-level fish identification from video
transects collected in Kenya and Tanzania. A curated dataset of 24 families was
tested under different configurations, providing the first region-specific
benchmark for automated reef fish monitoring in the Western Indian Ocean. The
best model achieved mAP@0.5 of 0.52, with high accuracy for abundant families
but weaker detection of rare or complex taxa. Results demonstrate the potential
of deep learning as a scalable complement to traditional monitoring methods.

</details>


### [4] [Mutual Information guided Visual Contrastive Learning](https://arxiv.org/abs/2511.00028)
*Hanyang Chen,Yanchao Yang*

Main category: cs.CV

TL;DR: 通过基于自然扰动下互信息挑选正样本用于对比学习，替代传统人工数据增强，能更好地学习泛化不变特征，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于InfoNCE的表示学习依赖于人工设计的数据增强（如颜色抖动）来模拟现实扰动，但这些假设可能不充分或次优；利用真实世界互信息选择样本可更直接捕捉自然扰动下的不变性。

Method: 计算场景中不同patch在自然扰动（如颜色变化和运动）下的互信息，并将互信息高的patch对作为对比学习的正样本进行训练；在多种代表性学习框架与基准上进行评估对比。

Result: 在多种最新的表征学习框架和基准上，所提方法展示了有效性，提升了学习特征在开放环境中的泛化表现，表明基于互信息的数据选择是一个有前景的研究方向。

Conclusion: 本文提出通过基于真实世界分布的互信息来选择训练样本作为对比学习中的正样本，从而替代或补充手工设计的数据增强策略，以提升表征的泛化能力。

Abstract: Representation learning methods utilizing the InfoNCE loss have demonstrated
considerable capacity in reducing human annotation effort by training invariant
neural feature extractors. Although different variants of the training
objective adhere to the information maximization principle between the data and
learned features, data selection and augmentation still rely on human
hypotheses or engineering, which may be suboptimal. For instance, data
augmentation in contrastive learning primarily focuses on color jittering,
aiming to emulate real-world illumination changes. In this work, we investigate
the potential of selecting training data based on their mutual information
computed from real-world distributions, which, in principle, should endow the
learned features with better generalization when applied in open environments.
Specifically, we consider patches attached to scenes that exhibit high mutual
information under natural perturbations, such as color changes and motion, as
positive samples for learning with contrastive loss. We evaluate the proposed
mutual-information-informed data augmentation method on several benchmarks
across multiple state-of-the-art representation learning frameworks,
demonstrating its effectiveness and establishing it as a promising direction
for future research.

</details>


### [5] [Benchmarking Federated Learning Frameworks for Medical Imaging Deployment: A Comparative Study of NVIDIA FLARE, Flower, and Owkin Substra](https://arxiv.org/abs/2511.00037)
*Riya Gupta,Alexander Chowdhury,Sahil Nalawade*

Main category: cs.CV

TL;DR: 在医疗影像的联邦学习基准测试中：NVIDIA FLARE适合生产部署，Flower适合研究与原型，Owkin Substra适合高度合规需求；选择应基于场景与权衡。


<details>
  <summary>Details</summary>
Motivation: 医疗影像领域对数据隐私与跨机构协作有迫切需求，联邦学习能在不共享原始数据下联合训练模型。本研究旨在为医疗机构和开发者提供对主流FL框架在真实应用场景中表现的实证比较，帮助选型与部署决策。

Method: 基于PathMNIST数据集在三个框架上训练相同的模型，比较模型性能（准确率/指标）、收敛速度、通信开销、扩展性（节点增多时的表现）及开发者体验（部署复杂度、文档与API友好性）。统计多次实验结果并记录训练时间、通信轮次与模型性能变化。

Result: 实验发现：在相同模型与数据划分下，三框架的最终模型性能相近；FLARE在多节点扩展与生产部署中训练时间与系统稳定性更佳；Flower在快速迭代、定制通信策略和实验可重复性方面最优；Substra提供强大的隐私、合规与审计功能，但部署与集成成本较高。通信开销和收敛轮次在各框架间差异有限，更多取决于算法与网络带宽配置。

Conclusion: 三种联邦学习框架各有侧重：NVIDIA FLARE在生产级可扩展性和部署方面表现最佳；Flower在原型验证和学术研究中最灵活；Owkin Substra在隐私合规与数据治理方面具有显著优势。没有单一框架适用于所有场景，应基于具体需求选择。

Abstract: Federated Learning (FL) has emerged as a transformative paradigm in medical
AI, enabling collaborative model training across institutions without direct
data sharing. This study benchmarks three prominent FL frameworks NVIDIA FLARE,
Flower, and Owkin Substra to evaluate their suitability for medical imaging
applications in real-world settings. Using the PathMNIST dataset, we assess
model performance, convergence efficiency, communication overhead, scalability,
and developer experience. Results indicate that NVIDIA FLARE offers superior
production scalability, Flower provides flexibility for prototyping and
academic research, and Owkin Substra demonstrates exceptional privacy and
compliance features. Each framework exhibits strengths optimized for distinct
use cases, emphasizing their relevance to practical deployment in healthcare
environments.

</details>


### [6] [Enhancing rice leaf images: An overview of image denoising techniques](https://arxiv.org/abs/2511.00046)
*Rupjyoti Chutia,Dibya Jyoti Bora*

Main category: cs.CV

TL;DR: 文章在稻叶图像上比较了多种去噪方法加CLAHE的组合，发现边缘保留型去噪+CLAHE在保细节且提升对比度方面效果最佳，有助于后续作物病害分析。


<details>
  <summary>Details</summary>
Motivation: 提高稻叶图像的质量以增强病害检测、营养缺陷评估和生长分析的准确性；鉴于噪声和对比度不佳会影响后续分割与特征提取，研究组合去噪与CLAHE的效果具有实际意义。

Method: 以稻叶图像数据集为基础，先对图像进行去噪（比较多种常见滤波器，如均值滤波、中值滤波、高斯滤波、双边滤波、非局部均值等），然后对去噪结果应用CLAHE进行对比度限制的自适应直方图均衡化。采用多种图像质量评价指标（如PSNR、SSIM、MSE、熵、边缘保存指标等）和可视化对比来评估方法效果。

Result: 实验表明：基于边缘保持能力较好的去噪方法（例如双边滤波或非局部均值）结合CLAHE通常在PSNR与SSIM指标上表现较好，同时能更好地保留叶脉和病斑细节；简单的均值滤波虽能去噪但会模糊细节，影响后续分析。不同方法在不同图像噪声类型下表现差异明显。

Conclusion: 本文比较了多种图像去噪方法与CLAHE结合用于稻叶图像增强，结论为某些去噪方法在保边缘和细节同时与CLAHE配合可显著提升稻叶图像的质量，从而有利于后续病害检测与特征提取。

Abstract: Digital image processing involves the systematic handling of images using
advanced computer algorithms, and has gained significant attention in both
academic and practical fields. Image enhancement is a crucial preprocessing
stage in the image-processing chain, improving image quality and emphasizing
features. This makes subsequent tasks (segmentation, feature extraction,
classification) more reliable. Image enhancement is essential for rice leaf
analysis, aiding in disease detection, nutrient deficiency evaluation, and
growth analysis. Denoising followed by contrast enhancement are the primary
steps. Image filters, generally employed for denoising, transform or enhance
visual characteristics like brightness, contrast, and sharpness, playing a
crucial role in improving overall image quality and enabling the extraction of
useful information. This work provides an extensive comparative study of
well-known image-denoising methods combined with CLAHE (Contrast Limited
Adaptive Histogram Equalization) for efficient denoising of rice leaf images.
The experiments were performed on a rice leaf image dataset to ensure the data
is relevant and representative. Results were examined using various metrics to
comprehensively test enhancement methods. This approach provides a strong basis
for assessing the effectiveness of methodologies in digital image processing
and reveals insights useful for future adaptation in agricultural research and
other domains.

</details>


### [7] [Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?](https://arxiv.org/abs/2511.00060)
*Zhiqi Qi,Runxin Zhao,Hanyang Zhuang,Chunxiang Wang,Ming Yang*

Main category: cs.CV

TL;DR: 在CARLA仿真中比较重复与非重复扫描路侧LiDAR，构建了公开基准并发现非重复扫描在检测性能上可与128线重复扫描相当，且更具成本效益，尽管感知距离受限。


<details>
  <summary>Details</summary>
Motivation: 目前关于路侧LiDAR最优布置已有大量研究，但不同扫描模式（重复 vs 非重复）对感知性能的影响尚缺乏系统性研究。由于扫描模式改变点云随距离的分布，可能显著影响目标检测和环境理解，亟需量化比较。

Method: 在CARLA仿真中同步部署多台基础设施LiDAR（包含重复与非重复扫描模式），构建了‘InfraLiDARs' Benchmark’数据集；对点云空间分布进行统计分析；使用多种主流3D目标检测器在该数据集上进行比较实验，评估不同扫描模式对检测效果的影响。

Result: 统计与实验结果表明：非重复扫描LiDAR在检测性能上与128线重复扫描LiDAR总体相近；非重复LiDAR感知范围较小但成本低；不同扫描模式在不同场景和距离下呈现差异，但整体可通过算法调整互相补偿。并公开发布了基准数据集以促进后续研究。

Conclusion: 本文研究了不同LiDAR扫描模式对路侧感知性能的影响，结论是非重复扫描LiDAR在检测性能上与128线重复扫描LiDAR相当，且在成本上具有优势，尽管其感知距离受限。

Abstract: LiDAR-based roadside perception is a cornerstone of advanced Intelligent
Transportation Systems (ITS). While considerable research has addressed optimal
LiDAR placement for infrastructure, the profound impact of differing LiDAR
scanning patterns on perceptual performance remains comparatively
under-investigated. The inherent nature of various scanning modes - such as
traditional repetitive (mechanical/solid-state) versus emerging non-repetitive
(e.g. prism-based) systems - leads to distinct point cloud distributions at
varying distances, critically dictating the efficacy of object detection and
overall environmental understanding. To systematically investigate these
differences in infrastructure-based contexts, we introduce the "InfraLiDARs'
Benchmark," a novel dataset meticulously collected in the CARLA simulation
environment using concurrently operating infrastructure-based LiDARs exhibiting
both scanning paradigms. Leveraging this benchmark, we conduct a comprehensive
statistical analysis of the respective LiDAR scanning abilities and evaluate
the impact of these distinct patterns on the performance of various leading 3D
object detection algorithms. Our findings reveal that non-repetitive scanning
LiDAR and the 128-line repetitive LiDAR were found to exhibit comparable
detection performance across various scenarios. Despite non-repetitive LiDAR's
limited perception range, it's a cost-effective option considering its low
price. Ultimately, this study provides insights for setting up roadside
perception system with optimal LiDAR scanning patterns and compatible
algorithms for diverse roadside applications, and publicly releases the
"InfraLiDARs' Benchmark" dataset to foster further research.

</details>


### [8] [World Simulation with Video Foundation Models for Physical AI](https://arxiv.org/abs/2511.00062)
*NVIDIA,:,Arslan Ali,Junjie Bai,Maciej Bala,Yogesh Balaji,Aaron Blakeman,Tiffany Cai,Jiaxin Cao,Tianshi Cao,Elizabeth Cha,Yu-Wei Chao,Prithvijit Chattopadhyay,Mike Chen,Yongxin Chen,Yu Chen,Shuai Cheng,Yin Cui,Jenna Diamond,Yifan Ding,Jiaojiao Fan,Linxi Fan,Liang Feng,Francesco Ferroni,Sanja Fidler,Xiao Fu,Ruiyuan Gao,Yunhao Ge,Jinwei Gu,Aryaman Gupta,Siddharth Gururani,Imad El Hanafi,Ali Hassani,Zekun Hao,Jacob Huffman,Joel Jang,Pooya Jannaty,Jan Kautz,Grace Lam,Xuan Li,Zhaoshuo Li,Maosheng Liao,Chen-Hsuan Lin,Tsung-Yi Lin,Yen-Chen Lin,Huan Ling,Ming-Yu Liu,Xian Liu,Yifan Lu,Alice Luo,Qianli Ma,Hanzi Mao,Kaichun Mo,Seungjun Nah,Yashraj Narang,Abhijeet Panaskar,Lindsey Pavao,Trung Pham,Morteza Ramezanali,Fitsum Reda,Scott Reed,Xuanchi Ren,Haonan Shao,Yue Shen,Stella Shi,Shuran Song,Bartosz Stefaniak,Shangkun Sun,Shitao Tang,Sameena Tasmeen,Lyne Tchapmi,Wei-Cheng Tseng,Jibin Varghese,Andrew Z. Wang,Hao Wang,Haoxiang Wang,Heng Wang,Ting-Chun Wang,Fangyin Wei,Jiashu Xu,Dinghao Yang,Xiaodong Yang,Haotian Ye,Seonghyeon Ye,Xiaohui Zeng,Jing Zhang,Qinsheng Zhang,Kaiwen Zheng,Andrew Zhu,Yuke Zhu*

Main category: cs.CV

TL;DR: Cosmos-Predict2.5：基于flow的多模态世界生成模型，结合物理视觉语言模型与RL后训练，提升视频合成质量和指令对齐；Cosmos-Transfer2.5：更小更强的Sim2Real/Real2Real翻译框架；两者助力实体智能研究并已开源。


<details>
  <summary>Details</summary>
Motivation: 降低构建大规模、可控物理世界模拟与合成数据的门槛，推动机器人和自动系统的闭环仿真、策略评估与实体智能扩展。

Method: 基于flow的单一架构统一Text2World、Image2World和Video2World，结合[Cosmos-Reason1]进行更强的文本与视觉对齐；在200M视频数据上训练，并使用基于强化学习的后训练精化；并通过控制网风格的[Cosmos-Transfer2.5]实现域翻译。

Result: 在2B和14B尺度上相较于[Cosmos-Predict1]在视频质量与指令对齐上有显著提升；[Cosmos-Transfer2.5]在体量更小（3.5×）的同时实现更高保真度和鲁棒的长时视频生成；并开源代码与模型。

Conclusion: [Cosmos-Predict2.5]显著提升了物理AI中世界生成和仿真的质量与指令对齐能力，并通过[Cosmos-Transfer2.5]改进了Sim2Real/Real2Real的长视野视频翻译。

Abstract: We introduce [Cosmos-Predict2.5], the latest generation of the Cosmos World
Foundation Models for Physical AI. Built on a flow-based architecture,
[Cosmos-Predict2.5] unifies Text2World, Image2World, and Video2World generation
in a single model and leverages [Cosmos-Reason1], a Physical AI vision-language
model, to provide richer text grounding and finer control of world simulation.
Trained on 200M curated video clips and refined with reinforcement
learning-based post-training, [Cosmos-Predict2.5] achieves substantial
improvements over [Cosmos-Predict1] in video quality and instruction alignment,
with models released at 2B and 14B scales. These capabilities enable more
reliable synthetic data generation, policy evaluation, and closed-loop
simulation for robotics and autonomous systems. We further extend the family
with [Cosmos-Transfer2.5], a control-net style framework for Sim2Real and
Real2Real world translation. Despite being 3.5$\times$ smaller than
[Cosmos-Transfer1], it delivers higher fidelity and robust long-horizon video
generation. Together, these advances establish [Cosmos-Predict2.5] and
[Cosmos-Transfer2.5] as versatile tools for scaling embodied intelligence. To
accelerate research and deployment in Physical AI, we release source code,
pretrained checkpoints, and curated benchmarks under the NVIDIA Open Model
License at https://github.com/nvidia-cosmos/cosmos-predict2.5 and
https://github.com/nvidia-cosmos/cosmos-transfer2.5. We hope these open
resources lower the barrier to adoption and foster innovation in building the
next generation of embodied intelligence.

</details>


### [9] [Habitat and Land Cover Change Detection in Alpine Protected Areas: A Comparison of AI Architectures](https://arxiv.org/abs/2511.00073)
*Harald Kristen,Daniel Kulmer,Manuela Hirschmugl*

Main category: cs.CV

TL;DR: 在高山复杂生境中，GFM（尤其Clay v1.0）对多类变化检测表现更好，直接CD在二元任务上IoU更高，LiDAR融合显著提升分割性能，但总体精度仍受复杂性限制。


<details>
  <summary>Details</summary>
Motivation: 高山生态系统快速变化需要高频次监测，人工制图成本高，且复杂生境存在模糊类别边界和类别严重不均衡，传统方法难以胜任。

Method: 比较两种范式：先分类后变化检测（使用Prithvi-EO-2.0、Clay v1.0和U-Net）与直接变化检测（ChangeViT与U-Net基线）。使用高分辨率多模态数据（RGB、NIR、LiDAR、地形属性）和4,480个变化样本进行评估，同时进行跨时序测试与LiDAR融合实验。

Result: Clay v1.0在多类生境变化检测上达51%总体准确率，优于U-Net的41%；在二元变化检测上两者均达67%。直接变化检测在二元IoU上优于后分类（0.53 vs 0.35），但多类准确率仅28%。跨时序评估显示Clay对未见年份稳健（2020年33% vs U-Net 23%）。融合LiDAR将语义分割准确率从30%提升到50%。

Conclusion: 本文表明在复杂的高山生境中，地理空间基础模型（GFM）在多类变化检测上优于传统U-Net，但总体精度仍低于均质景观，反映任务难度。

Abstract: Rapid climate change and other disturbances in alpine ecosystems demand
frequent habitat monitoring, yet manual mapping remains prohibitively expensive
for the required temporal resolution. We employ deep learning for change
detection using long-term alpine habitat data from Gesaeuse National Park,
Austria, addressing a major gap in applying geospatial foundation models (GFMs)
to complex natural environments with fuzzy class boundaries and highly
imbalanced classes. We compare two paradigms: post-classification change
detection (CD) versus direct CD. For post-classification CD, we evaluate GFMs
Prithvi-EO-2.0 and Clay v1.0 against U-Net CNNs; for direct CD, we test the
transformer ChangeViT against U-Net baselines. Using high-resolution multimodal
data (RGB, NIR, LiDAR, terrain attributes) covering 4,480 documented changes
over 15.3 km2, results show Clay v1.0 achieves 51% overall accuracy versus
U-Net's 41% for multi-class habitat change, while both reach 67% for binary
change detection. Direct CD yields superior IoU (0.53 vs 0.35) for binary but
only 28% accuracy for multi-class detection. Cross-temporal evaluation reveals
GFM robustness, with Clay maintaining 33% accuracy on 2020 data versus U-Net's
23%. Integrating LiDAR improves semantic segmentation from 30% to 50% accuracy.
Although overall accuracies are lower than in more homogeneous landscapes, they
reflect realistic performance for complex alpine habitats. Future work will
integrate object-based post-processing and physical constraints to enhance
applicability.

</details>


### [10] [LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation](https://arxiv.org/abs/2511.00090)
*Huanlin Gao,Ping Chen,Fuyuan Shi,Chao Tan,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: LeMiCa通过图优化的缓存调度在不训练的前提下减少全局误差累积，显著提升视频生成一致性与速度，效果优于现有缓存技术。


<details>
  <summary>Details</summary>
Motivation: 现有缓存策略多关注局部启发式误差，忽视全局误差累积导致加速后视频与原视频内容风格不一致。

Method: 将缓存调度建模为带误差权重边的有向图，并提出词典式（Lexicographic）极小化最小化最大路径误差（Minimax Path Optimization）策略，对最差路径误差进行显式约束以减少全局误差累积。

Result: 在多项文本到视频基准上双向提升推理速度与生成质量：如在Latte模型上实现2.9x加速，在Open-Sora上LPIPS达到0.05，且主观感知质量仅有极小降损。

Conclusion: LeMiCa能在不需额外训练的情况下，通过图论优化缓存调度显著提升扩散模型视频生成的一致性与效率，兼顾速度与质量。

Abstract: We present LeMiCa, a training-free and efficient acceleration framework for
diffusion-based video generation. While existing caching strategies primarily
focus on reducing local heuristic errors, they often overlook the accumulation
of global errors, leading to noticeable content degradation between accelerated
and original videos. To address this issue, we formulate cache scheduling as a
directed graph with error-weighted edges and introduce a Lexicographic Minimax
Path Optimization strategy that explicitly bounds the worst-case path error.
This approach substantially improves the consistency of global content and
style across generated frames. Extensive experiments on multiple text-to-video
benchmarks demonstrate that LeMiCa delivers dual improvements in both inference
speed and generation quality. Notably, our method achieves a 2.9x speedup on
the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming
prior caching techniques. Importantly, these gains come with minimal perceptual
quality degradation, making LeMiCa a robust and generalizable paradigm for
accelerating diffusion-based video generation. We believe this approach can
serve as a strong foundation for future research on efficient and reliable
video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

</details>


### [11] [Self-Improving Vision-Language-Action Models with Data Generation via Residual RL](https://arxiv.org/abs/2511.00091)
*Wenli Xiao,Haotian Lin,Andy Peng,Haoru Xue,Tairan He,Yuqi Xie,Fengyuan Hu,Jimmy Wu,Zhengyi Luo,Linxi "Jim" Fan,Guanya Shi,Yuke Zhu*

Main category: cs.CV

TL;DR: PLD提出残差RL与部署感知的数据收集流程，自动发现并改正VLA通用模型失败，显著提升模拟与真实机械臂任务成功率，提供可扩展的自我改进路径。


<details>
  <summary>Details</summary>
Motivation: 现有SFT依赖昂贵的人类示范，限制了规模化和泛化能力；目标是通过自动化数据收集和强化学习提升VLA模型性能并扩展其能力。

Method: 三阶段框架：1) 训练轻量残差actor去探测通用VLA的失败区域；2) 采用混合rollout方案（结合通用模型部署分布与恢复行为）收集轨迹；3) 将筛选后的轨迹通过标准SFT蒸馏回通用模型。

Result: 在多个基准和真实设备上获得大幅提升：LIBERO近饱和99%任务成功率，SimplerEnv提升50%以上，以及Franka和YAM机械臂上的100%成功率；消融实验表明残差探测与分布感知重放是关键。

Conclusion: 本文提出了PLD（Probe, Learn, Distill），通过残差强化学习与分布感知的数据收集，在SFT流程外实现VLA模型自我改进，显著提升任务成功率，尤其在真实机械臂任务上达到100%成功。

Abstract: Supervised fine-tuning (SFT) has become the de facto post-training strategy
for large vision-language-action (VLA) models, but its reliance on costly human
demonstrations limits scalability and generalization. We propose Probe, Learn,
Distill (PLD), a three-stage plug-and-play framework that improves VLAs through
residual reinforcement learning (RL) and distribution-aware data collection. In
Stage 1, we train lightweight residual actors to probe failure regions of the
VLA generalist. In Stage 2, we use a hybrid rollout scheme that aligns
collected trajectories with the generalist's deployment distribution while
capturing recovery behaviors. In Stage 3, we distill the curated trajectories
back into the generalist with standard SFT. PLD achieves near-saturated 99%
task success on LIBERO, over 50% gains in SimplerEnv, and 100% success on
real-world Franka and YAM arm manipulation tasks. Ablations show that residual
probing and distribution-aware replay are key to collecting deployment-aligned
data that improves both seen and unseen tasks, offering a scalable path toward
self-improving VLA models.

</details>


### [12] [SpinalSAM-R1: A Vision-Language Multimodal Interactive System for Spine CT Segmentation](https://arxiv.org/abs/2511.00095)
*Jiaming Liu,Dingwei Fan,Junyong Zhao,Chunlin Li,Haipeng Si,Liang Sun*

Main category: cs.CV

TL;DR: 提出SpinalSAM-R1：结合LoRA微调的SAM与DeepSeek-R1，加入解剖学注意力与语义交互，用于高效准确的脊柱CT分割，并提供响应迅速的PyQt5交互软件。


<details>
  <summary>Details</summary>
Motivation: 脊柱CT分割受低对比度与复杂边界影响，现有通用分割模型（如SAM）在注释需求高与领域适应性差的问题限制了其在医学影像中的直接应用，因此需要一种高效可交互且适应性强的方法。

Method: 使用LoRA对SAM进行低秩适配微调，加入解剖学引导注意力机制以增强脊柱结构特征提取，并通过DeepSeek-R1实现自然语言引导的细化；开发PyQt5交互软件支持点/框/文本提示。

Result: 在脊柱解剖结构CT分割验证中，方法显示出优越的分割性能；软件实现11项临床操作，解析准确率94.3%，响应时间低于800 ms。

Conclusion: SpinalSAM-R1通过将微调的SAM与DeepSeek-R1结合，并引入解剖学引导注意力和语义驱动交互协议，能在脊柱CT分割任务中实现更高的分割性能与交互效率。

Abstract: The anatomical structure segmentation of the spine and adjacent structures
from computed tomography (CT) images is a key step for spinal disease diagnosis
and treatment. However, the segmentation of CT images is impeded by low
contrast and complex vertebral boundaries. Although advanced models such as the
Segment Anything Model (SAM) have shown promise in various segmentation tasks,
their performance in spinal CT imaging is limited by high annotation
requirements and poor domain adaptability. To address these limitations, we
propose SpinalSAM-R1, a multimodal vision-language interactive system that
integrates a fine-tuned SAM with DeepSeek-R1, for spine CT image segmentation.
Specifically, our SpinalSAM-R1 introduces an anatomy-guided attention mechanism
to improve spine segmentation performance, and a semantics-driven interaction
protocol powered by DeepSeek-R1, enabling natural language-guided refinement.
The SpinalSAM-R1 is fine-tuned using Low-Rank Adaptation (LoRA) for efficient
adaptation. We validate our SpinalSAM-R1 on the spine anatomical structure with
CT images. Experimental results suggest that our method achieves superior
segmentation performance. Meanwhile, we develop a PyQt5-based interactive
software, which supports point, box, and text-based prompts. The system
supports 11 clinical operations with 94.3\% parsing accuracy and sub-800 ms
response times. The software is released on
https://github.com/6jm233333/spinalsam-r1.

</details>


### [13] [A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning](https://arxiv.org/abs/2511.00098)
*Nils Porsche,Flurin Müller-Diesing,Sweta Banerjee,Miguel Goncalves,Marc Aubreville*

Main category: cs.CV

TL;DR: 论文通过对CLE视频序列去冗余的过滤策略，结合SSL（ViT-small师生网络），在少量标注下显著提升了下游病变分类准确率并将自监督训练时间缩短约三分之二。


<details>
  <summary>Details</summary>
Motivation: CLE为实时微结构成像工具，但图像对无经验医生难以解释；受限于带病理标签的CLE序列稀缺，监督式模型易过拟合。利用大量未标注数据的自监督学习并结合去冗余的视频过滤能改善预训练效果。

Method: 提出对CLE视频序列的过滤功能以减少高帧间相关性导致的数据冗余；使用四种SOTA基线网络和基于ViT-small的SSL师生网络进行预训练；在两项下游任务（鼻窦肿瘤数据集与皮肤鳞状细胞癌数据集）上评估性能。

Result: 过滤后的SSL预训练模型在两数据集上的测试准确率分别达到67.48%和73.52%，均明显优于未使用SSL的基线；过滤机制使训练时间减少约67%。

Conclusion: 该论文证明了自监督学习（SSL）在CLE影像预训练中的有效性，并提出的视频序列过滤方法能显著降低数据冗余、提升训练收敛性与效率。

Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging
modality that can be used for in-situ, in-vivo imaging and the microstructural
analysis of mucous structures. The diagnosis using CLE is, however, complicated
by images being hard to interpret for non-experienced physicians. Utilizing
machine learning as an augmentative tool would hence be beneficial, but is
complicated by the shortage of histopathology-correlated CLE imaging sequences
with respect to the plurality of patterns in this domain, leading to
overfitting of machine learning models. To overcome this, self-supervised
learning (SSL) can be employed on larger unlabeled datasets. CLE is a
video-based modality with high inter-frame correlation, leading to a
non-stratified data distribution for SSL training. In this work, we propose a
filter functionality on CLE video sequences to reduce the dataset redundancy in
SSL training and improve SSL training convergence and training efficiency. We
use four state-of-the-art baseline networks and a SSL teacher-student network
with a vision transformer small backbone for the evaluation. These networks
were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous
cell carcinoma of the skin dataset. On both datasets, we found the highest test
accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both
considerably outperforming their non-SSL baselines. Our results show that SSL
is an effective method for CLE pretraining. Further, we show that our proposed
CLE video filter can be utilized to improve training efficiency in
self-supervised scenarios, resulting in a reduction of 67% in training time.

</details>


### [14] [FreeSliders: Training-Free, Modality-Agnostic Concept Sliders for Fine-Grained Diffusion Control in Images, Audio, and Video](https://arxiv.org/abs/2511.00103)
*Rotem Ezra,Hedi Zisling,Nimrod Berman,Ilan Naiman,Alexey Gorkor,Liran Nochumsohn,Eliya Nachmani,Omri Azencot*

Main category: cs.CV

TL;DR: FreeSliders是一种训练免费、模态不可知的推理级别近似CS方法，支持图像/视频/音频的细粒度概念控制，提出新的评估基准与指标，并通过两阶段尺度选择与重参数化实现均匀感知编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的Concept Sliders需要针对每个概念训练并依赖特定模型微调，难以扩展到新模态；目标是开发一种无需训练、跨模态且可插拔的细粒度概念控制方法。

Method: 通过在推理阶段近似求解CS公式，FreeSliders避免了逐概念训练和模型微调（如LoRA）；同时提出两阶段策略：自动检测饱和点并对遍历进行再参数化以实现感知上均匀的非线性编辑。扩展了CS基准至视频和音频，并定义了三项评估属性与新指标来规范评估。

Result: 在多模态（图像/视频/音频）基准上，FreeSliders在不进行训练的前提下实现了有效的概念控制，性能超过现有基线，并通过两阶段尺度选择与非线性遍历提升编辑的感知一致性。

Conclusion: FreeSliders提出了一种无需训练、与模态无关的方法，通过在推理阶段部分估计Concept Sliders（CS）公式实现细粒度可控生成。实验表明该方法可即插即用地在图像、视频和音频上进行概念控制，优于现有基线，并提供了改进的评估基准与指标。

Abstract: Diffusion models have become state-of-the-art generative models for images,
audio, and video, yet enabling fine-grained controllable generation, i.e.,
continuously steering specific concepts without disturbing unrelated content,
remains challenging. Concept Sliders (CS) offer a promising direction by
discovering semantic directions through textual contrasts, but they require
per-concept training and architecture-specific fine-tuning (e.g., LoRA),
limiting scalability to new modalities. In this work we introduce FreeSliders,
a simple yet effective approach that is fully training-free and
modality-agnostic, achieved by partially estimating the CS formula during
inference. To support modality-agnostic evaluation, we extend the CS benchmark
to include both video and audio, establishing the first suite for fine-grained
concept generation control with multiple modalities. We further propose three
evaluation properties along with new metrics to improve evaluation quality.
Finally, we identify an open problem of scale selection and non-linear
traversals and introduce a two-stage procedure that automatically detects
saturation points and reparameterizes traversal for perceptually uniform,
semantically meaningful edits. Extensive experiments demonstrate that our
method enables plug-and-play, training-free concept control across modalities,
improves over existing baselines, and establishes new tools for principled
controllable generation. An interactive presentation of our benchmark and
method is available at: https://azencot-group.github.io/FreeSliders/

</details>


### [15] [AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency](https://arxiv.org/abs/2511.00107)
*Piyushkumar Patel*

Main category: cs.CV

TL;DR: MOVAI通过CSP、TSAM和PVR三大模块，显著改善了文本到视频生成的质量和时序一致性，在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到视频方法在时序一致性、场景组合能力和细粒度控制上的不足，生成高保真、语义一致且动态合理的视频。

Method: 框架包含三部分：1) Compositional Scene Parser (CSP)将文本解构为带时间注释的层级场景图；2) Temporal-Spatial Attention Mechanism (TSAM)在时间和空间维度上保持运动连贯性和空间细节；3) Progressive Video Refinement (PVR)通过多尺度时序推理迭代提升视频质量。

Result: 在标准基准上，MOVAI在LPIPS、FVD等视频质量指标上分别提升15.3%和12.7%，并在用户偏好研究中提升18.9%，特别擅长生成复杂多对象场景及真实时序动态。

Conclusion: MOVAI提出了一个结合构成场景理解与时间感知扩散模型的分层框架，有效提升了文本到视频生成的时序一致性、组合理解和细粒度控制。

Abstract: Text to video generation has emerged as a critical frontier in generative
artificial intelligence, yet existing approaches struggle with maintaining
temporal consistency, compositional understanding, and fine grained control
over visual narratives. We present MOVAI (Multimodal Original Video AI), a
novel hierarchical framework that integrates compositional scene understanding
with temporal aware diffusion models for high fidelity text to video synthesis.
Our approach introduces three key innovations: (1) a Compositional Scene Parser
(CSP) that decomposes textual descriptions into hierarchical scene graphs with
temporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) that
ensures coherent motion dynamics across frames while preserving spatial
details, and (3) a Progressive Video Refinement (PVR) module that iteratively
enhances video quality through multi-scale temporal reasoning. Extensive
experiments on standard benchmarks demonstrate that MOVAI achieves
state-of-the-art performance, improving video quality metrics by 15.3% in
LPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existing
methods. Our framework shows particular strength in generating complex
multi-object scenes with realistic temporal dynamics and fine-grained semantic
control.

</details>


### [16] [Chain of Time: In-Context Physical Simulation with Image Generation Models](https://arxiv.org/abs/2511.00110)
*YingQiao Wang,Eric Bigelow,Boyi Li,Tomer Ullman*

Main category: cs.CV

TL;DR: 提出Chain of Time：在推理阶段生成中间帧以增强并解释视觉-语言模型的物理模拟，实验证明在多域上显著提高性能并揭示模型内部动力学与局限。


<details>
  <summary>Details</summary>
Motivation: 受到人类心理模拟与机器学习中上下文内推理（in-context reasoning）的启发，旨在提升视觉-语言模型对随时间展开的物理过程（如速度、加速度、流体动力学、动量守恒等）的模拟能力并增加可解释性。

Method: 在不做额外微调的情况下，推理时生成时间链（中间帧）并以此引导图像生成模型的物理预测；分析每一时间步的世界状态来理解模型如何编码和演绎物理规律。

Result: 在2D合成与3D真实视频等多种域上，Chain of Time在性能上大幅提升了最先进图像生成模型的物理推理能力；分析表明模型能模拟诸如速度、重力和碰撞等随时间展开的物理过程，但在从静态输入图像推断某些物理参数方面仍存在困难。

Conclusion: Chain of Time能在推理时通过生成一系列中间图像显著提升视觉-语言模型的物理模拟性能，并提供对模型内部动态的可解释性。

Abstract: We propose a novel cognitively-inspired method to improve and interpret
physical simulation in vision-language models. Our ``Chain of Time" method
involves generating a series of intermediate images during a simulation, and it
is motivated by in-context reasoning in machine learning, as well as mental
simulation in humans. Chain of Time is used at inference time, and requires no
additional fine-tuning. We apply the Chain-of-Time method to synthetic and
real-world domains, including 2-D graphics simulations and natural 3-D videos.
These domains test a variety of particular physical properties, including
velocity, acceleration, fluid dynamics, and conservation of momentum. We found
that using Chain-of-Time simulation substantially improves the performance of a
state-of-the-art image generation model. Beyond examining performance, we also
analyzed the specific states of the world simulated by an image model at each
time step, which sheds light on the dynamics underlying these simulations. This
analysis reveals insights that are hidden from traditional evaluations of
physical reasoning, including cases where an image generation model is able to
simulate physical properties that unfold over time, such as velocity, gravity,
and collisions. Our analysis also highlights particular cases where the image
generation model struggles to infer particular physical parameters from input
images, despite being capable of simulating relevant physical processes.

</details>


### [17] [End-to-End Framework Integrating Generative AI and Deep Reinforcement Learning for Autonomous Ultrasound Scanning](https://arxiv.org/abs/2511.00114)
*Hanae Elmekki,Amanda Spilkin,Ehsan Zakeri,Antonela Mariel Zanuttini,Ahmed Alagha,Hani Sami,Jamal Bentahar,Lyes Kadem,Wen-Fang Xie,Philippe Pibarot,Rabeb Mizouni,Hadi Otrok,Azzam Mourad,Sami Muhaidat*

Main category: cs.CV

TL;DR: 提出基于VAE-GAN的条件模拟器+DRL的首个端到端自主心脏超声扫描框架，公开数据集，实验证明模拟器与DRL策略有效。


<details>
  <summary>Details</summary>
Motivation: 当前心脏超声受操作员依赖强、训练者匮乏和人为误差影响，已有DRL方法存在可复现性差、依赖专有数据与模型简化的问题，因而需要一个可复现且生成逼真图像以支持DRL训练的端到端解决方案。

Method: 构建了一个条件生成模拟器（VAE-GAN混合模型）用于生成动作条件下的逼真超声图像；基于该模拟器训练DRL代理以学习扫描策略；并加入专家验证的图像分类与质量评估模块。发布了公开真实心脏超声数据集以保证可复现性。

Result: VAE-GAN在定性与定量评测中优于现有GAN变体；DRL扫描系统在多种配置下表现有效。并公开了数据集，支持向其他器官扩展。

Conclusion: 该论文提出首个将生成式AI与深度强化学习（DRL）整合以实现自主、可复现的心脏超声（US）扫描端到端框架。

Abstract: Cardiac ultrasound (US) is among the most widely used diagnostic tools in
cardiology for assessing heart health, but its effectiveness is limited by
operator dependence, time constraints, and human error. The shortage of trained
professionals, especially in remote areas, further restricts access. These
issues underscore the need for automated solutions that can ensure consistent,
and accessible cardiac imaging regardless of operator skill or location. Recent
progress in artificial intelligence (AI), especially in deep reinforcement
learning (DRL), has gained attention for enabling autonomous decision-making.
However, existing DRL-based approaches to cardiac US scanning lack
reproducibility, rely on proprietary data, and use simplified models. Motivated
by these gaps, we present the first end-to-end framework that integrates
generative AI and DRL to enable autonomous and reproducible cardiac US
scanning. The framework comprises two components: (i) a conditional generative
simulator combining Generative Adversarial Networks (GANs) with Variational
Autoencoders (VAEs), that models the cardiac US environment producing realistic
action-conditioned images; and (ii) a DRL module that leverages this simulator
to learn autonomous, accurate scanning policies. The proposed framework
delivers AI-driven guidance through expert-validated models that classify image
type and assess quality, supports conditional generation of realistic US
images, and establishes a reproducible foundation extendable to other organs.
To ensure reproducibility, a publicly available dataset of real cardiac US
scans is released. The solution is validated through several experiments. The
VAE-GAN is benchmarked against existing GAN variants, with performance assessed
using qualitative and quantitative approaches, while the DRL-based scanning
system is evaluated under varying configurations to demonstrate effectiveness.

</details>


### [18] [VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images](https://arxiv.org/abs/2511.00120)
*Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: VLM6D使用DINOv2+PointNet++双流融合RGB-D特征，用于更鲁棒的6D位姿估计，在Occluded-LineMOD上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前6D位姿估计在从合成到现实迁移、纹理缺失、光照变化和强遮挡场景下表现不稳定，需结合视觉语义和几何信息以提升鲁棒性与精度。

Method: 提出双流架构：RGB由DINOv2（自监督ViT）编码，深度点云由PointNet++编码，随后特征融合并输入多任务预测头，输出位姿估计。

Result: 在Occluded-LineMOD数据集上取得新的SOTA成绩，显示对纹理和光照变化、稀疏/碎片化点云的强鲁棒性与精确性。

Conclusion: 该论文提出的VLM6D通过融合DINOv2视觉编码器和PointNet++几何编码器，改善了6D位姿估计在纹理、光照变化和严重遮挡下的鲁棒性，实验在Occluded-LineMOD上实现了新的SOTA，结论可信但需更多细节验证。

Abstract: The primary challenge in computer vision is precisely calculating the pose of
6D objects, however many current approaches are still fragile and have trouble
generalizing from synthetic data to real-world situations with fluctuating
lighting, textureless objects, and significant occlusions. To address these
limitations, VLM6D, a novel dual-stream architecture that leverages the
distinct strengths of visual and geometric data from RGB-D input for robust and
precise pose estimation. Our framework uniquely integrates two specialized
encoders: a powerful, self-supervised Vision Transformer (DINOv2) processes the
RGB modality, harnessing its rich, pre-trained understanding of visual grammar
to achieve remarkable resilience against texture and lighting variations.
Concurrently, a PointNet++ encoder processes the 3D point cloud derived from
depth data, enabling robust geometric reasoning that excels even with the
sparse, fragmented data typical of severe occlusion. These complementary
feature streams are effectively fused to inform a multi task prediction head.
We demonstrate through comprehensive experiments that VLM6D obtained new SOTA
performance on the challenging Occluded-LineMOD, validating its superior
robustness and accuracy.

</details>


### [19] [Integrating ConvNeXt and Vision Transformers for Enhancing Facial Age Estimation](https://arxiv.org/abs/2511.00123)
*Gaby Maroun,Salah Eddine Bekhouche,Fadi Dornaika*

Main category: cs.CV

TL;DR: 将ConvNeXt与ViT混合，利用预训练、正则化和注意力适配，在多数据集上显著降低MAE，表明混合架构是年龄估计的有效方向。


<details>
  <summary>Details</summary>
Motivation: 年龄估计对局部纹理与全局结构信息均敏感，单一架构难以同时兼顾，因此尝试融合CNN与Transformer的互补优势。

Method: 使用预训练ConvNeXt作为特征提取骨干，接入ViT或自适应注意力模块，添加线性回归层以输出年龄，使用正则化与不同配置的消融实验优化架构。

Result: 在MORPH II、CACD和AFAD基准上，混合模型在MAE指标上优于传统方法，消融研究表明自适应注意力模块对改善年龄相关特征关注至关重要。

Conclusion: 本文提出将ConvNeXt与ViT混合，以结合CNN局部特征提取与Transformer全局注意力，能有效提升年龄估计性能。

Abstract: Age estimation from facial images is a complex and multifaceted challenge in
computer vision. In this study, we present a novel hybrid architecture that
combines ConvNeXt, a state-of-the-art advancement of convolutional neural
networks (CNNs), with Vision Transformers (ViT). While each model independently
delivers excellent performance on a variety of tasks, their integration
leverages the complementary strengths of the CNNs localized feature extraction
capabilities and the Transformers global attention mechanisms. Our proposed
ConvNeXt-ViT hybrid solution was thoroughly evaluated on benchmark age
estimation datasets, including MORPH II, CACD, and AFAD, and achieved superior
performance in terms of mean absolute error (MAE). To address computational
constraints, we leverage pre-trained models and systematically explore
different configurations, using linear layers and advanced regularization
techniques to optimize the architecture. Comprehensive ablation studies
highlight the critical role of individual components and training strategies,
and in particular emphasize the importance of adapted attention mechanisms
within the CNN framework to improve the model focus on age-relevant facial
features. The results show that the ConvNeXt-ViT hybrid not only outperforms
traditional methods, but also provides a robust foundation for future advances
in age estimation and related visual tasks. This work underscores the
transformative potential of hybrid architectures and represents a promising
direction for the seamless integration of CNNs and transformers to address
complex computer vision challenges.

</details>


### [20] [FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding](https://arxiv.org/abs/2511.00141)
*Janghoon Cho,Jungsoo Lee,Munawar Hayat,Kyuwoong Hwang,Fatih Porikli,Sungha Choi*

Main category: cs.CV

TL;DR: FLoC利用facility location与lazy greedy快速选取代表性视觉token，训练/模型/查询无关，大幅减少长视频token量，在多项基准上优于现有压缩方法。


<details>
  <summary>Details</summary>
Motivation: 长视频产生大量视觉token，导致视频-LLM不可扩展；需要一种快速、廉价且不需重训练的压缩策略以保留信息同时降低计算开销。

Method: 将facility location函数作为目标，采用lazy greedy算法在预设的token预算下快速选择具有代表性与多样性的视觉token子集，减少输入token数；方法不依赖训练，适配多种视频-LLM并与现有流程兼容。

Result: 在Video-MME、MLVU和LongVideoBench等大型基准上，FLoC在精度和处理速度上均优于近期压缩方法，证明其在效率、稳健性和效果上的优势。

Conclusion: FLoC通过基于facility location的贪心子集选择，有效压缩长视频的视觉token数量，在不训练模型且与模型/查询无关的条件下，提升了视频-LLM在长视频理解任务上的效率与性能。

Abstract: Recent studies in long video understanding have harnessed the advanced
visual-language reasoning capabilities of Large Multimodal Models (LMMs),
driving the evolution of video-LMMs specialized for processing extended video
sequences. However, the scalability of these models is severely limited by the
overwhelming volume of visual tokens generated from extended video sequences.
To address this challenge, this paper proposes FLoC, an efficient visual token
compression framework based on the facility location function, a principled
approach that swiftly selects a compact yet highly representative and diverse
subset of visual tokens within a predefined budget on the number of visual
tokens. By integrating the lazy greedy algorithm, our method achieves
remarkable efficiency gains by swiftly selecting a compact subset of tokens,
drastically reducing the number of visual tokens while guaranteeing
near-optimal performance. Notably, our approach is training-free,
model-agnostic, and query-agnostic, providing a versatile solution that
seamlessly integrates with diverse video-LLMs and existing workflows. Extensive
evaluations on large-scale benchmarks, such as Video-MME, MLVU, and
LongVideoBench, demonstrate that our framework consistently surpasses recent
compression techniques, highlighting not only its effectiveness and robustness
in addressing the critical challenges of long video understanding, but also its
efficiency in processing speed.

</details>


### [21] [BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing](https://arxiv.org/abs/2511.00143)
*Jinsu Kim,Yunhun Nam,Minseon Kim,Sangpil Kim,Jongheon Jeong*

Main category: cs.CV

TL;DR: 提出BlurGuard：对抗噪声做自适应分区高斯模糊以调整频谱，从而提高防护噪声对逆转攻击的鲁棒性，兼顾图像感知质量，实验表明效果稳健且实用。


<details>
  <summary>Details</summary>
Motivation: 动机是现有的图像保护对抗噪声虽不可见但易被逆转，无法有效阻止恶意图像编辑；因此需要既不可感知又不可逆的噪声保护策略。

Method: 方法为在原有对抗噪声上施加基于图像分区的自适应高斯模糊，调整噪声的频谱分布以降低噪声被逆转（如JPEG、过滤器、去噪等）的可行性，同时结合感知质量度量进行参数选择。

Result: 结果显示该方法在多种逆转技术和编辑场景下均能一致提高样本的最坏情况防护效果，同时在感知指标上比未模糊的噪声更少降低图像质量。

Conclusion: 论文结论：通过对抗噪声做自适应的分区高斯模糊可显著提高图像保护方法对噪声逆转技术的鲁棒性，使得在保持感知质量的同时更难被恢复或移除，从而有效阻止基于文本到图像的编辑滥用。

Abstract: Recent advances in text-to-image models have increased the exposure of
powerful image editing techniques as a tool, raising concerns about their
potential for malicious use. An emerging line of research to address such
threats focuses on implanting "protective" adversarial noise into images before
their public release, so future attempts to edit them using text-to-image
models can be impeded. However, subsequent works have shown that these
adversarial noises are often easily "reversed," e.g., with techniques as simple
as JPEG compression, casting doubt on the practicality of the approach. In this
paper, we argue that adversarial noise for image protection should not only be
imperceptible, as has been a primary focus of prior work, but also
irreversible, viz., it should be difficult to detect as noise provided that the
original image is hidden. We propose a surprisingly simple method to enhance
the robustness of image protection methods against noise reversal techniques.
Specifically, it applies an adaptive per-region Gaussian blur on the noise to
adjust the overall frequency spectrum. Through extensive experiments, we show
that our method consistently improves the per-sample worst-case protection
performance of existing methods against a wide range of reversal techniques on
diverse image editing scenarios, while also reducing quality degradation due to
noise in terms of perceptual metrics. Code is available at
https://github.com/jsu-kim/BlurGuard.

</details>


### [22] [CompAgent: An Agentic Framework for Visual Compliance Verification](https://arxiv.org/abs/2511.00171)
*Rahul Ghosh,Baishali Chaudhury,Hari Prasanna Das,Meghana Ashok,Ryan Razkenari,Sungmin Hong,Chun-Hao Liu*

Main category: cs.CV

TL;DR: CompAgent通过规划智能体动态调用视觉工具并由验证智能体整合工具输出与策略，用MLLM执行多模态推理，有效提升视觉合规检测性能，UnsafeBench上F1最高76%，领先SOTA约10%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工标注数据训练专用模型，成本高且泛化性差；MLLM虽具备丰富知识但难以处理细粒度视觉细节与结构化策略应用，因此需要将工具能力与智能体规划结合来提升视觉合规性验证。

Method: 设计两级智能体体系：规划智能体根据合规策略动态选取合适视觉工具；验证智能体将图像、工具输出与策略上下文整合，利用MLLM进行多模态推理与判断。使用多种工具（检测器、分析器、NSFW、captioner）增强MLLM细粒度视觉理解。

Result: 在公共基准上表现优于专用分类器、直接MLLM提示和人工路由基线。在UnsafeBench上达到了最高76% F1，较SOTA提升约10%，展示了智能体规划与工具增强推理在可扩展、准确和可适应视觉合规性验证上的有效性。

Conclusion: 提出了一种名为CompAgent的首个智能体框架，通过将多模态大模型（MLLM）与一套视觉工具（目标检测、人脸分析、NSFW检测、图像描述等）结合，并引入规划智能体动态选择工具与验证智能体整合信息进行多模态推理，实现了可扩展且准确的视觉合规性检测。

Abstract: Visual compliance verification is a critical yet underexplored problem in
computer vision, especially in domains such as media, entertainment, and
advertising where content must adhere to complex and evolving policy rules.
Existing methods often rely on task-specific deep learning models trained on
manually labeled datasets, which are costly to build and limited in
generalizability. While recent multi-modal large language models (MLLMs) offer
broad real-world knowledge and policy understanding, they struggle to reason
over fine-grained visual details and apply structured compliance rules
effectively on their own. In this paper, we propose CompAgent, the first
agentic framework for visual compliance verification. CompAgent augments MLLMs
with a suite of visual tools - such as object detectors, face analyzers, NSFW
detectors, and captioning models - and introduces a planning agent that
dynamically selects appropriate tools based on the compliance policy. A
verification agent then integrates image, tool outputs, and policy context to
perform multi-modal reasoning. Experiments on public benchmarks show that
CompAgent outperforms specialized classifiers, direct MLLM prompting, and
curated routing baselines, achieving up to 76% F1 score and a 10% improvement
over the state-of-the-art on the UnsafeBench dataset. Our results demonstrate
the effectiveness of agentic planning and tool-augmented reasoning for
scalable, accurate, and adaptable visual compliance verification.

</details>


### [23] [From Evidence to Verdict: An Agent-Based Forensic Framework for AI-Generated Image Detection](https://arxiv.org/abs/2511.00181)
*Mengfei Liang,Yiting Qu,Yukun Jiang,Michael Backes,Yang Zhang*

Main category: cs.CV

TL;DR: AIFo是一种训练-free的多代理图像取证框架，集合多种工具并由LLM代理协作与辩论、辅以记忆模块，实现对AI生成图像的高准确、可解释检测（实验中97.05%）。


<details>
  <summary>Details</summary>
Motivation: 动机在于弥补传统分类器缺乏可解释性和泛化性、以及现有VLM方法仅限单次分析与像素级推理的不足，面对不断演进的AI图像生成技术，需要一种更具程序化、可解释和跨源证据整合能力的检测范式。

Method: 方法为构建若干功能性取证工具并赋予专门LLM代理以收集、整合与推理跨源证据；在证据冲突或不足时采用结构化多代理辩论机制达成结论；并引入记忆增强推理模块以从历史案例中学习以提升检测精度。该框架无需额外训练，通过流程化代理协作完成判定。

Result: 在6000张图像（含实验室控制与现实场景、现代生成平台与在线多源图像）上评估，AIFo达成97.05%准确率，明显优于传统分类器与最新VLM方法，表明基于代理的流程化推理在鲁棒性、可解释性和适应性上具有优势。

Conclusion: 该论文提出了一种无需训练的多代理审查框架AIFo，通过模拟人类取证流程，结合反向图像搜索、元数据提取、预训练分类器与VLM分析等工具，并由基于LLM的代理协调、综合与论证，从而实现对AI生成图像的可解释、可扩展检测。

Abstract: The rapid evolution of AI-generated images poses unprecedented challenges to
information integrity and media authenticity. Existing detection approaches
suffer from fundamental limitations: traditional classifiers lack
interpretability and fail to generalize across evolving generative models,
while vision-language models (VLMs), despite their promise, remain constrained
to single-shot analysis and pixel-level reasoning. To address these challenges,
we introduce AIFo (Agent-based Image Forensics), a novel training-free
framework that emulates human forensic investigation through multi-agent
collaboration. Unlike conventional methods, our framework employs a set of
forensic tools, including reverse image search, metadata extraction,
pre-trained classifiers, and VLM analysis, coordinated by specialized LLM-based
agents that collect, synthesize, and reason over cross-source evidence. When
evidence is conflicting or insufficient, a structured multi-agent debate
mechanism allows agents to exchange arguments and reach a reliable conclusion.
Furthermore, we enhance the framework with a memory-augmented reasoning module
that learns from historical cases to improve future detection accuracy. Our
comprehensive evaluation spans 6,000 images across both controlled laboratory
settings and challenging real-world scenarios, including images from modern
generative platforms and diverse online sources. AIFo achieves 97.05% accuracy,
substantially outperforming traditional classifiers and state-of-the-art VLMs.
These results demonstrate that agent-based procedural reasoning offers a new
paradigm for more robust, interpretable, and adaptable AI-generated image
detection.

</details>


### [24] [A Retrospect to Multi-prompt Learning across Vision and Language](https://arxiv.org/abs/2511.00191)
*Ziliang Chen,Xin Huang,Quanlong Guan,Liang Lin,Weiqi Luo*

Main category: cs.CV

TL;DR: 论文提出用能量分布从VLM隐式定义的模型中采样生成多prompt嵌入的EMPL方法，理论+实验证明其在参数效率和域内/外泛化间取得良好平衡，优于传统单prompt方法。


<details>
  <summary>Details</summary>
Motivation: 当前工作多聚焦单prompt范式，忽视多prompt方法潜力。研究动机是探索多prompt在视觉-语言迁移学习中的理论与实证优势，提升适应性与泛化性同时保持参数效率。

Method: 作者先扩展并验证了常量模态差（constant modality gap）现象到可学习prompt，然后理论与实证分析多prompt增强的优越性。基于此，提出基于能量分布、由VLM隐式定义的采样机制来生成多个prompt嵌入，实现多prompt学习。

Result: 实验显示EMPL在开域（out-of-domain）与域内（in-domain）任务上均有竞争优势，且参数开销小，理论分析支持其在调和域间性能上的合理性。

Conclusion: 本文提出的Energy-based Multi-prompt Learning (EMPL)通过从能量分布中采样生成多个可学习的prompt嵌入，能在参数效率与域内/域外泛化间取得平衡，从而提升Vision-Language模型在下游任务的表现。

Abstract: The vision community is undergoing the unprecedented progress with the
emergence of Vision-Language Pretraining Models (VLMs). Prompt learning plays
as the holy grail of accessing VLMs since it enables their fast adaptation to
downstream tasks with limited resources. Whereas existing researches milling
around single-prompt paradigms, rarely investigate the technical potential
behind their multi-prompt learning counterparts. This paper aims to provide a
principled retrospect for vision-language multi-prompt learning. We extend the
recent constant modality gap phenomenon to learnable prompts and then, justify
the superiority of vision-language transfer with multi-prompt augmentation,
empirically and theoretically. In terms of this observation, we propose an
Energy-based Multi-prompt Learning (EMPL) to generate multiple prompt
embeddings by drawing instances from an energy-based distribution, which is
implicitly defined by VLMs. So our EMPL is not only parameter-efficient but
also rigorously lead to the balance between in-domain and out-of-domain
open-vocabulary generalization. Comprehensive experiments have been conducted
to justify our claims and the excellence of EMPL.

</details>


### [25] [An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals](https://arxiv.org/abs/2511.00211)
*Wenxuan Zhang,Peng Hu*

Main category: cs.CV

TL;DR: 提出基于迁移学习的地面终端天气相关状态检测方法，能在有限数据和资源下准确检测雪、湿等条件，优于YOLOv7/9、Faster R-CNN等，并具备较好泛化性。


<details>
  <summary>Details</summary>
Motivation: 随着LEO卫星星座广泛部署，地面终端在农村和偏远地区的普及使得天气（如雪、雨）对天线等关键组件的影响日益突显，亟需细粒度的组件状态检测能力以支持故障诊断与缓解措施，但现有解决方案在有效性和泛化性方面不足。

Method: 作者采用迁移学习方案，将预训练模型在相关目标检测任务上进行微调，以适配地面终端的有限数据和计算资源；并通过与YOLOv7、YOLOv9、Faster R-CNN、R-YOLO等典型深度学习检测模型进行对比实验，验证了方法在精度与泛化性上的优势。

Result: 该迁移学习方法能够有效识别雪、湿润及其他天气相关条件，在实验中优于所选的主流目标检测网络，并在多场景下表现出良好的泛化能力。

Conclusion: 本文提出了一种高效的迁移学习方法，用于在地面终端本地检测与天气相关的状态（如雪、湿滑等），从而提升LEO卫星互联网在恶劣天气下的可靠性。

Abstract: The increasing adoption of satellite Internet with low-Earth-orbit (LEO)
satellites in mega-constellations allows ubiquitous connectivity to rural and
remote areas. However, weather events have a significant impact on the
performance and reliability of satellite Internet. Adverse weather events such
as snow and rain can disturb the performance and operations of satellite
Internet's essential ground terminal components, such as satellite antennas,
significantly disrupting the space-ground link conditions between LEO
satellites and ground stations. This challenge calls for not only region-based
weather forecasts but also fine-grained detection capability on ground terminal
components of fine-grained weather conditions. Such a capability can assist in
fault diagnostics and mitigation for reliable satellite Internet, but its
solutions are lacking, not to mention the effectiveness and generalization that
are essential in real-world deployments. This paper discusses an efficient
transfer learning (TL) method that can enable a ground component to locally
detect representative weather-related conditions. The proposed method can
detect snow, wet, and other conditions resulting from adverse and typical
weather events and shows superior performance compared to the typical deep
learning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TL
method also shows the advantage of being generalizable to various scenarios.

</details>


### [26] [DM-QPMNET: Dual-modality fusion network for cell segmentation in quantitative phase microscopy](https://arxiv.org/abs/2511.00218)
*Rajatsubhra Chakraborty,Ana Espinosa-Momox,Riley Haskin,Depeng Xu,Rosario Porras-Aguilar*

Main category: cs.CV

TL;DR: 将偏振强度与相位作为独立模态通过双编码器和多头注意力中层融合，DM-QPMNet在ssQPM细胞分割上优于拼接与单模态方法。


<details>
  <summary>Details</summary>
Motivation: 传统阈值法对噪声和细胞密度敏感，简单通道拼接的深度学习方法未能充分利用偏振强度与相位图的互补信息，需设计模态感知的融合机制。

Method: 提出双流编码器分别对偏振强度图和相位图进行特征提取，并在中层通过多头注意力进行内容感知的模态融合，使用双源跳跃连接和逐模态归一化以保持训练稳定性，网络开销小。

Result: 在与通道拼接和单模态基线比较中，DM-QPMNet表现出显著改进（文中宣称为“大幅度”提升），表明模态特定编码加可学习融合能更稳健地利用ssQPM提供的照明与相位线索。

Conclusion: DM-QPMNet通过双编码器与跨模态注意力融合显著提升了ssQPM细胞分割性能，验证了将偏振强度和相位图视为互补模态并分别编码的有效性。

Abstract: Cell segmentation in single-shot quantitative phase microscopy (ssQPM) faces
challenges from traditional thresholding methods that are sensitive to noise
and cell density, while deep learning approaches using simple channel
concatenation fail to exploit the complementary nature of polarized intensity
images and phase maps. We introduce DM-QPMNet, a dual-encoder network that
treats these as distinct modalities with separate encoding streams. Our
architecture fuses modality-specific features at intermediate depth via
multi-head attention, enabling polarized edge and texture representations to
selectively integrate complementary phase information. This content-aware
fusion preserves training stability while adding principled multi-modal
integration through dual-source skip connections and per-modality normalization
at minimal overhead. Our approach demonstrates substantial improvements over
monolithic concatenation and single-modality baselines, showing that
modality-specific encoding with learnable fusion effectively exploits ssQPM's
simultaneous capture of complementary illumination and phase cues for robust
cell segmentation.

</details>


### [27] [Towards 1000-fold Electron Microscopy Image Compression for Connectomics via VQ-VAE with Transformer Prior](https://arxiv.org/abs/2511.00231)
*Fuming Yang,Yicong Li,Hanspeter Pfister,Jeff W. Lichtman,Yaron Meirovitch*

Main category: cs.CV

TL;DR: 提出一个可按需解码的VQ-VAE EM压缩框架（16x–1024x），通过Transformer先验+FiLM恢复纹理，并用ROI策略选择性重建高分辨率区域。


<details>
  <summary>Details</summary>
Motivation: Petascale级别的EM数据在存储、传输和下游分析方面带来巨大压力，需要高效且灵活的压缩方案，既能提供极致压缩以节省资源，又能在需要时以可控成本恢复高分辨率细节。

Method: 使用向量量化变分自编码器(VQ-VAE)构建多层编码器/解码器，允许仅解码顶层（top-only）以实现极端压缩。为恢复细节，引入一个Transformer先验预测底层令牌，结合FiLM和拼接操作将预测的底层信息注入解码器而不改变压缩比。还实现了基于ROI的流程，仅对1024x压缩的潜在表示中需要的区域进行高分辨率重建，从而节省计算和存储资源。

Result: 框架覆盖16x至1024x的压缩范围，支持按需解码（top-only）和可选的Transformer驱动的底层令牌预测以提升纹理恢复质量；ROI驱动的选择性高分辨率重建显著降低了总体资源消耗，同时保留重要区域的细节。

Conclusion: 该论文提出了一个基于VQ-VAE的电子显微镜（EM）图像压缩框架，支持16x到1024x的压缩率，并通过分层解码与可选Transformer先验实现按需解码与纹理恢复，同时引入基于感兴趣区域（ROI）的高分辨率重建工作流。

Abstract: Petascale electron microscopy (EM) datasets push storage, transfer, and
downstream analysis toward their current limits. We present a vector-quantized
variational autoencoder-based (VQ-VAE) compression framework for EM that spans
16x to 1024x and enables pay-as-you-decode usage: top-only decoding for extreme
compression, with an optional Transformer prior that predicts bottom tokens
(without changing the compression ratio) to restore texture via feature-wise
linear modulation (FiLM) and concatenation; we further introduce an ROI-driven
workflow that performs selective high-resolution reconstruction from
1024x-compressed latents only where needed.

</details>


### [28] [Hyperbolic Optimal Transport](https://arxiv.org/abs/2511.00244)
*Yan Bin Ng,Xianfeng Gu*

Main category: cs.CV

TL;DR: 拓展最优传输到双曲空间：提出几何变分算法并在合成与多曲面数据上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有最优传输方法主要针对欧氏与球面，而许多实际问题（如层次化数据、网络、以及多曲面几何）自然嵌入在双曲空间，亟需有效的双曲最优传输算法。

Method: 基于将欧氏和球面几何的变分技术推广到双曲几何，构造适用于负常曲率空间的目标泛函与Euler–Lagrange方程，并设计数值优化算法实现映射求解。

Result: 在合成数据和多曲面模型上的实验表明该方法能够稳定计算双曲空间中的映射，具有较好的数值性能和几何一致性。

Conclusion: 本文提出了一种在双曲空间中计算最优传输映射的几何变分方法扩展，证明了该方法在层次化数据和多曲面模型上的有效性。

Abstract: The optimal transport (OT) problem aims to find the most efficient mapping
between two probability distributions under a given cost function, and has
diverse applications in many fields such as machine learning, computer vision
and computer graphics. However, existing methods for computing optimal
transport maps are primarily developed for Euclidean spaces and the sphere. In
this paper, we explore the problem of computing the optimal transport map in
hyperbolic space, which naturally arises in contexts involving hierarchical
data, networks, and multi-genus Riemann surfaces. We propose a novel and
efficient algorithm for computing the optimal transport map in hyperbolic space
using a geometric variational technique by extending methods for Euclidean and
spherical geometry to the hyperbolic setting. We also perform experiments on
synthetic data and multi-genus surface models to validate the efficacy of the
proposed method.

</details>


### [29] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出MSDI与MSDS结合LLM语义的零样本对象感知4D人体运动生成方法，实现了物理合理、语义一致且可扩展的交互式动作生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成高质量视频的同时仍出现不现实的形变、语义冲突与物理不一致，主要因为缺乏3D物理先验。论文旨在通过引入3D高斯表示和运动扩散先验来解决这些问题，提升生成视频中人体与物体交互的物理合理性与语义一致性。

Method: 方法包括利用预生成的3D人体与物体几何表示，结合大语言模型提供的空间与语义提示，提出Motion Diffusion Score Distillation Sampling(MSDS)以从预训练的运动扩散模型中提取分数梯度，并通过Motion Score Distilled Interaction(MSDI)执行面向空间的运动优化来调整人体动作以适配物体与语义约束。

Result: 实验表明，该方法能在不重训练的零样本条件下产生遵循3D空间语境的自然且物理合理的人体运动，并能泛化到分布外的对象感知人体动作场景。

Conclusion: 该论文提出了基于3D高斯表示与运动扩散先验的对象感知4D人体运动生成框架，能够在零样本设置下生成符合语义和物理约束的自然人体动作。

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [30] [Merlin L48 Spectrogram Dataset](https://arxiv.org/abs/2511.00252)
*Aaron Sun,Subhransu Maji,Grant Van Horn*

Main category: cs.CV

TL;DR: 本文构建了L48，一个来自鸟声的真实细粒度SPML数据集，并在此上评估现有方法，发现合成基准不能代表现实复杂性，呼吁更现实的评测基准与方法改进。


<details>
  <summary>Details</summary>
Motivation: 现有SPML工作主要基于由全标注数据随机采样得到的合成单正标签数据集，这不能反映真实世界的细粒度复杂性和误分类挑战，因而需要真实的、更具挑战性的基准来推动方法改进。

Method: 构建来自鸟声录音的细粒度多标签数据集L48，保留单正标注并设计两种扩展设置以利用领域先验获得额外负标签；在L48上对现有SPML方法进行系统基准评测与比较，分析性能差异和失败模式。

Result: 在L48上，现有SPML方法总体性能下降并表现出与合成数据不同的弱点；具体方法在真实细粒度鸟类声音数据上存在显著差异，表明合成基准不能充分预测现实场景表现。

Conclusion: 作者提出了L48数据集，提供真实、细粒度的单正多标签（SPML）鸟类声音数据，并在此基础上评估现有SPML方法，发现这些方法在真实数据上的表现与在合成数据上的基准差异显著，揭示了现有方法的薄弱点，强调需要更现实、更具挑战性的基准。

Abstract: In the single-positive multi-label (SPML) setting, each image in a dataset is
labeled with the presence of a single class, while the true presence of other
classes remains unknown. The challenge is to narrow the performance gap between
this partially-labeled setting and fully-supervised learning, which often
requires a significant annotation budget. Prior SPML methods were developed and
benchmarked on synthetic datasets created by randomly sampling single positive
labels from fully-annotated datasets like Pascal VOC, COCO, NUS-WIDE, and
CUB200. However, this synthetic approach does not reflect real-world scenarios
and fails to capture the fine-grained complexities that can lead to difficult
misclassifications. In this work, we introduce the L48 dataset, a fine-grained,
real-world multi-label dataset derived from recordings of bird sounds. L48
provides a natural SPML setting with single-positive annotations on a
challenging, fine-grained domain, as well as two extended settings in which
domain priors give access to additional negative labels. We benchmark existing
SPML methods on L48 and observe significant performance differences compared to
synthetic datasets and analyze method weaknesses, underscoring the need for
more realistic and difficult benchmarks.

</details>


### [31] [BeetleFlow: An Integrative Deep Learning Pipeline for Beetle Image Processing](https://arxiv.org/abs/2511.00255)
*Fangxun Liu,S M Rayeed,Samuel Stevens,Alyson East,Cheng Hsuan Chiang,Colin Lee,Daniel Yi,Junke Yang,Tejas Naik,Ziyi Wang,Connor Kilrain,Elijah H Buckwalter,Jiacheng Hou,Saul Ibaven Bueno,Shuheng Wang,Xinyue Ma,Yifan Liu,Zhiyuan Tao,Ziheng Zhang,Eric Sokol,Michael Belitz,Sydne Record,Charles V. Stewart,Wei-Lun Chao*

Main category: cs.CV

TL;DR: 论文提出一个由开域Transformer检测、裁剪和微调分割模型组成的三阶段甲虫图像处理流水线，使用670张手工标注数据实现高精度细粒度分割，从而自动化并加速大规模甲虫图像的整理与分析。


<details>
  <summary>Details</summary>
Motivation: 生物学家需处理成千上万托盘拍摄的甲虫图片，人工逐张处理耗时费力，需自动化流水线以加速样本整理与后续形态学分析。

Method: 构建三阶段流水线：1）基于Transformer的开域目标检测器与视觉-语言模型的迭代检测策略，用于定位托盘图中所有甲虫；2）对检测框进行排序与裁剪，生成单个甲虫图像；3）手工标注670张甲虫图像并微调两种Transformer分割模型，进行细粒度形态分割。

Result: 通过微调的分割模型取得较高精度的甲虫细粒度分割；检测阶段采用迭代结合视觉-语言模型提高了检测覆盖率；整体流水线显著提升了大规模甲虫数据处理效率，利于生物学研究加速。

Conclusion: 该论文提出并实现了一套针对托盘上大量甲虫图像的自动化处理流水线，通过检测、裁剪和形态分割三阶段流程，提高了大规模甲虫图像处理效率。

Abstract: In entomology and ecology research, biologists often need to collect a large
number of insects, among which beetles are the most common species. A common
practice for biologists to organize beetles is to place them on trays and take
a picture of each tray. Given the images of thousands of such trays, it is
important to have an automated pipeline to process the large-scale data for
further research. Therefore, we develop a 3-stage pipeline to detect all the
beetles on each tray, sort and crop the image of each beetle, and do
morphological segmentation on the cropped beetles. For detection, we design an
iterative process utilizing a transformer-based open-vocabulary object detector
and a vision-language model. For segmentation, we manually labeled 670 beetle
images and fine-tuned two variants of a transformer-based segmentation model to
achieve fine-grained segmentation of beetles with relatively high accuracy. The
pipeline integrates multiple deep learning methods and is specialized for
beetle image processing, which can greatly improve the efficiency to process
large-scale beetle data and accelerate biological research.

</details>


### [32] [MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba](https://arxiv.org/abs/2511.00260)
*Linzhe Jiang,Jiayuan Huang,Sophia Bano,Matthew J. Clarkson,Zhehua Mao,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 提出MambaNetLK和大规模临床点云配准基准C3VD-Raycasting-10k，结合SSM特征提取与LK迭代对齐，在临床数据上显著提升配准精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 内镜组织纹理重复、局部几何同质导致特征退化，术前与术中数据域差异大，影响点云配准稳定性，因此需要更鲁棒的配准方法和临床级数据集进行评估。

Method: 在PointNetLK基础上引入Mamba状态空间模型(SSM)作为跨模态特征提取器，利用线性时间复杂度捕捉长程依赖，配合Lucas-Kanade迭代对齐。

Result: 在C3VD-Raycasting-10k数据集上，MambaNetLK较第二名将中位旋转误差减少56.04%，RMSE平移误差减少26.19%；在ModelNet40上泛化良好，对初始位姿扰动更鲁棒。

Conclusion: 提出了用于内镜导航的对应无关的配准框架MambaNetLK，并发布了临床数据集C3VD-Raycasting-10k。

Abstract: Accurate 3D point cloud registration underpins reliable image-guided
colonoscopy, directly affecting lesion localization, margin assessment, and
navigation safety. However, biological tissue exhibits repetitive textures and
locally homogeneous geometry that cause feature degeneracy, while substantial
domain shifts between pre-operative anatomy and intra-operative observations
further degrade alignment stability. To address these clinically critical
challenges, we introduce a novel 3D registration method tailored for endoscopic
navigation and a high-quality, clinically grounded dataset to support rigorous
and reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scale
benchmark dataset with 10,014 geometrically aligned point cloud pairs derived
from clinical CT data. We propose MambaNetLK, a novel correspondence-free
registration framework, which enhances the PointNetLK architecture by
integrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.
As a result, the proposed framework efficiently captures long-range
dependencies with linear-time complexity. The alignment is achieved iteratively
using the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,
MambaNetLK achieves the best performance compared with the state-of-the-art
methods, reducing median rotation error by 56.04% and RMSE translation error by
26.19% over the second-best method. The model also demonstrates strong
generalization on ModelNet40 and superior robustness to initial pose
perturbations. MambaNetLK provides a robust foundation for 3D registration in
surgical navigation. The combination of a globally expressive SSM-based feature
extractor and a large-scale clinical dataset enables more accurate and reliable
guidance systems in minimally invasive procedures like colonoscopy.

</details>


### [33] [Spot The Ball: A Benchmark for Visual Social Inference](https://arxiv.org/abs/2511.00261)
*Neha Balamurugan,Sarah Wu,Adam Chun,Gabe Gaw,Cristobal Eyzaguirre,Tobias Gerstenberg*

Main category: cs.CV

TL;DR: 提出Spot The Ball基准，显示现有VLM在利用社交行为线索进行视觉推理方面落后于人类，模型多依赖简单空间启发式，需新架构显式编码行为结构信息以缩小差距。


<details>
  <summary>Details</summary>
Motivation: 通过体育场景测试视觉社交推理，评估模型能否从凝视、姿态等行为线索推断被遮挡或移除的物体位置，以推动更类人化的AI代理发展。

Method: 构建“找球”基准：删除体育图片中的球，要求模型定位球位置；提供人工基线并设计可扩展的数据生成管线；用三种提示策略评估四个领先VLM（Gemini、GPT、LLaMA、Qwen）。

Result: 人类准确率显著高于模型（20–34% vs ≤17%）；模型倾向于中心或靠近球员等启发式猜测，人类利用视线和身体姿态等社交线索。

Conclusion: 论文表明当前视觉-语言模型在视觉社交推理任务上仍远不及人类，且依赖表面空间启发式而非行为线索。

Abstract: Humans excel at visual social inference, the ability to infer hidden elements
of a scene from subtle behavioral cues such as other people's gaze, pose, and
orientation. This ability drives everyday social reasoning in humans and is
critical for developing more human-like AI agents. We introduce Spot The Ball,
a challenging benchmark for evaluating visual social inference in
vision-language models (VLMs) using sports as a test domain. The task is to
localize a removed sports ball from soccer, basketball, and volleyball images.
We present a curated evaluation set with human baselines and a scalable
pipeline for generating additional test items. We evaluate four
state-of-the-art VLMs (Gemini, GPT, LLaMA, Qwen) using three prompting
strategies, finding that humans are consistently two to three times more
accurate (20-34%) than models ($\leq$ 17%) across all sports. Our analyses show
that models rely on superficial spatial heuristics--such as guessing near the
image center or nearby players--while humans leverage social cues like gaze
direction and body pose. These findings reveal a persistent human-model gap in
visual social reasoning and underscore the need for architectures that
explicitly encode structured behavioral cues to achieve robust, human-like
inference.

</details>


### [34] [FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture](https://arxiv.org/abs/2511.00269)
*Long Li,Jiajia Li,Dong Chen,Lina Pu,Haibo Yao,Yanbo Huang*

Main category: cs.CV

TL;DR: 用冻结的CLIP ViT提特征、只联邦训练小型Transformer分类器并共享每类1%不可逆特征，能在非IID情况下显著提升农业分类性能并降低通信与隐私风险。


<details>
  <summary>Details</summary>
Motivation: 在农业智能中需要高精度分类，但集中式训练带来隐私问题，标准联邦学习在非IID数据下性能差且通信开销大，故需一种保隐私、低通信、高效且适应非IID数据的方案。

Method: 冻结预训练的CLIP ViT作为特征提取器，仅在客户端/服务器之间传输和联合训练一个小型Transformer分类器；此外共享每类1%的CLIP特征子集以对齐类别表示，从而缓解非IID带来的性能下降。

Result: 在农业分类任务上实现86.6%准确率，相较于基线联邦学习方法提高超过4倍，证明了方法的有效性与通信效率。

Conclusion: 本文提出将冻结的CLIP ViT与轻量级Transformer分类器结合的联邦学习框架，能在保证隐私的前提下显著降低通信开销并缓解非IID问题，从而提升农业分类任务的准确率并提高效率。

Abstract: Accurate classification plays a pivotal role in smart agriculture, enabling
applications such as crop monitoring, fruit recognition, and pest detection.
However, conventional centralized training often requires large-scale data
collection, which raises privacy concerns, while standard federated learning
struggles with non-independent and identically distributed (non-IID) data and
incurs high communication costs. To address these challenges, we propose a
federated learning framework that integrates a frozen Contrastive
Language-Image Pre-training (CLIP) vision transformer (ViT) with a lightweight
transformer classifier. By leveraging the strong feature extraction capability
of the pre-trained CLIP ViT, the framework avoids training large-scale models
from scratch and restricts federated updates to a compact classifier, thereby
reducing transmission overhead significantly. Furthermore, to mitigate
performance degradation caused by non-IID data distribution, a small subset
(1%) of CLIP-extracted feature representations from all classes is shared
across clients. These shared features are non-reversible to raw images,
ensuring privacy preservation while aligning class representation across
participants. Experimental results on agricultural classification tasks show
that the proposed method achieve 86.6% accuracy, which is more than 4 times
higher compared to baseline federated learning approaches. This demonstrates
the effectiveness and efficiency of combining vision-language model features
with federated learning for privacy-preserving and scalable agricultural
intelligence.

</details>


### [35] [Multi-View Consistent Human Image Customization via In-Context Learning](https://arxiv.org/abs/2511.00293)
*Hengjia Li,Jianjin Xu,Keli Cheng,Lei Wang,Ning Bi,Boxi Wu,Fernando De la Torre,Deng Cai*

Main category: cs.CV

TL;DR: 提出PersonalView，通过条件化结构与语义对应对齐损失，在仅100张样本下实现预训练模型的多视角个性化生成，性能超越多数据训练的基线。


<details>
  <summary>Details</summary>
Motivation: 现有个性化生成方法无法控制生成图像的视角，也难以生成一致的多视角人物，需一种能在少量样本下实现多视角定制的高效适配方法。

Method: 设计了条件化结构以利用预训练扩散Transformer的上下文学习能力，并引入语义对应对齐损失以保持原始生成能力；仅对少量参数进行适配。

Result: 在多视角一致性、文本对齐、身份保持和视觉质量的评测中，PersonalView在仅100张训练样本的条件下显著优于使用大量多视角数据训练的基线方法。

Conclusion: 本文提出了PersonalView，一种轻量级个性化适配方法，能在仅100张样本下赋予预训练生成模型多视角生成功能，实验表明在多视角一致性、文本对齐、身份相似度和视觉质量方面显著优于基线。

Abstract: Recent advances in personalized generative models demonstrate impressive
results in creating identity-consistent images of the same person under diverse
settings. Yet, we note that most methods cannot control the viewpoint of the
generated image, nor generate consistent multiple views of the person. To
address this problem, we propose a lightweight adaptation method, PersonalView,
capable of enabling an existing model to acquire multi-view generation
capability with as few as 100 training samples. PersonalView consists of two
key components: First, we design a conditioning architecture to take advantage
of the in-context learning ability of the pre-trained diffusion transformer.
Second, we preserve the original generative ability of the pretrained model
with a new Semantic Correspondence Alignment Loss. We evaluate the multi-view
consistency, text alignment, identity similarity, and visual quality of
PersonalView and compare it to recent baselines with potential capability of
multi-view customization. PersonalView significantly outperforms baselines
trained on a large corpus of multi-view data with only 100 training samples.

</details>


### [36] [Towards Automated Petrography](https://arxiv.org/abs/2511.00328)
*Isai Daniel Chacón,Paola Ruiz Puentes,Jillian Pearse,Pablo Arbeláez*

Main category: cs.CV

TL;DR: 提出并公开LITHOS大规模偏光薄片数据集（211k图像块、106k颗粒标注、25类矿物），并给出一个融合两种偏光模态的双编码器Transformer基线，显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 手工薄片岩相学耗时且需专家经验，限制了规模化应用；因此需要自动化技术和公开数据集以促进可重复性与研究发展。

Method: 收集并标注211,604张高分辨率RGB偏光图像块和105,802个专家标注的颗粒（25类矿物），每个标注包含类别、空间坐标及表示颗粒几何与方向的主次轴向量路径；评估多种深度学习方法并提出一个整合两种偏光模态的双编码器Transformer架构作为强基线。

Result: LITHOS成为目前最大且最具多样性的公开薄片偏光图像与标注基准，双编码器Transformer在矿物分类上显著优于单模态模型，表明偏光模态间的协同效应。数据集、代码与预训练模型已公开。

Conclusion: 本文构建了一个大规模、多样化的偏光显微薄片图像数据集LITHOS，并提出了一个双编码器Transformer基线模型，证明结合两种偏光模态能提高矿物分类性能。

Abstract: Petrography is a branch of geology that analyzes the mineralogical
composition of rocks from microscopical thin section samples. It is essential
for understanding rock properties across geology, archaeology, engineering,
mineral exploration, and the oil industry. However, petrography is a
labor-intensive task requiring experts to conduct detailed visual examinations
of thin section samples through optical polarization microscopes, thus
hampering scalability and highlighting the need for automated techniques. To
address this challenge, we introduce the Large-scale Imaging and Thin section
Optical-polarization Set (LITHOS), the largest and most diverse publicly
available experimental framework for automated petrography. LITHOS includes
211,604 high-resolution RGB patches of polarized light and 105,802
expert-annotated grains across 25 mineral categories. Each annotation consists
of the mineral class, spatial coordinates, and expert-defined major and minor
axes represented as intersecting vector paths, capturing grain geometry and
orientation. We evaluate multiple deep learning techniques for mineral
classification in LITHOS and propose a dual-encoder transformer architecture
that integrates both polarization modalities as a strong baseline for future
reference. Our method consistently outperforms single-polarization models,
demonstrating the value of polarization synergy in mineral classification. We
have made the LITHOS Benchmark publicly available, comprising our dataset,
code, and pretrained models, to foster reproducibility and further research in
automated petrographic analysis.

</details>


### [37] [Beyond ImageNet: Understanding Cross-Dataset Robustness of Lightweight Vision Models](https://arxiv.org/abs/2511.00335)
*Weidong Zhang,Pak Lun Kevin Ding,Huan Liu*

Main category: cs.CV

TL;DR: TL;DR：在移动/嵌入式场景下，ImageNet不足以代表模型跨域泛化，xScore是更好的统一度量；保持更高分辨率的各向同性卷积和通道注意力比引入Transformer模块更有助于轻量级模型的广泛泛化。


<details>
  <summary>Details</summary>
Motivation: 动机：当前轻量级模型主要以ImageNet作为性能基准，无法反映模型在实际移动/嵌入式应用中面对多样化视觉域（如细粒度、医疗）时的泛化能力；需要一个系统化评估框架与统一指标来衡量跨数据集鲁棒性并指导移动友好架构设计。

Method: 方法：对11个约2.5M参数的轻量级模型，在统一的100轮训练计划下，在7个多样化数据集上进行系统评估；提出xScore作为统一指标，衡量模型在不同视觉域间的一致性与鲁棒性；通过消融和结构分析比较不同架构组件（卷积类型、分辨率、注意力、Transformer块）的影响；还评估了xScore在减少数据集数量时的可预测性。

Result: 结果：1) ImageNet准确率与细粒度/医疗数据集表现相关性较弱；2) xScore能作为移动模型跨域表现的可扩展预测器，且只需4个数据集即可估算；3) 架构分析显示高空间分辨率的各向同性卷积与通道注意力显著提升跨域泛化，Transformer块带来的增益有限且参数开销较大。

Conclusion: 论文结论：在轻量级视觉分类模型中，ImageNet准确率不能可靠预测在细粒度或医疗等其他数据集上的表现；引入的Cross-Dataset Score（xScore）可量化跨域鲁棒性，并能从四个数据集估算移动模型的整体表现；某些架构组件（如更高空间分辨率的各向同性卷积和通道注意力）有助于更广泛的泛化，而Transformer模块在资源受限设置下益处有限。

Abstract: Lightweight vision classification models such as MobileNet, ShuffleNet, and
EfficientNet are increasingly deployed in mobile and embedded systems, yet
their performance has been predominantly benchmarked on ImageNet. This raises
critical questions: Do models that excel on ImageNet also generalize across
other domains? How can cross-dataset robustness be systematically quantified?
And which architectural elements consistently drive generalization under tight
resource constraints? Here, we present the first systematic evaluation of 11
lightweight vision models (2.5M parameters), trained under a fixed 100-epoch
schedule across 7 diverse datasets. We introduce the Cross-Dataset Score
(xScore), a unified metric that quantifies the consistency and robustness of
model performance across diverse visual domains. Our results show that (1)
ImageNet accuracy does not reliably predict performance on fine-grained or
medical datasets, (2) xScore provides a scalable predictor of mobile model
performance that can be estimated from just four datasets, and (3) certain
architectural components--such as isotropic convolutions with higher spatial
resolution and channel-wise attention--promote broader generalization, while
Transformer-based blocks yield little additional benefit, despite incurring
higher parameter overhead. This study provides a reproducible framework for
evaluating lightweight vision models beyond ImageNet, highlights key design
principles for mobile-friendly architectures, and guides the development of
future models that generalize robustly across diverse application domains.

</details>


### [38] [A DeepONet joint Neural Tangent Kernel Hybrid Framework for Physics-Informed Inverse Source Problems and Robust Image Reconstruction](https://arxiv.org/abs/2511.00338)
*Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 将DeepONet与NTK相结合并引入物理损失与任务正则化，可有效解决非线性、稀疏与噪声下的逆问题，适用于流体源定位与图像重建，并在合成与真实数据上验证了其稳健性与精度。


<details>
  <summary>Details</summary>
Motivation: 目标是克服传统逆问题方法在处理高度非线性动力学（例如Navier–Stokes驱动的问题）、稀疏观测和噪声时的困难，通过结合数据驱动算子学习与理论上可解释的核方法来提升重建精度与鲁棒性。

Method: 方法通过将Deep Operator Network用于学习算子映射，并利用Neural Tangent Kernel分析模型训练动态和做正则化，结合物理约束（如Navier–Stokes方程）与任务特定正则项，共同构建损失函数。训练时引入NTK相关项以稳定训练、提高泛化，并在损失中加入稀疏/先验约束以处理欠定或噪声观测。

Result: 在若干合成与真实数据集上进行了验证，任务包括基于Navier–Stokes的源定位和图像重建。结果显示该方法在重建误差、鲁棒性（对噪声与稀疏观测）以及可扩展性方面优于若干基线方法，且重建解满足物理一致性。

Conclusion: 该论文提出了一种将DeepONet与NTK结合的混合方法，用于求解复杂逆问题，能够在非线性、稀疏和噪声数据条件下恢复物理场或图像，且在合成与真实数据上表现出稳健性与可扩展性。

Abstract: This work presents a novel hybrid approach that integrates Deep Operator
Networks (DeepONet) with the Neural Tangent Kernel (NTK) to solve complex
inverse problem. The method effectively addresses tasks such as source
localization governed by the Navier-Stokes equations and image reconstruction,
overcoming challenges related to nonlinearity, sparsity, and noisy data. By
incorporating physics-informed constraints and task-specific regularization
into the loss function, the framework ensures solutions that are both
physically consistent and accurate. Validation on diverse synthetic and real
datasets demonstrates its robustness, scalability, and precision, showcasing
its broad potential applications in computational physics and imaging sciences.

</details>


### [39] [Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities](https://arxiv.org/abs/2511.00344)
*Xihang Qiu,Jiarong Cheng,Yuhao Fang,Wanpeng Zhang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: 提出将联邦学习与语义一致性约束的扩散恢复相结合的FedDISC，用对话图与语义条件保持一致性，并通过交替冻结聚合提升训练稳定性，在多数据集上在缺失模态情境下表现更好。


<details>
  <summary>Details</summary>
Motivation: 现实场景中模态可能缺失，且传统依赖完整模态训练的恢复方法在极端分布（如固定模态缺失）下会产生语义失真，单客户端方法鲁棒性不足。引入联邦学习解决数据分布与隐私问题，提高恢复的泛化性与鲁棒性。

Method: 在客户端训练模态专属扩散恢复模型并进行联邦聚合，向缺失该模态的客户端广播；引入DISC-Diffusion模块，结合对话图网络建模会话依赖、语义条件网络进行语义对齐；采用交替冻结聚合策略交替冻结恢复器和分类器以稳定训练。

Result: 在IEMOCAP、CMUMOSI和CMUMOSEI数据集上，FedDISC在多种模态缺失模式下显著提升情绪分类性能，优于现有缺失模态恢复与情感识别方法。

Conclusion: FedDISC通过联邦学习整合扩散模型，实现缺失模态恢复并提升对话情绪识别性能；采用对话图网络与语义条件网络保持语义一致性；交替冻结聚合策略促进协同优化，实验证明在多种缺失模式下优于现有方法。

Abstract: Multimodal Emotion Recognition in Conversations (MERC) enhances emotional
understanding through the fusion of multimodal signals. However, unpredictable
modality absence in real-world scenarios significantly degrades the performance
of existing methods. Conventional missing-modality recovery approaches, which
depend on training with complete multimodal data, often suffer from semantic
distortion under extreme data distributions, such as fixed-modality absence. To
address this, we propose the Federated Dialogue-guided and Semantic-Consistent
Diffusion (FedDISC) framework, pioneering the integration of federated learning
into missing-modality recovery. By federated aggregation of modality-specific
diffusion models trained on clients and broadcasting them to clients missing
corresponding modalities, FedDISC overcomes single-client reliance on modality
completeness. Additionally, the DISC-Diffusion module ensures consistency in
context, speaker identity, and semantics between recovered and available
modalities, using a Dialogue Graph Network to capture conversational
dependencies and a Semantic Conditioning Network to enforce semantic alignment.
We further introduce a novel Alternating Frozen Aggregation strategy, which
cyclically freezes recovery and classifier modules to facilitate collaborative
optimization. Extensive experiments on the IEMOCAP, CMUMOSI, and CMUMOSEI
datasets demonstrate that FedDISC achieves superior emotion classification
performance across diverse missing modality patterns, outperforming existing
approaches.

</details>


### [40] [OSMGen: Highly Controllable Satellite Image Synthesis using OpenStreetMap Data](https://arxiv.org/abs/2511.00345)
*Amir Ziashahabi,Narges Ghasemi,Sajjad Shahabi,John Krumm,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: OSMGen用原始OSM JSON生成可控的卫星影像对，解决城市遥感数据稀缺与不平衡，并支持可视化规划与闭环更新。


<details>
  <summary>Details</summary>
Motivation: 现有自动化城市监测受限于专门标注数据稀缺，且以栅格化地图为输入会丢失语义与几何细节。需要一种能利用丰富OSM结构化数据生成成对（JSON, image）训练样本的方法，以缓解数据短缺并支持可控变化合成。

Method: OSMGen利用完整OSM JSON（向量几何、语义标签、位置和时间），而非栅格瓦片，作为生成输入。框架将OSM数据映射为合成卫星图像，支持对OSM输入进行用户编辑并生成对应的前后图像对，保证非编辑区域视觉上保持一致。

Result: OSMGen能生成现实感强的单帧卫星图像和编辑驱动的前后对比影像，支持针对性合成训练数据以解决类别不平衡，且为城市规划者提供通过编辑OSM预览干预效果的工具。源码已公开。

Conclusion: 本文提出了OSMGen，一个直接从原始OpenStreetMap（OSM）JSON生成逼真卫星影像的框架，能生成前后对比图像并保持场景一致性，为数据稀缺与类不平衡问题提供解决方案，并可用于规划预览和闭环更新流程。

Abstract: Accurate and up-to-date geospatial data are essential for urban planning,
infrastructure monitoring, and environmental management. Yet, automating urban
monitoring remains difficult because curated datasets of specific urban
features and their changes are scarce. We introduce OSMGen, a generative
framework that creates realistic satellite imagery directly from raw
OpenStreetMap (OSM) data. Unlike prior work that relies on raster tiles, OSMGen
uses the full richness of OSM JSON, including vector geometries, semantic tags,
location, and time, giving fine-grained control over how scenes are generated.
A central feature of the framework is the ability to produce consistent
before-after image pairs: user edits to OSM inputs translate into targeted
visual changes, while the rest of the scene is preserved. This makes it
possible to generate training data that addresses scarcity and class imbalance,
and to give planners a simple way to preview proposed interventions by editing
map data. More broadly, OSMGen produces paired (JSON, image) data for both
static and changed states, paving the way toward a closed-loop system where
satellite imagery can automatically drive structured OSM updates. Source code
is available at https://github.com/amir-zsh/OSMGen.

</details>


### [41] [Detecting AI-Generated Images via Diffusion Snap-Back Reconstruction: A Forensic Approach](https://arxiv.org/abs/2511.00352)
*Mohd Ruhul Ameen,Akif Islam*

Main category: cs.CV

TL;DR: 利用对图像在不同噪声强度下的扩散重建轨迹（LPIPS/SSIM/PSNR曲线）提取可解释特征，实现高性能且鲁棒的AI图像鉴别。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造检测依赖频域或像素级痕迹，对现代文本到图像系统效果欠佳。需要一种模型无关、可解释且在压缩、噪声等常见失真下鲁棒的方法来鉴别高逼真度的合成图像。

Method: 通过对图像在不同噪声强度下使用Stable Diffusion v1.5进行多强度重建，记录LPIPS、SSIM和PSNR随噪声强度变化的曲线，提取基于流形的可解释特征用于分类（例如曲线斜率、曲线形状统计量），并在4,000张平衡数据集上训练/验证分类器。

Result: 在4,000张图像的平衡数据集上，所提方法在交叉验证下达到0.993 AUROC；对压缩和噪声失真仍表现稳健；即使只用有限数据和单一扩散骨干也表现出良好泛化能力。

Conclusion: 本文提出利用扩散模型重建动态（diffusion snap-back）区分真实与AI合成图像，结论是该方法在有限数据与单一扩散骨干下仍能达到高性能并具有可解释性和鲁棒性。

Abstract: The rapid rise of generative diffusion models has made distinguishing
authentic visual content from synthetic imagery increasingly challenging.
Traditional deepfake detection methods, which rely on frequency or pixel-level
artifacts, fail against modern text-to-image systems such as Stable Diffusion
and DALL-E that produce photorealistic and artifact-free results. This paper
introduces a diffusion-based forensic framework that leverages multi-strength
image reconstruction dynamics, termed diffusion snap-back, to identify
AI-generated images. By analysing how reconstruction metrics (LPIPS, SSIM, and
PSNR) evolve across varying noise strengths, we extract interpretable
manifold-based features that differentiate real and synthetic images. Evaluated
on a balanced dataset of 4,000 images, our approach achieves 0.993 AUROC under
cross-validation and remains robust to common distortions such as compression
and noise. Despite using limited data and a single diffusion backbone (Stable
Diffusion v1.5), the proposed method demonstrates strong generalization and
interpretability, offering a foundation for scalable, model-agnostic synthetic
media forensics.

</details>


### [42] [Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation](https://arxiv.org/abs/2511.00357)
*Niklas Wölki,Lukas Kondmann,Christian Mollière,Martin Langer,Julia Gottfriedsen,Martin Werner*

Main category: cs.CV

TL;DR: 在单热红外、数据稀缺的CubeSat环境中，利用Landsat-7预训练和轻量UNet（MobileNet编码器）可将宏F1从0.850提升到0.877，并在Jetson Nano上实现<5s的全图推理，适合在轨实时云掩膜。


<details>
  <summary>Details</summary>
Motivation: CubeSat通常仅有单波段热红外观测且标注数据稀缺，现有云掩膜方法难以直接应用，故需探索轻量化、依赖公共数据的迁移学习方案以实现机载实时云掩膜。

Method: 使用UNet架构配合轻量的MobileNet编码器，在Landsat-7云掩膜公开数据集上进行预训练，再在少量FOREST-2任务特定样本上联合微调；随后将模型转换为TensorRT引擎并在NVIDIA Jetson Nano上验证推理速度。

Result: 通过预训练+联合微调，宏F1从仅用FOREST-2基线的0.850提升到0.877；在Jetson Nano上完成全图推理耗时低于5秒，验证了方法的有效性与实用性。

Conclusion: 该论文证明了在资源受限的CubeSat上，通过迁移学习与轻量化网络可以实现高效准确的热红外云掩膜推断，支持实时在轨决策。

Abstract: Onboard cloud segmentation is a critical yet underexplored task in thermal
Earth observation (EO), particularly for CubeSat missions constrained by
limited hardware and spectral information. CubeSats often rely on a single
thermal band and lack sufficient labeled data, making conventional cloud
masking techniques infeasible. This work addresses these challenges by applying
transfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, using
a UNet with a lightweight MobileNet encoder. We pretrain the model on the
public Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a small
set of mission-specific samples in a joint-training setup, improving the macro
F1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to a
TensorRT engine and demonstrate full-image inference in under 5 seconds on an
NVIDIA Jetson Nano. These results show that leveraging public datasets and
lightweight architectures can enable accurate, efficient thermal-only cloud
masking on-orbit, supporting real-time decision-making in data-limited EO
missions.

</details>


### [43] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D 用免费街景图和生成式 AI，实现快速、低成本、无需专家的文化遗产三维重建，适合资源受限国家推广。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产面临资金与技术短缺，传统三维数字化方法成本高且需专家操作，许多古迹未被数字化。提出零成本、易普及的解决方案以民主化遗产保护。

Method: 两阶段流水线：1) 使用 Gemini 2.5 Flash Image 进行多模态视觉推理以合成结构与纹理信息；2) 使用 Hexagen 将合成图像转换为几何体以恢复三维形状。整个流程依赖公开 Google Street View 图像，无需专业设备或人工监督，速度显著优于传统 SfM。

Result: 在 Ahsan Manzil、Choto Sona Mosque、Paharpur 等地标上实验表明，Oitijjo-3D 在秒级时间内生成光度真实且尺度一致的重建，保持视觉与结构细节，同时大幅降低经济与技术门槛。

Conclusion: Oitijjo-3D 提出了一种基于公开街景图像与生成式 AI 的零成本三维文化遗产重建框架，能在资源受限环境中快速生成具有视觉与结构保真度的三维模型。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>


### [44] [Who Can We Trust? Scope-Aware Video Moment Retrieval with Multi-Agent Conflict](https://arxiv.org/abs/2511.00370)
*Chaochen Wu,Guan Luo,Meiyun Zuo,Zhitao Fan*

Main category: cs.CV

TL;DR: 提出基于强化学习与证据学习的多智能体视频时刻检索框架，单次视频扫描定位并融合冲突，能无监督识别无匹配查询，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑不同模型或智能体之间的定位冲突，导致多模型无法有效融合；需要高效且能给出定位证据的检索方法，并能在实际应用中判断查询是否无对应时刻。

Method: 设计一个能在视频上单遍扫描定位边界的强化学习智能体，并构建多智能体框架引入证据学习用于融合和解决各智能体间定位冲突；通过竞争与冲突建模提升RL性能；利用冲突处理机制无监督识别out-of-scope查询。

Result: 在多项基准数据集上进行大量实验，方法在检索准确性和鲁棒性上优于最先进方法；同时展示了多智能体竞争/冲突建模和证据学习在提升RL检索性能与out-of-scope识别中的有效性。

Conclusion: 本文提出基于强化学习与多智能体的视屏时刻检索方法，通过单次扫描产生边界并输出证据，能有效解决不同模型定位冲突，并可无额外训练判断查询是否超出范围，实验表明优于现有方法。

Abstract: Video moment retrieval uses a text query to locate a moment from a given
untrimmed video reference. Locating corresponding video moments with text
queries helps people interact with videos efficiently. Current solutions for
this task have not considered conflict within location results from different
models, so various models cannot integrate correctly to produce better results.
This study introduces a reinforcement learning-based video moment retrieval
model that can scan the whole video once to find the moment's boundary while
producing its locational evidence. Moreover, we proposed a multi-agent system
framework that can use evidential learning to resolve conflicts between agents'
localization output. As a side product of observing and dealing with conflicts
between agents, we can decide whether a query has no corresponding moment in a
video (out-of-scope) without additional training, which is suitable for
real-world applications. Extensive experiments on benchmark datasets show the
effectiveness of our proposed methods compared with state-of-the-art
approaches. Furthermore, the results of our study reveal that modeling
competition and conflict of the multi-agent system is an effective way to
improve RL performance in moment retrieval and show the new role of evidential
learning in the multi-agent framework.

</details>


### [45] [VisionCAD: An Integration-Free Radiology Copilot Framework](https://arxiv.org/abs/2511.00381)
*Jiaming Li,Junlei Wu,Sheng Wang,Honglin Xiong,Jiangdong Cai,Zihao Zhao,Yitao Zhu,Yuan Yin,Dinggang Shen,Qian Wang*

Main category: cs.CV

TL;DR: VisionCAD 用相机拍摄显示器并通过检测—恢复—分析流水线将屏幕影像转成诊断级图像，达到与原始数字图像近似的AI诊断和自动报告效果，便于低成本部署。


<details>
  <summary>Details</summary>
Motivation: 现有CAD系统难以整合到医院既有IT基础设施，导致部署受限。提出一种无需改动医院系统、仅需相机和计算资源即可工作的替代方案。

Method: 提出一个模块化管道：屏幕图像检测、图像恢复（去噪、几何校正、颜色/对比修复）、以及下游诊断与报告生成；可插拔地采用现有先进诊断模型。

Result: 在多个医学影像数据集上的验证表明：分类任务F1仅下降<2%，自动报告的自然语言生成指标与原始图像相比下降<1%，表明性能接近传统CAD。

Conclusion: VisionCAD 通过摄像头直接采集屏幕上的医学图像，绕过医院IT集成障碍，实现了接近数字源图像的诊断性能，便于在多种临床环境中部署AI辅助诊断。

Abstract: Widespread clinical deployment of computer-aided diagnosis (CAD) systems is
hindered by the challenge of integrating with existing hospital IT
infrastructure. Here, we introduce VisionCAD, a vision-based radiological
assistance framework that circumvents this barrier by capturing medical images
directly from displays using a camera system. The framework operates through an
automated pipeline that detects, restores, and analyzes on-screen medical
images, transforming camera-captured visual data into diagnostic-quality images
suitable for automated analysis and report generation. We validated VisionCAD
across diverse medical imaging datasets, demonstrating that our modular
architecture can flexibly utilize state-of-the-art diagnostic models for
specific tasks. The system achieves diagnostic performance comparable to
conventional CAD systems operating on original digital images, with an F1-score
degradation typically less than 2\% across classification tasks, while natural
language generation metrics for automated reports remain within 1\% of those
derived from original images. By requiring only a camera device and standard
computing resources, VisionCAD offers an accessible approach for AI-assisted
diagnosis, enabling the deployment of diagnostic capabilities in diverse
clinical settings without modifications to existing infrastructure.

</details>


### [46] [Rethinking Facial Expression Recognition in the Era of Multimodal Large Language Models: Benchmark, Datasets, and Beyond](https://arxiv.org/abs/2511.00389)
*Fan Zhang,Haoxuan Li,Shengju Qian,Xin Wang,Zheng Lian,Hao Wu,Zhihong Zhu,Yuan Gao,Qiankun Li,Yefeng Zheng,Zhouchen Lin,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出FERBench并构建两阶段后训练数据（UniFER-CoT-230K, UniFER-RLVR-360K），通过后训练得到UniFER-7B，提升MLLM在FER上的推理与可解释性，性能超越多款大模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用MLLM虽在多领域表现优异，但在面部表情识别（FER）任务上的能力尚未被系统评估，且推理与可解释性不足。为促进统一FER方法并提升MLLM在FER的性能，作者提出将FER任务转为VQA以直接利用MLLM并设计后训练策略改进模型的表情推理能力。

Method: 将传统FER数据集转换为VQA格式以适配MLLM，提出FERBench评估框架；构建两大后训练数据集：UniFER-CoT-230K用于Chain-of-Thought风格的冷启动训练，UniFER-RLVR-360K用于基于可验证奖励的强化学习；基于这些数据对基础模型进行后训练得到UniFER-7B，并与多款模型进行对比实验。

Result: FERBench评估显示20个先进MLLM在四个FER数据集上的分类能力总体不错，但推理与可解释性差；通过后训练策略得到的UniFER-7B在FER任务上显著提升，优于众多开源和闭源大模型（如Gemini-2.5-Pro、Qwen2.5-VL-72B）。

Conclusion: 该论文提出了FERBench基准，评估了20个最先进的视觉-语言大模型在四个表情识别数据集上的表现，发现MLLMs在分类上表现尚可，但推理与可解释性存在显著不足。作者进一步通过两阶段的后训练策略（冷启动的UniFER-CoT-230K与用于可验证奖励的RL的UniFER-RLVR-360K）提升模型的表情推理能力，最终得到统一且可解释的基础模型UniFER-7B，性能超越多款开源与闭源通用MLLM。

Abstract: Multimodal Large Language Models (MLLMs) have revolutionized numerous
research fields, including computer vision and affective computing. As a
pivotal challenge in this interdisciplinary domain, facial expression
recognition (FER) has evolved from separate, domain-specific models to more
unified approaches. One promising avenue to unify FER tasks is converting
conventional FER datasets into visual question-answering (VQA) formats,
enabling the direct application of powerful generalist MLLMs for inference.
However, despite the success of cutting-edge MLLMs in various tasks, their
performance on FER tasks remains largely unexplored. To address this gap, we
provide FERBench, a systematic benchmark that incorporates 20 state-of-the-art
MLLMs across four widely used FER datasets. Our results reveal that, while
MLLMs exhibit good classification performance, they still face significant
limitations in reasoning and interpretability. To this end, we introduce
post-training strategies aimed at enhancing the facial expression reasoning
capabilities of MLLMs. Specifically, we curate two high-quality and large-scale
datasets: UniFER-CoT-230K for cold-start initialization and UniFER-RLVR-360K
for reinforcement learning with verifiable rewards (RLVR), respectively.
Building upon them, we develop a unified and interpretable FER foundation model
termed UniFER-7B, which outperforms many open-sourced and closed-source
generalist MLLMs (e.g., Gemini-2.5-Pro and Qwen2.5-VL-72B).

</details>


### [47] [VinciCoder: Unifying Multimodal Code Generation via Coarse-to-fine Visual Reinforcement Learning](https://arxiv.org/abs/2511.00391)
*Xuanle Zhao,Deyang Jiang,Zhixiong Zeng,Lei Chen,Haibo Qiu,Jing Huang,Yufeng Zhong,Liming Zheng,Yilin Cao,Lin Ma*

Main category: cs.CV

TL;DR: 提出VinciCoder：1.6M SFT语料 + 粗到细视觉强化学习，统一提升多模态代码生成与视觉代码修正，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多为单任务训练，限制了通用视觉代码智能（Vision Code Intelligence, VCI）的发展；因此需要统一的多模态代码生成模型与训练框架。

Method: 两阶段训练：首先构建1.6M图像-代码对的SFT语料，覆盖直接生成与基于视觉的代码精修；随后引入Visual Reinforcement Learning (ViRL)，采用粗到细的奖励机制，基于局部与全局图像块的视觉相似度来优化视觉保真性。

Result: 在多种多模态代码生成基准上，VinciCoder达到了新的最先进性能，证明了粗到细ViRL策略和大规模SFT语料的有效性。

Conclusion: VinciCoder通过结合大规模有监督微调和视觉强化学习，显著提升了视觉信息下的代码生成与代码修正能力，达到了多项基准的最先进水平。

Abstract: Multimodal code generation has garnered significant interest within the
research community. Despite the notable success of recent vision-language
models (VLMs) on specialized tasks like Chart-to-code generation, their
reliance on single-task training regimens fosters a narrow paradigm that
hinders the development of generalized \textbf{VI}sio\textbf{N} \textbf{C}ode
\textbf{I}ntelligence. In this work, we introduce \textbf{VinciCoder}, a
unified multimodal code generation model that addresses this limitation via a
two-stage training framework. We begin by constructing a large-scale Supervised
Finetuning (SFT) corpus comprising 1.6M image-code pairs for tasks involving
direct code generation and visual-based code refinement. Subsequently, we
introduce a Visual Reinforcement Learning (ViRL) strategy, which employs a
coarse-to-fine reward mechanism to improve visual fidelity by calculating
visual similarity across local and global image patches. Extensive experiments
on various multimodal code generation benchmarks demonstrate that VinciCoder
achieves state-of-the-art performance, underscoring the effectiveness of our
coarse-to-fine ViRL strategy. The code and model will be available at
https://github.com/DocTron-hub/VinciCoder.

</details>


### [48] [CoT-Saliency: Unified Chain-of-Thought Reasoning for Heterogeneous Saliency Tasks](https://arxiv.org/abs/2511.00396)
*Long Li,Shuichen Ji,Ziyang Luo,Nian Liu,Dingwen Zhang,Junwei Han*

Main category: cs.CV

TL;DR: 提出在VLM中用CoT统一SOD/CoSOD/SIS任务，结合SFT+RL和新颖的CGPO算法及输出到推理的数据构造，带来显著性能提升与数据效率。


<details>
  <summary>Details</summary>
Motivation: 三类显著性任务在操作上异构，难以用单一模型处理；希望通过在VLM中用CoT统一表述任务，以减少任务差异并利用语言推理能力。

Method: 两阶段CoT训练：先有监督微调（SFT），再用强化学习（RL）优化。引入了Confidence-Guided Policy Optimization（CGPO），一种单样本RL算法，用奖励与模型置信差作为样本优势信号，避免组采样并聚焦有信息的响应；同时提出“输出到推理”策略生成高保真SFT数据。

Result: 模型在所有任务上匹配或超越专用SOTA和强闭源VLM，尤其在CoSOD的CoCA数据集上S-measure达到0.899，比之前最好提升8.0个百分点，且使用训练数据远少于对手。

Conclusion: 该论文提出了一个统一框架，将SOD、CoSOD和SIS三种异构显著性任务统一为在视觉-语言模型中以链式思维（CoT）推理的形式来解决，实验显示性能优于或匹配专门方法和闭源强模型。

Abstract: We present the first unified framework that jointly handles three
operationally heterogeneous saliency tasks, eg, SOD, CoSOD, and SIS, by casting
each as a Chain-of-Thought (CoT) reasoning process in a Vision-Language Model
(VLM) to bridge task heterogeneity. CoT training follows a two-stage paradigm:
Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). To enhance CoT
quality in RL, we propose Confidence-Guided Policy Optimization (CGPO), a
lightweight single-sample algorithm that leverages the discrepancy between
reward and model confidence as a per-sample advantage signal. This design
naturally focuses updates on informative responses while eliminating group
sampling, thereby addressing GRPO's key limitations: confidence-agnostic
learning, signal dilution, and prohibitive computational overhead. We also
introduce an "output-to-reasoning" strategy to construct high-fidelity SFT data
that ensures logical consistency with ground-truth masks. Experiments show our
model matches or outperforms specialized SOTA methods and strong closed-source
VLMs across all tasks, especially achieving an S-measure of 0.899 on CoCA for
CoSOD, surpassing the prior best by 8.0 percentage points, despite using far
less training data.

</details>


### [49] [LGCA: Enhancing Semantic Representation via Progressive Expansion](https://arxiv.org/abs/2511.00419)
*Thanh Hieu Cao,Trung Khang Tran,Gia Thinh Pham,Tuong Nghiem Diep,Thanh Binh Nguyen*

Main category: cs.CV

TL;DR: LGCA通过选择并扩展最显著局部区域并结合原图进行相似度计算，解决随机裁剪带来的误导，显著提升CLIP类模型的零样本图像分类性能，同时保持原有时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有基于随机裁剪的方法会因CLIP对小尺度特征的敏感性而引入误导信息和偏差，导致性能不稳。需要一种既能利用局部显著区域信息又能避免误导的对齐方法，以提升零样本分类表现。

Method: 提出Localized-Globalized Cross-Alignment (LGCA)框架：首先对图像进行局部特征提取，反复选择最显著区域并进行扩展；设计新的相似度评分，将原图与扩展图同时纳入计算，融合局部与全局信息；保留原有模型推理流程，扩展步骤在不增加整体时间复杂度的情况下完成。

Result: 在多种数据集上进行大规模实验证明，LGCA在零样本图像分类任务上显著提升性能，优于当前最先进方法；理论分析表明其时间复杂度与未扩展的原始模型相同，证明了方法的高效性与可扩展性。

Conclusion: LGCA通过先捕获图像局部特征再扩展最显著区域，实现本地-全局特征对齐，有效降低随机裁剪引入的误导信息和偏差。该方法在零样本图像分类任务上显著优于现有基线，且在理论上保持与原模型相同的时间复杂度，兼具性能与效率。

Abstract: Recent advancements in large-scale pretraining in natural language processing
have enabled pretrained vision-language models such as CLIP to effectively
align images and text, significantly improving performance in zero-shot image
classification tasks. Subsequent studies have further demonstrated that
cropping images into smaller regions and using large language models to
generate multiple descriptions for each caption can further enhance model
performance. However, due to the inherent sensitivity of CLIP, random image
crops can introduce misinformation and bias, as many images share similar
features at small scales. To address this issue, we propose
Localized-Globalized Cross-Alignment (LGCA), a framework that first captures
the local features of an image and then repeatedly selects the most salient
regions and expands them. The similarity score is designed to incorporate both
the original and expanded images, enabling the model to capture both local and
global features while minimizing misinformation. Additionally, we provide a
theoretical analysis demonstrating that the time complexity of LGCA remains the
same as that of the original model prior to the repeated expansion process,
highlighting its efficiency and scalability. Extensive experiments demonstrate
that our method substantially improves zero-shot performance across diverse
datasets, outperforming state-of-the-art baselines.

</details>


### [50] [Leveraging Hierarchical Image-Text Misalignment for Universal Fake Image Detection](https://arxiv.org/abs/2511.00427)
*Daichi Zhang,Tong Zhang,Jianmin Bao,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 作者提出ITEM，通过在CLIP空间测量图文对齐差异并用分层策略提取全局与局部线索，借助轻量MLP实现对生成图像的鲁棒检测，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有仅基于视觉特征的生成图像检测器容易过拟合特定图像模式，难以泛化到未见的生成模型，作者发现生成图像与其描述文本在语义对齐上存在异常，因而通过多模态对齐差异来提高检测鲁棒性与泛化能力。

Method: 利用预训练CLIP编码器计算图像与对应描述文本的相似度作为特征，设计分层误配策略（先全图再对象级）提取全局与局部对齐差异，最后在这些特征上训练一个轻量MLP进行二分类检测。

Result: 在多种新近生成模型上进行大量实验，ITEM在泛化性和鲁棒性指标上显著超过若干最新基线方法，证明图文对齐差异是有效的判别线索。

Conclusion: 本文提出了基于图文对齐差异的生成图像检测方法ITEM，通过在CLIP空间测量图像与文本的对齐度并微调MLP分类头来区分真伪图像；采用分层对齐方案兼顾全局与局部语义，实验表明在泛化性和鲁棒性上优于现有方法。

Abstract: With the rapid development of generative models, detecting generated fake
images to prevent their malicious use has become a critical issue recently.
Existing methods frame this challenge as a naive binary image classification
task. However, such methods focus only on visual clues, yielding trained
detectors susceptible to overfitting specific image patterns and incapable of
generalizing to unseen models. In this paper, we address this issue from a
multi-modal perspective and find that fake images cannot be properly aligned
with corresponding captions compared to real images. Upon this observation, we
propose a simple yet effective detector termed ITEM by leveraging the
image-text misalignment in a joint visual-language space as discriminative
clues. Specifically, we first measure the misalignment of the images and
captions in pre-trained CLIP's space, and then tune a MLP head to perform the
usual detection task. Furthermore, we propose a hierarchical misalignment
scheme that first focuses on the whole image and then each semantic object
described in the caption, which can explore both global and fine-grained local
semantic misalignment as clues. Extensive experiments demonstrate the
superiority of our method against other state-of-the-art competitors with
impressive generalization and robustness on various recent generative models.

</details>


### [51] [Enhancing Frequency Forgery Clues for Diffusion-Generated Image Detection](https://arxiv.org/abs/2511.00429)
*Daichi Zhang,Tong Zhang,Shiming Ge,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 本文提出基于频域的频率选择加权策略来增强频率伪造线索，能更好区分扩散生成图像与真实图像，提升对未见模型及扰动的检测性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的高质量图像带来滥用风险，而现有检测器难以跨不同扩散模型和扰动保持判别能力，需寻找更具普适性的判别线索。

Method: 分析真实图像与扩散生成图像在低到高频段的差异，引入一个频率选择函数对傅里叶谱做加权滤波，抑制判别力弱的频段，增强判别力强的频段，从而得到增强的F^2C表征用于训练检测器。

Result: 在多种扩散生成图像数据集上的大量实验表明，该方法在泛化性和鲁棒性上优于最先进检测器。

Conclusion: 本文提出通过增强跨频带的频率伪造线索(F^2C)来检测扩散生成图像，结论是该方法能提升对未见扩散模型的泛化能力并增强对扰动的鲁棒性，效果优于现有方法。

Abstract: Diffusion models have achieved remarkable success in image synthesis, but the
generated high-quality images raise concerns about potential malicious use.
Existing detectors often struggle to capture discriminative clues across
different models and settings, limiting their generalization to unseen
diffusion models and robustness to various perturbations. To address this
issue, we observe that diffusion-generated images exhibit progressively larger
differences from natural real images across low- to high-frequency bands. Based
on this insight, we propose a simple yet effective representation by enhancing
the Frequency Forgery Clue (F^2C) across all frequency bands. Specifically, we
introduce a frequency-selective function which serves as a weighted filter to
the Fourier spectrum, suppressing less discriminative bands while enhancing
more informative ones. This approach, grounded in a comprehensive analysis of
frequency-based differences between natural real and diffusion-generated
images, enables general detection of images from unseen diffusion models and
provides robust resilience to various perturbations. Extensive experiments on
various diffusion-generated image datasets demonstrate that our method
outperforms state-of-the-art detectors with superior generalization and
robustness.

</details>


### [52] [ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training](https://arxiv.org/abs/2511.00446)
*Xin Yao,Haiyang Zhao,Yimin Chen,Jiawei Guo,Kecheng Huang,Ming Zhao*

Main category: cs.CV

TL;DR: 提出面向CLIP预训练的文本中毒框架ToxicTextCLIP，通过背景感知选择与背景驱动生成高质量中毒文本，实验证明其攻击效果强且能规避多种防御。


<details>
  <summary>Details</summary>
Motivation: 当前关于CLIP安全性的研究多聚焦图像层面的攻击，忽视了同等重要的文本模态；未经筛选的网络文本可能被恶意利用用于数据中毒与后门攻击。

Method: 提出迭代框架，包括背景感知选择器（筛选与目标类背景一致的文本）与背景驱动扩增器（生成语义连贯且多样的中毒文本），在预训练数据中注入这些文本以实现对CLIP的中毒和后门攻击。

Result: 在分类与检索任务上达到高中毒成功率（最高95.83%）和高后门Hit@1（最高98.68%），且能绕过RoCLIP、CleanCLIP和SafeCLIP等防御方法。

Conclusion: 本文提出ToxicTextCLIP，展示了CLIP预训练阶段通过文本模态进行高效中毒与后门植入的可行性，强调文本攻击的危险性并验证了其能绕过现有防御。

Abstract: The Contrastive Language-Image Pretraining (CLIP) model has significantly
advanced vision-language modeling by aligning image-text pairs from large-scale
web data through self-supervised contrastive learning. Yet, its reliance on
uncurated Internet-sourced data exposes it to data poisoning and backdoor
risks. While existing studies primarily investigate image-based attacks, the
text modality, which is equally central to CLIP's training, remains
underexplored. In this work, we introduce ToxicTextCLIP, a framework for
generating high-quality adversarial texts that target CLIP during the
pre-training phase. The framework addresses two key challenges: semantic
misalignment caused by background inconsistency with the target class, and the
scarcity of background-consistent texts. To this end, ToxicTextCLIP iteratively
applies: 1) a background-aware selector that prioritizes texts with background
content aligned to the target class, and 2) a background-driven augmenter that
generates semantically coherent and diverse poisoned samples. Extensive
experiments on classification and retrieval tasks show that ToxicTextCLIP
achieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, while
bypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can be
accessed via https://github.com/xinyaocse/ToxicTextCLIP/.

</details>


### [53] [Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations](https://arxiv.org/abs/2511.00456)
*Kiran Shahi,Anup Bagale*

Main category: cs.CV

TL;DR: 提出一种基于图像级标签和Grad-CAM的弱监督可解释框架，在无像素级标注下实现高性能的肺炎分类与定位，ResNet-18和EfficientNet-B0表现最佳，MobileNet-V2具成本效益。


<details>
  <summary>Details</summary>
Motivation: 动机是减少昂贵的像素级注释需求，通过弱监督与可解释性技术（Grad-CAM）实现肺炎的自动分类与定位，从而增进AI在放射诊断中的透明度与可信度。

Method: 方法包括：使用图像级标签训练多种ImageNet预训练的CNN与ViT（ResNet-18/50、DenseNet-121、EfficientNet-B0、MobileNet-V2/V3、ViT-B16），在相同训练条件下采用focal loss、患者级数据划分以防止泄漏，并用Grad-CAM生成可视化热图进行定位验证。

Result: 实验在Kermany CXR数据集上表明：ResNet-18和EfficientNet-B0达到最好测试准确率98%、ROC-AUC=0.997、F1=0.987；MobileNet-V2在准确率与计算成本间取得较优折衷。Grad-CAM可视化显示模型关注于临床相关的肺部区域。

Conclusion: 该论文结论为：弱监督的基于Grad-CAM的深度学习框架在胸片肺炎分类与定位任务上表现优异，能在仅用图像级标签的情况下生成临床相关的热图，提高了模型可解释性和临床信任。

Abstract: This study proposes a weakly supervised deep learning framework for pneumonia
classification and localization from chest X-rays, utilizing Grad-CAM
explanations. Instead of costly pixel-level annotations, our approach utilizes
image-level labels to generate clinically meaningful heatmaps that highlight
regions affected by pneumonia. We evaluate seven ImageNet-pretrained
architectures ResNet-18/50, DenseNet-121, EfficientNet-B0, MobileNet-V2/V3, and
ViT-B16 under identical training conditions with focal loss and patient-wise
splits to prevent data leakage. Experimental results on the Kermany CXR dataset
demonstrate that ResNet-18 and EfficientNet-B0 achieve the best overall test
accuracy of 98\%, ROC-AUC = 0.997, and F1 = 0.987, while MobileNet-V2 provides
an optimal trade-off between accuracy and computational cost. Grad-CAM
visualizations confirm that the proposed models focus on clinically relevant
lung regions, supporting the use of interpretable AI for radiological
diagnostics. This work highlights the potential of weakly supervised
explainable models that enhance pneumonia screening transparency, and clinical
trust in AI-assisted medical imaging.
  https://github.com/kiranshahi/pneumonia-analysis

</details>


### [54] [HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation](https://arxiv.org/abs/2511.00468)
*Panwang Pan,Tingting Shen,Chenxin Li,Yunlong Lin,Kairun Wen,Jingjing Zhao,Yixuan Yuan*

Main category: cs.CV

TL;DR: 提出HumanCrafter，一种结合几何与自监督语义先验的单图像前馈式多任务框架，通过像素对齐和交互式标注提升数据与模型质量，在3D人体重建与部位分割上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在3D人体重建上取得高保真度，但在特定任务（如3D人体部位分割）上的适用性受限，因此需要一种能联合处理外观与语义的统一框架，且要缓解带标签3D数据稀缺的问题。

Method: 在重建阶段整合人体几何先验，在分割阶段采用自监督语义先验；设计像素对齐聚合模块用于跨任务信息融合，并采用多任务损失同时优化纹理重建质量与语义一致性；开发交互式注释流程以扩充3D人体带标签数据集。

Result: 在单张图像的3D人体重建与人体部位分割任务上，HumanCrafter在大量实验中优于现有最先进的方法，表现出更高的分割准确率和重建保真度。

Conclusion: HumanCrafter提出了一种统一的前馈框架，同时建模外观和人体部位语义，从单张图像重建高保真3D人体并进行语义分割。通过在重建中引入几何先验、在分割中引入自监督语义先验，并通过像素对齐聚合实现任务间协同，配合交互式标注生成高质量训练数据，取得了优于现有方法的性能。

Abstract: Recent advances in generative models have achieved high-fidelity in 3D human
reconstruction, yet their utility for specific tasks (e.g., human 3D
segmentation) remains constrained. We propose HumanCrafter, a unified framework
that enables the joint modeling of appearance and human-part semantics from a
single image in a feed-forward manner. Specifically, we integrate human
geometric priors in the reconstruction stage and self-supervised semantic
priors in the segmentation stage. To address labeled 3D human datasets
scarcity, we further develop an interactive annotation procedure for generating
high-quality data-label pairs. Our pixel-aligned aggregation enables cross-task
synergy, while the multi-task objective simultaneously optimizes texture
modeling fidelity and semantic consistency. Extensive experiments demonstrate
that HumanCrafter surpasses existing state-of-the-art methods in both 3D
human-part segmentation and 3D human reconstruction from a single image.

</details>


### [55] [Longitudinal Vestibular Schwannoma Dataset with Consensus-based Human-in-the-loop Annotations](https://arxiv.org/abs/2511.00472)
*Navodini Wijethilake,Marina Ivory,Oscar MacCormac,Siddhant Kumar,Aaron Kujawa,Lorena Garcia-Foncillas Macias,Rebecca Burger,Amanda Hitchings,Suki Thomson,Sinan Barazi,Eleni Maratos,Rupert Obholzer,Dan Jiang,Fiona McClenaghan,Kazumi Chia,Omar Al-Salihi,Nick Thomas,Steve Connor,Tom Vercauteren,Jonathan Shapey*

Main category: cs.CV

TL;DR: 本文通过自举式人机交互深度学习框架，结合多中心专家共识标注，显著提升了前庭神经鞘瘤在MRI上的自动分割精度（DSC 0.9125→0.9670），在外部数据上保持稳健，注释效率约提升37%，并公开了534例带注释的T1CE扫描数据。


<details>
  <summary>Details</summary>
Motivation: 手动标注VS在MRI上既耗时又需专家参与，且现有自动化分割方法在跨中心、多样化临床场景下泛化能力有限。作者旨在提供一种资源高效且可推广的标注与训练策略，提高自动分割模型在目标分布上的性能并减少人工负担。

Method: 构建多中心、经专家共识审核的标注数据集；采用自举式DL模型进行初始分割；通过人机交互的迭代流程对分割结果进行质量审查和修正，形成高质量标注；将这些标注用于再训练模型以适应目标数据分布。

Result: 在内部目标验证集上，分割的DSC由0.9125提升到0.9670；在代表性外部数据集上性能保持稳定；专家对143例扫描的评估指出若干需要人工干预的细微情况；相较于传统全手工标注流程，估计效率提升约37.4%。数据集包含190名患者，184名患者的534次T1CE带注释，另有6名患者的T2序列未注释，并已公开发布于TCIA。

Conclusion: 作者提出了一个基于人机交互（human-in-the-loop）和自举（bootstrapped）深度学习框架，用于迭代地生成和优化前庭神经鞘瘤(VS)在MRI上的分割标注。该方法能显著提升目标数据分布上的分割精度，同时在外部数据集上保持稳定性能，并能在注释效率上带来明显改进。

Abstract: Accurate segmentation of vestibular schwannoma (VS) on Magnetic Resonance
Imaging (MRI) is essential for patient management but often requires
time-intensive manual annotations by experts. While recent advances in deep
learning (DL) have facilitated automated segmentation, challenges remain in
achieving robust performance across diverse datasets and complex clinical
cases. We present an annotated dataset stemming from a bootstrapped DL-based
framework for iterative segmentation and quality refinement of VS in MRI. We
combine data from multiple centres and rely on expert consensus for
trustworthiness of the annotations. We show that our approach enables effective
and resource-efficient generalisation of automated segmentation models to a
target data distribution. The framework achieved a significant improvement in
segmentation accuracy with a Dice Similarity Coefficient (DSC) increase from
0.9125 to 0.9670 on our target internal validation dataset, while maintaining
stable performance on representative external datasets. Expert evaluation on
143 scans further highlighted areas for model refinement, revealing nuanced
cases where segmentation required expert intervention. The proposed approach is
estimated to enhance efficiency by approximately 37.4% compared to the
conventional manual annotation process. Overall, our human-in-the-loop model
training approach achieved high segmentation accuracy, highlighting its
potential as a clinically adaptable and generalisable strategy for automated VS
segmentation in diverse clinical settings. The dataset includes 190 patients,
with tumour annotations available for 534 longitudinal contrast-enhanced
T1-weighted (T1CE) scans from 184 patients, and non-annotated T2-weighted scans
from 6 patients. This dataset is publicly accessible on The Cancer Imaging
Archive (TCIA) (https://doi.org/10.7937/bq0z-xa62).

</details>


### [56] [FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts](https://arxiv.org/abs/2511.00480)
*Weihao Bo,Yanpeng Sun,Yu Wang,Xinyu Zhang,Zechao Li*

Main category: cs.CV

TL;DR: 提出FedMGP：每客户端多组文本-视觉prompt+多样性损失，按相似度概率采样聚合，兼顾个性化与共享知识，参数高效，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统联邦提示学习往往只使用单一提示或无法兼顾多样性与个性化，导致对细粒度语义和实例级信息建模不足，且通信效率和参数量有待优化。

Method: 为每个客户端分配多个prompt组；设计多样性损失促使各组专注不同语义；通信时基于余弦相似度计算softmax概率采样s组进行聚合；保持固定prompt容量以实现参数效率。

Result: 理论分析表明动态聚合有助于增强共享语义并抑制客户端噪声；大量实验显示在个性化与域泛化任务上优于现有方法，在通信参数上也最优。

Conclusion: FedMGP通过多组成对文本与视觉prompt、引入多样性损失和基于相似度的概率聚合策略，实现了在保留公共知识的同时学习客户端特有特征，兼顾个性化与域泛化能力。

Abstract: In this paper, we introduce FedMGP, a new paradigm for personalized federated
prompt learning in vision-language models. FedMGP equips each client with
multiple groups of paired textual and visual prompts, enabling the model to
capture diverse, fine-grained semantic and instance-level cues. A diversity
loss is introduced to drive each prompt group to specialize in distinct and
complementary semantic aspects, ensuring that the groups collectively cover a
broader range of local characteristics. During communication, FedMGP employs a
dynamic prompt aggregation strategy based on similarity-guided probabilistic
sampling: each client computes the cosine similarity between its prompt groups
and the global prompts from the previous round, then samples s groups via a
softmax-weighted distribution. This soft selection mechanism preferentially
aggregates semantically aligned knowledge while still enabling exploration of
underrepresented patterns effectively balancing the preservation of common
knowledge with client-specific features. Notably, FedMGP maintains parameter
efficiency by redistributing a fixed prompt capacity across multiple groups,
achieving state-of-the-art performance with the lowest communication parameters
among all federated prompt learning methods. Theoretical analysis shows that
our dynamic aggregation strategy promotes robust global representation learning
by reinforcing shared semantics while suppressing client-specific noise.
Extensive experiments demonstrate that FedMGP consistently outperforms prior
approaches in both personalization and domain generalization across diverse
federated vision-language benchmarks. The code will be released on
https://github.com/weihao-bo/FedMGP.git.

</details>


### [57] [Diff4Splat: Controllable 4D Scene Generation with Latent Dynamic Reconstruction Models](https://arxiv.org/abs/2511.00503)
*Panwang Pan,Chenguo Lin,Jingjing Zhao,Chenxin Li,Yuchen Lin,Haopeng Li,Honglei Yan,Kairun Wen,Yunlong Lin,Yixuan Yuan,Yadong Mu*

Main category: cs.CV

TL;DR: Diff4Splat通过视频潜在变换器结合视频扩散先验与4D几何/运动约束，实现从单张图像在一次前向传递中快速生成可控且显式的四维场景，质量与优化方法相当但速度快得多（约30s）。


<details>
  <summary>Details</summary>
Motivation: 动机是希望将强大的生成先验（视频扩散模型）与几何和运动知识结合，实现在无需昂贵优化的情况下从单图像高效生成可控的4D动态场景。

Method: 方法核心是一个视频潜在变换器（video latent transformer），用于扩展视频扩散模型，联合建模时空依赖并直接预测随时间变化的3D高斯基元（deformable 3D Gaussian field）。输入包括单张图像、相机轨迹和可选文本提示，训练通过外观保真、几何精度和运动一致性等目标来约束。

Result: Diff4Splat在视频生成、新视角合成和几何提取任务中表现良好，能匹配或超越基于优化的方法，同时效率显著更高，可在约30秒内合成高质量4D场景。

Conclusion: Diff4Splat提出了一种从单张图像快速生成可控且明确的4D场景的前馈方法，兼容视频扩散模型的生成先验并结合4D数据集的几何和运动约束，能在单次前向推断中预测可变形的3D高斯场，从而无需测试时优化或后处理。

Abstract: We introduce Diff4Splat, a feed-forward method that synthesizes controllable
and explicit 4D scenes from a single image. Our approach unifies the generative
priors of video diffusion models with geometry and motion constraints learned
from large-scale 4D datasets. Given a single input image, a camera trajectory,
and an optional text prompt, Diff4Splat directly predicts a deformable 3D
Gaussian field that encodes appearance, geometry, and motion, all in a single
forward pass, without test-time optimization or post-hoc refinement. At the
core of our framework lies a video latent transformer, which augments video
diffusion models to jointly capture spatio-temporal dependencies and predict
time-varying 3D Gaussian primitives. Training is guided by objectives on
appearance fidelity, geometric accuracy, and motion consistency, enabling
Diff4Splat to synthesize high-quality 4D scenes in 30 seconds. We demonstrate
the effectiveness of Diff4Splatacross video generation, novel view synthesis,
and geometry extraction, where it matches or surpasses optimization-based
methods for dynamic scene synthesis while being significantly more efficient.

</details>


### [58] [VinDr-CXR-VQA: A Visual Question Answering Dataset for Explainable Chest X-Ray Analysis with Multi-Task Learning](https://arxiv.org/abs/2511.00504)
*Hai-Dang Nguyen,Ha-Hieu Pham,Hao T. Nguyen,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: VinDr-CXR-VQA是一个经放射科医生验证、带边界框和临床推理解释的大规模胸片VQA数据集（4,394张图，17,597问答），覆盖6类诊断问题并平衡阳性/阴性样本，公开可用，能提升Med-VQA模型的回答准确性与定位能力。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VQA缺乏大规模带空间定位与可解释性标注的数据，且在正常案例中容易产生幻觉（hallucination）；因此需一个临床验证、可复现且具定位解释能力的数据集以推动研究。

Method: 构建包含4,394张胸片和17,597个问答对的数据集，所有问答均配备放射科医生验证的边界框和临床推理解释；设计涵盖6种诊断类型的问题分类，并平衡正负样本（41.7%阳性，58.3%阴性）；用MedGemma-4B-it进行基准测试并评估回答与定位性能。

Result: 发布了VinDr-CXR-VQA数据集并公开评测工具；在MedGemma-4B-it基准上提高了性能（F1=0.624，较基线提升11.8%），且支持病灶定位评估。

Conclusion: 该论文提出了一个大规模、具备定位解释能力的胸片问答数据集VinDr-CXR-VQA，为可解释的医学视觉问答研究提供了资源。

Abstract: We present VinDr-CXR-VQA, a large-scale chest X-ray dataset for explainable
Medical Visual Question Answering (Med-VQA) with spatial grounding. The dataset
contains 17,597 question-answer pairs across 4,394 images, each annotated with
radiologist-verified bounding boxes and clinical reasoning explanations. Our
question taxonomy spans six diagnostic types-Where, What, Is there, How many,
Which, and Yes/No-capturing diverse clinical intents. To improve reliability,
we construct a balanced distribution of 41.7% positive and 58.3% negative
samples, mitigating hallucinations in normal cases. Benchmarking with
MedGemma-4B-it demonstrates improved performance (F1 = 0.624, +11.8% over
baseline) while enabling lesion localization. VinDr-CXR-VQA aims to advance
reproducible and clinically grounded Med-VQA research. The dataset and
evaluation tools are publicly available at
huggingface.co/datasets/Dangindev/VinDR-CXR-VQA.

</details>


### [59] [OmniTrack++: Omnidirectional Multi-Object Tracking by Learning Large-FoV Trajectory Feedback](https://arxiv.org/abs/2511.00510)
*Kai Luo,Hao Shi,Kunyu Peng,Fei Teng,Sheng Wu,Kaiwei Wang,Kailun Yang*

Main category: cs.CV

TL;DR: 针对360°全景MOT，OmniTrack++用特征稳定化、轨迹反馈实例化、专家记忆和自适应跟管理四模块联合提升短期关联与长期身份保持，在新数据集EmboTrack和JRDB上显著超越原方法。


<details>
  <summary>Details</summary>
Motivation: 传统窄视场相机的MOT方法在360°全景图像上表现欠佳，受分辨率稀释、视角依赖畸变和广域搜索空间影响，需新的方法来应对全景场景的特有挑战。

Method: 提出了DynamicSSM用于稳定化全景特征，FlexiTrack Instances在归一化表示上利用轨迹反馈进行柔性定位与短期关联，ExpertTrack Memory通过Mixture-of-Experts整合外观线索用于长期恢复，以及Tracklet Management在端到端与检测跟踪模式间自适应切换。

Result: 在JRDB和自建EmboTrack（含QuadTrack与BipTrack）上取得SOTA性能，相较原OmniTrack在JRDB上HOTA提高+25.5%，在QuadTrack上提高+43.07%。

Conclusion: OmniTrack++通过反馈驱动的多模块设计，有效提升了全景图像下的多目标跟踪性能，显著减小了因全景畸变和视角变化带来的身份漂移和跟踪碎片化问题。

Abstract: This paper investigates Multi-Object Tracking (MOT) in panoramic imagery,
which introduces unique challenges including a 360{\deg} Field of View (FoV),
resolution dilution, and severe view-dependent distortions. Conventional MOT
methods designed for narrow-FoV pinhole cameras generalize unsatisfactorily
under these conditions. To address panoramic distortion, large search space,
and identity ambiguity under a 360{\deg} FoV, OmniTrack++ adopts a
feedback-driven framework that progressively refines perception with trajectory
cues. A DynamicSSM block first stabilizes panoramic features, implicitly
alleviating geometric distortion. On top of normalized representations,
FlexiTrack Instances use trajectory-informed feedback for flexible localization
and reliable short-term association. To ensure long-term robustness, an
ExpertTrack Memory consolidates appearance cues via a Mixture-of-Experts
design, enabling recovery from fragmented tracks and reducing identity drift.
Finally, a Tracklet Management module adaptively switches between end-to-end
and tracking-by-detection modes according to scene dynamics, offering a
balanced and scalable solution for panoramic MOT. To support rigorous
evaluation, we establish the EmboTrack benchmark, a comprehensive dataset for
panoramic MOT that includes QuadTrack, captured with a quadruped robot, and
BipTrack, collected with a bipedal wheel-legged robot. Together, these datasets
span wide-angle environments and diverse motion patterns, providing a
challenging testbed for real-world panoramic perception. Extensive experiments
on JRDB and EmboTrack demonstrate that OmniTrack++ achieves state-of-the-art
performance, yielding substantial HOTA improvements of +25.5% on JRDB and
+43.07% on QuadTrack over the original OmniTrack. Datasets and code will be
made publicly available at https://github.com/xifen523/OmniTrack.

</details>


### [60] [ID-Composer: Multi-Subject Video Synthesis with Hierarchical Identity Preservation](https://arxiv.org/abs/2511.00511)
*Panwang Pan,Jingjing Zhao,Yuchen Lin,Chenguo Lin,Chenxin Li,Haopeng Li,Honglei Yan,Tingting Shen,Yadong Mu*

Main category: cs.CV

TL;DR: 提出ID-Composer：结合层级身份注意力、VLM语义理解与在线强化学习的多主体视频生成方法，显著改善身份一致性、语义遵循与时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型多被限制于文本或单张图像条件，缺乏对多主体视频生成的可控性与适用性，且要同时保持主体身份、跨主体与跨模态语义整合及时间一致性具有挑战性。

Method: 设计了层级身份保持注意力（在主体内与主体间以及跨模态聚合特征）、引入预训练视觉-语言模型提供细粒度语义引导，以及在标准扩散训练后加入在线强化学习阶段以强化关键概念对齐（如主体ID）。

Result: 大量实验表明，ID-Composer在身份保持、时间一致性和视频质量上优于现有方法。

Conclusion: ID-Composer提出了一个能够从文本提示和多张参考图像生成多主体视频的新框架，通过层级化的身份保持注意力机制、利用视觉-语言模型进行语义理解以及在线强化学习优化训练目标（RLVR）来提升身份保持、语义遵循和时间一致性。

Abstract: Video generative models pretrained on large-scale datasets can produce
high-quality videos, but are often conditioned on text or a single image,
limiting controllability and applicability. We introduce ID-Composer, a novel
framework that addresses this gap by tackling multi-subject video generation
from a text prompt and reference images. This task is challenging as it
requires preserving subject identities, integrating semantics across subjects
and modalities, and maintaining temporal consistency. To faithfully preserve
the subject consistency and textual information in synthesized videos,
ID-Composer designs a \textbf{hierarchical identity-preserving attention
mechanism}, which effectively aggregates features within and across subjects
and modalities. To effectively allow for the semantic following of user
intention, we introduce \textbf{semantic understanding via pretrained
vision-language model (VLM)}, leveraging VLM's superior semantic understanding
to provide fine-grained guidance and capture complex interactions between
multiple subjects. Considering that standard diffusion loss often fails in
aligning the critical concepts like subject ID, we employ an \textbf{online
reinforcement learning phase} to drive the overall training objective of
ID-Composer into RLVR. Extensive experiments demonstrate that our model
surpasses existing methods in identity preservation, temporal consistency, and
video quality.

</details>


### [61] [SegDebias: Test-Time Bias Mitigation for ViT-Based CLIP via Segmentation](https://arxiv.org/abs/2511.00523)
*Fangyu Wu,Yujun Cai*

Main category: cs.CV

TL;DR: 通过分割模型在测试时隔离目标属性并均匀化非目标区域嵌入，本文实现了无需训练与偏见注释的CLIP去偏，提升了群体鲁棒性和注意力对齐性。


<details>
  <summary>Details</summary>
Motivation: CLIP等视觉语言模型在零样本分类中表现优秀但易受与任务无关的视觉特征影响，既往去偏方法常需训练数据或偏见标签，限制了实际应用；提出一种无需这些额外资源的测试时去偏方法以提高泛化与可扩展性。

Method: 对输入图像使用预训练分割模型分离目标区域；对非目标区域调整（如替换或均匀化）使其在CLIP文本提示空间中对所有类别的嵌入相似；保持目标区域不变以保留合法判别信息；在测试时直接应用，无需Fine-tune或额外标签。

Result: 在Waterbirds和CelebA数据集上实验表明，该方法在群体鲁棒性指标和Attention IoU上均优于现有测试时去偏方法，验证了分割引导干预在消除偏见方面的有效性。

Conclusion: 本文提出了一种无需训练和偏见标注的测试时去偏方法，通过分割模型隔离目标属性并对非目标区域进行干预，从而消除混淆视觉信号，提升ViT基CLIP在群体鲁棒性和注意力IoU上的表现。

Abstract: Vision language models such as CLIP have shown remarkable performance in zero
shot classification, but remain susceptible to spurious correlations, where
irrelevant visual features influence predictions. Existing debiasing methods
often require access to training data and explicit group labels to perform
fine-tuning or adjust embeddings, which limits their practicality in real-world
settings. Test-time methods attempt to avoid this constraint, but many still
depend on prior knowledge of dataset specific biases, limiting their
generalizability in open set settings. In this work, we propose a test-time
debiasing method for ViT based CLIP models that requires no additional training
or assumptions of bias annotations. Our approach uses a pretrained segmentation
model to isolate the target visual attribute, then adjusts the non target
regions so that their embeddings are uniformly similar to all class specific
text prompts. This procedure removes unintended bias signals from confounding
visual regions while preserving the target attribute. Experiments on Waterbirds
and CelebA show that our method outperforms existing test-time debiasing
approaches in both group robustness metrics and Attention IoU. These results
demonstrate the effectiveness of segmentation guided interventions for scalable
and annotation free bias mitigation in vision language models.

</details>


### [62] [Text-guided Fine-Grained Video Anomaly Detection](https://arxiv.org/abs/2511.00524)
*Jihao Gu,Kun Li,He Wang,Kaan Akşit*

Main category: cs.CV

TL;DR: 基于LVLM的T-VAD通过AHD和RAE实现细粒度可文本化的视频异常检测，显著提升定位精度和可交互性并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 弥补传统VAD仅输出正常/异常二元标签、缺乏细粒度定位与人机交互能力的不足，使检测结果更可解释与交互式。

Method: 引入异常热图解码器（AHD）进行像素级视觉-文本特征对齐生成异常热图；设计区域感知异常编码器（RAE）将热图转为可学习文本嵌入，指导LVLM识别与定位异常。

Result: 在UBnormal数据集上实现微AUC 94.8%、热图RBDC/TBDC准确率67.8%/76.7%；在ShanghaiTech-based和UBnormal上分别在人类可读文本描述（BLEU-4与Yes/No准确率）上取得高性能。

Conclusion: 本文提出T-VAD框架，通过将大视觉语言模型（LVLM）用于视频异常检测，实现细粒度的异常定位与文本化描述。

Abstract: Video Anomaly Detection (VAD) aims to identify anomalous events within video
segments. In scenarios such as surveillance or industrial process monitoring,
anomaly detection is of critical importance. While existing approaches are
semi-automated, requiring human assessment for anomaly detection, traditional
VADs offer limited output as either normal or anomalous. We propose Text-guided
Fine-Grained Video Anomaly Detection (T-VAD), a framework built upon Large
Vision-Language Model (LVLM). T-VAD introduces an Anomaly Heatmap Decoder (AHD)
that performs pixel-wise visual-textual feature alignment to generate
fine-grained anomaly heatmaps. Furthermore, we design a Region-aware Anomaly
Encoder (RAE) that transforms the heatmaps into learnable textual embeddings,
guiding the LVLM to accurately identify and localize anomalous events in
videos. This significantly enhances both the granularity and interactivity of
anomaly detection. The proposed method achieving SOTA performance by
demonstrating 94.8% Area Under the Curve (AUC, specifically micro-AUC) and
67.8%/76.7% accuracy in anomaly heatmaps (RBDC/TBDC) on the UBnormal dataset,
and subjectively verified more preferable textual description on the
ShanghaiTech-based dataset (BLEU-4: 62.67 for targets, 88.84 for trajectories;
Yes/No accuracy: 97.67%), and on the UBnormal dataset (BLEU-4: 50.32 for
targets, 78.10 for trajectories; Yes/No accuracy: 89.73%).

</details>


### [63] [Real-IAD Variety: Pushing Industrial Anomaly Detection Dataset to a Modern Era](https://arxiv.org/abs/2511.00540)
*Wenbing Zhu,Chengjie Wang,Bin-Bin Gao,Jiangning Zhang,Guannan Jiang,Jie Hu,Zhenye Gan,Lidong Wang,Ziqing Zhou,Linjie Cheng,Yurui Pan,Bo Peng,Mingmin Chi,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出Real-IAD Variety——最大的多样化工业异常检测基准（198,960张图，160类），并证明当前多类无监督方法在大规模类别下性能下降，而视觉-语言模型具有更好的类别扩展鲁棒性，促进通用异常检测基础模型的发展。


<details>
  <summary>Details</summary>
Motivation: 现有IAD公开基准在类别多样性和规模上不足，导致模型在真实工业场景中的迁移性和泛化能力受限。需要更大、更复杂的数据集来推动通用异常检测模型的发展。

Method: 构建包含198,960张高分辨率图像、160个类别、覆盖28个行业、24种材料和22种颜色的多样化数据集；并设计严格的评估协议，包含多类无监督、多视角、零/少样本等设置；对比现有多类无监督方法及视觉-语言模型的性能，验证数据集难度和模型表现。

Result: 通过实验发现：1）当类别从30扩展到160时，现有多类无监督异常检测方法性能显著下降；2）视觉-语言模型对类别扩展表现出显著鲁棒性，性能随类别数变化很小，体现了更强的泛化能力。

Conclusion: 本文提出了Real-IAD Variety，一个大规模、多样化的工业异常检测基准，旨在解决现有数据集类别单一、规模不足导致的指标饱和和模型泛化差问题。

Abstract: Industrial Anomaly Detection (IAD) is critical for enhancing operational
safety, ensuring product quality, and optimizing manufacturing efficiency
across global industries. However, the IAD algorithms are severely constrained
by the limitations of existing public benchmarks. Current datasets exhibit
restricted category diversity and insufficient scale, frequently resulting in
metric saturation and limited model transferability to real-world scenarios. To
address this gap, we introduce Real-IAD Variety, the largest and most diverse
IAD benchmark, comprising 198,960 high-resolution images across 160 distinct
object categories. Its diversity is ensured through comprehensive coverage of
28 industries, 24 material types, and 22 color variations. Our comprehensive
experimental analysis validates the benchmark's substantial challenge:
state-of-the-art multi-class unsupervised anomaly detection methods experience
significant performance degradation when scaled from 30 to 160 categories.
Crucially, we demonstrate that vision-language models exhibit remarkable
robustness to category scale-up, with minimal performance variation across
different category counts, significantly enhancing generalization capabilities
in diverse industrial contexts. The unprecedented scale and complexity of
Real-IAD Variety position it as an essential resource for training and
evaluating next-generation foundation models for anomaly detection. By
providing this comprehensive benchmark with rigorous evaluation protocols
across multi-class unsupervised, multi-view, and zero-/few-shot settings, we
aim to accelerate research beyond domain-specific constraints, enabling the
development of scalable, general-purpose anomaly detection systems. Real-IAD
Variety will be made publicly available to facilitate innovation in this
critical field.

</details>


### [64] [MIFO: Learning and Synthesizing Multi-Instance from One Image](https://arxiv.org/abs/2511.00542)
*Kailun Su,Ziqi He,Xi Wang,Yang Zhou*

Main category: cs.CV

TL;DR: 提出基于惩罚的注意力优化与注意力层盒控的单图多实例语义学习与合成方法，有效解缠相似实例并防止语义泄露，结果显示高质量且可编辑。


<details>
  <summary>Details</summary>
Motivation: 在仅用单张图像进行多实例语义学习时，训练数据极度有限，且当实例语义或外观相近时更难以区分与合成，因而需要新的机制提高解缠能力与布局控制。

Method: 训练阶段：引入基于惩罚的注意力优化（penalty-based attention optimization）以解缠相似语义；合成阶段：在注意力层中加入并优化盒控（box control）以精确控制输出布局并进一步减少语义泄露。

Result: 实验表明该方法在语义解缠与合成质量上表现优异，对相似语义/视觉实例及少见对象依然保持鲁棒，且在可编辑性与实例一致性之间取得良好平衡。

Conclusion: 该论文提出了一种从单张图像精确学习与合成多实例语义的方法，通过优化注意力机制实现语义解缠并加入边界框控制以防止语义泄露，在编辑性与实例一致性之间取得平衡。

Abstract: This paper proposes a method for precise learning and synthesizing
multi-instance semantics from a single image. The difficulty of this problem
lies in the limited training data, and it becomes even more challenging when
the instances to be learned have similar semantics or appearance. To address
this, we propose a penalty-based attention optimization to disentangle similar
semantics during the learning stage. Then, in the synthesis, we introduce and
optimize box control in attention layers to further mitigate semantic leakage
while precisely controlling the output layout. Experimental results demonstrate
that our method achieves disentangled and high-quality semantic learning and
synthesis, strikingly balancing editability and instance consistency. Our
method remains robust when dealing with semantically or visually similar
instances or rare-seen objects. The code is publicly available at
https://github.com/Kareneveve/MIFO

</details>


### [65] [4D Neural Voxel Splatting: Dynamic Scene Rendering with Voxelized Guassian Splatting](https://arxiv.org/abs/2511.00560)
*Chun-Tin Wu,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 用神经体素+学习形变场替代每帧高斯复制，带来更低内存、更快训练和高质量动态场景实时渲染；并通过选择性视图精化提升难视角效果。


<details>
  <summary>Details</summary>
Motivation: 直接对每帧复制高斯会带来巨大的内存开销和计算冗余，难以扩展到高分辨率或长时间动态序列；为了解决内存与效率瓶颈，并保持或提升图像质量，提出用体素+形变场的紧凑表示。

Method: 核心方法是使用紧凑的神经体素网格作为静态场景基底，学习时间相关的形变场将体素在四维（空间+时间）中变形以表示动态内容；渲染采用神经高斯splatting思想对体素进行高效插值与光照合成；额外设计了针对难视角的选择性精化阶段，通过局部优化提高特定视角的渲染精度。

Result: 实验显示在内存占用和训练速度上相比3D Gaussian Splatting等方法有显著优势，同时在像素质量（PSNR/LPIPS等）和视觉保真度上相当或更好，并能实现实时渲染。

Conclusion: 该文提出了4D-NVS，通过用神经体素+形变场替代每帧高斯集合，显著减少内存并加速训练，同时保持高质量渲染；引入视图精化阶段提升难视角表现，最终实现实时渲染并在内存和训练速度上优于SOTA。

Abstract: Although 3D Gaussian Splatting (3D-GS) achieves efficient rendering for novel
view synthesis, extending it to dynamic scenes still results in substantial
memory overhead from replicating Gaussians across frames. To address this
challenge, we propose 4D Neural Voxel Splatting (4D-NVS), which combines
voxel-based representations with neural Gaussian splatting for efficient
dynamic scene modeling. Instead of generating separate Gaussian sets per
timestamp, our method employs a compact set of neural voxels with learned
deformation fields to model temporal dynamics. The design greatly reduces
memory consumption and accelerates training while preserving high image
quality. We further introduce a novel view refinement stage that selectively
improves challenging viewpoints through targeted optimization, maintaining
global efficiency while enhancing rendering quality for difficult viewing
angles. Experiments demonstrate that our method outperforms state-of-the-art
approaches with significant memory reduction and faster training, enabling
real-time rendering with superior visual fidelity.

</details>


### [66] [Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective](https://arxiv.org/abs/2511.00573)
*Wei Feng,Zongyuan Ge*

Main category: cs.CV

TL;DR: 提出FREE：一个基于频域的DS_GCD框架，通过频域域分离、跨/域内频域扰动、目标损失扩展和困难度重采样，应对未知域与未知类共存的挑战，实验表明性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在分布偏移（尤其存在未知域样本时）下性能下降，现实场景中未标注数据往往既包含未知类别也包含未知域，需设计能抵抗域偏移的GCD方法。

Method: 方法包括：1）频域域分离：通过幅度差异将样本划分为已知域与未知域；2）频域扰动：跨域策略（交换幅度成分）与域内策略（增强域内变异鲁棒性）；3）损失扩展：将自监督对比损失和语义聚类损失结合用于训练；4）困难度感知重采样：对难聚类的类别进行自适应采样。

Result: 在多个基准数据集上，FREE显著减轻了分布偏移的影响，在发现已知和未知类别上均取得优于对比方法的性能提升。

Conclusion: 本文提出了针对域偏移下的广义类别发现任务（DS_GCD）的频域引导框架FREE，通过利用频域信息进行域分离、频域扰动和自监督/语义聚类损失扩展，提升在未知类别和未知域混合情况下的发现性能。

Abstract: Generalized Category Discovery (GCD) aims to leverage labeled samples from
known categories to cluster unlabeled data that may include both known and
unknown categories. While existing methods have achieved impressive results
under standard conditions, their performance often deteriorates in the presence
of distribution shifts. In this paper, we explore a more realistic task:
Domain-Shifted Generalized Category Discovery (DS\_GCD), where the unlabeled
data includes not only unknown categories but also samples from unknown
domains. To tackle this challenge, we propose a
\textbf{\underline{F}}requency-guided Gene\textbf{\underline{r}}alized
Cat\textbf{\underline{e}}gory Discov\textbf{\underline{e}}ry framework (FREE)
that enhances the model's ability to discover categories under distributional
shift by leveraging frequency-domain information. Specifically, we first
propose a frequency-based domain separation strategy that partitions samples
into known and unknown domains by measuring their amplitude differences. We
then propose two types of frequency-domain perturbation strategies: a
cross-domain strategy, which adapts to new distributions by exchanging
amplitude components across domains, and an intra-domain strategy, which
enhances robustness to intra-domain variations within the unknown domain.
Furthermore, we extend the self-supervised contrastive objective and semantic
clustering loss to better guide the training process. Finally, we introduce a
clustering-difficulty-aware resampling technique to adaptively focus on
harder-to-cluster categories, further enhancing model performance. Extensive
experiments demonstrate that our method effectively mitigates the impact of
distributional shifts across various benchmark datasets and achieves superior
performance in discovering both known and unknown categories.

</details>


### [67] [TRACES: Temporal Recall with Contextual Embeddings for Real-Time Video Anomaly Detection](https://arxiv.org/abs/2511.00580)
*Yousuf Ahmed Siddiqui,Sufiyaan Usmani,Umer Tariq,Jawwad Ahmed Shamsi,Muhammad Burhan Khan*

Main category: cs.CV

TL;DR: 提出记忆增强的上下文敏感零样本视频异常检测方法，利用交叉注意力融合时序与视觉特征并与文本记忆比对实现实时零样本分类，在UCF-Crime与XD-Violence上报出SOTA性能，强调可实时部署与可解释性。


<details>
  <summary>Details</summary>
Motivation: 大多数异常检测器忽视上下文依赖性：某些动作在一种上下文中是正常的，在另一种上下文中可能是异常，且异常还依赖于时间演变。为提高零样本模型在实际监控场景中的泛化能力，需要将时序信息、视觉特征与语义记忆（文本）关联起来，以实现对新事件的自适应检测。

Method: 构建记忆增强管线：1) 使用视觉编码器提取帧或短时片段的视觉嵌入；2) 使用时序信号（如动作特征或时间编码）并通过交叉注意力模块将其与视觉嵌入融合以捕捉时空关联；3) 在外部文本记忆库中维护上下文文本/语义痕迹（如事件描述、场景说明）；4) 通过视觉—文本相似度（上下文相似度评分）进行零样本异常分类；5) 设计实时推理流程和可解释性机制（例如注意力热图或回溯到文本记忆的相关条目）。

Result: 文中报告的主要量化结果为：UCF-Crime上90.4% AUC，XD-Violence上83.67% AP，声称在零样本设置下达到新的最优；此外宣称方法具备实时推理能力、高精度与可解释性。

Conclusion: 该文提出了一种面向上下文的零样本视频异常检测方法，通过记忆增强模块将时序信息与视觉嵌入进行关联，使用交叉注意力实现时序融合，并通过上下文相似度评分进行实时零样本异常分类。作者宣称在UCF-Crime和XD-Violence数据集上分别获得90.4% AUC和83.67% AP，达到零样本模型的新SOTA，并具备实时推理、高精度与可解释性，适用于监控等实际场景。

Abstract: Video anomalies often depend on contextual information available and temporal
evolution. Non-anomalous action in one context can be anomalous in some other
context. Most anomaly detectors, however, do not notice this type of context,
which seriously limits their capability to generalize to new, real-life
situations. Our work addresses the context-aware zero-shot anomaly detection
challenge, in which systems need to learn adaptively to detect new events by
correlating temporal and appearance features with textual traces of memory in
real time. Our approach defines a memory-augmented pipeline, correlating
temporal signals with visual embeddings using cross-attention, and real-time
zero-shot anomaly classification by contextual similarity scoring. We achieve
90.4\% AUC on UCF-Crime and 83.67\% AP on XD-Violence, a new state-of-the-art
among zero-shot models. Our model achieves real-time inference with high
precision and explainability for deployment. We show that, by fusing
cross-attention temporal fusion and contextual memory, we achieve high fidelity
anomaly detection, a step towards the applicability of zero-shot models in
real-world surveillance and infrastructure monitoring.

</details>


### [68] [CueBench: Advancing Unified Understanding of Context-Aware Video Anomalies in Real-World](https://arxiv.org/abs/2511.00613)
*Yating Yu,Congqi Cao,Zhaoying Wang,Weihua Meng,Jie Li,Yuxin Li,Zihao Wei,Zhongpei Shen,Jiajun Zhang*

Main category: cs.CV

TL;DR: 提出CueBench基准与事件分层体系，统一评测上下文依赖的视频异常任务，并通过Cue-R1（一种R1风格的强化微调方法）显著提升模型在真实异常理解上的性能，平均提升约24%。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测/描述方法对“真实世界”的异常理解过于浅显，缺乏对复杂语义与上下文（如带安全装备攀爬与不带的区别）的细粒度判别能力，需构建更严谨的评测与改进方法。

Method: 提出事件中心的分层分类体系（14类条件异常、18类绝对异常），收集174场景、198属性；统一任务包括识别、时序定位、检测与预测；设计Cue-R1，基于R1风格强化微调，使用可验证、任务对齐和层级细化的奖励，以生成式方式训练模型。

Result: 在CueBench上，现有VLMs表现差距大，Cue-R1在各项任务上平均领先超过24%，显示强化微调与奖赏设计可显著提升情境感知异常理解能力。

Conclusion: 本文构建了CueBench，一个面向情境感知视频异常理解（VAU）的统一评测基准，表明现有视觉语言模型在真实异常理解上仍远不足，并提出Cue-R1提升性能。

Abstract: How far are deep models from real-world video anomaly understanding (VAU)?
Current works typically emphasize on detecting unexpected occurrences deviated
from normal patterns or comprehending anomalous events with interpretable
descriptions. However, they exhibit only a superficial comprehension of
real-world anomalies, with limited breadth in complex principles and subtle
context that distinguish the anomalies from normalities, e.g., climbing cliffs
with safety gear vs. without it. To this end, we introduce CueBench, the first
of its kind Benchmark, devoted to Context-aware video anomalies within a
Unified Evaluation framework. We comprehensively establish an event-centric
hierarchical taxonomy that anchors two core event types: 14 conditional and 18
absolute anomaly events, defined by their refined semantics from diverse
contexts across 174 scenes and 198 attributes. Based on this, we propose to
unify and benchmark context-aware VAU with various challenging tasks across
recognition, temporal grounding, detection, and anticipation. This also serves
as a rigorous and fair probing evaluation suite for generative-discriminative
as well as generalized-specialized vision-language models (VLMs). To address
the challenges underlying CueBench, we further develop Cue-R1 based on R1-style
reinforcement fine-tuning with verifiable, task-aligned, and hierarchy-refined
rewards in a unified generative manner. Extensive results on CueBench reveal
that, existing VLMs are still far from satisfactory real-world anomaly
understanding, while our Cue-R1 surpasses these state-of-the-art approaches by
over 24% on average.

</details>


### [69] [Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach](https://arxiv.org/abs/2511.00643)
*Oluwatosin Alabi,Meng Wei,Charlie Budd,Tom Vercauteren,Miaojing Shi*

Main category: cs.CV

TL;DR: 本文提出并定义了triplet segmentation任务，发布CholecTriplet-Seg数据集，并设计TargetFusionNet将器械实例分割与目标预测融合，有效实现空间级的手术动作三元组理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅限于帧级分类或用类激活映射进行空间定位，无法可靠地把动作与具体器械实例关联，且CAM定位精度不足，难以支持精细的器械-组织交互分析。

Method: 构建了大规模标注数据集CholecTriplet-Seg（3万+帧），将器械实例掩码与动作动词和解剖目标配对；提出TargetFusionNet，基于Mask2Former并加入目标感知融合模块，将弱解剖先验与器械实例查询融合以提升目标预测精度。

Result: 在识别、检测和三元组分割指标上，TargetFusionNet相比现有基线均有持续提升，表明强实例监督结合弱目标先验可显著提升手术动作理解的准确性与鲁棒性。

Conclusion: 该论文提出了将手术动作三元组与器械实例分割结合的“triplet segmentation”新任务，强调了空间定位在手术场景理解中的重要性，并通过数据集和模型证明了优势。

Abstract: Understanding surgical instrument-tissue interactions requires not only
identifying which instrument performs which action on which anatomical target,
but also grounding these interactions spatially within the surgical scene.
Existing surgical action triplet recognition methods are limited to learning
from frame-level classification, failing to reliably link actions to specific
instrument instances.Previous attempts at spatial grounding have primarily
relied on class activation maps, which lack the precision and robustness
required for detailed instrument-tissue interaction analysis.To address this
gap, we propose grounding surgical action triplets with instrument instance
segmentation, or triplet segmentation for short, a new unified task which
produces spatially grounded <instrument, verb, target> outputs.We start by
presenting CholecTriplet-Seg, a large-scale dataset containing over 30,000
annotated frames, linking instrument instance masks with action verb and
anatomical target annotations, and establishing the first benchmark for
strongly supervised, instance-level triplet grounding and evaluation.To learn
triplet segmentation, we propose TargetFusionNet, a novel architecture that
extends Mask2Former with a target-aware fusion mechanism to address the
challenge of accurate anatomical target prediction by fusing weak anatomy
priors with instrument instance queries.Evaluated across recognition,
detection, and triplet segmentation metrics, TargetFusionNet consistently
improves performance over existing baselines, demonstrating that strong
instance supervision combined with weak target priors significantly enhances
the accuracy and robustness of surgical action understanding.Triplet
segmentation establishes a unified framework for spatially grounding surgical
action triplets. The proposed benchmark and architecture pave the way for more
interpretable, surgical scene understanding.

</details>


### [70] [Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset](https://arxiv.org/abs/2511.00653)
*Lassi Ruoppa,Tarmo Hietala,Verneri Seppänen,Josef Taher,Teemu Hakala,Xiaowei Yu,Antero Kukko,Harri Kaartinen,Juha Hyyppä*

Main category: cs.CV

TL;DR: FGI-EMIT是首个大规模MS ALS个体树分割基准，评测显示DL方法（ForestFormer3D）明显优于无监督算法，MS反射率作为额外特征对当前DL模型改进有限，但单通道有助于林下小树检测。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏大规模多光谱LiDAR基准数据集，限制了基于MS信息改进个体树分割（ITS）方法的发展与比较；研究旨在填补这一空白并评估MS数据与现代深度学习方法在ITS中的作用。

Method: 构建包含三波段（532、905、1550 nm）反射率的MS点云数据集，手工标注1561棵树（强调小型林下树）；用贝叶斯优化调整无监督方法超参数；从头训练四种深度学习模型；进行消融研究以检验多光谱信息的作用；评估不同点密度下的性能。

Result: 无监督方法中Treeiso在测试集上F1最高为52.7%；最佳深度学习模型ForestFormer3D达73.3% F1，且在林下小树上比Treeiso高25.9个百分点；消融实验表明直接将多光谱反射率作为额外特征对于现有DL方法帮助有限，但单通道反射率能在一定程度上提升小树检测；在最低10点/m^2点密度下DL方法仍优于无监督方法。

Conclusion: 本文提出了首个大规模多光谱机载激光扫描（MS ALS）个体树分割基准数据集FGI-EMIT，并系统评估了传统无监督方法与四种监督深度学习方法的表现，结果显示深度学习方法整体优于无监督方法，尤其在林下小树检测上差距显著。此外，深度学习方法对多光谱反射率的利用有限，点云密度降低时仍保持优势。

Abstract: Individual tree segmentation (ITS) from LiDAR point clouds is fundamental for
applications such as forest inventory, carbon monitoring and biodiversity
assessment. Traditionally, ITS has been achieved with unsupervised
geometry-based algorithms, while more recent advances have shifted toward
supervised deep learning (DL). In the past, progress in method development was
hindered by the lack of large-scale benchmark datasets, and the availability of
novel data formats, particularly multispectral (MS) LiDAR, remains limited to
this day, despite evidence that MS reflectance can improve the accuracy of ITS.
This study introduces FGI-EMIT, the first large-scale MS airborne laser
scanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550
nm, the dataset consists of 1,561 manually annotated trees, with a particular
focus on small understory trees. Using FGI-EMIT, we comprehensively benchmarked
four conventional unsupervised algorithms and four supervised DL approaches.
Hyperparameters of unsupervised methods were optimized using a Bayesian
approach, while DL models were trained from scratch. Among the unsupervised
methods, Treeiso achieved the highest test set F1-score of 52.7%. The DL
approaches performed significantly better overall, with the best model,
ForestFormer3D, attaining an F1-score of 73.3%. The most significant difference
was observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9
percentage points. An ablation study demonstrated that current DL-based
approaches generally fail to leverage MS reflectance information when it is
provided as additional input features, although single channel reflectance can
improve accuracy marginally, especially for understory trees. A performance
analysis across point densities further showed that DL methods consistently
remain superior to unsupervised algorithms, even at densities as low as 10
points/m$^2$.

</details>


### [71] [Metadata-Aligned 3D MRI Representations for Contrast Understanding and Quality Control](https://arxiv.org/abs/2511.00681)
*Mehmet Yigit Avci,Pedro Borges,Virginia Fernandez,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP用DICOM采集参数作为监督信号进行图像-元数据对齐，得到可区分MRI序列的向量表示，支持少样本分类与无监督质量控制，有助于跨机构的标签高效MRI分析。


<details>
  <summary>Details</summary>
Motivation: MRI在不同扫描仪、协议和机构间存在显著数据异质性且缺乏标准化的对比标签，限制了大规模自动化分析，故需一种无需人工标注即可统一表示MRI对比的方法。

Method: 基于CLIP思想，使用图像-元数据对比学习框架，将体积影像编码与采集参数（DICOM）文本/向量编码对齐，从而获得对比表示。

Result: 学得的嵌入在序列聚类上表现出明显簇结构，在少样本序列分类任务上优于有监督3D基线，并可通过图像-元数据嵌入距离进行无监督数据质量控制，识别损坏或不一致的元数据。

Conclusion: 该工作提出了MR-CLIP，通过将MRI体积图像与DICOM采集参数对齐学习对比表示，解决了MRI对比不一致的问题。

Abstract: Magnetic Resonance Imaging suffers from substantial data heterogeneity and
the absence of standardized contrast labels across scanners, protocols, and
institutions, which severely limits large-scale automated analysis. A unified
representation of MRI contrast would enable a wide range of downstream
utilities, from automatic sequence recognition to harmonization and quality
control, without relying on manual annotations. To this end, we introduce
MR-CLIP, a metadata-guided framework that learns MRI contrast representations
by aligning volumetric images with their DICOM acquisition parameters. The
resulting embeddings shows distinct clusters of MRI sequences and outperform
supervised 3D baselines under data scarcity in few-shot sequence
classification. Moreover, MR-CLIP enables unsupervised data quality control by
identifying corrupted or inconsistent metadata through image-metadata embedding
distances. By transforming routinely available acquisition metadata into a
supervisory signal, MR-CLIP provides a scalable foundation for label-efficient
MRI analysis across diverse clinical datasets.

</details>


### [72] [Outlier-Aware Post-Training Quantization for Image Super-Resolution](https://arxiv.org/abs/2511.00682)
*Hailing Wang,jianglin Lu,Yitian Zhang,Yun Fu*

Main category: cs.CV

TL;DR: 通过将激活分区并对高敏感层进行优先微调，该方法在PTQ下显著提升SR性能，接近QAT且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有SR网络的PTQ方法忽视激活中的异常值（与图像颜色相关），直接去除异常值会损害性能，需要更合理的量化策略。

Method: 将激活分为异常区域和稠密区域，对每个区域独立进行均匀量化，同时引入敏感性感知微调以针对对量化更敏感的层进行重点微调。

Result: 在多种SR网络和数据集上优于已有PTQ方法，并在大多数场景下达到与QAT相当的性能，且实现至少75倍推理加速。

Conclusion: 提出了双区域量化策略和敏感性感知微调，有效处理激活中的异常值导致的量化问题，从而提升SR网络的PTQ性能，达到接近QAT且有大幅加速。

Abstract: Quantization techniques, including quantization-aware training (QAT) and
post-training quantization (PTQ), have become essential for inference
acceleration of image super-resolution (SR) networks. Compared to QAT, PTQ has
garnered significant attention as it eliminates the need for ground truth and
model retraining. However, existing PTQ methods for SR often fail to achieve
satisfactory performance as they overlook the impact of outliers in activation.
Our empirical analysis reveals that these prevalent activation outliers are
strongly correlated with image color information, and directly removing them
leads to significant performance degradation. Motivated by this, we propose a
dual-region quantization strategy that partitions activations into an outlier
region and a dense region, applying uniform quantization to each region
independently to better balance bit-width allocation. Furthermore, we observe
that different network layers exhibit varying sensitivities to quantization,
leading to different levels of performance degradation. To address this, we
introduce sensitivity-aware finetuning that encourages the model to focus more
on highly sensitive layers, further enhancing quantization performance.
Extensive experiments demonstrate that our method outperforms existing PTQ
approaches across various SR networks and datasets, while achieving performance
comparable to QAT methods in most scenarios with at least a 75 speedup.

</details>


### [73] [Evolve to Inspire: Novelty Search for Diverse Image Generation](https://arxiv.org/abs/2511.00686)
*Alex Inch,Passawis Chaiyapattanaporn,Yuchen Zhu,Yuan Lu,Ting-Wen Ko,Davide Paglieri*

Main category: cs.CV

TL;DR: WANDER通过在自然语言提示空间中结合LLM驱动的语义变异、CLIP新颖性评价与emitters引导，成功生成比现有方法更具多样性的图像集合。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成质量上表现良好，但输出多样性不足，阻碍了探索与创意任务；传统提示优化多关注美学或不适用于视觉创意领域，需一种专门面向多样性的优化策略。

Method: 方法在自然语言提示空间上直接工作，使用LLM（如GPT-4o-mini）进行语义变异，通过CLIP嵌入评估图像新颖性，并引入"emitters"引导搜索到提示空间的不同区域以提升多样性。

Result: 在FLUX-DEV为生成器、GPT-4o-mini为变异器的实验中，WANDER在多样性指标上显著优于现有进化提示优化基线；消融研究表明emitters有效提升多样性。

Conclusion: 本文提出WANDER，一种基于新颖性搜索的提示优化方法，旨在从单一文本提示生成多样化图像集。

Abstract: Text-to-image diffusion models, while proficient at generating high-fidelity
im- ages, often suffer from limited output diversity, hindering their
application in exploratory and ideation tasks. Existing prompt optimization
techniques typically target aesthetic fitness or are ill-suited to the creative
visual domain. To address this shortcoming, we introduce WANDER, a novelty
search-based approach to generating diverse sets of images from a single input
prompt. WANDER operates directly on natural language prompts, employing a Large
Language Model (LLM) for semantic evolution of diverse sets of images, and
using CLIP embeddings to quantify novelty. We additionally apply emitters to
guide the search into distinct regions of the prompt space, and demonstrate
that they boost the diversity of the generated images. Empirical evaluations
using FLUX-DEV for generation and GPT-4o-mini for mutation demonstrate that
WANDER significantly outperforms existing evolutionary prompt optimization
baselines in diversity metrics. Ablation studies confirm the efficacy of
emitters.

</details>


### [74] [Toward Better Optimization of Low-Dose CT Enhancement: A Critical Analysis of Loss Functions and Image Quality Assessment Metrics](https://arxiv.org/abs/2511.00698)
*Taifour Yousra,Beghdadi Azeddine,Marie Luong,Zuheng Ming*

Main category: cs.CV

TL;DR: 分析表明LDCT增强中的常用损失函数与图像质量指标存在不一致性，建议在设计损失时结合感知/临床相关指标以提高实际诊断效果。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习方法在LDCT去噪与增强方面在PSNR/SSIM上取得优异结果，但这些指标不能充分反映医学图像的感知/诊断质量，导致优化目标与临床可用性可能不一致。作者旨在评估不同损失函数与质量指标的一致性，以指导更合理的损失函数设计。

Method: 对比分析法：梳理并分类现有损失函数（MSE、对抗性损失、自定义结构化损失等），并将它们在多个图像质量指标（PSNR、SSIM及感知相关指标）上的表现进行客观比较和一致性检验。

Result: 实验证明多种常用损失函数在PSNR/SSIM上的提升并不必然对应感知质量或医学判读质量的改善，部分定制损失甚至与感知指标负相关。论文建议在开发新损失时应显式纳入与临床相关的图像质量度量并进行一致性验证。

Conclusion: 本文指出现有用于低剂量CT（LDCT）增强的损失函数与图像质量评估指标（如PSNR、SSIM）之间存在不一致性，强调在设计新损失函数时应考虑与感知及医学诊断相关的质量指标一致性。

Abstract: Low-dose CT (LDCT) imaging is widely used to reduce radiation exposure to
mitigate high exposure side effects, but often suffers from noise and artifacts
that affect diagnostic accuracy. To tackle this issue, deep learning models
have been developed to enhance LDCT images. Various loss functions have been
employed, including classical approaches such as Mean Square Error and
adversarial losses, as well as customized loss functions(LFs) designed for
specific architectures. Although these models achieve remarkable performance in
terms of PSNR and SSIM, these metrics are limited in their ability to reflect
perceptual quality, especially for medical images. In this paper, we focus on
one of the most critical elements of DL-based architectures, namely the loss
function. We conduct an objective analysis of the relevance of different loss
functions for LDCT image quality enhancement and their consistency with image
quality metrics. Our findings reveal inconsistencies between LFs and quality
metrics, and highlight the need of consideration of image quality metrics when
developing a new loss function for image quality enhancement.

</details>


### [75] [Validating Deep Models for Alzheimer's 18F-FDG PET Diagnosis Across Populations: A Study with Latin American Data](https://arxiv.org/abs/2511.00728)
*Hugo Massaroli,Hernan Chaves,Pilar Anania,Mauricio Farez,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CV

TL;DR: ADNI训练的PET影像深度模型在拉美临床队列泛化显著下降，归一化与采样是关键，架构差异小，需重视人群多样性与域适应。


<details>
  <summary>Details</summary>
Motivation: 旨在评估当前基于18F-FDG PET的深度学习AD诊断模型对被低估或未充分代表的人群（如拉丁美洲临床队列）的泛化能力，揭示潜在域偏移并找到提升泛化的关键因素。

Method: 在ADNI数据集上基准测试若干卷积神经网络和Transformer模型，并在FLENI（阿根廷）临床队列上评估泛化性能；通过消融实验分析预处理（每图像归一化）和采样选择对泛化的影响；使用遮挡敏感性分析（occlusion）检查模型注意力分布。

Result: 在ADNI上模型AUC最高可达0.96-0.97，但在FLENI上降至0.80-0.82；不同架构表现相近；每图像归一化和正确的采样选择显著改善泛化；遮挡分析显示模型在AD类关注典型低代谢区，但在其他类与FLENI扫描上关注区域不明确。

Conclusion: 本研究表明在ADNI上训练的深度学习模型在拉丁美洲真实临床队列（FLENI）上泛化能力显著下降，存在明显的域偏移问题，且不同架构（卷积与Transformer）间性能差异不大。

Abstract: Deep learning models have shown strong performance in diagnosing Alzheimer's
disease (AD) using neuroimaging data, particularly 18F-FDG PET scans, with
training datasets largely composed of North American cohorts such as those in
the Alzheimer's Disease Neuroimaging Initiative (ADNI). However, their
generalization to underrepresented populations remains underexplored. In this
study, we benchmark convolutional and Transformer-based models on the ADNI
dataset and assess their generalization performance on a novel Latin American
clinical cohort from the FLENI Institute in Buenos Aires, Argentina. We show
that while all models achieve high AUCs on ADNI (up to .96, .97), their
performance drops substantially on FLENI (down to .82, .80, respectively),
revealing a significant domain shift. The tested architectures demonstrated
similar performance, calling into question the supposed advantages of
transformers for this specific task. Through ablation studies, we identify
per-image normalization and a correct sampling selection as key factors for
generalization. Occlusion sensitivity analysis further reveals that models
trained on ADNI, generally attend to canonical hypometabolic regions for the AD
class, but focus becomes unclear for the other classes and for FLENI scans.
These findings highlight the need for population-aware validation of diagnostic
AI models and motivate future work on domain adaptation and cohort
diversification.

</details>


### [76] [Towards classification-based representation learning for place recognition on LiDAR scans](https://arxiv.org/abs/2511.00738)
*Dmitrii Khizbullin,Maksim Konoplia*

Main category: cs.CV

TL;DR: 把LiDAR地点识别改为多类分类：给扫描打离散位置标签，用编码器-解码器分类，实验证明在NuScenes上性能接近对比学习，但训练更高效、更稳定。


<details>
  <summary>Details</summary>
Motivation: 目前大多数位置信息识别方法依赖对比学习，需要复杂的样本构造和难以稳定的训练过程。将问题视为多类分类可以简化训练目标，提升训练效率和收敛稳定性。

Method: 将场景离散化为若干位置标签，对每个LiDAR扫描分配位置类别标签，设计并训练一个编码器-解码器（encoder-decoder）模型，直接对输入扫描进行位置分类。

Result: 在NuScenes数据集上的实验结果表明：分类方法的检索/识别性能与对比学习方法相当，同时训练时间更短，训练过程更稳定，可能降低超参数调节需求。

Conclusion: 本文将基于LiDAR的位置信息识别问题重新表述为多类分类任务，结论是这种方式在NuScenes数据集上能得到与对比学习方法相当的性能，同时在训练效率和稳定性上具有优势。

Abstract: Place recognition is a crucial task in autonomous driving, allowing vehicles
to determine their position using sensor data. While most existing methods rely
on contrastive learning, we explore an alternative approach by framing place
recognition as a multi-class classification problem. Our method assigns
discrete location labels to LiDAR scans and trains an encoder-decoder model to
classify each scan's position directly. We evaluate this approach on the
NuScenes dataset and show that it achieves competitive performance compared to
contrastive learning-based methods while offering advantages in training
efficiency and stability.

</details>


### [77] [Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models](https://arxiv.org/abs/2511.00749)
*Tanvi Dinkar,Aiqi Jiang,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CV

TL;DR: 研究发现生成式AI在美学表现上存在强烈偏见：偏向年轻、白皙和性化的形象，负面提示反而导致更多NSFW输出，提示模型会擦除并放大非主流美的特征，具有重要社会影响。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI是否以及如何在图像生成中编码“美”与“丑”，并探讨这种编码对社会（如自我形象、文化刻板印象、数据流污染等）的影响。

Method: 构建两条生成管道（文本到图像，文本到语言再到图像），制定结构化美容分类法，用该分类法提示三个语言模型和两个文本到图像模型，累计生成5984张图像，并招募女性和非二元社媒用户用Likert量表评价1200张图像；统计分析肤色、年龄分布、NSFW比例及交叉人口学效果。

Result: 生成图像中86.5%为肤色较浅，22%含NSFW内容，74%被评为更年轻人群；非二元个体图像被评为更年轻且更具性化；带有负面美学提示的图像NSFW评分更高。

Conclusion: 生成式AI在美学刻板印象上表现出明显偏见，倾向于产生更年轻、肤色更浅、并在许多情况下更具性化的形象，尤其对非二元性别有所放大；此外，负面提示（例如“丑”或具体特征如“宽鼻”）会导致更多NSFW输出，说明模型及其训练/调优过程在刻意或无意中擦除非主流“美”的特征。

Abstract: Social media has exacerbated the promotion of Western beauty norms, leading
to negative self-image, particularly in women and girls, and causing harm such
as body dysmorphia. Increasingly content on the internet has been artificially
generated, leading to concerns that these norms are being exaggerated. The aim
of this work is to study how generative AI models may encode 'beauty' and erase
'ugliness', and discuss the implications of this for society. To investigate
these aims, we create two image generation pipelines: a text-to-image model and
a text-to-language model-to image model. We develop a structured beauty
taxonomy which we use to prompt three language models (LMs) and two
text-to-image models to cumulatively generate 5984 images using our two
pipelines. We then recruit women and non-binary social media users to evaluate
1200 of the images through a Likert-scale within-subjects study. Participants
show high agreement in their ratings. Our results show that 86.5% of generated
images depicted people with lighter skin tones, 22% contained explicit content
despite Safe for Work (SFW) training, and 74% were rated as being in a younger
age demographic. In particular, the images of non-binary individuals were rated
as both younger and more hypersexualised, indicating troubling intersectional
effects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such
as "a wide nose") consistently produced higher Not SFW (NSFW) ratings
regardless of gender. This work sheds light on the pervasive demographic biases
related to beauty standards present in generative AI models -- biases that are
actively perpetuated by model developers, such as via negative prompting. We
conclude by discussing the implications of this on society, which include
pollution of the data streams and active erasure of features that do not fall
inside the stereotype of what is considered beautiful by developers.

</details>


### [78] [A Hybrid YOLOv5-SSD IoT-Based Animal Detection System for Durian Plantation Protection](https://arxiv.org/abs/2511.00777)
*Anis Suttan Shahrir,Zakiah Ayop,Syarulnaziah Anawar,Norulzahrah Mohd Zainudin*

Main category: cs.CV

TL;DR: 提出了一套基于IoT的榴莲园动物检测与驱赶系统，融合YOLOv5与SSD提高检测准确率，支持Telegram告警与自动声学威慑；白天效果良好，夜间性能下降。


<details>
  <summary>Details</summary>
Motivation: 榴莲种植受动物入侵造成产量与经济损失，但传统人工巡查成本高且难以24小时监控；因此利用机器学习与物联网实现自动化、实时的入侵检测与响应具有重要现实意义。

Method: 设计并实现了一个包含摄像头、边缘设备和IoT通信的监测系统，使用YOLOv5和SSD两种模型的融合策略进行目标检测；检测到动物后系统通过Telegram Bot发送实时通知给农户，并驱动播放预设的威慑音效。系统在白天与夜间、静态图片与视频流等不同条件下进行了性能评估。

Result: 融合YOLOv5与SSD的模型在白天取得较高识别率：大象90%、野猪85%、猴子70%；整体在夜间识别性能下降；系统成功实现了基于Telegram的实时告警与自动声音威慑功能，展示了实际部署的可行性。

Conclusion: 该研究提出了一种基于物联网的榴莲园动物检测系统，通过融合YOLOv5与SSD两种目标检测算法，实现了检测、通知与威慑机制的综合方案。实验证明融合模型在白天对大象、野猪和猴子的检测准确率分别为90%、85%和70%，夜间以及视频场景下准确率下降。系统通过Telegram实时通知农户，并在检测到入侵时触发自动声音驱赶（如虎啸），从而提高了对动物入侵的响应速度和即时威慑能力。

Abstract: Durian plantation suffers from animal intrusions that cause crop damage and
financial loss. The traditional farming practices prove ineffective due to the
unavailability of monitoring without human intervention. The fast growth of
machine learning and Internet of Things (IoT) technology has led to new ways to
detect animals. However, current systems are limited by dependence on single
object detection algorithms, less accessible notification platforms, and
limited deterrent mechanisms. This research suggests an IoT-enabled animal
detection system for durian crops. The system integrates YOLOv5 and SSD object
detection algorithms to improve detection accuracy. The system provides
real-time monitoring, with detected intrusions automatically reported to
farmers via Telegram notifications for rapid response. An automated sound
mechanism (e.g., tiger roar) is triggered once the animal is detected. The
YOLO+SSD model achieved accuracy rates of elephant, boar, and monkey at 90%,
85% and 70%, respectively. The system shows the highest accuracy in daytime and
decreases at night, regardless of whether the image is still or a video.
Overall, this study contributes a comprehensive and practical framework that
combines detection, notification, and deterrence, paving the way for future
innovations in automated farming solutions.

</details>


### [79] [Class-agnostic 3D Segmentation by Granularity-Consistent Automatic 2D Mask Tracking](https://arxiv.org/abs/2511.00785)
*Juan Wang,Yasutomo Kawanishi,Tomo Miyazaki,Zhijie Wang,Shinichiro Omachi*

Main category: cs.CV

TL;DR: 通过跨帧掩码跟踪保证伪标签粒度一致，配合逐步的三阶段课程学习，能从碎片化2D先验稳健蒸馏出一致的3D实例分割，达成SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有将2D foundation model掩码迁移到3D的方法忽略视频帧间的时序一致性，导致分割粒度不一致和伪标签冲突，影响最终3D分割精度。

Method: 设计了Granularity-Consistent自动2D掩码跟踪以在帧间维护时序对应，消除冲突伪标签；结合三阶段课程学习，从单视角碎片化标签到多视角统一注释，再到全景一致监督，逐步训练模型。

Result: 在标准基准上取得了SOTA性能，并展现了开放词汇能力；实验显示生成的一致且准确的3D分割。

Conclusion: 提出的GC自动2D掩码跟踪和三阶段课程学习能显著提高从2D伪标签到一致性3D实例分割的效果，解决了跨帧不一致和冲突伪标签问题。

Abstract: 3D instance segmentation is an important task for real-world applications. To
avoid costly manual annotations, existing methods have explored generating
pseudo labels by transferring 2D masks from foundation models to 3D. However,
this approach is often suboptimal since the video frames are processed
independently. This causes inconsistent segmentation granularity and
conflicting 3D pseudo labels, which degrades the accuracy of final
segmentation. To address this, we introduce a Granularity-Consistent automatic
2D Mask Tracking approach that maintains temporal correspondences across
frames, eliminating conflicting pseudo labels. Combined with a three-stage
curriculum learning framework, our approach progressively trains from
fragmented single-view data to unified multi-view annotations, ultimately
globally coherent full-scene supervision. This structured learning pipeline
enables the model to progressively expose to pseudo-labels of increasing
consistency. Thus, we can robustly distill a consistent 3D representation from
initially fragmented and contradictory 2D priors. Experimental results
demonstrated that our method effectively generated consistent and accurate 3D
segmentations. Furthermore, the proposed method achieved state-of-the-art
results on standard benchmarks and open-vocabulary ability.

</details>


### [80] [FedOnco-Bench: A Reproducible Benchmark for Privacy-Aware Federated Tumor Segmentation with Synthetic CT Data](https://arxiv.org/abs/2511.00795)
*Viswa Chaitanya Marella,Suhasnadh Reddy Veluru,Sai Teja Erukude*

Main category: cs.CV

TL;DR: FedOnco-Bench提供了一个开放基准，用合成肿瘤CT评测联邦学习在医疗图像分割中的隐私与效用权衡，显示FedAvg高效但隐私差，DP-SGD隐私好但精度下降，FedProx/FedBN在异构数据下更平衡。


<details>
  <summary>Details</summary>
Motivation: 在医疗影像分割中，数据敏感导致难以共享，联邦学习可以解决数据留在源头的问题，但仍面临成员推断攻击与异质性数据挑战，需要标准化基准来评估隐私-效用折衷。

Method: 使用合成的肿瘤CT数据，比较FedAvg、FedProx、FedBN以及在FedAvg上应用DP-SGD的方案，通过分割Dice和成员推断攻击AUC来评估效用和隐私。

Result: 实验证明：FedAvg在精度上最好（Dice≈0.85）但隐私泄露高（攻击AUC≈0.72）；加入DP-SGD显著降低隐私泄露（AUC≈0.25）但精度下降（Dice≈0.79）；FedProx和FedBN在非独立同分布场景下提供更平衡的性能。

Conclusion: 本文构建了FedOnco-Bench，一个用于隐私感知联邦学习的可复现基准，基于合成肿瘤CT与标注，评估分割性能与隐私泄露的权衡。

Abstract: Federated Learning (FL) allows multiple institutions to cooperatively train
machine learning models while retaining sensitive data at the source, which has
great utility in privacy-sensitive environments. However, FL systems remain
vulnerable to membership-inference attacks and data heterogeneity. This paper
presents FedOnco-Bench, a reproducible benchmark for privacy-aware FL using
synthetic oncologic CT scans with tumor annotations. It evaluates segmentation
performance and privacy leakage across FL methods: FedAvg, FedProx, FedBN, and
FedAvg with DP-SGD. Results show a distinct trade-off between privacy and
utility: FedAvg is high performance (Dice around 0.85) with more privacy
leakage (attack AUC about 0.72), while DP-SGD provides a higher level of
privacy (AUC around 0.25) at the cost of accuracy (Dice about 0.79). FedProx
and FedBN offer balanced performance under heterogeneous data, especially with
non-identical distributed client data. FedOnco-Bench serves as a standardized,
open-source platform for benchmarking and developing privacy-preserving FL
methods for medical image segmentation.

</details>


### [81] [Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided Medical Image Editing](https://arxiv.org/abs/2511.00801)
*Zhihui Chen,Mengling Feng*

Main category: cs.CV

TL;DR: Med-Banana-50K 是一个经过 LLM 医学质控和多轮修正的、面向医学图像编辑的50K级双向编辑数据集，附带大量失败样本与对话记录，旨在支持训练评估医学编辑模型。


<details>
  <summary>Details</summary>
Motivation: 现有通用图像编辑数据集缺乏满足严格解剖与临床约束的医学专用大规模数据集，制约了多模态大语言模型在医学图像编辑领域的发展。

Method: 利用 Gemini-2.5-Flash-Image 对真实医学图像生成添加与去除病灶的编辑样本，采用 LLM-as-Judge 按医学评分量表（指令合规性、结构合理性、逼真度、保真性）进行多轮（最多5轮）迭代修正，并保留完整的失败尝试对话日志。

Result: 构建了包含50K张经过医学验证的编辑图像数据集（含37K条失败尝试与对话日志），覆盖胸片、脑MRI、眼底照三种模态和23种疾病，并公开数据与代码以促进社区研究。

Conclusion: 本论文提出了一个名为 Med-Banana-50K 的大规模医学图像编辑数据集，覆盖三种模态和23种疾病，采用双向编辑并结合基于LLM的医学质控流程，旨在推动医学图像编辑模型的训练与评估。

Abstract: Recent advances in multimodal large language models have enabled remarkable
medical image editing capabilities. However, the research community's progress
remains constrained by the absence of large-scale, high-quality, and openly
accessible datasets built specifically for medical image editing with strict
anatomical and clinical constraints. We introduce Med-Banana-50K, a
comprehensive 50K-image dataset for instruction-based medical image editing
spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23
disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image
to generate bidirectional edits (lesion addition and removal) from real medical
images. What distinguishes Med-Banana-50K from general-domain editing datasets
is our systematic approach to medical quality control: we employ LLM-as-Judge
with a medically grounded rubric (instruction compliance, structural
plausibility, realism, and fidelity preservation) and history-aware iterative
refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K
includes 37K failed attempts with full conversation logs for preference
learning and alignment research. By providing this large-scale, medically
validated, and fully documented resource, Med-Banana-50K establishes a
foundation for training and evaluating the next generation of medical image
editing models.Our dataset and code are publicly available at
[https://github.com/richardChenzhihui/med-banana-50k].

</details>


### [82] [GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding](https://arxiv.org/abs/2511.00810)
*Shijie Zhou,Viet Dac Lai,Hao Tan,Jihyung Kil,Wanrong Zhu,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: 通过对齐MLLM内在注意力与补丁级监督信号，GUI-AIMA在低数据下实现高效、坐标自由的GUI grounding，且支持zoom-in以提升精度，在3B模型上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 直接从视觉输入生成精确坐标困难且计算开销大；更直观的做法是先选相关视觉补丁再在补丁内确定点击位置；观察到通用MLLM已具备一定的原生grounding能力，故设计方法以激活并利用这种能力。

Method: 利用MLLM的查询-视觉注意矩阵，进行多头聚合以自适应生成补丁级的grounding信号；通过监督微调使模型注意力与这些补丁信号对齐；采用坐标自由设计并支持插拔式放大（zoom-in）阶段以提升精确度。

Result: 在仅使用85k截图训练下，GUI-AIMA-3B在3B模型中达到SOTA，ScreenSpot-Pro平均准确率58.6%，OSWorld-G为62.2%，显示出出色的数据效率与性能。

Conclusion: GUI-AIMA提出了一种基于注意力且无坐标的监督微调框架，用于高效的GUI grounding，通过对齐多模态大模型（MLLM）内在的注意力与补丁级的定位信号，能在少量训练数据下触发模型的原生定位能力。

Abstract: Graphical user interface (GUI) grounding is a key function of computer-use
agents, which maps natural-language instructions to actionable screen regions.
Existing approaches based on Multimodal Large Language Models (MLLMs) typically
formulate it as a text-based coordinate generation task, yet directly
generating precise coordinates from visual inputs remains challenging and
computationally intensive. An intuitive way to implement GUI grounding is to
first select visual patches relevant to the instructions and then determine the
precise click location within those patches. Based on the observations that
general MLLMs have some native grounding capability, nested within their
attentions, we propose GUI-AIMA, an attention-based and coordinate-free
supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns
the intrinsic multimodal attention of MLLMs with patch-wise grounding signals.
These signals are calculated adaptively for diverse user instructions by
multi-head aggregation on simplified query-visual attention matrices. Besides,
its coordinate-free manner can easily integrate a plug-and-play zoom-in stage.
GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional
data efficiency and verifying that light training can trigger the native
grounding capability of MLLMs. It achieves state-of-the-art performance among
3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%
on OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA

</details>


### [83] [TA-LSDiff:Topology-Aware Diffusion Guided by a Level Set Energy for Pancreas Segmentation](https://arxiv.org/abs/2511.00815)
*Yue Gou,Fanghui Song,Yuming Xing,Shengzhu Shi,Zhichang Guo,Boying Wu*

Main category: cs.CV

TL;DR: TA-LSDiff将拓扑感知扩散模型与水平集能量结合，辅以像素自适应细化，在胰腺分割上实现更好结构保留和边界精度，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 胰腺分割困难在于器官小、与周围组织对比度低及拓扑变异大；传统水平集注重局部梯度驱动忽视拓扑信息，深度网络虽提取语义但易丢失结构细节，因而需要结合拓扑约束与深度特征以兼顾语义与结构。

Method: 方法包括构建拓扑感知扩散概率模型与一个含有四项互补能量项的水平集能量函数，能量项结合输入图像和深度语义特征以指导隐式曲线演化；引入像素自适应细化模块，利用邻域亲和权重局部调制能量以精细化边界；通过消融实验验证各组件贡献。

Result: 在四个公开胰腺数据集上，TA-LSDiff在分割精度上优于现有方法（作者称达到最先进水平），并通过消融分析展示各模块对性能的贡献，且像素自适应细化显著提升边界精细度。

Conclusion: 该论文提出一种结合拓扑感知扩散概率模型与水平集能量的胰腺分割方法TA-LSDiff，通过能量函数间接驱动曲线演化，避免显式几何演化，同时融入深度特征与图像信息，辅以像素自适应细化模块提升边界准确性。实验在四个公开数据集上达到并超越现有方法，证明其实用性与精度。

Abstract: Pancreas segmentation in medical image processing is a persistent challenge
due to its small size, low contrast against adjacent tissues, and significant
topological variations. Traditional level set methods drive boundary evolution
using gradient flows, often ignoring pointwise topological effects. Conversely,
deep learning-based segmentation networks extract rich semantic features but
frequently sacrifice structural details. To bridge this gap, we propose a novel
model named TA-LSDiff, which combined topology-aware diffusion probabilistic
model and level set energy, achieving segmentation without explicit geometric
evolution. This energy function guides implicit curve evolution by integrating
the input image and deep features through four complementary terms. To further
enhance boundary precision, we introduce a pixel-adaptive refinement module
that locally modulates the energy function using affinity weighting from
neighboring evidence. Ablation studies systematically quantify the contribution
of each proposed component. Evaluations on four public pancreas datasets
demonstrate that TA-LSDiff achieves state-of-the-art accuracy, outperforming
existing methods. These results establish TA-LSDiff as a practical and accurate
solution for pancreas segmentation.

</details>


### [84] [OMEGA: Optimized Multimodal Position Encoding Index Derivation with Global Adaptive Scaling for Vision-Language Models](https://arxiv.org/abs/2511.00821)
*Ruoxiang Huang,Xindian Ma,Rundong Kong,Zhen Yuan,Peng Zhang*

Main category: cs.CV

TL;DR: 提出OMEGA：通过模态特定位置编码与自适应步长缩放对齐信息密度，改进VLM位置表征，在多模型多基准上显著提升性能，尤其对视觉密集任务有效。


<details>
  <summary>Details</summary>
Motivation: 现有VLM使用统一的一维或二维位置索引策略，未区分文本与视觉的结构差异，导致位置编码无法充分表征文本的序列特性与视觉的空间特性，影响多模态对齐与下游任务性能。

Method: 提出Modality-Specific Position Encoding（MSPE），对文本和视觉分别在独立坐标维度上分配位置索引，保持文本的序列连续性和视觉的空间连贯性；引入Global Adaptive Encoding Step Scaling（GAESS），基于两种模态的嵌入熵自适应调整视觉位置编码步长，以在位置索引空间对齐信息密度。

Result: 在多种模型架构和VQA基准上均带来一致提升。在视觉密集型任务上，Qwen2.5-VL-3B较基线位置编码策略最高提升3.43%，在更大模型（Qwen2.5-VL-7B、LLaVA-v1.5-7B）上也有稳定增益。

Conclusion: OMEGA通过为不同模态设计独立的位置编码维度并引入自适应步长缩放，解决了现有VLM统一位置索引忽视模态内在结构的问题，从而提升了VLM在多模态任务特别是视觉密集型任务上的表现。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance across
various multimodal tasks, where position encoding plays a vital role in
modeling both the sequential structure of textual information and the spatial
structure of visual information. However, current VLMs commonly adopt
modality-unified 1D or 2D positional indexing strategies, which treat textual
and visual tokens uniformly without accounting for their distinct structural
properties and sequential continuity for text and spatial coherence for vision.
To address this limitation, we propose OMEGA, a novel position encoding
framework that employs Modality-Specific Position Encoding (MSPE) to assign
positional indices while preserving the inherent structures of each modality
across separate coordinate dimensions. Additionally, to align the information
density of multimodal data in the positional index space, OMEGA introduces
Global Adaptive Encoding Step Scaling (GAESS), which adaptively adjusts the
position encoding step size of visual tokens based on the embedding entropy of
both modalities. Experimental results demonstrate that OMEGA consistently
enhances VLM performance across diverse architectures and VQA benchmarks. On
visual-intensive tasks, OMEGA achieves up to 3.43% improvement over baseline
position encoding strategies on Qwen2.5-VL-3B, with consistent gains observed
across larger models including Qwen2.5-VL-7B and LLaVA-v1.5-7B.

</details>


### [85] [Enhancing Adversarial Transferability in Visual-Language Pre-training Models via Local Shuffle and Sample-based Attack](https://arxiv.org/abs/2511.00831)
*Xin Liu,Aoyang Zhou,Aoyang Zhou*

Main category: cs.CV

TL;DR: 提出LSSA：通过局部块打乱+周边采样扩充图文对多样性，生成更具迁移性的多模态对抗样本，实验验证优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态对抗攻击因在一种模态上过度依赖对抗样本导致输入多样性不足并产生过拟合，进而限制了攻击在不同模型之间的迁移性。作者借鉴对抗训练中的多样化策略以提高攻击泛化性。

Method: LSSA先对图像进行局部块随机打乱以扩增图像-文本配对，生成初始对抗图像并在其周围进行采样，随后利用原始及采样得到的图像共同生成对抗文本，强调在图像与文本间的多样化信息交互以提高迁移能力。

Result: 广泛实验表明LSSA在多个VLP模型与数据集上能显著提升多模态对抗样本的迁移成功率，并在大型视觉语言模型上超过其他先进攻击方法。

Conclusion: 本文提出的LSSA通过局部块随机打乱及样本采样扩充了图文对的多样性，从而缓解了跨模态对抗攻击的过拟合问题，显著提升了对抗样本的迁移性，且在多模型与大模型上均优于现有高级攻击方法。

Abstract: Visual-Language Pre-training (VLP) models have achieved significant
performance across various downstream tasks. However, they remain vulnerable to
adversarial examples. While prior efforts focus on improving the adversarial
transferability of multimodal adversarial examples through cross-modal
interactions, these approaches suffer from overfitting issues, due to a lack of
input diversity by relying excessively on information from adversarial examples
in one modality when crafting attacks in another. To address this issue, we
draw inspiration from strategies in some adversarial training methods and
propose a novel attack called Local Shuffle and Sample-based Attack (LSSA).
LSSA randomly shuffles one of the local image blocks, thus expanding the
original image-text pairs, generating adversarial images, and sampling around
them. Then, it utilizes both the original and sampled images to generate the
adversarial texts. Extensive experiments on multiple models and datasets
demonstrate that LSSA significantly enhances the transferability of multimodal
adversarial examples across diverse VLP models and downstream tasks. Moreover,
LSSA outperforms other advanced attacks on Large Vision-Language Models.

</details>


### [86] [Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials](https://arxiv.org/abs/2511.00833)
*Yifan Pu,Jixuan Ying,Qixiu Li,Tianzhu Ye,Dongchen Han,Xiaochen Wang,Ziyi Wang,Xinyu Shao,Gao Huang,Xiu Li*

Main category: cs.CV

TL;DR: 提出Visual-Contrast Attention：用空间池化生成少量对比tokens并分成正负流进行差分交互，降低注意力复杂度并增强判别力，带来分类与生成任务的显著性能提升且代价极小。


<details>
  <summary>Details</summary>
Motivation: 传统MHSA对所有token对做二次交互，浪费计算在不重要或冗余的视觉相关性上；希望通过显式对比和稀疏化注意力来提高效率与判别力。

Method: 在每个注意力头中先用空间池化将稠密查询场蒸馏为少量视觉对比tokens，再将tokens分为可学习的正负流，利用两者的差分交互突出区域间判别性；该模块保持参数少、FLOPs不增、可替换MHSA。

Result: 在DeiT-Tiny上ImageNet-1K top-1从72.2%提升到75.6%（+3.4）；对三种分层ViT提升最多3.1%；在条件ImageNet生成任务上，DiT与SiT的FID-50K分别降低2.1至5.2点；模块仅增加<0.3M参数，无额外FLOPs。

Conclusion: VCA通过引入可学习的正负对比流与空间池化的视觉对比tokens，有效降低了自注意力的计算复杂度并增强判别能力，从而在分类与生成任务上均获得显著提升。

Abstract: Vision Transformers (ViTs) have become a universal backbone for both image
recognition and image generation. Yet their Multi-Head Self-Attention (MHSA)
layer still performs a quadratic query-key interaction for every token pair,
spending the bulk of computation on visually weak or redundant correlations. We
introduce Visual-Contrast Attention (VCA), a drop-in replacement for MHSA that
injects an explicit notion of discrimination while reducing the theoretical
complexity from O(N N C) to O(N n C) with n << N. VCA first distils each head's
dense query field into a handful of spatially pooled visual-contrast tokens,
then splits them into a learnable positive and negative stream whose
differential interaction highlights what truly separates one region from
another. The module adds fewer than 0.3M parameters to a DeiT-Tiny backbone,
requires no extra FLOPs, and is wholly architecture-agnostic. Empirically, VCA
lifts DeiT-Tiny top-1 accuracy on ImageNet-1K from 72.2% to 75.6% (+3.4) and
improves three strong hierarchical ViTs by up to 3.1%, while in
class-conditional ImageNet generation it lowers FID-50K by 2.1 to 5.2 points
across both diffusion (DiT) and flow (SiT) models. Extensive ablations confirm
that (i) spatial pooling supplies low-variance global cues, (ii) dual
positional embeddings are indispensable for contrastive reasoning, and (iii)
combining the two in both stages yields the strongest synergy. VCA therefore
offers a simple path towards faster and sharper Vision Transformers. The source
code is available at https://github.com/LeapLabTHU/LinearDiff.

</details>


### [87] [Parameter Interpolation Adversarial Training for Robust Image Classification](https://arxiv.org/abs/2511.00836)
*Xin Liu,Yichen Yang,Kun He,John E. Hopcroft*

Main category: cs.CV

TL;DR: PIAT通过跨epoch参数插值和平滑logits的NMSE损失，缓解对抗训练振荡与过拟合，显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗训练在训练过程中存在鲁棒性振荡和过拟合，导致防御效果下降，需一种能平滑参数更新并稳定训练过程的方法。

Method: 提出Parameter Interpolation Adversarial Training（PIAT），在每个epoch结束时将当前参数与前一epoch参数按比例线性插值更新；并引入Normalized Mean Square Error（NMSE）损失，用于对齐干净样本与对抗样本logits的相对幅度。

Result: 在多个基准数据集上，PIAT显著提升了CNN与ViT的鲁棒性，显示出比传统对抗训练更好的收敛性与防御效果。

Conclusion: PIAT通过在训练轮次间对模型参数进行线性插值，使决策边界变化更平缓，从而缓解对抗训练中的振荡和过拟合问题，提升了模型的鲁棒性。

Abstract: Though deep neural networks exhibit superior performance on various tasks,
they are still plagued by adversarial examples. Adversarial training has been
demonstrated to be the most effective method to defend against adversarial
attacks. However, existing adversarial training methods show that the model
robustness has apparent oscillations and overfitting issues in the training
process, degrading the defense efficacy. To address these issues, we propose a
novel framework called Parameter Interpolation Adversarial Training (PIAT).
PIAT tunes the model parameters between each epoch by interpolating the
parameters of the previous and current epochs. It makes the decision boundary
of model change more moderate and alleviates the overfitting issue, helping the
model converge better and achieving higher model robustness. In addition, we
suggest using the Normalized Mean Square Error (NMSE) to further improve the
robustness by aligning the relative magnitude of logits between clean and
adversarial examples rather than the absolute magnitude. Extensive experiments
conducted on several benchmark datasets demonstrate that our framework could
prominently improve the robustness of both Convolutional Neural Networks (CNNs)
and Vision Transformers (ViTs).

</details>


### [88] [OmniBrainBench: A Comprehensive Multimodal Benchmark for Brain Imaging Analysis Across Multi-stage Clinical Tasks](https://arxiv.org/abs/2511.00846)
*Zhihao Peng,Cheng Wang,Shengyuan Liu,Zhiying Liang,Yixuan Yuan*

Main category: cs.CV

TL;DR: OmniBrainBench是首个全面的脑影像多模态VQA基准，覆盖多模态与临床任务，评测24个模型后发现现有MLLM在复杂临床推理上仍有显著差距，提示未来需关注视觉-临床推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有脑影像VQA基准要么模态单一，要么只关注粗粒度病理描述，无法全面评估MLLM在实际临床流程中的多模态理解与推理能力，故需构建更全面、临床相关的基准。

Method: 收集自30个经验证的医学来源的15种脑影像模态数据，构建9527对经验证的VQA问答对和31706张图像，模拟临床工作流程并设计15个多阶段临床任务，由专业放射科医师严格验证；随后在24个SOTA模型（开放源代码、医疗定制和专有模型）上进行基准测试与定量评估。

Result: 构建了OmniBrainBench并公开发布；实验发现：专有模型（如GPT-5）优于开源和医疗模型但仍落后医生；医疗模型表现差异大；开源模型总体落后但在特定任务表现突出；所有模型在复杂术前任务上表现显著下降，暴露视觉到临床推理的不足。

Conclusion: 该论文提出了一个名为OmniBrainBench的多模态脑影像VQA基准，覆盖15种影像模态和15个临床任务，旨在全面评估多模态大语言模型在脑影像分析中的能力，结果显示目前模型在复杂临床推理上仍落后于医生。

Abstract: Brain imaging analysis is vital for diagnosing and treating brain disorders,
and multimodal large language models (MLLMs) are increasingly assisting in that
analysis. However, current brain-oriented visual question-answering (VQA)
benchmarks either cover a few imaging modalities or are limited to
coarse-grained pathological descriptions, hindering a comprehensive assessment
of MLLMs throughout the full clinical continuum. To address these, we introduce
OmniBrainBench, the first comprehensive multimodal VQA benchmark specifically
designed to assess the multimodal comprehension capabilities of MLLMs in brain
imaging analysis.OmniBrainBench consists of 15 distinct brain imaging
modalities collected from 30 verified medical sources, yielding 9,527 validated
VQA pairs and 31,706 images. It simulates clinical workflows and encompasses 15
multi-stage clinical tasks rigorously validated by a professional radiologist.
Evaluation of 24 state-of-the-art models, including open-source, medical, and
proprietary MLLMs, highlights the substantial challenges posed by
OmniBrainBench. Our experiments reveal: (1) proprietary MLLMs (e.g., GPT-5)
beat open-source and medical models but lag physicians; (2) medical MLLMs vary
widely in performance; (3) open-source MLLMs trail overall but excel in
specific tasks; (4) MLLMs underperform sharply in complex preoperative tasks,
revealing a visual-to-clinical reasoning gap. OmniBrainBench sets a new
standard for evaluating and advancing MLLMs in brain imaging analysis,
highlighting gaps compared to expert clinical reasoning. We release it at
benchmark \& code.

</details>


### [89] [Occlusion-Aware Diffusion Model for Pedestrian Intention Prediction](https://arxiv.org/abs/2511.00858)
*Yu Liu,Zhijie Liu,Zedong Yang,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出一种遮挡感知的扩散模型（ODM），通过重建被遮挡的运动模式并结合遮挡掩码引导的反向过程，提高在遮挡场景下的行人过马路意图预测鲁棒性，且在PIE和JAAD上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 动机是传统基于深度学习的过马路意图预测方法在遮挡或观测不完整时性能下降，现有方法很少专门处理遮挡带来的信息缺失问题，因此需要一种能重建遮挡运动模式并用其改进意图预测的模型。


Method: 方法包括：1）在扩散模型的去噪阶段引入一种遮挡感知扩散Transformer，用以估算与被遮挡模式相关的噪声特征，增强模型在遮挡语义场景下对上下文关系的捕捉；2）设计一个遮挡掩码引导的反向过程（reverse process），在重构时有效利用可见观测信息，减少预测误差累积，从而提高被遮挡运动特征的重建精度。


Result: 在PIE和JAAD数据集上的多种遮挡场景下进行评估，对比现有方法，实验结果表明ODM在遮挡条件下的预测性能更稳健，重建和意图预测的准确性均有提升。


Conclusion: 该论文提出了一个针对遮挡情形下行人过马路意图预测的生成式方法——遮挡感知扩散模型（ODM），通过重建被遮挡的运动模式并利用其引导未来意图预测，从而提升在不完整观测下的预测鲁棒性。


Abstract: Predicting pedestrian crossing intentions is crucial for the navigation of
mobile robots and intelligent vehicles. Although recent deep learning-based
models have shown significant success in forecasting intentions, few consider
incomplete observation under occlusion scenarios. To tackle this challenge, we
propose an Occlusion-Aware Diffusion Model (ODM) that reconstructs occluded
motion patterns and leverages them to guide future intention prediction. During
the denoising stage, we introduce an occlusion-aware diffusion transformer
architecture to estimate noise features associated with occluded patterns,
thereby enhancing the model's ability to capture contextual relationships in
occluded semantic scenarios. Furthermore, an occlusion mask-guided reverse
process is introduced to effectively utilize observation information, reducing
the accumulation of prediction errors and enhancing the accuracy of
reconstructed motion features. The performance of the proposed method under
various occlusion scenarios is comprehensively evaluated and compared with
existing methods on popular benchmarks, namely PIE and JAAD. Extensive
experimental results demonstrate that the proposed method achieves more robust
performance than existing methods in the literature.

</details>


### [90] [Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion](https://arxiv.org/abs/2511.00859)
*Jaehyun Park,Konyul Park,Daehun Kim,Junseo Park,Jun Won Choi*

Main category: cs.CV

TL;DR: LMD为后验、模型无关的逐层模态分解方法，首次实现将多传感器融合感知模型的预测归因到单独模态，并在相机/雷达/LiDAR融合场景上通过评估指标与可视化验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，多传感器融合能提高感知鲁棒性，但传感器信息在网络内部高度耦合，导致无法判定各模态对单次预测的具体贡献；而单次误判可能致命，因此需要一种能逐层分解模态信息的可解释性方法。

Method: LMD是一种模型无关的后处理方法，通过逐层提取和分离融合网络中模态特定的信息，生成模态级的表示与可视化分解，进而量化各模态对最终预测的贡献。方法面向预训练的融合模型，适用于双模态和三模态（相机-雷达、相机-LiDAR、相机-雷达-LiDAR）设置。

Result: 在多种预训练融合模型和不同传感器组合上验证，LMD通过结构化扰动评估指标和模态可视化分解展示了其有效性，证明可应用于高容量多模态架构以解释模型决策。

Conclusion: 本文提出的LMD能对多传感器融合感知模型进行逐层模态分解，实现对每个输入模态对预测贡献的后验可解释性，从而提高自动驾驶感知系统的透明性与可靠性。

Abstract: In autonomous driving, transparency in the decision-making of perception
models is critical, as even a single misperception can be catastrophic. Yet
with multi-sensor inputs, it is difficult to determine how each modality
contributes to a prediction because sensor information becomes entangled within
the fusion network. We introduce Layer-Wise Modality Decomposition (LMD), a
post-hoc, model-agnostic interpretability method that disentangles
modality-specific information across all layers of a pretrained fusion model.
To our knowledge, LMD is the first approach to attribute the predictions of a
perception model to individual input modalities in a sensor-fusion system for
autonomous driving. We evaluate LMD on pretrained fusion models under
camera-radar, camera-LiDAR, and camera-radar-LiDAR settings for autonomous
driving. Its effectiveness is validated using structured perturbation-based
metrics and modality-wise visual decompositions, demonstrating practical
applicability to interpreting high-capacity multimodal architectures. Code is
available at https://github.com/detxter-jvb/Layer-Wise-Modality-Decomposition.

</details>


### [91] [GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2511.00908)
*Heng Zheng,Yuling Shi,Xiaodong Gu,Haochen You,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.CV

TL;DR: GraphGeo 用异质图+双层争论把多模型冲突转为有益信息，显著提升视觉地理定位。


<details>
  <summary>Details</summary>
Motivation: 单一 LVLM 在不同地理区域和复杂场景下表现欠佳，多智能体方法未能有效处理模型间冲突，因此通过结构化争论提升协作效果。

Method: 提出了带类型边的异质图神经网络，结合节点级精炼与边级论证的双层争论机制，以及跨层拓扑精炼策略以实现图结构与表征的协同演化。

Result: 在多个基准数据集上，GraphGeo 显著优于现有最先进方法，证明了通过结构化争论转换认知冲突可提升地理定位性能。

Conclusion: GraphGeo 将多模型争论结构化为带类型边的异质图，提升了视觉地理定位的准确性。

Abstract: Visual geo-localization requires extensive geographic knowledge and
sophisticated reasoning to determine image locations without GPS metadata.
Traditional retrieval methods are constrained by database coverage and quality.
Recent Large Vision-Language Models (LVLMs) enable direct location reasoning
from image content, yet individual models struggle with diverse geographic
regions and complex scenes. Existing multi-agent systems improve performance
through model collaboration but treat all agent interactions uniformly. They
lack mechanisms to handle conflicting predictions effectively. We propose
\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph
neural networks for visual geo-localization. Our approach models diverse debate
relationships through typed edges, distinguishing supportive collaboration,
competitive argumentation, and knowledge transfer. We introduce a dual-level
debate mechanism combining node-level refinement and edge-level argumentation
modeling. A cross-level topology refinement strategy enables co-evolution
between graph structure and agent representations. Experiments on multiple
benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art
methods. Our framework transforms cognitive conflicts between agents into
enhanced geo-localization accuracy through structured debate.

</details>


### [92] [Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs](https://arxiv.org/abs/2511.00916)
*Yan Shu,Chi Liu,Robin Chen,Derek Li,Bryan Dai*

Main category: cs.CV

TL;DR: 提出Fleming-VL：通过扩展长上下文预训练、补充稀有医学模态微调并扩展评测，结合SFT与GRPO训练，构建支持2D/3D/视频的统一医学多模态大模型，在多项医学视觉任务上达SOTA并公开发布。


<details>
  <summary>Details</summary>
Motivation: 医学多模态数据异构（2D图像、3D体积、时序视频）导致领域差距大且数据格式不一致，阻碍统一医学MLLM的构建，亟需一个能兼顾多模态、扩展性和可评估性的统一框架。

Method: 提出了Fleming-VL统一端到端框架；数据层面采用三大策略：扩大预训练语料（自然+医学长上下文数据）、在微调阶段加入稀有医学模态（超声、皮肤镜、视频）以及扩展评测到3D和视频基准；训练使用监督微调（SFT）和群体相对策略优化（GRPO）；并在多个模型规模上验证性能。

Result: 在多个基准上取得SOTA成绩，包括医学VQA、视频问答和3D医学图像理解任务，验证了方法的有效性；并公开发布模型以支持透明与可重复研究。

Conclusion: Fleming-VL成功构建了一个面向异构医学模态（2D、3D、视频）的统一多模态大模型，通过数据中心策略（扩展预训练长上下文数据、补充稀有医学模态数据、扩展评测任务）并结合SFT与GRPO训练方法，实现了多项医学视觉理解任务的SOTA表现，并公开发布以推动可重复与可审计的研究。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
effectiveness in various general-domain scenarios, such as visual question
answering and image captioning. Recently, researchers have increasingly focused
on empowering MLLMs with medical conversational abilities, which hold
significant promise for clinical applications. However, medical data presents
unique challenges due to its heterogeneous nature -- encompassing diverse
modalities including 2D images, 3D volumetric scans, and temporal video
sequences. The substantial domain gap and data format inconsistencies across
these modalities have hindered the development of unified medical MLLMs. To
address these challenges, we propose Fleming-VL, a unified end-to-end framework
for comprehensive medical visual understanding across heterogeneous modalities.
Fleming-VL tackles this problem from a data-centric perspective through three
key strategies: (1) scaling up pretraining by integrating long-context data
from both natural and medical-specific domains; (2) complementing fine-tuning
with rare medical data, including holistic video analysis and underrepresented
2D modalities such as ultrasound and dermoscopy images; (3) extending existing
evaluation frameworks to incorporate 3D volumetric and video understanding
benchmarks. Through supervised fine-tuning (SFT) and group relative policy
optimization (GRPO), we develop Fleming-VL in multiple model scales. Extensive
experiments demonstrate that Fleming-VL achieves state-of-the-art performance
across multiple benchmarks, including medical VQA, video QA, and 3D medical
image understanding. We publicly release Fleming-VL to promote transparent,
reproducible, and auditable progress in medical AI.

</details>


### [93] [Dynamic Multi-level Weighted Alignment Network for Zero-shot Sketch-based Image Retrieval](https://arxiv.org/abs/2511.00925)
*Hanwen Su,Ge Song,Jiyan Wang,Yuanbo Zhu*

Main category: cs.CV

TL;DR: 提出DMWA-Net：用CLIP+ViT提取特征，基于局部与全局聚合生成样本对齐权重，并将权重融入加权四元组损失，以缓解模态不平衡与低质干扰；在三数据集上表现领先。


<details>
  <summary>Details</summary>
Motivation: 作者指出现有ZS-SBIR方法在训练阶段存在模态样本不平衡与不一致的低质量信息，导致性能次优，提出通过动态多层加权来赋予可靠样本更高权重以提升对齐与检索效果。

Method: 方法包含三部分：1）单模态特征提取模块：使用CLIP文本编码器与ViT提取文本与视觉tokens；2）跨模态多层加权模块：通过局部与全局聚合块生成对齐权重列表，用于衡量素描与图像样本的对齐质量；3）加权四元组损失模块：在三元组/四元组损失中引入样本对齐权重以改善域平衡。

Result: 在Sketchy、TU-Berlin、QuickDraw三个基准上，作者报告的方法优于现有最先进方法。具体提升幅度、评价指标（如P@K、mAP）与统计显著性需在正文核实。

Conclusion: 本文提出的动态多层加权对齐网络（DMWA-Net）针对ZS-SBIR任务，通过多层次加权来缓解跨模态样本不平衡和不一致低质量信息的问题，实验在三个数据集上优于现有方法，结论合理但需更多消融与细节验证。

Abstract: The problem of zero-shot sketch-based image retrieval (ZS-SBIR) has achieved
increasing attention due to its wide applications, e.g. e-commerce. Despite
progress made in this field, previous works suffer from using imbalanced
samples of modalities and inconsistent low-quality information during training,
resulting in sub-optimal performance. Therefore, in this paper, we introduce an
approach called Dynamic Multi-level Weighted Alignment Network for ZS-SBIR. It
consists of three components: (i) a Uni-modal Feature Extraction Module that
includes a CLIP text encoder and a ViT for extracting textual and visual
tokens, (ii) a Cross-modal Multi-level Weighting Module that produces an
alignment weight list by the local and global aggregation blocks to measure the
aligning quality of sketch and image samples, (iii) a Weighted Quadruplet Loss
Module aiming to improve the balance of domains in the triplet loss.
Experiments on three benchmark datasets, i.e., Sketchy, TU-Berlin, and
QuickDraw, show our method delivers superior performances over the
state-of-the-art ZS-SBIR methods.

</details>


### [94] [EVTAR: End-to-End Try on with Additional Unpaired Visual Reference](https://arxiv.org/abs/2511.00956)
*Liuzhuozheng Li,Yue Gong,Shanyuan Liu,Bo Cheng,Yuhang Ma,Liebucha Wu,Dengyang Jiang,Zanyi Wang,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: EVTAR是一种端到端虚拟试穿方法，通过额外参考图像和两阶段训练，实现在无需复杂人体先验输入下更准确地保留服装纹理与细节。


<details>
  <summary>Details</summary>
Motivation: 当前试穿方法依赖复杂的人体先验（例如人体无关图像、姿态、densepose或关键点），在实际应用中不便；通过引入额外参考图像并简化输入，提升可用性与视觉质量。

Method: 提出端到端模型EVTAR，使用两阶段训练策略：训练阶段融合额外参考图像与未配对人物图像以增强纹理和细节保留；推理时仅需源人像与目标服装图像，无需mask、densepose或关键点。

Result: 在两个常用基准数据集和多种任务上进行评估，实验证明EVTAR在细节保留和整体试穿效果方面均优于或匹配现有方法。

Conclusion: EVTAR通过利用额外参考图像并采用两阶段训练，实现了无需复杂人体先验（如densepose或分割图）的端到端虚拟试穿，能更好保留服装细节并在多任务基准上取得优异效果。

Abstract: We propose EVTAR, an End-to-End Virtual Try-on model with Additional
Reference, that directly fits the target garment onto the person image while
incorporating reference images to enhance try-on accuracy. Most existing
virtual try-on approaches rely on complex inputs such as agnostic person
images, human pose, densepose, or body keypoints, making them labor-intensive
and impractical for real-world applications. In contrast, EVTAR adopts a
two-stage training strategy, enabling simple inference with only the source
image and the target garment inputs. Our model generates try-on results without
masks, densepose, or segmentation maps. Moreover, EVTAR leverages additional
reference images of different individuals wearing the same clothes to preserve
garment texture and fine-grained details better. This mechanism is analogous to
how humans consider reference models when choosing outfits, thereby simulating
a more realistic and high-quality dressing effect. We enrich the training data
with supplementary references and unpaired person images to support these
capabilities. We evaluate EVTAR on two widely used benchmarks and diverse
tasks, and the results consistently validate the effectiveness of our approach.

</details>


### [95] [A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis](https://arxiv.org/abs/2511.00962)
*Dongheng Lin,Mengxue Qu,Kunyang Han,Jianbo Jiao,Xiaojie Jin,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了一个基于链式测试时推理的统一零样本视频异常分析框架，可在不需训练或额外数据的情况下，将时间检测、空间定位与文本解释串联起来，实现强可解释性与SOTA零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常研究多停留在帧级检测，缺乏解释性（为何异常、在哪儿、语义原因）。已有定位与理解方法要么依赖标注数据，要么为特定任务设计，缺乏通用性。因此需要一个统一、可解释且无需额外训练的框架来桥接检测、定位与解释三者。

Method: 方法包括两个关键机制：1) intra-task reasoning：在单项任务内部通过迭代或提示工程精炼时间检测结果；2) inter-task chaining：将时间检测结果作为条件依次触发空间定位与语义解释，从而实现端到端的零样本异常检测、定位与解释。整个流程不使用额外数据或梯度更新，仅依赖基础模型和精心设计的任务级提示串联。

Result: 在多个视频异常检测、定位与解释基准上，该方法在纯零样本设置下取得了最先进的性能，显示出良好的泛化与可解释性。结果表明，合理的提示设计和任务级链式推理能够挖掘基础模型的推理能力，以实现实用的、可解释的视频异常分析。

Conclusion: 该论文提出了一个统一的零样本推理框架，通过链式测试时推理（chained test-time reasoning）将时间检测、空间定位和文本解释串联起来，实现无须额外训练的数据无关的异常分析。

Abstract: Most video-anomaly research stops at frame-wise detection, offering little
insight into why an event is abnormal, typically outputting only frame-wise
anomaly scores without spatial or semantic context. Recent video anomaly
localization and video anomaly understanding methods improve explainability but
remain data-dependent and task-specific. We propose a unified reasoning
framework that bridges the gap between temporal detection, spatial
localization, and textual explanation. Our approach is built upon a chained
test-time reasoning process that sequentially connects these tasks, enabling
holistic zero-shot anomaly analysis without any additional training.
Specifically, our approach leverages intra-task reasoning to refine temporal
detections and inter-task chaining for spatial and semantic understanding,
yielding improved interpretability and generalization in a fully zero-shot
manner. Without any additional data or gradients, our method achieves
state-of-the-art zero-shot performance across multiple video anomaly detection,
localization, and explanation benchmarks. The results demonstrate that careful
prompt design with task-wise chaining can unlock the reasoning power of
foundation models, enabling practical, interpretable video anomaly analysis in
a fully zero-shot manner. Project Page:
https://rathgrith.github.io/Unified_Frame_VAA/.

</details>


### [96] [VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel](https://arxiv.org/abs/2511.00981)
*Suzhong Fu,Rui Sun,Xuan Ding,Jingqi Dong,Yiming Yang,Yao Zhu,Min Chang Jordan Ren,Delin Deng,Angelica Aviles-Rivero,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: 针对血管分割改造SAM：卷积适配器+多提示编码器+轻量掩码解码器，自动生成多提示标注，跨模态、多数据集实验证明显著提升Dice/IoU且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有通用分割模型（如SAM）在血管这类细长、分支且对比度低的结构上表现不佳，需要专门化设计以提升局部纹理感知和结构一致性。

Method: 在SAM基础上引入卷积适配器增强局部纹理特征，设计多提示编码器通过分层交叉注意力融合骨架、分叉点和线段中点等解剖提示，并采用轻量级掩码解码器减少锯齿伪影；同时构建自动化管线生成结构化多提示标注。

Result: 在8个数据集、5种成像模态构成的基准上，VesSAM相较于基于PEFT的SAM变体在Dice上提升>10%、IoU提升>13%；在参数远少于完全微调方法的情况下，达到具有竞争力的性能，并在OoD测试中取得最佳平均Dice和IoU。

Conclusion: VesSAM是一种针对2D血管分割的高效框架，显著提升了对薄、分支血管结构的分割性能，且参数量小、泛化性好。

Abstract: Accurate vessel segmentation is critical for clinical applications such as
disease diagnosis and surgical planning, yet remains challenging due to thin,
branching structures and low texture contrast. While foundation models like the
Segment Anything Model (SAM) have shown promise in generic segmentation, they
perform sub-optimally on vascular structures. In this work, we present VesSAM,
a powerful and efficient framework tailored for 2D vessel segmentation. VesSAM
integrates (1) a convolutional adapter to enhance local texture features, (2) a
multi-prompt encoder that fuses anatomical prompts, including skeletons,
bifurcation points, and segment midpoints, via hierarchical cross-attention,
and (3) a lightweight mask decoder to reduce jagged artifacts. We also
introduce an automated pipeline to generate structured multi-prompt
annotations, and curate a diverse benchmark dataset spanning 8 datasets across
5 imaging modalities. Experimental results demonstrate that VesSAM consistently
outperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%
IoU, and achieves competitive performance compared to fully fine-tuned methods,
with significantly fewer parameters. VesSAM also generalizes well to
out-of-distribution (OoD) settings, outperforming all baselines in average OoD
Dice and IoU.

</details>


### [97] [MID: A Self-supervised Multimodal Iterative Denoising Framework](https://arxiv.org/abs/2511.00997)
*Chang Nie,Tianchen Deng,Zhe Liu,Hesheng Wang*

Main category: cs.CV

TL;DR: MID是一种无需配对数据的自监督迭代去噪框架，通过局部线性化非线性噪声并学习估计与减噪网络，能在多领域任务上实现稳健且领先的去噪效果。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据常受复杂非线性噪声污染，传统基于规则的方法难以处理且往往依赖干净-噪声配对样本。为此需要一种无需配对数据、能适应非线性噪声并具备稳健性的通用去噪框架。

Method: MID通过建模噪声在连续过程中的积累，引入两类神经网络：一类估计当前噪声步（噪声状态），另一类预测并减去相应的噪声增量。对于复杂非线性污染，使用一阶泰勒展开在局部线性化噪声过程，从而实现有效的迭代去除。训练过程通过在原始噪声数据上进一步加噪来形成自监督信号。

Result: 在四个经典计算机视觉任务上，MID表现出稳健性、适应性和一致的领先性能；同时在生物医学和生物信息学任务上也表现出较强的性能和适应性。

Conclusion: 该论文提出了一种自监督多模态迭代去噪（MID）框架，通过将噪声视为非线性累积过程并引入迭代加噪与减噪的策略，实现无需干净-噪声配对数据的去噪学习。

Abstract: Data denoising is a persistent challenge across scientific and engineering
domains. Real-world data is frequently corrupted by complex, non-linear noise,
rendering traditional rule-based denoising methods inadequate. To overcome
these obstacles, we propose a novel self-supervised multimodal iterative
denoising (MID) framework. MID models the collected noisy data as a state
within a continuous process of non-linear noise accumulation. By iteratively
introducing further noise, MID learns two neural networks: one to estimate the
current noise step and another to predict and subtract the corresponding noise
increment. For complex non-linear contamination, MID employs a first-order
Taylor expansion to locally linearize the noise process, enabling effective
iterative removal. Crucially, MID does not require paired clean-noisy datasets,
as it learns noise characteristics directly from the noisy inputs. Experiments
across four classic computer vision tasks demonstrate MID's robustness,
adaptability, and consistent state-of-the-art performance. Moreover, MID
exhibits strong performance and adaptability in tasks within the biomedical and
bioinformatics domains.

</details>


### [98] [Integrating Visual and X-Ray Machine Learning Features in the Study of Paintings by Goya](https://arxiv.org/abs/2511.01000)
*Hassan Ugail,Ismail Lujain Jaleel*

Main category: cs.CV

TL;DR: 对戈雅作品，论文通过在可见光与X光影像上统一提取纹理/颜色等特征并用一类SVM识别真伪，24幅真迹实验显示97.8%准确率，案例“Un Gigante”置信度92.3%。


<details>
  <summary>Details</summary>
Motivation: 戈雅作品风格多變且伪作历史悠久，单一模态或不一致的特征处理难以捕捉跨模态的鉴别信息，因此提出对视觉与X射线影像采用相同的特征提取以实现更稳健的鉴定。

Method: 论文在两种影像模态上统一计算纹理（GLCM）、局部二值模式（LBP）、熵、能量和颜色分布等特征；将特征输入经过超参数调优的一类SVM进行训练与检测；使用80/20分割与10折交叉验证评估性能。

Result: 在24幅真迹数据集上报道97.8%准确率与0.022的假阳性率；对“Un Gigante”一例给出92.3%的鉴定置信度，且声称优于单模态方法。

Conclusion: 该论文提出一种在可见光和X射线影像上使用统一特征提取流程的多模态艺术鉴定框架，并采用一类支持向量机进行鉴定，实验在24幅有X光的戈雅真迹上得到高准确率。

Abstract: Art authentication of Francisco Goya's works presents complex computational
challenges due to his heterogeneous stylistic evolution and extensive
historical patterns of forgery. We introduce a novel multimodal machine
learning framework that applies identical feature extraction techniques to both
visual and X-ray radiographic images of Goya paintings. The unified feature
extraction pipeline incorporates Grey-Level Co-occurrence Matrix descriptors,
Local Binary Patterns, entropy measures, energy calculations, and colour
distribution analysis applied consistently across both imaging modalities. The
extracted features from both visual and X-ray images are processed through an
optimised One-Class Support Vector Machine with hyperparameter tuning. Using a
dataset of 24 authenticated Goya paintings with corresponding X-ray images,
split into an 80/20 train-test configuration with 10-fold cross-validation, the
framework achieves 97.8% classification accuracy with a 0.022 false positive
rate. Case study analysis of ``Un Gigante'' demonstrates the practical efficacy
of our pipeline, achieving 92.3% authentication confidence through unified
multimodal feature analysis. Our results indicate substantial performance
improvement over single-modal approaches, establishing the effectiveness of
applying identical computational methods to both visual and radiographic
imagery in art authentication applications.

</details>


### [99] [HyFormer-Net: A Synergistic CNN-Transformer with Interpretable Multi-Scale Fusion for Breast Lesion Segmentation and Classification in Ultrasound Images](https://arxiv.org/abs/2511.01013)
*Mohammad Amanour Rahman*

Main category: cs.CV

TL;DR: HyFormer-Net通过CNN-Transformer融合与可解释设计显著提升乳腺B超的分割与分类性能，并在小样本微调下实现良好跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 传统B超图像存在斑点噪声、操作者依赖与边界模糊；现有深度学习方法受限于单任务学习、网络架构局限（CNN缺全局、Transformer缺局部）及黑盒决策，阻碍临床应用。

Method: 提出HyFormer-Net：双分支编码器（EfficientNet-B3与Swin Transformer）通过多尺度层次融合模块集成，注意力门控解码器用于精细分割与解释；同时采用双通道可解释性策略（注意力图与Grad-CAM）并进行消融与集成实验。

Result: 在BUSI数据集上，单模型Dice 0.761±0.072、准确率93.2%、恶性召回92.1±2.2%；集成后Dice 0.902、准确率99.5%、恶性召回100%；消融显示多尺度融合提高Dice+16.8%、注意力门提高+5.9%；跨域零样本转移Dice仅0.058，但用10%目标域数据微调可恢复至92.5%，50%数据可达到77.3%且超越源域性能。

Conclusion: HyFormer-Net在乳腺B超分割与分类任务上表现优异，提供了可解释性手段并在跨数据集泛化上展示了强适应性与小样本微调效果。

Abstract: B-mode ultrasound for breast cancer diagnosis faces challenges: speckle,
operator dependency, and indistinct boundaries. Existing deep learning suffers
from single-task learning, architectural constraints (CNNs lack global context,
Transformers local features), and black-box decision-making. These gaps hinder
clinical adoption.
  We propose HyFormer-Net, a hybrid CNN-Transformer for simultaneous
segmentation and classification with intrinsic interpretability. Its
dual-branch encoder integrates EfficientNet-B3 and Swin Transformer via
multi-scale hierarchical fusion blocks. An attention-gated decoder provides
precision and explainability. We introduce dual-pipeline interpretability: (1)
intrinsic attention validation with quantitative IoU verification (mean: 0.86),
and (2) Grad-CAM for classification reasoning.
  On the BUSI dataset, HyFormer-Net achieves Dice Score 0.761 +/- 0.072 and
accuracy 93.2%, outperforming U-Net, Attention U-Net, and TransUNet. Malignant
Recall of 92.1 +/- 2.2% ensures minimal false negatives. Ensemble modeling
yields exceptional Dice 90.2%, accuracy 99.5%, and perfect 100% Malignant
Recall, eliminating false negatives. Ablation studies confirm multi-scale
fusion contributes +16.8% Dice and attention gates add +5.9%.
  Crucially, we conduct the first cross-dataset generalization study for hybrid
CNN-Transformers in breast ultrasound. Zero-shot transfer fails (Dice: 0.058),
confirming domain shift. However, progressive fine-tuning with only 10%
target-domain data (68 images) recovers 92.5% performance. With 50% data, our
model achieves 77.3% Dice, exceeding source-domain performance (76.1%) and
demonstrating true generalization.

</details>


### [100] [FastBoost: Progressive Attention with Dynamic Scaling for Efficient Deep Learning](https://arxiv.org/abs/2511.01026)
*JunXi Yuan*

Main category: cs.CV

TL;DR: FastBoost 利用 DSPA 的三项创新在极少参数下实现 CIFAR 数据集的 SOTA，适合资源受限部署。


<details>
  <summary>Details</summary>
Motivation: 动机是为资源受限的边缘设备设计一个在参数和计算上都高效但仍能保持高精度的神经网络，突破现有轻量网络（如 MobileNetV3）在参数-精度权衡上的局限。

Method: 在方法上，作者将 DSPA 集成到增强的 MBConv 块中，构建双注意力路径并引入实时权重调整与级联细化层，使用训练阶段感知的强度调制（Phase Scaling）和自适应残差门控（Residual Adaptation）来控制信息流，提出了学习的通道-空间注意力融合（Adaptive Fusion）。架构注重硬件友好性，报告了低 FLOPs 和参数规模。

Result: 在 CIFAR-10 上达到 95.57%（0.85M 参数）和 93.80%（0.37M 参数）；在 CIFAR-100 上达到 81.37%（0.92M 参数）和 74.85%（0.44M 参数）。与 MobileNetV3 相比在参数减少约 2.1 倍的同时，CIFAR-10 精度提升约 +3.2%。报告还声称级联层增加了 12.7% 的梯度流，并提供 0.28G FLOPs 的硬件友好指标。

Conclusion: FastBoost 提出了一种结合动态注意力和高效卷积的参数高效神经架构，在 CIFAR 数据集上达到了优异性能，其核心贡献是提出了 Dynamically Scaled Progressive Attention (DSPA) 机制，并通过三项关键设计（Adaptive Fusion、Phase Scaling、Residual Adaptation）优化注意力融合、训练阶段缩放与残差连接，从而在参数数目与精度间达成良好折中。

Abstract: We present FastBoost, a parameter-efficient neural architecture that achieves
state-of-the-art performance on CIFAR benchmarks through a novel Dynamically
Scaled Progressive Attention (DSPA) mechanism. Our design establishes new
efficiency frontiers with: CIFAR-10: 95.57% accuracy (0.85M parameters) and
93.80% (0.37M parameters) CIFAR-100: 81.37% accuracy (0.92M parameters) and
74.85% (0.44M parameters) The breakthrough stems from three fundamental
innovations in DSPA: (1) Adaptive Fusion: Learnt channel-spatial attention
blending with dynamic weights. (2) Phase Scaling: Training-stage-aware
intensity modulation (from 0.5 to 1.0). (3) Residual Adaptation: Self-optimized
skip connections (gamma from 0.5 to 0.72). By integrating DSPA with enhanced
MBConv blocks, FastBoost achieves a 2.1 times parameter reduction over
MobileNetV3 while improving accuracy by +3.2 percentage points on CIFAR-10. The
architecture features dual attention pathways with real-time weight adjustment,
cascaded refinement layers (increasing gradient flow by 12.7%), and a
hardware-friendly design (0.28G FLOPs). This co-optimization of dynamic
attention and efficient convolution operations demonstrates unprecedented
parameter-accuracy trade-offs, enabling deployment in resource-constrained edge
devices without accuracy degradation.

</details>


### [101] [T-MLA: A Targeted Multiscale Log--Exponential Attack Framework for Neural Image Compression](https://arxiv.org/abs/2511.01079)
*Nikolay I. Kalmykov,Razan Dibo,Kaiyu Shen,Xu Zhonghan,Anh-Huy Phan,Yipeng Liu,Ivan Oseledets*

Main category: cs.CV

TL;DR: T-MLA：在小波子带中有策略地施加多尺度对数-指数扰动，可在不明显改变视觉感受的情况下显著降低神经图像压缩的重构质量，揭示了压缩系统的安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有针对NIC的对抗攻击多为像素域方法的简单移植，忽视了压缩管线的多尺度、小波结构与量化敏感性，导致效果有限。本工作旨在提出一种更符合压缩特性、能有效破坏重构质量且难以被视觉察觉的攻击。

Method: 提出一种有针对性的多尺度对数-指数（T-MLA）攻击框架，在小波域对特定子带施加扰动，通过直接优化被攻击与重构图像质量来生成离线扰动，兼顾扭曲最大化与感知隐蔽性。

Result: 在多种主流神经图像压缩架构和标准压缩基准上进行广泛评估，结果显示在保持视觉不可察觉的前提下，重构质量大幅下降，证明T-MLA能有效破坏NIC的鲁棒性并暴露关键安全隐患。

Conclusion: 本文提出的T-MLA在波段域施加针对性的扰动，能在不显著增加可见噪声的前提下显著降低神经图像压缩重构质量，揭示了NIC在内容分发和生成流水线中的安全漏洞。

Abstract: Neural image compression (NIC) has become the state-of-the-art for
rate-distortion performance, yet its security vulnerabilities remain
significantly less understood than those of classifiers. Existing adversarial
attacks on NICs are often naive adaptations of pixel-space methods, overlooking
the unique, structured nature of the compression pipeline. In this work, we
propose a more advanced class of vulnerabilities by introducing T-MLA, the
first targeted multiscale log--exponential attack framework. Our approach
crafts adversarial perturbations in the wavelet domain by directly targeting
the quality of the attacked and reconstructed images. This allows for a
principled, offline attack where perturbations are strategically confined to
specific wavelet subbands, maximizing distortion while ensuring perceptual
stealth. Extensive evaluation across multiple state-of-the-art NIC
architectures on standard image compression benchmarks reveals a large drop in
reconstruction quality while the perturbations remain visually imperceptible.
Our findings reveal a critical security flaw at the core of generative and
content delivery pipelines.

</details>


### [102] [GeoToken: Hierarchical Geolocalization of Images via Next Token Prediction](https://arxiv.org/abs/2511.01082)
*Narges Ghasemi,Amir Ziashahabi,Salman Avestimehr,Cyrus Shahabi*

Main category: cs.CV

TL;DR: 把地理定位建模为在S2层次网格上的自回归序列生成，训练+推理时借鉴语言模型的技巧（束搜索/多采样）以处理不确定性，在两个常用数据集上实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 动机源于人类定位时从大范围到具体位置逐步缩小搜索范围，同时现有方法面临视觉相似性和大量搜索空间的挑战，因此借鉴语言建模的自回归序列生成和测试时计算扩展策略以改善地理定位性能。

Method: 方法使用S2多分辨率网格作为地理token，模型基于视觉输入自回归地预测从粗到细的S2 cell序列；在训练中学习条件概率，在推理中采用顶-down遍历策略，包括束搜索、多个样本并结合不同选择策略来确定最终输出。

Result: 在Im2GPS3k和YFCC4k上，模型在不使用MLLM的条件下在几乎所有指标上优于可比基线，最高提升13.9%；与MLLM结合后进一步提升，全面超越所有基线，达到新的SOTA。

Conclusion: 该论文提出通过分层序列预测将全球地理定位任务建模为在S2栅格上的自回归多层级单元预测，从粗到细逐步细化位置信息；在推理阶段借鉴语言模型的搜索策略（如束搜索和多采样）以处理不确定性，最终在Im2GPS3k和YFCC4k数据集上获得了无MLLM和有MLLM两种设置下的最先进性能。

Abstract: Image geolocalization, the task of determining an image's geographic origin,
poses significant challenges, largely due to visual similarities across
disparate locations and the large search space. To address these issues, we
propose a hierarchical sequence prediction approach inspired by how humans
narrow down locations from broad regions to specific addresses. Analogously,
our model predicts geographic tokens hierarchically, first identifying a
general region and then sequentially refining predictions to increasingly
precise locations. Rather than relying on explicit semantic partitions, our
method uses S2 cells, a nested, multiresolution global grid, and sequentially
predicts finer-level cells conditioned on visual inputs and previous
predictions. This procedure mirrors autoregressive text generation in large
language models. Much like in language modeling, final performance depends not
only on training but also on inference-time strategy. We investigate multiple
top-down traversal methods for autoregressive sampling, incorporating
techniques from test-time compute scaling used in language models.
Specifically, we integrate beam search and multi-sample inference while
exploring various selection strategies to determine the final output. This
enables the model to manage uncertainty by exploring multiple plausible paths
through the hierarchy. We evaluate our method on the Im2GPS3k and YFCC4k
datasets against two distinct sets of baselines: those that operate without a
Multimodal Large Language Model (MLLM) and those that leverage one. In the
MLLM-free setting, our model surpasses other comparable baselines on nearly all
metrics, achieving state-of-the-art performance with accuracy gains of up to
13.9%. When augmented with an MLLM, our model outperforms all baselines,
setting a new state-of-the-art across all metrics. The source code is available
at https://github.com/NNargesNN/GeoToken.

</details>


### [103] [SliceVision-F2I: A Synthetic Feature-to-Image Dataset for Visual Pattern Representation on Network Slices](https://arxiv.org/abs/2511.01087)
*Md. Abid Hasan Rafi,Mst. Fatematuj Johora,Pankaj Bhowmik*

Main category: cs.CV

TL;DR: SliceVision-F2I：120k合成KPI→图像样本，4种编码，模拟噪声，面向视觉学习与异常检测。


<details>
  <summary>Details</summary>
Motivation: 随着5G/6G网络与网络切片的兴起，迫切需要可靠的数据集与更精细的特征识别方法来支持服务导向架构和网络状态监测。

Method: 将多变量KPI向量通过四种编码方法（物理启发映射、Perlin噪声、神经壁纸、分形分支）转换为低分辨率RGB图像；每种编码生成30,000个样本；同时在生成过程中引入噪声以模拟真实网络条件。

Result: 生成了包含原始KPI向量与对应RGB图像的SliceVision-F2I数据集，共120,000个样本，数据公开可用，能够支持多种研究任务和基准测试。

Conclusion: 本文提出了SliceVision-F2I数据集，为网络切片领域提供了从多变量KPI向图像的特征可视化研究资源，适用于视觉学习、分类和异常检测等任务。

Abstract: The emergence of 5G and 6G networks has established network slicing as a
significant part of future service-oriented architectures, demanding refined
identification methods supported by robust datasets. The article presents
SliceVision-F2I, a dataset of synthetic samples for studying feature
visualization in network slicing for next-generation networking systems. The
dataset transforms multivariate Key Performance Indicator (KPI) vectors into
visual representations through four distinct encoding methods: physically
inspired mappings, Perlin noise, neural wallpapering, and fractal branching.
For each encoding method, 30,000 samples are generated, each comprising a raw
KPI vector and a corresponding RGB image at low-resolution pixels. The dataset
simulates realistic and noisy network conditions to reflect operational
uncertainties and measurement imperfections. SliceVision-F2I is suitable for
tasks involving visual learning, network state classification, anomaly
detection, and benchmarking of image-based machine learning techniques applied
to network data. The dataset is publicly available and can be reused in various
research contexts, including multivariate time series analysis, synthetic data
generation, and feature-to-image transformations.

</details>


### [104] [Epanechnikov nonparametric kernel density estimation based feature-learning in respiratory disease chest X-ray images](https://arxiv.org/abs/2511.01098)
*Veronica Marsico,Antonio Quintero-Rincon,Hadj Batatia*

Main category: cs.CV

TL;DR: 作者提出EKDE+双模态逻辑回归用于胸片疾病诊断，结果显示准确率中等但对阳性样本检测敏感性不足，需进一步优化与临床验证。


<details>
  <summary>Details</summary>
Motivation: 利用EKDE无需预设分布形状的灵活性，适应像素强度变化，以提高医学影像中病灶特征提取的准确性与稳定性。

Method: 基于Epanechnikov核的非参数核密度估计（EKDE）从像素强度分布中提取特征，再通过二元/双模态逻辑回归进行分类，构成统计模型驱动的学习框架。

Result: 在13808张随机选取的胸片上测试，准确率70.14%，敏感性59.26%，特异性74.18%，表现为中等总体性能但敏感性较低。

Conclusion: 该方法展示了EKDE与双模态逻辑回归结合用于胸片呼吸疾病诊断的可行性，但在敏感性上存在明显不足，仍需改进和更多验证。

Abstract: This study presents a novel method for diagnosing respiratory diseases using
image data. It combines Epanechnikov's non-parametric kernel density estimation
(EKDE) with a bimodal logistic regression classifier in a
statistical-model-based learning scheme. EKDE's flexibility in modeling data
distributions without assuming specific shapes and its adaptability to pixel
intensity variations make it valuable for extracting key features from medical
images. The method was tested on 13808 randomly selected chest X-rays from the
COVID-19 Radiography Dataset, achieved an accuracy of 70.14%, a sensitivity of
59.26%, and a specificity of 74.18%, demonstrating moderate performance in
detecting respiratory disease while showing room for improvement in
sensitivity. While clinical expertise remains essential for further refining
the model, this study highlights the potential of EKDE-based approaches to
enhance diagnostic accuracy and reliability in medical imaging.

</details>


### [105] [Anatomically Constrained Transformers for Echocardiogram Analysis](https://arxiv.org/abs/2511.01109)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Jeremy Slivnick,Dipak Kotecha,Alberto Gomez,Jinming Duan*

Main category: cs.CV

TL;DR: ViACT通过将解剖点集与图像补丁共同作为token并在预训练中仅重建解剖补丁，强制模型关注心肌区域，从而提升了超声视频分析的可解释性与任务泛化。


<details>
  <summary>Details</summary>
Motivation: 视频Transformer在心脏超声图像分析中表现优异，但易从背景等非诊断区域学习虚假相关性，因此需要将解剖学先验直接整合到模型中以引导注意力集中于关键解剖结构。

Method: 将变形的解剖结构表示为点集，并将其几何空间信息与对应图像补丁一并编码为Transformer token；在预训练阶段采用只对解剖补丁进行遮盖并重建的masked autoencoding策略，使表征学习聚焦于解剖区域；微调阶段在该局部区域上进行下游任务学习。

Result: 在左室射血分数回归和心肌淀粉样变性检测等任务上，ViACT使注意力图更可解释并与已知病变区域对齐；同时可推广至心肌点跟踪任务，无需使用专门的相关体积等追踪组件。

Conclusion: ViACT通过在视频Transformer中直接引入解剖学先验，有效抑制了模型对背景等非诊断区域的虚假相关学习，从而提升了超声心动图任务的可解释性和泛化能力。

Abstract: Video transformers have recently demonstrated strong potential for
echocardiogram (echo) analysis, leveraging self-supervised pre-training and
flexible adaptation across diverse tasks. However, like other models operating
on videos, they are prone to learning spurious correlations from non-diagnostic
regions such as image backgrounds. To overcome this limitation, we propose the
Video Anatomically Constrained Transformer (ViACT), a novel framework that
integrates anatomical priors directly into the transformer architecture. ViACT
represents a deforming anatomical structure as a point set and encodes both its
spatial geometry and corresponding image patches into transformer tokens.
During pre-training, ViACT follows a masked autoencoding strategy that masks
and reconstructs only anatomical patches, enforcing that representation
learning is focused on the anatomical region. The pre-trained model can then be
fine-tuned for tasks localized to this region. In this work we focus on the
myocardium, demonstrating the framework on echo analysis tasks such as left
ventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)
detection. The anatomical constraint focuses transformer attention within the
myocardium, yielding interpretable attention maps aligned with regions of known
CA pathology. Moreover, ViACT generalizes to myocardium point tracking without
requiring task-specific components such as correlation volumes used in
specialized tracking networks.

</details>


### [106] [Boosting performance of computer vision applications through embedded GPUs on the edge](https://arxiv.org/abs/2511.01129)
*Fabio Diniz Rossi*

Main category: cs.CV

TL;DR: 在资源受限的边缘计算场景下，将计算机视觉任务卸载到带GPU的嵌入式设备能显著提高性能，优于仅用CPU的方案。


<details>
  <summary>Details</summary>
Motivation: 动机是移动设备上Augmented Reality等计算机视觉应用对资源需求高，而传统边缘计算节点通常资源有限，影响用户体验，因此探索用带GPU的嵌入式设备缓解这一问题。

Method: 本文通过将计算密集型任务从移动设备或轻量级边缘节点卸载到带GPU的嵌入式设备上进行实验比较，并对比了仅使用CPU的性能差异。

Result: 实验显示，GPU相较于仅用CPU在实验场景中带来了性能提升，从而能在资源受限的边缘环境中改善应用响应和用户体验。

Conclusion: 本文得出结论：在边缘计算场景下，采用带GPU的嵌入式设备可以显著提升计算机视觉增强现实应用的性能，从而改善用户体验。

Abstract: Computer vision applications, especially those using augmented reality
technology, are becoming quite popular in mobile devices. However, this type of
application is known as presenting significant demands regarding resources. In
order to enable its utilization in devices with more modest resources, edge
computing can be used to offload certain high intensive tasks. Still, edge
computing is usually composed of devices with limited capacity, which may
impact in users quality of experience when using computer vision applications.
This work proposes the use of embedded devices with graphics processing units
(GPUs) to overcome such limitation. Experiments performed shown that GPUs can
attain a performance gain when compared to using only CPUs, which guarantee a
better experience to users using such kind of application.

</details>


### [107] [Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis](https://arxiv.org/abs/2511.01131)
*Md Nahiduzzaman,Steven Korevaar,Alireza Bab-Hadiashar,Ruwan Tennakoon*

Main category: cs.CV

TL;DR: 提出PCP，用类级概念先验与KL和熵正则进行弱监督概念预测，显著提升概念F1并保持竞争性分类性能。


<details>
  <summary>Details</summary>
Motivation: Concept annotations are costly in medical imaging; zero-shot and LLM-based methods fail to capture domain-specific features; need weakly supervised approach to produce human-interpretable concept predictions aligned with clinical reasoning.

Method: Introduce Prior-guided Concept Predictor (PCP) that uses class-level concept priors as weak supervision; employs a refinement module with KL divergence and entropy regularization to align predictions to priors and encourage confident, clinically-reasoned outputs; trained end-to-end with image-level labels only.

Result: On PH2 and WBCatt, PCP improves concept-level F1 by >33% over zero-shot baselines; classification performance competitive on PH2, WBCatt, HAM10000, and CXR4 compared to fully supervised CBMs and V-IP.

Conclusion: PCP can predict concept answers without concept-level annotations by using class-level priors and regularized refinement, improving concept prediction and keeping competitive classification performance.

Abstract: Human-interpretable predictions are essential for deploying AI in medical
imaging, yet most interpretable-by-design (IBD) frameworks require concept
annotations for training data, which are costly and impractical to obtain in
clinical contexts. Recent attempts to bypass annotation, such as zero-shot
vision-language models or concept-generation frameworks, struggle to capture
domain-specific medical features, leading to poor reliability. In this paper,
we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised
framework that enables concept answer prediction without explicit supervision
or reliance on language models. PCP leverages class-level concept priors as
weak supervision and incorporates a refinement mechanism with KL divergence and
entropy regularization to align predictions with clinical reasoning.
Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves
concept-level F1-score by over 33% compared to zero-shot baselines, while
delivering competitive classification performance on four medical datasets
(PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept
bottleneck models (CBMs) and V-IP.

</details>


### [108] [Learning with Category-Equivariant Architectures for Human Activity Recognition](https://arxiv.org/abs/2511.01139)
*Yoshihiro Maruyama*

Main category: cs.CV

TL;DR: 通过把时间平移、幅值缩放和传感器层级以范畴化的方式建模并构造等变网络，CatEquiv在HAR任务上实现了更稳健的泛化。


<details>
  <summary>Details</summary>
Motivation: HAR数据具有天然的周期性（时间平移）、幅值变动（增益）和传感器间的层级结构，现有CNN未系统利用这些对称性导致泛化受限，故提出利用范畴理论/对称性来提升稳健性与泛化。

Method: 提出了分类对称性乘积（categorical symmetry product）将三类变换（循环时间平移、正增益、传感器层级poset）结合，设计对应的神经网络层使网络对该乘积群保持等变性，从而在特征提取过程中结构化地约束表示。

Result: 在UCI-HAR数据集的OOD扰动场景（如时间偏移、增益变化、传感器缺失/重排等）下，CatEquiv优于带循环填充的CNN和普通CNN，表现出更强的鲁棒性和泛化性，而模型参数量未增加。

Conclusion: CatEquiv通过对时间循环平移、正增益缩放以及传感器层级偏序结构建模，系统性地实现了对分类对称性的等变性，从而在不增加模型容量的情况下提高了对OOD扰动的鲁棒性。

Abstract: We propose CatEquiv, a category-equivariant neural network for Human Activity
Recognition (HAR) from inertial sensors that systematically encodes temporal,
amplitude, and structural symmetries. In particular, we introduce the
categorical symmetry product where cyclic time shifts, positive gains and the
sensor-hierarchy poset together capture the categorical symmetry structure of
the data. CatEquiv achieves equivariance with respect to the categorical
symmetry product. On UCI-HAR under out-of-distribution perturbations, CatEquiv
attains markedly higher robustness compared with circularly padded CNNs and
plain CNNs. These results demonstrate that enforcing categorical symmetries
yields strong invariance and generalization without additional model capacity.

</details>


### [109] [MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation](https://arxiv.org/abs/2511.01143)
*Ziyi Wang,Yuanmei Zhang,Dorna Esrafilzadeh,Ali R. Jalili,Suncheng Xiang*

Main category: cs.CV

TL;DR: MicroAUNet：轻量注意力分割网络+两阶段知识蒸馏，在低复杂度下实现精确、实时的结直肠息肉分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么输出边界模糊影响临床决策，要么模型过大导致实时推理不足，故需要兼顾精度与效率的轻量级分割网络。

Method: 模型结构上使用depthwise-separable dilated convolutions与单路径、参数共享的channel-spatial attention块；训练上引入了渐进式两阶段知识蒸馏，先蒸馏语义信息再蒸馏边界信息（或同时蒸馏两者的不同阶段），以提升轻量网络的表现。

Result: 在公开基准上，在极低模型复杂度下取得了SOTA准确率，并具备适用于实时内镜的推理速度；代码已开源。

Conclusion: 该文提出了MicroAUNet，一种轻量级的注意力分割网络，通过深度可分离膨胀卷积和单路径参数共享的通道-空间注意力模块来增强多尺度边界特征，并结合两阶段知识蒸馏以从高容量教师模型传递语义和边界信息，达到了在低模型复杂度下的SOTA精度，适合实时临床息肉分割。

Abstract: Early and accurate segmentation of colorectal polyps is critical for reducing
colorectal cancer mortality, which has been extensively explored by academia
and industry. However, current deep learning-based polyp segmentation models
either compromise clinical decision-making by providing ambiguous polyp margins
in segmentation outputs or rely on heavy architectures with high computational
complexity, resulting in insufficient inference speeds for real-time colorectal
endoscopic applications. To address this problem, we propose MicroAUNet, a
light-weighted attention-based segmentation network that combines
depthwise-separable dilated convolutions with a single-path, parameter-shared
channel-spatial attention block to strengthen multi-scale boundary features. On
the basis of it, a progressive two-stage knowledge-distillation scheme is
introduced to transfer semantic and boundary cues from a high-capacity teacher.
Extensive experiments on benchmarks also demonstrate the state-of-the-art
accuracy under extremely low model complexity, indicating that MicroAUNet is
suitable for real-time clinical polyp segmentation. The code is publicly
available at https://github.com/JeremyXSC/MicroAUNet.

</details>


### [110] [ROVER: Benchmarking Reciprocal Cross-Modal Reasoning for Omnimodal Generation](https://arxiv.org/abs/2511.01163)
*Yongyuan Liang,Wei Chow,Feng Li,Ziqiao Ma,Xiyao Wang,Jiageng Mao,Jiuhai Chen,Jiatao Gu,Yue Wang,Furong Huang*

Main category: cs.CV

TL;DR: ROVER是首个聚焦互惠跨模态推理的人工注释基准，显示出现有统一模型在用语言推动视觉生成和用视觉强化语言推理两方面的局限，强调构建交错式融合与视觉抽象能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前评测将文本与图像能力孤立考察，忽视了跨模态相互利用（用一种模态指导/验证另一模态）的关键能力。为实现真正的统一多模态智能，需要专门基准检验互惠跨模态推理。

Method: 构建并人工注释了ROVER基准（1312个任务，1876张图），包含两种互补任务设置：用语言增强的视觉生成（验证语言/推理链是否能指导图像合成）和用视觉增强的语言生成（评估生成中间可视化来强化问答推理）；在17个统一模型上进行大规模评测并对比交错与非交错架构及组合模型。

Result: 实验表明：1) 跨模态推理直接决定视觉生成质量，交错式模型显著优于非交错式；2) 模型在物理/感知层面推理能力较强，但在符号抽象构建方面失败，导致性能下降。

Conclusion: ROVER揭示了统一多模态模型在互惠跨模态推理方面存在显著差距，尤其在构建视觉抽象以支持符号推理时表现不足；交错式模型在用语言引导视觉生成上优于非交错模型，而简单组合强大的单模态模型无法匹敌。

Abstract: Unified multimodal models (UMMs) have emerged as a powerful paradigm for
seamlessly unifying text and image understanding and generation. However,
prevailing evaluations treat these abilities in isolation, such that tasks with
multimodal inputs and outputs are scored primarily through unimodal reasoning,
i.e., textual benchmarks emphasize language-based reasoning, while visual
benchmarks emphasize reasoning outcomes manifested in the pixels. We introduce
ROVER to address this pressing need to test reciprocal cross-modal reasoning,
the use of one modality to guide, verify, or refine outputs in the other, an
ability central to the vision of unified multimodal intelligence. ROVER is a
human-annotated benchmark that explicitly targets reciprocal cross-modal
reasoning, which contains 1312 tasks grounded in 1876 images, spanning two
complementary settings. Verbally-augmented reasoning for visual generation
evaluates whether models can use verbal prompts and reasoning chains to guide
faithful image synthesis. Visually-augmented reasoning for verbal generation
evaluates whether models can generate intermediate visualizations that
strengthen their own reasoning processes for question answering. Experiments on
17 unified models reveal two key findings: (i) Cross-modal reasoning determines
visual generation quality, with interleaved models significantly outperforming
non-interleaved ones; notably, combining strong unimodal models fails to
achieve comparable reasoning. (ii) Models show dissociation between physical
and symbolic reasoning: they succeed at interpreting perceptual concepts
literally but fail to construct visual abstractions for symbolic tasks, where
faulty reasoning harms performance. These results highlight reciprocal
cross-modal reasoning as a critical frontier for enabling true omnimodal
generation.

</details>


### [111] [Web-Scale Collection of Video Data for 4D Animal Reconstruction](https://arxiv.org/abs/2511.01169)
*Brian Nlong Zhao,Jiajun Wu,Shangzhe Wu*

Main category: cs.CV

TL;DR: 本文通过自动化YouTube挖掘与处理，构建了大规模动物视频资源并提出AiM基准，揭示了2D评价与3D真实感之间的矛盾，并通过序列级优化建立首个4D四足动物重建基线。


<details>
  <summary>Details</summary>
Motivation: 现有动物视频数据集规模小且缺乏面向3D/4D任务的处理，限制了无创、单视角动物三四维分析的发展；需大规模、非受控环境数据与评价基准来推进研究。

Method: 构建自动化爬取与处理流程：从YouTube采集视频，进行物体中心化裁剪、关键帧/追踪、附加注释生成；收集30K视频（约2M帧）；人工筛选出230条高质量序列组成AiM（11K帧）；在此基础上改进现有无模型方法，加入序列级优化，形成首个4D重建基线并与模型驱动方法比较。

Result: 构建并公开了大规模数据集（30K视频、2M帧）与AiM基准（230序列、11K帧）；实验证明当前评估指标存在偏差——2D指标偏好模型驱动方法但产生不真实3D形状；改进的无模型方法通过序列级优化在自然性与一致性上取得更好表现，建立了4D重建初始基线。

Conclusion: 该论文提出了一个自动化管道，从YouTube挖掘并处理动物视频，生成面向对象的剪辑及辅助注释，显著扩充了动物视频数据规模；基于此构建了AiM基准（Animal-in-Motion），并提出了首个4D四足动物重建基线。

Abstract: Computer vision for animals holds great promise for wildlife research but
often depends on large-scale data, while existing collection methods rely on
controlled capture setups. Recent data-driven approaches show the potential of
single-view, non-invasive analysis, yet current animal video datasets are
limited--offering as few as 2.4K 15-frame clips and lacking key processing for
animal-centric 3D/4D tasks. We introduce an automated pipeline that mines
YouTube videos and processes them into object-centric clips, along with
auxiliary annotations valuable for downstream tasks like pose estimation,
tracking, and 3D/4D reconstruction. Using this pipeline, we amass 30K videos
(2M frames)--an order of magnitude more than prior works. To demonstrate its
utility, we focus on the 4D quadruped animal reconstruction task. To support
this task, we present Animal-in-Motion (AiM), a benchmark of 230 manually
filtered sequences with 11K frames showcasing clean, diverse animal motions. We
evaluate state-of-the-art model-based and model-free methods on
Animal-in-Motion, finding that 2D metrics favor the former despite unrealistic
3D shapes, while the latter yields more natural reconstructions but scores
lower--revealing a gap in current evaluation. To address this, we enhance a
recent model-free approach with sequence-level optimization, establishing the
first 4D animal reconstruction baseline. Together, our pipeline, benchmark, and
baseline aim to advance large-scale, markerless 4D animal reconstruction and
related tasks from in-the-wild videos. Code and datasets are available at
https://github.com/briannlongzhao/Animal-in-Motion.

</details>


### [112] [Diffusion Transformer meets Multi-level Wavelet Spectrum for Single Image Super-Resolution](https://arxiv.org/abs/2511.01175)
*Peng Du,Hui Li,Han Xu,Paul Barom Jeon,Dongwook Lee,Daehyun Ji,Ran Yang,Feng Zhu*

Main category: cs.CV

TL;DR: DTWSR：基于多层小波谱的扩散Transformer，用金字塔分词与双解码器建模多尺度频带关系，提高图像超分辨率的感知与保真性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DWT的方法多关注捕捉细粒度频率信号，但往往忽视不同尺度频率子带之间的相互关系，导致重建图像出现不一致性和伪影。作者希望通过联合建模多尺度子带关系来改善超分辨率结果的自然性与一致性。

Method: 使用多层离散小波变换（MDWT）将图像分解为波谱，提出金字塔分词（pyramid tokenization）将各子带嵌入为序列token输入Transformer；采用扩散模型框架生成图像；设计双解码器分别处理低频与高频子带，同时保持它们在生成中的对齐。

Result: 在多个基准数据集上进行大量实验，结果表明DTWSR在感知质量和保真度两个方面都取得了较高的性能，表现优于若干对比方法（文中宣称但摘要未给出具体数值）。

Conclusion: 该工作通过将扩散模型、Transformer与多层小波变换相结合，提出了DTWSR用于图像超分辨率，旨在建模多尺度频带之间的内在关联以提升重建一致性与感知质量。

Abstract: Discrete Wavelet Transform (DWT) has been widely explored to enhance the
performance of image superresolution (SR). Despite some DWT-based methods
improving SR by capturing fine-grained frequency signals, most existing
approaches neglect the interrelations among multiscale frequency sub-bands,
resulting in inconsistencies and unnatural artifacts in the reconstructed
images. To address this challenge, we propose a Diffusion Transformer model
based on image Wavelet spectra for SR (DTWSR).DTWSR incorporates the
superiority of diffusion models and transformers to capture the interrelations
among multiscale frequency sub-bands, leading to a more consistence and
realistic SR image. Specifically, we use a Multi-level Discrete Wavelet
Transform (MDWT) to decompose images into wavelet spectra. A pyramid
tokenization method is proposed which embeds the spectra into a sequence of
tokens for transformer model, facilitating to capture features from both
spatial and frequency domain. A dual-decoder is designed elaborately to handle
the distinct variances in lowfrequency (LF) and high-frequency (HF) sub-bands,
without omitting their alignment in image generation. Extensive experiments on
multiple benchmark datasets demonstrate the effectiveness of our method, with
high performance on both perception quality and fidelity.

</details>


### [113] [A Topology-Aware Graph Convolutional Network for Human Pose Similarity and Action Quality Assessment](https://arxiv.org/abs/2511.01194)
*Minmin Zeng*

Main category: cs.CV

TL;DR: 提出基于骨架拓扑的GCN-PSN与孪生对比回归训练，有效提升姿态相似度学习与动作质量评估，在AQA-7和FineDiving上表现良好。


<details>
  <summary>Details</summary>
Motivation: AQA需要对人体微小且复杂的运动差异进行精细理解，传统基于坐标的方法忽视骨架拓扑结构，导致对姿态相似度评估不足；因此希望利用骨架拓扑信息来提升嵌入的判别性和评估精度。

Method: 将人体骨架建模为图结构，使用拓扑感知的GCN学习判别性且对拓扑敏感的姿态嵌入；采用Siamese（孪生）结构并以对比回归（contrastive regression）目标训练，以直接学习姿态间相似度与动作质量评分映射。

Result: 在AQA-7和FineDiving基准上，GCN-PSN优于坐标基线方法并取得竞争性表现；消融研究显示引入骨架拓扑和GCN结构对姿态相似度和动作质量评估均有显著贡献。

Conclusion: 该论文通过引入拓扑感知的图卷积网络（GCN-PSN），在基于骨架的人体动作质量评估（AQA）中有效提升了姿态相似性度量和评分性能，优于坐标基线并在AQA-7与FineDiving数据集上取得了有竞争力的结果。

Abstract: Action Quality Assessment (AQA) requires fine-grained understanding of human
motion and precise evaluation of pose similarity. This paper proposes a
topology-aware Graph Convolutional Network (GCN) framework, termed GCN-PSN,
which models the human skeleton as a graph to learn discriminative,
topology-sensitive pose embeddings. Using a Siamese architecture trained with a
contrastive regression objective, our method outperforms coordinate-based
baselines and achieves competitive performance on AQA-7 and FineDiving
benchmarks. Experimental results and ablation studies validate the
effectiveness of leveraging skeletal topology for pose similarity and action
quality assessment.

</details>


### [114] [MoSa: Motion Generation with Scalable Autoregressive Modeling](https://arxiv.org/abs/2511.01200)
*Mengyuan Liu,Sheng Yan,Yong Wang,Yingjie Li,Gui-Bin Bian,Hong Liu*

Main category: cs.CV

TL;DR: MoSa通过层级残差量化与多尺度token保留，并结合可扩展自回归生成与CAQ-VAE，实现在质量（FID 0.06）与效率（推理快27%）上的显著提升，并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 提升VQ-GT范式在文本到3D人体动作生成中的效率与质量，减少自回归步数同时保持或增强重建与生成保真度，并使模型易于泛化到编辑等下游任务。

Method: 提出了MTPS（多尺度token保留策略）与层级残差向量量化VAE（RQ-VAE），在每层量化时进行插值以保留粗细粒度token；引入可扩展自回归（SAR）生成器一次预测多尺度token，从而将推理步骤减少到与量化层数一致（10步）；为应对插值带来的重建退化，设计了轻量但 expressive 的卷积-注意力混合VQ-VAE（CAQ-VAE），改进残差模块并加入注意力以捕捉全局依赖。

Result: 在Motion-X数据集上取得FID=0.06，优于MoMask的0.20；推理时间减少27%；在质量与速度上均超越此前方法，且无需额外微调即可用于动作编辑等下游任务。

Conclusion: MoSa通过多尺度分层量化和可扩展自回归建模在文本驱动的3D人体动作生成上实现了显著提升，兼顾质量与速度，并具有下游任务的泛化能力。

Abstract: We introduce MoSa, a novel hierarchical motion generation framework for
text-driven 3D human motion generation that enhances the Vector
Quantization-guided Generative Transformers (VQ-GT) paradigm through a
coarse-to-fine scalable generation process. In MoSa, we propose a Multi-scale
Token Preservation Strategy (MTPS) integrated into a hierarchical residual
vector quantization variational autoencoder (RQ-VAE). MTPS employs
interpolation at each hierarchical quantization to effectively retain
coarse-to-fine multi-scale tokens. With this, the generative transformer
supports Scalable Autoregressive (SAR) modeling, which predicts scale tokens,
unlike traditional methods that predict only one token at each step.
Consequently, MoSa requires only 10 inference steps, matching the number of
RQ-VAE quantization layers. To address potential reconstruction degradation
from frequent interpolation, we propose CAQ-VAE, a lightweight yet expressive
convolution-attention hybrid VQ-VAE. CAQ-VAE enhances residual block design and
incorporates attention mechanisms to better capture global dependencies.
Extensive experiments show that MoSa achieves state-of-the-art generation
quality and efficiency, outperforming prior methods in both fidelity and speed.
On the Motion-X dataset, MoSa achieves an FID of 0.06 (versus MoMask's 0.20)
while reducing inference time by 27 percent. Moreover, MoSa generalizes well to
downstream tasks such as motion editing, requiring no additional fine-tuning.
The code is available at https://mosa-web.github.io/MoSa-web

</details>


### [115] [OmniVLA: Unifiying Multi-Sensor Perception for Physically-Grounded Multimodal VLA](https://arxiv.org/abs/2511.01210)
*Heyu Guo,Shanmu Wang,Ruichun Ma,Shiqi Jiang,Yasaman Ghasempour,Omid Abari,Baining Guo,Lili Qi*

Main category: cs.CV

TL;DR: 将多种物理传感器信息以遮罩图像叠加到RGB上，训练基于RGB预训练骨干的多感知VLA模型，显著提升真实操作任务的成功率和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型大多仅依赖RGB感知，限制了对物理世界的感知能力和操作性能；通过引入物理传感器模态（红外、雷达、声学）并以图像原生方式统一表示，可以增强空间和物理感知，提升操作任务的成功率。

Method: 提出传感器遮罩图像（sensor-masked image）作为统一表示，将红外、毫米波雷达、麦克风阵列等传感器的空间信息以可叠加的遮罩形式投影到RGB图像，从而保持图像统计特性并使用轻量级的每传感器投影器与基于RGB预训练的VLA骨干结合训练多感知模型。

Result: 在真实世界需要传感器感知以引导操作的任务上，OmniVLA平均任务成功率84%，分别比仅用RGB的基线高出59%和比直接使用原始传感器输入的基线高出28%，并显示出更高的学习效率与更强的泛化能力。

Conclusion: OmniVLA通过将多模态传感器信息以传感器遮罩图像的方式与RGB图像统一表示，显著提升了基于视觉-语言-动作的机器人操作性能，特别是在需要物理感知的任务上表现出更高的任务成功率、学习效率和泛化能力。

Abstract: Vision-language-action (VLA) models have shown strong generalization for
action prediction through large-scale vision-language pretraining. However,
most existing models rely solely on RGB cameras, limiting their perception and,
consequently, manipulation capabilities. We present OmniVLA, an omni-modality
VLA model that integrates novel sensing modalities for physically-grounded
spatial intelligence beyond RGB perception. The core of our approach is the
sensor-masked image, a unified representation that overlays spatially grounded
and physically meaningful masks onto the RGB images, derived from sensors
including an infrared camera, a mmWave radar, and a microphone array. This
image-native unification keeps sensor input close to RGB statistics to
facilitate training, provides a uniform interface across sensor hardware, and
enables data-efficient learning with lightweight per-sensor projectors. Built
on this, we present a multisensory vision-language-action model architecture
and train the model based on an RGB-pretrained VLA backbone. We evaluate
OmniVLA on challenging real-world tasks where sensor-modality perception is
needed to guide the manipulation. OmniVLA achieves an average task success rate
of 84%, significantly outperforms both RGB-only and raw-sensor-input baseline
models by 59% and 28% respectively, meanwhile showing higher learning
efficiency and stronger generalization capability.

</details>


### [116] [Thought-For-Food: Reasoning Chain Induced Food Visual Question Answering](https://arxiv.org/abs/2511.01213)
*Riddhi Jain,Manasi Patwardhan,Parijat Deshpande,Venkataramana Runkana*

Main category: cs.CV

TL;DR: 针对印度食品VQA，作者自动构建并验证多步推理链用于微调与强化学习训练，结合知识图谱后使小型模型性能平均提升约10%。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统偏向西方食物，印度美食多样化与复杂的烹饪语境需要多步推理来识别食物间关系与背景知识，单步生成答案并解释的方法不足以捕捉这些复杂性。

Method: 基于已有印度食品VQA数据，自动构建最小人工干预的推理链（reasoning chains），用于微调较小的LLM和VLM，随后使用更大规模数据和强化学习进一步训练模型；结合知识图谱增强关系理解，并采用自动验证机制筛选高质量推理链。

Result: 在加入推理链的增强训练后，模型在基线上的平均准确率提升约10个百分点；并提供了关于推理链对模型性能影响的详细消融与分析结果。

Conclusion: 通过在印度食品VQA任务中引入自动生成的多步推理链并结合强化学习训练，能显著提升小型LLM与视觉语言模型的回答准确性。

Abstract: The immense diversity in the culture and culinary of Indian cuisines calls
attention to the major shortcoming of the existing Visual Question
Answering(VQA) systems which are inclined towards the foods from Western
region. Recent attempt towards building a VQA dataset for Indian food is a step
towards addressing this challenge. However, their approach towards VQA follows
a two-step process in which the answer is generated first, followed by the
explanation of the expected answer. In this work, we claim that food VQA
requires to follow a multi-step reasoning process to arrive at an accurate
answer, especially in the context of India food, which involves understanding
complex culinary context and identifying relationships between various food
items. With this hypothesis we create reasoning chains upon the QA with minimal
human intervention. We fine-tune smaller LLMs and VLMs with auto-validated
reasoning chains and further train them using reinforcement learning with
larger data. With augmentation of reasoning chains, we observed accuracy
improvement of an average 10 percentage points on the baseline. We provide
detailed analysis in terms the effect of addition of reasoning chains for the
Indian Food VQA task.
  Index Terms - FoodVQA, Reasoning Chains, Reinforcement Learning, Knowledge
Graph.

</details>


### [117] [Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering](https://arxiv.org/abs/2511.01223)
*Zahra Mehraban,Sebastien Glaser,Michael Milford,Ronald Schroeter*

Main category: cs.CV

TL;DR: 在右侧驾驶数据上先做水平翻转预训练、再在左侧驾驶目标域微调，是一种简单有效的域适应策略，能提高转向预测准确性并引导模型关注左侧重要路况特征。


<details>
  <summary>Details</summary>
Motivation: 目标是提高自动驾驶模型在不同驾驶侧（右侧->左侧）和不同道路条件间的泛化能力，探索简单的数据预处理（水平翻转）是否能作为有效的域适应起点以减少在目标域上的重训练成本。

Method: 比较四种训练策略：1) 基线：仅在美国右侧驾驶数据训练；2) 在美国数据上进行水平翻转后训练；3) 在美国数据预训练后在澳大利亚数据微调；4) 在翻转的美国数据预训练后再在澳大利亚微调。评估指标为转向预测误差与基于显著图的注意力分析（关注重要路区的注意力移位）。在两种网络架构（PilotNet与ResNet）上复现实验以验证通用性。

Result: 实验结果：单独翻转预训练会引入特征表示不匹配，导致预测稳定性下降；但翻转预训练再微调能显著降低预测误差，并使模型更集中关注左侧路况线索。ResNet上的重复实验展示了相似的趋势，表明方法具有跨架构一定的鲁棒性。

Conclusion: 作者总结：在迁移到左侧驾驶(澳大利亚)时，先用翻转的右侧驾驶数据进行预训练，然后在目标域上微调，可以显著提升模型的转移性能和注意力集中于左侧线索，从而降低转向预测误差。单独用翻转数据预训练会因特征错配导致预测不稳定，但作为预训练步骤结合微调则效果最佳。该结论对不同架构（PilotNet、ResNet）均成立，强调预处理与微调结合的重要性。

Abstract: Domain adaptation is required for automated driving models to generalize well
across diverse road conditions. This paper explores a training method for
domain adaptation to adapt PilotNet, an end-to-end deep learning-based model,
for left-hand driving conditions using real-world Australian highway data. Four
training methods were evaluated: (1) a baseline model trained on U.S.
right-hand driving data, (2) a model trained on flipped U.S. data, (3) a model
pretrained on U.S. data and then fine-tuned on Australian highways, and (4) a
model pretrained on flipped U.S. data and then finetuned on Australian
highways. This setup examines whether incorporating flipped data enhances the
model adaptation by providing an initial left-hand driving alignment. The paper
compares model performance regarding steering prediction accuracy and
attention, using saliency-based analysis to measure attention shifts across
significant road regions. Results show that pretraining on flipped data alone
worsens prediction stability due to misaligned feature representations, but
significantly improves adaptation when followed by fine-tuning, leading to
lower prediction error and stronger focus on left-side cues. To validate this
approach across different architectures, the same experiments were done on
ResNet, which confirmed similar adaptation trends. These findings emphasize the
importance of preprocessing techniques, such as flipped-data pretraining,
followed by fine-tuning to improve model adaptation with minimal retraining
requirements.

</details>


### [118] [Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark](https://arxiv.org/abs/2511.01233)
*Rajmund Nagy,Hendric Voss,Thanh Hoang-Minh,Mihail Tsakov,Teodor Nikolov,Zeyi Zhang,Tenglong Ao,Sicheng Yang,Shaoli Huang,Yongkang Cheng,M. Hamza Mughal,Rishabh Dabral,Kiran Chhatre,Christian Theobalt,Libin Liu,Stefan Kopp,Rachel McDonnell,Michael Neff,Taras Kucherenko,Youngwoo Yoon,Gustav Eje Henter*

Main category: cs.CV

TL;DR: 该工作通过为BEAT2数据集制定规范化人工评估协议并进行大规模众包评测，揭示了该领域评估混乱问题，证实近期模型并非始终优于早期方法，强调需分离评估运动质量与语音-手势对齐，并公开大量数据与工具以推动标准化。


<details>
  <summary>Details</summary>
Motivation: 现有研究因评估方法不标准和设计缺陷，无法可靠比较不同手势生成方法及判断最先进技术；因此需要统一且严格的人工评估协议以推动真实进展。

Method: 作者提出并发布了针对BEAT2动作捕捉数据集的详细人工评估协议；利用该协议在众包平台上对六个近期模型（原作者训练的版本）进行大规模评测，评估维度为运动真实性和语音-手势对齐度；并公开合成动作、渲染视频、评测投票等资源以推进标准化。

Result: 通过大规模用户研究，发现1) 新模型并不总是优于早期方法；2) 许多论文中关于高运动真实性或良好语音-手势对齐的声明，在严格评测下不成立；3) 必须分别评估运动质量与多模态对齐才能获得准确基准。作者还将发布约5小时合成动作、750+渲染视频和1.6万次偏好投票数据。

Conclusion: 该论文指出当前语音驱动三维手势生成领域在人工评估方面缺乏统一标准，许多实验设计存在缺陷，导致方法间不可比并妨碍领域进展。

Abstract: We review human evaluation practices in automated, speech-driven 3D gesture
generation and find a lack of standardisation and frequent use of flawed
experimental setups. This leads to a situation where it is impossible to know
how different methods compare, or what the state of the art is. In order to
address common shortcomings of evaluation design, and to standardise future
user studies in gesture-generation works, we introduce a detailed human
evaluation protocol for the widely-used BEAT2 motion-capture dataset. Using
this protocol, we conduct large-scale crowdsourced evaluation to rank six
recent gesture-generation models -- each trained by its original authors --
across two key evaluation dimensions: motion realism and speech-gesture
alignment. Our results provide strong evidence that 1) newer models do not
consistently outperform earlier approaches; 2) published claims of high motion
realism or speech-gesture alignment may not hold up under rigorous evaluation;
and 3) the field must adopt disentangled assessments of motion quality and
multimodal alignment for accurate benchmarking in order to make progress.
Finally, in order to drive standardisation and enable new evaluation research,
we will release five hours of synthetic motion from the benchmarked models;
over 750 rendered video stimuli from the user studies -- enabling new
evaluations without model reimplementation required -- alongside our
open-source rendering script, and the 16,000 pairwise human preference votes
collected for our benchmark.

</details>


### [119] [Eyes on Target: Gaze-Aware Object Detection in Egocentric Video](https://arxiv.org/abs/2511.01237)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 本文用深度感知的凝视引导注意力机制改进ViT目标检测，专注观察者关注区域，在模拟器与公开egocentric数据集上均带来可靠的性能提升，并提供注意力头级别的解释性分析。


<details>
  <summary>Details</summary>
Motivation: 人类凝视携带关于视觉关注的强监督信号，在复杂场景下能够指示重要区域；将此信息用于自我中心视频的目标检测，可更好地评估模拟器中的人类行为并提升检测准确率。

Method: 在Vision Transformer中引入基于眼动的特征，通过与深度信息融合生成凝视引导的注意力偏置，强调观察者优先的空间区域；并通过消融实验和头重要性度量来分析凝视对注意力头的调制作用。

Result: 在自制的egocentric模拟器数据集以及公开数据集（Ego4D Ego-Motion、Ego-CH-Gaze）上，相较于不使用凝视信息的基线，模型在检测准确率上持续提高；并通过凝视感知的注意力头重要性指标解释了凝视如何影响Transformer的注意力分配。

Conclusion: 论文提出将凝视（gaze）信息与深度信息注入到ViT的注意力机制中，以偏向人类关注区域，从而提升自我中心（egocentric）视频中的目标检测性能。

Abstract: Human gaze offers rich supervisory signals for understanding visual attention
in complex visual environments. In this paper, we propose Eyes on Target, a
novel depth-aware and gaze-guided object detection framework designed for
egocentric videos. Our approach injects gaze-derived features into the
attention mechanism of a Vision Transformer (ViT), effectively biasing spatial
feature selection toward human-attended regions. Unlike traditional object
detectors that treat all regions equally, our method emphasises
viewer-prioritised areas to enhance object detection. We validate our method on
an egocentric simulator dataset where human visual attention is critical for
task assessment, illustrating its potential in evaluating human performance in
simulation scenarios. We evaluate the effectiveness of our gaze-integrated
model through extensive experiments and ablation studies, demonstrating
consistent gains in detection accuracy over gaze-agnostic baselines on both the
custom simulator dataset and public benchmarks, including Ego4D Ego-Motion and
Ego-CH-Gaze datasets. To interpret model behaviour, we also introduce a
gaze-aware attention head importance metric, revealing how gaze cues modulate
transformer attention dynamics.

</details>


### [120] [Beyond Deceptive Flatness: Dual-Order Solution for Strengthening Adversarial Transferability](https://arxiv.org/abs/2511.01240)
*Zhixuan Zhang,Pingyu Wang,Xingjian Zheng,Linbo Qing,Qi Liu*

Main category: cs.CV

TL;DR: 提出基于二阶信息的Adversarial Flatness概念，构建AFA和MCAS提升对抗样本迁移性，在多种测试场景下优于六个基线。


<details>
  <summary>Details</summary>
Motivation: 现有可迁移攻击尽管关注平坦损失，但仍容易陷入“平坦却陡峭”的子最优区域（欺骗性平坦性），导致迁移性不足；因此引入二阶信息与新的平坦性度量以提升迁移性能。

Method: 基于双阶信息引入Adversarial Flatness（AF）指标以度量对抗样本所在损失曲面平坦性，推导理论保证并构造可高效计算的近似目标；设计AFA攻击以处理梯度符号改变问题，并提出MCAS提升内循环采样效率以增强攻击能力。

Result: 在ImageNet兼容数据集上，相比六个基线方法，AFA生成的对抗样本位于更平坦的损失区域并显著提高不同架构间的迁移率；在输入变换防护和百度云API黑箱测试中也表现更好。

Conclusion: 本文提出了从二阶信息角度解决可迁移对抗攻击中“欺骗性平坦性”的方法，通过定义Adversarial Flatness（AF）并给出理论保证，提出了高效近似目标的Adversarial Flatness Attack（AFA），以及提升采样效率的MonteCarlo Adversarial Sampling（MCAS），在ImageNet兼容数据集和黑箱API上均优于六个基线方法。

Abstract: Transferable attacks generate adversarial examples on surrogate models to
fool unknown victim models, posing real-world threats and growing research
interest. Despite focusing on flat losses for transferable adversarial
examples, recent studies still fall into suboptimal regions, especially the
flat-yet-sharp areas, termed as deceptive flatness. In this paper, we introduce
a novel black-box gradient-based transferable attack from a perspective of
dual-order information. Specifically, we feasibly propose Adversarial Flatness
(AF) to the deceptive flatness problem and a theoretical assurance for
adversarial transferability. Based on this, using an efficient approximation of
our objective, we instantiate our attack as Adversarial Flatness Attack (AFA),
addressing the altered gradient sign issue. Additionally, to further improve
the attack ability, we devise MonteCarlo Adversarial Sampling (MCAS) by
enhancing the inner-loop sampling efficiency. The comprehensive results on
ImageNet-compatible dataset demonstrate superiority over six baselines,
generating adversarial examples in flatter regions and boosting transferability
across model architectures. When tested on input transformation attacks or the
Baidu Cloud API, our method outperforms baselines.

</details>


### [121] [CenterMamba-SAM: Center-Prioritized Scanning and Temporal Prototypes for Brain Lesion Segmentation](https://arxiv.org/abs/2511.01243)
*Yu Tian,Zhongheng Yang,Chenshi Liu,Yiyun Su,Ziwei Hong,Zexi Gong,Jingyuan Xu*

Main category: cs.CV

TL;DR: 提出一种冻结骨干、训练轻量适配器的CenterMamba-SAM，核心为3x3 corner-axis-center扫描的编码器、跨切片原型记忆提示生成器与多尺度记忆解码器，显著提升脑损伤分割的微小病灶识别与跨切片一致性，并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 脑损伤分割存在小目标、低对比、采样各向异性和切片间不连续等困难，需提高对微小低对比界面的敏感性并保持跨切片一致性同时保证计算高效。

Method: 提出CenterMamba编码器（3x3 corner-axis-center短序列扫描策略）、记忆驱动结构提示生成器（跨切片原型库）与记忆增强多尺度解码器（多层记忆注意、深度监督和渐进细化）。整体采用冻结骨干+轻量适配器的端到端训练方案。

Result: 在公开基准上，CenterMamba-SAM在脑损伤分割任务上取得了最新的SOTA性能，尤其在检测小病灶和弱边界区域表现优越，同时具备较高的跨切片连贯性与资源效率。

Conclusion: CenterMamba-SAM是针对脑损伤分割设计的高效轻量微调框架，通过冻结预训练骨干并仅训练适配器，实现资源友好的迁移学习，同时在结构上兼顾细节恢复与跨切片一致性。

Abstract: Brain lesion segmentation remains challenging due to small, low-contrast
lesions, anisotropic sampling, and cross-slice discontinuities. We propose
CenterMamba-SAM, an end-to-end framework that freezes a pretrained backbone and
trains only lightweight adapters for efficient fine-tuning. At its core is the
CenterMamba encoder, which employs a novel 3x3 corner-axis-center
short-sequence scanning strategy to enable center-prioritized, axis-reinforced,
and diagonally compensated information aggregation. This design enhances
sensitivity to weak boundaries and tiny foci while maintaining sparse yet
effective feature representation. A memory-driven structural prompt generator
maintains a prototype bank across neighboring slices, enabling automatic
synthesis of reliable prompts without user interaction, thereby improving
inter-slice coherence. The memory-augmented multi-scale decoder integrates
memory attention modules at multiple levels, combining deep supervision with
progressive refinement to restore fine details while preserving global
consistency. Extensive experiments on public benchmarks demonstrate that
CenterMamba-SAM achieves state-of-the-art performance in brain lesion
segmentation.

</details>


### [122] [Source-Only Cross-Weather LiDAR via Geometry-Aware Point Drop](https://arxiv.org/abs/2511.01250)
*YoungJae Cheong,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出一种训练期启用的轻量几何感知适配器，通过对齐方位角、环形填充和局部KNN统计生成几何特征，并用以区域感知正则化，显著提升跨天气条件下的LiDAR语义分割鲁棒性，推理开销可忽略。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恶劣天气下通过增强、领域随机化或不确定性正则等手段改进鲁棒性，但往往忽视了边界、拐角和稀疏区域等结构薄弱点，导致模型在这些区域性能下降。

Method: 提出Light Geometry-aware adapter：对极坐标的方位角进行对齐并采用水平环形填充以保持0~360度的邻接连续性；在局部窗口内使用KNN聚合邻点并计算简单局部统计量，压缩为紧凑的几何感知特征；训练阶段利用这些特征驱动区域感知正则化以稳定结构脆弱区域的预测。该模块可即插即用，仅在训练时启用，对推理开销几乎为零。

Result: 在源域（SemanticKITTI）训练、目标域（SemanticSTF）无标签评估的设置下，适配器相比数据增强基线提升mIoU 7.9个百分点，相比类中心正则基线提升0.6个百分点，证明几何驱动正则化对全天气LiDAR分割的重要性。

Conclusion: 该方法通过引入轻量的几何感知适配器，显著提升了恶劣天气下LiDAR语义分割的鲁棒性，尤其在边界、角落和稀疏区域的预测稳定性上有所改善。

Abstract: LiDAR semantic segmentation degrades in adverse weather because refraction,
scattering, and point dropouts corrupt geometry. Prior work in weather
simulation, mixing-based augmentation, domain randomization, and uncertainty or
boundary regularization improves robustness but still overlooks structural
vulnerabilities near boundaries, corners, and sparse regions. We present a
Light Geometry-aware adapter. The module aligns azimuth and applies horizontal
circular padding to preserve neighbor continuity across the 0~360 degree
wrap-around boundary. A local-window K-Nearest Neighbors gathers nearby points
and computes simple local statistics, which are compressed into compact
geometry-aware cues. During training, these cues drive region-aware
regularization that stabilizes predictions in structurally fragile areas. The
adapter is plug and play, complements augmentation, and can be enabled only
during training with negligible inference cost. We adopt a source-only
cross-weather setup where models train on SemanticKITTI and are evaluated on
SemanticSTF without target labels or fine-tuning. The adapter improves mIoU by
7.9 percentage points over the data-centric augmentation baseline and by 0.6
points over the class-centric regularization baseline. These results indicate
that geometry-driven regularization is a key direction for all-weather LiDAR
segmentation.

</details>


### [123] [MotionStream: Real-Time Video Generation with Interactive Motion Controls](https://arxiv.org/abs/2511.01266)
*Joonghyuk Shin,Zhengqi Li,Richard Zhang,Jun-Yan Zhu,Jaesik Park,Eli Schechtman,Xun Huang*

Main category: cs.CV

TL;DR: 提出MotionStream，通过将受控文本视频教师蒸馏为因果学生并引入滑动窗口因果注意力与注意力汇，结合自回滚训练与KV缓存滚动，实现单GPU上亚秒延迟、最高29FPS的无限长度实时流式视频生成，质量与运动跟随达到SOTA，速度快约100倍。


<details>
  <summary>Details</summary>
Motivation: 现有运动条件视频生成延迟高且非因果，无法实时交互；需要一种既能遵循全局文本与局部运动控制，又能实现低延迟、无限长度流式生成的方法。

Method: 先扩展文本到视频生成模型以添加运动控制，得到高质量的双向教师模型；然后通过自强制（Self Forcing）与分布匹配蒸馏（Distribution Matching Distillation）将教师蒸馏为因果学生。关键技术包括滑动窗口因果注意力、注意力汇（attention sinks）、自回滚（self-rollout）训练与KV缓存滚动，模拟推理时有限上下文窗口下的外推。

Result: 模型在运动跟随和视频质量上达到SOTA，同时速度提升约两个数量级，支持实时绘制轨迹、相机控制与动作迁移，首次实现无限长度流式交互体验。

Conclusion: MotionStream实现了在单GPU上实时流式生成（亚秒级延迟，最高29FPS），通过将受控文本到视频模型精蒸馏为因果学生模型，解决了长期、无限时域生成的挑战。

Abstract: Current motion-conditioned video generation methods suffer from prohibitive
latency (minutes per video) and non-causal processing that prevents real-time
interaction. We present MotionStream, enabling sub-second latency with up to 29
FPS streaming generation on a single GPU. Our approach begins by augmenting a
text-to-video model with motion control, which generates high-quality videos
that adhere to the global text prompt and local motion guidance, but does not
perform inference on the fly. As such, we distill this bidirectional teacher
into a causal student through Self Forcing with Distribution Matching
Distillation, enabling real-time streaming inference. Several key challenges
arise when generating videos of long, potentially infinite time-horizons: (1)
bridging the domain gap from training on finite length and extrapolating to
infinite horizons, (2) sustaining high quality by preventing error
accumulation, and (3) maintaining fast inference, without incurring growth in
computational cost due to increasing context windows. A key to our approach is
introducing carefully designed sliding-window causal attention, combined with
attention sinks. By incorporating self-rollout with attention sinks and KV
cache rolling during training, we properly simulate inference-time
extrapolations with a fixed context window, enabling constant-speed generation
of arbitrarily long videos. Our models achieve state-of-the-art results in
motion following and video quality while being two orders of magnitude faster,
uniquely enabling infinite-length streaming. With MotionStream, users can paint
trajectories, control cameras, or transfer motion, and see results unfold in
real-time, delivering a truly interactive experience.

</details>


### [124] [PRevivor: Reviving Ancient Chinese Paintings using Prior-Guided Color Transformers](https://arxiv.org/abs/2511.01274)
*Tan Tang,Yanhong Wu,Junming Gao,Yingcai Wu*

Main category: cs.CV

TL;DR: 提出PRevivor：一种基于先验的色彩Transformer，通过亮度恢复与双分支色相纠正联合局部先验成功复原古代中国画作色彩，性能优于现有上色方法。


<details>
  <summary>Details</summary>
Motivation: 古代中国画作不可逆的色彩退化难以复原，且缺乏大规模高质量数据集与端到端工具，作者借助近代画作的色彩先验来指导古画的恢复。

Method: 先将色彩恢复分为亮度增强与色相纠正两阶段：亮度增强使用两个变分U-Net与多尺度映射模块恢复褪色亮度；色相纠正通过双分支颜色查询模块并结合局部色相先验进行局部与全局相互补的修复。

Result: 与多种最新上色方法对比，PRevivor在定量指标与视觉效果上均显示出更优的性能，证明了方法有效性。

Conclusion: PRevivor通过利用近代真幅色彩先验并分解为亮度增强与色相纠正两步，实现了对古代画作不可逆褪色的高质量数字复原。

Abstract: Ancient Chinese paintings are a valuable cultural heritage that is damaged by
irreversible color degradation. Reviving color-degraded paintings is
extraordinarily difficult due to the complex chemistry mechanism. Progress is
further slowed by the lack of comprehensive, high-quality datasets, which
hampers the creation of end-to-end digital restoration tools. To revive colors,
we propose PRevivor, a prior-guided color transformer that learns from recent
paintings (e.g., Ming and Qing Dynasty) to restore ancient ones (e.g., Tang and
Song Dynasty). To develop PRevivor, we decompose color restoration into two
sequential sub-tasks: luminance enhancement and hue correction. For luminance
enhancement, we employ two variational U-Nets and a multi-scale mapping module
to translate faded luminance into restored counterparts. For hue correction, we
design a dual-branch color query module guided by localized hue priors
extracted from faded paintings. Specifically, one branch focuses attention on
regions guided by masked priors, enforcing localized hue correction, whereas
the other branch remains unconstrained to maintain a global reasoning
capability. To evaluate PRevivor, we conduct extensive experiments against
state-of-the-art colorization methods. The results demonstrate superior
performance both quantitatively and qualitatively.

</details>


### [125] [Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions](https://arxiv.org/abs/2511.01284)
*Karma Phuntsho,Abdullah,Kyungmi Lee,Ickjai Lee,Euijoon Ahn*

Main category: cs.CV

TL;DR: 综述指出FMs在医疗影像中具备巨大潜力，但需通过隐私保护、持续学习、数据高效策略与严格基准来实现安全可信的临床部署。


<details>
  <summary>Details</summary>
Motivation: 现有任务特定模型难以泛化、标注资源匮乏且临床应用对鲁棒性与隐私要求高，基础模型提供可迁移表征的潜力促使研究者探索如何将其可靠、安全地应用于真实医疗影像场景。

Method: 系统回顾并分类评估了多类适配方法：监督微调、领域特定预训练、参数高效微调、无监督/自监督学习、混合方法以及多模态/跨模态框架；对每类方法的性能提升、临床适用性、局限性和权衡进行批判性比较。

Result: 总结各方法在文献中报告的性能改进与局限性，指出多数研究在真实临床分布下泛化性与隐私保护实证不足，并提出若干未来研究方向以填补缺口，例如持续学习、差分隐私/联邦学习、合成数据与人机交互验证、以及跨机构系统性基准。

Conclusion: 本文综述了将基础模型(FMs)适应医疗影像的多种策略，认为尽管FMs在可迁移表示学习上具有显著潜力，但在领域迁移、数据标注不足、计算成本和隐私等方面仍面临重要挑战，需通过持续学习、联邦/隐私保护、混合自监督、数据中心化生成与人机交互验证及系统性基准测试等方向推进临床落地。

Abstract: Foundation models (FMs) have emerged as a transformative paradigm in medical
image analysis, offering the potential to provide generalizable, task-agnostic
solutions across a wide range of clinical tasks and imaging modalities. Their
capacity to learn transferable representations from large-scale data has the
potential to address the limitations of conventional task-specific models.
However, adaptation of FMs to real-world clinical practice remains constrained
by key challenges, including domain shifts, limited availability of
high-quality annotated data, substantial computational demands, and strict
privacy requirements. This review presents a comprehensive assessment of
strategies for adapting FMs to the specific demands of medical imaging. We
examine approaches such as supervised fine-tuning, domain-specific pretraining,
parameter-efficient fine-tuning, self-supervised learning, hybrid methods, and
multimodal or cross-modal frameworks. For each, we evaluate reported
performance gains, clinical applicability, and limitations, while identifying
trade-offs and unresolved challenges that prior reviews have often overlooked.
Beyond these established techniques, we also highlight emerging directions
aimed at addressing current gaps. These include continual learning to enable
dynamic deployment, federated and privacy-preserving approaches to safeguard
sensitive data, hybrid self-supervised learning to enhance data efficiency,
data-centric pipelines that combine synthetic generation with human-in-the-loop
validation, and systematic benchmarking to assess robust generalization under
real-world clinical variability. By outlining these strategies and associated
research gaps, this review provides a roadmap for developing adaptive,
trustworthy, and clinically integrated FMs capable of meeting the demands of
real-world medical imaging.

</details>


### [126] [Detecting Generated Images by Fitting Natural Image Distributions](https://arxiv.org/abs/2511.01293)
*Yonggang Zhang,Jun Nie,Xinmei Tian,Mingming Gong,Kun Zhang,Bo Han*

Main category: cs.CV

TL;DR: 利用流形几何差异与自监督损失敏感性检测生成图像，并用归一化流放大差异，摆脱对大量生成样本的依赖，实验有效并开源。


<details>
  <summary>Details</summary>
Motivation: 现有二分类检测依赖大量高质量生成图像样本，难以泛化。作者认为自然与生成图像分布在流形几何上存在可利用的差异，可用于更稳健的检测。

Method: 设计两函数对（输出一致性+梯度子空间正交），用预训练自监督模型在自然图像上计算损失变化作为判别依据；当生成模型更逼近真实流形时，采用归一化流将生成图像挤出真实流形以放大差异。

Result: 大量实验证明方法有效，特别在高级生成器样本上通过归一化流增强后仍能保持较高检测性能；代码已开源。

Conclusion: 本文提出基于流形几何差异的生成图像检测方法，利用一对函数在自然图像上输出一致而在生成图像上输出相异，且梯度位于相互正交子空间，从而通过沿数据流形变换时自监督模型损失显著变化来判别生成图像，并用归一化流放大差异。

Abstract: The increasing realism of generated images has raised significant concerns
about their potential misuse, necessitating robust detection methods. Current
approaches mainly rely on training binary classifiers, which depend heavily on
the quantity and quality of available generated images. In this work, we
propose a novel framework that exploits geometric differences between the data
manifolds of natural and generated images. To exploit this difference, we
employ a pair of functions engineered to yield consistent outputs for natural
images but divergent outputs for generated ones, leveraging the property that
their gradients reside in mutually orthogonal subspaces. This design enables a
simple yet effective detection method: an image is identified as generated if a
transformation along its data manifold induces a significant change in the loss
value of a self-supervised model pre-trained on natural images. Further more,
to address diminishing manifold disparities in advanced generative models, we
leverage normalizing flows to amplify detectable differences by extruding
generated images away from the natural image manifold. Extensive experiments
demonstrate the efficacy of this method. Code is available at
https://github.com/tmlr-group/ConV.

</details>


### [127] [UniREditBench: A Unified Reasoning-based Image Editing Benchmark](https://arxiv.org/abs/2511.01295)
*Feng Han,Yibin Wang,Chenglin Li,Zheming Liang,Dianyi Wang,Yang Jiao,Zhipeng Wei,Chao Gong,Cheng Jin,Jingjing Chen,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出UniREditBench与10万合成推理数据并微调得到UniREdit-Bagel，为复杂推理相关的图像编辑提供更全面的评估与改进途径。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑基准过于集中于单物体属性变换且只用文本参考评估，忽视多对象交互、游戏世界规则以及仅文本评估在复杂推理场景下可能导致的系统性误判。

Method: 构建2,700条包含真实与游戏场景的多维评估集；引入多模态双参考评估（文本+图像真值）；设计自动化多场景数据合成流水线，生成带链式思维注释的10万条合成数据（UniREdit-Data-100K）；在该数据上微调Bagel，得到UniREdit-Bagel，并对多模型进行横向基准测试。

Result: 提出的基准覆盖8个主维度与18个子维度；多模态双参考提高评估可靠性；基于合成数据微调的UniREdit-Bagel在内外域均显著提升；通过基准揭示现有开闭源模型在不同推理方面的优劣。

Conclusion: 该论文提出了一个针对基于推理的图像编辑评估的统一基准UniREditBench，并通过合成数据与微调模型提升编辑模型在推理场景下的表现。

Abstract: Recent advances in multi-modal generative models have driven substantial
improvements in image editing. However, current generative models still
struggle with handling diverse and complex image editing tasks that require
implicit reasoning, underscoring the need for a comprehensive benchmark to
systematically assess their performance across various reasoning scenarios.
Existing benchmarks primarily focus on single-object attribute transformation
in realistic scenarios, which, while effective, encounter two key challenges:
(1) they largely overlook multi-object interactions as well as game-world
scenarios that involve human-defined rules, which are common in real-life
applications; (2) they only rely on textual references to evaluate the
generated images, potentially leading to systematic misjudgments, especially in
complex reasoning scenarios. To this end, this work proposes UniREditBench, a
unified benchmark for reasoning-based image editing evaluation. It comprises
2,700 meticulously curated samples, covering both real- and game-world
scenarios across 8 primary dimensions and 18 sub-dimensions. To improve
evaluation reliability, we introduce multimodal dual-reference evaluation,
providing both textual and ground-truth image references for each sample
assessment. Furthermore, we design an automated multi-scenario data synthesis
pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with
high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel
on this dataset and develop UniREdit-Bagel, demonstrating substantial
improvements in both in-domain and out-of-distribution settings. Through
thorough benchmarking of both open-source and closed-source image editing
models, we reveal their strengths and weaknesses across various aspects.

</details>


### [128] [REASON: Probability map-guided dual-branch fusion framework for gastric content assessment](https://arxiv.org/abs/2511.01302)
*Nu-Fnag Xiao,De-Xing Huang,Le-Tian Wang,Mei-Jiang Gui,Qi Fu,Xiao-Liang Xie,Shi-Qi Liu,Shuangyi Wang,Zeng-Guang Hou,Ying-Wei Wang,Xiao-Hu Zhou*

Main category: cs.CV

TL;DR: 提出一种两阶段概率图引导的双视图融合框架（REASON），通过分割生成概率图并融合RLD/SUP视图的双支路分类器，在自建数据集上显著优于现有方法，可用于自动化术前误吸风险评估。


<details>
  <summary>Details</summary>
Motivation: 动机是传统超声评估依赖手工描记和经验公式，效率低且准确性受限，需要自动化、鲁棒且高效的胃内容物评估方法以改善术前误吸风险分层。

Method: 方法包括：阶段1使用分割模型生成概率图以抑制伪影并突出胃解剖结构；阶段2采用来自RLD和SUP两视图的双支路分类器融合特征以提高判别能力。整体为端到端的概率图引导双分支融合框架。

Result: 在作者自建数据集上实验，REASON显著优于现有最先进方法，表现出更高的准确率和鲁棒性，适合用于术前自动化误吸风险评估。

Conclusion: 本文提出的REASON框架通过两阶段流程（概率图引导的分割 + 双支路视图融合分类），在胃内内容物超声评估上提高了鲁棒性与准确性，结论可信且具有临床应用潜力。

Abstract: Accurate assessment of gastric content from ultrasound is critical for
stratifying aspiration risk at induction of general anesthesia. However,
traditional methods rely on manual tracing of gastric antra and empirical
formulas, which face significant limitations in both efficiency and accuracy.
To address these challenges, a novel two-stage probability map-guided
dual-branch fusion framework (REASON) for gastric content assessment is
proposed. In stage 1, a segmentation model generates probability maps that
suppress artifacts and highlight gastric anatomy. In stage 2, a dual-branch
classifier fuses information from two standard views, right lateral decubitus
(RLD) and supine (SUP), to improve the discrimination of learned features.
Experimental results on a self-collected dataset demonstrate that the proposed
framework outperforms current state-of-the-art approaches by a significant
margin. This framework shows great promise for automated preoperative
aspiration risk assessment, offering a more robust, efficient, and accurate
solution for clinical practice.

</details>


### [129] [Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning Instance Disentangled Learning for WSI Representation](https://arxiv.org/abs/2511.01304)
*Chentao Li,Behzad Bozorgtabar,Yifang Ping,Pan Huang,Jing Qin*

Main category: cs.CV

TL;DR: 提出了一种三阶段实例解缠学习框架（潜在因子分组、聚类推理反事实解缠、实例重加权决策），显著提升WSI的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统MIL在处理WSI时存在实例之间的空间、语义与决策纠缠，限制了表示能力与可解释性，需要一种能解构这些纠缠并提供病理学一致性解释的方法。

Method: 方法包括三阶段：1) 正半定潜在因子分组，将实例映射到潜在子空间以缓解空间纠缠；2) 通过实例概率反事实推断与聚类推理进行实例解缠以缓解语义纠缠；3) 采用广义线性加权决策和实例效应重加权来解决决策纠缠。

Result: 在多中心数据集上的大量实验表明，该模型优于现有所有最先进方法，并通过解缠表示与透明决策流程实现与病理学家一致的可解释性。

Conclusion: 该论文提出了一个用于全切片图像（WSI）可解释表示的实例解缠框架，通过潜在因子分组、聚类推理的反事实推断与实例重加权，解决了MIL中的空间、语义和决策三类纠缠问题。

Abstract: Multiple instance learning (MIL) has been widely used for representing
whole-slide pathology images. However, spatial, semantic, and decision
entanglements among instances limit its representation and interpretability. To
address these challenges, we propose a latent factor grouping-boosted
cluster-reasoning instance disentangled learning framework for whole-slide
image (WSI) interpretable representation in three phases. First, we introduce a
novel positive semi-definite latent factor grouping that maps instances into a
latent subspace, effectively mitigating spatial entanglement in MIL. To
alleviate semantic entanglement, we employs instance probability counterfactual
inference and optimization via cluster-reasoning instance disentangling.
Finally, we employ a generalized linear weighted decision via instance effect
re-weighting to address decision entanglement. Extensive experiments on
multicentre datasets demonstrate that our model outperforms all
state-of-the-art models. Moreover, it attains pathologist-aligned
interpretability through disentangled representations and a transparent
decision-making process.

</details>


### [130] [Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models](https://arxiv.org/abs/2511.01307)
*Tae-Young Lee,Juwon Seo,Jong Hwan Ko,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 提出APDM框架，通过DPO损失和L2P双路径优化，在模型层面阻断扩散模型对特定主体的个性化，从而比图像级对抗扰动更鲁棒地防止未授权生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于对抗扰动的图像保护方法在存在少量干净图像或图像简单变换时失效，且假设不现实。作者因此将防护对象上移到扩散模型以提高鲁棒性。

Method: 提出Direct Protective Optimization（DPO）损失函数用于在不破坏生成质量的前提下破坏目标模型的主体个性化能力；同时设计双路径优化策略Learning to Protect（L2P），通过在个性化路径和保护路径之间交替优化，模拟未来个性化轨迹并在每一步自适应加强保护。

Result: 实验表明APDM在阻止未授权个性化方面优于现有方法，取得了最先进的性能，同时不损害生成质量。

Conclusion: 该论文提出将保护目标从图像转向扩散模型本身，通过在模型层面阻止针对特定主体的个性化，从而更稳健地防止未授权的个性化生成。

Abstract: Recent advances in diffusion models have enabled high-quality synthesis of
specific subjects, such as identities or objects. This capability, while
unlocking new possibilities in content creation, also introduces significant
privacy risks, as personalization techniques can be misused by malicious users
to generate unauthorized content. Although several studies have attempted to
counter this by generating adversarially perturbed samples designed to disrupt
personalization, they rely on unrealistic assumptions and become ineffective in
the presence of even a few clean images or under simple image transformations.
To address these challenges, we shift the protection target from the images to
the diffusion model itself to hinder the personalization of specific subjects,
through our novel framework called Anti-Personalized Diffusion Models (APDM).
We first provide a theoretical analysis demonstrating that a naive approach of
existing loss functions to diffusion models is inherently incapable of ensuring
convergence for robust anti-personalization. Motivated by this finding, we
introduce Direct Protective Optimization (DPO), a novel loss function that
effectively disrupts subject personalization in the target model without
compromising generative quality. Moreover, we propose a new dual-path
optimization strategy, coined Learning to Protect (L2P). By alternating between
personalization and protection paths, L2P simulates future personalization
trajectories and adaptively reinforces protection at each step. Experimental
results demonstrate that our framework outperforms existing methods, achieving
state-of-the-art performance in preventing unauthorized personalization. The
code is available at https://github.com/KU-VGI/APDM.

</details>


### [131] [MVSMamba: Multi-View Stereo with State Space Model](https://arxiv.org/abs/2511.01315)
*Jianfei Jiang,Qiankun Liu,Hongyuan Liu,Haochen Yu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: 将线性复杂度的Mamba架构应用于MVS，提出动态Mamba模块以实现参考中心动态扫描的跨视图及多尺度全局特征聚合，从而在精度和效率上均超过现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer-based MVS在捕捉长程依赖方面有效但计算复杂度为二次，难以在性能与效率间取得平衡。Mamba架构具备线性复杂度与全局建模能力，因而被用于提升MVS的效率与表现。

Method: 设计了基于Mamba的MVSMamba网络，核心为提出的动态Mamba模块（DM-module），采用reference-centered动态扫描策略以实现自视图与源视图间高效特征交互、全方位多视图特征表示以及多尺度全局特征聚合。

Result: 在DTU和Tanks-and-Temples基准上，MVSMamba在性能和效率上均优于现有最先进的MVS方法。

Conclusion: 本论文提出MVSMamba，将Mamba架构引入多视图立体重建（MVS），实现了高效的全局特征聚合并改善匹配性能。

Abstract: Robust feature representations are essential for learning-based Multi-View
Stereo (MVS), which relies on accurate feature matching. Recent MVS methods
leverage Transformers to capture long-range dependencies based on local
features extracted by conventional feature pyramid networks. However, the
quadratic complexity of Transformer-based MVS methods poses challenges to
balance performance and efficiency. Motivated by the global modeling capability
and linear complexity of the Mamba architecture, we propose MVSMamba, the first
Mamba-based MVS network. MVSMamba enables efficient global feature aggregation
with minimal computational overhead. To fully exploit Mamba's potential in MVS,
we propose a Dynamic Mamba module (DM-module) based on a novel
reference-centered dynamic scanning strategy, which enables: (1) Efficient
intra- and inter-view feature interaction from the reference to source views,
(2) Omnidirectional multi-view feature representations, and (3) Multi-scale
global feature aggregation. Extensive experimental results demonstrate MVSMamba
outperforms state-of-the-art MVS methods on the DTU dataset and the
Tanks-and-Temples benchmark with both superior performance and efficiency. The
source code is available at https://github.com/JianfeiJ/MVSMamba.

</details>


### [132] [A Generative Adversarial Approach to Adversarial Attacks Guided by Contrastive Language-Image Pre-trained Model](https://arxiv.org/abs/2511.01317)
*Sampriti Soor,Alik Pramanick,Jothiprakash K,Arijit Sur*

Main category: cs.CV

TL;DR: 利用CLIP语义对齐和集中式扰动生成方法，提出了一种在多目标场景下对多标签分类器进行高保真对抗攻击的生成式方法，实验表现良好。


<details>
  <summary>Details</summary>
Motivation: 动机是利用CLIP将自然语言语义与图像表示对齐的能力，以在多目标场景中生成更有效且视觉上难以察觉的对抗扰动，专门用于欺骗多标签分类器。

Method: 方法结合了CLIP的文本-图像对齐能力与引导损失，利用SSAE的集中扰动策略以及类似GAMA的不相似文本嵌入生成器，生成针对多目标场景的扰动。

Result: 在多种黑盒受害模型上测试，实验结果显示该方法在保持更高结构相似度的同时，能达到或优于现有方法的攻击效果。

Conclusion: 本文提出了一种基于CLIP的生成式对抗攻击方法，能够在保持高视觉保真度的同时，生成对多标签分类器具有欺骗性的扰动。

Abstract: The rapid growth of deep learning has brought about powerful models that can
handle various tasks, like identifying images and understanding language.
However, adversarial attacks, an unnoticed alteration, can deceive models,
leading to inaccurate predictions. In this paper, a generative adversarial
attack method is proposed that uses the CLIP model to create highly effective
and visually imperceptible adversarial perturbations. The CLIP model's ability
to align text and image representation helps incorporate natural language
semantics with a guided loss to generate effective adversarial examples that
look identical to the original inputs. This integration allows extensive scene
manipulation, creating perturbations in multi-object environments specifically
designed to deceive multilabel classifiers. Our approach integrates the
concentrated perturbation strategy from Saliency-based Auto-Encoder (SSAE) with
the dissimilar text embeddings similar to Generative Adversarial Multi-Object
Scene Attacks (GAMA), resulting in perturbations that both deceive
classification models and maintain high structural similarity to the original
images. The model was tested on various tasks across diverse black-box victim
models. The experimental results show that our method performs competitively,
achieving comparable or superior results to existing techniques, while
preserving greater visual fidelity.

</details>


### [133] [RDTE-UNet: A Boundary and Detail Aware UNet for Precise Medical Image Segmentation](https://arxiv.org/abs/2511.01328)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: RDTE-UNet结合ResBlock+Transformer骨干与ASBE、HVDA、EulerFF三模块，统一局部与全局建模以增强医学图像细节和边界分割，在Synapse与BUSI数据集上表现可比。


<details>
  <summary>Details</summary>
Motivation: 在医学图像中，解剖变异性大且边界模糊，传统方法难以稳定分割细小结构，故需一种在局部细节和全局上下文间取得平衡的模型以提高边界描绘与结构一致性。

Method: 提出基于ResBlock的detail-aware Transformer骨干，结合三模块：ASBE（自适应边界增强）、HVDA（细粒度特征建模）和EulerFF（基于欧拉公式的融合权重）。整体采用混合局部卷积与全局注意力机制，以增强边界和细节保留。

Result: 在Synapse与BUSI数据集上，RDTE-UNet在分割精度和边界质量上达到可比（或优）水平，表明该方法在不同形态、方向和尺度的结构上具有鲁棒性。

Conclusion: RDTE-UNet通过结合局部细节建模与全局上下文信息，能在解剖多样性和边界模糊场景下改善细小结构的分割和边界精度，展示出结构一致性和边界质量的提升。

Abstract: Medical image segmentation is essential for computer-assisted diagnosis and
treatment planning, yet substantial anatomical variability and boundary
ambiguity hinder reliable delineation of fine structures. We propose RDTE-UNet,
a segmentation network that unifies local modeling with global context to
strengthen boundary delineation and detail preservation. RDTE-UNet employs a
hybrid ResBlock detail-aware Transformer backbone and three modules: ASBE for
adaptive boundary enhancement, HVDA for fine-grained feature modeling, and
EulerFF for fusion weighting guided by Euler's formula. Together, these
components improve structural consistency and boundary accuracy across
morphology, orientation, and scale. On Synapse and BUSI dataset, RDTE-UNet has
achieved a comparable level in terms of segmentation accuracy and boundary
quality.

</details>


### [134] [$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles](https://arxiv.org/abs/2511.01340)
*Trishanu Das,Abhilash Nandy,Khush Bajaj,Deepiha S*

Main category: cs.CV

TL;DR: 提供1333题的Rebus数据集与RebusDescProgICE框架，通过描述+代码式推理和推理导向示例选择，大幅提升VLM在图谜理解任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 图谜需要图像识别与复杂的文字游戏推理，现有 VLM 在这类任务上不足，缺少大规模、多样化的数据集与有效推理范式。论文旨在通过数据集与新框架提升模型在图谜理解上的表现。

Method: 作者构建了包含1333个英文图谜、18类主题的数据集，并提出 RebusDescProgICE 框架：结合无结构的图像描述（description）、基于代码的结构化推理（program）和基于推理能力的示例选择（ICE）。他们在闭源与开源视觉-语言模型上进行对比实验，并与Chain-of-Thought进行比较。

Result: 在 |⟳[BUS]| 数据集上，RebusDescProgICE 相较于Chain-of-Thought，使闭源模型提升2.1–4.1%，开源模型提升20–30%。数据集覆盖18类、多样艺术风格，且提供分难度的基准。

Conclusion: 该论文提出了一个名为 |⟳[BUS]| 的大型中文注释：英文 Rebus（图谜）数据集与一种通用的模型增强框架 RebusDescProgICE。作者认为图谜理解需要视觉识别、常识推理和多步符号推理等多种能力，传统视觉-语言模型表现欠佳。

Abstract: Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters
to represent words or phrases creatively) requires a variety of skills such as
image recognition, cognitive skills, commonsense reasoning, multi-step
reasoning, image-based wordplay, etc., making this a challenging task for even
current Vision-Language Models. In this paper, we present
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$, a large and diverse
benchmark of $1,333$ English Rebus Puzzles containing different artistic styles
and levels of difficulty, spread across 18 categories such as food, idioms,
sports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a
model-agnostic framework which uses a combination of an unstructured
description and code-based, structured reasoning, along with better,
reasoning-based in-context example selection, improving the performance of
Vision-Language Models on
$\left|\,\circlearrowright\,\boxed{\text{BUS}}\,\right|$ by $2.1-4.1\%$ and
$20-30\%$ using closed-source and open-source models respectively compared to
Chain-of-Thought Reasoning.

</details>


### [135] [MIQ-SAM3D: From Single-Point Prompt to Multi-Instance Segmentation via Competitive Query Refinement](https://arxiv.org/abs/2511.01345)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: 提出MIQ-SAM3D：将单点提示扩展为多实例查询，结合CNN-Transformer的边界感知编码和竞争式解码，实现3D多病灶并行分割，实验显示性能可比且对提示鲁棒。


<details>
  <summary>Details</summary>
Motivation: 动机是解决现有SAM式交互分割通常采用单点对单对象范式，难以处理多病灶场景；同时ViT虽捕捉全局信息但缺乏高保真局部细节，限制医学图像中细小/边界清晰度需求。

Method: 方法包括(1) 提示条件实例查询生成器：将单点提示映射为多条专门化查询以检索语义相似的所有病灶；(2) 混合CNN-Transformer编码器：通过空间门控将CNN提取的边界显著性注入ViT自注意力，以增强局部细节；(3) 竞争式查询解码器：通过查询间竞争机制实现端到端并行多实例预测。

Result: 在LiTS17和KiTS21上，MIQ-SAM3D取得了与现有方法可比的性能，并表现出对提示位置和数目的较强鲁棒性，能高效标注多病灶病例。

Conclusion: MIQ-SAM3D提出了一种从单点提示扩展到多实例预测的三维交互分割框架，通过实例查询生成器、混合CNN-Transformer编码器和竞争式解码器，实现对多病灶的并行检索与分割。实验在LiTS17和KiTS21上表现出可比的分割性能和对提示鲁棒性的增强，具有临床注释效率提升潜力。

Abstract: Accurate segmentation of medical images is fundamental to tumor diagnosis and
treatment planning. SAM-based interactive segmentation has gained attention for
its strong generalization, but most methods follow a
single-point-to-single-object paradigm, which limits multi-lesion segmentation.
Moreover, ViT backbones capture global context but often miss high-fidelity
local details. We propose MIQ-SAM3D, a multi-instance 3D segmentation framework
with a competitive query optimization strategy that shifts from
single-point-to-single-mask to single-point-to-multi-instance. A
prompt-conditioned instance-query generator transforms a single point prompt
into multiple specialized queries, enabling retrieval of all semantically
similar lesions across the 3D volume from a single exemplar. A hybrid
CNN-Transformer encoder injects CNN-derived boundary saliency into ViT
self-attention via spatial gating. A competitively optimized query decoder then
enables end-to-end, parallel, multi-instance prediction through inter-query
competition. On LiTS17 and KiTS21 dataset, MIQ-SAM3D achieved comparable levels
and exhibits strong robustness to prompts, providing a practical solution for
efficient annotation of clinically relevant multi-lesion cases.

</details>


### [136] [Expanding the Content-Style Frontier: a Balanced Subspace Blending Approach for Content-Style LoRA Fusion](https://arxiv.org/abs/2511.01355)
*Linhao Huang*

Main category: cs.CV

TL;DR: 提出内容-风格子空间混合与平衡损失，扩大内容-风格前沿，在更高风格强度下仍能保持内容，IGD/GD显著降低。


<details>
  <summary>Details</summary>
Motivation: 观察到随着风格强度增加，生成图像的内容特征显著丢失，导致当前方法的内容-风格前沿受限，因此希望扩展前沿以提升风格多样性同时保持内容一致性。

Method: 构建内容-风格子空间并在生成过程中进行子空间混合，同时引入内容-风格平衡损失以约束在高风格强度下保留内容特征；训练或微调模型以优化该损失，从而实现更好的内容-风格权衡。

Result: 在定性和定量实验中，方法在不同风格强度下均能保持更高的内容相似性，显著降低IGD和GD，表明内容-风格权衡更优。

Conclusion: 该论文提出通过内容-风格子空间混合与内容-风格平衡损失扩展内容-风格前沿，能在不同风格强度下保持更好的内容相似性，实验证明在IGD和GD指标上优于现有方法。

Abstract: Recent advancements in text-to-image diffusion models have significantly
improved the personalization and stylization of generated images. However,
previous studies have only assessed content similarity under a single style
intensity. In our experiments, we observe that increasing style intensity leads
to a significant loss of content features, resulting in a suboptimal
content-style frontier. To address this, we propose a novel approach to expand
the content-style frontier by leveraging Content-Style Subspace Blending and a
Content-Style Balance loss. Our method improves content similarity across
varying style intensities, significantly broadening the content-style frontier.
Extensive experiments demonstrate that our approach outperforms existing
techniques in both qualitative and quantitative evaluations, achieving superior
content-style trade-off with significantly lower Inverted Generational Distance
(IGD) and Generational Distance (GD) scores compared to current methods.

</details>


### [137] [CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering](https://arxiv.org/abs/2511.01357)
*Qiangguo Jin,Xianyao Zheng,Hui Cui,Changming Sun,Yuqi Fang,Cong Cong,Ran Su,Leyi Wei,Ping Xuan,Junbo Wang*

Main category: cs.CV

TL;DR: 提出CMI-MTL：通过细粒度对齐、交叉模态交错表示和自由答案增强多任务学习，改善Med-VQA跨模态对齐和开放式回答能力，在多数据集上达成SOTA且具更好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有自注意力方法难以充分对齐视觉与语言语义，且分类式方法依赖预定义答案集，无法适应自由形式答案的多样性和细节语义，因此需要新的跨模态表示与自由答案增强策略。

Method: 提出三模块框架：FVTA用于图文细粒度对齐以定位相关区域，CIFR用于捕捉跨模态序列交互，FFAE通过将自由形式答案作为辅助任务提升开放式回答能力，整体采用交叉模态交互的多任务训练。

Result: 在VQA-RAD、SLAKE和OVQA三个数据集上，CMI-MTL优于现有SOTA方法；并通过可解释性实验验证了模型对相关图像区域与文本信息的有效捕获。

Conclusion: CMI-MTL通过细粒度对齐、交叉模态交错表示和自由文本增强多任务学习，显著提升了Med-VQA在开放式问答上的表现，并在多个数据集上超过现有最优方法。

Abstract: Medical visual question answering (Med-VQA) is a crucial multimodal task in
clinical decision support and telemedicine. Recent self-attention based methods
struggle to effectively handle cross-modal semantic alignments between vision
and language. Moreover, classification-based methods rely on predefined answer
sets. Treating this task as a simple classification problem may make it unable
to adapt to the diversity of free-form answers and overlook the detailed
semantic information of free-form answers. In order to tackle these challenges,
we introduce a Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)
framework that learns cross-modal feature representations from images and
texts. CMI-MTL comprises three key modules: fine-grained visual-text feature
alignment (FVTA), cross-modal interleaved feature representation (CIFR), and
free-form answer-enhanced multi-task learning (FFAE). FVTA extracts the most
relevant regions in image-text pairs through fine-grained visual-text feature
alignment. CIFR captures cross-modal sequential interactions via cross-modal
interleaved feature representation. FFAE leverages auxiliary knowledge from
open-ended questions through free-form answer-enhanced multi-task learning,
improving the model's capability for open-ended Med-VQA. Experimental results
show that CMI-MTL outperforms the existing state-of-the-art methods on three
Med-VQA datasets: VQA-RAD, SLAKE, and OVQA. Furthermore, we conduct more
interpretability experiments to prove the effectiveness. The code is publicly
available at https://github.com/BioMedIA-repo/CMI-MTL.

</details>


### [138] [EREBUS: End-to-end Robust Event Based Underwater Simulation](https://arxiv.org/abs/2511.01381)
*Hitesh Kyatham,Arjun Suresh,Aadi Palnitkar,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: 提出一个生成水下事件相机合成数据的流水线，并在岩石检测任务上验证其在低能见度环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在水下光照差、高动态范围和能见度差的环境中表现不佳，事件相机通过仅记录像素亮度变化可缓解这些问题，因此需要合成的数据来训练和评估基于事件相机的水下视觉算法。

Method: 作者设计并实现了一个仿真流水线，将事件相机模型与AUV运动和水下光学条件（如散射、吸收和悬浮颗粒）耦合，生成逼真的事件流数据用于训练视觉模型。

Result: 通过在岩石检测任务上的实验，流水线生成的数据能够提高模型在低能见度及悬浮颗粒环境下的检测性能，证明了方法的实用性与可推广性。

Conclusion: 本文提出了一个用于在水下环境中生成事件相机（event-based camera）合成数据的流水线，并展示了在能见度低和悬浮颗粒存在条件下用于岩石检测任务的有效性。

Abstract: The underwater domain presents a vast array of challenges for roboticists and
computer vision researchers alike, such as poor lighting conditions and high
dynamic range scenes. In these adverse conditions, traditional vision
techniques struggle to adapt and lead to suboptimal performance. Event-based
cameras present an attractive solution to this problem, mitigating the issues
of traditional cameras by tracking changes in the footage on a frame-by-frame
basis. In this paper, we introduce a pipeline which can be used to generate
realistic synthetic data of an event-based camera mounted to an AUV (Autonomous
Underwater Vehicle) in an underwater environment for training vision models. We
demonstrate the effectiveness of our pipeline using the task of rock detection
with poor visibility and suspended particulate matter, but the approach can be
generalized to other underwater tasks.

</details>


### [139] [SEPS: Semantic-enhanced Patch Slimming Framework for fine-grained cross-modal alignment](https://arxiv.org/abs/2511.01390)
*Xinyu Mao,Junsi Li,Haoji Zhang,Yu Liang,Ming Sun*

Main category: cs.CV

TL;DR: 提出SEPS，用稀疏与致密文本统一语义并进行相关性感知的patch筛选，显著提升跨模态细粒度对齐与检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理因模态信息密度差异导致的patch冗余与语义歧义；MLLM虽能生成丰富语义但文本致密度与原始稀疏caption冲突，且难以量化rich patch与concise text的语义相关性。

Method: 采用两阶段机制：一是从MLLM生成的致密文本与原始稀疏描述融合统一语义以识别显著补丁；二是基于相关性和均值计算的选择策略，强化关键patch-word对应并用于相似度评估。

Result: 在Flickr30K和MS-COCO上，SEPS在rSum上相比现有方法提升23%-86%，在text-to-image检索任务上提升尤为显著。

Conclusion: SEPS通过结合稀疏与致密文本语义并进行相关性感知的patch筛选，有效缓解了视觉补丁的冗余与模糊问题，显著提升了细粒度跨模态对齐与检索性能。

Abstract: Fine-grained cross-modal alignment aims to establish precise local
correspondences between vision and language, forming a cornerstone for visual
question answering and related multimodal applications. Current approaches face
challenges in addressing patch redundancy and ambiguity, which arise from the
inherent information density disparities across modalities. Recently,
Multimodal Large Language Models (MLLMs) have emerged as promising solutions to
bridge this gap through their robust semantic generation capabilities. However,
the dense textual outputs from MLLMs may introduce conflicts with the original
sparse captions. Furthermore, accurately quantifying semantic relevance between
rich visual patches and concise textual descriptions remains a core challenge.
To overcome these limitations, we introduce the Semantic-Enhanced Patch
Slimming (SEPS) framework, which systematically addresses patch redundancy and
ambiguity. Our approach employs a two-stage mechanism to integrate unified
semantics from both dense and sparse texts, enabling the identification of
salient visual patches. Additionally, it leverages relevance-aware selection
with mean value computation to highlight crucial patch-word correspondences,
thereby improving cross-modal similarity assessment. Comprehensive experiments
on Flickr30K and MS-COCO datasets validate that SEPS achieves superior
performance, surpassing existing approaches by 23\%-86\% in rSum across diverse
model architectures, with notable enhancements in text-to-image retrieval
scenarios. Our implementation is available at
https://github.com/Sweet4tars/seps.git.

</details>


### [140] [Semantic BIM enrichment for firefighting assets: Fire-ART dataset and panoramic image-based 3D reconstruction](https://arxiv.org/abs/2511.01399)
*Ya Wen,Yutong Qiao,Chi Chiu Lam,Ioannis Brilakis,Sanghoon Lee,Mun On Wong*

Main category: cs.CV

TL;DR: 发布了Fire-ART数据集并提出基于改进立方映射与球面投影的全景图到BIM的消防资产语义化重建方法，实测F1 73%/88%、定位误差0.620/0.428m。


<details>
  <summary>Details</summary>
Motivation: 传统方法在自动识别与重建消防资产方面效率低、能力有限，亟需高质量数据集和针对全景图的重建方法来实现快速、准确的资产语义化入BIM。

Method: 构建了包含15类消防资产的Fire-ART数据集（2626张图片、6627个实例）；提出基于全景图的重建流程，结合改进的立方映射和基于半径的球面相机投影用于提高识别与定位精度。

Result: 在两个真实案例验证中，方法分别达到了F1分数73%和88%，定位误差分别为0.620米和0.428米，表明该方法在识别与定位上具有实用性与精度。

Conclusion: 提出了一个新的全景图像到BIM的资产语义重建方法，并发布了Fire-ART消防资产数据集，为消防设备的数字化管理提供了数据+方法支持。

Abstract: Inventory management of firefighting assets is crucial for emergency
preparedness, risk assessment, and on-site fire response. However, conventional
methods are inefficient due to limited capabilities in automated asset
recognition and reconstruction. To address the challenge, this research
introduces the Fire-ART dataset and develops a panoramic image-based
reconstruction approach for semantic enrichment of firefighting assets into BIM
models. The Fire-ART dataset covers 15 fundamental assets, comprising 2,626
images and 6,627 instances, making it an extensive and publicly accessible
dataset for asset recognition. In addition, the reconstruction approach
integrates modified cube-map conversion and radius-based spherical camera
projection to enhance recognition and localization accuracy. Through
validations with two real-world case studies, the proposed approach achieves
F1-scores of 73% and 88% and localization errors of 0.620 and 0.428 meters,
respectively. The Fire-ART dataset and the reconstruction approach offer
valuable resources and robust technical solutions to enhance the accurate
digital management of fire safety equipment.

</details>


### [141] [Extremal Contours: Gradient-driven contours for compact visual attribution](https://arxiv.org/abs/2511.01411)
*Reza Karimzadeh,Albert Alonso,Frans Zdyb,Julius B. Kirkegaard,Bulat Ibragimov*

Main category: cs.CV

TL;DR: 用截断傅里叶表示的星状凸光滑轮廓替代稠密掩码，通过在保留/删除目标下用分类器梯度优化，生成单一连通、紧凑、鲁棒且训练免费的视觉解释，效果在ImageNet及DINO模型上显著优于若干基线。


<details>
  <summary>Details</summary>
Motivation: 动机是现有稠密扰动掩码往往碎片化、过拟合且需要后处理，且容易受对抗性掩码伪装。希望用更紧凑、稳健且可解释的轮廓化掩码来提升忠实性与一致性，同时降低参数复杂度和对后处理的依赖。

Method: 方法将星状凸（star-convex）区域参数化为截断傅里叶级数，并在极值保留/删除（extremal preserve/delete）目标下使用分类器梯度进行优化。通过限制为低维光滑轮廓，大幅减少自由参数，保证简单连通的掩码、稳定的边界更新，并通过显式面积控制生成重要性轮廓图。扩展到多轮廓可以在同一框架下定位多个对象。

Result: 在ImageNet与各种基线比较中，该方法在保持或匹配稠密掩码的极值忠实度的同时，输出更紧凑、可解释且运行间一致性更好的区域；对自监督DINO模型尤为明显，相关性质量提高超过15%，且在相关性质量（relevance mass）与复杂度（complexity）指标上均优于梯度和扰动基线。

Conclusion: 该论文提出了一种训练免费（training-free）的视觉模型解释方法，用可调光滑轮廓替代常见的稠密扰动掩码，从而生成单一、连通、紧凑且稳定的解释区域。

Abstract: Faithful yet compact explanations for vision models remain a challenge, as
commonly used dense perturbation masks are often fragmented and overfitted,
needing careful post-processing. Here, we present a training-free explanation
method that replaces dense masks with smooth tunable contours. A star-convex
region is parameterized by a truncated Fourier series and optimized under an
extremal preserve/delete objective using the classifier gradients. The approach
guarantees a single, simply connected mask, cuts the number of free parameters
by orders of magnitude, and yields stable boundary updates without cleanup.
Restricting solutions to low-dimensional, smooth contours makes the method
robust to adversarial masking artifacts. On ImageNet classifiers, it matches
the extremal fidelity of dense masks while producing compact, interpretable
regions with improved run-to-run consistency. Explicit area control also
enables importance contour maps, yielding a transparent fidelity-area profiles.
Finally, we extend the approach to multi-contour and show how it can localize
multiple objects within the same framework. Across benchmarks, the method
achieves higher relevance mass and lower complexity than gradient and
perturbation based baselines, with especially strong gains on self-supervised
DINO models where it improves relevance mass by over 15% and maintains positive
faithfulness correlations.

</details>


### [142] [Towards One-step Causal Video Generation via Adversarial Self-Distillation](https://arxiv.org/abs/2511.01419)
*Yongqi Yang,Huayang Huang,Xu Peng,Xiaobin Hu,Donghao Luo,Jiangning Zhang,Chengjie Wang,Yu Wu*

Main category: cs.CV

TL;DR: 提出ASD+FFE的蒸馏框架，使因果视频扩散模型在1-2步内高效稳定生成，克服误差累积并支持多步数灵活推理。


<details>
  <summary>Details</summary>
Motivation: 现有混合自回归时序与扩散空间去噪的方法推理过程迭代且易累积误差，导致推理慢与质量下降。目标是在极少去噪步数下保持高质量并加速推理。

Method: 在Distribution Matching Distillation(DMD)基础上引入Adversarial Self-Distillation(ASD)，将学生模型n步输出与(n+1)步输出在分布层面对齐，结合对抗训练以提供更平滑和更具信息性的监督；同时提出First-Frame Enhancement(FFE)，对首帧分配更多去噪步数以缓解误差累积，对后续帧采用更大跳步。

Result: 在VBench数据集上，作者的方法在一步和两步视频生成任务上均超越了最新方法；生成模型为单一蒸馏模型，支持多种推理步数设置，省去反复蒸馏并高效合成高质量视频。

Conclusion: 该论文提出了一种基于蒸馏的高效因果视频生成框架，通过对学生模型不同步数去噪输出进行分布级对齐，从而在极少步数（1-2步）下仍保持高质量生成，且支持一个模型对应多种推理步数。

Abstract: Recent hybrid video generation models combine autoregressive temporal
dynamics with diffusion-based spatial denoising, but their sequential,
iterative nature leads to error accumulation and long inference times. In this
work, we propose a distillation-based framework for efficient causal video
generation that enables high-quality synthesis with extremely limited denoising
steps. Our approach builds upon the Distribution Matching Distillation (DMD)
framework and proposes a novel Adversarial Self-Distillation (ASD) strategy,
which aligns the outputs of the student model's n-step denoising process with
its (n+1)-step version at the distribution level. This design provides smoother
supervision by bridging small intra-student gaps and more informative guidance
by combining teacher knowledge with locally consistent student behavior,
substantially improving training stability and generation quality in extremely
few-step scenarios (e.g., 1-2 steps). In addition, we present a First-Frame
Enhancement (FFE) strategy, which allocates more denoising steps to the initial
frames to mitigate error propagation while applying larger skipping steps to
later frames. Extensive experiments on VBench demonstrate that our method
surpasses state-of-the-art approaches in both one-step and two-step video
generation. Notably, our framework produces a single distilled model that
flexibly supports multiple inference-step settings, eliminating the need for
repeated re-distillation and enabling efficient, high-quality video synthesis.

</details>


### [143] [UniSOT: A Unified Framework for Multi-Modality Single Object Tracking](https://arxiv.org/abs/2511.01427)
*Yinchao Ma,Yuyang Tang,Wenfei Yang,Tianzhu Zhang,Xu Zhou,Feng Wu*

Main category: cs.CV

TL;DR: 提出UniSOT，一个统一且参数共享的单目标跟踪器，能同时支持多种参考与视频模态，在多项基准上显著优于专用方法。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪器通常只针对少数视频或参考模态设计，导致模型分裂、实际应用受限。为满足实际场景中多样化的人机交互需求与复杂视频感知需求，亟需一个能同时处理多种参考与视频模态的统一跟踪器。

Method: 作者设计了一个统一架构（未在摘要给出具体细节），能处理不同的参考输入与视频输入，并在18个跟踪基准上进行训练与测试，从而在单模态、视觉-语言和RGB+X跟踪任务上共享参数并协同工作。

Result: 在18个基准测试上，UniSOT优于模态专用方法：在TNL2K数据集上对三种参考模态的平均AUC提升超过3.0%；在多种RGB+X视频模态上相较Un-Track在主要指标上提升超过2.0%。

Conclusion: 该论文提出了一个统一的单目标跟踪器UniSOT，能在相同参数下支持三种参考模态（边界框、自然语言或两者）与四种视频模态（RGB、RGB+Depth、RGB+Thermal、RGB+Event）的组合，实现多模态、多任务的跟踪统一。

Abstract: Single object tracking aims to localize target object with specific reference
modalities (bounding box, natural language or both) in a sequence of specific
video modalities (RGB, RGB+Depth, RGB+Thermal or RGB+Event.). Different
reference modalities enable various human-machine interactions, and different
video modalities are demanded in complex scenarios to enhance tracking
robustness. Existing trackers are designed for single or several video
modalities with single or several reference modalities, which leads to separate
model designs and limits practical applications. Practically, a unified tracker
is needed to handle various requirements. To the best of our knowledge, there
is still no tracker that can perform tracking with these above reference
modalities across these video modalities simultaneously. Thus, in this paper,
we present a unified tracker, UniSOT, for different combinations of three
reference modalities and four video modalities with uniform parameters.
Extensive experimental results on 18 visual tracking, vision-language tracking
and RGB+X tracking benchmarks demonstrate that UniSOT shows superior
performance against modality-specific counterparts. Notably, UniSOT outperforms
previous counterparts by over 3.0\% AUC on TNL2K across all three reference
modalities and outperforms Un-Track by over 2.0\% main metric across all three
RGB+X video modalities.

</details>


### [144] [Terrain-Enhanced Resolution-aware Refinement Attention for Off-Road Segmentation](https://arxiv.org/abs/2511.01434)
*Seongkyu Choi,Jhonghyun An*

Main category: cs.CV

TL;DR: 提出一种在低分辨率瓶颈进行大部分计算、用门控交叉注意力注入高分辨率细节并对不确定像素稀疏精炼的分辨率感知token解码器；配合边界带一致性正则化，提升越野语义分割的边界保真、局部一致性及噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 越野语义分割面临边界粗糙且不一致、罕见类别监督稀疏及标签噪声普遍的问题。低分辨率融合会模糊边界并传播局部错误，而保持或重复高分辨率融合代价高且对噪声敏感。故需一种在计算成本、边界保真与对噪声鲁棒性之间取得权衡的解码器设计。

Method: 方法包括：1) 在低分辨率瓶颈处进行大规模计算并使用全局自注意力恢复全局语义；2) 轻量级膨胀深度可分离卷积用于局部精炼以恢复局部一致性；3) 门控交叉注意力从高分辨率编码器流中选择性引入细尺度特征以避免噪声放大；4) 类别感知点精炼对不确定像素进行稀疏修正；5) 训练期间引入边界带一致性正则化以在注释边缘附近鼓励预测连贯性。

Result: 在不同过渡场景中，该方法表现出竞争力的性能以及更好的稳定性，尤其在边界保真和对标注噪声的鲁棒性方面有改善。

Conclusion: 该论文提出一种分辨率感知的token解码器，旨在在不完美标注下在低分辨率瓶颈处进行大部分计算，通过门控交叉注意力注入细节，并仅对不确定像素进行稀疏细化，从而在全局语义、局部一致性和边界保真之间取得平衡。

Abstract: Off-road semantic segmentation suffers from thick, inconsistent boundaries,
sparse supervision for rare classes, and pervasive label noise. Designs that
fuse only at low resolution blur edges and propagate local errors, whereas
maintaining high-resolution pathways or repeating high-resolution fusions is
costly and fragile to noise. We introduce a resolutionaware token decoder that
balances global semantics, local consistency, and boundary fidelity under
imperfect supervision. Most computation occurs at a low-resolution bottleneck;
a gated cross-attention injects fine-scale detail, and only a sparse,
uncertainty-selected set of pixels is refined. The components are co-designed
and tightly integrated: global self-attention with lightweight dilated
depthwise refinement restores local coherence; a gated cross-attention
integrates fine-scale features from a standard high-resolution encoder stream
without amplifying noise; and a class-aware point refinement corrects residual
ambiguities with negligible overhead. During training, we add a boundary-band
consistency regularizer that encourages coherent predictions in a thin
neighborhood around annotated edges, with no inference-time cost. Overall, the
results indicate competitive performance and improved stability across
transitions.

</details>


### [145] [Contrast-Guided Cross-Modal Distillation for Thermal Object Detection](https://arxiv.org/abs/2511.01435)
*SiWoo Kim,JhongHyun An*

Main category: cs.CV

TL;DR: 在训练阶段通过实例级特征聚合/分离和多层RGB教师蒸馏，强化热红外检测器的语义与判别边界，测试时保持单模态推理，从而在夜间热红外检测上取得SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 热红外夜间检测受低对比度与高频信息缺失影响，导致重复重叠框、小目标漏检与类别混淆。现有方法要么通过图像翻译（TIR->RGB）引入像素级伪彩色导致脆弱性，要么在测试时依赖RGB融合增加传感器与计算开销，均未直接塑造热模态的检测表征。作者希望在不增加推理复杂度的前提下，从训练阶段改善热模态特征。

Method: 提出两类训练专用目标：1) 实例级对比式损失（或中心/聚类约束），将同类目标的特征拉近、异类特征推远，以减少重复和混淆检测；2) 多层特征蒸馏，通过对齐学生（热红外）金字塔特征与在RGB上训练的教师网络的多尺度语义表示，注入纹理和语义先验，增强热图像的表征能力。训练阶段使用这些辅助损失，测试阶段仅保留单一热红外检测器，无需RGB输入。

Result: 在若干热红外检测数据集与场景下，所提方法优于先前基于图像翻译或多模态融合的方法，实现了更少的重复框、更高的小目标召回率与更低的类别混淆，整体达到或超越最先进方法的检测性能。

Conclusion: 本文提出了一种训练时的目标函数，通过类别内特征聚合与类别间特征分离并结合从RGB教师网络蒸馏的多层语义先验，提升热红外检测器在夜间的鲁棒性，在单模态推理下减少重复框、漏检和分类混淆，从而实现了优于先前方法的性能并达到了最先进水平。

Abstract: Robust perception at night remains challenging for thermal-infrared
detection: low contrast and weak high-frequency cues lead to duplicate,
overlapping boxes, missed small objects, and class confusion. Prior remedies
either translate TIR to RGB and hope pixel fidelity transfers to detection --
making performance fragile to color or structure artifacts -- or fuse RGB and
TIR at test time, which requires extra sensors, precise calibration, and higher
runtime cost. Both lines can help in favorable conditions, but do not directly
shape the thermal representation used by the detector. We keep mono-modality
inference and tackle the root causes during training. Specifically, we
introduce training-only objectives that sharpen instance-level decision
boundaries by pulling together features of the same class and pushing apart
those of different classes -- suppressing duplicate and confusing detections --
and that inject cross-modal semantic priors by aligning the student's
multi-level pyramid features with an RGB-trained teacher, thereby strengthening
texture-poor thermal features without visible input at test time. In
experiments, our method outperformed prior approaches and achieved
state-of-the-art performance.

</details>


### [146] [Privacy Preserving Ordinal-Meta Learning with VLMs for Fine-Grained Fruit Quality Prediction](https://arxiv.org/abs/2511.01449)
*Riddhi Jain,Manasi Patwardhan,Aayush Mishra,Parijat Deshpande,Beena Rai*

Main category: cs.CV

TL;DR: 提出MAOML，结合序数损失与元学习，提升小型开源VLM在水果新鲜度预测上的少样本和零样本性能，平均准确率92.71%。


<details>
  <summary>Details</summary>
Motivation: 获取细粒度的水果新鲜度标签成本高，导致数据稀缺；闭源大模型（如Gemini）虽效果好，但因隐私和企业使用限制无法普及；现有开源VLM性能不足且少量数据微调效果有限，因此需一种能在小数据下提升开源模型性能的方法。

Method: 提出了一种模型无关的序数元学习算法（MAOML），将序数回归和元学习框架结合，利用视觉-语言模型的小样本适配能力，通过设计元任务、任务内外优化及序数损失来学习更鲁棒的表征；可在零样本和少样本场景中应用。

Result: 在水果新鲜度分类任务的零样本和少样本设置下，MAOML相比直接微调和现有开源VLM显著提升性能，平均准确率达到92.71%，并达到行业可接受标准；论文还展示了方法对不同水果和样本规模的稳健性。

Conclusion: 本文提出的MAOML方法通过结合元学习和序数信息，针对小型视觉-语言模型在生鲜水果保鲜期分类任务中进行训练，解决了数据稀缺问题，显著提高了开源模型在零样本与少样本设置下的表现，最终在所有水果上平均达到92.71%的行业标准准确率。

Abstract: To effectively manage the wastage of perishable fruits, it is crucial to
accurately predict their freshness or shelf life using non-invasive methods
that rely on visual data. In this regard, deep learning techniques can offer a
viable solution. However, obtaining fine-grained fruit freshness labels from
experts is costly, leading to a scarcity of data. Closed proprietary Vision
Language Models (VLMs), such as Gemini, have demonstrated strong performance in
fruit freshness detection task in both zero-shot and few-shot settings.
Nonetheless, food retail organizations are unable to utilize these proprietary
models due to concerns related to data privacy, while existing open-source VLMs
yield sub-optimal performance for the task. Fine-tuning these open-source
models with limited data fails to achieve the performance levels of proprietary
models. In this work, we introduce a Model-Agnostic Ordinal Meta-Learning
(MAOML) algorithm, designed to train smaller VLMs. This approach utilizes
meta-learning to address data sparsity and leverages label ordinality, thereby
achieving state-of-the-art performance in the fruit freshness classification
task under both zero-shot and few-shot settings. Our method achieves an
industry-standard accuracy of 92.71%, averaged across all fruits.
  Keywords: Fruit Quality Prediction, Vision Language Models, Meta Learning,
Ordinal Regression

</details>


### [147] [Reg-DPO: SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation](https://arxiv.org/abs/2511.01450)
*Jie Du,Xinyu Gong,Qingshan Tan,Wen Li,Yangming Cheng,Weitao Wang,Chenlu Zhan,Suhui Wu,Hao Zhang,Jun Zhang*

Main category: cs.CV

TL;DR: 提出GT-Pair自动构建偏好对与Reg-DPO正则化DPO，并结合内存优化扩展训练容量，显著提升视频生成效果并省去人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法多沿用图像领域范式、在小规模模型上开发，难以应对视频任务的数据构建成本高、训练不稳定和内存消耗大的问题，亟需无监督高效且可扩展的优化方法。

Method: 构建GT-Pair（真实视频为正、模型生成视频为负）自动生成偏好对；将SFT损失作为正则项并入DPO目标（Reg-DPO）；在FSDP基础上结合多种内存优化技术扩展训练容量近3倍。

Result: 在I2V和T2V多数据集实验中，方法在视频生成质量上持续优于现有方法，训练容量和稳定性得到显著提升。

Conclusion: 本文提出的GT-Pair和Reg-DPO结合多项内存优化与FSDP，显著提升了视频生成模型的训练稳定性和生成质量，且无需外部标注。

Abstract: Recent studies have identified Direct Preference Optimization (DPO) as an
efficient and reward-free approach to improving video generation quality.
However, existing methods largely follow image-domain paradigms and are mainly
developed on small-scale models (approximately 2B parameters), limiting their
ability to address the unique challenges of video tasks, such as costly data
construction, unstable training, and heavy memory consumption. To overcome
these limitations, we introduce a GT-Pair that automatically builds
high-quality preference pairs by using real videos as positives and
model-generated videos as negatives, eliminating the need for any external
annotation. We further present Reg-DPO, which incorporates the SFT loss as a
regularization term into the DPO objective to enhance training stability and
generation fidelity. Additionally, by combining the FSDP framework with
multiple memory optimization techniques, our approach achieves nearly three
times higher training capacity than using FSDP alone. Extensive experiments on
both I2V and T2V tasks across multiple datasets demonstrate that our method
consistently outperforms existing approaches, delivering superior video
generation quality.

</details>


### [148] [When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA](https://arxiv.org/abs/2511.01458)
*Dennis Pierantozzi,Luca Carlini,Mauro Orazio Drago,Chiara Lena,Cesare Hassan,Elena De Momi,Danail Stoyanov,Sophia Bano,Mobarak I. Hoque*

Main category: cs.CV

TL;DR: 针对手术VQA的安全问题，提出QA-SNNE通过问题对齐的文本最近邻语义熵估计不确定性，显著提升零-shot LVLM的AUROC与幻觉检测，增强临床可信度。


<details>
  <summary>Details</summary>
Motivation: 手术用VQA对安全性与可靠性要求高，以往研究多关注准确性或语言质量，忽视了模糊识别、上报专家或触发复议等安全行为；受自动故障检测（AFD）启发，将不确定性估计作为实现更安全决策的关键手段。

Method: 提出Question Aligned Semantic Nearest Neighbor Entropy（QA-SNNE），在医疗文本嵌入空间中计算基于问题条件的语义熵；通过将生成答案与最近邻文本比较得到不确定性分数，作为黑盒置信度估计器。评估五个模型（3个LVLMs，2个PEFT），在EndoVis18-VQA与PitVQA数据集上对比模板内与模板外鲁棒性以及对轻微改写的敏感性。

Result: QA-SNNE在多数模板内场景下提高了AUROC；对零-shot模型，AUROC提升15-38%，且在模板外压力测试下保持提升；还改善了模型的幻觉检测能力。PEFT模型对轻微问题改写性能下降，LVLMs更具鲁棒性。

Conclusion: 该论文提出了一种面向手术场景的VQA不确定性估计方法QA-SNNE，将问题语义融入置信度评估，从而提高安全性与故障检测能力。实验表明在零-shot大模型上显著改善AUROC并增强幻觉检测，结合LVLM骨干可提升临床可信度。

Abstract: Safety and reliability are essential for deploying Visual Question Answering
(VQA) in surgery, where incorrect or ambiguous responses can harm the patient.
Most surgical VQA research focuses on accuracy or linguistic quality while
overlooking safety behaviors such as ambiguity awareness, referral to human
experts, or triggering a second opinion. Inspired by Automatic Failure
Detection (AFD), we study uncertainty estimation as a key enabler of safer
decision making. We introduce Question Aligned Semantic Nearest Neighbor
Entropy (QA-SNNE), a black box uncertainty estimator that incorporates question
semantics into prediction confidence. It measures semantic entropy by comparing
generated answers with nearest neighbors in a medical text embedding space,
conditioned on the question. We evaluate five models, including domain specific
Parameter-Efficient Fine-Tuned (PEFT) models and zero-shot Large
Vision-Language Models (LVLMs), on EndoVis18-VQA and PitVQA. PEFT models
degrade under mild paraphrasing, while LVLMs are more resilient. Across three
LVLMs and two PEFT baselines, QA-SNNE improves AUROC in most in-template
settings and enhances hallucination detection. The Area Under the ROC Curve
(AUROC) increases by 15-38% for zero-shot models, with gains maintained under
out-of-template stress. QA-SNNE offers a practical and interpretable step
toward AFD in surgical VQA by linking semantic uncertainty to question context.
Combining LVLM backbones with question aligned uncertainty estimation can
improve safety and clinician trust. The code and model are available at
https://github.com/DennisPierantozzi/QASNNE

</details>


### [149] [Efficiently Training A Flat Neural Network Before It has been Quantizated](https://arxiv.org/abs/2511.01462)
*Peng Xia,Junbiao Pang,Tianyang Cai*

Main category: cs.CV

TL;DR: 通过将AQE与WQE视为独立高斯噪声并用噪声注入优化得到平坦全精度模型，从而提高低位PTQ效果。


<details>
  <summary>Details</summary>
Motivation: 现有PTQ方法忽视了训练好的神经网络与量化模型间的关系，导致显著量化误差；需要高效训练对特定低位量化鲁棒的模型。

Method: 本文将激活量化误差(AQE)与权重量化误差(WQE)分别统计建模为独立高斯噪声，并通过注入噪声的优化方法寻找平坦极小值来预调模型。

Result: 实验表明，所提框架能有效减少量化误差并提高低位PTQ模型的性能。

Conclusion: 本文提出通过造就平坦的全精度模型来降低低位量化误差，从而提升PTQ性能。

Abstract: Post-training quantization (PTQ) for vision transformers (ViTs) has garnered
significant attention due to its efficiency in compressing models. However,
existing methods typically overlook the relationship between a well-trained NN
and the quantized model, leading to considerable quantization error for PTQ.
However, it is unclear how to efficiently train a model-agnostic neural network
which is tailored for a predefined precision low-bit model. In this paper, we
firstly discover that a flat full precision neural network is crucial for
low-bit quantization. To achieve this, we propose a framework that proactively
pre-conditions the model by measuring and disentangling the error sources.
Specifically, both the Activation Quantization Error (AQE) and the Weight
Quantization Error (WQE) are statistically modeled as independent Gaussian
noises. We study several noise injection optimization methods to obtain a flat
minimum. Experimental results attest to the effectiveness of our approach.
These results open novel pathways for obtaining low-bit PTQ models.

</details>


### [150] [HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA](https://arxiv.org/abs/2511.01463)
*Lei Hu,Yongjing Ye,Shihong Xia*

Main category: cs.CV

TL;DR: 提出HMVLM：基于MoE LoRA的多专家门控微调框架，加入零专家保留预训练能力，并用按身体部位的token化改进pose表示，旨在在不丢失语言能力的情况下融合3D人体动作实现优秀的多模态下游性能。


<details>
  <summary>Details</summary>
Motivation: 随着指令调优数据扩展，基础语言模型在指令遵循上改进，但将语义丰富的3D人体动作整合进这些模型时存在模态差距，带来灾难性遗忘风险；同时，自回归兼容且能泛化到异质下游任务的姿态表征仍是关键技术瓶颈。论文旨在同时解决这两方面问题，实现动作-视觉-语言统一建模。

Method: 采用Mixture of Expert Low-Rank Adaption（MoE LoRA）策略：1）门控网络根据输入提示动态选择和加权不同LoRA专家，允许在单一模型中同步处理多种任务；2）引入零专家（zero expert）保持预训练参数不变，用以保留语言能力，减轻灾难性遗忘；3）人体动作的表示采用按身体部位（关节组）划分的token化，以提高空间分辨率并兼容自回归生成。

Result: 实验表明，HMVLM在指令调优过程中有效缓解了知识遗忘问题，并在多种人体动作下游任务上取得显著性能提升，验证了MoE LoRA与零专家及按部位token化策略的有效性。

Conclusion: 该论文提出了HMVLM，一种基于MoE LoRA的统一框架，通过门控网络按提示动态分配LoRA专家权重，实现多任务同步微调，同时引入零专家以缓解指令调优期间的灾难性遗忘，并通过按身体部位分组的token化提高关节空间分辨率。整体目标是将语义丰富的三维人体动作与大基础语言模型融合，保留语言能力并提升多模态理解与跨模态生成能力。

Abstract: The expansion of instruction-tuning data has enabled foundation language
models to exhibit improved instruction adherence and superior performance
across diverse downstream tasks. Semantically-rich 3D human motion is being
progressively integrated with these foundation models to enhance multimodal
understanding and cross-modal generation capabilities. However, the modality
gap between human motion and text raises unresolved concerns about catastrophic
forgetting during this integration. In addition, developing
autoregressive-compatible pose representations that preserve generalizability
across heterogeneous downstream tasks remains a critical technical barrier. To
address these issues, we propose the Human Motion-Vision-Language Model
(HMVLM), a unified framework based on the Mixture of Expert Low-Rank
Adaption(MoE LoRA) strategy. The framework leverages the gating network to
dynamically allocate LoRA expert weights based on the input prompt, enabling
synchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting
during instruction-tuning, we introduce a novel zero expert that preserves the
pre-trained parameters for general linguistic tasks. For pose representation,
we implement body-part-specific tokenization by partitioning the human body
into different joint groups, enhancing the spatial resolution of the
representation. Experiments show that our method effectively alleviates
knowledge forgetting during instruction-tuning and achieves remarkable
performance across diverse human motion downstream tasks.

</details>


### [151] [SecDiff: Diffusion-Aided Secure Deep Joint Source-Channel Coding Against Adversarial Attacks](https://arxiv.org/abs/2511.01466)
*Changyuan Zhao,Jiacheng Wang,Ruichen Zhang,Dusit Niyato,Hongyang Du,Zehui Xiong,Dong In Kim,Ping Zhang*

Main category: cs.CV

TL;DR: SecDiff 用伪逆引导采样与自适应权重加速扩散解码，结合子载波掩蔽和 EM 驱动盲信道估计，有效提升深度 JSCC 在导频欺骗与子载波阻塞下的鲁棒性与低延迟实现。


<details>
  <summary>Details</summary>
Motivation: 现有深度 JSCC 在面对物理层对抗（如导频欺骗、子载波阻塞）时语义保真度下降，且已有扩散引导方法推理延迟高，需一种兼顾鲁棒性与低延迟的通用可插拔解决方案。

Method: 主要方法包括：1) 伪逆引导采样与自适应引导权重用于加速扩散重构；2) 基于功率的子载波掩蔽，将干扰下的恢复转化为掩蔽修补问题并用扩散引导求解；3) 对于导频欺骗，将信道估计表述为盲逆问题，设计 EM 驱动的交替恢复算法，在扩散过程中交替优化导频恢复与信道估计。

Result: 在 OFDM 信道对抗场景下，SecDiff 在重构质量与计算成本之间取得更好折中，优于现有安全或生成式 JSCC 基线方法，表现为较高的语义保真度与较低的运行延迟。

Conclusion: SecDiff 是一种基于扩散模型的解码框架，通过伪逆引导采样和自适应引导权重实现低延迟高效语义重构，同时结合子载波掩蔽和 EM 驱动的盲信道估计算法，提高了对干扰（子载波阻塞）和导频欺骗攻击的鲁棒性。

Abstract: Deep joint source-channel coding (JSCC) has emerged as a promising paradigm
for semantic communication, delivering significant performance gains over
conventional separate coding schemes. However, existing JSCC frameworks remain
vulnerable to physical-layer adversarial threats, such as pilot spoofing and
subcarrier jamming, compromising semantic fidelity. In this paper, we propose
SecDiff, a plug-and-play, diffusion-aided decoding framework that significantly
enhances the security and robustness of deep JSCC under adversarial wireless
environments. Different from prior diffusion-guided JSCC methods that suffer
from high inference latency, SecDiff employs pseudoinverse-guided sampling and
adaptive guidance weighting, enabling flexible step-size control and efficient
semantic reconstruction. To counter jamming attacks, we introduce a power-based
subcarrier masking strategy and recast recovery as a masked inpainting problem,
solved via diffusion guidance. For pilot spoofing, we formulate channel
estimation as a blind inverse problem and develop an expectation-minimization
(EM)-driven reconstruction algorithm, guided jointly by reconstruction loss and
a channel operator. Notably, our method alternates between pilot recovery and
channel estimation, enabling joint refinement of both variables throughout the
diffusion process. Extensive experiments over orthogonal frequency-division
multiplexing (OFDM) channels under adversarial conditions show that SecDiff
outperforms existing secure and generative JSCC baselines by achieving a
favorable trade-off between reconstruction quality and computational cost. This
balance makes SecDiff a promising step toward practical, low-latency, and
attack-resilient semantic communications.

</details>


### [152] [EPAN: Robust Pedestrian Re-Identification via Enhanced Alignment Network for IoT Surveillance](https://arxiv.org/abs/2511.01498)
*Zhiyang Jia,Hongyan Cui,Ge Gao,Bo Li,Minjie Zhang,Zishuo Gao,Huiwen Huang,Caisheng Zhuo*

Main category: cs.CV

TL;DR: EPAN通过双分支对齐与多尺度特征融合，提高了IoT监控场景下的行人重识别性能，在公开数据集上取得了显著结果并已开源。


<details>
  <summary>Details</summary>
Motivation: IoT环境下摄像头分布广、视角多变、光照与遮挡复杂，传统ReID方法在跨摄像头一致性和对齐能力上不足，需一种鲁棒的对齐和特征提取方法。

Method: 提出了双分支网络结构：一支用于提取尺度不变的全局特征，另一支专注于视角/透视校正的对齐信息；两分支特征融合后用于最终ReID判别。网络可能引入多尺度池化、注意力机制和对齐模块以应对视角与环境变化。

Result: 在Inspection-Personnel数据集上达到了Rank-1=90.09%和mAP=78.82%，表明方法在精确检索和排序上效果优秀，代码与数据已开源。

Conclusion: EPAN在IoT监控场景下能显著提升行人重识别性能，尤其在Inspection-Personnel数据集上表现优异，证明其在实际多摄像头环境中具有应用潜力。

Abstract: Person re-identification (ReID) plays a pivotal role in computer vision,
particularly in surveillance and security applications within IoT-enabled smart
environments. This study introduces the Enhanced Pedestrian Alignment Network
(EPAN), tailored for robust ReID across diverse IoT surveillance conditions.
EPAN employs a dual-branch architecture to mitigate the impact of perspective
and environmental changes, extracting alignment information under varying
scales and viewpoints. Here, we demonstrate EPAN's strong feature extraction
capabilities, achieving outstanding performance on the Inspection-Personnel
dataset with a Rank-1 accuracy of 90.09% and a mean Average Precision (mAP) of
78.82%. This highlights EPAN's potential for real-world IoT applications,
enabling effective and reliable person ReID across diverse cameras in
surveillance and security systems. The code and data are available at:
https://github.com/ggboy2580/EPAN

</details>


### [153] [SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation](https://arxiv.org/abs/2511.01501)
*Yufeng Jin,Niklas Funk,Vignesh Prasad,Zechu Li,Mathias Franzius,Jan Peters,Georgia Chalvatzaki*

Main category: cs.CV

TL;DR: 提出在SE(3)流匹配的6D位姿概率估计方法，通过样本估计建模多峰位姿分布，在基准数据集上达SOTA并能用于不确定性感知与抓取。


<details>
  <summary>Details</summary>
Motivation: Deterministic networks are overconfident and fail to model pose ambiguity from occlusions and symmetries; a probabilistic, sample-based approach on SE(3) better captures multi-modality and uncertainty.

Method: They propose a flow-matching approach on the SE(3) manifold to learn sample-based pose distributions, replacing deterministic regression with a probabilistic model that generates multiple pose hypotheses. Likely involves manifold-aware flows, loss functions aligning distributions, and sampling strategies for downstream use.

Result: State-of-the-art results on Real275, YCB-V, LM-O; improved handling of ambiguous/symmetric objects; benefits for active perception and uncertainty-aware grasping.

Conclusion: This paper introduces a probabilistic SE(3) flow-matching framework for 6D object pose estimation, effectively modeling multi-modal pose distributions and improving uncertainty handling. It demonstrates state-of-the-art performance on standard benchmarks and practical benefits for downstream robotic tasks.

Abstract: Object pose estimation is a fundamental problem in robotics and computer
vision, yet it remains challenging due to partial observability, occlusions,
and object symmetries, which inevitably lead to pose ambiguity and multiple
hypotheses consistent with the same observation. While deterministic deep
networks achieve impressive performance under well-constrained conditions, they
are often overconfident and fail to capture the multi-modality of the
underlying pose distribution. To address these challenges, we propose a novel
probabilistic framework that leverages flow matching on the SE(3) manifold for
estimating 6D object pose distributions. Unlike existing methods that regress a
single deterministic output, our approach models the full pose distribution
with a sample-based estimate and enables reasoning about uncertainty in
ambiguous cases such as symmetric objects or severe occlusions. We achieve
state-of-the-art results on Real275, YCB-V, and LM-O, and demonstrate how our
sample-based pose estimates can be leveraged in downstream robotic manipulation
tasks such as active perception for disambiguating uncertain viewpoints or
guiding grasp synthesis in an uncertainty-aware manner.

</details>


### [154] [Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning](https://arxiv.org/abs/2511.01502)
*Mengtan Zhang,Zizhan Guo,Hongbo Zhao,Yi Feng,Zuyi Xiong,Yue Wang,Shaoyi Du,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: DiMoDE通过对运动分量的几何对齐与判别式约束，实现更强的深度与自我运动联合估计，显著提升鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度与自我运动学习将自我运动视为辅助任务或混合各种运动类型，导致几何约束弱、在多样条件下鲁棒性差。

Method: 提出DiMoDE框架：先对源/目标相机的光轴和成像平面进行对齐，变换光流并分别对各运动分量施加几何约束，将联合学习转化为共轴(coaxial)和共面(coplanar)形式，并通过闭式几何关系互相推导深度与平移分量。

Result: DiMoDE在多个公共数据集及一份新采集的多样化真实数据集上取得领先性能，尤其在挑战条件下表现突出。

Conclusion: 本文提出通过对不同运动分量的判别式处理，利用各自刚性光流的几何规律来提升深度和自我运动估计，从而改进鲁棒性与可靠性。

Abstract: Unsupervised learning of depth and ego-motion, two fundamental 3D perception
tasks, has made significant strides in recent years. However, most methods
treat ego-motion as an auxiliary task, either mixing all motion types or
excluding depth-independent rotational motions in supervision. Such designs
limit the incorporation of strong geometric constraints, reducing reliability
and robustness under diverse conditions. This study introduces a discriminative
treatment of motion components, leveraging the geometric regularities of their
respective rigid flows to benefit both depth and ego-motion estimation. Given
consecutive video frames, network outputs first align the optical axes and
imaging planes of the source and target cameras. Optical flows between frames
are transformed through these alignments, and deviations are quantified to
impose geometric constraints individually on each ego-motion component,
enabling more targeted refinement. These alignments further reformulate the
joint learning process into coaxial and coplanar forms, where depth and each
translation component can be mutually derived through closed-form geometric
relationships, introducing complementary constraints that improve depth
robustness. DiMoDE, a general depth and ego-motion joint learning framework
incorporating these designs, achieves state-of-the-art performance on multiple
public datasets and a newly collected diverse real-world dataset, particularly
under challenging conditions. Our source code will be publicly available at
mias.group/DiMoDE upon publication.

</details>


### [155] [Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement](https://arxiv.org/abs/2511.01510)
*Derong Kong,Zhixiong Yang,Shengxi Li,Shuaifeng Zhi,Li Liu,Zhen Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 将LLIE从确定性映射转为基于功率律分层亮度的统计采样与扩散前向过程，实现更强的无监督泛化与有参考时的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于像素级确定性映射，忽视了真实环境中亮度连续变化的物理过程，导致在无正常光照参考的情况下泛化性能下降；基于对自然亮度动态的经验分析，发现亮度过渡服从功率律分布，从而提出统计化建模。

Method: 引入亮度感知统计量化（LASQ），以功率律分布近似自然亮度过渡，使用分层功率函数进行分段逼近，并设计了扩散式前向过程以在亮度层间自动搜索最优过渡路径，实现了无参考的分布仿真与采样恢复。

Result: 在有无正常光照参考的场景下均表现良好：无参考时提高了跨场景泛化与实际情况的恢复能力；有参考时在特定数据集上取得更好性能，同时对未见数据集也表现出更强的泛化能力。

Conclusion: 该论文提出了LASQ框架，通过将低光照增强任务从确定性像素映射转为基于分层亮度分布的统计采样过程，有效提升了无监督场景下的泛化能力与可适应性。

Abstract: Low-light image enhancement (LLIE) faces persistent challenges in balancing
reconstruction fidelity with cross-scenario generalization. While existing
methods predominantly focus on deterministic pixel-level mappings between
paired low/normal-light images, they often neglect the continuous physical
process of luminance transitions in real-world environments, leading to
performance drop when normal-light references are unavailable. Inspired by
empirical analysis of natural luminance dynamics revealing power-law
distributed intensity transitions, this paper introduces Luminance-Aware
Statistical Quantification (LASQ), a novel framework that reformulates LLIE as
a statistical sampling process over hierarchical luminance distributions. Our
LASQ re-conceptualizes luminance transition as a power-law distribution in
intensity coordinate space that can be approximated by stratified power
functions, therefore, replacing deterministic mappings with probabilistic
sampling over continuous luminance layers. A diffusion forward process is
designed to autonomously discover optimal transition paths between luminance
layers, achieving unsupervised distribution emulation without normal-light
references. In this way, it considerably improves the performance in practical
situations, enabling more adaptable and versatile light restoration. This
framework is also readily applicable to cases with normal-light references,
where it achieves superior performance on domain-specific datasets alongside
better generalization-ability across non-reference datasets.

</details>


### [156] [Example-Based Feature Painting on Textures](https://arxiv.org/abs/2511.01513)
*Andrei-Timotei Ardelean,Tim Weyrich*

Main category: cs.CV

TL;DR: 提出无需标注的异常检测+自动聚类+条件扩散生成流水线，实现对有局部瑕疵纹理的可控、可交互编辑与无限扩展生成。


<details>
  <summary>Details</summary>
Motivation: 自然界中大量材料表面存在局部外观变化，要生成真实感纹理必须能合成这些痕迹并允许用户可控编辑，但人工标注成本高且难以概括多样性，因此采用无监督检测与学习来自动发现并利用这些特征。

Method: 方法基于无监督异常检测从无标注样本中检测外观改变特征，随后对这些特征进行自动聚类以得到语义一致的类别，并基于这些类别训练条件生成模型；引入扩散（diffusion）基础的编辑算法和无限平稳纹理生成方法。

Result: 从少量图像出发，构建出一个交互式生成模型，用户可在任意尺寸纹理上绘制与创建特征；提出的扩散编辑与无限纹理生成算法具有通用性，可应用于其他场景。

Conclusion: 本文提出了一个完整的系统，实现了对具有局部特征（如污渍、破洞、磨损、褪色等）的纹理进行可控生成与编辑。

Abstract: In this work, we propose a system that covers the complete workflow for
achieving controlled authoring and editing of textures that present distinctive
local characteristics. These include various effects that change the surface
appearance of materials, such as stains, tears, holes, abrasions,
discoloration, and more. Such alterations are ubiquitous in nature, and
including them in the synthesis process is crucial for generating realistic
textures. We introduce a novel approach for creating textures with such
blemishes, adopting a learning-based approach that leverages unlabeled
examples. Our approach does not require manual annotations by the user;
instead, it detects the appearance-altering features through unsupervised
anomaly detection. The various textural features are then automatically
clustered into semantically coherent groups, which are used to guide the
conditional generation of images. Our pipeline as a whole goes from a small
image collection to a versatile generative model that enables the user to
interactively create and paint features on textures of arbitrary size. Notably,
the algorithms we introduce for diffusion-based editing and infinite stationary
texture generation are generic and should prove useful in other contexts as
well. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html

</details>


### [157] [NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation](https://arxiv.org/abs/2511.01517)
*Serkan Ozturk,Samet Hicsonmez,Pinar Duygulu*

Main category: cs.CV

TL;DR: NSYNC用负合成图像+梯度正交化的对比训练，去除共有属性干扰，提升文本到图像扩散模型的风格化能力。


<details>
  <summary>Details</summary>
Motivation: 现有条件文本生成模型虽能产生逼真图像，但难以精确捕捉特定艺术风格；单纯微调在目标风格数据上往往无法学到风格特征。作者利用合成数据构建对比学习信号，迫使模型区分目标风格与相近但不属于目标的属性。

Method: 利用文本到图像扩散模型生成与目标风格相似但非目标风格的负合成数据；在训练中同时输入正（真实目标风格）和负样本，分别计算梯度；将正梯度在负梯度上的投影去除，只保留正梯度的正交分量用于参数更新。

Result: 在多位画家和插画家的风格迁移任务上，NSYNC在定量指标和定性视觉效果上均优于基线方法，证明了通过负合成样本与梯度正交化能更好地提取独特风格特征。

Conclusion: 该论文提出NSYNC，一种基于对比学习的训练框架，通过生成“负”合成图像并在梯度空间中去除与负样本共享的成分，从而增强大规模文本到图像扩散模型的风格化能力。

Abstract: Current text conditioned image generation methods output realistic looking
images, but they fail to capture specific styles. Simply finetuning them on the
target style datasets still struggles to grasp the style features. In this
work, we present a novel contrastive learning framework to improve the
stylization capability of large text-to-image diffusion models. Motivated by
the astonishing advance in image generation models that makes synthetic data an
intrinsic part of model training in various computer vision tasks, we exploit
synthetic image generation in our approach. Usually, the generated synthetic
data is dependent on the task, and most of the time it is used to enlarge the
available real training dataset. With NSYNC, alternatively, we focus on
generating negative synthetic sets to be used in a novel contrastive training
scheme along with real positive images. In our proposed training setup, we
forward negative data along with positive data and obtain negative and positive
gradients, respectively. We then refine the positive gradient by subtracting
its projection onto the negative gradient to get the orthogonal component,
based on which the parameters are updated. This orthogonal component eliminates
the trivial attributes that are present in both positive and negative data and
directs the model towards capturing a more unique style. Experiments on various
styles of painters and illustrators show that our approach improves the
performance over the baseline methods both quantitatively and qualitatively.
Our code is available at https://github.com/giddyyupp/NSYNC.

</details>


### [158] [Driving scenario generation and evaluation using a structured layer representation and foundational models](https://arxiv.org/abs/2511.01541)
*Arthur Hubert,Gamal Elghazaly,Raphaël Frank*

Main category: cs.CV

TL;DR: 提出五层结构化驾驶场景模型，结合大模型生成稀有场景，并用多样性与原创性指标评估生成数据的质量与相关性。


<details>
  <summary>Details</summary>
Motivation: 罕见驾驶场景难以采集，需通过模拟或生成方法扩充数据以推进自动驾驶研发；为更好地评估与生成此类场景，提出更细化的结构化表示并配套指标。

Method: 构建五层层级表示（引入每个参与者的子类别与特征），使用大基础模型进行数据增强生成，基于该层模型学习嵌入并采用两种指标（多样性与原创性）评估合成数据，同时进行定性合成视频展示。

Result: 展示了在不同生成设置下多样性与原创性指标的表现，并通过定性视频验证合成场景的可用性，提供代码与扩展结果。

Conclusion: 本文提出了一个结构化的五层驾驶场景模型，并结合大模型生成与评价罕见驾驶场景，结果显示该方法能生成多样且与真实数据有一定原创性的合成场景。

Abstract: Rare and challenging driving scenarios are critical for autonomous vehicle
development. Since they are difficult to encounter, simulating or generating
them using generative models is a popular approach. Following previous efforts
to structure driving scenario representations in a layer model, we propose a
structured five-layer model to improve the evaluation and generation of rare
scenarios. We use this model alongside large foundational models to generate
new driving scenarios using a data augmentation strategy. Unlike previous
representations, our structure introduces subclasses and characteristics for
every agent of the scenario, allowing us to compare them using an embedding
specific to our layer-model. We study and adapt two metrics to evaluate the
relevance of a synthetic dataset in the context of a structured representation:
the diversity score estimates how different the scenarios of a dataset are from
one another, while the originality score calculates how similar a synthetic
dataset is from a real reference set. This paper showcases both metrics in
different generation setup, as well as a qualitative evaluation of synthetic
videos generated from structured scenario descriptions. The code and extended
results can be found at https://github.com/Valgiz/5LMSG.

</details>


### [159] [PCD-ReID: Occluded Person Re-Identification for Base Station Inspection](https://arxiv.org/abs/2511.01546)
*Ge Gao,Zishuo Gao,Hongyan Cui,Zhiyang Jia,Zhuang Luo,ChaoPeng Liu*

Main category: cs.CV

TL;DR: 提出Transformer为核心的PCD-ReID，通过组件级特征与大规模巡逻数据训练，有效提升遮挡行人重识别性能，具备实境部署价值。


<details>
  <summary>Details</summary>
Motivation: 传统ResNet基的ReID方法在遮挡场景下效果差，实际监控数据分布与公开数据不同，需新方法和数据提升鲁棒性。

Method: 提出基于Transformer的PCD网络，聚焦提取共享组件特征（如头盔、制服），并通过新收集的巡逻监控数据进行训练以减轻过拟合。

Result: 在新数据和对比实验中，PCD-ReID达成mAP=79.0%、Rank-1=82.7%，Rank-1相较ResNet50提升15.9%。

Conclusion: PCD-ReID在塔检场景下能显著提高遮挡行人重识别性能，尤其在Rank-1上较ResNet50提升明显，具备实用部署潜力。

Abstract: Occluded pedestrian re-identification (ReID) in base station environments is
a critical task in computer vision, particularly for surveillance and security
applications. This task faces numerous challenges, as occlusions often obscure
key body features, increasing the complexity of identification. Traditional
ResNet-based ReID algorithms often fail to address occlusions effectively,
necessitating new ReID methods. We propose the PCD-ReID (Pedestrian Component
Discrepancy) algorithm to address these issues. The contributions of this work
are as follows: To tackle the occlusion problem, we design a Transformer-based
PCD network capable of extracting shared component features, such as helmets
and uniforms. To mitigate overfitting on public datasets, we collected new
real-world patrol surveillance images for model training, covering six months,
10,000 individuals, and over 50,000 images. Comparative experiments with
existing ReID algorithms demonstrate that our model achieves a mean Average
Precision (mAP) of 79.0% and a Rank-1 accuracy of 82.7%, marking a 15.9% Rank-1
improvement over ResNet50-based methods. Experimental evaluations indicate that
PCD-ReID effectively achieves occlusion-aware ReID performance for personnel in
tower inspection scenarios, highlighting its potential for practical deployment
in surveillance and security applications.

</details>


### [160] [NOA: a versatile, extensible tool for AI-based organoid analysis](https://arxiv.org/abs/2511.01549)
*Mikhail Konov,Lion J. Gleiter,Khoa Co,Monica Yabal,Tingying Peng*

Main category: cs.CV

TL;DR: NOA: an open-source napari plugin providing an accessible GUI that consolidates multiple AI tools for comprehensive organoid image analysis, validated on three biological case studies.


<details>
  <summary>Details</summary>
Motivation: Existing AI tools for organoid microscopy are limited in accessibility and task scope, creating a need for an accessible, general-purpose GUI that enables biologists without programming skills to perform comprehensive AI-driven analysis.

Method: NOA integrates multiple state-of-the-art algorithms as modules within a napari plugin, providing GUIs for detection, segmentation, tracking, feature extraction, custom annotation, and ML-based feature prediction; it interfaces with various algorithms and is open-source for flexibility.

Result: Through three case studies—morphological quantification during differentiation, phototoxicity assessment, and prediction of viability/differentiation—NOA demonstrates comprehensive and effective AI-driven organoid image analysis in an accessible, extensible framework.

Conclusion: NOA is a versatile, user-friendly napari plugin that makes AI-based organoid image analysis accessible to non-programmers, integrating detection, segmentation, tracking, feature extraction, annotation and ML prediction into one extensible GUI.

Abstract: AI tools can greatly enhance the analysis of organoid microscopy images, from
detection and segmentation to feature extraction and classification. However,
their limited accessibility to biologists without programming experience
remains a major barrier, resulting in labor-intensive and largely manual
workflows. Although a few AI models for organoid analysis have been developed,
most existing tools remain narrowly focused on specific tasks. In this work, we
introduce the Napari Organoid Analyzer (NOA), a general purpose graphical user
interface to simplify AI-based organoid analysis. NOA integrates modules for
detection, segmentation, tracking, feature extraction, custom feature
annotation and ML-based feature prediction. It interfaces multiple
state-of-the-art algorithms and is implemented as an open-source napari plugin
for maximal flexibility and extensibility. We demonstrate the versatility of
NOA through three case studies, involving the quantification of morphological
changes during organoid differentiation, assessment of phototoxicity effects,
and prediction of organoid viability and differentiation state. Together, these
examples illustrate how NOA enables comprehensive, AI-driven organoid image
analysis within an accessible and extensible framework.

</details>


### [161] [PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model](https://arxiv.org/abs/2511.01571)
*Wenqi Liang,Gan Sun,Yao He,Jiahua Dong,Suyan Dai,Ivan Laptev,Salman Khan,Yang Cong*

Main category: cs.CV

TL;DR: PixelVLA引入像素级推理与视觉提示，通过Pixel-160K数据和新的微调框架，在更低预训练成本下显著提升操控成功率，增强VLA的精度与灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有Vision-Language-Action模型在像素级场景理解能力弱，且过度依赖文本提示，限制了现实场景中的灵活性与精度。

Method: 提出一种多尺度像素感知编码器和视觉提示编码器的视动指令微调框架；设计两阶段自动注释流水线，从机器人数据生成Pixel-160K像素级标注；在三个VLA基准与两个模型变体上对比实验。

Result: 在三个标准VLA基准与两个模型变体上，PixelVLA相比OpenVLA将操作成功率提高了10.1%至17.8%，同时仅需OpenVLA 1.5%的预训练成本。

Conclusion: PixelVLA通过引入像素级编码器与视觉提示编码器，并利用自动标注生成Pixel-160K数据集，显著提升了VLA在像素级理解与多模态提示下的操控性能。

Abstract: Vision-Language-Action models (VLAs) are emerging as powerful tools for
learning generalizable visuomotor control policies. However, current VLAs are
mostly trained on large-scale image-text-action data and remain limited in two
key ways: (i) they struggle with pixel-level scene understanding, and (ii) they
rely heavily on textual prompts, which reduces their flexibility in real-world
settings. To address these challenges, we introduce PixelVLA, the first VLA
model designed to support both pixel-level reasoning and multimodal prompting
with text and visual inputs. Our approach is built on a new visuomotor
instruction tuning framework that integrates a multiscale pixel-aware encoder
with a visual prompting encoder. To train PixelVLA effectively, we further
propose a two-stage automated annotation pipeline that generates Pixel-160K, a
large-scale dataset with pixel-level annotations derived from existing robot
data. Experiments on three standard VLA benchmarks and two VLA model variants
show that PixelVLA improves manipulation success rates by 10.1%-17.8% over
OpenVLA, while requiring only 1.5% of its pretraining cost. These results
demonstrate that PixelVLA can be integrated into existing VLAs to enable more
accurate, efficient, and versatile robot control in complex environments. The
dataset and code will be released as open source.

</details>


### [162] [Generative Adversarial Synthesis and Deep Feature Discrimination of Brain Tumor MRI Images](https://arxiv.org/abs/2511.01574)
*Md Sumon Ali,Muzammil Behzad*

Main category: cs.CV

TL;DR: 该工作用DC-GAN生成合成脑MRI并用CNN验证其分类效果，结果显示合成图像可在分类任务中替代部分真实数据。


<details>
  <summary>Details</summary>
Motivation: 由于真实MRI数据有限且获取难度大，研究者希望通过生成逼真合成医学影像来扩充数据集，以改进深度学习在医学图像分析中的表现。

Method: 采用Deep Convolutional GAN生成合成MRI图像，并使用CNN分类器对真实与合成图像进行分类比较，用分类性能评估合成图像质量。

Result: 实验结果表明，用合成图像训练的CNN在脑肿瘤分类任务上表现与用真实图像训练的模型相当，证明GAN生成的数据具有可用性。

Conclusion: 本文展示了使用DC-GAN生成MRI脑肿瘤图像并用CNN评估其在分类任务中的实用性，结论是GAN生成图像在下游分类任务中可达到与真实图像相当的性能，从而缓解了数据不足问题。

Abstract: Compared to traditional methods, Deep Learning (DL) becomes a key technology
for computer vision tasks. Synthetic data generation is an interesting use case
for DL, especially in the field of medical imaging such as Magnetic Resonance
Imaging (MRI). The need for this task since the original MRI data is limited.
The generation of realistic medical images is completely difficult and
challenging. Generative Adversarial Networks (GANs) are useful for creating
synthetic medical images. In this paper, we propose a DL based methodology for
creating synthetic MRI data using the Deep Convolutional Generative Adversarial
Network (DC-GAN) to address the problem of limited data. We also employ a
Convolutional Neural Network (CNN) classifier to classify the brain tumor using
synthetic data and real MRI data. CNN is used to evaluate the quality and
utility of the synthetic images. The classification result demonstrates
comparable performance on real and synthetic images, which validates the
effectiveness of GAN-generated images for downstream tasks.

</details>


### [163] [Wave-Particle (Continuous-Discrete) Dualistic Visual Tokenization for Unified Understanding and Generation](https://arxiv.org/abs/2511.01593)
*Yizhu Chen,Chen Ju,Zhicheng Wang,Shuai Xiao,Xu Chen,Jinsong Lan,Xiaoyong Zhu,Ying Chen*

Main category: cs.CV

TL;DR: 提出CDD-VT：一种基于自适应分配量化基元数量的视觉标记器，通过在连续与离散之间动态平衡，提升重建、检索与分类性能，同时降低工程复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型中连续标记器与离散标记器之间的矛盾：前者性能好但工程复杂，后者简洁但信息丢失导致性能下降。

Method: 提出Dualistic Visual Tokenizer，包括两个关键模块：Diverse Quantitative Primitives（促进基元正交以扩展信息覆盖）和Dynamic Primitive Allocator（根据样本复杂度自适应分配基元数量）。基于量化码本对图像进行表征，复杂图像使用更多基元，简单图像使用更少基元，从而在连续与离散之间平滑切换。

Result: 在重建、检索和分类任务上，CDD-VT优于专门的连续或离散方法，且在实现上更简洁、可扩展，兼顾理解与生成能力。

Conclusion: CDD-VT通过在连续和离散视觉标记化之间建立自适应中间地带，成功缓解了两者的各自缺陷，兼顾理解与生成任务，高效且可扩展。

Abstract: The unification of understanding and generation within a single multi-modal
large model (MLLM) remains one significant challenge, largely due to the
dichotomy between continuous and discrete visual tokenizations. Continuous
tokenizer (CT) achieves strong performance by bridging multiple
independently-trained understanding modules and generation modules, but suffers
from complex multi-stage pipelines and substantial engineering overhead.
Conversely, discrete tokenizers (DT) offer a conceptually elegant idea by
quantizing each image into a primitive, but inevitably leading to information
loss and performance degradation. To resolve this tension, we question the
binary choice between CT and DT, inspired by the wave-particle duality of
light, and propose the Continuous-Discrete Dualistic Visual Tokenizer (CDD-VT).
We treat visual data as a flexible composition of image primitives derived from
quantized codebooks, with the crucial insight that the primitive number
assigned to each visual sample is adaptively determined according to its
complexity: simple instances use a few primitives, emulating discrete
tokenization, while complex instances use many, approximating continuous
tokenization. Two core components are designed: Diverse Quantitative
Primitives, which encourage primitives orthogonality to better populate
information space, and Dynamic Primitive Allocator, which assesses sample
complexity to determine the optimal set of primitives. Extensive experiments on
reconstruction, retrieval and classification show that CDD-VT achieves superior
performance over to specialized CT and DT, effectively getting strong result
within a concise and scalable MLLM.

</details>


### [164] [Lite ENSAM: a lightweight cancer segmentation model for 3D Computed Tomography](https://arxiv.org/abs/2511.01600)
*Agnar Martin Bjørnstad,Elias Stenhede,Arian Ranjbar*

Main category: cs.CV

TL;DR: 提出一款面向RECIST注释的轻量化ENSAM模型，实现了可接受的分割性能与较低的计算资源需求，适合在CPU上进行高效体积分割。


<details>
  <summary>Details</summary>
Motivation: 传统RECIST基于单平面最长直径，体积测量更能可靠反映治疗效果，但人工体积分割耗时，限制了临床应用；因此需要轻量、高效的自动体积分割方法。

Method: 在ENSAM基础上进行架构简化与参数压缩，优化计算和内存效率，使之在CPU上也能较快推理；使用RECIST标注作为弱监督信号训练体积分割模型，参加MICCAI FLARE 2025任务并进行评估。

Result: 在MICCAI FLARE 2025公开验证集和隐藏测试集中，Lite ENSAM在隐藏测试集上取得DSC=60.7%、NSD=63.6%；在公开验证集上CPU平均内存占用约50.6 GB、平均推理时间14.4 s。

Conclusion: 本文提出了Lite ENSAM，一种轻量化的ENSAM架构改进，用于从带有RECIST标注的CT影像高效进行肿瘤体积分割。

Abstract: Accurate tumor size measurement is a cornerstone of evaluating cancer
treatment response. The most widely adopted standard for this purpose is the
Response Evaluation Criteria in Solid Tumors (RECIST) v1.1, which relies on
measuring the longest tumor diameter in a single plane. However, volumetric
measurements have been shown to provide a more reliable assessment of treatment
effect. Their clinical adoption has been limited, though, due to the
labor-intensive nature of manual volumetric annotation. In this paper, we
present Lite ENSAM, a lightweight adaptation of the ENSAM architecture designed
for efficient volumetric tumor segmentation from CT scans annotated with RECIST
annotations. Lite ENSAM was submitted to the MICCAI FLARE 2025 Task 1:
Pan-cancer Segmentation in CT Scans, Subtask 2, where it achieved a Dice
Similarity Coefficient (DSC) of 60.7% and a Normalized Surface Dice (NSD) of
63.6% on the hidden test set, and an average total RAM time of 50.6 GBs and an
average inference time of 14.4 s on CPU on the public validation dataset.

</details>


### [165] [DINO-MX: A Modular & Flexible Framework for Self-Supervised Learning](https://arxiv.org/abs/2511.01610)
*Mahmut Selman Gokmen,Cody Bumgardner*

Main category: cs.CV

TL;DR: DINO-MX 是一个配置驱动的、模块化的自监督视觉模型训练框架，兼容 DINO 系列思想与 Hugging Face，支持节约算力的训练策略与分布式训练，并在多数据集上实现了有竞争力的性能与更低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有自监督视觉训练流程在灵活性、领域通用性与计算成本上存在不足，限制了跨域和不同资源环境下的可用性。目标是构建一个可复现、可扩展且计算高效的训练框架，便于在多种场景中开发与基准测试 VFMs。

Method: 将 DINO 系列（DINO、DINOv2、DINOv3）的核心原理整合到统一配置系统，支持多种 transformer 架构、单/多通道图像输入；实现训练策略包括 LoRA、层冻结、知识蒸馏；支持 DDP 与 FSDP 的分布式训练；提供解释性工具与标签引导的数据增强用于注意力定位。

Result: 在多种数据集上的实验显示 DINO-MX 在性能上具有竞争力，同时显著降低了计算资源开销；标签引导的数据增强能在不引入额外检测/分割头的情况下改善基于注意力的定位效果；框架与 Hugging Face 兼容，支持可复现的训练与扩展。

Conclusion: DINO-MX 提供了一个模块化、配置驱动的自监督视觉模型训练框架，兼容多种 Transformer 架构与 Hugging Face 生态，支持 LoRA、层冻结、蒸馏及分布式训练，并能处理自然与专用数据类型。实验证明在多样数据集上其性能有竞争力且计算成本显著降低，同时提供可解释性工具与基于标签的增强提升注意力定位。

Abstract: Vision Foundation Models (VFMs) have advanced representation learning through
self-supervised methods. However, existing training pipelines are often
inflexible, domain-specific, or computationally expensive, which limits their
usability across different domains and resource settings. DINO-MX is a modular
and extensible training framework that combines the core principles of DINO,
DINOv2 and DINOv3 within a unified configuration-driven system. It supports a
variety of transformer-based architectures and is fully compatible with the
Hugging Face ecosystem. The framework includes multiple training strategies
such as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,
along with support for distributed training through both Distributed Data
Parallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed to
work with both natural and specialized data types, including single- and
multi-channel images. Experimental results on diverse datasets show that
DINO-MX achieves competitive performance while significantly reducing
computational costs. Additionally, it offers interpretability tools and a
label-guided data augmentation method that improves attention-based
localization without the need for extra detection or segmentation heads.
DINO-MX provides a reproducible and scalable foundation for developing,
adapting, and benchmarking self-supervised vision models across a range of
research and real-world applications.

</details>


### [166] [Benchmark-Ready 3D Anatomical Shape Classification](https://arxiv.org/abs/2511.01613)
*Tomáš Krsička,Tibor Kubík*

Main category: cs.CV

TL;DR: 提出PSPooling用于可并行可逆的结构保留网格池化，结合自监督图自编码器，在新建的MedShapeNet19基准上，在低标签条件下显著提升重构与分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前解剖3D形状分类受网格数据复杂性和缺乏标准基准限制，需要稳健的学习方法和可复现的评估。

Method: 提出PSPooling：通过几何邻近预计算节点对应集，实现结构保留的并行可逆池化/反池化；其与自监督图自编码器集成，学习无标签表面网格的解剖感知表示。构建并使用MedShapeNet19数据集进行评估。

Result: 在MedShapeNet19上，PSPooling提升了重构质量和在少量标注下的分类性能，证明了方法在高分辨率医疗网格上的实用性，并提供了代码、模型与数据集以促进可复现研究。

Conclusion: 本文提出PSPooling，一种预计算的不可学习的网格池化算子，适用于高分辨率医学网格的并行、可逆下采样与上采样，避免了基于选择方法的稀疏和重构问题以及基于边收缩方法的顺序开销。结合自监督图自编码器，在MedShapeNet19基准上，在低标签场景下显著提升了重构保真度和分类准确率，构建了医学3D形状学习的强基线。

Abstract: Progress in anatomical 3D shape classification is limited by the complexity
of mesh data and the lack of standardized benchmarks, highlighting the need for
robust learning methods and reproducible evaluation. We introduce two key steps
toward clinically and benchmark-ready anatomical shape classification via
self-supervised graph autoencoding. We propose Precomputed Structural Pooling
(PSPooling), a non-learnable mesh pooling operator designed for efficient and
structure-preserving graph coarsening in 3D anatomical shape analysis.
PSPooling precomputes node correspondence sets based on geometric proximity,
enabling parallelizable and reversible pooling and unpooling operations with
guaranteed support structure. This design avoids the sparsity and
reconstruction issues of selection-based methods and the sequential overhead of
edge contraction approaches, making it particularly suitable for
high-resolution medical meshes. To demonstrate its effectiveness, we integrate
PSPooling into a self-supervised graph autoencoder that learns anatomy-aware
representations from unlabeled surface meshes. We evaluate the downstream
benefits on MedShapeNet19, a new curated benchmark dataset we derive from
MedShapeNet, consisting of 19 anatomical classes with standardized training,
validation, and test splits. Experiments show that PSPooling significantly
improves reconstruction fidelity and classification accuracy in low-label
regimes, establishing a strong baseline for medical 3D shape learning. We hope
that MedShapeNet19 will serve as a widely adopted benchmark for anatomical
shape classification and further research in medical 3D shape analysis. Access
the complete codebase, model weights, and dataset information here:
https://github.com/TomasKrsicka/MedShapeNet19-PSPooling.

</details>


### [167] [Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers](https://arxiv.org/abs/2511.01617)
*Mohamed Eltahir,Ali Habibullah,Lama Ayash,Tanveer Hussain,Naeemullah Khan*

Main category: cs.CV

TL;DR: 把候选内容与检索器元信息一起喂给VLM作为零样本提示（使用S-Grid表示视频），实现基于VLM的列表级投票重排与融合，显著提升视频跨模态检索的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 传统无训练融合方法仅利用分数或排名，忽略了候选的表示与视觉-语言证据，导致复杂多模态（尤其视频）检索表现受限；希望利用强大的VLM进行语境化判断以改进融合与重排。

Method: 提出Vote-in-Context框架：把候选视频的视觉内容（S-Grid图像网格）与可选字幕及检索器的排名/分数元信息一并序列化进VLM提示，作为列表式输入进行零样本投票式重排与融合；适配单列rerank和多检索器ensemble fuser两种场景。

Result: 在多个视频检索基准（如MSR-VTT、ActivityNet、VATEX）上取得新的零样本SOTA：MSR-VTT Recall@1 达到87.1%（文本到视频）/89.0%（视频到文本），VATEX v2t Recall@1 达到99.6%，相较先前方法最高提升约+40 Recall@1，并优于CombSUM等强基线。

Conclusion: ViC将检索融合与重排问题视为VLM的零样本推理，通过在提示中序列化候选内容与检索器元信息，使模型能自适应地在视觉-语言证据与检索器一致性间权衡，显著提升视频跨模态检索的零样本性能。

Abstract: In the retrieval domain, candidates' fusion from heterogeneous retrievers is
a long-standing challenge, particularly for complex, multi-modal data such as
videos. While typical fusion techniques are training-free, they rely solely on
rank or score signals, disregarding candidates' representations. This work
introduces Vote-in-Context (ViC), a generalized, training-free framework that
re-thinks list-wise reranking and fusion as a zero-shot reasoning task for a
Vision-Language Model (VLM). The core insight is to serialize both content
evidence and retriever metadata directly within the VLM's prompt, allowing the
model to adaptively weigh retriever consensus against visual-linguistic
content. We demonstrate the generality of this framework by applying it to the
challenging domain of cross-modal video retrieval. To this end, we introduce
the S-Grid, a compact serialization map that represents each video as an image
grid, optionally paired with subtitles to enable list-wise reasoning over video
candidates. ViC is evaluated both as a single-list reranker, where it
dramatically improves the precision of individual retrievers, and as an
ensemble fuser, where it consistently outperforms strong baselines like
CombSUM. Across video retrieval benchmarks including ActivityNet and VATEX, the
framework establishes new state-of-the-art zero-shot retrieval performance,
demonstrating its effectiveness in handling complex visual and temporal signals
alongside text. In zero-shot settings, ViC achieves Recall@1 scores of 87.1%
(t2v) / 89.0% (v2t) on MSR-VTT and 99.6% (v2t) on VATEX, representing massive
gains of up to +40 Recall@1 over previous state-of-the-art baselines. We
present ViC as a simple, reproducible, and highly effective recipe for turning
modern VLMs into powerful zero-shot rerankers and fusers. Code and resources
are publicly available at: https://github.com/mohammad2012191/ViC

</details>


### [168] [Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models](https://arxiv.org/abs/2511.01618)
*Xiaoyu Zhan,Wenxuan Huang,Hao Sun,Xinyu Fu,Changfeng Ma,Shaosheng Cao,Bohan Jia,Shaohui Lin,Zhenfei Yin,Lei Bai,Wanli Ouyang,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CV

TL;DR: 构建100K视点配对数据集并采用SFT+GRPO两阶段微调与混合冷启动，显著提升MLLM的视点一致性与三维空间推理能力，有助于机器人与3D场景理解应用。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在二维视觉理解上进展显著，但缺乏对细粒度三维空间信息和视角一致性的评估与提升，影响在真实世界三维推理任务中的表现。

Method: 提出两阶段微调：1) 在Viewpoint-100K上进行监督微调（SFT）注入基础视点知识；2) 使用改进的强化学习算法（GRPO）在更广泛的问题集上提升泛化能力，并引入混合冷启动初始化同时学习视点表征和保持推理连贯性。

Result: 实验显示该方法显著激活MLLM的空间推理能力，在领域内与领域外任务上均取得性能提升，证明了构建基础空间技能的重要性。

Conclusion: 该论文证明通过专门构建的Viewpoint-100K数据集和两阶段微调策略，可以显著提升多模态大语言模型（MLLM）的视点一致性和三维空间推理能力。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly improved 2D visual understanding, prompting interest in their
application to complex 3D reasoning tasks. However, it remains unclear whether
these models can effectively capture the detailed spatial information required
for robust real-world performance, especially cross-view consistency, a key
requirement for accurate 3D reasoning. Considering this issue, we introduce
Viewpoint Learning, a task designed to evaluate and improve the spatial
reasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,
consisting of 100K object-centric image pairs with diverse viewpoints and
corresponding question-answer pairs. Our approach employs a two-stage
fine-tuning strategy: first, foundational knowledge is injected to the baseline
MLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in
significant improvements across multiple tasks; second, generalization is
enhanced through Reinforcement Learning using the Group Relative Policy
Optimization (GRPO) algorithm on a broader set of questions. Additionally, we
introduce a hybrid cold-start initialization method designed to simultaneously
learn viewpoint representations and maintain coherent reasoning thinking.
Experimental results show that our approach significantly activates the spatial
reasoning ability of MLLM, improving performance on both in-domain and
out-of-domain reasoning tasks. Our findings highlight the value of developing
foundational spatial skills in MLLMs, supporting future progress in robotics,
autonomous systems, and 3D scene understanding.

</details>


### [169] [Enhancing Diffusion-based Restoration Models via Difficulty-Adaptive Reinforcement Learning with IQA Reward](https://arxiv.org/abs/2511.01645)
*Xiaogang Xu,Ruihang Chu,Jian Wang,Kun Zhou,Wenjie Shu,Harry Yang,Ser-Nam Lim,Hao Chen,Liang Lin*

Main category: cs.CV

TL;DR: 用IQA驱动的奖励和基于样本难度的自适应RL+SFT混合策略，将RL高效应用于扩散模型的图像修复，显著提升恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法直接用于扩散基恢复模型效果欠佳，因为恢复任务更注重保真度，需要不同的奖励设计与训练策略。

Method: 通过实验比较多种奖励函数，发现IQA模型提供的奖励优于基于GT的监督；采用MLLM-based IQA对高质量图像分布进行对齐；对难样本重点使用RL，并在样本接近GT分布时自适应地将RL与SFT结合，采用自动权重调整训练策略。

Result: 在多个修复任务和基准上，提出的RL框架显著提升了性能，尤其在主观质量和难样本恢复上效果明显。

Conclusion: 本文提出将RL有效整合到扩散模型的图像修复任务中，强调基于IQA模型的奖励函数与动态权重策略能提升恢复任务的保真度与主观质量。

Abstract: Reinforcement Learning (RL) has recently been incorporated into diffusion
models, e.g., tasks such as text-to-image. However, directly applying existing
RL methods to diffusion-based image restoration models is suboptimal, as the
objective of restoration fundamentally differs from that of pure generation: it
places greater emphasis on fidelity. In this paper, we investigate how to
effectively integrate RL into diffusion-based restoration models. First,
through extensive experiments with various reward functions, we find that an
effective reward can be derived from an Image Quality Assessment (IQA) model,
instead of intuitive ground-truth-based supervision, which has already been
optimized during the Supervised Fine-Tuning (SFT) stage prior to RL. Moreover,
our strategy focuses on using RL for challenging samples that are significantly
distant from the ground truth, and our RL approach is innovatively implemented
using MLLM-based IQA models to align distributions with high-quality images
initially. As the samples approach the ground truth's distribution, RL is
adaptively combined with SFT for more fine-grained alignment. This dynamic
process is facilitated through an automatic weighting strategy that adjusts
based on the relative difficulty of the training samples. Our strategy is
plug-and-play that can be seamlessly applied to diffusion-based restoration
models, boosting its performance across various restoration tasks. Extensive
experiments across multiple benchmarks demonstrate the effectiveness of our
proposed RL framework.

</details>


### [170] [UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback](https://arxiv.org/abs/2511.01678)
*Ropeway Liu,Hangjie Yuan,Bo Dong,Jiazheng Xing,Jinwang Wang,Rui Zhao,Yan Xing,Weihua Chen,Fan Wang*

Main category: cs.CV

TL;DR: UniLumos在流匹配框架中引入RGB空间的深度与法线反馈与路径一致性学习，并配套六维标注与Lum osBench，实现了更真实且快速的图像/视频重光照。


<details>
  <summary>Details</summary>
Motivation: 现有基于语义潜在空间的扩散模型常产生物理不一致的光照伪影（过曝高光、阴影错位、遮挡错误），需要在视觉空间结合几何信息以提升物理合理性。

Method: 在流匹配（flow matching）骨干上，模型输出的深度与法线图被回馈用于监督，使得光照效果与场景结构显式对齐；同时使用路径一致性学习在少步训练下仍保持监督有效；并设计了六维照明属性标注协议和基于大模型的Lum osBench评估基准。

Result: 在图像与视频重光照任务上，UniLumos显著提升物理一致性并实现约20倍加速，同时在Lum osBench上展现更细粒度可控的光照调节能力。

Conclusion: UniLumos通过在RGB空间引入几何反馈并结合路径一致性学习，实现了在图像和视频重光照任务中的物理一致性提升和速度加速，达到了SOTA质量。

Abstract: Relighting is a crucial task with both practical demand and artistic value,
and recent diffusion models have shown strong potential by enabling rich and
controllable lighting effects. However, as they are typically optimized in
semantic latent space, where proximity does not guarantee physical correctness
in visual space, they often produce unrealistic results, such as overexposed
highlights, misaligned shadows, and incorrect occlusions. We address this with
UniLumos, a unified relighting framework for both images and videos that brings
RGB-space geometry feedback into a flow matching backbone. By supervising the
model with depth and normal maps extracted from its outputs, we explicitly
align lighting effects with the scene structure, enhancing physical
plausibility. Nevertheless, this feedback requires high-quality outputs for
supervision in visual space, making standard multi-step denoising
computationally expensive. To mitigate this, we employ path consistency
learning, allowing supervision to remain effective even under few-step training
regimes. To enable fine-grained relighting control and supervision, we design a
structured six-dimensional annotation protocol capturing core illumination
attributes. Building upon this, we propose LumosBench, a disentangled
attribute-level benchmark that evaluates lighting controllability via large
vision-language models, enabling automatic and interpretable assessment of
relighting precision across individual dimensions. Extensive experiments
demonstrate that UniLumos achieves state-of-the-art relighting quality with
significantly improved physical consistency, while delivering a 20x speedup for
both image and video relighting. Code is available at
https://github.com/alibaba-damo-academy/Lumos-Custom.

</details>


### [171] [Progressive Translation of H&E to IHC with Enhanced Structural Fidelity](https://arxiv.org/abs/2511.01698)
*Yuhang Kang,Ziyu Su,Tianyang Wang,Zaibo Li,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 提出结构-颜色-细胞边界渐进生成网络，结合DAB浓度和梯度损失，显著改善H&E到IHC的染色翻译效果。


<details>
  <summary>Details</summary>
Motivation: IHC成本高且劳动强度大，现有计算染色方法通常把多种损失线性加权，忽略损失间的相互依赖，导致结构和颜色不能兼顾。作者希望通过分阶段优化各视觉成分来同时保持结构真实性和颜色保真性。

Method: 基于ASP框架作为基线，引入结构-颜色-细胞边界的渐进式生成机制；新增基于DAB浓度和图像梯度的损失项以增强颜色保真度和边界清晰度。

Result: 在HER2和ER数据集上，所提方法在视觉质量和结构细节上有明显提升，生成的IHC图像在颜色和细胞边界上更精细。

Conclusion: 该论文提出了一种分阶段的网络结构，通过分离结构、颜色和细胞边界的生成逻辑来提升从H&E到IHC的染色翻译效果。

Abstract: Compared to hematoxylin-eosin (H&E) staining, immunohistochemistry (IHC) not
only maintains the structural features of tissue samples, but also provides
high-resolution protein localization, which is essential for aiding in
pathology diagnosis. Despite its diagnostic value, IHC remains a costly and
labor-intensive technique. Its limited scalability and constraints in
multiplexing further hinder widespread adoption, especially in resource-limited
settings. Consequently, researchers are increasingly exploring computational
stain translation techniques to synthesize IHC-equivalent images from
H&E-stained slides, aiming to extract protein-level information more
efficiently and cost-effectively. However, most existing stain translation
techniques rely on a linearly weighted summation of multiple loss terms within
a single objective function, strategy that often overlooks the interdepedence
among these components-resulting in suboptimal image quality and an inability
to simultaneously preserve structural authenticity and color fidelity. To
address this limitation, we propose a novel network architecture that follows a
progressive structure, incorporating color and cell border generation logic,
which enables each visual aspect to be optimized in a stage-wise and decoupled
manner. To validate the effectiveness of our proposed network architecture, we
build upon the Adaptive Supervised PatchNCE (ASP) framework as our baseline. We
introduce additional loss functions based on 3,3'-diaminobenzidine (DAB)
chromogen concentration and image gradient, enhancing color fidelity and cell
boundary clarity in the generated IHC images. By reconstructing the generation
pipeline using our structure-color-cell boundary progressive mechanism,
experiments on HER2 and ER datasets demonstrated that the model significantly
improved visual quality and achieved finer structural details.

</details>


### [172] [Learnable Fractional Reaction-Diffusion Dynamics for Under-Display ToF Imaging and Beyond](https://arxiv.org/abs/2511.01704)
*Xin Qiao,Matteo Poggi,Xing Wei,Pengchao Deng,Yanhui Zhou,Stefano Mattoccia*

Main category: cs.CV

TL;DR: 提出LFRD2：结合可学习时间分数阶反应-扩散和高效连续卷积的混合框架，有效提升TOLED下ToF深度成像质量。


<details>
  <summary>Details</summary>
Motivation: 目标是在屏幕下的ToF相机（尤其TOLED面板）中恢复准确深度地图，因TOLED引入信号衰减、MPI和时序噪声等严重退化，传统方法难以同时建模复杂时空非局部效应与物理退化，故提出结合可学习模型与物理可解释性的分数阶动力学方法。

Method: 方法包括：1) 设计时间分数阶反应-扩散模块，实现带动态生成的分数阶（可学习阶数）的迭代深度精化，以捕捉长时依赖；2) 提出基于系数预测和重复微分的高效连续卷积算子，用于增强恢复能力；3) 将可学习神经网络与物理建模相结合的混合框架，用于透明OLED引起的退化恢复。

Result: 在四个基准数据集上的实验表明，LFRD2在深度恢复任务中表现优于基线方法，能够更好地抑制MPI和噪声并恢复细节。代码已公开。

Conclusion: 该论文提出了一种混合的可学习分数阶反应-扩散动力学(LFRD2)框架，用于改善透明OLED下的ToF深度成像，通过引入时间分数阶反应-扩散模块和高效连续卷积算子，有效缓解信号衰减、多路径干扰和时序噪声，提高了深度恢复质量。

Abstract: Under-display ToF imaging aims to achieve accurate depth sensing through a
ToF camera placed beneath a screen panel. However, transparent OLED (TOLED)
layers introduce severe degradations-such as signal attenuation, multi-path
interference (MPI), and temporal noise-that significantly compromise depth
quality. To alleviate this drawback, we propose Learnable Fractional
Reaction-Diffusion Dynamics (LFRD2), a hybrid framework that combines the
expressive power of neural networks with the interpretability of physical
modeling. Specifically, we implement a time-fractional reaction-diffusion
module that enables iterative depth refinement with dynamically generated
differential orders, capturing long-term dependencies. In addition, we
introduce an efficient continuous convolution operator via coefficient
prediction and repeated differentiation to further improve restoration quality.
Experiments on four benchmark datasets demonstrate the effectiveness of our
approach. The code is publicly available at https://github.com/wudiqx106/LFRD2.

</details>


### [173] [Probabilistic Robustness for Free? Revisiting Training via a Benchmark](https://arxiv.org/abs/2511.01724)
*Yi Zhang,Zheng Wang,Chen Zhen,Wenjie Ruan,Qing Guo,Siddartha Khastgir,Carsten Maple,Xingyu Zhao*

Main category: cs.CV

TL;DR: 提出PRBench基准，系统比较AT与PR训练方法，发现AT更通用提升AR/PR，而PR训练在泛化与洁净精度上有优势；并公开了222模型排行榜。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注最坏情况的对抗鲁棒性(AR)，而PR作为统计视角的互补指标缺乏系统化的训练方法对比与评估基准；现有PR训练方法在评估协议、不充分与强AT比较、无统一泛化框架等方面存在不足。

Method: PRBench收集并统一评估多种对抗训练(AT)与PR目标训练方法，设计一致的评估协议与指标（洁净精度、PR、AR、训练效率、泛化误差），并进行理论GE分析，同时发布包含222个训练模型的排行榜。

Result: 实验表明：AT方法在广泛超参数设置下更能同时提升AR与PR；PR目标训练方法在泛化误差更低且洁净精度更高；PRBench公开了大规模模型排行榜用于后续比较。

Conclusion: PRBench建立了系统化评估概率鲁棒性(PR)训练方法的基准，并揭示对比实验与理论分析的主要结论。

Abstract: Deep learning models are notoriously vulnerable to imperceptible
perturbations. Most existing research centers on adversarial robustness (AR),
which evaluates models under worst-case scenarios by examining the existence of
deterministic adversarial examples (AEs). In contrast, probabilistic robustness
(PR) adopts a statistical perspective, measuring the probability that
predictions remain correct under stochastic perturbations. While PR is widely
regarded as a practical complement to AR, dedicated training methods for
improving PR are still relatively underexplored, albeit with emerging progress.
Among the few PR-targeted training methods, we identify three limitations: i
non-comparable evaluation protocols; ii limited comparisons to strong AT
baselines despite anecdotal PR gains from AT; and iii no unified framework to
compare the generalization of these methods. Thus, we introduce PRBench, the
first benchmark dedicated to evaluating improvements in PR achieved by
different robustness training methods. PRBench empirically compares most common
AT and PR-targeted training methods using a comprehensive set of metrics,
including clean accuracy, PR and AR performance, training efficiency, and
generalization error (GE). We also provide theoretical analysis on the GE of PR
performance across different training methods. Main findings revealed by
PRBench include: AT methods are more versatile than PR-targeted training
methods in terms of improving both AR and PR performance across diverse
hyperparameter settings, while PR-targeted training methods consistently yield
lower GE and higher clean accuracy. A leaderboard comprising 222 trained models
across 7 datasets and 10 model architectures is publicly available at
https://tmpspace.github.io/PRBenchLeaderboard/.

</details>


### [174] [Toward Strategy Identification and Subtask Decomposition In Task Exploration](https://arxiv.org/abs/2511.01728)
*Tom Odem*

Main category: cs.CV

TL;DR: 该工作提出并实现了一个以聚类、因子分析和编辑距离为核心的任务探索流水线，自动发现任务完成策略与子任务结构，辅助理解与预测用户知识、技能与行为。


<details>
  <summary>Details</summary>
Motivation: 在具备预期性的人人机交互中，机器需要提前理解用户的知识与行为以实现隐式配合，因此需要自动化方法从动作时间序列中提取关键策略与子任务。

Method: 提出了一个任务探索流水线：对动作序列进行因子分析与降维，使用聚类发现全局策略；基于字符串编辑距离对序列进行相似性度量以识别局部策略；结合分层结构编码用户运行并提取不同长度的子任务；开发了可视化的Task Explorer应用以便审查结果。

Result: 流水线成功自动识别了关键全局和局部策略，能够将用户运行编码为分层子任务结构，并通过Task Explorer应用支持结果审查；方法可推广到任意基于动作的时间序列数据。

Conclusion: 该研究构建了一个可扩展的任务探索流水线，通过聚类、因子分析和字符串编辑距离等方法，能够自动识别完成任务时的全局和局部策略，并分割有意义的子任务，从而为理解用户的知识、技能与行为提供量化支持。

Abstract: This research builds on work in anticipatory human-machine interaction, a
subfield of human-machine interaction where machines can facilitate
advantageous interactions by anticipating a user's future state. The aim of
this research is to further a machine's understanding of user knowledge, skill,
and behavior in pursuit of implicit coordination. A task explorer pipeline was
developed that uses clustering techniques, paired with factor analysis and
string edit distance, to automatically identify key global and local strategies
that are used to complete tasks. Global strategies identify generalized sets of
actions used to complete tasks, while local strategies identify sequences that
used those sets of actions in a similar composition. Additionally, meaningful
subtasks of various lengths are identified within the tasks. The task explorer
pipeline was able to automatically identify key strategies used to complete
tasks and encode user runs with hierarchical subtask structures. In addition, a
Task Explorer application was developed to easily review pipeline results. The
task explorer pipeline can be easily modified to any action-based time-series
data and the identified strategies and subtasks help to inform humans and
machines on user knowledge, skill, and behavior.

</details>


### [175] [CGF-DETR: Cross-Gated Fusion DETR for Enhanced Pneumonia Detection in Chest X-rays](https://arxiv.org/abs/2511.01730)
*Yefeng Wu,Yucheng Song,Ling Wu,Shan Wan,Yecheng Zhao*

Main category: cs.CV

TL;DR: 提出面向胸片肺炎检测的CGF-DETR，通过XFABlock、SPGA和GCFC3三模块改造RT-DETR，在保持实时性的同时显著提升mAP，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 尽管RT-DETR等变换器检测器在目标检测中表现良好，但在医学影像尤其是胸片肺炎检测上的应用尚少，且需兼顾精度与实时性，故提出针对性改进以提升在医疗场景的检测性能。

Method: 在RT-DETR基础上，提出三大改进模块：backbone中引入XFABlock（结合卷积注意力与CSP结构以强化多尺度特征提取）；替换标准多头注意力的SPGA模块（使用动态门控机制与单头自注意力以实现高效特征聚合）；neck中设计GCFC3（多路径卷积融合并通过结构重参数化保持实时性）。

Result: 在RSNA数据集上，CGF-DETR达到82.2% mAP@0.5，较RT-DETR-l提升3.7%，推理速度保持48.1 FPS；完整模型mAP@[0.5:0.95]为50.4%；消融实验表明每个模块均对性能有贡献。

Conclusion: CGF-DETR在RSNA Pneumonia Detection数据集上显著提升了检测精度，同时保持实时推理速度，证明所提模块在胸片肺炎检测任务中有效。

Abstract: Pneumonia remains a leading cause of morbidity and mortality worldwide,
necessitating accurate and efficient automated detection systems. While recent
transformer-based detectors like RT-DETR have shown promise in object detection
tasks, their application to medical imaging, particularly pneumonia detection
in chest X-rays, remains underexplored. This paper presents CGF-DETR, an
enhanced real-time detection transformer specifically designed for pneumonia
detection. We introduce XFABlock in the backbone to improve multi-scale feature
extraction through convolutional attention mechanisms integrated with CSP
architecture. To achieve efficient feature aggregation, we propose SPGA module
that replaces standard multi-head attention with dynamic gating mechanisms and
single-head self-attention. Additionally, GCFC3 is designed for the neck to
enhance feature representation through multi-path convolution fusion while
maintaining real-time performance via structural re-parameterization. Extensive
experiments on the RSNA Pneumonia Detection dataset demonstrate that CGF-DETR
achieves 82.2\% mAP@0.5, outperforming the baseline RT-DETR-l by 3.7\% while
maintaining comparable inference speed at 48.1 FPS. Our ablation studies
confirm that each proposed module contributes meaningfully to the overall
performance improvement, with the complete model achieving 50.4\%
mAP@[0.5:0.95]

</details>


### [176] [3EED: Ground Everything Everywhere in 3D](https://arxiv.org/abs/2511.01755)
*Rong Li,Yuhao Dong,Tianshuai Hu,Ao Liang,Youquan Liu,Dongyue Lu,Liang Pan,Lingdong Kong,Junwei Liang,Ziwei Liu*

Main category: cs.CV

TL;DR: 3EED是一个跨平台、多模态、超大规模的户外3D视觉指称数据集与基准，使用VLM提示+人工验证构建标注，并提出平台归一化和跨模态对齐方法，基准结果表明跨平台泛化仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有3D可视化定位基准多为室内、小规模、单一平台，难以满足户外开放世界中具身智能体的需求，因此需要大规模、多平台、多模态的数据集与评测方案。

Method: 构建了包含车辆、无人机和四足机器人三个平台的RGB和LiDAR数据集；采用视觉语言模型生成候选描述并结合人工验证形成高质量标注；提出了平台感知归一化和跨模态对齐方法以支持跨平台学习，并设计了域内与跨平台评测协议。

Result: 构建了包含128k对象和22k经验证的指称短语的数据集，规模约为现有数据集的10倍；基准测试显示当前方法在跨平台和跨场景泛化上存在显著差距，揭示了挑战与未来方向。

Conclusion: 该论文提出了一个大规模、跨平台的3D视觉定位基准3EED，扩展了现有数据集在场景、平台和规模上的覆盖，能够推动户外、跨平台的语言驱动3D感知研究。

Abstract: Visual grounding in 3D is the key for embodied agents to localize
language-referred objects in open-world environments. However, existing
benchmarks are limited to indoor focus, single-platform constraints, and small
scale. We introduce 3EED, a multi-platform, multi-modal 3D grounding benchmark
featuring RGB and LiDAR data from vehicle, drone, and quadruped platforms. We
provide over 128,000 objects and 22,000 validated referring expressions across
diverse outdoor scenes -- 10x larger than existing datasets. We develop a
scalable annotation pipeline combining vision-language model prompting with
human verification to ensure high-quality spatial grounding. To support
cross-platform learning, we propose platform-aware normalization and
cross-modal alignment techniques, and establish benchmark protocols for
in-domain and cross-platform evaluations. Our findings reveal significant
performance gaps, highlighting the challenges and opportunities of
generalizable 3D grounding. The 3EED dataset and benchmark toolkit are released
to advance future research in language-driven 3D embodied perception.

</details>


### [177] [HGFreNet: Hop-hybrid GraphFomer for 3D Human Pose Estimation with Trajectory Consistency in Frequency Domain](https://arxiv.org/abs/2511.01756)
*Kai Zhai,Ziyan Huang,Qiang Nie,Xiang Li,Bo Ouyang*

Main category: cs.CV

TL;DR: 提出HGFreNet：通过HGA模块扩展感受野并用Transformer建模全局时空关系，结合频域的3D轨迹一致性约束与初步3D估计网络，有效提升单目视频的3D姿态估计精度与时序连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅在时间域限制相邻帧差分以抑制抖动，但忽略了骨骼关节运动的全局时空相关性，导致深度歧义和2D估计误差引发的3D轨迹不连贯。

Method: 设计了HGA模块，将每个关节点的k跳邻居分组以扩展感受野，并对这些组施加全局注意力以发现潜在相关性；引入Transformer编码器建模全局时空相关性；在频域对3D轨迹一致性进行约束以减少深度模糊和2D估计误差引起的抖动；使用初步网络提供跨帧深度信息以维护时间连贯性。

Result: 在Human3.6M与MPI-INF-3DHP两个基准数据集上进行的广泛实验表明，HGFreNet在位置精度和时间一致性上优于现有SOTA方法。

Conclusion: 本文提出HGFreNet，通过结合Hop-Hybrid Graph Attention（HGA）模块与Transformer编码器，以及在频域上约束3D轨迹一致性，提升了单目视频的2D-to-3D人体姿态估计在位置精度与时间一致性方面的表现。

Abstract: 2D-to-3D human pose lifting is a fundamental challenge for 3D human pose
estimation in monocular video, where graph convolutional networks (GCNs) and
attention mechanisms have proven to be inherently suitable for encoding the
spatial-temporal correlations of skeletal joints. However, depth ambiguity and
errors in 2D pose estimation lead to incoherence in the 3D trajectory. Previous
studies have attempted to restrict jitters in the time domain, for instance, by
constraining the differences between adjacent frames while neglecting the
global spatial-temporal correlations of skeletal joint motion. To tackle this
problem, we design HGFreNet, a novel GraphFormer architecture with hop-hybrid
feature aggregation and 3D trajectory consistency in the frequency domain.
Specifically, we propose a hop-hybrid graph attention (HGA) module and a
Transformer encoder to model global joint spatial-temporal correlations. The
HGA module groups all $k$-hop neighbors of a skeletal joint into a hybrid group
to enlarge the receptive field and applies the attention mechanism to discover
the latent correlations of these groups globally. We then exploit global
temporal correlations by constraining trajectory consistency in the frequency
domain. To provide 3D information for depth inference across frames and
maintain coherence over time, a preliminary network is applied to estimate the
3D pose. Extensive experiments were conducted on two standard benchmark
datasets: Human3.6M and MPI-INF-3DHP. The results demonstrate that the proposed
HGFreNet outperforms state-of-the-art (SOTA) methods in terms of positional
accuracy and temporal consistency.

</details>


### [178] [Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image](https://arxiv.org/abs/2511.01767)
*Yuxiao Yang,Xiao-Xiao Long,Zhiyang Dou,Cheng Lin,Yuan Liu,Qingsong Yan,Yuexin Ma,Haoqian Wang,Zhiqiang Wu,Wei Yin*

Main category: cs.CV

TL;DR: 提出Wonder3D++：通过跨域扩散生成多视角法线和彩色图，结合多视角跨域注意力与级联网格提取，实现在3分钟级别的单视图高质量纹理网格重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于SDS的方法虽能从2D扩散先验恢复3D几何但优化慢且几何不一致；直接网络推理方法速度快但质量低且缺乏细节。作者希望在质量、一致性与效率间取得平衡。

Method: 提出跨域扩散模型生成多视角法线图与对应彩色图像，采用多视角跨域注意力机制保证视角和模态间信息一致性，并引入级联粗到细的3D网格提取算法从2D表示快速提取高质量表面（约3分钟）。

Result: 在大量评估中，该方法在重建质量、泛化性与效率上优于先前工作，能快速生成高质量网格模型。

Conclusion: 该论文提出了Wonder3D++，用于从单视图图像高效生成高保真带纹理网格，综合提高质量、一致性与效率。

Abstract: In this work, we introduce \textbf{Wonder3D++}, a novel method for
efficiently generating high-fidelity textured meshes from single-view images.
Recent methods based on Score Distillation Sampling (SDS) have shown the
potential to recover 3D geometry from 2D diffusion priors, but they typically
suffer from time-consuming per-shape optimization and inconsistent geometry. In
contrast, certain works directly produce 3D information via fast network
inferences, but their results are often of low quality and lack geometric
details. To holistically improve the quality, consistency, and efficiency of
single-view reconstruction tasks, we propose a cross-domain diffusion model
that generates multi-view normal maps and the corresponding color images. To
ensure the consistency of generation, we employ a multi-view cross-domain
attention mechanism that facilitates information exchange across views and
modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that
drives high-quality surfaces from the multi-view 2D representations in only
about $3$ minute in a coarse-to-fine manner. Our extensive evaluations
demonstrate that our method achieves high-quality reconstruction results,
robust generalization, and good efficiency compared to prior works. Code
available at https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.

</details>


### [179] [UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs](https://arxiv.org/abs/2511.01768)
*Zhe Liu,Jinghua Hou,Xiaoqing Ye,Jingdong Wang,Hengshuang Zhao,Xiang Bai*

Main category: cs.CV

TL;DR: 用线性group RNN替代Transformer注意力，UniLION实现了一个高效统一的自动驾驶多模态多任务模型，在多项3D感知、预测与规划任务上取得强劲表现。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次注意力在长序列输入（如大规模点云与高分辨率影像序列）下成本过高，需寻找一个计算更高效且能自然支持多模态多任务的统一模型。

Method: 提出线性group RNN算子（对分组特征执行线性RNN），替代传统Transformer的二次注意力，形成单一网络结构可通过不同输入配置（LiDAR-only、temporal LiDAR、多模态及其时间序列）得到专用变体而无需额外的时序或多模态融合模块。

Result: UniLION在3D感知（检测、跟踪、占用预测、BEV分割）、轨迹预测与端到端规划等多项任务上表现出与或优于现有方法的竞争力甚至SOTA成绩，且模型结构简洁，易扩展。

Conclusion: UniLION通过将线性群RNN算子应用于分组特征，成功在统一架构下高效处理大规模LiDAR点云、高分辨率多视角图像及时间序列，实现了多模态、多任务的端到端支持。

Abstract: Although transformers have demonstrated remarkable capabilities across
various domains, their quadratic attention mechanisms introduce significant
computational overhead when processing long-sequence data. In this paper, we
present a unified autonomous driving model, UniLION, which efficiently handles
large-scale LiDAR point clouds, high-resolution multi-view images, and even
temporal sequences based on the linear group RNN operator (i.e., performs
linear RNN for grouped features). Remarkably, UniLION serves as a single
versatile architecture that can seamlessly support multiple specialized
variants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modal
temporal fusion configurations) without requiring explicit temporal or
multi-modal fusion modules. Moreover, UniLION consistently delivers competitive
and even state-of-the-art performance across a wide range of core tasks,
including 3D perception (e.g., 3D object detection, 3D object tracking, 3D
occupancy prediction, BEV map segmentation), prediction (e.g., motion
prediction), and planning (e.g., end-to-end planning). This unified paradigm
naturally simplifies the design of multi-modal and multi-task autonomous
driving systems while maintaining superior performance. Ultimately, we hope
UniLION offers a fresh perspective on the development of 3D foundation models
in autonomous driving. Code is available at
https://github.com/happinesslz/UniLION

</details>


### [180] [How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment](https://arxiv.org/abs/2511.01775)
*Zhen Chen,Qing Xu,Jinlin Wu,Biao Yang,Yuhao Zhai,Geng Guo,Jing Zhang,Yinlu Ding,Nassir Navab,Jiebo Luo*

Main category: cs.CV

TL;DR: 提出SurgVeo基准与SPP评估框架，用专家评估显示大型视频模型在外科因果层面失败，强调需针对医疗场景设计具有因果推理能力的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有视频基础模型对物理世界的泛化良好，但能否在要求深度专业因果知识的高风险外科领域胜任尚不明确，迫切需要系统评估与基准。

Method: 构建SurgVeo外科视频基准并提出四层Surgical Plausibility Pyramid（外科合理性金字塔, SPP）；使用Veo-3在零样本任务上生成术中视频，并由四位主刀外科医师依据SPP进行盲评。

Result: Veo-3在视觉感知层面表现优异，但在器械操作合理性、环境反馈合理性与外科意图合理性三层均表现不足，定量上体现为评分显著下降，揭示了视觉逼真与因果理解之间的差距。

Conclusion: 本文证实现有视频基础模型在外科场景下存在“可行性鸿沟”：尽管视觉表现逼真，但在因果操作与策略层面严重失效，需发展专门的、因果感知的模型。

Abstract: Foundation models in video generation are demonstrating remarkable
capabilities as potential world models for simulating the physical world.
However, their application in high-stakes domains like surgery, which demand
deep, specialized causal knowledge rather than general physical rules, remains
a critical unexplored gap. To systematically address this challenge, we present
SurgVeo, the first expert-curated benchmark for video generation model
evaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,
four-tiered framework tailored to assess model outputs from basic appearance to
complex surgical strategy. On the basis of the SurgVeo benchmark, we task the
advanced Veo-3 model with a zero-shot prediction task on surgical clips from
laparoscopic and neurosurgical procedures. A panel of four board-certified
surgeons evaluates the generated videos according to the SPP. Our results
reveal a distinct "plausibility gap": while Veo-3 achieves exceptional Visual
Perceptual Plausibility, it fails critically at higher levels of the SPP,
including Instrument Operation Plausibility, Environment Feedback Plausibility,
and Surgical Intent Plausibility. This work provides the first quantitative
evidence of the chasm between visually convincing mimicry and causal
understanding in surgical AI. Our findings from SurgVeo and the SPP establish a
crucial foundation and roadmap for developing future models capable of
navigating the complexities of specialized, real-world healthcare domains.

</details>


### [181] [PROPEX-RAG: Enhanced GraphRAG using Prompt-Driven Prompt Execution](https://arxiv.org/abs/2511.01802)
*Tejas Sarnaik,Manan Shah,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出Prompt-driven GraphRAG：通过提示语驱动的实体抽取、三元组知识图谱构建、PPR图遍历检索和LLM语义过滤/生成，显著提升多跳问答性能并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是当前基于图的RAG在复杂推理上表现良好，但提示语（prompt）对检索与推理流程的影响尚未充分研究，作者希望通过将prompt与图检索结合，提升多跳问答的检索精度与可解释性。

Method: 方法包括：1) 从文本中抽取实体并构建三元组形式的符号知识图谱；2) 采用基于提示语的LLM在在线检索阶段进行语义过滤与事实选择；3) 使用实体引导的个性化PageRank（PPR）图遍历进行高效可扩展的多跳检索；4) 最终利用LLM进行答案生成与段落重排序。

Result: 在HotpotQA和2WikiMultiHopQA数据集上取得了SOTA结果：F1分别为80.7%和78.9%，Recall@5分别为97.1%和98.1%。结果表明提示语设计能显著改善检索准确率与回答质量。

Conclusion: 该论文提出的Prompt-driven GraphRAG框架强调了提示语设计在多跳问答中对实体抽取、事实选择和段落重排序的关键作用，通过将文本构建为符号化的知识图谱并结合LLM进行语义过滤与生成，实现了提升检索与推理性能。

Abstract: Retrieval-Augmented Generation (RAG) has become a robust framework for
enhancing Large Language Models (LLMs) with external knowledge. Recent advances
in RAG have investigated graph based retrieval for intricate reasoning;
however, the influence of prompt design on enhancing the retrieval and
reasoning process is still considerably under-examined. In this paper, we
present a prompt-driven GraphRAG framework that underscores the significance of
prompt formulation in facilitating entity extraction, fact selection, and
passage reranking for multi-hop question answering. Our approach creates a
symbolic knowledge graph from text data by encoding entities and factual
relationships as structured facts triples. We use LLMs selectively during
online retrieval to perform semantic filtering and answer generation. We also
use entity-guided graph traversal through Personalized PageRank (PPR) to
support efficient, scalable retrieval based on the knowledge graph we built.
Our system gets state-of-the-art performance on HotpotQA and 2WikiMultiHopQA,
with F1 scores of 80.7% and 78.9%, and Recall@5 scores of 97.1% and 98.1%,
respectively. These results show that prompt design is an important part of
improving retrieval accuracy and response quality. This research lays the
groundwork for more efficient and comprehensible multi-hop question-answering
systems, highlighting the importance of prompt-aware graph reasoning.

</details>


### [182] [SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art](https://arxiv.org/abs/2511.01817)
*Sagi Eppel,Alona Strugatski*

Main category: cs.CV

TL;DR: SciTextures提供了1,200+模型、100k+纹理图像及其生成代码，用于评估AI将图像模式映射到形成机制及重建过程的能力，实验显示VLM在此任务上具备一定理解与模拟能力。


<details>
  <summary>Details</summary>
Motivation: 建立一个跨学科的大规模纹理与生成模型集，研究并评估AI将视觉模式与形成机制连接、理解并重建物理/社会/艺术等系统的能力。

Method: 通过一个自主的AI流水线自动收集并标准化实现超过1,200个模型的代码，生成超过100,000张来自各学科领域的纹理图像；并设计基准任务，包括匹配视觉模式到模型/代码，以及从真实图像推断并运行生成机制以比较模拟结果。

Result: 基准测试表明领先的视觉-语言模型能够在一定程度上理解并模拟生成物理系统的机制，能够识别模式对应的模型并生成相似的模拟图像；同时揭示任务存在挑战和模型局限。

Conclusion: 该论文提出并发布了SciTextures数据集，规模大、覆盖面广，旨在将可视纹理与生成机制连接起来，并用该数据集评估AI识别和重建生成机制的能力。

Abstract: The ability to connect visual patterns with the processes that form them
represents one of the deepest forms of visual understanding. Textures of clouds
and waves, the growth of cities and forests, or the formation of materials and
landscapes are all examples of patterns emerging from underlying mechanisms. We
present the Scitextures dataset, a large-scale collection of textures and
visual patterns from all domains of science, tech, and art, along with the
models and code that generate these images. Covering over 1,200 different
models and 100,000 images of patterns and textures from physics, chemistry,
biology, sociology, technology, mathematics, and art, this dataset offers a way
to explore the connection between the visual patterns that shape our world and
the mechanisms that produce them. Created by an agentic AI pipeline that
autonomously collects and implements models in standardized form, we use
SciTextures to evaluate the ability of leading AI models to link visual
patterns to the models and code that generate them, and to identify different
patterns that emerged from the same process. We also test AIs ability to infer
and recreate the mechanisms behind visual patterns by providing a natural image
of a real-world pattern and asking the AI to identify, model, and code the
mechanism that formed the pattern, then run this code to generate a simulated
image that is compared to the real image. These benchmarks show that
vision-language models (VLMs) can understand and simulate the physical system
beyond a visual pattern. The dataset and code are available at:
https://zenodo.org/records/17485502

</details>


### [183] [TIR-Bench: A Comprehensive Benchmark for Agentic Thinking-with-Images Reasoning](https://arxiv.org/abs/2511.01833)
*Ming Li,Jike Zhong,Shitian Zhao,Haoquan Zhang,Shaoheng Lin,Yuxiang Lai,Wei Chen,Konstantinos Psounis,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 提出TIR-Bench评估具工具化图像链式思维的模型，测试22款MLLMs，结果显示大多数模型表现差，强能力需要真正的agentic图像思维；并比较了微调策略。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如Visual Search）仅测试简单操作，难以衡量最新能够创造并操作工具以解决图像问题的模型能力，需更全面的评估框架。

Method: 构建13个任务集，设计需要链式思维和工具使用的图像处理/操作任务；对22个MLLMs（开放源与专有、含工具增强与否）进行统一评测；并进行pilot study对比直接微调与agentic微调效果。

Result: TIR-Bench对被测模型普遍具有挑战性，只有具备真正thinking-with-images能力的模型才能表现良好；另外提供了关于微调策略的初步比较结果。

Conclusion: 该论文提出了TIR-Bench，一个评估“agentic thinking-with-images”能力的综合基准，覆盖13项需要工具化图像处理的任务，表明现有模型普遍困难，强性能需要真实的图像思维能力，并比较了直接与agentic微调。

Abstract: The frontier of visual reasoning is shifting toward models like OpenAI o3,
which can intelligently create and operate tools to transform images for
problem-solving, also known as thinking-\textit{with}-images in
chain-of-thought. Yet existing benchmarks fail to fully capture this advanced
capability. Even Visual Search, the most common benchmark for current
thinking-\textit{with}-images methods, tests only basic operations such as
localization and cropping, offering little insight into more complex, dynamic,
and tool-dependent reasoning. We introduce \textbf{TIR-Bench}, a comprehensive
benchmark for evaluating agentic thinking-with-images across 13 diverse tasks,
each requiring novel tool use for image processing and manipulation in
chain-of-thought. We evaluate 22 multimodal large language models (MLLMs), from
leading open-sourced and proprietary models to those with explicit tool-use
augmentation. Results show that TIR-Bench is universally challenging, and
strong performance requires genuine thinking-with-images capabilities. Finally,
we present a pilot study comparing direct versus agentic fine-tuning.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [184] [NOMAD - Navigating Optimal Model Application to Datastreams](https://arxiv.org/abs/2511.00290)
*Ashwin Gerard Colaco,Sharad Mehrotra,Michael J De Lucia,Kevin Hamlen,Murat Kantarcioglu,Latifur Khan,Ananthram Swami,Bhavani Thuraisingham*

Main category: cs.DB

TL;DR: NOMAD 通过动态、基于效用的模型链管理和链安全性保障，在数据摄取时实现低成本且高质量的实时多类分类。


<details>
  <summary>Details</summary>
Motivation: 在线数据流摄取中需在有限计算资源下实现高质量实时多类分类，单一高质量模型计算代价高，静态模型组合/选择缺乏灵活性与适应性；因此需要一种在保证分类质量的同时减少计算开销的动态策略。

Method: 基于效用准则动态选取不同代价-质量权衡的模型序列（模型链），借鉴数据库谓词排序思想，用廉价模型做初级过滤并在必要时调用更昂贵的模型；引入链安全机制保证质量不低于角色模型；采用动态信念更新根据每个事件预测和数据分布漂移自适应选择模型；并扩展到模型相关性场景（如早退DNN和堆叠集成）。

Result: 在多个数据集上的评估表明，NOMAD 在保持与最准确模型相当的分类质量的同时，相较于静态或简单方法实现了显著的计算节省。

Conclusion: NOMAD 提供了一种在数据摄取时通过动态构建模型链来优化实时多类分类的智能框架，能够在保持与指定角色模型相当的分类质量前提下显著降低计算成本。

Abstract: NOMAD (Navigating Optimal Model Application for Datastreams) is an
intelligent framework for data enrichment during ingestion that optimizes
realtime multiclass classification by dynamically constructing model chains,
i.e ,sequences of machine learning models with varying cost-quality tradeoffs,
selected via a utilitybased criterion. Inspired by predicate ordering
techniques from database query processing, NOMAD leverages cheaper models as
initial filters, proceeding to more expensive models only when necessary, while
guaranteeing classification quality remains comparable to a designated role
model through a formal chain safety mechanism. It employs a dynamic belief
update strategy to adapt model selection based on per event predictions and
shifting data distributions, and extends to scenarios with dependent models
such as earlyexit DNNs and stacking ensembles. Evaluation across multiple
datasets demonstrates that NOMAD achieves significant computational savings
compared to static and naive approaches while maintaining classification
quality comparable to that achieved by the most accurate (and often the most
expensive) model.

</details>


### [185] [Embedding based Encoding Scheme for Privacy Preserving Record Linkage](https://arxiv.org/abs/2511.00414)
*Sirintra Vaiwsri,Thilina Ranbaduge*

Main category: cs.DB

TL;DR: 提出把q-gram嵌入并二值化为隐私保护编码的方法用于PPRL，实验证明在短文本记录上准确性和隐私保护均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 不同组织间需要合并敏感数据以发现新洞见，但出于隐私与合规原因不能直接共享数据；因此需要一种在不泄露实体信息的前提下实现跨库匹配的技术。

Method: 将单个q-gram映射到向量嵌入空间，随后将一条记录中所有q-gram的嵌入聚合并量化为二进制表示（编码），最终使用这些二进制表示计算记录间相似性以进行匹配。

Result: 在多个真实数据集上的实证评估显示，该编码在短记录值（如姓名、地址片段等）上比现有PPRL方法具有更高的链接精度，并能更好地抵抗针对隐私的攻击。

Conclusion: 本文提出一种基于嵌入的编码方法用于隐私保护记录链接（PPRL），通过将q-gram先映射到嵌入空间，再将记录的q-gram集合嵌入转换为二进制表示，从而在不泄露敏感信息的前提下进行匹配。实验表明该方法在短文本记录上相较于现有方法能提供更好的匹配准确性并增强对攻击的抵抗力。

Abstract: To discover new insights from data, there is a growing need to share
information that is often held by different organisations. One key task in data
integration is the calculation of similarities between records in different
databases to identify pairs or sets of records that correspond to the same
real-world entities. Due to privacy and confidentiality concerns, however, the
owners of sensitive databases are often not allowed or willing to exchange or
share their data with other organisations to allow such similarity
calculations. Privacy-preserving record linkage (PPRL) is the process of
matching records that refer to the same entity across sensitive databases held
by different organisations while ensuring no information about the entities is
revealed to the participating parties. In this paper, we study how embedding
based encoding techniques can be applied in the PPRL context to ensure the
privacy of the entities that are being linked. We first convert individual
q-grams into the embedded space and then convert the embedding of a set of
q-grams of a given record into a binary representation. The final binary
representations can be used to link records into matches and non-matches. We
empirically evaluate our proposed encoding technique against different
real-world datasets. The results suggest that our proposed encoding approach
can provide better linkage accuracy and protect the privacy of individuals
against attack compared to state-of-the-art techniques for short record values.

</details>


### [186] [Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries](https://arxiv.org/abs/2511.00693)
*Saba Latif,Huma Latif,Muhammad Rameez Ur Rahman*

Main category: cs.DB

TL;DR: 本文提出将 OCEDO 与 SPARQL 结合用于 BPIC2013 数据集，克服 XES 在对象-事件依赖表达上的限制，提升日志可读性与分析深度。


<details>
  <summary>Details</summary>
Motivation: XES 标准在表达事件间依赖和对象关系方面存在局限，难以揭示更丰富的过程信息；OCEDO 可填补该空白，使过程矿工更好理解事件依赖与过程动态。

Method: 提出将 OCEDO 本体与 SPARQL 查询结合，针对 BPIC2013 数据集进行建模与查询，实现事件与对象关系的显式化表示。

Result: 在 BPIC2013 上的实验展示 OCEDO 能提高日志的完整性和可读性，使得对象中心的建模比传统方法能支持更丰富的分析。

Conclusion: OCEDO 提升了事件日志的表达能力，使对象-事件关系和依赖更清晰，从而支持更丰富的过程分析。

Abstract: Object Centric Event Data (OCED) has gained attention in recent years within
the field of process mining. However, there are still many challenges, such as
connecting the XES format to object-centric approaches to enable more
insightful analysis. It is important for a process miner to understand the
insights and dependencies of events in the event log to see what is going on in
our processes. In previous standards, the dependencies of event logs are only
used to show events, but not their dependencies among each other and actions in
detail as described in OCEDO. There is more information in the event log when
it is revealed using the OCEDO model. It becomes more understandable and easier
to grasp the concepts and deal with the processes. This paper proposes the use
of Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of
the XES standard in event logs for process mining. We demonstrate how the OCEDO
approach, integrated with SPARQL queries, can be applied to the BPIC 2013
dataset to make the relationships between events and objects more explicit. It
describes dealing with the meta descriptions of the OCEDO model on a business
process challenge as an event log. It improves the completeness and readability
of process data, suggesting that object-centric modeling allows for richer
analyses than traditional approaches.

</details>


### [187] [Finding Non-Redundant Simpson's Paradox from Multidimensional Data](https://arxiv.org/abs/2511.00748)
*Yi Yang,Jian Pei,Jun Yang,Jichun Xie*

Main category: cs.DB

TL;DR: 提出一个识别并去重辛普森悖论的理论与算法框架，形式化冗余类型、证明等价关系并实现冗余感知的高效发现，显著降低冗余比例与计算开销，结果在规模与稳定性上均有优势。


<details>
  <summary>Details</summary>
Motivation: 现有辛普森悖论检测方法常产生大量冗余悖论（来自等价子集选择、相同子群划分或相关结果变量），这不仅掩盖了关键模式也浪费计算资源，因而需要一个能去重并高效发现实质悖论的框架。

Method: 作者定义了三种冗余类型（sibling child、separator、statistic equivalence），证明冗余构成等价关系，并设计了结合深度优先基表物化与冗余感知的悖论发现算法，用以系统化组织和剔除等价悖论。

Result: 在真实数据集和合成基准上，作者发现冗余悖论普遍存在（某些数据集占比超过40%），所提算法可扩展至百万级记录，运行时间最多减少60%，并能发现对数据扰动结构上稳健的悖论。

Conclusion: 本文提出了首个用于发现非冗余辛普森悖论（Simpson's paradox）的框架，通过形式化三类冗余并将其视作等价关系，从而实现了冗余悖论的紧凑表示与高效发现。

Abstract: Simpson's paradox, a long-standing statistical phenomenon, describes the
reversal of an observed association when data are disaggregated into
sub-populations. It has critical implications across statistics, epidemiology,
economics, and causal inference. Existing methods for detecting Simpson's
paradox overlook a key issue: many paradoxes are redundant, arising from
equivalent selections of data subsets, identical partitioning of
sub-populations, and correlated outcome variables, which obscure essential
patterns and inflate computational cost. In this paper, we present the first
framework for discovering non-redundant Simpson's paradoxes. We formalize three
types of redundancy - sibling child, separator, and statistic equivalence - and
show that redundancy forms an equivalence relation. Leveraging this insight, we
propose a concise representation framework for systematically organizing
redundant paradoxes and design efficient algorithms that integrate depth-first
materialization of the base table with redundancy-aware paradox discovery.
Experiments on real-world datasets and synthetic benchmarks show that redundant
paradoxes are widespread, on some real datasets constituting over 40% of all
paradoxes, while our algorithms scale to millions of records, reduce run time
by up to 60%, and discover paradoxes that are structurally robust under data
perturbation. These results demonstrate that Simpson's paradoxes can be
efficiently identified, concisely summarized, and meaningfully interpreted in
large multidimensional datasets.

</details>


### [188] [Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints](https://arxiv.org/abs/2511.00772)
*Raymond M. Xiong,Panyu Chen,Tianze Dong,Jian Lu,Benjamin Goldstein,Danyang Zhuo,Anru R. Zhang*

Main category: cs.DB

TL;DR: CELEC 用 LLM 将自然语言转为 SQL，结合元数据、少样本与链式思维，既保隐私又提高 EHR 查询可用性，性能与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 许多研究者缺乏编写复杂 SQL 和生成有效可视化的数据库技能，限制了 EHR 数据的高效利用与科学发现。

Method: 通过融合架构信息、少样本示例与链式思维提示策略驱动 LLM 生成 SQL，并在机构内执行以保障隐私。

Result: 在 EHRSQL 基准子集上，CELEC 达到与既有系统可比的执行准确率，同时保持低延迟、成本效益与严格隐私（仅暴露元数据给 LLM）。消融研究显示少样本示例尤为关键。

Conclusion: CELEC 是一个基于大语言模型的框架，能将自然语言查询转为 SQL 并在机构内部安全执行，从而降低研究者访问 EHR 数据库的门槛。

Abstract: Electronic health records (EHRs) are central to modern healthcare delivery
and research; yet, many researchers lack the database expertise necessary to
write complex SQL queries or generate effective visualizations, limiting
efficient data use and scientific discovery. To address this barrier, we
introduce CELEC, a large language model (LLM)-powered framework for automated
EHR data extraction and analytics. CELEC translates natural language queries
into SQL using a prompting strategy that integrates schema information,
few-shot demonstrations, and chain-of-thought reasoning, which together improve
accuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves
execution accuracy comparable to prior systems while maintaining low latency,
cost efficiency, and strict privacy by exposing only database metadata to the
LLM. CELEC also adheres to strict privacy protocols: the LLM accesses only
database metadata (e.g., table and column names), while all query execution
occurs securely within the institutional environment, ensuring that no
patient-level data is ever transmitted to or shared with the LLM. Ablation
studies confirm that each component of the SQL generation pipeline,
particularly the few-shot demonstrations, plays a critical role in performance.
By lowering technical barriers and enabling medical researchers to query EHR
databases directly, CELEC streamlines research workflows and accelerates
biomedical discovery.

</details>


### [189] [Efficient Query Repair for Aggregate Constraints](https://arxiv.org/abs/2511.00826)
*Shatha Algarni,Boris Glavic,Seokki Lee,Adriane Chapman*

Main category: cs.DB

TL;DR: 提出一种基于候选集合边界和区间算术的查询修复方法，通过高效剪枝降低搜索成本，实验优于逐候选基线。


<details>
  <summary>Details</summary>
Motivation: 现实场景中查询结果需满足特定领域约束（例如性别比例），这些约束可表示为对查询结果上若干聚合算术组合的约束。已有方法逐一检查候选修改效率低，需更高效的修复策略。

Method: 基于候选解集合的边界估计和区间算术进行剪枝：论文构造候选谓词的集合，对这些集合计算聚合表达式的上下界，从而快速排除不可能满足约束的集合，减少需要逐一检查的候选数目。实现时使用搜索策略遍历可能的谓词修改并结合区间运算进行早期剪枝。

Result: 实验显示，所提方法在多种基准上的运行时间和需要检查的候选数均显著优于逐个候选的基线方法，表明边界+区间算术剪枝策略能有效提升查询修复效率。

Conclusion: 该论文提出了一种通过修改查询的筛选谓词来修复不满足领域约束的查询的方法，利用候选解集合的界限和区间算术高效剪枝搜索空间，显著优于逐个候选考虑的基线方法。

Abstract: In many real-world scenarios, query results must satisfy domain-specific
constraints. For instance, a minimum percentage of interview candidates
selected based on their qualifications should be female. These requirements can
be expressed as constraints over an arithmetic combination of aggregates
evaluated on the result of the query. In this work, we study how to repair a
query to fulfill such constraints by modifying the filter predicates of the
query. We introduce a novel query repair technique that leverages bounds on
sets of candidate solutions and interval arithmetic to efficiently prune the
search space. We demonstrate experimentally, that our technique significantly
outperforms baselines that consider a single candidate at a time.

</details>


### [190] [All-in-one Graph-based Indexing for Hybrid Search on GPUs](https://arxiv.org/abs/2511.00855)
*Zhonggen Li,Yougen Li,Yifan Zhu,Zhaoqiang Chen,Yunjun Gao*

Main category: cs.DB

TL;DR: Allan-Poe通过GPU加速的统一图索引与动态融合框架，解决了混合检索的三难困境，实现高效、灵活、低存储的多路径检索，并在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合检索方法在效率、灵活性和存储开销之间存在三难困境：部分方法为效率牺牲灵活性；单独检索导致精度下降；灵活组合路径则需要高存储开销。

Method: 提出一个统一的图索引，将稠密向量、稀疏向量、全文本和知识图谱四种检索路径整合进单一结构；设计GPU加速构建流水线（warp级混合距离核、RNG-IP联合剪枝、关键词感知邻居回收）；引入动态融合框架支持任意路径组合与权重并利用知识图谱逻辑边处理多跳查询。

Result: 在6个真实数据集上的大量实验表明，Allan-Poe在端到端查询精度上表现更优，吞吐量较最先进方法提升1.5-186.4倍，同时显著降低存储开销。

Conclusion: Allan-Poe在保持高吞吐和低存储开销的同时，实现了灵活且精确的混合检索，适用于多路径检索场景。

Abstract: Hybrid search has emerged as a promising paradigm to overcome the limitations
of single-path retrieval, enhancing accuracy for applications like
recommendations, information retrieval, and Retrieval-Augmented Generation.
However, existing methods are constrained by a trilemma: they sacrifice
flexibility for efficiency, suffer from accuracy degradation due to separate
retrievals, or incur prohibitive storage overhead for flexible combinations of
retrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph
index accelerated by GPUs for efficient hybrid search. We first analyze the
limitations of existing retrieval paradigms and distill key design principles
for an effective hybrid search index. Guided by these principles, we architect
a unified graph-based index that flexibly integrates four retrieval paths-dense
vector, sparse vector, full-text, and knowledge graph-within a single, cohesive
structure. To enable efficient construction, we design a GPU-accelerated
pipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,
and keyword-aware neighbor recycling. For query processing, we introduce a
dynamic fusion framework that supports any combination of retrieval paths and
weights without index reconstruction, leveraging logical edges from the
knowledge graph to resolve complex multi-hop queries. Extensive experiments on
6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end
query accuracy and outperforms state-of-the-art methods by 1.5-186.4x in
throughput, while significantly reducing storage overhead.

</details>


### [191] [FlowLog: Efficient and Extensible Datalog via Incrementality](https://arxiv.org/abs/2511.00865)
*Hangdong Zhao,Zhenghong Yu,Srinag Rao,Simon Frisk,Zhiwei Fan,Paraschos Koutris*

Main category: cs.DB

TL;DR: 提出 FlowLog：基于每条规则的关系 IR，将递归控制与逻辑计划分离，结合 SQL 优化与递归感知优化（布尔特化），构建于 Differential Dataflow，实现高效且鲁棒的 Datalog 执行，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有 Datalog 系统在效率与可扩展性之间存在权衡：专用引擎（如 Souffle）高效但缺乏通用性，基于数据库的解决方案（如 RecStep）模块化但难以整合 Datalog 专有优化。需要一种既能保持 Datalog 优化能力又能复用数据库执行基础设施的设计。

Method: 在每条规则使用显式关系 IR，将递归控制（如半朴素执行）与规则的逻辑计划分离；在 IR 层应用 SQL 类优化（逻辑融合、子计划重用）；采用结构性优化器（避免最坏情况的笛卡尔/笛合并）和侧向信息传递（早期过滤）以提高鲁棒性；构建于 Differential Dataflow 之上，支持批处理与增量计算，并引入布尔/代数特化等递归感知优化。

Result: 实验表明 FlowLog 在各种递归工作负载下均优于现有最先进 Datalog 引擎和现代数据库，展现出更好的可扩展性，同时保持简单且可扩展的架构。

Conclusion: FlowLog 在效率与可扩展性之间取得平衡，通过在每条规则级别引入显式关系中间表示（IR），将递归控制与逻辑计划分离，从而既能保留 Datalog 特定优化，又能复用数据库执行原语，最终实现高性能与可扩展性。

Abstract: Datalog-based languages are regaining popularity as a powerful abstraction
for expressing recursive computations in domains such as program analysis and
graph processing. However, existing systems often face a trade-off between
efficiency and extensibility. Engines like Souffle achieve high efficiency
through domain-specific designs, but lack general-purpose flexibility. Others,
like RecStep, offer modularity by layering Datalog on traditional databases,
but struggle to integrate Datalog-specific optimizations.
  This paper bridges this gap by presenting FlowLog, a new Datalog engine that
uses an explicit relational IR per-rule to cleanly separate recursive control
(e.g., semi-naive execution) from each rule's logical plan. This boundary lets
us retain fine-grained, Datalog-aware optimizations at the logical layer, but
also reuse off-the-shelf database primitives at execution. At the logical level
(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan
reuse. To address high volatility in recursive workloads, we adopt a
robustness-first approach that pairs a structural optimizer (avoiding
worst-case joins) with sideways information passing (early filtering). Built
atop Differential Dataflow--a mature framework for streaming analytics--FlowLog
supports both batch and incremental Datalog and adds novel recursion-aware
optimizations called Boolean (or algebraic) specialization. Our evaluation
shows that FlowLog outperforms state-of-the-art Datalog engines and modern
databases across a broad range of recursive workloads, achieving superior
scalability while preserving a simple and extensible architecture.

</details>


### [192] [ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL](https://arxiv.org/abs/2511.00985)
*Yiwen Jiao,Tonghui Ren,Yuche Gao,Zhenying He,Yinan Jing,Kai Zhang,X. Sean Wang*

Main category: cs.DB

TL;DR: ORANGE从翻译日志增量构建数据库知识库，结合嵌套CoT与tuple追踪降低语义错误，持续提升Text-to-SQL性能。


<details>
  <summary>Details</summary>
Motivation: LLM虽能将自然语言翻成SQL，但缺乏对特定数据库模式和数据语义的长期积累，历史翻译日志蕴含了运行时的真实使用模式，可用于弥补这一领域知识缺口。

Method: 提出在线自演化框架ORANGE：解析翻译日志中的SQL生成包含schema和数据语义的知识项；采用嵌套Chain-of-Thought的SQL-to-Text策略并加入tuple语义追踪以降低语义错误；在生成过程中不断更新知识库并用于后续翻译提示或检索。

Result: 在多种基准测试上，ORANGE在处理复杂与领域特定查询时显著提升Text-to-SQL的准确率，验证了其在真实部署场景中的实用性。

Conclusion: ORANGE通过从历史翻译日志中解析SQL构建数据库专属知识库，逐步弥合通用LLM与数据库领域语义之间的差距，从而提升Text-to-SQL的准确性与鲁棒性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
translating natural language to SQL, but a significant semantic gap persists
between their general knowledge and domain-specific semantics of databases.
Historical translation logs constitute a rich source of this missing in-domain
knowledge, where SQL queries inherently encapsulate real-world usage patterns
of database schema. Existing methods primarily enhance the reasoning process
for individual translations but fail to accumulate in-domain knowledge from
past translations. We introduce ORANGE, an online self-evolutionary framework
that constructs database-specific knowledge bases by parsing SQL queries from
translation logs. By accumulating in-domain knowledge that contains schema and
data semantics, ORANGE progressively reduces the semantic gap and enhances the
accuracy of subsequent SQL translations. To ensure reliability, we propose a
novel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic
tracking, which reduces semantic errors during knowledge generation.
Experiments on multiple benchmarks confirm the practicality of ORANGE,
demonstrating its effectiveness for real-world Text-to-SQL deployment,
particularly in handling complex and domain-specific queries.

</details>


### [193] [PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search](https://arxiv.org/abs/2511.00995)
*Tianming Wu,Dixin Tang*

Main category: cs.DB

TL;DR: 提出PathFinder：可按属性构建图ANNS索引并用代价优化器组合以高效处理复杂过滤，带来显著性能提升（最多9.8x，recall=0.95）。


<details>
  <summary>Details</summary>
Motivation: 现有图索引在带过滤条件的ANNS上要么只对单一属性高效但不支持复杂多属性过滤，要么不能在准确率和性能间取得良好平衡；受关系型数据库多索引与优化器思想启发，提出可选择建立属性索引并由优化器利用它们处理复杂过滤。

Method: 提出一种框架，支持按属性创建优化的图索引，并引入三个技术：新的优化度量（在查询时间与准确率间权衡）、用于处理与/或混合过滤的两阶段优化流程、以及索引借用（用某属性索引处理另一属性过滤）。系统在执行时基于代价模型选取并组合已有索引。

Result: 在四个真实数据集上实验表明，PathFinder在召回率0.95下查询吞吐量优于最优基线，最高提升约9.8倍。

Conclusion: PathFinder通过允许选择性构建针对具体属性优化的ANNS索引并用基于代价的优化器组合这些索引来处理复杂过滤条件，从而在保证召回率的同时显著提升过滤近似最近邻查询效率。

Abstract: Filtered approximate nearest neighbor search (ANNS) restricts the search to
data objects whose attributes satisfy a given filter and retrieves the top-$K$
objects that are most semantically similar to the query object. Many
graph-based ANNS indexes are proposed to enable efficient filtered ANNS but
remain limited in applicability or performance: indexes optimized for a
specific attribute achieve high efficiency for filters on that attribute but
fail to support complex filters with arbitrary conjunctions and disjunctions
over multiple attributes. Inspired by the design of relational databases, this
paper presents PathFinder, a new indexing framework that allows users to
selectively create ANNS indexes optimized for filters on specific attributes
and employs a cost-based optimizer to efficiently utilize them for processing
complex filters. PathFinder includes three novel techniques: 1) a new
optimization metric that captures the tradeoff between query execution time and
accuracy, 2) a two-phase optimization for handling filters with conjunctions
and disjunctions, and 3) an index borrowing optimization that uses an
attribute-specific index to process filters on another attribute. Experiments
on four real-world datasets show that PathFinder outperforms the best baseline
by up to 9.8x in query throughput at recall 0.95.

</details>


### [194] [Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index](https://arxiv.org/abs/2511.01025)
*Huihui Yang,Pingpeng Yuan*

Main category: cs.DB

TL;DR: 提出了可表达复杂标签约束的复合模式和相应的PCR问题；由于NP-hard，设计了两维哈希+路径的TDR索引将可达集分块并双重过滤以剪枝；实验在16个数据集上证明索引更小、构建更快、查询高效。


<details>
  <summary>Details</summary>
Motivation: 现有的标签约束可达性（LCR）和正则路径查询（RPQ）无法通过组合查询模式来表达复杂的标签约束，需更灵活的查询模式来满足复杂应用需求。

Method: 引入复合模式（logical expressions of label sets）定义PCR问题，证明PCR为NP-hard；设计TDR索引，水平维度为多路哈希索引存储按标签组合分块的可达顶点，垂直维度为路径索引；将每个顶点的可达顶点分解为多个块，分别哈希到两维索引，查询时先用水平索引做全局过滤再用垂直索引做局部精检以剪枝搜索空间。

Result: 通过在16个真实数据集上的实验，TDR在索引大小和构建时间上优于最先进的LCR索引技术，同时能够高效地处理PCR查询（包含LCR作为特例）。

Conclusion: 本文提出了用于复杂边标记有向图的模式约束可达性查询（PCR），并设计了二维可达性索引（TDR）来提高查询性能。TDR通过水平的多路索引和垂直的路径索引作为全局与局部过滤器，利用块哈希减少指数组合带来的开销，实验表明在索引大小和构建时间上优于现有方法，且能高效回答PCR和LCR查询。

Abstract: Reachability queries ask whether there exists a path from the source vertex
to the target vertex on a graph. Recently, several powerful reachability
queries, such as Label-Constrained Reachability (LCR) queries and Regular Path
Queries (RPQ), have been proposed for emerging complex edge-labeled digraphs.
However, they cannot allow users to describe complex query requirements by
composing query patterns. Here, we introduce composite patterns, a logical
expression of patterns that can express complex constraints on the set of
labels. Based on pattern, we propose pattern-constrained reachability queries
(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the
performance to answer PCR queries, we build a two-dimensional reachability (TDR
for short) index which consists of a multi-way index (horizontal dimension) and
a path index (vertical dimension). Because the number of combinations of both
labels and vertices is exponential, it is very expensive to build full indices
that contain all the reachability information. Thus, the reachable vertices of
a vertex are decomposed into blocks, each of which is hashed into the
horizontal dimension index and the vertical dimension index, respectively. The
indices in the horizontal dimension and the vertical dimension serve as a
global filter and a local filter, respectively, to prune the search space.
Experimental results demonstrate that our index size and indexing time
outperform the state-of-the-art label-constrained reachability indexing
technique on 16 real datasets. TDR can efficiently answer pattern-constrained
reachability queries, including label-constrained reachability queries.

</details>


### [195] [L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3](https://arxiv.org/abs/2511.01602)
*Xinyue Yang,Chen Zheng,Yaoyang Hou,Renhao Zhang,Yiyan Zhang,Yanjun Wu,Heng Zhang*

Main category: cs.DB

TL;DR: L2T-Tune结合LLM提示与TD3强化学习的三阶段框架，通过温启动样本池、LLM挖掘提示与降维微调，显著加速收敛并提高性能，平均提升37.1%，在线仅需30步达到最优。


<details>
  <summary>Details</summary>
Motivation: 动机在于克服当前数据库调优面临的三大挑战：高维旋钮空间导致的优化不稳定且收敛慢；强化学习管线缺乏有效的热启动与需长时间离线训练；以及模型在硬件或负载变化时迁移能力差，需要大量重训练。

Method: 三阶段方法：1) 温启动阶段：生成并记录均匀覆盖旋钮空间的样本到共享池；2) LLM引导阶段：利用大语言模型从手册和社区文档中挖掘并优先级排序调优提示，加速收敛；3) 强化学习阶段：用温启动样本池做降维（旋钮与状态特征），然后用TD3算法微调配置。

Result: 实验对比表明L2T-Tune在所有工作负载上的平均性能提升为37.1%，在TPC-C上最高达73%。与纯强化学习模型相比，离线调优阶段收敛更快；在线阶段仅需30步即可达到最优性能。

Conclusion: L2T-Tune通过混合LLM引导与强化学习的三阶段管线，有效解决了传统数据库参数调优中收敛慢、热启动不足与迁移性差的问题，实验显示在多种工作负载下取得显著性能提升。

Abstract: Configuration tuning is critical for database performance. Although recent
advancements in database tuning have shown promising results in throughput and
latency improvement, challenges remain. First, the vast knob space makes direct
optimization unstable and slow to converge. Second, reinforcement learning
pipelines often lack effective warm-start guidance and require long offline
training. Third, transferability is limited: when hardware or workloads change,
existing models typically require substantial retraining to recover
performance.
  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid
database tuning framework that features a three-stage pipeline: Stage one
performs a warm start that simultaneously generates uniform samples across the
knob space and logs them into a shared pool; Stage two leverages a large
language model to mine and prioritize tuning hints from manuals and community
documents for rapid convergence. Stage three uses the warm-start sample pool to
reduce the dimensionality of knobs and state features, then fine-tunes the
configuration with the Twin Delayed Deep Deterministic Policy Gradient
algorithm.
  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared
with the best-performing alternative, our approach improves performance by an
average of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with
models trained with reinforcement learning, it achieves rapid convergence in
the offline tuning stage on a single server. Moreover, during the online tuning
stage, it only takes 30 steps to achieve best results.

</details>


### [196] [UniDataBench: Evaluating Data Analytics Agents Across Structured and Unstructured Data](https://arxiv.org/abs/2511.01625)
*Han Weng,Zhou Liu,Yuanfeng Song,Xiaoming Yin,Xing Chen,Wentao Zhang*

Main category: cs.DB

TL;DR: 提出UniDataBench基准与LLM代理ReActInsight，用于评测和实现跨关系型、CSV与NoSQL等多源数据的端到端自动化分析。


<details>
  <summary>Details</summary>
Motivation: 现实业务中数据分散在多种格式和存储系统，现有基准无法全面评估代理跨多源数据的分析能力，因此需要构建更贴近工业场景的评测基准并设计相应的智能代理。

Method: 从真实行业分析报告出发，构建包含关系型数据库、CSV文件及NoSQL数据的多样化数据集，设计隐私敏感信息去除管道；提出统一评测框架评价代理在探索多格式数据、提取洞察与生成总结建议的能力；并实现ReActInsight——一个基于LLM的自动化代理，包含跨源链路发现、任务分解与自校正代码生成模块。

Result: 构建了UniDataBench并基于其开发了ReActInsight。论文声称该组合能更真实地评估和提升数据分析代理在跨源、多格式场景下的表现，并展示了ReActInsight在自动发现关联、生成自校正代码和产出可操作洞察方面的有效性（具体实验指标和数值需看正文）。

Conclusion: 该论文提出了一个面向多源数据分析代理评测的基准UniDataBench，并设计了一个可去隐私化的真实行业报告到数据集的构建流程；同时提出了基于大模型的代理ReActInsight，用以自动发现跨源关联、分解任务并生成自校正代码，完成端到端分析。

Abstract: In the real business world, data is stored in a variety of sources, including
structured relational databases, unstructured databases (e.g., NoSQL
databases), or even CSV/excel files. The ability to extract reasonable insights
across these diverse source is vital for business success. Existing benchmarks,
however, are limited in assessing agents' capabilities across these diverse
data types. To address this gap, we introduce UniDataBench, a comprehensive
benchmark designed to evaluate the performance of data analytics agents in
handling diverse data sources. Specifically, UniDataBench is originating from
real-life industry analysis report and we then propose a pipeline to remove the
privacy and sensitive information. It encompasses a wide array of datasets,
including relational databases, CSV files to NoSQL data, reflecting real-world
business scenarios, and provides unified framework to assess how effectively
agents can explore multiple data formats, extract valuable insights, and
generate meaningful summaries and recommendations. Based on UniDataBench, we
propose a novel LLM-based agent named ReActInsight, an autonomous agent that
performs end-to-end analysis over diverse data sources by automatically
discovering cross-source linkages, decomposing goals, and generating robust,
self-correcting code to extract actionable insights. Our benchmark and agent
together provide a powerful framework for advancing the capabilities of data
analytics agents in real-world applications.

</details>


### [197] [SemBench: A Benchmark for Semantic Query Processing Engines](https://arxiv.org/abs/2511.01716)
*Jiale Lao,Andreas Zimmerer,Olga Ovcharenko,Tianji Cong,Matthew Russo,Gerardo Vitagliano,Michael Cochez,Fatma Özcan,Gautam Gupta,Thibaud Hottelier,H. V. Jagadish,Kris Kissel,Sebastian Schelter,Andreas Kipf,Immanuel Trummer*

Main category: cs.DB

TL;DR: 提出面向语义查询处理引擎的多场景多模态基准，扩展SQL支持自然语言配置的语义操作符，评估了四套系统并揭示其能力边界与待改进点。


<details>
  <summary>Details</summary>
Motivation: 随着基于大模型的系统兴起，出现了新型的语义查询处理引擎，现有评测不足以衡量其在多模态、多场景、复杂语义操作上的能力，故需设计专门基准以揭示系统优劣与研究方向。

Method: 论文构建了一个多维度的基准测试，涵盖三类多样性：场景（如电影评论分析、医学问答）、模态（图像、音频、文本）和操作符（语义过滤、连接、映射、排序、分类等）。在三个学术系统（LOTUS、Palimpzest、ThalamusDB）及工业系统（Google BigQuery）上进行了评估。

Result: 评测揭示了各系统在不同场景和操作上的强项与短板，尽管具体结果受系统持续改进影响，但提供了有价值的洞见并指出未来研究的方向。

Conclusion: 该论文提出了针对“语义查询处理引擎”的基准评测，强调其依赖生成式大模型与推理能力，通过扩展SQL加入由自然语言配置的语义操作符来对多模态数据执行查询。

Abstract: We present a benchmark targeting a novel class of systems: semantic query
processing engines. Those systems rely inherently on generative and reasoning
capabilities of state-of-the-art large language models (LLMs). They extend SQL
with semantic operators, configured by natural language instructions, that are
evaluated via LLMs and enable users to perform various operations on multimodal
data.
  Our benchmark introduces diversity across three key dimensions: scenarios,
modalities, and operators. Included are scenarios ranging from movie review
analysis to medical question-answering. Within these scenarios, we cover
different data modalities, including images, audio, and text. Finally, the
queries involve a diverse set of operators, including semantic filters, joins,
mappings, ranking, and classification operators.
  We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and
ThalamusDB) and one industrial system, Google BigQuery. Although these results
reflect a snapshot of systems under continuous development, our study offers
crucial insights into their current strengths and weaknesses, illuminating
promising directions for future research.

</details>
