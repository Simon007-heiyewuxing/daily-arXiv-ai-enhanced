<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

TL;DR: 提出一个基于预训练3D生成模型和动态感知3D反演的管线（SuperHead），将低分辨率动态头部提升为高分辨率、可动画的3DGS头像，并通过多视角2D渲染+深度联合监督保证3D与时间一致性和身份保持。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，很多可动画3D头像来自低质量图像/视频源，导致3D重建质量低，影响沉浸式应用；现有图像/视频/3D超分辨方法无法很好处理动态3D输入，因此需要一种能保持3D与时间一致性且保留身份特征的超分辨方法。

Method: 提出动态感知的3D反演：将预训练3D生成模型的潜变量优化为生成高分辨率的3D Gaussian Splatting头模型；反演过程联合监督来自多视角、多表情的上采样2D人脸渲染图和对应深度图；将生成的3DGS模型绑定到参数化头模型（如FLAME）以实现可动画驱动。

Result: 实验表明，SuperHead在动态表情和不同视角下生成了细腻的面部细节，视觉质量显著优于基线方法。

Conclusion: 该论文提出了SuperHead框架，通过利用预训练3D生成模型的先验和动态感知的3D反演方案，实现了对低分辨率动态3D头部模型的超分辨率重建，从而得到高保真且可动画的3D头像。

Abstract: Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [2] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

TL;DR: 论文通过自动化生成高质量的自我视角音视联合数据（EgoAVU），并用其构建大规模训练集与评测集，成功提升MLLM在egocentric视频中对音频与视觉联合理解的能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM能接受视觉和音频输入，但缺乏带有一致跨模态信息的标注数据，导致其在自我视角视频中往往偏重视觉信号，忽视音频或无法将音频与视觉源匹配。作者希望通过大规模自动化数据生成来填补这一空白，提升模型的音画联合理解能力。

Method: 提出EgoAVU数据流水线，包括：基于跨模态相关性建模将人工叙述扩展为音视联合叙述；基于token的视频筛选和模块化图结构数据策划以保证多样性与质量；自动生成叙述、问答对并构建大规模训练集（3M样本）及人工验证的测试集。对MLLM进行微调并在EgoAVU-Bench和其他数据集上评估性能提升。

Result: 构建了3M样本的EgoAVU-Instruct训练集和人工验证的EgoAVU-Bench评估集。发现未微调的MLLM在EgoAVU-Bench上存在强视觉偏向和音频-视觉匹配失败。对MLLM在EgoAVU-Instruct上微调后，在EgoAVU-Bench上性能提升最高达113%，并在EgoTempo和EgoIllusion上转移提升最多达28%。

Conclusion: 这篇论文提出了EgoAVU数据引擎，用于自动生成包含音频和视觉信息的自我视角（egocentric）视频叙述、问答数据，从而用于训练和评估多模态大语言模型（MLLM）在听觉—视觉联合理解上的能力。作者构建了3M规模的训练集EgoAVU-Instruct和人工验证的测试集EgoAVU-Bench，证明现有MLLM在该类任务上偏向视觉、忽视音频；在EgoAVU-Instruct上微调可显著提升模型在EgoAVU-Bench及其他基准（如EgoTempo、EgoIllusion）上的表现。

Abstract: Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [3] [MGP-KAD: Multimodal Geometric Priors and Kolmogorov-Arnold Decoder for Single-View 3D Reconstruction in Complex Scenes](https://arxiv.org/abs/2602.06158)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: 提出MGP-KAD：结合类级几何先验与KAN混合解码器的多模态融合框架，在Pix3D上实现SOTA单视图3D重建，显著改善几何与细节表现。


<details>
  <summary>Details</summary>
Motivation: 复杂真实场景中噪声多、物体多样且数据集有限，传统单视图重建难以恢复精细几何与全局一致性，因此引入类级几何先验与更强的解码器以提高重建鲁棒性与精度。

Method: 通过对真实物体点云进行采样与聚类构建类级几何先验，并在训练过程中动态更新这些特征；将几何先验与RGB特征进行多模态融合；设计基于KAN的混合解码器以替代传统线性解码器，更好地处理复杂输入并生成高质量3D重建。

Result: 在Pix3D上，MGP-KAD在多个评价指标上优于现有方法，具体体现在几何完整性、表面平滑度与细节保留的显著提升，证明了类级几何先验与KAN解码器的有效性。

Conclusion: MGP-KAD在单视图三维重建上提出了结合RGB与类级几何先验的多模态特征融合框架，配合基于Kolmogorov-Arnold网络的混合解码器，能够在Pix3D数据集上显著提升几何完整性、平滑性与细节保留，达到SOTA性能。

Abstract: Single-view 3D reconstruction in complex real-world scenes is challenging due to noise, object diversity, and limited dataset availability. To address these challenges, we propose MGP-KAD, a novel multimodal feature fusion framework that integrates RGB and geometric prior to enhance reconstruction accuracy. The geometric prior is generated by sampling and clustering ground-truth object data, producing class-level features that dynamically adjust during training to improve geometric understanding. Additionally, we introduce a hybrid decoder based on Kolmogorov-Arnold Networks (KAN) to overcome the limitations of traditional linear decoders in processing complex multimodal inputs. Extensive experiments on the Pix3D dataset demonstrate that MGP-KAD achieves state-of-the-art (SOTA) performance, significantly improving geometric integrity, smoothness, and detail preservation. Our work provides a robust and effective solution for advancing single-view 3D reconstruction in complex scenes.

</details>


### [4] [Driving with DINO: Vision Foundation Features as a Unified Bridge for Sim-to-Real Generation in Autonomous Driving](https://arxiv.org/abs/2602.06159)
*Xuyang Chen,Conglang Zhang,Chuanheng Fu,Zihao Yang,Kaixuan Zhou,Yizhi Zhang,Jianan He,Yanfeng Zhang,Mingwei Sun,Zengmao Wang,Zhen Dong,Xiaoxiao Long,Liqiu Meng*

Main category: cs.CV

TL;DR: DwD leverages DINOv3 features with subspace projection, random channel drop, spatial alignment, and causal temporal aggregation to solve the Consistency-Realism Dilemma in Sim2Real driving video generation, achieving photorealistic, temporally-stable, and controllable outputs.


<details>
  <summary>Details</summary>
Motivation: Existing Sim2Real methods face a Consistency-Realism Dilemma: low-level signals give precise control but look synthetic; high-level priors are realistic but lack structural detail for consistent guidance. Need a unified representation that balances both.

Method: Use DINOv3 features as unified representation; apply Principal Subspace Projection to remove high-frequency texture components; introduce Random Channel Tail Drop to retain structural detail; add learnable Spatial Alignment Module to align high-resolution DINO features to diffusion model; build Causal Temporal Aggregator with causal convolutions to preserve motion history; integrate into controllable diffusion pipeline for video generation.

Result: DwD demonstrates improved photorealism without sacrificing control precision, less motion blur, and better temporal stability in generated driving videos, validated qualitatively and presumably quantitatively (project page provided).

Conclusion: DwD successfully reconciles the trade-off between realism and control consistency in Sim2Real autonomous driving video generation by using VFM (DINOv3) features, dimension reduction, random channel drop, spatial alignment, and causal temporal aggregation, leading to improved photorealism and temporal stability.

Abstract: Driven by the emergence of Controllable Video Diffusion, existing Sim2Real methods for autonomous driving video generation typically rely on explicit intermediate representations to bridge the domain gap. However, these modalities face a fundamental Consistency-Realism Dilemma. Low-level signals (e.g., edges, blurred images) ensure precise control but compromise realism by "baking in" synthetic artifacts, whereas high-level priors (e.g., depth, semantics, HDMaps) facilitate photorealism but lack the structural detail required for consistent guidance. In this work, we present Driving with DINO (DwD), a novel framework that leverages Vision Foundation Module (VFM) features as a unified bridge between the simulation and real-world domains. We first identify that these features encode a spectrum of information, from high-level semantics to fine-grained structure. To effectively utilize this, we employ Principal Subspace Projection to discard the high-frequency elements responsible for "texture baking," while concurrently introducing Random Channel Tail Drop to mitigate the structural loss inherent in rigid dimensionality reduction, thereby reconciling realism with control consistency. Furthermore, to fully leverage DINOv3's high-resolution capabilities for enhancing control precision, we introduce a learnable Spatial Alignment Module that adapts these high-resolution features to the diffusion backbone. Finally, we propose a Causal Temporal Aggregator employing causal convolutions to explicitly preserve historical motion context when integrating frame-wise DINO features, which effectively mitigates motion blur and guarantees temporal stability. Project page: https://albertchen98.github.io/DwD-project/

</details>


### [5] [MetaSSP: Enhancing Semi-supervised Implicit 3D Reconstruction through Meta-adaptive EMA and SDF-aware Pseudo-label Evaluation](https://arxiv.org/abs/2602.06163)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

TL;DR: MetaSSP通过梯度重要性正则化和SDF感知伪标签加权的半监督框架，有效利用未标注图像，在单视图SDF重建上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 隐式SDF方法在单视图三维重建上效果好，但对大量有标签数据依赖大，限制了可扩展性；目标是利用大量无标签图像改善性能，减少对标注的依赖。

Method: 提出基于梯度的参数重要性估计来正则化自适应EMA更新；设计结合增强一致性与SDF方差的SDF感知伪标签权重策略；采用10%有监督预热，然后统一训练带标签与不带标签的数据。

Result: 在Pix3D基准上，Chamfer Distance约减少20.61%，IoU提升约24.09%，优于现有半监督基线并刷新最优水平。

Conclusion: MetaSSP通过引入梯度基参数重要性估计和SDF感知伪标签加权，有效利用未标注图像，提高了隐式SDF单视图重建的性能；在Pix3D上显著优于现有半监督方法，取得最好的指标。

Abstract: Implicit SDF-based methods for single-view 3D reconstruction achieve high-quality surfaces but require large labeled datasets, limiting their scalability. We propose MetaSSP, a novel semi-supervised framework that exploits abundant unlabeled images. Our approach introduces gradient-based parameter importance estimation to regularize adaptive EMA updates and an SDF-aware pseudo-label weighting mechanism combining augmentation consistency with SDF variance. Beginning with a 10% supervised warm-up, the unified pipeline jointly refines labeled and unlabeled data. On the Pix3D benchmark, our method reduces Chamfer Distance by approximately 20.61% and increases IoU by around 24.09% compared to existing semi-supervised baselines, setting a new state of the art.

</details>


### [6] [M3: High-fidelity Text-to-Image Generation via Multi-Modal, Multi-Agent and Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.06166)
*Bangji Yang,Ruihan Guo,Jiajun Fan,Chaoran Cheng,Ge Liu*

Main category: cs.CV

TL;DR: M3通过多智能体、多轮迭代在推理阶段分解并逐项修正组合性文本提示，训练Free且可插拔地显著提升开源T2I模型在复杂合成任务上的表现，达到了超越多款商用系统的效果。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在文本到图像任务上虽然保真度高，但面对包含多重约束的复杂组合提示时经常失败，尤其是空间关系、多个对象、属性与交互等组合性问题。为避免代价高昂的重训练，提出基于多模态多智能体的推理级解决方案。

Method: M3构建了一个多智能体循环体系：Planner将复杂提示分解为可验证的清单，Checker识别违反约束的部分，Refiner按项修复，Editor执行图像修改，Verifier确保每步单调改进。所有模块均使用现成的大模型和工具，整个流程无需重新训练，作为插拔式前处理或后处理模块与任意T2I模型配合使用。

Result: 在OneIG-EN基准上，Qwen-Image结合M3达到0.532的整体性能，超过商用旗舰系统Imagen4(0.515)和Seedream3.0(0.530)。在GenEval组合性指标上也有显著提升，特别是在艰难测试集上，空间推理性能约翻倍。

Conclusion: M3是一个训练Free、基于多智能体的推理框架，能在推理时通过多轮迭代分解并修正复杂合成文本到图像的约束，从而显著提升开源模型在复杂组合提示下的生成质量。

Abstract: Generative models have achieved impressive fidelity in text-to-image synthesis, yet struggle with complex compositional prompts involving multiple constraints. We introduce \textbf{M3 (Multi-Modal, Multi-Agent, Multi-Round)}, a training-free framework that systematically resolves these failures through iterative inference-time refinement. M3 orchestrates off-the-shelf foundation models in a robust multi-agent loop: a Planner decomposes prompts into verifiable checklists, while specialized Checker, Refiner, and Editor agents surgically correct constraints one at a time, with a Verifier ensuring monotonic improvement. Applied to open-source models, M3 achieves remarkable results on the challenging OneIG-EN benchmark, with our Qwen-Image+M3 surpassing commercial flagship systems including Imagen4 (0.515) and Seedream 3.0 (0.530), reaching state-of-the-art performance (0.532 overall). This demonstrates that intelligent multi-agent reasoning can elevate open-source models beyond proprietary alternatives. M3 also substantially improves GenEval compositional metrics, effectively doubling spatial reasoning performance on hardened test sets. As a plug-and-play module compatible with any pre-trained T2I model, M3 establishes a new paradigm for compositional generation without costly retraining.

</details>


### [7] [Unsupervised Anomaly Detection of Diseases in the Female Pelvis for Real-Time MR Imaging](https://arxiv.org/abs/2602.06179)
*Anika Knupfer,Johanna P. Müller,Jordina A. Verdera,Martin Fenske,Claudius S. Mathy,Smiti Tripathy,Sebastian Arndt,Matthias May,Michael Uder,Matthias W. Beckmann,Stefanie Burghaus,Jana Hutter*

Main category: cs.CV

TL;DR: 提出并验证了一个基于残差VAE的无监督、实时兼容盆腔MRI异常检测基线：在仅用健康数据训练下，通过重构误差热图检测病灶，公共数据集AUC≈0.736，重构速度≈92.6 FPS，可支持临床实时应用的后续研究。


<details>
  <summary>Details</summary>
Motivation: 临床上盆腔疾病具有高度解剖异质性，导致MRI解读困难且诊断常延迟。现有AI多为疾病特定且不可实时应用，限制了泛化性和临床集成。故提出疾病和参数无关、支持实时的无监督异常检测基线框架。

Method: 使用仅含健康矢状位T2加权序列的294例扫描训练残差VAE，辅以扩散生成的合成数据增强鲁棒性。推理时通过重构误差热图检测偏离正常结构的病灶。评估包括在公开子宫肌瘤MRI数据集上的定量AUC、灵敏度和特异度，以及多病种的临床读片者间评估。实现了约92.6帧/秒的重构速度，满足实时兼容性要求。

Result: 在Uterine Myoma MRI数据集上平均AUC=0.736，灵敏度=0.828，特异度=0.692；临床多读者评估扩展到子宫内膜癌、子宫腺肌症和子宫内膜异位症，分析显示解剖异质性和读者间差异会影响性能解读。代码可按需获得，支持未来与实时MRI集成。

Conclusion: 该论文提出了一个基于残差变分自编码器(residual VAE)的无监督异常检测框架，用于女性盆腔MRI的疾病检测，能够在无异常标签的数据上学习正常解剖结构并通过重构误差热图定位异常区域。

Abstract: Pelvic diseases in women of reproductive age represent a major global health burden, with diagnosis frequently delayed due to high anatomical variability, complicating MRI interpretation. Existing AI approaches are largely disease-specific and lack real-time compatibility, limiting generalizability and clinical integration. To address these challenges, we establish a benchmark framework for disease- and parameter-agnostic, real-time-compatible unsupervised anomaly detection in pelvic MRI. The method uses a residual variational autoencoder trained exclusively on healthy sagittal T2-weighted scans acquired across diverse imaging protocols to model normal pelvic anatomy. During inference, reconstruction error heatmaps indicate deviations from learned healthy structure, enabling detection of pathological regions without labeled abnormal data. The model is trained on 294 healthy scans and augmented with diffusion-generated synthetic data to improve robustness. Quantitative evaluation on the publicly available Uterine Myoma MRI Dataset yields an average area-under-the-curve (AUC) value of 0.736, with 0.828 sensitivity and 0.692 specificity. Additional inter-observer clinical evaluation extends analysis to endometrial cancer, endometriosis, and adenomyosis, revealing the influence of anatomical heterogeneity and inter-observer variability on performance interpretation. With a reconstruction time of approximately 92.6 frames per second, the proposed framework establishes a baseline for unsupervised anomaly detection in the female pelvis and supports future integration into real-time MRI. Code is available upon request (https://github.com/AniKnu/UADPelvis), prospective data sets are available for academic collaboration.

</details>


### [8] [PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining](https://arxiv.org/abs/2602.06184)
*Cheng Liang,Chaoyi Wu,Weike Zhao,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 通过构建PhenoKG并提出PhenoLIP，将表型本体知识融入医学VLM，显著提升表型识别与跨模态检索表现。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM多依赖粗粒度图文对比学习，难以捕捉医学表型本体中系统性的结构化视觉知识，故需引入表型中心的知识以提升可解释性与识别精度。

Method: 构建PhenoKG大规模表型多模态知识图谱（520K图文对、3000+表型），并提出PhenoLIP两阶段预训练：先从文本本体学习知识增强的表型嵌入，再通过教师引导的知识蒸馏将结构化知识蒸馏到多模态预训练中。

Result: 在PhenoBench（7.8K图注、1K+表型）与多项基线比较中，PhenoLIP在表型分类较BiomedCLIP提升8.85%，在跨模态检索较BIOMEDICA提升15.03%，显示明显优势。

Conclusion: 本文提出将表型本体知识融入医学视觉语言模型，可显著提升表型识别与跨模态检索性能。

Abstract: Recent progress in large-scale CLIP-like vision-language models(VLMs) has greatly advanced medical image analysis. However, most existing medical VLMs still rely on coarse image-text contrastive objectives and fail to capture the systematic visual knowledge encoded in well-defined medical phenotype ontologies. To address this gap, we construct PhenoKG, the first large-scale, phenotype-centric multimodal knowledge graph that encompasses over 520K high-quality image-text pairs linked to more than 3,000 phenotypes. Building upon PhenoKG, we propose PhenoLIP, a novel pretraining framework that explicitly incorporates structured phenotype knowledge into medical VLMs through a two-stage process. We first learn a knowledge-enhanced phenotype embedding space from textual ontology data and then distill this structured knowledge into multimodal pretraining via a teacher-guided knowledge distillation objective. To support evaluation, we further introduce PhenoBench, an expert-verified benchmark designed for phenotype recognition, comprising over 7,800 image--caption pairs covering more than 1,000 phenotypes. Extensive experiments demonstrate that PhenoLIP outperforms previous state-of-the-art baselines, improving upon BiomedCLIP in phenotype classification accuracy by 8.85\% and BIOMEDICA in cross-modal retrieval by 15.03%, underscoring the value of integrating phenotype-centric priors into medical VLMs for structured and interpretable medical image understanding.

</details>


### [9] [DeDPO: Debiased Direct Preference Optimization for Diffusion Models](https://arxiv.org/abs/2602.06195)
*Khiem Pham,Quang Nguyen,Tung Nguyen,Jingsen Zhu,Michele Santacatterina,Dimitris Metaxas,Ramin Zabih*

Main category: cs.CV

TL;DR: DeDPO在DPO中引入因果去偏估计，使得有限人工标签加大量合成标签的半监督训练鲁棒且高效，能用低成本合成监督实现与全人工标注相媲美的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统DPO依赖大规模高质量人工偏好标签，成本高且难扩展；为降低成本并扩大规模，利用廉价合成注释代替部分人工标注，但合成注释含有系统性偏差与噪声，需要方法保证从不完美反馈中稳健学习。

Method: 提出DeDPO：在DPO目标中加入去偏估计模块，首先建模合成注释器的偏差与噪声（例如自监督、自训练和视觉-语言模型产生的偏差），然后用因果推断技术对样本级或批次级的偏倚进行校正，最后在半监督框架中联合训练有限人工标注对和大量合成标注对。

Result: 实验证明DeDPO对合成标注方法的差异具有鲁棒性，在多种合成注释来源（自训练、VLM等）上均能稳定表现，性能可匹配甚至偶尔超越仅用人工标注训练的上限，表明该方法在成本-效果权衡下具有实际应用价值。

Conclusion: DeDPO通过在DPO目标中引入来自因果推断的去偏估计，能有效纠正合成注释器的系统性偏差与噪声，从而在有限人工标签与大量低成本合成标签混合的半监督场景中实现稳健训练，且在实验中能达到或超越仅用人工标签训练的理论上限。

Abstract: Direct Preference Optimization (DPO) has emerged as a predominant alignment method for diffusion models, facilitating off-policy training without explicit reward modeling. However, its reliance on large-scale, high-quality human preference labels presents a severe cost and scalability bottleneck. To overcome this, We propose a semi-supervised framework augmenting limited human data with a large corpus of unlabeled pairs annotated via cost-effective synthetic AI feedback. Our paper introduces Debiased DPO (DeDPO), which uniquely integrates a debiased estimation technique from causal inference into the DPO objective. By explicitly identifying and correcting the systematic bias and noise inherent in synthetic annotators, DeDPO ensures robust learning from imperfect feedback sources, including self-training and Vision-Language Models (VLMs). Experiments demonstrate that DeDPO is robust to the variations in synthetic labeling methods, achieving performance that matches and occasionally exceeds the theoretical upper bound of models trained on fully human-labeled data. This establishes DeDPO as a scalable solution for human-AI alignment using inexpensive synthetic supervision.

</details>


### [10] [AnyThermal: Towards Learning Universal Representations for Thermal Perception](https://arxiv.org/abs/2602.06203)
*Parv Maheshwari,Jay Karhade,Yogesh Chawla,Isaiah Adu,Florian Heisen,Andrew Porco,Andrew Jong,Yifei Liu,Santosh Pitla,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: AnyThermal通过把DINOv2等视觉基础模型的特征蒸馏到热编码器并借助作者采集的多场景TartanRGBT数据集，得到了可用于多任务、多环境的通用热成像骨干，在多项任务上显著优于现有方法（最多提升36%）。


<details>
  <summary>Details</summary>
Motivation: 现有热成像骨干常在小规模、任务特定数据上训练，导致对环境和任务的适应性差，作者希望构建一个无需任务特定训练、能在多种环境和多任务上通用的热成像特征提取器。

Method: 作者使用DINOv2这样的视觉基础模型作为教师网络，通过特征蒸馏方式训练热成像编码器（学生网络），并在多样化的热-可见光数据上进行训练以减少域差异。为此，他们提出并开源了TartanRGBT数据采集平台，并收集了覆盖室内、航拍、越野和城市四种环境的平衡数据集以支持训练。

Result: AnyThermal在多种环境和下游任务上实现了最先进的性能，在现有数据集上相较先前方法最多提升约36%，证明了通过从视觉基础模型蒸馏得到的热成像特征的有效性和泛化能力。

Conclusion: AnyThermal提出了一个通用热成像骨干网络，通过将可见光视觉基础模型（如DINOv2）的特征蒸馏到热成像编码器来学习任务不可知的鲁棒热特征。结合作者自己采集的多场景RGB-热成像数据集TartanRGBT，AnyThermal在跨模态地点识别、热图分割和单目深度估计等多种下游任务上表现出显著提升，验证了其在多环境下的泛化能力。

Abstract: We present AnyThermal, a thermal backbone that captures robust task-agnostic thermal features suitable for a variety of tasks such as cross-modal place recognition, thermal segmentation, and monocular depth estimation using thermal images. Existing thermal backbones that follow task-specific training from small-scale data result in utility limited to a specific environment and task. Unlike prior methods, AnyThermal can be used for a wide range of environments (indoor, aerial, off-road, urban) and tasks, all without task-specific training. Our key insight is to distill the feature representations from visual foundation models such as DINOv2 into a thermal encoder using thermal data from these multiple environments. To bridge the diversity gap of the existing RGB-Thermal datasets, we introduce the TartanRGBT platform, the first open-source data collection platform with synced RGB-Thermal image acquisition. We use this payload to collect the TartanRGBT dataset - a diverse and balanced dataset collected in 4 environments. We demonstrate the efficacy of AnyThermal and TartanRGBT, achieving state-of-the-art results with improvements of up to 36% across diverse environments and downstream tasks on existing datasets.

</details>


### [11] [DroneKey++: A Size Prior-free Method and New Benchmark for Drone 3D Pose Estimation from Sequential Images](https://arxiv.org/abs/2602.06211)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: 提出无先验的DroneKey++框架和大规模合成数据集6DroneSyn，实现了无需物理尺寸或3D模型的实时无人机3D位姿估计并验证了良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖无人机的物理尺寸或3D网格作为先验，且数据集规模小、样本单一、采集环境受限，导致方法泛化性验证不足。作者旨在提出无先验方法并提供大规模多样化数据集以提升泛化性与实用性。

Method: 方法包括一个关键点编码器用于同时进行关键点检测与无人机分类；以及一个位姿解码器，利用射线几何推理结合类别嵌入来估计3D位姿。同时为了解决数据集规模和多样性的不足，构建了大规模合成数据集6DroneSyn（50K+图像，7种机型，88种户外背景，360度全景合成）。

Result: 在实验中，DroneKey++在旋转估计上取得MAE 17.34度、MedAE 17.1度；平移估计上MAE 0.135米、MedAE 0.242米。推理速度为CPU 19.25 FPS，GPU 414.07 FPS，展示了跨机型的良好泛化能力与实时性。数据集已公开。

Conclusion: 本文提出了一个无先验的无人机3D位姿估计框架DroneKey++，通过联合关键点检测、无人机分类与基于射线的几何位姿解码，实现了在无需已知物理尺寸或3D网格的条件下估计无人机姿态。

Abstract: Accurate 3D pose estimation of drones is essential for security and surveillance systems. However, existing methods often rely on prior drone information such as physical sizes or 3D meshes. At the same time, current datasets are small-scale, limited to single models, and collected under constrained environments, which makes reliable validation of generalization difficult. We present DroneKey++, a prior-free framework that jointly performs keypoint detection, drone classification, and 3D pose estimation. The framework employs a keypoint encoder for simultaneous keypoint detection and classification, and a pose decoder that estimates 3D pose using ray-based geometric reasoning and class embeddings. To address dataset limitations, we construct 6DroneSyn, a large-scale synthetic benchmark with over 50K images covering 7 drone models and 88 outdoor backgrounds, generated using 360-degree panoramic synthesis. Experiments show that DroneKey++ achieves MAE 17.34 deg and MedAE 17.1 deg for rotation, MAE 0.135 m and MedAE 0.242 m for translation, with inference speeds of 19.25 FPS (CPU) and 414.07 FPS (GPU), demonstrating both strong generalization across drone models and suitability for real-time applications. The dataset is publicly available.

</details>


### [12] [Addressing the Waypoint-Action Gap in End-to-End Autonomous Driving via Vehicle Motion Models](https://arxiv.org/abs/2602.06214)
*Jorge Daniel Rodríguez-Vidal,Gabriel Villalonga,Diego Porres,Antonio M. López Peña*

Main category: cs.CV

TL;DR: 作者通过一个可微分车辆模型将动作序列映射为航路点轨迹，使动作型端到端驾驶策略可以在现有航路点评测框架下训练与评估，从而提高了动作型策略的性能，且在NAVSIM navhard上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前多数基准和训练流程以航路点为主，导致动作型端到端自动驾驶策略难以训练、评估和比较。需要一个桥梁使动作型方法能在航路点评测框架下使用相同协议进行训练与评估。

Method: 构建一个可微分的前向车辆动力学模型，用于将网络预测的动作（油门、方向盘角、制动等）在时间步内展开成航路点序列，并在航路点空间计算损失；在训练和评估时保持现有航路点评测协议不变，允许动作型网络直接在航路点评测基准上训练与对比。

Result: 在多个困难基准上进行评估，均优于对应基线；特别是在NAVSIM的navhard任务上，达到了最新的最优性能。作者声明代码将在论文接收后公开。

Conclusion: 该论文提出了一个可微分的车辆模型框架，将动作序列展开为自车坐标系下的航路点轨迹，从而在航路点空间提供监督，弥合了基于航路点与基于动作方法之间的差距。

Abstract: End-to-End Autonomous Driving (E2E-AD) systems are typically grouped by the nature of their outputs: (i) waypoint-based models that predict a future trajectory, and (ii) action-based models that directly output throttle, steer and brake. Most recent benchmark protocols and training pipelines are waypoint-based, which makes action-based policies harder to train and compare, slowing their progress. To bridge this waypoint-action gap, we propose a novel, differentiable vehicle-model framework that rolls out predicted action sequences to their corresponding ego-frame waypoint trajectories while supervising in waypoint space. Our approach enables action-based architectures to be trained and evaluated, for the first time, within waypoint-based benchmarks without modifying the underlying evaluation protocol. We extensively evaluate our framework across multiple challenging benchmarks and observe consistent improvements over the baselines. In particular, on NAVSIM \texttt{navhard} our approach achieves state-of-the-art performance. Our code will be made publicly available upon acceptance.

</details>


### [13] [Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings](https://arxiv.org/abs/2602.06218)
*Grégoire Dhimoïla,Thomas Fel,Victor Boutin,Agustin Picard*

Main category: cs.CV

TL;DR: 提出Iso-Energy归纳偏置并在Aligned SAE中实现，揭示VLM嵌入可分解为承载对齐的双模态原子与引起偏差的单模态原子；该分解可消除模态差距、改善编辑与检索，同时保持重建与下游性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在对齐图像与文本上表现优异，但其共享嵌入空间的几何结构未知；作者提出利用跨模态冗余的能量一致性作为归纳偏置，以获得更可解释的潜在几何并探究对齐机制。

Method: 在自编码器框架中加入能量一致性正则化（Iso-Energy）以训练Aligned SAE，并用可控合成数据及多个基础VLMs做对比实验；通过分析稀疏字典原子（atoms）将表征分解为双模态与单模态分量，并进行消融和下游任务评估（检索、向量运算编辑）。

Result: Aligned SAE在合成与真实VLM上验证：当Iso-Energy成立时对齐性提高，若不成立则不受影响；稀疏双模态原子承担全部对齐信息，单模态原子解释并造成模态差距；删除单模态原子消除模态差距且不损失性能；对双模态子空间的向量算术产生合理编辑并提升检索表现。

Conclusion: 作者提出并验证了Iso-Energy假设及其在对齐稀疏自编码器（Aligned SAE）中的应用，证明在保持重建质量的同时可使跨模态嵌入几何更可解释；实验表明稀疏双模态原子承载对齐信号，单模态原子产生模态差距，移除单模态原子可消除差距且不损失性能，限制计算于双模态子空间能改善检索与编辑。

Abstract: Vision-language models (VLMs) align images and text with remarkable success, yet the geometry of their shared embedding space remains poorly understood. To probe this geometry, we begin from the Iso-Energy Assumption, which exploits cross-modal redundancy: a concept that is truly shared should exhibit the same average energy across modalities. We operationalize this assumption with an Aligned Sparse Autoencoder (SAE) that encourages energy consistency during training while preserving reconstruction. We find that this inductive bias changes the SAE solution without harming reconstruction, giving us a representation that serves as a tool for geometric analysis. Sanity checks on controlled data with known ground truth confirm that alignment improves when Iso-Energy holds and remains neutral when it does not. Applied to foundational VLMs, our framework reveals a clear structure with practical consequences: (i) sparse bimodal atoms carry the entire cross-modal alignment signal; (ii) unimodal atoms act as modality-specific biases and fully explain the modality gap; (iii) removing unimodal atoms collapses the gap without harming performance; (iv) restricting vector arithmetic to the bimodal subspace yields in-distribution edits and improved retrieval. These findings suggest that the right inductive bias can both preserve model fidelity and render the latent geometry interpretable and actionable.

</details>


### [14] [ForeHOI: Feed-forward 3D Object Reconstruction from Daily Hand-Object Interaction Videos](https://arxiv.org/abs/2602.06226)
*Yuantao Chen,Jiahao Chang,Chongjie Ye,Chaoran Zhang,Zhaojie Fang,Chenghong Li,Xiaoguang Han*

Main category: cs.CV

TL;DR: ForeHOI通过联合2D掩码修复与3D形状补全的前馈模型，高效且准确地从单目手持交互视频恢复被遮挡的物体3D形状，并发布了大规模合成数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 单目手持交互视频中物体常被手部严重遮挡，加之相机、手与物体的复杂耦合运动，使得从日常视频中重建3D物体非常具有挑战性；现有方法要么计算昂贵、要么恢复效果有限。

Method: 核心方法是联合预测2D掩码修复（mask inpainting）和3D形状补全（shape completion），并在前馈框架中实现2D与3D信息的交互，利用相互增强来应对严重遮挡。模型端到端预测物体形状，避免了基于优化方法的高昂计算开销。作者还构建了首个大规模高保真合成手物交互数据集用于训练。

Result: ForeHOI在物体重建任务上达到SOTA性能，相较于先前方法在精度上有显著提升，同时推理速度约为100倍加速。作者公开了代码与数据集。

Conclusion: 本文提出了ForeHOI，一种前馈（feed-forward）模型，可在不到一分钟的推理时间内直接从单目手持交互视频重建物体的3D几何形状，无需任何预处理步骤。

Abstract: The ubiquity of monocular videos capturing daily hand-object interactions presents a valuable resource for embodied intelligence. While 3D hand reconstruction from in-the-wild videos has seen significant progress, reconstructing the involved objects remains challenging due to severe occlusions and the complex, coupled motion of the camera, hands, and object. In this paper, we introduce ForeHOI, a novel feed-forward model that directly reconstructs 3D object geometry from monocular hand-object interaction videos within one minute of inference time, eliminating the need for any pre-processing steps. Our key insight is that, the joint prediction of 2D mask inpainting and 3D shape completion in a feed-forward framework can effectively address the problem of severe occlusion in monocular hand-held object videos, thereby achieving results that outperform the performance of optimization-based methods. The information exchanges between the 2D and 3D shape completion boosts the overall reconstruction quality, enabling the framework to effectively handle severe hand-object occlusion. Furthermore, to support the training of our model, we contribute the first large-scale, high-fidelity synthetic dataset of hand-object interactions with comprehensive annotations. Extensive experiments demonstrate that ForeHOI achieves state-of-the-art performance in object reconstruction, significantly outperforming previous methods with around a 100x speedup. Code and data are available at: https://github.com/Tao-11-chen/ForeHOI.

</details>


### [15] [ASMa: Asymmetric Spatio-temporal Masking for Skeleton Action Representation Learning](https://arxiv.org/abs/2602.06251)
*Aman Anand,Amir Eskandari,Elyas Rahsno,Farhana Zulkernine*

Main category: cs.CV

TL;DR: ASMa通过两种互补时空掩码和可学习对齐模块，学习更全面的骨架动作表示，结合知识蒸馏实现高压缩率与快速推理，在多个数据集上显著优于现有自监督方法并接近监督性能。


<details>
  <summary>Details</summary>
Motivation: 现有SSL骨架动作方法常偏向掩盖高运动帧或高连接度关节，导致学习到的表示不完整、偏向某类运动模式，难以泛化到多样化或噪声数据。

Method: 提出ASMa包含两种互补掩码策略：一是掩盖高连接度关节与低运动帧，二是掩盖低连接度关节与高运动帧；引入可学习的特征对齐模块对两种视图进行表示对齐；最后通过知识蒸馏将对齐后的表示压缩到轻量模型以适配资源受限设备。

Result: 在NTU RGB+D 60/120与PKU-MMD上，ASMa在微调任务上平均提升2.7–4.4%，在迁移到噪声数据集上最高提升5.9%；蒸馏后模型参数减少91.4%，在边缘设备上推理速度提升3倍，同时保持具有竞争力的准确率，与监督方法性能接近。

Conclusion: 该论文通过提出不对称时空掩码（ASMa）和可学习特征对齐模块，有效缓解了现有自监督骨架动作表示学习中因掩码策略单一导致的偏差，提升了模型对不同运动模式的泛化能力，并结合知识蒸馏实现轻量化部署。

Abstract: Self-supervised learning (SSL) has shown remarkable success in skeleton-based action recognition by leveraging data augmentations to learn meaningful representations. However, existing SSL methods rely on data augmentations that predominantly focus on masking high-motion frames and high-degree joints such as joints with degree 3 or 4. This results in biased and incomplete feature representations that struggle to generalize across varied motion patterns. To address this, we propose Asymmetric Spatio-temporal Masking (ASMa) for Skeleton Action Representation Learning, a novel combination of masking to learn a full spectrum of spatio-temporal dynamics inherent in human actions. ASMa employs two complementary masking strategies: one that selectively masks high-degree joints and low-motion, and another that masks low-degree joints and high-motion frames. These masking strategies ensure a more balanced and comprehensive skeleton representation learning. Furthermore, we introduce a learnable feature alignment module to effectively align the representations learned from both masked views. To facilitate deployment in resource-constrained settings and on low-resource devices, we compress the learned and aligned representation into a lightweight model using knowledge distillation. Extensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD datasets demonstrate that our approach outperforms existing SSL methods with an average improvement of 2.7-4.4% in fine-tuning and up to 5.9% in transfer learning to noisy datasets and achieves competitive performance compared to fully supervised baselines. Our distilled model achieves 91.4% parameter reduction and 3x faster inference on edge devices while maintaining competitive accuracy, enabling practical deployment in resource-constrained scenarios.

</details>


### [16] [An Interpretable Vision Transformer as a Fingerprint-Based Diagnostic Aid for Kabuki and Wiedemann-Steiner Syndromes](https://arxiv.org/abs/2602.06282)
*Marilyn Lionts,Arnhildur Tomasdottir,Viktor I. Agustsson,Yuankai Huo,Hans T. Bjornsson,Lotta M. Ellingsen*

Main category: cs.CV

TL;DR: 研究表明基于视觉变换器的AI可利用指纹区分Kabuki与Wiedemann–Steiner综合征及健康对照，性能中等偏上且可解释，提示指纹AI有望作为无创早筛工具。


<details>
  <summary>Details</summary>
Motivation: 许多KS和WSS患者因基因检测获取困难或专业知识不足而未被确诊；指纹作为一种无创、易采集的表型信息，可能包含未被充分利用的诊断信号，因此研究探索基于指纹的AI诊断辅助工具。

Method: 使用视觉变换器（vision transformer）对指纹图像进行分类，训练并评估三个二分类任务（对照 vs KS、对照 vs WSS、KS vs WSS），并结合基于注意力的可视化以标注模型关注的指纹区域以提升可解释性。

Result: 模型在三个任务中分别达成AUC为0.80、0.73、0.85，F1为0.71、0.72、0.83；注意力图显示特定指纹区域对预测有重要贡献，支持存在综合征特异性指纹特征的结论。

Conclusion: 指纹图像中存在与Kabuki综合征和Wiedemann–Steiner综合征相关的可识别特征，基于视觉变换器的深度学习模型能在一定程度上区分两种综合征与健康对照以及相互之间的差异。

Abstract: Kabuki syndrome (KS) and Wiedemann-Steiner syndrome (WSS) are rare but distinct developmental disorders that share overlapping clinical features, including neurodevelopmental delay, growth restriction, and persistent fetal fingertip pads. While genetic testing remains the diagnostic gold standard, many individuals with KS or WSS remain undiagnosed due to barriers in access to both genetic testing and expertise. Dermatoglyphic anomalies, despite being established hallmarks of several genetic syndromes, remain an underutilized diagnostic signal in the era of molecular testing. This study presents a vision transformer-based deep learning model that leverages fingerprint images to distinguish individuals with KS and WSS from unaffected controls and from one another. We evaluate model performance across three binary classification tasks. Across the three classification tasks, the model achieved AUC scores of 0.80 (control vs. KS), 0.73 (control vs. WSS), and 0.85 (KS vs. WSS), with corresponding F1 scores of 0.71, 0.72, and 0.83, respectively. Beyond classification, we apply attention-based visualizations to identify fingerprint regions most salient to model predictions, enhancing interpretability. Together, these findings suggest the presence of syndrome-specific fingerprint features, demonstrating the feasibility of a fingerprint-based artificial intelligence (AI) tool as a noninvasive, interpretable, and accessible future diagnostic aid for the early diagnosis of underdiagnosed genetic syndromes.

</details>


### [17] [MMEarth-Bench: Global Model Adaptation via Multimodal Test-Time Training](https://arxiv.org/abs/2602.06285)
*Lucia Gordon,Serge Belongie,Christian Igel,Nico Lang*

Main category: cs.CV

TL;DR: 提出全球多模态基准MMEarth-Bench并引入TTT-MMR方法，显示预训练提升鲁棒性但地理泛化不足，TTT-MMR可在测试时利用额外模态改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间基准缺乏模态多样性与全球代表性，难以评估多模态预训练模型在全球尺度的泛化与鲁棒性，因此需要新的基准与方法促进跨域适应。

Method: 构建包含5个任务、12种模态的全球多模态基准数据集（含域内/域外测试集），评估多种预训练模型，并提出一种模型无关的测试时训练方法TTT-MMR，利用测试时可用的所有模态作为辅助重建任务；使用地理分批策略优化训练。

Result: MMEarth-Bench包含更丰富的模态与全球分布，实验证明多模态/自监督预训练在有限标注下提高鲁棒性，但模型地理泛化能力有限；TTT-MMR在随机与地理测试集上均能提升性能，且地理分批在TTT中提供良好正则化与特化权衡。

Conclusion: 本文提出了MMEarth-Bench数据集与TTT-MMR测试时训练方法，展示了多模态自监督预训练能提升有限数据下的鲁棒性，但地理泛化仍然不足。

Abstract: Recent research in geospatial machine learning has demonstrated that models pretrained with self-supervised learning on Earth observation data can perform well on downstream tasks with limited training data. However, most of the existing geospatial benchmark datasets have few data modalities and poor global representation, limiting the ability to evaluate multimodal pretrained models at global scales. To fill this gap, we introduce MMEarth-Bench, a collection of five new multimodal environmental tasks with 12 modalities, globally distributed data, and both in- and out-of-distribution test splits. We benchmark a diverse set of pretrained models and find that while (multimodal) pretraining tends to improve model robustness in limited data settings, geographic generalization abilities remain poor. In order to facilitate model adaptation to new downstream tasks and geographic domains, we propose a model-agnostic method for test-time training with multimodal reconstruction (TTT-MMR) that uses all the modalities available at test time as auxiliary tasks, regardless of whether a pretrained model accepts them as input. Our method improves model performance on both the random and geographic test splits, and geographic batching leads to a good trade-off between regularization and specialization during TTT. Our dataset, code, and visualization tool are linked from the project page at lgordon99.github.io/mmearth-bench.

</details>


### [18] [Unsupervised MRI-US Multimodal Image Registration with Multilevel Correlation Pyramidal Optimization](https://arxiv.org/abs/2602.06288)
*Jiazheng Wang,Zeyu Liu,Min Liu,Xiang Chen,Hang Zhang*

Main category: cs.CV

TL;DR: 提出无监督的多层相关金字塔优化（MCPO）方法，结合MIND特征与多尺度耦合凸优化，实现了领先的术前—术中多模态配准性能（Learn2Reg冠军，Resect平均TRE 1.798 mm）。


<details>
  <summary>Details</summary>
Motivation: 在术中因组织位移或切除导致的解剖变形以及多模态图像间外观差异，使得术前与术中影像配准成为挑战。为提高配准鲁棒性与精度，需设计对模态不敏感的特征描述与多尺度优化策略以同时兼顾全局一致性与局部细节。

Method: 1) 使用模态无关邻域描述符（MIND）将不同模态图像映射到特征空间；2) 构建多层金字塔，对不同尺度的特征进行密集相关性分析；3) 通过权重平衡的耦合凸优化在每一层上求解位移场，并自上而下细化以补充局部细节。整个流程为无监督训练。

Result: 在Learn2Reg 2025的ReMIND2Reg任务中，该方法在验证集与测试集均获得第一名；在Resect数据集上平均TRE为1.798 mm，证明方法在术前—术中多模态配准场景的广泛适用性。

Conclusion: 本文提出的MCPO方法通过模态无关邻域描述符提取多模态特征，并采用多层金字塔融合优化（基于密集相关分析与权重平衡耦合凸优化）实现全局与局部位移场配准，能够有效应对术中图像形变与模态差异。

Abstract: Surgical navigation based on multimodal image registration has played a significant role in providing intraoperative guidance to surgeons by showing the relative position of the target area to critical anatomical structures during surgery. However, due to the differences between multimodal images and intraoperative image deformation caused by tissue displacement and removal during the surgery, effective registration of preoperative and intraoperative multimodal images faces significant challenges. To address the multimodal image registration challenges in Learn2Reg 2025, an unsupervised multimodal medical image registration method based on multilevel correlation pyramidal optimization (MCPO) is designed to solve these problems. First, the features of each modality are extracted based on the modality independent neighborhood descriptor, and the multimodal images is mapped to the feature space. Second, a multilevel pyramidal fusion optimization mechanism is designed to achieve global optimization and local detail complementation of the displacement field through dense correlation analysis and weight-balanced coupled convex optimization for input features at different scales. Our method focuses on the ReMIND2Reg task in Learn2Reg 2025. Based on the results, our method achieved the first place in the validation phase and test phase of ReMIND2Reg. The MCPO is also validated on the Resect dataset, achieving an average TRE of 1.798 mm. This demonstrates the broad applicability of our method in preoperative-to-intraoperative image registration. The code is avaliable at https://github.com/wjiazheng/MCPO.

</details>


### [19] [Accelerating Vision Transformers on Brain Processing Unit](https://arxiv.org/abs/2602.06300)
*Jinchi Tang,Yan Guo*

Main category: cs.CV

TL;DR: 通过将Transformer的线性层和LayerNorm替换为卷积实现，作者实现了无需重训练即可在BPU上部署DeiT并获得显著加速，代价是小幅精度下降。


<details>
  <summary>Details</summary>
Motivation: BPU为CNN卷积操作提供了高效的INT8加速，但Transformer主要由作用于三维张量的线性层构成，直接在BPU上部署效果差或不可行。需要架构改造以利用BPU优势。

Method: 提出重构策略：用四维卷积算子模拟Transformer中的三维线性变换和层归一化，保持数值兼容以直接载入原始权重，并采用INT8量化以适配BPU的计算单元。

Result: 量化后的DeiT-Base在ImageNet上取得80.4%精度（原始81.8%），推理速度最高提升3.8倍；在花类数据集上微调后，DeiT-Base仅下降0.5%精度。

Conclusion: 通过将Transformer中的线性层和层归一化替换为设计的卷积算子，本文成功在BPU上高效部署了DeiT，几乎无需重训练即可继承原始权重，取得了较小的精度损失和显著的速度提升。

Abstract: With the advancement of deep learning technologies, specialized neural processing hardware such as Brain Processing Units (BPUs) have emerged as dedicated platforms for CNN acceleration, offering optimized INT8 computation capabilities for convolutional operations. Meanwhile, Vision Transformer (ViT) models, such as the Data-efficient Image Transformer (DeiT), have demonstrated superior performance and play increasingly crucial roles in computer vision tasks. However, due to the architectural mismatch between CNN-optimized hardware and Vision Transformer computation characteristics--namely, that linear layers in Transformers operate on three-dimensional data while BPU acceleration is designed for four-dimensional convolution operations-it is difficult or even impossible to leverage BPU's advantages when deploying Vision Transformers. To address this challenge, we propose a novel approach that restructures the Vision Transformer by replacing linear layers and layer normalization operations with carefully designed convolutional operators. This enables DeiT to fully utilize the acceleration capabilities of BPUs, while allowing the original weight parameters to be inherited by the restructured models without retraining or fine-tuning. To the best of our knowledge, this is the first successful deployment of Vision Transformers that fully leverages BPU classification datasets demonstrate the effectiveness of our approach. Specifically, the quantized DeiT-Base model achieves 80.4% accuracy on ImageNet, compared to the original 81.8%, while obtaining up to a 3.8* inference speedup. Our finetuned DeiT model on the flower classification dataset also achieves excellent performance, with only a 0.5% accuracy drop for the DeiT-Base model, further demonstrating the effectiveness of our method.

</details>


### [20] [Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation](https://arxiv.org/abs/2602.06328)
*Yanshuo Wang,Jinguang Tong,Jun Lan,Weiqiang Wang,Huijia Zhu,Haoxing Chen,Xuesong Li,Jie Hong*

Main category: cs.CV

TL;DR: 论文提出ABR：基于标签翻转自适应调整重置间隔，提高CTTA在长期非平稳环境中的稳定性与性能。


<details>
  <summary>Details</summary>
Motivation: CTTA方法在短期表现良好但长期面对持续非平稳环境时性能可能退化。作者观察到长期性能与标签翻转轨迹相关，因此通过重置策略来维持长期稳健性。

Method: 提出Adaptive-and-Balanced Re-initialization (ABR)策略：1) 监测标签翻转（label flip）的变化轨迹；2) 根据标签翻转的变化自适应地决定模型权重重置的间隔；3) 重置操作在保持模型适应性的同时防止累积错误与过拟合。

Result: 在多个CTTA基准上进行广泛验证，ABR在长期性能上优于现有方法，显示出更稳定的准确率和更小的性能下降。

Conclusion: 该论文提出基于重置（re-initialization）的策略ABR，通过自适应且平衡的重置间隔来改善CTTA在长期变化环境中的表现。实验证明ABR在多个CTTA基准上显著提升了长期性能。

Abstract: Continual test-time domain adaptation (CTTA) aims to adjust models so that they can perform well over time across non-stationary environments. While previous methods have made considerable efforts to optimize the adaptation process, a crucial question remains: Can the model adapt to continually changing environments over a long time? In this work, we explore facilitating better CTTA in the long run using a re-initialization (or reset) based method. First, we observe that the long-term performance is associated with the trajectory pattern in label flip. Based on this observed correlation, we propose a simple yet effective policy, Adaptive-and-Balanced Re-initialization (ABR), towards preserving the model's long-term performance. In particular, ABR performs weight re-initialization using adaptive intervals. The adaptive interval is determined based on the change in label flip. The proposed method is validated on extensive CTTA benchmarks, achieving superior performance.

</details>


### [21] [Halt the Hallucination: Decoupling Signal and Semantic OOD Detection Based on Cascaded Early Rejection](https://arxiv.org/abs/2602.06330)
*Ningkang Peng,Chuanjie Cheng,Jingyang Mao,Xiaoqian Peng,Feng Xing,Bo Zhang,Chao Tan,Zhichao Zheng,Peiheng Li,Yanhui Gu*

Main category: cs.CV

TL;DR: CER通过SES早期拦截物理异常与SHE在中间层识别语义异常，达到更高检测性能和更低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低层噪声上做全尺度推理造成资源浪费并引发语义幻觉，需一种能在早期低成本拒绝显著物理异常并在高层精准识别语义异常的方法。

Method: 提出级联早期拒绝(CER)框架，包含两个模块：入口处用拉普拉斯算子构成非参数结构能量筛(SES)以拦截物理异常；中间层用球面能量(SHE)将特征幅值与方向解耦以检测语义偏差。

Result: CER将计算开销减少约32%，在CIFAR-100上将FPR95从33.58%降到22.84%，AUROC提升到93.97%，在模拟传感器故障的现实场景中明显优于SOTA，可作为通用插件集成到多种模型。

Conclusion: CER通过粗到细的层次化过滤，实现高效稳健的OOD检测，能减少计算并抑制语义幻觉。

Abstract: Efficient and robust Out-of-Distribution (OOD) detection is paramount for safety-critical applications.However, existing methods still execute full-scale inference on low-level statistical noise. This computational mismatch not only incurs resource waste but also induces semantic hallucination, where deep networks forcefully interpret physical anomalies as high-confidence semantic features.To address this, we propose the Cascaded Early Rejection (CER) framework, which realizes hierarchical filtering for anomaly detection via a coarse-to-fine logic.CER comprises two core modules: 1)Structural Energy Sieve (SES), which establishes a non-parametric barrier at the network entry using the Laplacian operator to efficiently intercept physical signal anomalies; and 2) the Semantically-aware Hyperspherical Energy (SHE) detector, which decouples feature magnitude from direction in intermediate layers to identify fine-grained semantic deviations. Experimental results demonstrate that CER not only reduces computational overhead by 32% but also achieves a significant performance leap on the CIFAR-100 benchmark:the average FPR95 drastically decreases from 33.58% to 22.84%, and AUROC improves to 93.97%. Crucially, in real-world scenarios simulating sensor failures, CER exhibits performance far exceeding state-of-the-art methods. As a universal plugin, CER can be seamlessly integrated into various SOTA models to provide performance gains.

</details>


### [22] [Taming SAM3 in the Wild: A Concept Bank for Open-Vocabulary Segmentation](https://arxiv.org/abs/2602.06333)
*Gensheng Pei,Xiruo Jiang,Yazhou Yao,Xiangbo Shu,Fumin Shen,Byeungwoo Jeon*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: The recent introduction of \texttt{SAM3} has revolutionized Open-Vocabulary Segmentation (OVS) through \textit{promptable concept segmentation}, which grounds pixel predictions in flexible concept prompts. However, this reliance on pre-defined concepts makes the model vulnerable: when visual distributions shift (\textit{data drift}) or conditional label distributions evolve (\textit{concept drift}) in the target domain, the alignment between visual evidence and prompts breaks down. In this work, we present \textsc{ConceptBank}, a parameter-free calibration framework to restore this alignment on the fly. Instead of adhering to static prompts, we construct a dataset-specific concept bank from the target statistics. Our approach (\textit{i}) anchors target-domain evidence via class-wise visual prototypes, (\textit{ii}) mines representative supports to suppress outliers under data drift, and (\textit{iii}) fuses candidate concepts to rectify concept drift. We demonstrate that \textsc{ConceptBank} effectively adapts \texttt{SAM3} to distribution drifts, including challenging natural-scene and remote-sensing scenarios, establishing a new baseline for robustness and efficiency in OVS. Code and model are available at https://github.com/pgsmall/ConceptBank.

</details>


### [23] [SPDA-SAM: A Self-prompted Depth-Aware Segment Anything Model for Instance Segmentation](https://arxiv.org/abs/2602.06335)
*Yihan Shang,Wei Wang,Chao Huang,Xinghui Dong*

Main category: cs.CV

TL;DR: 提出自提示与深度感知的SPDA-SAM，通过SSSPM和C2FFM改进SAM的提示依赖与空间感知，在多数据集上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: SAM依赖人工提示且RGB缺乏深度信息，导致空间结构感知与边界分割能力不足；因此希望通过自生成提示和引入深度信息弥补不足。

Method: 设计Semantic-Spatial Self-prompt Module(SSSPM)从图像编码器和掩码解码器分别提取语义与空间提示；提出Coarse-to-Fine RGB-D Fusion Module(C2FFM)，融合单目估计深度与RGB特征，粗粒度利用深度结构引导融合，细粒度编码深度局部变化。

Result: 在十二个数据集上，SPDA-SAM优于最先进方法，显示自提示与粗到细RGB-D融合有效提升性能。

Conclusion: 本文提出了SPDA-SAM，通过自提示和深度感知的方式提升SAM在实例分割中的性能，实验表明在12个数据集上优于现有方法。

Abstract: Recently, Segment Anything Model (SAM) has demonstrated strong generalizability in various instance segmentation tasks. However, its performance is severely dependent on the quality of manual prompts. In addition, the RGB images that instance segmentation methods normally use inherently lack depth information. As a result, the ability of these methods to perceive spatial structures and delineate object boundaries is hindered. To address these challenges, we propose a Self-prompted Depth-Aware SAM (SPDA-SAM) for instance segmentation. Specifically, we design a Semantic-Spatial Self-prompt Module (SSSPM) which extracts the semantic and spatial prompts from the image encoder and the mask decoder of SAM, respectively. Furthermore, we introduce a Coarse-to-Fine RGB-D Fusion Module (C2FFM), in which the features extracted from a monocular RGB image and the depth map estimated from it are fused. In particular, the structural information in the depth map is used to provide coarse-grained guidance to feature fusion, while local variations in depth are encoded in order to fuse fine-grained feature representations. To our knowledge, SAM has not been explored in such self-prompted and depth-aware manners. Experimental results demonstrate that our SPDA-SAM outperforms its state-of-the-art counterparts across twelve different data sets. These promising results should be due to the guidance of the self-prompts and the compensation for the spatial information loss by the coarse-to-fine RGB-D fusion operation.

</details>


### [24] [Uncertainty-Aware 4D Gaussian Splatting for Monocular Occluded Human Rendering](https://arxiv.org/abs/2602.06343)
*Weiquan Wang,Feifei Shao,Lin Li,Zhen Wang,Jun Xiao,Long Chen*

Main category: cs.CV

TL;DR: 提出基于像素不确定性的U-4DGS，通过概率变形网络与双重光栅化产生不确定性图并作为梯度调制器，结合可信度正则化，显著提升单目视频动态人体在遮挡下的渲染质量与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在遮挡下表现差：生成模型的修补会引入时间抖动，刚性几何启发式方法又不足以表达多样外观。需要一种能自适应区分可靠/不可靠观测并稳健融合的方法。

Method: 将渲染问题视为带异方差观测噪声的MAP估计，设计了Probabilistic Deformation Network来建模变形与不确定性，结合Double Rasterization生成像素级不确定性图；不确定性图作为自适应梯度调制器，减弱不可靠观测的影响；并引入Confidence-Aware Regularizations，利用不确定性选择性传播时空有效性以防止几何漂移。

Result: 在ZJU-MoCap和OcMotion数据集上，U-4DGS在渲染保真度和鲁棒性上达到或超过SOTA，表现出更少的伪影和几何漂移，尤其在强遮挡场景中改进显著。

Conclusion: U-4DGS在处理单目视频中的遮挡导致的高保真动态人体渲染退化问题上，提出了基于不确定性建模的稳健框架，从而显著提升在遮挡场景下的渲染保真度与几何稳定性。

Abstract: High-fidelity rendering of dynamic humans from monocular videos typically degrades catastrophically under occlusions. Existing solutions incorporate external priors-either hallucinating missing content via generative models, which induces severe temporal flickering, or imposing rigid geometric heuristics that fail to capture diverse appearances. To this end, we reformulate the task as a Maximum A Posteriori estimation problem under heteroscedastic observation noise. In this paper, we propose U-4DGS, a framework integrating a Probabilistic Deformation Network and a Double Rasterization pipeline. This architecture renders pixel-aligned uncertainty maps that act as an adaptive gradient modulator, automatically attenuating artifacts from unreliable observations. Furthermore, to prevent geometric drift in regions lacking reliable visual cues, we enforce Confidence-Aware Regularizations, which leverage the learned uncertainty to selectively propagate spatial-temporal validity. Extensive experiments on ZJU-MoCap and OcMotion demonstrate that U-4DGS achieves SOTA rendering fidelity and robustness.

</details>


### [25] [FlowConsist: Make Your Flow Consistent with Real Trajectory](https://arxiv.org/abs/2602.06346)
*Tianyi Zhang,Chengcheng Liu,Jinwei Chen,Chun-Le Guo,Chongyi Li,Ming-Ming Cheng,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出FlowConsist，通过用模型自预测边际速度与逐步轨迹纠正消除轨迹漂移与误差累积，实现单步采样高质量生成（ImageNet 256×256 FID 1.52）。


<details>
  <summary>Details</summary>
Motivation: 当前快速流训练范式中，随机噪声-数据配对导致条件速度引入系统性轨迹漂移，且模型误差随时间累积，导致长时程偏离严重，影响一步或少步生成质量。

Method: 用模型自身预测的边际速度替代随机配对构造的条件速度，使优化对齐真实ODE轨迹；并通过轨迹纠正（在每一时刻对齐生成与真实样本的边际分布）来抑制误差累积。

Result: 在ImageNet 256×256上，FlowConsist以仅1步采样达成FID 1.52，刷新了该任务的一步采样SOTA。

Conclusion: FlowConsist通过强制流模型轨迹一致性，有效减少轨迹漂移和误差累积，从而显著提升一阶采样性能。

Abstract: Fast flow models accelerate the iterative sampling process by learning to directly predict ODE path integrals, enabling one-step or few-step generation. However, we argue that current fast-flow training paradigms suffer from two fundamental issues. First, conditional velocities constructed from randomly paired noise-data samples introduce systematic trajectory drift, preventing models from following a consistent ODE path. Second, the model's approximation errors accumulate over time steps, leading to severe deviations across long time intervals. To address these issues, we propose FlowConsist, a training framework designed to enforce trajectory consistency in fast flows. We propose a principled alternative that replaces conditional velocities with the marginal velocities predicted by the model itself, aligning optimization with the true trajectory. To further address error accumulation over time steps, we introduce a trajectory rectification strategy that aligns the marginal distributions of generated and real samples at every time step along the trajectory. Our method establishes a new state-of-the-art on ImageNet 256$\times$256, achieving an FID of 1.52 with only 1 sampling step.

</details>


### [26] [Di3PO -- Diptych Diffusion DPO for Targeted Improvements in Image](https://arxiv.org/abs/2602.06355)
*Sanjana Reddy,Ishaan Malhi,Sally Ma,Praneet Dutta*

Main category: cs.CV

TL;DR: Di3PO通过只改变图像中目标区域（例如文本）并保持背景不变来构建正负样本，从而提高偏好调优效率和文本渲染质量，优于SFT和DPO。


<details>
  <summary>Details</summary>
Motivation: 现有T2I偏好调优方法依赖完整生成的大量图像对，导致样本噪声大、不相关像素差异多且采样成本高，特别是像文本渲染这类局部问题更难以被有效学习。

Method: 提出了一种基于局部替换/修正的方法构造正负对：首先识别或指定需要改进的目标区域（如字符所在区域），然后对该区域进行针对性生成或编辑获得较好/较差的变体，同时保持周围上下文不变，最后以这些局部差异作为训练信号进行偏好调优。

Result: 在文本渲染任务上，Di3PO在生成文本清晰度、一致性以及训练效率上均优于SFT和DPO，示范了通过局部化构造正负样本可以提升偏好调优的效果。

Conclusion: Di3PO通过在图像中锁定背景上下文并只替换或修改目标区域（如文本）来构建更有效的正负样本对，从而提高了偏好调优的效率和效果。与SFT和DPO等基线方法相比，Di3PO在文本渲染任务上表现更好，减少了不相关像素的方差并降低了采样成本。

Abstract: Existing methods for preference tuning of text-to-image (T2I) diffusion models often rely on computationally expensive generation steps to create positive and negative pairs of images. These approaches frequently yield training pairs that either lack meaningful differences, are expensive to sample and filter, or exhibit significant variance in irrelevant pixel regions, thereby degrading training efficiency. To address these limitations, we introduce "Di3PO", a novel method for constructing positive and negative pairs that isolates specific regions targeted for improvement during preference tuning, while keeping the surrounding context in the image stable. We demonstrate the efficacy of our approach by applying it to the challenging task of text rendering in diffusion models, showcasing improvements over baseline methods of SFT and DPO.

</details>


### [27] [Robust Pedestrian Detection with Uncertain Modality](https://arxiv.org/abs/2602.06363)
*Qian Bie,Xiao Wang,Bin Yang,Zhixi Yu,Jun Chen,Xin Xu*

Main category: cs.CV

TL;DR: 构建TRNT三模态数据集，提出AUNet（UMVR+MAI）用于在模态不确定/缺失情形下自适应判定可用模态并进行不确定性感知的互补融合，从而提升任意模态组合下的行人检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-TIR跨模态行人检测在模态不全或夜间场景下性能受限：TIR缺乏纹理细节导致漏检，RGB在弱光下失效，而NIR可补充低光纹理信息。实际采集环境中三模态并非总能同时可用，现有方法难以应对任意模态缺失。

Method: 提出Adaptive Uncertainty-aware Network (AUNet)，包含两个关键模块：1) Unified Modality Validation Refinement (UMVR)，其中不确定性感知路由器用于判定每个模态是否可用，语义精炼子模块负责过滤并增强模态内可靠信息；2) Modality-Aware Interaction (MAI)模块，根据UMVR输出自适应激活或关闭内部交互路径，实现可用模态之间的自适应互补融合。同时构建TRNT三模态对齐数据集用于训练与评估。

Result: 作者构建了包含8,281个像素对齐RGB-NIR-TIR三元组的TRNT数据集，并在该数据集上验证AUNet。在任意模态组合缺失的情况下，AUNet通过UMVR与MAI实现对可用模态的可靠判定与自适应融合，较传统固定融合方法获得明显性能提升（文中报告了各模态组合下的定量增益与消融分析）。

Conclusion: 本文提出的AUNet通过检测模态可用性并基于不确定性动态调整融合策略，在不完整多模态输入下能够更稳健地进行行人检测，实验（通过新构建的TRNT数据集）表明其在任意模态组合下性能优于传统固定融合方法。

Abstract: Existing cross-modal pedestrian detection (CMPD) employs complementary information from RGB and thermal-infrared (TIR) modalities to detect pedestrians in 24h-surveillance systems.RGB captures rich pedestrian details under daylight, while TIR excels at night. However, TIR focuses primarily on the person's silhouette, neglecting critical texture details essential for detection. While the near-infrared (NIR) captures texture under low-light conditions, which effectively alleviates performance issues of RGB and detail loss in TIR, thereby reducing missed detections. To this end, we construct a new Triplet RGB-NIR-TIR (TRNT) dataset, comprising 8,281 pixel-aligned image triplets, establishing a comprehensive foundation for algorithmic research. However, due to the variable nature of real-world scenarios, imaging devices may not always capture all three modalities simultaneously. This results in input data with unpredictable combinations of modal types, which challenge existing CMPD methods that fail to extract robust pedestrian information under arbitrary input combinations, leading to significant performance degradation. To address these challenges, we propose the Adaptive Uncertainty-aware Network (AUNet) for accurately discriminating modal availability and fully utilizing the available information under uncertain inputs. Specifically, we introduce Unified Modality Validation Refinement (UMVR), which includes an uncertainty-aware router to validate modal availability and a semantic refinement to ensure the reliability of information within the modality. Furthermore, we design a Modality-Aware Interaction (MAI) module to adaptively activate or deactivate its internal interaction mechanisms per UMVR output, enabling effective complementary information fusion from available modalities.

</details>


### [28] [Revisiting Salient Object Detection from an Observer-Centric Perspective](https://arxiv.org/abs/2602.06369)
*Fuxi Zhang,Yifan Wang,Hengrun Zhao,Zhuohan Sun,Changxing Xia,Lijun Wang,Huchuan Lu,Yangrui Shao,Chen Yang,Long Teng*

Main category: cs.CV

TL;DR: 提出以观察者为中心的显著性检测（OC-SOD），构建大规模文本-对象配对数据集OC-SODBench，并设计OC-SODAgent基线，通过多模态LLM和“感知-反思-调整”流程实现个性化显著性预测，验证了该方向的有效性并开源资源。


<details>
  <summary>Details</summary>
Motivation: 传统显著性检测使用单一真值图忽略观察者偏好与意图，导致问题病态且无法反映人类感知的多样性。引入观察者中心视角可建模主观性，实现个性化与上下文相关的显著性预测。

Method: 构建OC-SODBench数据集（33k图像，152k文本提示与目标对），通过多模态大语言模型生成标注提示与对象对；提出基线OC-SODAgent，仿人式“Perceive-Reflect-Adjust”（感知—反思—调整）流程进行个性化显著性预测。

Result: 作者公开OC-SODBench并展示OC-SODAgent在该数据集上的有效性，实验表明考虑观察者因素能更好地捕捉显著性多样性与歧义性。代码与数据集已开源。

Conclusion: 本文提出Observer-Centric Salient Object Detection (OC-SOD)，将显著性检测从单一客观标签转为考虑观察者个体差异的主观任务，从而更贴近人类感知的多样性与模糊性。

Abstract: Salient object detection is inherently a subjective problem, as observers with different priors may perceive different objects as salient. However, existing methods predominantly formulate it as an objective prediction task with a single groundtruth segmentation map for each image, which renders the problem under-determined and fundamentally ill-posed. To address this issue, we propose Observer-Centric Salient Object Detection (OC-SOD), where salient regions are predicted by considering not only the visual cues but also the observer-specific factors such as their preferences or intents. As a result, this formulation captures the intrinsic ambiguity and diversity of human perception, enabling personalized and context-aware saliency prediction. By leveraging multi-modal large language models, we develop an efficient data annotation pipeline and construct the first OC-SOD dataset named OC-SODBench, comprising 33k training, validation and test images with 152k textual prompts and object pairs. Built upon this new dataset, we further design OC-SODAgent, an agentic baseline which performs OC-SOD via a human-like "Perceive-Reflect-Adjust" process. Extensive experiments on our proposed OC-SODBench have justified the effectiveness of our contribution. Through this observer-centric perspective, we aim to bridge the gap between human perception and computational modeling, offering a more realistic and flexible understanding of what makes an object truly "salient." Code and dataset are publicly available at: https://github.com/Dustzx/OC_SOD

</details>


### [29] [POINTS-GUI-G: GUI-Grounding Journey](https://arxiv.org/abs/2602.06391)
*Zhongyin Zhao,Yuan Liu,Yikun Liu,Haicheng Wang,Le Tian,Xiao Zhou,Yangxiu You,Zilin Yu,Yang Yu,Jie Zhou*

Main category: cs.CV

TL;DR: 从弱基线模型出发，通过精细化数据工程、训练策略和可验证奖励的强化学习，作者构建了在GUI定位任务上表现优异的POINTS-GUI-G-8B模型。


<details>
  <summary>Details</summary>
Motivation: 动机是从零起步（使用定位能力弱的基础模型）完整掌握GUI grounding的技术链，而非在已有强空间感知模型上微调，从而推动更普适、轻量的解决方案。

Method: 方法包括：1) 统一并扩充多源开源数据集，进行增强、过滤与难度分级；2) 对视觉编码器进行连续微调并保持训练与推理分辨率一致；3) 引入基于可验证奖励的强化学习来优化感知精度。

Result: POINTS-GUI-G-8B在ScreenSpot-Pro达到59.9，OSWorld-G为66.0，ScreenSpot-v2为95.7，UI-Vision为49.9，展示了显著性能提升。

Conclusion: 本文提出POINTS-GUI-G-8B模型，从基础弱定位模型出发，通过数据工程、训练策略改进和强化学习三方面提升GUI grounding性能，在多个基准上达到或超越现有最优成绩。

Abstract: The rapid advancement of vision-language models has catalyzed the emergence of GUI agents, which hold immense potential for automating complex tasks, from online shopping to flight booking, thereby alleviating the burden of repetitive digital workflows. As a foundational capability, GUI grounding is typically established as a prerequisite for end-to-end task execution. It enables models to precisely locate interface elements, such as text and icons, to perform accurate operations like clicking and typing. Unlike prior works that fine-tune models already possessing strong spatial awareness (e.g., Qwen3-VL), we aim to master the full technical pipeline by starting from a base model with minimal grounding ability, such as POINTS-1.5. We introduce POINTS-GUI-G-8B, which achieves state-of-the-art performance with scores of 59.9 on ScreenSpot-Pro, 66.0 on OSWorld-G, 95.7 on ScreenSpot-v2, and 49.9 on UI-Vision. Our model's success is driven by three key factors: (1) Refined Data Engineering, involving the unification of diverse open-source datasets format alongside sophisticated strategies for augmentation, filtering, and difficulty grading; (2) Improved Training Strategies, including continuous fine-tuning of the vision encoder to enhance perceptual accuracy and maintaining resolution consistency between training and inference; and (3) Reinforcement Learning (RL) with Verifiable Rewards. While RL is traditionally used to bolster reasoning, we demonstrate that it significantly improves precision in the perception-intensive GUI grounding task. Furthermore, GUI grounding provides a natural advantage for RL, as rewards are easily verifiable and highly accurate.

</details>


### [30] [TFusionOcc: Student's t-Distribution Based Object-Centric Multi-Sensor Fusion Framework for 3D Occupancy Prediction](https://arxiv.org/abs/2602.06400)
*Zhenxing Ming,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: TFusionOcc提出对象中心的多阶段多传感器融合、基于Student’s t与TMM的概率模型及可形变超二次曲面基元，从而在nuScenes上实现了更精细且鲁棒的3D语义占据预测。


<details>
  <summary>Details</summary>
Motivation: 现有的3D语义占据预测方法过度依赖体素或简单的3D高斯集合，难以高效且精细地捕捉驾驶场景中物体的复杂几何细节，限制了感知精度与鲁棒性，因此需要更灵活的表示和更有效的多传感器融合策略。

Method: 提出了一种对象中心（object-centric）的多传感器融合框架TFusionOcc，采用多阶段融合策略融合相机与激光雷达信息；在表示和建模上引入Student’s t分布和T-混合模型以替代传统体素或单一高斯表征，并使用可形变超二次曲面（superquadric with inverse warp）作为更具几何适应性的基元来刻画物体形状细节。

Result: 在nuScenes数据集上取得了SOTA性能提升，并在nuScenes-C上通过不同相机与激光雷达扰动情形的广泛实验验证了方法的鲁棒性；代码将开源。

Conclusion: TFusionOcc通过引入基于对象中心的多传感器多阶段融合、Student’s t分布、T-混合模型（TMM）以及可形变超二次曲面等几何灵活基元，显著提升了3D语义占据预测的细粒度几何表达能力，并在nuScenes基准上达到了SOTA性能，同时在nuScenes-C上的鲁棒性实验表明其对相机和激光雷达的扰动具有较强的抵抗力。

Abstract: 3D semantic occupancy prediction enables autonomous vehicles (AVs) to perceive fine-grained geometric and semantic structure of their surroundings from onboard sensors, which is essential for safe decision-making and navigation. Recent models for 3D semantic occupancy prediction have successfully addressed the challenge of describing real-world objects with varied shapes and classes. However, the intermediate representations used by existing methods for 3D semantic occupancy prediction rely heavily on 3D voxel volumes or a set of 3D Gaussians, hindering the model's ability to efficiently and effectively capture fine-grained geometric details in the 3D driving environment. This paper introduces TFusionOcc, a novel object-centric multi-sensor fusion framework for predicting 3D semantic occupancy. By leveraging multi-stage multi-sensor fusion, Student's t-distribution, and the T-Mixture model (TMM), together with more geometrically flexible primitives, such as the deformable superquadric (superquadric with inverse warp), the proposed method achieved state-of-the-art (SOTA) performance on the nuScenes benchmark. In addition, extensive experiments were conducted on the nuScenes-C dataset to demonstrate the robustness of the proposed method in different camera and lidar corruption scenarios. The code will be available at: https://github.com/DanielMing123/TFusionOcc

</details>


### [31] [MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing](https://arxiv.org/abs/2602.06402)
*Wenjie Wang,Wei Wu,Ying Liu,Yuan Zhao,Xiaole Lv,Liang Diao,Zengjian Fan,Wenfeng Xie,Ziling Lin,De Shi,Lin Huang,Kaihe Xu,Hong Li*

Main category: cs.CV

TL;DR: MeDocVL通过标签优化与噪声感知后训练，将强化学习和监督微调结合，显著提升了在噪声标注医疗票据上的字段级信息抽取性能。


<details>
  <summary>Details</summary>
Motivation: 医疗文档OCR面临复杂排版、领域术语和噪声标注等挑战，且应用场景要求字段级精确匹配，现有OCR与通用视觉-语言模型在此类任务上表现不可靠，因此需要专门的方法。

Method: 方法包括两部分：1) Training-driven Label Refinement（训练驱动的标签优化），用于从噪声注释中构建高质量监督标签；2) Noise-aware Hybrid Post-training（噪声感知混合后训练），结合强化学习与有监督微调以提高稳健性和精确性。

Result: 在医疗发票基准数据集上，MeDocVL在噪声监督下持续优于传统OCR系统和强VLM基线，达到了最先进的性能。

Conclusion: 本文提出了MeDocVL，一种针对医疗票据的后训练视觉-语言模型，能够在噪声标注下实现精确的字段级提取，显著优于传统OCR与通用VLM基线。

Abstract: Medical document OCR is challenging due to complex layouts, domain-specific terminology, and noisy annotations, while requiring strict field-level exact matching. Existing OCR systems and general-purpose vision-language models often fail to reliably parse such documents. We propose MeDocVL, a post-trained vision-language model for query-driven medical document parsing. Our framework combines Training-driven Label Refinement to construct high-quality supervision from noisy annotations, with a Noise-aware Hybrid Post-training strategy that integrates reinforcement learning and supervised fine-tuning to achieve robust and precise extraction. Experiments on medical invoice benchmarks show that MeDocVL consistently outperforms conventional OCR systems and strong VLM baselines, achieving state-of-the-art performance under noisy supervision.

</details>


### [32] [A neuromorphic model of the insect visual system for natural image processing](https://arxiv.org/abs/2602.06405)
*Adam D. Hines,Karin Nordström,Andrew B. Barron*

Main category: cs.CV

TL;DR: 提出一个受昆虫视觉启发的自监督对比学习模型，生成稀疏判别性表征，支持ANN与SNN实现，在花识别、自然图像基准和模拟定位任务上表现优于下采样基线，展现出生物学处理路径的实用价值。


<details>
  <summary>Details</summary>
Motivation: 许多现有计算机视觉模型强调任务性能但忽视生物学上的处理路径。昆虫视觉以资源受限下的高效稀疏编码和广泛行为能力（联想学习、导航、目标检测）为特点，提供了可供借鉴的计算原则。论文旨在构建一个既符合生物学启发又能在多任务上泛化的视觉表示模型。

Method: 构建了基于昆虫视觉处理原理的前处理与编码模块，将输入图像转为稀疏表征；使用完全自监督的对比学习目标训练表示，避免依赖标签和任务特定分类器；实现了两种实现形式：人工神经网络和脉冲（spiking）神经网络；在下采样基线对比、花卉识别与自然图像基准上进行评估，还在模拟定位任务中与简单下采样比较。

Result: 模型产生了可靠的稀疏代码，能够区分视觉上相似的输入；在花朵识别和若干自然图像基准上表现良好；在模拟定位任务中，相较于简单的图像下采样基线表现更优，展示了神经形态视觉处理路径的功能性好处；模型在ANNS与SNN两种实现中均可用，适配不同部署需求。

Conclusion: 该论文提出了一个启发昆虫视觉系统的自监督对比学习模型，能将稠密视觉输入转化为稀疏且具判别性的表征，适用于多任务且无需标签或专门分类器。实现包括传统人工神经网络与脉冲神经网络两种形式，实验在花朵识别、自然图像基准与模拟定位任务上证明了性能和稀疏性的优势，显示出将神经形态视觉处理路径融入模型的功能性收益。

Abstract: Insect vision supports complex behaviors including associative learning, navigation, and object detection, and has long motivated computational models for understanding biological visual processing. However, many contemporary models prioritize task performance while neglecting biologically grounded processing pathways. Here, we introduce a bio-inspired vision model that captures principles of the insect visual system to transform dense visual input into sparse, discriminative codes. The model is trained using a fully self-supervised contrastive objective, enabling representation learning without labeled data and supporting reuse across tasks without reliance on domain-specific classifiers. We evaluated the resulting representations on flower recognition tasks and natural image benchmarks. The model consistently produced reliable sparse codes that distinguish visually similar inputs. To support different modelling and deployment uses, we have implemented the model as both an artificial neural network and a spiking neural network. In a simulated localization setting, our approach outperformed a simple image downsampling comparison baseline, highlighting the functional benefit of incorporating neuromorphic visual processing pathways. Collectively, these results advance insect computational modelling by providing a generalizable bio-inspired vision model capable of sparse computation across diverse tasks.

</details>


### [33] [Point Virtual Transformer](https://arxiv.org/abs/2602.06406)
*Veerain Sood,Bnalin,Gaurav Pandey*

Main category: cs.CV

TL;DR: PointViT通过选择性引入基于RGB的虚拟点并在BEV上结合Transformer query精炼，实现了在KITTI上对Car类极高的3D/BEV/2D检测精度，同时兼顾效率。


<details>
  <summary>Details</summary>
Motivation: LiDAR在长距离处点云稀疏、几何信息不足，单纯LiDAR难以检测远场目标；利用RGB补全的虚拟点可以补充几何线索，但全部引入会带来计算负担并增加虚实融合难度，因此需要选择性融合并设计高效检测架构。

Method: 方法包括：从RGB深度补全生成虚拟点并进行选择性采样；比较多种融合策略（点级早期融合到BEV门控融合）；将融合点云体素化并用稀疏卷积编码形成BEV表示；基于BEV初始化高置信度目标query，并用基于Transformer的上下文聚合模块进行迭代精炼。

Result: 在KITTI上取得优异表现：Car类3D AP 91.16%，BEV AP 95.94%，2D AP 99.36%，表明选择性虚拟点融合与Transformer上下文聚合能在精度与效率间取得良好平衡。

Conclusion: 本文提出PointViT，通过选择性采样的虚拟点与原始LiDAR点联合建模，有效提升远距物体检测性能。

Abstract: LiDAR-based 3D object detectors often struggle to detect far-field objects due to the sparsity of point clouds at long ranges, which limits the availability of reliable geometric cues. To address this, prior approaches augment LiDAR data with depth-completed virtual points derived from RGB images; however, directly incorporating all virtual points leads to increased computational cost and introduces challenges in effectively fusing real and virtual information. We present Point Virtual Transformer (PointViT), a transformer-based 3D object detection framework that jointly reasons over raw LiDAR points and selectively sampled virtual points. The framework examines multiple fusion strategies, ranging from early point-level fusion to BEV-based gated fusion, and analyses their trade-offs in terms of accuracy and efficiency. The fused point cloud is voxelized and encoded using sparse convolutions to form a BEV representation, from which a compact set of high-confidence object queries is initialised and refined through a transformer-based context aggregation module. Experiments on the KITTI benchmark report 91.16% 3D AP, 95.94% BEV AP, and 99.36% AP on the KITTI 2D detection benchmark for the Car class.

</details>


### [34] [Learning Human Visual Attention on 3D Surfaces through Geometry-Queried Semantic Priors](https://arxiv.org/abs/2602.06419)
*Soham Pahari,Sandeep C. Kumain*

Main category: cs.CV

TL;DR: 提出一种几何-语义双流、非对称跨模态注意网络并扩展为考虑网格拓扑与抑制返回的时序扫描路径生成，在多数据集上显著优于现有3D显著性方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D显著性方法仅依赖手工几何特征或缺乏语义意识的学习方法，无法解释人类为何注视语义重要但几何上不突出的位置，因此需要同时建模几何与语义并解释二者如何交互。

Method: 方法包括：1) 使用点云Transformer对几何信息建模；2) 基于几何条件多视图渲染并借助扩散模型（diffusion-based）提取语义先验；3) 通过交叉注意力机制让几何特征去查询语义内容，实现底层显著性驱动的顶层检索；4) 将框架扩展为基于强化学习的时序扫描路径生成，首次在3D网格拓扑上引入抑制返回（Inhibition-of-Return）动力学。

Result: 在SAL3D、NUS3D与3DVA数据集上的评估显示，所提方法在显著图和扫描路径预测上均有显著提升，证明认知动机的架构能更好地拟合人类对三维表面的视觉注意模式。

Conclusion: 本文提出了SemGeo-AttentionNet，一种将底层几何处理与顶层语义识别显式分离并通过非对称跨模态融合连接的双流网络。

Abstract: Human visual attention on three-dimensional objects emerges from the interplay between bottom-up geometric processing and top-down semantic recognition. Existing 3D saliency methods rely on hand-crafted geometric features or learning-based approaches that lack semantic awareness, failing to explain why humans fixate on semantically meaningful but geometrically unremarkable regions. We introduce SemGeo-AttentionNet, a dual-stream architecture that explicitly formalizes this dichotomy through asymmetric cross-modal fusion, leveraging diffusion-based semantic priors from geometry-conditioned multi-view rendering and point cloud transformers for geometric processing. Cross-attention ensures geometric features query semantic content, enabling bottom-up distinctiveness to guide top-down retrieval. We extend our framework to temporal scanpath generation through reinforcement learning, introducing the first formulation respecting 3D mesh topology with inhibition-of-return dynamics. Evaluation on SAL3D, NUS3D and 3DVA datasets demonstrates substantial improvements, validating how cognitively motivated architectures effectively model human visual attention on three-dimensional surfaces.

</details>


### [35] [Alleviating Sparse Rewards by Modeling Step-Wise and Long-Term Sampling Effects in Flow-Based GRPO](https://arxiv.org/abs/2602.06422)
*Yunze Tong,Mushui Liu,Canyu Zhao,Wanggui He,Shiyi Zhang,Hongwei Zhang,Peng Zhang,Jinlong Liu,Ju Huang,Jiamang Wang,Hao Jiang,Pipei Huang*

Main category: cs.CV

TL;DR: 用步级增量奖励和基于符号变化的转折点长期奖励，TP-GRPO解决了去噪轨迹奖励稀疏与长期影响建模问题，提升了文本到图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有将最终结果回溯到所有去噪步骤的做法忽略了每一步的局部效应且奖励稀疏；组内排序按匹配时间步比较轨迹，忽视了早期动作对后续状态的延迟影响。需要一种既密集又能捕捉长期依赖的奖励设计。

Method: 在GRPO框架下，作者引入逐步增量奖励（incremental rewards）来提供密集的、步级别的学习信号；通过检测增量奖励符号变化识别转折点（turning points），并对这些转折点分配聚合的长期奖励以捕捉延迟影响；组内排序仍在，但改为考虑步骤内的长期依赖。转折点检测仅依赖增量奖励符号变化，无需额外超参数。

Result: 作者在若干文本到图像的Flow Matching模型上进行了大量实验，结果表明TP-GRPO能更有效地利用奖励信号并在生成质量上持续提升。方法高效且无超参数，代码已开源。

Conclusion: 该论文提出了TurningPoint-GRPO（TP-GRPO），通过替换基于结果的稀疏奖励为逐步增量奖励，并识别“转折点”以赋予聚合的长期奖励，从而缓解了去噪轨迹中的奖励稀疏性并显式建模长期影响。实验显示TP-GRPO更有效地利用奖励信号并稳定提升生成质量。

Abstract: Deploying GRPO on Flow Matching models has proven effective for text-to-image generation. However, existing paradigms typically propagate an outcome-based reward to all preceding denoising steps without distinguishing the local effect of each step. Moreover, current group-wise ranking mainly compares trajectories at matched timesteps and ignores within-trajectory dependencies, where certain early denoising actions can affect later states via delayed, implicit interactions. We propose TurningPoint-GRPO (TP-GRPO), a GRPO framework that alleviates step-wise reward sparsity and explicitly models long-term effects within the denoising trajectory. TP-GRPO makes two key innovations: (i) it replaces outcome-based rewards with step-level incremental rewards, providing a dense, step-aware learning signal that better isolates each denoising action's "pure" effect, and (ii) it identifies turning points-steps that flip the local reward trend and make subsequent reward evolution consistent with the overall trajectory trend-and assigns these actions an aggregated long-term reward to capture their delayed impact. Turning points are detected solely via sign changes in incremental rewards, making TP-GRPO efficient and hyperparameter-free. Extensive experiments also demonstrate that TP-GRPO exploits reward signals more effectively and consistently improves generation. Demo code is available at https://github.com/YunzeTong/TurningPoint-GRPO.

</details>


### [36] [POPL-KF: A Pose-Only Geometric Representation-Based Kalman Filter for Point-Line-Based Visual-Inertial Odometry](https://arxiv.org/abs/2602.06425)
*Aiping Wang,Zhaolong Yang,Shuwen Chen,Hai Zhang*

Main category: cs.CV

TL;DR: 提出POP L-KF：一种在卡尔曼滤波VIO中对点线特征都采用位姿仅表示的方法，消除特征坐标以减小线性化误差并实现即时视觉更新，结合基帧选择和线特征质量过滤，在公开数据集和实测中优于多种SOTA方法并保持实时性。


<details>
  <summary>Details</summary>
Motivation: 传统VIO依赖点特征，在恶劣场景下性能下降；MSCKF类方法因特征三维坐标线性化误差和延迟更新影响定位精度。为提升在挑战场景下的鲁棒性与精度，提出位姿仅表示以消除这些误差并加快更新。

Method: 提出了（1）一种仅基于位姿的线特征几何表示；（2）在卡尔曼滤波框架下同时对点和线采用位姿仅表示的POP L-KF系统，消除特征坐标以降低线性化误差并支持即时更新；（3）统一的基帧选择算法，保证在位姿仅量测模型中对相机位姿提供最优约束；（4）基于图像网格分割与双向光流一致性的线特征滤波器以提升线特征质量。系统实现并在数据集与实测中进行评估。

Result: 在公共数据集及真实场景实验中，POPL-KF在定位精度与鲁棒性上超过了若干滤波器基（OpenVINS、PO-KF）和基于优化的方法（PL-VINS、EPLF-VINS），同时保持实时性。具体改进体现在更低的定位误差和更稳定的轨迹估计（文中给出量化误差表与轨迹对比）。

Conclusion: 本文提出了基于位姿仅表示的点线特征融合卡尔曼滤波VIO系统（POPL-KF），通过从量测方程中显式消除特征三维坐标来减轻线性化误差，并实现视觉量测的及时更新，最终在公共数据集和真实场景中优于若干SOTA方法。

Abstract: Mainstream Visual-inertial odometry 
(VIO) systems rely on point features for motion estimation and localization. However, their performance degrades in challenging scenarios. Moreover, the localization accuracy of multi-state constraint Kalman filter (MSCKF)-based VIO systems suffers from linearization errors associated with feature 3D coordinates and delayed measurement updates. To improve the performance of VIO in challenging scenes, we first propose a pose-only geometric representation for line features. Building on this, we develop POPL-KF, a Kalman filter-based VIO system that employs a pose-only geometric representation for both point and line features. POPL-KF mitigates linearization errors by explicitly eliminating both point and line feature coordinates from the measurement equations, while enabling immediate update of visual measurements. We also design a unified base-frames selection algorithm for both point and line features to ensure optimal constraints on camera poses within the pose-only measurement model. To further improve line feature quality, a line feature filter based on image grid segmentation and bidirectional optical flow consistency is proposed. Our system is evaluated on public datasets and real-world experiments, demonstrating that POPL-KF outperforms the state-of-the-art (SOTA) filter-based methods (OpenVINS, PO-KF) and optimization-based methods (PL-VINS, EPLF-VINS), while maintaining real-time performance.

</details>


### [37] [Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters](https://arxiv.org/abs/2602.06427)
*Yuxiang Zhao,Yirong Yang,Yanqing Zhu,Yanfen Shen,Chiyu Wang,Zhining Gu,Pei Shi,Wei Guo,Mu Xu*

Main category: cs.CV

TL;DR: 本文提出了无需外部先验的指令驱动out-to-in具身导航任务，设计了图像提示驱动的视觉导航框架并发布了首个相关数据集，实验显示该方法在成功率与路径效率上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么局限于室内/室外场景，要么依赖精确坐标等外部先验，无法实现从户外精确进门的细粒度导航，限制了真实场景（如送货）中的可用性。

Method: 提出了一个以视觉为中心的导航框架，使用基于图像的提示（image-based prompts）来驱动决策，并在数据生成中引入基于轨迹条件的视频合成管线，构建首个公开数据集。

Result: 通过大量实验，新方法在成功率和路径效率等关键指标上持续优于现有最先进基线。

Conclusion: 该论文提出并验证了一个面向实际部署的新的导航任务与方法：无需外部先验、指令驱动的户外到室内（out-to-in）具身导航。

Abstract: Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.

</details>


### [38] [ChatUMM: Robust Context Tracking for Conversational Interleaved Generation](https://arxiv.org/abs/2602.06442)
*Wenxun Dai,Zhiyuan Zhao,Yule Zhong,Yiji Cheng,Jianwei Zhang,Linqing Wang,Shiyi Zhang,Yunlong Lin,Runze He,Fellix Song,Wayne Zhuang,Yong Liu,Haoji Zhang,Yansong Tang,Qinglin Lu,Chunyu Wang*

Main category: cs.CV

TL;DR: ChatUMM通过交错多轮训练和对话数据合成，把单轮多模态数据转为自然的多轮对话，从而让统一模型具备持续上下文跟踪与交错多模态生成的能力，效果优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多限于单轮请求求解器，缺乏持续对话能力和跨轮上下文理解，难以作为真正的多轮助理使用。

Method: 提出交错多轮训练策略，将文本-图像序列建模为连续对话流；构建系统化对话数据合成流水线，将单轮数据逐步转换为有状态对话、引入“干扰”轮以强化长程依赖和历史相关的查询重写、并合成自然交错的多模态响应。

Result: 在公开基准上，ChatUMM在视觉理解和指令引导编辑任务中达到开源统一模型的最先进性能，并在文本到图像生成上保持竞争性的保真度；在复杂多轮场景中展现出更强鲁棒性和上下文感知生成能力。

Conclusion: ChatUMM通过将统一多模态模型扩展为对话式模型，成功提升了多轮、多模态交互的上下文跟踪与生成一致性。

Abstract: Unified multimodal models (UMMs) have achieved remarkable progress yet remain constrained by a single-turn interaction paradigm, effectively functioning as solvers for independent requests rather than assistants in continuous dialogue. To bridge this gap, we present ChatUMM. As a conversational unified model, it excels at robust context tracking to sustain interleaved multimodal generation. ChatUMM derives its capabilities from two key innovations: an interleaved multi-turn training strategy that models serialized text-image streams as a continuous conversational flow, and a systematic conversational data synthesis pipeline. This pipeline transforms a diverse set of standard single-turn datasets into fluid dialogues through three progressive stages: constructing basic stateful dialogues, enforcing long-range dependency resolution via ``distractor'' turns with history-dependent query rewriting, and synthesizing naturally interleaved multimodal responses. Extensive evaluations demonstrate that ChatUMM achieves state-of-the-art performance among open-source unified models on visual understanding and instruction-guided editing benchmarks, while maintaining competitive fidelity in text-to-image generation. Notably, ChatUMM exhibits superior robustness in complex multi-turn scenarios, ensuring fluid, context-aware dialogues.

</details>


### [39] [What Is Wrong with Synthetic Data for Scene Text Recognition? A Strong Synthetic Engine with Diverse Simulations and Self-Evolution](https://arxiv.org/abs/2602.06450)
*Xingsong Ye,Yongkun Du,JiaXin Zhang,Chen Li,Jing LYU,Zhineng Chen*

Main category: cs.CV

TL;DR: 通过增强合成数据多样性（语料/字体/布局）构建UnionST-S，并配合自我演化学习，显著缩小合成与真实域差距，能在少量真实标签下获得高性能STR模型。


<details>
  <summary>Details</summary>
Motivation: 现有渲染型合成数据在语料、字体和布局多样性不足，导致与真实数据存在域差距，使得使用合成数据训练的STR模型在复杂场景下性能欠佳。

Method: 提出UnionST数据引擎以合成覆盖更复杂样本的文本，构建大规模数据集UnionST-S，并设计自我演化学习（SEL）框架用于逐步标注和优化真实数据；在训练中比较了现有合成数据、UnionST-S以及与有限真实标签结合的性能。

Result: 在实验中，用UnionST-S训练的模型在多个场景上相比现有合成数据有显著提升，在某些场景甚至超过仅用真实数据训练的模型；结合SEL后，仅使用约9%的真实标签也能达到具有竞争力的识别性能。

Conclusion: UnionST通过扩大合成文本数据在语料、字体和布局方面的多样性，显著提升了合成数据与真实场景的匹配，从而提高场景文本识别（STR）模型性能；结合自我演化学习（SEL）用于高效真实数据标注，可在只使用少量真实标签（约9%）下达到有竞争力的效果。

Abstract: Large-scale and categorical-balanced text data is essential for training effective Scene Text Recognition (STR) models, which is hard to achieve when collecting real data. Synthetic data offers a cost-effective and perfectly labeled alternative. However, its performance often lags behind, revealing a significant domain gap between real and current synthetic data. In this work, we systematically analyze mainstream rendering-based synthetic datasets and identify their key limitations: insufficient diversity in corpus, font, and layout, which restricts their realism in complex scenarios. To address these issues, we introduce UnionST, a strong data engine synthesizes text covering a union of challenging samples and better aligns with the complexity observed in the wild. We then construct UnionST-S, a large-scale synthetic dataset with improved simulations in challenging scenarios. Furthermore, we develop a self-evolution learning (SEL) framework for effective real data annotation. Experiments show that models trained on UnionST-S achieve significant improvements over existing synthetic datasets. They even surpass real-data performance in certain scenarios. Moreover, when using SEL, the trained models achieve competitive performance by only seeing 9% of real data labels.

</details>


### [40] [Exploring Specular Reflection Inconsistency for Generalizable Face Forgery Detection](https://arxiv.org/abs/2602.06452)
*Hongyan Fei,Zexi Jia,Chuanwei Huang,Jinchao Zhang,Jie Zhou*

Main category: cs.CV

TL;DR: 利用Retinex分离镜面反射，并通过SRI-Net捕捉镜面反射与纹理/直射光的不一致性，从而更鲁棒地检测扩散模型生成的高质量人脸伪造。


<details>
  <summary>Details</summary>
Motivation: 当前高质量合成图像（尤其是扩散模型产生的）在空间和频谱特征上很难被区分，然而由复杂物理过程控制的面部属性（如照明中的镜面反射）更难被生成模型准确复现，因此可以作为鲁棒的伪造证据。

Method: 基于Retinex理论快速估计人脸纹理并精确分离出镜面反射分量；提出SRI-Net网络，采用两阶段交叉注意力机制捕捉镜面反射与人脸纹理、直射光之间的相关性，并将这些特征与图像特征融合进行伪造检测。

Result: 在多种深度伪造数据集（含扩散模型生成样本）上，SRI-Net在检测准确率和鲁棒性上优于基线方法，证明了基于镜面反射不一致性的有效性。

Conclusion: 该论文提出通过检测人脸高光反射与纹理及直射光的不一致性来识别由扩散模型等生成的高质量人脸伪造。实验表明方法在传统与生成式深度伪造数据集上均优于现有方法，尤其对扩散生成的伪造人脸效果显著。

Abstract: Detecting deepfakes has become increasingly challenging as forgery faces synthesized by AI-generated methods, particularly diffusion models, achieve unprecedented quality and resolution. Existing forgery detection approaches relying on spatial and frequency features demonstrate limited efficacy against high-quality, entirely synthesized forgeries. In this paper, we propose a novel detection method grounded in the observation that facial attributes governed by complex physical laws and multiple parameters are inherently difficult to replicate. Specifically, we focus on illumination, particularly the specular reflection component in the Phong illumination model, which poses the greatest replication challenge due to its parametric complexity and nonlinear formulation. We introduce a fast and accurate face texture estimation method based on Retinex theory to enable precise specular reflection separation. Furthermore, drawing from the mathematical formulation of specular reflection, we posit that forgery evidence manifests not only in the specular reflection itself but also in its relationship with corresponding face texture and direct light. To address this issue, we design the Specular-Reflection-Inconsistency-Network (SRI-Net), incorporating a two-stage cross-attention mechanism to capture these correlations and integrate specular reflection related features with image features for robust forgery detection. Experimental results demonstrate that our method achieves superior performance on both traditional deepfake datasets and generative deepfake datasets, particularly those containing diffusion-generated forgery faces.

</details>


### [41] [LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection](https://arxiv.org/abs/2602.06474)
*Xu Zhang,Zhe Chen,Jing Zhang,Dacheng Tao*

Main category: cs.CV

TL;DR: 利用将单个示例转为描述性文本并条件化冻结检测器的策略，LAB-Det实现了训练自由的一次样本域适应，在专业稀缺数据集上显著优于微调基线。


<details>
  <summary>Details</summary>
Motivation: 基础检测器在通用域表现良好，但在水下或工业缺陷等专业且数据稀缺的域上性能下降；传统少样本方法依赖微调易过拟合且代价高，故探索无需训练的一次样本域泛化策略。

Method: 提出LAB-Det：将每个示例的视觉信息投影为描述性文本，用该文本对冻结的检测器进行条件化，从而替代基于梯度的微调；不改变视觉特征，只通过语言提示实现适配。

Result: 在UODD（海洋）与NEU-DET（工业缺陷）基准上，LAB-Det在不更新任何参数的情况下，相较于最先进的微调基线最多提升5.4点mAP，证明了方法的有效性与可解释性。

Conclusion: 本文提出了在不更新模型参数的前提下，利用语言作为域不变桥梁实现对基础目标检测器的一次样本域适应，证明了语言条件化可在数据稀缺专业领域显著提升检测性能。

Abstract: Foundation object detectors such as GLIP and Grounding DINO excel on general-domain data but often degrade in specialized and data-scarce settings like underwater imagery or industrial defects. Typical cross-domain few-shot approaches rely on fine-tuning scarce target data, incurring cost and overfitting risks. We instead ask: Can a frozen detector adapt with only one exemplar per class without training? To answer this, we introduce training-free one-shot domain generalization for object detection, where detectors must adapt to specialized domains with only one annotated exemplar per class and no weight updates. To tackle this task, we propose LAB-Det, which exploits Language As a domain-invariant Bridge. Instead of adapting visual features, we project each exemplar into a descriptive text that conditions and guides a frozen detector. This linguistic conditioning replaces gradient-based adaptation, enabling robust generalization in data-scarce domains. We evaluate on UODD (underwater) and NEU-DET (industrial defects), two widely adopted benchmarks for data-scarce detection, where object boundaries are often ambiguous, and LAB-Det achieves up to 5.4 mAP improvement over state-of-the-art fine-tuned baselines without updating a single parameter. These results establish linguistic adaptation as an efficient and interpretable alternative to fine-tuning in specialized detection settings.

</details>


### [42] [Efficient-LVSM: Faster, Cheaper, and Better Large View Synthesis Model via Decoupled Co-Refinement Attention](https://arxiv.org/abs/2602.06478)
*Xiaosong Jia,Yihang Sun,Junqi You,Songbur Wong,Zichen Zou,Junchi Yan,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Efficient-LVSM通过双流与解耦共精炼，显著减少注意力计算并提升效率与性能，兼具泛化与增量推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer全自注意力在输入视图数量增加时计算复杂度呈二次增长，并且对不同类型token采用刚性参数共享，导致计算冗余与表达不匹配，需要更高效且灵活的设计。

Method: 提出双流架构：对输入视图采用仅内视图（intra-view）自注意力，对目标视图采用先自注意力再交叉注意力（self-then-cross）机制，从而避免在所有视图间的全局自注意力；引入解耦共精炼以允许不同类型token使用不同参数及KV-cache支持增量推理。

Result: 在RealEstate10K上，使用2个输入视图时PSNR达29.86 dB，超越LVSM 0.2 dB；训练收敛速度提升2倍，推理速度提升4.4倍。模型在多个基准上达到SOTA，并具备对未见视图数量的零样本泛化能力和KV-cache增量推理能力。

Conclusion: 该论文提出了Efficient-LVSM，一种针对基于Transformer的新视角合成（NVS）模型的高效双流结构，通过解耦共精炼（co-refinement）机制减少不必要的自注意力计算，改进训练与推理效率，并在多个基准上达到或超越现有最优结果。

Abstract: Feedforward models for novel view synthesis (NVS) have recently advanced by transformer-based methods like LVSM, using attention among all input and target views. In this work, we argue that its full self-attention design is suboptimal, suffering from quadratic complexity with respect to the number of input views and rigid parameter sharing among heterogeneous tokens. We propose Efficient-LVSM, a dual-stream architecture that avoids these issues with a decoupled co-refinement mechanism. It applies intra-view self-attention for input views and self-then-cross attention for target views, eliminating unnecessary computation. Efficient-LVSM achieves 29.86 dB PSNR on RealEstate10K with 2 input views, surpassing LVSM by 0.2 dB, with 2x faster training convergence and 4.4x faster inference speed. Efficient-LVSM achieves state-of-the-art performance on multiple benchmarks, exhibits strong zero-shot generalization to unseen view counts, and enables incremental inference with KV-cache, thanks to its decoupled designs.

</details>


### [43] [Instance-Free Domain Adaptive Object Detection](https://arxiv.org/abs/2602.06484)
*Hengfu Yu,Jinhong Deng,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: 提出IF-DAOD问题与RSCN方法，通过背景原型对齐与前后景关系一致性，实现无实例域自适应目标检测，在三类基准上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 在许多现实场景（如野生动物监测、病灶检测）中，收集含目标实例的目标域数据非常困难，而仅有背景图像更容易获取。现有DAOD方法依赖目标域的前景实例，无法在无实例条件下有效适配。

Method: 提出了关系与结构一致性网络（RSCN）。RSCN以背景特征原型为对齐目标，同时在源域内部鼓励前景特征与背景特征之间的关系一致性，并在目标域利用背景信息进行无实例适配。此外设计了相关损失项来维护结构一致性与关系一致性。

Result: 在三个专门设计的基准（模拟自动驾驶检测、野生动物检测、肺结节检测）上，RSCN在无实例设置下显著优于现有DAOD方法，证明了方法的有效性与泛化能力。

Conclusion: 该论文提出并解决了“无实例域自适应目标检测”（IF-DAOD）问题，证明即使目标域缺少前景实例，也能通过基于背景原型的对齐与关系一致性实现有效域适配。

Abstract: While Domain Adaptive Object Detection (DAOD) has made significant strides, most methods rely on unlabeled target data that is assumed to contain sufficient foreground instances. However, in many practical scenarios (e.g., wildlife monitoring, lesion detection), collecting target domain data with objects of interest is prohibitively costly, whereas background-only data is abundant. This common practical constraint introduces a significant technical challenge: the difficulty of achieving domain alignment when target instances are unavailable, forcing adaptation to rely solely on the target background information. We formulate this challenge as the novel problem of Instance-Free Domain Adaptive Object Detection. To tackle this, we propose the Relational and Structural Consistency Network (RSCN) which pioneers an alignment strategy based on background feature prototypes while simultaneously encouraging consistency in the relationship between the source foreground features and the background features within each domain, enabling robust adaptation even without target instances. To facilitate research, we further curate three specialized benchmarks, including simulative auto-driving detection, wildlife detection, and lung nodule detection. Extensive experiments show that RSCN significantly outperforms existing DAOD methods across all three benchmarks in the instance-free scenario. The code and benchmarks will be released soon.

</details>


### [44] [Rebenchmarking Unsupervised Monocular 3D Occupancy Prediction](https://arxiv.org/abs/2602.06488)
*Zizhan Guo,Yi Feng,Mengtan Zhang,Haoran Zhang,Wei Ye,Rui Fan*

Main category: cs.CV

TL;DR: 提出物理一致的占据概率表示与改进评估协议，并引入多视图遮挡感知机制，显著提升无监督单目3D占据预测，性能可比肩有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督单目3D占据估计在训练与评估协议上存在不一致（将体渲染网络输出视为占据概率但训练目标不同），且常用的2D真值不能揭示因几何约束不足导致的遮挡区域模糊性，需要一个物理一致且能量化遮挡区域性能的重构基准与方法。

Method: 分析体渲染过程中的变量，推导出最符合物理意义的占据概率表示；将该表示与体素级3D占据真值对齐来改进评估协议；设计遮挡感知的极化机制，引入多视图视觉信息以在遮挡区域提供显式约束；基于改进的评估协议对比广泛实验。

Result: 改进的评估协议和遮挡感知极化机制使所提方法在标准数据集上显著优于现有无监督方法，并能够达到或接近有监督方法的性能，代码与评估协议将在发表后公开。

Conclusion: 本文提出了一个针对无监督单目3D占据预测的重构基准，通过解析体渲染过程中的变量，识别并采用与物理最一致的占据概率表示，改进评估协议并与体素级3D占据真值对齐，从而解决训练与评估不一致的问题；同时引入遮挡感知的极化机制，利用多视图视觉线索在遮挡区域施加显式约束，提高占满与空闲空间的判别能力。实验表明方法显著超越现有无监督方法并能达到有监督方法的性能水平。

Abstract: Inferring the 3D structure from a single image, particularly in occluded regions, remains a fundamental yet unsolved challenge in vision-centric autonomous driving. Existing unsupervised approaches typically train a neural radiance field and treat the network outputs as occupancy probabilities during evaluation, overlooking the inconsistency between training and evaluation protocols. Moreover, the prevalent use of 2D ground truth fails to reveal the inherent ambiguity in occluded regions caused by insufficient geometric constraints. To address these issues, this paper presents a reformulated benchmark for unsupervised monocular 3D occupancy prediction. We first interpret the variables involved in the volume rendering process and identify the most physically consistent representation of the occupancy probability. Building on these analyses, we improve existing evaluation protocols by aligning the newly identified representation with voxel-wise 3D occupancy ground truth, thereby enabling unsupervised methods to be evaluated in a manner consistent with that of supervised approaches. Additionally, to impose explicit constraints in occluded regions, we introduce an occlusion-aware polarization mechanism that incorporates multi-view visual cues to enhance discrimination between occupied and free spaces in these regions. Extensive experiments demonstrate that our approach not only significantly outperforms existing unsupervised approaches but also matches the performance of supervised ones. Our source code and evaluation protocol will be made available upon publication.

</details>


### [45] [DreamHome-Pano: Design-Aware and Conflict-Free Panoramic Interior Generation](https://arxiv.org/abs/2602.06494)
*Lulu Chen,Yijiang Hu,Yuanqing Liu,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: 提出一种基于 Prompt-LLM 和 Conflict-Free Control 的全景室内可控生成框架，通过结构感知先验和多阶段训练，有效抑制风格对布局的破坏，提升美观与几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有多条件生成模型在融合严格结构约束与风格偏好时会产生“条件冲突”，导致风格干扰损害布局几何精度，需在美学与结构一致性间寻求平衡。

Method: 提出 Prompt-LLM 将布局约束与风格参考映射为专业描述性提示；设计 Conflict-Free Control 架构，结合结构感知几何先验与多条件解耦策略；构建全景室内基准与多阶段训练流程（渐进监督微调 + 强化学习）。

Result: 在全景室内合成任务上，DreamHome-Pano 在美观性与结构一致性之间取得更好平衡，实验显示其在视觉质量与保持建筑完整性方面超过现有方法。

Conclusion: DreamHome-Pano 有效解决了风格条件与布局几何之间的冲突，通过语义桥接与结构感知控制实现高保真全景室内生成。

Abstract: In modern interior design, the generation of personalized spaces frequently necessitates a delicate balance between rigid architectural structural constraints and specific stylistic preferences. However, existing multi-condition generative frameworks often struggle to harmonize these inputs, leading to "condition conflicts" where stylistic attributes inadvertently compromise the geometric precision of the layout. To address this challenge, we present DreamHome-Pano, a controllable panoramic generation framework designed for high-fidelity interior synthesis. Our approach introduces a Prompt-LLM that serves as a semantic bridge, effectively translating layout constraints and style references into professional descriptive prompts to achieve precise cross-modal alignment. To safeguard architectural integrity during the generative process, we develop a Conflict-Free Control architecture that incorporates structural-aware geometric priors and a multi-condition decoupling strategy, effectively suppressing stylistic interference from eroding the spatial layout. Furthermore, we establish a comprehensive panoramic interior benchmark alongside a multi-stage training pipeline, encompassing progressive Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). Experimental results demonstrate that DreamHome-Pano achieves a superior balance between aesthetic quality and structural consistency, offering a robust and professional-grade solution for panoramic interior visualization.

</details>


### [46] [Forest canopy height estimation from satellite RGB imagery using large-scale airborne LiDAR-derived training data and monocular depth estimation](https://arxiv.org/abs/2602.06503)
*Yongkang Lai,Xihan Mu,Tim R. McVicar,Dasheng Fan,Donghui Xie,Shanxin Guo,Wenli Huang,Tianjie Zhao,Guangjian Yan*

Main category: cs.CV

TL;DR: 利用约1.6万 km2航空LiDAR衍生CHM训练单目深度网络（Depth2CHM），可从3 m PlanetScope RGB影像高效估算连续树冠高度，验证显示精度优于现有全球米级产品。


<details>
  <summary>Details</summary>
Motivation: 空间激光雷达（如ICESat-2、GEDI）虽可提供全球森林结构观测但空间稀疏且存在不确定性；近地面航空/UAV LiDAR提供高精度但覆盖有限。希望通过利用大规模航空LiDAR数据训练单目深度网络，实现从高时空分辨率的立体感较差的卫星RGB影像中推估连续高分辨率树冠高度，从而弥补现有方法的不足。

Method: 收集约1.6万 km2的公开航空LiDAR点云衍生CHM与3 m分辨率的PlanetScope和航空RGB影像，采用Depth Anything V2进行训练，生成Depth2CHM模型；在中国（≈1 km2）和美国（≈116 km2）两处独立现场进行验证，并与现有全球米级CHM产品对比评估。

Result: 在两处独立验证点，Depth2CHM偏差分别为0.59 m和0.41 m，RMSE分别为2.54 m和5.75 m；与现有全球米级CHM产品相比，平均绝对误差降低约1.5 m，RMSE降低约2 m，表明该方法在空间连续性和精度上具有显著优势。

Conclusion: 本文提出并验证了将单目深度估计模型（Depth Anything V2）与大规模航空激光雷达（LiDAR）生成的树冠高度模型（CHM）联合训练，以实现从3 m分辨率PlanetScope RGB影像直接估算连续树冠高度的可行性和高精度。

Abstract: Large-scale, high-resolution forest canopy height mapping plays a crucial role in understanding regional and global carbon and water cycles. Spaceborne LiDAR missions, including the Ice, Cloud, and Land Elevation Satellite-2 (ICESat-2) and the Global Ecosystem Dynamics Investigation (GEDI), provide global observations of forest structure but are spatially sparse and subject to inherent uncertainties. In contrast, near-surface LiDAR platforms, such as airborne and unmanned aerial vehicle (UAV) LiDAR systems, offer much finer measurements of forest canopy structure, and a growing number of countries have made these datasets openly available. In this study, a state-of-the-art monocular depth estimation model, Depth Anything V2, was trained using approximately 16,000 km2 of canopy height models (CHMs) derived from publicly available airborne LiDAR point clouds and related products across multiple countries, together with 3 m resolution PlanetScope and airborne RGB imagery. The trained model, referred to as Depth2CHM, enables the estimation of spatially continuous CHMs directly from PlanetScope RGB imagery. Independent validation was conducted at sites in China (approximately 1 km2) and the United States (approximately 116 km2). The results showed that Depth2CHM could accurately estimate canopy height, with biases of 0.59 m and 0.41 m and root mean square errors (RMSEs) of 2.54 m and 5.75 m for these two sites, respectively. Compared with an existing global meter-resolution CHM product, the mean absolute error is reduced by approximately 1.5 m and the RMSE by approximately 2 m. These results demonstrated that monocular depth estimation networks trained with large-scale airborne LiDAR-derived canopy height data provide a promising and scalable pathway for high-resolution, spatially continuous forest canopy height estimation from satellite RGB imagery.

</details>


### [47] [FloorplanVLM: A Vision-Language Model for Floorplan Vectorization](https://arxiv.org/abs/2602.06507)
*Yuanqing Liu,Ziming Yang,Yulong Li,Yue Yang*

Main category: cs.CV

TL;DR: FloorplanVLM通过像素到序列的图像条件序列建模直接输出结构化JSON矢量化结果，配合大规模数据集和渐进训练（SFT+GRPO），在复杂布局的基准上显著提升几何精度与结构完整性。


<details>
  <summary>Details</summary>
Motivation: 光栅楼层图转工程级矢量图面临复杂拓扑与严格几何约束，现有像素或查询方法容易导致不连续或几何失真，故需要一种直接生成全局结构的方案以保证精度和有效性。

Method: 引入“像素到序列”范式，构建大规模数据集Floorplan-2M及高保真子集Floorplan-HQ-300K；采用渐进训练：先Supervised Fine-Tuning (SFT)进行结构引导与质量退火，再用Group Relative Policy Optimization (GRPO)实现严格几何对齐；并开放基准FPBench-2K用于评估。

Result: 在FPBench-2K上取得优异结构有效性，外墙IoU达92.52%，并能鲁棒泛化到非曼哈顿结构（斜墙、弧线等复杂几何）。

Conclusion: 提出FloorplanVLM，将楼层平面图矢量化问题重构为图像条件序列建模，直接输出结构化JSON序列以表示全局拓扑，从而更好地满足几何约束。

Abstract: Converting raster floorplans into engineering-grade vector graphics is challenging due to complex topology and strict geometric constraints. To address this, we present FloorplanVLM, a unified framework that reformulates floorplan vectorization as an image-conditioned sequence modeling task. Unlike pixel-based methods that rely on fragile heuristics or query-based transformers that generate fragmented rooms, our model directly outputs structured JSON sequences representing the global topology. This 'pixels-to-sequence' paradigm enables the precise and holistic constraint satisfaction of complex geometries, such as slanted walls and curved arcs. To support this data-hungry approach, we introduce a scalable data engine: we construct a large-scale dataset (Floorplan-2M) and a high-fidelity subset (Floorplan-HQ-300K) to balance geometric diversity and pixel-level precision. We then employ a progressive training strategy, using Supervised Fine-Tuning (SFT) for structural grounding and quality annealing, followed by Group Relative Policy Optimization (GRPO) for strict geometric alignment. To standardize evaluation on complex layouts, we establish and open-source FPBench-2K. Evaluated on this rigorous benchmark, FloorplanVLM demonstrates exceptional structural validity, achieving $\textbf{92.52%}$ external-wall IoU and robust generalization across non-Manhattan architectures.

</details>


### [48] [DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving](https://arxiv.org/abs/2602.06521)
*Feiyang jia,Lin Liu,Ziying Song,Caiyan Jia,Hangjun Ye,Xiaoshuai Hao,Long Chen*

Main category: cs.CV

TL;DR: DriveWorld-VLA通过在潜在空间将世界模型与VLA规划器共享表示，实现动作条件化的场景想象与规划联动，带来更好的决策性能与更低的监督需求。


<details>
  <summary>Details</summary>
Motivation: 当前方法未能在单一架构内有效统一未来场景演化与动作规划，主要因为潜在状态共享不足，导致视觉想象对动作决策影响有限。作者希望通过共享潜在表示将世界模型的前瞻性场景建模能力直接注入VLA规划器，从而提升决策质量并降低监督成本。

Method: 在潜在空间中联合建模世界演化和动作规划：1）将VLA与世界模型在表征层紧密集成，统一共享潜在状态；2）将世界模型的潜在状态作为VLA规划器的核心输入，使规划器能够评估候选动作对未来场景的影响；3）在潜在空间进行动作条件化的特征级想象，避免昂贵的像素级滚动；并在模型训练中减少对密集注释的依赖。

Result: 在多项基准上取得领先结果：NAVSIMv1上91.3 PDMS；NAVSIMv2上86.8 EPDMS；nuScenes上3秒平均碰撞率0.16。支持在潜在空间进行可控、动作条件化的想象并避免像素级开销。

Conclusion: 提出的DriveWorld-VLA在表征层紧密融合视觉-语言-动作（VLA）与世界模型，通过将世界模型的潜在状态作为VLA规划器的决策核心，实现了对未来场景演化的特征级、可控、动作条件化想象，从而提升了规划质量并减少对密集标注的依赖。实验表明在公开基准上取得了最先进的性能。

Abstract: End-to-end (E2E) autonomous driving has recently attracted increasing interest in unifying Vision-Language-Action (VLA) with World Models to enhance decision-making and forward-looking imagination. However, existing methods fail to effectively unify future scene evolution and action planning within a single architecture due to inadequate sharing of latent states, limiting the impact of visual imagination on action decisions. To address this limitation, we propose DriveWorld-VLA, a novel framework that unifies world modeling and planning within a latent space by tightly integrating VLA and world models at the representation level, which enables the VLA planner to benefit directly from holistic scene-evolution modeling and reducing reliance on dense annotated supervision. Additionally, DriveWorld-VLA incorporates the latent states of the world model as core decision-making states for the VLA planner, facilitating the planner to assess how candidate actions impact future scene evolution. By conducting world modeling entirely in the latent space, DriveWorld-VLA supports controllable, action-conditioned imagination at the feature level, avoiding expensive pixel-level rollouts. Extensive open-loop and closed-loop evaluations demonstrate the effectiveness of DriveWorld-VLA, which achieves state-of-the-art performance with 91.3 PDMS on NAVSIMv1, 86.8 EPDMS on NAVSIMv2, and 0.16 3-second average collision rate on nuScenes. Code and models will be released in https://github.com/liulin815/DriveWorld-VLA.git.

</details>


### [49] [MicroBi-ConvLSTM: An Ultra-Lightweight Efficient Model for Human Activity Recognition on Resource Constrained Devices](https://arxiv.org/abs/2602.06523)
*Mridankan Mandal*

Main category: cs.CV

TL;DR: 提出 MicroBi-ConvLSTM：11.4K 参数、双阶段卷积+双向 LSTM，适合 MCU 的 23KB 部署，保持强 HAR 性能并支持 INT8 量化。


<details>
  <summary>Details</summary>
Motivation: 为资源受限的可穿戴设备（受限 SRAM）设计在准确性和内存/计算预算间平衡的超轻量级 HAR 模型，解决现有轻量模型在操作系统开销后仍超出 MCU 内存限制的问题。

Method: 两阶段卷积特征提取（含 4x 时序池化）+ 单层双向 LSTM；模型平均 11.4K 参数，保持 O(N) 复杂度；使用 INT8 后训练量化以减少部署占用。

Result: 相比 TinierHAR（34K 参数）和 DeepConvLSTM，参数分别减少约 2.9x 和 11.9x；在八个 HAR 基准上取得高宏 F1 分数（如 UCI-HAR 93.41%、SKODA 94.46%、Daphnet 88.98%）；INT8 量化平均仅下降 0.21% F1，部署占用约 23.0 KB。

Conclusion: MicroBi-ConvLSTM 在超轻量级约束下实现了显著参数压缩，同时在多个 HAR 基准上保持竞争性性能，适合嵌入式 MCU 部署。

Abstract: Human Activity Recognition (HAR) on resource constrained wearables requires models that balance accuracy against strict memory and computational budgets. State of the art lightweight architectures such as TinierHAR (34K parameters) and TinyHAR (55K parameters) achieve strong accuracy, but exceed memory budgets of microcontrollers with limited SRAM once operating system overhead is considered. We present MicroBi-ConvLSTM, an ultra-lightweight convolutional-recurrent architecture achieving 11.4K parameters on average through two stage convolutional feature extraction with 4x temporal pooling and a single bidirectional LSTM layer. This represents 2.9x parameter reduction versus TinierHAR and 11.9x versus DeepConvLSTM while preserving linear O(N) complexity. Evaluation across eight diverse HAR benchmarks shows that MicroBi-ConvLSTM maintains competitive performance within the ultra-lightweight regime: 93.41% macro F1 on UCI-HAR, 94.46% on SKODA assembly gestures, and 88.98% on Daphnet gait freeze detection. Systematic ablation reveals task dependent component contributions where bidirectionality benefits episodic event detection, but provides marginal gains on periodic locomotion. INT8 post training quantization incurs only 0.21% average F1-score degradation, yielding a 23.0 KB average deployment footprint suitable for memory constrained edge devices.

</details>


### [50] [AdaptOVCD: Training-Free Open-Vocabulary Remote Sensing Change Detection via Adaptive Information Fusion](https://arxiv.org/abs/2602.06529)
*Mingyu Dou,Shi Qiu,Ming Hu,Yifan Chen,Huping Ye,Xiaohan Liao,Zhe Sun*

Main category: cs.CV

TL;DR: AdaptOVCD为无训练的开域变化检测提出数据—特征—决策三级自适应融合方案，结合SAM-HQ、DINOv3与DGTRS-CLIP，能零样本检测任意类别变化并接近监督上限。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测依赖预定义类别和大规模像素级标注，限制了模型在开放世界场景中的泛化与适用性，需设计无需训练且能处理任意类别变化的检测方法。

Method: 提出一个training-free的OVCD框架，采用双维多层信息融合：数据层的Adaptive Radiometric Alignment(ARA)与SAM-HQ结合实现辐射一致性分割；特征层的Adaptive Change Thresholding(ACT)结合全局差异分布与边缘先验并利用DINOv3实现鲁棒变化检测；决策层的Adaptive Confidence Filtering(ACF)融合语义置信与空间约束并结合DGTRS-CLIP实现高置信语义识别。

Result: 在九个场景上的综合评估表明，AdaptOVCD可零样本检测任意类别变化，超越现有训练免费方法；在跨数据集评估中达到84.89%的全监督性能上界，展现优越的泛化能力。

Conclusion: AdaptOVCD在不依赖训练的前提下，通过数据、特征、决策三层的自适应融合设计，实现了开域变化检测的强泛化能力与高精度语义识别，显著优于现有无监督方法并在跨数据集评估中达到接近监督上限的性能。

Abstract: Remote sensing change detection plays a pivotal role in domains such as environmental monitoring, urban planning, and disaster assessment. However, existing methods typically rely on predefined categories and large-scale pixel-level annotations, which limit their generalization and applicability in open-world scenarios. To address these limitations, this paper proposes AdaptOVCD, a training-free Open-Vocabulary Change Detection (OVCD) architecture based on dual-dimensional multi-level information fusion. The framework integrates multi-level information fusion across data, feature, and decision levels vertically while incorporating targeted adaptive designs horizontally, achieving deep synergy among heterogeneous pre-trained models to effectively mitigate error propagation. Specifically, (1) at the data level, Adaptive Radiometric Alignment (ARA) fuses radiometric statistics with original texture features and synergizes with SAM-HQ to achieve radiometrically consistent segmentation; (2) at the feature level, Adaptive Change Thresholding (ACT) combines global difference distributions with edge structure priors and leverages DINOv3 to achieve robust change detection; (3) at the decision level, Adaptive Confidence Filtering (ACF) integrates semantic confidence with spatial constraints and collaborates with DGTRS-CLIP to achieve high-confidence semantic identification. Comprehensive evaluations across nine scenarios demonstrate that AdaptOVCD detects arbitrary category changes in a zero-shot manner, significantly outperforming existing training-free methods. Meanwhile, it achieves 84.89\% of the fully-supervised performance upper bound in cross-dataset evaluations and exhibits superior generalization capabilities. The code is available at https://github.com/Dmygithub/AdaptOVCD.

</details>


### [51] [Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance](https://arxiv.org/abs/2602.06530)
*Haipeng Li,Rongxuan Peng,Anwei Luo,Shunquan Tan,Changsheng Chen,Anastasia Antsiferova*

Main category: cs.CV

TL;DR: 利用VLM共享特征空间漏洞，设计一种基于多模态引导损失的黑盒反取证攻击（ForgeryEraser），能在无需访问目标检测器的情况下，有效抹除伪造痕迹并破坏AIGC检测器性能。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC检测评估忽视反取证攻击，且许多检测器共享公开VLM骨干（如CLIP），这带来了对抗性脆弱性；需要构建能在现实中检验检测器鲁棒性的攻击框架。

Method: 提出基于多模态引导损失的优化方法：将伪造图像在VLM特征空间里拉向由文本生成的真实锚点，并远离伪造锚点；不使用传统logit级别优化，而是在VLM嵌入空间进行对齐/驱散操作，以实现无访问黑盒通用攻击。

Result: 大量实验表明，ForgeryEraser在多个先进检测器与基准上显著降低检测性能，并能让解释性检测模型输出与真实图像一致的解释，验证了方法有效性和可迁移性。

Conclusion: 本文提出的ForgeryEraser能在无需访问目标检测器的情况下，通过操控VLM（如CLIP）共享特征空间实现通用反取证攻击，从而显著降低现有AIGC检测器在全局合成和局部编辑场景下的性能，且能让可解释法医模型给出与真实图像一致的解释。

Abstract: The rapid advancement of AI-Generated Content (AIGC) technologies poses significant challenges for authenticity assessment. However, existing evaluation protocols largely overlook anti-forensics attack, failing to ensure the comprehensive robustness of state-of-the-art AIGC detectors in real-world applications. To bridge this gap, we propose ForgeryEraser, a framework designed to execute universal anti-forensics attack without access to the target AIGC detectors. We reveal an adversarial vulnerability stemming from the systemic reliance on Vision-Language Models (VLMs) as shared backbones (e.g., CLIP), where downstream AIGC detectors inherit the feature space of these publicly accessible models. Instead of traditional logit-based optimization, we design a multi-modal guidance loss to drive forged image embeddings within the VLM feature space toward text-derived authentic anchors to erase forgery traces, while repelling them from forgery anchors. Extensive experiments demonstrate that ForgeryEraser causes substantial performance degradation to advanced AIGC detectors on both global synthesis and local editing benchmarks. Moreover, ForgeryEraser induces explainable forensic models to generate explanations consistent with authentic images for forged images. Our code will be made publicly available.

</details>


### [52] [NECromancer: Breathing Life into Skeletons via BVH Animation](https://arxiv.org/abs/2602.06548)
*Mingxi Xu,Qi Wang,Zhengyu Wen,Phong Dao Thien,Zhengyu Li,Ning Zhang,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: 提出NECromancer，一套对任意BVH骨架适用的运动token化框架，包含结构感知编码器、拓扑无关分词器和大规模BVH数据集，实验证明其在压缩重建与跨骨架泛化上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖物种特定骨架的限制，推动跨物种、跨骨架的运动建模与合成。

Method: 包括三部分：OwO编码BVH的结构先验，TAT将运动序列压缩为拓扑无关的离散token，UvU构建了大规模异构BVH数据集。

Result: 在重大压缩下仍能高保真重建并成功解耦运动与骨架结构，支持跨物种迁移、组合、去噪、生成与文本检索等任务。

Conclusion: NECromancer提出了一个面向任意BVH骨架的通用运动分词器，能在多种形态学间共享运动表示。

Abstract: Motion tokenization is a key component of generalizable motion models, yet most existing approaches are restricted to species-specific skeletons, limiting their applicability across diverse morphologies. We propose NECromancer (NEC), a universal motion tokenizer that operates directly on arbitrary BVH skeletons. NEC consists of three components: (1) an Ontology-aware Skeletal Graph Encoder (OwO) that encodes structural priors from BVH files, including joint semantics, rest-pose offsets, and skeletal topology, into skeletal embeddings; (2) a Topology-Agnostic Tokenizer (TAT) that compresses motion sequences into a universal, topology-invariant discrete representation; and (3) the Unified BVH Universe (UvU), a large-scale dataset aggregating BVH motions across heterogeneous skeletons. Experiments show that NEC achieves high-fidelity reconstruction under substantial compression and effectively disentangles motion from skeletal structure. The resulting token space supports cross-species motion transfer, composition, denoising, generation with token-based models, and text-motion retrieval, establishing a unified framework for motion analysis and synthesis across diverse morphologies. Demo page: https://animotionlab.github.io/NECromancer/

</details>


### [53] [LIBERO-X: Robustness Litmus for Vision-Language-Action Models](https://arxiv.org/abs/2602.06556)
*Guodong Wang,Chenkai Zhang,Qingjie Liu,Jinjin Zhang,Jiancheng Cai,Junjie Liu,Xinmin Liu*

Main category: cs.CV

TL;DR: 提出LIBERO-X：一个含分层难度的评估协议和人类遥操作收集的多样化训练集，用以更真实地评估VLA模型的泛化与稳健性，实验显示现有模型在复杂扰动下显著退化。


<details>
  <summary>Details</summary>
Motivation: 现有基准评估协议不足，无法充分反映真实世界中的分布偏移，导致对VLA模型能力的误判；因此需要更细粒度和更具挑战性的评估来驱动模型进步。

Method: 设计分层的评估协议（考察空间泛化、物体识别、任务理解三大能力）并构建通过人工远程操作收集的多样化训练数据集；在基准上对代表性VLA模型进行实验，分析性能在累计扰动下的退化。

Result: 实验证明在累积扰动下代表性模型性能显著下降，暴露了场景理解和指令绑定的持续局限；LIBERO-X通过结合分层评估和高多样性训练数据，提供了更可靠的评估基础。

Conclusion: 该论文提出了针对视觉-语言-动作（VLA）模型更可靠的评估框架LIBERO-X，通过分层评估协议和高多样性训练数据来揭示模型在场景理解和指令对齐方面的薄弱点。

Abstract: Reliable benchmarking is critical for advancing Vision-Language-Action (VLA) models, as it reveals their generalization, robustness, and alignment of perception with language-driven manipulation tasks. However, existing benchmarks often provide limited or misleading assessments due to insufficient evaluation protocols that inadequately capture real-world distribution shifts. This work systematically rethinks VLA benchmarking from both evaluation and data perspectives, introducing LIBERO-X, a benchmark featuring: 1) A hierarchical evaluation protocol with progressive difficulty levels targeting three core capabilities: spatial generalization, object recognition, and task instruction understanding. This design enables fine-grained analysis of performance degradation under increasing environmental and task complexity; 2) A high-diversity training dataset collected via human teleoperation, where each scene supports multiple fine-grained manipulation objectives to bridge the train-evaluation distribution gap. Experiments with representative VLA models reveal significant performance drops under cumulative perturbations, exposing persistent limitations in scene comprehension and instruction grounding. By integrating hierarchical evaluation with diverse training data, LIBERO-X offers a more reliable foundation for assessing and advancing VLA development.

</details>


### [54] [SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs](https://arxiv.org/abs/2602.06566)
*Niccolo Avogaro,Nayanika Debnath,Li Mi,Thomas Frick,Junling Wang,Zexue He,Hang Hua,Konrad Schindler,Mattia Rigotti*

Main category: cs.CV

TL;DR: SPARC separates perception and reasoning via region-based visual search followed by focused reasoning, yielding better accuracy and much lower token/computation use across visual reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Monolithic test-time scaling is brittle as chains-of-thought entangle perception and reasoning, causing cascading errors and high compute; need modular approach to improve robustness and efficiency.

Method: Two-stage pipeline: explicit visual search to localize relevant regions, then reasoning conditioned on selected regions; supports asymmetric compute allocation and compressed contexts by low-res global search + high-res local processing.

Result: SPARC outperforms monolithic baselines and strong visual-grounding methods; improves Qwen3VL-4B on V* VQA by 6.7 points and beats 'thinking with images' by 4.6 points on OOD task with 200x lower token budget.

Conclusion: Paper proposes SPARC, a modular two-stage framework separating perception from reasoning to improve test-time scaling for vision-language models.

Abstract: Despite recent successes, test-time scaling - i.e., dynamically expanding the token budget during inference as needed - remains brittle for vision-language models (VLMs): unstructured chains-of-thought about images entangle perception and reasoning, leading to long, disorganized contexts where small perceptual mistakes may cascade into completely wrong answers. Moreover, expensive reinforcement learning with hand-crafted rewards is required to achieve good performance. Here, we introduce SPARC (Separating Perception And Reasoning Circuits), a modular framework that explicitly decouples visual perception from reasoning. Inspired by sequential sensory-to-cognitive processing in the brain, SPARC implements a two-stage pipeline where the model first performs explicit visual search to localize question-relevant regions, then conditions its reasoning on those regions to produce the final answer. This separation enables independent test-time scaling with asymmetric compute allocation (e.g., prioritizing perceptual processing under distribution shift), supports selective optimization (e.g., improving the perceptual stage alone when it is the bottleneck for end-to-end performance), and accommodates compressed contexts by running global search at lower image resolutions and allocating high-resolution processing only to selected regions, thereby reducing total visual tokens count and compute. Across challenging visual reasoning benchmarks, SPARC outperforms monolithic baselines and strong visual-grounding approaches. For instance, SPARC improves the accuracy of Qwen3VL-4B on the $V^*$ VQA benchmark by 6.7 percentage points, and it surpasses "thinking with images" by 4.6 points on a challenging OOD task despite requiring a 200$\times$ lower token budget.

</details>


### [55] [An Integer Linear Programming Approach to Geometrically Consistent Partial-Partial Shape Matching](https://arxiv.org/abs/2602.06590)
*Viktoria Ehm,Paul Roetzer,Florian Bernard,Daniel Cremers*

Main category: cs.CV

TL;DR: 本文首次提出基于几何一致性的整数线性规划框架，联合估计部分-部分三维形状的重叠区域与邻域保留对应，得到精确、平滑且可扩展的匹配结果。


<details>
  <summary>Details</summary>
Motivation: 现实中许多三维数据（如扫描）仅为部分观测，部分-部分匹配更贴近实际但具挑战性：既要精确计算对应关系，又要发现未知的重叠区域，现有工作大多关注完整-完整或部分-完整情形。

Method: 建立基于几何一致性的ILP模型，将几何一致性作为强先验，联合求解未知的重叠区域和邻域保持的点对应问题；通过优化保证匹配的平滑性与鲁棒性，并设计更可扩展的求解策略（如稀疏化或分块求解）以应对规模问题。

Result: 实验证明方法在匹配误差和匹配平滑性方面均取得高质量结果，且在可扩展性上优于先前的方法。

Conclusion: 提出了一种专为部分-部分（三维形状）匹配设计的整数线性规划（ILP）方法，能够同时估计重叠区域并计算保持邻域关系的对应点，具有较高匹配精度、平滑性和更好的可扩展性。

Abstract: The task of establishing correspondences between two 3D shapes is a long-standing challenge in computer vision. While numerous studies address full-full and partial-full 3D shape matching, only a limited number of works have explored the partial-partial setting, very likely due to its unique challenges: we must compute accurate correspondences while at the same time find the unknown overlapping region. Nevertheless, partial-partial 3D shape matching reflects the most realistic setting, as in many real-world cases, such as 3D scanning, shapes are only partially observable. In this work, we introduce the first integer linear programming approach specifically designed to address the distinctive challenges of partial-partial shape matching. Our method leverages geometric consistency as a strong prior, enabling both robust estimation of the overlapping region and computation of neighbourhood-preserving correspondences. We empirically demonstrate that our approach achieves high-quality matching results both in terms of matching error and smoothness. Moreover, we show that our method is more scalable than previous formalisms.

</details>


### [56] [ProtoQuant: Quantization of Prototypical Parts For General and Fine-Grained Image Classification](https://arxiv.org/abs/2602.06592)
*Mikołaj Janusz,Adam Wróbel,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 通过在潜在空间对原型进行离散化约束，ProtoQuant提供了稳定且可扩展的可解释分类头，能在不微调backbone的条件下在ImageNet级别数据上取得竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的部件模型在大规模数据集上泛化能力差，训练时常需昂贵的主干微调，且存在原型漂移问题使解释性失真。

Method: 在网络的潜在表示上学习一个离散码本，将原型约束为码本向量，从而防止原型漂移；将该量化头作为可插拔的解释性分类头，不需要更新主干网络参数。

Result: 在ImageNet及细粒度数据集（CUB-200、Cars-196）上，ProtoQuant在分类准确率上具有竞争力，并在可解释性度量上与其他原型方法可比，同时避免了原型漂移并无需更新backbone。

Conclusion: ProtoQuant通过在潜在空间中对原型进行向量量化，稳定了原型表示并保持其对训练数据的可解释性，实现了在不微调backbone的情况下可扩展到ImageNet级别的数据集。

Abstract: Prototypical parts-based models offer a "this looks like that" paradigm for intrinsic interpretability, yet they typically struggle with ImageNet-scale generalization and often require computationally expensive backbone finetuning. Furthermore, existing methods frequently suffer from "prototype drift," where learned prototypes lack tangible grounding in the training distribution and change their activation under small perturbations. We present ProtoQuant, a novel architecture that achieves prototype stability and grounded interpretability through latent vector quantization. By constraining prototypes to a discrete learned codebook within the latent space, we ensure they remain faithful representations of the training data without the need to update the backbone. This design allows ProtoQuant to function as an efficient, interpretable head that scales to large-scale datasets. We evaluate ProtoQuant on ImageNet and several fine-grained benchmarks (CUB-200, Cars-196). Our results demonstrate that ProtoQuant achieves competitive classification accuracy while generalizing to ImageNet and comparable interpretability metrics to other prototypical-parts-based methods.

</details>


### [57] [DAVE: Distribution-aware Attribution via ViT Gradient Decomposition](https://arxiv.org/abs/2602.06613)
*Adam Wróbel,Siddhartha Gairola,Jacek Tabor,Bernt Schiele,Bartosz Zieliński,Dawid Rymarczyk*

Main category: cs.CV

TL;DR: 提出一种基于输入梯度结构分解的ViT归因方法DAVE，隔离架构引入的伪影，生成更稳定、细粒度的像素级解释。


<details>
  <summary>Details</summary>
Motivation: 现有对ViT的归因方法常受patch结构和注意力路由导致的条纹/块状伪影影响，只能给出粗粒度的patch级归因；需要一种能利用ViT结构特性以生成稳定、高分辨率像素级解释的方法。

Method: 基于ViT的架构属性，对输入梯度进行分解，识别并隔离与patch嵌入和注意力机制相关的非稳态或结构化噪声分量；保留局部等变（locally equivariant）且稳定的梯度成分用于生成像素级归因。

Result: DAVE能有效减少架构诱导的伪影，生成更平滑且局部一致的像素级归因图，提升可解释性和稳定性（在论文中通过定量和定性实验验证）。

Conclusion: DAVE通过对ViT输入梯度进行结构化分解，分离出稳定且局部等变的有效输入-输出映射分量，从而减少由patch嵌入和注意力路由等架构组件引入的伪影，提供更细粒度且数学上有根据的像素级归因图。

Abstract: Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet producing stable and high-resolution attribution maps for these models remains challenging. Architectural components such as patch embeddings and attention routing often introduce structured artifacts in pixel-level explanations, causing many existing methods to rely on coarse patch-level attributions. We introduce DAVE \textit{(\underline{D}istribution-aware \underline{A}ttribution via \underline{V}iT Gradient D\underline{E}composition)}, a mathematically grounded attribution method for ViTs based on a structured decomposition of the input gradient. By exploiting architectural properties of ViTs, DAVE isolates locally equivariant and stable components of the effective input--output mapping. It separates these from architecture-induced artifacts and other sources of instability.

</details>


### [58] [CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling](https://arxiv.org/abs/2602.06619)
*Yuxin He,An Li,Cheng Xue*

Main category: cs.CV

TL;DR: CauCLIP结合频率域增强和因果抑制损失，利用CLIP学习面向手术阶段识别的领域不变表征，在SurgVisDom基准上显著提升合成到真实的域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 外科手术阶段识别需要上下文感知，但真实临床视频标注稀缺，且合成与真实数据存在大领域差异。因此需构建无需访问目标域数据的领域不变表征，提升模型在真实外科视频上的泛化能力。

Method: 采用频率域增强对域特定属性进行扰动以保留语义结构，并设计因果抑制损失以减弱与因果无关的偏置，同时利用CLIP的视觉-语言对齐学习稳定表征。将两者整合到统一训练框架中，无需目标域数据即可实现域泛化。

Result: 在SurgVisDom困难适配基准上，CauCLIP显著优于所有比较方法，验证了基于因果导向的视觉-语言模型在领域泛化手术视频理解中的有效性。

Conclusion: 本文提出的CauCLIP通过因果视角和视觉-语言预训练（CLIP）联合学习，旨在解决合成到真实外科视频的领域迁移问题。实验表明方法在SurgVisDom基准上表现优异，能更好地学习领域不变的手术阶段特征。

Abstract: Surgical phase recognition is a critical component for context-aware decision support in intelligent operating rooms, yet training robust models is hindered by limited annotated clinical videos and large domain gaps between synthetic and real surgical data. To address this, we propose CauCLIP, a causality-inspired vision-language framework that leverages CLIP to learn domain-invariant representations for surgical phase recognition without access to target domain data. Our approach integrates a frequency-based augmentation strategy to perturb domain-specific attributes while preserving semantic structures, and a causal suppression loss that mitigates non-causal biases and reinforces causal surgical features. These components are combined in a unified training framework that enables the model to focus on stable causal factors underlying surgical workflows. Experiments on the SurgVisDom hard adaptation benchmark demonstrate that our method substantially outperforms all competing approaches, highlighting the effectiveness of causality-guided vision-language models for domain-generalizable surgical video understanding.

</details>


### [59] [PlanViz: Evaluating Planning-Oriented Image Generation and Editing for Computer-Use Tasks](https://arxiv.org/abs/2602.06663)
*Junxian Li,Kai Liu,Leyang Chen,Weida Wang,Zhixin Wang,Jiaqi Xu,Fan Li,Renjing Pei,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 提出PlanViz基准与PlanScore评分，评估UMMs在路径规划、工作图示和网页/UI展示三类计算机使用相关图像生成编辑任务的能力，构建高质量人工注释数据并揭示当前模型的局限与改进机会。


<details>
  <summary>Details</summary>
Motivation: 尽管UMMs在生成图像和多模态推理方面表现突出，但其在支持日常计算机使用相关的规划性图像生成与编辑任务（要求空间推理与步骤推理）方面的能力尚未被充分研究，故提出专门基准进行评估。

Method: 设计三个子任务（路径规划、工作图示、网页与UI展示），构建人工标注的问题与参考图像并实施质量控制流程，且提出任务自适应评分机制PlanScore用于衡量生成图像的正确性、视觉质量与效率，并在多个模型上进行实验评估。

Result: 通过实验，发现现有UMMs在完成PlanViz上的任务时存在关键局限，展现出在空间与程序性推理、细节一致性和效率上不足，同时指出了未来研究方向与改进机会。

Conclusion: 该论文提出了PlanViz基准，用于评估统一多模态模型在与日常计算机操作相关的图像生成与编辑任务中的能力，指出现有UMMs在空间推理与程序性理解方面存在待测性能缺陷。

Abstract: Unified multimodal models (UMMs) have shown impressive capabilities in generating natural images and supporting multimodal reasoning. However, their potential in supporting computer-use planning tasks, which are closely related to our lives, remain underexplored. Image generation and editing in computer-use tasks require capabilities like spatial reasoning and procedural understanding, and it is still unknown whether UMMs have these capabilities to finish these tasks or not. Therefore, we propose PlanViz, a new benchmark designed to evaluate image generation and editing for computer-use tasks. To achieve the goal of our evaluation, we focus on sub-tasks which frequently involve in daily life and require planning steps. Specifically, three new sub-tasks are designed: route planning, work diagramming, and web&UI displaying. We address challenges in data quality ensuring by curating human-annotated questions and reference images, and a quality control process. For challenges of comprehensive and exact evaluation, a task-adaptive score, PlanScore, is proposed. The score helps understanding the correctness, visual quality and efficiency of generated images. Through experiments, we highlight key limitations and opportunities for future research on this topic.

</details>


### [60] [CytoCrowd: A Multi-Annotator Benchmark Dataset for Cytology Image Analysis](https://arxiv.org/abs/2602.06674)
*Yonghao Si,Xingyuan Zeng,Zhao Chen,Libin Zheng,Caleb Chen Cao,Lei Chen,Jian Yin*

Main category: cs.CV

TL;DR: CytoCrowd: 446 cytology images with 4 raw pathologist annotations and a senior-expert gold standard; supports detection/classification benchmarks and evaluation of annotation-aggregation methods; baselines show task difficulty and dataset value.


<details>
  <summary>Details</summary>
Motivation: Most medical image datasets either hide expert disagreement by providing a single consensus label or provide multiple annotations without a separate gold standard, hindering objective evaluation of aggregation methods.

Method: Collected 446 high-res cytology images; obtained four independent pathologist annotations per image plus a senior-expert gold-standard; prepared dataset for detection and classification tasks; ran baseline models/aggregation algorithms to evaluate.

Result: CytoCrowd released with baselines demonstrating challenge: notable disagreement among annotators and gaps between aggregation outputs and gold-standard; models perform reasonably on ground-truth tasks but aggregation remains hard.

Conclusion: CytoCrowd fills an important dataset gap by providing both raw multiple annotations and a separate gold-standard, enabling evaluation of both detection/classification and annotator-aggregation methods; baselines show its difficulty.

Abstract: High-quality annotated datasets are crucial for advancing machine learning in medical image analysis. However, a critical gap exists: most datasets either offer a single, clean ground truth, which hides real-world expert disagreement, or they provide multiple annotations without a separate gold standard for objective evaluation. To bridge this gap, we introduce CytoCrowd, a new public benchmark for cytology analysis. The dataset features 446 high-resolution images, each with two key components: (1) raw, conflicting annotations from four independent pathologists, and (2) a separate, high-quality gold-standard ground truth established by a senior expert. This dual structure makes CytoCrowd a versatile resource. It serves as a benchmark for standard computer vision tasks, such as object detection and classification, using the ground truth. Simultaneously, it provides a realistic testbed for evaluating annotation aggregation algorithms that must resolve expert disagreements. We provide comprehensive baseline results for both tasks. Our experiments demonstrate the challenges presented by CytoCrowd and establish its value as a resource for developing the next generation of models for medical image analysis.

</details>


### [61] [Can We Build a Monolithic Model for Fake Image Detection? SICA: Semantic-Induced Constrained Adaptation for Unified-Yet-Discriminative Artifact Feature Space Reconstruction](https://arxiv.org/abs/2602.06676)
*Bo Du,Xiaochen Ma,Xuekang Zhu,Zhe Yang,Chaogun Niu,Jian Liu,Ji-Zhe Zhou*

Main category: cs.CV

TL;DR: 本文发现跨子域伪影存在异质性导致特征坍塌，提出SICA以语义先验约束重构统一且判别性的伪影特征空间，实验证明优于多种方法。


<details>
  <summary>Details</summary>
Motivation: 现有单一模型在联合检测四个伪图像子域时表现不如集成方法，原因是子域间伪影存在本质差异（“异质现象”），导致伪影特征空间坍塌，需构建统一且具判别性的特征表示。

Method: 提出Semantic-Induced Constrained Adaptation (SICA)，利用高层语义作为结构先验，通过约束适配策略重构统一且具判别性的伪影特征空间，实现近正交重构。

Result: 在作者构建的OpenMMSec基准上，SICA优于15个最新方法，实验显示其能够将各子域的伪影特征重构为近正交结构，提升泛化与检测性能。

Conclusion: 本文提出SICA方法，有效解决单一模型在跨子域伪图像检测中表现不佳的问题，通过语义先验重构伪影特征空间，实验在OpenMMSec数据集上超过15个SOTA方法，验证了方法有效性。

Abstract: Fake Image Detection (FID), aiming at unified detection across four image forensic subdomains, is critical in real-world forensic scenarios. Compared with ensemble approaches, monolithic FID models are theoretically more promising, but to date, consistently yield inferior performance in practice. In this work, by discovering the ``heterogeneous phenomenon'', which is the intrinsic distinctness of artifacts across subdomains, we diagnose the cause of this underperformance for the first time: the collapse of the artifact feature space driven by such phenomenon. The core challenge for developing a practical monolithic FID model thus boils down to the ``unified-yet-discriminative" reconstruction of the artifact feature space. To address this paradoxical challenge, we hypothesize that high-level semantics can serve as a structural prior for the reconstruction, and further propose Semantic-Induced Constrained Adaptation (SICA), the first monolithic FID paradigm. Extensive experiments on our OpenMMSec dataset demonstrate that SICA outperforms 15 state-of-the-art methods and reconstructs the target unified-yet-discriminative artifact feature space in a near-orthogonal manner, thus firmly validating our hypothesis. The code and dataset are available at:https: //github.com/scu-zjz/SICA_OpenMMSec.

</details>


### [62] [Clinical-Prior Guided Multi-Modal Learning with Latent Attention Pooling for Gait-Based Scoliosis Screening](https://arxiv.org/abs/2602.06743)
*Dong Chen,Zizhuang Wei,Jialei Xu,Xinyang Sun,Zonglin He,Meiru An,Huili Peng,Yong Hu,Kenneth MC Cheung*

Main category: cs.CV

TL;DR: 提出ScoliGait数据集与基于临床先验的多模态可解释框架，在无重复受试者测试集上实现AIS评估的新SOTA，推动可扩展且可解释的无创筛查。


<details>
  <summary>Details</summary>
Motivation: 现有步态视频数据集和方法存在数据泄露（同一受试者重复片段导致性能虚高）以及模型缺乏临床可解释性的问题；需要一个真实、无重复的基准与可解释的多模态方法以推动可扩展的AIS筛查。

Method: 构建了包含1572个训练片段和300个独立测试片段的视频基准数据集，每个片段标注了Cobb角和基于临床运动学先验的描述性文本；提出多模态框架，包括：1) 基于临床先验的运动学知识图以提供可解释的特征表示；2) 潜在注意力池化机制用于融合视频、文本和知识图模态；在受试者独立的测试集上评估并与基线方法比较。

Result: 在ScoliGait受试者独立基准上，本方法显著优于现有方法，在关键指标（如Cobb角回归误差和分类准确率）上取得明显提升，并提供了基于运动学知识图的可解释性分析，有助于临床理解模型决策。

Conclusion: 本研究提出了ScoliGait数据集和一套多模态、可解释的AIS（青少年特发性脊柱侧弯）步态视频评估方法，并在严格的、受试者独立的基准上取得了新的最先进性能，证明了该方法在无创、可扩展的AIS筛查中的潜力。

Abstract: Adolescent Idiopathic Scoliosis (AIS) is a prevalent spinal deformity whose progression can be mitigated through early detection. Conventional screening methods are often subjective, difficult to scale, and reliant on specialized clinical expertise. Video-based gait analysis offers a promising alternative, but current datasets and methods frequently suffer from data leakage, where performance is inflated by repeated clips from the same individual, or employ oversimplified models that lack clinical interpretability. To address these limitations, we introduce ScoliGait, a new benchmark dataset comprising 1,572 gait video clips for training and 300 fully independent clips for testing. Each clip is annotated with radiographic Cobb angles and descriptive text based on clinical kinematic priors. We propose a multi-modal framework that integrates a clinical-prior-guided kinematic knowledge map for interpretable feature representation, alongside a latent attention pooling mechanism to fuse video, text, and knowledge map modalities. Our method establishes a new state-of-the-art, demonstrating a significant performance gap on a realistic, non-repeating subject benchmark. Our approach establishes a new state of the art, showing a significant performance gain on a realistic, subject-independent benchmark. This work provides a robust, interpretable, and clinically grounded foundation for scalable, non-invasive AIS assessment.

</details>


### [63] [Gold Exploration using Representations from a Multispectral Autoencoder](https://arxiv.org/abs/2602.06748)
*Argyro Tsandalidou,Konstantinos Dogeas,Eleftheria Tetoula Tsonga,Elisavet Parselia,Georgios Tsimiklis,George Arvanitakis*

Main category: cs.CV

TL;DR: 预训练自编码器（Isometric）在Sentinel-2多光谱影像上学习的表征能在有限标注下显著提升金矿识别，补丁级与图像级准确率分别提升到0.68和0.73，显示基础模型表征在找矿应用上的潜力。


<details>
  <summary>Details</summary>
Motivation: 现场矿产勘探数据昂贵且稀缺，卫星遥感提供覆盖广、成本低的替代数据。研究动机在于探索预训练生成式表征是否能从多光谱遥感影像中捕捉可迁移的矿物学模式，从而在有限标注下提高金矿识别性能。

Method: 构建名为Isometric的自编码器基础模型，使用FalconSpace-S2 v1.0大规模数据集预训练以学习光谱-空间信息密集的表征；在63幅已知金矿/非金矿位置的Sentinel-2影像上提取补丁级嵌入，使用轻量XGBoost分类器进行补丁级与图像级金/非金分类，并与直接使用原始光谱作为特征的基线方法比较。

Result: 在63幅图像的实验中，基于Isometric表征的方法使补丁级准确率由基线的0.51提升至0.68，图像级准确率由0.55提升至0.73，表明生成式嵌入能捕获可迁移矿物学信息并提升分类性能。

Conclusion: 该论文证明了用预训练自编码器生成的表征能显著提升基于多光谱Sentinel-2影像的找矿（黄金）可预测性，在小样本标注下仍具迁移能力，表明基础模型表征对大范围、低成本找矿具有潜力。

Abstract: Satellite imagery is employed for large-scale prospectivity mapping due to the high cost and typically limited availability of on-site mineral exploration data. In this work, we present a proof-of-concept framework that leverages generative representations learned from multispectral Sentinel-2 imagery to identify gold-bearing regions from space. An autoencoder foundation model, called Isometric, which is pretrained on the large-scale FalconSpace-S2 v1.0 dataset, produces information-dense spectral-spatial representations that serve as inputs to a lightweight XGBoost classifier. We compare this representation-based approach with a raw spectral input baseline using a dataset of 63 Sentinel-2 images from known gold and non-gold locations. The proposed method improves patch-level accuracy from 0.51 to 0.68 and image-level accuracy from 0.55 to 0.73, demonstrating that generative embeddings capture transferable mineralogical patterns even with limited labeled data. These results highlight the potential of foundation-model representations to make mineral exploration more efficient, scalable, and globally applicable.

</details>


### [64] [Revisiting Emotions Representation for Recognition in the Wild](https://arxiv.org/abs/2602.06778)
*Joao Baptista Cardia Neto,Claudio Ferrari,Stefano Berretti*

Main category: cs.CV

TL;DR: 作者提出基于VAD到多情绪分布映射的自动重标注方法，将单一情绪标注扩展为概率分布，从而更自然地表示复杂与混合情绪，并在初步实验中验证其可行性，数据已公开。


<details>
  <summary>Details</summary>
Motivation: 传统把情绪识别视为六类原型情绪的单标签问题过于简化，无法表示自发情绪的多面性。将情绪描述为概率分布可以更好地反映多情绪混合与感知不确定性。

Method: 利用一项研究中对基础和复合情绪在VAD空间的概率分布映射，自动为现有图像数据集（带VAD标注）生成每个情绪类别的概率标签；并用这些重标注数据进行分布学习，展示方法在实验中的初步优势。

Result: 通过候选方法对带VAD标注的数据自动重标注为情绪概率分布，实验表明该表征能丰富情绪描述并应对感知模糊，作者公开了重标注数据用于进一步研究。

Conclusion: 本文提出将面部情绪识别从单标签分类扩展为情绪分布学习，通过将已有数据集的VAD标注映射到多情绪概率分布，实现对复杂情绪的混合描述，并缓解感知歧义性。

Abstract: Facial emotion recognition has been typically cast as a single-label classification problem of one out of six prototypical emotions. However, that is an oversimplification that is unsuitable for representing the multifaceted spectrum of spontaneous emotional states, which are most often the result of a combination of multiple emotions contributing at different intensities. Building on this, a promising direction that was explored recently is to cast emotion recognition as a distribution learning problem. Still, such approaches are limited in that research datasets are typically annotated with a single emotion class. In this paper, we contribute a novel approach to describe complex emotional states as probability distributions over a set of emotion classes. To do so, we propose a solution to automatically re-label existing datasets by exploiting the result of a study in which a large set of both basic and compound emotions is mapped to probability distributions in the Valence-Arousal-Dominance (VAD) space. In this way, given a face image annotated with VAD values, we can estimate the likelihood of it belonging to each of the distributions, so that emotional states can be described as a mixture of emotions, enriching their description, while also accounting for the ambiguous nature of their perception. In a preliminary set of experiments, we illustrate the advantages of this solution and a new possible direction of investigation. Data annotations are available at https://github.com/jbcnrlz/affectnet-b-annotation.

</details>


### [65] [Machine Learning for Detection and Severity Estimation of Sweetpotato Weevil Damage in Field and Lab Conditions](https://arxiv.org/abs/2602.06786)
*Doreen M. Chelangat,Sudi Murindanyi,Bruce Mugizi,Paul Musana,Benard Yada,Milton A. Otema,Florence Osaru,Andrew Katumba,Joyce Nakatumba-Nabende*

Main category: cs.CV

TL;DR: 提出基于计算机视觉的自动化评估流程：田间分类（71.4%准确率）+实验室两阶段YOLO12检测（mAP77.7%），可替代主观人工评分，促进甘薯抗虫育种。


<details>
  <summary>Details</summary>
Motivation: 传统人工评分耗时、主观且不一致，限制育种项目中抗虫性品系的高效筛选。研究旨在引入自动、客观、可扩展的表型评估方法以加速育种流程。

Method: 在田间构建分类数据集并训练分类模型预测根部损伤等级（田间模型测试准确率71.43%）；在实验室构建标注数据并采用两阶段目标检测管线（先进行根部分割，再切图以提高小目标检测），使用YOLO12进行实时检测以识别小型取食孔。

Result: 田间分类模型测试准确率71.43%；实验室目标检测模型在识别微小取食孔上达到mAP 77.7%。整体表明该方法可提升甘薯育种表型学效率并协助减轻象甲对粮食安全的影响。

Conclusion: 该研究证明了计算机视觉能有效自动评估甘薯(地瓜)害虫Cylas spp.（地瓜象甲）造成的根部损伤，提供比传统人工评分更高的客观性与可扩展性。

Abstract: Sweetpotato weevils (Cylas spp.) are considered among the most destructive pests impacting sweetpotato production, particularly in sub-Saharan Africa. Traditional methods for assessing weevil damage, predominantly relying on manual scoring, are labour-intensive, subjective, and often yield inconsistent results. These challenges significantly hinder breeding programs aimed at developing resilient sweetpotato varieties. This study introduces a computer vision-based approach for the automated evaluation of weevil damage in both field and laboratory contexts. In the field settings, we collected data to train classification models to predict root-damage severity levels, achieving a test accuracy of 71.43%. Additionally, we established a laboratory dataset and designed an object detection pipeline employing YOLO12, a leading real-time detection model. This methodology incorporated a two-stage laboratory pipeline that combined root segmentation with a tiling strategy to improve the detectability of small objects. The resulting model demonstrated a mean average precision of 77.7% in identifying minute weevil feeding holes. Our findings indicate that computer vision technologies can provide efficient, objective, and scalable assessment tools that align seamlessly with contemporary breeding workflows. These advancements represent a significant improvement in enhancing phenotyping efficiency within sweetpotato breeding programs and play a crucial role in mitigating the detrimental effects of weevils on food security.

</details>


### [66] [A Unified Formula for Affine Transformations between Calibrated Cameras](https://arxiv.org/abs/2602.06805)
*Levente Hajder*

Main category: cs.CV

TL;DR: 本技术说明推导了两个已标定视图之间局部图像补丁的闭式仿射映射，依赖相对相机位姿、像坐标和局部表面法线。


<details>
  <summary>Details</summary>
Motivation: 在多视图几何与局部特征匹配中，需要精确描述小图像区域间如何变形，尤其用于图像配准、立体重建和特征稳定性分析，因而需要从相机几何和表面形状出发给出明确的变换公式。

Method: 推导基于几何成像模型，利用两视图的相对位姿（旋转和平移）、像平面坐标与局部平面近似（由表面法线描述），通过透视投影和局部平面映射的线性化得到仿射变换的闭式表达。

Result: 给出一个显式的仿射矩阵表达式（闭式），并证明其为相对相机姿态、像点位置和局部表面法线的函数；该表达式可用于精确预测局部补丁的变形。

Conclusion: 本文导出并给出封闭形式的仿射变换表达式，用于将两个已标定视图之间的局部图像补丁映射起来，结论是该仿射变换由相对相机姿态、图像坐标和局部表面法线共同决定。

Abstract: In this technical note, we derive a closed-form expression for the affine transformation mapping local image patches between two calibrated views. We show that the transformation is a function of the relative camera pose, the image coordinates, and the local surface normal.

</details>


### [67] [RAIGen: Rare Attribute Identification in Text-to-Image Generative Models](https://arxiv.org/abs/2602.06806)
*Silpa Vadakkeeveetil Sreelatha,Dan Wang,Serge Belongie,Muhammad Awais,Anjan Dutta*

Main category: cs.CV

TL;DR: 提出RAIGen，用稀疏自编码器和结合激活频率与语义区分度的少数度量，无监督发现并放大扩散模型中的稀有可解释属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖预定义的公平性类别（封闭集），要么只关注多数主导属性（开放集），都忽视了数据分布中低频但存在的社会、文化或风格属性。发现并增强这些稀有属性有助于更全面地审计和控制生成模型的语义覆盖。

Method: 使用Matryoshka稀疏自编码器提取层级稀疏神经元表示，并设计少数度量，将神经元激活频率与语义区分度结合，用于筛选出语义可解释的“少数”神经元；通过查看这些神经元的高激活图像集合来发现稀有属性，进而在生成阶段对相应神经元进行放大以增强目标属性。

Result: 在Stable Diffusion和更大模型SDXL上进行实验，RAIGen能发现超出预设公平性类别的稀有属性，支持跨架构系统化审计，并能通过目标放大实现稀有属性的生成增强。

Conclusion: RAIGen是一个用于扩散模型中未监督稀有属性发现的框架，能识别并放大训练数据中低频但可解释的语义特征，从而补充现有封闭集与开放集偏差研究。

Abstract: Text-to-image diffusion models achieve impressive generation quality but inherit and amplify training-data biases, skewing coverage of semantic attributes. Prior work addresses this in two ways. Closed-set approaches mitigate biases in predefined fairness categories (e.g., gender, race), assuming socially salient minority attributes are known a priori. Open-set approaches frame the task as bias identification, highlighting majority attributes that dominate outputs. Both overlook a complementary task: uncovering rare or minority features underrepresented in the data distribution (social, cultural, or stylistic) yet still encoded in model representations. We introduce RAIGen, the first framework, to our knowledge, for un-supervised rare-attribute discovery in diffusion models. RAIGen leverages Matryoshka Sparse Autoencoders and a novel minority metric combining neuron activation frequency with semantic distinctiveness to identify interpretable neurons whose top-activating images reveal underrepresented attributes. Experiments show RAIGen discovers attributes beyond fixed fairness categories in Stable Diffusion, scales to larger models such as SDXL, supports systematic auditing across architectures, and enables targeted amplification of rare attributes during generation.

</details>


### [68] [GaussianPOP: Principled Simplification Framework for Compact 3D Gaussian Splatting via Error Quantification](https://arxiv.org/abs/2602.06830)
*Soonbin Lee,Yeong-Gyu Kim,Simon Sasse,Tomas M. Borges,Yago Sanchez,Eun-Seok Ryu,Thomas Schierl,Cornelius Hellge*

Main category: cs.CV

TL;DR: GaussianPOP通过基于渲染方程的解析误差量化，为3D Gaussian Splatting提供高效且更准确的简化策略，显著提升压缩后渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有简化方法依赖混合权重或灵敏度等重要性得分，但这些得分未直接以视觉误差驱动，可能导致在紧凑性与视觉质量之间的次优折衷。

Method: 提出了一个从3DGS渲染方程导出的新误差准则，并设计出高效算法实现单次前向传播中实用的误差计算；框架支持在训练中剪枝和训练后通过迭代重量化误差进行逐步简化以提升稳定性。

Result: 在两种应用场景（在训剪枝与训后简化）上，GaussianPOP始终优于现有最先进的剪枝方法，展示在模型压缩与高质量渲染间的更优权衡。

Conclusion: 本文提出了GaussianPOP，一种基于解析误差量化的3D Gaussian Splatting简化框架，通过直接从渲染方程推导出的误差准则精确衡量每个高斯对渲染图像的贡献，从而在模型紧凑性与渲染保真度间取得更好权衡。

Abstract: Existing 3D Gaussian Splatting simplification methods commonly use importance scores, such as blending weights or sensitivity, to identify redundant Gaussians. However, these scores are not driven by visual error metrics, often leading to suboptimal trade-offs between compactness and rendering fidelity. We present GaussianPOP, a principled simplification framework based on analytical Gaussian error quantification. Our key contribution is a novel error criterion, derived directly from the 3DGS rendering equation, that precisely measures each Gaussian's contribution to the rendered image. By introducing a highly efficient algorithm, our framework enables practical error calculation in a single forward pass. The framework is both accurate and flexible, supporting on-training pruning as well as post-training simplification via iterative error re-quantification for improved stability. Experimental results show that our method consistently outperforms existing state-of-the-art pruning methods across both application scenarios, achieving a superior trade-off between model compactness and high rendering quality.

</details>


### [69] [Rethinking Multi-Condition DiTs: Eliminating Redundant Attention via Position-Alignment and Keyword-Scoping](https://arxiv.org/abs/2602.06850)
*Chao Zhou,Tianyi Wei,Yiling Chen,Wenbo Zhou,Nenghai Yu*

Main category: cs.CV

TL;DR: PKA通过位置对齐和关键词范围注意力去冗余多条件交互，并配合敏感度感知采样加速训练，显著提升DiT在多条件生成的效率和条件一致性。


<details>
  <summary>Details</summary>
Motivation: 现有DiT在多条件控制时采用"concatenate-and-attend"导致随着条件数量增加呈二次级的计算和内存爆炸，而很多跨模态交互在空间或语义上是冗余的，亟需更高效的注意力机制。

Method: 提出Position-Aligned Attention(PAA)用于线性化空间控制并强制局部patch对齐；提出Keyword-Scoped Attention(KSA)通过语义感知掩码剪枝无关的主体交互；并引入Conditional Sensitivity-Aware Sampling(CSAS)在训练中重加权关键去噪阶段以加速收敛和提高条件保真度。

Result: 实验显示PKA在推理速度上实现约10.0×加速，显存节省约5.1×，并在条件保真度和收敛速度上有明显提升。

Conclusion: 该论文提出PKA框架，通过位置对齐注意力和关键词范围注意力去除多条件交互中的空间和语义冗余，从而在保持或提升条件一致性的同时显著降低计算和内存开销。

Abstract: While modern text-to-image models excel at prompt-based generation, they often lack the fine-grained control necessary for specific user requirements like spatial layouts or subject appearances. Multi-condition control addresses this, yet its integration into Diffusion Transformers (DiTs) is bottlenecked by the conventional ``concatenate-and-attend'' strategy, which suffers from quadratic computational and memory overhead as the number of conditions scales. Our analysis reveals that much of this cross-modal interaction is spatially or semantically redundant. To this end, we propose Position-aligned and Keyword-scoped Attention (PKA), a highly efficient framework designed to eliminate these redundancies. Specifically, Position-Aligned Attention (PAA) linearizes spatial control by enforcing localized patch alignment, while Keyword-Scoped Attention (KSA) prunes irrelevant subject-driven interactions via semantic-aware masking. To facilitate efficient learning, we further introduce a Conditional Sensitivity-Aware Sampling (CSAS) strategy that reweights the training objective towards critical denoising phases, drastically accelerating convergence and enhancing conditional fidelity. Empirically, PKA delivers a 10.0$\times$ inference speedup and a 5.1$\times$ VRAM saving, providing a scalable and resource-friendly solution for high-fidelity multi-conditioned generation.

</details>


### [70] [Parameters as Experts: Adapting Vision Models with Dynamic Parameter Routing](https://arxiv.org/abs/2602.06862)
*Meng Lou,Stanley Yu,Yizhou Yu*

Main category: cs.CV

TL;DR: AdaRoute使用共享专家中心和动态参数路由生成输入相关的低秩权重，改进了PEFT在复杂密集视觉任务上的表现，兼顾参数效率与表现力。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT在复杂的密集预测任务中受限于输入不可知的建模和层间表示冗余，难以在少量可训练参数下达到与全量微调相当的性能。

Method: 提出AdaRoute模块：每个专家为可训练的参数矩阵，所有层共享同一专家中心；在前向时，通过动态参数路由按输入自适应地选择并聚合专家矩阵以生成当前模块的权重，从而实现低秩且输入相关的适配。

Result: 在语义分割、目标检测、实例分割和全景分割等多种视觉任务上，AdaRoute表现优于现有方法，展示了更好的性能和特征多样性（因共享专家中心促进跨层交互）。

Conclusion: AdaRoute通过在共享专家中心上动态路由并生成输入相关的权重矩阵，实现了在密集视觉任务上比现有PEFT方法更高效的参数利用与更强的表示能力。

Abstract: Adapting pre-trained vision models using parameter-efficient fine-tuning (PEFT) remains challenging, as it aims to achieve performance comparable to full fine-tuning using a minimal number of trainable parameters. When applied to complex dense prediction tasks, existing methods exhibit limitations, including input-agnostic modeling and redundant cross-layer representations. To this end, we propose AdaRoute, a new adapter-style method featuring a simple mixture-of-experts (MoE) architecture. Specifically, we introduce shared expert centers, where each expert is a trainable parameter matrix. During a feedforward pass, each AdaRoute module in the network dynamically generates weight matrices tailored for the current module via a simple dynamic parameter routing mechanism, which selectively aggregates parameter matrices in the corresponding expert center. Dynamic weight matrices in AdaRoute modules facilitate low-rank adaptation in an input-dependent manner, thus generating more customized and powerful feature representations. Moreover, since AdaRoute modules across multiple network layers share the same expert center, they improve feature diversity by promoting implicit cross-layer feature interaction. Extensive experiments demonstrate the superiority of AdaRoute on diverse vision tasks, including semantic segmentation, object detection and instance segmentation, and panoptic segmentation. Code will be available at: https://bit.ly/3NZcr0H.

</details>


### [71] [RFDM: Residual Flow Diffusion Model for Efficient Causal Video Editing](https://arxiv.org/abs/2602.06871)
*Mohammadreza Salehi,Mehdi Noroozi,Luca Morreale,Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Ramos,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: 提出RFDM：一种面向视频编辑的自回归残差流扩散模型，通过预测帧间残差实现高效逐帧编辑，在新基准上超过I2I方法并接近3D模型性能，计算成本低且支持可变长度视频。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的教学视频编辑多依赖固定长度输入或昂贵的3D时空模型；而自回归可变长度视频生成效率高但在编辑任务中未充分利用。作者旨在融合二者优点，提供既高效又能处理可变长度的编辑模型。

Method: 从预训练的2D I2I扩散模型出发，通过在时间步t将编辑条件建立在t-1的模型预测上，实现自回归V2V编辑；提出新的I2I扩散前向过程，使模型预测目标输出与前一帧预测之间的残差，从而关注帧间变化（即Residual Flow）；在训练时使用成对视频数据，任务包括全局/局部风格迁移与物体移除。

Result: 在新提出的评测基准上，RFDM优于基于I2I的方法，并在性能上与全时空（3D）V2V模型竞争，同时保持与图像模型相当的计算开销和与视频长度无关的可扩展性。

Conclusion: 本文提出了Residual Flow Diffusion Model（RFDM），一种基于自回归框架的高效可变长度视频编辑方法，能逐帧编辑视频并仅以图像到图像扩散模型为基础，计算量与图像模型相当且与视频长度无关。

Abstract: Instructional video editing applies edits to an input video using only text prompts, enabling intuitive natural-language control. Despite rapid progress, most methods still require fixed-length inputs and substantial compute. Meanwhile, autoregressive video generation enables efficient variable-length synthesis, yet remains under-explored for video editing. We introduce a causal, efficient video editing model that edits variable-length videos frame by frame. For efficiency, we start from a 2D image-to-image (I2I) diffusion model and adapt it to video-to-video (V2V) editing by conditioning the edit at time step t on the model's prediction at t-1. To leverage videos' temporal redundancy, we propose a new I2I diffusion forward process formulation that encourages the model to predict the residual between the target output and the previous prediction. We call this Residual Flow Diffusion Model (RFDM), which focuses the denoising process on changes between consecutive frames. Moreover, we propose a new benchmark that better ranks state-of-the-art methods for editing tasks. Trained on paired video data for global/local style transfer and object removal, RFDM surpasses I2I-based methods and competes with fully spatiotemporal (3D) V2V models, while matching the compute of image models and scaling independently of input video length. More content can be found in: https://smsd75.github.io/RFDM_page/

</details>


### [72] [NanoFLUX: Distillation-Driven Compression of Large Text-to-Image Generation Models for Mobile Devices](https://arxiv.org/abs/2602.06879)
*Ruchika Chavhan,Malcolm Chadwick,Alberto Gil Couto Pimentel Ramos,Luca Morreale,Mehdi Noroozi,Abhinav Mehrotra*

Main category: cs.CV

TL;DR: NanoFLUX 将 17B FLUX.1-Schnell 蒸馏为 2.4B 的流匹配模型，采用剪枝、ResNet token 下采样与基于去噪器视觉信号的文本编码器蒸馏，实现在手机上约 2.5s 生成 512×512 的高质量图像。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本到图像扩散模型虽质量高，但参数量大、计算资源要求高，难以在移动设备上部署。目标是缩小性能-效率差距，实现高质量的本地化文本到图像生成。

Method: 采用渐进压缩管线：对 diffusion transformer 进行结构性剪枝（将 12B 缩减至 2B）；引入基于 ResNet 的 token 下采样机制，让中间块在低分辨率 token 上运行以降低延迟，同时保留部分高分辨率处理；提出一种新的文本编码器蒸馏方法，利用去噪器早期层的视觉信号在采样时引导文本嵌入学习。整体采用从 17B 模型到 2.4B 的流匹配蒸馏。

Result: 在移动设备上，NanoFLUX 能在约 2.5 秒内生成 512×512 图像，表明该方法在延迟和质量之间取得了良好折衷，适用于实用级别的 on-device 文生图应用。

Conclusion: NanoFLUX 成功在保持视觉质量的前提下，将 17B FLUX.1-Schnell 蒸馏并压缩为 2.4B 模型，证明了通过逐步压缩和协同蒸馏技术可以在移动设备上实现高质量 512×512 文生图生成。

Abstract: While large-scale text-to-image diffusion models continue to improve in visual quality, their increasing scale has widened the gap between state-of-the-art models and on-device solutions. To address this gap, we introduce NanoFLUX, a 2.4B text-to-image flow-matching model distilled from 17B FLUX.1-Schnell using a progressive compression pipeline designed to preserve generation quality. Our contributions include: (1) A model compression strategy driven by pruning redundant components in the diffusion transformer, reducing its size from 12B to 2B; (2) A ResNet-based token downsampling mechanism that reduces latency by allowing intermediate blocks to operate on lower-resolution tokens while preserving high-resolution processing elsewhere; (3) A novel text encoder distillation approach that leverages visual signals from early layers of the denoiser during sampling. Empirically, NanoFLUX generates 512 x 512 images in approximately 2.5 seconds on mobile devices, demonstrating the feasibility of high-quality on-device text-to-image generation.

</details>


### [73] [Prompt Reinjection: Alleviating Prompt Forgetting in Multimodal Diffusion Transformers](https://arxiv.org/abs/2602.06886)
*Yuxuan Yao,Yuxuan Chen,Hui Li,Kaihui Cheng,Qipeng Guo,Yuwei Sun,Zilong Dong,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: MMDiTs存在文本提示在深层被遗忘的问题，作者通过层级探测证实并提出无需训练的prompt reinjection，将早期提示重注入后层，显著提升指令遵循性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 在MMDiTs中，尽管文本和图像分支在去噪过程中双向交互，但观察到文本分支的提示语义在深层被弱化，导致指令遵循能力下降，需找到轻量且无需训练的修复策略。

Method: 通过在三种代表性MMDiTs（SD3、SD3.5、FLUX.1）上对文本分支各层表示进行语言属性探测（probing），验证提示语义随深度衰减；提出训练自由的方法“prompt reinjection”，将早期层的提示表示重新注入到后层以缓解遗忘。

Result: prompt reinjection在多个数据集/基准（GenEval、DPG、T2I-CompBench++）上带来一致提升：指令遵循能力增强，同时在偏好、审美和整体文本-图像生成质量等指标上均有改进。

Conclusion: 本文发现多模态扩散Transformer（MMDiTs）在文本分支存在提示遗忘（prompt forgetting）：随着层数增加，文本表示的语义信息被渐进遗忘。

Abstract: Multimodal Diffusion Transformers (MMDiTs) for text-to-image generation maintain separate text and image branches, with bidirectional information flow between text tokens and visual latents throughout denoising. In this setting, we observe a prompt forgetting phenomenon: the semantics of the prompt representation in the text branch is progressively forgotten as depth increases. We further verify this effect on three representative MMDiTs--SD3, SD3.5, and FLUX.1 by probing linguistic attributes of the representations over the layers in the text branch. Motivated by these findings, we introduce a training-free approach, prompt reinjection, which reinjects prompt representations from early layers into later layers to alleviate this forgetting. Experiments on GenEval, DPG, and T2I-CompBench++ show consistent gains in instruction-following capability, along with improvements on metrics capturing preference, aesthetics, and overall text--image generation quality.

</details>


### [74] [PANC: Prior-Aware Normalized Cut for Object Segmentation](https://arxiv.org/abs/2602.06912)
*Juan Gutiérrez,Victor Gutiérrez-Garcia,José Luis Blanco-Murillo*

Main category: cs.CV

TL;DR: PANC通过将少量标注Tokens与TokenCut谱图拓扑结合，无需训练即可显著提升分割的稳定性、可控性与性能，尤其在细粒度与纹理受限数据集上效果优异。


<details>
  <summary>Details</summary>
Motivation: 现有完全无监督分割方法倾向于寻找最显著对象，导致分割结果对初始化、种子顺序和阈值启发式敏感、不可复现。提出PANC以少量标注弥补这些弱点，实现可控且稳定的分割。

Method: 在TokenCut的token-token亲和图中加入基于注释的先验和锚节点，通过改变图的拓扑结构来偏置谱特征空间，使分割结果符合注释。方法为无训练需求，使用5–30个注释样本即可运行；保留自监督视觉特征的全局聚类能力，同时通过锚点普适化用户控制。

Result: 在DUTS-TE、ECSSD、MS COCO等基准上，使用少量注释达到了弱监督/无监督类别的最优表现；在细粒度或纹理受限域（CFD、CUB-200、HAM10000）表现尤为突出，分别达成96.8%、78.0%、78.8% mIoU，较SotA提升显著。并在多目标场景提供显式用户可控语义分割。

Conclusion: PANC通过结合少量标注视觉tokens与基于TokenCut的谱分割图拓扑操控，实现了稳定且可控的对象分割，在弱监督/无监督设置下显著提高了可重复性与质量。

Abstract: Fully unsupervised segmentation pipelines naively seek the most salient object, should this be present. As a result, most of the methods reported in the literature deliver non-deterministic partitions that are sensitive to initialization, seed order, and threshold heuristics.
  We propose PANC, a weakly supervised spectral segmentation framework that uses a minimal set of annotated visual tokens to produce stable, controllable, and reproducible object masks. From the TokenCut approach, we augment the token-token affinity graph with a handful of priors coupled to anchor nodes. By manipulating the graph topology, we bias the spectral eigenspace toward partitions that are consistent with the annotations. Our approach preserves the global grouping enforced by dense self-supervised visual features, trading annotated tokens for significant gains in reproducibility, user control, and segmentation quality.
  Using 5 to 30 annotations per dataset, our training-free method achieves state-of-the-art performance among weakly and unsupervised approaches on standard benchmarks (e.g., DUTS-TE, ECSSD, MS COCO). Contrarily, it excels in domains where dense labels are costly or intra-class differences are subtle. We report strong and reliable results on homogeneous, fine-grained, and texture-limited domains, achieving 96.8% (+14.43% over SotA), 78.0% (+0.2%), and 78.8% (+0.37%) average mean intersection-over-union (mIoU) on CrackForest (CFD), CUB-200-2011, and HAM10000 datasets, respectively. For multi-object benchmarks, the framework showcases explicit, user-controllable semantic segmentation.

</details>


### [75] [Seeing Beyond Redundancy: Task Complexity's Role in Vision Token Specialization in VLLMs](https://arxiv.org/abs/2602.06914)
*Darryl Hannan,John Cooper,Dylan White,Yijing Watkins*

Main category: cs.CV

TL;DR: 通过合成基准和冗余度量，论文表明视觉信息在VLLMs中被压缩/冗余化导致细节丢失，但加入更多高复杂度视觉训练数据能减少压缩、改善性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLMs的视觉能力落后于语言能力，尤其在需要细粒度信息或空间推理的任务上失败。需要理解为什么会丢失这些视觉细节，以及如何通过训练策略和数据构成改善模型的视觉表征分配。

Method: 作者构建了一个合成基准数据集用于探测不同视觉特征，提出衡量视觉冗余的度量，并在多种复杂视觉任务上对VLLMs进行微调以观察冗余与压缩的变化。

Result: 发现任务复杂度与视觉压缩存在联系：训练中包含足够比例的高复杂度视觉数据可以改变VLLM对视觉信息的分配，降低对细节的压缩，从而提升复杂视觉任务的表现。

Conclusion: VLLMs在细粒度视觉信息和空间推理任务上表现不佳，部分原因是视觉表征被压缩/冗余化，导致关键细节丢失；但任务复杂度能改变这种压缩分配，增加高复杂度视觉数据有助于保留更多细节并改善性能。

Abstract: Vision capabilities in vision large language models (VLLMs) have consistently lagged behind their linguistic capabilities. In particular, numerous benchmark studies have demonstrated that VLLMs struggle when fine-grained visual information or spatial reasoning is required. However, we do not yet understand exactly why VLLMs struggle so much with these tasks relative to others. Some works have focused on visual redundancy as an explanation, where high-level visual information is uniformly spread across numerous tokens and specific, fine-grained visual information is discarded. In this work, we investigate this premise in greater detail, seeking to better understand exactly how various types of visual information are processed by the model and what types of visual information are discarded. To do so, we introduce a simple synthetic benchmark dataset that is specifically constructed to probe various visual features, along with a set of metrics for measuring visual redundancy, allowing us to better understand the nuances of their relationship. Then, we explore fine-tuning VLLMs on a number of complex visual tasks to better understand how redundancy and compression change based upon the complexity of the data that a model is trained on. We find that there is a connection between task complexity and visual compression, implying that having a sufficient ratio of high complexity visual data is crucial for altering the way that VLLMs distribute their visual representation and consequently improving their performance on complex visual tasks. We hope that this work will provide valuable insights for training the next generation of VLLMs.

</details>


### [76] [Reliable Mislabel Detection for Video Capsule Endoscopy Data](https://arxiv.org/abs/2602.06938)
*Julia Werner,Julius Oexle,Oliver Bause,Maxime Le Floch,Franz Brinkmann,Hannah Tolle,Jochen Hampe,Oliver Bringmann*

Main category: cs.CV

TL;DR: 提出一个针对VCE医学影像的错标检测与数据清洗框架，专家复核验证，清洗后异常检测性能优于基线。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，获取大量准确标注困难且昂贵，医生注释资源有限，且类别边界常有模糊，导致训练数据存在噪声标签；为提高深度网络分类性能，需要有效检测并修正错标。

Method: 作者构建了一个错标检测管线（可能包括基于模型的异常检测、样本不确定性评估或一致性检验等方法），在两个最大的公开VCE数据集上进行验证；对被识别为可能错标的样本，邀请三位资深胃肠病学专家重新注释以确认标签错误。

Result: 实验显示所提框架能成功检测出错误标注样本；在清洗数据集后，异常检测指标（如准确率/召回/ROC-AUC等）相较现有基线方法有明显提升；专家复核证实了部分自动识别出的错标。

Conclusion: 该论文提出并验证了一个用于医学影像数据集错标注检测的框架，能够发现并纠正内窥胶囊视频（Video Capsule Endoscopy，VCE）数据集中的标签错误，清洗后可提升异常检测性能。

Abstract: The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.

</details>


### [77] [CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation](https://arxiv.org/abs/2602.06959)
*Kaiyi Huang,Yukun Huang,Yu Li,Jianhong Bai,Xintao Wang,Zinan Lin,Xuefei Ning,Jiwen Yu,Pengfei Wan,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: CineScene用隐式3D感知的上下文条件机制把场景先验注入文本到视频模型，并借助合成数据与随机打乱策略，实现按相机轨迹生成场景一致且主体动态的电影级视频。


<details>
  <summary>Details</summary>
Motivation: 动机是降低实拍成本，通过合成方式在保持场景一致性的同时控制构图与相机运动，从而实现电影级画面效果而无需搭建实体场景。

Method: 方法包括：1) 使用VGGT将多张静态场景图像编码为隐式3D感知特征，并通过上下文串联（context concatenation）注入到预训练文本到视频生成模型中；2) 在训练阶段对输入场景图像进行随机打乱以提高鲁棒性；3) 利用用Unreal Engine 5合成的场景可解耦数据集（含有无主体/有主体的视频对、全景场景图像及相机轨迹）进行训练。

Result: 实验表明CineScene在场景一致性的电影级视频生成任务上优于先前方法，能够处理大幅相机移动并在不同环境间具有良好泛化能力。

Conclusion: 本文提出的CineScene通过将场景图像编码为3D感知的视觉表示并作为上下文并入预训练文本到视频模型，实现了在保持静态场景一致性的前提下，按用户指定相机轨迹生成动态主体的高质量电影级视频。

Abstract: Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model's robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.

</details>


### [78] [MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images](https://arxiv.org/abs/2602.06965)
*Ankan Deria,Komal Kumar,Adinath Madhavrao Dukre,Eran Segal,Salman Khan,Imran Razzak*

Main category: cs.CV

TL;DR: MedMO通过三阶段训练与可验证奖励显著提升医学MLLM的跨模态对齐、任务性能与空间定位能力，在多个医学任务与模态上大幅优于现有开源基线。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在医学领域受限于领域覆盖不足、模态对齐不佳和缺乏有根据的推理，导致临床应用推广受阻，因而需要一个专门在大规模医学数据上训练并强化空间与事实能力的模型。

Method: 采用三阶段训练：1) 跨模态预训练以对齐视觉编码器与医学语言主体；2) 基于多任务指令调优涵盖图像描述、VQA、报告生成、检索与基于框的病灶定位；3) 结合事实性检验与框级GIoU奖励的强化学习以增强空间定位与推理能力。

Result: MedMO在多项基准上显著领先：VQA平均提升+13.7%，接近SOTA Fleming-VL（仅差1.9%）；文本问答提升+6.9%（相较Fleming-VL +14.5%）；报告生成在语义与临床准确性均有提升；定位任务IoU提升+40.4%（相较Fleming-VL +37.0%）；在放射学、眼科与病理显微镜等多模态数据上表现稳健。

Conclusion: MedMO是一种针对医疗领域的多模态大语言模型，通过分阶段训练与可验证奖励机制提高了跨模态对齐、任务指导与空间定位能力，显著超越现有开源医学MLLM基线，在VQA、文本问答、报告生成与定位任务上均有明显提升。

Abstract: Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO's broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [79] [Machine Learning Practitioners' Views on Data Quality in Light of EU Regulatory Requirements: A European Online Survey](https://arxiv.org/abs/2602.06594)
*Yichun Wang,Kristina Irion,Paul Groth,Hazar Harmouch*

Main category: cs.DB

TL;DR: 本文提出对齐数据质量与欧盟监管要求的框架，并基于180+名欧盟数据从业者的调查揭示实践中工具、协作与知识缺口，提出加强工具集成和跨专业协作的建议。


<details>
  <summary>Details</summary>
Motivation: 在不断演进的欧盟监管环境中，帮助实践者理解并对齐数据质量与法规要求，以降低合规风险并提升ML系统的可靠性与可信度。

Method: 提出了一种将既有数据质量维度与具体欧盟监管要求对齐的实用框架；并通过对180余名欧盟数据从业者的线上问卷调查，收集其实践方法、挑战与未满足需求。

Result: 发现目前实践与监管期望之间存在关键差距：缺乏与法规对齐的集成数据质量工具；技术团队与法律/合规团队之间协作不足；从业者对实现合规的数据质量控制方法存在知识与资源上的未满足需求。

Conclusion: 研究总结指出当前数据质量实践与欧盟法规要求存在显著脱节，尤其在工具集成与技术-法律协作方面。论文建议加强工具适配、跨专业沟通与法规可解释性以促成合规且可信的ML部署。

Abstract: Understanding how data quality aligns with regulatory requirements in machine learning (ML) systems presents a critical challenge for practitioners navigating the evolving EU regulatory landscape. To address this, we first propose a practical framework aligning established data quality dimensions with specific EU regulatory requirements. Second, we conducted a comprehensive online survey with over 180 EU-based data practitioners, investigating their approaches, key challenges, and unmet needs when ensuring data quality in ML systems that align with regulatory requirements. Our findings highlight crucial gaps between current practices and regulatory expectations, underscoring practitioners' need for more integrated data quality tools and better collaboration between technical and legal practitioners. These insights inform recommendations for bridging technical expertise and regulatory compliance, ultimately fostering responsible and trustworthy ML deployments.

</details>


### [80] [Filtered Approximate Nearest Neighbor Search Cost Estimation](https://arxiv.org/abs/2602.06721)
*Wenxuan Xia,Mingyu Yang,Wentao Li,Wei Wang*

Main category: cs.DB

TL;DR: 本文针对带过滤条件的近似k近邻搜索，提出E2E代价估计框架，显式建模向量与属性选择性的相关性，用于早停优化，实现2x-3x效率提升且不牺牲准确率。


<details>
  <summary>Details</summary>
Motivation: 当前过滤AKNN查询的成本高度可变，受向量相似度和结构化属性过滤共同影响，现有估计方法未能捕捉两者之间的相关性，导致优化效果受限。

Method: 提出一个端到端（E2E）代价估计框架，显式建模查询向量分布与属性-值选择性之间的相关性，利用估计结果优化搜索终止条件。

Result: 在真实数据集上，利用E2E的早停策略相比最先进基线在保持高准确率的同时，将检索效率提高了2x-3x。

Conclusion: E2E能显著提高过滤AKNN搜索的代价估计准确性，从而优化早停等下游策略，提升检索效率。

Abstract: Hybrid queries combining high-dimensional vector similarity with structured attribute filtering have garnered significant attention across both academia and industry. A critical instance of this paradigm is filtered Approximate k Nearest Neighbor (AKNN) search, where embeddings (e.g., image or text) are queried alongside constraints such as labels or numerical range. While essential for rich retrieval, optimizing these queries remains challenging due to the highly variable search cost induced by combined filters. In this paper, we propose a novel cost estimation framework, E2E, for filtered AKNN search and demonstrate its utility in downstream optimization tasks, specifically early termination. Unlike existing approaches, our model explicitly captures the correlation between the query vector distribution and attribute-value selectivity, yielding significantly higher estimation accuracy. By leveraging these estimates to refine search termination conditions, we achieve substantial performance gains. Experimental results on real-world datasets demonstrate that our approach improves retrieval efficiency by 2x-3x over state-of-the-art baselines while maintaining high search accuracy.

</details>
