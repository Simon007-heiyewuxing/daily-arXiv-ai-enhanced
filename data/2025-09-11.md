<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [3D and 4D World Modeling: A Survey](https://arxiv.org/abs/2509.07996)
*Lingdong Kong,Wesley Yang,Jianbiao Mei,Youquan Liu,Ao Liang,Dekai Zhu,Dongyue Lu,Wei Yin,Xiaotao Hu,Mingkai Jia,Junyuan Deng,Kaiwen Zhang,Yang Wu,Tianyi Yan,Shenyuan Gao,Song Wang,Linfeng Li,Liang Pan,Yong Liu,Jianke Zhu,Wei Tsang Ooi,Steven C. H. Hoi,Ziwei Liu*

Main category: cs.CV

TL;DR: 本综述首次系统整理3D/4D世界建模领域，给出定义、分类（VideoGen/OccGen/LiDARGen）、数据集、指标、应用与未来挑战，提供在线文献汇总。


<details>
  <summary>Details</summary>
Motivation: 现有工作多集中于2D图像/视频生成，忽视使用原生3D/4D表示的大规模场景建模，且缺乏标准化定义与分类。

Method: 通过文献综述，构建了包括VideoGen、OccGen、LiDARGen在内的三级分类法，整理了相关数据集与评估指标，并汇总了实际应用与挑战。

Result: 提供了首个专注于3D/4D世界建模的全面综述，附带系统化的论文清单与在线资源，明确研究空白并提出未来方向。

Conclusion: 本文提出并系统化了3D/4D世界建模领域的定义与分类，强调了现有研究的碎片化问题并给出统一框架。

Abstract: World modeling has become a cornerstone in AI research, enabling agents to
understand, represent, and predict the dynamic environments they inhabit. While
prior work largely emphasizes generative methods for 2D image and video data,
they overlook the rapidly growing body of work that leverages native 3D and 4D
representations such as RGB-D imagery, occupancy grids, and LiDAR point clouds
for large-scale scene modeling. At the same time, the absence of a standardized
definition and taxonomy for ``world models'' has led to fragmented and
sometimes inconsistent claims in the literature. This survey addresses these
gaps by presenting the first comprehensive review explicitly dedicated to 3D
and 4D world modeling and generation. We establish precise definitions,
introduce a structured taxonomy spanning video-based (VideoGen),
occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and
systematically summarize datasets and evaluation metrics tailored to 3D/4D
settings. We further discuss practical applications, identify open challenges,
and highlight promising research directions, aiming to provide a coherent and
foundational reference for advancing the field. A systematic summary of
existing literature is available at https://github.com/worldbench/survey

</details>


### [2] [An Explainable Deep Neural Network with Frequency-Aware Channel and Spatial Refinement for Flood Prediction in Sustainable Cities](https://arxiv.org/abs/2509.08003)
*Shahid Shafi Dar,Bharat Kaurav,Arnav Jain,Chandravardhan Singh Raghaw,Mohammad Zia Ur Rehman,Nagendra Kumar*

Main category: cs.CV

TL;DR: XFloodNet通过层级跨模态门控注意力、异质卷积自适应多尺度注意力与级联卷积Transformer特征精炼三项创新模块，实现视觉-文本动态对齐、多尺度频域空间特征优先级提取及鲁棒的层级融合，在三个洪水数据集上达到或超越最先进水平的F1性能。


<details>
  <summary>Details</summary>
Motivation: 现有洪水检测方法依赖单一模态或静态规则、注意力机制与集成学习在层级精炼、跨模态融合及对噪声/非结构化环境的适应性方面存在不足，导致分类性能受限。为此提出XFloodNet以克服这些局限。

Method: 方法包括：1) 层级跨模态门控注意力（Hierarchical Cross-Modal Gated Attention），用于动态对齐图像与文本特征并实现多粒度交互；2) 异质卷积自适应多尺度注意力（Heterogeneous Convolutional Adaptive Multi-Scale Attention），结合频域增强的通道注意力与频率调制的空间注意力以提取判别性洪水特征；3) 级联卷积Transformer特征精炼（Cascading Convolutional Transformer Feature Refinement），通过自适应缩放与级联操作融合不同层次特征，增强抗噪性与鲁棒性。

Result: 在三个基准数据集（Chennai Floods、Rhine18 Floods、Harz17 Floods）上，XFloodNet分别取得F1得分93.33%、82.24%与88.60%，明显优于现有方法。

Conclusion: XFloodNet提出了一种面向城市洪水分类的深度学习框架，通过多模态注意力、异质卷积自适应多尺度注意力与级联卷积Transformer特征精炼三大模块，实现对视觉与文本特征的动态对齐、多尺度频域与空间域特征提取及噪声鲁棒的层级特征融合，从而显著提升洪水分类性能。

Abstract: In an era of escalating climate change, urban flooding has emerged as a
critical challenge for sustainable cities, threatening lives, infrastructure,
and ecosystems. Traditional flood detection methods are constrained by their
reliance on unimodal data and static rule-based systems, which fail to capture
the dynamic, non-linear relationships inherent in flood events. Furthermore,
existing attention mechanisms and ensemble learning approaches exhibit
limitations in hierarchical refinement, cross-modal feature integration, and
adaptability to noisy or unstructured environments, resulting in suboptimal
flood classification performance. To address these challenges, we present
XFloodNet, a novel framework that redefines urban flood classification through
advanced deep-learning techniques. XFloodNet integrates three novel components:
(1) a Hierarchical Cross-Modal Gated Attention mechanism that dynamically
aligns visual and textual features, enabling precise multi-granularity
interactions and resolving contextual ambiguities; (2) a Heterogeneous
Convolutional Adaptive Multi-Scale Attention module, which leverages
frequency-enhanced channel attention and frequency-modulated spatial attention
to extract and prioritize discriminative flood-related features across spectral
and spatial domains; and (3) a Cascading Convolutional Transformer Feature
Refinement technique that harmonizes hierarchical features through adaptive
scaling and cascading operations, ensuring robust and noise-resistant flood
detection. We evaluate our proposed method on three benchmark datasets, such as
Chennai Floods, Rhine18 Floods, and Harz17 Floods, XFloodNet achieves
state-of-the-art F1-scores of 93.33%, 82.24%, and 88.60%, respectively,
surpassing existing methods by significant margins.

</details>


### [3] [Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs](https://arxiv.org/abs/2509.08016)
*Hyungjin Chung,Hyelin Nam,Jiyeon Kim,Hyojun Go,Byeongjun Park,Junho Kim,Joonseok Lee,Seongsu Ha,Byung-Hoon Kim*

Main category: cs.CV

TL;DR: VPS通过并行处理互斥帧子集并聚合概率，在不扩大上下文窗口下高效提升VideoLLM的时间理解能力，跨模型与基准表现稳定优越。


<details>
  <summary>Details</summary>
Motivation: 增加输入帧以捕获细粒度时间信息会导致计算成本激增和长上下文长度带来的性能下降，故需在不扩展上下文窗口下提升时间推理能力。

Method: 在推理时运行多个并行流，每个流处理视频的互斥帧子集；对各流输出的概率分布进行聚合以得到更丰富的视觉证据。理论上通过利用不相关视觉证据收缩Chinchilla标度律；无需额外训练，兼容不同模型架构与尺度（2B-32B）。

Result: 在Video-MME和EventHallusion等基准上，VPS在各类模型（2B-32B）上持续且显著地提升了性能；比其他并行替代方案（如Self-consistency）具有更优扩展性，并能与其他解码策略配合，内存高效且鲁棒。

Conclusion: VPS在不增加模型上下文窗口的情况下，通过并行不重叠的视频帧子集推理并聚合概率分布，有效扩展了VideoLLM的感知带宽，从而在推理阶段提升细粒度时间信息捕获能力和推理性能。

Abstract: Video Large Language Models (VideoLLMs) face a critical bottleneck:
increasing the number of input frames to capture fine-grained temporal detail
leads to prohibitive computational costs and performance degradation from long
context lengths. We introduce Video Parallel Scaling (VPS), an inference-time
method that expands a model's perceptual bandwidth without increasing its
context window. VPS operates by running multiple parallel inference streams,
each processing a unique, disjoint subset of the video's frames. By aggregating
the output probabilities from these complementary streams, VPS integrates a
richer set of visual information than is possible with a single pass. We
theoretically show that this approach effectively contracts the Chinchilla
scaling law by leveraging uncorrelated visual evidence, thereby improving
performance without additional training. Extensive experiments across various
model architectures and scales (2B-32B) on benchmarks such as Video-MME and
EventHallusion demonstrate that VPS consistently and significantly improves
performance. It scales more favorably than other parallel alternatives (e.g.
Self-consistency) and is complementary to other decoding strategies, offering a
memory-efficient and robust framework for enhancing the temporal reasoning
capabilities of VideoLLMs.

</details>


### [4] [Two Stage Context Learning with Large Language Models for Multimodal Stance Detection on Climate Change](https://arxiv.org/abs/2509.08024)
*Lata Pangtey,Omkar Kabde,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

TL;DR: 本文提出将LLM摘要与领域感知图像描述通过分层Transformer融合的多模态立场检测方法，在MultiClimate数据集上显著超越现有方法（F1=76.2%）。


<details>
  <summary>Details</summary>
Motivation: 现实社交媒体帖子通常同时包含文本与视觉元素，单纯基于文本的方法无法充分利用视觉信号，因此需要一种能在语义层面融合多模态信息以提升立场检测鲁棒性的模型。

Method: 方法包括：1) 使用大型语言模型（LLM）从来源文本中检索与立场相关的摘要；2) 使用领域感知的图像描述生成器，将视觉信息转化为与主题相关的文本；3) 将回复文本、LLM摘要和图像描述输入专门设计的Transformer模块，采用分层融合策略捕捉文本-图像交互；4) 在MultiClimate数据集上训练与评估，采用准确率、精确率、召回率与F1为指标。

Result: 在MultiClimate基准上取得了准确率76.2%、精确率76.3%、召回率76.2%和F1-score76.2%，优于现有最先进方法。

Conclusion: 本文提出的多模态分层融合方法在气候变化立场检测任务上表现优异，证明融合文本和图像的互补信息能提升分类性能。

Abstract: With the rapid proliferation of information across digital platforms, stance
detection has emerged as a pivotal challenge in social media analysis. While
most of the existing approaches focus solely on textual data, real-world social
media content increasingly combines text with visual elements creating a need
for advanced multimodal methods. To address this gap, we propose a multimodal
stance detection framework that integrates textual and visual information
through a hierarchical fusion approach. Our method first employs a Large
Language Model to retrieve stance-relevant summaries from source text, while a
domain-aware image caption generator interprets visual content in the context
of the target topic. These modalities are then jointly modeled along with the
reply text, through a specialized transformer module that captures interactions
between the texts and images. The proposed modality fusion framework integrates
diverse modalities to facilitate robust stance classification. We evaluate our
approach on the MultiClimate dataset, a benchmark for climate change-related
stance detection containing aligned video frames and transcripts. We achieve
accuracy of 76.2%, precision of 76.3%, recall of 76.2% and F1-score of 76.2%,
respectively, outperforming existing state-of-the-art approaches.

</details>


### [5] [Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles](https://arxiv.org/abs/2509.08026)
*Zeinab Ghasemi Darehnaei,Mohammad Shokouhifar,Hossein Yazdanjouei,S. M. J. Rastegar Fatemi*

Main category: cs.CV

TL;DR: 提出基于三种Faster R-CNN特征提取器与五种迁移分类器组成的15基学习器的两阶段群体智能集成迁移学习（SI-EDTL），并用鲸鱼优化算法调参，在AU-AIR数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无人机图像场景复杂、尺度和视角变化大，单一模型难以兼顾多类车辆的检测性能，故采用集成与迁移学习结合以增强鲁棒性和泛化能力，并用群体智能优化超参数。

Method: 使用三个预训练 Faster R-CNN 特征提取器（InceptionV3、ResNet50、GoogLeNet）与五种迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯）构成15个基学习器，通过加权平均融合输出；采用鲸鱼优化算法调整超参数以在准确率、精确率和召回率间取得平衡；在 MATLAB R2020b 下并行实现。

Result: 在 AU-AIR 数据集上，SI-EDTL 在准确率、精确率和召回率上均超过现有方法，展示出更稳定和更高的多类车辆检测性能。

Conclusion: SI-EDTL 提出了一种两阶段群体智能集成深度迁移学习框架，显著提高了无人机图像中多类车辆检测的性能。

Abstract: This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deep
transfer learning model for detecting multiple vehicles in UAV images. It
combines three pre-trained Faster R-CNN feature extractor models (InceptionV3,
ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,
Na\"ive Bayes), resulting in 15 different base learners. These are aggregated
via weighted averaging to classify regions as Car, Van, Truck, Bus, or
background. Hyperparameters are optimized with the whale optimization algorithm
to balance accuracy, precision, and recall. Implemented in MATLAB R2020b with
parallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAV
dataset.

</details>


### [6] [MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery](https://arxiv.org/abs/2509.08027)
*Rafał Osadnik,Pablo Gómez,Eleni Bohacek,Rickbir Bahia*

Main category: cs.CV

TL;DR: MCTED：一个包含80,898个CTX影像-DEM-掩码样本的火星DEM预测数据集，带完整预处理管线与训练/验证划分，实验证明针对性训练能超越通用零样本基线。


<details>
  <summary>Details</summary>
Motivation: 现有大尺度火星DEM常含伪影和缺失点，缺乏面向机器学习的标准化、高质量训练数据集；因此需要一个经预处理、带掩码标注并可直接用于深度学习的火星DEM预测数据集。

Method: 从Mars Reconnaissance Orbiter的CTX影像与现有DEM对中，设计了完整的数据处理管线，包括去伪影、填充缺失数据、生成影像/DEM/掩码三元样本；划分训练/验证集时避免空间重叠；并训练小型U-Net与DepthAnythingV2对比。

Result: 生成约80,898个样本，提供统计分析（空间分布、高程与坡度分布等），并证明在定制训练下小型U-Net优于DepthAnythingV2的零样本深度估计性能；数据集与代码开放。

Conclusion: 作者构建了一个用于火星数字高程模型（DEM）预测的公开数据集MCTED，并展示其有效性。

Abstract: This work presents a new dataset for the Martian digital elevation model
prediction task, ready for machine learning applications called MCTED. The
dataset has been generated using a comprehensive pipeline designed to process
high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a
dataset consisting of 80,898 data samples. The source images are data gathered
by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very
diverse and comprehensive coverage of the Martian surface. Given the complexity
of the processing pipelines used in large-scale DEMs, there are often artefacts
and missing data points in the original data, for which we developed tools to
solve or mitigate their impact. We divide the processed samples into training
and validation splits, ensuring samples in both splits cover no mutual areas to
avoid data leakage. Every sample in the dataset is represented by the optical
image patch, DEM patch, and two mask patches, indicating values that were
originally missing or were altered by us. This allows future users of the
dataset to handle altered elevation regions as they please. We provide
statistical insights of the generated dataset, including the spatial
distribution of samples, the distributions of elevation values, slopes and
more. Finally, we train a small U-Net architecture on the MCTED dataset and
compare its performance to a monocular depth estimation foundation model,
DepthAnythingV2, on the task of elevation prediction. We find that even a very
small architecture trained on this dataset specifically, beats a zero-shot
performance of a depth estimation foundation model like DepthAnythingV2. We
make the dataset and code used for its generation completely open source in
public repositories.

</details>


### [7] [APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction](https://arxiv.org/abs/2509.08104)
*Sasan Sharifipour,Constantino Álvarez Casado,Mohammad Sabokrou,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 提出APML：可微的概率性一对一匹配近似，自动确定温度，近二次复杂度，改善点云预测的覆盖性与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有点云损失（CD、HyperCD、InfoCD）因最近邻导致多对一匹配、点聚集与稀疏区覆盖不足，且索引选择带来不可微问题；EMD理想但复杂度太高。

Method: 基于两两距离构建相似度矩阵并进行温度缩放，使用Sinkhorn迭代得到概率性一对一匹配近似；解析推导温度以保证最小分配概率，避免手工调参。

Result: 在ShapeNet和基于WiFi CSI的3D人体点云生成任务中，将APML整合到PoinTr、PCN、FoldingNet和CSI2PC，带来更快收敛、更优空间分布（尤其稀疏区），以及等同或更好的定量性能，无需额外超参搜索。

Conclusion: APML在保持接近EMD一对一匹配特性的同时，具有接近平方复杂度、可微并自动计算温度参数，提升点云预测模型在密度不均区域的覆盖与收敛速度。

Abstract: Training deep learning models for point cloud prediction tasks such as shape
completion and generation depends critically on loss functions that measure
discrepancies between predicted and ground-truth point sets. Commonly used
functions such as Chamfer Distance (CD), HyperCD, and InfoCD rely on
nearest-neighbor assignments, which often induce many-to-one correspondences,
leading to point congestion in dense regions and poor coverage in sparse
regions. These losses also involve non-differentiable operations due to index
selection, which may affect gradient-based optimization. Earth Mover Distance
(EMD) enforces one-to-one correspondences and captures structural similarity
more effectively, but its cubic computational complexity limits its practical
use. We propose the Adaptive Probabilistic Matching Loss (APML), a fully
differentiable approximation of one-to-one matching that leverages Sinkhorn
iterations on a temperature-scaled similarity matrix derived from pairwise
distances. We analytically compute the temperature to guarantee a minimum
assignment probability, eliminating manual tuning. APML achieves near-quadratic
runtime, comparable to Chamfer-based losses, and avoids non-differentiable
operations. When integrated into state-of-the-art architectures (PoinTr, PCN,
FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)
that generates 3D human point clouds from WiFi CSI measurements, APM loss
yields faster convergence, superior spatial distribution, especially in
low-density regions, and improved or on-par quantitative performance without
additional hyperparameter search. The code is available at:
https://github.com/apm-loss/apml.

</details>


### [8] [Lightweight Deep Unfolding Networks with Enhanced Robustness for Infrared Small Target Detection](https://arxiv.org/abs/2509.08205)
*Jingjing Liu,Yinchao Han,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出L-RPCANet：通过层次瓶颈、噪声抑制模块与SENet注意力，实现更轻量且鲁棒的红外小目标检测，且优于若干现有RPCA系方法。


<details>
  <summary>Details</summary>
Motivation: 现有DUNs在可解释性和适应性虽好，但参数繁重且对复杂噪声鲁棒性不足，需设计更轻量且鲁棒的模型。

Method: 构建基于RPCA的深度展开网络，采用层次化瓶颈结构在单通道红外图像上先降维再升维以实现通道细化；在每个模块内设计瓶颈层用于特征提取；嵌入噪声抑制模块并引入SENet通道注意力以增强不同通道特征的重要性建模。

Result: 在多组ISTD数据集上进行大量实验，结果显示L-RPCANet在检测性能、参数量以及对噪声的鲁棒性方面均超过RPCANet、DRPCANet和RPCANet++等最先进方法。

Conclusion: 提出了轻量且鲁棒的基于RPCA的红外小目标检测模型L-RPCANet，在参数数量和噪声鲁棒性上优于现有方法。

Abstract: Infrared small target detection (ISTD) is one of the key techniques in image
processing. Although deep unfolding networks (DUNs) have demonstrated promising
performance in ISTD due to their model interpretability and data adaptability,
existing methods still face significant challenges in parameter lightweightness
and noise robustness. In this regard, we propose a highly lightweight framework
based on robust principal component analysis (RPCA) called L-RPCANet.
Technically, a hierarchical bottleneck structure is constructed to reduce and
increase the channel dimension in the single-channel input infrared image to
achieve channel-wise feature refinement, with bottleneck layers designed in
each module to extract features. This reduces the number of channels in feature
extraction and improves the lightweightness of network parameters. Furthermore,
a noise reduction module is embedded to enhance the robustness against complex
noise. In addition, squeeze-and-excitation networks (SENets) are leveraged as a
channel attention mechanism to focus on the varying importance of different
features across channels, thereby achieving excellent performance while
maintaining both lightweightness and robustness. Extensive experiments on the
ISTD datasets validate the superiority of our proposed method compared with
state-of-the-art methods covering RPCANet, DRPCANet, and RPCANet++. The code
will be available at https://github.com/xianchaoxiu/L-RPCANet.

</details>


### [9] [Sparse Transformer for Ultra-sparse Sampled Video Compressive Sensing](https://arxiv.org/abs/2509.08228)
*Miao Cao,Siming Zheng,Lishun Wang,Ziyang Chen,David Brady,Xin Yuan*

Main category: cs.CV

TL;DR: 引入Ultra-Sparse Sampling（每像素仅在一个子帧采样）并设计稀疏Transformer（BSTFormer）处理实际DMD-CCD测量，能在模拟与真实场景下显著提升SCI视频恢复性能、降低能耗并提高动态范围，适合片上实现高帧率低功耗成像。


<details>
  <summary>Details</summary>
Motivation: 现有视频采集在高分辨率高帧率下能耗不可持续；物理层压缩测量（如SCI）能降低每像素功耗，受启发于图像修补的低采样恢复能力，提出更稀疏的采样策略以进一步降低功耗并保持可恢复性。

Method: 构建DMD编码系统采集USS测量；将理论上可分解的USS测量近似为子测量并尝试用图像修补算法恢复；为处理无法完全分解的实际测量，设计BSTFormer——包含Block局部注意力、Sparse全局注意力和Temporal全局注意力的稀疏Transformer架构。

Result: 在模拟与真实DMD-CCD采集数据上，USS + BSTFormer在恢复质量（视觉与定量指标）上显著优于随机采样(RS)及现有视频SCI算法；USS策略同时带来更高动态范围且便于片上实现固定曝光时间设计。

Conclusion: 提出的Ultra-Sparse Sampling (USS) 通过对每个空间位置只在一个子帧采样，能显著降低每像素采样率并提高动态范围，适合片上实现高帧率SCI系统。BSTFormer利用分块局部注意力、全局稀疏注意力和时间注意力，有效应对USS测量的稀疏性与DMD-CCD不匹配，实验在模拟与真实数据上优于现有方法。

Abstract: Digital cameras consume ~0.1 microjoule per pixel to capture and encode
video, resulting in a power usage of ~20W for a 4K sensor operating at 30 fps.
Imagining gigapixel cameras operating at 100-1000 fps, the current processing
model is unsustainable. To address this, physical layer compressive measurement
has been proposed to reduce power consumption per pixel by 10-100X. Video
Snapshot Compressive Imaging (SCI) introduces high frequency modulation in the
optical sensor layer to increase effective frame rate. A commonly used sampling
strategy of video SCI is Random Sampling (RS) where each mask element value is
randomly set to be 0 or 1. Similarly, image inpainting (I2P) has demonstrated
that images can be recovered from a fraction of the image pixels. Inspired by
I2P, we propose Ultra-Sparse Sampling (USS) regime, where at each spatial
location, only one sub-frame is set to 1 and all others are set to 0. We then
build a Digital Micro-mirror Device (DMD) encoding system to verify the
effectiveness of our USS strategy. Ideally, we can decompose the USS
measurement into sub-measurements for which we can utilize I2P algorithms to
recover high-speed frames. However, due to the mismatch between the DMD and
CCD, the USS measurement cannot be perfectly decomposed. To this end, we
propose BSTFormer, a sparse TransFormer that utilizes local Block attention,
global Sparse attention, and global Temporal attention to exploit the sparsity
of the USS measurement. Extensive results on both simulated and real-world data
show that our method significantly outperforms all previous state-of-the-art
algorithms. Additionally, an essential advantage of the USS strategy is its
higher dynamic range than that of the RS strategy. Finally, from the
application perspective, the USS strategy is a good choice to implement a
complete video SCI system on chip due to its fixed exposure time.

</details>


### [10] [GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation](https://arxiv.org/abs/2509.08232)
*Seongho Kim,Sejong Ryu,Hyoukjun You,Je Hyeong Hong*

Main category: cs.CV

TL;DR: 作者用GTA5合成多样化致命暴力监控视频，提出Wasserstein片段级域适配，将合成数据与真实数据对齐，从而提升真实监控中枪击/刺伤检测效果，且公开数据与生成框架。


<details>
  <summary>Details</summary>
Motivation: 现实监控中致命暴力事件极其稀少且涉及伦理问题，导致真实标注数据匮乏，影响VAD模型对枪击/刺伤等致命场景的检测能力；因此利用虚拟环境合成数据并进行域适配成为可行途径。

Method: 构建基于GTA5的合成数据集（多视角、天气/时间/视点多样化），提供数据生成框架；提出片段级（snippet-level）域适配方法，使用Wasserstein对抗训练对齐合成特征与真实特征。

Result: 实验表明：使用GTA-Crime合成数据并应用所提域适配策略，能持续提高在真实数据集（如UCF-Crime）上对致命暴力检测的准确率；并公开了数据集与生成工具。

Conclusion: GTA-Crime通过在GTA5中合成致命暴力场景并结合片段级域适配，有效缓解了现实世界数据稀缺问题，提升了真实监控视频中致命暴力（枪击、刺伤）检测性能。

Abstract: Recent advancements in video anomaly detection (VAD) have enabled
identification of various criminal activities in surveillance videos, but
detecting fatal incidents such as shootings and stabbings remains difficult due
to their rarity and ethical issues in data collection. Recognizing this
limitation, we introduce GTA-Crime, a fatal video anomaly dataset and
generation framework using Grand Theft Auto 5 (GTA5). Our dataset contains
fatal situations such as shootings and stabbings, captured from CCTV multiview
perspectives under diverse conditions including action types, weather, time of
day, and viewpoints. To address the rarity of such scenarios, we also release a
framework for generating these types of videos. Additionally, we propose a
snippet-level domain adaptation strategy using Wasserstein adversarial training
to bridge the gap between synthetic GTA-Crime features and real-world features
like UCF-Crime. Experimental results validate our GTA-Crime dataset and
demonstrate that incorporating GTA-Crime with our domain adaptation strategy
consistently enhances real world fatal violence detection accuracy. Our dataset
and the data generation framework are publicly available at
https://github.com/ta-ho/GTA-Crime.

</details>


### [11] [RepViT-CXR: A Channel Replication Strategy for Vision Transformers in Chest X-ray Tuberculosis and Pneumonia Classification](https://arxiv.org/abs/2509.08234)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 通过把灰度CXR图像复制成三通道输入，RepViT-CXR让预训练ViT无缝用于胸部X光诊断，实验显示在TB和肺炎检测上显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有ViT多在三通道自然图像上预训练，而CXR为单通道灰度图，简单复制通道是否能保留信息并让ViT发挥表征能力尚不明确。

Method: 提出RepViT-CXR：对单通道CXR图像进行通道复制（将灰度图复制到三通道）以兼容在自然图像上预训练的ViT模型，直接微调ViT进行分类评估。

Result: 在TB-CXR、Pediatric Pneumonia和Shenzhen TB三个数据集上分别取得（Accuracy/AUC）: 99.9%/99.9%，99.0%/99.0%，91.1%/91.2%，均优于先前CNN或其他基线方法。

Conclusion: RepViT-CXR通过简单的通道复制策略，将单通道胸部X光影像高效地适配给需三通道输入的Vision Transformer，从而在多个胸部病变检测基准上显著提升性能并达成新的SOTA。

Abstract: Chest X-ray (CXR) imaging remains one of the most widely used diagnostic
tools for detecting pulmonary diseases such as tuberculosis (TB) and pneumonia.
Recent advances in deep learning, particularly Vision Transformers (ViTs), have
shown strong potential for automated medical image analysis. However, most ViT
architectures are pretrained on natural images and require three-channel
inputs, while CXR scans are inherently grayscale. To address this gap, we
propose RepViT-CXR, a channel replication strategy that adapts single-channel
CXR images into a ViT-compatible format without introducing additional
information loss. We evaluate RepViT-CXR on three benchmark datasets. On the
TB-CXR dataset,our method achieved an accuracy of 99.9% and an AUC of 99.9%,
surpassing prior state-of-the-art methods such as Topo-CXR (99.3% accuracy,
99.8% AUC). For the Pediatric Pneumonia dataset, RepViT-CXR obtained 99.0%
accuracy, with 99.2% recall, 99.3% precision, and an AUC of 99.0%,
outperforming strong baselines including DCNN and VGG16. On the Shenzhen TB
dataset, our approach achieved 91.1% accuracy and an AUC of 91.2%, marking a
performance improvement over previously reported CNN-based methods. These
results demonstrate that a simple yet effective channel replication strategy
allows ViTs to fully leverage their representational power on grayscale medical
imaging tasks. RepViT-CXR establishes a new state of the art for TB and
pneumonia detection from chest X-rays, showing strong potential for deployment
in real-world clinical screening systems.

</details>


### [12] [Symmetry Interactive Transformer with CNN Framework for Diagnosis of Alzheimer's Disease Using Structural MRI](https://arxiv.org/abs/2509.08243)
*Zheng Yang,Yanteng Zhang,Xupeng Kou,Yang Liu,Chao Ren*

Main category: cs.CV

TL;DR: 提出3D CNN+对称交互变换器（SIT）端到端模型，显著捕捉左右半球结构不对称性，在ADNI上达92.5%准确率，且有良好可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖预训练或忽视由脑部病变引起的左右不对称性，本工作希望直接建模这种不对称性以提升诊断准确率和可解释性。

Method: 方法包括先用3D CNN对sMRI进行编码，然后通过‘交互等距网格块抓取’操作对左右半球特征进行对齐，之后输入对称交互变换器（SIT）以捕捉左右半球间的结构不对称性并用于分类。

Result: 在ADNI数据集上的实验表明，该方法诊断准确率达92.5%，优于若干传统CNN方法及结合通用变换器的模型。可视化显示网络更关注脑萎缩区域，尤其是不对称病理特征。

Conclusion: 本文提出了一种端到端的网络（3D CNN 编码器 + 对称交互变换器SIT），旨在检测由左右脑萎缩引起的疾病不对称性，从而提升AD的诊断效果。

Abstract: Structural magnetic resonance imaging (sMRI) combined with deep learning has
achieved remarkable progress in the prediction and diagnosis of Alzheimer's
disease (AD). Existing studies have used CNN and transformer to build a
well-performing network, but most of them are based on pretraining or ignoring
the asymmetrical character caused by brain disorders. We propose an end-to-end
network for the detection of disease-based asymmetric induced by left and right
brain atrophy which consist of 3D CNN Encoder and Symmetry Interactive
Transformer (SIT). Following the inter-equal grid block fetch operation, the
corresponding left and right hemisphere features are aligned and subsequently
fed into the SIT for diagnostic analysis. SIT can help the model focus more on
the regions of asymmetry caused by structural changes, thus improving
diagnostic performance. We evaluated our method based on the ADNI dataset, and
the results show that the method achieves better diagnostic accuracy (92.5\%)
compared to several CNN methods and CNNs combined with a general transformer.
The visualization results show that our network pays more attention in regions
of brain atrophy, especially for the asymmetric pathological characteristics
induced by AD, demonstrating the interpretability and effectiveness of the
method.

</details>


### [13] [EVDI++: Event-based Video Deblurring and Interpolation via Self-Supervised Learning](https://arxiv.org/abs/2509.08260)
*Chi Zhang,Xiang Zhang,Chenxu Jiang,Gui-Song Xia,Lei Yu*

Main category: cs.CV

TL;DR: EVDI++利用事件相机与自监督学习，通过LDI、分段重构与置信度融合，实现鲁棒的去模糊与插帧，在合成和真实数据上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 帧式相机长曝光导致帧间运动模糊与信息丢失，事件相机高时间分辨率可补偿运动信息以实现去模糊和中间帧预测，且希望在真实模糊视频上进行自监督训练。

Method: 设计了LDI网络估计参考帧与清晰潜在图像的映射关系；引入学习型分段重构模块将图像转换为不同暴光间隔以提升细化与训练效率；提出基于LDI输出置信度的自适应无参融合策略融合事件信息；采用自监督损失约束模糊帧、潜在图像与事件流之间的一致性。

Result: 在合成与使用DAVIS346c采集的真实模糊-事件数据集上，EVDI++在视频去模糊与插帧任务上达到或超过当前最先进方法的性能。

Conclusion: EVDI++提出了统一的自监督事件辅助视频去模糊与插帧框架，结合可学习双重积分网络（LDI）、学习型分段重构模块与自适应无参融合策略，在合成与真实数据上均表现优异，适用于真实场景。

Abstract: Frame-based cameras with extended exposure times often produce perceptible
visual blurring and information loss between frames, significantly degrading
video quality. To address this challenge, we introduce EVDI++, a unified
self-supervised framework for Event-based Video Deblurring and Interpolation
that leverages the high temporal resolution of event cameras to mitigate motion
blur and enable intermediate frame prediction. Specifically, the Learnable
Double Integral (LDI) network is designed to estimate the mapping relation
between reference frames and sharp latent images. Then, we refine the coarse
results and optimize overall training efficiency by introducing a
learning-based division reconstruction module, enabling images to be converted
with varying exposure intervals. We devise an adaptive parameter-free fusion
strategy to obtain the final results, utilizing the confidence embedded in the
LDI outputs of concurrent events. A self-supervised learning framework is
proposed to enable network training with real-world blurry videos and events by
exploring the mutual constraints among blurry frames, latent images, and event
streams. We further construct a dataset with real-world blurry images and
events using a DAVIS346c camera, demonstrating the generalizability of the
proposed EVDI++ in real-world scenarios. Extensive experiments on both
synthetic and real-world datasets show that our method achieves
state-of-the-art performance in video deblurring and interpolation tasks.

</details>


### [14] [Hyperspectral Mamba for Hyperspectral Object Tracking](https://arxiv.org/abs/2509.08265)
*Long Gao,Yunhe Zhang,Yan Jiang,Weiying Xie,Yunsong Li*

Main category: cs.CV

TL;DR: 提出HyMamba，利用SSMs在SSI与HSM模块中统一建模光谱、跨深度和时间信息，实现高光谱目标跟踪的性能提升，在多个基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱跟踪器常通过伪彩色转换或模态融合处理数据，但难以充分捕捉内在光谱信息、时间依赖性及跨深度交互，作者旨在设计能同时建模这三类信息的网络。

Method: 核心方法为Spectral State Integration (SSI)模块和嵌入SSI的Hyperspectral Mamba (HSM)模块。SSI用于融合和传播光谱、跨深度和时间信息；HSM通过三向扫描的SSMs同步学习空间与光谱信息。整体网络将伪彩色与原始高光谱特征联合构建并交互增强。

Result: 在七个数据集上做了广泛实验，HyMamba在HOTC2020上达到了73.0% AUC和96.3% DP@20等最先进指标，表明所提模块对高光谱跟踪有效。

Conclusion: 该文提出HyMamba，一种基于状态空间模块（SSMs）统一建模光谱、跨深度和时间信息的高光谱目标跟踪网络，通过SSI和嵌入的HSM模块实现光谱特征的逐步精炼与传播，并在七个基准数据集上取得最先进性能。

Abstract: Hyperspectral object tracking holds great promise due to the rich spectral
information and fine-grained material distinctions in hyperspectral images,
which are beneficial in challenging scenarios. While existing hyperspectral
trackers have made progress by either transforming hyperspectral data into
false-color images or incorporating modality fusion strategies, they often fail
to capture the intrinsic spectral information, temporal dependencies, and
cross-depth interactions. To address these limitations, a new hyperspectral
object tracking network equipped with Mamba (HyMamba), is proposed. It unifies
spectral, cross-depth, and temporal modeling through state space modules
(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)
module, which enables progressive refinement and propagation of spectral
features with cross-depth and temporal spectral information. Embedded within
each SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatial
and spectral information synchronously via three directional scanning SSMs.
Based on SSI and HSM, HyMamba constructs joint features from false-color and
hyperspectral inputs, and enhances them through interaction with original
spectral features extracted from raw hyperspectral images. Extensive
experiments conducted on seven benchmark datasets demonstrate that HyMamba
achieves state-of-the-art performance. For instance, it achieves 73.0\% of the
AUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code will
be released at https://github.com/lgao001/HyMamba.

</details>


### [15] [Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features](https://arxiv.org/abs/2509.08266)
*Saurav Sengupta,Nazanin Moradinasab,Jiebei Liu,Donald E. Brown*

Main category: cs.CV

TL;DR: 研究通过系统化改变图像与提示特征，发现即便轻微修改也会显著改变开源VLM的注意力分布和答案生成，表明VLM在细粒度视觉问题上高度依赖训练偏差。


<details>
  <summary>Details</summary>
Motivation: 动机是理解和表征VLM在面对需细致视觉推理任务时为何会依赖偏置而忽视视觉证据，从而找到可解释模型行为和改进策略的线索。

Method: 提出一个多维度检验框架，系统化地改变输入图像和提示的特性（如图像尺寸、目标数量、背景颜色、提示具体性），在开源VLM上测量性能和注意力值的变化，以识别导致性能差异的关键因素。

Result: 结果表明，轻微的图像或提示变化即可引发模型回答策略和注意力分配的大幅变动，进而显著影响性能；并确认注意力随输入参数变化呈可测量的波动。

Conclusion: 本文探讨了视觉-语言模型（VLMs）在回答图像细节问题时受训练偏差影响的现象，指出模型常忽视视觉证据而依赖先验偏好，尤其在需要聚焦图像特定区域或高细节问题（如计数）时表现显著下降。

Abstract: Recent research on Vision Language Models (VLMs) suggests that they rely on
inherent biases learned during training to respond to questions about visual
properties of an image. These biases are exacerbated when VLMs are asked highly
specific questions that require focusing on specific areas of the image. For
example, a VLM tasked with counting stars on a modified American flag (e.g.,
with more than 50 stars) will often disregard the visual evidence and fail to
answer accurately. We build upon this research and develop a multi-dimensional
examination framework to systematically determine which characteristics of the
input data, including both the image and the accompanying prompt, lead to such
differences in performance. Using open-source VLMs, we further examine how
attention values fluctuate with varying input parameters (e.g., image size,
number of objects in the image, background color, prompt specificity). This
research aims to learn how the behavior of vision language models changes and
to explore methods for characterizing such changes. Our results suggest, among
other things, that even minor modifications in image characteristics and prompt
specificity can lead to large changes in how a VLM formulates its answer and,
subsequently, its overall performance.

</details>


### [16] [Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration](https://arxiv.org/abs/2509.08280)
*Hyeonseok Kim,Byeongkeun Kang,Yeejin Lee*

Main category: cs.CV

TL;DR: 提出E3DPC-GZSL：用证据不确定性估计和动态校准stacking来纠正见过类偏置，并通过文本与可学习参数融合优化语义空间，在ScanNet v2和S3DIS上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 3D广义零样本语义分割中模型偏向训练中见过类别，且3D数据规模小导致过度自信问题更严重，需要一种不依赖单独分类器而能抑制见过类过度自信的方法。

Method: 在分类器中集成基于证据的不确定性估计器，通过动态校准的stacking因子利用点级不确定性调整预测概率；并通过将可学习参数与文本特征融合来细化语义空间，从而改进不确定性估计训练策略。

Result: 在ScanNet v2和S3DIS数据集上进行的大量实验证明E3DPC-GZSL在GZSL任务上达到了最先进的性能，表现出更平衡的seen/unseen预测能力。

Conclusion: 该论文提出E3DPC-GZSL方法，有效缓解了3D点云广义零样本语义分割中对见过类别的偏置问题，实现了SOTA性能。

Abstract: Generalized zero-shot semantic segmentation of 3D point clouds aims to
classify each point into both seen and unseen classes. A significant challenge
with these models is their tendency to make biased predictions, often favoring
the classes encountered during training. This problem is more pronounced in 3D
applications, where the scale of the training data is typically smaller than in
image-based tasks. To address this problem, we propose a novel method called
E3DPC-GZSL, which reduces overconfident predictions towards seen classes
without relying on separate classifiers for seen and unseen data. E3DPC-GZSL
tackles the overconfidence problem by integrating an evidence-based uncertainty
estimator into a classifier. This estimator is then used to adjust prediction
probabilities using a dynamic calibrated stacking factor that accounts for
pointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a novel
training strategy that improves uncertainty estimation by refining the semantic
space. This is achieved by merging learnable parameters with text-derived
features, thereby improving model optimization for unseen data. Extensive
experiments demonstrate that the proposed approach achieves state-of-the-art
performance on generalized zero-shot semantic segmentation datasets, including
ScanNet v2 and S3DIS.

</details>


### [17] [Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection](https://arxiv.org/abs/2509.08289)
*Yuelin Guo,Haoyu He,Zhiyuan Chen,Zitong Huang,Renhao Lu,Lu Shi,Zejun Wang,Weizhe Zhang*

Main category: cs.CV

TL;DR: 通过热图引导的候选选择、带背景表征的检测网络和对忽略proposal的负确定性监督，提升了弱监督目标检测的完整性、区分邻近实例能力与收敛速度，在VOC上取得了有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 现有WSOD方法常只关注可区分的局部部位或过度覆盖导致难以分辨邻近实例；WSDDN缺少proposal级别的背景类以及分支间存在语义鸿沟；同时优化时丢弃忽略proposal导致收敛慢。

Method: 方法包括三部分：1）HGPS：基于热图对候选框应用双阈值预选，以生成既覆盖整体物体又能区分紧邻实例的伪GT；2）WSBDN：在每个proposal上加入背景类表征，并利用热图作为矩阵间的预监督以缩小分支语义差距；3）负确定性监督损失：对被忽略的proposal施加监督以加快收敛。

Result: 在PASCAL VOC 2007上取得mAP 58.5%和mCorLoc 81.8%；在VOC 2012上取得mAP 55.6%和mCorLoc 80.5%，优于多数现有WSOD方法。

Conclusion: 本文提出的框架有效解决了WSOD中伪GT生成、背景表征缺失和收敛慢三大问题，通过热图引导的候选框选择（HGPS）、带背景表征的WSBDN以及对忽略候选框的负确定性监督损失，提升了检测完整性和区分相邻同类实例能力，在PASCAL VOC 2007/2012上实现了有竞争力的mAP和CorLoc。

Abstract: Weakly supervised object detection (WSOD) has attracted significant attention
in recent years, as it does not require box-level annotations. State-of-the-art
methods generally adopt a multi-module network, which employs WSDDN as the
multiple instance detection network module and multiple instance refinement
modules to refine performance. However, these approaches suffer from three key
limitations. First, existing methods tend to generate pseudo GT boxes that
either focus only on discriminative parts, failing to capture the whole object,
or cover the entire object but fail to distinguish between adjacent intra-class
instances. Second, the foundational WSDDN architecture lacks a crucial
background class representation for each proposal and exhibits a large semantic
gap between its branches. Third, prior methods discard ignored proposals during
optimization, leading to slow convergence. To address these challenges, we
first design a heatmap-guided proposal selector (HGPS) algorithm, which
utilizes dual thresholds on heatmaps to pre-select proposals, enabling pseudo
GT boxes to both capture the full object extent and distinguish between
adjacent intra-class instances. We then present a weakly supervised basic
detection network (WSBDN), which augments each proposal with a background class
representation and uses heatmaps for pre-supervision to bridge the semantic gap
between matrices. At last, we introduce a negative certainty supervision loss
on ignored proposals to accelerate convergence. Extensive experiments on the
challenging PASCAL VOC 2007 and 2012 datasets demonstrate the effectiveness of
our framework. We achieve mAP/mCorLoc scores of 58.5%/81.8% on VOC 2007 and
55.6%/80.5% on VOC 2012, performing favorably against the state-of-the-art WSOD
methods. Our code is publicly available at
https://github.com/gyl2565309278/DTH-CP.

</details>


### [18] [An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia](https://arxiv.org/abs/2509.08303)
*M. Warizmi Wafiq,Peter Cutter,Ate Poortinga,Daniel Marc G. dela Torre,Karis Tenneson,Vanna Teck,Enikoe Bihari,Chanarun Saisaward,Weraphong Suaruang,Andrea McMahon,Andi Vika Faradiba Muin,Karno B. Batiran,Chairil A,Nurul Qomar,Arya Arismaya Metananda,David Ganz,David Saah*

Main category: cs.CV

TL;DR: 开放、面向训练的高分辨率油棕矢量数据集（2020–2024），多解释者标注+实地验证，支持监管与模型开发。


<details>
  <summary>Details</summary>
Motivation: 油棕种植是印尼森林砍伐的主要驱动力之一，亟需详尽可靠的地物图斑以便监管、可持续性评估及训练先进遥感模型。

Method: 利用专家对2020–2024年高分辨率卫星影像进行格网化的wall-to-wall多解释者一致性人工矢量化标注，包含分层类型学区分种植阶段与相似多年生作物，并辅以实地验证以确保质量。

Result: 生成了开放许可（CC-BY）下的多区域、面向多模型训练的矢量数据集，覆盖各种农业生态带，提供多类别分层标签，已通过多解释者一致性和实地验证提升准确性，可用于训练CNN与地理空间基础模型并支持监测油棕扩张。

Conclusion: 该数据集为印度尼西亚油棕种植及相关地表覆盖提供了高质量、开放获取的多时相多类注释，能够显著提升遥感分类和监测能力，支持可持续性监管与去森林化治理。

Abstract: Oil palm cultivation remains one of the leading causes of deforestation in
Indonesia. To better track and address this challenge, detailed and reliable
mapping is needed to support sustainability efforts and emerging regulatory
frameworks. We present an open-access geospatial dataset of oil palm
plantations and related land cover types in Indonesia, produced through expert
labeling of high-resolution satellite imagery from 2020 to 2024. The dataset
provides polygon-based, wall-to-wall annotations across a range of
agro-ecological zones and includes a hierarchical typology that distinguishes
oil palm planting stages as well as similar perennial crops. Quality was
ensured through multi-interpreter consensus and field validation. The dataset
was created using wall-to-wall digitization over large grids, making it
suitable for training and benchmarking both conventional convolutional neural
networks and newer geospatial foundation models. Released under a CC-BY
license, it fills a key gap in training data for remote sensing and aims to
improve the accuracy of land cover types mapping. By supporting transparent
monitoring of oil palm expansion, the resource contributes to global
deforestation reduction goals and follows FAIR data principles.

</details>


### [19] [SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training](https://arxiv.org/abs/2509.08311)
*Rongsheng Wang,Fenghe Tang,Qingsong Yao,Rui Yan,Xu Zhang,Zhen Huang,Haoran Lai,Zhiyang He,Xiaodong Tao,Zihang Jiang,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 针对CT病灶空间稀疏与报告描述对齐难题，SimCroP通过相似性驱动对齐与跨粒度融合，结合多模态掩码建模，提升了病变定位与多尺度下游任务表现。


<details>
  <summary>Details</summary>
Motivation: CT中病变空间稀疏且结构复杂，报告中病理描述与图像子区域之间关系隐含且复杂，传统VL预训练难以精确对齐与捕捉稀疏关键结构。

Method: 提出Similarity-Driven Cross-Granularity Pre-training(SimCroP)，包括多模态掩码建模来提升低层语义理解、相似性驱动对齐用于自适应选择并对齐报告句子对应的图像patch、以及跨粒度融合模块整合实例级与词-块级信息。

Result: 在大规模配对CT-报告数据上预训练后，在五个公开数据集的分类与分割任务中，SimCroP优于当前领先的医学自监督与影像-语言预训练方法。

Conclusion: SimCroP在胸部CT的影像-文本预训练上取得了更好的表征能力，能够更精确地定位病变结构并提升多尺度下游任务的分类与分割性能。

Abstract: Medical vision-language pre-training shows great potential in learning
representative features from massive paired radiographs and reports. However,
in computed tomography (CT) scans, the distribution of lesions which contain
intricate structures is characterized by spatial sparsity. Besides, the complex
and implicit relationships between different pathological descriptions in each
sentence of the report and their corresponding sub-regions in radiographs pose
additional challenges. In this paper, we propose a Similarity-Driven
Cross-Granularity Pre-training (SimCroP) framework on chest CTs, which combines
similarity-driven alignment and cross-granularity fusion to improve radiograph
interpretation. We first leverage multi-modal masked modeling to optimize the
encoder for understanding precise low-level semantics from radiographs. Then,
similarity-driven alignment is designed to pre-train the encoder to adaptively
select and align the correct patches corresponding to each sentence in reports.
The cross-granularity fusion module integrates multimodal information across
instance level and word-patch level, which helps the model better capture key
pathology structures in sparse radiographs, resulting in improved performance
for multi-scale downstream tasks. SimCroP is pre-trained on a large-scale
paired CT-reports dataset and validated on image classification and
segmentation tasks across five public datasets. Experimental results
demonstrate that SimCroP outperforms both cutting-edge medical self-supervised
learning methods and medical vision-language pre-training methods. Codes and
models are available at https://github.com/ToniChopp/SimCroP.

</details>


### [20] [Boosted Training of Lightweight Early Exits for Optimizing CNN Image Classification Inference](https://arxiv.org/abs/2509.08318)
*Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CV

TL;DR: BTS-EE通过顺序训练与CPM校准，解决早退推理中的分布不匹配问题，配合轻量1D卷积分支在嵌入式场景显著提升计算效率，实验展示了明显的计算减少与微小的准确率下降。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的实时图像分类场景中，早退分支训练存在协方差偏移（训练时分支见到全部样本，推理时只处理难样本），导致实际效率受限，需使训练分布更接近推理分布以提升早退性能。

Method: BTS-EE采用分支逐一训练与校准：先训练并校准前一分支，再在剩余未退出样本上训练下一分支；提出基于1D卷积的轻量分支结构和Class Precision Margin (CPM)的按类阈值校准方法。

Result: 在CINIC-10上以ResNet18为骨干网络，BTS-EE在64种配置下均优于非boosted训练，最高可在仅损失2%准确率的前提下将计算量减少45%。

Conclusion: 该论文提出的Boosted Training Scheme for Early Exits (BTS-EE) 通过顺序化训练和校准分支，缓解了早退机制中训练与推理分布不匹配的问题，从而在资源受限环境下提升效率—准确率权衡。

Abstract: Real-time image classification on resource-constrained platforms demands
inference methods that balance accuracy with strict latency and power budgets.
Early-exit strategies address this need by attaching auxiliary classifiers to
intermediate layers of convolutional neural networks (CNNs), allowing "easy"
samples to terminate inference early. However, conventional training of early
exits introduces a covariance shift: downstream branches are trained on full
datasets, while at inference they process only the harder, non-exited samples.
This mismatch limits efficiency--accuracy trade-offs in practice. We introduce
the Boosted Training Scheme for Early Exits (BTS-EE), a sequential training
approach that aligns branch training with inference-time data distributions.
Each branch is trained and calibrated before the next, ensuring robustness
under selective inference conditions. To further support embedded deployment,
we propose a lightweight branch architecture based on 1D convolutions and a
Class Precision Margin (CPM) calibration method that enables per-class
threshold tuning for reliable exit decisions. Experiments on the CINIC-10
dataset with a ResNet18 backbone demonstrate that BTS-EE consistently
outperforms non-boosted training across 64 configurations, achieving up to 45
percent reduction in computation with only 2 percent accuracy degradation.
These results expand the design space for deploying CNNs in real-time image
processing systems, offering practical efficiency gains for applications such
as industrial inspection, embedded vision, and UAV-based monitoring.

</details>


### [21] [Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis](https://arxiv.org/abs/2509.08338)
*Jihyun Moon,Charmgil Hong*

Main category: cs.CV

TL;DR: 将语义相似病例检索到的图像与元数据拼入VLM提示，可在无需微调的情况下显著提升皮肤镜图像恶性黑色素瘤的诊断性能与纠错能力。


<details>
  <summary>Details</summary>
Motivation: 传统CNN专注于图像，常忽视临床元数据且需要大量预处理；通用领域训练的VLM缺乏临床特异性。为解决这两方面限制，作者提出通过利用相似病例检索将相关临床信息注入提示，从而提升诊断性能。

Method: 使用检索模块在病例库中找到与当前病人相似的临床案例（包括图像与元数据），并将这些案例作为附加上下文拼接进VLM的提示中；VLM基于该增强提示直接输出诊断，无需对模型进行参数微调。

Result: 在多项基准实验中，检索增强提示显著提高了分类准确率并减少诊断错误，且在纠错任务上优于传统基线方法，证明该方法在临床决策支持场景中的有效性与鲁棒性。

Conclusion: 该论文提出了一种检索增强视觉-语言模型（VLM）框架，通过在提示中加入语义相似的病历，以提升皮肤镜图像中恶性黑色素瘤的诊断准确性，从而在无需微调的情况下实现有根据的预测与临床决策支持。

Abstract: Accurate and early diagnosis of malignant melanoma is critical for improving
patient outcomes. While convolutional neural networks (CNNs) have shown promise
in dermoscopic image analysis, they often neglect clinical metadata and require
extensive preprocessing. Vision-language models (VLMs) offer a multimodal
alternative but struggle to capture clinical specificity when trained on
general-domain data. To address this, we propose a retrieval-augmented VLM
framework that incorporates semantically similar patient cases into the
diagnostic prompt. Our method enables informed predictions without fine-tuning
and significantly improves classification accuracy and error correction over
conventional baselines. These results demonstrate that retrieval-augmented
prompting provides a robust strategy for clinical decision support.

</details>


### [22] [InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection](https://arxiv.org/abs/2509.08374)
*Zhongyu Xia,Hansong Yang,Yongtao Wang*

Main category: cs.CV

TL;DR: InsFusion通过从原始与融合特征提取候选并在原始特征上查询、结合注意力机制，减轻了特征融合过程的误差累积，在nuScenes上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多视角相机与LiDAR融合的三维目标检测在特征提取、透视变换与融合过程中会产生并放大噪声和误差，传统方法在融合后直接检测会受到累积误差影响，因而需要一种能利用原始特征进行纠正的机制。

Method: 在网络中并行提取两类候选（来自原始特征和融合特征），将这些候选用于在原始（未融合）特征上进行查询并结合注意力机制，以减少透视变换和特征融合过程中的误差传播。

Result: 在nuScenes数据集上，InsFusion与多种先进基线兼容，并实现了新的最先进（state-of-the-art）3D目标检测性能。

Conclusion: InsFusion通过从原始特征与融合特征中提取候选框并用这些候选框去查询原始特征、结合注意力机制来抑制在特征提取与融合过程中累积的噪声和误差，从而提升多模态（多视角相机+LiDAR）三维目标检测的性能。

Abstract: Three-dimensional Object Detection from multi-view cameras and LiDAR is a
crucial component for autonomous driving and smart transportation. However, in
the process of basic feature extraction, perspective transformation, and
feature fusion, noise and error will gradually accumulate. To address this
issue, we propose InsFusion, which can extract proposals from both raw and
fused features and utilizes these proposals to query the raw features, thereby
mitigating the impact of accumulated errors. Additionally, by incorporating
attention mechanisms applied to the raw features, it thereby mitigates the
impact of accumulated errors. Experiments on the nuScenes dataset demonstrate
that InsFusion is compatible with various advanced baseline methods and
delivers new state-of-the-art performance for 3D object detection.

</details>


### [23] [Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video](https://arxiv.org/abs/2509.08376)
*Xiao Li,Qi Chen,Xiulian Peng,Kai Yu,Xie Chen,Yan Lu*

Main category: cs.CV

TL;DR: 作者提出一个基于transformer+低码率向量量化+去噪扩散的自监督框架，实现了运动/内容解耦并形成离散运动空间，在talking head与2D像素精灵上验证了运动迁移和生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作在视频分解（运动/内容）上通常依赖较多假设或归纳偏置，限制了泛化性和表示的灵活性；作者希望构建更通用、假设更少的自监督框架以获得有意义且离散的运动表示。

Method: 使用transformer联合生成帧级运动和片段级内容隐式特征；采用低比特率向量量化强制信息瓶颈并构造离散运动空间；以量化的运动与内容潜码作为条件输入到去噪扩散模型，进行自监督表示学习，支持运动传递和自回归运动生成。

Result: 在真实的talking head视频上在运动迁移与自回归运动生成任务上取得验证，并能推广到像2D卡通角色的像素精灵等其他视频类型，表明方法具有一定泛化能力。

Conclusion: 该论文提出了一种自监督的视频表示学习框架，通过变压器生成逐帧运动和片段级静态内容的隐式特征，并使用低码率向量量化作为信息瓶颈，从而实现运动与内容的解耦。

Abstract: We propose a novel and general framework to disentangle video data into its
dynamic motion and static content components. Our proposed method is a
self-supervised pipeline with less assumptions and inductive biases than
previous works: it utilizes a transformer-based architecture to jointly
generate flexible implicit features for frame-wise motion and clip-wise
content, and incorporates a low-bitrate vector quantization as an information
bottleneck to promote disentanglement and form a meaningful discrete motion
space. The bitrate-controlled latent motion and content are used as conditional
inputs to a denoising diffusion model to facilitate self-supervised
representation learning. We validate our disentangled representation learning
framework on real-world talking head videos with motion transfer and
auto-regressive motion generation tasks. Furthermore, we also show that our
method can generalize to other types of video data, such as pixel sprites of 2D
cartoon characters. Our work presents a new perspective on self-supervised
learning of disentangled video representations, contributing to the broader
field of video analysis and generation.

</details>


### [24] [Semantic Causality-Aware Vision-Based 3D Occupancy Prediction](https://arxiv.org/abs/2509.08388)
*Dubing Chen,Huan Zheng,Yucheng Zhou,Xianfei Li,Wenlong Liao,Tao He,Pai Peng,Jianbing Shen*

Main category: cs.CV

TL;DR: 提出一种基于2D到3D语义因果损失的端到端可微分转换模块，包含通道分组提升、可学习相机偏移和归一化卷积，在Occ3D上实现SOTA且对相机扰动更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有方法多采用模块化流水线，模块间独立优化或依赖预设输入，导致误差级联；因此希望通过统一的因果监督使2D到3D转换可微分并整体训练以提升语义一致性和鲁棒性。

Method: 设计了一个2D到3D语义因果损失（causal loss），并基于此提出Semantic Causality-Aware 2D-to-3D Transformation，包括：1) Channel-Grouped Lifting用于自适应语义映射，2) Learnable Camera Offsets增强对相机扰动的鲁棒性，3) Normalized Convolution用于有效的特征传播。该损失使得原本不可训练的组件可以端到端学习。

Result: 在Occ3D基准上取得了最先进的性能，对相机扰动表现出显著鲁棒性，并提高了2D到3D语义一致性。

Conclusion: 该论文提出了一种基于因果损失的端到端2D到3D语义占据预测方法，通过调节从3D体素到2D特征的梯度流，实现模块间的统一学习，解决了传统模块化流水线中误差级联的问题。

Abstract: Vision-based 3D semantic occupancy prediction is a critical task in 3D vision
that integrates volumetric 3D reconstruction with semantic understanding.
Existing methods, however, often rely on modular pipelines. These modules are
typically optimized independently or use pre-configured inputs, leading to
cascading errors. In this paper, we address this limitation by designing a
novel causal loss that enables holistic, end-to-end supervision of the modular
2D-to-3D transformation pipeline. Grounded in the principle of 2D-to-3D
semantic causality, this loss regulates the gradient flow from 3D voxel
representations back to the 2D features. Consequently, it renders the entire
pipeline differentiable, unifying the learning process and making previously
non-trainable components fully learnable. Building on this principle, we
propose the Semantic Causality-Aware 2D-to-3D Transformation, which comprises
three components guided by our causal loss: Channel-Grouped Lifting for
adaptive semantic mapping, Learnable Camera Offsets for enhanced robustness
against camera perturbations, and Normalized Convolution for effective feature
propagation. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on the Occ3D benchmark, demonstrating significant
robustness to camera perturbations and improved 2D-to-3D semantic consistency.

</details>


### [25] [VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring](https://arxiv.org/abs/2509.08392)
*Cuong Nguyen,Dung T. Tran,Hong Nguyen,Xuan-Vu Phan,Nam-Phong Nguyen*

Main category: cs.CV

TL;DR: 提出一种在编码阶段注入输入感知特征的VRAE，用于实时交通监控图像增强，在恢复质量和计算成本上比AE/GAN/Flow表现更优。


<details>
  <summary>Details</summary>
Motivation: 现实交通监控图像常受恶劣天气、弱光、高速运动导致的噪声和模糊影响，车牌在整图中占比小，直接影响车牌识别准确率，因此需要快速实时的图像增强作为预处理。

Method: 设计了Vertical Residual Autoencoder (VRAE)，通过在每个编码阶段加入辅助模块（input-aware feature injection），保持全局信息并引导表示学习，从而在不显著增加参数的情况下提升恢复性能。

Result: 在含可见车牌的数据集上，VRAE相比相同深度的AE提高PSNR约20%，降低NMSE约50%，提升SSIM约1%，参数量仅增加约1%，并整体优于GAN和Flow-Based方法。

Conclusion: 提出的VRAE在交通监控图像增强任务中有效提升了车牌可识别性和图像质量，尤其在噪声和模糊条件下优于AE、GAN和Flow方法。

Abstract: In real-world traffic surveillance, vehicle images captured under adverse
weather, poor lighting, or high-speed motion often suffer from severe noise and
blur. Such degradations significantly reduce the accuracy of license plate
recognition systems, especially when the plate occupies only a small region
within the full vehicle image. Restoring these degraded images a fast realtime
manner is thus a crucial pre-processing step to enhance recognition
performance. In this work, we propose a Vertical Residual Autoencoder (VRAE)
architecture designed for the image enhancement task in traffic surveillance.
The method incorporates an enhancement strategy that employs an auxiliary
block, which injects input-aware features at each encoding stage to guide the
representation learning process, enabling better general information
preservation throughout the network compared to conventional autoencoders.
Experiments on a vehicle image dataset with visible license plates demonstrate
that our method consistently outperforms Autoencoder (AE), Generative
Adversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE at
the same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,
and enhances SSIM by 1\%, while requiring only a marginal increase of roughly
1\% in parameters.

</details>


### [26] [Sparse BEV Fusion with Self-View Consistency for Multi-View Detection and Tracking](https://arxiv.org/abs/2509.08421)
*Keisuke Toida,Taigo Sakai,Naoki Kato,Kazutoyo Yokota,Takeshi Nakamura,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: SCFusion通过稀疏投影、密度感知加权和多视图一致性损失，改进BEV多视图特征融合，显著提升多视图检测与跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 多视图多目标跟踪中，BEV投影增强遮挡鲁棒性，但投影会引入特征失真和由于物体随距离缩放导致的非均匀密度，从而降低融合表示质量和跟踪检测性能，因此需改进投影与融合策略。

Method: 提出SCFusion框架，包含：1) 稀疏投影（sparse transformation）以避免投影过程中的不自然插值；2) 密度感知加权（density-aware weighting），根据空间置信度与相机距离自适应融合特征；3) 多视图一致性损失（multi-view consistency loss），鼓励各相机在融合前独立学习判别性特征。团队在WildTrack和MultiviewX上与TrackTacular对比验证性能提升。

Result: 在WildTrack上达到IDF1=95.9%，在MultiviewX上MODP=89.2%，优于基线TrackTacular，表明在定位一致性和检测准确度上都有显著提升。

Conclusion: SCFusion有效缓解了传统BEV投影导致的特征失真和密度不均问题，通过稀疏变换、密度感知加权和多视图一致性损失三大技术提升了多视角特征融合的鲁棒性和检测/跟踪精度，在公开数据集上取得了优异成绩，证明方法可行。

Abstract: Multi-View Multi-Object Tracking (MVMOT) is essential for applications such
as surveillance, autonomous driving, and sports analytics. However, maintaining
consistent object identities across multiple cameras remains challenging due to
viewpoint changes, lighting variations, and occlusions, which often lead to
tracking errors.Recent methods project features from multiple cameras into a
unified Bird's-Eye-View (BEV) space to improve robustness against occlusion.
However, this projection introduces feature distortion and non-uniform density
caused by variations in object scale with distance. These issues degrade the
quality of the fused representation and reduce detection and tracking
accuracy.To address these problems, we propose SCFusion, a framework that
combines three techniques to improve multi-view feature integration. First, it
applies a sparse transformation to avoid unnatural interpolation during
projection. Next, it performs density-aware weighting to adaptively fuse
features based on spatial confidence and camera distance. Finally, it
introduces a multi-view consistency loss that encourages each camera to learn
discriminative features independently before fusion.Experiments show that
SCFusion achieves state-of-the-art performance, reaching an IDF1 score of 95.9%
on WildTrack and a MODP of 89.2% on MultiviewX, outperforming the baseline
method TrackTacular. These results demonstrate that SCFusion effectively
mitigates the limitations of conventional BEV projection and provides a robust
and accurate solution for multi-view object detection and tracking.

</details>


### [27] [LD-ViCE: Latent Diffusion Model for Video Counterfactual Explanations](https://arxiv.org/abs/2509.08422)
*Payal Varshney,Adriano Lucieri,Christoph Balada,Sheraz Ahmed,Andreas Dengel*

Main category: cs.CV

TL;DR: LD-ViCE通过在潜在空间使用扩散模型并加入目标模型引导与细化步骤，有效且高效地生成语义和时序一致的反事实视频解释，在多数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频解释方法在时间一致性、鲁棒性和可操作因果洞见方面不足，且现有反事实方法通常缺乏对目标模型的引导，导致语义保真度和实用性下降。

Method: 在潜在空间运行扩散模型以降低计算成本，结合目标模型引导和额外的细化步骤以提高语义保真度与现实感；在三个不同数据集（心脏超声、面部表情、动作识别）上进行评估并与现有方法对比。

Result: 在三个数据集上，LD-ViCE在R2得分上最多提升68%，推理时间缩短一半；定性分析表明生成的视频具有语义意义和时序连贯性。

Conclusion: LD-ViCE提出了一种基于潜在扩散模型的视频反事实解释框架，能在潜在空间高效生成语义合理且时序连贯的反事实视频，从而更好地解释视频模型决策。

Abstract: Video-based AI systems are increasingly adopted in safety-critical domains
such as autonomous driving and healthcare. However, interpreting their
decisions remains challenging due to the inherent spatiotemporal complexity of
video data and the opacity of deep learning models. Existing explanation
techniques often suffer from limited temporal coherence, insufficient
robustness, and a lack of actionable causal insights. Current counterfactual
explanation methods typically do not incorporate guidance from the target
model, reducing semantic fidelity and practical utility. We introduce Latent
Diffusion for Video Counterfactual Explanations (LD-ViCE), a novel framework
designed to explain the behavior of video-based AI models. Compared to previous
approaches, LD-ViCE reduces the computational costs of generating explanations
by operating in latent space using a state-of-the-art diffusion model, while
producing realistic and interpretable counterfactuals through an additional
refinement step. Our experiments demonstrate the effectiveness of LD-ViCE
across three diverse video datasets, including EchoNet-Dynamic (cardiac
ultrasound), FERV39k (facial expression), and Something-Something V2 (action
recognition). LD-ViCE outperforms a recent state-of-the-art method, achieving
an increase in R2 score of up to 68% while reducing inference time by half.
Qualitative analysis confirms that LD-ViCE generates semantically meaningful
and temporally coherent explanations, offering valuable insights into the
target model behavior. LD-ViCE represents a valuable step toward the
trustworthy deployment of AI in safety-critical domains.

</details>


### [28] [Beyond Distribution Shifts: Adaptive Hyperspectral Image Classification at Test Time](https://arxiv.org/abs/2509.08436)
*Xia Yue,Anfeng Liu,Ning Chen,Chenjia Huang,Hui Liu,Zhou Huang,Leyuan Fang*

Main category: cs.CV

TL;DR: 提出HyperTTA：包含多降解基准、SSTC分类器与置信感知LayerNorm测试时自适应（CELA），无需源数据/目标标注即可提升HSI在多种降解下的分类鲁棒性，实验效果优于基线。


<details>
  <summary>Details</summary>
Motivation: HSI分类模型对噪声、模糊、压缩、大气效应等真实世界降解极其敏感，现有方法在降解分布偏移下表现不稳。需要一个统一且轻量的框架，既提升基础分类器的鲁棒性，又能在测试时动态自适应目标域分布而无需源数据或标签。

Method: 方法包含三部分：1) 构建包含九类典型降解的多降解HSI数据集作为评测基准；2) 设计SSTC，采用多尺度感受野机制和标签平滑正则化以联合建模多尺度光谱-空间信息；3) 提出轻量级TTA策略CELA，仅通过最小化高置信度无标签目标样本的预测熵来更新LayerNorm的仿射参数，避免噪声预测导致的不可靠更新，且不访问源数据或目标标注。

Result: 在两个基准数据集的广泛降解情形下，HyperTTA在整体性能和稳健性上均超越现有基线方法；同时SSTC和CELA各自的有效性通过消融验证得到证明。代码将公开。

Conclusion: 本文提出了HyperTTA框架，通过构建多降解HSI基准、设计光谱-空间Transformer分类器（SSTC）并结合置信感知的LayerNorm适配器（CELA）用于测试时自适应，从而提升在多种实测降解下的分类鲁棒性。实验表明在两个基准数据集上均优于现有基线，验证了方法有效性。

Abstract: Hyperspectral image (HSI) classification models are highly sensitive to
distribution shifts caused by various real-world degradations such as noise,
blur, compression, and atmospheric effects. To address this challenge, we
propose HyperTTA, a unified framework designed to enhance model robustness
under diverse degradation conditions. Specifically, we first construct a
multi-degradation hyperspectral dataset that systematically simulates nine
representative types of degradations, providing a comprehensive benchmark for
robust classification evaluation. Based on this, we design a spectral-spatial
transformer classifier (SSTC) enhanced with a multi-level receptive field
mechanism and label smoothing regularization to jointly capture multi-scale
spatial context and improve generalization. Furthermore, HyperTTA incorporates
a lightweight test-time adaptation (TTA) strategy, the confidence-aware
entropy-minimized LayerNorm adapter (CELA), which updates only the affine
parameters of LayerNorm layers by minimizing prediction entropy on
high-confidence unlabeled target samples. This confidence-aware adaptation
prevents unreliable updates from noisy predictions, enabling robust and dynamic
adaptation without access to source data or target annotations. Extensive
experiments on two benchmark datasets demonstrate that HyperTTA outperforms
existing baselines across a wide range of degradation scenarios, validating the
effectiveness of both its classification backbone and the proposed TTA scheme.
Code will be made available publicly.

</details>


### [29] [Spherical Brownian Bridge Diffusion Models for Conditional Cortical Thickness Forecasting](https://arxiv.org/abs/2509.08442)
*Ivan Stoyanov,Fabian Bongratz,Christian Wachinger*

Main category: cs.CV

TL;DR: 提出SBDM和CoS-UNet，用球面扩散模型在顶点级预测个体皮质厚度轨迹，结果在ADNI/OASIS上优于既有方法并支持反事实生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在非欧几里得皮质几何上进行顶点级、高分辨率的个体化皮质厚度预测，且难以有效融合多模态临床/人口统计条件以做时间序列预测与反事实生成。

Method: 构建了双向条件布朗桥扩散过程（SBDM），在去噪网络上设计了条件球面U-Net（CoS-UNet），结合球面卷积和密集跨注意力机制以融合顶点级皮质表面数据与表格条件信息。

Result: 在ADNI和OASIS等纵向数据集上，SBDM在预测误差上显著优于之前方法，并能生成合理的事实和反事实皮质厚度轨迹。

Conclusion: 该论文提出了基于球面布朗桥扩散的皮质厚度轨迹预测方法，能够生成个体化、高分辨率的皮质厚度时间序列，并能做事实与反事实分析。

Abstract: Accurate forecasting of individualized, high-resolution cortical thickness
(CTh) trajectories is essential for detecting subtle cortical changes,
providing invaluable insights into neurodegenerative processes and facilitating
earlier and more precise intervention strategies. However, CTh forecasting is a
challenging task due to the intricate non-Euclidean geometry of the cerebral
cortex and the need to integrate multi-modal data for subject-specific
predictions. To address these challenges, we introduce the Spherical Brownian
Bridge Diffusion Model (SBDM). Specifically, we propose a bidirectional
conditional Brownian bridge diffusion process to forecast CTh trajectories at
the vertex level of registered cortical surfaces. Our technical contribution
includes a new denoising model, the conditional spherical U-Net (CoS-UNet),
which combines spherical convolutions and dense cross-attention to integrate
cortical surfaces and tabular conditions seamlessly. Compared to previous
approaches, SBDM achieves significantly reduced prediction errors, as
demonstrated by our experiments based on longitudinal datasets from the ADNI
and OASIS. Additionally, we demonstrate SBDM's ability to generate individual
factual and counterfactual CTh trajectories, offering a novel framework for
exploring hypothetical scenarios of cortical development.

</details>


### [30] [First-order State Space Model for Lightweight Image Super-resolution](https://arxiv.org/abs/2509.08458)
*Yujie Zhu,Xinyi Zhang,Yekai Lu,Guang Yang,Faming Fang,Guixu Zhang*

Main category: cs.CV

TL;DR: 作者提出FSSM：在Mamba SSM中引入一阶保持条件和新离散化以强化token相关性建模，从而在不增加参数量下显著提升轻量级超分任务性能，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba-based视觉模型多聚焦于网络结构与扫描路径，忽视了SSM模块本身的潜力。作者旨在通过改进SSM计算过程来提升轻量级图像超分辨率（SR）任务的性能，而不增加模型参数。

Method: 在原始Mamba SSM基础上引入一阶保持（first-order hold）条件，推导新的离散化形式，修改计算流程以加强token间相关性建模，同时保持参数不变。进行理论的累积误差分析并在MambaIR框架中替换为FSSM模块。

Result: 在五个基准数据集上的大量实验表明，FSSM在不增加参数量的情况下提升了MambaIR的表现，并优于现有轻量级超分方法，达到了SOTA效果。

Conclusion: 本文提出了一阶状态空间模型（FSSM），在不增加参数量的前提下改进了Mamba的SSM模块，通过引入一阶保持条件与新的离散化形式并考虑token间关联来降低累积误差并提升轻量级超分辨率性能。

Abstract: State space models (SSMs), particularly Mamba, have shown promise in NLP
tasks and are increasingly applied to vision tasks. However, most Mamba-based
vision models focus on network architecture and scan paths, with little
attention to the SSM module. In order to explore the potential of SSMs, we
modified the calculation process of SSM without increasing the number of
parameters to improve the performance on lightweight super-resolution tasks. In
this paper, we introduce the First-order State Space Model (FSSM) to improve
the original Mamba module, enhancing performance by incorporating token
correlations. We apply a first-order hold condition in SSMs, derive the new
discretized form, and analyzed cumulative error. Extensive experimental results
demonstrate that FSSM improves the performance of MambaIR on five benchmark
datasets without additionally increasing the number of parameters, and
surpasses current lightweight SR methods, achieving state-of-the-art results.

</details>


### [31] [Maximally Useful and Minimally Redundant: The Key to Self Supervised Learning for Imbalanced Data](https://arxiv.org/abs/2509.08469)
*Yash Kumar Sharma,Vineet Nair,Wilson Naik*

Main category: cs.CV

TL;DR: 提出基于互信息的多于两视图自监督目标和新损失，能更好区分并学习尾类特征，从而在不平衡数据集上显著提升表示和分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统CSSL依赖双视图假设，在类别不平衡场景下难以学习到尾类的判别特征；受Yann LeCun关于扩展到多视图的建议启发，作者尝试用多视图来改善不平衡问题。

Method: 基于互信息理论为>2视图目标提供理论依据，引入区分"intra"和"inter"判别特征的损失函数，并过滤极端特征；在多种自监督框架上（对比式与非对比式）进行实验验证。

Result: 在CIFAR10-LT（ResNet-18）上提升约2%，CIFAR100-LT（ResNet-18）提升约5%，ImageNet-LT (1k, ResNet-50)提升约3%，并在多种自监督方法上验证了有效性。

Conclusion: CSSL在不平衡数据集上性能下降，但扩展到多于两视图可改善尾类表征，从而提升不平衡自监督学习效果。

Abstract: The robustness of contrastive self-supervised learning (CSSL) for imbalanced
datasets is largely unexplored. CSSL usually makes use of \emph{multi-view}
assumptions to learn discriminatory features via similar and dissimilar data
samples. CSSL works well on balanced datasets, but does not generalize well for
imbalanced datasets. In a very recent paper, as part of future work, Yann LeCun
pointed out that the self-supervised multiview framework can be extended to
cases involving \emph{more than two views}. Taking a cue from this insight we
propose a theoretical justification based on the concept of \emph{mutual
information} to support the \emph{more than two views} objective and apply it
to the problem of dataset imbalance in self-supervised learning. The proposed
method helps extract representative characteristics of the tail classes by
segregating between \emph{intra} and \emph{inter} discriminatory
characteristics. We introduce a loss function that helps us to learn better
representations by filtering out extreme features. Experimental evaluation on a
variety of self-supervised frameworks (both contrastive and non-contrastive)
also prove that the \emph{more than two view} objective works well for
imbalanced datasets. We achieve a new state-of-the-art accuracy in
self-supervised imbalanced dataset classification (2\% improvement in
Cifar10-LT using Resnet-18, 5\% improvement in Cifar100-LT using Resnet-18, 3\%
improvement in Imagenet-LT (1k) using Resnet-50).

</details>


### [32] [Prompt-Driven Image Analysis with Multimodal Generative AI: Detection, Segmentation, Inpainting, and Interpretation](https://arxiv.org/abs/2509.08489)
*Kaleem Ahmad*

Main category: cs.CV

TL;DR: 提出并实现了一个从单一提示驱动的端到端图像分析与编辑流水线，注重中间产物保存、可复现性与稳定性，实验显示生成掩码高成功率，修补耗时显著，并给出实用的实现与调参建议。


<details>
  <summary>Details</summary>
Motivation: 简化并统一从自然语言提示到多步图像编辑与描述的流程，解决模块集成中脆弱性与可重复性问题，为实际应用提供可调试与可靠的实现模式。

Method: 将开放词汇检测、可提示分割、文本条件修补（基于扩散模型）和视觉-语言描述模块串联，提供交互式UI与脚本化CLI；通过阈值调整、掩码形态学处理与资源感知默认设置来降低脆弱性；记录版本、日志、随机种子以支持重放。

Result: 在单词提示的测试中，检测与分割在90%以上的案例生成可用掩码，准确率超过85%；高端GPU上，扩散修补占总耗时的60%至75%；提供了关于阈值、掩码紧致度、扩散参数及运行时策略的实施建议。

Conclusion: 该论文提出了一个统一的、可复现的端到端图像处理流水线，能从单一自然语言提示完成检测、可提示分割、基于文本的图像修补和视觉语言描述，并注重中间产物保存与调试，提高系统可靠性。

Abstract: Prompt-driven image analysis converts a single natural-language instruction
into multiple steps: locate, segment, edit, and describe. We present a
practical case study of a unified pipeline that combines open-vocabulary
detection, promptable segmentation, text-conditioned inpainting, and
vision-language description into a single workflow. The system works end to end
from a single prompt, retains intermediate artifacts for transparent debugging
(such as detections, masks, overlays, edited images, and before and after
composites), and provides the same functionality through an interactive UI and
a scriptable CLI for consistent, repeatable runs. We highlight integration
choices that reduce brittleness, including threshold adjustments, mask
inspection with light morphology, and resource-aware defaults. In a small,
single-word prompt segment, detection and segmentation produced usable masks in
over 90% of cases with an accuracy above 85% based on our criteria. On a
high-end GPU, inpainting makes up 60 to 75% of total runtime under typical
guidance and sampling settings, which highlights the need for careful tuning.
The study offers implementation-guided advice on thresholds, mask tightness,
and diffusion parameters, and details version pinning, artifact logging, and
seed control to support replay. Our contribution is a transparent, reliable
pattern for assembling modern vision and multimodal models behind a single
prompt, with clear guardrails and operational practices that improve
reliability in object replacement, scene augmentation, and removal.

</details>


### [33] [A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models](https://arxiv.org/abs/2509.08490)
*Edwine Nabahirwa,Wei Song,Minghua Zhang,Yi Fang,Zhou Ni*

Main category: cs.CV

TL;DR: 综述性论文将UOD挑战分为五类，评估传统与现代方法，探讨LVLMs与合成数据的应用，认为LVLMs有前景但需在数据真实度与实时性方面进一步研究。


<details>
  <summary>Details</summary>
Motivation: 水下环境复杂多变，现有检测方法在真实海洋应用中表现受限；因此有必要系统梳理挑战、评估新兴LVLMs的可行性，并探索合成数据与模型优化策略以提升UOD性能。

Method: 系统性分类UOD面临的五类挑战，回顾从传统图像处理与目标检测方法到现代深度学习与LVLMs的技术演进，结合案例研究（DALL-E 3合成数据、Florence-2微调）评估方法有效性。

Result: 总结出三点关键见解：1) 现有方法不足以完全解决复杂退化与小目标问题；2) 使用LVLM生成的合成数据能够扩充数据集但需提升真实感；3) LVLM在UOD中具潜力但实时部署与优化研究不足。

Conclusion: 本文综述了水下目标检测（UOD）面临的挑战及现有方法的局限，指出当前方法难以全面解决水下图像退化与小目标检测等问题，并强调大视觉-语言模型（LVLMs）在合成数据增强与多模态检测中的潜力，但其实时性与泛化能力尚需改进。

Abstract: Underwater object detection (UOD) is vital to diverse marine applications,
including oceanographic research, underwater robotics, and marine conservation.
However, UOD faces numerous challenges that compromise its performance. Over
the years, various methods have been proposed to address these issues, but they
often fail to fully capture the complexities of underwater environments. This
review systematically categorizes UOD challenges into five key areas: Image
quality degradation, target-related issues, data-related challenges,
computational and processing constraints, and limitations in detection
methodologies. To address these challenges, we analyze the progression from
traditional image processing and object detection techniques to modern
approaches. Additionally, we explore the potential of large vision-language
models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated
in other domains. We also present case studies, including synthetic dataset
generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review
identifies three key insights: (i) Current UOD methods are insufficient to
fully address challenges like image degradation and small object detection in
dynamic underwater environments. (ii) Synthetic data generation using LVLMs
shows potential for augmenting datasets but requires further refinement to
ensure realism and applicability. (iii) LVLMs hold significant promise for UOD,
but their real-time application remains under-explored, requiring further
research on optimization techniques.

</details>


### [34] [Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening](https://arxiv.org/abs/2509.08502)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一种基于自监督自编码器、在潜在空间引入感知直线化先验的时间敏感视频表示，能在线性层面区分时序相反动作并在多个数据集上优于大型预训练视频模型。


<details>
  <summary>Details</summary>
Motivation: 许多视频嵌入无法区分时间相反的动作，而这类动作在日常生活中频繁出现且依赖简单的视觉时间变化（物体状态、大小、位置、数量等）。因此需要紧凑且对时间变化敏感的表示，能在线性分类器下区分“打开/关闭”“靠近/远离”等动作对。

Method: 以自监督自编码器为核心，输入为冻结的图像特征序列；在潜在空间引入感知直线化（perceptual straightening）诱导偏置以鼓励时间维上的线性变化；训练目标使编码器-解码器重构并保持时序可分性；最终得到低维时间敏感的特征用于线性分类。

Result: 在Something-Something、EPIC-Kitchens和Charades三大数据集上，该方法在chiral动作识别任务上优于在大规模视频数据上预训练的更大模型；且与这些现有模型结合后，能够进一步提升标准基准的分类性能。表示既紧凑又能带来性能增益。

Conclusion: 该论文提出了一种自监督的时间敏感视频表示学习方法，旨在区分时间上相反的动作（chiral action）。通过对冻结的图像特征序列使用带有感知直线化先验的自编码器结构，作者在潜在空间中注入时间敏感性，从而获得紧凑且能线性分离chiral对的表示。

Abstract: Our objective is to develop compact video representations that are sensitive
to visual change over time. To measure such time-sensitivity, we introduce a
new task: chiral action recognition, where one needs to distinguish between a
pair of temporally opposite actions, such as "opening vs. closing a door",
"approaching vs. moving away from something", "folding vs. unfolding paper",
etc. Such actions (i) occur frequently in everyday life, (ii) require
understanding of simple visual change over time (in object state, size, spatial
position, count . . . ), and (iii) are known to be poorly represented by many
video embeddings. Our goal is to build time aware video representations which
offer linear separability between these chiral pairs. To that end, we propose a
self-supervised adaptation recipe to inject time-sensitivity into a sequence of
frozen image features. Our model is based on an auto-encoder with a latent
space with inductive bias inspired by perceptual straightening. We show that
this results in a compact but time-sensitive video representation for the
proposed task across three datasets: Something-Something, EPIC-Kitchens, and
Charade. Our method (i) outperforms much larger video models pre-trained on
large-scale video datasets, and (ii) leads to an improvement in classification
performance on standard benchmarks when combined with these existing models.

</details>


### [35] [HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning](https://arxiv.org/abs/2509.08519)
*Liyang Chen,Tianxiang Ma,Jiawei Liu,Bingchuan Li,Zhuowei Chen,Lijie Liu,Xu He,Gen Li,Qian He,Zhiyong Wu*

Main category: cs.CV

TL;DR: HuMo通过配对数据集、两阶段渐进训练及time-adaptive引导，实现了对文本、参考图像和音频的协同可控人类视频生成，在主体保持与音画同步等指标上优于专门方法。


<details>
  <summary>Details</summary>
Motivation: 现有HCVG方法很难协调文本、图像与音频三种异构条件，主要受限于配对三元条件训练数据稀缺以及主体保持与音画同步两个子任务难以协同，导致生成的视频在语义一致性、主体身份保持与音画同步性方面表现不足。

Method: 方法包括：(1) 构建高质量的配对数据集，包含文本、参考图像与音频；(2) 两阶段渐进多模态训练范式：先进行主体保持相关训练，采用最小侵入式图像注入以保留基础模型的提示遵循与视觉生成能力；再逐步加入音画同步任务，使用常见的音频跨注意力层并提出focus-by-predicting策略，促使模型隐式将音频与面部区域关联；(3) 推理时设计time-adaptive Classifier-Free Guidance，在去噪步骤中动态调整引导权重以实现细粒度多模态控制。

Result: 大量实验证明HuMo在主体保持和音画同步等子任务上超过专门的SOTA方法，同时作为统一框架能在多模态控制精细性和灵活性上表现优越。

Conclusion: HuMo提出了一个统一的人类中心视频生成框架，通过构建配对的多模态数据集和两阶段渐进式训练策略，有效协调文本、参考图像和音频三类条件，解决了样本稀缺与子任务协同困难问题。实验显示在主体保持和音画同步等子任务上优于专门方法，证明了其在多模态可控视频生成领域的有效性与泛化性。

Abstract: Human-Centric Video Generation (HCVG) methods seek to synthesize human videos
from multimodal inputs, including text, image, and audio. Existing methods
struggle to effectively coordinate these heterogeneous modalities due to two
challenges: the scarcity of training data with paired triplet conditions and
the difficulty of collaborating the sub-tasks of subject preservation and
audio-visual sync with multimodal inputs. In this work, we present HuMo, a
unified HCVG framework for collaborative multimodal control. For the first
challenge, we construct a high-quality dataset with diverse and paired text,
reference images, and audio. For the second challenge, we propose a two-stage
progressive multimodal training paradigm with task-specific strategies. For the
subject preservation task, to maintain the prompt following and visual
generation abilities of the foundation model, we adopt the minimal-invasive
image injection strategy. For the audio-visual sync task, besides the commonly
adopted audio cross-attention layer, we propose a focus-by-predicting strategy
that implicitly guides the model to associate audio with facial regions. For
joint learning of controllabilities across multimodal inputs, building on
previously acquired capabilities, we progressively incorporate the audio-visual
sync task. During inference, for flexible and fine-grained multimodal control,
we design a time-adaptive Classifier-Free Guidance strategy that dynamically
adjusts guidance weights across denoising steps. Extensive experimental results
demonstrate that HuMo surpasses specialized state-of-the-art methods in
sub-tasks, establishing a unified framework for collaborative
multimodal-conditioned HCVG. Project Page:
https://phantom-video.github.io/HuMo.

</details>


### [36] [MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models](https://arxiv.org/abs/2509.08538)
*Garry Yang,Zizhe Chen,Man Hon Wong,Haoyu Lei,Yongqiang Chen,Zhenguo Li,Kaiwen Zhou,James Cheng*

Main category: cs.CV

TL;DR: MESH是一个基于问答、自下而上的基准，专门用于检测LVM在视频理解中的幻觉，结果显示模型在细节和复杂多主体动作对齐上更易产生幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视频幻觉基准过度依赖人工对视频内容的分类，未能反映人类感知解读视频的过程；因此需要一个能更加系统、贴合人类感知的基准来评估LVM的幻觉行为。

Method: MESH采用问答框架，包含二元和多选题，设置目标（target）与诱饵（trap）实例，以自下而上的层级评估：从基本物体、粗到细的主体特征，到主体-动作对。通过这些任务，系统化检测模型在不同粒度上的错误与幻觉倾向。

Result: 实验显示，LVM在基础物体和特征识别任务上表现良好，但在更精细的特征判定和需要跨主体、跨动作对齐的长视频理解任务上，幻觉问题显著上升，表明当前模型对细节与复杂时序关系的建模仍不足。

Conclusion: 该论文提出了MESH基准用于系统评估大视频模型（LVM）中的幻觉问题，尤其强调了比现有基准更贴近人类感知的自下而上评估流程。作者发现LVM在识别基本物体和粗略特征上表现良好，但在处理细节或多主体多动作的长视频时幻觉显著增加。

Abstract: Large Video Models (LVMs) build on the semantic capabilities of Large
Language Models (LLMs) and vision modules by integrating temporal information
to better understand dynamic video content. Despite their progress, LVMs are
prone to hallucinations-producing inaccurate or irrelevant descriptions.
Current benchmarks for video hallucination depend heavily on manual
categorization of video content, neglecting the perception-based processes
through which humans naturally interpret videos. We introduce MESH, a benchmark
designed to evaluate hallucinations in LVMs systematically. MESH uses a
Question-Answering framework with binary and multi-choice formats incorporating
target and trap instances. It follows a bottom-up approach, evaluating basic
objects, coarse-to-fine subject features, and subject-action pairs, aligning
with human video understanding. We demonstrate that MESH offers an effective
and comprehensive approach for identifying hallucinations in videos. Our
evaluations show that while LVMs excel at recognizing basic objects and
features, their susceptibility to hallucinations increases markedly when
handling fine details or aligning multiple actions involving various subjects
in longer videos.

</details>


### [37] [ViewSparsifier: Killing Redundancy in Multi-View Plant Phenotyping](https://arxiv.org/abs/2509.08550)
*Robin-Nico Kampa,Fabian Deuser,Konrad Habel,Norbert Oswald*

Main category: cs.CV

TL;DR: 通过在多视角图像中随机选择视角并学习视角不变嵌入，ViewSparsifier在植物年龄和叶片计数任务中取得冠军，且扩展到更多视角作为未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 单视角分类/回归模型难以捕捉植物表型所需的全部信息，尤其在多视角拍摄导致大量重叠与冗余信息时，亟需一种能学习视角不变嵌入的多视角方法以提升表型估计准确性。

Method: 方法基于多视图输入，使用一个称为selection vector的机制在24个视角中随机选择视角进行训练，旨在减少信息冗余并学习视角不变表示；此外还探索了在5个高度层（共120视角）上进行随机选择的selection matrices以进一步提升性能或作为未来方向。

Result: 提出的ViewSparsifier在GroMo Grand Challenge（ACM Multimedia 2025）中分别在Plant Age Prediction和Leaf Count Estimation两项任务中获得第一名；并显示通过随机选择视角（24视角或扩展到120视角的随机矩阵）可有效降低冗余并提升模型表现。

Conclusion: 本文提出了ViewSparsifier方法，通过对多视角数据进行随机视角选择并学习视角不变嵌入，从而提高了植物年龄预测和叶片计数两项任务的性能，在GroMo挑战中取得了双项冠军。

Abstract: Plant phenotyping involves analyzing observable characteristics of plants to
better understand their growth, health, and development. In the context of deep
learning, this analysis is often approached through single-view classification
or regression models. However, these methods often fail to capture all
information required for accurate estimation of target phenotypic traits, which
can adversely affect plant health assessment and harvest readiness prediction.
To address this, the Growth Modelling (GroMo) Grand Challenge at ACM Multimedia
2025 provides a multi-view dataset featuring multiple plants and two tasks:
Plant Age Prediction and Leaf Count Estimation. Each plant is photographed from
multiple heights and angles, leading to significant overlap and redundancy in
the captured information. To learn view-invariant embeddings, we incorporate 24
views, referred to as the selection vector, in a random selection. Our
ViewSparsifier approach won both tasks. For further improvement and as a
direction for future research, we also experimented with randomized view
selection across all five height levels (120 views total), referred to as
selection matrices.

</details>


### [38] [Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation](https://arxiv.org/abs/2509.08570)
*Wenjun Yu,Yinchen Zhou,Jia-Xuan Jiang,Shubin Zeng,Yuee Li,Zhong Wang*

Main category: cs.CV

TL;DR: 通过EM聚合将像素特征聚为语义中心并用文本引导的像素解码器弥合语义鸿沟，从而改善多模态模型在医学图像分割任务的泛化性并优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 观察到现有多模态模型在医学域表现不足，主要源于抽象文本提示与细粒度医学视觉特征之间存在显著语义差距以及由此导致的特征离散，因而从语义聚合角度出发提高跨模态对应。

Method: 提出EM聚合机制（基于期望-最大化的动态聚类，把像素特征聚合为紧凑的语义中心以减少特征扩散）与文本引导像素解码器（利用域不变的文本知识引导深层视觉特征），两者协同用于多模态融合。

Result: 在公开的心脏和眼底数据集上，方法在多种域泛化基准上持续优于现有SOTA，表明提出的EM聚合和文本引导像素解码器能显著提升医学图像分割的泛化性能。

Conclusion: 本文提出通过语义聚合改善多模态模型在医学图像分割上的泛化能力，使用EM聚合机制减小特征离散，并用文本引导的像素解码器弥合文本与细粒度视觉特征的语义差距，从而提升跨模态对应和模型鲁棒性。

Abstract: Multimodal models have achieved remarkable success in natural image
segmentation, yet they often underperform when applied to the medical domain.
Through extensive study, we attribute this performance gap to the challenges of
multimodal fusion, primarily the significant semantic gap between abstract
textual prompts and fine-grained medical visual features, as well as the
resulting feature dispersion. To address these issues, we revisit the problem
from the perspective of semantic aggregation. Specifically, we propose an
Expectation-Maximization (EM) Aggregation mechanism and a Text-Guided Pixel
Decoder. The former mitigates feature dispersion by dynamically clustering
features into compact semantic centers to enhance cross-modal correspondence.
The latter is designed to bridge the semantic gap by leveraging
domain-invariant textual knowledge to effectively guide deep visual
representations. The synergy between these two mechanisms significantly
improves the model's generalization ability. Extensive experiments on public
cardiac and fundus datasets demonstrate that our method consistently
outperforms existing SOTA approaches across multiple domain generalization
benchmarks.

</details>


### [39] [Improving Greenland Bed Topography Mapping with Uncertainty-Aware Graph Learning on Sparse Radar Data](https://arxiv.org/abs/2509.08571)
*Bayu Adhi Tama,Homayra Alam,Mostafa Cham,Omar Faruque,Jianwu Wang,Vandana Janeja*

Main category: cs.CV

TL;DR: GraphTopoNet fuses surface observables into spatial graphs, uses hybrid loss and Monte Carlo dropout to handle sparse radar data and uncertainty, achieving up to 60% error reduction in Greenland bed mapping versus baselines.


<details>
  <summary>Details</summary>
Motivation: Sparse, uneven radar observations limit accurate mapping of Greenland's bed; need methods that leverage heterogeneous surface data, handle data gaps, and quantify uncertainty for better sea-level projections and operational models.

Method: Construct spatial graphs from surface observables augmented with gradient features and polynomial trends; train with a hybrid loss combining confidence-weighted radar supervision and dynamically balanced regularization; model uncertainty via Monte Carlo dropout.

Result: On three Greenland subregions, GraphTopoNet reduces error by up to 60% compared to interpolation, convolutional, and graph baselines, while preserving fine-scale glacial features and improving map reliability for operational modeling.

Conclusion: GraphTopoNet is an effective graph-learning approach that fuses heterogeneous surface data and radar supervision with uncertainty modeling, producing more accurate Greenland subglacial bed maps and improving operational modeling reliability.

Abstract: Accurate maps of Greenland's subglacial bed are essential for sea-level
projections, but radar observations are sparse and uneven. We introduce
GraphTopoNet, a graph-learning framework that fuses heterogeneous supervision
and explicitly models uncertainty via Monte Carlo dropout. Spatial graphs built
from surface observables (elevation, velocity, mass balance) are augmented with
gradient features and polynomial trends to capture both local variability and
broad structure. To handle data gaps, we employ a hybrid loss that combines
confidence-weighted radar supervision with dynamically balanced regularization.
Applied to three Greenland subregions, GraphTopoNet outperforms interpolation,
convolutional, and graph-based baselines, reducing error by up to 60 percent
while preserving fine-scale glacial features. The resulting bed maps improve
reliability for operational modeling, supporting agencies engaged in climate
forecasting and policy. More broadly, GraphTopoNet shows how graph machine
learning can convert sparse, uncertain geophysical observations into actionable
knowledge at continental scale.

</details>


### [40] [Implicit Shape-Prior for Few-Shot Assisted 3D Segmentation](https://arxiv.org/abs/2509.08580)
*Mathilde Monvoisin,Louise Piecuch,Blanche Texier,Cédric Hémon,Anaïs Barateau,Jérémie Huet,Antoine Nordez,Anne-Sophie Boureau,Jean-Claude Nunes,Diana Mateus*

Main category: cs.CV

TL;DR: 提出一种基于隐式形状先验并结合自动切片选择的交互式多器官三维分割框架，能在稀疏切片标注下减少专家工作量，实验证明在放疗器官和肌肉萎缩数据集构建两类任务上有效。


<details>
  <summary>Details</summary>
Motivation: 动机是减少放疗规划和肌肉萎缩等医学场景中对繁重、耗时且易受主观影响的人工体积分割工作的依赖，特别是在尚无法完全自动化的复杂三维分割任务中，通过最少的人为交互达到可靠分割结果。

Method: 方法包含两部分：1）基于隐式形状先验（implicit shape prior）的体积分割模型，将多器官建模为隐式函数，利用稀疏切片标注进行体积重建和分割；2）一个简单的信息性切片选择策略，用于自动选择最有助于提高分割精度的下一次人工交互切片，从而将交互次数最小化。

Result: 在两类医学任务上进行了验证：脑癌患者的危及器官（organs at risk）辅助分割，以及肌少症患者的新肌肉形状数据库构建。实验表明方法在保持分割精度的同时显著减少了必要的手工切片标注和交互次数，加速了数据集构建。

Conclusion: 该论文提出了一种基于隐式形状先验的交互式多器官三维分割方法，能在稀疏切片人工标注下高效推断完整体积分割，显著减少医学专家的手工工作量。

Abstract: The objective of this paper is to significantly reduce the manual workload
required from medical professionals in complex 3D segmentation tasks that
cannot be yet fully automated. For instance, in radiotherapy planning, organs
at risk must be accurately identified in computed tomography (CT) or magnetic
resonance imaging (MRI) scans to ensure they are spared from harmful radiation.
Similarly, diagnosing age-related degenerative diseases such as sarcopenia,
which involve progressive muscle volume loss and strength, is commonly based on
muscular mass measurements often obtained from manual segmentation of medical
volumes. To alleviate the manual-segmentation burden, this paper introduces an
implicit shape prior to segment volumes from sparse slice manual annotations
generalized to the multi-organ case, along with a simple framework for
automatically selecting the most informative slices to guide and minimize the
next interactions. The experimental validation shows the method's effectiveness
on two medical use cases: assisted segmentation in the context of at risks
organs for brain cancer patients, and acceleration of the creation of a new
database with unseen muscle shapes for patients with sarcopenia.

</details>


### [41] [EfficientIML: Efficient High-Resolution Image Manipulation Localization](https://arxiv.org/abs/2509.08583)
*Jinhan Li,Haoyang He,Lei Xie,Jiangning Zhang*

Main category: cs.CV

TL;DR: 构建高分辨率扩散篡改数据并提出基于EfficientRWKV的轻量级三阶段模型，实现在定位精度与实时推理间的优越折中。


<details>
  <summary>Details</summary>
Motivation: 现有取证检测器缺乏对扩散生成伪造的暴露，且高分辨率带来计算资源瓶颈，需要轻量且高效的检测方法以满足实时应用。

Method: 构建了1200+高分辨率SIF（扩散生成）篡改数据，使用语义掩码；提出EfficientIML，采用三阶段EfficientRWKV骨干（混合状态空间与注意力），并结合多尺度监督策略。

Result: 在自建数据集和标准基准上，EfficientIML在定位性能、FLOPs和推理速度上均优于ViT系和其他轻量基线，适用于实时取证。

Conclusion: 该工作提出了高分辨率扩散生成篡改数据集和轻量化模型EfficientIML，能更好地检测扩散式伪造并具备实时推理能力。

Abstract: With imaging devices delivering ever-higher resolutions and the emerging
diffusion-based forgery methods, current detectors trained only on traditional
datasets (with splicing, copy-moving and object removal forgeries) lack
exposure to this new manipulation type. To address this, we propose a novel
high-resolution SIF dataset of 1200+ diffusion-generated manipulations with
semantically extracted masks. However, this also imposes a challenge on
existing methods, as they face significant computational resource constraints
due to their prohibitive computational complexities. Therefore, we propose a
novel EfficientIML model with a lightweight, three-stage EfficientRWKV
backbone. EfficientRWKV's hybrid state-space and attention network captures
global context and local details in parallel, while a multi-scale supervision
strategy enforces consistency across hierarchical predictions. Extensive
evaluations on our dataset and standard benchmarks demonstrate that our
approach outperforms ViT-based and other SOTA lightweight baselines in
localization performance, FLOPs and inference speed, underscoring its
suitability for real-time forensic applications.

</details>


### [42] [CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging](https://arxiv.org/abs/2509.08618)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Shahrooz Faghihroohi,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: CLAPS通过CLIP预训练、GroundingDINO自动提示与模态签名文本提示，构建了一个自动化、统一的视网膜影像分割流水线，表现接近或优于专用模型并超越多数基线。


<details>
  <summary>Details</summary>
Motivation: 解决当前SAM在医学（视网膜）分割中的三大问题：文本疾病描述的模态歧义、依赖人工提示、及缺乏统一跨模态任务框架。

Method: 1) 在大规模多模态视网膜数据集上用CLIP进行图像编码器预训练；2) 使用GroundingDINO自动检测局部病变并生成空间边界框提示；3) 为每种成像模态设计独特的“模态签名”并将其融入文本提示以消除文本描述模态歧义；4) 将自动生成的文本和空间提示同时输入SAM以获得最终分割。

Result: 在12个数据集、11个分割类别上广泛实验，CLAPS在大多数指标上超越现有基线，并与专用专家模型性能持平，显示出良好的泛化性和作为基础模型的潜力。

Conclusion: CLAPS提出了一个针对视网膜影像的统一分割框架，通过CLIP预训练、GroundingDINO自动生成空间提示和为每种模态设计的模态签名文本提示，最终驱动SAM实现自动化分割，达到或优于专用模型性能。

Abstract: Recent advancements in foundation models, such as the Segment Anything Model
(SAM), have significantly impacted medical image segmentation, especially in
retinal imaging, where precise segmentation is vital for diagnosis. Despite
this progress, current methods face critical challenges: 1) modality ambiguity
in textual disease descriptions, 2) a continued reliance on manual prompting
for SAM-based workflows, and 3) a lack of a unified framework, with most
methods being modality- and task-specific. To overcome these hurdles, we
propose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method for
unified segmentation across diverse tasks and modalities in retinal imaging.
Our approach begins by pre-training a CLIP-based image encoder on a large,
multi-modal retinal dataset to handle data scarcity and distribution imbalance.
We then leverage GroundingDINO to automatically generate spatial bounding box
prompts by detecting local lesions. To unify tasks and resolve ambiguity, we
use text prompts enhanced with a unique "modality signature" for each imaging
modality. Ultimately, these automated textual and spatial prompts guide SAM to
execute precise segmentation, creating a fully automated and unified pipeline.
Extensive experiments on 12 diverse datasets across 11 critical segmentation
categories show that CLAPS achieves performance on par with specialized expert
models while surpassing existing benchmarks across most metrics, demonstrating
its broad generalizability as a foundation model.

</details>


### [43] [AdsQA: Towards Advertisement Video Understanding](https://arxiv.org/abs/2509.08621)
*Xinwei Long,Kai Tian,Peng Xu,Guoli Jia,Jingxuan Li,Sa Yang,Yihua Shao,Kaiyan Zhang,Che Jiang,Hao Xu,Yang Liu,Jiaheng Ma,Bowen Zhou*

Main category: cs.CV

TL;DR: 提出AdsQA广告视频问答基准与ReAd-R强化学习反思模型，验证LLM在广告语义理解上的能力并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 广告视频信息密集、包含营销逻辑、说服策略和受众互动等线索，是检验LLM超越客观视觉内容、理解更高阶语义与策略性信息能力的理想试验场。

Method: 构建AdsQA数据集（1,544个广告视频，10,962个片段，5种任务），设计ReAd-R——一种基于Deepseek-R1风格的强化学习模型，采用反思（reflect）机制和奖励优化来生成答案；对14个顶级LLM进行基准测试。

Result: ReAd-R在AdsQA基准上取得SOTA，明显优于其他具备长链推理能力的强基线；数据集规模22.7小时，提供多样且具有挑战性的任务。

Conclusion: 本文提出使用广告视频作为测试基准，生成AdsQA数据集并提出ReAd-R模型，通过奖励驱动优化提高LLM在广告视频理解与问答任务上的表现，取得领先结果。

Abstract: Large language models (LLMs) have taken a great step towards AGI. Meanwhile,
an increasing number of domain-specific problems such as math and programming
boost these general-purpose models to continuously evolve via learning deeper
expertise. Now is thus the time further to extend the diversity of specialized
applications for knowledgeable LLMs, though collecting high quality data with
unexpected and informative tasks is challenging. In this paper, we propose to
use advertisement (ad) videos as a challenging test-bed to probe the ability of
LLMs in perceiving beyond the objective physical content of common visual
domain. Our motivation is to take full advantage of the clue-rich and
information-dense ad videos' traits, e.g., marketing logic, persuasive
strategies, and audience engagement. Our contribution is three-fold: (1) To our
knowledge, this is the first attempt to use ad videos with well-designed tasks
to evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmark
derived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing
5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model that
reflects on questions, and generates answers via reward-driven optimization.
(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achieves
the state-of-the-art outperforming strong competitors equipped with long-chain
reasoning capabilities by a clear margin.

</details>


### [44] [UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation](https://arxiv.org/abs/2509.08624)
*Zhihao Zhao,Yinzheng Zhao,Junjie Yang,Xiangtong Yao,Quanmin Liang,Daniel Zapp,Kai Huang,Nassir Navab,M. Ali Nasseri*

Main category: cs.CV

TL;DR: 通过在无配对OCT中学习病灶位置矩阵并在仅用眼底图像时作为空间先验注入，UOPSL显著提升了眼科疾病分类效果，尤其在数据稀缺的OCT场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 获取配对多模态眼科影像（尤其是OCT）成本高且不易获得，单一模态（眼底或文本）难以捕捉细粒度空间信息，而OCT包含关于病灶位置的重要线索。故希望利用无配对OCT的大量空间先验来增强眼底图像诊断。

Method: 先用对比学习在大规模无配对OCT与眼底图像上进行训练，同时在OCT特征空间中学习并优化病灶倾向位点矩阵；在下游仅眼底图像的分类微调/推理阶段，去除OCT输入，使用该矩阵作为先验辅助眼底图像特征学习。

Result: 在9个数据集、28个重要类别上的大量实验显示，UOPSL在各种基准上优于现有方法，表明该无配对多模态空间先验策略有效提升眼底图像的疾病识别性能。

Conclusion: 本文提出的UOPSL框架通过在无配对的OCT与眼底图像间构建基于文本扩展的桥接，并在OCT潜在空间中学习病灶倾向位点矩阵，从而在仅有眼底图像时也能利用OCT衍生的空间先验改善疾病识别。

Abstract: Significant advancements in AI-driven multimodal medical image diagnosis have
led to substantial improvements in ophthalmic disease identification in recent
years. However, acquiring paired multimodal ophthalmic images remains
prohibitively expensive. While fundus photography is simple and cost-effective,
the limited availability of OCT data and inherent modality imbalance hinder
further progress. Conventional approaches that rely solely on fundus or textual
features often fail to capture fine-grained spatial information, as each
imaging modality provides distinct cues about lesion predilection sites. In
this study, we propose a novel unpaired multimodal framework \UOPSL that
utilizes extensive OCT-derived spatial priors to dynamically identify
predilection sites, enhancing fundus image-based disease recognition. Our
approach bridges unpaired fundus and OCTs via extended disease text
descriptions. Initially, we employ contrastive learning on a large corpus of
unpaired OCT and fundus images while simultaneously learning the predilection
sites matrix in the OCT latent space. Through extensive optimization, this
matrix captures lesion localization patterns within the OCT feature space.
During the fine-tuning or inference phase of the downstream classification task
based solely on fundus images, where paired OCT data is unavailable, we
eliminate OCT input and utilize the predilection sites matrix to assist in
fundus image classification learning. Extensive experiments conducted on 9
diverse datasets across 28 critical categories demonstrate that our framework
outperforms existing benchmarks.

</details>


### [45] [LADB: Latent Aligned Diffusion Bridges for Semi-Supervised Domain Translation](https://arxiv.org/abs/2509.08628)
*Xuqin Wang,Tao Wu,Yanfeng Zhang,Lu Liu,Dong Wang,Mingwei Sun,Yongliang Wang,Niclas Zeller,Daniel Cremers*

Main category: cs.CV

TL;DR: LADB通过在共享潜在空间上对齐源/目标分布、利用部分配对潜在表示训练目标扩散模型，实现了在数据稀缺且配对不完全情况下的可控、高质量样本到样本翻译。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在数据稀缺域难以直接训练，配对数据昂贵或不足，现有无监督方法可控性差，完全监督方法需大量标注；因此需要一种在部分配对条件下既能保持可控性又兼顾多样性的翻译框架。

Method: 提出Latent Aligned Diffusion Bridges（LADB）：先将源域样本通过预训练的源域扩散模型映射到潜在空间，再训练目标域的Latent Aligned Diffusion Model(LADM)在该潜在空间上使用部分配对的潜在表示进行学习；通过混合配对与未配对的潜在-目标耦合实现确定性域映射和可控多目标/多源翻译。

Result: 在部分监督的深度图到图像翻译任务中，LADB优于无监督方法且在保真度和多样性间取得更好平衡；同时成功扩展到多源（深度+分割）与多目标（类别条件风格迁移）任务，展示出适应多样、异质数据的能力。

Conclusion: LADB在部分配对数据下，通过在共享潜在空间对齐源域和目标域分布，实现了半监督样本到样本翻译，解决了数据稀缺场景下可控性与多样性之间的权衡。

Abstract: Diffusion models excel at generating high-quality outputs but face challenges
in data-scarce domains, where exhaustive retraining or costly paired data are
often required. To address these limitations, we propose Latent Aligned
Diffusion Bridges (LADB), a semi-supervised framework for sample-to-sample
translation that effectively bridges domain gaps using partially paired data.
By aligning source and target distributions within a shared latent space, LADB
seamlessly integrates pretrained source-domain diffusion models with a
target-domain Latent Aligned Diffusion Model (LADM), trained on partially
paired latent representations. This approach enables deterministic domain
mapping without the need for full supervision. Compared to unpaired methods,
which often lack controllability, and fully paired approaches that require
large, domain-specific datasets, LADB strikes a balance between fidelity and
diversity by leveraging a mixture of paired and unpaired latent-target
couplings. Our experimental results demonstrate superior performance in
depth-to-image translation under partial supervision. Furthermore, we extend
LADB to handle multi-source translation (from depth maps and segmentation
masks) and multi-target translation in a class-conditioned style transfer task,
showcasing its versatility in handling diverse and heterogeneous use cases.
Ultimately, we present LADB as a scalable and versatile solution for real-world
domain translation, particularly in scenarios where data annotation is costly
or incomplete.

</details>


### [46] [Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network](https://arxiv.org/abs/2509.08661)
*Liangjin Liu,Haoyang Zheng,Pei Zhou*

Main category: cs.CV

TL;DR: DSLNet通过腕心与面部双参考系分别建模手形与轨迹，并用拓扑图卷积与芬斯勒编码器加最优传输融合，有效解决形似手语的几何歧义，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决ISLR中形态相似但语义不同的手语难以区分的问题，归因于手形与运动轨迹的耦合以及单一参考帧的几何歧义。

Method: 设计双流双参考架构：腕心坐标系用于视角不变的手形建模，使用拓扑感知图卷积网络；面部坐标系用于语境感知的轨迹建模，使用基于芬斯勒几何的编码器；两流通过几何驱动的最优传输进行特征融合。

Result: 在WLASL-100、WLASL-300和LSA64数据集上分别达到93.70%、89.97%和99.79%的准确率，参数量显著少于竞争模型，达到新的SOTA。

Conclusion: 提出DSLNet，通过双参考系（腕心和面部）解耦手形与轨迹，采用拓扑感知图卷积和芬斯勒几何编码器，并用几何驱动的最优传输融合，实现更好区分形似手语。

Abstract: Isolated Sign Language Recognition (ISLR) is challenged by gestures that are
morphologically similar yet semantically distinct, a problem rooted in the
complex interplay between hand shape and motion trajectory. Existing methods,
often relying on a single reference frame, struggle to resolve this geometric
ambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a
dual-reference, dual-stream architecture that decouples and models gesture
morphology and trajectory in separate, complementary coordinate systems. Our
approach utilizes a wrist-centric frame for view-invariant shape analysis and a
facial-centric frame for context-aware trajectory modeling. These streams are
processed by specialized networks-a topology-aware graph convolution for shape
and a Finsler geometry-based encoder for trajectory-and are integrated via a
geometry-driven optimal transport fusion mechanism. DSLNet sets a new
state-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the
challenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with
significantly fewer parameters than competing models.

</details>


### [47] [FractalPINN-Flow: A Fractal-Inspired Network for Unsupervised Optical Flow Estimation with Total Variation Regularization](https://arxiv.org/abs/2509.08670)
*Sara Behnamian,Rasoul Khaksarinezhad,Andreas Langer*

Main category: cs.CV

TL;DR: 提出基于递归分形编码器-解码器与变分能量（L1/L2+TV）的无监督光流方法，对高分辨率和少标注场景有效。


<details>
  <summary>Details</summary>
Motivation: 减少对光流真值标注的依赖，同时提升高分辨率和长程运动建模能力；引入分形/递归结构以同时捕获细节与长距离运动。

Method: 核心是Fractal Deformation Network(FDN)，一种受分形自相似启发的递归编码器-解码器，采用嵌套的encoder-decoder结构并保留跳跃连接；训练以变分能量最小化为目标，结合L1和L2数据项（亮度恒常性）与全变差(TV)正则化以保证平滑且保边缘的流场。

Result: 在合成和基准数据集上取得准确、平滑且保边缘的光流估计，尤其在高分辨率数据和注释有限的场景表现优越。

Conclusion: 该论文提出了FractalPINN-Flow，一种无监督高密度光流估计框架，通过连续灰度帧训练无需真值标签，适用于高分辨率与标注稀缺场景。

Abstract: We present FractalPINN-Flow, an unsupervised deep learning framework for
dense optical flow estimation that learns directly from consecutive grayscale
frames without requiring ground truth. The architecture centers on the Fractal
Deformation Network (FDN) - a recursive encoder-decoder inspired by fractal
geometry and self-similarity. Unlike traditional CNNs with sequential
downsampling, FDN uses repeated encoder-decoder nesting with skip connections
to capture both fine-grained details and long-range motion patterns. The
training objective is based on a classical variational formulation using total
variation (TV) regularization. Specifically, we minimize an energy functional
that combines $L^1$ and $L^2$ data fidelity terms to enforce brightness
constancy, along with a TV term that promotes spatial smoothness and coherent
flow fields. Experiments on synthetic and benchmark datasets show that
FractalPINN-Flow produces accurate, smooth, and edge-preserving optical flow
fields. The model is especially effective for high-resolution data and
scenarios with limited annotations.

</details>


### [48] [Multi-Modal Robust Enhancement for Coastal Water Segmentation: A Systematic HSV-Guided Framework](https://arxiv.org/abs/2509.08694)
*Zhen Tian,Christos Anagnostopoulos,Qiyuan Wang,Zhiwei Gao*

Main category: cs.CV

TL;DR: 提出Robust U-Net，利用HSV监督与五个协同模块改善海岸线分割的稳定性与精度，HSV监督贡献最大，整体框架显著降低训练方差并提升分割质量，代码已开源。


<details>
  <summary>Details</summary>
Motivation: 动机是解决传统基于RGB的海岸水体分割在光谱复杂性、边界不规则性与训练不稳定性方面的局限，提升模型在多样海洋环境下的泛化能力与训练鲁棒性。

Method: 方法包括五个协同模块：1) HSV引导的颜色监督——在HSV色域上加入额外损失以稳定色彩判别；2) 基于梯度的海岸线优化——利用图像梯度信息强化边界定位；3) 形态学后处理——去噪与闭合小缺口；4) 海域清理模块——移除小型误分割区域；5) 连通性控制——确保分割结果的连通一致性。通过消融实验评估各模块贡献并展示整体框架的协同效应。

Result: 结果显示：HSV监督为最主要影响因子（影响分数0.85）；完整框架使训练稳定性提升，方差降低约84%；在多个评价指标上均有一致性改进，同时保持计算效率。文中给出详尽消融研究并公开了训练配置与代码实现以利复现。

Conclusion: 该工作提出了一个名为Robust U-Net的系统化增强框架，通过在U-Net训练中引入HSV颜色空间监督及多模态约束，针对海岸线分割不稳定与泛化差的问题，显著提升了分割稳定性与质量。

Abstract: Coastal water segmentation from satellite imagery presents unique challenges
due to complex spectral characteristics and irregular boundary patterns.
Traditional RGB-based approaches often suffer from training instability and
poor generalization in diverse maritime environments. This paper introduces a
systematic robust enhancement framework, referred to as Robust U-Net, that
leverages HSV color space supervision and multi-modal constraints for improved
coastal water segmentation. Our approach integrates five synergistic
components: HSV-guided color supervision, gradient-based coastline
optimization, morphological post-processing, sea area cleanup, and connectivity
control. Through comprehensive ablation studies, we demonstrate that HSV
supervision provides the highest impact (0.85 influence score), while the
complete framework achieves superior training stability (84\% variance
reduction) and enhanced segmentation quality. Our method shows consistent
improvements across multiple evaluation metrics while maintaining computational
efficiency. For reproducibility, our training configurations and code are
available here: https://github.com/UofgCoastline/ICASSP-2026-Robust-Unet.

</details>


### [49] [Computational Imaging for Enhanced Computer Vision](https://arxiv.org/abs/2509.08712)
*Humera Shaikh,Kaur Jashanpreet*

Main category: cs.CV

TL;DR: 本文综述计算成像技术如何增强计算机视觉任务性能，强调面向任务的自适应成像流水线的潜力，并指出当前研究在数据、评估与实时实现方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统成像在复杂场景下采集的视觉数据质量不足，限制了先进CV算法的性能。计算成像通过改进采集与重建过程，有潜力在恶劣条件下恢复或增强信息，从而提升CV系统在实际场景中的表现。

Method: 通过归纳分类现有CI方法（如光场、HDR、去模糊、高速成像、眩光抑制），分析每类方法与核心CV任务的结合方式与案例，比较性能提升的实证结果并讨论实现细节、实验设置与限制。

Result: 总结表明：CI技术在多项CV任务中能显著提升性能，尤其在低光、运动、和高动态范围场景中新颖成像方案能带来大幅增益；但现有研究在通用性、实时性、硬件-算法协同及统一评估上存在不足。

Conclusion: 本文系统综述计算成像技术在计算机视觉任务中的作用，认为CI可提高在低光、运动模糊、高动态范围等挑战场景下的视觉数据质量，从而提升目标检测、深度估计、光流、人脸识别和关键点检测等CV任务的鲁棒性和精度。文章提出面向任务的自适应成像流水线作为未来方向，指出需解决的数据集、评估基准、实时性和硬件协同等挑战。

Abstract: This paper presents a comprehensive survey of computational imaging (CI)
techniques and their transformative impact on computer vision (CV)
applications. Conventional imaging methods often fail to deliver high-fidelity
visual data in challenging conditions, such as low light, motion blur, or high
dynamic range scenes, thereby limiting the performance of state-of-the-art CV
systems. Computational imaging techniques, including light field imaging, high
dynamic range (HDR) imaging, deblurring, high-speed imaging, and glare
mitigation, address these limitations by enhancing image acquisition and
reconstruction processes. This survey systematically explores the synergies
between CI techniques and core CV tasks, including object detection, depth
estimation, optical flow, face recognition, and keypoint detection. By
analyzing the relationships between CI methods and their practical
contributions to CV applications, this work highlights emerging opportunities,
challenges, and future research directions. We emphasize the potential for
task-specific, adaptive imaging pipelines that improve robustness, accuracy,
and efficiency in real-world scenarios, such as autonomous navigation,
surveillance, augmented reality, and robotics.

</details>


### [50] [BcQLM: Efficient Vision-Language Understanding with Distilled Q-Gated Cross-Modal Fusion](https://arxiv.org/abs/2509.08715)
*Sike Xiang,Shuang Chen,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 提出BcQLM：以BreezeCLIP为核心的1.2B参数轻量多模态模型，面向VQA实现高效部署，取得接近标准模型的性能并支持模块化扩展。


<details>
  <summary>Details</summary>
Motivation: 大规模多模态模型在部署时受限于算力、能耗和环境可持续性，亟需轻量高效且性能可比拟于标准模型的方案，以便在实际硬件受限场景中应用。

Method: 构建紧凑的视觉-语言编码器BreezeCLIP，结合Q-Gated多模态语言模块，实现端到端训练与推理；设计模块化结构便于扩展至其他多模态任务；在多个数据集上进行对比实验以验证效果。

Result: 在多个基准数据集上，BcQLM以约1.2B参数显著降低计算成本的同时，性能接近标准规模的MLLM，展现出良好的精度-效率平衡与可扩展性。

Conclusion: 作者提出了轻量级多模态大模型框架BcQLM，以1.2B参数的BreezeCLIP为核心，旨在在资源受限设备上实现端到端视觉问答（VQA）并兼顾精度与效率。

Abstract: As multimodal large language models (MLLMs) advance, their large-scale
architectures pose challenges for deployment in resource-constrained
environments. In the age of large models, where energy efficiency,
computational scalability and environmental sustainability are paramount, the
development of lightweight and high-performance models is critical for
real-world applications. As such, we propose a lightweight MLLM framework for
end-to-end visual question answering. Our proposed approach centres on
BreezeCLIP, a compact yet powerful vision-language encoder optimised for
efficient multimodal understanding. With only 1.2 billion parameters overall,
our model significantly reduces computational cost while achieving performance
comparable to standard-size MLLMs. Experiments conducted on multiple datasets
further validate its effectiveness in balancing accuracy and efficiency. The
modular and extensible design enables generalisation to broader multimodal
tasks. The proposed lightweight vision-language framework is denoted as BcQLM
(BreezeCLIP-enhanced Q-Gated Multimodal Language Model). It offers a promising
path toward deployable MLLMs under practical hardware constraints. The source
code is available at https://github.com/thico0224/BcQLM.

</details>


### [51] [CrowdQuery: Density-Guided Query Module for Enhanced 2D and 3D Detection in Crowded Scenes](https://arxiv.org/abs/2509.08738)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 提出CrowdQuery，通过预测并嵌入包含包围盒尺寸的密度图到Transformer解码器中的查询，改善拥挤场景的2D/3D检测，实验证明能显著提升性能并具普适性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer检测器在拥挤场景下性能受限，密度图能提供物体分布信息，但传统密度定义仅依赖头部位置或空间统计，未利用包围盒尺度。作者希望利用密度信息改善查询初始化和解码，以提升拥挤场景下的检测准确性，并实现2D/3D统一方法。

Method: 设计了CrowdQuery(CQ)模块：先预测对象密度图并将其嵌入，然后将嵌入的密度信息系统性地融入Transformer解码器中；扩展传统密度定义以包含单个目标的包围盒尺寸；基于密度引导的查询改进拥挤场景检测；提出CQ2D和CQ3D架构，分别在2D/3D检测器中集成CQ。

Result: 在STCrowd的2D和3D实验中，集成CQ的模型相比基线有显著提升，并超过多数SOTA方法；在CrowdHuman上将CQ集成到最先进的拥挤检测器中也带来进一步性能提升，展示了良好的泛化性。

Conclusion: 该论文提出了一种将密度图信息嵌入到基于Transformer的检测器中的方法，从而提升拥挤场景下的端到端目标检测性能，适用于2D和3D；实验证明在STCrowd和CrowdHuman数据集上都能显著提升基线并优于多数SOTA方法。

Abstract: This paper introduces a novel method for end-to-end crowd detection that
leverages object density information to enhance existing transformer-based
detectors. We present CrowdQuery (CQ), whose core component is our CQ module
that predicts and subsequently embeds an object density map. The embedded
density information is then systematically integrated into the decoder.
Existing density map definitions typically depend on head positions or
object-based spatial statistics. Our method extends these definitions to
include individual bounding box dimensions. By incorporating density
information into object queries, our method utilizes density-guided queries to
improve detection in crowded scenes. CQ is universally applicable to both 2D
and 3D detection without requiring additional data. Consequently, we are the
first to design a method that effectively bridges 2D and 3D detection in
crowded environments. We demonstrate the integration of CQ into both a general
2D and 3D transformer-based object detector, introducing the architectures CQ2D
and CQ3D. CQ is not limited to the specific transformer models we selected.
Experiments on the STCrowd dataset for both 2D and 3D domains show significant
performance improvements compared to the base models, outperforming most
state-of-the-art methods. When integrated into a state-of-the-art crowd
detector, CQ can further improve performance on the challenging CrowdHuman
dataset, demonstrating its generalizability. The code is released at
https://github.com/mdaehl/CrowdQuery.

</details>


### [52] [ArgoTweak: Towards Self-Updating HD Maps through Structured Priors](https://arxiv.org/abs/2509.08764)
*Lena Wild,Rafael Valencia,Patric Jensfelt*

Main category: cs.CV

TL;DR: ArgoTweak 是首个提供真实地图先验的三元组数据集，采用双射原子级别地图修改实现可解释的改变注释，显著减少合成先验带来的 sim2real 差距，并推动可扩展的自我改进 HD 地图研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集缺乏包含先验地图、当前地图和传感器数据的三元组，导致研究者只能使用合成先验，产生不一致性并造成显著的 sim2real 差距，需要现实且可解释的先验地图来改进自我验证和自我更新的 HD 映射方法。

Method: 引入双射（bijective）映射框架，将大尺度地图修改分解为原子级别的细粒度变更（map element level），并提供结构化的先验与详细的变更注释；构建数据集 ArgoTweak 并设计基准与基线模型进行对比实验。

Result: 在实验中，使用 ArgoTweak 训练的模型相比于使用合成先验显著减少了 sim2real 差距；消融研究显示结构化先验和详尽的变更注释对性能有显著影响。

Conclusion: ArgoTweak 提出了首个包含现实地图先验（prior maps）、当前地图（current maps）和传感器数据三元组的数据集，通过细粒度、双射的地图元素层级修改框架，提供可解释的地图变更注释与保真度高的未变元素保留，从而显著缩小了合成先验与真实世界之间的 sim2real 差距。

Abstract: Reliable integration of prior information is crucial for self-verifying and
self-updating HD maps. However, no public dataset includes the required triplet
of prior maps, current maps, and sensor data. As a result, existing methods
must rely on synthetic priors, which create inconsistencies and lead to a
significant sim2real gap. To address this, we introduce ArgoTweak, the first
dataset to complete the triplet with realistic map priors. At its core,
ArgoTweak employs a bijective mapping framework, breaking down large-scale
modifications into fine-grained atomic changes at the map element level, thus
ensuring interpretability. This paradigm shift enables accurate change
detection and integration while preserving unchanged elements with high
fidelity. Experiments show that training models on ArgoTweak significantly
reduces the sim2real gap compared to synthetic priors. Extensive ablations
further highlight the impact of structured priors and detailed change
annotations. By establishing a benchmark for explainable, prior-aided HD
mapping, ArgoTweak advances scalable, self-improving mapping solutions. The
dataset, baselines, map modification toolbox, and further resources are
available at https://kth-rpl.github.io/ArgoTweak/.

</details>


### [53] [Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles](https://arxiv.org/abs/2509.08777)
*Eric Slyman,Mehrab Tanjim,Kushal Kafle,Stefan Lee*

Main category: cs.CV

TL;DR: 提出了一种面向多模态的贝叶斯提示集成方法（MMB），通过图像聚类实现动态提示权重分配，显著改善TTI评判器的准确性与校准性，在两个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前使用的多模态大模型作为图像生成评判器存在偏差、自信过度和在不同图像域间表现不一致的问题；传统的提示集成在纯文本任务中有效，但直接应用于多模态文本-图像评估无法很好泛化，因此需要一种针对多模态特性设计的提示集成方法。

Method: 提出Multimodal Mixture-of-Bayesian Prompt Ensembles（MMB），在贝叶斯提示集成框架下加入基于图像特征的聚类模块，使得评判模型能根据样本的视觉特征动态分配提示权重，从而得到更鲁棒的评估结果与更好的不确定性估计。

Result: 在HPSv2和MJBench两个文本图像评估基准上，MMB在与人工注释的一致性以及校准指标方面均优于现有基线，特别是在成对偏好判断的准确率和置信度校准上有显著提升。

Conclusion: MMB通过结合贝叶斯提示集成和图像聚类，显著提升了多模态评判模型在文本生成图像任务中的判断准确性和校准性，解决了传统提示集成方法在多模态场景中泛化不足的问题。

Abstract: Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

</details>


### [54] [An End-to-End Deep Learning Framework for Arsenicosis Diagnosis Using Mobile-Captured Skin Images](https://arxiv.org/abs/2509.08780)
*Asif Newaz,Asif Ur Rahman Adib,Rajit Sahil,Mashfique Mehzad*

Main category: cs.CV

TL;DR: 作者构建了11000+图像数据集，比较CNN与Transformer，Swin Transformer达86%准确率，并结合LIME/Grad-CAM做解释，最终部署为Web工具，适用于资源匮乏地区的砷中毒筛查。


<details>
  <summary>Details</summary>
Motivation: 南亚和东南亚长期饮用砷污染水导致广泛砷中毒，早期皮肤表现重要但常被忽视，尤其在缺乏皮肤科专家的农村地区。基于图像的自动诊断可支持早期发现和及时干预。

Method: 构建包含20类、11000+张图像的数据集；比较多种深度学习模型（CNN与Transformer）；使用LIME与Grad-CAM进行可解释性分析；部署为基于Web的诊断工具并进行外部验证。

Result: Transformer模型优于CNN，Swin Transformer表现最佳，准确率86%。LIME和Grad-CAM显示模型关注病变相关区域，提升临床透明度；在外部验证集上也表现良好，具有一定泛化能力。

Conclusion: 该研究提出了基于手机拍摄皮肤图像的砷中毒诊断端到端框架，表明深度学习可用于无创、可解释的砷中毒筛查，适合资源受限地区的早期检测与干预。

Abstract: Background: Arsenicosis is a serious public health concern in South and
Southeast Asia, primarily caused by long-term consumption of
arsenic-contaminated water. Its early cutaneous manifestations are clinically
significant but often underdiagnosed, particularly in rural areas with limited
access to dermatologists. Automated, image-based diagnostic solutions can
support early detection and timely interventions.
  Methods: In this study, we propose an end-to-end framework for arsenicosis
diagnosis using mobile phone-captured skin images. A dataset comprising 20
classes and over 11000 images of arsenic-induced and other dermatological
conditions was curated. Multiple deep learning architectures, including
convolutional neural networks (CNNs) and Transformer-based models, were
benchmarked for arsenicosis detection. Model interpretability was integrated
via LIME and Grad-CAM, while deployment feasibility was demonstrated through a
web-based diagnostic tool.
  Results: Transformer-based models significantly outperformed CNNs, with the
Swin Transformer achieving the best results (86\\% accuracy). LIME and Grad-CAM
visualizations confirmed that the models attended to lesion-relevant regions,
increasing clinical transparency and aiding in error analysis. The framework
also demonstrated strong performance on external validation samples, confirming
its ability to generalize beyond the curated dataset.
  Conclusion: The proposed framework demonstrates the potential of deep
learning for non-invasive, accessible, and explainable diagnosis of arsenicosis
from mobile-acquired images. By enabling reliable image-based screening, it can
serve as a practical diagnostic aid in rural and resource-limited communities,
where access to dermatologists is scarce, thereby supporting early detection
and timely intervention.

</details>


### [55] [Quantifying Accuracy of an Event-Based Star Tracker via Earth's Rotation](https://arxiv.org/abs/2509.08794)
*Dennis Melamed,Connor Hashemi,Scott McCloskey*

Main category: cs.CV

TL;DR: 通过固定事件相机观测夜空并用地球自转作为高精度真值，与IERS数据对比，证明事件相机可实现约18.5" RMS的星跟踪姿态精度，适用于低成本低延迟方案，代码数据已公开。


<details>
  <summary>Details</summary>
Motivation: 先前研究在真实数据上难以获得准确的地面真实值，限制了对事件相机在星跟踪任务中性能的可信评估。利用地球自转作为高精度可测参照，能提供可靠的基准用于验证。

Method: 将事件相机固定并通过地基望远镜指向夜空，利用地球自转在天球参考系中产生的已知运动作为地面真实值。采集事件流并处理以生成姿态估计，与IERS提供的地球定向测量值进行对比，评估误差。

Result: 事件相机系统实现了18.47角秒的RMS跨轴误差和约78.84角秒的均值误差，结合事件相机在数据稀疏性、高动态范围、低能耗和高更新率等优势，表明其在低成本、低延迟星跟踪应用中的可行性。代码与数据已开源。

Conclusion: 本文证明使用地球自转作为基准，可以对事件相机（EBC）星跟踪系统的姿态估计进行精确评估，实测结果表明该系统在全局均方根（RMS）误差为18.47角秒，平均误差约78.84角秒。

Abstract: Event-based cameras (EBCs) are a promising new technology for star
tracking-based attitude determination, but prior studies have struggled to
determine accurate ground truth for real data. We analyze the accuracy of an
EBC star tracking system utilizing the Earth's motion as the ground truth for
comparison. The Earth rotates in a regular way with very small irregularities
which are measured to the level of milli-arcseconds. By keeping an event camera
static and pointing it through a ground-based telescope at the night sky, we
create a system where the only camera motion in the celestial reference frame
is that induced by the Earth's rotation. The resulting event stream is
processed to generate estimates of orientation which we compare to the
International Earth Rotation and Reference System (IERS) measured orientation
of the Earth. The event camera system is able to achieve a root mean squared
across error of 18.47 arcseconds and an about error of 78.84 arcseconds.
Combined with the other benefits of event cameras over framing sensors (reduced
computation due to sparser data streams, higher dynamic range, lower energy
consumption, faster update rates), this level of accuracy suggests the utility
of event cameras for low-cost and low-latency star tracking. We provide all
code and data used to generate our results:
https://gitlab.kitware.com/nest-public/telescope_accuracy_quantification.

</details>


### [56] [Handling Multiple Hypotheses in Coarse-to-Fine Dense Image Matching](https://arxiv.org/abs/2509.08805)
*Matthieu Vilain,Rémi Giraud,Yannick Berthoumieu,Guillaume Bourmaud*

Main category: cs.CV

TL;DR: 通过在每个尺度保留并传播多个对应候选并将其融入cross-attention，BEAMER提高了密集图像匹配在复杂场景下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有粗到细方法每个尺度仅预测单一对应，在深度断层或目标图像大幅放大时，邻域像素的真实对应常分散，单个假设易导致错误匹配。

Method: 在多尺度的密集匹配框架中使用beam search在每个尺度保留多个候选对应，并将这些候选整合进cross-attention层，形成新架构BEAMER，从而在尺度间传播多重假设。

Result: BEAMER在含深度不连续和强缩放场景下显著优于现有方法，表现更稳健。

Conclusion: 提出在每个尺度为每个源像素预测多个对应假设并保留传播，能更鲁棒地处理深度不连续和大比例缩放情况。

Abstract: Dense image matching aims to find a correspondent for every pixel of a source
image in a partially overlapping target image. State-of-the-art methods
typically rely on a coarse-to-fine mechanism where a single correspondent
hypothesis is produced per source location at each scale. In challenging cases
-- such as at depth discontinuities or when the target image is a strong
zoom-in of the source image -- the correspondents of neighboring source
locations are often widely spread and predicting a single correspondent
hypothesis per source location at each scale may lead to erroneous matches. In
this paper, we investigate the idea of predicting multiple correspondent
hypotheses per source location at each scale instead. We consider a beam search
strategy to propagat multiple hypotheses at each scale and propose integrating
these multiple hypotheses into cross-attention layers, resulting in a novel
dense matching architecture called BEAMER. BEAMER learns to preserve and
propagate multiple hypotheses across scales, making it significantly more
robust than state-of-the-art methods, especially at depth discontinuities or
when the target image is a strong zoom-in of the source image.

</details>


### [57] [GeneVA: A Dataset of Human Annotations for Generative Text to Video Artifacts](https://arxiv.org/abs/2509.08818)
*Jenna Kang,Maria Silva,Patsorn Sangkloy,Kenneth Chen,Niall Williams,Qi Sun*

Main category: cs.CV

TL;DR: 本文发布GeneVA，一个带有丰富人工注释的大规模生成视频伪影数据集，专注于时空伪影的识别与评估，旨在为文本驱动视频生成模型的评估与改进提供基准。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型从静态图像扩展到文本驱动的视频生成，但随机性导致不合理物理现象和时间不一致等伪影，且缺乏针对视频时空复杂性的系统性基准数据集。

Method: 通过从自然语言提示生成的视频中收集样本，并进行丰富的人类注释，构建包含多种时空伪影标注的大规模数据集GeneVA。

Result: 生成了一个用于评估和改进生成视频质量的大规模带注释数据集GeneVA，可用于基准测试模型性能与促进模型改进。

Conclusion: 该论文提出了一个名为GeneVA的大规模数据集，专注于生成视频中的时空伪影，旨在为评估和改进文本驱动视频生成模型提供基准。

Abstract: Recent advances in probabilistic generative models have extended capabilities
from static image synthesis to text-driven video generation. However, the
inherent randomness of their generation process can lead to unpredictable
artifacts, such as impossible physics and temporal inconsistency. Progress in
addressing these challenges requires systematic benchmarks, yet existing
datasets primarily focus on generative images due to the unique spatio-temporal
complexities of videos. To bridge this gap, we introduce GeneVA, a large-scale
artifact dataset with rich human annotations that focuses on spatio-temporal
artifacts in videos generated from natural text prompts. We hope GeneVA can
enable and assist critical applications, such as benchmarking model performance
and improving generative video quality.

</details>


### [58] [RewardDance: Reward Scaling in Visual Generation](https://arxiv.org/abs/2509.08826)
*Jie Wu,Yu Gao,Zilyu Ye,Ming Li,Liang Li,Hanzhong Guo,Jie Liu,Zeyue Xue,Xiaoxia Hou,Wei Liu,Yan Zeng,Weilin Huang*

Main category: cs.CV

TL;DR: RewardDance通过将奖励定义为预测“yes” token的概率，实现在VLM上的天然对齐与两维可扩展（模型与上下文），显著提升多模态生成质量并有效抑制reward hacking与模式崩溃。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP类RM存在架构与输入模态限制，Bradley-Terry损失与VLM的下一个token预测不一致，且RLHF训练易被reward hacking攻击，阻碍RM在视觉生成方向的扩展和性能提升。

Method: 提出生成式奖励范式：把评分建模为模型预测“yes”token的概率，使得奖励与VLM的下一个token预测机制天然对齐；系统性扩展模型参数量至260亿，并将任务指令、参考示例和CoT推理纳入上下文进行训练与评估；在text-to-image、text-to-video和image-to-video上进行广泛实验。

Result: RewardDance在多模态生成任务上显著优于现有方法；大型RM在RL微调过程中保持较高的reward方差，抵抗reward hacking并产生多样且高质量的输出，缓解模式崩溃现象。

Conclusion: RewardDance通过将奖励重新表述为生成模型预测“yes”标记的概率，解决了与VLM架构不对齐的问题，从而实现了可扩展的奖励建模框架，提升了视觉生成任务效果并缓解了reward hacking和模式崩溃问题。

Abstract: Reward Models (RMs) are critical for improving generation models via
Reinforcement Learning (RL), yet the RM scaling paradigm in visual generation
remains largely unexplored. It primarily due to fundamental limitations in
existing approaches: CLIP-based RMs suffer from architectural and input
modality constraints, while prevalent Bradley-Terry losses are fundamentally
misaligned with the next-token prediction mechanism of Vision-Language Models
(VLMs), hindering effective scaling. More critically, the RLHF optimization
process is plagued by Reward Hacking issue, where models exploit flaws in the
reward signal without improving true quality. To address these challenges, we
introduce RewardDance, a scalable reward modeling framework that overcomes
these barriers through a novel generative reward paradigm. By reformulating the
reward score as the model's probability of predicting a "yes" token, indicating
that the generated image outperforms a reference image according to specific
criteria, RewardDance intrinsically aligns reward objectives with VLM
architectures. This alignment unlocks scaling across two dimensions: (1) Model
Scaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context
Scaling: Integration of task-specific instructions, reference examples, and
chain-of-thought (CoT) reasoning. Extensive experiments demonstrate that
RewardDance significantly surpasses state-of-the-art methods in text-to-image,
text-to-video, and image-to-video generation. Crucially, we resolve the
persistent challenge of "reward hacking": Our large-scale RMs exhibit and
maintain high reward variance during RL fine-tuning, proving their resistance
to hacking and ability to produce diverse, high-quality outputs. It greatly
relieves the mode collapse problem that plagues smaller models.

</details>


### [59] [SAFT: Shape and Appearance of Fabrics from Template via Differentiable Physical Simulations from Monocular Video](https://arxiv.org/abs/2509.08828)
*David Stotko,Reinhard Klein*

Main category: cs.CV

TL;DR: 该工作用单目RGB视频结合物理布料仿真与可微分渲染，同时优化几何与外观；通过两项深度正则化显著提升重建精度（误差降低2.64倍），并在约30分钟内生成用于外观估计的高质量变形模型。


<details>
  <summary>Details</summary>
Motivation: 动机是解决单目视频下布料动态场景在三维几何重建与外观估计上的挑战，尤其是单目导致的深度歧义和运动估计不准确，从而实现高质量的物理可渲染模型生成。

Method: 方法上，结合物理驱动的布料仿真与可微分渲染框架，利用单目RGB视频作为输入，通过优化变形参数使仿真结果与观测图像对齐。同时在3D重建中引入了两种新的正则化项以约束深度方向的解空间，最终在优化后进行外观估计以恢复细节纹理。

Result: 结果显示引入正则化项和物理仿真后，3D重建误差相比最近方法降低2.64倍，场景优化耗时约30分钟，恢复出的运动质量足以进行外观估计并从单目视频中还原出清晰细节。

Conclusion: 该论文提出了一种基于单目RGB视频同时重建布料三维几何和估计外观的系统，通过物理仿真与可微分渲染结合，提高了变形和渲染的真实感。引入了两项新的正则化项以缓解单目深度歧义，在重建精度上相较最近方法降低了2.64倍误差，并能在约30分钟场景优化时间内恢复出可用于外观估计的运动与细节。

Abstract: The reconstruction of three-dimensional dynamic scenes is a well-established
yet challenging task within the domain of computer vision. In this paper, we
propose a novel approach that combines the domains of 3D geometry
reconstruction and appearance estimation for physically based rendering and
present a system that is able to perform both tasks for fabrics, utilizing only
a single monocular RGB video sequence as input. In order to obtain realistic
and high-quality deformations and renderings, a physical simulation of the
cloth geometry and differentiable rendering are employed. In this paper, we
introduce two novel regularization terms for the 3D reconstruction task that
improve the plausibility of the reconstruction by addressing the depth
ambiguity problem in monocular video. In comparison with the most recent
methods in the field, we have reduced the error in the 3D reconstruction by a
factor of 2.64 while requiring a medium runtime of 30 min per scene.
Furthermore, the optimized motion achieves sufficient quality to perform an
appearance estimation of the deforming object, recovering sharp details from
this single monocular RGB video.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems](https://arxiv.org/abs/2509.08014)
*Festim Halili,Anila Nuhiji,Diellza Mustafai Veliu*

Main category: cs.DB

TL;DR: Polyglot persistence offers performance and domain alignment benefits for microservices but adds operational and governance costs; apply patterns (saga, event sourcing, outbox) and careful trade-off analysis.


<details>
  <summary>Details</summary>
Motivation: Microservices require managing heterogeneous, distributed data; polyglot persistence is a pragmatic strategy to match database technologies to service needs.

Method: The paper combines theoretical analysis, a comparative framework across database types (relational, document, key-value, column-family, graph), benchmarks, and empirical industry case studies and surveys.

Result: Comparative assessment shows different databases excel on different axes (scalability, consistency, expressiveness, overhead, integration). Case studies show real-world adoption with benefits and operational challenges; architectural patterns help mitigate complexities.

Conclusion: Polyglot persistence in microservices enhances adaptability, performance, and domain fit but increases governance and operational complexity; trade-offs can be managed with patterns like saga, event sourcing, and outbox.

Abstract: Microservices architectures have become the foundation for developing
scalable and modern software systems, but they also bring significant
challenges in managing heterogeneous and distributed data. The pragmatic
solution is polyglot persistence, the deliberate use of several different
database technologies adapted to a given microservice requirement - is one such
strategy. This paper examines polyglot persistence in microservice based
systems. This paper brings together theoretical concepts with evidence from
practical implementations and comparative benchmarks of standard database
platforms. A comparative framework is applied to relational, document,
key-value, column-family and graph databases to assess scalability,
consistency, query expressiveness, operational overhead and integration ease.
Empirical data drawn from industry case studies such as Netflix, Uber, and
Shopify, and survey data illustrate real-life adoption trends and challenges.
These findings demonstrate that polyglot persistence increases adaptability ,
performance , domain alignment but also governance or operational complexity.
To cope with such trade-offs, architectural patterns such as saga workflows,
event sourcing, and outbox integration are discussed.

</details>


### [61] [Infinite Stream Estimation under Personalized $w$-Event Privacy](https://arxiv.org/abs/2509.08387)
*Leilei Du,Peng Cheng,Lei Chen,Heng Tao Shen,Xuemin Lin,Wei Xi*

Main category: cs.DB

TL;DR: 提出面向个性化隐私需求的PWSM，并基于此提出PBD和PBA两种预算策略，既保障(w,ε)-EPDP又显著降低流数据统计估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有的w-事件隐私研究仅考虑对所有用户统一的隐私强度，忽略了用户隐私需求的差异性。为提高估计精度且满足个体化隐私要求，需设计允许不同用户在流数据场景中拥有不同w和ε的机制与预算调度策略。

Method: 设计了Personalized Window Size Mechanism (PWSM) 以支持每个用户在每个时间步保持恒定的个性化窗口大小，并基于此提出两种预算分配策略：Personalized Budget Distribution (PBD) 和 Personalized Budget Absorption (PBA)。PBD在每个时间步至少分配等于上一步消耗的隐私预算，PBA则从前k步吸收预算并可借用后k步预算，以提升当前步的预算。两者均在流估计发布中结合PWSM实施噪声添加以满足(w,ε)-EPDP。

Result: 理论上证明PBD和PBA在满足每个用户的(w,ε)-EPDP条件下，相较于同类统一策略（BD和BA）能降低估计误差。实验证明：在真实数据集上PBD较BD平均减少68%的误差；在合成数据集上PBA较BA平均减少24.9%的误差。

Conclusion: 本文提出了针对分布式流数据中个体差异化隐私需求的个性化w事件隐私保护机制，并证明所提方法在保证隐私的同时能够显著降低估计误差。

Abstract: Streaming data collection is indispensable for stream data analysis, such as
event monitoring. However, publishing these data directly leads to privacy
leaks. $w$-event privacy is a valuable tool to protect individual privacy
within a given time window while maintaining high accuracy in data collection.
Most existing $w$-event privacy studies on infinite data stream only focus on
homogeneous privacy requirements for all users. In this paper, we propose
personalized $w$-event privacy protection that allows different users to have
different privacy requirements in private data stream estimation. Specifically,
we design a mechanism that allows users to maintain constant privacy
requirements at each time slot, namely Personalized Window Size Mechanism
(PWSM). Then, we propose two solutions to accurately estimate stream data
statistics while achieving $w$-event level $\epsilon$ personalized differential
privacy ( ($w$, $\epsilon$)-EPDP), namely Personalized Budget Distribution
(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the
same privacy budget for the next time step as the amount consumed in the
previous release. PBA fully absorbs the privacy budget from the previous $k$
time slots, while also borrowing from the privacy budget of the next $k$ time
slots, to increase the privacy budget for the current time slot. We prove that
both PBD and PBA outperform the state-of-the-art private stream estimation
methods while satisfying the privacy requirements of all users. We demonstrate
the efficiency and effectiveness of our PBD and PBA on both real and synthetic
data sets, compared with the recent uniformity $w$-event approaches, Budget
Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error
than BD on average on real data sets. Besides, our PBA achieves 24.9% less
error than BA on average on synthetic data sets.

</details>


### [62] [SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors](https://arxiv.org/abs/2509.08395)
*Ruoxuan Li,Xiaoyao Zhong,Jiabao Jin,Peng Cheng,Wangze Ni,Lei Chen,Zhitao Shen,Wei Jia,Xiangyu Wang,Xuemin Lin,Heng Tao Shen,Jingkuan Song*

Main category: cs.DB

TL;DR: SINDI通过SIMD加速、顺序倒排表访问和向量剪枝三项优化，解决稀疏向量MIPS中的冗余计算和内存瓶颈，显著提高查询吞吐并在真实数据上验证效果，已落地开源。


<details>
  <summary>Details</summary>
Motivation: 现有倒排/图索引在稀疏向量MIPS中存在大量冗余距离计算与频繁随机内存访问，且稀疏向量的压缩格式阻碍SIMD加速，限制了生产环境中的吞吐与延迟表现。

Method: 提出Sparse Inverted Non-redundant Distance Index (SINDI)，包含三大技术：1) 将稀疏向量表示与倒排索引结合，利用批量SIMD计算内积并消除冗余ID查找；2) 用顺序访问倒排表替代对原向量的随机访问以改善内存访问模式；3) 对向量进行高幅值非零项保留的剪枝以减少计算量并保持准确率。

Result: 在多种真实数据集上评测，SINDI在不同规模、语言与模型下均相较于SEISMIC和PyANNs显著提升QPS。以MsMarco数据集为例，在Recall@50>99%时，单线程QPS提升4.2至26.4倍。已集成到Ant Group的开源库VSAG。

Conclusion: SINDI通过SIMD加速、减少冗余标识符查找、顺序访问倒排表并进行向量剪枝，显著提升稀疏向量MIPS的吞吐和效率，在多数据集上达到或超过现有方法的性能，并已在工程实践中落地。

Abstract: Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path
retrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based
and graph-based algorithms have achieved high search accuracy with practical
efficiency. However, their performance in production environments is often
limited by redundant distance computations and frequent random memory accesses.
Furthermore, the compressed storage format of sparse vectors hinders the use of
SIMD acceleration. In this paper, we propose the sparse inverted non-redundant
distance index (SINDI), which incorporates three key optimizations: (i)
Efficient Inner Product Computation: SINDI leverages SIMD acceleration and
eliminates redundant identifier lookups, enabling batched inner product
computation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses
to original vectors with sequential accesses to inverted lists, substantially
reducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the
high-magnitude non-zero entries of vectors, improving query throughput while
maintaining accuracy. We evaluate SINDI on multiple real-world datasets.
Experimental results show that SINDI achieves state-of-the-art performance
across datasets of varying scales, languages, and models. On the MsMarco
dataset, when Recall@50 exceeds 99%, SINDI delivers single-thread
query-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared
with SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's
open-source vector search library, VSAG.

</details>


### [63] [Un cadre paraconsistant pour l'{é}valuation de similarit{é} dans les bases de connaissances](https://arxiv.org/abs/2509.08433)
*José-Luis Vilchis Medina*

Main category: cs.DB

TL;DR: 本文提出一种在矛盾存在时仍能评估知识相似性的可矛盾框架，包含新度量S^*、超类别Ξ_K^*、矛盾提取器与修复机制，并证明了度量的数学性质；方法可提升冲突知识的鲁棒性与可解释性，但需要更多实验和复杂度分析验证。


<details>
  <summary>Details</summary>
Motivation: 传统相似性度量通常假设知识库一致性，无法有效应对冲突信息。目标是将矛盾显式化并在度量中考虑，以提升在存在冲突时的鲁棒性和可解释性，适用于多智能体等场景。

Method: 作者构建了一个可矛盾框架，定义了新相似性度量S^*，通过对共享属性奖励和对不一致性惩罚来计算相似性；引入可矛盾超类别Ξ_K^*进行层次组织，并设计了矛盾提取器E和修复机制以保证评估的一致性；理论证明了S^*的若干数学性质。

Result: 提出了S^*度量、可矛盾超类别结构、矛盾提取与修复模块，并证明了S^*满足自反、对称、有界等性质；论文声称该方法能更好地管理冲突知识并具有多智能体应用前景，但缺乏实证评价细节。

Conclusion: 该论文提出的基于可矛盾逻辑的相似性评估框架在处理知识库中冲突信息方面具有潜力，理论性质（自反性、对称性、有界性）保证了度量的基本合理性；但论文在实验验证、复杂度分析及与现有方法比较上描述不足。

Abstract: This article proposes a paraconsistent framework for evaluating similarity in
knowledge bases. Unlike classical approaches, this framework explicitly
integrates contradictions, enabling a more robust and interpretable similarity
measure. A new measure $ S^* $ is introduced, which penalizes inconsistencies
while rewarding shared properties. Paraconsistent super-categories $ \Xi_K^* $
are defined to hierarchically organize knowledge entities. The model also
includes a contradiction extractor $ E $ and a repair mechanism, ensuring
consistency in the evaluations. Theoretical results guarantee reflexivity,
symmetry, and boundedness of $ S^* $. This approach offers a promising solution
for managing conflicting knowledge, with perspectives in multi-agent systems.

</details>


### [64] [SQLGovernor: An LLM-powered SQL Toolkit for Real World Application](https://arxiv.org/abs/2509.08575)
*Jie Jiang,Siqi Shen,Haining Xie,Yang Li,Yu Shen,Danqing Huang,Bo Qian,Yinjun Wu,Wentao Zhang,Bin Cui,Peng Chen*

Main category: cs.DB

TL;DR: 提出SQLGovernor：基于大模型、片段化处理和专家引导的混合自学习的SQL工具集，能对SQL进行语法修复、重写与一致性校验，在基准和工业数据上将模型性能提升至多10%。


<details>
  <summary>Details</summary>
Motivation: 现实OLAP场景中SQL查询常出现语法错误、效率问题与语义对齐困难，现有方法无法兼顾细粒度修复与持续学习，且对专家依赖较大，需设计能减少大模型认知负担并能迭代提升的系统。

Method: 引入片段化处理策略（fragment wise processing）用于精细化重写和局部错误修复；结合知识管理的结构化框架；引入基于专家反馈的混合自学习机制，通过DBMS输出分析和规则校验持续改进；功能包括语法校正、查询重写、查询修改和一致性校验。

Result: 在BIRD和BIRD CRITIC基准以及工业数据集上，SQLGovernor使基础模型性能提升最高约10%，并减少对人工专家的依赖；在生产环境部署表现出良好的实用性与效果。

Conclusion: 本论文提出了SQLGovernor，一套基于大模型的SQL工具集，旨在解决现实分析场景中SQL语法错误、效率低下和语义不匹配问题。

Abstract: SQL queries in real world analytical environments, whether written by humans
or generated automatically often suffer from syntax errors, inefficiency, or
semantic misalignment, especially in complex OLAP scenarios. To address these
challenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies
multiple functionalities, including syntax correction, query rewriting, query
modification, and consistency verification within a structured framework
enhanced by knowledge management. SQLGovernor introduces a fragment wise
processing strategy to enable fine grained rewriting and localized error
correction, significantly reducing the cognitive load on the LLM. It further
incorporates a hybrid self learning mechanism guided by expert feedback,
allowing the system to continuously improve through DBMS output analysis and
rule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as
well as industrial datasets, show that SQLGovernor consistently boosts the
performance of base models by up to 10%, while minimizing reliance on manual
expertise. Deployed in production environments, SQLGovernor demonstrates strong
practical utility and effective performance.

</details>
