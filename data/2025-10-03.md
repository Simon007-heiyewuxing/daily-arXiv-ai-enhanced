<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: 本文提出首个基于VCM的零样本视频反演器LVTINO，通过蒸馏的视频潜空间扩散先验和无需自动微分的条件机制，实现在高清恢复任务上既具测量一致性又有平滑时间一致性，性能和效率均超越逐帧图像LDM方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本到图像潜空间扩散模型（LDM）的零样本反演方法在单帧图像上表现优异，但直接逐帧应用到高清视频恢复会导致时间不一致和细节恢复不足，因此需要同时捕捉空间细节与时间依赖性的解法。

Method: 作者将视频潜空间扩散模型蒸馏为快速生成器（VCM），并设计了一种无需自动微分的条件机制用于反演求解；通过少量神经函数评估保持测量一致性并实现平滑时间过渡。

Result: 在多种视频反演问题上，LVTINO在主观感知质量和重建保真度上显著优于逐帧应用图像LDM的方法，同时计算效率更高，成为新的基准。

Conclusion: 该论文提出了LVTINO，一种基于视频一致性模型（VCMs）的零样本/即插即用高分辨率视频恢复反演求解器，克服了单帧图像LDM方法时间不一致的问题，实现了高质量、低计算量的恢复。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 从单张风格图像中提取并投影到文本表示空间的风格向量，注入预训练生成模型，实现精细可控的风格化文本到图像生成；并构建了包含图像、风格标签和文本描述的Style30k-captions数据集用于训练。


<details>
  <summary>Details</summary>
Motivation: 文本难以精确表达细粒度风格，现有文本引导方法难以将风格化参考图像的信息与文本条件对齐，因而需从参考图像中提取可与文本条件对齐的风格表示以增强生成控制能力。

Method: 提出三阶段训练的风格提取与注入方法：训练风格编码器提取单图像风格表示，使用风格投影层将风格表示对齐到文本表示空间，并将投影后的风格向量注入预训练的文本到图像生成模型中以实现风格引导生成。

Result: 构建了Style30k-captions数据集（图像-风格标签-文本描述三元组）用于训练；在实验中展示了利用单图风格参考进行细粒度风格控制的可行性，并在不改动下游生成器结构的情况下提升了生成的风格一致性与可控性。

Conclusion: 该论文提出通过从单一风格参考图像提取细粒度风格表示，并在不改变下游生成模型结构的前提下注入这些表示，以实现精细可控的风格化图像生成。

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 作者构建了EvoStruggle数据集（61.68小时，2793视频，5385挣扎段），将挣扎判定建模为时序动作定位任务。实验表明现有模型可以检测挣扎并在跨任务上有部分泛化（mAP≈34.6%），但跨活动泛化较差（mAP≈19.2%），表明挣扎检测可迁移但仍需改进。


<details>
  <summary>Details</summary>
Motivation: 动机是：识别学习过程中何时用户处于挣扎状态对于优化教学、辅助系统设计与个性化干预至关重要；但现有操作性数据集没有关注挣扎如何随技能进展而演变，因此需要一个专门的数据集来研究挣扎的时空演化及其可检测性与迁移性。

Method: 方法包括：采集76名参与者完成18项任务（分为打结、折纸、七巧板、洗牌四类活动）的录像，每人每项任务重复五次，总计61.68小时、2793段视频，并人为标注5385个挣扎时间段；将挣扎判定定义为时序动作定位问题，通过训练和评估现有Temporal Action Localization模型（如常见的基线）来检测和定位挣扎片段，并在跨任务与跨活动设置上测试模型泛化能力。

Result: 结果显示：现有时序动作定位模型能够学习挣扎线索并进行检测，但泛化能力有限；在跨任务评估上总体mAP为34.56%，跨活动评估为19.24%，表明挣扎概念在任务间有可迁移性但在更广泛活动间仍具挑战性。数据集与代码已公开。

Conclusion: 论文结论是：通过构建包含多任务、多次重复录像及精确标注的“EvoStruggle”数据集，证明了在技能学习过程中“挣扎”片段可以被视作可检测的时序动作并且具一定迁移性；现有时序动作定位模型在跨任务和跨活动泛化上能取得可观但仍有限的性能（跨任务mAP≈34.56%，跨活动mAP≈19.24%），表明挣扎检测既可迁移又具有挑战性，需要进一步改进。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: 用残差U-Net和自回归预训练构建的小型PDE基础模型，在多类未见PDE上以更少参数达到或超越大型Transformer模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为克服现有PDE基础模型（主要是大型Transformer）参数多、计算开销大、泛化性限制等问题，探索一种更小巧且高效的架构作为统一神经算子。

Method: 采用残差U-Net作为基础架构，结合模仿数值求解器行为的自回归预训练策略，在多种流体动力学PDE上进行预训练，并在6个未见下游PDE任务上微调或直接推广。

Result: 在6个未见下游PDE任务上，SPUS在参数显著更少且微调数据极少的条件下仍实现了最先进的泛化性能，展示出高度参数效率与迁移能力。

Conclusion: SPUS提出了一种轻量级残差U-Net框架与自回归预训练相结合的方法，能作为统一神经算子高效求解多类PDE，具有参数高效与良好泛化能力。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo用基于RL的多样性约束与GRPO微调流匹配模型，通过组合奖励有效消除多人生成中的身份复制与计数错误，达到高身份独立性且无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在多人场景下常出现复制脸、身份合并和计数错误，缺乏直接优化身份多样性的训练方法。论文旨在提出可扩展且无需标注的解决方案来恢复并保持多人生成中的身份独立性。

Method: 对流匹配（flow-matching）模型进行微调，采用Group-Relative Policy Optimization (GRPO)。设计了复合奖励：惩罚图内面部相似度、避免跨样本身份重复、强制正确人数、通过人类偏好分数保留视觉保真度。使用单阶段课程训练来稳定复杂性扩展，无需额外标注。

Result: 在DiverseHumans测试集上取得98.6的Unique Face Accuracy和近乎完美的Global Identity Spread，优于开源与专有方法（如Gemini, GPT-Image），同时保持竞争性的感知质量。

Conclusion: 该论文提出了DisCo，一种基于强化学习并带多样性约束的框架，用于解决文本到图像模型在多人生成时的身份崩溃问题，声称在保持视觉质量的同时显著提高身份多样性和人数准确性。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 将查询图像的视觉-语义特征与分层地理嵌入对齐的思路，有效提升了全球视觉地理定位性能，在多数评测指标上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法在学习地理表示和融合视觉语义信息方面仍有改进空间，作者认为通过显式建模地理层级结构并更好地融合语义分割信息可以提升地理定位性能。

Method: 提出两部分方法：1）一种层级地理嵌入表示，将世界建模为多个层级的地理嵌入；2）一种高效融合方法，将查询图像的外观特征与语义分割图融合，得到鲁棒的视觉表征。然后通过对齐视觉表征与地理表征进行定位检索。

Result: 在5个基准数据集上对25个指标进行评测，论文在22个指标上刷新了全时最佳记录，表明方法具有显著优越性。消融实验表明性能提升主要来自地理表征与视觉表征的组合。

Conclusion: 该论文提出将视觉表征与分层地理表征对齐以提升全球图像地理定位效果，实验显示在多数指标上超过了现有SOTA和LVLMs，结论可信且有实际意义。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过基于注意力矩阵奇异值轨迹的聚类采样，XMAS 有效去除LVLM训练数据冗余，在大幅减小训练集的同时保持模型性能并加速训练。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在LVLM上效果不佳，甚至无法超越随机选择；需要一个能有效去冗余、减少训练开销同时保持下游性能的方法。

Method: 理论上证明了在指令微调中，具有相似跨模态注意力矩阵的样本具有相似梯度；基于该洞见，用一个小型代理LVLM微调并提取注意力矩阵的前若干奇异值轨迹，对样本进行聚类，再从每簇中平衡采样构成子集。

Result: 在若干基准上，XMAS 能在保持 LLaVA-1.5-7B 性能不变的前提下，从 LLaVA-665k 與 Vision-Flan 数据集中分别去除 50% 与 85% 的样本，训练加速约 1.2x，比最优基线多减少约 30% 的数据。

Conclusion: 本文提出了首个面向大视觉-语言模型（LVLM）指令微调的数据高效选择方法XMAS，能在保持性能的同时显著剔除训练数据冗余。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception: variational flow matching on VQ latents—learn categorical posteriors, compute continuous velocities—faster training and competitive FID on ImageNet-1k.


<details>
  <summary>Details</summary>
Motivation: Bridge gap between geometric awareness of continuous generative methods and discrete supervision of vector-quantized representations to gain both efficient training and discrete control/uncertainty.

Method: Adapt Variational Flow Matching to VQ-latents by learning categorical posteriors over codebook indices and computing velocity fields in continuous embedding space; allows uncertainty quantification and temperature-controlled generation.

Result: Faster convergence than continuous and discrete flow matching baselines; competitive FID with state-of-the-art models on ImageNet-1k 256x256.

Conclusion: Purrception successfully integrates categorical supervision into continuous variational flow matching, improving training efficiency while maintaining competitive sample quality on ImageNet-1k 256x256.

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 作者提出一种无需初始掩码的端到端多任务条件扩散模型，能从NCCT生成合成CECT并同时分割腔腔与血栓，采用半监督训练，在图像重建与临床分割测量上均超越现有单任务和多阶段方法。


<details>
  <summary>Details</summary>
Motivation: 常规CECT需使用含碘造影剂，存在肾毒性、过敏及环境问题；因此希望从NCCT生成合成CECT以减少造影剂使用，同时希望同时完成解剖分割以获取临床量化指标，克服多阶段方法的误差传播和标签缺失问题。

Method: 将条件扩散模型与多任务学习结合，设计共享编码器与解码器的统一网络结构，无需初始预测掩码，并采用半监督训练以利用缺失分割标签的临床数据。模型在图像合成与分割任务上联合优化，避免多阶段方法的误差累积。

Result: 在264例患者队列上，本方法在图像质量（PSNR 25.61 dB vs 23.80 dB）和分割性能（腔腔Dice 0.89 vs 0.87；血栓Dice 0.53 vs 0.48）上均优于对比模型；临床测量误差也降低（腔径MAE 4.19 mm vs 5.78 mm；血栓面积误差33.85% vs 41.45%）。

Conclusion: 本文提出了一种端到端多任务条件扩散模型（CDM），可同时从非增强CT生成合成CECT图像并分割腹主动脉动脉瘤的腔腔与血栓，显著优于单任务或多阶段方法。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出一个把预训练模型集成到视频分析中的框架，输出时序半结构化数据并构建帧级可查询知识图谱，支持交互式持续学习，降低工程复杂度，提高原型化速度。


<details>
  <summary>Details</summary>
Motivation: 多模态视频内容分析复杂且工程开销大，将开源预训练模型融合到视频等复杂数据中存在挑战；希望提供一种高效原型化手段，减少工程工作，便于快速试验与扩展。

Method: 作者设计了一个候选流水线配方：集成多种开源预训练模型（例如视觉、音频和语言模型），将视频转为时间索引的半结构化表示；随后将该表示转译为帧级索引的知识图谱，支持查询接口和交互式持续学习机制。

Result: 论文展示了该框架的可行性：成功将视频转为时序半结构化数据并构建帧级知识图谱，系统支持可查询性和通过交互引入新知识的持续学习能力，整体提高了多模态分析开发效率。

Conclusion: 该论文提出了一个高效原型化框架，将预训练模型组合成视频多模态分析流水线，并将输出结构化为时序半结构化数据和帧级索引的知识图谱，支持可查询性和持续学习，以便动态加入领域知识。

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT把网站的内置功能抽象成可复用工具，代理通过调用这些工具而非逐步点击，实现更稳健、少步且更少依赖LLM推理的网页自动化。


<details>
  <summary>Details</summary>
Motivation: 现有Web代理在动态布局和长任务中脆弱，人类利用网站提供的高层操作更高效可靠，因而应当让代理学习并调用这些网站已有功能。

Method: 提出一个框架，通过逆向发现网站潜在功能（如搜索、筛选、排序、发布、评论、创建/编辑/删除内容），将其封装为高层工具接口，代理直接调用这些工具以完成任务，减少低层点击/输入的需求。

Result: 在VisualWebArena和WebArena两个基准上，WALT以更少步骤、更少依赖LLM推理取得了更高成功率，展示了更稳健和可泛化的浏览器自动化范式。

Conclusion: WALT通过将网站内置的功能抽象为可调用工具，提高了自动化浏览器任务的鲁棒性和通用性，减少了对逐步UI操作和LLM复杂推理的依赖。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 本文通过多次扰动预测与拓扑一致性约束，以及结合空间与全局对齐的匹配策略，在半监督病理图像分割中有效恢复并保留生物学拓扑结构，提高分割质量。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，尤其是病理图像中目标密集分布时，从未标注数据中提取有意义的语义结构并保持拓扑属性具有挑战性；需要方法区分生物学真实结构与短暂噪声。

Method: 利用随机dropout和训练快照生成多个扰动预测，通过在这些预测间强制拓扑一致性来区分真实结构与噪声。提出一种结合空间重叠与全局结构对齐的新匹配策略，在无标注情况下匹配对应拓扑特征，最小化预测间差异。

Result: 实验表明，该方法能显著减少拓扑错误，产生更稳健、准确的分割结果，利于后续分析。

Conclusion: 该论文提出的拓扑一致性机制可在半监督病理图像分割中有效减少拓扑错误，提升分割的鲁棒性和准确性。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 作者提出Diffusion-LPO，用Plackett-Luce将DPO扩展到listwise偏好数据，使扩散模型在利用排名反馈时比pairwise方法更准确和高效。


<details>
  <summary>Details</summary>
Motivation: 现实中人类反馈常包含隐含的排名信息，比成对比较能更精确地反映偏好，现有DPO主要用于pairwise数据，未充分利用listwise信息；因此需设计适用于扩散模型的listwise优化方法。

Method: 基于Plackett-Luce概率模型，将用户对同一caption下的图像按偏好排序，构建listwise DPO损失，具体为对排名中每对样本施加约束，使高排名样本相对于低排名样本的概率提升；在扩散模型训练中直接优化该目标，无需显式回报模型。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐任务上，Diffusion-LPO在视觉质量和偏好一致性上均优于pairwise DPO基线，显示出更好的偏好对齐能力和泛化性。

Conclusion: 该论文提出了Diffusion-LPO，在扩散模型中将DPO推广到listwise偏好数据，通过Plackett-Luce模型导出listwise DPO目标，鼓励排名中每个样本优于更低排名样本，从而在多项任务上优于pairwise DPO。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge 是一个纯自回归的统一 MLLM，通过 Mixture-of-Transformers 与语义-到-像素离散表示，实现高质量图像生成与强语义对齐，同时保持训练高效。


<details>
  <summary>Details</summary>
Motivation: 现有统一 MLLM 要么通过混合连续嵌入与扩散/流模型生成高质量图像但破坏自回归性，要么通过纯自回归离散视觉 token 统一文本与图像预测但面临语义对齐与像素保真度的权衡。作者旨在构建既保持自回归范式又在理解与生成两方面表现优秀的统一模型。

Method: 在预训练的视觉理解模型上引入生成能力，采用 Mixture-of-Transformers（多分支变换器）来处理理解与生成任务；提出语义到像素的离散表示，结合紧凑的语义 token 与细粒度像素 token，序列长度仅增加约 7.9%。

Result: Bridge 在多种多模态基准上在理解与生成任务中取得了具有竞争力或更好的结果，并且相比先前的统一 MLLM 需要更少的训练数据和更短的训练时间。

Conclusion: Bridge 提出了一种纯自回归统一多模态大模型，通过 Mixture-of-Transformers 架构在单一下一个 token 预测框架内同时支持图像理解与生成，兼顾语义对齐与像素级保真度。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 针对小样本口腔癌照片，采用CNN+贝叶斯变分推断提升泛化与可靠性，真实场景准确率显著优于传统CNN，并能通过不确定性区分错误预测。


<details>
  <summary>Details</summary>
Motivation: 口腔癌早期诊断重要，但在医疗资源匮乏地区存在样本少、基础设施不足和从业人员短缺等问题；传统深度学习在小样本下易过拟合且给出过度自信的点估计，需提高可靠性和泛化性。

Method: 提出将卷积神经网络与贝叶斯深度学习结合，采用变分推断对模型权重进行概率建模，训练时使用智能手机拍摄的彩色照片，评估在三个不同测试集上性能并分析预测不确定性。

Result: 在与训练分布相似的测试集上准确率94%，与传统CNN相当；在真实世界多样化照片数据上，贝叶斯混合模型准确率88%，优于传统CNN的72.94%；且模型在正确分类样本上表现低不确定性，在错分样本上表现高不确定性。

Conclusion: Bayesian-CNN在小样本和分布漂移条件下提高了口腔癌影像分类的可靠性和泛化能力，且能通过不确定性量化区分正确与错误预测，从而增强临床应用价值。

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 提出CADTrans：通过辅助域和一致性策略构建不变特征，并用CMK-MMD对齐难样本，显著提升SFDA效果。


<details>
  <summary>Details</summary>
Motivation: 在SFDA中源域数据不可用，导致难以获得确定性不变特征；现有方法易受难样本和域偏差影响，且缺乏足够多样性表示。

Method: 构建辅助域模块，从中间全局注意力聚合中获得多样化表示；基于辅助域与目标域应用多种一致性策略得到不变特征；设计条件多核最大均值差异(CMK-MMD)用于在类别条件下对齐难易样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等基准上，CADTrans取得显著性能提升。

Conclusion: CADTrans通过引入辅助域和一致性策略构建不变特征表示，并用条件多核MMD对难样本进行对齐，能够在无源域自适应下提升性能。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 利用BLV用户的历史视觉问题作为上下文检索并提示MLLM，可显著提升视觉描述的相关性与用户偏好。


<details>
  <summary>Details</summary>
Motivation: MLLM生成的视觉描述往往过于详尽且与用户实际需求脱节，BLV用户更需要针对性的信息；因此希望通过历史问题来预测并满足用户可能的询问。

Method: 从VizWiz-LF数据集中检索与输入图像视觉上下文相似的历史图像，并用这些图像对应的BLV用户问题作为提示，引导MLLM生成上下文化的描述。

Result: 在92份描述的人工评估中，上下文化描述在76.1%（70/92）情况下能够预测并回答用户问题，并在54.4%（50/92）比较中被评为更优。

Conclusion: 该论文证明通过利用BLV用户历史问题作为上下文，可以引导MLLM生成更符合BLV用户需求的视觉描述，从而提高信息相关性和用户偏好。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 提出ImageNet-Think：基于ImageNet21k、由两款VLM生成的25万图像的多模态“思考-答案”合成数据集，旨在提升VLM显式推理能力并公开数据集与评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在推理透明性与显式思维链条方面能力有限，作者希望通过构建带中间推理过程的数据来提升模型的显式推理能力与可解释性。

Method: 作者使用两种先进的VLM（GLM-4.1V-9B-Thinking与Kimi-VL-A3B-Thinking-2506）对25万张ImageNet21k图片生成每张图像两组“思考-答案”序列，形成包含结构化思考tokens与对应答案的大规模合成数据集。

Result: 生成了一个包含思考步骤与最终描述性答案的合成数据集，并计划公开数据集与评估基准以推动推理/思考型多模态VLM的研究。

Conclusion: 该论文提出了ImageNet-Think，一个基于ImageNet21k的多模态推理合成数据集，旨在通过提供“思考”步骤与答案对来促进带显式推理能力的视觉-语言模型（VLM）研究。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: NPN通过神经网络学习感知矩阵零空间的低维非线性投影，提供可解释且广泛适用的零空间先验，能与多类重建方法结合并提升成像反问题的重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统的图像域先验忽略了感知矩阵零空间的任务特定结构，导致在欠定观测下难以消除零空间带来的模糊和伪影；因此需要针对零空间结构的先验以补足盲点。

Method: 设计了神经网络来学习感知矩阵零空间的低维投影作为先验（NPN），将该先验嵌入plug-and-play方法、展开网络、Deep Image Prior和扩散模型等重建框架；并给出收敛性和重建精度的理论保证。

Result: 在多种感知矩阵和成像任务（CS、去模糊、超分辨、CT、MRI）上，NPN作为先验能稳定提升重建质量，与多种重建方法兼容并表现优于或互补于传统图像域先验。

Conclusion: 作者提出了一种新的正则化方法NPN，通过对感知矩阵零空间进行非线性投影来约束解空间，从而解决成像反问题中的不确定性；该方法可解释性强且具有通用性和可扩展性，并能与多种重建框架结合。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 将CGR与CBM结合，通过概念监督和不确定性校准实现可解释且校准良好的基因组分类，并附带成本感知决策层，提升HIV亚型分类性能与临床/自动化可用性。


<details>
  <summary>Details</summary>
Motivation: 动机在于填补可解释基因组建模与自动化决策之间的鸿沟，使机器和临床系统能基于生物学可验证的证据做出可靠决策。

Method: 方法结合了Chaos Game Representation(CGR)将序列转为图像特征，使用Concept Bottleneck Model(CBM)强制模型通过生物学概念（如GC含量、CpG密度、k-mer模式）进行预测，并引入概念一致性监督、先验一致性对齐、KL分布匹配和不确定性校准以提高可靠性。

Result: 在内部和LANL的HIV亚型数据集上实现了最先进的分类性能、优越的概念预测一致性，并在成本-效益权衡上优于基线；还提供了可验证的生物学证据和成本感知的决策层以降低重复检测。

Conclusion: 该论文提出了一个将原始DNA序列转化为可执行、可解释决策的自动化基因组解释模块，为机器人和临床自动化提供基础。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: 提出VLA-R1：结合可验证奖励的RL后训练和组相对策略优化，配合VLA-CoT-13K链式思考数据，显著提升VLA模型的推理与实际操控表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型缺乏逐步可验证的推理，常直接输出最终动作而忽略可操作性约束与几何关系，且后训练阶段较少强化推理质量，主要依赖弱监督微调。

Method: 提出基于RLVR的后训练策略，设计可验证奖励（区域对齐、轨迹一致性、输出格式化），并引入GRPO以系统优化推理与执行；同时构建VLA-CoT-13K数据集用于连贯思路（chain-of-thought）监督并对齐可操作注释。

Result: 在域内/域外、模拟与真实机器人测试中，VLA-R1在推理鲁棒性、执行准确性和泛化能力上均优于先前VLA方法，作者计划公开模型、代码与数据集。

Conclusion: VLA-R1在引入可验证奖励和组相对策略优化后，显著提升了VLA模型的推理与执行能力，尤其在区域对齐、轨迹一致性与输出格式化方面得到强化，实验显示其在多场景与真实机器人平台上优于现有方法。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 本文提出一个基于可微渲染的自监督联合优化框架，从少量多视图微距模糊图像中同时恢复像素级散焦核、清晰图像和高保真三维外观。


<details>
  <summary>Details</summary>
Motivation: 微距摄影能提供高分辨率和大倍数放大，但散焦模糊严重影响图像清晰度和高质量三维重建；现有去模糊方法依赖大量带标注图像，且缺乏针对微距场景的多视图三维重建方法。

Method: 基于可微渲染（differentiable rendering）自监督框架，逐像素估计散焦模糊核并与三维模型参数共同优化；输入为多视角模糊图像，输出为去模糊后的图像和三维外观模型。

Result: 在少量多视角图像下，方法在图像去模糊和三维外观恢复上均表现良好，实验显示能够获得高质量去模糊结果与高保真三维重建。

Conclusion: 提出了一个用于微距摄影的联合去模糊与三维重建方法，通过对像素级散焦模糊核和三维模型的联合优化，从少量多视图模糊图像中同时恢复清晰图像与高保真三维外观。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff通过重构模糊轨迹并训练一致性模型，实现单步高保真去模糊，结合模糊核估计与自适应时间步预测，在质量和速度间取得平衡，优于此前扩散方法并可用于工业应用。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的去模糊尽管生成能力强，但推理慢且保真度受限，因而提出单步扩散策略以提升效率同时保持高质量恢复。

Method: 将每个时间步定义为不同程度的模糊图像，训练一致性模型使所有时间步的输出对齐到同一干净图像；引入Kernel ControlNet进行模糊核估计并采用自适应时间步预测以提高性能。

Result: 在全参考指标上优于已有扩散类方法，并达到其他最先进模型的性能，提供了适用于工业场景的高保真去模糊基线。

Conclusion: 本文提出了一种名为FideDiff的单步扩散模型用于高保真图像去模糊，通过将运动去模糊重构为扩散样式过程并训练一致性模型，实现一步去模糊并兼顾速度与恢复质量。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 构建大规模跨域长尾青铜铭文数据集，提出基于LadderMoE增强CLIP的两阶段检测-识别方法，显著提升铭文识别的跨域与长尾性能。


<details>
  <summary>Details</summary>
Motivation: 青铜铭文图像存在严重退化、多域（照片、拓片、描图）差异及极度长尾字符分布，传统场景文本识别方法难以满足考古与历史研究需求，因此需要新的数据与模型设计以提升识别准确率与泛化能力。

Method: 构建包含22454页图像和198598字符标注（6658类）的数据集；采用先检测铭文再识别单字的两阶段流程；在CLIP编码器上加入梯式专家混合（LadderMoE）适配器，实现动态专家分工以增强跨域与长尾鲁棒性。

Result: 在单字和整页任务上，提出的方法显著优于现有场景文本识别基线，尤其在头部、中部和尾部分布以及所有采集模态上均取得更高准确率，为铭文识别与下游考古分析奠定基础。

Conclusion: 本文构建了大规模青铜铭文数据集并提出两阶段检测-识别流水线，结合LadderMoE增强的CLIP编码器，有效提升跨域与长尾字符识别性能。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: 通过在固定骨干前插入可训练的视觉重编程层，VirDA实现了低参数、高效的无监督领域自适应，显著减少每域训练参数并保持接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需为每个源-目标对微调整个骨干，导致训练参数和存储按对数线性增长，并且无法重用已训练好的骨干；作者观察到预训练骨干具有纹理偏差，提出利用域特定纹理偏差通过视觉重编程来适配新域，从而实现参数和存储高效。

Method: 在固定预训练骨干的前端加入可训练的域特定视觉重编程层，产生视觉提示（visual prompts）以改变输入图像的“风格”以匹配目标域；通过多目标损失优化视觉提示以最小化域内差异并最大化域间可分性，从而实现领域自适应。

Result: 在Office-31上，VirDA以仅1.5M可训练参数实现92.8%平均准确率，优于参数高效基线PDA（高1.6%且只用其46%参数）；相比完整骨干微调方法，VirDA以极少参数接近或超越多项最先进方法，在参数-性能折中上表现优异。

Conclusion: 该文提出了一种通过视觉重编程（VirDA）实现参数高效的无监督领域自适应（UDA）方法，避免了对骨干网络的微调，能够在保持模型性能的同时大幅减少每个域对训练参数和存储的需求。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出基于 3DMM+RVQ-VAE 的离散面部编码（DFE），无监督学习共享码本 token，能更细粒度、可复用地表示面部动作，在多项心理任务上超越 FACS 与强表示学习基线。


<details>
  <summary>Details</summary>
Motivation: FACS 覆盖有限且人工标注成本高，需要一种可扩展、自动化且可解释的面部表达表征方法以服务心理与情感计算任务。

Method: 使用 3DMM 提取身份不变的表达特征以去除头部姿态与面部几何影响，随后用 Residual Vector Quantized VAE 对这些特征进行编码，生成共享码本的离散 token 序列，每个 token 表示可复用的面部变形模式；最后以 Bag-of-Words 在心理任务上评估性能。

Result: DFE 在压力检测、人格预测与抑郁检测三项任务上均优于基于 FACS 的流水线和如 MAE 的强基线模型；并显示其能覆盖更多样的面部表情模式。

Conclusion: DFE 提供了一种无监督、数据驱动的离散面部表情编码，能更精细地捕捉面部行为，并在若干心理任务上优于 FACS 及主流表示学习方法。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: 提出一种用于共形变形的NRSfM方法，通过图优化与并行可分离迭代策略恢复局部共形尺度并解耦深度与尺度约束，结合自监督网络生成稠密带纹理点云，在合成与真实数据上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有NRSfM方法通常依赖严格假设（如局部平面或线性变形），且无法恢复共形尺度，导致深度估计受限。提出消除这些约束、恢复共形尺度并改进深度估计的新方法。

Method: 基于图的点级重建：通过对选定二维图像块（warp）进行优化，用并行可分离迭代优化策略求解非刚结构重建问题；解耦深度和共形尺度约束；结合自监督的编码器-解码器网络生成带纹理的稠密三维点云。

Result: 在合成和真实数据集上的仿真与实验表明，Con-NRSfM在重建精度和鲁棒性上优于现有方法；论文将公开代码。

Conclusion: 本文提出的Con-NRSfM方法能够在共形变形场景下恢复局部共形尺度并提高深度估计精度，实验表明其在重建精度和鲁棒性上优于现有方法。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 作者用视频扩散模型将不一致多视图图像先修复为一致图像，再进行三维重建，提出的UniVerse在少量视图和真实退化场景下泛化与性能优于以往方法并能控制风格。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图像退化建模直接融入神经三维表示，依赖密集观测来鲁棒优化参数，在观测稀疏或不一致时性能下降。将问题分解可简化优化并提高泛化性。

Method: 提出UniVerse框架：先将多视角不一致图像转换为初始视频序列；使用定制的视频扩散模型对视频进行修复以生成一致图像；最后基于修复后的图像进行三维重建。模型通过大规模数据学习通用图像/场景先验，支持风格控制。

Result: 在合成与真实数据集上，UniVerse在鲁棒重建任务中表现出更好的泛化性和重建性能，同时支持重建风格控制。

Conclusion: 该文提出将鲁棒重建任务分解为恢复与重建两个子任务，通过视频扩散模型学习通用场景先验，从而在少量视图或观测不一致时仍能进行可靠的三维重建。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出PoseMatch-TDCM，通过模板感知动态卷积与旋转-剪切增强实现端到端位置与几何回归，轻量高效，适用于工业实时模板匹配与对齐任务。


<details>
  <summary>Details</summary>
Motivation: 传统模板匹配在角度与尺度上多采用枚举，效率低且难以应对复合变换；深度方法多只输出相似度而不建模姿态，不能满足实际工业部署对位置与几何状态精确估计的需求。

Method: 核心方法包含：1) 模板感知动态卷积模块（TDCM），在推理时注入模板特征以引导匹配；2) 轻量级网络结构，使用深度可分离卷积与像素重排提高效率；3) 无几何标注训练策略，采用基于旋转-剪切的增强与结构感知伪标签；4) 轻量级精调模块通过局部优化提升角度与尺度精度。

Result: 作者提出的3.07M参数模型在复合变换下达到高精度与14ms推理速度；在小模板与多目标场景表现鲁棒，适合实时工业应用。

Conclusion: 该文提出了一种轻量级端到端模板匹配框架（PoseMatch-TDCM），通过联合定位与几何回归直接输出目标中心、旋转角与独立横纵尺度，适应复杂背景与复合变换，适合工业实时部署。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出自适应像素推理：先做操作感知微调再用rollout强化学习动态决定何时调用像素工具，实现更高精度与更低工具使用率。


<details>
  <summary>Details</summary>
Motivation: 动机是当前VLM在处理需要精细视觉细节的任务时受限于图像编码的信息损失或对关键区域关注不足，而直接引入像素级信息虽有助于推理但易导致效率低下和被无关细节干扰，因此需要一种能在必要时才调用像素操作的自适应机制。

Method: 方法包含两阶段：1) 面向操作的有监督微调以建立文本推理与视觉操作基础能力；2) 基于rollout的强化学习框架，利用模型自身响应的反馈来学习在何时触发像素操作，从而根据查询难度自适应调用工具。

Result: 在大规模多模态推理基准上，所提方法在性能和效率上均优于现有方法：例如在HR-Bench 4K上达成73.4%准确率，同时工具使用率仅为20.1%，相较之前方法提升准确率并使工具使用减少66.5%。

Conclusion: 本论文提出了首个自适应像素推理框架，通过在查询难度基础上动态决定是否调用像素级视觉操作，从而在提升细粒度视觉理解能力的同时避免不必要的像素工具滥用。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出ASRS通过旋转增强与RAD-DINO嵌入位移评估样本稳定性，能无标签识别易错CXR案例，召回下降明显，适用于选择性预测与人工复核以提升模型公平与安全。


<details>
  <summary>Details</summary>
Motivation: 现有基于置信度校准或OOD检测的方法难以发现分布内的微妙错误；图像及表示层面的一致性方法在医学影像中未被充分探索。

Method: 构建augmentation-sensitivity risk scoring（ASRS）框架：对CXR图像进行临床可行的旋转（±15°/±30°），使用RAD-DINO编码器提取图像嵌入，度量原图与增强图嵌入的位移并据此计算敏感性得分，将样本按稳定性分为四分位以识别高度敏感（易错）样本。

Result: 高度敏感样本尽管在整体AUROC和置信度上表现良好，但召回率显著下降（-0.2到-0.3）。ASRS能无标签地提升选择性预测和指导临床审查，从而改善公平性和安全性。

Conclusion: ASRS能在无标签情况下识别出易出错的CXR样本，从而用于选择性预测和人工复核，改善模型公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS 是一个训练-free 的视频风格化框架，通过融合多参考、频率补偿与光流引导实现高质量且时间一致的风格化视频。


<details>
  <summary>Details</summary>
Motivation: 解决逐帧图像风格化导致的时间不一致和专用视频风格化模型训练成本高、需配对视频数据的问题。

Method: 将多个风格化参考集成到预训练的图像到视频(I2V)模型，结合高频补偿约束内容布局与运动，并使用基于光流的运动线索保护低显著性区域的风格纹理。

Result: FreeViS 在风格保真度和时间一致性上均优于近期基线方法，并获得较高的人类偏好评分，提供了实用且经济的训练免费视频风格化方案。

Conclusion: FreeViS 提出了一种无需训练的高质量视频风格化方法，能够在保持丰富风格细节的同时提升时间一致性。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench提出感知—推理范式，构建大规模、多模态、语言化的医学图像质量基准与多维评判协议，评估显示现有MLLM在医学IQA上表现初步但不够稳定，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 现有医学IQA以标量分数为主，缺乏描述性、类人推理的评价方法；需要一个能检验并推动MLLM在医学图像质量评估中感知与推理能力的系统化基准。

Method: 提出包含两类任务的基准：MedQ-Perception通过人为策划的低层视觉属性问题评估模型感知能力；MedQ-Reasoning涵盖无参考和比较推理任务，并设计多维度评判协议对输出进行四轴评价；数据涵盖五种模态、超40种质量属性，并包含真实临床、物理退化仿真和AI生成图像；对14个现有MLLMs进行基线测试并与放射科医生进行对齐验证。

Result: 在14个最先进的多模态大模型上测试表明：模型在感知和推理上有初步但不稳定的能力，准确性不足以直接用于临床，提示需针对性优化MLLM以提升医学IQA表现。

Conclusion: MedQ-Bench填补了以往基于标量评分的医学图像质量评估方法与专家式、语言化推理评价之间的差距，通过构建感知-推理范式和多维度评价协议，为多模态大模型在医学IQA的研究提供了系统化基准。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer通过对象查询与潜在掩码描述符交互，在单次前向传递下从RGB图像预测所有实例的遮挡与深度顺序，减少输入与推理成本，并在实验中表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的输入格式（类别标签、二值分割掩码）或多次前向推理（平方级的成本），限制了广泛应用；因此需要一种更高效、仅用RGB输入即可得到实例级几何关系的方案。

Method: 核心为对象查询与潜在掩码描述符之间的交互，前者与后者语义上表示相同对象但携带互补信息；模型设计使得只需一次前向传递即可输出完整的实例顺序。

Result: 通过全面的基准测试和消融实验验证了方法的有效性；作者开源了代码和模型。

Conclusion: 本文提出了一种名为InstaFormer的网络，能够在单次前向传播中仅从RGB图像预测场景中所有实例的遮挡关系和深度顺序，实现了全局实例顺序预测。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler通过金字塔位置编码与强化学习提升Transformer风格迁移的质量与效率，实验中显示显著损失降低与接近实时的推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有CNN与Transformer模型在处理复杂风格及高分辨率输入时的扩展性和效率问题，既要保留局部细节又要兼顾全局结构并提高收敛速度。

Method: 基于Transformer的PyramidStyler引入Pyramidal Positional Encoding（多尺度层次位置编码）以同时捕捉局部与全局信息，并通过强化学习动态优化风格合成过程以加速收敛；在COCO与WikiArt数据集上训练并在4000个epoch后报告损失和推理时间。

Result: 报告在4000个epoch后未使用RL时内容损失下降62.6%至2.07，风格损失下降57.4%至0.86，推理时间1.39s；使用RL时进一步降为内容2.03、风格0.75，推理时间1.40s。

Conclusion: PyramidStyler提出了一个结合金字塔位置编码和强化学习的Transformer风格迁移框架，声称在损失、速度和高分辨率处理方面均有显著改进。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS通过深度感知分区与可见Gaussian负载均衡等优化，将大尺度3D Gaussian Splatting的预处理从数小时降到数分钟，并把训练速度提高近2倍，保持视觉质量并扩展到更大场景。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在大尺度无界场景下受内存与计算瓶颈限制，传统分区方法导致负载不均和粗细阶段切换开销，亟需高效分区与负载均衡机制。

Method: 引入深度感知分区、基于优化的可见Gaussian负载均衡、可视性裁剪与选择性稠密化两种轻量化技术，重构大规模3DGS的流水线以减少预处理与训练开销。

Result: 在城市与户外大场景数据集上，LoBE-GS比最先进基线在端到端训练时间上加速至多2倍，同时保持重建质量并能处理原始3DGS无法胜任的更大场景。

Conclusion: LoBE-GS通过负载均衡与高效分区显著加速大规模3D Gaussian Splatting训练，在保持重建质量的同时将端到端训练时间提升约2倍并支持更大场景的可扩展性。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack（图文联合可学习检索记忆）与Direct Forcing（单步近似强制对齐）以线性复杂度建模长期依赖并减轻自回归推理误差累积，从而提升长视频生成的一致性与可靠性。


<details>
  <summary>Details</summary>
Motivation: Autoregressive long-form video generation struggles with capturing long-range dependencies and suffers from error accumulation during stepwise decoding; the paper aims to address both issues to improve temporal consistency and reliability for minute-level videos.

Method: They introduce MemoryPack, a learnable context-retrieval mechanism combining textual and image cues to jointly model short- and long-term dependencies with linear complexity, and Direct Forcing, a single-step approximation strategy to align training and inference and reduce error propagation.

Result: MemoryPack scales with video length, preserves computational efficiency, and achieves minute-level temporal consistency; Direct Forcing reduces error propagation by improving training-inference alignment; together they substantially enhance context consistency and reliability of autoregressive video models.

Conclusion: This paper presents MemoryPack and Direct Forcing to improve long-form video generation by modeling dynamic context and reducing error accumulation, achieving minute-level temporal consistency and better training-inference alignment.

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 提出针对3D目标检测分类的置信度校准方法：关注完整预测分布，设计两种校准正则化损失，并发现“完整向量正则化+等距回归”在CenterPoint和PillarNet上效果最佳，但对DSVT-Pillar不通用。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等自治系统中，精确的目标检测与不确定性估计对安全运行至关重要。现有工作多关注主导类置信度的校准，而忽视了完整预测分布（含次要类别）的校准，這可能导致系统对次优类别的不确定性评估不可信。作者主张应同时校准主导与次要类别的置信度分布。

Method: 提出两种辅助正则化损失作为训练目标，分别促使主导预测或完整类预测向量校准；并比较多种后处理（post-hoc）与训练时（train-time）校准方法，在CenterPoint、PillarNet和DSVT-Pillar上评估效果。最佳策略为对完整预测向量正则化+等距回归。

Result: 实验表明：1) 对完整预测向量进行正则化的训练损失能改善主导与次要类的校准；2) 对CenterPoint和PillarNet，结合等距回归的训练正则化取得最佳效果；3) DSVT-Pillar无法用同一方法同时校准主导与次要预测，表明不同模型架构对校准方法的适应性不同。

Conclusion: 本文提出了针对3D目标检测分类任务的置信度校准方法，强调需要对完整预测置信度分布（包含主导类与次要类）进行校准，并通过设计指标评估该分布的校准效果。提出两项辅助正则化损失：一种针对主导预测的校准，另一种针对完整预测向量的校准。实验证明，将对完整预测向量进行正则化的损失与等距回归（isotonic regression）结合，在CenterPoint和PillarNet上能同时改进主导类和次要类的校准表现；而对DSVT-Pillar则无法用同一方法同时校准主导与次要预测。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: 通过引入扩散模型先验并解耦检测与重识别，DiffPS设计了DGRPN、MSFRN和SFAN三大模块，显著提升了行人搜索的定位与识别性能，在两个主流数据集上创下新高。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖ImageNet预训练主干网络，难以捕捉复杂空间上下文和细粒度身份线索；且检测与重识别共享主干导致优化目标冲突，限制性能。利用扩散模型的多尺度语义及生成先验可提供更适合行人搜索的表征。

Method: 提出DiffPS框架：引入扩散先验并设计三个模块——1) DGRPN：扩散引导的区域建议网络，用于增强行人定位；2) MSFRN：多尺度频域细化网络，用以减轻形状偏差，增强细粒度特征；3) SFAN：语义自适应特征聚合，利用与文本对齐的扩散特征提升重识别表征；同时通过结构设计消除检测与重识别之间的优化冲突。

Result: 在CUHK-SYSU和PRW数据集上，DiffPS超越了现有方法，达到了新的最好成绩，表明扩散先验和模块化设计可有效提升行人搜索性能。

Conclusion: 本文提出将预训练扩散模型作为先验知识引入到行人搜索任务，通过解耦检测与重识别的优化冲突并设计三大模块提升定位与表征能力。实验证明在CUHK-SYSU和PRW上取得了新的最先进性能。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出首个将流匹配生成先验融入深度展开的HSI重构方法FMU，并通过引入均值速度损失提升流的一致性，在模拟和真实数据上显著超过现有方法。


<details>
  <summary>Details</summary>
Motivation: HSI获取成本高且从压缩测量中重建光谱细节困难。现有CASSI等压缩感知方法仍难以恢复精细光谱与空间细节，需引入更强的先验与可解释恢复框架。

Method: 构建Flow-Matching-guided Unfolding (FMU)网络：在迭代展开的优化步骤中引入流匹配模型作为概率生成先验，并设计均值速度损失(mean velocity loss)以增强流的全局一致性。网络结合了物理测量模型的可解释性和流匹配的生成能力。

Result: 在模拟和真实数据集上的大量实验显示，FMU在重建质量上显著优于现有方法（定量与定性均有提升）。代码与模型将开源。

Conclusion: 本文提出将流匹配(flow matching)生成先验嵌入到深度展开框架中，用于高光谱图像(HSI)重构，证明了混合优化-生成方法能够显著提升重建质量。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 文章通过ConSinGAN扩充数据并以YOLOv7为主的深度学习检测器，实现了对DIP表面与引脚缺陷的高效自动检测，准确率95.50%，适用于样本稀缺场景并配套了SCADA部署方案。


<details>
  <summary>Details</summary>
Motivation: 传统工业元件的缺陷检测耗时耗力且样本不足，难以保证质量检验效率与一致性，故提出自动化、能在少样本下工作的检测方案。

Method: 使用数字相机采集DIP图像，采用ConSinGAN生成缺陷样本扩充数据集，比较YOLOv3/v4/v7/v9四种目标检测模型，并在YOLOv7+ConSinGAN配置下实现最佳性能；同时设计了SCADA系统与传感器架构用于现场部署。

Result: YOLOv7结合ConSinGAN在缺陷检测上取得95.50%准确率，检测时间285 ms，性能显著优于基于阈值的方法与其他YOLO版本。

Conclusion: 本文提出了一种基于深度学习与ConSinGAN数据增强的DIP封装缺陷自动检测系统，能在缺陷样本稀缺情形下生成训练集并达到较高检测准确率，具有工程可部署性。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD利用预训练编码器特征并学习非线性投影到自然图像流形，通过测量嵌入差异实现高效少样本异常检测，支持多类检测且参数量小。


<details>
  <summary>Details</summary>
Motivation: 工业安全检查场景中样本有限，现有方法在类别不可知设置下难以区分正常与异常特征。利用大规模预训练的编码器所学到的自然图像分布，可以在少样本条件下更好地识别异常。

Method: 以大规模预训练的视觉编码器（如DINOv3）为特征提取器，学习一个简单的非线性投影算子，将特征投影到自然图像流形上。通过测量原始嵌入与投影后嵌入之间的差异来判断异常，并在多个基础编码器上进行评估。

Result: 实验证明FoundAD支持多类检测，在使用远少于先前方法的参数量情况下取得了有竞争力的性能，并在不同基础编码器（包含DINOv3）上获得稳定效果。

Conclusion: 该工作提出FoundAD，通过学习一个非线性投影算子将图像嵌入投影到自然图像流形，从而利用预训练基础视觉编码器的特征来进行少样本异常检测。作者观察到图像中异常量与编码器学习的嵌入差异相关，并据此设计检测策略。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: 通过在ViT中引入受伪分割监督的可训练聚类与恢复模块，ClustViT在语义分割上显著降低计算并加速推理，同时维持相似精度。


<details>
  <summary>Details</summary>
Motivation: 传统ViT注意力的二次复杂度限制了其在机器人等资源受限设备上的实用性。现有基于动态合并token的方法对分类效果良好，但不适合需要保留空间细节的密集预测任务（如语义分割）。因此提出在保持分割精度的前提下减少计算的方案。

Method: 在ViT主干中插入可学习的Cluster模块，该模块利用来自语义分割伪聚类的监督将相似token合并以减少计算；随后用Regenerator模块重建被合并token丢失的细节以供下游分割头使用。结合端到端训练策略，实现效率与性能的权衡。

Result: 在三个数据集上，ClustViT在保持相当分割精度的情况下，实现最多2.18倍更少GFLOPs和1.64倍更快的推理速度。

Conclusion: ClustViT通过在ViT中引入可训练的聚类模块与重建模块，实现了在语义分割任务上在保持相似精度的同时显著降低计算量并加速推理，适合资源受限的机器人视觉场景。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT提出用图像patch作为可解码token（VRTs），与LLM输出混合并由轻量解码器直接生成检测、分割与定位等视觉输出，从而提升定位与密集预测能力并在多项任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视觉任务中多使用间接文本表示（如文本坐标）限制了性能与无法支持密集预测任务（如分割），需要一种能让模型直接输出视觉结果的统一机制。

Method: 在输入端将图像patch嵌入作为Visual Reference Tokens与LLM输出token交织，LLM生成包含VRT信息的序列，再由轻量解码器将这些输出解码为检测、分割和定位预测；训练时随机选择VRT并采用逐token交叉熵损失，对VRT表做动态扩展，每次前向独立处理VRT。

Result: 在四项视觉感知与理解任务上，PaDT持续取得SOTA表现，且在与更大规模MLLM比较时仍保持竞争力。

Conclusion: PaDT通过引入可解码的图像Patch token（VRTs），实现了MLLM同时直接生成文本与多种视觉输出的统一范式，从而克服了基于文本坐标等间接表示的性能和密集预测限制。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 提出信任金字塔与三角信任指数（TTI），并设计可解释AI框架TriAlignXA及其三大引擎和预映射机制，在生鲜电商中平衡质量、生物特性、时效与成本，提升分级准确率与交易信任。


<details>
  <summary>Details</summary>
Motivation: 动机为解决生鲜果蔬电商中由于缺乏感官感知导致的信任缺口，以及传统绝对分级标准无法兼顾农产品的生物特性、时效性和经济性，从而需要新的量化与可解释方法来建立消费者信任。

Method: 方法包括：构建“信任金字塔”模型并基于双源验证，提出用于量化权衡的三角信任指数（TTI）；设计可解释AI框架TriAlignXA，包含生物自适应引擎、时效优化引擎与经济优化引擎，通过多目标优化实现权衡；以及引入“预映射机制”将过程数据编码到二维码以提高透明度。

Result: 实验结果表明：在分级任务上，TriAlignXA显著优于基线模型；理论与实证分析验证了该框架在处理“不可兼得三角”权衡方面的能力；预映射机制提高了信息透明度，从而增强消费者信任。

Conclusion: 本论文结论是：在生鲜农产品电商中，质量是信任的基石；通过构建信任金字塔和提出三角信任指数（TTI），并采用可解释AI框架TriAlignXA及其三大引擎和预映射机制，可以在生物特性、时效性与经济性三者之间实现平衡，从而增强线上交易的可信度。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出4DGS-Craft：通过4D感知编辑模型、多视图一致性模块、高斯选择保留非编辑区及LLM意图解析，实现更一致、可控且能处理复杂指令的4D Gaussian Splatting编辑框架。


<details>
  <summary>Details</summary>
Motivation: 现有4D Gaussian Splatting编辑在视点一致性、时间连贯性、非编辑区域保持以及处理复杂文本指令方面存在不足，需提升编辑一致性与交互性。

Method: 引入4D-aware InstructPix2Pix，使用初始场景提取的4D VGGT几何特征；增加多视图网格模块，迭代细化多视图输入并联合优化4D场景；提出Gaussian选择机制，只优化被编辑区域的高斯；设计LLM模块通过指令模板将复杂用户命令分解为原子编辑操作并推理执行顺序。

Result: 方法在一致性、可控性和处理复杂指令方面优于相关工作，支持更稳定的多视图和时序编辑，并保持非编辑区域不变。代码将在接受后开源。

Conclusion: 该论文提出了4DGS-Craft，一种面向4D Gaussian Splatting编辑的框架，通过4D感知的InstructPix2Pix模型、多视图网格模块、Gaussian选择机制及LLM驱动的意图解析，实现了视角、时间和非编辑区的一致性及对复杂指令的拆解与执行。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出像素级Pure-Pass掩码，通过固定颜色中心点识别并跳过纯像素，集成到ATD-light后在保持自适应和低开销下比CAMixer表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级SR方法在计算复杂度上仍受限，CAMixer虽然按内容难度动态路由计算，但存在适应性差、掩码粗糙、空间灵活性不足的问题，需要更细粒度且高效的掩码机制。

Method: PP使用固定颜色中心点对像素进行分类，识别出无需复杂计算的纯像素并跳过昂贵的操作；将该机制集成到ATD-light，形成PP-ATD-light。

Result: 在与CAMixer-ATD-light节省相近计算量的情况下，PP-ATD-light在重建质量（SR性能）和参数效率上均更优，同时开销极小。

Conclusion: 本文提出的Pure-Pass（PP）通过像素级掩码筛除“纯像素”以降低计算量，并与ATD-light结合在保持自适应性的同时实现更细粒度和空间灵活的掩码策略，从而在相近计算节省下优于CAMixer的重建质量与参数效率。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 提出SLSO自纠环+结构化输出方法，用GPT-4o自动生成颌囊肿影像学报告，显著改善若干项目的准确性并抑制幻觉，但对大范围多牙病变与小样本统计显著性仍有限，需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 利用大型多模态模型自动化生成口腔全景片中的颌囊肿影像学描述，降低幻觉、提高结构化输出一致性与临床可用性。

Method: 基于GPT-4o多模态能力，构建Self-correction Loop with Structured Output (SLSO)框架，设计10步流程：图像输入与分析、结构化数据生成、牙位号提取与一致性检查、检测到不一致时迭代再生、以及最终发现生成后的重构与一致性验证。与传统Chain-of-Thought (CoT)方法在7个评价项上对比。

Result: 在22例数据上，SLSO在牙位号、牙齿移动和根吸收项的准确率分别提高66.9%、33.3%和28.6%；成功案例中最多5次再生即可达到结构化一致输出；总体上减少了幻觉并强化了阴性描述，但未达到统计显著。

Conclusion: SLSO框架在提高GPT-4o多模态生成牙颌囊肿影像描述的准确性方面表现出潜力，尤其在牙位号识别、牙齿移动和根吸收的描述上有明显改进。但对跨多牙的大范围病变识别仍存在局限，且由于样本量小未达统计显著。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: 提出一种简化且高效的3D自编码器LiLa-Net，用精简编码器与简化跳跃连接在实时车载LiDAR点云重建中取得高重建质量与良好泛化。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的条件下（例如嵌入式或车载系统），希望设计出既高效又能保留重建质量的点云自编码器，避免大型复杂网络带来的计算与内存开销。

Method: 基于Velodyne LiDAR采集的点云数据，设计了精简的编码器层和简化的跳跃连接（skip connections），在编码器-解码器结构中平衡跳跃连接信息与潜在向量信息，训练自编码器以重构输入点云。

Result: 通过减少编码器层数和简化跳跃连接，LiLa-Net在保留重建精度的同时降低计算资源需求，并展示了对与训练场景不相关物体的良好重建泛化能力。

Conclusion: LiLa-Net提出了对点云重建有效且计算高效的3D自编码器，兼顾了跳跃连接与简化编码器结构，实现了在真实交通场景下的高质量重建与良好泛化能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools 是一套开放源代码的无人机+机器学习行为监测流水线，能自动提取多物种行为与社交空间指标，扩大观测规模并提升数据质量，对生态研究与保护具有显著价值。


<details>
  <summary>Details</summary>
Motivation: 传统地面观测在范围、时间和劳动力上受限，难以衡量跨景观的复杂多维行为模式，因而需要可扩展的自动化工具来提高数据捕获与分析能力。

Method: 构建了从无人机影像到行为指标的流水线：目标检测、跟踪、行为分类，生成时间预算、行为转换、社交互动、栖息地关联和群体组成动态等指标，并通过三个案例研究与969段行为序列进行验证。

Result: 无人机观测相比地面方法可见性损失降低15%，捕获更多行为转换并具更高准确性与连续性；发现斑马类在群体大小与警觉性关系上的差异、行为惯性高且稀少向警觉转变，以及混合物种群体中的空间隔离现象。

Conclusion: kabr-tools 成功实现了基于无人机视频与机器学习的多物种自动行为监测，显著提升了观测规模与细粒度，验证结果支持其在生态学与保护中的应用。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: 提出将3DGaussians锚定到网格片的无监督语义感知变形框架，在TexMorph基准上显著提升了颜色与结构一致性，尤其适用于有纹理的多视图场景。


<details>
  <summary>Details</summary>
Motivation: 弥补现有方法依赖点云或需预定义同胚映射的缺陷，以实现对有纹理的场景和物体的高质量语义变形，同时保留局部细节与全局语义一致性，且无需标签数据。

Method: 使用重建的网格片段（mesh patches）将3D高斯（3DGaussians）锚定到网格上，结合拓扑感知约束和物理可行的点轨迹策略来确保形变过程中的结构完整性；同时利用网格拓扑作为几何先验在无监督条件下建立语义对应。基于3D Gaussian Splatting进行高保真几何与外观建模。

Result: 在作者提出的TexMorph基准上，GaussianMorphing相比已有2D/3D方法在色彩一致性误差（ΔE）上降低了22.2%，在EI指标上提升了26.2%，表现显著优于先前方法。

Conclusion: GaussianMorphing提出了一种基于mesh引导的3D Gaussian Splatting的统一形变框架，能够在无标签多视图图像下实现语义感知的三维形状与纹理变形，保持几何一致性和纹理保真度。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: InPose：用旋转测量作为条件、位置测量作为似然引导预训练扩散模型，实现稀疏传感器下的零样本人体姿态恢复。


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散的姿态估计依赖位置与旋转测量，位置对身体尺寸敏感导致跨用户泛化差，需要一种只依赖旋转测量并能适应任意用户的泛化方法。

Method: 构建为逆问题，利用预训练扩散模型作为先验，仅以旋转测量作为条件生成姿态序列，随后用来自位置测量的似然项对生成过程进行引导以匹配观测位置。

Result: 提出的InPose能在不需要针对新用户训练的情况下，通过将预训练扩散先验和位置似然结合，生成解释稀疏传感器数据的高概率姿态序列。

Conclusion: 本文提出的InPose方法能在零样本情况下用稀疏传感器测量恢复人体姿态，解决了基于位置测量对用户体型敏感的问题。

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出将Vision Transformer嵌入扩散分割框架（VGDM），通过全局建模+迭代精化改善脑肿瘤MRI分割，性能优于U-Net基线。


<details>
  <summary>Details</summary>
Motivation: 传统卷积网络（如U-Net）难以捕捉长程依赖，导致复杂肿瘤结构分割性能受限；扩散模型在生成高保真医学图像和边界细化方面表现优异，二者结合可望弥补短板。

Method: 在扩散模型的迭代去噪流程中嵌入Vision Transformer作为主干网络，使模型在每一步利用全局上下文建模体积间的空间关系，同时扩散过程负责细化像素/体素级错误并恢复精细肿瘤结构。

Result: 在脑肿瘤MRI数据集上，VGDM在Dice系数和Hausdorff距离上均取得稳定提升，表明方法在体积准确度与边界精细度上均有改善。

Conclusion: VGDM将视觉Transformer与扩散模型结合，显著提升了脑肿瘤MRI分割的边界精度与体积一致性，优于传统U-Net基线。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 作者利用双阶段U-Net从1925-1950年历史地图批量提取城市边界，构建并公开了第一个全国级历史城市足迹栅格数据集，整体准确率73%，显著降低了地图伪影误报。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全国范围的历史数字化城市足迹数据，阻碍了对法国1970年前城市扩张的定量分析，作者希望通过从历史地图自动提取城市边界来填补这一空白。

Method: 提出了一种双阶段U-Net深度学习管线：第一阶段从初始标注数据训练U-Net以生成初步城市掩模并识别混淆区域，用于有针对性的数据增强；第二阶段在改进的训练集上训练第二个U-Net，并将第一阶段的二值输出作为辅助输入以减小放射噪声，降低误报。采用高性能计算集群处理941张高分辨率切片并拼接成全国马赛克。

Result: 生成并公开了覆盖法国都会区的栅格城市足迹，整体准确率73%。方法能有效应对历史地图中的标签、等高线等伪影，支持长时间尺度的城市化研究。

Conclusion: 该论文成功生成了第一个覆盖全国的20世纪中期（1925-1950）历史城市足迹数据集，为研究法国战前城市化提供了新资料支持。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 点基跟踪对手术工具有效，对解剖组织不足；推荐在工具上使用点跟踪，在器官上使用掩码或多点与约束的混合策略以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估点基追踪在手术视频中的可靠性与失败模式，以指导临床或研究中更有效的输入选择与标注策略，降低人工标注成本同时保证跟踪质量。

Method: 使用 SAM2 等 VOS 模型进行零样本点初始化与分割掩码初始化比较，在腹腔镜胆囊切除手术视频中针对胆囊、grasper 与 L-hook 进行定量评估，并辅以定性失效案例分析，分析影响因素如视角变化、遮挡、器官相似度与边界模糊。

Result: 定量结果显示：对手术工具点基跟踪与掩码初始化差距小且稳定；对解剖结构（胆囊）点基跟踪显著差距且不稳定。定性分析指出失败常因组织相似、光照/视角变化及模糊边界。提出的建议（点位选择原则、多点/掩码混合策略、时序一致性约束）可显著改善性能。

Conclusion: Point-based tracking 在手术工具（grasper, L-hook）上表现良好但对器官（胆囊）表现不足，主要因组织外观相似性与边界模糊导致失败；建议在工具上使用单点即可获得稳定跟踪，在解剖目标上优先使用分割掩码或多点+位置约束。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出FFREEDG任务和FRIEREN方法：用服务器端有标签预训练，客户端无标签联邦训练，结合CLIP引导的视觉-语言解码器和弱到强一致性伪标签策略，实现在多个基准上有竞争力的语义分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习在语义分割领域大多假设客户端有标签或没有充分利用现代视觉基础模型；现实场景中客户端往往无标签且存在领域差异，需要一种能在不重访问源数据前提下进行无标签域自适应/泛化的方法，并希望利用VFM的语义能力。

Method: 提出FRIEREN框架：在服务器端用有标签源数据进行预训练，然后在客户端仅用无标签数据进行联邦训练。关键技术包括：1) 引入视觉-语言解码器，利用CLIP文本嵌入引导语义去歧义；2) 采用弱到强一致性学习（weak-to-strong consistency）基于伪标签稳健训练本地模型。

Result: 在合成到真实和晴到恶劣天气的基准上，FRIEREN在无标签联邦设置下取得了与现有域泛化和域自适应方法竞争的性能，证明了该框架的有效性并为该新任务提供了强基线。

Conclusion: 该论文提出并解决了一个新的、更现实的联邦无标签领域泛化任务（FFREEDG），通过利用视觉基础模型（VFM）和联邦学习策略，能在不访问源端带标签数据的情况下对客户端无标签数据进行有效训练，展示了在合成到真实和晴天到恶劣天气的基准上的竞争性表现。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint通过语义分组和细粒度问题化提示，将动作知识注入冻结VLM，实现了训练免费、可解释且泛化性强的视频异常检测提升。


<details>
  <summary>Details</summary>
Motivation: 现有提示过于抽象，忽视了复杂监控视频中决定性的人物-物体交互和动作语义，导致检测效果和可解释性不足。

Method: 将提示按语义类别分组（如暴力、财产犯罪、公共安全），并为每组设计细粒度引导性问题，使模型关注判别性视觉线索。该方法在不训练模型权重的前提下使用提示来引出VLM对异常行为的解释性推理。

Result: 在UCF-Crime和XD-Violence数据集上，ASK-Hint较先前基线显著提升AUC，达到与微调方法和训练无关方法相比的最先进性能，并展现了跨数据集和不同VLM骨干的强泛化能力。

Conclusion: ASK-Hint通过结构化、动作中心的提示设计，提高了冻结视觉-语言模型在视频异常检测任务中的性能和可解释性。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify用小型亲和网络和几何引导池化，净化从2D VLM投射到3D的点特征，利用3D自监督蒸馏的几何先验，实现低数据量下的高性能3D语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有的分割-匹配范式无法有效融合2D语义与3D几何结构，导致直接投影特征噪声大或需昂贵的几何一致性训练与大量标注数据。作者认为几何线索仍隐含在视图聚合的噪声特征中，可被提取利用以提升性能与数据效率。

Method: 提出使用一个小型学生亲和网络（Student Affinity Network），利用从3D自监督教师模型蒸馏的几何先验对2D VLM生成的3D点特征进行净化；推理阶段加入Geometry-Guided Pooling模块进一步去噪并保证语义与结构一致性。

Result: 在主要3D基准数据集上进行广泛实验，GeoPurify在仅约1.5%训练数据的情况下达到或超越最先进性能，证明了方法的高数据效率和有效性。

Conclusion: GeoPurify通过引入基于几何先验的净化机制，解决了2D VLM特征直接投射到3D时产生噪声与结构不一致的问题，从而在数据效率与性能之间取得更好平衡。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 本文提出一种基于耳部静脉的非侵入性猪只识别方法，使用手机背光拍摄并结合图像处理与SVM分类，在混合品系上实现98.12%准确率，实时性良好，适合推广至资源受限的农户。


<details>
  <summary>Details</summary>
Motivation: 现有耳标和微芯片方法在可靠性、成本和对小规模农户的可行性上存在问题，作者提出利用耳部静脉独特性作为无创生物识别方式以解决这些限制。

Method: 使用智能手机与背光拍摄收集800张20头混合品系猪的耳部图像，构建多阶段计算机视觉流程（增强静脉可见性、提取结构和空间特征、生成生物特征签名），并用机器学习分类器（SVM等）进行识别。

Result: 在混合品系数据上，SVM达到了98.12%的识别精度，平均处理时间8.3秒，显示可用于实时农场部署。

Conclusion: 该研究证明了通过耳部静脉纹理进行猪只识别的可行性，在混合品系猪群上表现出高精度，具有替代传统物理标识的潜力。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一种基于Twins金字塔ViT和分割引导的多类别密度图计数框架，使用两任务设计与区域损失抑制类间串扰并提升密集场景计数精度，在多项基准上显著优于既有方法并可应用于生态监测。


<details>
  <summary>Details</summary>
Motivation: 在密集和遮挡严重的场景下，基于检测的离散计数方法效果差，现有多类计数方法在类间干扰和多尺度目标上表现不足，需一种能同时处理多类别、密集分布和尺度变化的密度估计方案。

Method: 使用Twins pyramid vision-transformer作为特征提取骨干，结合一种多类计数头（基于最先进的多尺度解码器）输出密度图；引入两任务设计：主任务为密度图回归，辅助任务为语义分割的Category Focus Module以抑制类间干扰；采用区域化损失（regional loss）来增强局部计数准确性并支持多类场景。

Result: 在VisDrone和iSAID基准上，相较于先前的多类别人群计数方法，MAE分别下降约33%、43%和64%；与YOLOv11比较表明在密集场景下基于密度的计数方法更必要。方法通过区域损失成功扩展到生物多样性监测数据集，展示了在生态保护领域的应用潜力。

Conclusion: 本文提出了一种基于Twins金字塔ViT主干和多尺度解码的多类别密度图估计框架，通过两任务设计和基于分割的类别聚焦模块在训练时抑制类别间串扰，从而在密集、遮挡场景中有效估计各类目标计数。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl通过在推理时操控跨注意力图（相关性、能量、熵三原则）实现无需重训的精细时间控制，能进行对象时序重排及与动作/音频对齐，且保持视频质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型缺乏细粒度时间控制，用户无法指定视觉元素在序列中何时出现，需一种无需重训练即可在推理时实现时间对齐的方法。

Method: 在推理阶段利用跨注意力图，通过三重优化目标引导注意力：相关性（使注意力的时间形状与控制信号对齐）、能量（在需要可见性时放大注意力）、熵（保持空间聚焦）。该方法无需改动模型，只在采样过程中调整注意力分布。

Result: 在多种生成任务（单/多对象时序重排、动作与音频对齐）中，TempoControl展示了精确时间控制能力，同时保留高质量和多样性的生成结果。

Conclusion: TempoControl在不需重训练或额外监督下，实现了基于跨注意力映射的精细时间控制，能在保持视频质量与多样性的前提下对概念出现时序进行精确对齐。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 通过为细粒度视觉推理设计密集奖励并采用多阶段RL训练，RewardMap显著提升了MLLM的视觉理解与推理性能（平均+3.47%）。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在结构化、信息丰富场景（如交通地图）中的空间与细粒度视觉推理表现欠佳；标准RL受稀疏奖励与优化不稳定影响，难以有效提升能力。

Method: 构建ReasonMap-Plus提供基于VQA的密集奖励信号；提出RewardMap框架，包括难度感知细节奖励设计和从感知到推理的多阶段强化学习方案，用以逐步训练视觉理解与推理能力。

Result: 在ReasonMap及ReasonMap-Plus上，各组件均带来稳定增益，组合效果最佳；在6个基准上平均提升3.47%，证明方法能推广到超出交通地图的空间与细粒度视觉推理任务。

Conclusion: RewardMap能有效改善MLLM在细粒度视觉推理任务中的表现，通过引入密集奖励与多阶段RL训练，有助于缓解稀疏奖励与冷启动问题，整体提升模型在空间与细粒度视觉推理任务上的能力。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: 提出DragFlow：用区域仿射监督、个性化适配器与梯度掩码约束，成功把FLUX/DiT的强先验应用于拖拽式图像编辑，显著超越现有方法并发布新基准。


<details>
  <summary>Details</summary>
Motivation: UNet-based DDPMs的先验不足导致拖拽编辑目标区失真；新一代DiT+flow模型（如FLUX）具备更强先验，但直接将点级拖拽方法移植到DiT效果差，因为DiT特征不适合点级运动监督，需新的方法来利用更强的生成先验。

Method: 提出基于区域的编辑范式，使用仿射变换提供更一致的特征监督；集成预训练个性化适配器（如IP-Adapter）增强主体一致性；通过梯度掩码的硬约束保持背景保真；使用多模态大模型（MLLMs）解析任务歧义。并构建ReD Bench用于评测。

Result: 在DragBench-DR和新构建的ReD Bench上，DragFlow在主观与客观指标上均优于点级和已有区域级基线，成为（当前）拖拽式编辑的新SOTA。

Conclusion: 本文提出DragFlow框架，有效利用FLUX等DiT+flow匹配模型的强生成先验，显著改进了拖拽式图像编辑的质量，并在两个基准上达到领先性能。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 在固定token预算下，把关键帧扩展为关键短片段并自适应调整分辨率，能在长视频理解任务上显著提升Video LLM表现（训练免费方法F2C）。


<details>
  <summary>Details</summary>
Motivation: 原有方法通过稀疏采样帧来减少token，但丢失重要的时序信息，影响对运动和事件连续性的推理，需要保留短时序连贯性同时控制token预算。

Method: 提出训练-free方法F2C：以关键短片段替代关键帧，并使用动态分辨率策略保持恒定的token数；具体包括片段提取、分辨率-长度自适应分配和与Video LLM结合进行推理。

Result: 在三个长视频基准（Video-MME、LongVideoBench、MLVU）上，F2C在不需训练的情况下分别比均匀采样提升约8.1%、5.6%和10.3%。

Conclusion: 作者提出通过选择关键短片段（key clips）而非孤立关键帧来保留时序连贯性，从而提升长视频理解性能；采用自适应分辨率在固定token预算下平衡空间分辨率与片段长度。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 在VIDIMU数据集上，最新视频姿态估计模型（以MotionAGFormer为首）能在健康成人日常活动下给出临床可用的关节角估计（RMSE≈9.3°），但仍不及IMU精确，且结果仅限健康人群。


<details>
  <summary>Details</summary>
Motivation: 在真实世界场景下评估人体运动对远程医疗、康复和运动科学至关重要，需比较低成本视频方法与可穿戴IMU的可行性与准确性，填补实验室外运动学评估的基准不足。

Method: 使用VIDIMU数据集（13种日常活动，商品摄像头+5个IMU）将四种视频模型(MotionAGFormer, MotionBERT, MMPose 2D-to-3D, NVIDIA BodyTrack)估计的17关键点关节角与基于IMU并经OpenSim逆运动学计算的关节角进行比较，评价指标包括RMSE、MAE、Pearson相关和R^2。

Result: MotionAGFormer表现最佳：总体RMSE 9.27°±4.80°，MAE 7.86°±4.18°，Pearson 0.86±0.15，R^2 0.67±0.28；总体上视频与IMU均可用于场外运动学评估，但视频方法在某些关节/动作上误差更大，且仅在健康受试者数据上验证，无法推广到病理群体。

Conclusion: 视频基于单目3D人体姿态估计模型在健康成年人日常活动条件下可提供临床可用的关节角度估计，但总体精度仍低于IMU；不同方法在精确度与可访问性、成本间存在权衡。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift在扩散模型中加入AutoKL和在合成图像上训练的CLIP适配器，通过在一个被试预训练并仅微调17%参数，实现了轻量级、快速且在跨被试fMRI视觉重建任务上优于现有方法的表现。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI跨被试视觉重建面临的神经表征个体差异和大脑对复杂视觉输入的抽象语义编码两大挑战，使得重建既准确又计算高效。

Method: 提出在Stable Diffusion框架内加入两个互补的适配器：AutoKL用于低级视觉特征的编码，CLIP适配器用于语义层面；CLIP适配器在用COCO描述匹配的Stable Diffusion生成图像上训练以模拟高级视觉皮层编码。跨被试泛化通过在一个被试上预训练，然后仅微调全连接层（约17%参数），其余组件冻结实现。

Result: 在三张RTX 4090 GPU上一小时微调即可实现跨被试重建，性能超越现有方法，展现出更好的语义和低级细节恢复能力。

Conclusion: NeuroSwift通过在扩散模型中整合AutoKL和CLIP适配器，实现了跨被试的视觉重建，在仅微调17%参数和一小时训练的条件下，取得了优于现有方法的重建效果。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP通过显著性引导的TokenFusion与双头LLM分类器及动态知识聚合，在保持轻量化的前提下，从CLIP中挖掘并对齐细粒度信息，实现对细粒度分类的稳健改进（平均+2.90%）。


<details>
  <summary>Details</summary>
Motivation: CLIP零样本转移主要依赖粗全局特征，限制了细粒度分类表现。现有方法将LLM描述对齐到[CLS]忽视了空间精度，因而需要在视觉与文本表示中注入并对齐细粒度信息。

Method: 引入SOAP（Saliency-Oriented Attention Pooling）在TokenFusion模块中构造一个基于显著性引导的[FG] token并与全局[CLS] token融合；采用双头LLM派生分类器（一个冻结作为稳定文本先验通过多视图对齐提供伪标签，一个可学习从LLM描述初始化并随TokenFusion微调）；并提出Dynamic Knowledge Aggregation以凸组合固定LLM/CLIP先验与TokenFusion演化的logits迭代精化伪标签。

Result: 在13个细粒度基准上平均获得2.90%的准确率提升，同时仅需轻量级适配。

Conclusion: 本文提出microCLIP，通过在视觉和文本表示上联合自训练并引入细粒度引导的注意力池化与双头分类器稳定伪标签，从CLIP中挖掘细粒度信号，显著提升细粒度分类性能。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: 提出了 VidGuard-R1：用 GRPO 微调的 MLLM 做视频真伪检测，兼顾高精度与可解释性，在大规模困难数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: AI 生成视频快速发展带来错误信息与名誉风险，检测工具需既准确又可解释以满足监管与用户透明性要求。

Method: 构建了一个包含 140k 真视频与 AI 生成视频的数据集，针对时间伪影与生成复杂度设计两个奖励模型，基于 Qwen-VL 使用 GRPO 进行微调，使模型输出判定与可解释的推理。

Result: 在现有基准上实现最先进的零-shot 性能，进阶训练将准确率提升到 95%以上；案例研究显示模型能产出精确且可解释的预测理由。

Conclusion: VidGuard-R1 提出了一种基于多模态大模型并通过群体相对策略优化（GRPO）微调的方法，能够在视频真伪检测上同时提供高准确率与解释性推理，实验表明其零-shot 性能领先，进一步训练可达 >95% 准确率。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 通过让短时教师在学生自生成的长视频片段上提供采样指导，论文实现了无需长视频教师即可稳定生成远超训练时长的长视频，显著提升保真与一致性，最长可达4分15秒。


<details>
  <summary>Details</summary>
Motivation: 扩展基于扩散-变换器的视频生成到长时域时计算代价高且缺乏长视频教师，导致学生模型在超出训练时长后画质下降与误差累积。作者旨在在不依赖长视频教师或数据的前提下缓解这一问题。

Method: 基于学生模型自生成的长视频片段，使用短时教师模型对这些片段进行采样指导（self-forcing with teacher guidance），在连续潜在空间中保持时间一致性，避免过曝与误差累积，同时不需重算重叠帧。

Result: 在标准基准和改进基准上均显著优于基线，在扩展计算能力时可以生成最长达4分15秒的视频（接近基模型位置编码最大跨度的99.9%），比基线长度提高50倍以上，保持高保真度与一致性。

Conclusion: 该论文提出了一种在无需长视频教师监督或长视频数据重训练的情况下，显著提升长时域视频生成质量的方法，能将视频长度扩展至教师能力的20倍以上并在实验中取得优于基线的方法效果。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask 用两阶段掩码监督策略将物理引导的刚体运动控制引入视频扩散模型，实现从单张图像和给定速度合成更物理真实的交互视频，同时支持文本驱动的复杂动力学合成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在创造性应用上进展迅速，但在生成物理合理的物体交互和提供以物理为依据的控制机制方面仍不足，限制了在机器人模拟和具身决策等需要物理一致性的应用。

Method: 核心方法包括：1) 基于对象掩码的两阶段训练策略，逐步去除未来运动监督；2) 在合成简单交互场景上训练视频扩散模型（VDM）；3) 将低级（物体速度/掩码）运动控制与高级（文本）条件通过预测场景描述融合，实现复杂动力学现象的控制。

Result: 在合成到真实场景的实验中，KineMask 在物体交互真实性上相比同等规模的近似方法有显著提升；消融研究表明低级控制（速度/掩码）与高级文本条件具有互补作用。

Conclusion: KineMask 提出了一种结合物理引导的两阶段训练策略，用于视频扩散模型，以实现在单幅图像和指定物体速度下生成具有刚体控制和物体交互的更真实视频。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 通过对本体感受、运动觉、力觉和肌电等多模态动作进行对齐学习并加入因果性正则化，本文实现了更准确、更稳健的细粒度动作模拟，利于家庭机器人精细控制任务的执行。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型缺乏用于通用家庭机器人所需的实时精细运动控制能力；文本条件生成模型难以模拟细致的多感官交互，需引入多模态传感器信号以捕获精细动作。

Method: 设计一种特征学习范式，对多模态数据进行对齐同时保持各模态的独特信息；引入正则化方案以增强动作轨迹特征的因果性，从而更好地表示复杂交互动力学。

Result: 在模拟精度、时间漂移降低方面取得进步；消融实验和下游应用展示了方法的有效性与实用性。

Conclusion: 本文提出通过引入细粒度多模态动作（本体感受、运动觉、力觉、肌电）来增强视频/世界模型对精细控制的建模能力，实验表明可提升模拟精度并减少时间漂移。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过在视频模态采用硬件感知的Native Sparse Attention，端到端训练Qwen2.5-VL，实现对超长视频（可扩展到128K tokens）的更好理解和时序推理，相较于压缩或训练自由的稀疏方法有明显优势。


<details>
  <summary>Details</summary>
Motivation: 现有多模态语言模型受限于上下文长度，常遗漏关键过渡帧并难以在长时尺度上保持连贯性。为解决长视频中关键帧捕捉和时序一致性问题，采用稀疏注意力提高可处理的Token数目。

Method: 提出VideoNSA：对视频模态使用硬件感知的混合注意力机制（文本保持密集注意力，视频采用NSA稀疏注意力），并在216K视频指令数据集上对Qwen2.5-VL进行端到端训练。

Result: 与Token压缩和无需训练的稀疏基线相比，VideoNSA在长视频理解、时序推理和空间基准上均有提升。消融得到四项关键发现：可靠扩展到128K tokens；在固定预算下存在最优的全局-局部注意力分配；不同任务表现出不同的分支使用模式；可学习的组合稀疏注意力有助于形成动态注意力汇聚点。

Conclusion: 本文提出将Native Sparse Attention (NSA) 引入视频-语言模型，通过在Qwen2.5-VL上进行端到端训练以扩展上下文到超长视频，从而提升长视频理解和时序推理能力。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift通过按分辨率重新校准去噪噪声水平，训练-free提升低分辨率图像生成质量，兼容现有扩散模型并在多个基准上显著降低FID。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在固定训练分辨率下弱于泛化到较低分辨率，导致无法为需要低分辨率和低成本的用户提供有效选择。识别噪声调度器带来的分辨率敏感性以解决这一问题。

Method: 分析发现噪声调度器对不同分辨率的图像有不等的感知影响：相同噪声水平会对低分辨率图像移除更多信号。NoiseShift在采样阶段对去噪器的噪声水平按分辨率重新校准，无需改变模型架构或训练。

Result: 在多种模型（Stable Diffusion 3/3.5, Flux-Dev）和数据集（LAION-COCO, CelebA）上，NoiseShift在低分辨率生成任务中显著降低FID：SD3.5平均改善15.89%（LAION-COCO），10.36%（CelebA）；SD3改善8.56%、5.19%；Flux-Dev改善2.44%、3.02%。

Conclusion: NoiseShift是一个训练-free的方法，通过根据分辨率调整噪声水平来缓解扩散模型在低分辨率生成中的性能下降，显著提升常见模型在低分辨率下的FID与视觉质量。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 本文提出并构建了用于从视频预测弹性、粘度和动态摩擦的数据集，比较了oracle视觉特征、基于视觉提示的读出机制和MLLM提示策略。结果显示oracle最好，视频基础模型接近oracle但仍有差距，MLLM初始性能较弱可通过提示改进。


<details>
  <summary>Details</summary>
Motivation: 许多物理属性（如弹性、流体粘度、动态摩擦）依赖时间序列信息，单帧图像难以推断，且现有研究多关注静态属性或基于显式模拟。作者希望评估不同视觉与多模态模型在从视频中推断动态物理属性的能力，并提供数据集与基线以推动研究。

Method: 构建三套数据集（每个物理属性的合成训练/测试和真实测试）；实现并比较三种推断方法：1) Oracle—提取与物理属性直接相关的视觉特征（例如弹跳高度、形变、流动模式、滑动减速曲线）；2) Visual-prompt readout—用视觉提示与可训练提示向量通过cross-attention接入预训练视频生成或自监督模型，读出物理属性；3) MLLM prompt—设计多模态大语言模型的提示策略以从视频理解物理属性。

Result: 合成与真实数据集上的实验显示：1) Oracle方法表现最好，表明这些视觉线索确实能反映目标物理量；2) 预训练的视频基础模型（生成/self-supervised）表现相近，但仍落后于oracle，说明现有基础模型尚未充分编码可用于精确物理推断的时序细节；3) MLLMs当前效果偏弱，但通过专门的提示策略可获得提升。

Conclusion: 作者提出了从视频中预测需要时序信息的物理属性（弹性、粘度、动态摩擦）的任务，并构建了合成与真实视频数据集。比较了三类方法：基于经典视觉线索的oracle、基于视觉prompt的可训练读出机制（在视频生成与自监督基础模型上）、以及面向多模态大模型的prompt策略。实验证明：视频基础模型（生成/自监督）性能接近但不及oracle，MLLM表现较差但可通过合适提示改进。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出sounding object detection任务，利用自动分割掩码和slot attention视觉编码器的多模态框架，从第一人称视频中学习将声音与直接交互物体关联，取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互会产生能反映参与物体特性的声音，研究能否将声音与直接参与的物体对应起来，以提升多模态理解和实际应用（如机器人感知）。

Method: 使用自动化管道从日常第一人称视频中生成交互物体的分割掩码；以这些掩码引导模型在训练时关注交互最相关区域；采用slot attention视觉编码器强化物体先验；结合音频和视觉特征进行多模态学习与检测。

Result: 在提出的新任务（sounding object detection）上取得了SOTA表现，同时在现有的多模态动作理解任务上也表现优异，验证了方法的有效性。

Conclusion: 该论文提出了一种面向声音物体检测的新任务，并通过多模态、面向对象的框架以及自动分割掩码和槽注意力视觉编码器取得了优异性能。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 基于KDE在低密度区域插入高斯点并加自适应噪声，提出对3DGS的高效图像级投毒攻击，同时给出客观的评估协议，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法被广泛应用，其安全性和鲁棒性成为关键问题。作者旨在研究并揭示3DGS对图像级投毒攻击的脆弱性，提出有效的攻击手段并建立评估基准以促使未来防御工作。

Method: 方法主要包括：1) 使用核密度估计（KDE）识别场景中低密度区域；2) 在这些区域注入高斯点以构造视角依赖的幻象对象；3) 采用自适应噪声策略破坏多视图一致性以增强攻击效果；4) 提出基于KDE的评估协议来量化攻击难度。

Result: 实验结果表明，该密度引导的投毒方法在攻击可见性、隐蔽性和破坏多视图一致性方面均超过现有最先进方法，并通过KDE协议系统地评估了不同攻击难度下的性能。

Conclusion: 论文提出了一种针对3D Gaussian Splatting（3DGS）的图像级投毒攻击方法，通过在低密度区域注入高斯点并引入自适应噪声，生成在被污染视角清晰可见但对正常视角影响最小的幻象对象。实验显示该方法在多种评估下优于现有技术。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出基于流匹配和随机最优控制的理论与两种算法（测试时控制与Adjoint Matching），解决T2I多主体提示下的解缠问题，能在多款大模型上显著提高多主体保真度并高效运行。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在多主体提示下表现差，容易出现属性混淆、身份纠缠与遗漏，缺乏理论上可优化的目标与方法来在采样阶段引导模型生成更符合多主体描述的图像。作者希望提出第一套有原则且可优化的目标，将采样视为控制问题，从而设计测试时与微调的可行算法以提高多主体保真度。

Method: 将流匹配视为随机最优控制问题，设计控制信号作用于训练好的FM采样器以实现主体解缠。提出两种算法：1）测试时控制：在采样过程中对基速度场施加一次更新，实时调整生成路径（无需训练）；2）Adjoint Matching微调：通过回归后向伴随（adjoint）信号训练一个控制网络，使其在保持基础模型能力下改进多主体表现。并通过流-扩散对应关系把方法推广到扩散模型，形式上统一此前注意力类启发式方法。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL上，测试时控制与Adjoint Matching均能持续提升多主体对齐度，同时保持基础模型的风格特征。测试时控制在普通GPU上高效运行；用有限提示训练的微调控制器可泛化到看不见的提示。提出的FOCUS方法在多模型上达到了最先进的多主体保真度。

Conclusion: 该论文提出了一个基于流匹配（FM）与随机最优控制（SOC）相结合的理论框架，用以提高文本到图像模型在多主体提示下的表现，减少属性泄露、身份混淆与主体遗漏问题。作者给出两种无关架构的算法：一种是训练免费、测试时控制器（单次通行速度扰动）；另一种是轻量级微调规则Adjoint Matching，通过回归后向伴随信号来训练控制网络，同时保留基础模型能力。该方法统一了先前的注意力启发式方法，扩展到扩散模型，并首次提出针对多主体保真度的微调路径。实验证明在多个大模型上均提升多主体对齐且保持风格，测试时高效，有限提示数据的微调具备泛化能力，FOCUS实现了最先进的多主体保真度。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>
