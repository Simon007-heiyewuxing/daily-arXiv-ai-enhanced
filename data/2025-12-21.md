<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 112]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real](https://arxiv.org/abs/2512.15774)
*Yan Yang,George Bebis,Mircea Nicolescu*

Main category: cs.CV

TL;DR: 本文提出结合规则化口罩变形和无配对GAN翻译的两步生成式增强框架，加入非掩码保持损失与随机噪声，可生成更真实、多样的戴口罩人脸样本，实验证明优于仅规则化方法并可补充现有GAN生成。


<details>
  <summary>Details</summary>
Motivation: 受限数据和分布漂移使戴口罩人脸检测与识别性能下降，纯合成或单一步骤生成难以达到现实感与多样性，需兼顾结构一致性和外观真实感的增强方法。

Method: 第一步用规则化掩码变形把口罩贴到人脸（保证形状与位置），第二步使用无配对图像到图像翻译GAN对规则化结果进行风格化和细节修正。引入非掩码保持损失以避免对非口罩区域的破坏，并通过随机噪声注入增加生成样本的多样性和稳定性。

Result: 与仅规则化变形相比，方法在视觉上更真实且多样；与现有GAN方法（如IAMGAN）互为补充。消融实验显示非掩码保持损失和噪声注入均提升质量和训练稳定性。研究指出未来可改进生成多样性、控制性和与下游识别任务的联合优化。

Conclusion: 提出的方法通过两步生成式数据增强（规则掩码变形+无配对图像到图像GAN翻译）在真实感和多样性上优于单纯规则化方法，并可与现有GAN方法互补。非掩码保持损失和随机噪声注入稳定训练并提升样本多样性。

Abstract: Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.

</details>


### [2] [Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://arxiv.org/abs/2512.15885)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Pier Luigi Dovesi,Shaghayegh Roohi,Mark Granroth-Wilding,Rita Cucchiara*

Main category: cs.CV

TL;DR: Use JEPA self-supervision during vision-language alignment so MLLMs learn image structure beyond text, boosting visual task performance


<details>
  <summary>Details</summary>
Motivation: MLLMs rely on subjective text supervision and small multimodal tuning, causing overfit to language priors and poor visual detail understanding

Method: Incorporate I-JEPA: freeze vision foundation models as context/target encoders and train predictor (early LLM layers) to predict image representations, augmenting standard multimodal instruction tuning

Result: Improves visual reasoning in MLLMs via self-supervised JEPA-style learning

Conclusion: JARVIS integrates I-JEPA into MLLM training using frozen vision encoders and trains LLM early layers as predictors, improving vision-centric benchmarks without harming reasoning

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.

</details>


### [3] [City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs](https://arxiv.org/abs/2512.15933)
*Dwip Dalal,Utkarsh Mishra,Narendra Ahuja,Nebojsa Jojic*

Main category: cs.CV

TL;DR: New benchmark CityNav tests MLLMs on sparse-grounded city navigation; standard MLLMs fail; VoP improves results by verbalizing paths/landmarks.


<details>
  <summary>Details</summary>
Motivation: Address gap between language-centric/simulated evaluations and real-world knowledge-intensive sequential navigation requiring visual grounding and reasoning.

Method: Analyze abstract of CityNav paper

Result: Proposes new benchmark CityNav for sparsely grounded visual navigation; evaluates MLLMs; current models underperform; proposes Verbalization of Path (VoP) to improve.

Conclusion: VoP helps by extracting explicit cognitive map info, improving navigation success; current MLLMs insufficient for real-world sparse-grounding navigation.

Abstract: Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/

</details>


### [4] [R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space](https://arxiv.org/abs/2512.15940)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.

</details>


### [5] [The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs](https://arxiv.org/abs/2512.15949)
*Tejas Anvekar,Fenil Bardoliya,Pavan K. Turaga,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: 提出The Perceptual Observatory，系统评估MLLM的感知能力与稳健性，通过多任务与扰动揭示视觉绑定与脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究多集中在语言尺度扩展且复用相同视觉编码器，导致无法判定性能提升是否源自视觉能力增强或仅靠文本知识。需要更精细的评测来量化视觉感知与稳健性。

Method: 分析方法

Result: 构建多垂直评测框架，对人脸匹配、图像内文字理解、图像匹配、网格指点、属性定位等任务使用带有地面实测标注的数据集；对输入进行像素级增扰和基于扩散的风格化幻觉扰动；在不同MMLL模型上测量准确率、稳健性、归因一致性及在扰动下的关系结构保持性。

Conclusion: The Perceptual Observatory提供了一种超越榜单准确率的系统化评测方法，揭示当前MLLMs在视觉感知及其稳健性上的薄弱环节，强调需要更丰富的视觉编码器和更严格的稳健性测试来确保真正的视觉绑定。

Abstract: Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.

</details>


### [6] [Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models](https://arxiv.org/abs/2512.15957)
*Utsav Panchal,Yuchen Liu,Luigi Palmieri,Ilche Georgievski,Marco Aiello*

Main category: cs.CV

TL;DR: CAMP-VLM: a context-aware VLM using scene graphs and synthetic data, fine-tuned with SFT and DPO, significantly boosts multi-human behavior prediction (up to +66.9%) and generalizes to real data


<details>
  <summary>Details</summary>
Motivation: Robots need accurate multi-human behavior prediction from third-person views; existing work focuses on single-human egocentric prediction and lacks datasets, so a VLM with scene graph spatial awareness and synthetic training data is proposed

Method: Vision-Language model fine-tuned with synthetic data, integrates scene graphs and visual context; uses SFT and DPO for training

Result: CAMP-VLM achieves up to 66.9% higher prediction accuracy than baselines on multi-human behavior prediction tasks; generalizes from synthetic to real-world sequences

Conclusion: Incorporating contextual visual features and spatial scene graphs into VLMs and training with synthetic photorealistic data plus SFT/DPO greatly improves multi-human behavior prediction from third-person views

Abstract: Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.

</details>


### [7] [From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection](https://arxiv.org/abs/2512.15971)
*Manuel Nkegoum,Minh-Tan Pham,Élisa Fromont,Bruno Avignon,Sébastien Lefèvre*

Main category: cs.CV

TL;DR: Adapt Grounding DINO and YOLO-World for multispectral detection; integrate text, visual, thermal; outperform specialized models in few-shot and competitive in full supervision on FLIR and M3FD.


<details>
  <summary>Details</summary>
Motivation: Multispectral data scarce; use textual class info and VLM priors to improve few-shot multispectral detection

Method: Modify input pipelines of Grounding DINO and YOLO-World to accept RGB+thermal, design fusion module to combine text embeddings with visual and thermal features, train with few-shot and full datasets, evaluate on FLIR and M3FD

Result: VLM-based detectors adapted to multispectral inputs

Conclusion: Semantic priors from VLMs transfer to thermal modality enabling data-efficient detection

Abstract: Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.

</details>


### [8] [Are vision-language models ready to zero-shot replace supervised classification models in agriculture?](https://arxiv.org/abs/2512.15977)
*Earl Ranario,Mason J. Earles*

Main category: cs.CV

TL;DR: 本工作评估多种视觉-语言模型（VLM）在27个农业分类数据集上的表现。结果显示零样本VLM普遍显著落后于有监督的YOLO11基线；在多项选择提示下最佳模型（Gemini-3 Pro）平均准确率约62%，而开放式提示下原始准确率通常低于25%。使用LLM语义判定可将开放式准确率提升并改变模型排名。开源模型中Qwen-VL-72B表现最佳但仍落后于顶级专有系统。植株/杂草识别相对容易，害虫/损伤识别最难。结论是当前VLM不能作为独立农业诊断系统，但可作为辅助组件。


<details>
  <summary>Details</summary>
Motivation: 评估现成VLM在农业决策支持中的可靠性，特别是在不同提示和评估方法下的零样本分类性能，确定其能否替代或辅助传统有监督系统。

Method: 在AgML集合的27个农业分类数据集（162类）上，比较多种开源与闭源VLM。实验包括多项选择和开放式提示两种评估方式，并引入LLM语义判定来对开放式输出进行语义匹配；与有监督YOLO11基线进行比较，按任务类别（植物疾病、害虫/损伤、植物/杂草种类）进行细分分析。

Result: 零样本VLM普遍显著低于YOLO11；在多项选择提示下Gemini-3 Pro平均约62%准确率，开放式提示原始准确率通常<25%，经LLM语义判定可提升（例如从21%到30%）。开源最佳Qwen-VL-72B在受限提示下接近闭源表现但仍落后。植物/杂草分类较易，害虫/损伤最难。

Conclusion: 当前现成VLM尚不足以作为独立的农业诊断系统，但在受限界面、明确标签本体和领域感知评估策略下可作为辅助组件。进一步改进需要更好的领域适应、提示设计及评估方法。

Abstract: Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.

</details>


### [9] [Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings](https://arxiv.org/abs/2512.15993)
*Lars Beckers,Arno Waes,Aaron Van Campenhout,Toon Goedemé*

Main category: cs.CV

TL;DR: Paper proposes a mower that preserves visually diverse vegetation by using PlantNet-pretrained ResNet50 embeddings and a dispersion metric to decide when to stop blades, validated on testbeds and showing strong correlation with expert assessments.


<details>
  <summary>Details</summary>
Motivation: Convert monocultural lawns into biodiverse biotopes by actively preserving visually diverse vegetation patches via automated mowing decisions guided by ecological-aware visual features.

Method: Analyse methods and approach

Result: Robotic mowing framework using visual perception. Uses ResNet50 pretrained on PlantNet300K to get embeddings; computes global deviation metric (embedding-space dispersion) as biodiversity proxy; selective mowing algorithm toggles blades to preserve diverse patches; implemented on modified commercial mower; validated in mock-up and real garden datasets; correlation between embedding dispersion and expert biodiversity assessment.

Conclusion: Deep visual diversity embeddings can serve as an effective proxy for ecological richness; selective mowing based on embeddings can enhance garden biodiversity; approach feasible on real robotic mowers and scalable to urban lawns.

Abstract: This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.

</details>


### [10] [CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion](https://arxiv.org/abs/2512.16023)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Ziyuan Liu,Abhinav Valada*

Main category: cs.CV

TL;DR: 并行保留视频预训练知识的动作扩散＋Bridge Attention＋动作精炼，自动生成高质视频-动作配对，推动机器人策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决视频-动作配对标注匮乏问题，使视频扩散模型可用于机器人策略学习。

Method: 在预训练视频扩散模型旁并行引入专用动作扩散模型以保留预训练知识；提出Bridge Attention实现跨模态交互；设计动作精炼模块将粗糙动作转为精确控制，适应低分辨率数据。

Result: 在多个公开基准和真实世界数据集上进行全面评估，生成更高质量视频与更准确动作，显著优于现有基线。

Conclusion: 所提框架可规模化利用大规模视频数据为机器人学习提供带动作标注的视频-动作对，弥补现有两阶段或单模态联合方法的不足。

Abstract: We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.

</details>


### [11] [Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.16055)
*Jiaheng Geng,Jiatong Du,Xinyu Zhang,Ye Li,Panqu Wang,Yanjun Huang*

Main category: cs.CV

TL;DR: 作者构建了一个在真实场景下生成对抗交互的闭环评估平台：用基于flow matching的图像生成器根据交通信息生成真实驾驶图像，并用高效的对抗周边车辆策略制造角落案例，平台能高效生成真实图像并揭示端到端模型（如UniAD、VAD）在角落案例中的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现实世界中安全关键的角落案例稀少且难以收集，而现有对抗评估方法多在简化的仿真环境中进行，缺乏对真实场景端到端自动驾驶模型的评估能力，需设计能在真实图像层面进行对抗交互的评估平台。

Method: 构建闭环评估平台，由两部分组成：1) 基于flow matching的真实世界图像生成器，输入交通环境信息（如周边车辆状态）高效稳定地生成对应真实驾驶图像；2) 高效的对抗周边车辆策略，用以建模具有挑战性的交互并生成安全关键角落案例。平台将生成的图像反馈给端到端模型以完成闭环评估。

Result: 实验显示平台能高效生成逼真的驾驶图像；在基于该对抗策略下评估UniAD和VAD等端到端模型时，观测到模型在角落案例中的性能明显下降，表明平台能有效发现模型潜在问题并有助提升系统安全与鲁棒性。

Conclusion: 该论文提出了一个用于端到端自动驾驶的闭环对抗评估平台，能够在基于真实场景的图像生成器和对抗交通策略协同作用下生成安全关键的角落案例，从而评估实际训练模型的鲁棒性与安全性。

Abstract: Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.

</details>


### [12] [FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution](https://arxiv.org/abs/2512.16075)
*Hao Tang,Hanyu Liu,Alessandro Perelli,Xi Chen,Chao Li*

Main category: cs.CV

TL;DR: 提出基于3D多通道patch扩散模型，将单壳低角分辨率FOD映射为多壳高角分辨率FOD，设计FOD-patch适配器、体素级协调模块和SH注意力模块，实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单壳低角分辨率dMRI采集快但FOD估计精度差；多壳高角分辨率能提高FOD质量但采集慢。希望用生成模型从LAR-FOD高效重建HAR-FOD。

Method: 构建3D多通道patch扩散模型：1) FOD-patch适配器融入先验脑结构信息以提升patch学习效率；2) 体素级条件协调模块增强全局上下文理解；3) SH注意力模块用于学习球谐系数间复杂相关性；基于这些模块进行训练并在数据集上评估与SOTA比较。

Result: Proposes a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD, introduces FOD-patch adapter with prior brain anatomy, voxel-level conditional coordinating module, SH attention module, and shows superior experimental performance.

Conclusion: Their model effectively generates high-quality HAR-FOD from single-shell LAR-FOD, improving accuracy over prior methods and enabling efficient patch-based learning with global coordination and SH coefficient correlation modeling.

Abstract: Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.

</details>


### [13] [Auto-Vocabulary 3D Object Detection](https://arxiv.org/abs/2512.16077)
*Haomeng Zhang,Kuan-Chuan Peng,Suhas Lohit,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 提出自动词汇3D检测（AV3DOD），结合2D VLM生成语义候选与特征扩展，引入Semantic Score评估，在两大数据集上超过CoDA取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D检测依赖用户指定类别，限制实用性。研究自动生成类别名称可以让系统在真实场景中更加泛化与自适应，减少人工干预。

Method: 框架利用2D VLM进行图像描述以生成语义候选，结合伪3D框生成和特征空间语义扩展来匹配3D检测结果，并通过Semantic Score评估生成类别的语义质量。

Result: AV3DOD提出了自动化词汇生成的3D目标检测新任务，并设计了一个利用2D视觉-语言模型生成语义候选的框架。该方法通过图像描述、伪3D框生成以及特征空间语义扩展来丰富候选语义，并引入Semantic Score评估自动生成类别名称的质量。实验在ScanNetV2和SUNRGB-D上取得了SOTA的定位与语义质量表现，显著优于CoDA。

Conclusion: AV3DOD有效实现了无需用户输入的开放词汇3D检测，提升了3D检测的语义表达能力和定位性能，证明了利用2D VLMs和多种语义扩展策略在3D场景理解上的可行性和优势。

Abstract: Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.

</details>


### [14] [LAPX: Lightweight Hourglass Network with Global Context](https://arxiv.org/abs/2512.16089)
*Haopeng Zhao,Marsha Mariya Kappan,Mahdi Bamdad,Francisco Cruz*

Main category: cs.CV

TL;DR: LAPX: lightweight Hourglass with self-attention achieving SOTA-like accuracy on MPII/COCO with 2.3M params and real-time edge performance


<details>
  <summary>Details</summary>
Motivation: Reduce model size and computational cost of SOTA pose estimators while keeping high accuracy and making them efficient for edge-device deployment by integrating global context via self-attention and refining lightweight modules

Method: Hourglass network with self-attention, stage design and refined lightweight attention modules

Result: Competitive results on MPII and COCO with 2.3M parameters and real-time performance suitable for edge devices

Conclusion: LAPX provides a compact, accurate, and efficient pose estimator balancing accuracy and edge deployment constraints

Abstract: Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.

</details>


### [15] [Collimator-assisted high-precision calibration method for event cameras](https://arxiv.org/abs/2512.16092)
*Zibin Liu,Shunkun Liang,Banglei Guan,Dongcai Tan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于准直器与闪烁星形图案的事件相机远距高精度标定方法：先用准直器的球面运动模型线性求解，再通过非线性优化精化参数，实验证明精度和鲁棒性优于现有方法。


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of accurately calibrating event cameras (intrinsic and extrinsic parameters) for long-range, high-precision measurements, leveraging advantages of event cameras like high dynamic range and temporal resolution.

Method: Use a collimator projecting flickering star-based patterns; model the collimator's motion as sphere motion to obtain linear initial solution for camera parameters, then apply nonlinear optimization to refine parameters.

Result: Real-world experiments across varying conditions show the proposed method outperforms existing event camera calibration methods in accuracy and reliability.

Conclusion: The collimator with flickering star-based patterns plus sphere motion model and nonlinear refinement provides a robust, accurate calibration method for long-distance event camera calibration.

Abstract: Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.

</details>


### [16] [TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times](https://arxiv.org/abs/2512.16093)
*Jintao Zhang,Kaiwen Zheng,Kai Jiang,Haoxu Wang,Ion Stoica,Joseph E. Gonzalez,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: TurboDiffusion通过注意力加速、rCM步蒸馏和W8A8量化等方法，在单卡RTX 5090上为多种大模型提供100-200×的视频生成加速且保持可比的质量，已开源实现。


<details>
  <summary>Details</summary>
Motivation: 提出一种框架以显著加速基于扩散模型的视频生成，在保证视频质量前提下实现100-200倍的推理加速。

Method: 结合多种加速技术：1) 注意力加速：采用低位SageAttention和可训练的稀疏线性注意力（SLA）；2) 步骤蒸馏：使用rCM方法进行高效步数蒸馏；3) W8A8量化：将参数与激活量化为8位以加速线性层并压缩模型；并辅以工程优化。

Result: 在多个Wan2系列大模型（包括1.3B、14B等，不同分辨率）上测试，单卡RTX 5090实现100-200倍的视频生成速度提升，同时视频质量与原始模型可比。代码与模型检查点已开源。

Conclusion: 通过注意力优化、步数蒸馏与8位量化的组合，TurboDiffusion在实际硬件上大幅提升扩散视频生成效率，为高分辨率视频生成提供可行的低成本部署途径。

Abstract: We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.
  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.

</details>


### [17] [Flexible Camera Calibration using a Collimator System](https://arxiv.org/abs/2512.16113)
*Shunkun Liang,Banglei Guan,Zhenbao Yu,Dongcai Tan,Pengju Sun,Zibin Liu,Qifeng Yu,Yang Shang*

Main category: cs.CV

TL;DR: New collimator system enables calibration with spherical-motion constraint; offers multi-image linear, two-image minimal, and single-image algorithms; validated in simulations and real tests


<details>
  <summary>Details</summary>
Motivation: Create reliable controllable environment to simplify camera calibration and enable fast flexible calibration without extensive camera movement

Method: Design collimator creating rays with known angular relationships; derive angle invariance constraint; show relative target-camera motion is spherical (pure rotation); develop closed-form linear solver for multiple images, minimal solver for two, and single-image algorithm; validate with experiments and comparisons

Result: Proposes collimator-based calibration reducing motion DOF and providing single-image method

Conclusion: Collimator geometry yields angle invariance and spherical motion; allows closed-form and minimal solvers and single-image calibration; outperforms baselines

Abstract: Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration

</details>


### [18] [Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space](https://arxiv.org/abs/2512.16133)
*Ren Nakagawa,Yang Yang,Risa Shinoda,Hiroaki Santo,Kenji Oyama,Fumio Okura,Takenao Ohkawa*

Main category: cs.CV

TL;DR: 提出CattleAct：通过从大量牛只动作学习潜在动作空间，并用对比学习微调以嵌入稀有交互，从而实现从单张图像检测放牧牛只的行为交互，结合视频和GPS构建实用系统，在商业牧场上表现出优于基线的准确性。


<details>
  <summary>Details</summary>
Motivation: 放牧牛只的行为交互（如发情）是畜牧管理的重要信息，但相关交互在视频中是稀有事件，且缺乏包含交互的大规模数据集，需提出数据高效的方法用于从单张图像检测牛只交互。

Method: 先从大规模牛只动作数据集中学习动作潜在空间；再用对比学习对预训练潜在空间进行微调以嵌入稀有交互，构建动作与交互的统一潜在空间；在此基础上结合视频和GPS输入构建实际系统进行交互检测。

Result: Proposes CattleAct, a data-efficient method for cattle interaction detection from single images, learning an action latent space from large-scale cattle action data and fine-tuning with contrastive learning to embed rare interactions; integrates video and GPS in a practical system and reports accurate detection on commercial pasture; code available.

Conclusion: CattleAct effectively detects rare cattle interactions by decomposing interactions into individual actions and leveraging a unified latent space learned from action data and fine-tuned with contrastive learning, showing superior performance in real-world pasture experiments.

Abstract: This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.

</details>


### [19] [ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT](https://arxiv.org/abs/2512.16140)
*Ze Yuan,Wenbin Li,Shusen Zhao*

Main category: cs.CV

TL;DR: Use OPMT for fast initial decomposition, then ResDynUNet++ to refine, achieving better DSCT reconstructions.


<details>
  <summary>Details</summary>
Motivation: Combine fast-converging physics-based reconstruction with learned refinement to exploit prior knowledge and data-driven correction of artifacts.

Method: Hybrid reconstruction: OPMT + ResDynUNet++

Result: OPMT gives fast intermediate basis material decomposition; ResDynUNet++ (UNet++ with residual dynamic conv blocks) refines to reduce channel imbalance and near-interface artifacts; experiments show superior performance on synthetic and clinical data.

Conclusion: Hybrid of model-based OPMT and data-driven ResDynUNet++ yields accurate, artifact-reduced DSCT basis material images.

Abstract: We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.

</details>


### [20] [SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation](https://arxiv.org/abs/2512.16143)
*Yueyang Hu,Haiyong Jiang,Haoxuan Song,Jun Xiao,Hao Pan*

Main category: cs.CV

TL;DR: 提出SegGraph：构建SAM分割图节点为段、边表示重叠/相邻，节点自适应调制2D特征并通过GNN传播，视角加权融合到3D点，显著提升few-shot 3D分割


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽视3D几何结构学习，要么未充分利用SAM提供的高质量分割线索，导致欠分割与部件标签不一致，需一种将SAM的分割信息与3D几何显式结合的机制

Method: 构建segment图（节点为SAM段，边表示重叠/相邻），节点自适应调制2D foundation model特征，通过GNN在图上传播学习全局几何结构；使用视角方向加权的段到点映射以保持段内语义一致并衰减低质量段贡献。

Result: 提出SegGraph，一种基于SAM分割图的传播方法用于few-shot 3D部分分割，通过构建段（segment）图建模重叠/相邻关系，使用图神经网络传播并结合视角加权融合到点云，显著提升PartNet-E上mIoU

Conclusion: SegGraph通过显式学习SAM掩码中的几何结构并进行视角权重融合，提升了小部件和边界处的分割性能，且在PartNet-E上优于所有比较基线至少6.9%的mIoU

Abstract: This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.

</details>


### [21] [C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation](https://arxiv.org/abs/2512.16164)
*Chao Li,Dasha Hu,Chengyang Li,Yuming Jiang,Yuncheng Shen*

Main category: cs.CV

TL;DR: 提出C-DGPA：一个包含边际对齐的动态对抗分支与条件对齐的类映射机制（CMM）的双分支提示适配方法，从而解决VLM提示调优在UDA中忽视条件分布的问题并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有VLM提示调优在UDA中多只对齐边际分布，忽略条件分布差异，导致类别原型错位和语义判别性下降，需同时解决两类分布差异以提升性能。

Method: 双分支架构：1) 边际对齐分支采用动态对抗训练以缩小源与目标边际分布差异；2) 条件对齐分支引入类映射机制（CMM），标准化语义提示理解并防止对源域过度依赖；两分支通过协同优化将领域知识注入提示学习。

Result: 提出了C-DGPA方法用于无监督领域自适应（UDA）场景下的视觉-语言模型（VLM）提示调优，通过双分支架构同时对边际分布和条件分布进行对齐，从而提升类别原型对齐和语义可区分性。

Conclusion: C-DGPA在OfficeHome、Office31和VisDA-2017上取得了新的最先进结果，证明了双重对齐策略的有效性，能生成领域不变且语义判别性强的表示。

Abstract: Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.

</details>


### [22] [Towards Closing the Domain Gap with Event Cameras](https://arxiv.org/abs/2512.16178)
*M. Oltan Sevinc,Liao Wu,Francisco Cruz*

Main category: cs.CV

TL;DR: 使用事件相机替代传统相机可以在昼夜光照变化下保持更稳定的自动驾驶性能，域迁移损失不大且在跨域场景中表现更好。


<details>
  <summary>Details</summary>
Motivation: Address the domain gap problem between training and deployment lighting conditions (day vs night) for end-to-end driving systems; seek sensors that are robust without extra adaptation.

Method: Analyze method: paper proposes using event cameras instead of traditional cameras to handle day-night domain gap, comparing performance across lighting without extra adjustments; likely evaluates end-to-end driving models on datasets with day and night, measures domain-shift penalty and baseline cross-domain performance.

Result: Event cameras maintain more consistent performance across day-night lighting conditions, showing domain-shift penalties comparable or smaller than grayscale frames, and superior baseline cross-domain performance.

Conclusion: Event cameras are a promising alternative sensor for end-to-end driving across lighting-domain gaps, reducing need for domain adaptation when moving between day and night.

Abstract: Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.

</details>


### [23] [Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation](https://arxiv.org/abs/2512.16199)
*Jerrin Bright,Zhibo Wang,Dmytro Klepachevskyi,Yuhao Chen,Sirisha Rambhatla,David Clausi,John Zelek*

Main category: cs.CV

TL;DR: Avatar4D提出一个可定制的合成人类动作数据集生成流水线，强调对姿态、外观、视角和环境的细粒度控制，无需人工标注；在体育场景中构建了Syn2Sport数据集（棒球、冰球），并用于评估姿态估计模型的监督训练、零样本迁移和跨运动泛化能力，同时分析合成与真实数据在特征空间的相似性，显示合成数据在特定领域任务中的可扩展性与可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据多针对日常动作，灵活性低且难以适应领域特定的动作（如体育），因此需要一个能在不依赖人工标注下生成可控、多样的4D人类动作数据的流水线以促进领域特定任务（如体育动作理解）的研究。

Method: 设计Avatar4D流水线以生成高保真4D人体动作序列，提供对身体姿态、人物外观、摄像机视角与环境背景的细粒度控制；在体育领域构建Syn2Sport大规模合成数据集，覆盖棒球和冰球等运动，并渲染不同外观与环境条件；对若干先进姿态估计模型进行训练与评估，包括监督学习、零样本迁移与跨运动泛化测试，同时在特征空间比较合成与真实数据的相似性。

Result: 在Syn2Sport上，多个主流姿态估计模型显示出良好的监督学习效果；合成数据在零样本转移到真实数据集时表现出一定的可迁移性，并在跨运动泛化任务中取得合理结果；特征空间分析表明合成数据在某些特征维度上与真实数据接近，支持合成数据作为替代或补充真实域数据的可行性。

Conclusion: Avatar4D与Syn2Sport表明，可控高保真合成4D人体动作数据能有效支持领域特定任务（如体育），减少对真实标注数据的依赖，提供可扩展且可迁移的数据来源，对未来特定领域动作理解研究有重要价值。

Abstract: We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.

</details>


### [24] [Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation](https://arxiv.org/abs/2512.16201)
*Sarosij Bose,Ravi K. Rajendran,Biplob Debnath,Konstantinos Karydis,Amit K. Roy-Chowdhury,Srimat Chakradhar*

Main category: cs.CV

TL;DR: 提出VALOR，通过两阶段基于强化学习的后对齐框架（GRPO）提升医学视觉语言模型在胸部X光报告生成中的事实准确性与视觉对齐，减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VLM在交叉模态对齐差导致报告生成幻觉，且依赖大量标注预训练或检索/偏好数据成本高，亟需一种能提升视觉-语言对齐、增强临床用语准确性的高效方法。

Method: 提出VALOR：使用Group-Relative Proximal Optimization (GRPO)的强化学习后对齐框架，分两阶段训练：1）以文本奖励优化Med-VLM，使输出术语更临床精准；2）在文本引导下对视觉投影模块进行对齐，使模型关注与疾病发现相关的图像区域，增强视觉诱导能力。

Result: 在多基准数据集上广泛实验表明，VALOR较现有最先进报告生成方法在事实准确性和视觉对齐方面显著提升，取得显著性能增益。

Conclusion: VALOR通过强化学习后对齐有效降低幻觉、增强视觉-语言对齐，为更可靠的自动化放射学报告生成提供可行路径。

Abstract: Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.

</details>


### [25] [Open Ad-hoc Categorization with Contextualized Feature Learning](https://arxiv.org/abs/2512.16202)
*Zilin Wang,Sangwoo Mo,Stella X. Yu,Sima Behpour,Liu Ren*

Main category: cs.CV

TL;DR: 本文提出OAK模型，用少量可学习的上下文tokens接入冻结的CLIP，结合图文对齐与视觉聚类目标，在少样本+大量无标签数据下进行开放式临时分类（open ad-hoc categorization），在多个数据集上显著提升新类别识别和概念发现，并生成可解释显著图。


<details>
  <summary>Details</summary>
Motivation: 传统分类依赖固定类别，但实际任务常需临时创建满足当前目标的“ad-hoc”类别。目标是在只有少量标注示例且有大量未标注数据时，自动发现上下文、语义扩展并以视觉聚类扩展类别。

Method: 提出OAK：在输入端加入一小组可学习的上下文tokens，冻结CLIP模型，联合优化两类目标——CLIP的图像-文本对齐损失（利用文本提示的语义信息）与GCD的视觉聚类损失（促进无监督视觉组群化），以此在语义与视觉空间同时推动类别扩展与发现。

Result: 在Stanford和Clevr-4数据集的多种分类任务上，OAK在准确率与概念发现指标上达SOTA。例如在Stanford Mood数据集上，新类别准确率达87.4%，较CLIP与GCD提升超过50%。此外生成的显著图在不同任务上聚焦于合理区域（动作聚焦手部、情绪聚焦面部、位置聚焦背景）。

Conclusion: 通过在冻结的强大表示（CLIP）前加入少量可学习上下文并结合跨模态与视觉聚类目标，OAK在开放式临时分类场景下实现了高性能和可解释性，展示了简单模型架构结合目标设计在少样本+无标签条件下的有效性与广泛适用性。

Abstract: Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.
  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.
  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.

</details>


### [26] [Enhanced 3D Shape Analysis via Information Geometry](https://arxiv.org/abs/2512.16213)
*Amit Vishwakarma,K. S. Subrahamanian Moosath*

Main category: cs.CV

TL;DR: 提出将点云表示为高斯混合模型（GMM）并构建信息几何框架，提出具有有界性且数值稳定的修改对称KL（MSKL）散度，用于点云形状比较；在两数据集上实验表明其优于传统距离和现有KL近似。


<details>
  <summary>Details</summary>
Motivation: 点云无结构且表面几何复杂，传统几何度量（Hausdorff/Chamfer）对异常值敏感且无法捕捉整体统计信息；现有GMM的KL近似可能产生无界或数值不稳定值，需稳定且能反映整体几何变化的比较方法。

Method: 证明GMM空间构成统计流形，基于信息几何定义修改对称KL（MSKL）散度，推导出其上下界以保证数值稳定性；将点云拟合为GMM并在流形上计算MSKL作为比较度量，设计实验以检验稳定性和单调性。

Result: MSKL在MPI-FAUST（人体姿态判别）和G-PCD（动物形状比较）数据集上表现出稳定且单调变化的距离值，能直接反映几何差异，并在多个任务中优于Hausdorff、Chamfer和现有KL近似（如近似KL或交叉熵方法）。

Conclusion: MSKL为点云形状分析提供了理论有界且数值稳定的概率性比较度量，结合信息几何框架能更好地捕捉全局统计结构并提高对几何变化的敏感性，适用于复杂形状的比较与检索。

Abstract: Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.

</details>


### [27] [Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models](https://arxiv.org/abs/2512.16219)
*Zhihao Zhang,Xuejun Yang,Weihua Liu,Mouquan Shen*

Main category: cs.CV

TL;DR: 设计了离散化Euler反演构建随机噪声与语义引导的高质量噪声对；提出EDN编码器-解码器学习将随机噪声映射为高质量噪声；可插入多种NVS模型，实验显示显著提升。


<details>
  <summary>Details</summary>
Motivation: 观察到扩散模型生成结果受初始噪声模式影响显著；现有NVS模型缺乏专门学习高质量初始噪声的框架，故希望将图像语义注入初始噪声以提升生成质量和一致性。

Method: (1) 设计离散化Euler反演方法，将目标图像语义信息注入随机高斯噪声，从而构造随机噪声—高质量噪声的配对数据集；(2) 基于编码器-解码器网络（EDN）训练一个模型，直接将随机噪声映射为高质量噪声；(3) 将训练好的EDN作为模块无缝插入现有NVS扩散模型（如SV3D、MV-Adapter）用于生成过程的初始噪声替换。

Result: 在多个数据集和不同NVS模型（例如SV3D和MV-Adapter）上进行实验，结果表明加入EDN能显著提升生成质量，证明了学习得到的高质量初始噪声能带来一致且可迁移的性能增益。代码已开源。

Conclusion: 本文提出了EDN，一个将随机高斯噪声直接转换为“高质量”初始噪声的编码器-解码器学习框架，配合离散化的Euler反演用于构建噪声对，从而提升单视图新视角合成（NVS）中基于扩散模型的生成质量。

Abstract: Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.

</details>


### [28] [Image Compression Using Singular Value Decomposition](https://arxiv.org/abs/2512.16226)
*Justin Jiang*

Main category: cs.CV

TL;DR: 研究评估SVD低秩近似用于灰度和多通道图像压缩，视觉上可行但压缩效率远低于主流编码器；在严格误差限制下甚至会增加存储开销


<details>
  <summary>Details</summary>
Motivation: 图像占据网络大量存储和带宽，寻找替代或补充的压缩方法以降低资源消耗，评估简单线性代数方法（SVD）在实际压缩任务中的表现

Method: 对灰度与多通道图像分别应用SVD，根据不同秩k构建低秩近似，计算相对Frobenius误差和压缩率，并与JPEG、JPEG2000、WEBP在相同误差水平下比较性能。

Result: SVD和低秩矩阵近似用于图像压缩的实验研究

Conclusion: 低秩近似在视觉效果上有时可接受，但在压缩率和误差权衡上不如JPEG、JPEG2000和WEBP，且在低容忍误差下可能导致压缩后表示比原图更大，因此不具备实际竞争力

Abstract: Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.

</details>


### [29] [ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation](https://arxiv.org/abs/2512.16234)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Hesheng Wang,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出ARMFlow：结合因果上下文编码器和BSCE训练策略的单步自回归模型，实现在线高质量、低延迟3D人类反应生成；离线ReMFlow在性能和速度上领先。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法同时兼顾运动保真度、实时推理和自回归在线适应性，需一种既能建模行为-反应时序依赖又能在单步推理下保持精度与低延迟的模型。

Method: 构建基于MeanFlow的自回归框架，包括因果上下文编码器和MLP速度预测器；训练时引入Bootstrap Contextual Encoding（BSCE），使用模型生成的历史作为输入以减少训练与推理分布差异；同时提出离线变体ReMFlow用于速度与性能优化。

Result: 单步在线生成在InterHuman和InterX数据集上FID指标比现有在线方法提升超40%，在仅使用部分序列条件下也能匹配离线最先进水平；ReMFlow在离线设置中实现最佳性能并拥有最快推理速度。

Conclusion: ARMFlow通过MeanFlow自回归框架、因果上下文编码器和MLP速度预测器，在在线3D人类反应生成上实现了高保真、低延迟并显著降低累积误差。离线变体ReMFlow进一步在离线设置中取得最速推理和最优性能。

Abstract: 3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.

</details>


### [30] [AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection](https://arxiv.org/abs/2512.16235)
*Satya Narayana Panda,Vaishnavi Kukkala,Spandana Iyer*

Main category: cs.CV

TL;DR: 提出一种将家族史与影像和临床数据融合的可解释多模态AI框架，初步验证显示提高了遗传性皮肤病诊断准确性，专家反馈积极，计划开展前瞻性临床试验


<details>
  <summary>Details</summary>
Motivation: 家族史常被忽视但对皮肤病易感性和治疗反应有重要影响；将其与AI影像分析结合可提高诊断效果并支持个体化治疗和临床试验验证

Method: Multi-modal AI framework combining CNN imaging and clinical decision trees with family history

Result: Integrated AI system shows enhanced diagnostic accuracy when family history included; expert validation and proposed prospective trials

Conclusion: Incorporating family history into AI-assisted dermatological diagnosis improves accuracy and personalization; clinical trials planned for further validation

Abstract: Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?
  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.
  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.

</details>


### [31] [Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models](https://arxiv.org/abs/2512.16243)
*Qi Zhang,Yunfei Gong,Zhidan Xie,Zhizi Wang,Antoni B. Chan,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出两种半监督多视角人群计数框架，通过对不同输入视角数目的多视角融合模型进行排序约束来利用未标注数据；一种基于预测值的排序（少视角预测不应大于多视角），另一种基于不确定性的排序（多视角不确定性应不大于少视角），并将这些约束融入训练以提高有限标注数据下的计数性能。


<details>
  <summary>Details</summary>
Motivation: 多视角计数能缓解遮挡问题，但多视角数据难以收集与标注，数据集样本有限。需要方法利用合成数据或更少标注的数据，本文选择半监督策略，通过模型内部多视角一致性约束扩充监督信号。

Method: 设计两种基于排序的半监督约束：1) Vanilla模型：对同一场景中不同数量摄像头输入的融合模型预测进行排序，要求少视角预测不超过多视角预测；2) 不确定性模型：估计不同视角输入下模型的不确定性，并以有标签时的预测误差指导不确定性排序，要求随视角数增加不确定性不增大。将这些约束作为损失项与有标签数据联合训练。

Result: 在多视角计数基准实验中，加入预测值或不确定性排序约束的半监督方法均优于其他半监督计数方法，能在有限标注下提升计数精度与鲁棒性。

Conclusion: 通过对多视角融合模型在不同输入视角数量上的预测与不确定性进行排序约束，可以有效利用未标注数据提升多视角人群计数性能，缓解标注稀缺问题。

Abstract: Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.

</details>


### [32] [Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning](https://arxiv.org/abs/2512.16266)
*Paloma Casteleiro Costa,Parnian Ghapandar Kashani,Xuhui Liu,Alexander Chen,Ary Portes,Julien Bec,Laura Marcu,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 本文提出FLIM_PSR_k，一种基于cGAN的多通道像素超分辨框架，可将像素放大至5倍重建高分辨FLIM图像，显著提升采集速度与信噪比，推断快于扩散模型，盲测显示在病人来源肿瘤样本上可实现k=5的超分辨并显著改善多项图像质量指标。


<details>
  <summary>Details</summary>
Motivation: FLIM临床应用受限于像素停留时间长和信噪比低，导致分辨率-速度权衡比常规光学成像更严苛；需要在加快采集和改善SNR的同时保持高空间分辨率以推动转化应用。

Method: 提出多通道像素超分辨框架FLIM_PSR_k，使用条件生成对抗网络(cGAN)训练模型，从低采样（像素放大至5倍）FLIM数据恢复高分辨率图像；与扩散模型相比，cGAN在重建稳健性与推理速度上优越。

Result: 在病人来源肿瘤组织的盲测中，FLIM_PSR_k在k=5情况下实现25倍空间带宽积提升，恢复出低分辨率输入中丢失的细微结构，并在多个图像质量指标上具有统计学显著改善。

Conclusion: FLIM_PSR_k可提升FLIM的有效空间分辨率、加快成像、缓解SNR限制，兼容低NA与微型化平台，推动FLIM在临床转化中的实用性。

Abstract: Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.

</details>


### [33] [TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering](https://arxiv.org/abs/2512.16270)
*Rui Gui,Yang Wan,Haochen Han,Dongxing Mao,Fangming Liu,Min Li,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出TextEditBench评估图像中文本编辑能力，着重推理相关任务并引入语义期望（SE）评测模型的语义与上下文一致性，发现现有模型在复杂推理与物理/布局一致性方面表现不足。


<details>
  <summary>Details</summary>
Motivation: 当前大规模扩散与多模态模型在图像文本生成上取得进展，但图像内文本编辑（需生成可读字符且保持语义/几何/上下文一致）尚未被充分研究。鉴于此，提出专门针对文本区域并强调推理能力的评估基准以填补该空白。

Method: 构建TextEditBench基准，设计强调物理可行性、语言语义与跨模态一致性的编辑场景，并提出新的评估维度SE。对多种先进图像编辑系统进行广泛测试，比较其在简单指令执行与复杂推理任务上的表现。

Result: TextEditBench是一个专注于图像中“文本区域”编辑评估的新基准。论文贡献包括定义了以文本为中心的评估场景，强调需要推理的编辑任务（包括物理可行性、语言语义与跨模态一致性），并提出了新的评估维度“语义期望”（SE），用于衡量模型在文本编辑中保持语义一致性、上下文连贯性与跨模态对齐的能力。实验显示，现有先进编辑系统在遵循简单文本指令方面表现尚可，但在上下文依赖推理、物理一致性和排版感知整合等方面仍有明显不足。

Conclusion: TextEditBench提出的评估框架和SE指标为文本驱动的图像编辑与多模态推理提供了新的测试基准，推动未来模型在更严苛的文本生成与语义理解任务上改进。

Abstract: Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.

</details>


### [34] [GFLAN: Generative Functional Layouts](https://arxiv.org/abs/2512.16275)
*Mohamed Abouagour,Eleftherios Garyfallidis*

Main category: cs.CV

TL;DR: GFLAN通过拓扑计划（房间质心分配）与几何实现（Transformer增强GNN回归边界）两阶段生成平面图，改善了建筑拓扑与功能约束的捕捉，优于端到端像素基方法。


<details>
  <summary>Details</summary>
Motivation: 解决自动生成平面图时，组合搜索、几何约束满足与功能设计要求的统一处理缺失，深度学习方法难以捕捉建筑推理如拓扑优先性、功能约束传播和路径模式生成的问题。

Method: 提出GFLAN：两阶段生成框架。阶段A用双编码器卷积网络（分离不变空间上下文与布局状态）通过可行位置的离散概率图序列分配房间质心；阶段B构建将房间节点与边界顶点连接的异构图，使用融合Transformer的图神经网络联合回归房间边界。

Result: 在给定外界边界和前门位置的条件下，方法能更好地捕捉拓扑关系和功能约束，通过隐式规划与显式几何回归提高布局一致性与连通性（论文报告了在合成度量上优于直接像素生成方法）。

Conclusion: 通过将拓扑规划与几何实现明确分离，GFLAN能更符合建筑推理，提高平面图生成的合理性与可控性，为后续基于约束的建筑设计自动化提供了新的范式。

Abstract: Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.

</details>


### [35] [MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval](https://arxiv.org/abs/2512.16294)
*Amna Amir,Erchan Aptoula*

Main category: cs.CV

TL;DR: 针对多标签遥感检索提出的MACL通过标签感知采样、频率敏感加权和动态温度缩放改进对比学习，能更好处理语义不平衡，在三数据集上优于基线并将开源实现。


<details>
  <summary>Details</summary>
Motivation: 解决多标签遥感图像检索中语义重叠、标签不平衡与复杂的类间共现模式带来的挑战，旨在提升检索表现和类别表示的公平性。

Method: 提出多标签自适应对比学习（MACL），结合标签感知采样、频率敏感加权和动态温度缩放，将对比学习扩展到多标签场景以平衡常见与稀有类别的表征学习。

Result: 在三个基准数据集（DLRSD、ML-AID、WHDLD）上进行大量实验，MACL 在对比损失基线之上持续取得更好表现，显示能有效缓解语义不平衡并提高大规模遥感档案的检索可靠性。

Conclusion: MACL 是一种有效的多标签对比学习方法，能处理语义重叠与标签不平衡问题，提升检索性能；作者将在接受后开源代码、预训练模型和评估脚本。

Abstract: Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.

</details>


### [36] [PixelArena: A benchmark for Pixel-Precision Visual Intelligence](https://arxiv.org/abs/2512.16303)
*Feng Liang,Sizhe Cheng,Chenqi Yi*

Main category: cs.CV

TL;DR: PixelArena用语义分割作为客观基准评估图像生成模型的细粒度能力，发现Gemini 3 Pro Image在零样本下能生成高精度语义掩码，显示显著进步与若干失败模式。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成基准多侧重审美而非精细语义生成，难以衡量模型在像素级语义理解与生成上的能力。作者希望构建一个客观、可量化的评测框架（PixelArena）来检验多模态大模型的细粒度生成智能。

Method: 提出PixelArena基于语义分割任务，利用真实标注的语义掩码对模型生成的掩码进行像素级评估。对比多款模型（包含Gemini 3 Pro Image），在零样本设置下生成语义掩码，进行定量指标和定性分析，并记录失败案例用于深入分析。

Result: 发现Gemini 3 Pro Image在零样本下能生成高保真语义掩码，表现出超出以往模型的视觉推理与泛化能力；同时作者展示与其他模型的对比结果及若干失败模式，指出研究在多模态推理、可解释性与基准构建上的潜在方向。

Conclusion: 该论文提出PixelArena基于语义分割任务评估具图像输出的多模态大模型，以像素级精度衡量细粒度生成能力，发现Gemini 3 Pro Image在零样本下能高保真生成语义掩码，展示了新型视觉智能并具有推广能力，同时给出比较、失败案例与对未来研究的启示。

Abstract: Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.

</details>


### [37] [LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation](https://arxiv.org/abs/2512.16313)
*Haiyu Zhao,Yiwen Shan,Yuanbiao Gou,Xi Peng*

Main category: cs.CV

TL;DR: 提出LaverNet，一种轻量（362K参数）的视频全能修复网络，通过选择性在帧间传递与退化无关的特征来减轻时变退化对时序建模的干扰，在多个基准上以远小于现有模型的参数量实现可比或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的一体化视频修复模型在处理时变退化时面临两大问题：退化信息主导时序建模，使模型关注伪影而非视频内容；且现有方法依赖大型网络掩盖了这类难题。作者希望通过紧凑模型明确研究这些挑战并提出有效机制。

Method: 提出LaverNet，一种仅362K参数的轻量网络。关键创新是一个选择性传播机制（propagation），仅在帧间传递与退化无关的特征，从而避免退化信息干扰时序信息聚合。整体设计用于在小模型容量下实现强的全能修复能力。

Result: 尽管参数少于现有模型的1%，LaverNet在多个基准上取得了可比甚至更好的修复效果，证明选择性传播与紧凑架构能有效应对时变退化问题。

Conclusion: 通过设计选择性传递退化无关特征的传播机制，LaverNet展示了在非常紧凑的模型规模下实现强大一体化视频修复的可能性，提示未来研究可关注更高效的时序信息提取与冗余去除。

Abstract: Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.

</details>


### [38] [Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs](https://arxiv.org/abs/2512.16314)
*Huayu Huang,Chen Chen,Banglei Guan,Ze Tan,Yang Shang,Zhang Li,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出将岭回归用于无人机影像与激光测距融合定位，能在观测受限时缓解多重共线性导致的病态问题，提升定位精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 提高无人机多传感器定位在限制观察条件（远距离、小交角、大倾角）下的稳定性与精度，解决最小二乘估计中因设计矩阵列间多重共线性导致的病态问题。

Method: 提出基于岭估计的融合定位方法，将序列影像的信息与激光测距高精度结合；在带约束的观测条件下，用岭回归替代普通最小二乘以稳定求解，减小估计方程的条件数，增强鲁棒性。

Result: 实验表明，与基于单一信息的地面定位算法相比，所提方法在定位精度上更优；在有限观测条件下引入岭估计显著提高了估计的稳定性与鲁棒性。

Conclusion: 在复杂几何观测条件下（如长距离、小交角、大倾角），通过将岭估计应用于无人机多传感器融合定位，可有效缓解多重共线性引发的病态问题，从而提高定位精度与稳定性。

Abstract: Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.

</details>


### [39] [QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing](https://arxiv.org/abs/2512.16325)
*Nan Zhou,Zuxin Li,Fanhang Man,Xuecheng Chen,Susu Xu,Fan Dang,Chaopeng Hong,Yunhao Liu,Xiao-Ping Zhang,Xinlei Chen*

Main category: cs.CV

TL;DR: QUIDS improves urban NVMCS QoI by combining a new ASQ metric with a belief-aware dispatching and incentive mechanism, yielding substantial ASQ gains and reduced reconstruction errors in real deployments.


<details>
  <summary>Details</summary>
Motivation: Achieve optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) by addressing sensing coverage, sensing reliability, and dynamic participation under budget constraints.

Method: Propose QUIDS: introduces Aggregated Sensing Quality (ASQ) metric combining coverage and reliability; develop Mutually Assisted Belief-aware Vehicle Dispatching algorithm to estimate reliability and allocate incentives under uncertainty; jointly optimize dispatching and incentives.

Result: Using metropolitan real-world data, QUIDS improves ASQ by 38% over non-dispatching and 10% over state-of-the-art; reduces reconstruction map errors by 39–74% across algorithms.

Conclusion: Quality-informed incentive-driven joint optimization of coverage and reliability enables low-cost, high-quality urban monitoring in NVMCS for smart-city applications like traffic and environmental sensing.

Abstract: This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.

</details>


### [40] [Collaborative Edge-to-Server Inference for Vision-Language Models](https://arxiv.org/abs/2512.16349)
*Soochang Song,Yongjune Kim*

Main category: cs.CV

TL;DR: 提出一种边缘-服务器协同推理方法：服务器先用低分辨率全局图像推理并基于注意力找ROI与最小熵置信度决定是否请求高分辨率局部图像，选择性重传显著降低通信开销且保持精度。


<details>
  <summary>Details</summary>
Motivation: 减少边缘设备到服务器在视觉语言模型推理中的通信成本，同时保持推理精度，通过选择性重传局部高分辨率图像。

Method: 提出两阶段协作框架：服务器先对全局缩放图像进行推理，利用模型内部注意力定位ROI，并用输出标记的最小熵作为置信度衡量；若最小熵超过阈值则请求边缘设备发送保留细节的局部图像，然后服务器结合全局与局部图像进行联合推理。

Result: 在多个VLM架构上的实验表明，该选择性重传策略显著降低了通信成本，同时在精度上保持与完整重传接近的性能。

Conclusion: 通过基于注意力的ROI定位和基于最小熵的置信度判定，框架实现了有效的带宽节省与精度保留，适用于边缘-云协作的视觉语言任务部署。

Abstract: We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.

</details>


### [41] [GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction](https://arxiv.org/abs/2512.16357)
*Tao Hu,Weiyu Zhou,Yanjie Tu,Peng Wu,Wei Dong,Qingsen Yan,Yanning Zhang*

Main category: cs.CV

TL;DR: 将HDR重建改为生成增益图的单步条件扩散，结合回归先验以兼顾保真与感知，达到高质量且快速的重建。


<details>
  <summary>Details</summary>
Motivation: 针对直接用预训练潜变量扩散模型处理HDR时的位深限制、多步去噪开销及生成幻觉问题，提出用增益图表征扩展动态范围并用回归先验引导单步扩散以解决这些挑战。

Method: 提炼方法与创新点

Result: GMODiff: 将HDR重建转化为条件化的增益图估计任务，使用LDM在潜变量空间生成增益图，保持LDR位深，减小动态范围表示损失。采用从回归估计初始化的单步去噪（one-step diffusion），避免多步扩散的高昂推理成本。融合回归先验引导去噪与潜空间解码，抑制生成性幻觉并保留结构一致性。

Conclusion: GMODiff在保持较好感知质量的同时显著提高推理速度（比先前LDM方法快100倍），并在多项基准上优于若干最新方法。

Abstract: Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.

</details>


### [42] [EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation](https://arxiv.org/abs/2512.16360)
*Haotian Ling,Zequn Chen,Qiuying Chen,Donglin Di,Yongjia Ma,Hao Li,Chen Wei,Zhulin Tao,Xun Yang*

Main category: cs.CV

TL;DR: 提出EverybodyDance：用基于IMG的图匹配和MQA来保证多角色动画的身份一致性，并提供评估基准，显著优于前沿方法。


<details>
  <summary>Details</summary>
Motivation: 现有姿势驱动动画在单角色上表现良好，但扩展到多角色场景（尤其位置互换）时，关键问题是如何保证生成帧与参考帧间的身份对应（IC），这是简单扩展规模无法解决的结构性问题。

Method: 构建Identity Matching Graph，将生成与参考帧中的角色作为二分图节点集。使用Mask-Query Attention计算每对角色间的亲和度权重，形成加权完全二分图。将IC正确性定义为图结构指标并在训练中优化。同时设计身份嵌入引导、多尺度匹配策略和预分类采样以强化多角色匹配学习。

Result: 通过提出的IMG+MQA框架与配套策略，模型在新构建的Identity Correspondence Evaluation基准上，在身份一致性和视觉质量上均显著超过多种最先进基线。

Conclusion: EverybodyDance提出通过构建Identity Matching Graph (IMG)并使用Mask-Query Attention (MQA)计算节点间匹配权重，将多角色动画中的身份一致性（IC）问题形式化为图结构优化问题，从而在训练中直接优化IC正确性，辅以身份嵌入引导、多尺度匹配与预分类采样等策略，并提供专门的评估基准Identity Correspondence Evaluation。实验表明在IC和视觉保真度上均优于现有方法。

Abstract: Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.

</details>


### [43] [Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models](https://arxiv.org/abs/2512.16371)
*Mariam Hassan,Bastien Van Delft,Wuyang Li,Alexandre Alahi*

Main category: cs.CV

TL;DR: 本文提出Factorized Video Generation (FVG)，将文本到视频生成分解为三个阶段：使用大语言模型生成初始帧描述、用文本到图像模型合成高质量锚帧、再由视频模型在该锚帧上合成时间连贯的视频。该方法在T2V基准上取得新SOTA，并能在不损失性能下将采样步数减少70%。


<details>
  <summary>Details</summary>
Motivation: 当前T2V扩散模型在构图复杂场景或遵循时间逻辑指令时常失败，许多错误源于模型无法生成语义正确或逻辑一致的初始帧。作者提出通过解耦初始构图与时序合成来缓解这一问题。

Method: FVG由三阶段组成：1) Reasoning：用LLM改写视频提示为仅描述初始场景以消除时序歧义；2) Composition：用T2I模型根据改写提示生成高质量、构图正确的锚帧；3) Temporal Synthesis：微调视频模型以理解该锚帧，专注于为场景赋予动画并遵循提示。作者还利用视觉锚定减少采样步数以加速生成。

Result: 在T2V CompBench上达到新的SOTA，并在VBench2上显著提升所有测试模型表现。利用锚帧可在不损失性能的情况下将采样步数减少70%，显著加速采样过程。

Conclusion: 通过将T2V生成因子化为推理、构图和时序合成三步，FVG提高了鲁棒性、可控性与效率，为更可靠的视频合成提供了实用路径。

Abstract: State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis

</details>


### [44] [Adaptive Frequency Domain Alignment Network for Medical image segmentation](https://arxiv.org/abs/2512.16393)
*Zhanwei Li,Liang Li,Jiawan Zhang*

Main category: cs.CV

TL;DR: 提出AFDAN，一种在频域对齐的域自适应网络，通过对抗域学习、源-目标频率融合、时空-频率集成三模块，提升跨域医学图像分割。VITILIGO2025数据集上Vitiligo IoU 90.9%，DRIVE血管分割IoU 82.6%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割标注稀缺且昂贵，域间分布差异限制了模型泛化。文章旨在通过频域对齐和跨域特征融合来缓解标注稀缺、提升跨域分割性能。

Method: AFDAN由三部分组成：1) 对抗域学习模块（Adversarial Domain Learning Module），通过对抗训练将源域特征迁移到目标域；2) 源-目标频率融合模块（Source-Target Frequency Fusion Module），在频域混合两域的频率表示以实现更鲁棒的特征对齐；3) 空间-频率集成模块（Spatial-Frequency Integration Module），将频域特征与空间特征融合以增强分割精度。整体在训练中实现频域与空间域的信息互补，并使用标准分割损失与对抗损失联合优化。

Result: 在新构建的VITILIGO2025数据集上，AFDAN在白癜风分割任务上达到IoU 90.9%。在DRIVE视网膜血管分割基准上取得IoU 82.6%，超过现有多种先进方法，表明频域对齐和跨域融合在医学分割中的有效性。

Conclusion: 通过在频域进行自适应对齐并结合空间信息，AFDAN有效缓解了医学分割中的标注稀缺和域差异问题，显著提升了跨域分割性能，适用于不同医学影像分割任务。

Abstract: High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.

</details>


### [45] [Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture](https://arxiv.org/abs/2512.16397)
*Haodi He,Jihun Yu,Ronald Fedkiw*

Main category: cs.CV

TL;DR: 本文提出基于Gaussian Splatting的三维人脸重建方法，利用分割对齐和弱约束三角化表面，实现从少量未标定图像恢复中性姿态、生成可用于标准图形管线的三角网格与高质量去光照漫反射纹理。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF的三维人脸重建在约束、显式几何提取和与传统图形管线兼容性方面存在不足；作者希望用更显式易约束的Gaussian Splatting从少量未校准图像中获得一致的几何与可编辑纹理。

Method: 采用Gaussian Splatting表示并用分割标注对面部语义区域对齐，软约束高斯点到基础三角化表面以结构化重建，再通过迭代扰动精化三角面片；引入可再照明高斯模型将高质量高斯点投射到纹理空间，分离纹理与光照得到去光照高分辨率漫反射（albedo）纹理，支持异构光照下的训练与正则化，并可在标准图形管线中使用。

Result: 从仅11张未标定图像恢复出中性姿态的三角形网格与去光照漫反射纹理，保持高视觉保真度，且能将高斯点转换为纹理空间作为视角依赖神经纹理，用于任意场景资产而无需修改其他资产或渲染设置；支持文本驱动的资产创建示例。

Conclusion: 通过将Gaussian Splatting与语义对齐、三角化软约束和可再照明纹理分离相结合，作者实现了从少量未校准图像中恢复出可用于传统图形管线的高质量几何与去光照纹理，提升了可用性与灵活性。

Abstract: We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.

</details>


### [46] [BrepLLM: Native Boundary Representation Understanding with Large Language Models](https://arxiv.org/abs/2512.16413)
*Liyuan Deng,Hao Guo,Yunpeng Bai,Yongkang Dai,Huaxi Huang,Yilei Shi*

Main category: cs.CV

TL;DR: 提出BrepLLM：用自适应UV采样+分层BrepEncoder把Brep转成全局/节点tokens，先做全局对齐（与CLIP文本向量对比学习），再把节点tokens接入LLM并通过三阶段进阶微调（语义映射、LLM微调、MQE）训练；并构建269k Brep-文本对数据集，在分类与描述任务上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于令牌序列的LLM不擅长直接处理包含复杂几何与拓扑信息的原始Brep模型，需跨模态桥接三维结构化几何与自然语言。

Method: 两阶段流程：1) 跨模态对齐预训练：自适应UV采样将Brep转换为带几何/拓扑信息的图，分层BrepEncoder提取面/边等几何与拓扑特征，生成一个全局token和节点序列token；通过对比学习将全局token与冻结的CLIP文本编码器（ViT-L/14）输出对齐。2) 多阶段LLM微调：把预训练的BrepEncoder接入LLM，并用三阶段进阶训练对齐节点序列：(1) 用MLP将Brep表示语义映射到具2D-LLM先验的空间，(2) 对LLM进行微调，(3) 设计Mixture-of-Query Experts提升几何多样性建模。同时构建Bre p2Text 数据集用于监督训练。

Result: 在3D物体分类和captioning任务上，BrepLLM在所报实验中达到SOTA性能，验证了对齐策略与三阶段微调的有效性。

Conclusion: BrepLLM提出了一个将Brep（三维边界表示）与LLM结合的两阶段训练框架，能够将几何/拓扑信息编码为全局与节点序列tokens并与文本向量对齐，实现对原始Brep数据的理解与推理。

Abstract: Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.

</details>


### [47] [CountZES: Counting via Zero-Shot Exemplar Selection](https://arxiv.org/abs/2512.16415)
*Muhammad Ibraheem Siddiqui,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出CountZES：三阶段（DAE、DGE、FCE）无训练零样本示例选择方法，结合检测精炼、密度自监督与特征聚类，显著提升跨域目标计数性能。


<details>
  <summary>Details</summary>
Motivation: 现有ZOC方法在从类名生成实例示例时存在两类问题：开放词汇检测器常输出含多实例的候选窗，随机补丁采样无法准确划定单个实例。需要一种无训练、可适配不同域的示例选择方法，以获得准确、代表性且互补的单实例示例集合，从而提高计数精度。

Method: CountZES包含三阶段示例发现流程：1) DAE：利用开放词汇检测器并通过策略（如分割/置信与尺寸约束）精炼检测框以筛选单实例示例；2) DGE：采用密度引导的自监督准则，基于预测密度图或相对响应稳定性选取在计数上统计一致的示例；3) FCE：在视觉特征空间进行聚类和一致性验证，去除异常或语义漂移样本，最终构建多样且互补的示例集用于零样本计数推理。

Result: 在多个数据集（自然场景、航空影像、医学图像）上，CountZES在标准ZOC基线之上实现了最优或显著提升，证明了其在跨域泛化与示例质量方面的有效性。

Conclusion: CountZES提出了一种无训练且稳健的零样本示例选择框架，通过检测锚定、密度引导与特征一致性三阶段互补策略，提升了从文本到实例的示例提取质量，从而在不同域的目标计数任务中取得优于现有ZOC方法的表现。

Abstract: Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.

</details>


### [48] [Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt](https://arxiv.org/abs/2512.16443)
*Shangxun Li,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出一种无训练的几何方法，细化文本嵌入以抑制跨帧语义纠缠，从而提高多帧生成中的主体一致性和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在多帧视觉叙事中主体不一致和已有方法（微调或基于图像的条件化）计算代价高且需每主体优化的问题。

Method: Geometric refinement of text embeddings to suppress unwanted semantics, training-free, concatenates scene descriptions but refines embeddings per-frame to reduce entanglement

Result: Improved subject consistency and text alignment compared to baselines like 1Prompt1Story; extensive experiments validate effectiveness

Conclusion: Training-free geometric embedding refinement reduces semantic leakage and yields better multi-frame subject consistency and alignment

Abstract: Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.

</details>


### [49] [Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach](https://arxiv.org/abs/2512.16456)
*Masashi Hatano,Saptarshi Sinha,Jacob Chalk,Wei-Hong Li,Hideo Saito,Dima Damen*

Main category: cs.CV

TL;DR: Curated 23.7K gaze-primed reaching sequences; trained diffusion model; achieves 60% prime and 89% reach success on HD-EPIC.


<details>
  <summary>Details</summary>
Motivation: To model realistic human reaching motions that include gaze priming (spotting before approach), enabling generation of more natural human motion.

Method: Diffusion-based, text-conditioning pretrain and fine-tune on gaze-primed sequences; goal-conditioned generation.

Result: On HD-EPIC: 60% Prime Success, 89% Reach Success; dataset curated 23.7K sequences from five datasets.

Conclusion: Model can generate gaze-primed reaching motions with reasonable success; shows effectiveness of fine-tuning on curated gaze-primed data.

Abstract: Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.

</details>


### [50] [SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning](https://arxiv.org/abs/2512.16461)
*Tin Stribor Sohn,Maximilian Dillitzer,Jason J. Corso,Eric Sax*

Main category: cs.CV

TL;DR: 提出SNOW，一个训练免费、骨干无关的框架，将VLM语义与点云几何和时间一致性融合，生成可查询的4D场景图用于机器人推理；方法包括基于HDBSCAN聚类的提议、SAM2分割、STEP编码和SLAM对齐；在多项基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有VLM缺乏3D几何和时间动态的约束，而纯几何感知语义稀少，二者结合对自主机器人在动态环境中的鲁棒导航和交互至关重要。

Method: 输入同步RGB图像与点云，使用HDBSCAN生成对象级提议并引导SAM2分割；对每个分割区采用STEP（时空标记化补丁编码）生成多模态token，包含局部语义、几何和时间属性；逐步将token集成入4D场景图（4DSG）；轻量SLAM后端为STEP token提供全局空间锚定，保证时间上的无歧义定位。

Result: 在多种基准实验中，SNOW在4D场景理解与空间锚定推理任务上取得了多项最先进成绩，表明结构化4D先验对具身推理的重要性。

Conclusion: SNOW展示了将开放世界语义与时空几何结合的可行路径，为机器人系统提供了一个强大的、可查询的4D世界模型，从而提升了跨时空的语义理解和推理能力。

Abstract: Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.

</details>


### [51] [StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models](https://arxiv.org/abs/2512.16483)
*Senmao Li,Kai Wang,Salman Khan,Fahad Shahbaz Khan,Jian Yang,Yaxing Wang*

Main category: cs.CV

TL;DR: 提出StageVAR：保留早期尺度预测、对晚期步骤用语义不相关性和低秩近似加速，无需训练，最高3.4x提速且质量仅小幅下降。


<details>
  <summary>Details</summary>
Motivation: VAR在大规模步骤下计算量急剧增长，现有方法依赖人工选择步骤且忽视各阶段重要性差异，需提出自动化且阶段感知的加速策略。

Method: 分析生成过程各阶段的重要性，保留早期关键步骤；对晚期步骤利用语义无关性剪枝和低秩近似以减少计算与时间开销，方案为plug-and-play、无需额外训练。

Result: StageVAR通过阶段感知的方式对Visual Autoregressive模型进行加速，主张保留关键的早期尺度预测步骤并对后期细化步骤采用剪枝和低秩近似，从而在不需额外训练的情况下显著降低计算开销。

Conclusion: 阶段感知的设计能有效平衡生成质量与计算效率，StageVAR在保证视觉质量的前提下取得了显著的加速，优于现有加速基线。

Abstract: Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.

</details>


### [52] [Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment](https://arxiv.org/abs/2512.16484)
*Yuan Li,Yahan Yu,Youyuan Lin,Yong-Hao Yang,Chenhui Chu,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文提出一种通过强化学习使盲图像质量评估（BIQA）模型同时具备类人感知与自洽推理能力的方法。作者收集人类感知-推理链数据，将人类标注作为奖励信号，设计促使模型从自生成描述中推断质量的自洽奖励，能在预测分数和生成解释上与人类表现对齐。实验在相关指标上表现良好，解释的ROUGE-1显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有BIQA主要关注分数预测，缺乏对人类感知与推理过程的模拟，也缺乏可解释和自洽的推理链。作者希望模型不仅预测与人类一致的质量分数，还能生成与人类相似且自洽的推理链，提升可解释性与可信度。

Method: 1) 收集带有人类感知-推理链的标注数据；2) 使用强化学习（人类标注作为奖励）训练模型，使其学习类人感知与推理策略；3) 设计自洽性奖励，促使模型仅依靠自身生成的描述推断质量分数；4) 同时优化评分预测与解释生成，评估包括Pearson/Spearman及ROUGE-1。

Result: 模型在Pearson与Spearman相关系数上达到与最新BIQA系统可比的性能；在解释生成上，基于超过1000个人类标注样本，模型的ROUGE-1为0.512，高于基线0.443，表明生成的解释覆盖了更多人类关注点。

Conclusion: 通过以人类标注为奖励并引入自洽性约束的强化学习训练，模型在BIQA任务上既能提供与人类一致的质量评分，又能生成更接近人类的解释链，推动了可解释且自洽的类人BIQA研究。

Abstract: Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.

</details>


### [53] [Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors](https://arxiv.org/abs/2512.16485)
*Kejun Liu,Yuanyuan Liu,Lin Wei,Chang Tang,Yibing Zhan,Zijing Chen,Zhe Chen*

Main category: cs.CV

TL;DR: 提出EMER数据集与EMERT模型，采集自发诱发情绪时的眼动和面部视频，标注多视角情绪标签，实验证明眼动能显著提高情感识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有情感识别过度依赖面部表情，而面部表情常作为社交工具与真实内在情绪脱节，需引入更多能反映真实情绪的生理/行为信号——眼部行为。

Method: 构建包含眼动序列与注视图的EMER数据集；分别标注用于ER和FER的多视角情绪标签；提出EMERT，包含模态对抗特征解耦模块和多任务Transformer，用以融合眼部行为与面部特征并消除面部表情与内在情绪之间的混淆。

Result: 构建了一个Eye-behavior-aided Multimodal Emotion Recognition (EMER) 数据集，并提出了EMERT模型。

Conclusion: 眼部行为是补充面部表情进行情感识别的重要线索；EMERT通过模态对抗特征解耦和多任务Transformer显著提升多模态情感识别性能。

Abstract: Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.

</details>


### [54] [YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images](https://arxiv.org/abs/2512.16493)
*Huma Hafeez,Matthew Garratt,Jo Plested,Sankaran Iyer,Arcot Sowmya*

Main category: cs.CV

TL;DR: 提出针对4K全景的YOLO11-4K，加入P2层和GhostConv以提升小目标检测与效率；手工标注CVIP360基准；实现更低延迟（28.3ms）和更高mAP（0.95）。


<details>
  <summary>Details</summary>
Motivation: 处理全景360度图像在物体检测中存在挑战，包括空间畸变、大视场和超高分辨率；常规模型（如YOLO）对4K以上图像计算负担大且精度下降。

Method: 提出YOLO11-4K：引入带P2层的多尺度检测头以提升小目标检测；采用GhostConv轻量化主干以降低计算复杂度；对CVIP360数据集进行手工标注，生成4K检测基准。

Result: 在标注的CVIP360上，YOLO11-4K在0.50 IoU下达到0.95 mAP，推理延迟28.3 ms/帧，相比YOLO11的112.3 ms减少75%，且精度从0.908提升到0.95。

Conclusion: YOLO11-4K在效率和精度上取得显著平衡，适合4K全景场景检测，并可推广至自动导航、监控与增强现实等高分辨率检测任务。

Abstract: The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.

</details>


### [55] [PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2512.16494)
*Mengyuan Liu,Jiajie Liu,Jinyan Zhang,Wenhao Li,Junsong Yuan*

Main category: cs.CV

TL;DR: 提出PoseMoE：用Mixture-of-Experts分离2D与深度特征学习，并通过跨专家聚合提高单目3D人体姿态估计精度，优于传统lifting方法。


<details>
  <summary>Details</summary>
Motivation: 传统lifting方法将检测到的2D姿态与未知深度混合编码，深度不确定性会污染2D特征，限制精度；若深度先被网络粗估为可靠，再与2D共同编码则有利，因此需要先区分处理并专门学习深度表示。

Method: Mixture-of-Experts for monocular 3D pose (PoseMoE). Experts refine 2D pose features and learn depth features; cross-expert aggregation fuses spatio-temporal context.

Result: PoseMoE disentangles 2D and depth encoding; uses expert modules and cross-expert aggregation; outperforms lifting methods on Human3.6M, MPI-INF-3DHP, 3DPW.

Conclusion: Disentangling depth and 2D representations via MoE and cross-expert aggregation reduces negative influence of uncertain depth and improves 3D pose accuracy.

Abstract: The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.

</details>


### [56] [VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks](https://arxiv.org/abs/2512.16501)
*Beitong Zhou,Zhexiao Huang,Yuan Guo,Zhangxuan Gu,Tianyu Xia,Zichen Luo,Fei Tang,Dehan Kong,Yanyi Shang,Suling Ou,Zhenlin Guo,Changhua Meng,Shuheng Shen*

Main category: cs.CV

TL;DR: 本文提出 VenusBench-GD：一个跨平台、双语、层级评估的 GUI grounding 基准，包含大规模、注释质量高的数据集与六类细分子任务。实验显示通用多模态模型在基础任务上可与专用 GUI 模型相当或更好，但高级任务仍需专用模型且存在过拟合与鲁棒性差问题。


<details>
  <summary>Details</summary>
Motivation: 现有 GUI grounding 基准要么数据量小、覆盖域窄，要么局限于单一平台并需专门领域知识，难以全面评估模型实际应用能力。需要一个大规模、跨平台、层级化的评测基准以衡量模型在现实场景中的表现。

Method: 构建 VenusBench-GD：收集多平台、多应用的大规模双语界面数据；设计高质量标注流水线以提升注释准确率；提出层级任务分类（基础与高级），将元素定位/识别等划分为六个子任务以从互补视角评估模型。

Result: 实验表明：通用多模态模型在基本 grounding 任务上达到或超过专用 GUI 模型；高级任务则仍由专用模型领先，但这些模型表现出明显过拟合与鲁棒性问题。

Conclusion: 需要更全面的多层级评测框架来推动 GUI grounding 研究，鼓励发展兼具通用性和专用能力且更具鲁棒性的模型。

Abstract: GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.

</details>


### [57] [Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization](https://arxiv.org/abs/2512.16504)
*Qiushuo Cheng,Jingjing Liu,Catherine Morgan,Alan Whone,Majid Mirmehdi*

Main category: cs.CV

TL;DR: 针对骨架序列的时序动作定位提出自监督片段判别预训练任务，结合U形模块提升帧级特征分辨率，在BABEL和PKUMMD上获得提升和迁移SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的自监督预训练在视频/骨架动作识别效果好，但对时序定位欠缺；定位要求对相邻帧的细微差异高度敏感，需要时序敏感的特征。

Method: 设计snippet discrimination任务：将骨架序列密集划分为不重叠片段，通过对比学习促使模型区分来自不同视频的片段；在强backbone基础上加入U形特征融合模块以提高中间特征分辨率，便于帧级边界检测。

Result: 在BABEL数据集不同子集和评估协议上，所提方法持续改善现有骨架对比学习方法的定位表现；在PKUMMD上，通过在NTU RGB+D和BABEL上的预训练，取得了迁移学习的最先进性能。

Conclusion: 通过面向片段的自监督任务与U形分辨率增强模块，可以从骨架数据中学到更具时序敏感性的特征，显著提升骨架时序动作定位和跨数据集迁移能力。

Abstract: The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.

</details>


### [58] [Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images](https://arxiv.org/abs/2512.16511)
*Hossein Javidnia*

Main category: cs.CV

TL;DR: MAGINet通过层次残差编码、瓶颈处的空域与通道注意力、解码器的自适应多尺度特征融合以及高分辨率微调与Pix2PixHD翻译器，实现在单张人脸图片上高质量的光照不变漫反射估计与完整的五通道物理渲染分解。


<details>
  <summary>Details</summary>
Motivation: 提升单张人脸在任意光照条件下的内在分解精度，以便用于真实感重光照、高保真数字替身与增强现实效果。

Method: 使用MAGINet主干（层次残差编码、瓶颈双重注意力、自适应多尺度解码），之后用三层RefinementNet进行高分辨率细化，最后以Pix2PixHD风格翻译器生成其余五个渲染通道，训练损失包括masked-MSE、VGG、边缘和patch-LPIPS。

Result: MAGINet提出了一种多尺度注意力引导的头像内在分解网络，能从单张RGB人脸预测512x512的光照归一化漫反射反照率（albedo），并通过RefinementNet提升到1024x1024，再由Pix2PixHD翻译器生成环境遮蔽、法线、高光反射、次表面散射和带残余光照的原始漫反射共五个渲染通道，构成完整的物理渲染通道集合。

Conclusion: MAGINet在FFHQ-UV-Intrinsics数据集上使用混合损失训练后，在漫反射反照率估计上达到了SOTA，并且在完整渲染栈上的真实头像重光照和材质编辑上表现出更高的保真度。

Abstract: Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.

</details>


### [59] [TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models](https://arxiv.org/abs/2512.16523)
*Zhiwei Li,Yitian Pang,Weining Wang,Zhenan Sun,Qi Li*

Main category: cs.CV

TL;DR: TTP通过在推理前后对输入做空间padding并比对CLIP特征余弦相似度来检测对抗样本，检测到后使用可训练padding和相似性感知集成进行有针对性的调整；对干净样本则保持不变或可选应用现有测试时适应方法，从而在多种CLIP骨干和细粒度数据集上同时提升对抗鲁棒性和保持干净精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在零样本识别表现优异但对对抗扰动高度脆弱；训练时防御需要有标签数据和昂贵的再训练，而现有测试时策略难以同时保证对抗鲁棒性与干净精度，故需一种轻量且无需标签的测试时方法。

Method: TTP首先在推理阶段计算输入在原始与空间padding后的CLIP特征余弦相似度变化，以此设定通用阈值检测对抗样本；对检测为对抗的输入，使用可训练的padding模块恢复被扰乱的注意力，并通过相似性感知的模型集成获得更稳健的预测；对未检测为对抗的干净输入则默认不变或可与其他测试时自适应方法结合。

Result: 在多种CLIP骨干网络与细粒度基准上，TTP在不降低干净样本精度的情况下显著超过现有测试时防御方法，在对抗鲁棒性上取得大幅提升，并展示了跨架构和跨数据集的通用检测阈值性能。

Conclusion: 本文提出的Test-Time Padding (TTP)为VLMs提供了在推理时检测并修复对抗样本的轻量级方案，有效提升了对抗鲁棒性且不牺牲干净样本精度。

Abstract: Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.

</details>


### [60] [N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.16561)
*Yuxin Wang,Lei Ke,Boqiang Zhang,Tianyuan Qu,Hanxun Yu,Zhenpeng Huang,Meng Yu,Dan Xu,Dong Yu*

Main category: cs.CV

TL;DR: Novel framework combining native 3D perception and 3D reasoning; large-scale depth-lifted data; SOTA in 3D grounding and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs lack intrinsic 3D object perception, limiting spatial and depth understanding; need for interpretable 3D grounding and reasoning.

Method: Summarize methodology, results, conclusions, TL;DR, and motivation of the paper described in the abstract.

Result: See fields

Conclusion: N3D-VLM provides strong 3D grounding and reasoning by integrating native 3D perception with VLMs, using depth-lifted large-scale data and CoT-style QA training, achieving SOTA.

Abstract: While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.

</details>


### [61] [4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction](https://arxiv.org/abs/2512.16564)
*Kirill Mazur,Marwan Taher,Andrew J. Davison*

Main category: cs.CV

TL;DR: 方法将场景分解为可运动的刚性3D基元，利用密集2D对应联合优化其刚性运动并外推不可见物体运动，生成可回放的4D持久重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单目视频动态场景重建中，要获得对整个场景随时间完整且持久的几何表示（包括过去已看见但当前不可见的部分），以支持回放、多物体扫描和物体永久性等能力。

Method: 1) 将场景表示为多个可移动的刚性3D基元；2) 通过估计的密集2D对应关系，联合优化这些基元的三维刚性运动，得到随时间变化的4D几何；3) 引入基于运动分组的外推机制来维持对不可见对象运动的连续性；4) 输出完整持久的重建并可在任意时间步回放。

Result: Dynamic reconstruction system producing persistent 4D scene reconstructions from monocular RGB video.

Conclusion: Decomposing scenes into moving rigid 3D primitives and optimizing their motions using dense 2D correspondences yields superior replayable 4D reconstructions with object permanence; motion extrapolation preserves continuity when objects become invisible.

Abstract: We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.
  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.
  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.

</details>


### [62] [Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2512.16567)
*Yin Zhang,Yongqiang Zhang,Yaoyue Zheng,Bogdan Raducanu,Dan Liu*

Main category: cs.CV

TL;DR: 本文提出Causal-Tune，一种在频域识别并分离VFM特征中因果与非因果成分的微调方法，通过DCT提取频谱、用高斯带通滤波分离、引入可学习因果token并丢弃非因果分量，逆DCT回到空间域，从而提升跨域语义分割的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 长期预训练的视觉基础模型含有会损害泛化的伪影，这些伪影与非因果因素相关，集中在低高频谱成分，现有轻量微调方法未显式处理这些非因果信息，导致在未知域上性能下降。

Method: Causal-Tune流程：对每层特征做DCT得到频谱；用高斯带通滤波器将频谱分成因果与非因果部分；保留因果频段并引入因果感知可学习token在频域进一步精炼；丢弃非因果频段；最后逆DCT回到空间域并传递至下一层。仅微调少量参数，聚焦频域token。

Result: 在多种跨域语义分割任务上做了广泛实验，尤其在恶劣天气下表现显著。示例：在雪天条件下，相较基线mIoU提升约4.8%。

Conclusion: 通过在频域上识别并抑制非因果成分，Causal-Tune能更好地利用VFM中有用表征、提高未知域下的分割性能，为轻量级微调提供了一条有效路径。

Abstract: Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.

</details>


### [63] [CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series](https://arxiv.org/abs/2512.16577)
*Nico Albert Disch,Saikat Roy,Constantin Ulrich,Yannick Kirchhoff,Maximilian Rokuss,Robin Peretzke,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: 提出CRONOS：在3D体素空间学习时空速度场，支持多上下文和连续时间的序列到图像预测，实验在Cine-MRI、灌注CT和纵向MRI数据上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于单帧先验、固定格网时间或仅预测全局标签，无法在不规则采样下进行体素级连续时间预测；临床应用需要能够从多次过往扫描预测任意时间的三维图像。

Method: CRONOS在3D体素空间直接学习一个时空速度场，用以将多个上下文体积通过流场（transport）传送至任意实值时间点的目标体积；模型同时支持离散网格时间和连续时间预测，并在三组公共数据集上进行了定量比较与效率评估。

Result: CRONOS提出一个用于多上下文、多时间戳3D医学影像预测的统一框架。

Conclusion: CRONOS通过学习三维体素空间的时空速度场实现从多个过去扫描到任意连续时间点目标体积的预测，性能在三个数据集上优于基线并具备计算效率。

Abstract: Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.

</details>


### [64] [Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs](https://arxiv.org/abs/2512.16584)
*Jintao Tong,Jiaqi Gu,Yujing Lou,Lubin Fan,Yixiong Zou,Yue Wu,Jieping Ye,Ruixuan Li*

Main category: cs.CV

TL;DR: SkiLa lets multimodal LLMs alternate between textual thinking and generating latent visual tokens, grounding these tokens via semantic reconstruction to improve visual reasoning.


<details>
  <summary>Details</summary>
Motivation: Human thinking flexibly interleaves visual and textual imagination in a unified internal space; SkiLa mimics this by inserting visual tokens into the text reasoning stream since current MLLMs already share visual-text feature spaces.

Method: Introduce Sketch-in-Latents (SkiLa)

Result: SkiLa extends autoregressive MLLMs to generate continuous visual embeddings (latent sketch tokens) alongside text tokens, enabling unified visual-text reasoning.

Conclusion: SkiLa enables MLLMs to perform internal visual imagination via latent sketch tokens, improving vision-centric task performance and generalization.

Abstract: While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.

</details>


### [65] [Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks](https://arxiv.org/abs/2512.16586)
*Shaohua Wu,Tong Yu,Shenling Wang,Xudong Zhao*

Main category: cs.CV

TL;DR: Replace CNNs with Swin-transformer in diffusion U-Net, enhance text conditioning and use adaptive time-step search to get SOTA ImageNet FID 1.37 and highly realistic images.


<details>
  <summary>Details</summary>
Motivation: Diffusion models with U-shaped architecture using CNNs are limited by convolution locality, constraining long-range semantic understanding. The paper aims to enhance non-local modeling and text-conditioned image synthesis.

Method: Replace CNN blocks in encoder and decoder with Swin-transformer blocks; improve text-image alignment via a better text encoder and embedding utilization; carefully design text condition incorporation; use adapted time-step search across diffusion stages during inference to boost performance.

Result: Yuan-TecSwin attains state-of-the-art FID 1.37 on ImageNet generation without extra models at denoising stages; inference improved by ~10% with adapted time-step search; human evaluators often cannot distinguish generated images from human-painted ones.

Conclusion: Incorporating Swin-transformers into U-shaped diffusion architectures and improving text conditioning plus adaptive inference yields significant gains in image synthesis, achieving SOTA FID and highly realistic outputs.

Abstract: Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.

</details>


### [66] [Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment](https://arxiv.org/abs/2512.16609)
*Ayush Bhavsar*

Main category: cs.CV

TL;DR: Hazedefy 是基于 DCP 的简化实时去雾方法，包含伽马自适应重建、快速透射估计并保证数值稳定、分数顶像素平均的大气光稳健估计及可选色彩平衡，适合无 GPU 的消费设备


<details>
  <summary>Details</summary>
Motivation: 针对现有去雾算法在移动/嵌入式设备上计算复杂或难以实时部署，Hazedefy 追求计算简单且实用的在线去雾解决方案

Method: 基于大气散射模型与暗通道先验，采用伽马自适应重建、引入下界的快速透射近似、基于分数顶像素平均的稳健大气光估计，及可选色彩平衡模块；优化数值稳定性并避免昂贵的滤波或深度学习网络

Result: 提出了 Hazedefy，一个轻量级、面向应用的去雾流水线，旨在实时视频与摄像头增强

Conclusion: 在不依赖 GPU 的情况下，Hazedefy 在消费级硬件上能提高能见度与对比度，适用于移动与嵌入式场景

Abstract: This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.

</details>


### [67] [Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers](https://arxiv.org/abs/2512.16615)
*Yifan Zhou,Zeqi Xiao,Tianyi Wei,Shuai Yang,Xingang Pan*

Main category: cs.CV

TL;DR: 提出LLSA：通过层级Top-K选择和KV富集把自注意力从二次降到对数线性，显著加速DiT训练/推理且保持质量


<details>
  <summary>Details</summary>
Motivation: 单级Top-K在压缩表示上选取有限关键块导致在长序列上仍有二次选择成本且需增大K维持质量，因单一粗粒度难以表达全局结构，故引入多级层次化选择与KV富集以保全全局信息并降低计算

Method: Hierarchical Top-K sparse attention with KV enrichment and GPU sparse-index implementation

Result: LLSA reduces selection and attention complexity from quadratic to O(n log n) via hierarchical Top-K selection and Hierarchical KV Enrichment; achieves 28.27x attention inference speedup and 6.09x DiT training speedup on 256x256 tokens while preserving generation quality

Conclusion: LLSA enables efficient long-sequence DiTs by combining multi-level selection and KV enrichment with a sparse-index GPU implementation, offering significant speedups without degrading image generation quality

Abstract: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

</details>


### [68] [Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation](https://arxiv.org/abs/2512.16620)
*Kanwal Aftab,Graham Adams,Mark Scanlon*

Main category: cs.CV

TL;DR: 本文提出基于电源插座的三阶段深度学习室内定位流水线：插座检测（YOLOv11）、插座类型分类（Xception）和基于类型映射到国家，数据集包括检测和分类两部分，结果在TraffickCam子集上表现良好，代码与模型开源。


<details>
  <summary>Details</summary>
Motivation: 室内多媒体地理定位在打击人口贩运和儿童性剥削等犯罪中有重要价值，但面临布局相似、翻新、光线变化、GPS失效及敏感领域数据稀缺等挑战。作者认为插座类型受国家/地区标准化约束，可作为稳定的地理标记。

Method: 提出三阶段流水线：1) 使用YOLOv11检测图像中的插座（检测数据集2,328张，经增强扩展到4,072张）；2) 使用Xception对检测到的插座进行12类插座类型分类（分类数据集3,187张）；3) 将插座类型映射到国家/地区，并基于置信度阈值进行最终定位决策。评估使用Hotels-50K中TraffickCam子集，模拟真实世界拍摄条件。

Result: 检测mAP@0.5 = 0.843；分类准确率 = 0.912；基于>90%置信度阈值的国家映射准确率 = 0.96。数据集与模型开源，且在TraffickCam现实条件下表现稳健。

Conclusion: 利用插座作为国家级室内地理标记是一种可行且实用的数字取证方法，尤其在真实世界、光线差和角度随意的场景中仍能取得高准确率，填补了室内地理定位的研究空白。

Abstract: Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.

</details>


### [69] [DeContext as Defense: Safe Image Editing in Diffusion Transformers](https://arxiv.org/abs/2512.16625)
*Linghui Shen,Mingyue Cui,Xingyi Yang*

Main category: cs.CV

TL;DR: DeContext perturbs images to break cross-attention in DiT-based models, focusing on early denoising and key transformer blocks, preventing in-context edits while keeping image quality.


<details>
  <summary>Details</summary>
Motivation: Prevent unauthorized manipulation of personal images by in-context diffusion models by blocking contextual information flow through cross-attention using minimal perturbations.

Method: Analyze abstract: perturb attention cross-attention perturbations targeting early denoising and specific transformer blocks

Result: DeContext injects small targeted perturbations into input images to weaken multimodal cross-attention pathways in DiT-based in-context diffusion models, disrupting context propagation and decoupling input-output link. Focuses on early denoising steps and specific transformer blocks for efficiency. Shows consistent blocking of unwanted edits on Flux Kontext and Step1X-Edit while preserving visual quality.

Conclusion: Attention-based perturbations effectively defend against unauthorized in-context image editing by targeting cross-attention during dominant propagation stages, offering an efficient and robust privacy-preserving approach.

Abstract: In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.

</details>


### [70] [SARMAE: Masked Autoencoder for SAR Representation Learning](https://arxiv.org/abs/2512.16635)
*Danxu Liu,Di Wang,Hebaixu Wang,Haoyang Chen,Wentao Jiang,Yilin Cheng,Haonan Guo,Wei Cui,Jing Zhang*

Main category: cs.CV

TL;DR: 提出SARMAE，一种针对SAR图像自监督表征学习的方法：构建百万级SAR-1M数据集，设计Speckle-Aware Representation Enhancement在MAE中注入散斑噪声，和Semantic Anchor Representation Constraint利用光学配对先验对齐语义。实验表明在分类、检测、分割任务上均达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有SAR深度学习受限于数据稀缺与SAR特有的散斑噪声，导致精细语义表征学习受阻。作者希望通过大规模数据与噪声感知的自监督预训练，提高SAR下游任务性能。

Method: (1) 构建SAR-1M百万级数据集并配对光学图像；(2) 在Masked Autoencoder基础上提出SARE，通过在掩码自编码器的输入或重建目标中注入SAR散斑噪声来学习对噪声稳健的表示；(3) 提出SARC，利用光学图像作为语义锚点，在特征空间对齐SAR与光学特征以保证语义一致性；训练目标可能包含重建损失、对比/对齐损失与噪声感知项。

Result: 在多个公开SAR数据集上进行评估，SARMAE在分类、目标检测和语义分割任务上均优于现有方法，显示出明显的性能提升，并且模型与代码将开源。

Conclusion: 通过百万规模预训练数据与散斑噪声感知的自监督设计，SARMAE有效提升了SAR图像的语义表征能力，为下游任务提供了更鲁棒的初始化和更高的精度。

Abstract: Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.

</details>


### [71] [REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion](https://arxiv.org/abs/2512.16636)
*Giorgos Petsangourakis,Christos Sgouropoulos,Bill Psomas,Theodoros Giannakopoulos,Giorgos Sfikas,Ioannis Kakogeorgiou*

Main category: cs.CV

TL;DR: REGLUE fuses VAE latents, patch-level VFM semantics, and a global token via nonlinear compressor and SiT backbone, boosting ImageNet 256 FID and convergence.


<details>
  <summary>Details</summary>
Motivation: LDM denoising lacks direct semantic supervision; VFMs provide rich semantics but are underutilized by prior integration methods.

Method: Use a lightweight conv semantic compressor to aggregate multi-layer VFM features into low-dim spatial maps, entangle them with VAE latents in diffusion, add global CLS token, and apply external alignment loss to frozen VFM targets.

Result: REGLUE integrates VAE latents with multi-layer spatial VFM features and a global CLS token in a single SiT backbone.

Conclusion: Joint modeling with nonlinear spatial compression and external alignment improves sample quality and training speed over prior methods.

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .

</details>


### [72] [FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering](https://arxiv.org/abs/2512.16670)
*Ole Beisswenger,Jan-Niklas Dihlmann,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: FrameDiffuser 是一个面向交互应用的自回归视频渲染模型，利用 G-buffer 与上一帧输出作为条件，结合 ControlNet 与 ControlLoRA 实现时间一致且高质量的逐帧扩散生成，适配环境专化以提高质量与速度。


<details>
  <summary>Details</summary>
Motivation: 现有扩散渲染方法要么逐帧独立生成缺乏时间一致性，要么为全视频模型需高计算并需完整序列输入，不适合交互式实时渲染；需要一种能在低延迟、逐帧输入并保持时间一致性的方案。

Method: 提出 FrameDiffuser，自回归生成，输入为当前帧 G-buffer（几何、材质、表面属性）与模型先前生成帧，双重条件化架构：使用 ControlNet 提供结构/空间引导，使用 ControlLoRA 提供时间一致性引导；采用三阶段训练策略以稳定自回归推理，并对单一环境进行专化训练以换取一致性与速度。

Result: 在专化环境中，FrameDiffuser 在光照、阴影与反射的写实性与时间一致性上优于通用 RGBX（逐帧）与 DiffusionRenderer（全视频但昂贵）等基线，同时实现适配交互应用的推理开销和可扩展到数百到数千帧的稳定输出。

Conclusion: FrameDiffuser 提出了一种自回归神经渲染框架，结合 ControlNet 和 ControlLoRA，通过在每帧使用 G-buffer 与模型先前生成的帧进行双重条件化，能够在保持实时交互适用的推理速度下生成数百到数千帧时间一致且光照逼真的图像；该方法通过三阶段训练和场景专化获得比通用模型更好的光照、阴影与反射质量。

Abstract: Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.

</details>


### [73] [Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray](https://arxiv.org/abs/2512.16685)
*Gonçalo Gaspar Alves,Shekoufeh Gorgi Zadeh,Andreas Husch,Ben Bausch*

Main category: cs.CV

TL;DR: 作者使用ResNet-50与三元组损失训练一个少样本指纹模型，在ChestXray-14和BraTS-2021上获得高的Recall@K（如20-way 1-shot约99%），可用于检测跨数据集的重复受试者以避免数据泄漏。


<details>
  <summary>Details</summary>
Motivation: 合并多个开源医疗影像数据库时，同一受试者可能出现在不同数据集或切片中，若不去重会导致训练/评估时数据泄漏、性能高估。需要一种高效的受试者重识别方法以保障模型评估的可靠性。

Method: 采用ResNet-50作为特征提取器，使用triplet margin loss训练嵌入空间，将同一受试者的图像拉近、不同受试者的图像推远。评估采用few-shot设置（如N-way K-shot），在2D胸片和3D MRI切片上测试不同难度的任务。

Result: 在ChestXray-14上，20-way 1-shot Recall@K达99.10%，500-way 5-shot达90.06%；在BraTS-2021上，20-way 1-shot达99.20%，100-way 3-shot达98.86%，表明模型在少样本、多类场景下能高效识别受试者。

Conclusion: 本文通过构建“subject fingerprinting”在潜在空间中将同一受试者的影像映射到靠近的区域，从而实现基于相似性的受试者重识别，有效防止合并开源数据集时的样本泄漏问题。

Abstract: Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.

</details>


### [74] [Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?](https://arxiv.org/abs/2512.16688)
*Serafino Pandolfini,Lorenzo Pellegrini,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 对大量生成器训练的深度伪造检测模型能部分泛化到局部修复检测，对中/大掩码或再生成式修复表现较好，但对小面积或风格差异大的修补仍弱。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使图像修复和局部编辑高度逼真，这类保留大部分真实视觉背景的伪造在安全场景中风险增大。现有检测器多聚焦整体合成图像，需评估其对局部修复的泛化能力。

Method: 对多数据集进行系统性评估：包含不同生成器、掩码大小和修复技术（区域修复、再生成式等）；使用在全图深度伪造检测上训练的SOTA模型，比较其在局部inpainting检测上的性能，并与若干专门方法进行对比分析。

Result: 实验表明，基于大量生成器训练的模型能较好检测中大面积或再生成式inpainting，显示部分可迁移性；但对小面积或高保真局部修改检测能力有限，暗示需要专门训练或数据增强以提升全面鲁棒性。

Conclusion: 训练用于检测全合成图像的最先进模型在局部修复（inpainting）检测任务上具有部分迁移能力，尤其对于中大面积修改和再生成式修复能可靠检测，且优于许多现有的专门检测方法。

Abstract: The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.

</details>


### [75] [SDFoam: Signed-Distance Foam for explicit surface reconstruction](https://arxiv.org/abs/2512.16706)
*Antonella Rech,Nicola Conci,Nicola Garau*

Main category: cs.CV

TL;DR: SDFoam结合显式VD与隐式SDF，通过Eikonal正则化和光线追踪得到更好的网格重建，同时保留渲染质量和速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于体积光线追踪的NeRF与基于光栅化的splatting方法在网格重建上存在不足，显式VD方法尽管速度快但仍难以精确重建网格，故提出将SDF与VD结合以提升网格质量。

Method: 联合学习显式Voronoi Diagram和隐式SDF，通过光线追踪和Eikonal正则化训练。

Result: 生成的模型SDFoam在多场景中提高了网格重建精度（Chamfer距离），在PSNR和SSIM上保持可比的外观质量，训练速度与RadiantFoam相当。

Conclusion: 将SDF引入显式VD能对齐近表面Voronoi面与零水平集，从而获得更清晰、一致和拓扑更好的网格，同时保持光度质量和效率。

Abstract: Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.

</details>


### [76] [A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry](https://arxiv.org/abs/2512.16710)
*Chiara Di Vece,Zhehua Mao,Netanell Avisdris,Brian Dromey,Raffaele Napolitano,Dafna Ben Bashat,Francisco Vasconcelos,Danail Stoyanov,Leo Joskowicz,Sophia Bano*

Main category: cs.CV

TL;DR: 公开了一个包含4513张多中心多设备胎儿超声图像及专家地标的基准数据集，提供标准化分割、评估和基线，揭示单中心评估会高估模型性能，促进域适应与泛化研究。


<details>
  <summary>Details</summary>
Motivation: 提升胎儿生长评估的准确性与泛化性；解决手工标注耗时、操作依赖及设备/中心间差异问题；提供多源公开数据集以促进AI方法发展。

Method: 构建并公开多中心、多设备胎儿超声图像基准数据集，包含4513张来自1904例的图像，7种设备、3个临床中心；为头围、腹围、股骨长等主要生物测量提供专家解剖学地标标注；提供标准化的训练/测试划分、评估代码及基线模型，量化域偏移影响。

Result: 发布了首个覆盖全部主要胎儿生物测量的多中心多设备地标注公开数据集；基线自动生物测量模型在跨中心测试中表现下降，证明单中心训练/评估会高估性能；提供可复现的评估管线。

Conclusion: 该数据集为域适应与多中心泛化研究提供了坚实基准，有助于开发更可靠的AI辅助胎儿生长评估工具，并推动跨中心可推广的方法开发。

Abstract: Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.

</details>


### [77] [OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition](https://arxiv.org/abs/2512.16727)
*Haochen Chang,Pengfei Ren,Buyuan Zhang,Da Li,Tianhao Han,Haoyang Zhang,Liang Xie,Hongbo Chen,Erwei Yin*

Main category: cs.CV

TL;DR: They build a semi-automatic multi-view pipeline to create OMG-Bench and propose HMATr with hierarchical memory and position-aware queries, achieving +7.6% detection rate over SOTA


<details>
  <summary>Details</summary>
Motivation: Lack of public datasets and task-specific algorithms for subtle, rapid micro gestures; need automated dataset creation and an online detection/classification model that captures fine-grained temporal context

Method: Multi-view self-supervised pipeline + heuristic + expert refinement

Result: OMG-Bench dataset (40 classes, 13,948 instances, 1,272 sequences) and HMATr model outperforming prior methods by 7.6% detection rate

Conclusion: The paper provides a large-scale benchmark and a strong baseline (HMATr) for online skeleton-based micro gesture recognition, addressing data scarcity and online detection challenges

Abstract: Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/

</details>


### [78] [Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2512.16740)
*Yunkai Yang,Yudong Zhang,Kunquan Zhang,Jinxiao Zhang,Xinying Chen,Haohuan Fu,Runmin Dong*

Main category: cs.CV

TL;DR: 提出基于DiT的MM-DiT与CRFM采样策略的任务导向合成框架，提升遥感语义分割合成数据质量和下游表现


<details>
  <summary>Details</summary>
Motivation: 解决语义掩码控制复杂性与采样质量不确定性，提升合成数据在RS分割任务中的实用性和稳定性

Method: 提出了TODSynth框架，包含MM-DiT和任务反馈引导的采样策略

Result: 在少样本和复杂场景的RS语义分割数据合成上明显优于现有可控生成方法，生成数据更稳定、面向任务

Conclusion: 文本-图像-掩码联合注意力与图像/掩码分支的全量微调，以及CRFM在高塑性阶段用语义损失引导采样方向，是提高合成数据任务有效性的关键

Abstract: With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.

</details>


### [79] [TreeNet: A Light Weight Model for Low Bitrate Image Compression](https://arxiv.org/abs/2512.16743)
*Mahadev Prasad Panda,Purnachandra Rao Makkena,Srivatsa Prativadibhayankaram,Siegfried Fößel,André Kaup*

Main category: cs.CV

TL;DR: TreeNet: binary tree encoder-decoder + attention fusion yields better low-bitrate compression than JPEG AI with far fewer parameters; ablations clarify latent roles


<details>
  <summary>Details</summary>
Motivation: Reduce computational complexity of learning-based image compression while maintaining or improving rate-distortion performance

Method: Binary tree encoder-decoder with attention-based feature fusion; ablation on latent representations

Result: TreeNet outperforms JPEG AI at low bitrates with 4.83% BD-rate improvement and 87.82% model complexity reduction; evaluated on three benchmarks

Conclusion: Tree-structured architectures with attentional fusion provide effective low-complexity image compression; latent representation choices significantly affect reconstruction quality

Abstract: Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.

</details>


### [80] [Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation](https://arxiv.org/abs/2512.16767)
*Zhiyang Guo,Ori Zhang,Jax Xiang,Alan Zhao,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: 提出Make-It-Poseable：通过在隐空间操控形状token并引入密集姿势表示、隐空间监督和自适应补全，实现鲁棒高保真3D角色换位与编辑


<details>
  <summary>Details</summary>
Motivation: 解决现有自动绑定与条件生成在蒙皮权重预测、拓扑不完善与姿势一致性方面的不足，通过隐空间重建以避免直接顶点变形的困难

Method: latent-space transformation via latent posing transformer with dense pose tokens

Result: superior posing quality, extends to part replacement and refinement, handles topology changes

Conclusion: latent manipulation with adaptive completion and latent supervision yields robust, high-fidelity posing

Abstract: Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.

</details>


### [81] [FlowDet: Unifying Object Detection and Generative Transport Flows](https://arxiv.org/abs/2512.16771)
*Enis Baty,C. P. Bridges,Simon Hadfield*

Main category: cs.CV

TL;DR: 提出FlowDet，将条件流匹配用于目标检测，改进自DiffusionDet，通过学习直线传输路径比扩散更快并提高性能，在COCO/LVIS上显著超越DiffusionDet。


<details>
  <summary>Details</summary>
Motivation: 将生成模型的优点（灵活可变候选框数、可控制推理步骤）引入目标检测，同时克服扩散模型路径弯曲、推理步数扩展性能受限的问题。

Method: 用Conditional Flow Matching替换扩散过程，建立在生成传输问题的更广泛框架上，使得从噪声到目标框的传输路径更直、更简单；保持无需重训即可改变框数和推理步的能力。

Result: 相比DiffusionDet和非生成基线，在多种骨干网络和数据集上取得更好性能，尤其在召回受限设置下表现更优；在COCO上AP提升至+3.6%，在LVIS上AP_rare提升+4.2%。

Conclusion: 通过将条件流匹配应用于检测，FlowDet实现了更高效的生成传输，带来更快的性能增长和整体检测精度提升，证明了该思路优于扩散式检测。

Abstract: We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.

</details>


### [82] [Kling-Omni Technical Report](https://arxiv.org/abs/2512.16776)
*Kling Team,Jialu Chen,Yuanzheng Ci,Xiangyu Du,Zipeng Feng,Kun Gai,Sainan Guo,Feng Han,Jingbin He,Kang He,Xiao Hu,Xiaohua Hu,Boyuan Jiang,Fangyuan Kong,Hang Li,Jie Li,Qingyu Li,Shen Li,Xiaohan Li,Yan Li,Jiajun Liang,Borui Liao,Yiqiao Liao,Weihong Lin,Quande Liu,Xiaokun Liu,Yilun Liu,Yuliang Liu,Shun Lu,Hangyu Mao,Yunyao Mao,Haodong Ouyang,Wenyu Qin,Wanqi Shi,Xiaoyu Shi,Lianghao Su,Haozhi Sun,Peiqin Sun,Pengfei Wan,Chao Wang,Chenyu Wang,Meng Wang,Qiulin Wang,Runqi Wang,Xintao Wang,Xuebo Wang,Zekun Wang,Min Wei,Tiancheng Wen,Guohao Wu,Xiaoshi Wu,Zhenhua Wu,Da Xie,Yingtong Xiong,Yulong Xu,Sile Yang,Zikang Yang,Weicai Ye,Ziyang Yuan,Shenglong Zhang,Shuaiyu Zhang,Yuanxing Zhang,Yufan Zhang,Wenzheng Zhao,Ruiliang Zhou,Yan Zhou,Guosheng Zhu,Yongjie Zhu*

Main category: cs.CV

TL;DR: Kling-Omni是一个端到端多模态视频生成框架，统一处理文本、图像和视频输入，结合大规模预训练与高效推理优化，实现高质量视频合成和智能编辑。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成、编辑与推理通常由分离的流水线或专用模块完成，限制了多模态理解与生成的一致性与灵活性。作者致力于构建一个统一的系统，支持多种用户输入并实现更智能、更高保真的视频创作。

Method: 构建统一多模态表示，将文本、参考图像和视频上下文编码为统一输入；设计大规模预训练策略和高效推理基础设施；搭建综合数据系统用于支撑多模态视频训练；实现端到端框架以整合生成、编辑与推理任务。

Result: 评估显示Kling-Omni在上下文生成、基于推理的编辑和多模态指令跟随方面表现优秀，能够生成电影级质量的视频，具备作为多模态世界模拟器的潜力。

Conclusion: Kling-Omni提出了一个端到端的多模态视频生成与编辑通用框架，能够从文本、图片、视频上下文等多种输入生成高保真视频，并支持推理驱动的编辑与多模态指令跟随。

Abstract: We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.

</details>


### [83] [R3ST: A Synthetic 3D Dataset With Realistic Trajectories](https://arxiv.org/abs/2512.16784)
*Simone Teglia,Claudia Melis Tonti,Francesco Pro,Leonardo Russo,Andrea Alfarano,Leonardo Pentassuglia,Irene Amerini*

Main category: cs.CV

TL;DR: 提出R3ST：在合成3D环境中嵌入来自无人机鸟瞰数据SinD的真实轨迹，生成既有精确标注又具真实人类驾驶轨迹的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据集缺乏真实车辆运动轨迹，而真实数据集虽真实但缺少精确标注。为了解决这一矛盾，作者希望将真实轨迹引入合成环境中，同时保留合成数据的高质量标注能力。

Method: 构建合成3D场景并将SinD数据中的鸟瞰真实轨迹映射到该场景中，生成带有精准多模态地面真值注释的合成数据集R3ST。

Result: This paper presents R3ST, a synthetic dataset integrating real-world trajectories into a synthetic 3D environment to improve realism of vehicle motion for trajectory forecasting research.

Conclusion: R3ST bridges the gap between synthetic datasets and realistic trajectories by embedding authentic human-driven trajectories from SinD into a synthetic 3D environment, providing accurate multimodal ground-truth annotations and enabling better trajectory forecasting studies.

Abstract: Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.

</details>


### [84] [KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals](https://arxiv.org/abs/2512.16791)
*Shuting Zhao,Zeyu Xiao,Xinrong Chen*

Main category: cs.CV

TL;DR: KineST利用运动学引导的双向状态扫描与混合时空表示学习，并辅以角速度损失，在轻量化模型里实现更准确且时间连贯的全身姿态重建。


<details>
  <summary>Details</summary>
Motivation: 解决头戴设备（HMD）稀疏信号下重建逼真、多样的全身姿态的困难，兼顾精度、时间连贯性和效率。

Method: 提出KineST，一种运动学引导的状态空间模型；将状态空间双向扫描重构为融合运动学先验的运动学引导双向扫描；采用混合时空表示学习紧密耦合空间与时间上下文；引入几何角速度损失约束旋转变化。

Result: 在轻量级框架下，KineST在精度和时间一致性上优于现有方法，实验展示了更好的姿态重建质量和平滑性。

Conclusion: KineST通过融合运动学先验与混合时空学习，在保持高效性的同时提升了全身姿态重建的准确性与稳定性，适用于AR/VR场景。

Abstract: Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/

</details>


### [85] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict trains VLA policies with predicted 3D keypoint trajectories and Gaussian workspace geometry as auxiliary supervision; at inference only lightweight tokens are used, leading to better 3D reasoning and stronger task performance.


<details>
  <summary>Details</summary>
Motivation: Reactive, 2D-centric VLA models lack precise 3D reasoning needed for many manipulation tasks; incorporating predictive kinematic and geometric priors can provide necessary 3D structure during training without inference cost.

Method: Analyze paper's approach: trajectory-level predictive 3D keypoints and predictive 3D Gaussian geometry used as training supervision; inference uses query tokens without 3D decoding.

Result: GeoPredict improves geometry reasoning for VLA by adding predictive kinematic and geometric priors, leading to better performance on benchmark and real tasks.

Conclusion: Predictive, geometry-aware supervision at training time yields more robust continuous-action VLA policies without inference overhead, improving performance especially in geometry-intensive tasks.

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [86] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 用BEV格点直接作为检测锚点，配合BEV-NMS和先验检测的混合时序建模，提升多摄像头3D检测性能，尤其对小目标效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有DETR类或BEV-based Transformer方法使用随机queries或外部检测器生成queries，训练复杂且对小目标不友好；直接使用密集BEV格点作为锚点更直观且能自然包含时序信息。

Method: 两阶段anchor生成：用BEV特征格点作为初始anchors；引入BEV-NMS使梯度仅流向未被抑制的对象以控制注意力规模；结合先前帧检测以做混合时序建模；可直接将BEVFormer等编码器输出作为queries，减少额外网络。

Result: 提出将BEV特征格点作为锚点（anchors），形成端到端的两阶段anchor生成方法，并引入基于BEV的NMS以缩放注意力计算，结合先前检测进行混合时序建模。

Conclusion: DenseBEV在nuScenes和Waymo上均显著提升检测性能，尤其对小目标（行人）效果明显，在Waymo上达到SOTA LET-mAP 60.7%。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [87] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本文评估YOLOv8系列在车牌检测与字符识别上的性能，提出用YOLOv8 Nano检测车牌、YOLOv8 Small识别字符并按x轴排序重建车牌，取得高精度且具边缘设备可行性的方案。


<details>
  <summary>Details</summary>
Motivation: 车牌检测与识别是智能交通和监控系统的核心任务，但在多样化环境下要同时保证实时性和高准确率仍具挑战。本文旨在探索轻量化YOLOv8变体在实际LPR流水线中的可行性，以利边缘设备部署。

Method: 使用两个独立数据集分别训练与评估模型：YOLOv8 Nano用于车牌位置检测，YOLOv8 Small用于字符检测与分类；识别后的字符通过基于x轴的排序算法进行序列重组；评估指标包括precision与mAP50。

Result: 实验结果显示：在LPR任务上YOLOv8 Nano达precision=0.964、mAP50=0.918；在字符识别任务上YOLOv8 Small达precision=0.92、mAP50=0.91。基于此提出的混合流水线兼顾计算效率与识别精度，适合边缘部署。

Conclusion: 本文通过比较YOLOv8不同模型变体在车牌检测与字符识别任务上的表现，提出了一个在边缘设备上兼顾精度与效率的流水线：使用YOLOv8 Nano进行车牌检测、YOLOv8 Small进行字符识别，并通过基于x轴位置的字符排序方法重建车牌序列。实验证明该方案在两个数据集上均达到高精度（LPR任务Nano: precision=0.964、mAP50=0.918；字符识别Small: precision=0.92、mAP50=0.91），适合部署于智能交通系统的实时场景。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [88] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: Small image-to-text model using frozen DINOv3 encoder and GPT-2 with layer-wise anatomical attention markedly improves pathology detection and structural coherence on MIMIC-CXR, despite being single-view and parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: Make radiology report generation accessible and less resource-intensive by designing a compact model that uses anatomical priors to focus attention on clinically relevant regions, avoiding large-scale multimodal training and clinical metadata.

Method: Compact image-to-text with frozen DINOv3 ViT encoder + GPT-2 decoder with layer-wise anatomical attention using segmentation masks via hierarchical Gaussian smoothing; single frontal chest X-ray input, no extra metadata, small model

Result: Substantial improvements on MIMIC-CXR: CheXpert Macro-F1 (5 key) 0.083 -> 0.238 (+168%), Micro-F1 0.137 -> 0.337 (+146%); 14-observation CheXpert 0.170 -> 0.316 (+86%); RadGraph F1 +9.7%

Conclusion: Decoder-level anatomical guidance via segmentation masks improves spatial grounding and clinical coherence in a compact, image-only report-generation model, achieving large relative gains over prior small baselines without multimodal scaling.

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [89] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch：5.1 小时同步视频-触觉-姿态数据，2,900 条带注释片段，提供检索与分类基准，证明触觉增强视觉感知与抓握理解。


<details>
  <summary>Details</summary>
Motivation: 现有可穿戴触觉传感器与真实环境中第一视角完整手部接触数据稀缺，导致视觉感知无法准确感知接触发生的时间、位置和力度。作者旨在弥合视觉感知与物理交互之间的差距。

Method: 作者构建并发布 OpenTouch 数据集，包含同步录制的视频、触觉和手部姿态数据，并对 2,900 条片段进行文本化注释；基于该数据集设计了检索与分类基准来评估视觉与触觉的跨模态对齐与抓握理解能力。

Result: 实验表明触觉信号是紧凑且有效的抓握理解线索，可增强跨模态对齐，并能从真实世界视频中可靠检索触觉信息。

Conclusion: OpenTouch 提供了首个真实环境下的第一视角全手触觉数据集，并展示触觉在抓握理解、跨模态对齐与视频检索中的有效性。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [90] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: GenEval has drifted from human judgment up to 17.7% error; authors create GenEval 2 and Soft-TIFA to better capture primitives and compositionality and to reduce drift


<details>
  <summary>Details</summary>
Motivation: Automate T2I evaluation while avoiding judge/model/test-prompt selection causing benchmark drift over time

Method: Survey GenEval drift via large-scale human study, design GenEval 2 with broader primitives/compositionality, propose Soft-TIFA combining primitive judgments for alignment

Result: GenEval benchmark drift quantified; introduced GenEval 2 and Soft-TIFA

Conclusion: Original GenEval is saturated and misaligned with humans for current models; GenEval 2 + Soft-TIFA improve alignment and robustness

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [91] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出RePlan（Region-aligned Planning），一种“先规划后执行”的图像编辑框架：用视觉-语言规划器将自然指令分解并显式定位到目标区域，再由无训练的扩散编辑器通过注意力区域注入执行修改。通过GRPO强化学习用1K指令仅示例增强规划模块，并构建IV-Edit基准。RePlan在复杂指令-视觉场景下显著提高区域精确度与整体一致性，优于大数据训练的强基线。


<details>
  <summary>Details</summary>
Motivation: 现有指令驱动的图像编辑模型在面对复杂指令与复杂场景（IV-Complexity）时，表现不稳定：难以将多步、细粒度或知识密集的指令精确映射到图像中对应区域，导致编辑错误或遗漏。需要明确的分解与区域对齐机制以提升可控性和精确度。

Method: 提出RePlan框架：1) 规划器（vision-language planner）对自然语言指令进行逐步推理和分解，输出格式化的操作步骤并显式给出每步的目标区域坐标或语义区分；规划器通过GRPO强化学习在1K条仅含指令的数据上进行微调，以提升推理一致性与格式可靠性。2) 编辑器（diffusion editor）采用训练空白的注意力-区域注入机制（attention-region injection），在扩散模型中注入区域注意力以并行对多个目标区域进行精准修改，无需重复掩膜与inpainting迭代。3) 构建IV-Edit基准用于评估细粒度标定与知识密集编辑。

Result: 在IV-Complexity设置下，RePlan在区域精确度（regional precision）和整体编辑一致性（fidelity）上持续领先强基线；即使这些基线使用远大于RePlan的数据训练，RePlan仍表现更好。GRPO微调显著提升规划模块的推理准确性与输出格式可靠性。

Conclusion: 通过将显式的分解与区域对齐引入指令驱动图像编辑，并结合无训练的注意力注入编辑器与少量指令数据的强化学习微调，RePlan在复杂指令与复杂场景的编辑任务上实现了更高的精确性和一致性，为可控、多区域、细粒度图像编辑提供了有效范式。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [92] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: Pixel Seal uses adversarial-only training, a three-stage decoupled schedule, and JND-based high-res adaptation to produce more imperceptible and robust watermarks for images and videos.


<details>
  <summary>Details</summary>
Motivation: Current watermarking models struggle to balance robustness with imperceptibility, suffer from unstable optimization due to conflicting losses, and don't scale well to high-resolution images/videos.

Method: Adversarial-only training replacing pixel losses; three-stage training decoupling objectives; high-resolution adaptation using JND attenuation and inference simulation; temporal watermark pooling for video.

Result: Pixel Seal achieves state-of-the-art invisible watermarking with improved imperceptibility and robustness.

Conclusion: Adversarial-only training, three-stage schedule, and high-resolution adaptation enable stable training and better performance; temporal pooling extends to video.

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [93] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3是一个无训练、增强记忆的SAM3扩展：引入相关性感知记忆过滤与遮挡前记忆、分段插值增大有效记忆、以及基于特征的重识别和时间投票，零样本下在EndoVis17/18上分别提升约7%和16% mcIoU。


<details>
  <summary>Details</summary>
Motivation: SAM3在视频对象分割中虽强，但在外科内窥镜场景受频繁遮挡、快速运动、镜面高光和器械反复进出等影响：不辨优先的记忆更新、固定记忆容量和遮挡后的身份恢复能力弱，导致误差累积与身份混淆。

Method: 1) 相关性感知记忆过滤：基于相关性筛选并维护一个专门的遮挡前记忆缓冲，避免被噪声帧污染主记忆；2) 分段插值（piecewise interpolation）：在时间轴上对长序列分段插值以扩展有效记忆容量；3) 基于特征的重识别模块结合时间投票：提取器械特征并在遮挡后通过多帧投票恢复身份，抑制瞬时错误。整个方法为训练无关的模块，可插入SAM3流水线。

Result: 在零样本设置下于EndoVis17和EndoVis18数据集相比原始SAM3分别平均mcIoU提升约7%和16%，并优于先前一些基于训练的方法，说明在遮挡与长期再现场景下能可靠恢复与降低误差累计。

Conclusion: 本文提出的ReMeDI-SAM3通过记忆过滤、分段插值扩展记忆容量及基于特征的重识别+时间投票机制，有效缓解了遮挡引起的身份丢失与误差累积，显著提升了SAM3在内窥镜视频工具分割的零样本性能。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [94] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: 提出M-PhyGs，从短视频同时进行多材料分割与连续机械参数恢复，针对复杂自然对象（花），并提供Phlowers数据集；通过新损失设计与训练策略提高估计准确性。


<details>
  <summary>Details</summary>
Motivation: 现实物体通常由多种材料组成且形状复杂，现有从视觉估计物理参数的方法多假设单一材料或已知动力学模型，不能处理自然场景中的多材料复杂对象，因而需要新的方法来同时识别材料区域并恢复每种材料的力学参数。

Method: 方法提出Multi-material Physical Gaussians (M-PhyGs)，通过联合优化实现材料分割与连续力学参数恢复；引入级联的3D和2D损失来提升空间-像素级一致性，并使用时间小批量（temporal mini-batching）提高训练效率与稳定性；考虑重力影响以更真实模拟物体响应。

Result: M-PhyGs提出了一种从短视频中联合完成多材料分割与连续力学参数估计的方法，聚焦于复杂自然对象（如花）的物理属性恢复。该方法通过引入级联的3D和2D损失以及时间小批量技术，并提出了Phlowers数据集以评估性能。

Conclusion: 在Phlowers数据集上的实验表明，M-PhyGs能有效分割不同材料区域并准确估计各自的物理参数，优于仅假设单一材料或预学动力学的现有方法。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [95] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: 提出LinkedOut，一种从VLLM提取知识感知像素级表示的方案，用于高效多视频推荐，克服语言瓶颈，实现低延迟、支持多视频输入并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLLM在世界知识理解上有优势，但在实际推荐系统部署中有三大问题：解码式生成导致顺序推理延迟高、接口不支持多视频输入、仅输出语言会丢失对下游视觉任务重要的细粒度像素信息。需要一种既保留像素细节又能利用世界知识的表示。

Method: 提出LinkedOut：利用VLLM和可提示查询从原始视频帧中提取语义化、知识感知的token，支持多视频历史并去除语言瓶颈。引入跨层知识融合MoE，从VLLM的多层特征中选择合适抽象层次，实现个性化、可解释、低延迟推荐。

Result: LinkedOut在无手工标签、直接使用原始帧的条件下，在标准基准上达到最先进的推荐效果。消融与可解释性研究证明了多层多样性和逐层融合的优势。

Conclusion: LinkedOut表明通过从VLLM提取层级多样化的像素级知识表示，可以实现对下游视频推荐任务的高效、可解释和低延迟利用，为VLLM在实际视觉系统应用提供了实用路径。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [96] [Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation](https://arxiv.org/abs/2512.16893)
*Kaiwen Jiang,Xueting Li,Seonwook Park,Ravi Ramamoorthi,Shalini De Mello,Koki Nagano*

Main category: cs.CV

TL;DR: 将2D扩散模型的表现力蒸馏到3D感知的前馈编码器，得到兼具3D一致性、实时速度(107.31 FPS)和高表达力的人像动画方法。


<details>
  <summary>Details</summary>
Motivation: 2D diffusion models give high-quality portrait animation but lack 3D consistency and speed; 3D-aware feedforward methods ensure 3D consistency and speed but lack expressive details. The paper aims to combine strengths of both.

Method: Distill knowledge from a 2D diffusion-based method into a feed-forward encoder that converts a single image to a 3D-consistent, fast, expressive animatable representation. Representation is decoupled from 3D face representation, learns motion implicitly without parametric models, and uses an efficient lightweight local fusion strategy instead of global fusion like attention.

Result: Method achieves comparable animation quality to state-of-the-art while running at 107.31 FPS for animation and pose control, outperforming alternatives that trade speed for quality.

Conclusion: Knowledge distillation from 2D diffusion to a 3D-aware feedforward encoder enables fast, 3D-consistent, expressive portrait animation, offering a practical balance of speed and quality.

Abstract: Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d

</details>


### [97] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait通过表情特征标准化和高阶潜变量导数预测，在保持身份一致性的同时实现了最多6x的扩散推理加速，适用于无限长度人像动画生成。


<details>
  <summary>Details</summary>
Motivation: 解决长人像动画生成中身份一致性(ID)难题，同时加速扩散模型推理。

Method: 提出FlashPortrait：使用预训练表情特征提取器提取身份无关的表情特征，设计Normalized Facial Expression Block通过均值和方差标准化将表情特征与扩散潜变量对齐；推理时采用动态滑动窗口并在重叠区加权融合保证过渡平滑；基于潜变量在各时间步的变化率和扩散层导数比例，使用高阶潜变量导数预测未来时间步潜变量，从而跳过若干去噪步骤实现加速。

Result: 在基准数据集上，FlashPortrait在定性和定量评估上均表现良好，能在保持ID一致性的同时实现最多6倍的推理加速。

Conclusion: 通过特征标准化对齐与基于高阶导数的跳步预测相结合，FlashPortrait在生成长时序人像动画时实现了ID稳定性和高效推理的平衡。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [98] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 提出Alchemist：首个用于Text-to-Image的元梯度数据选择框架，通过训练轻量评分器估计样本影响并用Shift-Gsampling剪枝，使用50%数据可优于全量训练，提高视觉质量与效率。


<details>
  <summary>Details</summary>
Motivation: Web-crawled/synthetic datasets contain low-quality/redundant samples hurting T2I model quality;需要自动、可扩展的数据选择方法，现有方法多依赖人工或单维度启发式评分，未在图像模态上应用元学习。

Method: Meta-gradient based data selection for T2I: train a lightweight rater using gradient info with multi-granularity perception; then Shift-Gsampling to prune and select subsets.

Result: Alchemist improves visual quality and downstream performance; selecting 50% of data can outperform full dataset training; scalable and automatic for T2I.

Conclusion: Meta-gradient rater + Shift-Gsampling enable efficient, effective data selection for Text-to-Image models, reducing compute and improving fidelity.

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [99] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出VIVA：用VLM生成视觉指令表征并用Edit-GRPO进行后训练，通过合成数据增强，实现更强的指令跟随与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散式视频编辑模型多依赖简单配对数据，难以泛化到复杂真实指令；需要更具语义和空间理解的指令表示与面向目标的优化策略。

Method: 1) VLM-based instructor：把文本指令、首帧和可选参考图输入视觉语言模型，生成视觉化指令表征供扩散Transformer使用；2) Edit-GRPO：将Group Relative Policy Optimization调整到视频编辑领域，利用相对奖励直接优化指令忠实度、内容保持和美学；3) 合成数据管线：生成多样高保真视频-指令配对用于训练。

Result: 大规模实验证明VIVA在指令执行、泛化能力和编辑质量上优于现有最先进方法。

Conclusion: VIVA提出一种结合视觉语言模型(VLM)引导编码和基于相对奖励的后训练策略(Edit-GRPO)的可扩展指令驱动视频编辑框架，显著提升了指令遵循性、保持内容一致性与编辑质量，并通过合成数据管线增强训练多样性。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [100] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: New EgoMAN dataset (219K 6DoF trajectories, 3M QA) + EgoMAN model that connects reasoning and motion via trajectory-token interface, yielding better stage-aware 3D hand trajectory predictions.


<details>
  <summary>Details</summary>
Motivation: Existing datasets separate motion from semantic supervision and models weakly connect reasoning to action; need integrated dataset and model to reason semantically and generate motion.

Method: Construct egocentric dataset with interaction stages and QA pairs; design reasoning-to-motion model that maps vision-language reasoning to motion via trajectory tokens, trained progressively to align reasoning outputs with motion dynamics.

Result: EgoMAN dataset and model improve 3D hand trajectory prediction with semantic reasoning and stage awareness.

Conclusion: EgoMAN provides large-scale paired motion and QA data; EgoMAN model links vision-language reasoning to motion via trajectory tokens, trained progressively to align reasoning and dynamics, achieving accurate stage-aware trajectories and generalization.

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [101] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 提出首个多视图物体变更检测基准SceneDiff Benchmark与训练免费方法SceneDiff：通过3D对齐+区域提取+语义与空间特征比较，有效抑制视角误报，在多项基准上显著提升AP（+94%/+37.4%）。


<details>
  <summary>Details</summary>
Motivation: 解决在不同时刻从不同视角拍摄的同一场景图像/视频中检测物体添加、移除或移动的问题，克服视角变化导致的伪变更误报，并支持机器人整理、施工进度与安全监测等应用。

Method: 提出SceneDiff Benchmark（首个带有物体实例标注的多视图变更检测基准，包含350对视频和大量变更对象）和SceneDiff方法（无需训练，结合预训练的3D、分割与图像编码模型）：先在3D上对齐捕获帧，提取物体区域，再比较空间与语义区域特征以检测变更）。

Result: 在多视图和双视图基准上的实验表明，SceneDiff方法在检测性能上明显优于现有方法，分别取得约94%和37.4%的相对AP提升；并且在多个基准上表现稳健。

Conclusion: SceneDiff通过利用预训练3D与视觉模型并在3D对齐基础上进行区域级比较，能有效抑制视角带来的伪变更，显著提升多视图物体级变更检测性能；相关基准与代码将公开发布，促进后续研究。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [102] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 提出MomaGraph：统一的场景图表示+大规模注释数据集和评测套件，并训练7B视觉语言模型MomaGraph-R1，通过Graph-then-Plan实现零样本任务规划，性能显著领先。


<details>
  <summary>Details</summary>
Motivation: 现有场景图研究割裂空间与功能关系、忽视物体状态与可交互部件、以及未聚焦任务相关信息，限制了移动操控机器人在家庭环境的导航与操作能力。

Method: 构建融合空间-功能关系与部件级可交互元素的场景图表示；发布MomaGraph-Scenes数据集和MomaGraph-Bench评测；训练7B视觉语言模型MomaGraph-R1并用强化学习微调以预测任务导向场景图，采用Graph-then-Plan框架进行零样本规划。

Result: MomaGraph introduces a unified scene graph representation combining spatial, functional, and part-level interactive information for embodied household manipulators.

Conclusion: MomaGraph, its dataset (MomaGraph-Scenes), benchmark (MomaGraph-Bench), and model (MomaGraph-R1) advance task-driven scene understanding and planning, achieving state-of-the-art performance and real-robot transfer.

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [103] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出SFTok，一种带多步自驱动重建与去偏拟合训练的离散tokenizer；在64 token/图像下实现ImageNet rFID=1.21与gFID=2.29。


<details>
  <summary>Details</summary>
Motivation: 离散tokenizer与自回归生成模型兼容但在重建质量上落后于连续tokenizer；通过改进多步迭代重建和训练策略，可缩小差距并提升多模态生成效果。

Method: 引入multi-step iterative reconstruction、self-forcing guided visual reconstruction用于在推理阶段模拟训练过程，以及debias-and-fitting策略解决训练-推理不一致，最终在高压缩率下优化token表示和重建网络。

Result: SFTok是一种用于高分辨率图像生成的离散tokenizer，通过多步迭代机制增强重建精度。

Conclusion: 通过self-forcing和debias-and-fitting训练策略，SFTok在高压缩率下实现了比现有离散tokenizer更好的重建质量和生成性能，证明了解决训练-推理不一致的重要性。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [104] [Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation](https://arxiv.org/abs/2512.16913)
*Xin Lin,Meixi Song,Dizhe Zhang,Wenxuan Lu,Haodong Li,Bo Du,Ming-Hsuan Yang,Truong Nguyen,Lu Qi*

Main category: cs.CV

TL;DR: 作者构建大规模真实+合成+文本生成全景深度数据集，通过三阶段伪标签净化与专门的模型组件，使基于DINOv3-Large的模型在多基准上实现鲁棒的零样本度量深度估计。


<details>
  <summary>Details</summary>
Motivation: 现有深度估计受限于场景距离变化大、室内外与合成/真实域差异以及全景图像特性，缺少对跨距离、跨域的稳健度量深度模型；因此需要大规模多源数据与伪标签净化策略，以及面向范围和几何一致性的模型设计来提升泛化能力。

Method: （1）数据：融合公开数据、UE5合成数据、文本生成图像与网络全景图，构建大规模数据集；提出三阶段伪标签净化管线为未标注图像生成可靠深度伪标签以减少域差异；（2）模型：采用DINOv3-Large主干，新增可插拔的范围（range）掩码头以处理不同距离范围，提出基于图像清晰度的优化（sharpness-centric）和基于几何的一致性优化（geometry-centric）以增强跨视图/跨距离的几何稳健性；训练与评估在多数据集上进行。

Result: 在Stanford2D3D、Matterport3D、Deep360等基准上，模型展示出强的性能和零样本泛化能力，尤其在不同真实环境场景下的度量深度预测更稳健、稳定。

Conclusion: 该论文提出了一个面向全景的度量深度基础模型，通过大型混合数据集与三阶段伪标签净化管线减少域差异，并结合DINOv3-Large主干与三个针对距离变异与几何一致性的模块（范围掩码头、基于清晰度的优化、基于几何的优化）；在多个基准上显示出强的零样本泛化与稳健的度量预测。

Abstract: In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}

</details>


### [105] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 本文提出UniStereo数据集和StereoPilot模型，用于单目转立体视频转换，解决了DWI流水线的问题，支持平行与会聚格式，采用端到端前馈直接合成目标视图并引入可学习域切换器和循环一致性损失，实验显示在视觉质量和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着立体显示（VR、3D影院）需求增长，获取高质量3D视频昂贵且复杂；传统“深度-变换-修补”多阶段流水线存在误差累积、深度歧义和立体格式不一致问题，需要统一数据和更直接高效的模型。

Method: 构建大规模统一数据集UniStereo，包含平行和会聚两种立体格式以便公平评测和鲁棒训练；提出StereoPilot，一种端到端前馈网络直接从单目合成目标视图，无需显式深度或扩散式迭代采样。模型含可学习域切换模块以适应不同立体格式，并加入循环一致性损失以提升一致性。

Result: 在多个基准和消融实验中，StereoPilot在视觉保真度和计算效率上显著优于现有最先进方法，展示了更好的格式适应性和一致性。

Conclusion: 通过提供统一数据集和高效端到端模型，本文推动了单目到立体视频转换研究，解决了DWI管线的核心弊端，为实际应用（多格式立体显示）提供了实用方案。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [106] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出AdaTooler-V与AT-GRPO算法，通过样本级工具收益评分自适应调整奖励，配合两套大规模数据集，实现有选择的视觉工具调用，显著提升多模态推理性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有开源MLLM常无差别调用视觉工具，导致计算开销大和性能下降，需一种能判别是否需要工具的自适应策略。

Method: 1) 提出AT-GRPO强化学习算法，根据每个样本的Tool Benefit Score自适应调整奖励尺度；2) 构建AdaTooler-V-CoT-100k用于SFT冷启动和AdaTooler-V-300k用于RL训练，涵盖单图、多图和视频；3) 训练模型使其仅在工具确有益时调用视觉工具。

Result: AdaTooler-V提出了一种在多模态大语言模型中自适应调用视觉工具的方法，通过强化学习和专门数据集避免盲目调用工具，提高推理效率与性能。

Conclusion: AdaTooler-V能显著减少不必要的视觉工具调用，在多项视觉推理基准上优于现有方法，并在高分辨率基准上超越商业模型。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [107] [DVGT: Driving Visual Geometry Transformer](https://arxiv.org/abs/2512.16919)
*Sicheng Zuo,Zixun Xie,Wenzhao Zheng,Shaoqing Xu,Fang Li,Shengyin Jiang,Long Chen,Zhi-Xin Yang,Jiwen Lu*

Main category: cs.CV

TL;DR: DVGT是一种无需精确相机参数与3D先验的Transformer框架，直接从未标定多视图图像序列输出第一帧车体坐标系下的度量稠密点图与相对位姿，在多个驾驶数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 提出面向驾驶场景的稠密几何感知模型，弥补现有方法无法适应多场景多相机配置的不足。

Method: 设计Driving Visual Geometry Transformer (DVGT)：以DINO作为视觉特征提取器，结合交替的视内局部注意力、跨视图空间注意力与跨帧时间注意力推断图像间几何关系；使用多个解码头直接预测第一帧惯性坐标系下的全局稠密点云以及每帧的自车位姿。关键是不依赖精确相机参数与显式3D几何先验。

Result: 在nuScenes、OpenScene、Waymo、KITTI、DDAD等大规模驾驶数据集混合训练后，DVGT在多种场景下明显优于现有模型；能直接从图像序列预测具度量尺度的几何，无需与外部传感器后对齐。

Conclusion: DVGT提供了灵活且鲁棒的视觉驱动几何重建方案，适用于任意相机配置并提升了驾驶场景下的稠密几何感知性能。

Abstract: Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.

</details>


### [108] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V: build rich video edit pairs from images and clips, fine-tune pretrained text-to-video models with lightweight conditioning and LoRA, and use a unified mask-based control to get SOTA video editing.


<details>
  <summary>Details</summary>
Motivation: Address gaps in video editing—consistency, control, and generalization—by exploring data, model, and control design to create a simple, effective instruction-based video editor.

Method: Summarize methods: data, architecture, control

Result: EasyV2V constructs diverse training data by composing expert edit methods with fast inverses, lifting image edit pairs to videos via single-frame supervision and pseudo pairs using shared affine motion, mining dense-captioned clips, and adding transition supervision; uses pretrained text-to-video models with simple sequence concatenation conditioning and light LoRA fine-tuning; unifies spatiotemporal control via a single mask mechanism and optional reference images.

Conclusion: A simple framework leveraging data composition and minimal architectural changes achieves state-of-the-art instruction-based video editing with flexible input modalities.

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [109] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 提出AuditDM框架：用RL微调的MLLM作为审计员，生成高争议问题与反事实图像以最大化目标模型分歧，发现多样可解释故障并用于无标注微调，显著提升多模型性能与小模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM评估缺乏可解释性，无法充分揭示模型间能力差距；数据规模扩增收益递减，需要更有针对性的诊断与修复方法。

Method: 将一名MLLM通过强化学习微调为审计员，目标是生成能最大化目标模型输出分歧的问题和反事实图像；利用审计员产出的示例作为可解释失败类型的实例并用于无标注微调以纠正目标模型。

Result: 在Gemma-3、PaliGemma-2等SOTA模型上发现20+类失败，基于发现的示例微调后，在16个基准上一致提升性能，且使3B模型超过28B模型表现。

Conclusion: AuditDM通过训练一个强化学习审计员来自动发现并修复多模态大模型（MLLM）的失败模式，提升了模型可解释性与纠错能力。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [110] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 本文提出 NEPA：用 Transformer 预测未来 patch 嵌入（因果掩码、stop-grad），无像素重建/对比/离散 token，取得优异分类与分割迁移性能。


<details>
  <summary>Details</summary>
Motivation: 受 NLP 生成式预训练成功启发，作者探索能否将相同原则用于视觉自监督学习，通过预测嵌入而非仅学习表征来直接训练可用于任务的模型，从而保持架构简单并具可扩展性。

Method: 使用 Transformer 对图像 patch 的嵌入做自回归预测（下一步嵌入预测），训练时只用该预测目标，采用因果掩码和 stop-gradient，未使用像素重建、离散 token、对比损失或任务特定头；在 ImageNet-1k 上预训练，然后微调 ViT-B/ViT-L，评估分类与语义分割迁移。

Result: NEPA 提出用 next-embedding 预测替代传统表征学习，直接生成嵌入以进行预测任务，使用因果掩码与 stop-gradient。

Conclusion: 在 ImageNet-1k 上用 ViT-B/L 仅用 NEPA 预训练并微调可达 83.8%/85.3% top-1，且能迁移到 ADE20K 分割，表明从嵌入进行生成式预训练是一种简单可扩展的自监督视觉学习替代方案。

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [111] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Generative Refocusing：用DeblurNet恢复全对焦图像，用BokehNet合成可控散景；半监督训练结合合成配对与带EXIF的未配对真实散景，性能在若干基准上领先，支持文本引导和自定义光圈。


<details>
  <summary>Details</summary>
Motivation: 单张图像重聚焦需要同时恢复清晰细节和生成真实散景，但现有方法依赖全对焦输入、仿真数据或光圈受限，不能充分捕捉真实光学成像特性。

Method: 两阶段架构：1) DeblurNet负责从各种输入（含散焦）恢复全对焦图像；2) BokehNet基于可控光圈参数生成散景。训练采用半监督策略：利用合成配对数据提供精确监督，同时用未配对的真实散景图像（并结合EXIF信息）进行域适配和风格学习，从而学习真实光学模糊特征并支持光圈控制与文本引导。

Result: 在去散焦去模糊、散景合成与重聚焦基准上取得领先结果；额外展示了文本引导调整（例如散景强度、样式）与自定义光圈形状的应用能力。

Conclusion: 本文提出的Generative Refocusing通过两阶段网络（DeblurNet和BokehNet）以及半监督训练，解决了单张图像重聚焦中的去模糊与真实散景合成问题，结合合成配对数据和带EXIF元数据的未配对真实散景图像，提升了真实光学特性建模能力并支持文本引导和自定义光圈形状。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [112] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas结合文本、运动轨迹与参考图像，实现了可控、连贯且具对象一致性的多主体事件生成视频，支持复杂交互和参考外观引导。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频或仅基于轨迹的方法在可控性、语义表达和对象视觉一致性方面有限。作者希望构建一个能同时利用语言意图、运动控制与参考外观，从而生成更富表达力和一致性的可提示世界事件模拟框架。

Method: 提出WorldCanvas框架，将轨迹（编码运动、时间与可见性）与自然语言（语义意图）及参考图像（视觉锚定）结合，用以控制图像到视频生成。系统能处理多主体交互、物体进出场、参考导向的外观保持以及违反直觉的事件，保持时间连贯性和对象一致性。

Result: 生成的视频在时间连贯性、对象身份保持和场景一致性方面表现良好，能实现多主体交互和复杂事件，展示出将世界模型从预测器推进为用户驱动模拟器的潜力。

Conclusion: WorldCanvas提出了一种多模态可提示世界事件生成框架，通过结合文本、轨迹和参考图像，实现了对复杂场景和多主体交互的可控视频生成。该方法在时间连贯性和对象一致性上表现良好，使世界模型从被动预测器向可交互、用户引导的模拟器转变。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [113] [DP-Bench: A Benchmark for Evaluating Data Product Creation Systems](https://arxiv.org/abs/2512.15798)
*Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.DB

TL;DR: 提出DP-Bench——首个用于评估自动数据产品生成的基准，结合ELT与Text-to-SQL资源，提供若干LLM基线并公开数据集。


<details>
  <summary>Details</summary>
Motivation: 尽管工业界已有许多手工或半自动的数据产品实践，但缺乏标准化基准来评估自动化数据产品生成方法，阻碍了该领域方法的比较和进步。

Method: 通过重用和整合现有ELT与Text-to-SQL基准资源，构建了DP-Bench；并设计了若干基于大模型（LLM）的自动生成方案作为基线，具体实现细节包括任务定义、输入/输出格式与评估指标。

Result: 发布了DP-Bench基准及补充材料，并在Hugging Face上公开数据集，提供可供研究者复现与比较的资源和若干初步基线性能。

Conclusion: 本文提出了第一个用于自动化数据产品创建的基准DP-Bench，并提供了基于ELT与Text-to-SQL工作构建基准的方法与若干LLM基线方法；数据集和材料已公开可用。

Abstract: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .

</details>


### [114] [Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive](https://arxiv.org/abs/2512.15815)
*Valeria Granata,Francois Liot,Xing Wang,Steen Lysgaard,Ivano E. Castelli,Tejs Vegge,Nicola Marzari,Giovanni Pizzi*

Main category: cs.DB

TL;DR: BIG-MAP Archive is a cloud InvenioRDM-based private repository for consortia, offering fine-grained permissions, community scoping, formal upload processes, and redeployability to other initiatives


<details>
  <summary>Details</summary>
Motivation: Enable secure, organized data sharing within large, multi-institution consortia while ensuring readiness for public dissemination and reuse

Method: Describe system and features; modular InvenioRDM deployment; fine-grained permissions; cloud-based archive

Result: Implemented a private, consortium-scoped repository with community-level access control, formalized upload workflows, metadata templating, and export-ready formatting for open repos

Conclusion: BIG-MAP Archive provides secure, scalable, and reusable repository infrastructure tailored to large consortia, balancing confidentiality with controlled sharing and easy redeployment

Abstract: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).

</details>


### [115] [Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers](https://arxiv.org/abs/2512.16083)
*Thanh Dat Hoang,Thanh Tam Nguyen,Thanh Trung Huynh,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 本文提出GrAST，一种高效的schema过滤框架，用于在Text2SQL任务中压缩提示以适应大型数据库。方法结合基于查询的列编码器、利用函数依赖的图变换器重排序以及基于Steiner树的子模式选择。实验表明在召回率接近100%、精度优于现有重排序器和检索器的同时实现亚秒延迟，并能扩展到2.3万列以上的模式。源码开源。


<details>
  <summary>Details</summary>
Motivation: 当前Text2SQL系统通常将整个数据库模式（主要是列信息）与用户问题一并提示LLM，但在现实大规模模式下会超出模型上下文限制，导致性能下降。现有解决方案要么代价高昂，要么忽视列间结构关系，无法有效缩减模式同时保留必要连接性。

Method: 提出GrAST框架：(1) 使用查询感知的LLM编码器对列进行初步排名，编码器融合了列值和元信息；(2) 构建基于函数依赖的列图，采用轻量级图变换器对相互连接的列进行重排序（rerank），捕捉列间结构关系；(3) 用Steiner树启发式算法在列图上选取保持连接性的子模式，作为压缩后的模式供Text2SQL模型提示。

Result: 在真实数据集上的实验显示，GrAST在召回率接近完美的同时比CodeS、SchemaExP、Qwen重排序器和嵌入检索器具有更高精度；中位延迟低于1秒，并能扩展到超过23,000列的大规模模式。

Conclusion: GrAST为大规模Text2SQL提供了一种高效且可扩展的schema过滤解决方案，通过结合查询感知编码、图重排序和连通性保持的子模式选择，在保持高召回的同时提升精度和速度，适合工业级场景。

Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.

</details>


### [116] [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)
*Zhengyuan Dong,Victor Zhong,Renée J. Miller*

Main category: cs.DB

TL;DR: ModelTables是首个大规模模型相关结构化表格基准（60K模型/90K表），用引用、显式链接和共享数据集构建多源真值；对比多种检索方法表明仍有显著提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有检索多聚焦文本，忽视模型性能与配置等结构化表格信息。构建ModelTables以捕捉这些被忽视的结构化语义，支持更精确的表格检索与模型比较。

Method: 收集Hugging Face模型卡、GitHub README及引用论文，抽取并链接表格与模型及发表上下文；用引用、模型卡继承和共享训练集构建多源相关性真值；评估多种表格/元数据检索算子与IR基线，报告P@1等指标。

Result: ModelTables构建了来自模型卡、README和论文的结构化表格语料库，覆盖60K+模型和90K表，强调表格在模型性能与配置语义检索中的价值。

Conclusion: 当前基准显示表格密集的互联关系和现有检索方法的局限性；表格密集检索（dense）效果最好但仍有提升空间，鼓励发展更精准的语义表格检索与结构化知识组织方法。

Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.

</details>


### [117] [Multi-granularity Spatiotemporal Flow Patterns](https://arxiv.org/abs/2512.16255)
*Chrysanthi Kosyfaki,Nikos Mamoulis,Reynold Cheng,Ben Kai*

Main category: cs.DB

TL;DR: 论文提出枚举起点-终点-时间（ODT）流动模式的自底向上算法及若干优化、变体和近似方法，在真实数据上高效发现有意义的客流趋势。


<details>
  <summary>Details</summary>
Motivation: 挖掘不同时间和空间粒度下的出行流动模式，以发现有价值的趋势（如区域间客流），帮助交通公司和相关决策。

Method: 定义三元模式O（起点）、D（终点）、T（时间）即ODT模式，提出自底向上的枚举算法并加入多项搜索空间与计算开销优化；提出受限模式和Top-k模式变体；给出基于生成-测试的近似算法用于快速识别特定规模的ODT模式。

Result: 在三个真实数据集上进行实验：优化显著减少搜索空间与运行时间；约束和Top-k变体满足不同应用需求；近似方法在速度与质量间取得良好折衷；并发现若干有趣且实用的ODT流动模式。

Conclusion: 所提方法能高效、可扩展地枚举与发现不同空间/时间粒度下的重要ODT流动模式，且变体与近似算法增强了实用性，为交通分析和决策提供支持。

Abstract: Analyzing flow of objects or data at different granularities of space and time can unveil interesting insights or trends. For example, transportation companies, by aggregating passenger travel data (e.g., counting passengers traveling from one region to another), can analyze movement behavior. In this paper, we study the problem of finding important trends in passenger movements between regions at different granularities. We define Origin (O), Destination (D), and Time (T ) patterns (ODT patterns) and propose a bottom-up algorithm that enumerates them. We suggest and employ optimizations that greatly reduce the search space and the computational cost of pattern enumeration. We also propose pattern variants (constrained patterns and top-k patterns) that could be useful to differ- ent applications scenarios. Finally, we propose an approximate solution that fast identifies ODT patterns of specific sizes, following a generate-and-test approach. We evaluate the efficiency and effectiveness of our methods on three real datasets and showcase interesting ODT flow patterns in them.

</details>


### [118] [Subset Sampling over Joins](https://arxiv.org/abs/2512.16321)
*Aryan Esmailpour,Xiao Hu,Jinchao Huang,Stavros Sintos*

Main category: cs.DB

TL;DR: Presents static, one-shot, and dynamic algorithms for Poisson subset sampling over acyclic joins achieving near-optimal time/space in input and expected output size.


<details>
  <summary>Details</summary>
Motivation: Enable independent inclusion sampling from implicitly defined join results without materializing exponential-size joins, applicable to ML and analytics over relational data.

Method: Analyze algorithms for subset sampling over acyclic joins, including static index, one-shot, and dynamic index; detail their guarantees and complexities.

Result: Algorithms: (1) Static index builds data structure to output multiple independent subset samples efficiently by exploring join tree and using rejection sampling or precomputed weights. (2) One-shot algorithm produces a single subset sample without full index by traversing join tree with probabilistic decisions, using reservoir-like or recursive sampling to avoid materialization. (3) Dynamic index supports insertions by updating local structures and maintaining samples using localized resampling or lazy updates; can also output multiple independent samples. Complexity: near-optimal in input size and expected sample size; time and space roughly linear in input and proportional to expected sample size plus polylog factors; update time polylog or amortized relative to join tree height.

Conclusion: Provides first efficient methods for subset sampling over acyclic joins with provable guarantees, enabling practical sampling without materializing joins.

Abstract: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \emph{static index} for generating multiple (independent) subset samples over joins; (2) a \emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.

</details>
