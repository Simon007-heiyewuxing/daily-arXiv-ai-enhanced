<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 102]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Gaussian See, Gaussian Do: Semantic 3D Motion Transfer from Multiview Video](https://arxiv.org/abs/2511.14848)
*Yarin Bekor,Gal Michael Harari,Or Perel,Or Litany*

Main category: cs.CV

TL;DR: 提出了‘Gaussian See, Gaussian Do’，通过从源视频反演运动嵌入并在目标静态形体上渲染作为监督，结合锚点视图感知嵌入和鲁棒4D Gaussian Splatting重建，实现高质量、无装配的跨类别语义3D运动迁移。


<details>
  <summary>Details</summary>
Motivation: 目标是实现无需装配（rig-free）且可跨类别的语义三维运动迁移，克服现有隐式方法在跨视图一致性、语义对应以及从2D监督到稳健4D重建的挑战。

Method: 基于隐式运动迁移框架，通过条件反演从源视频提取运动嵌入（motion embeddings），将这些嵌入应用到渲染的目标静态形状帧，生成监督视频，再用这些视频指导动态3D Gaussian Splatting（高斯点云溅射）重建。引入锚点（anchor）- 基于视角的运动嵌入机制以确保跨视图一致性并加速收敛；并设计鲁棒的4D重建流水线以整合噪声监督视频。

Result: 构建了首个语义三维运动迁移基准，实验证明方法在运动保真度和结构一致性方面优于改编的现有基线，并提供代码与数据公开。

Conclusion: 该论文提出了用于从多视角视频进行语义级三维运动迁移的新方法“Gaussian See, Gaussian Do”。方法实现了无绑定、跨类别的运动迁移，保持语义对应并在静态目标形状上生成动态监督视频用于4D高斯点云重建，最终在运动保真度和结构一致性上优于适配基线。

Abstract: We present Gaussian See, Gaussian Do, a novel approach for semantic 3D motion transfer from multiview video. Our method enables rig-free, cross-category motion transfer between objects with semantically meaningful correspondence. Building on implicit motion transfer techniques, we extract motion embeddings from source videos via condition inversion, apply them to rendered frames of static target shapes, and use the resulting videos to supervise dynamic 3D Gaussian Splatting reconstruction. Our approach introduces an anchor-based view-aware motion embedding mechanism, ensuring cross-view consistency and accelerating convergence, along with a robust 4D reconstruction pipeline that consolidates noisy supervision videos. We establish the first benchmark for semantic 3D motion transfer and demonstrate superior motion fidelity and structural consistency compared to adapted baselines. Code and data for this paper available at https://gsgd-motiontransfer.github.io/

</details>


### [2] [When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation](https://arxiv.org/abs/2511.14860)
*Aashish Ghimire,Jun Zeng,Roshan Paudel,Nikhil Kumar Tomar,Deepak Ranjan Nayak,Harshith Reddy Nalla,Vivek Jha,Glenda Reynolds,Debesh Jha*

Main category: cs.CV

TL;DR: Benchmark on DC1000 shows CNNs (DoubleU-Net) beat transformers and mamba; architecture-task fit and dataset size matter more than complexity.


<details>
  <summary>Details</summary>
Motivation: Automate accurate segmentation of dental caries in panoramic radiographs despite challenges (low contrast, variability, limited annotations) and determine which architecture classes perform best.

Method: Benchmarked 12 architectures (CNNs, transformers, mamba/state-space) trained under identical settings on DC1000, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, ResUNet++ etc., evaluated with dice, mIoU, precision, recall.

Result: DoubleU-Net (CNN) achieved best performance: dice 0.7345, mIoU 0.5978, precision 0.8145; top-3 across metrics were CNNs; transformers and mamba underperformed due to limited data and weaker spatial priors.

Conclusion: CNN-based architectures outperform transformers and state-space mamba models for dental caries segmentation on the DC1000 panoramic radiograph dataset; model-task alignment and dataset size are critical.

Abstract: Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.

</details>


### [3] [B-Rep Distance Functions (BR-DF): How to Represent a B-Rep Model by Volumetric Distance Functions?](https://arxiv.org/abs/2511.14870)
*Fuyang Zhang,Pradeep Kumar Jayaraman,Xiang Xu,Yasutaka Furukawa*

Main category: cs.CV

TL;DR: 提出 BR-DF：用 SDF+per-face UDF 表示 B-Rep，配合扩展 Marching Cubes 与多分支潜变量扩散器，能稳定、可靠地生成面片化 B-Rep，成功率达 100%。


<details>
  <summary>Details</summary>
Motivation: 解决现有 CAD 生成方法在转换为 B-Rep 时常失败或不稳定的问题，寻求一种稳定且能表达拓扑信息的体素化表示以保证成功生成 B-Rep。

Method: 将 CAD 表面几何编码为 SDF，并对每个面使用无符号距离函数（UDF）表示顶点/边/面及其拓扑；设计扩展的 Marching Cubes 将其转换为面片化 B-Rep；提出基于 3D U-Net 的多分支潜变量扩散模型同时生成 SDF 与 per-face UDF。

Result: 在与现有 SOTA 方法对比下，性能可比且在生成（面片化）B-Rep 模型时达到了史无前例的 100% 成功率。

Conclusion: BR-DF 提出了一种基于体积距离函数的 B-Rep 表示方法，能稳定将体积表示转换为面片化的 CAD B-Rep，生成过程具有 100% 成功率。

Abstract: This paper presents a novel geometric representation for CAD Boundary Representation (B-Rep) based on volumetric distance functions, dubbed B-Rep Distance Functions (BR-DF). BR-DF encodes the surface mesh geometry of a CAD model as signed distance function (SDF). B-Rep vertices, edges, faces and their topology information are encoded as per-face unsigned distance functions (UDFs). An extension of the Marching Cubes algorithm converts BR-DF directly into watertight CAD B-Rep model (strictly speaking a faceted B-Rep model). A surprising characteristic of BR-DF is that this conversion process never fails. Leveraging the volumetric nature of BR-DF, we propose a multi-branch latent diffusion with 3D U-Net backbone for jointly generating the SDF and per-face UDFs of a BR-DF model. Our approach achieves comparable CAD generation performance against SOTA methods while reaching the unprecedented 100% success rate in producing (faceted) B-Rep models.

</details>


### [4] [GeoSceneGraph: Geometric Scene Graph Diffusion Model for Text-guided 3D Indoor Scene Synthesis](https://arxiv.org/abs/2511.14884)
*Antonio Ruiz,Tao Wu,Andrew Melnik,Qing Cheng,Xuqin Wang,Lu Liu,Yongliang Wang,Yanfeng Zhang,Helge Ritter*

Main category: cs.CV

TL;DR: 提出GeoSceneGraph：用条件化文本特征的等变图神经网络，利用场景图与几何对称性，在无需预定义或标注关系的情况下从文本生成高质量室内3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有从文本合成室内3D场景的方法要么从头训练生成模型，要么依赖视觉-语言模型（VLMs）；前者常忽视场景的图结构导致场景一致性差，后者虽表现强但难以在资源受限设备上部署。另外，现有利用场景图的方法要么需要用户提供语义图，要么依赖真实关系注释，均不便或限制多样关系建模。

Method: 基于等变图神经网络（EGNN），提出了一种将文本特征条件化到EGNN的简单有效策略，利用场景图结构与几何对称性来建模对象间关系，同时不依赖事先定义的关系类别或真实关系标注。通过消融研究验证设计有效性。

Result: 在不使用真实关系注释的设定下，GeoSceneGraph在生成质量上与使用关系标注的方法表现相当，并通过消融实验展示了文本条件化策略与图结构建模对性能的贡献。

Conclusion: GeoSceneGraph通过在不依赖预定义关系类别或人工标注关系的情况下，利用场景的图结构与几何对称性，成功从文本提示生成连贯且现实的室内3D场景，性能可与使用真实关系注释的方法相媲美。

Abstract: Methods that synthesize indoor 3D scenes from text prompts have wide-ranging applications in film production, interior design, video games, virtual reality, and synthetic data generation for training embodied agents. Existing approaches typically either train generative models from scratch or leverage vision-language models (VLMs). While VLMs achieve strong performance, particularly for complex or open-ended prompts, smaller task-specific models remain necessary for deployment on resource-constrained devices such as extended reality (XR) glasses or mobile phones. However, many generative approaches that train from scratch overlook the inherent graph structure of indoor scenes, which can limit scene coherence and realism. Conversely, methods that incorporate scene graphs either demand a user-provided semantic graph, which is generally inconvenient and restrictive, or rely on ground-truth relationship annotations, limiting their capacity to capture more varied object interactions. To address these challenges, we introduce GeoSceneGraph, a method that synthesizes 3D scenes from text prompts by leveraging the graph structure and geometric symmetries of 3D scenes, without relying on predefined relationship classes. Despite not using ground-truth relationships, GeoSceneGraph achieves performance comparable to methods that do. Our model is built on equivariant graph neural networks (EGNNs), but existing EGNN approaches are typically limited to low-dimensional conditioning and are not designed to handle complex modalities such as text. We propose a simple and effective strategy for conditioning EGNNs on text features, and we validate our design through ablation studies.

</details>


### [5] [HULFSynth : An INR based Super-Resolution and Ultra Low-Field MRI Synthesis via Contrast factor estimation](https://arxiv.org/abs/2511.14897)
*Pranav Indrakanti,Ivor Simpson*

Main category: cs.CV

TL;DR: 论文提出基于物理的无监督单张MRI双向合成方法，结合组织SNR估计与INR超分辨率重建，在合成与实测ULF数据上均显著提升WM-GM对比并具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI合成模型缺乏物理一致性且在HF与ULF对比变化建模不足；因此作者引入物理驱动的前向模型以更真实地再现对比差异并提高合成图像质量，尤其是ULF成像中白质-灰质对比。

Method: 提出了一个前向模型通过估计基于目标对比度的组织类型SNR来模拟HF->ULF转化；对超分辨率（HF重建）使用隐式神经表征（INR）网络，同时预测组织分割和像素强度，无需真实HF监督数据；训练为无监督，双向合成（HF->ULF和ULF->HF），并进行了灵敏度分析。

Result: 在合成的ULF样本（由3T T1加噪声生成）上白质-灰质对比度提升52%，在真实配对的3T-64mT数据上提升37%；并展示了对目标对比、噪声和初始条件的鲁棒性。

Conclusion: 该论文提出了一个基于物理启发的无监督双向单张MRI合成器，能在高场（HF）与超低场（ULF）MR影像间进行互转，并在感兴趣的白质-灰质对比度及重建稳健性上取得显著提升。

Abstract: We present an unsupervised single image bidirectional Magnetic Resonance Image (MRI) synthesizer that synthesizes an Ultra-Low Field (ULF) like image from a High-Field (HF) magnitude image and vice-versa. Unlike existing MRI synthesis models, our approach is inspired by the physics that drives contrast changes between HF and ULF MRIs. Our forward model simulates a HF to ULF transformation by estimating the tissue-type Signal-to-Noise ratio (SNR) values based on target contrast values. For the Super-Resolution task, we used an Implicit Neural Representation (INR) network to synthesize HF image by simultaneously predicting tissue-type segmentations and image intensity without observed HF data. The proposed method is evaluated using synthetic ULF-like data from generated from standard 3T T$_1$-weighted images for qualitative assessments and paired 3T-64mT T$_1$-weighted images for validation experiments. WM-GM contrast improved by 52% in synthetic ULF-like images and 37% in 64mT images. Sensitivity experiments demonstrated the robustness of our forward model to variations in target contrast, noise and initial seeding.

</details>


### [6] [InstructMix2Mix: Consistent Sparse-View Editing Through Multi-View Model Personalization](https://arxiv.org/abs/2511.14899)
*Daniel Gilo,Or Litany*

Main category: cs.CV

TL;DR: Introduce I-Mix2Mix: distill 2D diffusion editing into a multi-view diffusion model with new SDS adaptations, achieving better cross-view consistency and edit quality.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view editing methods (neural fields or temporal attention) produce inconsistent or artifact-prone edits from sparse heterogeneous views; leveraging a 2D diffusion model's editing ability together with a multi-view model's 3D prior can improve results.

Method: Distill a 2D diffusion teacher into a multi-view diffusion student, replace neural-field consolidator in SDS with the multi-view student, use incremental student updates across timesteps, design a teacher noise scheduler, and modify attention to boost cross-view coherence.

Result: Significant improvements in multi-view consistency and retained high per-frame edit quality in experiments.

Conclusion: I-Mix2Mix effectively transfers 2D diffusion editing strengths into multi-view settings, yielding consistent cross-view edits and improved per-frame quality compared to prior neural-field and temporal-attention approaches.

Abstract: We address the task of multi-view image editing from sparse input views, where the inputs can be seen as a mix of images capturing the scene from different viewpoints. The goal is to modify the scene according to a textual instruction while preserving consistency across all views. Existing methods, based on per-scene neural fields or temporal attention mechanisms, struggle in this setting, often producing artifacts and incoherent edits. We propose InstructMix2Mix (I-Mix2Mix), a framework that distills the editing capabilities of a 2D diffusion model into a pretrained multi-view diffusion model, leveraging its data-driven 3D prior for cross-view consistency. A key contribution is replacing the conventional neural field consolidator in Score Distillation Sampling (SDS) with a multi-view diffusion student, which requires novel adaptations: incremental student updates across timesteps, a specialized teacher noise scheduler to prevent degeneration, and an attention modification that enhances cross-view coherence without additional cost. Experiments demonstrate that I-Mix2Mix significantly improves multi-view consistency while maintaining high per-frame edit quality.

</details>


### [7] [Skin-R1: Toward Trustworthy Clinical Reasoning for Dermatological Diagnosis](https://arxiv.org/abs/2511.14900)
*Zehao Liu,Wejieying Ren,Jipeng Zhang,Tianxiang Zhao,Jingxi Zhu,Xiaoting Li,Vasant G. Honavar*

Main category: cs.CV

TL;DR: SkinR1 builds expert-like grounded reasoning from textbooks and uses RL with disease hierarchy to generalize this reasoning to large sparse datasets, improving diagnostic accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Address data heterogeneity, lack of grounded diagnostic rationales, and limited scalability/generalization of existing dermatological VLMs.

Method: Generate hierarchy-aware DDx-informed reasoning trajectories from textbooks; use them for supervised fine-tuning; apply a novel RL paradigm that incorporates disease hierarchy to transfer reasoning to sparse large-scale data.

Result: Superior diagnostic accuracy across multiple dermatology datasets; ablation shows SFT reasoning foundation is critical.

Conclusion: SkinR1 effectively combines textbook-based reasoning and RL to improve dermatological VLMs’ diagnostic accuracy and generalization.

Abstract: The emergence of vision-language models (VLMs) has opened new possibilities for clinical reasoning and has shown promising performance in dermatological diagnosis. However, their trustworthiness and clinical utility are often limited by three major factors: (1) Data heterogeneity, where diverse datasets lack consistent diagnostic labels and clinical concept annotations; (2) Absence of grounded diagnostic rationales, leading to a scarcity of reliable reasoning supervision; and (3) Limited scalability and generalization, as models trained on small, densely annotated datasets struggle to transfer nuanced reasoning to large, sparsely-annotated ones.
  To address these limitations, we propose SkinR1, a novel dermatological VLM that combines deep, textbook-based reasoning with the broad generalization capabilities of reinforcement learning (RL). SkinR1 systematically resolves the key challenges through a unified, end-to-end framework. First, we design a textbook-based reasoning generator that synthesizes high-fidelity, hierarchy-aware, and differential-diagnosis (DDx)-informed trajectories, providing reliable expert-level supervision. Second, we leverage the constructed trajectories for supervised fine-tuning (SFT) empowering the model with grounded reasoning ability. Third, we develop a novel RL paradigm that, by incorporating the hierarchical structure of diseases, effectively transfers these grounded reasoning patterns to large-scale, sparse data. Extensive experiments on multiple dermatology datasets demonstrate that SkinR1 achieves superior diagnostic accuracy. The ablation study demonstrates the importance of the reasoning foundation instilled by SFT.

</details>


### [8] [FarSLIP: Discovering Effective CLIP Adaptation for Fine-Grained Remote Sensing Understanding](https://arxiv.org/abs/2511.14901)
*Zhenshi Li,Weikang Yu,Dilxat Muhtar,Xueliang Zhang,Pengfeng Xiao,Pedram Ghamisi,Xiao Xiang Zhu*

Main category: cs.CV

TL;DR: 通过MGRS-200k和FarSLIP（patch-to-patch蒸馏+CLS token区域对齐），该工作解决了遥感领域CLIP的细粒度对齐问题，提升了分割、分类与检索性能并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP及其遥感变体全局对齐能力有限，难以捕捉细粒度空间信息；现有RS图文数据只用全局caption而忽略了对象级标签；通用域的区域-文本对齐方法直接应用于遥感常导致性能下降。

Method: 构建MGRS-200k数据集提供对象级文本监督；分析现有细粒度对齐方法导致语义一致性退化的问题；提出FarSLIP，包含patch-to-patch自蒸馏以同时对齐局部与全局视觉特征，以及基于CLS token的区域-类别对齐而非显式patch-level对齐。

Result: 在遥感开放词汇语义分割、零样本分类和图文检索等任务上均实现了新的SOTA，表明FarSLIP在提升细粒度视觉-语言对齐及保留语义一致性方面的有效性。

Conclusion: 该论文提出FarSLIP框架，通过构建多粒度遥感图文数据集MGRS-200k并采用patch-to-patch蒸馏及CLS token区域-类别对齐，改善了CLIP在遥感领域的细粒度对齐问题，保持语义一致性并提升了空间感知能力，取得了多项任务的新SOTA。

Abstract: As CLIP's global alignment limits its ability to capture fine-grained details, recent efforts have focused on enhancing its region-text alignment. However, current remote sensing (RS)-specific CLIP variants still inherit this limited spatial awareness. We identify two key limitations behind this: (1) current RS image-text datasets generate global captions from object-level labels, leaving the original object-level supervision underutilized; (2) despite the success of region-text alignment methods in general domain, their direct application to RS data often leads to performance degradation. To address these, we construct the first multi-granularity RS image-text dataset, MGRS-200k, featuring rich object-level textual supervision for RS region-category alignment. We further investigate existing fine-grained CLIP tuning strategies and find that current explicit region-text alignment methods, whether in a direct or indirect way, underperform due to severe degradation of CLIP's semantic coherence. Building on these, we propose FarSLIP, a Fine-grained Aligned RS Language-Image Pretraining framework. Rather than the commonly used patch-to-CLS self-distillation, FarSLIP employs patch-to-patch distillation to align local and global visual cues, which improves feature discriminability while preserving semantic coherence. Additionally, to effectively utilize region-text supervision, it employs simple CLS token-based region-category alignment rather than explicit patch-level alignment, further enhancing spatial awareness. FarSLIP features improved fine-grained vision-language alignment in RS domain and sets a new state of the art not only on RS open-vocabulary semantic segmentation, but also on image-level tasks such as zero-shot classification and image-text retrieval. Our dataset, code, and models are available at https://github.com/NJU-LHRS/FarSLIP.

</details>


### [9] [nnMIL: A generalizable multiple instance learning framework for computational pathology](https://arxiv.org/abs/2511.14907)
*Xiangde Luo,Jinxi Xiang,Yuanfeng Ji,Ruijiang Li*

Main category: cs.CV

TL;DR: nnMIL通过在patch与特征层的随机采样与轻量聚合器，提供可扩展、稳健且可不确定性估计的切片级预测方法，显著提升病理AI的泛化与临床可用性。


<details>
  <summary>Details</summary>
Motivation: 现有将病理基础模型提取的patch级表示汇总为切片级预测的方法受设计限制，难以在泛化性与可靠性上满足临床部署需求。

Method: 在patch和特征层引入随机采样以支持大批量优化和任务感知采样；使用轻量聚合器与滑动窗口推理生成集成切片预测，并提供不确定性量化。

Result: 在4万张WSI、35个临床任务和4个基础模型上，nnMIL在诊断、组织学亚型划分、分子生物标志物检测与跨癌种预后预测上均优于现有MIL方法；展现出强的跨模型泛化、可靠的不确定性估计与外部队列的稳健生存分层能力。

Conclusion: nnMIL是一个简洁且通用的多实例学习框架，能将病理基础模型的patch级特征可靠地聚合为切片级临床预测，提升了性能、泛化性与不确定性估计能力。

Abstract: Computational pathology holds substantial promise for improving diagnosis and guiding treatment decisions. Recent pathology foundation models enable the extraction of rich patch-level representations from large-scale whole-slide images (WSIs), but current approaches for aggregating these features into slide-level predictions remain constrained by design limitations that hinder generalizability and reliability. Here, we developed nnMIL, a simple yet broadly applicable multiple-instance learning framework that connects patch-level foundation models to robust slide-level clinical inference. nnMIL introduces random sampling at both the patch and feature levels, enabling large-batch optimization, task-aware sampling strategies, and efficient and scalable training across datasets and model architectures. A lightweight aggregator performs sliding-window inference to generate ensemble slide-level predictions and supports principled uncertainty estimation. Across 40,000 WSIs encompassing 35 clinical tasks and four pathology foundation models, nnMIL consistently outperformed existing MIL methods for disease diagnosis, histologic subtyping, molecular biomarker detection, and pan- cancer prognosis prediction. It further demonstrated strong cross-model generalization, reliable uncertainty quantification, and robust survival stratification in multiple external cohorts. In conclusion, nnMIL offers a practical and generalizable solution for translating pathology foundation models into clinically meaningful predictions, advancing the development and deployment of reliable AI systems in real-world settings.

</details>


### [10] [X-WIN: Building Chest Radiograph World Model via Predictive Sensing](https://arxiv.org/abs/2511.14918)
*Zefan Yang,Ge Wang,James Hendler,Mannudeep K. Kalra,Pingkun Yan*

Main category: cs.CV

TL;DR: X-WIN在潜在空间预测CT投影并用亲和对比对齐和真实CXR自监督技术桥接域差距，从而把CT的3D信息蒸馏到CXR模型，提升了下游诊断性能并能用于体积重建。


<details>
  <summary>Details</summary>
Motivation: CXR为二维投影，存在结构叠加问题，难以获得三维解剖信息，进而制约表征学习与疾病诊断性能，因此希望通过利用CT的体积信息提升CXR模型的3D感知能力。

Method: 在CT体积上生成多视角投影作为监督，模型学习在潜在空间预测这些投影；引入亲和引导的对比对齐损失以利用同一体积不同投影间的相似性；结合掩码图像建模和领域分类器将真实CXR纳入训练，缩小模拟与真实分布差距。

Result: 在线性探测和少样本微调的多种下游任务上，X-WIN优于现有基础模型；还可渲染2D投影并用于CT体积重建，展示了学习到的3D结构知识。

Conclusion: X-WIN通过在潜在空间中预测CT的2D投影并引入亲和引导的对比对齐损失，有效蒸馏了体积信息，从而提升了CXR的表征能力。

Abstract: Chest X-ray radiography (CXR) is an essential medical imaging technique for disease diagnosis. However, as 2D projectional images, CXRs are limited by structural superposition and hence fail to capture 3D anatomies. This limitation makes representation learning and disease diagnosis challenging. To address this challenge, we propose a novel CXR world model named X-WIN, which distills volumetric knowledge from chest computed tomography (CT) by learning to predict its 2D projections in latent space. The core idea is that a world model with internalized knowledge of 3D anatomical structure can predict CXRs under various transformations in 3D space. During projection prediction, we introduce an affinity-guided contrastive alignment loss that leverages mutual similarities to capture rich, correlated information across projections from the same volume. To improve model adaptability, we incorporate real CXRs into training through masked image modeling and employ a domain classifier to encourage statistically similar representations for real and simulated CXRs. Comprehensive experiments show that X-WIN outperforms existing foundation models on diverse downstream tasks using linear probing and few-shot fine-tuning. X-WIN also demonstrates the ability to render 2D projections for reconstructing a 3D CT volume.

</details>


### [11] [CPSL: Representing Volumetric Video via Content-Promoted Scene Layers](https://arxiv.org/abs/2511.14927)
*Kaiyuan Hu,Yili Jin,Junhua Liu,Xize Duan,Hong Kang,Xue Liu*

Main category: cs.CV

TL;DR: CPSL 用轻量的 2.5D 图层组合（含软 alpha 和边缘深度缓存）实现低成本的视差校正与帧间连贯，兼顾体感效果与高效编码，适合可扩展的沉浸式视频场景。


<details>
  <summary>Details</summary>
Motivation: 现有显式点云与隐式神经场的体积视频在采集、计算与渲染上代价高昂，不适用于按需视频与实时通信，需一种更高效且具有体感优势的表示。

Method: 基于每帧深度与内容显著性，将帧分解为少量几何一致的图层（含软 alpha 带和边缘深度缓存），通过深度加权的视点扭曲与前到后的 alpha 复合进行视差校正，利用运动引导传播与逐层编码保持帧间连贯并支持标准视频编解码器播放。

Result: 在多个基准上，CPSL 在感知质量与边界保真度上优于基于图层与神经场的基线，同时在存储与渲染成本上降低数倍。

Conclusion: CPSL 在保持体积感与遮挡关系的同时，极大降低了存储与渲染成本，为将 2D 视频扩展为可交互的 2.5D 沉浸式媒体提供了实用路径。

Abstract: Volumetric video enables immersive and interactive visual experiences by supporting free viewpoint exploration and realistic motion parallax. However, existing volumetric representations from explicit point clouds to implicit neural fields, remain costly in capture, computation, and rendering, which limits their scalability for on-demand video and reduces their feasibility for real-time communication.
  To bridge this gap, we propose Content-Promoted Scene Layers (CPSL), a compact 2.5D video representation that brings the perceptual benefits of volumetric video to conventional 2D content. Guided by per-frame depth and content saliency, CPSL decomposes each frame into a small set of geometry-consistent layers equipped with soft alpha bands and an edge-depth cache that jointly preserve occlusion ordering and boundary continuity. These lightweight, 2D-encodable assets enable parallax-corrected novel-view synthesis via depth-weighted warping and front-to-back alpha compositing, bypassing expensive 3D reconstruction. Temporally, CPSL maintains inter-frame coherence using motion-guided propagation and per-layer encoding, supporting real-time playback with standard video codecs. Across multiple benchmarks, CPSL achieves superior perceptual quality and boundary fidelity compared with layer-based and neural-field baselines while reducing storage and rendering cost by several folds. Our approach offer a practical path from 2D video to scalable 2.5D immersive media.

</details>


### [12] [Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities](https://arxiv.org/abs/2511.14945)
*Fan Yang,Quanting Xie,Atsunori Moteki,Shoichi Masui,Shan Jiang,Yonatan Bisk,Graham Neubig*

Main category: cs.CV

TL;DR: 提供首个长周期低对比度多模态人类活动基准和三项任务，并提出免训练轻量基线，在无监督设置下显著领先且具备实际部署优势。


<details>
  <summary>Details</summary>
Motivation: 短期高对比度周期性活动已有大量研究，但长期低对比度的周期性工作流在制造、体育与日常生活中普遍存在却被忽视；现有方法对这种长期低对比度模式效果差且需标注，因而需要新的数据基准与无需标注的有效方法。

Method: 构建长周期低对比度人类活动数据集，设计三项与现实应用对齐的评估任务；提出一种轻量、免训练的基线模型用于建模多样的周期性工作流模式；对比无监督周期检测方法、基于大型语言模型的零-shot 方法及传统监督方法进行广泛实验评估。

Result: 基准对无监督检测方法与强大的零-shot LLM方法构成挑战；所提基线在三项任务上均显著优于比较方法；在真实应用中，基线在部署优势上可与传统监督检测方法相媲美，消除了标注与重训练需求。

Conclusion: 本文提出了首个包含580条多模态长周期人类活动序列的基准数据集，并定义了三项评估任务：无监督周期工作流检测、任务完成追踪与程序性异常检测；同时提出了一个轻量、免训练的基线方法，在所有任务上均显著优于现有方法，并在实际部署中展现出无需标注与重训练的优势。

Abstract: Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.

</details>


### [13] [RocSync: Millisecond-Accurate Temporal Synchronization for Heterogeneous Camera Systems](https://arxiv.org/abs/2511.14948)
*Jaro Meyer,Frédéric Giraud,Joschua Wüthrich,Marc Pollefeys,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 作者通过红/红外LED时钟在视频帧中编码曝光窗口，实现对异构摄像头的毫秒级同步（1.34 ms RMSE），优于传统方法，并提升下游多视角视觉任务，在手术等真实场景中成功部署。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多摄像头同步困难，尤其是异构设备和受限环境（如手术室）无法使用硬件同步或音频/时间码等常用方法，迫切需要一种廉价、通用且支持RGB与IR的同步方案。

Method: 构建一个由红色与红外LED组成的LED时钟，在显示端编码时间，摄像头录制后从每帧中解码曝光开始和结束时间，基于这些曝光窗口计算各摄像头的时间偏差并进行对齐。评估包括与硬件同步比较、与基于光、音频、时间码方法对比，并应用于多视角姿态估计与3D重建。

Result: 在多次录制中，相对于硬件同步实现了1.34 ms RMSE的残差误差；优于光、音频和时间码方法；在多视角姿态估计与3D重建任务上表现改善；并在包含25+台异构摄像头的大规模手术录制中成功验证。

Conclusion: 该论文提出了一种低成本、通用的多摄像头时间同步方法，通过自制“LED时钟”使用红外与可见光LED在视频帧中编码曝光窗口，实现毫秒级别对齐，能在异构系统（RGB/IR、无音频等）中工作，并在手术等大规模场景验证。

Abstract: Accurate spatiotemporal alignment of multi-view video streams is essential for a wide range of dynamic-scene applications such as multi-view 3D reconstruction, pose estimation, and scene understanding. However, synchronizing multiple cameras remains a significant challenge, especially in heterogeneous setups combining professional and consumer-grade devices, visible and infrared sensors, or systems with and without audio, where common hardware synchronization capabilities are often unavailable. This limitation is particularly evident in real-world environments, where controlled capture conditions are not feasible. In this work, we present a low-cost, general-purpose synchronization method that achieves millisecond-level temporal alignment across diverse camera systems while supporting both visible (RGB) and infrared (IR) modalities. The proposed solution employs a custom-built \textit{LED Clock} that encodes time through red and infrared LEDs, allowing visual decoding of the exposure window (start and end times) from recorded frames for millisecond-level synchronization. We benchmark our method against hardware synchronization and achieve a residual error of 1.34~ms RMSE across multiple recordings. In further experiments, our method outperforms light-, audio-, and timecode-based synchronization approaches and directly improves downstream computer vision tasks, including multi-view pose estimation and 3D reconstruction. Finally, we validate the system in large-scale surgical recordings involving over 25 heterogeneous cameras spanning both IR and RGB modalities. This solution simplifies and streamlines the synchronization pipeline and expands access to advanced vision-based sensing in unconstrained environments, including industrial and clinical applications.

</details>


### [14] [Artificial intelligence approaches for energy-efficient laser cutting machines](https://arxiv.org/abs/2511.14952)
*Mohamed Abdallah Salem,Hamdy Ahmed Ashour,Ahmed Elshenawy*

Main category: cs.CV

TL;DR: 通过基于无透镜散斑与摄像头的深度学习材料识别以及烟雾检测的闭环控制，激光切割抽吸泵能耗在实验中减少20%~50%。


<details>
  <summary>Details</summary>
Motivation: 激光切割过程中抽吸泵普遍为开环、功率不可自适应，导致能耗高且不利于环境可持续性，故需开发能根据材料与烟雾水平自适应调节的节能控制方案。

Method: 提出两种材料分类方法（无透镜散斑传感结合定制CNN与基于USB相机并使用VGG16迁移学习）及独立的烟雾检测深度学习模型，基于分类与检测结果实时闭环调整泵功率并在空闲时自动停机。

Result: 实验结果显示，通过材料识别与烟雾水平共同控制，抽吸泵能耗可降低20%至50%，系统能在空闲时自动停止并根据实时烟雾强度调整功率，验证了方法的有效性。

Conclusion: 本文通过闭环控制和深度学习实现对激光切割排烟抽吸泵的能耗优化，结论是：采用材料分类与烟雾检测联合调节泵功率，可在实验中将抽吸泵能耗降低20%至50%，对制造业可持续发展具有积极意义。

Abstract: This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.

</details>


### [15] [EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects](https://arxiv.org/abs/2511.14970)
*Gbenga Omotara,Ramy Farag,Seyed Mohamad Ali Tousi,G. N. DeSouza*

Main category: cs.CV

TL;DR: 提出边缘引导的注意力融合（EGSA）与从RGB边缘向预测深度边缘过渡的训练策略，有效提升透明物体的深度估计并维持分割性能。


<details>
  <summary>Details</summary>
Motivation: 透明物体使深度估计和语义分割都变得困难，且多任务学习中任务间的负面影响会降低整体表现，因此需要一种能保留各任务有益信息、抑制破坏性交互的融合机制。

Method: 设计了基于边界信息的融合模块EGSA，将语义特征与几何特征在融合前通过边缘指导加权；同时提出多模态渐进训练策略，从RGB边缘过渡到预测深度边缘以引导学习，训练时无需真值深度。

Result: 在Syn-TODD和ClearPose基准上，EGSA在保留分割性能的同时相较于MODEST显著提升了深度精度，尤其在透明区域的改进最明显；多模态渐进训练则使模型不依赖训练时的真实深度数据。

Conclusion: 本文提出的Edge-Guided Spatial Attention (EGSA) 有效缓解了透明物体场景中语义与几何任务的负交互，从而提升了深度估计精度并保持分割性能。

Abstract: Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.

</details>


### [16] [Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation](https://arxiv.org/abs/2511.14981)
*Nicholas Cooper,Lijun Chen,Sailesh Dwivedy,Danna Gurari*

Main category: cs.CV

TL;DR: 提出一种仅基于特征的蒸馏框架，利用潜在表示几何度量筛选教师层，显著提升学生模型准确率（最多 +15%），并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有特征蒸馏通常结合 logits（预 softmax）损失和中间特征损失，但作者认为可以仅靠高质量的特征信息完成有效蒸馏；因此需要方法自动识别哪些教师层提供有价值的特征并避免对 logits 的依赖。

Method: 方法通过仅在骨干网络层之间应用特征匹配损失来训练学生模型；引入一个基于潜在表示几何性质的“知识质量”度量，用于自动选择最有利于蒸馏的教师层；在训练中省略 logits/交叉熵损失，聚焦于中间特征的对齐。

Result: 在三个图像分类数据集和四组不同的学生-教师配置（包括 CNN 与 ViT）上，方法取得了最先进的结果，部分配置相比基准方法在 top-1 准确率上提升最多达 15%。作者并公开了代码库。

Conclusion: 本文提出了一个仅基于特征损失（不使用对数概率/交叉熵）的知识蒸馏框架，通过分析潜在表示的几何结构来选择高质量教师层，实验证明在多种学生-教师对与数据集上均能显著提升性能。

Abstract: Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.

</details>


### [17] [Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation](https://arxiv.org/abs/2511.14993)
*Vladimir Arkhipkin,Vladimir Korviakov,Nikolai Gerasimenko,Denis Parkhomenko,Viacheslav Vasilev,Alexey Letunovskiy,Maria Kovaleva,Nikolai Vaulin,Ivan Kirillov,Lev Novitskiy,Denis Koposov,Nikita Kiselev,Alexander Varlamov,Dmitrii Mikhailov,Vladimir Polovnikov,Andrey Shutkin,Ilya Vasiliev,Julia Agafonova,Anastasiia Kargapoltseva,Anna Dmitrienko,Anastasia Maltseva,Anna Averchenkova,Olga Kim,Tatiana Nikulina,Denis Dimitrov*

Main category: cs.CV

TL;DR: Kandinsky 5.0是一套公开的高分辨率图像与10秒短视频生成基础模型家族，结合大规模数据处理、多阶段预训练、SFT与RL优化及架构/推理加速，在生成质量与速度上达到行业领先并公开资源。


<details>
  <summary>Details</summary>
Motivation: 构建一个公开、可适配、在图像与短视频生成上达到业界领先质量且速度可接受的生成式大模型框架，推动高质量生成模型的可用性与研究发展。

Method: 采用多阶段训练流程：大规模数据收集与清洗（过滤、聚类）、广泛的预训练，随后进行自监督微调（SFT）与基于强化学习的后期训练以提升质量；在模型架构上引入针对高分辨率与视频的优化结构，并在训练与推理阶段使用效率优化以加速生成。

Result: 发布三条产品线（Image Lite 6B、Video Lite 2B、Video Pro 19B），在速度与质量上取得显著提升，尤其Video Pro在视频质量上表现突出；通过人类评估证明性能，并开源代码与训练检查点以便社区使用。

Conclusion: Kandinsky 5.0是一套面向高分辨率图像与10秒视频合成的生成式基础模型家族，涵盖轻量级与性能型模型，结合多阶段大规模预训练、SFT与RL后训练，以及数据生命周期管理与架构/训练/推理优化，达到人类评估下的领先生成质量并公开代码与权重以推进研究社区发展。

Abstract: This report introduces Kandinsky 5.0, a family of state-of-the-art foundation models for high-resolution image and 10-second video synthesis. The framework comprises three core line-up of models: Kandinsky 5.0 Image Lite - a line-up of 6B parameter image generation models, Kandinsky 5.0 Video Lite - a fast and lightweight 2B parameter text-to-video and image-to-video models, and Kandinsky 5.0 Video Pro - 19B parameter models that achieves superior video generation quality. We provide a comprehensive review of the data curation lifecycle - including collection, processing, filtering and clustering - for the multi-stage training pipeline that involves extensive pre-training and incorporates quality-enhancement techniques such as self-supervised fine-tuning (SFT) and reinforcement learning (RL)-based post-training. We also present novel architectural, training, and inference optimizations that enable Kandinsky 5.0 to achieve high generation speeds and state-of-the-art performance across various tasks, as demonstrated by human evaluation. As a large-scale, publicly available generative framework, Kandinsky 5.0 leverages the full potential of its pre-training and subsequent stages to be adapted for a wide range of generative applications. We hope that this report, together with the release of our open-source code and training checkpoints, will substantially advance the development and accessibility of high-quality generative models for the research community.

</details>


### [18] [FinCriticalED: A Visual Benchmark for Financial Fact-Level OCR Evaluation](https://arxiv.org/abs/2511.14998)
*Yueru He,Xueqing Peng,Yupeng Cao,Yan Wang,Lingfei Qian,Haohang Li,Yi Han,Ruoyu Xiang,Mingquan Lin,Prayag Tiwari,Jimin Huang,Guojun Xiong,Sophia Ananiadou*

Main category: cs.CV

TL;DR: FinCriticalED：第一个面向金融文档的事实级视觉评估基准，500 图-HTML、700+ 专家注释事实，使用 LLM 作为评判器，揭示现有模型在数值与时间理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统 OCR 评估指标（如 ROUGE、编辑距离）只能衡量表面文本相似性，无法反映对金融文档中符号、量级与时间等关键事实的敏感性；因此需要事实级评估来保障财务解读的严谨性。

Method: 构建包含500张图像-HTML 对的数据集，专家注释700+个数值与时间事实；设计 LLM-as-Judge 管线进行结构化事实抽取与上下文校验；基准测试多种 OCR、开源与专有视觉语言模型。

Result: 在基准测试中，最强的专有模型在事实准确率上表现最佳，但在复杂视觉数值与时间场景仍有大量错误；FinCriticalED 为提升视觉系统在精密域的事实精确性提供了量化与质化证据。

Conclusion: FinCriticalED 提出并展示了一个面向金融文档的事实级评估基准，强调在高风险场景下对数值与时间信息的精确识别，并表明现有 OCR 与 VLM 在细粒度数值/时间理解上仍存在显著误差。

Abstract: We introduce FinCriticalED (Financial Critical Error Detection), a visual benchmark for evaluating OCR and vision language models on financial documents at the fact level. Financial documents contain visually dense and table heavy layouts where numerical and temporal information is tightly coupled with structure. In high stakes settings, small OCR mistakes such as sign inversion or shifted dates can lead to materially different interpretations, while traditional OCR metrics like ROUGE and edit distance capture only surface level text similarity. \ficriticaled provides 500 image-HTML pairs with expert annotated financial facts covering over seven hundred numerical and temporal facts. It introduces three key contributions. First, it establishes the first fact level evaluation benchmark for financial document understanding, shifting evaluation from lexical overlap to domain critical factual correctness. Second, all annotations are created and verified by financial experts with strict quality control over signs, magnitudes, and temporal expressions. Third, we develop an LLM-as-Judge evaluation pipeline that performs structured fact extraction and contextual verification for visually complex financial documents. We benchmark OCR systems, open source vision language models, and proprietary models on FinCriticalED. Results show that although the strongest proprietary models achieve the highest factual accuracy, substantial errors remain in visually intricate numerical and temporal contexts. Through quantitative evaluation and expert case studies, FinCriticalED provides a rigorous foundation for advancing visual factual precision in financial and other precision critical domains.

</details>


### [19] [CKDA: Cross-modality Knowledge Disentanglement and Alignment for Visible-Infrared Lifelong Person Re-identification](https://arxiv.org/abs/2511.15016)
*Zhenyu Cui,Jiahuan Zhou,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出CKDA：用MCP/MSP解耦模态共有与特定知识，并用CKA在双模态原型空间中对齐新旧知识，显著改善VI-LReID中的遗忘与性能。


<details>
  <summary>Details</summary>
Motivation: 当前VI-LReID方法依赖跨模态知识蒸馏以缓解旧知识遗忘，但忽视了模态特定知识学习与模态共有知识抗遗忘之间的相互冲突，导致学习冲突进而协同遗忘。论文旨在通过知识解耦与对齐解决该问题。

Method: 提出Modality-Common Prompting (MCP)和Modality-Specific Prompting (MSP)用于显式解耦并净化模态共有与模态特定判别信息，避免两类知识的互相干扰；设计Cross-modal Knowledge Alignment (CKA)模块，在互独立的模态内与模态间特征空间中，基于双模态原型对新旧知识进行平衡对齐，减少灾难性遗忘。

Result: 在四个基准数据集上的大量实验表明，CKDA在平均性能上优于现有最先进方法，证明了分离与对齐策略在缓解灾难性遗忘与提升跨模态终身ReID性能方面的有效性。作者已公开代码。

Conclusion: 本文提出CKDA方法，通过显式分离模态特定与模态共有知识并进行对齐，能够缓解跨模态互相干扰与遗忘问题，从而提升可见—红外终身行人重识别的持续学习性能。

Abstract: Lifelong person Re-IDentification (LReID) aims to match the same person employing continuously collected individual data from different scenarios. To achieve continuous all-day person matching across day and night, Visible-Infrared Lifelong person Re-IDentification (VI-LReID) focuses on sequential training on data from visible and infrared modalities and pursues average performance over all data. To this end, existing methods typically exploit cross-modal knowledge distillation to alleviate the catastrophic forgetting of old knowledge. However, these methods ignore the mutual interference of modality-specific knowledge acquisition and modality-common knowledge anti-forgetting, where conflicting knowledge leads to collaborative forgetting. To address the above problems, this paper proposes a Cross-modality Knowledge Disentanglement and Alignment method, called CKDA, which explicitly separates and preserves modality-specific knowledge and modality-common knowledge in a balanced way. Specifically, a Modality-Common Prompting (MCP) module and a Modality-Specific Prompting (MSP) module are proposed to explicitly disentangle and purify discriminative information that coexists and is specific to different modalities, avoiding the mutual interference between both knowledge. In addition, a Cross-modal Knowledge Alignment (CKA) module is designed to further align the disentangled new knowledge with the old one in two mutually independent inter- and intra-modality feature spaces based on dual-modality prototypes in a balanced manner. Extensive experiments on four benchmark datasets verify the effectiveness and superiority of our CKDA against state-of-the-art methods. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/CKDA-AAAI2026.

</details>


### [20] [Complex-Valued 2D Gaussian Representation for Computer-Generated Holography](https://arxiv.org/abs/2511.15022)
*Yicheng Zhan,Xiangjun Gao,Long Quan,Kaan Akşit*

Main category: cs.CV

TL;DR: 用可微化的结构化复数2D高斯基元表示全息图，减少参数、加速优化并提高重建质量，同时提供实用的相位仅转换流程以抑制噪声。


<details>
  <summary>Details</summary>
Motivation: 逐像素的全息表示参数量大、优化耗时且显存占用高，现有方法在噪声与可扩展性方面存在局限。通过结构化基元可压缩表示以提升效率与重建质量。

Method: 提出用结构化复数2D高斯基元参数化全息图，并为其开发了可微的光栅化器，结合基于GPU优化的自由空间光传播核实现端到端训练。还设计了转换流程将该表示适配为实际的相位仅幽光图（平滑和随机相位）格式。

Result: 与现有方法相比，本方法在实验中实现了高达2.5倍更低的显存使用、50%更快的优化速度，同时重建保真度更高；转换流程能有效抑制先前方法出现的噪声伪影。

Conclusion: 本文提出了一种基于结构化复数二维高斯基元的新型全息表示，显著减少逐像素存储并将参数搜索空间最多缩减至10:1。

Abstract: We propose a new hologram representation based on structured complex-valued 2D Gaussian primitives, which replaces per-pixel information storage and reduces the parameter search space by up to 10:1. To enable end-to-end training, we develop a differentiable rasterizer for our representation, integrated with a GPU-optimized light propagation kernel in free space. Our extensive experiments show that our method achieves up to 2.5x lower VRAM usage and 50% faster optimization while producing higher-fidelity reconstructions than existing methods. We further introduce a conversion procedure that adapts our representation to practical hologram formats, including smooth and random phase-only holograms. Our experiments show that this procedure can effectively suppress noise artifacts observed in previous methods. By reducing the hologram parameter search space, our representation enables a more scalable hologram estimation in the next-generation computer-generated holography systems.

</details>


### [21] [Computer Vision Modeling of the Development of Geometric and Numerical Concepts in Humans](https://arxiv.org/abs/2511.15029)
*Zekun Wang,Sashank Varma*

Main category: cs.CV

TL;DR: ResNet-50在训练过程中部分再现了儿童在几何与数感概念上的发展轨迹，显示出发展层面的认知对齐，但并非对所有概念均成立，提示进一步扩展模型与基准的必要性。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明CV模型在分类任务中学到的隐含表征与成人的几何与数感表征相似，研究动机是进一步考察这种“认知对齐”是否扩展到发展层面，即模型随经验增加的性能改进是否模拟儿童的认知发展轨迹。

Method: 以ResNet-50为案例，使用在图像分类任务上训练的模型中间表征，针对几何（欧几里得几何、几何图形、度量属性、拓扑等）和数感（心理数线上表示）类别进行表征分析与性能随训练进程的变化比较，评估是否与儿童发展阶段的表现相匹配。

Result: 发现ResNet-50在某些几何概念（欧几里得几何、几何图形、度量属性、拓扑）和数感（心理数线的出现）上呈现出与儿童发展相似的进展，但在手性图形、几何变换和对称图形等类别未见对齐。研究表明模型可作为探讨数学认知发展机制的工具，但需扩展到更多架构和更大基准以确认普遍性。

Conclusion: CV模型在训练过程中表现出与儿童数学认知发展某些方面相似的进展，表明模型的表征学习与人类认知在发展轨迹上具有部分对齐性。

Abstract: Mathematical thinking is a fundamental aspect of human cognition. Cognitive scientists have investigated the mechanisms that underlie our ability to thinking geometrically and numerically, to take two prominent examples, and developmental scientists have documented the trajectories of these abilities over the lifespan. Prior research has shown that computer vision (CV) models trained on the unrelated task of image classification nevertheless learn latent representations of geometric and numerical concepts similar to those of adults. Building on this demonstrated cognitive alignment, the current study investigates whether CV models also show developmental alignment: whether their performance improvements across training to match the developmental progressions observed in children. In a detailed case study of the ResNet-50 model, we show that this is the case. For the case of geometry and topology, we find developmental alignment for some classes of concepts (Euclidean Geometry, Geometrical Figures, Metric Properties, Topology) but not others (Chiral Figures, Geometric Transformations, Symmetrical Figures). For the case of number, we find developmental alignment in the emergence of a human-like ``mental number line'' representation with experience. These findings show the promise of computer vision models for understanding the development of mathematical understanding in humans. They point the way to future research exploring additional model architectures and building larger benchmarks.

</details>


### [22] [UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space](https://arxiv.org/abs/2511.15046)
*Panqi Yang,Haodong Jing,Nanning Zheng,Yongqiang Ma*

Main category: cs.CV

TL;DR: UniHOI通过统一token空间与对称注意力及半监督学习，将HOI检测与生成联合建模，显著提升长尾检测与开放词汇生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOI研究将检测与生成任务分离，限制了交互理解的全面性与效率；通过联合建模可实现信息互通与样本利用最大化。

Method: 引入对称的交互感知注意力模块以及统一的半监督学习范式，在统一的token空间中实现图像与交互语义间的双向映射。

Result: 在大量实验中，UniHOI在HOI检测与生成任务上均取得SOTA：长尾HOI检测提升4.9%，开放词汇生成的交互指标提升42.0%。

Conclusion: 该文提出了一个统一的HOI（人-物交互）模型UniHOI，通过统一的token表示同时处理检测与生成任务，从而促进知识共享并提升泛化能力。

Abstract: In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.

</details>


### [23] [Hyperspectral Super-Resolution with Inter-Image Variability via Degradation-based Low-Rank and Residual Fusion Method](https://arxiv.org/abs/2511.15052)
*Yue Wen,Kunjing Yang,Minru Bai*

Main category: cs.CV

TL;DR: DLRRF通过将光谱变异建模为退化算子变化并把HSI分解为低秩+残差，结合外部去噪器的隐式正则化，在PAO框架下稳健求解，有效提升含影像间变异情形下的HSI-MSI融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有HSI与MSI融合方法面对影像间的光谱与局部空间变化时通常直接对影像做变换，加剧了模型病态性，导致融合性能下降。为此需要一种能在模型层面处理光谱变异并通过分解恢复丢失细节的稳健方法。

Method: 构建退化模型：将光谱变异建模为光谱退化算子的变化；将目标高光谱图像分解为低秩分量与残差分量（残差用于捕捉局部空间变化导致的细节损失）；对两个分量在低维子空间上进行降维以利用光谱相关性；引入隐式正则项并在Plug-and-Play框架中用外部去噪器处理该子问题；采用近端交替优化（PAO）算法求解并给出收敛性分析。

Result: 在大量数值实验中，DLRRF在存在影像间变异的情况下表现优于现有方法，能够更好地恢复空间细节并保持光谱一致性，同时算法在P n P-PAO框架下具有理论收敛保证。

Conclusion: 该论文提出的DLRRF模型通过把光谱差异建模为光谱退化算子变化，并将目标HSI分解为低秩与残差分量，有效应对了HSI与MSI之间的影像间变异，从而改善融合结果的空间细节恢复与光谱保真。

Abstract: The fusion of hyperspectral image (HSI) with multispectral image (MSI) provides an effective way to enhance the spatial resolution of HSI. However, due to different acquisition conditions, there may exist spectral variability and spatially localized changes between HSI and MSI, referred to as inter-image variability, which can significantly affect the fusion performance. Existing methods typically handle inter-image variability by applying direct transformations to the images themselves, which can exacerbate the ill-posedness of the fusion model. To address this challenge, we propose a Degradation-based Low-Rank and Residual Fusion (DLRRF) model. First, we model the spectral variability as change in the spectral degradation operator. Second, to recover the lost spatial details caused by spatially localized changes, we decompose the target HSI into low rank and residual components, where the latter is used to capture the lost details. By exploiting the spectral correlation within the images, we perform dimensionality reduction on both components. Additionally, we introduce an implicit regularizer to utilize the spatial prior information from the images. The proposed DLRRF model is solved using the Proximal Alternating Optimization (PAO) algorithm within a Plug-and-Play (PnP) framework, where the subproblem regarding implicit regularizer is addressed by an external denoiser. We further provide a comprehensive convergence analysis of the algorithm. Finally, extensive numerical experiments demonstrate that DLRRF achieves superior performance in fusing HSI and MSI with inter-image variability.

</details>


### [24] [CellGenNet: A Knowledge-Distilled Framework for Robust Cell Segmentation in Cancer Tissues](https://arxiv.org/abs/2511.15054)
*Srijan Ray,Bikesh K. Nirala,Jason T. Yustein,Sundaresh Ram*

Main category: cs.CV

TL;DR: 提出基于教师-学生软标签与BCE+Tversky混合损失的CellGenNet，在有限标注下提升WSI跨组织细胞核分割的准确性与泛化。


<details>
  <summary>Details</summary>
Motivation: WSI中染色、成像与组织形态多样性导致核分割困难，且全标注耗时昂贵，需在有限监督下提升跨组织泛化。

Method: 使用容量较大的教师网络在稀疏标注上训练并生成软伪标签；学生网络以联合目标优化，融合真实标签、教师概率标签及混合损失（BCE+Tversky），并辅以相容性正则化与逐层dropout。

Result: 在多类癌组织WSI上的实验表明，CellGenNet较纯监督和半监督基线在分割精度与泛化性上均有提升，尤其在少数类核结构的保持上效果明显。

Conclusion: CellGenNet通过教师-学生蒸馏和混合损失在有限标注条件下提高了跨组织核分割的鲁棒性与泛化能力。

Abstract: Accurate nuclei segmentation in microscopy whole slide images (WSIs) remains challenging due to variability in staining, imaging conditions, and tissue morphology. We propose CellGenNet, a knowledge distillation framework for robust cross-tissue cell segmentation under limited supervision. CellGenNet adopts a student-teacher architecture, where a capacity teacher is trained on sparse annotations and generates soft pseudo-labels for unlabeled regions. The student is optimized using a joint objective that integrates ground-truth labels, teacher-derived probabilistic targets, and a hybrid loss function combining binary cross-entropy and Tversky loss, enabling asymmetric penalties to mitigate class imbalance and better preserve minority nuclear structures. Consistency regularization and layerwise dropout further stabilize feature representations and promote reliable feature transfer. Experiments across diverse cancer tissue WSIs show that CellGenNet improves segmentation accuracy and generalization over supervised and semi-supervised baselines, supporting scalable and reproducible histopathology analysis.

</details>


### [25] [ProPL: Universal Semi-Supervised Ultrasound Image Segmentation via Prompt-Guided Pseudo-Labeling](https://arxiv.org/abs/2511.15057)
*Yaxiong Chen,Qicong Wang,Chunlei Li,Jingliang Hu,Yilei Shi,Shengwu Xiong,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: 提出ProPL，一种可处理多器官多任务的通用半监督超声分割框架，核心为共享编码器、提示引导双解码器及不确定性伪标签校准；在新建立的数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 目前超声图像分割方法多为针对特定结构或任务定制，缺乏通用性，限制了临床应用；因此提出通用半监督分割以提升实用性并利用未标注数据。

Method: 使用共享视觉编码器和提示引导的双解码器（在解码时引入task prompt以实现任务适配），结合不确定性驱动的伪标签校准（UPLC）进行可靠的自训练；框架在有标签与无标签数据上联合训练。

Result: 在包含5个器官、8个任务的新数据集上进行大量实验，ProPL在不同指标上均超越现有最优方法，证明了方法的有效性并树立了新的基准。

Conclusion: ProPL通过共享编码器、提示引导的双解码器和不确定性伪标签校准模块，实现了对多器官、多任务的通用半监督超声图像分割；在构建的包含5个器官和8个分割任务的数据集上，ProPL在多项指标上超越了现有最先进方法，成为新的基准。

Abstract: Existing approaches for the problem of ultrasound image segmentation, whether supervised or semi-supervised, are typically specialized for specific anatomical structures or tasks, limiting their practical utility in clinical settings. In this paper, we pioneer the task of universal semi-supervised ultrasound image segmentation and propose ProPL, a framework that can handle multiple organs and segmentation tasks while leveraging both labeled and unlabeled data. At its core, ProPL employs a shared vision encoder coupled with prompt-guided dual decoders, enabling flexible task adaptation through a prompting-upon-decoding mechanism and reliable self-training via an uncertainty-driven pseudo-label calibration (UPLC) module. To facilitate research in this direction, we introduce a comprehensive ultrasound dataset spanning 5 organs and 8 segmentation tasks. Extensive experiments demonstrate that ProPL outperforms state-of-the-art methods across various metrics, establishing a new benchmark for universal ultrasound image segmentation.

</details>


### [26] [Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://arxiv.org/abs/2511.15059)
*Keito Sasagawa,Shuhei Kurita,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 作者构建了包含水平与垂直书写日文的合成OCR数据集并收集真实垂直日文图像，发现现有MLLMs对垂直日文表现较差，但通过在合成数据上微调可显著改善识别能力，数据与代码已公开。


<details>
  <summary>Details</summary>
Motivation: 随着MLLMs在视觉文档理解的应用扩展，模型需处理不同语言与书写方向的文档。日语存在垂直书写现象，但针对垂直日文的研究与评测数据有限，故需要评估并提升现有模型对垂直书写日文的读取能力。

Method: 作者构建了一个合成日文OCR数据集，通过将日文文本渲染为图像（包含水平与垂直书写），并用于模型微调与评估；同时收集真实文档图像中垂直书写的日文作为评测集。对比了微调前后及水平/垂直文本上的模型性能差异。

Result: 实验表明：1) 未经针对性训练的MLLMs在垂直日文识别上性能下降；2) 使用合成数据集微调可明显提高垂直日文的识别率；3) 合成数据能帮助模型从无法处理垂直文本转为可处理。数据集与代码已开源。

Conclusion: 本文结论为：现有多模态大模型（MLLMs）在垂直书写的日文文本识别上表现明显逊于水平书写，通过使用合成的日文OCR数据集对模型进行微调，可显著提升对垂直书写日文的识别能力，部分原本不支持垂直书写的模型也能获益。

Abstract: Multimodal Large Language Models (MLLMs) have seen rapid advances in recent years and are now being applied to visual document understanding tasks. They are expected to process a wide range of document images across languages, including Japanese. Understanding documents from images requires models to read what are written in them. Since some Japanese documents are written vertically, support for vertical writing is essential. However, research specifically focused on vertically written Japanese text remains limited. In this study, we evaluate the reading capability of existing MLLMs on vertically written Japanese text. First, we generate a synthetic Japanese OCR dataset by rendering Japanese texts into images, and use it for both model fine-tuning and evaluation. This dataset includes Japanese text in both horizontal and vertical writing. We also create an evaluation dataset sourced from the real-world document images containing vertically written Japanese text. Using these datasets, we demonstrate that the existing MLLMs perform worse on vertically written Japanese text than on horizontally written Japanese text. Furthermore, we show that training MLLMs on our synthesized Japanese OCR dataset results in improving the performance of models that previously could not handle vertical writing. The datasets and code are publicly available https://github.com/llm-jp/eval_vertical_ja.

</details>


### [27] [Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks](https://arxiv.org/abs/2511.15065)
*Cheng Yang,Haiyuan Wan,Yiran Peng,Xin Cheng,Zhaoyang Yu,Jiayi Zhang,Junchi Yu,Xinlei Yu,Xiawu Zheng,Dongzhan Zhou,Chenglin Wu*

Main category: cs.CV

TL;DR: 作者提出VR-Bench并展示视频生成模型可用于空间多步推理，SFT与测试时多样化采样提升性能，表明该范式具备实用性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有视频模型在高保真视频生成与运动一致性方面成功，而视频本身具备显式的空间布局与时间连续性，适合作为空间推理的载体。论文探讨视频生成能否被用来执行推理任务。

Method: 构建VR-Bench基准：基于程序生成的5类迷宫与多样视觉风格，包含7,920个视频样本；使用SFT（监督微调）训练视频生成模型，并在测试时进行多样化采样以放大推理可靠性。

Result: 实验证明：1) SFT能有效激发视频模型的推理能力；2) 视频模型在空间感知与多步规划上优于领先视觉语言模型，并能跨场景、任务与复杂度泛化；3) 测试时多样化采样可使推理可靠性提升约10--20%。

Conclusion: 该论文提出并验证了“通过视频生成进行推理”的范式，证明视频模型在空间推理任务（迷宫求解）上表现优越且可扩展。

Abstract: Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.

</details>


### [28] [BokehFlow: Depth-Free Controllable Bokeh Rendering via Flow Matching](https://arxiv.org/abs/2511.15066)
*Yachuan Huang,Xianrui Luo,Qiwen Wang,Liao Shen,Jiaqi Li,Huiqiang Sun,Zihao Huang,Wei Jiang,Zhiguo Cao*

Main category: cs.CV

TL;DR: BokehFlow：无深度、基于流匹配并通过跨注意力实现文本可控的高质量、高效散景渲染方法，作者提供四个数据集并在多项实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖精确深度图（经典和神经可控方法），要么在生成式方法中存在可控性和效率不足的问题；因此需要一个无需深度输入且可控性强、效率高的散景渲染方法。

Method: BokehFlow采用流匹配模型作为生成骨干，并引入跨注意力机制以通过文本提示实现对焦区域和模糊强度的语义可控。作者收集并合成四个数据集用于训练和评估。

Result: 实验表明BokehFlow在视觉质量和可控性方面优于现有依赖深度的方法和其他生成式方法，同时在效率上也有优势。

Conclusion: 本文提出BokehFlow，一种基于流匹配（flow matching）的无深度可控散景渲染框架，能够从全对焦图像直接生成逼真的散景，无需深度输入。

Abstract: Bokeh rendering simulates the shallow depth-of-field effect in photography, enhancing visual aesthetics and guiding viewer attention to regions of interest. Although recent approaches perform well, rendering controllable bokeh without additional depth inputs remains a significant challenge. Existing classical and neural controllable methods rely on accurate depth maps, while generative approaches often struggle with limited controllability and efficiency. In this paper, we propose BokehFlow, a depth-free framework for controllable bokeh rendering based on flow matching. BokehFlow directly synthesizes photorealistic bokeh effects from all-in-focus images, eliminating the need for depth inputs. It employs a cross-attention mechanism to enable semantic control over both focus regions and blur intensity via text prompts. To support training and evaluation, we collect and synthesize four datasets. Extensive experiments demonstrate that BokehFlow achieves visually compelling bokeh effects and offers precise control, outperforming existing depth-dependent and generative methods in both rendering quality and efficiency.

</details>


### [29] [MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation](https://arxiv.org/abs/2511.15077)
*Shengjing Tian,Yinan Han,Xiantong Zhao,Xuehu Liu,Qi Lang*

Main category: cs.CV

TL;DR: MambaTrack3D通过MIP和GFEM在保证近线性复杂度的同时减少时序冗余并利用几何先验，显著提升HTV场景下的3D单目标点云跟踪性能，同时在常规数据集上表现稳健。


<details>
  <summary>Details</summary>
Motivation: 动态户外环境中的高时间变化（HTV）导致传统记忆型点云跟踪器在计算复杂度、时序冗余和几何先验利用上存在不足，亟需一种既高效又能利用历史空间关系的跟踪框架。

Method: 提出Mamba-based Inter-frame Propagation（MIP）替代传统单帧特征提取，实现跨帧高效传播并显式建模历史帧空间关系；引入Grouped Feature Enhancement Module（GFEM）按通道分离前后景语义以减小记忆库的时序冗余；在KITTI-HTV和nuScenes-HTV上进行大量对比实验。

Result: 在KITTI-HTV和nuScenes-HTV上，MambaTrack3D在中等时间间隔下相比HVTrack最高提升6.5的success和9.5的precision；在标准KITTI上与最先进的常规场景跟踪器相比仍具高竞争力，体现良好泛化性和准确率-效率平衡。

Conclusion: MambaTrack3D通过引入基于状态空间模型Mamba的跨帧传播模块（MIP）和按通道分离前后景语义的分组特征增强模块（GFEM），在保持近线性复杂度的同时，有效利用历史帧的空间关系与几何先验，减少了时序冗余，提升了HTV场景下的跟踪精度与效率。

Abstract: Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.

</details>


### [30] [TiCAL:Typicality-Based Consistency-Aware Learning for Multimodal Emotion Recognition](https://arxiv.org/abs/2511.15085)
*Wen Yin,Siyu Zhan,Cencen Liu,Xin Hu,Guiduo Duan,Xiurui Xie,Yuan-Fang Li,Tao He*

Main category: cs.CV

TL;DR: TiCAL通过典型性估计与伪单模标签评估样本一致性，并在双曲空间中学习情绪表示，将一致性信息融入训练以有效缓解模态情绪冲突，显著提升多模态情绪识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法以统一标签监督忽略了同一样本中不同模态可能传达不同情绪的模态冲突问题，导致模型在不一致样本上性能下降。

Method: TiCAL首先生成伪单模情绪标签并估计每个样本的典型性以衡量其模态一致性，然后在双曲空间中嵌入情绪特征以捕捉细粒度类别差异，将一致性权重融入损失函数以强调一致性高或需关注的样本。

Result: 在CMU-MOSEI和MER2023等基准数据集上，TiCAL较SOTA方法（如DMD）取得约2.6%的总体性能提升，尤其在模态不一致样本上提升更明显。

Conclusion: 提出TiCAL框架，通过典型性估计与伪单模标签动态评估样本一致性，并将一致性融入训练以缓解模态间情绪冲突，从而提升多模态情绪识别性能。

Abstract: Multimodal Emotion Recognition (MER) aims to accurately identify human emotional states by integrating heterogeneous modalities such as visual, auditory, and textual data. Existing approaches predominantly rely on unified emotion labels to supervise model training, often overlooking a critical challenge: inter-modal emotion conflicts, wherein different modalities within the same sample may express divergent emotional tendencies. In this work, we address this overlooked issue by proposing a novel framework, Typicality-based Consistent-aware Multimodal Emotion Recognition (TiCAL), inspired by the stage-wise nature of human emotion perception. TiCAL dynamically assesses the consistency of each training sample by leveraging pseudo unimodal emotion labels alongside a typicality estimation. To further enhance emotion representation, we embed features in a hyperbolic space, enabling the capture of fine-grained distinctions among emotional categories. By incorporating consistency estimates into the learning process, our method improves model performance, particularly on samples exhibiting high modality inconsistency. Extensive experiments on benchmark datasets, e.g, CMU-MOSEI and MER2023, validate the effectiveness of TiCAL in mitigating inter-modal emotional conflicts and enhancing overall recognition accuracy, e.g., with about 2.6% improvements over the state-of-the-art DMD.

</details>


### [31] [Jointly Conditioned Diffusion Model for Multi-View Pose-Guided Person Image Synthesis](https://arxiv.org/abs/2511.15092)
*Chengyu Xie,Zhi Gong,Junchi Ren,Linkun Yu,Si Shen,Fei Shen,Xiaoyu Du*

Main category: cs.CV

TL;DR: JCDM通过APM与JCI在扩散框架内融合多视图先验，实现高质量且跨视图一致的人像生成，兼容标准扩散主干并支持可变参考数。


<details>
  <summary>Details</summary>
Motivation: 单视图参考导致纹理缺失且缺乏显式跨视图交互，影响姿态引导下的人像生成一致性和保真度。需利用多视图先验来提升跨姿态的纹理和身份一致性。

Method: 提出外观先验模块（APM）用于从不完整参考视图推断整体身份保持先验；提出联合条件注入（JCI）机制，将多视图线索融合并将共享条件注入到去噪主干网络，以对齐身份、颜色和纹理。同时支持可变数量参考视图并可无缝集成到标准扩散主干。

Result: 在多个实验中，JCDM在图像质量（保真度）和跨视图一致性上达到了SOTA水平，同时结构修改少、能处理不同数量参考视图。

Conclusion: JCDM通过在扩散模型中联合条件注入和外观先验模块，有效解决单视图纹理不完整和跨视图交互不足的问题，实现了更一致的人像生成。

Abstract: Pose-guided human image generation is limited by incomplete textures from single reference views and the absence of explicit cross-view interaction. We present jointly conditioned diffusion model (JCDM), a jointly conditioned diffusion framework that exploits multi-view priors. The appearance prior module (APM) infers a holistic identity preserving prior from incomplete references, and the joint conditional injection (JCI) mechanism fuses multi-view cues and injects shared conditioning into the denoising backbone to align identity, color, and texture across poses. JCDM supports a variable number of reference views and integrates with standard diffusion backbones with minimal and targeted architectural modifications. Experiments demonstrate state of the art fidelity and cross-view consistency.

</details>


### [32] [A Comprehensive Study on Visual Token Redundancy for Discrete Diffusion-based Multimodal Large Language Models](https://arxiv.org/abs/2511.15098)
*Duo Li,Zuhao Yang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: 本文系统研究了视觉token冗余在离散扩散多模态LLMs中的表现及其对推理效率的影响，指出冗余仅在从头训练的模型与长回答任务中出现，并比较了层跳过与不同剪枝策略，提出针对性加速方案以降低推理开销。


<details>
  <summary>Details</summary>
Motivation: 现有离散扩散多模态LLM在推理时由于每步去噪都进行全序列注意力计算导致开销大，尽管存在通用的优化方法（键值缓存、采样优化），但大多忽视了视觉模态的冗余特性，因此研究视觉token冗余及针对性的加速方法具有实际价值。

Method: 通过系统性实验研究视觉token冗余随dMLLM架构与任务类型的演变，评估视觉token剪枝对模型响应与效率的影响；比较不同加速策略（如键值缓存、有效采样、层跳过、不同剪枝时机）在AR-to-diffusion模型与from-scratch模型上的效果；分析信息损失可恢复性的时间依赖性。

Result: 发现视觉冗余仅在from-scratch模型和长答案任务中显著出现；视觉剪枝带来不可忽视的信息损失，且仅from-scratch模型能在后期去噪中逐步恢复；不同架构适合不同加速策略：AR-to-diffusion适合层跳过，加速效果好；from-scratch适合渐进或后期剪枝。

Conclusion: 论文结论是：视觉token冗余在从头训练（from-scratch）型离散扩散多模态大模型（dMLLMs）中、尤其在处理长回答任务时会出现；视觉token剪枝会带来显著信息损失，而只有从头训练的dMLLMs能够在后期去噪步骤中逐步恢复丢失信息；针对不同架构，应采用不同加速策略：对由自回归（AR）模型改造为扩散模型的dMLLMs，跳层（layer-skipping）更有效；而对从头训练的dMLLMs，渐进式或后期剪枝更有效。总体上，本文提出了基于模态特性的效率优化视角，提升了dMLLM在多模态理解任务中的实用性。

Abstract: Discrete diffusion-based multimodal large language models (dMLLMs) have emerged as a promising alternative to autoregressive MLLMs thanks to their advantages in parallel decoding and bidirectional context modeling, but most existing dMLLMs incur significant computational overhead during inference due to the full-sequence attention computation in each denoising step. Pioneer studies attempt to resolve this issue from a modality-agnostic perspective via key-value cache optimization or efficient sampling but most of them overlook modality-specific visual token redundancy. In this work, we conduct a comprehensive study on how visual token redundancy evolves with different dMLLM architectures and tasks and how visual token pruning affects dMLLM responses and efficiency. Specifically, our study reveals that visual redundancy emerges only in from-scratch dMLLMs while handling long-answer tasks. In addition, we validate that visual token pruning introduces non-negligible information loss in dMLLMs and only from-scratch dMLLMs can recover the lost information progressively during late denoising steps. Furthermore, our study shows that layer-skipping is promising for accelerating AR-to-diffusion dMLLMs, whereas progressive or late-step pruning is more effective for from-scratch dMLLMs. Overall, this work offers a new perspective on efficiency optimization for dMLLMs, greatly advancing their applicability across various multimodal understanding tasks.

</details>


### [33] [Gaussian Blending: Rethinking Alpha Blending in 3D Gaussian Splatting](https://arxiv.org/abs/2511.15102)
*Junseo Koo,Jinseo Jeong,Gunhee Kim*

Main category: cs.CV

TL;DR: 用高斯分布替换像素级标量alpha进行混合，解决3DGS在不同采样率下的模糊与阶梯伪影，兼容现有系统且无额外开销。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS在训练未见的采样率下出现(1)放大时侵蚀导致的模糊、(2)缩小时膨胀导致的阶梯伪影，推测源于传统alpha混合作为像素标量的固有限制。

Method: 引入Gaussian Blending：将alpha和透射率视为像素内的空间分布（高斯），在混合时考虑像素面积内相邻splat的alpha分布，从而让背景splat在缩放变化时更合理贡献，保持实时渲染和不增加内存，作为对现有3DGS框架的直接替换。

Result: 实验显示Gaussian Blending在未见和已见采样率下都能更好保留细节，定量与定性均优于现有新视图合成模型，同时保持实时性与零额外内存开销。

Conclusion: 本文提出将3D Gaussian Splatting中的传统标量alpha混合替换为空间可变的高斯混合，以解决在训练未见采样率下的模糊与阶梯伪影问题。

Abstract: The recent introduction of 3D Gaussian Splatting (3DGS) has significantly advanced novel view synthesis. Several studies have further improved the rendering quality of 3DGS, yet they still exhibit noticeable visual discrepancies when synthesizing views at sampling rates unseen during training. Specifically, they suffer from (i) erosion-induced blurring artifacts when zooming in and (ii) dilation-induced staircase artifacts when zooming out. We speculate that these artifacts arise from the fundamental limitation of the alpha blending adopted in 3DGS methods. Instead of the conventional alpha blending that computes alpha and transmittance as scalar quantities over a pixel, we propose to replace it with our novel Gaussian Blending that treats alpha and transmittance as spatially varying distributions. Thus, transmittances can be updated considering the spatial distribution of alpha values across the pixel area, allowing nearby background splats to contribute to the final rendering. Our Gaussian Blending maintains real-time rendering speed and requires no additional memory cost, while being easily integrated as a drop-in replacement into existing 3DGS-based or other NVS frameworks. Extensive experiments demonstrate that Gaussian Blending effectively captures fine details at various sampling rates unseen during training, consistently outperforming existing novel view synthesis models across both unseen and seen sampling rates.

</details>


### [34] [An Event-triggered System for Social Persuasion and Danger Alert in Elder Home Monitoring](https://arxiv.org/abs/2511.15117)
*Jun-Yi Liu,Chung-Hao Chen,Ya-Chi Tsao,Ssu-Yao Wu,Yu-Ting Tsao,Lyn Chao-ling Chen*

Main category: cs.CV

TL;DR: 提出一套基于GMM与SVM的居家老年人事件触发监护系统，涵盖看护、危险提醒和照片联动，5户家庭场景验证并配合直观社交媒体交互设计，但缺乏详细定量评估。


<details>
  <summary>Details</summary>
Motivation: 动机是提高居家老年人的安全监护与亲属沟通效率，考虑老年人技术接受度低，设计无需复杂操作的直观互动方式，并通过自动事件检测减轻监护人员负担。

Method: 方法上，系统采用GMM（高斯混合模型）进行背景建模以检测运动行为，结合事件触发逻辑区分访客（watch dog）和老年人危险（danger notice）事件；对捕获的图像使用SVM进行分类分析；在交互方面设计了模拟正常生活活动的直观操作界面实现照片联动与社交媒体通知。

Result: 在5户家庭的场景实验中，系统能够检测并记录三类事件；SVM对图像的分析被用于辅助判别；并实现了通过直观操作将信息与亲属在社交媒体上联通。文章未给出详细的定量性能指标（如检测精度、误报率、召回率）和系统部署细节。

Conclusion: 本文提出了一个面向居家老年人监护的事件触发系统，结合物理与心理状态监测，实现了看护、危险提醒和照片联动三类事件检测与处理。系统在5个家庭的真实场景中验证，结合GMM背景建模检测运动行为，SVM分析捕获图像，并设计了符合老年人使用习惯的直观交互以便通过社交媒体与亲属沟通。

Abstract: In the study, the physical state and mental state of elders are both considered, and an event-triggered system has developed to detect events: watch dog, danger notice and photo link. By adopting GMM background modeling, the motion behavior of visitors and elders can be detected in the watch dog event and danger notice event respectively. Experiments set in home scenarios and 5 families participated in the experiments for detecting and recording three types of events from their life activities. In addition, the captured images were analyzed using SVM machine learning. For lack of technical experiences of elders, an intuitive operation as normal life activity was designed to create communication between elder and relatives via social media.

</details>


### [35] [Unbiased Semantic Decoding with Vision Foundation Models for Few-shot Segmentation](https://arxiv.org/abs/2511.15118)
*Jin Wang,Bingfeng Zhang,Jian Pang,Weifeng Liu,Baodi Liu,Honglong Chen*

Main category: cs.CV

TL;DR: 提出USD策略把CLIP语义与SAM结合：图像级全局补充与像素级局部引导增强SAM特征，另用可学习的视觉-文本提示生成器生成目标提示，无需重训基础模型，提升少样本分割对未知类别的无偏性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有将SAM用于少样本分割的方法过度依赖从支持集提取的显式提示，无法充分激活SAM的泛化能力并易造成对未知类的解码偏差；因此希望通过同时利用查询信息及CLIP语义引导，实现更无偏且语义敏感的解码。

Method: 设计两种特征增强策略：1) 图像级全局补充（提供支持图像的通用类别指示）；2) 像素级局部引导（提供查询图像的目标位置信息）；并提出可学习的视觉-文本目标提示生成器，通过交互目标文本嵌入与CLIP视觉特征生成富含目标信息的提示嵌入，整个方法无需重训基础视觉模型。

Result: 方法在保持基础模型不重训的前提下，通过语义增强的特征与目标提示，可使SAM对目标区域关注更精确、对未知类别适应性更好。论文宣称在少样本分割场景（尤其跨类适配）中提升了预测一致性与分割性能（具体数值需看实验章）。

Conclusion: 本文提出将SAM与CLIP联合的无偏语义解码（USD）策略，通过同时从支持集和查询集中提取目标信息，利用CLIP的语义对齐能力增强SAM特征，从而减少对显式提示的依赖并提升对未知类别的泛化与不偏性。

Abstract: Few-shot segmentation has garnered significant attention. Many recent approaches attempt to introduce the Segment Anything Model (SAM) to handle this task. With the strong generalization ability and rich object-specific extraction ability of the SAM model, such a solution shows great potential in few-shot segmentation. However, the decoding process of SAM highly relies on accurate and explicit prompts, making previous approaches mainly focus on extracting prompts from the support set, which is insufficient to activate the generalization ability of SAM, and this design is easy to result in a biased decoding process when adapting to the unknown classes. In this work, we propose an Unbiased Semantic Decoding (USD) strategy integrated with SAM, which extracts target information from both the support and query set simultaneously to perform consistent predictions guided by the semantics of the Contrastive Language-Image Pre-training (CLIP) model. Specifically, to enhance the unbiased semantic discrimination of SAM, we design two feature enhancement strategies that leverage the semantic alignment capability of CLIP to enrich the original SAM features, mainly including a global supplement at the image level to provide a generalize category indicate with support image and a local guidance at the pixel level to provide a useful target location with query image. Besides, to generate target-focused prompt embeddings, a learnable visual-text target prompt generator is proposed by interacting target text embeddings and clip visual features. Without requiring re-training of the vision foundation models, the features with semantic discrimination draw attention to the target region through the guidance of prompt with rich target information.

</details>


### [36] [WaveFuse-AL: Cyclical and Performance-Adaptive Multi-Strategy Active Learning for Medical Images](https://arxiv.org/abs/2511.15132)
*Nishchala Thakur,Swati Kochhar,Deepti R. Bathula,Sukrit Gupta*

Main category: cs.CV

TL;DR: WaveFuse-AL结合周期性权重与性能自适应融合多策略采样，稳定提升医疗影像主动学习效果。


<details>
  <summary>Details</summary>
Motivation: 单一采样策略在主动学习循环的不同阶段表现不稳定，交替策略虽有改进但缺乏对何时使用何种策略的动态调整机制；因此设计一种能随训练阶段与模型表现自动切换或融合多策略的方法以提高标注效益。

Method: 提出WaveFuse-AL框架：使用四种已知采集策略（BALD、BADGE、Entropy、CoreSet），对每个策略维护时间相关的正弦周期性权重（周期与相位可设置）并结合基于验证集表现的自适应调整项；最终策略权重为周期项与性能项的加权和，按权重混合各策略得分来选择样本；在三个医疗影像基准上进行批次式主动学习评估。

Result: 在APTOS-2019、RSNA Pneumonia Detection和ISIC-2018三个数据集上的实验显示，WaveFuse-AL在10/12次评估指标上显著优于单一策略和交替策略基线，在有限注释预算下提高了性能（例如分类准确率/AUC和分割指标）。

Conclusion: WaveFuse-AL通过在主动学习过程中自适应融合多种采样策略，并结合周期性时间先验与基于性能的权重调整，实现了对不同阶段数据需求的动态平衡，从而提升了在医疗影像任务上的标注效率与最终模型性能。

Abstract: Active learning reduces annotation costs in medical imaging by strategically selecting the most informative samples for labeling. However, individual acquisition strategies often exhibit inconsistent behavior across different stages of the active learning cycle. We propose Cyclical and Performance-Adaptive Multi-Strategy Active Learning (WaveFuse-AL), a novel framework that adaptively fuses multiple established acquisition strategies-BALD, BADGE, Entropy, and CoreSet throughout the learning process. WaveFuse-AL integrates cyclical (sinusoidal) temporal priors with performance-driven adaptation to dynamically adjust strategy importance over time. We evaluate WaveFuse-AL on three medical imaging benchmarks: APTOS-2019 (multi-class classification), RSNA Pneumonia Detection (binary classification), and ISIC-2018 (skin lesion segmentation). Experimental results demonstrate that WaveFuse-AL consistently outperforms both single-strategy and alternating-strategy baselines, achieving statistically significant performance improvements (on ten out of twelve metric measurements) while maximizing the utility of limited annotation budgets.

</details>


### [37] [DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging](https://arxiv.org/abs/2511.15151)
*Meihua Zhou,Xinyu Tong,Jiarui Zhao,Min Cheng,Li Yang,Lei Tian,Nan Wan*

Main category: cs.CV

TL;DR: 提出将ARP编码与动态课程学习相结合的DCL-SE，通过低维动态表征与逐步细化训练，实现对多任务神经影像临床分析的性能与可解释性提升。


<details>
  <summary>Details</summary>
Motivation: 动机为克服高维神经影像临床分析中时空分辨率与大规模通用模型可适应性不足的矛盾，设计紧凑且任务专用的表征与训练方案以提高性能与可解释性。

Method: 方法包括：1) 数据驱动时空编码(DaSE)，使用ARP将3D体数据映射为信息丰富的2D动态表示以降低计算与参数代价；2) 动态课程学习策略，依据样本/特征难度由粗到细训练；3) 动态组机制(DGM)用于在训练过程中调节任务组与注意力以促进从全局结构到局部病变的特征收敛；整体为端到端框架DCL-SE。

Result: 在6个公开数据集（阿尔茨海默病与脑肿瘤分类、脑动脉分割、脑龄预测等）上，DCL-SE在准确率、鲁棒性和可解释性上均优于现有方法，证明紧凑任务特定架构在大规模预训练网络时代的价值。

Conclusion: DCL-SE通过将三维脑影像用近似秩池化(ARP)编码为二维动态表征，并结合动态课程学习与动态组机制(DGM)逐步训练解码器，从宏观解剖到病灶细节进行特征提取，从而在多个任务和数据集上提升诊断准确性、鲁棒性与可解释性。

Abstract: High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.

</details>


### [38] [SceneEdited: A City-Scale Benchmark for 3D HD Map Updating via Image-Guided Change Detection](https://arxiv.org/abs/2511.15153)
*Chun-Jung Lin,Tat-Jun Chin,Sourav Garg,Feras Dayoub*

Main category: cs.CV

TL;DR: SceneEdited是首个支持城市规模3D点云地图更新的数据集，包含多传感器数据与合成变化，附带基线方法与工具，促进HD地图维护研究。


<details>
  <summary>Details</summary>
Motivation: 现有方法多集中于2D变化检测，但无法直接用于更新3D高精地图；需要一个专门的数据集和工具链来研究点云级别的地图更新问题。

Method: 构建包含RGB图像、LiDAR点云和变化掩码的城市级数据集（800+场景，73 km驾驶、约3 km^2）、手动与自动合成2.3万+对象变化，提供基线基于图像的SfM更新流程和扩展工具包。

Result: 发布SceneEdited数据集与工具包，包含2000+过时场景版本与注释，基线实验展示了基于图像SfM的可行性，并建立了统一的评估基准。

Conclusion: SceneEdited填补了从2D图像变化检测到3D点云地图更新的空白，提供了大规模、真实感的合成变化场景，推动HD地图维护研究。

Abstract: Accurate, up-to-date High-Definition (HD) maps are critical for urban planning, infrastructure monitoring, and autonomous navigation. However, these maps quickly become outdated as environments evolve, creating a need for robust methods that not only detect changes but also incorporate them into updated 3D representations. While change detection techniques have advanced significantly, there remains a clear gap between detecting changes and actually updating 3D maps, particularly when relying on 2D image-based change detection. To address this gap, we introduce SceneEdited, the first city-scale dataset explicitly designed to support research on HD map maintenance through 3D point cloud updating. SceneEdited contains over 800 up-to-date scenes covering 73 km of driving and approximate 3 $\text{km}^2$ of urban area, with more than 23,000 synthesized object changes created both manually and automatically across 2000+ out-of-date versions, simulating realistic urban modifications such as missing roadside infrastructure, buildings, overpasses, and utility poles. Each scene includes calibrated RGB images, LiDAR scans, and detailed change masks for training and evaluation. We also provide baseline methods using a foundational image-based structure-from-motion pipeline for updating outdated scenes, as well as a comprehensive toolkit supporting scalability, trackability, and portability for future dataset expansion and unification of out-of-date object annotations. Both the dataset and the toolkit are publicly available at https://github.com/ChadLin9596/ScenePoint-ETK, establising a standardized benchmark for 3D map updating research.

</details>


### [39] [Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation](https://arxiv.org/abs/2511.15159)
*Firdavs Nasriddinov,Rafal Kocielnik,Anima Anandkumar,Andrew J. Hung*

Main category: cs.CV

TL;DR: 通过学习并利用IAT三元组结构，该工作提高了视频到动作识别和基于语言模型的培训反馈的临床相关性与质量，提升了可审计性并在多个指标上显著优于不使用结构化表征的基线。


<details>
  <summary>Details</summary>
Motivation: 动机是实现可扩展、及时且一致的手术培训反馈，自动化“培训师式”自然反馈需要模型理解临床相关表示，因而引入结构化IAT表征以提高生成的临床可证性与可审计性。

Method: 方法包括（1）从真实的训练文本中抽取Instrument-Action-Target (IAT)三元组并进行规范化聚类，（2）训练一个视频到IAT的模型，融合手术过程、任务上下文和细粒度仪器运动的时间跟踪信息，（3）使用IAT三元组作为条件，指导GPT-4o生成临床相关且风格匹配的培训者式反馈。

Result: 在视频到IAT识别任务上，注入上下文和时间跟踪后AUC有显著提升（Instrument: 0.67->0.74；Action: 0.60->0.63；Tissue: 0.74->0.79）。在反馈生成任务上，GPT-4o单纯从视频生成得分2.17，加入IAT条件后得分提升到2.44（+12.4%），可接受（>=3分）的比例从21%翻倍至42%。此外，词错误率下降15-31%，ROUGE增加9-64%。

Conclusion: 该论文提出了一个基于结构感知的管线，通过从真实培训对话中学习手术动作本体（IAT三元组），并将其用于指导反馈生成，从而在视频到IAT识别和基于IAT的反馈生成上均取得改进。

Abstract: High-quality intraoperative feedback from a surgical trainer is pivotal for improving trainee performance and long-term skill acquisition. Automating natural, trainer-style feedback promises timely, accessible, and consistent guidance at scale but requires models that understand clinically relevant representations. We present a structure-aware pipeline that learns a surgical action ontology from real trainer-to-trainee transcripts (33 surgeries) and uses it to condition feedback generation. We contribute by (1) mining Instrument-Action-Target (IAT) triplets from real-world feedback text and clustering surface forms into normalized categories, (2) fine-tuning a video-to-IAT model that leverages the surgical procedure and task contexts as well as fine-grained temporal instrument motion, and (3) demonstrating how to effectively use IAT triplet representations to guide GPT-4o in generating clinically grounded, trainer-style feedback. We show that, on Task 1: Video-to-IAT recognition, our context injection and temporal tracking deliver consistent AUC gains (Instrument: 0.67 to 0.74; Action: 0.60 to 0.63; Tissue: 0.74 to 0.79). For Task 2: feedback text generation (rated on a 1-5 fidelity rubric where 1 = opposite/unsafe, 3 = admissible, and 5 = perfect match to a human trainer), GPT-4o from video alone scores 2.17, while IAT conditioning reaches 2.44 (+12.4%), doubling the share of admissible generations with score >= 3 from 21% to 42%. Traditional text-similarity metrics also improve: word error rate decreases by 15-31% and ROUGE (phrase/substring overlap) increases by 9-64%. Grounding generation in explicit IAT structure improves fidelity and yields clinician-verifiable rationales, supporting auditable use in surgical training.

</details>


### [40] [Multimodal Continual Instruction Tuning with Dynamic Gradient Guidance](https://arxiv.org/abs/2511.15164)
*Songze Li,Mingyu Gao,Tonghua Su,Xu-Yao Zhang,Zhongjie Wang*

Main category: cs.CV

TL;DR: 把灾难性遗忘看作旧任务梯度缺失，使用当前与历史最优参数方向向量近似缺失梯度，结合有限重放与伯努利采样，在不扩展模型下显著减轻遗忘并取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 多模态持续指令调优中，随着新任务学习，模型会遗忘之前学到的任务，作者将这一问题归结为旧任务梯度在新任务学习中缺失，并尝试通过近似这些缺失梯度来解决遗忘问题。

Method: 利用参数空间的几何结构，使用当前参数与之前最佳参数之间的方向向量作为梯度引导，并将该近似梯度与来自有限重放缓冲区的真实梯度结合，最后通过伯努利采样策略动态平衡稳定性与可塑性。

Result: 在多模态持续指令调优数据集上，方法在不增加模型参数的前提下实现了最先进的性能，有效缓解了灾难性遗忘并保持紧凑的架构。

Conclusion: 该方法通过将遗忘视为旧任务梯度缺失问题，并用当前参数与先前最优参数的方向向量来近似这些梯度，从而在不扩展模型的情况下缓解灾难性遗忘。

Abstract: Multimodal continual instruction tuning enables multimodal large language models to sequentially adapt to new tasks while building upon previously acquired knowledge. However, this continual learning paradigm faces the significant challenge of catastrophic forgetting, where learning new tasks leads to performance degradation on previous ones. In this paper, we introduce a novel insight into catastrophic forgetting by conceptualizing it as a problem of missing gradients from old tasks during new task learning. Our approach approximates these missing gradients by leveraging the geometric properties of the parameter space, specifically using the directional vector between current parameters and previously optimal parameters as gradient guidance. This approximated gradient can be further integrated with real gradients from a limited replay buffer and regulated by a Bernoulli sampling strategy that dynamically balances model stability and plasticity. Extensive experiments on multimodal continual instruction tuning datasets demonstrate that our method achieves state-of-the-art performance without model expansion, effectively mitigating catastrophic forgetting while maintaining a compact architecture.

</details>


### [41] [Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation](https://arxiv.org/abs/2511.15167)
*Jing Cao,Kui Jiang,Shenyi Li,Xiaocheng Feng,Yong Huang*

Main category: cs.CV

TL;DR: SEC-Depth利用训练中历史模型作为负样本构建自演化对比学习，通过动态更新延迟模型与SECL损失提升了自监督深度估计在雨雾等恶劣天气下的鲁棒性，并能无缝集成到多种基线中。


<details>
  <summary>Details</summary>
Motivation: 现有自监督深度估计在雨雾等恶劣天气下性能显著下降，能见度降低导致深度预测准确性下降，需一种无需人工标注且能自适应环境变化的方法提升鲁棒性。

Method: 设计了动态更新策略以保存不同训练阶段的延迟模型；使用这些历史模型输出作为负样本构建自演化对比损失，动态调整学习目标并隐式感知天气恶化程度；可无缝集成到不同基线模型中，支持零样本评估。

Result: 在多种基线模型上进行零样本评估，SEC-Depth显著提高了恶劣天气场景下的深度估计性能，减少了人工干预需求。

Conclusion: 该论文提出了SEC-Depth，通过利用训练中产生的中间参数构建时序演化的延迟模型，并引入自演化对比损失（SECL），将历史模型输出作为负样本，从而提升在雨雾等恶劣天气下的自监督深度估计鲁棒性。

Abstract: Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.

</details>


### [42] [MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction](https://arxiv.org/abs/2511.15179)
*Kyotaro Tokoro,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出MMCM：基于聚类的多模态与有效性评估指标，用数据定义有效模式，能更可靠地评价概率性人类运动预测的质量。


<details>
  <summary>Details</summary>
Motivation: 单一历史序列可以对应多种未来，概率性预测会输出多样结果，但现有评价方法只看分布广度，缺乏对多模态覆盖与运动学有效性的显式评价，导致对不合理或集中在单模态内的分布也会被高估。

Method: 对未来运动进行聚类以定义模式；用模式覆盖（预测样本落到多少不同簇）衡量多样性；用从数据集中收集的真实未来运动簇来判断哪些簇是“有效模式”，从而评估预测样本的有效性。

Result: 实验证明所学聚类能得到合理的模式划分，MMCM能更准确地对比并评分不同的多模态预测方法，区分分布广但无效与既多模态又有效的预测。

Conclusion: 本文提出的MMCM指标通过聚类将运动空间划分为多个模式，并基于模式覆盖率和运动有效性评估概率性人类运动预测的多样性与合理性，能够纠正现有指标偏好只看分布广度而忽视模式集中与运动合法性的缺陷。

Abstract: This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM

</details>


### [43] [Instruction-Guided Lesion Segmentation for Chest X-rays with Automatically Generated Large-Scale Dataset](https://arxiv.org/abs/2511.15186)
*Geon Choi,Hangyul Yoon,Hyunju Shin,Hyunki Park,Sang Hoon Seo,Eunho Yang,Edward Choi*

Main category: cs.CV

TL;DR: 提出ILS范式并发布MIMIC-ILS大规模数据集，训练的ROSALIA可用简单指令在胸片上高质量地定位并解释多类病灶，有利于像素级病灶定位研究与应用。


<details>
  <summary>Details</summary>
Motivation: 现有CXR分割方法受限于目标标签数量少且需要长篇专家级文本作为输入，限制了临床实用性；因此需一种基于简单用户指令、覆盖多种病灶类型的通用分割方案与大规模数据支持。

Method: 作者设计了一个全自动多模态流水线，从胸片及其报告自动生成像素级标注并配套多样化指令，构建MIMIC-ILS数据集；基于该数据集微调视觉-语言模型，得到ROSALIA，可根据用户指令输出分割掩码并生成文本解释。

Result: 构建了包含1.1M条指令-答案对的MIMIC-ILS（来自192K张图像、91K独特分割掩码，覆盖7类主要病灶）；ROSALIA在新任务上实现高分割和文本准确率，验证了流水线与数据集的有效性。

Conclusion: 本文提出了指令引导病灶分割（ILS）新范式，通过简化用户输入并构建大规模指令-答案数据集解决了胸片病灶分割模型标签少与依赖专家文本的问题。

Abstract: The applicability of current lesion segmentation models for chest X-rays (CXRs) has been limited both by a small number of target labels and the reliance on long, detailed expert-level text inputs, creating a barrier to practical use. To address these limitations, we introduce a new paradigm: instruction-guided lesion segmentation (ILS), which is designed to segment diverse lesion types based on simple, user-friendly instructions. Under this paradigm, we construct MIMIC-ILS, the first large-scale instruction-answer dataset for CXR lesion segmentation, using our fully automated multimodal pipeline that generates annotations from chest X-ray images and their corresponding reports. MIMIC-ILS contains 1.1M instruction-answer pairs derived from 192K images and 91K unique segmentation masks, covering seven major lesion types. To empirically demonstrate its utility, we introduce ROSALIA, a vision-language model fine-tuned on MIMIC-ILS. ROSALIA can segment diverse lesions and provide textual explanations in response to user instructions. The model achieves high segmentation and textual accuracy in our newly proposed task, highlighting the effectiveness of our pipeline and the value of MIMIC-ILS as a foundational resource for pixel-level CXR lesion grounding.

</details>


### [44] [BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI](https://arxiv.org/abs/2511.15188)
*Wasif Jalal,Md Nafiu Rahman,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 提出的BrainRotViT通过先训ViT再用残差CNN回归的混合策略，在大规模多中心MRI数据上实现高精度、可解释且泛化良好的脑龄估计，兼具Transformer的全局建模与CNN的局部信息提炼，揭示与多种神经精神疾病相关的衰老模式。


<details>
  <summary>Details</summary>
Motivation: 传统回归和CNN方法在手工特征、视野范围和异构数据上的泛化能力有限，而纯Transformer需大数据与高计算成本；因此设计混合模型以兼顾全局上下文建模与局部精化，提升性能与泛化。

Method: 先在切片级别用ViT编码器在辅助任务（年龄和性别分类）上进行训练并冻结；将该编码器应用于所有矢状切片以生成2D嵌入矩阵；随后用残差CNN对嵌入矩阵进行回归，最终全连接层融合受试者性别以估计连续脑龄。

Result: 在11个MRI数据集（>130采集点）验证集上达到MAE=3.34年，Pearson r=0.98，Spearman ρ=0.97，R^2=0.95；在4个独立队列上MAE在3.77–5.04年间；模型注意力图聚焦小脑蚓、中央回、颞叶与内侧上额回；脑龄差与阿尔茨海默、认知受损、自闭谱系相关。

Conclusion: 该文章提出的BrainRotViT是一种将ViT与残差CNN回归器结合的混合架构，能够在大规模、多中心MRI数据上实现高精度、可解释的脑龄估计，优于现有基线和最先进模型，并能捕获与疾病相关的衰老模式。

Abstract: Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.

</details>


### [45] [Insert In Style: A Zero-Shot Generative Framework for Harmonious Cross-Domain Object Composition](https://arxiv.org/abs/2511.15197)
*Raghu Vamsi Chittersu,Yuvraj Singh Rathore,Pranav Adlinge,Kunal Swami*

Main category: cs.CV

TL;DR: Insert In Style: first zero-shot, high-fidelity generative method for inserting real objects into stylized images, using disentangled multi-stage training and masked-attention; trained on a curated 100k dataset; achieves SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing reference-based composition methods either rely on practical blenders with low generative fidelity or generators that need per-subject finetuning. The motivation is to build a practical, high-fidelity, zero-shot solution.

Method: The method uses a multi-stage training protocol to disentangle identity, style, and composition, combined with a masked-attention architecture to enforce disentanglement during generation. Trained on a 100k curated dataset via large-scale generation and two-stage filtering, and evaluated on a new public benchmark.

Result: The model outperforms prior methods on identity and style metrics and is validated by user studies.

Conclusion: This paper presents Insert In Style, a zero-shot generative framework for inserting real-world objects into stylized domains, achieving high fidelity without per-subject finetuning.

Abstract: Reference-based object composition methods fail when inserting real-world objects into stylized domains. This under-explored problem is currently split between practical "blenders" that lack generative fidelity and "generators" that require impractical, per-subject online finetuning. In this work, we introduce Insert In Style, the first zero-shot generative framework that is both practical and high-fidelity. Our core contribution is a unified framework with two key innovations: (i) a novel multi-stage training protocol that disentangles representations for identity, style, and composition, and (ii) a specialized masked-attention architecture that surgically enforces this disentanglement during generation. This approach prevents the concept interference common in general-purpose, unified-attention models. Our framework is trained on a new 100k sample dataset, curated from a novel data pipeline. This pipeline couples large-scale generation with a rigorous, two-stage filtering process to ensure both high-fidelity semantic identity and style coherence. Unlike prior work, our model is truly zero-shot and requires no text prompts. We also introduce a new public benchmark for stylized composition. We demonstrate state-of-the-art performance, significantly outperforming existing methods on both identity and style metrics, a result strongly corroborated by user studies.

</details>


### [46] [Towards Unbiased Cross-Modal Representation Learning for Food Image-to-Recipe Retrieval](https://arxiv.org/abs/2511.15201)
*Qing Wang,Chong-Wah Ngo,Ee-Peng Lim*

Main category: cs.CV

TL;DR: 本文用因果推断把食材作为混杂变量，通过反门控调整和多标签食材分类器去偏，提高了食谱-图像跨模态检索性能并在Recipe1M上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法把食谱视为描述菜品外观的文本，导致模型过度依赖显著的视觉-文本对应关系而忽略食谱中未被图像完整反映的细节（如工艺、佐料、呈现方式等），从而在相似度判断中引入偏差。作者希望通过因果建模消除该偏差，提高检索相关性。

Method: 基于因果推断，将食材视为混杂变量，采用反门控（backdoor adjustment）进行因果干预，推导出在原始相似度基础上增加一个去偏项。提出一个可插拔的多标签食材分类器模块用于实现去偏与估计调整项，并将其集成到传统跨模态表示学习框架中进行训练与检索。

Result: 在Recipe1M数据集上实验表明，经过因果干预与食材分类器去偏后，检索效果显著提升，报告了新的最先进结果；且在1K、10K和50K测试规模下声称理论上可达到MedR=1的oracle性能。

Conclusion: 论文通过因果视角识别并校正食谱-图像跨模态检索中的偏差，提出以食材作为混杂变量并应用反门控调整避免相似度判断被主导视觉-文本对齐误导。实验证明在Recipe1M数据集上取得了新的SOTA性能，且理论上在不同测试规模下可达MedR=1。

Abstract: This paper addresses the challenges of learning representations for recipes and food images in the cross-modal retrieval problem. As the relationship between a recipe and its cooked dish is cause-and-effect, treating a recipe as a text source describing the visual appearance of a dish for learning representation, as the existing approaches, will create bias misleading image-and-recipe similarity judgment. Specifically, a food image may not equally capture every detail in a recipe, due to factors such as the cooking process, dish presentation, and image-capturing conditions. The current representation learning tends to capture dominant visual-text alignment while overlooking subtle variations that determine retrieval relevance. In this paper, we model such bias in cross-modal representation learning using causal theory. The causal view of this problem suggests ingredients as one of the confounder sources and a simple backdoor adjustment can alleviate the bias. By causal intervention, we reformulate the conventional model for food-to-recipe retrieval with an additional term to remove the potential bias in similarity judgment. Based on this theory-informed formulation, we empirically prove the oracle performance of retrieval on the Recipe1M dataset to be MedR=1 across the testing data sizes of 1K, 10K, and even 50K. We also propose a plug-and-play neural module, which is essentially a multi-label ingredient classifier for debiasing. New state-of-the-art search performances are reported on the Recipe1M dataset.

</details>


### [47] [Physics-Based Benchmarking Metrics for Multimodal Synthetic Images](https://arxiv.org/abs/2511.15204)
*Kishor Datta Gupta,Marufa Kamal,Md. Mahfuzur Rahman,Fahad Rahman,Mohd Ariful Haque,Sunzida Siddique*

Main category: cs.CV

TL;DR: PCMDE通过目标检测+VLM特征、置信度加权融合与基于大模型的物理约束推理，提出一种改进的多模态评估指标，旨在解决现有指标对语义/结构准确性检测不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有指标（BLEU等）擅长词级或全局相似度，却难以捕捉结构性和物理一致性；因此需要一种能结合视觉、知识与推理能力的评估方法，特别是在需要物理或领域约束的任务中。

Method: 体系结构由三部分组成：1) 通过目标检测与视觉语言模型提取空间与语义多模态特征；2) 采用置信度加权的组件融合机制，对各组件（如对象、关系、属性）进行自适应验证；3) 利用大语言模型进行物理/结构约束推理，校验对齐、位置与一致性等关系。

Result: 作者声称PCMDE在捕捉结构、关系与物理一致性方面优于传统指标，能更可靠地评估领域相关生成输出，但摘要未提供具体实验数据或定量提升。

Conclusion: 该论文提出了PCMDE，一种结合大语言模型、知识映射和视觉语言模型的多模态评估指标，旨在弥补现有评估方法在语义与结构准确性方面的不足，特别是领域/上下文依赖场景中。

Abstract: Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.

</details>


### [48] [SkinGPT-R1: Adapter-Only Dual Distillation for Efficient Dermatology Reasoning](https://arxiv.org/abs/2511.15242)
*Yuhao Shen,Jiahe Qian,Zhangtianyi Chen,Yuanhao He,Juexiao Zhou*

Main category: cs.CV

TL;DR: 通过DermCoT的CoT监督与皮肤科视觉蒸馏，SkinGPT-R1在叙事质量和分类性能上显著优于基线，提供可验证的逐步诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在医学诊断推理上缺乏显式、可验证的链式推理叙事，皮肤科诊断需要逐步、可审计的解读以提高信任与可用性。

Method: 构建DermCoT语料（10,000个筛选训练病例 + 3,000个皮肤科医师评分的认证病例），以CoT形式训练视觉-语言模型，并引入皮肤科感知的视觉蒸馏；定义DermEval（六维医师对齐评估器）与DermBench（对应基准）用于量化CoT质量。

Result: 在DermBench上，SkinGPT-R1在14个模型中六维平均得分4.031/5，位居第一，较Vision-R1平均提升约41%；在三项皮肤科分类基准上也呈现稳定精度提升；消融实验显示DermCoT监督和皮肤科视觉蒸馏均带来显著增益。

Conclusion: SkinGPT-R1通过在皮肤病学专用的链式思维（chain of thought, CoT）监督和视觉蒸馏上进行优化，显著提升了皮肤病图像诊断叙事质量与分类性能。

Abstract: We present SkinGPT-R1, a dermatology focused vision language model that makes diagnostic chain of thought reasoning explicit, step by step, and verifiable. To support skin specific reasoning, we build DermCoT, a corpus of standardized dermatologic chain of thought narratives that combines 10,000 DermEval filtered training cases with 3,000 dermatologist scored certified cases, and we define DermEval as a physician aligned six dimensional evaluator and DermBench as the corresponding benchmark for dermatologic chain of thought quality. On DermBench, across 14 general, reasoning, and medical vision language models, SkinGPT-R1 achieves an average score of 4.031 out of 5 over the six clinician defined dimensions, ranks 1st among all systems, and improves the average score over Vision-R1 by about 41%. On three dermatology classification benchmarks, SkinGPT-R1 delivers stable accuracy gains over Vision-R1 and remains competitive among strong vision language models. Ablation results further show that DermCoT based chain of thought supervision provides substantial improvements over the base model and that adding dermatology aware visual distillation yields consistent additional gains in both narrative quality and recognition.

</details>


### [49] [SplitFlux: Learning to Decouple Content and Style from a Single Image](https://arxiv.org/abs/2511.15258)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Yongjun Zhang,Ziyang Chen,Shuting He*

Main category: cs.CV

TL;DR: 提出 SplitFlux：对 Flux 的单 Dream Blocks 用两种定制 LoRA 方案（秩约束和视觉门控）进行微调，实现在新上下文中高保真内容重嵌和优质风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有 SDXL 方法难以兼顾高质量生成与内容保真，而 Flux 未充分利用其结构特性，导致内容-风格分离效果欠佳；因此需要一种能保持主体内容结构同时灵活迁移风格的方法。

Method: 基于对 Flux 的系统性分析，识别单 Dream Block 的重要性与早晚块分别负责内容与风格的规律；提出通过 LoRA 对单 Dream Blocks 做细粒度微调，包含秩约束适配（压缩秩并放大更新幅度）和视觉门控 LoRA（按显著性分支不同秩），以实现内容-风格 disentangle 与可重嵌入。

Result: 在大量实验中，SplitFlux 在内容保留与风格化质量上优于现有最先进方法，表现稳定且适用于多种场景。

Conclusion: SplitFlux 有效解决了 Flux 在内容-风格分离上的不足，通过针对单一 Dream Block 的分段微调实现高质量内容重嵌与风格迁移。

Abstract: Disentangling image content and style is essential for customized image generation. Existing SDXL-based methods struggle to achieve high-quality results, while the recently proposed Flux model fails to achieve effective content-style separation due to its underexplored characteristics. To address these challenges, we conduct a systematic analysis of Flux and make two key observations: (1) Single Dream Blocks are essential for image generation; and (2) Early single stream blocks mainly control content, whereas later blocks govern style. Based on these insights, we propose SplitFlux, which disentangles content and style by fine-tuning the single dream blocks via LoRA, enabling the disentangled content to be re-embedded into new contexts. It includes two key components: (1) Rank-Constrained Adaptation. To preserve content identity and structure, we compress the rank and amplify the magnitude of updates within specific blocks, preventing content leakage into style blocks. (2) Visual-Gated LoRA. We split the content LoRA into two branches with different ranks, guided by image saliency. The high-rank branch preserves primary subject information, while the low-rank branch encodes residual details, mitigating content overfitting and enabling seamless re-embedding. Extensive experiments demonstrate that SplitFlux consistently outperforms state-of-the-art methods, achieving superior content preservation and stylization quality across diverse scenarios.

</details>


### [50] [Graph Query Networks for Object Detection with Automotive Radar](https://arxiv.org/abs/2511.15271)
*Loveneet Saini,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: GQN通过图查询将雷达BEV转换为对象专属图，并用EdgeFocus与DeepContext Pooling捕捉关系与上下文，显著提升雷达目标检测性能并降低图构建开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格或序列的卷积/变换器方法难以处理雷达反射稀疏且不规则的特性，因此需要面向对象并能建模关系与上下文的表示。

Method: 构建基于图的表示，将BEV空间动态聚焦为对象专属图。引入graph queries用于在BEV上动态注意，生成对象节点与边；使用EdgeFocus模块进行关系推理，使用DeepContext Pooling模块进行上下文聚合；以注意力机制处理图并输出检测结果。

Result: 在NuScenes上相较先前最强雷达方法，GQN在mAP上提高了8.2%，相对mAP提升最高达53%；同时将峰值图构建开销减少80%，FLOPs仅适度增加。

Conclusion: 该论文提出Graph Query Networks (GQN)，通过图查询和图卷积式注意力模块改善雷达目标检测，显著提升NuScenes上mAP并降低图构建开销。

Abstract: Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.

</details>


### [51] [Edge-Centric Relational Reasoning for 3D Scene Graph Prediction](https://arxiv.org/abs/2511.15288)
*Yanni Ma,Hao Liu,Yulan Guo,Theo Gevers,Martin R. Oswald*

Main category: cs.CV

TL;DR: 提出LEO：先预测连边、构建线图并在其上进行边为中心推理，再将关系特征回融入对象图，从而捕获高阶关系依赖并提高3D场景图的关系预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统对象中心的图神经网络仅通过对象节点间的消息聚合更新边的表示，限制了关系表征只能基于成对对象上下文，难以捕捉高阶关系依赖，导致关系预测不准确。

Method: LEO先预测对象对之间的潜在链接以去除无关边，并将原始场景图转换为线图（将每条关系视为节点），在该线图上应用线图神经网络进行边为中心的关系推理以捕获关系间语境，然后将富化的关系特征回融入对象-中心图以增强对象级推理。框架对基础模型无关，可与现有对象-中心方法集成。

Result: 在3DSSG数据集上，基于两个有竞争力的基线进行集成实验，LEO带来了稳定且一致的性能提升，证明了从边到对象的推理范式的有效性。

Conclusion: LEO通过引入边为中心的关系推理与目标感知融合，突破了传统对象-中心图神经网络只利用成对上下文的局限，提升了高阶关系依赖的建模能力，从而改善3D场景图关系预测性能。

Abstract: 3D scene graph prediction aims to abstract complex 3D environments into structured graphs consisting of objects and their pairwise relationships. Existing approaches typically adopt object-centric graph neural networks, where relation edge features are iteratively updated by aggregating messages from connected object nodes. However, this design inherently restricts relation representations to pairwise object context, making it difficult to capture high-order relational dependencies that are essential for accurate relation prediction. To address this limitation, we propose a Link-guided Edge-centric relational reasoning framework with Object-aware fusion, namely LEO, which enables progressive reasoning from relation-level context to object-level understanding. Specifically, LEO first predicts potential links between object pairs to suppress irrelevant edges, and then transforms the original scene graph into a line graph where each relation is treated as a node. A line graph neural network is applied to perform edge-centric relational reasoning to capture inter-relation context. The enriched relation features are subsequently integrated into the original object-centric graph to enhance object-level reasoning and improve relation prediction. Our framework is model-agnostic and can be integrated with any existing object-centric method. Experiments on the 3DSSG dataset with two competitive baselines show consistent improvements, highlighting the effectiveness of our edge-to-object reasoning paradigm.

</details>


### [52] [Taming Generative Synthetic Data for X-ray Prohibited Item Detection](https://arxiv.org/abs/2511.15299)
*Jialong Sun,Hongguang Zhu,Weizhe Liu,Yunda Sun,Renshuai Tao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出一阶段基于扩散模型的X光安检图像合成Xsyn，包含CAR和BOM两策略，免除人工前景抠图，能提高检测任务mAP约1.2%。


<details>
  <summary>Details</summary>
Motivation: 传统X光安检图像合成多采用两阶段流水线，需人工抠图导致额外劳动成本且效率低。目标是设计无额外人工开销且能生成实用合成图像的方法，提升检测器在数据匮乏条件下的性能。

Method: 基于扩散模型的文本到图像生成，提出两项策略：1) Cross-Attention Refinement (CAR)：利用扩散模型的cross-attention图来细化目标的边界框标注；2) Background Occlusion Modeling (BOM)：在潜在空间中显式建模背景遮挡以增强图像成像复杂度。整体为一阶段流程，省去传统方法的前景提取步骤。

Result: 在若干X光安检数据集和检测器上，Xsyn相比先前方法取得了约1.2% mAP的提升，且合成图像能普遍提升被检测器的性能。作者提供了开源代码。

Conclusion: 本文提出了一种基于文本到图像生成的一阶段X光安检图像合成流水线Xsyn，无需额外人工前景抠图即可生成高质量合成图像，从而有效缓解训练数据短缺问题。

Abstract: Training prohibited item detection models requires a large amount of X-ray security images, but collecting and annotating these images is time-consuming and laborious. To address data insufficiency, X-ray security image synthesis methods composite images to scale up datasets. However, previous methods primarily follow a two-stage pipeline, where they implement labor-intensive foreground extraction in the first stage and then composite images in the second stage. Such a pipeline introduces inevitable extra labor cost and is not efficient. In this paper, we propose a one-stage X-ray security image synthesis pipeline (Xsyn) based on text-to-image generation, which incorporates two effective strategies to improve the usability of synthetic images. The Cross-Attention Refinement (CAR) strategy leverages the cross-attention map from the diffusion model to refine the bounding box annotation. The Background Occlusion Modeling (BOM) strategy explicitly models background occlusion in the latent space to enhance imaging complexity. To the best of our knowledge, compared with previous methods, Xsyn is the first to achieve high-quality X-ray security image synthesis without extra labor cost. Experiments demonstrate that our method outperforms all previous methods with 1.2% mAP improvement, and the synthetic images generated by our method are beneficial to improve prohibited item detection performance across various X-ray security datasets and detectors. Code is available at https://github.com/pILLOW-1/Xsyn/.

</details>


### [53] [Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language](https://arxiv.org/abs/2511.15308)
*Yan Xia,Letian Shi,Yilin Di,Joao F. Henriques,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出Text2Loc++：结合分层语言建模、注意力点云编码、MIT与MHCL的粗到细定位框架，显著提升基于自然语言的城市级点云定位性能，公开新数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 解决复杂、多样自然语言描述与城市尺度3D点云子图定位的跨模态对齐问题，提升在真实城市场景下的精确定位与泛化能力。

Method: 提出Text2Loc++包含全球检索阶段（结合预训练语言模型+HTM与注意力点云编码器），Masked Instance Training用于过滤不对齐对象，Modality-aware Hierarchical Contrastive Learning在多层次上增强嵌入；精细定位阶段采用Prototype-based Map Cloning与Cascaded Cross-Attention Transformer，去除显式文本-实例匹配。

Result: 在KITTI360Pose上比现有方法提升最多15%，并在新构建的数据集上表现出较强的泛化能力和对复杂语言的适应性。

Conclusion: Text2Loc++在城市规模点云与语言对齐的任务上取得显著提升，提出的阶段化方法（全局检索+精细定位）、新损失以及训练策略提升了鲁棒性与泛化能力。

Abstract: We tackle the problem of localizing 3D point cloud submaps using complex and diverse natural language descriptions, and present Text2Loc++, a novel neural network designed for effective cross-modal alignment between language and point clouds in a coarse-to-fine localization pipeline. To support benchmarking, we introduce a new city-scale dataset covering both color and non-color point clouds from diverse urban scenes, and organize location descriptions into three levels of linguistic complexity. In the global place recognition stage, Text2Loc++ combines a pretrained language model with a Hierarchical Transformer with Max pooling (HTM) for sentence-level semantics, and employs an attention-based point cloud encoder for spatial understanding. We further propose Masked Instance Training (MIT) to filter out non-aligned objects and improve multimodal robustness. To enhance the embedding space, we introduce Modality-aware Hierarchical Contrastive Learning (MHCL), incorporating cross-modal, submap-, text-, and instance-level losses. In the fine localization stage, we completely remove explicit text-instance matching and design a lightweight yet powerful framework based on Prototype-based Map Cloning (PMC) and a Cascaded Cross-Attention Transformer (CCAT). Extensive experiments on the KITTI360Pose dataset show that Text2Loc++ outperforms existing methods by up to 15%. In addition, the proposed model exhibits robust generalization when evaluated on the new dataset, effectively handling complex linguistic expressions and a wide variety of urban environments. The code and dataset will be made publicly available.

</details>


### [54] [Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models](https://arxiv.org/abs/2511.15311)
*Mehran Tamjidi,Hamidreza Dastmalchi,Mohammadreza Alimoradijazi,Ali Cheraghian,Aijun An,Morteza Saberi*

Main category: cs.CV

TL;DR: Uni-Adapter通过动态原型缓存、图标签平滑及熵加权融合，为3D视觉-语言模型提供训练自由的在线测试时自适应，显著缓解分布偏移。


<details>
  <summary>Details</summary>
Motivation: 现有3D VLFMs在开放世界和零样本能力上强，但在含噪、缺失或分布转移的实际数据上表现下降；因此需要一种无需重训练、能在线自适应分布变化的方法。

Method: 在3D VLFM上构建一个在线更新的3D缓存（按类存储聚类中心/原型），利用原型与样本的相似度计算缓存logit，并通过图模型进行原型间的标签平滑以保持相似原型的一致性，最终用熵加权将原始模型预测与缓存预测融合，无需任何参数更新。

Result: 在多个3D基准（ModelNet-40C、ScanObjectNN-C、ShapeNet-C）上，Uni-Adapter在不同3D VLFMs上均实现显著提升：ModelNet-40C提升10.55%，ScanObjectNN-C提升8.26%，ShapeNet-C提升4.49%。

Conclusion: 提出了一种无需训练的在线测试时适配方法Uni-Adapter，通过动态原型学习与图平滑、熵加权融合改进3D视觉-语言基础模型在分布偏移下的性能，显著提升多个受扰动数据集上的分类准确率。

Abstract: 3D Vision-Language Foundation Models (VLFMs) have shown strong generalization and zero-shot recognition capabilities in open-world point cloud processing tasks. However, these models often underperform in practical scenarios where data are noisy, incomplete, or drawn from a different distribution than the training data. To address this, we propose Uni-Adapter, a novel training-free online test-time adaptation (TTA) strategy for 3D VLFMs based on dynamic prototype learning. We define a 3D cache to store class-specific cluster centers as prototypes, which are continuously updated to capture intra-class variability in heterogeneous data distributions. These dynamic prototypes serve as anchors for cache-based logit computation via similarity scoring. Simultaneously, a graph-based label smoothing module captures inter-prototype similarities to enforce label consistency among similar prototypes. Finally, we unify predictions from the original 3D VLFM and the refined 3D cache using entropy-weighted aggregation for reliable adaptation. Without retraining, Uni-Adapter effectively mitigates distribution shifts, achieving state-of-the-art performance on diverse 3D benchmarks over different 3D VLFMs, improving ModelNet-40C by 10.55%, ScanObjectNN-C by 8.26%, and ShapeNet-C by 4.49% over the source 3D VLFMs.

</details>


### [55] [A Multimodal Transformer Approach for UAV Detection and Aerial Object Recognition Using Radar, Audio, and Video Data](https://arxiv.org/abs/2511.15312)
*Mauro Larrat,Claudomiro Sales*

Main category: cs.CV

TL;DR: 提出一种融合雷达、RGB、IR与音频的多模态Transformer用于无人机检测，达到近乎完美的分类性能且计算开销低，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 单一模态在复杂环境下存在观测盲区与噪声敏感性，目标是通过多模态数据融合利用互补信息提高鲁棒性与识别精度，满足无人机检测在复杂空域中的实时性与准确性要求。

Method: 设计了一个将四种模态输入（雷达、RGB视频、IR视频、音频）分别提取特征并通过Transformer自注意力机制进行跨模态融合的架构。使用宏平均指标评估分类性能，并报告计算复杂度（GFLOPs）、参数量与推理帧率来证明实时性。

Result: 在独立测试集上取得宏平均准确率0.9812、召回0.9873、精确率0.9787、F1 0.9826、特异性0.9954。模型参数约1.22M、1.09 GFLOPs，推理速度41.11 FPS，且在无人机与其他空中目标区分上表现尤为优异。

Conclusion: 该论文提出并验证了一种多模态Transformer模型用于无人机检测与空中目标识别，展示了融合雷达、可见光(RGB)、红外(IR)与音频的有效性，并在独立测试集上取得了极高的分类性能指标，且计算资源占用低，适合实时部署。

Abstract: Unmanned aerial vehicle (UAV) detection and aerial object recognition are critical for modern surveillance and security, prompting a need for robust systems that overcome limitations of single-modality approaches. This research addresses these challenges by designing and rigorously evaluating a novel multimodal Transformer model that integrates diverse data streams: radar, visual band video (RGB), infrared (IR) video, and audio. The architecture effectively fuses distinct features from each modality, leveraging the Transformer's self-attention mechanisms to learn comprehensive, complementary, and highly discriminative representations for classification. The model demonstrated exceptional performance on an independent test set, achieving macro-averaged metrics of 0.9812 accuracy, 0.9873 recall, 0.9787 precision, 0.9826 F1-score, and 0.9954 specificity. Notably, it exhibited particularly high precision and recall in distinguishing drones from other aerial objects. Furthermore, computational analysis confirmed its efficiency, with 1.09 GFLOPs, 1.22 million parameters, and an inference speed of 41.11 FPS, highlighting its suitability for real-time applications. This study presents a significant advancement in aerial object classification, validating the efficacy of multimodal data fusion via a Transformer architecture for achieving state-of-the-art performance, thereby offering a highly accurate and resilient solution for UAV detection and monitoring in complex airspace.

</details>


### [56] [What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs](https://arxiv.org/abs/2511.15316)
*Zhihan Ren,Lijun He,Jiaxi Liang,Xinzhu Fu,Haixia Bi,Fan Li*

Main category: cs.CV

TL;DR: FIA-Flow通过LFSAM与DIFM实现高保真中间特征反演，并用视觉-语言度量量化隐私泄漏，表明Split DNNs的隐私风险被低估。


<details>
  <summary>Details</summary>
Motivation: 现有的特征反演攻击重建质量有限，难以真实评估中间特征泄露的隐私风险，因此需要更强的黑盒FIA方法以揭示潜在威胁。

Method: 提出LFSAM以对齐中间特征空间与潜在空间的语义，并设计DIFM通过一次推理将离流形特征投射到目标流形，采用解耦训练以用少量图像-特征对进行有效学习；同时用大型视觉-语言模型构建两个人类感知度量。

Result: 在多种模型（AlexNet、ResNet、Swin Transformer、DINO、YOLO11）和多层特征上，FIA-Flow在重建逼真度和语义一致性上均优于现有方法，暴露出更严重的隐私威胁。

Conclusion: FIA-Flow显著提升了从中间特征恢复高保真图像的能力，表明Split DNNs面临比之前更严重的隐私泄露风险。

Abstract: Split DNNs enable edge devices by offloading intensive computation to a cloud server, but this paradigm exposes privacy vulnerabilities, as the intermediate features can be exploited to reconstruct the private inputs via Feature Inversion Attack (FIA). Existing FIA methods often produce limited reconstruction quality, making it difficult to assess the true extent of privacy leakage. To reveal the privacy risk of the leaked features, we introduce FIA-Flow, a black-box FIA framework that achieves high-fidelity image reconstruction from intermediate features. To exploit the semantic information within intermediate features, we design a Latent Feature Space Alignment Module (LFSAM) to bridge the semantic gap between the intermediate feature space and the latent space. Furthermore, to rectify distributional mismatch, we develop Deterministic Inversion Flow Matching (DIFM), which projects off-manifold features onto the target manifold with one-step inference. This decoupled design simplifies learning and enables effective training with few image-feature pairs. To quantify privacy leakage from a human perspective, we also propose two metrics based on a large vision-language model. Experiments show that FIA-Flow achieves more faithful and semantically aligned feature inversion across various models (AlexNet, ResNet, Swin Transformer, DINO, and YOLO11) and layers, revealing a more severe privacy threat in Split DNNs than previously recognized.

</details>


### [57] [Adaptive thresholding pattern for fingerprint forgery detection](https://arxiv.org/abs/2511.15322)
*Zahra Farzadpour,Masoumeh Azghani*

Main category: cs.CV

TL;DR: 本文提出结合各向异性扩散、三层小波与自适应阈值的指纹活体检测方法，特征通过SVM分类；实验表明在高比例像素缺失和大块缺失下对比基线有明显性能提升，显示良好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 指纹识别受伪造威胁，需开发能自动区分伪造/真实指纹的软件方法，并能抵抗噪声、像素缺失和块缺失等攻击或环境干扰。

Method: 先对输入指纹做各向异性扩散去噪，再进行三层小波变换，对各层系数采用自适应阈值处理，阈值化系数串联构成特征向量，最后用SVM进行二分类。还在数据上施加像素缺失、块缺失和噪声干扰以验证鲁棒性。

Result: 在模拟的极端缺失条件下（90%像素缺失、70x70块缺失），方法分别比对比方法提高约8%和5%准确率，整体在各种扰动下表现更稳健。

Conclusion: 提出的自适应阈值模式与多层小波系数融合并用SVM分类，可以较好区分真伪指纹，并在高缺失/阻断扰动情形下表现更鲁棒。

Abstract: Fingerprint liveness detection systems have been affected by spoofing, which is a severe threat for fingerprint-based biometric systems. Therefore, it is crucial to develop some techniques to distinguish the fake fingerprints from the real ones. The software based techniques can detect the fingerprint forgery automatically. Also, the scheme shall be resistant against various distortions such as noise contamination, pixel missing and block missing, so that the forgers cannot deceive the detector by adding some distortions to the faked fingerprint. In this paper, we propose a fingerprint forgery detection algorithm based on a suggested adaptive thresholding pattern. The anisotropic diffusion of the input image is passed through three levels of the wavelet transform. The coefficients of different layers are adaptively thresholded and concatenated to produce the feature vector which is classified using the SVM classifier. Another contribution of the paper is to investigate the effect of various distortions such as pixel missing, block missing, and noise contamination. Our suggested approach includes a novel method that exhibits improved resistance against a range of distortions caused by environmental phenomena or manipulations by malicious users. In quantitative comparisons, our proposed method outperforms its counterparts by approximately 8% and 5% in accuracy for missing pixel scenarios of 90% and block missing scenarios of size 70x70 , respectively. This highlights the novelty approach in addressing such challenges.

</details>


### [58] [Fast Post-Hoc Confidence Fusion for 3-Class Open-Set Aerial Object Detection](https://arxiv.org/abs/2511.15343)
*Spyridon Loukovitis,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: Lightweight MLP fusion of multiple confidences and features enables real-time three-way ID/OOD/background detection for UAVs, improving AUROC and mAP while preserving throughput.


<details>
  <summary>Details</summary>
Motivation: Closed-set detectors fail in open-world UAV scenarios; need to distinguish ID, OOD, and background reliably for safe UAV navigation; single-score thresholding conflates OOD with background and lacks flexibility.

Method: Use fusion of multiple confidence estimates and per-detection features including different logit variants, fed into a compact MLP as a post-processing module that is model-agnostic and lightweight; perform ablation and comparative experiments.

Result: Outperforms threshold-based baselines by 2.7% AUROC in two-class tasks, retains or improves open-set mAP, enables robust three-class classification, surpasses competitive methods in AUROC and improves closed-set mAP by up to 9 points (18% relative).

Conclusion: This paper proposes a lightweight, model-agnostic post-processing framework for UAV object detection that enables real-time three-way classification (ID targets, OOD objects, background) by fusing multiple confidence estimates and per-detection features using a compact MLP, improving AUROC and open-set mAP while preserving throughput.

Abstract: Developing reliable UAV navigation systems requires robust air-to-air object detectors capable of distinguishing between objects seen during training and previously unseen objects. While many methods address closed-set detection and achieve high-confidence recognition of in-domain (ID) targets, they generally do not tackle open-set detection, which requires simultaneous handling of both ID and out-of-distribution (OOD) objects. Existing open-set approaches typically rely on a single uncertainty score with thresholding, limiting flexibility and often conflating OOD objects with background clutter. In contrast, we propose a lightweight, model-agnostic post-processing framework that explicitly separates background from unknown objects while preserving the base detector's performance. Our approach extends open-set detection beyond binary ID/OOD classification to real-time three-way classification among ID targets, OOD objects, and background. To this end, we employ a fusion scheme that aggregates multiple confidence estimates and per-detection features using a compact multilayer perceptron (MLP). Incorporating different logit variants into the MLP consistently enhances performance across both binary and three-class classification without compromising throughput. Extensive ablation and comparative experiments confirm that our method surpasses threshold-based baselines in two-class classification by an average of 2.7% AUROC, while retaining or improving open-set mAP. Furthermore, our study uniquely enables robust three-class classification, a critical capability for safe UAV navigation, where OOD objects must be actively avoided and background regions safely ignored. Comparative analysis highlights that our method surpasses competitive techniques in AUROC across datasets, while improving closed-set mAP by up to 9 points, an 18% relative gain.

</details>


### [59] [IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers](https://arxiv.org/abs/2511.15369)
*Gihwan Kim,Jemin Lee,Hyungshin Kim*

Main category: cs.CV

TL;DR: IPTQ-ViT 是一种无重训练的全整数 PTQ 方法，通过为 GELU 与 Softmax 设计高精度整数近似并用统一指标为每层选择最优近似，实现视觉 Transformer 的整数推理，显著提升量化后精度且保持低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有 QAT 需要昂贵重训练以恢复非线性层量化带来的精度损失，限制了其在资源受限场景中的应用；而现有 PTQ 要么部分保留浮点、要么调整激活分布，无法实现完全整数推理或仍带来较大精度损失。故需一种无需重训练且能实现全整数推理的 PTQ 方法。

Method: 提出两类近似函数：基于多项式的 GELU（针对视觉数据优化）和基于位移的 Softmax；并设计统一度量（融合量化敏感度、扰动和计算成本）用于为每个激活层选择最优近似函数，从而实现全整数化而无需重新训练。

Result: 在多个任务上优于现有 PTQ 方法：图像分类上最多提升 6.44 百分点（平均 1.78 百分点）Top-1，目标检测提升 1.0 mAP；在 W8A8 和 W4A8 下优于部分浮点 PTQ 方法，并达到与整数化 QAT 相当的精度与延迟。

Conclusion: IPTQ-ViT 提出一种无需重训练的全整数化 PTQ 框架，能够在视觉 Transformer 上实现端到端整数推理并显著提升量化后精度。

Abstract: Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.

</details>


### [60] [Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training](https://arxiv.org/abs/2511.15379)
*Yunjiao Zhou,Xinyan Chen,Junlang Qian,Lihua Xie,Jianfei Yang*

Main category: cs.CV

TL;DR: ZOMG：用大模型做语言分段+软掩码学时序分割，实现零样本、无注释的语义动作分割，显著优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作类和密集标注，在开放词汇与真实场景下不可行，需无注释、可泛化的运动语义分割方案。

Method: 结合大语言模型进行语言语义拆分（将指令分解为有序子动作单元）与软掩码优化（学习实例级时间掩码以聚焦关键帧，同时保持段内连续性并增强段间分离），并保持预训练编码器不变。

Result: 在三个动作-语言数据集上取得最先进性能，在HumanML3D基准上mAP提升+8.7%，并在下游检索任务显著改进，验证了标注免疫的运动理解新范式。

Conclusion: ZOMG 能在无注释、零样本情形下，把动作序列分割为语义对齐的子动作单元，有效替代需密集监督的方法。

Abstract: Understanding complex human activities demands the ability to decompose motion into fine-grained, semantic-aligned sub-actions. This motion grounding process is crucial for behavior analysis, embodied AI and virtual reality. Yet, most existing methods rely on dense supervision with predefined action classes, which are infeasible in open-vocabulary, real-world settings. In this paper, we propose ZOMG, a zero-shot, open-vocabulary framework that segments motion sequences into semantically meaningful sub-actions without requiring any annotations or fine-tuning. Technically, ZOMG integrates (1) language semantic partition, which leverages large language models to decompose instructions into ordered sub-action units, and (2) soft masking optimization, which learns instance-specific temporal masks to focus on frames critical to sub-actions, while maintaining intra-segment continuity and enforcing inter-segment separation, all without altering the pretrained encoder. Experiments on three motion-language datasets demonstrate state-of-the-art effectiveness and efficiency of motion grounding performance, outperforming prior methods by +8.7\% mAP on HumanML3D benchmark. Meanwhile, significant improvements also exist in downstream retrieval, establishing a new paradigm for annotation-free motion understanding.

</details>


### [61] [Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models](https://arxiv.org/abs/2511.15390)
*Haidong Kang,Lihong Lin,Enneng Yang,Hongning Dai,Hao Wang*

Main category: cs.CV

TL;DR: AutoPrune让LLM自己设计剪枝算法，使用GCoT改进提示和SDSA处理异常值，实现了比现有方法更好的剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 现有面向LLM的剪枝方法依赖人工设计算法且需专家知识，并且统一稀疏导致高剪枝率下性能急剧下降，发现了异常值问题。

Method: 提出利用LLM自我设计剪枝算法，通过图驱动链式思维(GCoT)优化提示并生成可解释算法；引入偏态感知动态稀疏分配(SDSA)解决异常值问题。

Result: 在主流LLM基准上广泛实验证明AutoPrune优于现有最先进方法，在高稀疏率下能减轻性能退化。

Conclusion: AutoPrune能让LLM自动生成剪枝策略并在高剪枝率下保持性能。

Abstract: Large language models (LLMs) have achieved remarkable performance on a wide range of tasks, hindering real-world deployment due to their massive size. Existing pruning methods (e.g., Wanda) tailored for LLMs rely heavily on manual design pruning algorithms, thereby leading to \textit{huge labor costs} and \textit{requires expert knowledge}. Furthermore, we are the first to identify the serious \textit{outlier value issue} behind dramatic performance degradation under high pruning ratios that are caused by uniform sparsity, raising an additional concern about how to design adaptive pruning sparsity ideal for LLMs. Can LLMs prune by themselves? In this work, we introduce an affirmative answer by proposing a novel pruning method called \textbf{AutoPrune}, which first overcomes expert knowledge limits by leveraging LLMs to design optimal pruning algorithms for themselves automatically without any expert knowledge. Specifically, to mitigate the black-box nature of LLMs, we propose a Graph-driven Chain-of-Thought (GCoT) to optimize prompts, significantly enhancing the reasoning process in learning the pruning algorithm and enabling us to generate pruning algorithms with superior performance and interpretability in the next generation. Finally, grounded in insights of outlier value issue, we introduce Skew-aware Dynamic Sparsity Allocation (SDSA) to overcome the outlier value issue, mitigating performance degradation under high pruning ratios. We conduct extensive experiments on mainstream LLMs benchmarks, demonstrating the superiority of AutoPrune, which consistently excels state-of-the-art competitors. The code is available at: https://anonymous.4open.science/r/AutoPrune.

</details>


### [62] [ShelfOcc: Native 3D Supervision beyond LiDAR for Vision-Based Occupancy Estimation](https://arxiv.org/abs/2511.15396)
*Simon Boeder,Fabian Gigengack,Simon Roesler,Holger Caesar,Benjamin Risse*

Main category: cs.CV

TL;DR: ShelfOcc creates LiDAR-free, metrically consistent 3D voxel supervision from video by filtering and aggregating vision-derived geometry, enabling robust occupancy learning and large improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: 2D projection/rendering supervision causes geometric inconsistencies and depth bleeding; need true 3D supervision without LiDAR or manual 3D annotations.

Method: Generate metrically consistent semantic voxel labels from video by filtering and accumulating static geometry from vision-based 3D geometry models, handling dynamic objects and propagating semantics into a stable voxel representation; use these voxels to supervise standard occupancy model architectures.

Result: On Occ3D-nuScenes benchmark, ShelfOcc outperforms previous weakly/shelf-supervised methods with up to 34% relative improvement.

Conclusion: ShelfOcc demonstrates that high-quality metrically consistent 3D supervision derived from video can replace LiDAR for occupancy estimation, leading to substantial performance gains.

Abstract: Recent progress in self- and weakly supervised occupancy estimation has largely relied on 2D projection or rendering-based supervision, which suffers from geometric inconsistencies and severe depth bleeding. We thus introduce ShelfOcc, a vision-only method that overcomes these limitations without relying on LiDAR. ShelfOcc brings supervision into native 3D space by generating metrically consistent semantic voxel labels from video, enabling true 3D supervision without any additional sensors or manual 3D annotations. While recent vision-based 3D geometry foundation models provide a promising source of prior knowledge, they do not work out of the box as a prediction due to sparse or noisy and inconsistent geometry, especially in dynamic driving scenes. Our method introduces a dedicated framework that mitigates these issues by filtering and accumulating static geometry consistently across frames, handling dynamic content and propagating semantic information into a stable voxel representation. This data-centric shift in supervision for weakly/shelf-supervised occupancy estimation allows the use of essentially any SOTA occupancy model architecture without relying on LiDAR data. We argue that such high-quality supervision is essential for robust occupancy learning and constitutes an important complementary avenue to architectural innovation. On the Occ3D-nuScenes benchmark, ShelfOcc substantially outperforms all previous weakly/shelf-supervised methods (up to a 34% relative improvement), establishing a new data-driven direction for LiDAR-free 3D scene understanding.

</details>


### [63] [Controlling False Positives in Image Segmentation via Conformal Prediction](https://arxiv.org/abs/2511.15406)
*Luca Mossina,Corentin Friedrich*

Main category: cs.CV

TL;DR: 提出用顺序预测在预训练分割器上构建收缩置信掩码，实现在有限样本下对假阳性率的分布无关控制，无需重训，适用于临床风险敏感的分割任务。


<details>
  <summary>Details</summary>
Motivation: 临床场景要求对分割结果的可靠置信评估，尤其要控制过分割带来的风险，但现有深度模型通常缺乏可证的误差边界。作者旨在提供一种模型无关且有有限样本性质保证的实用方案。

Method: 对任意预训练分割模型，定义一族嵌套的“收缩”掩码（通过提高分数阈值或形态学腐蚀），利用带标签的校准集和顺序预测（conformal prediction）选择一个收缩参数，使得对与校准数据可交换的新图像，置信掩码中保留的假阳性比例在高概率下不超过用户设定的容忍度；该方法无需重训，对任意预测器在有限样本下都给出保证。

Result: 在息肉分割基准上实验证明了方法在目标层面的经验有效性，支持在对过分割敏感的临床场景中采用风险感知的分割决策。代码已开源。

Conclusion: 本文提出一种简单的后验框架，用于构建具有分布无关、图像级别假阳性控制的置信掩码，从而为分割结果提供统计保证。

Abstract: Reliable semantic segmentation is essential for clinical decision making, yet deep models rarely provide explicit statistical guarantees on their errors. We introduce a simple post-hoc framework that constructs confidence masks with distribution-free, image-level control of false-positive predictions. Given any pretrained segmentation model, we define a nested family of shrunken masks obtained either by increasing the score threshold or by applying morphological erosion. A labeled calibration set is used to select a single shrink parameter via conformal prediction, ensuring that, for new images that are exchangeable with the calibration data, the proportion of false positives retained in the confidence mask stays below a user-specified tolerance with high probability. The method is model-agnostic, requires no retraining, and provides finite-sample guarantees regardless of the underlying predictor. Experiments on a polyp-segmentation benchmark demonstrate target-level empirical validity. Our framework enables practical, risk-aware segmentation in settings where over-segmentation can have clinical consequences. Code at https://github.com/deel-ai-papers/conseco.

</details>


### [64] [D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models](https://arxiv.org/abs/2511.15411)
*Wenlun Zhang,Yunshan Zhong,Zihao Ding,Xinyu Li,Kentaro Yoshioka*

Main category: cs.CV

TL;DR: 提出D4C：通过提示语义注入、结构对比生成与扰动增强合成高质量伪图像，显著改善CLIP的无数据量化性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DFQ方法直接应用到CLIP会显著降性能，主要因合成样本缺乏语义内容和图内多样性，需提出专门面向视觉-语言模型的DFQ框架以弥补这一空白。

Method: D4C包含三部分：1) Prompt-Guided Semantic Injection：利用文本提示将生成图像与真实语义对齐；2) Structural Contrastive Generation：通过前景-背景对比合成，重现自然图像的组合结构；3) Perturbation-Aware Enhancement：对样本施加受控扰动以提升多样性和鲁棒性。采用这些伪图像对CLIP模型进行数据无关的量化校准。

Result: 在多种比特宽与模型上，D4C显著优于现有DFQ基线。在W4A8设置下，CLIP ResNet-50与ViT-B/32的零样本Top-1在CIFAR-10分别提升12.4%和18.9%，CIFAR-100提升6.8%和19.7%，ImageNet-1K提升1.4%和5.7%。

Conclusion: D4C是首个针对CLIP的无数据量化（DFQ）方法，通过语义注入、结构对比生成与扰动感知增强三大模块，合成具备语义信息与结构多样性的伪图像，从而显著提升CLIP在低位宽量化下的零样本分类性能。

Abstract: Data-Free Quantization (DFQ) offers a practical solution for model compression without requiring access to real data, making it particularly attractive in privacy-sensitive scenarios. While DFQ has shown promise for unimodal models, its extension to Vision-Language Models such as Contrastive Language-Image Pre-training (CLIP) models remains underexplored. In this work, we reveal that directly applying existing DFQ techniques to CLIP results in substantial performance degradation due to two key limitations: insufficient semantic content and low intra-image diversity in synthesized samples. To tackle these challenges, we propose D4C, the first DFQ framework tailored for CLIP. D4C synthesizes semantically rich and structurally diverse pseudo images through three key components: (1) Prompt-Guided Semantic Injection aligns generated images with real-world semantics using text prompts; (2) Structural Contrastive Generation reproduces compositional structures of natural images by leveraging foreground-background contrastive synthesis; and (3) Perturbation-Aware Enhancement applies controlled perturbations to improve sample diversity and robustness. These components jointly empower D4C to synthesize images that are both semantically informative and structurally diverse, effectively bridging the performance gap of DFQ on CLIP. Extensive experiments validate the effectiveness of D4C, showing significant performance improvements on various bit-widths and models. For example, under the W4A8 setting with CLIP ResNet-50 and ViT-B/32, D4C achieves Top-1 accuracy improvement of 12.4% and 18.9% on CIFAR-10, 6.8% and 19.7% on CIFAR-100, and 1.4% and 5.7% on ImageNet-1K in zero-shot classification, respectively.

</details>


### [65] [WarNav: An Autonomous Driving Benchmark for Segmentation of Navigable Zones in War Scenes](https://arxiv.org/abs/2511.15429)
*Marc-Emmanuel Coupvent des Graviers,Hejer Ammar,Christophe Guettier,Yann Dumortier,Romaric Audigier*

Main category: cs.CV

TL;DR: WarNav: a new dataset from DATTALION images for semantic segmentation in war-zones, benchmarks showing domain gaps from urban-trained models, and initial label-free adaptation steps to enable navigability in unstructured hostile environments.


<details>
  <summary>Details</summary>
Motivation: Existing datasets focus on structured urban driving and do not capture the visual and operational complexities of conflict-affected, unstructured terrains; need for data supporting robust autonomous navigation in hazardous war-zones and to minimize annotation costs.

Method: Construct dataset from DATTALION images, curate classes relevant to navigation, handle heterogeneity and ethics, benchmark several SOTA semantic segmentation models pre-trained on urban scenes, evaluate domain transfer, and propose unsupervised or domain-adaptive approaches to infer navigability without target annotations.

Result: Released WarNav dataset and baseline benchmarks showing significant performance drops when using urban-trained models; analysis of training data environment impact; demonstration of preliminary unsupervised/domain-adaptive method improving navigability estimation without target labels.

Conclusion: WarNav fills a dataset gap by providing real-world imagery for semantic segmentation of autonomous ground vehicles in war-damaged, unstructured environments; baselines show reduced performance when models trained on structured urban data are applied, highlighting domain shift; proposed methods for adapting without target annotations improve navigability assessment but remain preliminary.

Abstract: We introduce WarNav, a novel real-world dataset constructed from images of the open-source DATTALION repository, specifically tailored to enable the development and benchmarking of semantic segmentation models for autonomous ground vehicle navigation in unstructured, conflict-affected environments. This dataset addresses a critical gap between conventional urban driving resources and the unique operational scenarios encountered by unmanned systems in hazardous and damaged war-zones. We detail the methodological challenges encountered, ranging from data heterogeneity to ethical considerations, providing guidance for future efforts that target extreme operational contexts. To establish performance references, we report baseline results on WarNav using several state-of-the-art semantic segmentation models trained on structured urban scenes. We further analyse the impact of training data environments and propose a first step towards effective navigability in challenging environments with the constraint of having no annotation of the targeted images. Our goal is to foster impactful research that enhances the robustness and safety of autonomous vehicles in high-risk scenarios while being frugal in annotated data.

</details>


### [66] [Representation Space Constrained Learning with Modality Decoupling for Multimodal Object Detection](https://arxiv.org/abs/2511.15433)
*YiKang Shao,Tao Shi*

Main category: cs.CV

TL;DR: 本文从理论上识别出多模态检测中的融合退化：多模态架构下单模态分支梯度被严重抑制且低质量模态受抑制更强。作者提出RSC与MD两模块放大梯度并解耦模态干扰，实现各模态骨干的充分优化，在多数据集上验证了方法有效性和领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法多关注融合策略改进，却忽视了融合退化现象及其成因，缺乏理论分析与针对性解决方案。本工作旨在系统分析导致融合退化的优化缺陷，并提出相应的理论与方法。

Method: 提出Representation Space Constrained Learning (RSC)与 Modality Decoupling (MD)两模块：RSC放大被抑制的单模态分支梯度，MD消除模态间耦合干扰与模态不平衡，从而实现各模态骨干网络的全面优化。

Result: 在FLIR、LLVIP、M3FD与MFAD四个数据集上的大量实验表明，RSC-MD能有效缓解融合退化问题，并在多项基准上取得了最先进的检测性能。

Conclusion: 该论文通过理论分析与方法设计，揭示并有效缓解了多模态目标检测中的融合退化问题，提出的RSC-MD方法提升了各模态分支的梯度流与独立优化，实验证明在多数据集上达到或超过现有最佳性能。

Abstract: Multimodal object detection has attracted significant attention in both academia and industry for its enhanced robustness. Although numerous studies have focused on improving modality fusion strategies, most neglect fusion degradation, and none provide a theoretical analysis of its underlying causes. To fill this gap, this paper presents a systematic theoretical investigation of fusion degradation in multimodal detection and identifies two key optimization deficiencies: (1) the gradients of unimodal branch backbones are severely suppressed under multimodal architectures, resulting in under-optimization of the unimodal branches; (2) disparities in modality quality cause weaker modalities to experience stronger gradient suppression, which in turn results in imbalanced modality learning. To address these issues, this paper proposes a Representation Space Constrained Learning with Modality Decoupling (RSC-MD) method, which consists of two modules. The RSC module and the MD module are designed to respectively amplify the suppressed gradients and eliminate inter-modality coupling interference as well as modality imbalance, thereby enabling the comprehensive optimization of each modality-specific backbone. Extensive experiments conducted on the FLIR, LLVIP, M3FD, and MFAD datasets demonstrate that the proposed method effectively alleviates fusion degradation and achieves state-of-the-art performance across multiple benchmarks. The code and training procedures will be released at https://github.com/yikangshao/RSC-MD.

</details>


### [67] [HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation](https://arxiv.org/abs/2511.15435)
*Linyin Luo,Yujuan Ding,Yunshan Ma,Wenqi Fan,Hanjiang Lai*

Main category: cs.CV

TL;DR: 通过两阶段分层图像扰动，攻击者可在不改动检索库或模型的情况下，使MRAG系统检索并生成错误或无关回答，具备现实威胁。


<details>
  <summary>Details</summary>
Motivation: 尽管已有针对MRAG的知识投毒攻击研究，本工作关注更实用且更隐蔽的视觉攻击：仅对用户端图像添加微小扰动而不篡改检索库或模型参数，探究这种攻击在现实场景中能否破坏检索-生成链路。

Method: 设计了一种两阶段分层策略：第一阶段通过破坏跨模态对齐使检索器无法正确匹配图像与文本描述，第二阶段进一步扰乱多模态语义对齐以召回更不相关的知识；整个优化目标在图像像素级添加不可察觉扰动，基于CLIP检索器的特征空间最大化检索错误，最终用LMM（BLIP-2和LLaVA）评估生成混淆效果。

Result: 在OK-VQA和InfoSeek两个常用MRAG数据集上进行实验，使用CLIP检索器和BLIP-2、LLaVA作为生成器，结果显示在攻击后检索召回质量和最终生成回答的性能均显著下降，证明方法有效。

Conclusion: 本文提出的分层视觉攻击(Hierarchical Visual Attack)通过对输入图像添加不可察觉扰动，使多模态检索增强生成(MRAG)系统的检索器召回无关知识，并进一步破坏生成器的输入对齐，从而显著降低检索与生成性能，验证了视觉层面对MRAG系统的有效威胁性。

Abstract: Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.

</details>


### [68] [A Dataset and Baseline for Deep Learning-Based Visual Quality Inspection in Remanufacturing](https://arxiv.org/abs/2511.15440)
*Johannes C. Bauer,Paul Geng,Stephan Trattnig,Petr Dokládal,Rüdiger Daub*

Main category: cs.CV

TL;DR: 新建齿轮箱零件图像数据集并提出对比正则化损失，能在分布转移下提升缺陷分类模型对未见部件的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 再制造过程中对拆解部件的质量检测主要依赖人工，因零件种类和缺陷模式多样，自动化视觉检测模型难以泛化到新产品或新缺陷，亟需数据集和方法来评估并提升模型的泛化能力。

Method: 构建包含来自两款汽车变速箱的齿轮箱零件（良品与缺陷品）的图像数据集，设计不同的train-test分割以产生分布转移；在该数据集上对比评估多种分类模型，并在训练过程中加入一种新的对比正则化损失以增强特征判别与泛化能力。

Result: 实验表明：提出的数据集和划分能够有效模拟不同分布转移场景；所提出的对比正则化损失在多个分割情形下提高了模型在未见部件类型上的分类性能，证明了其提升泛化能力的有效性。

Conclusion: 该论文提出了一个用于再制造齿轮箱零件质量检测的新图像数据集，并设计了用于评估模型泛化能力的不同训练/测试划分；提出了一种对比正则化损失以提升模型对未见部件类型的鲁棒性。

Abstract: Remanufacturing describes a process where worn products are restored to like-new condition and it offers vast ecological and economic potentials. A key step is the quality inspection of disassembled components, which is mostly done manually due to the high variety of parts and defect patterns. Deep neural networks show great potential to automate such visual inspection tasks but struggle to generalize to new product variants, components, or defect patterns. To tackle this challenge, we propose a novel image dataset depicting typical gearbox components in good and defective condition from two automotive transmissions. Depending on the train-test split of the data, different distribution shifts are generated to benchmark the generalization ability of a classification model. We evaluate different models using the dataset and propose a contrastive regularization loss to enhance model robustness. The results obtained demonstrate the ability of the loss to improve generalisation to unseen types of components.

</details>


### [69] [Driving in Spikes: An Entropy-Guided Object Detector for Spike Cameras](https://arxiv.org/abs/2511.15459)
*Ziyan Liu,Qi Su,Lulu Tang,Zhaofei Yu,Tiejun Huang*

Main category: cs.CV

TL;DR: 提出EASD：针对刺针相机的双分支端到端检测器，并发布DSEC Spike驾驶模拟刺针检测基准，旨在解决刺针数据稀疏性带来的检测挑战。


<details>
  <summary>Details</summary>
Motivation: 在高速运动和极端光照条件下，传统相机产生模糊或饱和，影响自动驾驶中的目标检测。而刺针相机具有微秒级延迟和极高动态范围，但其稀疏离散的像素触发输出无法被标准基于图像的检测器直接使用，故需要专门的端到端检测方法及适配数据集。

Method: 提出双分支网络：1)基于时序的纹理+特征融合分支（Temporal Based Texture + Feature Fusion），负责跨时间片的全局语义建模；2)熵选择注意力分支（Entropy Selective Attention），聚焦于目标中心细节；端到端训练以直接处理刺针流数据而非将其转换为图像。

Result: 本文构建并发布了DSEC Spike模拟驾驶检测基准，并在该基准上验证了EASD的有效性（摘要未给出具体数值），展示了在高动态范围和快速运动场景下对目标检测性能的提升。

Conclusion: 本文提出了EASD，一种面向刺针相机（spike camera）的端到端目标检测器，通过双分支设计同时捕获全局时序语义与对象中心细节，解决刺针相机输出稀疏离散数据与标准图像检测器不兼容的问题，并引入首个驾驶场景模拟刺针数据集DSEC Spike以缩小数据差距。

Abstract: Object detection in autonomous driving suffers from motion blur and saturation under fast motion and extreme lighting. Spike cameras, offer microsecond latency and ultra high dynamic range for object detection by using per pixel asynchronous integrate and fire. However, their sparse, discrete output cannot be processed by standard image-based detectors, posing a critical challenge for end to end spike stream detection. We propose EASD, an end to end spike camera detector with a dual branch design: a Temporal Based Texture plus Feature Fusion branch for global cross slice semantics, and an Entropy Selective Attention branch for object centric details. To close the data gap, we introduce DSEC Spike, the first driving oriented simulated spike detection benchmark.

</details>


### [70] [SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome](https://arxiv.org/abs/2511.15464)
*Dabin Jeong,Amirhossein Vahidi,Ciro Ramírez-Suástegui,Marie Moullet,Kevin Ly,Mohammad Vali Sanian,Sebastian Birk,Yinshui Chang,Adam Boxall,Daniyal Jafree,Lloyd Steele,Vijaya Baskar MS,Muzlifah Haniffa,Mohammad Lotfollahi*

Main category: cs.CV

TL;DR: Sigmma通过多尺度对比对齐与细胞图建模，提高了HE图像与空间转录组的跨模态表示质量，显著提升基因表达预测与检索性能，并能揭示组织级别结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在单一尺度上对齐HE图像与ST表达，忽视细胞层面的精细结构与空间组织，导致跨模态对应弱和下游任务性能受限。

Method: 提出多尺度对比对齐框架，保证不同尺度上模态间表示的一致性；将细胞交互建模为图，整合子图内外（intra-/inter-subgraph）关系以捕捉从精细到粗糙的细胞-细胞相互作用。

Result: 在若干数据集上，Sigmma在基因表达预测任务平均提升9.78%，在跨模态检索任务平均提升26.93%；并在下游分析中展现出更有意义的多组织结构学习能力。

Conclusion: Sigmma通过多尺度对比对齐与图结构建模细胞交互，有效增强了HE图像与空间转录组表达之间的跨模态对应，提升了基因表达预测与跨模态检索性能，并能反映组织级别的生物学结构。

Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.

</details>


### [71] [Deep Learning for Accurate Vision-based Catch Composition in Tropical Tuna Purse Seiners](https://arxiv.org/abs/2511.15468)
*Xabier Lekunberri,Ahmad Kamal,Izaro Goienetxea,Jon Ruiz,Iñaki Quincoces,Jaime Valls Miro,Ignacio Arganda-Carreras,Jose A. Fernandes-Salvador*

Main category: cs.CV

TL;DR: 分析专家辨识难度并提出基于YOLOv9+SAM2分割与分层分类的多阶段管线，可在电子监控视频上以低误差高召回率估计金枪鱼的物种组成。


<details>
  <summary>Details</summary>
Motivation: 由于电子监控产生大量视频需人工分析，且AI在物种鉴定上受限于训练数据，研究旨在评估专家区分黄鳍鱼与长鳍金枪鱼的难度，并开发能在EM视频中估计捕获物种组成的AI管线。

Method: 使用三种分割方法（Mask R-CNN、DINOv2+SAM2、YOLOv9+SAM2）比较分割效果，使用ByteTrack进行跟踪，并比较多类分类与分层分类方法；基于观察员提供的地面真实数据进行交叉验证与在完全已知捕获组成的作业上测试。

Result: 专家在区分BET与YFT上存在较低一致性（BET 42.9%±35.6%，YFT 57.1%±35.6%）；YOLOv9+SAM2分割效果最佳（mAP 0.66±0.03，召回0.88±0.03）；分层分类优于多类分类；最终组合（YOLOv9-SAM2+分层分类）能分割并识别84.8%个体，平均误差4.5%。

Conclusion: 该研究展示了在电子监控视频上对金枪鱼物种识别的挑战与解决方案，提出了多阶段处理管线并验证了最佳的分割与分类组合，能在真实捕捞操作中实现较高的分割与识别精度。

Abstract: Purse seiners play a crucial role in tuna fishing, as approximately 69% of the world's tropical tuna is caught using this gear. All tuna Regional Fisheries Management Organizations have established minimum standards to use electronic monitoring (EM) in fisheries in addition to traditional observers. The EM systems produce a massive amount of video data that human analysts must process. Integrating artificial intelligence (AI) into their workflow can decrease that workload and improve the accuracy of the reports. However, species identification still poses significant challenges for AI, as achieving balanced performance across all species requires appropriate training data. Here, we quantify the difficulty experts face to distinguish bigeye tuna (BET, Thunnus Obesus) from yellowfin tuna (YFT, Thunnus Albacares) using images captured by EM systems. We found inter-expert agreements of 42.9% $\pm$ 35.6% for BET and 57.1% $\pm$ 35.6% for YFT. We then present a multi-stage pipeline to estimate the species composition of the catches using a reliable ground-truth dataset based on identifications made by observers on board. Three segmentation approaches are compared: Mask R-CNN, a combination of DINOv2 with SAM2, and a integration of YOLOv9 with SAM2. We found that the latest performs the best, with a validation mean average precision of 0.66 $\pm$ 0.03 and a recall of 0.88 $\pm$ 0.03. Segmented individuals are tracked using ByteTrack. For classification, we evaluate a standard multiclass classification model and a hierarchical approach, finding a superior generalization by the hierarchical. All our models were cross-validated during training and tested on fishing operations with fully known catch composition. Combining YOLOv9-SAM2 with the hierarchical classification produced the best estimations, with 84.8% of the individuals being segmented and classified with a mean average error of 4.5%.

</details>


### [72] [RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection](https://arxiv.org/abs/2511.15476)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出把结构化CNN与定制化Transformer相融合的RS-CA-HSICT，用通道与空间注意增强特征，达到约98%准确率与F1，适合MPox皮损检测。


<details>
  <summary>Details</summary>
Motivation: 传统CNN缺乏长距离依赖建模而Transformer缺失局部细节提取，单一模型在MPox皮损检测上难以同时兼顾全局语义与局部微细差异。为提高鲁棒性与判别能力，作者提出混合架构以强化多尺度、多通道特征表示。

Method: 设计了HSICT模块（融合stem CNN与定制ICT块）、残差CNN模块、空间CNN模块和通道注意（CA）模块。HSICT利用多头注意和结构化CNN（H层学习空间同质性，S层学习细结构），ICT块负责全局上下文交互与局部纹理提取；逆残差学习和分阶段下采样增强训练稳定性与尺度不变性；最终通过通道融合注意和空间注意精炼特征并抑制冗余。

Result: 在Kaggle基准和一套多样化的MPox数据集上，模型达到高达98.30%的准确率和98.13%的F1-score，优于已有的CNN和ViT基线，显示出更好的微小样本与细粒度区分能力。

Conclusion: 该论文提出的RS-CA-HSICT框架在MPox检测上表现优异，结合了CNN和Transformer优势，能够同时捕获局部纹理与全局上下文，显著提升分类性能。

Abstract: This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.

</details>


### [73] [FunnyNodules: A Customizable Medical Dataset Tailored for Evaluating Explainable AI](https://arxiv.org/abs/2511.15481)
*Luisa Gallée,Yiheng Xiong,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: FunnyNodules是一个可参数化的合成肺结节数据集，具有可控视觉属性和确定性决策规则，支持对xAI模型的属性级推理和注意力对齐进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像数据集缺乏描述诊断推理过程的稠密标注，阻碍对xAI模型是否‘为正确理由做出正确预测’的评估与改进，需要一个可控且带完全地面真实信息的基准。

Method: 构建可参数化的合成数据生成器，生成肺结节样式的抽象形状并可控地调整圆度、边界清晰度、毛刺等视觉属性；根据预设属性组合决定目标类别；支持改变复杂度、类别平衡和目标定义以生成多样数据集。

Result: 展示了如何基于FunnyNodules进行模型不可知的评估：检测模型是否学习了属性-目标关系、诊断属性预测过/欠表现的原因解释、以及注意力与属性相关感兴趣区域的对齐分析。提供了一个灵活工具用于开发和基准测试可解释医学影像AI。

Conclusion: FunnyNodules填补了可解释医学影像数据标注不足的空白，提供可控属性与目标类之间的完全地面真实因果关系，适合系统性评估xAI方法是否基于正确理由作出诊断。

Abstract: Densely annotated medical image datasets that capture not only diagnostic labels but also the underlying reasoning behind these diagnoses are scarce. Such reasoning-related annotations are essential for developing and evaluating explainable AI (xAI) models that reason similarly to radiologists: making correct predictions for the right reasons. To address this gap, we introduce FunnyNodules, a fully parameterized synthetic dataset designed for systematic analysis of attribute-based reasoning in medical AI models. The dataset generates abstract, lung nodule-like shapes with controllable visual attributes such as roundness, margin sharpness, and spiculation. Target class is derived from a predefined attribute combination, allowing full control over the decision rule that links attributes to the diagnostic class. We demonstrate how FunnyNodules can be used in model-agnostic evaluations to assess whether models learn correct attribute-target relations, to interpret over- or underperformance in attribute prediction, and to analyze attention alignment with attribute-specific regions of interest. The framework is fully customizable, supporting variations in dataset complexity, target definitions, class balance, and beyond. With complete ground truth information, FunnyNodules provides a versatile foundation for developing, benchmarking, and conducting in-depth analyses of explainable AI methods in medical image analysis.

</details>


### [74] [Evaluating Low-Light Image Enhancement Across Multiple Intensity Levels](https://arxiv.org/abs/2511.15496)
*Maria Pilligua,David Serrano-Lozano,Pai Peng,Ramon Baldrich,Michael S. Brown,Javier Vazquez-Corral*

Main category: cs.CV

TL;DR: 作者发布MILL多照度低光数据集，揭示现有增强方法的照度敏感性并通过针对性改进在多照度下显著提高重建质量（DSLR上PSNR提升可达10 dB）。


<details>
  <summary>Details</summary>
Motivation: 现有学习型低光增强方法多依赖单一低光-参考配对数据，缺乏照度多样性，导致无法评估或保证算法在不同低光强度下的表现。作者希望通过构建多照度数据集来填补这一空白并推动更稳健算法的设计。

Method: 在固定相机参数与精确照度测量的受控环境下采集多照度图像构建MILL数据集；基于该数据集对多种现有低光增强方法进行逐照度评测；利用数据集的多照度结构提出若干改进策略以提升算法在不同照度下的鲁棒性，并在Full HD图像上测试改进效果。

Result: 构建了具有精确照度标注的MILL数据集，基于此发现现有方法在不同照度下表现差异显著；提出的改进使得在Full HD DSLR图像上PSNR提升最多达10 dB，在智能手机图像上提升约2 dB。

Conclusion: 该论文提出了MILL数据集，展示了在多照度条件下评估和改进低光增强方法的必要性，证明了现有方法在不同亮度下性能波动明显，且通过针对性改进显著提升了鲁棒性。

Abstract: Imaging in low-light environments is challenging due to reduced scene radiance, which leads to elevated sensor noise and reduced color saturation. Most learning-based low-light enhancement methods rely on paired training data captured under a single low-light condition and a well-lit reference. The lack of radiance diversity limits our understanding of how enhancement techniques perform across varying illumination intensities. We introduce the Multi-Illumination Low-Light (MILL) dataset, containing images captured at diverse light intensities under controlled conditions with fixed camera settings and precise illuminance measurements. MILL enables comprehensive evaluation of enhancement algorithms across variable lighting conditions. We benchmark several state-of-the-art methods and reveal significant performance variations across intensity levels. Leveraging the unique multi-illumination structure of our dataset, we propose improvements that enhance robustness across diverse illumination scenarios. Our modifications achieve up to 10 dB PSNR improvement for DSLR and 2 dB for the smartphone on Full HD images.

</details>


### [75] [Learning to Expand Images for Efficient Visual Autoregressive Modeling](https://arxiv.org/abs/2511.15499)
*Ruiqing Yang,Kaixin Zhang,Zheng Zhang,Shan You,Tao Huang*

Main category: cs.CV

TL;DR: 提出基于中心向外螺旋展开的自回归生成方法 EAR，配合长度自适应解码，在 ImageNet 上实现了更高效且更符合认知的图像生成。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉中心向外感知模式启发，旨在解决现有自回归视觉生成中逐 token 解码低效或多尺度表示复杂度高的问题，通过对齐生成顺序与感知相关性以提升效率和质量。

Method: 核心方法为螺旋状从中心向外展开的 token 生成顺序（Expanding Autoregressive Representation，EAR），结合长度自适应的解码策略动态决定每步预测的 token 数量，以保持空间连续性并提高并行解码效率。

Result: 在 ImageNet 实验中，EAR 在单尺度自回归模型上实现了保真度与计算成本之间的更好折衷，宣称达到了 SOTA 的效率-质量平衡。

Conclusion: EAR 提出了一种以中心向外展开的自回归表示方法，通过螺旋展开图像 token 并结合自适应长度解码，实现更高效的并行解码和更好的生成质量。作者声称在 ImageNet 上达到了单尺度自回归模型在保真度与效率之间的最优权衡。

Abstract: Autoregressive models have recently shown great promise in visual generation by leveraging discrete token sequences akin to language modeling. However, existing approaches often suffer from inefficiency, either due to token-by-token decoding or the complexity of multi-scale representations. In this work, we introduce Expanding Autoregressive Representation (EAR), a novel generation paradigm that emulates the human visual system's center-outward perception pattern. EAR unfolds image tokens in a spiral order from the center and progressively expands outward, preserving spatial continuity and enabling efficient parallel decoding. To further enhance flexibility and speed, we propose a length-adaptive decoding strategy that dynamically adjusts the number of tokens predicted at each step. This biologically inspired design not only reduces computational cost but also improves generation quality by aligning the generation order with perceptual relevance. Extensive experiments on ImageNet demonstrate that EAR achieves state-of-the-art trade-offs between fidelity and efficiency on single-scale autoregressive models, setting a new direction for scalable and cognitively aligned autoregressive image generation.

</details>


### [76] [Multi-Text Guided Few-Shot Semantic Segmentation](https://arxiv.org/abs/2511.15515)
*Qiang Jiao,Bin Yan,Yi Yang,Mengrui Shi,Qiang Zhang*

Main category: cs.CV

TL;DR: MTGNet通过多文本提示融合与跨模态引导、以及支持前景自相似性加权，解决了单文本先验覆盖不足与噪声敏感问题，显著提升了少样本语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 单一文本提示无法覆盖复杂类别的语义多样性，且缺乏显式跨模态交互且对噪声支持特征敏感，导致目标区域激活不完整与先验质量下降。

Method: 提出双分支框架，核心模块包括MTPR（多文本先验精化）、TAFF（文本锚特征融合）和FCWA（前景置信度加权注意力），分别用于增强文本先验、通过文本锚传递判别性原型以及自适应下调不一致支持区域。

Result: 在标准FSS基准上取得显著性能，1-shot在PASCAL-5i上mIoU为76.8%，在COCO-20i上为57.4%，在类内差异大的fold上提升明显。

Conclusion: MTGNet通过融合多文本提示并结合跨模态引导与支持前景自相似性加权，有效提升了少样本语义分割的前景激活完整性与鲁棒性。

Abstract: Recent CLIP-based few-shot semantic segmentation methods introduce class-level textual priors to assist segmentation by typically using a single prompt (e.g., a photo of class). However, these approaches often result in incomplete activation of target regions, as a single textual description cannot fully capture the semantic diversity of complex categories. Moreover, they lack explicit cross-modal interaction and are vulnerable to noisy support features, further degrading visual prior quality. To address these issues, we propose the Multi-Text Guided Few-Shot Semantic Segmentation Network (MTGNet), a dual-branch framework that enhances segmentation performance by fusing diverse textual prompts to refine textual priors and guide the cross-modal optimization of visual priors. Specifically, we design a Multi-Textual Prior Refinement (MTPR) module that suppresses interference and aggregates complementary semantic cues to enhance foreground activation and expand semantic coverage for structurally complex objects. We introduce a Text Anchor Feature Fusion (TAFF) module, which leverages multi-text embeddings as semantic anchors to facilitate the transfer of discriminative local prototypes from support images to query images, thereby improving semantic consistency and alleviating intra-class variations. Furthermore, a Foreground Confidence-Weighted Attention (FCWA) module is presented to enhance visual prior robustness by leveraging internal self-similarity within support foreground features. It adaptively down-weights inconsistent regions and effectively suppresses interference in the query segmentation process. Extensive experiments on standard FSS benchmarks validate the effectiveness of MTGNet. In the 1-shot setting, it achieves 76.8% mIoU on PASCAL-5i and 57.4% on COCO-20i, with notable improvements in folds exhibiting high intra-class variations.

</details>


### [77] [A Hybrid CNN-ViT-GNN Framework with GAN-Based Augmentation for Intelligent Weed Detection in Precision Agriculture](https://arxiv.org/abs/2511.15535)
*Pandiyaraju V,Abishek Karthik,Sreya Mynampati,Poovarasan L,D. Saraswathi*

Main category: cs.CV

TL;DR: 论文提出CNN+ViT+GNN的混合框架，辅以GAN增强与自监督对比预训练，应对多变田间条件与数据问题，报告近乎完美的99.33%分类指标，强调可解释性与边缘部署应用。


<details>
  <summary>Details</summary>
Motivation: 动机是为了解决田间环境下杂草与作物外观相似、光照与背景变化大、标注样本稀缺及类别不平衡等问题，从而实现高精度、可解释且可部署在边缘设备的自动化杂草检测，降低化学除草用量并推进可持续农业。

Method: 方法包括：1）构建一个融合局部特征提取的CNN、全局上下文建模的ViT和建模样本间关系的GNN的混合体系；2）使用基于GAN的图像增强来平衡类别分布并扩充训练集；3）应用自监督对比学习进行预训练以从有限标注数据中学习更丰富特征；4）在融合特征上进行监督微调并评估分类指标。

Result: 在多基准数据集上实验报告准确率、精确率、召回率和F1均为99.33%，表明该框架在所测数据上表现非常优异，且声称能实时部署于边缘设备。

Conclusion: 该论文提出了一种结合CNN、ViT与GNN的混合深度学习框架，并通过GAN增强与自监督对比预训练来应对数据不平衡与标注稀缺问题，最终在多基准数据集上获得接近完美的分类性能；总体结论是该方法在检测鲁棒性、可解释性与边缘部署效率方面具有优势，能促进精准农业中的选择性除草。

Abstract: The task of weed detection is an essential element of precision agriculture since accurate species identification allows a farmer to selectively apply herbicides and fits into sustainable agriculture crop management. This paper proposes a hybrid deep learning framework recipe for weed detection that utilizes Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), and Graph Neural Networks (GNNs) to build robustness to multiple field conditions. A Generative Adversarial Network (GAN)-based augmentation method was imposed to balance class distributions and better generalize the model. Further, a self-supervised contrastive pre-training method helps to learn more features from limited annotated data. Experimental results yield superior results with 99.33% accuracy, precision, recall, and F1-score on multi-benchmark datasets. The proposed model architecture enables local, global, and relational feature representations and offers high interpretability and adaptability. Practically, the framework allows real-time, efficient deployment to edge devices for automated weed detecting, reducing over-reliance on herbicides and providing scalable, sustainable precision-farming options.

</details>


### [78] [Scriboora: Rethinking Human Pose Forecasting](https://arxiv.org/abs/2511.15565)
*Daniel Bermuth,Alexander Poeppel,Wolfgang Reif*

Main category: cs.CV

TL;DR: 建立统一评估，借用语音模型提升绝对姿态预测，并在真实估计噪声下评估与用无监督微调部分恢复性能。


<details>
  <summary>Details</summary>
Motivation: 当前姿态预测工作在实验设置、可复现性和现实感噪声评估方面缺乏统一标准；鉴于语音建模与时序预测的类比，探索语音模型在姿态预测中的潜力；评估在真实估计噪声下的实际性能是提升实际应用可靠性的关键。

Method: 建立统一训练与评估流水线，评估多种姿态预测算法；将近年语音理解模型高效适配到姿态预测任务以提升性能；引入基于姿态估计器输出的噪声数据集变体，评估模型在现实噪声下的表现，并尝试无监督微调恢复性能。

Result: 适配后的语音模型在绝对姿态预测上优于先前最先进方法；使用估计的带噪姿态会显著降低性能，但通过无监督微调可以部分恢复性能；并公开了统一评估流水线和带噪数据变体以促进可复现研究。

Conclusion: 本文总结了针对绝对人体姿态预测任务的全面评估，指出现有方法在可复现性和在真实噪声下的鲁棒性方面存在问题，并提出用语音模型迁移以及无监督微调来改进性能和恢复降级。

Abstract: Human pose forecasting predicts future poses based on past observations, and has many significant applications in areas such as action recognition, autonomous driving or human-robot interaction. This paper evaluates a wide range of pose forecasting algorithms in the task of absolute pose forecasting, revealing many reproducibility issues, and provides a unified training and evaluation pipeline. After drawing a high-level analogy to the task of speech understanding, it is shown that recent speech models can be efficiently adapted to the task of pose forecasting, and improve current state-of-the-art performance. At last the robustness of the models is evaluated, using noisy joint coordinates obtained from a pose estimator model, to reflect a realistic type of noise, which is more close to real-world applications. For this a new dataset variation is introduced, and it is shown that estimated poses result in a substantial performance degradation, and how much of it can be recovered again by unsupervised finetuning.

</details>


### [79] [Computer-Use Agents as Judges for Generative User Interface](https://arxiv.org/abs/2511.15567)
*Kevin Qinghong Lin,Siyuan Hu,Linjie Li,Zhengyuan Yang,Lijuan Wang,Philip Torr,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出AUI-Gym和Coder-CUA协作框架，使用CUA作为评审基于可解性优化自动生成的GUI，并提供CUA Dashboard用于可解释反馈，推动界面向代理友好方向发展。


<details>
  <summary>Details</summary>
Motivation: 现有GUI以人为设计，强制代理采用不必要的行为，且代码生成模型使自动GUI设计成为可能，因而探讨是否可用CUA作为评审来辅助Coder改进界面以提高代理执行效率。

Method: 构建AUI-Gym基准（52个应用、1560个任务），开发任务可执行性验证器；提出Coder-CUA协作框架和CUA Dashboard，将多步导航历史压缩为可解释的反馈，迭代生成和评估网站。

Result: 展示了基于任务可解性和CUA导航成功率的评价指标，并通过基准和验证器证明了协作框架能引导生成更利于代理的GUI设计。代码与数据集公开。

Conclusion: 该论文提出将代码生成模型（Coder）与计算机使用代理（CUA）结合，用于自动化GUI开发，通过让CUA担任评审者、Coder担任设计者，优化界面以便代理而非人类使用。

Abstract: Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans--prioritizing aesthetics and usability--forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce AUI-Gym, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesize 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder-CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design a CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments. Our code and dataset are available at https://github.com/showlab/AUI.

</details>


### [80] [Transferable Dual-Domain Feature Importance Attack against AI-Generated Image Detector](https://arxiv.org/abs/2511.15571)
*Weiheng Zhu,Gang Cao,Jing Liu,Lifang Yu,Shaowei Weng*

Main category: cs.CV

TL;DR: DuFIA通过融合空间插值梯度与频率感知扰动来指导对抗样本生成，显著增强了对AIGI检测器的迁移攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有AIGI检测器在干净条件下表现良好，但在对抗性和抗取证（antiforensics）场景下的鲁棒性尚未充分评估，需设计先进对抗攻击以评估检测器安全性。

Method: DuFIA采用空间插值梯度捕获空间域重要特征，并利用频率感知扰动获取频率域重要特征。将两者融合后作为指导，采用优化方法生成对抗样本以提高对不同检测器的迁移性。

Result: 在大量实验中，DuFIA在不同AIGI检测器上表现出较好的跨模型迁移能力，同时保持较高的视觉透明性和对抗鲁棒性。

Conclusion: 本文提出的DuFIA通过联合建模空间和频率域重要特征来生成对抗样本，从而在一定程度上使AIGI检测器失效，验证了其跨模型迁移性、透明性和鲁棒性。

Abstract: Recent AI-generated image (AIGI) detectors achieve impressive accuracy under clean condition. In view of antiforensics, it is significant to develop advanced adversarial attacks for evaluating the security of such detectors, which remains unexplored sufficiently. This letter proposes a Dual-domain Feature Importance Attack (DuFIA) scheme to invalidate AIGI detectors to some extent. Forensically important features are captured by the spatially interpolated gradient and frequency-aware perturbation. The adversarial transferability is enhanced by jointly modeling spatial and frequency-domain feature importances, which are fused to guide the optimization-based adversarial example generation. Extensive experiments across various AIGI detectors verify the cross-model transferability, transparency and robustness of DuFIA.

</details>


### [81] [From Low-Rank Features to Encoding Mismatch: Rethinking Feature Distillation in Vision Transformers](https://arxiv.org/abs/2511.15572)
*Huiyuan Tian,Bonan Xu,Shijian Li,Xin Jin*

Main category: cs.CV

TL;DR: 全局低秩并不意味着 token 级别低秩；token 在大多数通道上分配能量导致宽窄模型不匹配。解决办法是对齐宽度或临时升维，从而使简单的特征图蒸馏在 ViT 上重获效力。


<details>
  <summary>Details</summary>
Motivation: 理解为何现有的特征图知识蒸馏在 ViT 上表现不佳，并据此设计简洁有效的修正方法。

Method: 先对每层完整特征矩阵做 SVD，发现最终层全局低秩；再做 token 级别的谱能量模式（SEP）分析，测量每个 token 在通道维上的能量分配，揭示高带宽编码；基于此提出两种策略：后置特征升维（推理保留轻量投影器）与原生宽度对齐（仅把学生最后一块拓宽到教师宽度），并在 ImageNet-1K 上验证。

Result: 在 ImageNet-1K 上，使用所提策略可将 DeiT-Tiny 在从 CaiT-S24 蒸馏时的准确率从 74.86% 提升到 77.53%（后置投影）与 78.23%（最后块拓宽）；同时也能提高无教师单独训练学生的性能。

Conclusion: ViT 特征蒸馏失败源于宽教师与窄学生之间的通道使用不匹配；尽管全局表示呈低秩，单个 token 在大多数通道上分布能量，形成高带宽编码。通过修复匹配问题（轻量投影器或最后块原生拓宽）可恢复特征图蒸馏效果并提升学生性能。

Abstract: Feature-map knowledge distillation (KD) is highly effective for convolutional networks but often fails for Vision Transformers (ViTs). To understand this failure and guide method design, we conduct a two-view representation analysis of ViTs. First, a layer-wise Singular Value Decomposition (SVD) of full feature matrices shows that final-layer representations are globally low-rank: for CaiT-S24, only $121/61/34/14$ dimensions suffice to capture $99\%/95\%/90\%/80\%$ of the energy. In principle, this suggests that a compact student plus a simple linear projector should be enough for feature alignment, contradicting the weak empirical performance of standard feature KD. To resolve this paradox, we introduce a token-level Spectral Energy Pattern (SEP) analysis that measures how each token uses channel capacity. SEP reveals that, despite the global low-rank structure, individual tokens distribute energy over most channels, forming a high-bandwidth encoding pattern. This results in an encoding mismatch between wide teachers and narrow students. Motivated by this insight, we propose two minimal, mismatch-driven strategies: (1) post-hoc feature lifting with a lightweight projector retained during inference, or (2) native width alignment that widens only the student's last block to the teacher's width. On ImageNet-1K, these strategies reactivate simple feature-map distillation in ViTs, raising DeiT-Tiny accuracy from $74.86\%$ to $77.53\%$ and $78.23\%$ when distilling from CaiT-S24, while also improving standalone students trained without any teacher. Our analysis thus explains why ViT feature distillation fails and shows how exploiting low-rank structure yields effective, interpretable remedies and concrete design guidance for compact ViTs.

</details>


### [82] [AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning](https://arxiv.org/abs/2511.15578)
*Urjitkumar Patel,Fang-Chun Yeh,Chinmay Gondhalekar*

Main category: cs.CV

TL;DR: AVATAAR通过模块化设计与反馈回路，将全局摘要与局部检索结合，显著提升长视频问答的准确性与可解释性，且在CinePile基准上取得多项显著增益。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型在长视频的细粒度与多步推理问题上表现不足，尤其涉及时序、技术细节与主题/叙事理解时。需要一种可解释且可扩展的方法来模拟人类迭代检索與反思过程以提升性能。

Method: 构建模块化框架：1) 全局摘要模块维持持久视频摘要；2) 局部检索结合预检索思考代理生成检索策略；3) 重思模块根据部分答案与全局摘要反馈调整检索和推理；4) 将局部与全局信息融合并由LVLM生成最终答案。

Result: 在CinePile基准上，AVATAAR相较基线在若干维度显著提升：时序推理+5.6%、技术问题+5%、主题问题+8%、叙事理解+8.2%。消融实验显示每个模块均带来正向贡献，反馈回路对适应性至关重要。

Conclusion: AVATAAR通过结合全局与局部视频上下文、预检索思考代理（Pre Retrieval Thinking Agent）与重思模块（Rethink Module），并引入全局持久摘要与反馈回路，实现了对长视频更准确和可解释的问答能力。

Abstract: With the increasing prevalence of video content, effectively understanding and answering questions about long form videos has become essential for numerous applications. Although large vision language models (LVLMs) have enhanced performance, they often face challenges with nuanced queries that demand both a comprehensive understanding and detailed analysis. To overcome these obstacles, we introduce AVATAAR, a modular and interpretable framework that combines global and local video context, along with a Pre Retrieval Thinking Agent and a Rethink Module. AVATAAR creates a persistent global summary and establishes a feedback loop between the Rethink Module and the Pre Retrieval Thinking Agent, allowing the system to refine its retrieval strategies based on partial answers and replicate human-like iterative reasoning. On the CinePile benchmark, AVATAAR demonstrates significant improvements over a baseline, achieving relative gains of +5.6% in temporal reasoning, +5% in technical queries, +8% in theme-based questions, and +8.2% in narrative comprehension. Our experiments confirm that each module contributes positively to the overall performance, with the feedback loop being crucial for adaptability. These findings highlight AVATAAR's effectiveness in enhancing video understanding capabilities. Ultimately, AVATAAR presents a scalable solution for long-form Video Question Answering (QA), merging accuracy, interpretability, and extensibility.

</details>


### [83] [CompTrack: Information Bottleneck-Guided Low-Rank Dynamic Token Compression for Point Cloud Tracking](https://arxiv.org/abs/2511.15580)
*Sifan Zhou,Yichao Cao,Jiahao Nie,Yuqian Fu,Ziyu Zhao,Xiaobo Lu,Shuo Wang*

Main category: cs.CV

TL;DR: CompTrack通过信息熵过滤背景和基于信息瓶颈的在线SVD前景压缩，有效去冗余，兼顾高精度与90FPS高效跟踪。


<details>
  <summary>Details</summary>
Motivation: 点云稀疏性带来双重冗余：背景的空间冗余和前景的内部信息冗余，影响跟踪精度与实时性。

Method: 提出了Spatial Foreground Predictor(SFP)基于信息熵筛除背景，并设计Information Bottleneck-guided Dynamic Token Compression(IB-DTC)模块，结合在线SVD进行自适应前景代理token压缩，理论上基于低秩近似。

Result: 在KITTI、nuScenes和Waymo上达到领先的跟踪性能，且在RTX 3090上实现90 FPS实时运行。

Conclusion: CompTrack通过先去除背景噪声再压缩前景信息冗余，在点云3D单目标跟踪中兼顾了精度与效率，证明了信息瓶颈与低秩近似在该任务中的有效性。

Abstract: 3D single object tracking (SOT) in LiDAR point clouds is a critical task in computer vision and autonomous driving. Despite great success having been achieved, the inherent sparsity of point clouds introduces a dual-redundancy challenge that limits existing trackers: (1) vast spatial redundancy from background noise impairs accuracy, and (2) informational redundancy within the foreground hinders efficiency. To tackle these issues, we propose CompTrack, a novel end-to-end framework that systematically eliminates both forms of redundancy in point clouds. First, CompTrack incorporates a Spatial Foreground Predictor (SFP) module to filter out irrelevant background noise based on information entropy, addressing spatial redundancy. Subsequently, its core is an Information Bottleneck-guided Dynamic Token Compression (IB-DTC) module that eliminates the informational redundancy within the foreground. Theoretically grounded in low-rank approximation, this module leverages an online SVD analysis to adaptively compress the redundant foreground into a compact and highly informative set of proxy tokens. Extensive experiments on KITTI, nuScenes and Waymo datasets demonstrate that CompTrack achieves top-performing tracking performance with superior efficiency, running at a real-time 90 FPS on a single RTX 3090 GPU.

</details>


### [84] [Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition](https://arxiv.org/abs/2511.15597)
*Xufei Wang,Junqiao Zhao,Siyue Tao,Qiwen Gu,Wonbong Kim,Tiantian Feng*

Main category: cs.CV

TL;DR: 提出KDF+：通过损失感知的重放采样和复习增强，提升LiDAR地点识别的持续学习性能，减少灾难性遗忘并在多个基准上取得更好结果。


<details>
  <summary>Details</summary>
Motivation: LiDAR地点识别需要在不同环境中持续学习，但现有方法在新任务学习时往往遗忘先前知识（灾难性遗忘）；因此需要一种能在保留旧知识的同时有效学习新环境的持续学习框架。

Method: 在KDF基础上增加两部分：1) 损失感知采样（loss-aware sampling）：估计每个样本的学习难度（以损失为指标），按照困难程度有偏抽样，同时保持整体分布覆盖；2) 复习增强机制（rehearsal enhancement）：在新任务训练时对记忆样本施加轻微的损失降低约束，以强化对旧知识的记忆。

Result: 在多个基准数据集上进行广泛实验，结果显示KDF+持续稳定优于现有的持续学习方法，并能与当前最先进的方法结合带来显著性能提升。

Conclusion: KDF+通过引入基于损失的采样策略和复习增强机制，有效缓解了LiDAR地点识别的灾难性遗忘问题，在多个基准上稳定优于现有的持续学习方法，并可与现有框架无缝集成以提升性能。

Abstract: LiDAR place recognition plays a crucial role in SLAM, robot navigation, and autonomous driving. However, existing LiDAR place recognition methods often struggle to adapt to new environments without forgetting previously learned knowledge, a challenge widely known as catastrophic forgetting. To address this issue, we propose KDF+, a novel continual learning framework for LiDAR place recognition that extends the KDF paradigm with a loss-aware sampling strategy and a rehearsal enhancement mechanism. The proposed sampling strategy estimates the learning difficulty of each sample via its loss value and selects samples for replay according to their estimated difficulty. Harder samples, which tend to encode more discriminative information, are sampled with higher probability while maintaining distributional coverage across the dataset. In addition, the rehearsal enhancement mechanism encourages memory samples to be further refined during new-task training by slightly reducing their loss relative to previous tasks, thereby reinforcing long-term knowledge retention. Extensive experiments across multiple benchmarks demonstrate that KDF+ consistently outperforms existing continual learning methods and can be seamlessly integrated into state-of-the-art continual learning for LiDAR place recognition frameworks to yield significant and stable performance gains. The code will be available at https://github.com/repo/KDF-plus.

</details>


### [85] [US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery](https://arxiv.org/abs/2511.15600)
*Miruna-Alexandra Gafencu,Yordanka Velikova,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: 利用单张X光投影与部分可视3D超声输入，多模态深度学习可以显著改善被声影遮挡的椎体三维重建，实现更完整的术中腰椎可视化。


<details>
  <summary>Details</summary>
Motivation: 超声在术中成像无辐射、实时便宜，但受骨产生的声影导致椎体体积结构难以完整成像；利用X光投影的互补信息来恢复超声中被遮挡的骨性解剖结构。

Method: 构建配对训练数据：模拟2D侧位X光投影与对应的3D部分椎体（模拟超声可视性和遮挡），并设计融合两模态形态信息的深度网络进行空间补全，最后在体模（phantom）实验中评估重建精度并与现有方法比较。

Result: 在体模研究中，所提方法在椎体重建上显著优于现有方法（p<0.001），可直接在超声图像上叠加更完整的腰椎体积可视化，无需与术前CT配准，代码与数据已公开。

Conclusion: 本文提出了一种通过单张X光投影来补全3D超声中被遮挡的椎体结构的多模态深度学习方法，验证显示该方法显著优于现有的3D超声椎体补全技术。

Abstract: Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete

</details>


### [86] [MaskMed: Decoupled Mask and Class Prediction for Medical Image Segmentation](https://arxiv.org/abs/2511.15603)
*Bin Xie,Gady Agam*

Main category: cs.CV

TL;DR: 将多类分割解耦为类无关掩码与类别预测，并通过全尺度可变形Transformer实现高效对齐融合，显著提升医学分割性能（AMOS +2.0% Dice，BTCV +6.9% Dice）。


<details>
  <summary>Details</summary>
Motivation: 传统逐点卷积分割头将输出通道与特定类别一一绑定，限制了特征共享与语义泛化，且多尺度融合在内存与空间对齐上存在挑战。为提高泛化与效率，作者提出了解耦设计与可变形注意力的全尺度融合模块。

Method: 提出了一个解耦分割头（分为掩码预测和类别预测）与Full-Scale Aware Deformable Transformer模块。解耦分割头使用共享对象查询来同时生成类无关的掩码和对应类别概率；可变形Transformer在低分辨率编码器特征上，通过可变形注意力跨越高分辨率特征进行注意力计算，实现内存高效且空间对齐的全尺度融合。

Result: 在两个医学分割基准上取得了显著提升：在AMOS 2022上相比nnUNet提升+2.0% Dice，在BTCV上提升+6.9% Dice，达到或接近最新的SOTA水平。

Conclusion: 该论文通过将多类分割任务解耦为类无关的掩码预测和类别标签预测，并使用共享的对象查询与全尺度感知可变形Transformer，实现了更灵活的特征共享与空间对齐融合，从而提升了医学图像分割性能。

Abstract: Medical image segmentation typically adopts a point-wise convolutional segmentation head to predict dense labels, where each output channel is heuristically tied to a specific class. This rigid design limits both feature sharing and semantic generalization. In this work, we propose a unified decoupled segmentation head that separates multi-class prediction into class-agnostic mask prediction and class label prediction using shared object queries. Furthermore, we introduce a Full-Scale Aware Deformable Transformer module that enables low-resolution encoder features to attend across full-resolution encoder features via deformable attention, achieving memory-efficient and spatially aligned full-scale fusion. Our proposed method, named MaskMed, achieves state-of-the-art performance, surpassing nnUNet by +2.0% Dice on AMOS 2022 and +6.9% Dice on BTCV.

</details>


### [87] [When to Think and When to Look: Uncertainty-Guided Lookback](https://arxiv.org/abs/2511.15613)
*Jing Bi,Filippos Bellos,Junjia Guo,Yayuan Li,Chao Huang,Yunlong,Tang,Luchuan Song,Susan Liang,Zhongfei,Zhang,Jason J. Corso,Chenliang Xu*

Main category: cs.CV

TL;DR: Systematic study shows unbounded chain-of-thought can hurt LVLM visual reasoning; short image-referential lookbacks are key; a training-free uncertainty-guided lookback decoding yields robust gains and generalizes.


<details>
  <summary>Details</summary>
Motivation: No systematic analysis existed on how test-time thinking affects visual reasoning in LVLMs; to understand when and why thinking helps or harms and to design better decoding strategies.

Method: Large-scale controlled comparison of ten LVLM variants (InternVL3.5, Qwen3-VL) on MMMU-val with generous token budgets and multi-pass decoding; analysis of trajectories to find lookback phrases; propose training-free uncertainty-guided lookback combining uncertainty signals, adaptive lookback prompts, and breadth search; evaluate on MMMU and five additional benchmarks.

Result: Found long chains often produce incorrect, image-ignoring trajectories; short lookback phrases correlate with success and grounding; uncertainty-guided lookback improves MMMU performance, especially where standard thinking is weak, outperforms strong baselines, sets new SOTA under fixed families/token budgets, and generalizes across five other benchmarks including math-visual datasets.

Conclusion: Thinking (test-time chain generation) helps LVLMs but more thinking can hurt; targeted short lookbacks improve grounding; uncertainty-guided lookback decoding improves performance and generalizes.

Abstract: Test-time thinking (that is, generating explicit intermediate reasoning chains) is known to boost performance in large language models and has recently shown strong gains for large vision language models (LVLMs). However, despite these promising results, there is still no systematic analysis of how thinking actually affects visual reasoning. We provide the first such analysis with a large scale, controlled comparison of thinking for LVLMs, evaluating ten variants from the InternVL3.5 and Qwen3-VL families on MMMU-val under generous token budgets and multi pass decoding. We show that more thinking is not always better; long chains often yield long wrong trajectories that ignore the image and underperform the same models run in standard instruct mode. A deeper analysis reveals that certain short lookback phrases, which explicitly refer back to the image, are strongly enriched in successful trajectories and correlate with better visual grounding. Building on this insight, we propose uncertainty guided lookback, a training free decoding strategy that combines an uncertainty signal with adaptive lookback prompts and breadth search. Our method improves overall MMMU performance, delivers the largest gains in categories where standard thinking is weak, and outperforms several strong decoding baselines, setting a new state of the art under fixed model families and token budgets. We further show that this decoding strategy generalizes, yielding consistent improvements on five additional benchmarks, including two broad multimodal suites and math focused visual reasoning datasets.

</details>


### [88] [FlashMesh: Faster and Better Autoregressive Mesh Synthesis via Structured Speculation](https://arxiv.org/abs/2511.15618)
*Tingrui Shen,Yiheng Zhang,Chen Tang,Chuan Ping,Zixing Zhao,Le Wan,Yuwang Wang,Ronggang Wang,Shengfeng He*

Main category: cs.CV

TL;DR: FlashMesh通过利用网格的结构先验和投机性并行解码，在hourglass transformer上实现2x加速并提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 自回归生成网格虽能产生高质量结果，但逐令牌解码导致推理缓慢，限制了在交互和大规模场景中的应用。作者观察到网格令牌存在结构和几何上的强相关性，可用于多令牌并行预测以加速解码。

Method: 针对常见的hourglass transformer架构，设计了一种适配多级（面、点、坐标）并行预测的投机性解码方案，并结合纠正与验证模块来修正潜在错误。

Result: 在广泛实验中，FlashMesh在保持或提升生成保真度的同时，实现了最多2倍的加速。

Conclusion: 该论文提出了一种名为FlashMesh的快速高保真网格生成框架，通过在自回归解码中引入预测-纠正-验证范式，实现了显著加速与质量提升。

Abstract: Autoregressive models can generate high-quality 3D meshes by sequentially producing vertices and faces, but their token-by-token decoding results in slow inference, limiting practical use in interactive and large-scale applications. We present FlashMesh, a fast and high-fidelity mesh generation framework that rethinks autoregressive decoding through a predict-correct-verify paradigm. The key insight is that mesh tokens exhibit strong structural and geometric correlations that enable confident multi-token speculation. FlashMesh leverages this by introducing a speculative decoding scheme tailored to the commonly used hourglass transformer architecture, enabling parallel prediction across face, point, and coordinate levels. Extensive experiments show that FlashMesh achieves up to a 2 x speedup over standard autoregressive models while also improving generation fidelity. Our results demonstrate that structural priors in mesh data can be systematically harnessed to accelerate and enhance autoregressive generation.

</details>


### [89] [The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification](https://arxiv.org/abs/2511.15622)
*Dante Francisco Wasmuht,Otto Brookes,Maximillian Schall,Pablo Palencia,Chris Beirne,Tilo Burghardt,Majid Mirmehdi,Hjalmar Kühl,Mimi Arandjelovic,Sam Pottie,Peter Bermant,Brandon Asheim,Yi Jin Toh,Adam Elzinga,Jason Holmberg,Andrew Whitworth,Eleanor Flatt,Laura Gustafson,Chaitanya Ryali,Yuan-Ting Hu,Baishan Guo,Andrew Westbury,Kate Saenko,Didac Suris*

Main category: cs.CV

TL;DR: SA-FARI是迄今最大且物种多样、跨地区的野生动物多动物追踪数据集，含大量密集注释与基准测试，旨在推动通用MAT模型的研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模小、物种覆盖和时空多样性不足，无法训练或评估可在野外不同物种与地区泛化的多动物追踪模型，因此需要一个更大、更广、更高质量的开源基准。

Method: 收集并整理来自741个摄像点、共11609段摄像头视频（2014-2024）的数据；对每段视频进行密集标注，生成个体边界框、分割掩码与物种标签；发布匿名化摄像机位置信息；在数据集上以先进的视觉-语言模型（含SAM-3）及专门的野生动物视觉方法进行检测与追踪基准测试，比较物种特定与通用提示的效果。

Result: SA-FARI包含11609段视频、约46小时密集注释视频、16224个masklet身份与942,702个个体边界框/分割掩码/物种标签；提供匿名摄像机位置信息；并给出若干基线结果，显示视觉-语言模型与野生动物专用方法在该大规模多样化数据集上的性能差异。

Conclusion: SA-FARI为多动物追踪研究提供了首个大规模、跨物种、跨区域的公开基准数据集，其高质量的时空注释和大样本量将显著推动通用野生动物MAT模型的发展。

Abstract: Automated video analysis is critical for wildlife conservation. A foundational task in this domain is multi-animal tracking (MAT), which underpins applications such as individual re-identification and behavior recognition. However, existing datasets are limited in scale, constrained to a few species, or lack sufficient temporal and geographical diversity - leaving no suitable benchmark for training general-purpose MAT models applicable across wild animal populations. To address this, we introduce SA-FARI, the largest open-source MAT dataset for wild animals. It comprises 11,609 camera trap videos collected over approximately 10 years (2014-2024) from 741 locations across 4 continents, spanning 99 species categories. Each video is exhaustively annotated culminating in ~46 hours of densely annotated footage containing 16,224 masklet identities and 942,702 individual bounding boxes, segmentation masks, and species labels. Alongside the task-specific annotations, we publish anonymized camera trap locations for each video. Finally, we present comprehensive benchmarks on SA-FARI using state-of-the-art vision-language models for detection and tracking, including SAM 3, evaluated with both species-specific and generic animal prompts. We also compare against vision-only methods developed specifically for wildlife analysis. SA-FARI is the first large-scale dataset to combine high species diversity, multi-region coverage, and high-quality spatio-temporal annotations, offering a new foundation for advancing generalizable multianimal tracking in the wild. The dataset is available at $\href{https://www.conservationxlabs.com/sa-fari}{\text{conservationxlabs.com/SA-FARI}}$.

</details>


### [90] [Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning](https://arxiv.org/abs/2511.15633)
*Tao Hu,Lan Li,Zhen-Hao Xie,Da-Wei Zhou*

Main category: cs.CV

TL;DR: HASTEN用外部知识图谱把视觉-文本特征嵌入到双曲空间并对梯度做零空间投影，以锚定层级语义，显著减少CLIP基类增量学习中的遗忘。


<details>
  <summary>Details</summary>
Motivation: 动机是现实世界中视觉与语言概念具有固有的层级性（如“狗”包含“拉布拉多”等细粒度类别），而现有基于CLIP的CIL方法未显式建模这种层级关系，导致细粒度类特征在增量更新时发生漂移并造成遗忘。

Method: 方法包括两部分：1) 利用外部知识图谱监督，将视觉和文本特征嵌入到双曲空间，从而保留概念的层级结构；2) 为缓解遗忘，将梯度投影到共享双曲映射器的零空间，避免更新干扰先前任务。两者协同维护层级关系并减少特征漂移。

Result: 实验显示HASTEN在多个基准上持续优于现有方法，同时提供了统一的结构化表示，验证了保留层级信息与梯度投影对减少灾难性遗忘的有效性。

Conclusion: 该论文提出HASTEN，通过将层级语义信息锚定到类增量学习中以减少灾难性遗忘，证明了在CLIP类视觉-语言预训练模型上引入层级结构可以有效提升持续学习性能。

Abstract: Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.

</details>


### [91] [Multi-Stage Residual-Aware Unsupervised Deep Learning Framework for Consistent Ultrasound Strain Elastography](https://arxiv.org/abs/2511.15640)
*Shourov Joarder,Tushar Talukder Showrav,Md. Kamrul Hasan*

Main category: cs.CV

TL;DR: 提出一种残差感知、多阶段无监督网络（MUSSE-Net）改善超声应变成像的鲁棒性和一致性，通过多流编码-解码、注意力模块和一致性损失并辅以残差精炼，在仿真及临床数据上显著提升噪声抑制与病灶可见性。


<details>
  <summary>Details</summary>
Motivation: 现有超声应变弹性成像受组织相关退相干噪声、缺乏真实标注以及不同形变条件下估计结果不一致的限制，需一种能在无监督条件下提高鲁棒性和稳定性的深度学习方法。

Method: 提出 USSE-Net 主干：多流编码器-解码器并行处理形变前后 RF 序列，采用 CACFF 编码器、TCA 瓶颈、CAF 顺序解码器；引入定制一致性损失保证不同形变水平下的时间相干性；在 MUSSE-Net 框架下增加第二阶段残差精炼以提高精度并抑制噪声。

Result: 在仿真、体内及 BUET 临床数据集上广泛验证，MUSSE-Net 在仿真数据上达成目标 SNR 24.54、背景 SNR 132.76、CNR 59.81、弹性影像 SNR 9.73；在 BUET 数据上显著提高病灶-背景对比并抑制噪声，使应变图具有临床可解释性，优于现有无监督方法。

Conclusion: MUSSE-Net 提出并验证了一种残差感知、多阶段的无监督序列深度学习框架，显著提升超声应变弹性成像的位移与轴向应变估计的鲁棒性与一致性，特别在噪声抑制和病灶-背景对比方面优于现有无监督方法。

Abstract: Ultrasound Strain Elastography (USE) is a powerful non-invasive imaging technique for assessing tissue mechanical properties, offering crucial diagnostic value across diverse clinical applications. However, its clinical application remains limited by tissue decorrelation noise, scarcity of ground truth, and inconsistent strain estimation under different deformation conditions. Overcoming these barriers, we propose MUSSE-Net, a residual-aware, multi-stage unsupervised sequential deep learning framework designed for robust and consistent strain estimation. At its backbone lies our proposed USSE-Net, an end-to-end multi-stream encoder-decoder architecture that parallelly processes pre- and post-deformation RF sequences to estimate displacement fields and axial strains. The novel architecture incorporates Context-Aware Complementary Feature Fusion (CACFF)-based encoder with Tri-Cross Attention (TCA) bottleneck with a Cross-Attentive Fusion (CAF)-based sequential decoder. To ensure temporal coherence and strain stability across varying deformation levels, this architecture leverages a tailored consistency loss. Finally, with the MUSSE-Net framework, a secondary residual refinement stage further enhances accuracy and suppresses noise. Extensive validation on simulation, in vivo, and private clinical datasets from Bangladesh University of Engineering and Technology (BUET) medical center, demonstrates MUSSE-Net's outperformed existing unsupervised approaches. On MUSSE-Net achieves state-of-the-art performance with a target SNR of 24.54, background SNR of 132.76, CNR of 59.81, and elastographic SNR of 9.73 on simulation data. In particular, on the BUET dataset, MUSSE-Net produces strain maps with enhanced lesion-to-background contrast and significant noise suppression yielding clinically interpretable strain patterns.

</details>


### [92] [MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling](https://arxiv.org/abs/2511.15645)
*Shanshan Zhang*

Main category: cs.CV

TL;DR: 本文质疑全局坐标系在行人惯性里程计中的普适性，提出基于Laplacian金字塔的频带分解与Mamba+卷积混合网络（MambaIO），在多数据集上实现SOTA定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有惯性里程计多采用全局坐标系，但有研究在无人机场景显示机体坐标系能显著提高精度，因而需要重新评估全局坐标系在行人场景下的适用性并改进模型以提升定位性能。

Method: 理论分析、定性检视与定量实验评估全局坐标系在行人IO中的有效性；提出基于Laplacian金字塔的频带分解，将低频分量输入Mamba架构提取上下文运动线索，高频分量用卷积模块捕捉精细局部运动；在多公开数据集上进行对比实验。

Result: MambaIO在多数据集上显著降低定位误差，达到了最新SOTA性能；据称为Mamba架构首次应用于惯性里程计任务。

Conclusion: 本文认为全局坐标系在行人惯性里程计中并非始终优于机体坐标系，针对该问题提出了MambaIO方法，将IMU测量分解为低频和高频分量并分别用Mamba架构与卷积网络处理，从而提升定位精度。

Abstract: Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.

</details>


### [93] [INQUIRE-Search: A Framework for Interactive Discovery in Large-Scale Biodiversity Databases](https://arxiv.org/abs/2511.15656)
*Edward Vendrow,Julia Chae,Rupa Kurinchi-Vendhan,Isaac Eckert,Jazlynn Hall,Marta Jarzyna,Reymond Miyajima,Ruth Oliver,Laura Pollock,Lauren Schrack,Scott Yanco,Oisin Mac Aodha,Sara Beery*

Main category: cs.CV

TL;DR: INQUIRE-Search用自然语言搜索跨大规模生物图像库，快速发现与导出生态学感兴趣的图像与观测，显著提速并扩展了可研究的问题空间，同时提出了AI工具对科研流程与方法学的影响与挑战。


<details>
  <summary>Details</summary>
Motivation: 大型社区科学平台（如iNaturalist）积累了大量含生态上下文的图像，但这些图像中的行为、相互作用和物候等二级信息难以通过现有基于元数据或人工的方法规模化获取，限制了其科学价值的发挥。

Method: 构建一个自然语言驱动的检索系统，结合图像-文本模型与用户交互界面，允许用户用自然语言查询图像数据库、验证并导出相关观测数据，进而用于后续科学分析。通过实现原型并对比传统基于元数据过滤或人工检查的方法，展示该系统在时间效率和可扩展性上的优势。

Result: 通过五个案例研究（例如跨物种的季节行为变化、野火后森林再生等），展示了INQUIRE-Search在不同生态学问题上的适用性与效率提升。系统显著减少了检索和标注所需时间，解锁了此前难以规模化分析的科学问题。作者还讨论了AI发现工具对科学方法论的影响，呼吁专家重新设计实验、数据收集与不确定性分析方法。

Conclusion: 该论文提出并验证了INQUIRE-Search，一种基于自然语言的开源交互式搜索系统，能在大规模生物多样性图像库中高效发现与生态学研究相关的概念，从而显著加速数据发现与分析流程。

Abstract: Large community science platforms such as iNaturalist contain hundreds of millions of biodiversity images that often capture ecological context on behaviors, interactions, phenology, and habitat. Yet most ecological workflows rely on metadata filtering or manual inspection, leaving this secondary information inaccessible at scale. We introduce INQUIRE-Search, an open-source system that enables scientists to rapidly and interactively search within an ecological image database for specific concepts using natural language, verify and export relevant observations, and utilize this discovered data for novel scientific analysis. Compared to traditional methods, INQUIRE-Search takes a fraction of the time, opening up new possibilities for scientific questions that can be explored. Through five case studies, we show the diversity of scientific applications that a tool like INQUIRE-Search can support, from seasonal variation in behavior across species to forest regrowth after wildfires. These examples demonstrate a new paradigm for interactive, efficient, and scalable scientific discovery that can begin to unlock previously inaccessible scientific value in large-scale biodiversity datasets. Finally, we emphasize using such AI-enabled discovery tools for science call for experts to reframe the priorities of the scientific process and develop novel methods for experiment design, data collection, survey effort, and uncertainty analysis.

</details>


### [94] [GEO-Bench-2: From Performance to Capability, Rethinking Evaluation in Geospatial AI](https://arxiv.org/abs/2511.15658)
*Naomi Simumba,Nils Lehmann,Paolo Fraccaro,Hamed Alemohammad,Geeth De Mel,Salman Khan,Manil Maskey,Nicolas Longepe,Xiao Xiang Zhu,Hannah Kerner,Juan Bernabe-Moreno,Alexander Lacoste*

Main category: cs.CV

TL;DR: GEO-Bench-2提出了覆盖19个数据集与五类任务的GeoFM评估框架，通过能力分组与统一协议揭示不同预训练策略在特定任务/模态上的优劣，表明单一通用GeoFM仍是未解决的问题。


<details>
  <summary>Details</summary>
Motivation: 当前GeoFM评估缺乏标准化协议与多模态、多任务的综合比较，导致难以判断模型在不同实际应用场景（如农业、灾害响应、高分辨率分析）中的优劣与改进方向。

Method: 收集19个可开放许可的数据集，覆盖分类、分割、回归、目标检测和实例分割五大任务，提出“能力（capability）”分组（按分辨率、波段、时序等特征划分数据集），并制定了既具规范性又灵活的评估协议以平衡公平比较与方法创新。通过对比多类预训练模型（自然图像预训练与EO专用预训练），在各能力组上进行系统化实验与排行榜构建。

Result: 实验结果表明：自然图像预训练模型（如ConvNeXt ImageNet、DINO V3）在高分辨率任务中表现优异；EO专用预训练模型（TerraMind、Prithvi、Clay）在多光谱应用中更适合；没有模型能覆盖所有能力组的最佳性能。GEO-Bench-2发布了代码、数据与排行榜，促进可复现评估与后续研究。

Conclusion: GEO-Bench-2构建了一个全面且可复现的GeoFM评估框架，证明目前没有单一模型能在所有地球观测任务上取得最佳表现，且不同预训练选择在特定能力组上表现差异明显。

Abstract: Geospatial Foundation Models (GeoFMs) are transforming Earth Observation (EO), but evaluation lacks standardized protocols. GEO-Bench-2 addresses this with a comprehensive framework spanning classification, segmentation, regression, object detection, and instance segmentation across 19 permissively-licensed datasets. We introduce ''capability'' groups to rank models on datasets that share common characteristics (e.g., resolution, bands, temporality). This enables users to identify which models excel in each capability and determine which areas need improvement in future work. To support both fair comparison and methodological innovation, we define a prescriptive yet flexible evaluation protocol. This not only ensures consistency in benchmarking but also facilitates research into model adaptation strategies, a key and open challenge in advancing GeoFMs for downstream tasks.
  Our experiments show that no single model dominates across all tasks, confirming the specificity of the choices made during architecture design and pretraining. While models pretrained on natural images (ConvNext ImageNet, DINO V3) excel on high-resolution tasks, EO-specific models (TerraMind, Prithvi, and Clay) outperform them on multispectral applications such as agriculture and disaster response. These findings demonstrate that optimal model choice depends on task requirements, data modalities, and constraints. This shows that the goal of a single GeoFM model that performs well across all tasks remains open for future research. GEO-Bench-2 enables informed, reproducible GeoFM evaluation tailored to specific use cases. Code, data, and leaderboard for GEO-Bench-2 are publicly released under a permissive license.

</details>


### [95] [VisPlay: Self-Evolving Vision-Language Models from Images](https://arxiv.org/abs/2511.15661)
*Yicheng He,Chengsong Huang,Zongxia Li,Jiaxin Huang,Yonghui Yang*

Main category: cs.CV

TL;DR: VisPlay 用两个自我对话的角色和 GRPO 无需人工标签生成高质量银级数据，能规模化提升 VLM 的视觉推理与减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖人工标注或任务特定启发式奖励，成本高且难以扩展；目标是利用大量无标签图像数据实现 VLM 的自主改进。

Method: 将单一基础 VLM 分配为两个角色：图像条件问答者负责生成具有挑战性且可回答的问题；多模态推理者给出银级答案。采用 Group Relative Policy Optimization (GRPO) 联合训练，设计多样性和难度奖励以平衡问题复杂度与答案质量。

Result: 在 Qwen2.5-VL 和 MiMo-VL 上训练时，VisPlay 在八个基准（含 MM-Vet、MMMU）上表现出一致提升，改善视觉推理、组合泛化及减少幻觉，展示了可扩展的自我进化多模态智能路径。

Conclusion: VisPlay 提出了一种无需人工标注、自主进化的强化学习框架，通过图像条件问答者与多模态推理者的协同训练，利用多样性和难度奖励生成高质量的“银级”训练数据，从而提升 VLM 的视觉推理能力与抗幻觉性。

Abstract: Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/

</details>


### [96] [MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features](https://arxiv.org/abs/2511.15675)
*Sejuti Rahman,Swakshar Deb,MD. Sameer Iqbal Chowdhury,MD. Jubair Ahmed Sourov,Mohammad Shamsuddin*

Main category: cs.CV

TL;DR: MF-GCN: a trimodal multi-frequency GCN leveraging low and high-frequency signals yields state-of-the-art depression detection performance and good generalization.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based models focus on low-frequency information and miss important high-frequency signals relevant for depression-related behavior captured in eye-tracking, audio, and video data.

Method: Introduces Multi-Frequency Graph Convolutional Network with a Multi-Frequency Filter Bank Module to extract and fuse low/high-frequency graph features across modalities; evaluated against traditional ML and deep models on binary and 3-class tasks and on CMDC.

Result: MF-GCN achieves binary sensitivity 0.96 and F2 0.94; 3-class sensitivity 0.79 and specificity 0.87; on CMDC sensitivity 0.95 and F2 0.96, outperforming other models.

Conclusion: This paper proposes MF-GCN, which uses a multi-frequency filter bank to incorporate both low and high-frequency graph signals for improved depression detection from trimodal (eye-tracking, audio, video) data. The model outperforms baselines and generalizes to CMDC dataset, achieving high sensitivity and F2 scores.

Abstract: Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.

</details>


### [97] [MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping](https://arxiv.org/abs/2511.15690)
*Yushi Huang,Zining Wang,Zhihang Yuan,Yifu Ding,Ruihao Gong,Jinyang Guo,Xianglong Liu,Jun Zhang*

Main category: cs.CV

TL;DR: MoDES为训练无关的自适应专家跳过框架，通过层级重要性调制的路由与双模态阈值策略，在大幅减少MoE推理开销的同时保持甚至提升多模态性能，并用快速的边界搜索确定最优阈值。


<details>
  <summary>Details</summary>
Motivation: 现有为单模态LLM设计的专家跳过方法直接用于多模态MoE模型时导致性能大幅下降，原因在于不同层的专家重要性异质性及不同模态token在层内的行为差异。需设计能兼顾层级与模态差异的跳过策略以在降低计算量同时保持模型性能。

Method: 提出GMLG（全局调制的局部门控）将层级重要性融入每-token的路由概率，结合DMT（双模态阈值）分别对视觉与文本token处理以生成跳过策略；使用基于单调性性质的边界搜索算法快速确定最佳阈值。整个框架为训练无关，可直接应用于已有MoE MLLMs。

Result: 在3个模型系列与13个基准上实验，MoDES在高达88%专家跳过率下，仍能显著提升性能（例如Qwen3-VL-MoE-30B在88%跳过时性能从86.66%提升到97.33%，提高10.67%）；推理速度亦大幅提升，prefill加速2.16×，decode加速1.26×。

Conclusion: MoDES在无需训练的前提下，通过引入全局调制的局部路由和双模态阈值策略，有效解决了将单模态专家跳跃方法直接应用于多模态MoE模型导致的性能下降问题，实现了在高跳过率下保持或提高性能，同时显著加速推理。

Abstract: Mixture-of-Experts (MoE) Multimodal large language models (MLLMs) excel at vision-language tasks, but they suffer from high computational inefficiency. To reduce inference overhead, expert skipping methods have been proposed to deactivate redundant experts based on the current input tokens. However, we find that applying these methods-originally designed for unimodal large language models (LLMs)-to MLLMs results in considerable performance degradation. This is primarily because such methods fail to account for the heterogeneous contributions of experts across MoE layers and modality-specific behaviors of tokens within these layers. Motivated by these findings, we propose MoDES, the first training-free framework that adaptively skips experts to enable efficient and accurate MoE MLLM inference. It incorporates a globally-modulated local gating (GMLG) mechanism that integrates global layer-wise importance into local routing probabilities to accurately estimate per-token expert importance. A dual-modality thresholding (DMT) method is then applied, which processes tokens from each modality separately, to derive the skipping schedule. To set the optimal thresholds, we introduce a frontier search algorithm that exploits monotonicity properties, cutting convergence time from several days to a few hours. Extensive experiments for 3 model series across 13 benchmarks demonstrate that MoDES far outperforms previous approaches. For instance, when skipping 88% experts for Qwen3-VL-MoE-30B-A3B-Instruct, the performance boost is up to 10.67% (97.33% vs. 86.66%). Furthermore, MoDES significantly enhances inference speed, improving the prefilling time by 2.16$\times$ and the decoding time by 1.26$\times$.

</details>


### [98] [Hyperspectral Image Classification using Spectral-Spatial Mixer Network](https://arxiv.org/abs/2511.15692)
*Mohammed Q. Alkhatib*

Main category: cs.CV

TL;DR: 提出SS-MixNet：轻量3D卷积+并行MLP-mixer+深度卷积注意力，在1%标注数据下实现了领先的HSI分类表现（Tangdaowan 95.68%，Qingyun 93.86%）。


<details>
  <summary>Details</summary>
Motivation: 在带标签样本极少的情况下，提高HSI分类的准确性与鲁棒性，同时保持模型轻量和低计算开销。

Method: 结合3D卷积用于局部光谱-空间特征提取、两个并行的MLP风格mixers分别建模光谱与空间长程依赖，并引入基于深度可分离卷积的轻量注意力机制提升判别能力。

Result: 在QUH-Tangdaowan和QUH-Qingyun上分别达到95.68%和93.86%总体精度，优于2D-CNN、3D-CNN、IP-SWIN、SimPoolFormer和HybridKAN，且通过定量指标和分类图展示了稳定性和准确性。

Conclusion: SS-MixNet在有限监督（1%带标签样本）下实现了在两个QUH数据集上的最高分类性能，证明了其在提取局部光谱-空间特征与捕捉长程依赖方面的有效性。

Abstract: This paper introduces SS-MixNet, a lightweight and effective deep learning model for hyperspectral image (HSI) classification. The architecture integrates 3D convolutional layers for local spectral-spatial feature extraction with two parallel MLP-style mixer blocks that capture long-range dependencies in spectral and spatial dimensions. A depthwise convolution-based attention mechanism is employed to enhance discriminative capability with minimal computational overhead. The model is evaluated on the QUH-Tangdaowan and QUH-Qingyun datasets using only 1% of labeled data for training and validation. SS-MixNet achieves the highest performance among compared methods, including 2D-CNN, 3D-CNN, IP-SWIN, SimPoolFormer, and HybridKAN, reaching 95.68% and 93.86% overall accuracy on the Tangdaowan and Qingyun datasets, respectively. The results, supported by quantitative metrics and classification maps, confirm the model's effectiveness in delivering accurate and robust predictions with limited supervision. The code will be made publicly available at: https://github.com/mqalkhatib/SS-MixNet

</details>


### [99] [First Frame Is the Place to Go for Video Content Customization](https://arxiv.org/abs/2511.15700)
*Jingxi Chen,Zongxia Li,Zhichao Liu,Guangyao Shi,Xiyang Wu,Fuxiao Liu,Cornelia Fermuller,Brandon Y. Feng,Yiannis Aloimonos*

Main category: cs.CV

TL;DR: First frame acts as conceptual memory; exploit this for efficient video content customization


<details>
  <summary>Details</summary>
Motivation: First frame is more than a seed; it's a memory store of visual entities reused later

Method: Analyze first-frame as memory buffer for video generation

Result: Achieve robust reference-based video customization with 20-50 examples, no architecture changes or large-scale finetuning

Conclusion: Video models can be adapted to reference-based customization via first-frame memory exploitation

Abstract: What role does the first frame play in video generation models? Traditionally, it's viewed as the spatial-temporal starting point of a video, merely a seed for subsequent animation. In this work, we reveal a fundamentally different perspective: video models implicitly treat the first frame as a conceptual memory buffer that stores visual entities for later reuse during generation. Leveraging this insight, we show that it's possible to achieve robust and generalized video content customization in diverse scenarios, using only 20-50 training examples without architectural changes or large-scale finetuning. This unveils a powerful, overlooked capability of video generation models for reference-based video customization.

</details>


### [100] [Think Visually, Reason Textually: Vision-Language Synergy in ARC](https://arxiv.org/abs/2511.15703)
*Beichen Zhang,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Haodong Duan,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 作者发现单纯将网格渲染为图像会降低性能，提出视觉与语言在不同推理阶段互补：视觉用于全局模式抽象与验证，语言用于符号规则生成与精确执行；提出VLSR与MSSC方法，并在ARC-AGI上取得提升。


<details>
  <summary>Details</summary>
Motivation: 当前大型模型在从少量示例中学习抽象规则方面表现不足，而人类在该类任务中依赖视觉抽象；因此结合视觉与语言可能弥补现有方法的不足。

Method: (1) Vision-Language Synergy Reasoning (VLSR)：将任务拆分为与模态对齐的子任务，视觉负责模式抽象与验证，语言负责规则生成与执行；(2) Modality-Switch Self-Correction (MSSC)：在推理过程中切换模态，用视觉验证语言推理的结果以进行自我纠错。

Result: 提出将视觉抽象与语言推理结合以提升ARC-AGI任务表现，核心贡献为VLSR与MSSC两种方法，并通过实验在多模型和多任务上取得最高4.33%提升。

Conclusion: 将视觉抽象与语言推理联合能改善从少量示例中推断结构化变换规则的能力，是向类似人类智能迈进的重要方向。

Abstract: Abstract reasoning from minimal examples remains a core unsolved problem for frontier foundation models such as GPT-5 and Grok 4. These models still fail to infer structured transformation rules from a handful of examples, which is a key hallmark of human intelligence. The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) provides a rigorous testbed for this capability, demanding conceptual rule induction and transfer to novel tasks. Most existing methods treat ARC-AGI as a purely textual reasoning task, overlooking the fact that humans rely heavily on visual abstraction when solving such puzzles. However, our pilot experiments reveal a paradox: naively rendering ARC-AGI grids as images degrades performance due to imprecise rule execution. This leads to our central hypothesis that vision and language possess complementary strengths across distinct reasoning stages: vision supports global pattern abstraction and verification, whereas language specializes in symbolic rule formulation and precise execution. Building on this insight, we introduce two synergistic strategies: (1) Vision-Language Synergy Reasoning (VLSR), which decomposes ARC-AGI into modality-aligned subtasks; and (2) Modality-Switch Self-Correction (MSSC), which leverages vision to verify text-based reasoning for intrinsic error correction. Extensive experiments demonstrate that our approach yields up to a 4.33% improvement over text-only baselines across diverse flagship models and multiple ARC-AGI tasks. Our findings suggest that unifying visual abstraction with linguistic reasoning is a crucial step toward achieving generalizable, human-like intelligence in future foundation models. Source code will be released soon.

</details>


### [101] [GeoVista: Web-Augmented Agentic Visual Reasoning for Geolocalization](https://arxiv.org/abs/2511.15705)
*Yikun Wang,Zuyan Liu,Ziyi Wang,Pengfei Liu,Han Hu,Yongming Rao*

Main category: cs.CV

TL;DR: 本工作提出GeoBench基准和GeoVista模型，针对地理定位任务扩展了工具化视觉推理能力，包含图像缩放和网络搜索工具，并通过SFT + 强化学习训练与分层奖励提升性能。实验显示GeoVista优于其他开源模型，在大多数指标上与闭源模型（如Gemini-2.5-flash, GPT-5）相当。


<details>
  <summary>Details</summary>
Motivation: 现有具代理性的视觉推理研究主要集中于图像编辑类工具，缺乏更通用的代理模型以及对需要网络搜索与高分辨率视觉推理的地理定位问题的评估。为弥补这一空白，提出新的基准与模型。

Method: 作者构建GeoBench数据集（包含照片、全景图和部分城市卫星图），提出GeoVista模型：在推理循环中可调用两个工具（图像缩放、网页搜索）；训练流程包括冷启动的监督微调学习工具使用与推理模式，随后用强化学习微调，采用分层奖励利用多级地理信息。

Result: 在GeoBench评测上，GeoVista显著优于其他开源代理模型，并在大多数指标上与Gemini-2.5-flash和GPT-5等闭源模型表现相当。

Conclusion: GeoVista通过在推理环中集成图像放大与网络搜索工具、结合SFT和RL训练及分层奖励机制，有效提升了地理定位能力，在GeoBench上取得显著领先并接近闭源最先进模型的表现。

Abstract: Current research on agentic visual reasoning enables deep multimodal understanding but primarily focuses on image manipulation tools, leaving a gap toward more general-purpose agentic models. In this work, we revisit the geolocalization task, which requires not only nuanced visual grounding but also web search to confirm or refine hypotheses during reasoning. Since existing geolocalization benchmarks fail to meet the need for high-resolution imagery and the localization challenge for deep agentic reasoning, we curate GeoBench, a benchmark that includes photos and panoramas from around the world, along with a subset of satellite images of different cities to rigorously evaluate the geolocalization ability of agentic models. We also propose GeoVista, an agentic model that seamlessly integrates tool invocation within the reasoning loop, including an image-zoom-in tool to magnify regions of interest and a web-search tool to retrieve related web information. We develop a complete training pipeline for it, including a cold-start supervised fine-tuning (SFT) stage to learn reasoning patterns and tool-use priors, followed by a reinforcement learning (RL) stage to further enhance reasoning ability. We adopt a hierarchical reward to leverage multi-level geographical information and improve overall geolocalization performance. Experimental results show that GeoVista surpasses other open-source agentic models on the geolocalization task greatly and achieves performance comparable to closed-source models such as Gemini-2.5-flash and GPT-5 on most metrics.

</details>


### [102] [RoMa v2: Harder Better Faster Denser Feature Matching](https://arxiv.org/abs/2511.15706)
*Johan Edstedt,David Nordström,Yushan Zhang,Georg Bökman,Jonathan Astermark,Viktor Larsson,Anders Heyden,Fredrik Kahl,Mårten Wadenbäck,Michael Felsberg*

Main category: cs.CV

TL;DR: 本文提出了一种改进的密集特征匹配器，通过新的匹配架构与损失、精心构建的训练数据分布、两阶段匹配-精炼流程、定制CUDA内核减少内存以及引入DINOv3等基础模型，显著提高了匹配精度与速度，达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有密集匹配方法在许多复杂真实场景仍然失败或表现差，高精度模型通常速度慢，限制了实用性，因此需要在准确性、鲁棒性和速度上同时改进。

Method: 提出了新的匹配架构与损失函数；使用多样化、经过策划的训练集；采用解耦的两阶段匹配然后精炼流程以加速训练；实现自定义CUDA内核以显著降低精炼阶段的内存占用；并引入DINOv3等预训练基础模型来增强鲁棒性和减少偏差。

Result: 在大量实验中，新方法显著优于先前方法，在精度和稳健性上都取得提升，并在多项基准上达到新的最佳结果；代码已开源。

Conclusion: 经过系统性改进后，所提出的匹配器在多个复杂真实场景中表现更鲁棒、精度更高且运行更快，确立了新的state-of-the-art。

Abstract: Dense feature matching aims to estimate all correspondences between two images of a 3D scene and has recently been established as the gold-standard due to its high accuracy and robustness. However, existing dense matchers still fail or perform poorly for many hard real-world scenarios, and high-precision models are often slow, limiting their applicability. In this paper, we attack these weaknesses on a wide front through a series of systematic improvements that together yield a significantly better model. In particular, we construct a novel matching architecture and loss, which, combined with a curated diverse training distribution, enables our model to solve many complex matching tasks. We further make training faster through a decoupled two-stage matching-then-refinement pipeline, and at the same time, significantly reduce refinement memory usage through a custom CUDA kernel. Finally, we leverage the recent DINOv3 foundation model along with multiple other insights to make the model more robust and unbiased. In our extensive set of experiments we show that the resulting novel matcher sets a new state-of-the-art, being significantly more accurate than its predecessors. Code is available at https://github.com/Parskatt/romav2

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [103] [Castle: Causal Cascade Updates in Relational Databases with Large Language Models](https://arxiv.org/abs/2511.14762)
*Yongye Su,Yucheng Zhang,Zeru Shi,Bruno Ribeiro,Elisa Bertino*

Main category: cs.DB

TL;DR: Castle 提出了一种模式仅依赖的 LLM 驱动级联 UPDATE 生成方法，通过任务分解与嵌套查询实现数据保密下的因果一致更新，实验表明有效。


<details>
  <summary>Details</summary>
Motivation: 现有 Text2SQL 以 SELECT 为主，忽视了 UPDATE 及其级联影响。传统 CASCADE UPDATE 对现代非规范化数据库不灵活，需一种动态、上下文感知的更新生成方法，同时又不能泄露表内容。

Method: 将 UPDATE 生成任务分解为子问题（divide-and-conquer），利用 LLM 的链式推理能力判断需直接更新的列并推断更新如何沿模式传播，使用嵌套查询和子结构保证数据机密性。

Result: 在真实世界的因果更新场景评估中，Castle 能生成准确的级联 SQL UPDATE，展示 LLM 在自动化数据库管理中解决复杂因果更新问题的推理能力。

Conclusion: Castle 构建了首个仅基于模式(schema-only)的级联更新生成框架，使大模型在不访问表数据的情况下生成多列、因果一致的 SQL UPDATE 语句。

Abstract: This work introduces Castle, the first framework for schema-only cascade update generation using large language models (LLMs). Despite recent advances in LLMs for Text2SQL code generation, existing approaches focus primarily on SELECT queries, neglecting the challenges of SQL update operations and their ripple effects. Traditional CASCADE UPDATE constraints are static and unsuitable for modern, denormalized databases, which demand dynamic, context-aware updates. Castle enables natural language instructions to trigger multi-column, causally consistent SQL UPDATE statements, without revealing table content to the model. By framing UPDATE SQL generation as a divide-and-conquer task with LLMs' reasoning capacity, Castle can determine not only which columns must be directly updated, but also how those updates propagate through the schema, causing cascading updates -- all via nested queries and substructures that ensure data confidentiality. We evaluate it on real-world causal update scenarios, demonstrating its ability to produce accurate SQL updates, and thereby highlighting the reasoning ability of LLMs in automated DBMS.

</details>


### [104] [BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer](https://arxiv.org/abs/2511.15090)
*Wenhan Yu,Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Lei Sha,Deguo Xia,Jizhou Huang*

Main category: cs.DB

TL;DR: 引入BBox DocVQA：一个带边界框定位的DocVQA大规模数据集及自动化构建管道，针对空间推理与证据定位提供标注并证明可用于提升VLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA数据集仅限页级，缺乏细粒度空间对齐标注，限制VLM可解释性与空间推理能力。

Method: 提出自动化构建管道Segment Judge and Generate：结合区域分割模型、VLM语义判定模型与VLM问答生成模型，并辅以人工校验。数据包含单/多区域与单/多页场景，所有QA均以显式边界框为grounding。

Result: 构建3.6K文档、32K QA对的边界框标注数据集；基准测试显示SOTA VLM在空间定位与推理上仍有不足；在该数据集上微调能显著提升定位与答案生成性能。

Conclusion: BBox DocVQA通过引入基于边界框的标注扩展了DocVQA的数据稀疏性，提升了空间推理与证据定位的能力。

Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

</details>


### [105] [B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index](https://arxiv.org/abs/2511.15557)
*Selim Furkan Tekin,Rajesh Bordawekar*

Main category: cs.DB

TL;DR: 提出B+ANN：一种块化+B+树的磁盘化ANN索引，提升局部性，减小缓存未命中与资源消耗，在召回与QPS上优于HNSW并显著快于DiskANN，同时支持不相似性查询。


<details>
  <summary>Details</summary>
Motivation: 当前主流VDB多采用HNSW，这带来内存设计限制、随机内存访问导致的缓存行为差、加速范围受限以及只支持相似度查询等问题；需要一种磁盘友好、支持更多查询类型且在性能与资源开销上更优的ANN索引。

Method: 先将输入向量按语义相似性分块，构建包含相似项的块；使用一种B+树变体同时在内存与磁盘上存储这些块；并引入混合的基于边与基于块的内存遍历策略以进行查询；实现磁盘友好的数据布局以提升时空局部性并减少缓存未命中。

Result: 实验显示B+ANN在召回率和QPS上均优于HNSW；通过改进局部性将缓存未命中减少约19.23%（相对提升），并在内存消耗与基于磁盘的构建时间上相比DiskANN减少约24倍，同时支持dissimilarity queries。

Conclusion: B+ANN提出了一种基于磁盘的ANN索引结构，通过块化数据与B+树变体结合，改善了HNSW的若干缺陷，实现了更好的召回率和查询吞吐量，并支持异构查询（如dissimilarity queries）；总体结论是B+ANN在性能、内存与构建时间上相较于HNSW和DiskANN具有明显优势。

Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

</details>


### [106] [A Decade of Systems for Human Data Interaction](https://arxiv.org/abs/2511.15585)
*Eugene Wu,Yiru Chen,Haneen Mohammed,Zezhou Huang*

Main category: cs.DB

TL;DR: 作者回顾其十年工作，主张将界面与数据系统联合设计以满足以可用性为中心的延迟、正确性和一致性需求，使HDI成为可靠互动AI应用的基础。


<details>
  <summary>Details</summary>
Motivation: 现有数据库和系统研究通常以查询语义为中心，未能充分解决因用户体验需求带来的实时性和一致性问题；因此需要将界面与系统联合设计，推动HDI作为互动、AI 驱动应用的基础。

Method: 综述性回顾了作者实验室十年的研究成果，聚焦于将界面设计与系统实现协同考虑的案例和方法，强调通过共同优化满足可用性驱动的延迟、正确性和一致性需求。

Result: 总结了多项系统与交互协同设计的成果，展示这些HDI系统如何支撑可靠的交互式AI应用，并提出研究方向，指出HDI系统是构建可用性优先应用的关键。

Conclusion: 本文认为人-数据交互（HDI）系统在延迟、正确性与一致性方面面临与传统数据管理不同的挑战，这些挑战源于可用性需求而非查询语义；接口与系统紧密耦合，需共同设计；系统与数据库理论也能推动新的交互与可视化设计。

Abstract: Human-data interaction (HDI) presents fundamentally different challenges from traditional data management. HDI systems must meet latency, correctness, and consistency needs that stem from usability rather than query semantics; failing to meet these expectations breaks the user experience. Moreover, interfaces and systems are tightly coupled; neither can easily be optimized in isolation, and effective solutions demand their co-design. This dependence also presents a research opportunity: rather than adapt systems to interface demands, systems innovations and database theory can also inspire new interaction and visualization designs. We survey a decade of our lab's work that embraces this coupling and argue that HDI systems are the foundation for reliable, interactive, AI-driven applications.

</details>


### [107] [Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs](https://arxiv.org/abs/2511.15623)
*Leopoldo Bertossi,Nina Pardal*

Main category: cs.DB

TL;DR: 研究‘充分解释’在数据库中的定义及其与修复和必要解释的关系，并给出计算复杂性结果。


<details>
  <summary>Details</summary>
Motivation: 现有工作多聚焦于Halpern-Pearl风格的因果解释与必要解释，作者提出研究‘充分解释’以丰富解释范式并连接数据库修复领域，期望为解释性查询提供新的视角。

Method: 基于Halpern和Pearl的因果模型，构建数据库情景下的充分解释定义，分析其与修复（repair）及必要解释的形式关系，并通过理论证明与复杂性分析得到计算结果。

Result: 建立了充分解释与数据库修复的形式对应、证明了与必要解释的包含与差异关系，并给出若干判定问题的复杂度（例如NP或更高）界定。

Conclusion: 本论文将‘充分解释’引入数据库因果性研究，展示其与数据库修复和必要解释的联系，并给出复杂性结果。

Abstract: The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.

</details>
