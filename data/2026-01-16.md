<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Diffusion-Driven Deceptive Patches: Adversarial Manipulation and Forensic Detection in Facial Identity Verification](https://arxiv.org/abs/2601.09806)
*Shahrzad Sayyafzadeh,Hongmei Chi,Shonda Bernadin*

Main category: cs.CV

TL;DR: 提出了一个结合FGSM、扩散模型与视觉-语言模型的端到端对抗补丁生成与评估管道，并给出检测方法与实验指标。


<details>
  <summary>Details</summary>
Motivation: 据称为了在面部生物识别系统中生成能避开识别的对抗补丁，并用于取证分析与安全测试。

Method: 使用FGSM攻击身份分类器生成初始对抗噪声；用扩散模型的反向扩散、高斯平滑与自适应亮度校正精炼补丁；将补丁合成到面部图像；用ViT-GPT2生成caption；用感知哈希与分割检测补丁并评估SSIM等指标。

Result: 提出了一个端到端管道：用FGSM生成对抗噪声，用扩散模型与反向扩散进行高斯平滑和自适应亮度校正以提高不可察觉性。将精炼后的补丁贴到面部图像上，使用ViT-GPT2生成身份语义描述，评估身份分类、caption结果及在身份验证与表情识别任务上的脆弱性。还用感知哈希与分割检测对抗补丁，报告SSIM=0.95。

Conclusion: 该管道能生成较为自然的对抗补丁、降低识别率、生成语义描述支持取证，并能通过感知哈希与分割检测和分析补丁（SSIM=0.95）。

Abstract: This work presents an end-to-end pipeline for generating, refining, and evaluating adversarial patches to compromise facial biometric systems, with applications in forensic analysis and security testing. We utilize FGSM to generate adversarial noise targeting an identity classifier and employ a diffusion model with reverse diffusion to enhance imperceptibility through Gaussian smoothing and adaptive brightness correction, thereby facilitating synthetic adversarial patch evasion. The refined patch is applied to facial images to test its ability to evade recognition systems while maintaining natural visual characteristics. A Vision Transformer (ViT)-GPT2 model generates captions to provide a semantic description of a person's identity for adversarial images, supporting forensic interpretation and documentation for identity evasion and recognition attacks. The pipeline evaluates changes in identity classification, captioning results, and vulnerabilities in facial identity verification and expression recognition under adversarial conditions. We further demonstrate effective detection and analysis of adversarial patches and adversarial samples using perceptual hashing and segmentation, achieving an SSIM of 0.95.

</details>


### [2] [LCF3D: A Robust and Real-Time Late-Cascade Fusion Framework for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2601.09812)
*Carlo Sgaravatti,Riccardo Pieroni,Matteo Corno,Sergio M. Savaresi,Luca Magri,Giacomo Boracchi*

Main category: cs.CV

TL;DR: 提出LCF3D，使用晚期融合匹配并去除误检的LiDAR检测，同时用级联融合为图像独有的检测生成3D棱柱来补回漏检，提升对行人、自行车等小目标的性能并改善域泛化。


<details>
  <summary>Details</summary>
Motivation: LiDAR检测易产生误检且对小目标或跨域传感器配置敏感，单一模态难以兼顾精度与召回，故通过结合RGB提供的视觉信息与LiDAR的深度精准性来互补提升检测性能。

Method: 先用RGB图像跑2D检测器和LiDAR点云跑3D检测器；对两者检测结果进行匹配，未匹配的LiDAR检测被过滤（晚期融合）；对未匹配的RGB检测在点云中生成3D棱柱/视锥（frustum）并送入LiDAR检测器以恢复漏检（级联融合）。

Result: LCF3D提出了一种结合2D图像检测器与3D LiDAR检测器的传感器融合框架，通过晚期融合与级联融合两大策略，既能过滤LiDAR误检，又能利用图像未检测到的目标生成3D候选，从而提高检测的完整性与准确性。

Conclusion: LCF3D通过将RGB 2D检测与LiDAR 3D检测结合为互补流程，显著降低LiDAR的误报并补偿漏检，尤其在小目标和跨域配置下提升明显，实验证明在KITTI和nuScenes上有显著改进。

Abstract: Accurately localizing 3D objects like pedestrians, cyclists, and other vehicles is essential in Autonomous Driving. To ensure high detection performance, Autonomous Vehicles complement RGB cameras with LiDAR sensors, but effectively combining these data sources for 3D object detection remains challenging. We propose LCF3D, a novel sensor fusion framework that combines a 2D object detector on RGB images with a 3D object detector on LiDAR point clouds. By leveraging multimodal fusion principles, we compensate for inaccuracies in the LiDAR object detection network. Our solution combines two key principles: (i) late fusion, to reduce LiDAR False Positives by matching LiDAR 3D detections with RGB 2D detections and filtering out unmatched LiDAR detections; and (ii) cascade fusion, to recover missed objects from LiDAR by generating new 3D frustum proposals corresponding to unmatched RGB detections. Experiments show that LCF3D is beneficial for domain generalization, as it turns out to be successful in handling different sensor configurations between training and testing domains. LCF3D achieves significant improvements over LiDAR-based methods, particularly for challenging categories like pedestrians and cyclists in the KITTI dataset, as well as motorcycles and bicycles in nuScenes. Code can be downloaded from: https://github.com/CarloSgaravatti/LCF3D.

</details>


### [3] [Explainable Deep Learning for Pediatric Pneumonia Detection in Chest X-Ray Images](https://arxiv.org/abs/2601.09814)
*Adil O. Khadidos,Aziida Nanyonga,Alaa O. Khadidos,Olfat M. Mirza,Mustafa Tahsin Yilmaz*

Main category: cs.CV

TL;DR: 在相同设置下，EfficientNet-B0优于DenseNet121，且结合Grad-CAM/LIME的解释性可视化支持其临床可信度。


<details>
  <summary>Details</summary>
Motivation: 儿童肺炎是全球主要致病原因之一，急需准确高效的辅助诊断工具。深度学习在胸片分析中前景良好，因此比较两种先进CNN以筛选适合临床部署的模型。

Method: 使用公开的5863张儿科胸片数据集，图像做归一化、重采样和数据增强。对DenseNet121与EfficientNet-B0进行ImageNet预训练权重微调，训练设置一致。评估指标包括准确率、F1、MCC和召回率，并用Grad-CAM与LIME实现可解释性可视化。

Result: EfficientNet-B0: 准确率84.6%，F1=0.8899，MCC=0.6849；DenseNet121: 准确率79.7%，F1=0.8597，MCC=0.5852。两模型召回率均>0.99，Grad-CAM与LIME显示对肺部病灶区域的关注一致。

Conclusion: EfficientNet-B0在本研究中表现更优，体现为更高的准确率、F1和MCC，同时计算效率更好，适合临床应用。解释性可视化（Grad-CAM、LIME）显示模型关注肺部相关区域，增强了可信度。

Abstract: Background: Pneumonia remains a leading cause of morbidity and mortality among children worldwide, emphasizing the need for accurate and efficient diagnostic support tools. Deep learning has shown strong potential in medical image analysis, particularly for chest X-ray interpretation. This study compares two state-of-the-art convolutional neural network (CNN) architectures for automated pediatric pneumonia detection. Methods: A publicly available dataset of 5,863 pediatric chest X-ray images was used. Images were preprocessed through normalization, resizing, and data augmentation to enhance generalization. DenseNet121 and EfficientNet-B0 were fine-tuned using pretrained ImageNet weights under identical training settings. Performance was evaluated using accuracy, F1-score, Matthews Correlation Coefficient (MCC), and recall. Model explainability was incorporated using Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME) to visualize image regions influencing predictions. Results: EfficientNet-B0 outperformed DenseNet121, achieving an accuracy of 84.6%, F1-score of 0.8899, and MCC of 0.6849. DenseNet121 achieved 79.7% accuracy, an F1-score of 0.8597, and MCC of 0.5852. Both models demonstrated high recall values above 0.99, indicating strong sensitivity to pneumonia detection. Grad-CAM and LIME visualizations showed consistent focus on clinically relevant lung regions, supporting the reliability of model decisions. Conclusions: EfficientNet-B0 provided a more balanced and computationally efficient performance compared to DenseNet121, making it a strong candidate for clinical deployment. The integration of explainability techniques enhances transparency and trustworthiness in AI-assisted pediatric pneumonia diagnosis.

</details>


### [4] [NanoSD: Edge Efficient Foundation Model for Real Time Image Restoration](https://arxiv.org/abs/2601.09823)
*Subhajit Sanyal,Srinivas Soumitri Miriyala,Akshay Janardan Bankar,Sravanth Kodavanti,Harshit,Abhishek Ameta,Shreyas Pandith,Amit Satish Unde*

Main category: cs.CV

TL;DR: 通过对U-Net和VAE编码器-解码器联合进行网络手术、特征级生成蒸馏与结构化尺度调整，NanoSD在保持生成性能的同时达到在移动NPU上实时推理（如20ms）的参数-延迟-精度折中，适用于多种图像修复任务。


<details>
  <summary>Details</summary>
Motivation: 目标是在边缘设备上实现具备强生成先验的扩散基础模型，既要保留Stable Diffusion带来的高质量图像修复能力，又要满足实时性和模型大小的部署约束。

Method: 1) 网络手术裁剪Stable Diffusion的U-Net与VAE；2) 特征层面的生成蒸馏以保持潜在空间分布；3) 对U-Net和VAE同时进行结构化缩放以达到不同的参数/延迟点；4) 在移动NPU上分析延迟与架构均衡、特征路由、潜在空间保留的关系。

Result: NanoSD提出了一种对Stable Diffusion 1.5进行全流程裁剪与蒸馏的方法，产出适合移动边缘设备的轻量级扩散基础模型族，兼顾生成先验保留与高效推理。

Conclusion: NanoSD证明了全流水线协同压缩（不仅仅是参数缩减）能够在不损失生成先验的情况下实现边缘设备上的高效扩散模型，并且在超分、去模糊、人脸修复和单目深度估计等任务上优于现有轻量扩散方案。

Abstract: Latent diffusion models such as Stable Diffusion 1.5 offer strong generative priors that are highly valuable for image restoration, yet their full pipelines remain too computationally heavy for deployment on edge devices. Existing lightweight variants predominantly compress the denoising U-Net or reduce the diffusion trajectory, which disrupts the underlying latent manifold and limits generalization beyond a single task. We introduce NanoSD, a family of Pareto-optimal diffusion foundation models distilled from Stable Diffusion 1.5 through network surgery, feature-wise generative distillation, and structured architectural scaling jointly applied to the U-Net and the VAE encoder-decoder. This full-pipeline co-design preserves the generative prior while producing models that occupy distinct operating points along the accuracy-latency-size frontier (e.g., 130M-315M parameters, achieving real-time inference down to 20ms on mobile-class NPUs). We show that parameter reduction alone does not correlate with hardware efficiency, and we provide an analysis revealing how architectural balance, feature routing, and latent-space preservation jointly shape true on-device latency. When used as a drop-in backbone, NanoSD enables state-of-the-art performance across image super-resolution, image deblurring, face restoration, and monocular depth estimation, outperforming prior lightweight diffusion models in both perceptual quality and practical deployability. NanoSD establishes a general-purpose diffusion foundation model family suitable for real-time visual generation and restoration on edge devices.

</details>


### [5] [UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval](https://arxiv.org/abs/2601.09828)
*Xiaoxu Ma,Runhao Li,Hanwen Liu,Xiangbo Zhang,Zhenyu Weng*

Main category: cs.CV

TL;DR: UniHash unifies pointwise and pairwise hashing via dual branches, bidirectional knowledge transfer (mutual loss and SM-MoH), improving discriminability and generalization; SOTA results on standard datasets.


<details>
  <summary>Details</summary>
Motivation: Pointwise methods excel on seen categories while pairwise methods generalize better to unseen; need a unified approach to balance both.

Method: Dual-branch unified hashing combining pointwise and pairwise paradigms

Result: Proposes UniHash with center-based (pointwise) and pairwise branches, mutual learning loss, and SM-MoH module; achieves state-of-the-art on CIFAR-10, MSCOCO, ImageNet for seen and unseen retrieval.

Conclusion: UniHash effectively balances retrieval performance for seen and unseen categories by combining complementary paradigms and enabling cross-branch knowledge transfer, validated theoretically and empirically.

Abstract: Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.

</details>


### [6] [ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning](https://arxiv.org/abs/2601.09851)
*Po-han Li,Shenghui Chen,Ufuk Topcu,Sandeep Chinchali*

Main category: cs.CV

TL;DR: 提出ViSIL：以VLM推理为基础的统一信息损失度量，可跨模态比较视频摘要，且与VQA表现显著相关，能指导摘要选择以在信息覆盖和效率间取得更好权衡。


<details>
  <summary>Details</summary>
Motivation: 传统自动评测指标（如BLEU、ROUGE）无法衡量跨模态摘要對原视频信息覆盖度，且难以比较文本段落与关键帧序列等不同结构的摘要，故需一个统一且与下游任务表现相关的度量。

Method: 利用视觉-语言模型（VLM）作为评价器，通过计算原始视频与摘要（文本+关键帧等）在VLM编码空间或推理输出上的信息差异来估计信息损失；将该损失作为统一分数ViSIL，从而比较结构不同的摘要格式；并通过实验在VQA任务上验证相关性与摘要选择效果。

Result: ViSIL与人类评分及VLM在VQA任务的表现有统计显著的相关性；使用ViSIL进行摘要选择可在不增加处理成本下，使关键帧+文本摘要在VQA准确率上比仅文本摘要高约7%；并能绘制信息损失与处理速度的帕累托前沿。

Conclusion: ViSIL提出了一种基于信息论的统一评估指标，用以量化多模态视频摘要相对于原始视频的信息损失，证明其能与人类和VLM在VQA任务上的表现显著相关，并能在信息覆盖与处理速度间实现帕累托最优选择。

Abstract: Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.

</details>


### [7] [Breaking the Limits of Open-Weight CLIP: An Optimization Framework for Self-supervised Fine-tuning of CLIP](https://arxiv.org/abs/2601.09859)
*Anant Mehta,Xiyuan Wei,Xingyu Chen,Tianbao Yang*

Main category: cs.CV

TL;DR: TuneCLIP通过warm-up恢复优化器状态与新的对比损失，在仅用自监督数据的情况下，稳定提升开源CLIP模型的通用性能，避免性能下降并在多个基准上取得显著增益。


<details>
  <summary>Details</summary>
Motivation: 提升开源CLIP模型在多下游任务上的通用性能，而无需从头在海量数据上重训练；解决直接继续训练导致性能下降的问题

Method: Self-supervised fine-tuning of CLIP using existing datasets

Result: 提出TuneCLIP框架，包括恢复优化统计的warm-up和针对假负样本惩罚的对比损失微调；在多模型和尺度上稳定提升性能，SigLIP (ViT-B/16) 在ImageNet上提升至多+2.5%，DataComp上+1.2%

Conclusion: TuneCLIP为高效的后预训练适配提供了可行方法，可在不重训大规模数据的前提下，利用现有自监督数据提升开源CLIP模型的表现，成为新的强基线。

Abstract: CLIP has become a cornerstone of multimodal representation learning, yet improving its performance typically requires a prohibitively costly process of training from scratch on billions of samples. We ask a different question: Can we improve the performance of open-weight CLIP models across various downstream tasks using only existing self-supervised datasets? Unlike supervised fine-tuning, which adapts a pretrained model to a single downstream task, our setting seeks to improve general performance across various tasks. However, as both our experiments and prior studies reveal, simply applying standard training protocols starting from an open-weight CLIP model often fails, leading to performance degradation. In this paper, we introduce TuneCLIP, a self-supervised fine-tuning framework that overcomes the performance degradation. TuneCLIP has two key components: (1) a warm-up stage of recovering optimization statistics to reduce cold-start bias, inspired by theoretical analysis, and (2) a fine-tuning stage of optimizing a new contrastive loss to mitigate the penalization on false negative pairs. Our extensive experiments show that TuneCLIP consistently improves performance across model architectures and scales. Notably, it elevates leading open-weight models like SigLIP (ViT-B/16), achieving gains of up to +2.5% on ImageNet and related out-of-distribution benchmarks, and +1.2% on the highly competitive DataComp benchmark, setting a new strong baseline for efficient post-pretraining adaptation.

</details>


### [8] [VibrantSR: Sub-Meter Canopy Height Models from Sentinel-2 Using Generative Flow Matching](https://arxiv.org/abs/2601.09866)
*Kiarie Ndegwa,Andreas Gros,Tony Chang,David Diaz,Vincent A. Landau,Nathan E. Rutenbeck,Luke J. Zachmann,Guy Bayes,Scott Conway*

Main category: cs.CV

TL;DR: VibrantSR用生成式超分辨率把10m Sentinel-2影像转为0.5m CHM，在22个生态区上对>=2m冠层实现4.39m MAE，优于其他卫星基线，支持多尺度、常态化的森林监测与碳核算。


<details>
  <summary>Details</summary>
Motivation: 空基（航空）影像获取昂贵且时间稀疏，限制了长期、大范围的森林监测与碳核算需求；Sentinel-2数据全球可用且具有季节性覆盖，适合构建连续时序监测系统，因此希望通过超分辨率技术将其用于高分辨率冠层高度估计。

Method: 提出一种生成式超分辨率框架，将10米分辨率的Sentinel-2季节性合成影像上采样到0.5米分辨率并估计冠层高度；采用空间不重叠的验证分割，在22个EPA三级生态区上评估性能，并与多种卫星和航空基线方法比较。

Result: 在22个生态区的测试中，VibrantSR对高度>=2m的冠层高度估计MAE为4.39m，优于其他卫星基线（Meta 4.83m、LANDFIRE 5.96m、ETH 7.05m）；航空基线VibrantVS仍更好（2.71m）。VibrantSR可实现无需昂贵航空数据的大陆级森林监测与碳核算。

Conclusion: 本文提出VibrantSR，一种基于生成式模型的超分辨率框架，用于从10米Sentinel-2影像估计0.5米分辨率的冠层高度模型（CHMs）。该方法利用全球可用的Sentinel-2季节性合成影像，支持季节至年度的持续监测，弥补了空中影像获取不规律且昂贵的不足。实验在美国西部22个EPA三级生态区的空间上不重叠验证集上进行，对于高度>=2米的冠层，VibrantSR的平均绝对误差（MAE）为4.39米，优于Meta（4.83米）、LANDFIRE（5.96米）和ETH（7.05米）基于卫星的基线方法。尽管基于航空影像的VibrantVS在精度上更好（MAE 2.71米），VibrantSR仍实现了在大陆尺度上无需昂贵航空采集的操作化森林监测和碳核算。

Abstract: We present VibrantSR (Vibrant Super-Resolution), a generative super-resolution framework for estimating 0.5 meter canopy height models (CHMs) from 10 meter Sentinel-2 imagery. Unlike approaches based on aerial imagery that are constrained by infrequent and irregular acquisition schedules, VibrantSR leverages globally available Sentinel-2 seasonal composites, enabling consistent monitoring at a seasonal-to-annual cadence. Evaluated across 22 EPA Level 3 eco-regions in the western United States using spatially disjoint validation splits, VibrantSR achieves a Mean Absolute Error of 4.39 meters for canopy heights >= 2 m, outperforming Meta (4.83 m), LANDFIRE (5.96 m), and ETH (7.05 m) satellite-based benchmarks. While aerial-based VibrantVS (2.71 m MAE) retains an accuracy advantage, VibrantSR enables operational forest monitoring and carbon accounting at continental scales without reliance on costly and temporally infrequent aerial acquisitions.

</details>


### [9] [MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation](https://arxiv.org/abs/2601.09879)
*Yang Xing,Jiong Wu,Savas Ozdemir,Ying Zhang,Yang Yang,Wei Shao,Kuang Gong*

Main category: cs.CV

TL;DR: 提出MedVL-SAM2，一种统一的3D医疗多模态模型，支持报告生成、VQA与多范式分割，结合SAM2体积分割模块，通过多阶段训练在CT体积数据上实现像素级定位与语义推理的统一，实验显示性能领先并具备可控交互分割与稳健的跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有医疗VLM在图像级文本任务上表现好，但缺乏细粒度视觉定位和体积级空间推理能力，且难以在统一框架中兼顾高层语义推理与精确3D分割。

Method: 模型采用多阶段训练：先在大规模3D CT图文对上预训练以对齐体视觉特征与放射学语言嵌入，然后在包含3D CT分割的大规模数据集上进行联合优化，结合语言理解与分割目标，并在架构中融入基于SAM2的体积分割模块以支持点、框、语言等交互提示。

Result: 在报告生成、VQA与多种3D分割任务上达到或超越现有最先进水平；提供可靠的3D视觉定位、可控交互分割与稳健的跨模态推理。

Conclusion: MedVL-SAM2能在单一框架中同时实现高层语义推理与精确的3D定位，显著提升3D医学影像在报告生成、VQA及多种分割任务上的表现，验证了将SAM2引入体积分割并与语言理解联合训练的有效性。

Abstract: Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.

</details>


### [10] [Transition Matching Distillation for Fast Video Generation](https://arxiv.org/abs/2601.09881)
*Weili Nie,Julius Berner,Nanye Ma,Chao Liu,Saining Xie,Arash Vahdat*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Large video diffusion and flow models have achieved remarkable success in high-quality video generation, but their use in real-time interactive applications remains limited due to their inefficient multi-step sampling process. In this work, we present Transition Matching Distillation (TMD), a novel framework for distilling video diffusion models into efficient few-step generators. The central idea of TMD is to match the multi-step denoising trajectory of a diffusion model with a few-step probability transition process, where each transition is modeled as a lightweight conditional flow. To enable efficient distillation, we decompose the original diffusion backbone into two components: (1) a main backbone, comprising the majority of early layers, that extracts semantic representations at each outer transition step; and (2) a flow head, consisting of the last few layers, that leverages these representations to perform multiple inner flow updates. Given a pretrained video diffusion model, we first introduce a flow head to the model, and adapt it into a conditional flow map. We then apply distribution matching distillation to the student model with flow head rollout in each transition step. Extensive experiments on distilling Wan2.1 1.3B and 14B text-to-video models demonstrate that TMD provides a flexible and strong trade-off between generation speed and visual quality. In particular, TMD outperforms existing distilled models under comparable inference costs in terms of visual fidelity and prompt adherence. Project page: https://research.nvidia.com/labs/genair/tmd

</details>


### [11] [OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport](https://arxiv.org/abs/2601.09952)
*Zhihua Zhao,Guoqiang Li,Chen Min,Kangping Lu*

Main category: cs.CV

TL;DR: 该论文提出OT-Drive，一种基于最优传输(Optimal Transport)的多模态融合框架，用于提高在非结构化环境下可通行区域分割的OOD泛化能力。通过场景锚(Scene Anchor Generator)将场景分解为天气、时间和道路类型的联合分布，再用OT Fusion将RGB和法线特征映射到由语义锚定义的流形上，从而在OOD场景下实现鲁棒分割。实验表明在ORFD OOD场景上mIoU达95.16%，在跨数据集迁移任务上达89.79%，均显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在OOD场景下分割性能下降，影响自动驾驶的决策与规划。需要一种能在未见场景中仍保持高鲁棒性的多模态融合方法。

Method: 设计Scene Anchor Generator(SAG)将场景表示为天气、时段、路面类型的联合分布生成语义锚；提出基于最优传输的OT Fusion模块，将RGB与表面法线特征传输到锚定义的语义流形上进行融合；基于该融合特征进行可通行区域分割。

Result: 在ORFD OOD场景上mIoU=95.16%（优于先前方法6.35%），跨数据集迁移任务mIoU=89.79%（优于基线13.99%），显示在有限训练数据下具有强泛化能力。

Conclusion: OT-Drive通过语义锚与最优传输融合策略，显著提升了在OOD场景和跨数据集迁移中的可通行区域分割性能，且仅需有限训练数据，具备较强实用性。

Abstract: Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.

</details>


### [12] [The Spatial Blindspot of Vision-Language Models](https://arxiv.org/abs/2601.09954)
*Nahid Alam,Leema Krishna Murali,Siddhant Bharadwaj,Patrick Liu,Timothy Chung,Drishti Sharma,Akshata A,Kranthi Kiran,Wesley Tam,Bala Krishna S Vegesna*

Main category: cs.CV

TL;DR: VLMs lose spatial info because image encoders flatten images; using alternative training objectives and 2D positional encodings improves spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: Spatial reasoning is crucial for robotics and embodied AI, but current VLMs lack 2D structural awareness due to CLIP-style training that flattens image patches.

Method: Evaluate image encoders trained with alternative objectives and incorporate 2D positional encodings; run experiments across spatial reasoning benchmarks to compare performance.

Result: The paper identifies a limitation in current VLMs: poor spatial reasoning due to loss of 2D structure in CLIP-style encoders and flattened 1D patch sequences.

Conclusion: Changing encoder objectives and adding 2D positional encodings enhances spatial grounding in VLMs, benefiting spatial benchmarks and applications like robotics.

Abstract: Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.

</details>


### [13] [DR$^2$Seg: Decomposed Two-Stage Rollouts for Efficient Reasoning Segmentation in Multimodal Large Language Models](https://arxiv.org/abs/2601.09981)
*Yulin He,Wei Chen,Zhikang Jian,Tianhang Guo,Wenjuan Zhou,Minglong Li*

Main category: cs.CV

TL;DR: 提出DR^2Seg，一种自奖励两阶段推理分割框架：先生成自包含目标描述，再用该描述替换原查询进行验证，通过两种自奖励促进目标导向推理并抑制冗余思考，从而提升推理效率和分割精度，无需额外思考监督。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割任务中MLLM往往产生冗长的推理链（overthinking），这些多余的中间推理干扰了目标定位，降低分割精度。希望无需额外思考监督的情况下，提高推理效率与分割准确性。

Method: 两阶段rollout：第一阶段为多模态推理，生成明确指出目标的自包含描述；第二阶段为指代分割，将该描述替换原始复杂查询以验证其自包含性。设计两种自奖励：一是强化目标导向（鼓励生成能准确定位目标的描述）；二是抑制冗余思考（惩罚多余或冗长的推理链）。训练通过自奖励信号调整MLLM输出，最终接入分割模型执行精确分割。

Result: 在多种MLLM和分割基线上，DR^2Seg在推理长度、定位准确率和分割指标（如mIoU或IoU）上均有稳定提升，证明自奖励与两阶段策略能有效压缩冗余推理并提升性能。

Conclusion: DR^2Seg在不同规模的多模态大模型和分割模型上均稳定提升了推理效率与分割性能，表明通过自奖励与两阶段回滚策略可以在不增加监督的情况下减少冗余推理并增强定位能力。

Abstract: Reasoning segmentation is an emerging vision-language task that requires reasoning over intricate text queries to precisely segment objects. However, existing methods typically suffer from overthinking, generating verbose reasoning chains that interfere with object localization in multimodal large language models (MLLMs). To address this issue, we propose DR$^2$Seg, a self-rewarding framework that improves both reasoning efficiency and segmentation accuracy without requiring extra thinking supervision. DR$^2$Seg employs a two-stage rollout strategy that decomposes reasoning segmentation into multimodal reasoning and referring segmentation. In the first stage, the model generates a self-contained description that explicitly specifies the target object. In the second stage, this description replaces the original complex query to verify its self-containment. Based on this design, two self-rewards are introduced to strengthen goal-oriented reasoning and suppress redundant thinking. Extensive experiments across MLLMs of varying scales and segmentation models demonstrate that DR$^2$Seg consistently improves reasoning efficiency and overall segmentation performance.

</details>


### [14] [DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis](https://arxiv.org/abs/2601.10001)
*Chengjia Liang,Zhenjiong Wang,Chao Chen,Ruizhi Zhang,Songxi Liang,Hai Xie,Haijun Lei,Zhongwei Huang*

Main category: cs.CV

TL;DR: 提出DW-DGAT，一种用于早期PD/AD诊断的动态图注意力网络，融合多结构多模态数据、双图注意力（脑区与样本图）和动态类别权重，以缓解高维、多样性与类别不平衡问题；在PPMI与ADNI数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 早期诊断ND受限于多模态、多结构、高维数据的异构性以及样本类别不平衡，现有方法难以同时处理微观脑区信息和宏观样本关系，且在不平衡数据上性能下降，因此需一种能融合多结构数据、提取多层次特征并自适应处理类别不平衡的模型。

Method: 1) 通用数据融合策略将三种结构形式的多指标数据映射至统一表示；2) 双图注意力网络：基于脑区构建节点并用图注意力提取微观（脑区内）特征，基于样本间相似性构建样本图提取宏观（群体）特征；两者交互或级联以获得综合表征；3) 动态类别权重生成机制与两个稳定有效的损失函数（可能为加权交叉熵与焦点损失或类平衡损失变体）联合使用以缓解类别不平衡。

Result: 在PPMI（PD）与ADNI（AD）数据集上进行严格实验，DW-DGAT在诊断准确率、AUC等指标上超越基线方法，证明其在早期ND诊断上的有效性。

Conclusion: DW-DGAT在多数据形式融合、微观与宏观特征提取及不平衡问题处理上有效，提高了PD和AD早期诊断性能，并在PPMI和ADNI数据集上达到或超越现有方法。

Abstract: Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.

</details>


### [15] [VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models](https://arxiv.org/abs/2601.10010)
*Zefan Zhang,Kehua Zhu,Shijie Jiang,Hongyuan Lu,Shengkai Sun,Tian Bai*

Main category: cs.CV

TL;DR: VERHallu benchmark reveals VideoLLMs hallucinate event relations; models rely on priors and miss subevents. KFP reallocates attention to key frames, improving reasoning and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Introduce and evaluate hallucination in VideoLLMs specifically for event relation reasoning, which previous work neglected.

Method: Constructed benchmark with counterintuitive videos and annotated bias candidates; evaluated SOTA VideoLLMs; analyzed failures; introduced KFP to reallocate intermediate-layer attention to key frames and tested improvements.

Result: Created VERHallu benchmark covering causal, temporal, and subevent relations with classification, QA, counterfactual QA; provided human-annotated biased candidates; found SOTA VideoLLMs struggle due to reliance on priors and weak frame-level cue use; proposed Key-Frame Propagating (KFP) strategy that reallocates frame-level attention to improve multi-event understanding; experiments show mitigation of relation hallucination without slowing inference.

Conclusion: VERHallu exposes event relation hallucination in VideoLLMs; KFP improves frame-level reasoning and reduces hallucinations while preserving inference speed.

Abstract: Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.

</details>


### [16] [Disentangled Concept Representation for Text-to-image Person Re-identification](https://arxiv.org/abs/2601.10053)
*Giyeol Kim,Chanho Eom*

Main category: cs.CV

TL;DR: DiCo用可分解的slot-block表征实现图文层级对齐，增强细粒度TIReID性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决图像与文本之间的模态鸿沟，并建模细粒度对应以区分具有相似属性的个体，例如相似服装颜色或纹理。

Method: 提出共享的基于slot的表征，每个slot作为跨模态的部位级锚点，并在slot内部进一步分解为多个概念block，用于解耦颜色、纹理、形状等互补属性，同时保持图文之间的部位级一致性。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上进行了大规模实验，结果显示DiCo在与最先进方法竞争的同时，通过显式的slot和block级表示提升了解释性和检索的细粒度性能。

Conclusion: DiCo通过引入共享slot-和block级表示，实现了层级且可解耦的跨模态对齐，从而提升了TIReID在细粒度属性区分上的能力。

Abstract: Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.

</details>


### [17] [UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow](https://arxiv.org/abs/2601.10054)
*Nick Truong,Pritam P. Karmokar,William J. Beksi*

Main category: cs.CV

TL;DR: 首个合成水下事件相机光流基准：物理光线追踪渲染RGBD并生成事件数据，提供密集真值并评测若干光流方法，公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 水下成像受波长衰减、散射、浑浊模糊和非均匀照明等复杂光学效应影响，且难以获得真实的光流真值；事件相机具有高时间分辨率和高动态范围，适合水下场景，但缺乏带真值的基准数据，因此需要合成数据集以推动研究。

Method: 通过物理基光线追踪渲染水下RGBD视频，使用视频到事件转换生成事件流，输出密集光流、深度和相机运动真值；随后在该数据集上测试多种学习型与模型型光流算法并比较性能。

Result: 该论文构建了首个用于事件相机水下光流的合成基准数据集，基于物理光线追踪生成RGBD序列并通过视频到事件的管线生成真实事件流，附带密集真值光流、深度和相机运动；并基于该数据集评测了多种基于学习和模型的光流方法，分析水下光传输对事件生成和运动估计的影响，公开代码与数据。

Conclusion: 该数据集为研究水下事件相机感知提供了基准，揭示了水下光学对事件形成和光流估计的显著影响，为未来算法开发和评估奠定基础。

Abstract: Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.

</details>


### [18] [CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation](https://arxiv.org/abs/2601.10061)
*Chengzhuo Tong,Mingkun Chang,Shenglong Zhang,Yuran Wang,Cheng Liang,Zhizheng Zhao,Ruichuan An,Bohan Zeng,Yang Shi,Yifan Dai,Ziming Zhao,Guanbin Li,Pengfei Wan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Introduce CoF-T2I, use CoF trajectories dataset and independent frame encoding to enable stepwise T2I generation, improving quality


<details>
  <summary>Details</summary>
Motivation: Leverage CoF reasoning from video models to improve interpretability and performance of T2I by using intermediate frames as explicit reasoning steps

Method: Progressive visual refinement using Chain-of-Frame reasoning

Result: CoF-T2I outperforms base video models and achieves competitive results on benchmarks (GenEval 0.86, Imagine-Bench 7.468)

Conclusion: Video models with CoF reasoning can substantially advance high-quality T2I generation when guided by curated trajectories and independent frame encoding

Abstract: Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.

</details>


### [19] [ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology](https://arxiv.org/abs/2601.10073)
*Hyun Do Jung,Jungwon Choi,Hwiyoung Kim*

Main category: cs.CV

TL;DR: ReaMIL通过带预算的选择头在MIL中学出稀疏且紧凑的证据集，无需额外监督，保持或略增性能，同时提供量化的证据效率指标。


<details>
  <summary>Details</summary>
Motivation: 为WSI分类提供效率高、可解释且不需额外标注的证据选择机制，使模型能用少量关键tile给出稳定的高置信决策。

Method: 在强MIL骨干上添加轻量的选择头，输出每个patch的软门控；训练使用预算充分性目标（hinge loss），在稀疏预算下保证保留的证据能使真类概率≥τ。

Result: 在TCGA-NSCLC、TCGA-BRCA和PANDA上，ReaMIL与基线AUC相当或略优；在NSCLC上取得AUC 0.983，平均最小充分K≈8.2（τ=0.90），AUKC≈0.864，表明少量tile即可稳定置信度。

Conclusion: ReaMIL在不牺牲性能的前提下，通过预算充分性目标学得小且空间上紧凑的证据集，实现了对全切片图像的稀疏可解释推理。

Abstract: We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.

</details>


### [20] [Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting](https://arxiv.org/abs/2601.10075)
*Zhendong Wang,Lebin Zhou,Jingchuan Xiao,Rongduo Han,Nam Ling,Cihan Ruan*

Main category: cs.CV

TL;DR: Introduce mesh-free flow guidance to deform 3D Gaussians according to 2D painting flows, decouple luminance from structure, and evaluate with vision-language model aesthetics


<details>
  <summary>Details</summary>
Motivation: Existing 3D style transfer treats geometry as static and only transfers surface textures; to mimic Post-Impressionist emphasis on exaggerated form, geometry must be abstracted as primary expression

Method: flow-guided geometric advection for 3DGS

Result: A mesh-free method that backpropagates 2D flow into 3D Gaussian primitives to form flow-aligned brushstrokes, plus luminance-structure decoupling and VLM-based evaluation

Conclusion: Flow-guided geometric advection enables painterly geometric abstraction in 3DGS, producing expressive, topology-aware stylizations beyond texture-only methods

Abstract: In 1888, Vincent van Gogh wrote, "I am seeking exaggeration in the essential." This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.
  We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.
  Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.

</details>


### [21] [Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks](https://arxiv.org/abs/2601.10090)
*Mingzhuo Li,Guang Li,Linfeng Ye,Jiafeng Mao,Takahiro Ogawa,Konstantinos N. Plataniotis,Miki Haseyama*

Main category: cs.CV

TL;DR: Introduce difficulty-guided sampling and difficulty-aware guidance to align dataset distillation with downstream image classification tasks by selecting samples according to target difficulty distributions, improving distillation performance


<details>
  <summary>Details</summary>
Motivation: Existing dataset distillation methods ignore task-specific difficulty, causing a gap between distillation objective and downstream task performance; incorporate difficulty to better align distilled data with downstream classification tasks

Method: difficulty-guided sampling for dataset distillation

Result: Proposed DGS (post-stage sampling) and DAG (difficulty-aware guidance) applied to image classification; sampled distilled datasets from pools following target difficulty distributions; extensive experiments show improved performance and applicability to diverse tasks

Conclusion: Difficulty is a useful criterion for dataset distillation; DGS and DAG improve distilled dataset quality and downstream performance, suggesting broader potential of difficulty-aware methods for other tasks

Abstract: In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.

</details>


### [22] [V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation](https://arxiv.org/abs/2601.10094)
*Han Wang,Yi Yang,Jingyuan Hu,Minfeng Zhu,Wei Chen*

Main category: cs.CV

TL;DR: V-Zero通过Questioner与Solver的自我对弈及GRPO优化，在仅用未标注图像的情况下实现VLM自我提升，取得明显性能增益。


<details>
  <summary>Details</summary>
Motivation: 减少对大规模人工标注数据的依赖，探索仅用未标注图像实现多模态模型自监督自我提升的方法。

Method: 提出双角色系统：Questioner生成高质量挑战性问题，依靠对直觉猜测与推理结果的对比进行双轨推理奖励；Solver通过对自身采样答案进行多数投票生成伪标签进行训练。两者使用Group Relative Policy Optimization(GRPO)迭代训练，形成相互增强循环。

Result: 在未使用任何人工标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct上取得稳定性能提升：视觉数学推理提升+1.7，通用视觉任务提升+2.6，验证了无监督自我提升在多模态系统中的可行性。

Conclusion: 本文提出V-Zero，一种仅用未标注图像进行自我提升的后训练框架，通过问者-解答者协同进化来提升视觉-语言模型性能。

Abstract: Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero

</details>


### [23] [InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery](https://arxiv.org/abs/2601.10098)
*Wenwen Liao,Hang Ruan,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 从信息论视角重构GCD问题，InfoSculpt在标注数据上做类别级CMI、在全部数据上做实例级CMI，协同去除增强噪声并保留类别信息。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法依赖伪标签或两阶段聚类，缺乏从信息论上区分类别定义信号与实例特异噪声的原则性机制，因而提出基于信息瓶颈的表示雕刻方法。

Method: 通过两个互补的CMI目标：1）类别级CMI（仅在标注数据）用于学习紧凑判别表示；2）实例级CMI（在所有数据）用于压缩增强引入的噪声以提取不变特征。两者协同作用构建鲁棒的潜空间。

Result: 提出InfoSculpt框架，通过最小化双重条件互信息（CMI）来雕刻表示空间，实现已知类别紧凑判别表示与样本不变性特征的提取。

Conclusion: InfoSculpt在8个基准上取得显著效果，证明基于信息瓶颈的设计能更好地分离类别信号与实例噪声，提高GCD性能。

Abstract: Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.

</details>


### [24] [FlowAct-R1: Towards Interactive Humanoid Video Generation](https://arxiv.org/abs/2601.10103)
*Lizhen Wang,Yongming Zhu,Zhipeng Ge,Youwei Zheng,Longhao Zhang,Tianshu Hu,Shiyang Qin,Mingshuang Luo,Jiaxu Zhang,Xin Chen,Yulong Wang,Zerong Zheng,Jianwen Jiang,Chao Liang,Weifeng Chen,Xing Wang,Yuan Zhang,Mingyuan Gao*

Main category: cs.CV

TL;DR: 提出基于MMDiT架构的FlowAct-R1，通过chunkwise diffusion forcing及self-forcing、蒸馏与系统级优化，实现480p下25fps、TTFF约1.5s的低延迟实时人形视频生成，同时保证长期时间一致性与自然行为过渡。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成方法在高保真和实时交互之间的权衡，目标是实现低延迟、长时稳定且能进行细粒度全身控制的交互式人形视频合成。

Method: 基于MMDiT的流式生成架构；引入chunkwise diffusion forcing和self-forcing策略以缓解误差累积并维持长时一致性；采用高效蒸馏和系统级优化以提升速度（25fps@480p，TTFF≈1.5s）。

Result: FlowAct-R1提出了一种面向实时交互的人形视频生成框架，能够在低延迟下连续生成任意时长的视频，并兼顾高保真与交互响应性。

Conclusion: FlowAct-R1在实时交互场景下实现了高感知真实度与行为生动性，并在不同角色风格间具备良好泛化能力，可用于低延迟全身控制的视频交互系统。

Abstract: Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.

</details>


### [25] [MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers](https://arxiv.org/abs/2601.10104)
*Chenyue Zhou,Jiayi Tuo,Shitong Qin,Wei Dai,Mingxuan Wang,Ziwei Zhao,Duoyang Li,Shiyang Su,Yanxi Lu,Yanbiao Ma*

Main category: cs.CV

TL;DR: 本工作提出MathDoc：首个基于真实高中数学试卷的问题级结构化信息抽取基准，包含3609道带真实噪声的题目，并显式包含不可识别样本用于评估模型主动拒绝能力。设计多维评估（题干准确性、视觉相似性、拒绝能力）。SOTA多模态大模型在提取上表现较强，但无法拒绝不可识别输入，倾向于自信地输出错误结果，凸显可靠性缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注干净文档或通用布局分析，忽视数学题目的结构完整性与模型在不可识别输入上的拒绝能力；真实纸质试卷含大量噪声，亟需针对性基准评估模型的鲁棒性与可靠性。

Method: 收集并精心标注3609道真实高中数学题，保留真实世界伪影与不可识别样本；构建多维评估指标：题干准确性、视觉相似性、拒绝（refusal）能力；在多种SOTA MLLMs上做基准测试并分析其输出行为。

Result: 在Qwen3-VL、Gemini-2.5-Pro等模型上实验表明：端到端模型在结构化提取上总体表现良好，但在面对不可识别样本时无法正确拒绝，反而产生自信但无效的输出，揭示模型可靠性缺口。

Conclusion: MathDoc作为首个真实纸质数学试卷文档级信息抽取基准，暴露了现有MLLMs在面对严重视觉噪声时的主动拒绝能力不足；需要在模型训练与评估中引入拒绝机制和更鲁棒的视觉处理。

Abstract: The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \href{https://github.com/winnk123/papers/tree/master}{GitHub repository}

</details>


### [26] [Enhancing Visual In-Context Learning by Multi-Faceted Fusion](https://arxiv.org/abs/2601.10107)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Qingchao Jiang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 传统检索-提示方法只选单一最佳提示或将top-K合并为单一表示，丢失多样性。本文提出生成三种不同组合的上下文分支并用MULTI-VQGAN进行协同推理，从而更好融合上下文，提升多视觉任务性能和泛化。


<details>
  <summary>Details</summary>
Motivation: 单一提示或将多提示合并为一会丢失多样化信号，限制模型推理能力；需要一种能保留并协同利用多提示互补性的融合方法以提升VICL性能和泛化。

Method: 检索top-K视觉提示，基于不同组合策略生成三条互补上下文表示分支；设计MULTI-VQGAN架构以并行/联合解码这些分支并进行协同推理，输出目标任务预测。实现细节可能包括提示组合策略、分支编码器、融合模块及多任务损失。

Result: 提出了一种多组合协同融合框架，通过生成三条上下文表示分支并输入MULTI-VQGAN来联合利用多源信息；在前景分割、单目标检测和图像着色等任务上表现优于现有方法。

Conclusion: 多组合协同融合能够保留并利用来自多个高质量提示的互补信息，通过MULTI-VQGAN联合解释这些信号，可提高视觉上下文学习的鲁棒性和准确性，且具备跨任务泛化能力。

Abstract: Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant "retrieve-then-prompt" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.

</details>


### [27] [Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL](https://arxiv.org/abs/2601.10117)
*Wenwen Liao,Jianbo Yu,Yuansong Wang,Shifu Yan,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出通过多提示自适应融合、排列感知轻量MLP与双向微调的端到端VICL框架，解决信息丢失和布局先验未利用问题，在多任务上表现优越并具良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有VICL方法存在两点主要缺陷：一是仅挑选最相似的单一提示会舍弃其他高质量提示中的互补信息；二是未充分利用不同提示排列所暗含的结构信息，导致模型泛化与性能受限。因此需要一种能聚合多提示信息并显式处理排列先验的框架。

Method: 主要方法包括：1) 自适应融合模块（Adaptive Fusion Module），对来自多个提示的关键模式与注释进行加权聚合，生成更精准的上下文提示；2) 安排特定的轻量级MLP，用于将布局先验从核心模型中解耦，从而对模型影响最小地引入排列信息；3) 双向微调机制（bidirectional fine-tuning），通过交换查询与提示的角色，促使模型从融合后的上下文重建原始提示，强化融合模块与修复（inpainting）模型之间的协同。训练为端到端联合优化。

Result: 在前景分割、单目标检测和图像上色三类任务上的实验证明：1) 自适应融合能汲取多提示互补信息，提升上下文表示的精确性；2) 排列特定MLP带来布局先验建模且对主模型扰动小；3) 双向微调促进模块协同，整体带来更好的任务性能与跨任务泛化性。

Conclusion: 本文提出一种端到端的VICL框架，通过自适应融合模块与排列相关轻量MLP模块解决了多提示信息丢失与布局先验未利用的问题，并借助双向微调增强模块协作性，实验在前景分割、单目标检测与图像上色任务上展示了优越性能与跨任务泛化能力。

Abstract: Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.
  We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.

</details>


### [28] [VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2601.10124)
*Sicheng Yang,Zhaohu Xing,Lei Zhu*

Main category: cs.CV

TL;DR: 提出VQ-Seg：用VQ离散化特征并通过重排列码本索引进行可控扰动，配合重建分支和PFA引入基础模型语义，显著提升肺部肿瘤分割效果，公开了828例CT数据集。


<details>
  <summary>Details</summary>
Motivation: 现有一致性学习常用dropout做扰动，但dropout率敏感且难调参，可能导致次优正则化。因此需要一种更可控且无需敏感超参的扰动手段。

Method: 1) 使用向量量化将特征离散化；2) 提出Quantized Perturbation Module (QPM)，通过打乱码本索引的空间位置来扰动离散表征；3) 设计双分支架构，共享量化后特征用于图像重建与分割以减轻量化信息丢失；4) 加入Post-VQ Feature Adapter (PFA)，利用基础模型提供的高层语义补偿量化造成的信息损失。

Result: 在作者收集的828例肺癌CT数据集及其他公开基准上，VQ-Seg优于现有方法，表现出更好的分割精度和可控正则化能力。

Conclusion: 本论文提出VQ-Seg，通过向量量化替代dropout实现更可控的特征扰动，并结合双分支重建与分割以及Post-VQ特征适配器引入基础模型语义指导，从而在肺部肿瘤分割上取得优于现有方法的性能。

Abstract: Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.

</details>


### [29] [LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning](https://arxiv.org/abs/2601.10129)
*Linquan Wu,Tianxiang Jiang,Yifei Dong,Haoyu Yang,Fengji Zhang,Shichaang Meng,Ai Xuan,Linqi Song,Jacky Keung*

Main category: cs.CV

TL;DR: 提出LaViT，一种通过对齐学生模型与教师模型的视觉“潜在思路”（包括视觉语义和注意力轨迹）来缩小感知差距的方法；通过自回归重建和课程感官门控防止捷径学习；在复杂推理任务上显著提升视觉定位性能，提升可达16.9%，并使3B模型优于更大与专有模型。


<details>
  <summary>Details</summary>
Motivation: 发现蒸馏中存在“感知差距”：学生虽复制教师文本输出，但关注不同视觉区域，依赖语言先验而非真实视觉感知，因此需要对齐隐含视觉注意力而非仅对齐静态表征。

Method: 在蒸馏过程中，引导学生以自回归方式重建教师的视觉语义和注意力轨迹（即视觉思想），并使用课程感官门控机制逐步开放视觉信息以避免学生走捷径；训练先重建视觉思想再生成文本。

Result: 在多项复杂视觉推理任务上显著提升视觉接地性能，最高带来+16.9%提升；并使得3B学生模型超过更大开源模型与GPT-4o等专有模型。

Conclusion: LaViT有效降低学生模型对语言先验的依赖，通过对齐视觉潜在思路与注意力轨迹提升视觉接地能力，带来明显性能提升和模型压缩优势。

Abstract: Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.

</details>


### [30] [Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method](https://arxiv.org/abs/2601.10165)
*Chao Huang,Benfeng Wang,Wei Wang,Jie Wen,Li Shen,Wenqi Ren,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 本文提出Video Anomaly Reasoning任务与大规模注释数据集，采用感知-认知-行动链式CoT框架与异常感知策略优化，设计Vad-R1-Plus模型，显著提升MLLM在视频异常推理和决策上的表现


<details>
  <summary>Details</summary>
Motivation: 弥补现有多模态大模型在视频异常检测与理解领域缺乏明确推理、风险意识及决策导向解释的不足，提出分层的推理任务和数据集，以提高模型从感知到决策的能力

Method: 分析方法和模型设计

Result: 构建了包含8641个视频、5万+样本的VAR数据集，设计PerCoAct-CoT推理框架，提出Anomaly-Aware Group Relative Policy Optimization优化策略，并实现端到端模型Vad-R1-Plus，在多阶段推理与风险感知决策上优于开源和商业基线

Conclusion: VAR任务、PerCoAct-CoT注释规范与Anomaly-Aware优化策略为视频异常理解提供了结构化、多阶段的推理范式；所构建数据集与Vad-R1-Plus模型显著增强了MLLM在VAR上的可靠性与风险感知能力，为后续研究奠定基础

Abstract: Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.

</details>


### [31] [RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation](https://arxiv.org/abs/2601.10168)
*Yue Chang,Rufeng Chen,Zhaofan Zhang,Yi Chen,Sihong Xie*

Main category: cs.CV

TL;DR: RAG-3DSG uses re-shot guided uncertainty estimation to select reliable objects for retrieval-augmented generation and a dynamic downsample-mapping strategy to speed aggregation, yielding better accuracy and ~3x faster mapping


<details>
  <summary>Details</summary>
Motivation: Improve object-level recognition accuracy and speed for open-vocabulary 3DSG by reducing aggregation noise and accelerating cross-image object aggregation

Method: RAG-3DSG: re-shot guided uncertainty + dynamic downsample-mapping

Result: Improved node captioning accuracy and mapping time reduced by two-thirds on Replica dataset

Conclusion: RAG-3DSG effectively mitigates aggregation noise and accelerates aggregation, improving open-vocabulary 3DSG generation performance.

Abstract: Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.

</details>


### [32] [From Physical Degradation Models to Task-Aware All-in-One Image Restoration](https://arxiv.org/abs/2601.10192)
*Hu Gao,Xiaoning Lei,Xichen Xu,Xingjian Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出基于逆退化算子的两阶段不确定性引导图像恢复方法OPIR，兼顾效率与效果，避免额外模块导致的复杂性


<details>
  <summary>Details</summary>
Motivation: 减少系统复杂度并提升实时性：通过物理退化建模，预测任务相关的逆退化算子来实现高效的all-in-one图像恢复，避免额外学习模块带来的复杂性

Method: Predict inverse degradation operator with uncertainty-guided two-stage refinement

Result: 提出了OPIR框架：两阶段流程（初始恢复+不确定性指导的精细化），共享逆算子预测网络并在算子后引入任务感知参数，同时通过加速算子卷积实现高效性，实验显示在all-in-one和任务对齐恢复上均表现优越

Conclusion: 通过预测可加速的任务感知逆退化算子并结合不确定性引导的两阶段恢复，OPIR实现了高效且性能优越的all-in-one图像恢复。

Abstract: All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.

</details>


### [33] [ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation](https://arxiv.org/abs/2601.10200)
*Kim Youwang,Lee Hyoseok,Subin Park,Gerard Pons-Moll,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: ELITE提出了一个高效的高斯头像合成方法，从单目视频通过学习初始化和测试时生成式自适应实现。它结合了3D数据先验和2D生成先验的优点：用Mesh2Gaussian先验模型快速初始化高斯头像，再通过测试时的生成式自适应（利用真实与合成图像监督）弥补领域差距。提出的单步渲染引导扩散增强器比全扩散更快且更不易产生身份幻觉。结果表明在视觉效果和速度（约60倍）上优于基线。


<details>
  <summary>Details</summary>
Motivation: 动机是克服单目视频缺失视觉信息的挑战：单一使用3D先验泛化差，2D生成先验计算开销大且易出现身份幻觉。通过结合两类先验并设计高效的初始化和测试时适配，期望同时获得高质量和高效性。

Method: 方法包括：1) Mesh2Gaussian Prior Model (MGPM)用于从网格快速前向推断高斯头像初始化；2) 测试时生成式自适应，结合真实与合成图像监督，使用渲染引导的单步扩散增强器修复细节；3) 整体流水线使合成速度大幅提升并减少身份幻觉。

Result: 在实验中，ELITE在可驱动头像的视觉质量上优于先前方法，能更好处理复杂表情，并在合成速度上比基于2D生成先验的方法快约60倍。

Conclusion: ELITE能高效生成高保真、可驱动的头像，兼具在野外场景的泛化能力与极高的合成速度，优于仅依赖单一先验的方法。

Abstract: We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.

</details>


### [34] [Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation](https://arxiv.org/abs/2601.10214)
*Dong-Yu Chen,Yixin Guo,Shuojin Yang,Tai-Jiang Mu,Shi-Min Hu*

Main category: cs.CV

TL;DR: DepthDirector通过视图-内容双流条件（源视频+目标视图的深度渲染）和LoRA微调，在视频重渲染中实现更精确的相机可控性与一致性，且构建了8K/1K场景数据集辅助训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于视角变换的相机控制方法未充分利用视频扩散模型的3D先验，且常陷入Inpainting Trap导致主体一致性差和生成质量下降。

Method: 深度导演（DepthDirector）

Result: 提出DepthDirector：通过将源视频和目标视点下经变换的深度序列作为双流条件输入预训练视频生成模型，并用轻量LoRA适配器微调，能够在保持内容一致性的同时实现精确相机轨迹控制；构建了大规模MultiCam-WarpData数据集。

Conclusion: 利用几何深度引导结合VDM的3D理解能力，可以避免Inpainting Trap，提高相机控制精度与视觉质量；轻量LoRA适配器有助于保留预训练模型先验。

Abstract: Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.

</details>


### [35] [Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge](https://arxiv.org/abs/2601.10228)
*Sicheng Yang,Yukai Huang,Shitong Sun,Weitong Cai,Jiankang Deng,Jifei Song,Zhensong Zhang*

Main category: cs.CV

TL;DR: 该论文提出一个面向复杂视频问答的端到端优化框架，通过查询/选项预处理、基于Qwen2.5-VL的领域微调、新颖的时间链式思维（T-CoT）提示以及鲁棒的后处理，提升了对长时序视频理解的能力。在HD-EPIC VQA上达到41.6%准确率，强调了在高难度视频理解任务中需从数据、模型、提示和输出四个环节整体优化。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在复杂、长时序、主观含糊的视角视频QA（如HD-EPIC VQA）上表现较差，主要原因是查询/选项歧义、长时间轴推理能力不足和输出不规范，需从输入、模型能力、推理策略和输出四方面协同改进。

Method: 方法包括四部分：1) 查询/选项预处理：清洗/规范化问题与候选项以减少歧义；2) 基于Qwen2.5-VL的领域微调：在领域数据上微调模型以提升视觉-语言对齐；3) 时间链式思维（T-CoT）提示：设计分步时间线推理提示，促使模型进行多步、长程时间推理；4) 后处理：标准化并校验模型输出以匹配评测格式。

Result: 在HD-EPIC VQA数据集上，提出的系统实现了41.6%准确率，显著提升了对复杂、长时序视频问答的处理能力，并将代码与微调模型开源。

Conclusion: 综合预处理、模型微调、时间链式推理提示与后处理的协同设计，可显著改善MLLM在复杂视频QA上的表现；本文方法在HD-EPIC VQA上取得41.6%准确率，表明全流程优化是提升长时序视频理解的关键。

Abstract: Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.

</details>


### [36] [Attend to what I say: Highlighting relevant content on slides](https://arxiv.org/abs/2601.10244)
*Megha Mariam K M,C. V. Jawahar*

Main category: cs.CV

TL;DR: 系统性地将讲者话语与幻灯片内容对齐，自动突出最相关的幻灯片区域以减少认知负担并提升理解。


<details>
  <summary>Details</summary>
Motivation: reduce cognitive disconnect between spoken narrative and slide visuals in presentations by identifying and highlighting slide regions relevant to current speech

Method: automatic slide region highlighting based on speech-slide alignment

Result: a method that analyzes spoken content and matches it to textual/graphical slide elements to highlight relevant regions; explores multiple solution variants and evaluates successes/failures; provides code and dataset

Conclusion: 基于语音-幻灯片对齐的区域高亮能改善观看讲座类视频的注意力同步性，实验分析了不同方法的优劣并公开了代码与数据集。

Abstract: Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight

</details>


### [37] [DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset](https://arxiv.org/abs/2601.10305)
*Hengyu Shen,Tiancheng Gu,Bin Qin,Lan Wu,Yuling Wu,Shuo Tan,Zelong Sun,Jun Wang,Nan Wu,Xiang An,Weidong Cai,Ziyong Feng,Kaicheng Yang*

Main category: cs.CV

TL;DR: 本文构建了一个严格筛选的1亿条中文图文数据集DanQing，基于2024-2025年网络数据，显著提升中文VLP模型的零样本分类、跨模态检索和LMM评估性能，并将以CC-BY 4.0开源。


<details>
  <summary>Details</summary>
Motivation: 解决中文图文预训练数据短缺问题，提升中文视觉-语言预训练模型性能和实用性。

Method: 从Common Crawl收集2024-2025年网页图文对，设计更严的筛选流程以保证文本质量和模态对齐，采用持续预训练SigLIP2并在多项中文任务上对比评估。

Result: 提出DanQing数据集，包含1亿高质量中文图文对，来源于2024-2025年Common Crawl，经过严格筛选，显著提升SigLIP2在多项中文下游任务上的表现。

Conclusion: DanQing为中文视觉语言预训练提供了高质量、时间更近的数据资源，可提高模型对新语义趋势的理解，有助于推动中文VLP研究与应用。

Abstract: Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.

</details>


### [38] [Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models](https://arxiv.org/abs/2601.10313)
*Peng-Fei Zhang,Zi Huang*

Main category: cs.CV

TL;DR: HRA is a scalable multimodal universal attack for VLPs that refines image and text UAPs and uses ScMix and temporal gradient hierarchy to improve effectiveness and stability.


<details>
  <summary>Details</summary>
Motivation: This paper addresses the inefficiency of sample-specific adversarial attacks on vision-language pre-training (VLP) models by proposing a universal attack framework that scales across datasets and scenarios.

Method: HRA disentangles images into clean images + perturbations, applies ScMix augmentation, refines optimization using historical and predicted future gradients, and selects globally influential words as text UAPs by combining intra- and inter-sentence importance measures.

Result: They propose Hierarchical Refinement Attack (HRA), which refines universal adversarial perturbations at sample and optimization levels, introduces ScMix augmentation, temporal hierarchy of gradients, and selects universal text perturbations via word importance; experiments show superior performance across tasks and models.

Conclusion: HRA effectively produces universal multimodal adversarial perturbations that generalize across samples, models, and tasks, outperforming prior sample-specific and universal attacks.

Abstract: Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.

</details>


### [39] [ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding](https://arxiv.org/abs/2601.10323)
*Xueyun Tian,Wei Li,Bingbing Xu,Heng Dong,Yuanzhuo Wang,Huawei Shen*

Main category: cs.CV

TL;DR: ROMA aligns dense audio with discrete video frames, adds a speak head for trigger decoupling, trains with streaming dataset and two-stage curriculum, and evaluates on unified benchmark suite showing strong proactive performance


<details>
  <summary>Details</summary>
Motivation: Improve streaming omni-multimodal understanding and proactive/reactive interaction

Method: Paper Analysis

Result: ROMA achieves SOTA on proactive tasks and competitive on reactive tasks across 12 benchmarks

Conclusion: ROMA is robust for unified real-time omni-multimodal understanding with effective streaming adaptations and proactive capabilities

Abstract: Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.

</details>


### [40] [SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition](https://arxiv.org/abs/2601.10324)
*Yiming Zhang,Weibo Qin,Yuntian Liu,Feng Wang*

Main category: cs.CV

TL;DR: SRAW crafts adversarial examples via spatial deformation with reweighted foreground/background budgets, achieving strong, imperceptible, and transferable attacks on SAR-ATR.


<details>
  <summary>Details</summary>
Motivation: Improve attack stealthiness and effectiveness for SAR-ATR by leveraging spatial deformation and region-aware budget reweighting to exploit SAR image sparsity and model reliance on background.

Method: Optimize spatial deformation field (warping) with region-aware budget reweighting between foreground and background; generate adversarial SAR images that are imperceptible and transferable; evaluate extensively against SOTA models.

Result: Proposed SRAW method that uses optimized spatial warping with separate budget allocation for foreground and background; outperforms existing attacks on imperceptibility and transferability, significantly degrading SOTA SAR-ATR models; code released.

Conclusion: SRAW is an effective and stealthy attack for SAR-ATR, leveraging spatial warping and region-aware budgets to better exploit SAR image characteristics and model weaknesses; it outperforms prior methods and code is available.

Abstract: Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.

</details>


### [41] [Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders](https://arxiv.org/abs/2601.10332)
*Siqi Kou,Jiachun Jin,Zetong Zhou,Ye Ma,Yugang Wang,Quan Chen,Peng Jiang,Xiao Yang,Jun Zhu,Kai Yu,Zhijie Deng*

Main category: cs.CV

TL;DR: 提出T2G范式：激活LLM文本编码器的推理与重写能力，并与扩散模型联合优化（Dual-GRPO）；显著提升语义一致性和视觉质量，在WISE基准上达0.79。


<details>
  <summary>Details</summary>
Motivation: Address limitation of T2I diffusion models that act as literal text-pixel mappers and do not leverage LLM reasoning to infer visual content beyond prompts.

Method: 1) 用轻量监督微调激活LLM的think-then-rewrite行为；2) 通过Dual-GRPO联合优化文本编码器与扩散主干；3) 对文本编码器使用基于图像的奖励来加强世界知识回忆；4) 推动扩散模型生成语义一致的图像。

Result: Propose think-then-generate (T2G): LLM-based text encoder rewrites prompts through reasoning; fine-tune encoder and co-optimize with diffusion model via Dual-GRPO; use image-grounded rewards to reinforce encoder; achieves improved factual consistency and realism, WISE score 0.79 nearly GPT-4 level.

Conclusion: T2G显著推进了将推理能力融入T2I生成模型，提升事实一致性、语义对齐和视觉现实性，朝统一的具备推理、表达和示范能力的模型方向迈进。

Abstract: Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.

</details>


### [42] [An analytic theory of convolutional neural network inverse problems solvers](https://arxiv.org/abs/2601.10334)
*Minh Hai Nguyen,Quoc Bao Do,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: 将CNN的平移等变与局部性偏置放入经验MMSE框架，得到可解释的LE-MMSE解析解，并在实证上与深度网络输出高度吻合，揭示了多种影响因素。


<details>
  <summary>Details</summary>
Motivation: 尽管监督CNN在成像逆问题上效果优异，但缺乏理论可解释性，研究旨在把网络行为与统计估计理论（MMSE）联系起来，揭示CNN中平移等变与局部性偏置如何决定其在经验分布下的最优估计形式。

Method: 在经验训练分布上，将MMSE估计问题引入函数空间约束（平移等变与局部有限感受野），推导出解析且可计算的LE-MMSE公式；通过数值实验使用不同逆问题（去噪、修补、去卷积）、数据集（FFHQ、CIFAR-10、FashionMNIST）与网络结构（U-Net、ResNet、PatchMLP）对理论预测与网络输出进行对比验证。

Result: 推导出LE-MMSE解析公式并在多任务、多数据集与多架构上验证理论预测与训练网络输出高度一致（PSNR ≳25 dB）；同时分析了物理感知（physics-aware）与物理无感知估计器的差异、训练（patch）分布中高密度区域的影响及数据集大小、patch尺寸等因素对估计的影响。

Conclusion: 本文通过将训练好的卷积神经网络输出视为在经验分布下的受约束最小均方误差（MMSE）估计，提出并解析了局部平移等变MMSE（LE-MMSE）这一可解释模型，从而将CNN常见的平移等变性与有限感受野的局部性归纳偏置形式化。

Abstract: Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\gtrsim25$dB). Furthermore, we provide insights into the differences between \emph{physics-aware} and \emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).

</details>


### [43] [Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs](https://arxiv.org/abs/2601.10369)
*Ningyu Sun,Zhaolin Cai,Zitong Xu,Peihang Chen,Huiyu Duan,Yichao Yan,Xiongkuo Min,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出HPE-Bench基准与基于层选择的MLLM评估框架，通过对比LoRA与层敏感性分析实现对文本引导人体姿态编辑的高效真实性检测与细粒度质量评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法同时兼顾伪造真实性检测与细粒度的姿态质量评估，难以定位结构性异常与生成伪影；因此需要一个专门的基准和统一评估框架来弥合取证与质量评估的空白。

Method: 构建包含1700样本的基准数据集（来自17个模型），为每个样本提供真实性标签与多维质量评分。提出基于MLLM的统一框架，采用对比LoRA微调和层敏感性分析（LSA）来选择最优特征层用于姿态评估，并在真实性检测与质量回归任务上训练评估网络。

Result: 所提框架在真实性检测和多维质量回归任务上均优于现有方法，展示了更高的判别准确率和回归相关性，证明了LSA在选择评估层面具有显著效果。

Conclusion: HPE-Bench有效提升了对文本引导人体姿态编辑生成内容的真实性检测与质量评估，提供了更精细的姿态一致性诊断，证实了基于层选择的MLLM特征提取在姿态评估中优于传统方法。

Abstract: Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.

</details>


### [44] [Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement](https://arxiv.org/abs/2601.10373)
*Yichong Xia,Yimin Zhou,Jinpeng Wang,Bin Chen*

Main category: cs.CV

TL;DR: DiffCR refines pre-trained diffusion priors with a FaSE module and FDA, enabling two-step fast decoding and better bit allocation, yielding large bitrate savings and >10x speed-up.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based image compression methods produce high-quality results at low bitrates but are slow due to many sampling steps and have suboptimal bit allocation because training is fragmented.

Method: Introduce FaSE to refine ε-prediction priors and align them to compressed latents via FDA; use a consistency estimator to preserve semantic trajectory and enable two-step decoding; keep backbone diffusion model frozen.

Result: Proposed DiffCR framework with Frequency-aware Skip Estimation (FaSE) and Frequency Decoupling Attention (FDA), plus a lightweight consistency estimator enabling two-step decoding. Achieves 27.2% BD-rate saving (LPIPS), 65.1% BD-rate saving (PSNR), and over 10x speed-up versus SOTA without updating backbone diffusion model.

Conclusion: DiffCR significantly accelerates diffusion-based image compression and improves fidelity and bitrate efficiency by aligning diffusion priors with compressed latents and using a consistency estimator for two-step decoding, all without retraining the backbone diffusion model.

Abstract: Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \textbf{Diff}usion-based Image Compression via \textbf{C}onsistency Prior \textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\% BD-rate (LPIPS) and 65.1\% BD-rate (PSNR)) and over $10\times$ speed-up compared to SOTA diffusion-based compression baselines.

</details>


### [45] [Global Context Compression with Interleaved Vision-Text Transformation](https://arxiv.org/abs/2601.10378)
*Dian Jiao,Jiaxin Duan,Shuai Zhao,Jiabing Leng,Yiran Zhang,Feng Huang*

Main category: cs.CV

TL;DR: VIST2通过把文本块渲成草图图像并在Transformer中交错视觉与文本输入，实现预填充和推理阶段的全局上下文压缩，带来显著的速度和资源节省（4×压缩下3×加速，内存降77%，FLOPS降74%）。


<details>
  <summary>Details</summary>
Motivation: 早期将Transformer输入渲染为图像以减少token数量，但这些方法只在预填充阶段压缩，未在逐token推理时节省计算和内存，因而需要一种可以在两阶段均压缩的全局上下文压缩方法。

Method: 提出将文本分块转成草图图像并与对应的文本块交错输入Transformer；训练采用多阶段流程：课程调度的光学语言建模预训练，随后进行模态交错的指令微调。模型从0.6B到8B规模进行实验。

Result: 在4×压缩比下，VIST2在长文本写作任务上优于基线：首次token生成平均加速3×，内存使用减少77%，FLOPS降低74%。

Conclusion: VIST2通过在预上下文阶段仅依赖视觉token来预测下一个文本token，从而实现了在预填充和逐token推理阶段的全局上下文压缩，显著降低了计算与内存开销。

Abstract: Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.

</details>


### [46] [Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer](https://arxiv.org/abs/2601.10386)
*Filippo Ruffini,Camillo Maria Caruso,Claudia Tacconi,Lorenzo Nibid,Francesca Miccolis,Marta Lovino,Carlo Greco,Edy Ippolito,Michele Fiore,Alessio Cortellini,Bruno Beomonte Zobel,Giuseppe Perrone,Bruno Vincenzi,Claudio Marrocco,Alessandro Bria,Elisa Ficarra,Sara Ramella,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出一个对缺失模态鲁棒的多模态生存预测方法，利用FM提取特征并做缺失感知中间融合，在II-III期不可切除NSCLC上表现优于其他融合策略，WSI+临床融合达到73.30 C-index。


<details>
  <summary>Details</summary>
Motivation: 临床数据中模态缺失与样本量小限制了多模态深度学习在NSCLC生存预测中的应用，需设计能处理缺失模态且能整合异构数据的模型。

Method: 利用Foundation Models进行每种模态（CT、WSI、临床变量）的特征提取，结合一种缺失感知编码策略进行中间层融合，从而在自然不完整的模态配置下进行训练和推理，无需丢弃患者或进行激进插补。

Result: 中间融合在测试中持续优于单模态及早/晚期融合；WSI+临床融合效果最佳，C-index为0.7330；模型能自适应下调信息量较低的模态（如CT）的权重。

Conclusion: 该论文提出了一个对缺失模态鲁棒的多模态生存预测框架，适用于不可切除II-III期NSCLC，并表明中间融合策略在含缺失模态的场景下优于单模态、早期和晚期融合。

Abstract: Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.

</details>


### [47] [Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy](https://arxiv.org/abs/2601.10392)
*Hassan Eshkiki,Sarah Costa,Mostafa Mohammadpour,Farinaz Tanhaei,Christopher H. George,Fabio Caraffini*

Main category: cs.CV

TL;DR: 提出一种将多帧荧光显微镜时间序列融合为单张高质量图像的可解释计算框架，显著提高可视化与细胞检测率（平均+44%）。


<details>
  <summary>Details</summary>
Motivation: 解决荧光显微镜录像噪声大、时间波动和信号随时间振荡导致的可视化不一致问题，通过整合多帧时序信息生成高质量单张图像以保留生物学信息。

Method: 结合可解释的计算机视觉技术，从不同应用领域汲取方法（如时间域信息整合、去噪与增强、可解释性模块等），在多达111种配置下优化并评估生成复合图像的流程以增强信号并保留动态信息。

Result: 提出一个将多帧时间解析信息融合为单张高质量图像的计算框架，在111种配置和复杂心肌细胞二维单层数据集上评估，生成的复合图像在保持并增强帧信息的同时，比现有方法平均提高44%的细胞计数。

Conclusion: 该管道能在保留生物内容的前提下融合时序图像堆栈，提升图像质量与下游标注/分割性能，并可推广至其他需多时相融合的成像领域。

Abstract: Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.

</details>


### [48] [Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation](https://arxiv.org/abs/2601.10449)
*Clementine Grethen,Nicolas Menga,Roland Brochard,Geraldine Morin,Simone Gasparini,Jeremy Lebreton,Manuel Sanchez Gestido*

Main category: cs.CV

TL;DR: Lunar-G2R通过U-Net与可微渲染从DEM直接推断空间可变BRDF，无需影像或特殊硬件，在Tycho陨石坑上显著提升光度拟合与视觉质量，是首个从地形直接推断空间反射率的工作。


<details>
  <summary>Details</summary>
Motivation: 现有月面渲染多依赖简化或空间均匀的BRDF，难以估计参数且无法表现局部反射差异，限制了光度真实感与基于视觉的导航。该工作旨在解决从地形获取可变反射率的缺口。

Method: 提出Lunar-G2R框架：基于U-Net并结合可微渲染，在已知观测与光照几何下，通过最小化真实轨道图像与物理渲染之间的光度差异来学习从地形到反射率的映射；推理时只需DEM，无需多视图或专用设备。

Result: 在Tycho陨石坑的地理学上留出区域上，方法相比最先进基线在光度误差上降低约38%，同时在PSNR、SSIM和感知相似性指标上优于基线，能捕捉到空间均匀模型无法表现的细尺度反射变化。

Conclusion: 该论文提出了一种从月球地形（DEM）直接预测空间变化的BRDF参数的方法，实现了更真实的月面渲染。

Abstract: We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.

</details>


### [49] [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/abs/2601.10477)
*Yu Wang,Yi Wang,Rui Dai,Yujie Wang,Kaikui Liu,Xiangxiang Chu,Yansheng Li*

Main category: cs.CV

TL;DR: 提出SocioSeg数据集与SocioReasoner方法，利用视觉-语言模型和强化学习实现城市社会语义实体的像素级分割，性能和泛化能力优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统分割模型对按物理属性定义的实体表现良好，但对按社会语义定义的类别（如学校、公园）仍存在困难，需引入跨模态和推理能力以理解更抽象的语义。

Method: 构建了SocioSeg数据集（卫星影像、数字地图、分层的像素级社会语义标签），提出SocioReasoner框架：跨模态识别+多阶段推理模拟人工识别注释流程，并通过强化学习优化非可微过程以激发视觉-语言模型的推理能力。

Result: 在实验中，SocioReasoner相较于最先进的模型取得了性能提升，同时在零样本场景下表现强劲，数据集与代码已开源。

Conclusion: 本文通过利用视觉-语言模型的推理能力，实现了对社会语义实体的像素级分割，展示出优于现有模型的性能并具备较强的零样本泛化能力。

Abstract: As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.

</details>


### [50] [mergetune: Continued fine-tuning of vision-language models](https://arxiv.org/abs/2601.10497)
*Wenqing Wang,Da Li,Xiatian Zhu,Josef Kittler*

Main category: cs.CV

TL;DR: 提出Continued Fine-Tuning（CFT）范式与MERGETUNE方法，通过线性模态连通性在无额外参数与免重放数据条件下将零-shot与微调解合并，从而恢复被微调过程遗忘的预训练知识，显著提升基准与泛化性能。


<details>
  <summary>Details</summary>
Motivation: Fine-tuning VLMs causes catastrophic forgetting of pretrained knowledge; instead of only preventing forgetting during adaptation, the paper aims to recover that lost knowledge after adaptation by merging zero-shot and fine-tuned solutions.

Method: Start from a fine-tuned model and continue fine-tuning its trainable parameters to find a model with low-loss paths to both the zero-shot and fine-tuned checkpoints; use LMC as guidance and approximate the LMC constraint with a second-order surrogate to avoid replaying pretraining data.

Result: MERGETUNE improves harmonic mean for base-novel generalisation by +5.6% over CoOp, surpasses ensemble baselines in robust fine-tuning with lower inference cost, and achieves state-of-the-art when ensembled with zero-shot model; also achieves first-time superior CLIP performance on DTD and EuroSAT in cross-dataset transfer.

Conclusion: The paper proposes Continued Fine-Tuning (CFT) and MERGETUNE to recover pretrained knowledge after fine-tuning VLMs, leveraging linear mode connectivity to merge zero-shot and fine-tuned solutions without extra parameters, approximating LMC via a second-order surrogate to avoid data replay, and demonstrates improved performance over baselines.

Abstract: Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.

</details>


### [51] [SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction](https://arxiv.org/abs/2601.10512)
*Kanak Mazumder,Fabian B. Flohr*

Main category: cs.CV

TL;DR: SatMap fuses satellite BEV imagery with multi-view camera inputs to estimate vectorized HD maps online, significantly improving mAP over camera-only and camera-LiDAR baselines and proving robust in long-range/adverse weather.


<details>
  <summary>Details</summary>
Motivation: Improve online HD map construction for autonomous driving by leveraging satellite imagery to overcome camera depth limits and occlusion, providing global context and priors for better vectorized map prediction.

Method: Method: propose SatMap, integrates satellite maps with multi-view camera observations, predicts vectorized HD map directly, uses lane-level semantics and texture from satellite BEV as global prior to mitigate depth ambiguity and occlusion.

Result: On nuScenes, SatMap yields 34.8% mAP improvement over camera-only baseline and 8.5% over camera-LiDAR fusion baseline; also shows benefits in long-range and adverse weather conditions.

Conclusion: Using satellite prior maps in online HD map estimation substantially improves accuracy and robustness, helping downstream AD modules; SatMap outperforms baselines on nuScenes and is effective in challenging conditions.

Abstract: Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.

</details>


### [52] [BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition](https://arxiv.org/abs/2601.10521)
*Max A. Buettner,Kanak Mazumder,Luca Koecher,Mario Finkbeiner,Sebastian Niebler,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 该论文提出了FUSE-Bike骑行者视角的感知平台与BikeActions多模态数据集，聚焦密集共享空间中易受伤路用户（骑行者）的动作预测，公开硬件设计、数据与基准代码。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于车辆视角下的行人过街行为，缺乏针对密集共享空间中VRU（尤其骑行者）交互与动作预测的数据与平台。

Method: 构建装有双LiDAR、相机和GNSS的FUSE-Bike平台，采集近距离高保真数据；标注并发布BikeActions数据集（852个样本、5类动作）；在公开数据划分上评估图卷积和Transformer模型以建立基线；提供数据整理工具、开源硬件与基准代码。

Result: 公开发布FUSE-Bike平台、BikeActions数据集（852样本、5类）、数据工具、硬件设计与基准代码，并报告基于GNN与Transformer模型的性能基线。

Conclusion: 作者发布了首个骑行者视角的开放感知平台与数据集，并提供基准实验，促进VRU动作理解研究。

Abstract: Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.

</details>


### [53] [SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery](https://arxiv.org/abs/2601.10535)
*Chong Liu,Luxuan Fu,Yang Jia,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: 该文提出了SVII-3D，一种面向稀疏影像的统一资产数字孪生框架，通过LoRA微调的开集检测与空间注意力匹配实现跨视图关联，结合几何引导的精细化实现分米级定位，并引入多模态视觉-语言代理进行运行状态诊断，从而在识别精度和定位误差上显著提升。


<details>
  <summary>Details</summary>
Motivation: 在智慧城市和设施生命周期管理中，基于稀疏且廉价的影像实现高精度的资产数字孪生面临鲁棒性差、定位不准及缺乏细粒度状态理解的挑战，现有方法难以同时满足跨视图关联、精确几何恢复与语义诊断三方面需求。

Method: 1) 使用LoRA微调的开集目标检测模型，结合空间注意力匹配网络进行跨视图对象关联；2) 设计几何引导的细化模块，利用几何约束修正三维结构并达到分米级定位精度；3) 集成多模态视觉-语言模型代理，使用多模态提示自动诊断设备运行状态。

Result: 实验表明SVII-3D显著提升了识别准确率并减少定位误差，实现分米级三维定位和细粒度状态识别，证明其在大规模基础设施数字孪生任务中的有效性与成本效益。

Conclusion: SVII-3D在稀疏影像条件下提升了目标识别鲁棒性与跨视图匹配能力，借助几何约束实现了精确3D定位，并通过VLM代理实现细粒度状态理解，整体上为基础设施数字孪生提供了可扩展、低成本且高保真的解决方案。

Abstract: The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.

</details>


### [54] [Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning](https://arxiv.org/abs/2601.10537)
*Oscar H. Ramírez-Agudelo,Akshay N. Shewatkar,Edoardo Milana,Roland C. Aydin,Kai Franke*

Main category: cs.CV

TL;DR: 本文提出使用深度学习模型（FFA-Net与AECR-Net）增强烟雾与雾霾环境下的模拟仪表盘图像，可生成合成数据集（≈14,000张），在合成雾霾上取得SSIM≈0.98、PSNR≈43dB，AECR-Net表现优于FFA-Net；对烟雾场景效果差但仍有意义。


<details>
  <summary>Details</summary>
Motivation: 在雾霾与烟雾环境下仪表可见性下降，影响基础设施监测与应急救援，需提高仪表自动读取的鲁棒性以辅助救援。

Method: 作者使用Unreal Engine生成合成数据集（14k张），将图像按80/10/10分为训练/验证/测试集；采用FFA-Net和AECR-Net对图像去雾/去烟，评估指标为SSIM和PSNR，比较两模型表现。

Result: 在合成雾霾数据上，模型性能优异（SSIM≈0.98，PSNR≈43dB），AECR-Net比FFA-Net更稳健；在合成烟雾数据上效果较差但仍可改善图像以支持自动读取。

Conclusion: 深度去雾网络能显著提升模拟仪表图像在雾霾/烟雾环境下的可读性，AECR-Net整体更鲁棒；烟雾去除更具挑战性，但处理后图像可用于后续自动读数。

Abstract: Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\% train, 10\% validation, and 10\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges

</details>


### [55] [Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure](https://arxiv.org/abs/2601.10551)
*Luxuan Fu,Chong Liu,Bisheng Yang,Zhen Dong*

Main category: cs.CV

TL;DR: Domain-adapted VLM pipeline: Grounding DINO + LoRA-tuned Qwen-VL + dual-modality RAG yields strong detection and attribute recognition for roadside infrastructure


<details>
  <summary>Details</summary>
Motivation: Bridge gap between general VLMs and domain-specific accuracy for urban infrastructure analysis

Method: Open-vocabulary + LoRA + RAG

Result: 58.9 mAP detection; 95.5% attribute recognition accuracy on new dataset

Conclusion: Combining open-vocabulary localization, lightweight fine-tuning, and knowledge-grounded retrieval produces reliable, standards-compliant infrastructure perception.

Abstract: Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.

</details>


### [56] [Inference-time Physics Alignment of Video Generative Models with Latent World Models](https://arxiv.org/abs/2601.10553)
*Jianhao Yuan,Xiaofeng Zhang,Felix Friedrich,Nicolas Beltran-Velez,Melissa Hall,Reyhane Askari-Hemmat,Xiaochuang Han,Nicolas Ballas,Michal Drozdzal,Adriana Romero-Soriano*

Main category: cs.CV

TL;DR: They improve physics plausibility in video generation by using a latent world model to reward and guide inference-time trajectory selection (WMReward), achieving strong empirical gains and winning ICCV 2025 PhysicsIQ Challenge.


<details>
  <summary>Details</summary>
Motivation: Current video generative models produce realistic visuals but often violate physical laws, limiting practical use; authors hypothesize inference strategies contribute to this and propose treating physics plausibility as an inference-time alignment problem.

Method: Treat physics plausibility as an inference-time alignment; use VJEPA-2 latent world model to compute a physics reward (WMReward) and perform search over multiple candidate denoising trajectories to select/steer outputs, scaling test-time compute for better performance; validated across image-, multiframe-, and text-conditioned settings and via human preference studies.

Result: Introduction of WMReward: using a latent world model (VJEPA-2) as a physics prior reward to search and steer multiple denoising trajectories at inference, improving physics plausibility across various conditioning settings; won ICCV 2025 PhysicsIQ Challenge with 62.64% score, +7.42% over prior SOTA.

Conclusion: Latent world models can be used at inference time as physics priors to steer denoising trajectories and substantially improve physics plausibility of video generation; approach is general beyond VJEPA-2.

Abstract: State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.

</details>


### [57] [DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery](https://arxiv.org/abs/2601.10554)
*Constantin Selzer,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 这篇论文引入了DeepUrban数据集——一个由无人机俯拍的城市密集交通场景数据集，包含3D交通对象、地图与场景信息，旨在提升轨迹预测与规划基准在稠密城市环境下的效果。作者在nuScenes上结合DeepUrban评估了多种SOTA方法，报告了在ADE/FDE上显著提升（最高44.1%/44.3%）。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏稠密城市交通场景，导致模型在复杂交互建模上受限，因此需要一个覆盖高密度交互的真实数据集来推动预测与规划研究。

Method: 通过与工业伙伴DeepScenario合作，使用大约100米高度无人机拍摄的高分辨率城市路口影像，提取3D交通对象并配套详细地图与场景信息，构建数据集；使用现有SOTA预测与规划模型在nuScenes与nuScenes+DeepUrban上训练与评估，并做泛化能力实验。

Result: 添加DeepUrban到nuScenes后，车辆预测与规划的ADE/FDE分别提高最高达44.1%与44.3%，表明在稠密场景下的数据扩充能大幅提升模型性能。

Conclusion: 将DeepUrban加入训练可以显著提升车辆轨迹预测与规划性能，尤其在密集城市场景下对泛化能力有积极影响。该数据集为研究复杂交通交互提供了有价值的稀缺场景资源。

Abstract: The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban

</details>


### [58] [Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation](https://arxiv.org/abs/2601.10577)
*Serena Grazia De Benedictis,Amedeo Altavilla,Nicoletta Del Buono*

Main category: cs.CV

TL;DR: 提出无监督的数学严格的拓扑一致性检验：把分割掩码映射到数字4-曲线并用Betti数判断其是否将平面划分为两个连通区域。


<details>
  <summary>Details</summary>
Motivation: 传统像素/区域/边界指标无法反映分割的整体拓扑结构，导致在形状或连通性关键的应用中掩码虽得分高但拓扑上错误，故提出基于Jordan定理的拓扑一致性评估。

Method: 从二值掩码提取4-曲线候选（基于数字拓扑像素邻接规则），计算掩码及其补集的Betti数并验证β0=β1=1，同时检查补集是否分为恰好两个8-连通分量。

Result: 提出了一种基于Jordan Curve Theorem的拓扑感知分割评估方法，适配数字平面，通过检测分割掩码能否被提取出满足拓扑约束的4-曲线来判定掩码是否Jordan-segmentatable。方法利用数字拓扑和同调群（Betti数）验证候选曲线的拓扑有效性，若β0=β1=1或补集划分为两个8-连通分量则视为通过。

Conclusion: 该方法提供了一个补充传统评价指标的拓扑正确性判据，特别适用于需要保持形状或连通性完整性的场景（如医学图像），能发现边界小误差下传统指标无法捕捉的拓扑缺陷。

Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.

</details>


### [59] [Adversarial Evasion Attacks on Computer Vision using SHAP Values](https://arxiv.org/abs/2601.10587)
*Frank Mollard,Marcus Becker,Florian Roehrbein*

Main category: cs.CV

TL;DR: Use SHAP values to identify influential pixels/features and perturb them to create imperceptible adversarial examples; more robust than FGSM under gradient hiding.


<details>
  <summary>Details</summary>
Motivation: Gradient-based attacks can be ineffective when gradients are hidden or obfuscated; using model-agnostic SHAP importance provides an alternative route to craft adversarial examples that remain imperceptible to humans.

Method: Compute SHAP values per input feature at inference to rank importance; apply optimized perturbations (likely limited L_p norm) to top-k features to lower model confidence or flip labels; compare success rates and imperceptibility with FGSM.

Result: Paper proposes a SHAP-based white-box adversarial attack on vision models, showing it reduces confidence and induces misclassification, often outperforming FGSM especially when gradients are hidden.

Conclusion: SHAP-guided attacks can effectively generate misclassifications and lower confidence, proving more robust than gradient-based FGSM in scenarios with gradient obfuscation; SHAP provides a reliable importance measure at inference aiding attack design.

Abstract: The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.

</details>


### [60] [Action100M: A Large-scale Video Action Dataset](https://arxiv.org/abs/2601.10592)
*Delong Chen,Tejaswi Kasarla,Yejin Bang,Mustafa Shukor,Willy Chung,Jade Yu,Allen Bolourchi,Theo Moutakanni,Pascale Fung*

Main category: cs.CV

TL;DR: Action100M是用自动化流水线从1.2M教学视频生成≈1亿开放词汇、时序定位的动作片段与层级字幕，训练出的VL-JEPA在多任务上表现优异


<details>
  <summary>Details</summary>
Motivation: 当前动作理解受限于数据规模与开放词汇，需大规模、开放词汇、跨域的视频动作数据来推动物理世界智能

Method: 自动化构建大规模动作视频数据集并用于训练视觉-语言模型

Result: 构建了Action100M：从1.2M教学视频生成约1亿带时序定位、开放词汇动作标注和层级字幕的片段；通过V-JEPA 2分割、Tree-of-Captions和GPT-OSS-120B多轮自我精化产出结构化注释；在Action100M上训练的VL-JEPA展现出数据扩展提升和强零样本性能

Conclusion: Action100M为视频理解与世界建模提供了可扩展的基础数据集，自动化注释流水线和基于大模型的证据聚合是其关键贡献

Abstract: Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.

</details>


### [61] [RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation](https://arxiv.org/abs/2601.10606)
*Peng Chen,Xiaobao Wei,Yi Yang,Naiming Yao,Hui Chen,Feng Tian*

Main category: cs.CV

TL;DR: RSATalker结合mesh驱动的语音到面部运动与3D Gaussian Splatting渲染，新增社会关系编码模块支持血缘/非血缘与平等/不平等关系，训练采用三阶段范式并构建带社交标签的数据集。


<details>
  <summary>Details</summary>
Motivation: 为解决现有方法在真实感、计算效率及社交建模之间的权衡，设计一种既高效又能表达社交关系的多轮对话头像生成方法。

Method: 先用语音驱动mesh面部动作，再将3D Gaussians绑定到网格面以渲染高保真2D视频；引入可学习query的社交感知模块编码关系；采用三阶段训练并用含关系注释的语音-网格-图像三元组数据训练。

Result: RSATalker提出了一种基于3D Gaussian Splatting的多轮对话头像生成框架，结合了网格驱动的面部运动与3D高斯点渲染，从而兼顾逼真外观与高效渲染。

Conclusion: RSATalker在现实感与社交意识方面均优于现有方法，适合VR多轮对话场景。

Abstract: Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.

</details>


### [62] [Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding](https://arxiv.org/abs/2601.10611)
*Christopher Clark,Jieyu Zhang,Zixian Ma,Jae Sung Park,Mohammadreza Salehi,Rohun Tripathi,Sangho Lee,Zhongzheng Ren,Chris Dongjoo Kim,Yinuo Yang,Vincent Shao,Yue Yang,Weikai Huang,Ziqi Gao,Taira Anderson,Jianrui Zhang,Jitesh Jain,George Stoica,Winson Han,Ali Farhadi,Ranjay Krishna*

Main category: cs.CV

TL;DR: Molmo2 是一套基于新公开数据集与高效训练设计的开放VLM，显著提升了视频理解与像素级定位能力，在多项任务上超越现有开源和部分闭源模型。


<details>
  <summary>Details</summary>
Motivation: 当前最强VLM多为闭源，开放模型往往依赖合成数据或未公开数据/方法，导致社区无法推进性能与可解释像素级对齐；且许多下游任务需要像素级定位能力，现有（包括闭源）模型通常不具备。

Method: 收集并公开7个新视频数据集与2个多图像数据集（包括详细视频字幕、自由式视频问答、复杂查询的目标跟踪、视频指向数据集），数据采集未使用闭源VLM；提出高效的packing与message-tree编码方案，使用双向视觉tokens注意力与新型token权重策略进行训练。

Result: 在开放权重/数据模型中达到最佳表现；8B模型在短视频、计数、字幕生成表现优越，对长视频也具竞争力；视频定位任务上显著超越现有开放模型（如Qwen3-VL）并在部分任务超过闭源模型（如在视频指向和跟踪任务上分别优于Gemini 3 Pro）。

Conclusion: Molmo2 提供了开放权重与数据的视频语言模型（VLM）家族，显著提升了开放源代码模型在视频理解与像素级定位（pointing、tracking）上的能力。

Abstract: Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).

</details>


### [63] [CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos](https://arxiv.org/abs/2601.10632)
*Chengfeng Zhao,Jiazhi Shu,Yubo Zhao,Tianyu Huang,Jiahao Lu,Zekai Gu,Chengwei Ren,Zhiyang Dou,Qing Shuai,Yuan Liu*

Main category: cs.CV

TL;DR: 提出CoMoVi，通过耦合两路视频扩散模型在单次去噪中同步生成3D动作与视频，利用预训练VDM的先验并设计双分支互交互架构与新数据集，提升动作与视频生成一致性与多样性。


<details>
  <summary>Details</summary>
Motivation: 观察到3D动作与2D视频生成相互依赖：3D提供结构先验保证连贯性，预训练视频模型提供运动泛化能力，因而应将两者生成过程耦合以互补优势并提高生成质量与一致性。

Method: 设计双分支扩散模型（视频分支+运动分支），使用新的2D人体动作表示以继承预训练VDM先验，采用互特征交互与3D-2D交叉注意力在单一去噪循环中同步生成，并构建CoMoVi数据集用于训练与评估。

Result: CoMoVi: 联合生成3D动作与视频的扩散框架，通过耦合双分支视频扩散模型在单一去噪循环中同步生成3D人体动作与2D视频。提出有效的2D人体动作表示、3D-2D交互注意力与互通特征交互，并构建大规模带文本与动作标注的数据集。

Conclusion: CoMoVi实现了3D动作与视频的同步生成，证明耦合生成能提升动作合理性与视频质量，且自建数据集推动了任务发展。

Abstract: In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.

</details>


### [64] [CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning](https://arxiv.org/abs/2601.10649)
*Darshan Singh,Arsha Nagrani,Kawshik Manikantan,Harman Singh,Dinesh Tewari,Tobias Weyand,Cordelia Schmid,Anelia Angelova,Shachi Dave*

Main category: cs.CV

TL;DR: CURVE是一个覆盖18地区、本地语言、人工注释的多文化视频推理基准；基于推理链的证据图和迭代策略可定位模型在文化感知与推理上的缺陷，现有Video-LLM表现远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 当前长视频理解基准对西方与英语数据偏重，导致评估偏差，需要一个多文化、多语言的基准来评估模型在文化语境下的理解和推理能力。

Method: 构建含18个地区本地语言的视频问答数据集，所有标注由人工完成，包含问题、答案与多步推理链；基于推理链构建证据图，并提出一种迭代策略利用证据图定位细粒度推理错误。

Result: 实验证明最先进的Video-LLMs在CURVE上远低于人类水平，主要错误来源于对文化元素的视觉感知失误；提出的方法能用于更细致地识别模型推理错误。

Conclusion: 该论文提出了CURVE基准，填补了多文化多语言视频理解评估空白，强调文化语境理解重要性并展示现有Video-LLM在此任务上性能显著不足。

Abstract: Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural

</details>


### [65] [A continental-scale dataset of ground beetles with high-resolution images and validated morphological trait measurements](https://arxiv.org/abs/2601.10687)
*S M Rayeed,Mridul Khurana,Alyson East,Isadora E. Fluck,Elizabeth G. Campolongo,Samuel Stevens,Iuliia Zarubiieva,Scott C. Lowe,Michael W. Denslow,Evan D. Donoso,Jiaman Wu,Michelle Ramirez,Benjamin Baiser,Charles V. Stewart,Paula Mabee,Tanya Berger-Wolf,Anuj Karpatne,Hilmar Lapp,Robert P. Guralnick,Graham W. Taylor,Sydne Record*

Main category: cs.CV

TL;DR: 该研究将NEON地面甲虫实体标本数字化，构建包含13,200+枚Carabidae样本的多模态数据集（高分辨率图像+数字化外形特征），并用AI方法实现亚毫米级精度的鞘翅长度/宽度自动提取，推动无脊椎动物性状数据库与自动化物种识别研究。


<details>
  <summary>Details</summary>
Motivation: 现有全球性状数据库严重偏向脊椎动物与植物，限制了对高多样性无脊椎动物群（如地面甲虫）的生态学研究；NEON虽保存大量实体标本，但缺乏数字化资源与可扩展的自动化性状提取工具，阻碍大尺度分析与AI应用。

Method: 从NEON 30个站点采集13,200余枚Carabidae标本的高分辨率多视角图像；研发并校准电子测量流程以提取每个标本的鞘翅长度与宽度；与人工测量进行对比验证精度；构建可供AI训练的标注数据集用于后续自动化鉴定与性状提取。

Result: 成功数字化13,200+枚地面甲虫并提取每枚样本的鞘翅长宽，数字测量与人工测量差异在亚毫米范围内；数据覆盖美国本土与夏威夷30个站点，为AI驱动的物种识别与性状研究提供大规模可靠数据。

Conclusion: 通过高分辨率成像与可信的数字化测量，该数据集显著降低了NEON地面甲虫标本的获取门槛，填补无脊椎动物性状数据空白，并为基于AI的物种鉴定和性状研究提供可靠基础，促进生物多样性监测与保护研究。

Abstract: Despite the ecological significance of invertebrates, global trait databases remain heavily biased toward vertebrates and plants, limiting comprehensive ecological analyses of high-diversity groups like ground beetles. Ground beetles (Coleoptera: Carabidae) serve as critical bioindicators of ecosystem health, providing valuable insights into biodiversity shifts driven by environmental changes. While the National Ecological Observatory Network (NEON) maintains an extensive collection of carabid specimens from across the United States, these primarily exist as physical collections, restricting widespread research access and large-scale analysis. To address these gaps, we present a multimodal dataset digitizing over 13,200 NEON carabids from 30 sites spanning the continental US and Hawaii through high-resolution imaging, enabling broader access and computational analysis. The dataset includes digitally measured elytra length and width of each specimen, establishing a foundation for automated trait extraction using AI. Validated against manual measurements, our digital trait extraction achieves sub-millimeter precision, ensuring reliability for ecological and computational studies. By addressing invertebrate under-representation in trait databases, this work supports AI-driven tools for automated species identification and trait-based research, fostering advancements in biodiversity monitoring and conservation.

</details>


### [66] [See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection](https://arxiv.org/abs/2601.10707)
*Amir Mallak,Erfan Aasi,Shiva Sreeram,Tsun-Hsuan Wang,Daniela Rus,Alaa Maalouf*

Main category: cs.CV

TL;DR: 通过随机遮掩patch特征并保持空间布局，SPS 强制策略依赖对不同子集不变的稳健特征，提升 OOD 泛化与效率。


<details>
  <summary>Details</summary>
Motivation: 发现来自foundation model（BLIP2）的patch特征高度冗余，导致策略学到脆弱/虚假的相关性，降低 OOD 鲁棒性，提出通过随机选择patch减少冗余并提升泛化。

Method: 对输入帧的patch描述符进行随机遮掩（保留空间位置），政策只接收部分token，训练时每帧采样不同子集，保持完整但随机的场景投影；评估包含闭环仿真和真实车转移。

Result: Stochastic-Patch-Selection (SPS) 通过随机屏蔽部分 patch 描述符，减少 BLIP2 提取的patch特征冗余，从而提高策略的 OOD 鲁棒性、泛化性和效率。实验显示在多种 OOD 场景中优于 SOTA，平均提升6.2%，最高可达20.4%，并且速度提升2.4倍，训练了9个系统并做了mask率与特征重组消融，其中8个超过SOTA，且策略可零调到真实车辆。

Conclusion: SPS 能有效缓解由自注意力产生的patch特征冗余导致的过拟合，从而提高 OOD 性能和推理速度，并能迁移到真实车辆。

Abstract: Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.

</details>


### [67] [From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion](https://arxiv.org/abs/2601.10710)
*Cheng Chen,Yuyu Guo,Pengpeng Zeng,Jingkuan Song,Peng Di,Hang Yu,Lianli Gao*

Main category: cs.CV

TL;DR: CLI通过多层视觉特征与LLM上下文门控融合，解决VLM中的视觉特征瓶颈，使LLM按需访问视觉层次信息，显著提升多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 现行VLM通常只将视觉编码器的最终输出接入LLM，形成单点、静态连接，导致视觉信息的层次结构（从低级细节到高级语义）难以被LLM充分利用，限制了多模态推理能力。

Method: 提出CLI框架，包括Adaptive Multi-Projection (AMP) 用于对视觉编码器不同层的特征进行投影与规范化，以及Adaptive Gating Fusion (AGF) 允许LLM在解码时基于上下文动态选择并融合最相关视觉信息。将CLI作为轻量模块集成到现有VLM（如LLaVA-OneVision和LLaVA-1.5），通过端到端或部分训练优化参数效率。

Result: 在18个多样化基准上对LLaVA-OneVision与LLaVA-1.5集成CLI后均获得显著改进，显示了CLI在提升多模态理解、细粒度推理与语义一致性方面的优势，且参数开销小，易于扩展到其他VLM。

Conclusion: 本文提出的Cross-Layer Injection (CLI) 能显著缓解现有视觉-语言模型中视觉特征瓶颈问题，通过构建视觉编码器多层与大模型多点之间的动态双向桥接，实现更细粒度的跨层信息融合，提升模型对局部细节与全局语义的协调能力，实验显示在多项基准上性能显著提升，验证了方法的有效性和可扩展性。

Abstract: Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.

</details>


### [68] [Alterbute: Editing Intrinsic Attributes of Objects in Images](https://arxiv.org/abs/2601.10714)
*Tal Reiss,Daniel Winter,Matan Cohen,Alex Rav-Acha,Yael Pritch,Ariel Shamir,Yedid Hoshen*

Main category: cs.CV

TL;DR: 提出Alterbute，一种基于扩散的图像编辑方法，可在保持物体身份和场景上下文的同时修改物体的固有属性（颜色、纹理、材料、形状）。关键贡献包括松弛训练目标允许模型在训练时对内在与外在属性都进行改变，而推理时通过重用原始背景与遮罩限制外在变化；以及引入视觉命名实体（VNEs）作为细粒度身份类别，并用视觉-语言模型自动从大规模图像数据集提取VNE标签与属性描述以进行可扩展监督。实验证明Alterbute在保持身份的前提下编辑固有属性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么不依赖监督但难以保持物体身份，要么依赖过度严格的监督而无法产生有意义的内在属性变化，故需一种既能保持身份又能产生丰富内在变化的方法。

Method: 采用条件扩散模型，训练时使用身份参考图像、描述目标固有属性的文本提示、背景图和物体遮罩，训练目标允许模型同时改变内在与外在属性；推理时固定原始背景与遮罩以限制外在变化；利用视觉-语言模型自动构建细粒度VNE标签和属性文本用于监督。

Result: 在身份保持的固有属性编辑任务上，Alterbute在定性与定量评估中均超越现有方法，展示出在颜色、纹理、材料乃至形状修改时的有效性与鲁棒性。

Conclusion: Alterbute能在不破坏物体识别身份和场景一致性的情况下，有效编辑物体的固有属性，且通过VNE和松弛训练目标实现可扩展的身份保留监督，从而优于现有方法。

Abstract: We introduce Alterbute, a diffusion-based method for editing an object's intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., ''Porsche 911 Carrera'') that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.

</details>


### [69] [WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments](https://arxiv.org/abs/2601.10716)
*Xuweiyi Chen,Wentao Zhou,Zezhou Cheng*

Main category: cs.CV

TL;DR: WildRayZer identifies transient regions via residuals from a static camera-only renderer, distills a motion estimator to mask and gate learning, and provides datasets D-RE10K and D-RE10K-iPhone; achieves superior single-pass NVS in dynamic scenes


<details>
  <summary>Details</summary>
Motivation: handle dynamic scenes where moving objects break multi-view consistency for NVS

Method: analysis-by-synthesis with residual masking

Result: pseudo motion masks, distilled motion estimator, improved transient-region removal and full-frame NVS quality outperforming baselines

Conclusion: residual-based pseudo masks and motion distillation enable robust transient-aware NVS, validated on new dynamic datasets

Abstract: We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [70] [Multiverse: Transactional Memory with Dynamic Multiversioning](https://arxiv.org/abs/2601.09735)
*Gaetano Coccimiglio,Trevor Brown,Srivatsan Ravi*

Main category: cs.DB

TL;DR: TLDR


<details>
  <summary>Details</summary>
Motivation: Summarize motivation

Method: Summarize method

Result: Summarize result

Conclusion: Summarize conclusion

Abstract: Software transactional memory (STM) allows programmers to easily implement concurrent data structures. STMs simplify atomicity. Recent STMs can achieve good performance for some workloads but they have some limitations. In particular, STMs typically cannot support long-running reads which access a large number of addresses that are frequently updated. Multiversioning is a common approach used to support this type of workload. However, multiversioning is often expensive and can reduce the performance of transactions where versioning is not necessary. In this work we present Multiverse, a new STM that combines the best of both unversioned TM and multiversioning. Multiverse features versioned and unversioned transactions which can execute concurrently. A main goal of Multiverse is to ensure that unversioned transactions achieve performance comparable to the state of the art unversioned STM while still supporting fast versioned transactions needed to enable long running reads. We implement Multiverse and compare it against several STMs. Our experiments demonstrate that Multiverse achieves comparable or better performance for common case workloads where there are no long running reads. For workloads with long running reads and frequent updates Multiverse significantly outperforms existing STMS. In several cases for these workloads the throughput of Multiverse is several orders of magnitude faster than other STMs.

</details>


### [71] [The "I" in FAIR: Translating from Interoperability in Principle to Interoperation in Practice](https://arxiv.org/abs/2601.10008)
*Evan Morris,Gaurav Vaidya,Phil Owen,Jason Reilly,Karamarie Fecho,Patrick Wang,Yaphet Kebede,E. Kathleen Carter,Chris Bizon*

Main category: cs.DB

TL;DR: 该论文提出了两种工具：Babel（解决标识符体系异构问题，通过映射等价标识符构建簇并提供API）和ORION（通过摄取知识库并转换为社区管理的统一数据模型来解决数据模型异构问题）。作者展示了这两者如何支持数据互操作，并提供了一个可下载的、完全互操作的知识库库。


<details>
  <summary>Details</summary>
Motivation: 虽然很多资源在单独层面遵循FAIR原则，但由于标识符模式和数据模型的差异，集合层面仍难以实现互操作，亟需工具将这些异构资源整合。

Method: Babel通过整理并映射不同标识符模式形成等价标识符簇（cliques），并通过高性能API暴露这些映射；ORION则摄取多个知识库并将其转换为一个社区管理的统一数据模型，从而解决数据模型不一致的问题。两者配合用于生成一套完全互操作的知识库。

Result: 作者实现并展示了Babel和ORION的功能，说明其能有效支持数据互操作，并提供了在Robokop平台上的可下载互操作知识库。

Conclusion: Babel与ORION联合能够将遵循FAIR原则但实际难以互操作的资源整合为可互操作的知识库，从而拉近原则与实践之间的差距，相关知识库可在Robokop平台访问与下载。

Abstract: The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles [1] promote the interoperability of scientific data by encouraging the use of persistent identifiers, standardized vocabularies, and formal metadata structures. Many resources are created using vocabularies that are FAIR-compliant and well-annotated, yet the collective ecosystem of these resources often fails to interoperate effectively in practice. This continued challenge is mainly due to variation in identifier schemas and data models used in these resources. We have created two tools to bridge the chasm between interoperability in principle and interoperation in practice. Babel solves the problem of multiple identifier schemes by producing a curated set of identifier mappings to create cliques of equivalent identifiers that are exposed through high-performance APIs. ORION solves the problems of multiple data models by ingesting knowledge bases and transforming them into a common, community-managed data model. Here, we describe Babel and ORION and demonstrate their ability to support data interoperation. A library of fully interoperable knowledge bases created through the application of Babel and ORION is available for download and use at https://robokop.renci.org.

</details>


### [72] [Redundancy-Driven Top-$k$ Functional Dependency Discovery](https://arxiv.org/abs/2601.10130)
*Xiaolong Wan,Xixian Han*

Main category: cs.DB

TL;DR: 提出SDP：基于冗余上界剪枝和三项工程优化的top-k FD发现方法，在多数据集上比穷举法更快更省内存。


<details>
  <summary>Details</summary>
Motivation: 传统FD发现算法要枚举所有满足的FD，导致计算复杂度和结果规模都不可接受，尤其在大规模或高维数据上。作者希望通过选择性发现top-k重要FD来降低代价并产出更有用的结果。

Method: SDP对FD搜索空间进行基于冗余上界的剪枝；证明了该上界随属性集增大单调递减；引入三项优化：按分区基数排序属性、使用成对统计信息的分区基数矩阵以收紧上界、全局调度器优先搜索高潜力分支。

Result: 在40+个数据集上的实验表明，SDP在速度和内存消耗方面均明显优于穷举式方法，能够高效找到按冗余计数排序的前k个FD。

Conclusion: 本文提出的SDP方法通过基于冗余计数的top-k选择和上界剪枝，有效减少了FD发现的计算开销和结果规模问题，从而在大多数实验数据集上显著优于穷举方法。

Abstract: Functional dependencies (FDs) are basic constraints in relational databases and are used for many data management tasks. Most FD discovery algorithms find all valid dependencies, but this causes two problems. First, the computational cost is prohibitive: computational complexity grows quadratically with the number of tuples and exponentially with the number of attributes, making discovery slow on large-scale and high-dimensional data. Second, the result set can be huge, making it hard to identify useful dependencies. We propose SDP (Selective-Discovery-and-Prune), which discovers the top-$k$ FDs ranked by redundancy count. Redundancy count measures how much duplicated information an FD explains and connects directly to storage overhead and update anomalies. SDP uses an upper bound on redundancy to prune the search space. It is proved that this upper bound is monotone: adding attributes refines partitions and thus decreases the bound. Once the bound falls below the top-$k$ threshold, the entire branch can be skipped. We improve SDP with three optimizations: ordering attributes by partition cardinality, using pairwise statistics in a Partition Cardinality Matrix to tighten bounds, and a global scheduler to explore promising branches first. Experiments on over 40 datasets show that SDP is much faster and uses less memory than exhaustive methods.

</details>


### [73] [Improving Database Performance by Application-side Transaction Merging](https://arxiv.org/abs/2601.10596)
*Xueyuan Ren,Frank Li,Yang Wang*

Main category: cs.DB

TL;DR: 通过在中间件层合并相似事务并消除冗余读/预计算冲突语句，可以显著提高在线事务处理吞吐，TPC-C和Spree上分别提升至最多2.65X/3.52X。


<details>
  <summary>Details</summary>
Motivation: 观察到不同客户端提交的事务在结构上存在相似性及冗余操作，若在应用层合并这些操作可减少数据库负载和冲突，从而提升整体吞吐。

Method: 提出TransactionMerger中间件：收集来自不同客户端的事务，基于SQL语义合并相似语句、消除冗余读取并预计算跨事务的聚合效果；同时提供静态分析工具检测合并安全性，并对现有应用（TPC-C、Spree）进行事务重写实现。

Result: TransactionMerger提出了一种在应用侧通过合并结构相似的语句/事务来提升事务处理性能的方法，包含语句合并、冗余读消除和跨事务有争用语句的聚合预计算。设计了一个中间件和静态分析工具，并在TPC-C和Spree上实现改写，评测显示吞吐提升最高分别为2.65X和3.52X。

Conclusion: 事务合并在不少实际工作负载上能带来显著吞吐提升，且可通过静态分析保证不违反隔离前提下实现。

Abstract: This paper explores a new opportunity to improve the performance of transaction processing at the application side by merging structurely similar statements or transactions. Concretely, we re-write transactions to 1) merge similar statements using specific SQL semantics; 2) eliminate redundant reads; and 3) merge contending statements across transactions by pre-computing their aggregated effect. Following this idea, we present the design of TransactionMerger, a middleware to collect and merge transactions across different clients. We further present a static analysis tool to identify the merging opportunity without violating isolation as well as our experience of re-writing transactions in TPC-C and Spree, a popular real-world application. Our evaluation shows that such transaction merging can improve TPC-C throughput by up to 2.65X and Spree throughput by 3.52X.

</details>


### [74] [Translating database mathematical schemes into relational database software applications with MatBase](https://arxiv.org/abs/2601.10604)
*Christian Mancas,Diana Christina Mancas*

Main category: cs.DB

TL;DR: 提出将Elementary Mathematical Data Model方案自动翻译为关系模式及非关系约束的伪代码算法，证明其高效、健全、完备且最优，示例应用于族谱树并给出SQL/VBA实现示例与指导


<details>
  <summary>Details</summary>
Motivation: Enable MatBase to enforce constraints and interoperate with relational DBs by converting mathematical data models into relational schemas plus non-relational constraints

Method: Translate EMDM schemes to relational

Result: Pseudocode algorithm presented; proven fast, solid, complete, optimal; applied to genealogical tree scheme; examples of SQL/VBA for enforcing constraints and guidelines for implementation

Conclusion: 算法可高效且完整地将数学数据模型映射为可在关系数据库中实现的模式与约束，实用性通过族谱树案例与代码示例得到验证。

Abstract: We present a pseudocode algorithm for translating our (Elementary) Mathematical Data Model schemes into relational ones and associated sets of non-relational constraints, used by MatBase, our intelligent database management system prototype. We prove that this algorithm is very fast, solid, complete, and optimal. We apply it to a mathematical scheme modeling the genealogical trees subuniverse. We also provide examples of SQL and VBA code for enforcing some of its non-relational constraints, as well as guidelines to develop code for enforcing such constraints.

</details>
