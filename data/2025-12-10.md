<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 110]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

TL;DR: 本文收集了约4100个来自Twitter/GIPHY的GIF，构建了欺凌/非欺凌数据集，并用预训练VGG16进行检测，报告97%准确率，同时公开了数据集。


<details>
  <summary>Details</summary>
Motivation: 动机是当前研究多集中于文本和图像的网络欺凌检测，而针对GIF/贴纸（动图）检测的研究稀缺，且社交媒体上大量使用GIF表达情感，可能被用于传播欺凌内容，因此构建相应的数据集并研究检测方法具有实际意义。

Method: 作者首先从Twitter上提取与网络欺凌相关的标签，然后使用这些标签通过GIPHY API下载GIF，构建了包含约4100个GIF的二分类数据集（欺凌/非欺凌）。接着将GIF转换为帧或图像输入，采用预训练的VGG16模型进行特征提取和分类，实现端到端的深度学习检测。

Result: 在所构建的约4100个GIF数据集上，使用VGG16预训练模型进行了训练与测试，论文报告的最高分类准确率为97%。此外，作者公开了他们的GIF数据集以促进后续研究。

Conclusion: 该论文旨在通过构建推特来源的GIF数据集并使用预训练深度学习模型检测GIF中的网络欺凌。作者得出结论：基于VGG16的模型在所构建数据集上达到了97%的准确率，表明该方法在该数据集上性能良好，并且他们公开了数据集以供后续研究。

Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [2] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 用Planet Labs 4波段影像+深度学习能近实时检测战争中的火灾与烧灼区域，在苏丹5个案例中胜过基线，8波段或时间序列只带来边际收益。


<details>
  <summary>Details</summary>
Motivation: 持续的苏丹战争带来监测挑战，迫切需要快速、近实时的冲突损毁检测手段；深度学习与商业卫星影像的发展使得自动化监测成为可能。

Method: 使用Planet Labs的4波段（可见光+近红外）卫星影像输入深度学习模型（未指明具体架构），对比基线方法，进行火灾与烧灼区域的自动检测和分割；在五个苏丹冲突区域进行案例研究并评估性能；还比较了8波段影像和时间序列数据的增益。

Result: 在五个苏丹案例中，基于4波段影像的深度学习模型比基线方法更准确地捕捉到活动火点和烧黑区域；相较于使用更多波段（8波段）或时间序列影像，性能提升有限。

Conclusion: 该论文结论是：基于Planet Labs的4波段卫星影像与深度学习模型，可以在近实时条件下有效监测战争中火灾及烧灼区域，且在五个苏丹案例中优于基线方法；使用8波段或时间序列影像仅带来边际改进。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [3] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出LivingSwap：使用关键帧条件和视频参考指导的时序拼接来提升长视频换脸的保真度和时间一致性；构建Face2Face数据集并取得SOTA效果，减少人工工作。


<details>
  <summary>Details</summary>
Motivation: 提升视频换脸在长时序和复杂场景下的保真度与时间一致性，通过借鉴参考引导图像编辑利用源视频的丰富视觉属性来改善换脸效果。

Method: 提出LivingSwap：首个视频参考引导换脸模型。使用关键帧作为条件信号注入目标身份，结合视频参考引导执行时间拼接以保证长期序列的身份稳定性与高保真重建。为训练构建Face2Face配对数据集并反转数据对以获得可靠监督。

Result: 在大量实验中，方法在将目标身份与源视频的表情、光照和运动无缝融合方面取得了最先进结果，并显著减少影片制作中的人工工作量。

Conclusion: 通过关键帧条件和视频参考引导的时序拼接策略，LivingSwap在长视频换脸任务上实现了高保真与时间一致性的双重提升，证明了参考引导策略在视频换脸中的有效性。

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [4] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出一个将显式解剖学层级嵌入语义分割的通用框架，通过逐级递归预测、受限输出头、top-down特征调节和概率组合规则实现父子一致性，在牙科全景片数据集TL-pano上显著提升细粒度解剖结构的IoU、Dice和召回，但带来较多假阳性。


<details>
  <summary>Details</summary>
Motivation: 现有的层级感知分割多通过损失函数间接编码解剖学结构，监督薄弱。为提高对解剖结构的直接建模和预测一致性，提出显式嵌入层级的方法以提升细粒度解剖结构分割的准确性和临床合理性。

Method: 提出逐层递归预测框架：在每个类树深度，骨干网络在原图与上一级logits拼接的输入上重新运行。使用Feature-wise Linear Modulation(FiLM)以父类概率调制子类特征空间；设计受限输出头并引入概率组合规则强制父类与子类一致；层级损失结合按层的加权Dice与交叉熵以及一致性项，确保父类预测等于子类之和。

Result: 在新建TL-pano数据集（194张全景片，含牙层和牙槽骨的密集实例与语义标注）上，基于UNet与HRNet的层级变体在5折交叉验证中持续提高IoU、Dice和召回，尤其对细粒度解剖结构效果提升明显，同时生成更具解剖一致性的分割掩码。但层级模型相较基线显示召回上升快于精度，假阳性增加。

Conclusion: 将显式解剖学层级结构嵌入分割模型能在低数据的牙科影像场景下提升性能与临床合理性，尽管需权衡假阳性增加的问题。

Abstract: Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [5] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: 提出专门面向多步、跨图制图推理的FRIEDA基准，评测11款LVLM，结果显示模型性能远落后于人类，揭示空间推理能力的显著缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前图表/信息图理解研究被直接套用于地图，但地图包含分层符号学与跨图定向/距离关系，现有评估不能有效测量地图推理能力，因此需专门基准。

Method: 构建FRIEDA基准：收集多来源真实地图图像并按GIS文献分类，覆盖拓扑、度量、方向三类空间关系；问题设计要求多步推理和跨图推理；在两种设置下评估11个最先进LVLM（直接提供相关地图或需先检索相关地图）。

Result: 在FRIEDA上，顶尖模型如Gemini-2.5-Pro和GPT-5-Think分别只达38.20%和37.20%，表明多步骤制图推理仍未解决，基准为促进空间智能研究提供挑战性测试。

Conclusion: FRIEDA显著揭示了现有大型视觉语言模型在多步骤制图推理任务上的严重不足，最强模型准确率仅约38%，远低于人类84.87%。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [6] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

TL;DR: 该论文提出SSplain，一种针对早产儿视网膜病变（ROP）眼底图像的像素级解释器，通过在优化中同时引入稀疏性和光滑性约束（用ADMM求解带组合约束的优化问题）生成更真实、更符合临床理解的解释图。实验表明在后验准确性和平滑性上优于常用解释器，并能识别与临床判别因素一致的特征，在其他数据集上也有良好泛化性。代码开源。


<details>
  <summary>Details</summary>
Motivation: 神经网络在医学诊断中表现优秀但为黑盒，现有解释方法生成的高亮图常缺乏真实图像结构（平滑与稀疏性），难以被临床医生直观理解与信任。论文希望生成既保留输入图像结构又能解释模型决策的像素级解释图。

Method: 提出Sparse and Smooth Explainer (SSplain)，通过在解释图生成的优化目标中同时加入稀疏性和光滑性约束来保留图像结构。将问题建模为带组合约束的优化问题，并使用交替方向乘子法（ADMM）求解。生成逐像素的重要性图以解释黑盒分类模型。

Result: 在ROP眼底图像任务上，SSplain在后验准确性（post-hoc accuracy）和光滑性评估上优于常用解释器方法。对比显示生成的解释更稀疏、更平滑、且更符合临床可理解的判别特征。在其它公开数据集上也展示了良好的泛化能力。

Conclusion: SSplain通过在解释生成中同时强制稀疏性与光滑性，能产生更真实且更有临床可解释性的像素级解释图，从而提高对黑盒模型输出的理解与信任。代码已公开以便重现与推广。

Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [7] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,Gül Varol,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一个统一模型同时做手语翻译（SLT）和手语-字幕对齐（SSA），三部分：轻量视觉骨干（人体关键点与唇部图像以保护隐私），Sliding Perceiver聚合网络把连续视觉特征映射为词级嵌入，多任务可扩展训练联合优化SLT与SSA。跨语言预训练（BSL与ASL大规模语料），在BOBSL上达SOTA，并在How2Sign上有良好零样本与微调效果。


<details>
  <summary>Details</summary>
Motivation: 目标是构建一个实用且可扩展的手语理解模型，既能将连续手语视频翻译为口语文本，又能将手语与字幕进行时间对齐，便于沟通、语料构建和教育应用。

Method: 模型由三部分组成：1) 轻量视觉骨干：输入人体关键点与口型区域图像，兼顾手势与非手势信息并保护隐私；2) Sliding Perceiver映射器：滑动窗口聚合连续视觉帧，生成词级表示以缩小视觉与文本差距；3) 多任务可扩展训练策略：联合训练SLT与SSA，通过共享表示增强语言与时间对齐能力。并在BSL/ASL大规模数据上进行多语言预训练。

Result: 在BOBSL（BSL）数据集上，模型在SLT与SSA任务上均达到SOTA性能；在How2Sign（ASL）上展示出强的零样本泛化能力并通过微调取得良好SLT表现，证明跨手语预训练的有效性。

Conclusion: 提出的轻量隐私保护视觉骨干、Sliding Perceiver及多任务训练构成了一个统一且可扩展的手语理解框架，在多语言预训练下实现了强泛化与SOTA效果，具备实际应用价值。

Abstract: Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [8] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 通过在训练中随机遮挡频域分量并结合几何变换，能显著提升检测器对未知生成器的泛化能力，同时在模型剪枝下保持性能，提供一种可扩展且节能的深度伪造检测方案。


<details>
  <summary>Details</summary>
Motivation: 提出一种在频域上进行随机遮挡的训练策略，以提高面向未知生成器的通用深度伪造检测模型的泛化能力与计算效率，适应Green AI对低资源消耗的需求。

Method: 在训练阶段对输入图像施加多种遮挡策略（随机遮挡、几何变换），重点是频域遮挡：将图像变换到频域后随机屏蔽部分频率分量；在此基础上训练轻量级检测器并评估在不同生成器（GAN与扩散模型）上的泛化能力，同时研究模型在结构化剪枝下的鲁棒性。

Result: 频域遮挡在多个GAN和扩散模型生成的数据集上显著提升了对未见生成器的检测准确率；相比空间域遮挡和不遮挡的方法，频域遮挡更能保持推广性能；此外，模型在大幅度结构化剪枝后仍保持良好性能，证明了方法的资源效率与可扩展性。

Conclusion: 频域遮挡是一种有效且高效的训练策略，可同时提高深度伪造检测的泛化性与剪枝鲁棒性，适合用于大规模、低资源的深度伪造筛查，推动可持续的通用深伪检测发展。

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [9] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: M2A使用随机空间/频率遮挡+一致性损失+熵最小化进行持续测试时自适应，空间遮挡效果最好。


<details>
  <summary>Details</summary>
Motivation: 在测试时分布偏移导致分类性能下降，但现有CTTA方法依赖复杂掩码设计或不确定性估计。本工作探究是否可用简单随机遮挡来实现稳健适配。

Method: 为每个测试样本生成一短序列遮挡视图（空间或频率），训练目标为跨视图预测一致性损失和输出熵最小化；比较空间（patch/pixel）和频率（all/low/high）遮挡的多种子类型，并在CIFAR/ImageNetC上评估。

Result: M2A提出了一个简单有效的CTTA方法，通过生成一系列随机遮挡视图并结合一致性损失与熵最小化进行适配，使得在强扰动下无需复杂的不确定性估计或注意力机制也能取得良好效果。

Conclusion: 随机空间遮挡结合一致性与熵损失即可在CIFAR10C/CIFAR100C/ImageNetC上显著改善测试时性能，优于或匹配现有CTTA基线；频率遮挡表现较差。

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [10] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: 在统一基准上比较卷积与Transformer变化检测模型，加入预/后处理与融合策略，最终融合模型F1=80.41%。


<details>
  <summary>Details</summary>
Motivation: 保护亚马逊雨林、防止砍伐、维护生物多样性与文化，利用卫星数据与机器学习支持PRODES进行多时相变化检测。

Method: 在统一数据集上评估多种变化检测模型，包括全卷积网络和基于Transformer的自注意力网络；测试不同预处理与后处理技术（连通组件大小过滤、纹理替换、图像增强）；并尝试模型融合策略。

Result: 通过预处理、后处理与模型融合，单模型效果显著提升，融合后达到F1=80.41%，与文献中近期工作相当。

Conclusion: 结合现代架构（如自注意力）、标准化评估流程及工程手段（预/后处理、融合）可提升基于PRODES的森林砍伐检测性能，为更可靠的自动化监测提供参考。

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [11] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: CVP通过target-affinity token和allocentric grid两种类人类视觉组件，提供显式结构化场景表示，显著提升三维空间推理与基准性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法多依赖无结构的表示（点云、体素、patch特征）并通过坐标嵌入隐式注入场景信息，导致缺乏显式高层结构理解，限制了空间推理能力。为此提出受人类视觉机制启发的显式结构化表示。

Method: 在大模型架构中引入两种互补组件：1) target-affinity token（类中央视野），用于聚焦与查询相关的目标对象，引导注意力；2) allocentric grid（类周边视野），用于捕捉全局场景上下文与空间布局；两者结合以形成结构化、上下文感知的三维场景理解。

Result: 在多个3D场景理解基准上，CVP表现优于现有方法，取得了SOTA成绩。实验展示了其在复杂三维环境中的上下文感知与空间推理优势。

Conclusion: 提出的CVP框架通过模拟人类中央视野与周边视野，有效提升了多模态模型在三维场景空间推理任务中的表现，达到了多项基准的最先进水平。

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [12] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出Fourier-RWKV：结合DQ-Shift、Fourier Mix（傅里叶域WKV）与SBM（DSK-Fusion）的多状态感知去雾框架，在保证全局长程建模的同时实现线性复杂度，达成性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决真实世界非均匀雾霾下图像去雾的挑战，目标是在保持全局依赖建模能力的同时降低Transformers的二次复杂度，从而实现实时部署。

Method: 提出Fourier-RWKV框架，基于多状态感知范式：1) 空间形态感知：通过可变形四向令牌位移（DQ-Shift）调整感受野以适应局部雾变；2) 频域感知：在Fourier Mix模块中将RWKV的WKV注意力从空间域扩展到傅里叶域，保持长程依赖并减轻空间衰减；3) 语义关系感知：语义桥模块（SBM）利用动态语义核融合（DSK-Fusion）对齐编码器-解码器特征并抑制伪影。模型实现线性复杂度并兼顾全局与局部信息。

Result: 在多个基准数据集上进行了大量实验，Fourier-RWKV在不同雾霾场景下取得了最先进性能，同时显著降低计算开销，在恢复质量与效率之间达成良好权衡。代码已公开。

Conclusion: Fourier-RWKV通过多状态感知与傅里叶域WKV扩展，有效提升了去雾性能并降低复杂度，适合实际实时应用。

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [13] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: 作者在KITTI上评估69个单目深度模型，通过仿射误差分解发现：模型越准确不一定越“像人”，呼吁建立多维的人本评估指标。


<details>
  <summary>Details</summary>
Motivation: 动机在于探索深度估计模型表示是否与人类感知对齐，因为在人类感知与鲁棒性、可解释性之间存在潜在联系，且物体识别领域已发现准确性与人类相似性之间的复杂权衡。

Method: 作者系统性地评估了69个单目深度估计器在KITTI数据集上的表现，采用仿射拟合将预测误差分解为可解释的分量，并计算模型与人类估计之间的相关性和相似性度量以分析误差结构。

Result: 发现人类和DNN在某些估计偏差上有正相关，但总体上模型准确性与人类相似性之间存在不同的权衡关系，表明仅优化准确率无法保证人类式行为。

Conclusion: 该论文结论是：提升单目深度估计模型在传感器基准上准确率并不必然使其更符合人类感知行为，二者存在权衡关系，因此需要超越传统准确率的多维度人本评估指标。

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [14] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: GeoLoom利用自动形式化与蒙特卡罗约束求解，在新构建的GeoNF数据集上实现了高结构保真度的文本到几何图形生成。


<details>
  <summary>Details</summary>
Motivation: 几何图形生成要求严格的空间精度且有明确约束，结合形式语言与符号求解器能提高正确性和可解释性，因此提出面向生成的形式化方法。

Method: 提出两部分：1) 自动形式化模块将自然语言描述翻译为GeoLingua；2) 坐标求解器采用高效的蒙特卡罗优化将形式约束映射为精确坐标。并构建了约束驱动的评估指标用于迭代优化。

Result: 构建了GeoNF数据集并通过实验表明GeoLoom在结构保真度上显著优于现有基线方法。

Conclusion: GeoLoom通过将自然语言自动形式化为生成导向的形式语言GeoLingua，并使用蒙特卡罗优化解约束为坐标，显著提升了几何图形生成的结构保真度，提供了解释性与可扩展性的解决方案。

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [15] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

TL;DR: 通过分析、轻量化网络设计和少量样本微调，论文实现了可在MCU设备上高效、可适配的动物重识别，模型尺寸大幅减小且保持竞争性准确率。


<details>
  <summary>Details</summary>
Motivation: 在野外大范围部署动物监测或畜牧管理时，通信受限且需要在项圈或低功耗边缘设备上本地推理，但现有Animal Re-ID模型过大或要求高分辨率输入，难以直接部署到MCU级设备。

Method: 从三方面入手：1) 量化并分析现有SOTA模型与MCU硬件之间的差距，发现直接知识蒸馏在低内存和低分辨率下收益有限；2) 基于MobileNetV2骨干网络，系统性地为低分辨率输入缩放网络架构以得到高精度且紧凑模型；3) 提出数据高效的微调方法，在新站点仅需每个个体三张图像即可快速适配。

Result: 在六个公开数据集上，该紧凑模型在检索准确率上具备竞争力，同时模型体积缩小了两个数量级以上。在自采牛群数据集上，部署到MCU级设备实现了完全本地推理，准确率仅小幅下降，Top-1准确率与集群版本持平。

Conclusion: 本论文证明在MCU级设备上实现可行且实用的动物重识别（Animal Re-ID），通过设计紧凑的网络结构、分析蒸馏局限并提出小样本微调策略，实现高效部署。

Abstract: Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [16] [Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement](https://arxiv.org/abs/2512.08215)
*Chia-Hern Lai,I-Hsuan Lo,Yen-Ku Yeh,Thanh-Nguyen Truong,Ching-Chun Huang*

Main category: cs.CV

TL;DR: Blur2Sharp 用 Human NeRF 提供几何约束，再用条件扩散模型精修渲染图，结合 SMPL 提取的纹理/法线/语义特征进行层次融合，实现从单视图生成清晰且几何一致的新视角/新姿态人像。


<details>
  <summary>Details</summary>
Motivation: 当前单视图生成多视角/新姿态人像要么几何不一致，要么牺牲清晰度。目标是同时保证几何一致性与高分辨率细节。

Method: 分析方法

Result: 提出 Blur2Sharp 框架，结合3D-aware Neural Radiance Fields（Human NeRF）用于生成几何一致的多视图渲染，随后用条件扩散模型（diffusion）对渲染图像进行细化。采用双重条件（NeRF渲染 + 分层纹理/法线/语义先验），并通过层次特征融合提高细节与一致性。

Conclusion: Blur2Sharp 在单视图到多视角/新姿态生成任务上优于现有方法，特别在宽松服装和遮挡场景下能生成更清晰、几何一致的图像。

Abstract: The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

</details>


### [17] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 提出VisKnow框架构建视觉知识库（示例AnimalKB），将多模态文本与图像区域注释结构化为知识图谱，能提升零样本识别、细粒度VQA等任务并提供新基准。


<details>
  <summary>Details</summary>
Motivation: 构建系统化的多模态、对象级知识库以提升计算机视觉中对对象的深入理解，弥补现有任务导向数据缺乏系统组织的问题。

Method: 提出VisKnow框架，将多模态对象知识结构化为图谱，将增强对齐文本与图像源知识与对象与部位级别的区域标注结合，融合专家设计与大规模模型自动化抽取，生成面向类别的知识三元组、图像和区域注释。并以AnimalKB为案例，覆盖406类动物、22K文本三元组、420K图像及对应区域标注。

Result: 构建出AnimalKB后，在零样本识别、细粒度视觉问答等对象级视觉任务上表现出增强性能，并可作为知识图谱补全与部件分割的挑战性基准。

Conclusion: 自动构建的视觉知识库通过将文本知识与图像与区域标注结合，能推进视觉理解研究并支持实用应用。

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [18] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文探讨将提示调优方法从NLP迁移到3D目标检测，研究基于Waymo大规模预训练模型的适配性，提出场景导向提示池（SOP^2）并验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLM在NLP中通过提示调优实现低成本适配激发了把类似方法应用于3D目标检测的想法。作者希望验证大规模3D检测模型（如在Waymo上训练）能否作为基础模型，并通过提示机制高效适配其他场景或数据集。

Method: 文章按序研究了两类提示方法：可学习的提示tokens与提示生成器（prompt generators），并进一步提出Scene-Oriented Prompt Pool（SOP^2），即为不同场景维护一个提示池，按场景选择或组合提示以引导检测模型。研究可能包含在Waymo上预训练的基线检测器、提示插入位置、提示长度、聚类或检索机制来选择场景提示等实施细节。

Result: 实验表明：1) 提示调优在3D目标检测中可行，能在保持大部分预训练权重不变的情况下提升跨场景适应性；2) 提示池（SOP^2）优于单一全局提示或随机提示，能针对不同场景提供更合适的调整，从而提升检测精度和鲁棒性；3) 不同提示设计（token数、生成器结构）对效果有显著影响。

Conclusion: 提示调优方法可有效迁移到3D目标检测，且场景导向的提示池（SOP^2）是一种简单而有效的策略，未来可进一步探索提示的自动选择、跨域迁移与与更大模型结合的潜力。

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [19] [New VVC profiles targeting Feature Coding for Machines](https://arxiv.org/abs/2512.08227)
*Md Eimran Hossain Eimon,Ashan Perera,Juan Merlos,Velibor Adzic,Hari Kalva*

Main category: cs.CV

TL;DR: 针对机器/特征编码，分析VVC工具影响并提出三种轻量化配置，在显著加速编码的同时保持或小幅改善压缩效率。


<details>
  <summary>Details</summary>
Motivation: 传统视频编码以视觉感知为优化目标，但在分布式推理场景中传输的是抽象中间特征，感知保真不再重要，需针对特征数据重新优化编码器以提升速度并保持机器任务性能。

Method: 对MPEG-AI FCM标准下的VVC编码工具进行逐项消融/工具级分析，评估各工具对压缩效率（BD-Rate）和下游视觉任务准确率的影响；基于分析结果选择和组合必要工具，形成三种轻量化配置，并在编码时间与BD-Rate上进行对比基准测试。

Result: 提出的三种VVC精简配置：Fast（BD-Rate提升2.96%，编码时间减少21.8%）；Faster（BD-Rate提升1.85%，编码时间减少51.5%）；Fastest（编码时间减少95.6%，仅1.71% BD-Rate损失）。这些配置在不显著牺牲下游任务准确率前提下大幅加速编码。

Conclusion: VVC在编码中间特征（非像素数据）时表现良好，但需重新评估和简化工具集以在保持任务准确率下提高速度。三种精简配置（Fast、Faster、Fastest）在不同速度/效率权衡下表现优秀。

Abstract: Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.

</details>


### [20] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: 提出MM-CoT基准，通过选择题与对抗干扰检测多模态模型的视觉落地与逻辑连贯性。实验表明许多模型在此任务上失败，说明生成流畅与真实推理能力存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准侧重生成解释而忽视验证能力，无法评估模型所生成推理链是否真实依据视觉证据且逻辑上有效。需要一个专门测量视觉落地与逻辑连贯性的诊断工具。

Method: 设计了一个选择题式的诊断基准，要求模型从多个事件链中选出唯一满足两个正交约束（视觉一致性与逻辑连贯性）的项。通过构造对抗性干扰项（每个干扰项违反其中一个约束）来暴露模型的不同推理失败模式。对主流视觉-语言模型进行了评测并与现有基准进行相关性分析。

Result: 实验结果显示即便是最先进的系统也难以在MM-CoT上取得高分，MM-CoT与现有基准相关性低，证明其测量的是视觉落地与逻辑推理的独特组合。

Conclusion: 该论文提出了MM-CoT基准用于评估多模态模型的链式思维是否在视觉证据上有据可依且逻辑连贯。实验显示现有先进视觉-语言模型在该任务上表现欠佳，表明生成流畅性与真实推理能力存在明显脱节。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [21] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: 提出基于PCA法线估计的几何感知稀疏深度采样，按深度可靠性分布抽样并用于训练扩散式深度补全，提升了准确性并减少边缘伪影，更贴近真实传感器行为。


<details>
  <summary>Details</summary>
Motivation: 现有深度补全工作通常采用均匀随机从真值生成稀疏深度，但真实传感器的深度可靠性随几何和空间变化（例如平面上更可靠、边缘和视角倾斜处不可靠），均匀采样无法反映这种非均匀性，导致训练与真实传感器差距大和边界处伪影。

Method: 对RGB-D点云进行局部PCA以估计表面法线和曲率，基于法线一致性/曲率构建每像素深度可靠性度量，再按该概率分布对密集真值深度进行抽样生成稀疏深度输入，最后将该采样流程用于训练Marigold-DC扩散式深度补全网络并在NYU Depth v2上评估。

Result: 在NYU Depth v2上实验表明，使用法线引导采样比均匀随机采样在标准度量（如RMSE、MAE、abs rel等）上取得更好数值表现，同时在边缘/不连续处伪影显著减少，生成的稀疏输入更符合真实传感器模式。

Conclusion: 本文提出了一种基于法线引导的稀疏深度采样策略，通过PCA表面法线估计计算每像素的深度可靠性分布，进而按该分布采样稀疏深度并与Marigold-DC扩散式深度补全模型结合，在NYU Depth v2数据集上验证，结果显示在准确性、边界伪影减少和生成更符合真实传感器行为的训练条件方面均有改进。

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [22] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: 将视图变换分解为原生算子流水线并结合联合学习的深度调制与时序融合，FastBEV++在保持TensorRT可移植性的同时实现SOTA性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 当前相机BEV受限于计算昂贵的视图变换和平台专用内核，导致难以在车载环境部署，需一种兼顾精度与部署便捷的设计方案。

Method: 提出将单体投影拆分为Index-Gather-Reshape流程，使用确定性预排序以仅用Gather、矩阵乘等原生算子实现视图变换，消除自定义CUDA内核需求；基于该分解结构设计端到端深度感知融合、时序聚合和数据增强以提升几何一致性与性能。

Result: FastBEV++提出了一种既高效又易部署的纯相机BEV感知框架，通过将视图变换分解为Index-Gather-Reshape流水线并采用确定性预排序，完全使用原生算子实现TensorRT原生移植；同时引入端到端深度感知融合、时序聚合和鲁棒数据增强以提升BEV几何精度。

Conclusion: FastBEV++在nuScenes上达到0.359 NDS，并能在Tesla T4等汽车级硬件上实现超过134 FPS的实时性能，兼具高精度与无需自定义插件的易部署性，适合生产系统。

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [23] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

TL;DR: HTC-VLM用连续+离散双通道和解耦注意力将580个视觉token压缩为1个voco token，达成高压缩比下优秀的语义与细节保留，效率优于纯连续方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在将大量视觉patch送入LLM时成本呈二次增长，且压缩方法在保留细粒度外观与高层语义间存在权衡，需一种同时兼顾两者的高效方案。

Method: 设计双路径架构：连续路径保留ViT patch细节，离散路径用多语义向量量化(MGVQ)投影为四个符号token；通过解耦注意力掩码与瓶颈将580 token压缩为单个voco token，并与LLM融合。

Result: 在七个基准上平均保留率87.2%，优于连续基线的81.0%，实现580:1压缩比。注意力分析表明voco token更依赖离散锚点，验证了语义引导作用。

Conclusion: HTC-VLM通过双通道分离语义与外观信息，成功在高压缩比下保留大部分推理性能，提出的解耦注意力与瓶颈机制有效将混合信息凝练为单一voco token，从而缓解计算与内存成本。

Abstract: Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [24] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出一款结合残差CNN与改进Swin Transformer并配合MSCAS与像素注意力的分割网络，对BUSI乳腺超声分割取得很高指标。


<details>
  <summary>Details</summary>
Motivation: 目标是解决乳腺超声图像中模糊边界、噪声、形态多样性和局部/全局特征融合困难，提升分割精度从而辅助临床诊断。

Method: 方法包括：1) 预处理使用拉普拉斯-高斯区域算子与边界导向算子以抑制噪声并保持病灶形态；2) 架构由残差CNN模块提取局部稳健特征，并在编码器/解码器中自定义内残差Swin Transformer块以捕获全局依赖；3) 逐步收缩策略在多个尺度上减少特征图以增强尺度不变性；4) 每个解码器级别引入MSCAS（多尺度通道注意与压缩）模块以突出显著编码器特征并抑制冗余；5) 最终使用像素注意力模块自适应加权病灶像素以抑制背景干扰。

Result: 在公开BUSI数据集上，Residual-SwinCA-Net达到平均准确率99.29%、IoU98.74%和Dice0.9041，优于比较的CNN和ViT方法。

Conclusion: 该论文提出了Residual-SwinCA-Net，一种结合残差CNN和Swin Transformer的混合分割框架，以提高乳腺超声图像中的病灶分割性能。

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [25] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: 提出FTKD，一种基于稀疏query的知识蒸馏方法，把离线教师的未来帧信息蒸馏给在线学生，包含未来感知特征重建和未来引导logit蒸馏，在nuScenes上提升0.0~1.3 mAP和1.3 NDS并改善速度估计，且不增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 当前相机时序3D检测中，离线模型利用未来帧能显著提升精度，但在线模型无法访问未来。现有知识蒸馏方法主要在空间特征或时序关系上对齐，忽视如何传递未来帧信息，导致在线模型难以学习到未来知识。需要一种能在不严格帧对齐下将未来信息高效迁移到在线模型的蒸馏策略。

Method: 提出Future Temporal Knowledge Distillation (FTKD)，基于稀疏query机制：1) 未来感知特征重建：通过设计重建目标，使学生在当前帧条件下预测或拟合教师的未来特征表示，避免严格的帧级对齐；2) 未来引导的logit蒸馏：利用教师关于前景/背景的稳定预测作为软标签，增强学生对目标存在性和分类/回归输出的学习。将FTKD集成到两个主流3D检测基线中。

Result: 在nuScenes数据集上，FTKD分别为两种基线带来最高1.3 mAP和1.3 NDS的提升，同时在速度估计指标上表现最好。方法在不增加在线推理成本的前提下，提升检测性能与速度估计准确性。

Conclusion: FTKD能在不依赖未来帧输入的在线模型中有效注入未来信息，通过特征重建与logit蒸馏两步协同，改善检测精度和速度估计，具有实用价值并可推广到不同稀疏query基线。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [26] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: 提出Query-aware Hub Prototype (QHP)，通过Hub Prototype Generation和Prototype Distribution Optimization生成更具query相关性的原型与纯度重加权对比损失，提升FS-3DSeg性能


<details>
  <summary>Details</summary>
Motivation: 现有方法仅用support生成原型导致原型偏差，无法适应query分布变化，影响少样本3D语义分割表现

Method: 提出HPG模块构建support-query二部图识别高频连接的support hubs并生成query相关原型；提出PDO模块通过纯度重加权对比损失拉近坏hub和离群原型到类中心，优化原型分布

Result: QHP通过在support与query间构建二部图、识别support hub并生成query相关的原型，缓解了原型偏差问题

Conclusion: QHP在S3DIS和ScanNet上显著优于SOTA，降低了原型与query分布的语义差距

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [27] [SFP: Real-World Scene Recovery Using Spatial and Frequency Priors](https://arxiv.org/abs/2512.08254)
*Yun Liu,Tao Li,Cosmin Ancuti,Wenqi Ren,Weisi Lin*

Main category: cs.CV

TL;DR: 提出一种结合空间透射先验与频域频率先验的场景恢复方法（SFP），通过估计透射图、自适应频率掩码及加权融合，在真实退化条件下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖单一先验难以同时应对多种退化，要么采用在合成数据上训练的复杂网络导致对真实场景泛化能力差，因此提出结合空间与频率双重先验的轻量化可推广方法以提升真实世界场景恢复效果。

Method: 在空间域，利用退化图像的倒数在其谱方向上的投影类似场景透射率的观察，估计透射图以恢复散射退化的场景；在频率域，构建自适应频率增强掩码，并提出两项新先验来估计掩码参数：一是退化图像频域直流（DC）分量三通道的均值近似清晰图像各通道的DC均值；二是清晰图像低径向频率（<0.001）幅值约占总谱的1%；最后设计加权融合策略整合空间恢复、频率增强和输入图像的显著特征以产出最终结果。

Result: 通过大量评估，SFP在多种退化条件下显示出优越性，证明空间先验与频率先验的结合以及加权融合策略能有效恢复细节并抑制伪影。

Conclusion: 本文提出的SFP方法通过结合空间域和频率域先验，有效提升了在多种真实退化条件下的场景恢复性能，实验证明其优于仅依赖单一先验或在合成数据上训练的复杂网络的方法。

Abstract: Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.

</details>


### [28] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

TL;DR: RLCNet是一种端到端深度学习方法，结合加权移动平均与异常值剔除实现LiDAR/RADAR/摄像机的在线联合外参标定，在真实数据上优于现有方法，适合实时部署。


<details>
  <summary>Details</summary>
Motivation: 提高自动驾驶系统中多模态传感器（LiDAR、RADAR、摄像机）外参标定的准确性与鲁棒性，解决动态环境中机械振动和传感器漂移导致的标定挑战，支持在线实时校准以便实际部署。

Method: 提出RLCNet，一种端到端可训练的深度网络用于同时在线估计三类传感器的外参；结合加权移动平均与异常值剔除的在线校准框架以平滑预测并抵抗漂移；在真实数据集上训练与验证，并进行架构消融研究及与现有方法的比较。

Result: 在真实世界数据上表现出比现有方法更高的精度与鲁棒性；在线框架在实时运行中能减少预测噪声并改善对漂移的抵抗能力；消融实验证明关键架构设计对性能有显著影响。

Conclusion: RLCNet为多模态传感器的在线外参标定提供了一种实用且鲁棒的解决方案，适合在动态真实场景中部署，能有效减轻机械振动与累积漂移带来的影响。

Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [29] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: EgoX通过LoRA适配大规模视频扩散模型、统一的条件拼接和几何引导自注意力，从单个第三人称视频生成几何一致且高质量的第一人称视频。


<details>
  <summary>Details</summary>
Motivation: 将第三人称视频转换为第一人称视角可增强沉浸式理解，但面临摄像机姿态极端变化和视角重叠少、需要在保持可见内容的同时生成未见区域并保证几何一致性的挑战。

Method: 使用预训练的视频扩散模型的时空知识，采用LoRA低秩适配来微调模型；提出将外视和内视先验通过宽度与通道级拼接的统一条件输入；引入基于几何的自注意力模块，使网络优先关注空间相关区域以保持几何一致性。

Result: EgoX在未知和真实场景视频上展示了高一致性与真实感的第一人称视频生成，具有良好的可扩展性和鲁棒性。

Conclusion: 该论文提出了EgoX，一个从单一第三人称视频生成第一人称视频的框架，通过轻量级LoRA适配大规模视频扩散模型并结合统一条件策略与几何引导自注意力，实现几何一致且高保真度的场景合成。

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [30] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出PAVAS：通过PPE估计质量与速度等物理参数，并由Phy-Adapter将其融入潜在扩散式视频到音频生成，从而提升生成音频的物理合理性；并提出VGG-Impact数据集和APCC评估指标，实验证明方法有效。


<details>
  <summary>Details</summary>
Motivation: 现有V2A模型多为外观驱动，仅利用视觉—声学关联，忽略了决定真实世界声音的物理因素（如质量、速度、碰撞力等），因此生成的声音在物理合理性上存在缺陷。作者希望通过显式引入物理信息，使生成的音频更符合物理因果关系。

Method: 基于潜在扩散模型（latent diffusion），在生成器中加入Phy-Adapter以接收来自物理参数估计器（PPE）的输入。PPE利用视觉-语言模型（VLM）推断物体质量，并通过基于分割的动态三维重建模块恢复运动轨迹以计算速度，从而提取质量和速度等物理参数作为条件信息引导音频生成。

Result: 作者构建了VGG-Impact数据集专注于物体—物体交互场景，提出Audio-Physics Correlation Coefficient（APCC）用于评估物理与音频属性的一致性。实验表明PAVAS在主观感知和物理一致性方面均优于现有V2A模型，能够生成更具物理合理性的声音。

Conclusion: 该论文提出在视频到音频生成中引入物理推理，通过物理驱动的适配器（Phy-Adapter）结合物体级物理参数来提升生成声音的物理合理性和时序一致性，实验证明在新构建的VGG-Impact基准和提出的APCC指标上优于现有方法。

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [31] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: 提出OpenSubject：一个基于视频的大规模主体驱动生成语料库与管道，通过跨帧身份挖掘、分割导向合成与VLM验证，提高了身份保真与复杂场景下的生成与操控表现。


<details>
  <summary>Details</summary>
Motivation: 解决主题驱动图像生成模型在保留参考身份和处理多主体复杂场景方面的不足，通过构建大规模视频衍生语料库以提供更丰富的跨帧身份先验。

Method: 构建OpenSubject数据集，包含4.35M图像和2.5M样本，采用四阶段管道：视频筛选、基于VLM的跨帧主体挖掘与配对、基于分割图的扩展外绘与基于框的修补生成参考图像（并加几何增强与边界腐蚀）、以及VLM验证与自动描述生成；同时构建基准并用VLM评估多项指标。

Result: 使用OpenSubject训练的模型在身份保真度、提示遵从性、操控一致性和背景一致性等方面均有显著提升，尤其在包含多主体的复杂场景中表现更好。

Conclusion: OpenSubject通过视频级跨帧身份信息和合成驱动的增强流程，有效提升了主题驱动生成与操控任务的性能，为进一步研究提供了高质量大规模数据集与评估基准。

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [32] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 将扩散模型与新算法（InfiniteDiffusion）、分层结构、拉普拉斯编码和无界张量框架结合，提供可实时、无界、种子一致的高质量地形生成替代传统过程噪声。


<details>
  <summary>Details</summary>
Motivation: 现有过程噪声（如Perlin噪声）速度快且可无界生成，但在真实感与大尺度一致性上受限；引入扩散模型以提升保真度同时保留过程噪声的关键属性。

Method: 设计了InfiniteDiffusion算法用于无限生成；使用分层扩散模型栈结合行星级上下文与局部细节；采用紧凑拉普拉斯编码稳定地形的地球尺度动态范围；实现开源无界张量框架支持常数内存操作；通过少步一致性蒸馏提升生成效率。

Result: 系统能够实时合成无界地形，支持行星级一致性与可控性，基本满足无缝、种子一致、常数时间随机访问等需求，适合大规模程序化世界生成。

Conclusion: 本文提出了Terrain Diffusion，将扩散模型用于无界地形生成，兼顾细节与全局一致性，实现种子一致、无缝无限和平行常数时间访问。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [33] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: 提出GeoDM：在欧氏/双曲/球面乘积流形上用可学习曲率与权重并结合最优传输损失进行几何感知的数据蒸馏，理论和实验证明优于纯欧氏方法。


<details>
  <summary>Details</summary>
Motivation: 动机是现有分布匹配蒸馏方法局限于欧氏空间，仅能捕捉线性结构，忽视数据的内在几何（如曲率）；而高维数据常位于低维流形上，蒸馏应使精简数据流形与原始流形对齐。

Method: 方法上，作者将数据表示放在乘积流形（欧氏+双曲+球面），引入可学习的曲率和权重参数以自适应地选择三种几何类型的贡献，并设计基于最优传输的损失以增强分布保真性。理论上给出在乘积空间进行几何感知分布匹配能比纯欧氏方法获得更小的泛化误差界。

Result: 实验在标准基准上表明GeoDM在多种分布匹配策略下均优于现有最先进的数据蒸馏方法，且方法对单一几何的分布匹配策略也保持有效。

Conclusion: 本论文提出了一种几何感知的数据蒸馏框架GeoDM，通过在欧氏、双曲和球面流形的笛卡尔乘积空间中进行分布匹配，能够同时捕捉平坦、层次和循环结构，从而更好地对齐精简数据流形与原始数据流形，提升蒸馏数据的表示能力。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [34] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 本文介绍了与MICCAI 2024合作的3DTeethLand挑战赛，发布首个公开3D牙齿标注数据集并评测参赛深度学习算法，旨在推动牙齿标志点检测研究并促进临床应用。


<details>
  <summary>Details</summary>
Motivation: 作者指出牙齿标志点检测对临床正畸学重要，需要精确识别以支持诊断和个性化治疗。现有挑战来自牙齿几何复杂性和个体差异，促使用深度学习等先进方法解决3D牙齿标注问题。组织了3DTeethLand挑战赛并与MICCAI 2024合作，发布首个公开3D牙齿标注数据集，评估和推动该领域方法发展。

Method: 举办挑战赛，提供首个公开的牙齿3D标注数据集，吸引社区提交基于深度学习的算法用于从口内3D扫描中检测牙齿标志点，通过竞赛平台统一评测并比较不同方法性能。

Result: 挑战赛发布了数据集并收集了参赛方法，建立了评测基准和比较标准，从而揭示当前方法在3D牙齿标志点检测上的性能水平与存在的局限性（如对复杂几何和个体差异的适应性问题）。

Conclusion: 3DTeethLand挑战赛及其数据集为研究社区提供了标准化评测平台，推动了基于深度学习的3D牙齿标志点检测研究，但仍需进一步方法改进以应对几何复杂性和个体差异，促进临床应用。

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [35] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: 提出GeoDiffMM，一种基于扩散模型并以光流为几何条件的拉格朗日视频运动放大方法，通过无噪声光流增强、条件化去噪与流场驱动的视频合成，有效区分光子噪声与真实微运动，提升放大质量。


<details>
  <summary>Details</summary>
Motivation: 现有欧拉方法在分解纹理/形状/频率上仍难以在极小位移下区分光子噪声与真实微运动，导致放大噪声和结构不一致。需要一种能利用几何信息、抑制噪声并保持结构一致性的放大框架。

Method: 提出GeoDiffMM：1) Noise-free Optical Flow Augmentation：合成多样的非刚性无噪声运动场作为监督，帮助学习几何感知的光流；2) Diffusion Motion Magnifier：在扩散去噪过程中以光流作为几何先验并加入可学习放大因子，选择性放大语义/结构一致的运动分量并抑制无关扰动；3) Flow-based Video Synthesis：将放大后运动通过流场映射回图像域以保证高保真度。

Result: 在真实与合成数据集上的大量实验表明，GeoDiffMM优于最先进方法，在运动放大质量、噪声抑制和结构一致性方面显著改善。

Conclusion: 通过将条件扩散模型与光流几何先验结合，并辅以无噪声光流增强与流场驱动合成，GeoDiffMM能够更准确地区分光子噪声和真实微运动，实现更高质量的结构一致性视频运动放大。

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [36] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 将RGB用纯四元数建模并引入四元数核范数低秩正则化，提出LSQMM分类器并用ADMM求解，实验显示优于SVM/矩阵机/张量机。


<details>
  <summary>Details</summary>
Motivation: 传统方法将特征展开为实域向量/矩阵，可能破坏RGB通道间的耦合关系。借鉴四元数在图像恢复/去噪中的成功，希望在分类任务中保留通道间固有关联并利用低秩先验提高鲁棒性与准确率。

Method: 将图像表示为四元数矩阵，构建低秩支持四元数矩阵机（LSQMM）模型：在铰链损失上加入四元数核范数正则项；采用基于ADMM的迭代算法求解四元数域优化问题。

Result: 在多个彩色图像分类数据集上，LSQMM在准确率、鲁棒性和计算效率方面均优于若干基线方法（传统SVM、支持矩阵机、支持张量机）。

Conclusion: 该论文提出将彩色图像的RGB通道视为纯四元数，并在支持向量机框架中引入四元数核范数正则化以促进低秩结构，从而提高分类性能。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [37] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 研究表明，当代图像保护通过与内容耦合的结构化特征变形实现对生成模型的干扰，信号虽视觉微妙但在表征、空间与频率域均可被检测。


<details>
  <summary>Details</summary>
Motivation: 理解Glaze与Nightshade等图像保护扰动的内部结构、可检测性及其在表示空间中的行为，以提高解释性并指导防御与检测策略设计。

Method: 结合白盒特征空间检查与黑盒信号级探测，包括潜在空间聚类、特征通道激活分析、遮挡敏感性映射和频域表征。

Result: 发现保护扰动为与图像内容紧密耦合的低熵结构化信号：保留了内容驱动的特征组织并引入保护专属子结构；检测性由扰动熵、空间部署与频率对齐共同决定；顺序保护增强可检测结构；频域上能量沿图像主导频率轴重新分布。

Conclusion: 保护机制通过结构化、低熵的特征级变形而非语义位移实现对生成模型的干扰，因而在表观上微妙但在表征上可检测。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [38] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: PointDico: diffusion model guides contrastive learning via distillation; hierarchical pyramid generator and dual-channel local-global design; SOTA on 3D benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive methods overfit and 3D MAE struggles with unordered point clouds; combining diffusion generative modeling with contrastive learning can leverage both paradigms' strengths for robust 3D representations.

Method: Analyze methods: diffusion-guided contrastive distillation with hierarchical pyramid conditional generator and dual-channel local-global design.

Result: PointDico integrates diffusion-based denoising generative modeling as a teacher to guide a contrastive student via knowledge distillation; introduces hierarchical pyramid conditional generator for multi-scale geometry extraction; dual-channel merges local and global context; optimizes jointly for contrastive and generative objectives.

Conclusion: PointDico effectively combines strengths of diffusion and contrastive paradigms to overcome overfitting in contrastive models and unordered point cloud issues in MAE, achieving state-of-the-art on ScanObjectNN and ShapeNetPart.

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [39] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: 提出Bi^2MAC，用软/硬掩码将特征分流到轻量或精细分支，兼顾适应性与计算效率，在全色融合任务上实现SOTA且资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习的卷积在处理图像中区域异质性（不同区域特征差异）时适应性不足，现有自适应卷积虽改进适应性但计算代价高且对异质区捕捉能力有限。需要一种既能区分区域类型又能节省计算资源的方法。

Method: 设计了一个轻量模块生成软掩码与硬掩码：软掩码对输入特征做初步调制，硬掩码用于引导区域进入不同分支。将冗余特征送入轻量全局分支进行低成本处理，将异质特征送入高资源精细分支。该模块称为Bi^2MAC，集成到深度网络中进行并行特征处理以提高效率。

Result: 在多个遥感全色融合基准数据集上，Bi^2MAC取得了SOTA性能，同时在训练时间、参数数量及计算成本上显著优于其他自适应卷积方法，证明了其高效性与有效性。

Conclusion: 本文提出的Bi^2MAC通过双模态双自适应掩码感知卷积，有效分流不同区域特征到轻量或精细分支，实现高效且精确的全色融合。实验证明在多基准数据集上达到SOTA，同时减少训练时间、参数量与计算成本。

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [40] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: 通过将视图相关反射烘焙进高斯基元并用hybrid splatting和剪枝加速，HybridSplat在复杂反射场景上用更少基元实现约7×加速与更低内存，同时保持高反射质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯splatting的方法在复杂反射场景上存在渲染速度慢和内存高的瓶颈，需在不显著损失反射效果下提高效率。

Method: 提出reflection-baked Gaussian tracing，把视图相关反射打包到每个高斯基元；使用tile-based Gaussian splatting渲染反射；在统一的hybrid splatting框架中融合反射高斯与基础高斯；并引入pipeline级加速与反射敏感的高斯剪枝来减少模型规模。

Result: 在Ref-NeRF和NeRF-Casting的复杂反射场景上，相比类似的基于光线追踪的高斯splatting基线，HybridSplat在用4倍更少高斯基元的同时实现约7倍的渲染加速，成为复杂反射场景的新的最先进方法。

Conclusion: HybridSplat通过将反射信息预烘焙进高斯基元并结合tile-based splatting与基础高斯混合渲染，实现了在保留反射质量的同时显著加速和压缩模型。

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [41] [DINO-BOLDNet: A DINOv3-Guided Multi-Slice Attention Network for T1-to-BOLD Generation](https://arxiv.org/abs/2512.08337)
*Jianwei Wang,Qing Wang,Menglan Ruan,Rongjun Ge,Chunfeng Yang,Yang Chen,Chunming Xie*

Main category: cs.CV

TL;DR: DINO-BOLDNet利用DINOv3提取切片内结构特征、切片注意力融合邻近切片上下文，并用多尺度解码器及DINO感知损失恢复细粒度功能对比，在248例临床数据上在PSNR和MS-SSIM上优于条件GAN，是首个直接从T1w生成均值BOLD图像的框架。


<details>
  <summary>Details</summary>
Motivation: 当BOLD图像缺失或受损时，能从常见的结构像（T1w）生成BOLD可用于恢复缺失信息并支持下游功能分析与临床应用。

Method: 框架由冻结的DINOv3编码器（提取切片内表征）、独立的切片注意力模块（跨切片融合上下文）、多尺度生成解码器（恢复细节功能对比），以及基于DINO特征的感知损失，训练轻量解码器以生成均值BOLD图像。

Result: 在包含248名受试者的临床数据集上，DINO-BOLDNet在PSNR和MS-SSIM指标上均优于条件GAN基线，展示了自监督Transformer引导在结构到功能映射中的潜力。

Conclusion: 该文提出了DINO-BOLDNet，一种结合冻结的自监督DINOv3编码器与轻量可训练解码器的多切片注意力框架，用于从T1加权结构像生成平均BOLD功能像。

Abstract: Generating BOLD images from T1w images offers a promising solution for recovering missing BOLD information and enabling downstream tasks when BOLD images are corrupted or unavailable. Motivated by this, we propose DINO-BOLDNet, a DINOv3-guided multi-slice attention framework that integrates a frozen self-supervised DINOv3 encoder with a lightweight trainable decoder. The model uses DINOv3 to extract within-slice structural representations, and a separate slice-attention module to fuse contextual information across neighboring slices. A multi-scale generation decoder then restores fine-grained functional contrast, while a DINO-based perceptual loss encourages structural and textural consistency between predictions and ground-truth BOLD in the transformer feature space. Experiments on a clinical dataset of 248 subjects show that DINO-BOLDNet surpasses a conditional GAN baseline in both PSNR and MS-SSIM. To our knowledge, this is the first framework capable of generating mean BOLD images directly from T1w images, highlighting the potential of self-supervised transformer guidance for structural-to-functional mapping.

</details>


### [42] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: 提出TrackingWorld：通过2D轨迹上采样与基于优化的反投影，实现在世界中心坐标系下对几乎所有像素的稠密单目三维跟踪，能更好分离相机与前景运动并处理新出现对象。


<details>
  <summary>Details</summary>
Motivation: 现有单目三维跟踪方法在分离相机运动与前景动态运动方面效果不足，且无法稠密跟踪视频中后期出现的新动态主体，因而需要一种能在世界中心坐标系内对几乎所有像素进行稠密3D跟踪的新方法。

Method: 主要方法包括：1) 引入tracking upsampler，将任意稀疏2D轨迹有效上采样为稠密2D轨迹；2) 对所有帧应用上采样并通过去除重叠区域中的轨迹来减少冗余，从而能对新出现的对象进行泛化；3) 提出基于优化的高效框架，通过估计相机位姿和2D轨迹的3D坐标将稠密2D轨迹反投影至世界中心坐标系生成3D轨迹。

Result: 在合成和真实数据集上的大量评估表明，TrackingWorld在世界中心坐标系下可以实现精确且稠密的三维跟踪。

Conclusion: 本文提出TrackingWorld，一种世界中心的稠密单目三维跟踪管线，能够将稀疏2D轨迹上采样为稠密2D轨迹，并通过优化估计相机位姿与3D坐标将其反投影为世界坐标系下的稠密3D轨迹，从而更好地分离相机运动与前景动态，并处理新出现的动态主体。

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [43] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 提出结合U-Net、CBAM与额外判别器的SCU-CGAN用于从非火图像生成高质量火灾图像，KID优于CycleGAN 41.5%；用生成数据增强后，YOLOv5 nano的mAP@0.5:0.95提高56.5%。


<details>
  <summary>Details</summary>
Motivation: 家庭物联网环境下火灾检测受限于火灾图像数据不足，导致检测模型泛化能力差；为此通过生成模型扩充数据集以提升检测器性能。

Method: 将U-Net作为生成器骨架，结合注意力模块CBAM增强特征表达，并引入额外判别器（可能针对局部或特定特征）构成对抗网络，用于将非火灾图像转换为火灾图像。训练过程中使用图像质量评估指标（如KID）与下游检测器（YOLOv5 nano）性能提升作为评估。

Result: 在图像质量上，SCU-CGAN在KID指标上比CycleGAN提升41.5%。在下游检测任务上，使用SCU-CGAN增强的数据集对YOLOv5 nano的mAP@0.5:0.95提升了56.5%，整体表明生成的数据对检测性能有显著帮助。

Conclusion: 本文提出的SCU-CGAN能有效从非火灾图像生成真实感火灾图像，提升数据多样性，从而显著改善下游火灾检测模型性能。

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [44] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: Pre-Norm架构下视觉token范数高于文本token，引发表示更新不对称；在视觉投影后加一个LayerNorm即可有效对齐范数并带来多模态及文本任务性能提升。


<details>
  <summary>Details</summary>
Motivation: MLLMs中视觉token范数普遍高于文本token，导致视觉表示更新缓慢（‘惯性’），妨碍跨模态融合与下游任务表现。

Method: 理论分析结合实证验证：首先在Pre-Norm Transformer框架下推导范数不平衡导致的‘非对称更新动力学’，然后在多种主流MLLM上测量范数与更新率；最后在视觉投影后插入单一初始化的LayerNorm并在LLaVA-1.5上进行评估。

Result: 在LLaVA-1.5上插入视觉投影后的LayerNorm带来多项多模态基准显著提升，并且在纯文本任务（如MMLU）上也观察到性能提升，表明架构层面的范数对齐改善了模型的整体能力。

Conclusion: 加入LayerNorm层后，视觉与文本token范数不平衡问题显著缓解，从而改善了跨模态特征融合与下游性能。

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [45] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: New framework uses GDWGIF for illumination estimation, Retinex decomposition, parallel enhancement of illumination and reflection, plus multi-exposure fusion and linear stretching to enhance contrast and suppress noise.


<details>
  <summary>Details</summary>
Motivation: Existing enhancement methods either amplify noise or work only for certain lighting; need a method that enhances while suppressing noise under varied illumination

Method: Gradient-domain weighted guided filter + Retinex + multi-exposure fusion

Result: Improved contrast and reduced noise on real-world datasets; outperforms SOTA methods

Conclusion: Proposed framework effectively enhances images and suppresses noise under complex illumination; combines GDWGIF, Retinex decomposition, parallel processing of layers, and dynamic range optimization

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [46] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 本文提出结合面部美学评分与多种AI特征提取方法来检测人脸美化，在未知攻击算法情境下，单图检测达成1.1% D-EER。


<details>
  <summary>Details</summary>
Motivation: 随着图像社交媒体和商业修图的普及，经过美化的人脸图像可能作为生物特征样本被用于识别系统，导致识别错误或攻击风险。因此需要有效检测被美化处理的图像以保证生物识别系统的可靠性。

Method: 本文分析了现有美学评估算法在经美化图像上的变化，比较并评估多种基于AI的特征提取方法用于美化检测（可能包括深度学习特征、手工特征等），并将面部美学评分作为辅助特征融入检测器中。采用在不同美化算法未知的攻击场景下进行测试，并以D-EER作为主要性能指标。

Result: 在未知攻击算法的场景下，提出的方法在单张图像的检测任务上达到了1.1% 的检测等错误率（D-EER），表明结合美学评分与AI特征可以显著提高美化检测性能。

Conclusion: 本文针对人脸美化（retouching）图像在生物特征识别系统中带来的挑战，提出了利用美学/美丽评分与AI特征提取方法来提升人脸美化检测的研究。研究结论表明，在不知攻击美化算法具体细节的场景下，所提方法在单张图像检测上达到了1.1% 的D-EER，证明面部美学特征能有效辅助美化检测。

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [47] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: 本文针对电子监控下大量视频数据的无人化审核难题，提出基于AutoFish数据集的鱼类个体重识别优化深度学习流程，通过困难三元组挖掘与定制图像变换（含数据集归一化）提升R1与mAP@k指标，Swin-T优于ResNet-50，最高达41.65% mAP@k与90.43% Rank-1；主要错误来自同种内视觉相似个体，视角不一致比遮挡影响更大。


<details>
  <summary>Details</summary>
Motivation: EM系统带来海量视频，人工审阅不可行；需要自动化鱼类个体重识别以支持渔业资源管理与可持续执法。

Method: 使用新构建的AutoFish数据集（传送带场景、6个外观相似物种）比较Swin-T与ResNet-50，采用困难三元组挖掘训练策略，并设计数据集特定的图像变换管线（包含专门归一化、增强策略）以提升判别能力；评估指标为Rank-1与mAP@k，另做错误类型分析（种间/种内、视角/遮挡等）。

Result: 困难三元组挖掘与定制变换显著提升重识别性能；Swin-T持续优于ResNet-50，达成最高90.43% Rank-1与41.65% mAP@k；错误分析显示最多为同种个体混淆，且视角不一致对性能影响最大。

Conclusion: 结合硬样本挖掘与数据集适配的预处理/增强，可显著提高EM场景下鱼类Re-ID效果；未来应聚焦视角一致性与更鲁棒的特征表示以减少同种内误判。

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [48] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: 提出一个训练免费的视频人体重建框架，通过身份一致的masklets和遮挡修复引导SAM 3D Body，实现时间一致且遮挡稳健的全身网格恢复。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的HMR方法在视频上采用逐帧推理，导致时间不一致和在遮挡情形下性能下降。作者利用视频中人体连续性来克服这些问题，同时保持无需重新训练的特点。

Method: 方法包括：1) 使用可提示的视频分割模型生成保持身份一致的masklets；2) 通过一个遮挡感知模块修复缺失区域；3) 将精修后的masklets作为引导，驱动SAM 3D Body生成一致的全身网格轨迹；4) 采用基于padding的并行策略实现高效多人体推理。整个流程无需重新训练现有HMR模型。

Result: 实验表明，SAM-Body4D在真实场景的视频中提升了时间稳定性和遮挡下的鲁棒性，且能高效处理多人体场景，代码已开源。

Conclusion: 本文提出了SAM-Body4D，一种无需额外训练、用于视频的人体重建框架，通过利用视频中人体的连续性提升时间一致性和遮挡鲁棒性。

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [49] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: OneClip-RAG通过一次性基于视频片段的检索增强与查询引导的分块策略，高效提升MLLM对长视频的理解能力，同时降低内存与时间成本；并通过SynLongVideo数据集与渐进训练验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM受限于显存限制仅能处理有限帧数，影响长视频理解；需要一种既高效又能保持语义与知识完整性的方案来扩展模型对长视频的处理能力。

Method: 提出OneClip-RAG范式：1) 利用视频片段作为检索与增强的单元以兼顾知识完整性与语义连贯；2) 设计查询引导的视频分块算法（query-guided chunking），将分块与跨模态检索合并为一步以避免冗余计算；3) 构建SynLongVideo数据集并设计渐进训练策略以改善模型的指令遵从性；将OneClip-RAG无缝插入多款现有MLLM进行验证。

Result: 实验证明OneClip-RAG在多个长视频基准上显著提升了性能，例如将InternLV2 8B与Qwen2-VL 7B的表现提升至接近GPT-4o在MLVU上的水平；并在效率上优于现有方法，例如在单块4090 GPU上使LLaVA-Video在2.2分钟内理解一小时视频。

Conclusion: OneClip-RAG通过基于视频片段的一次检索增强（one-shot clip-based retrieval augmentation）有效提升了多模态大模型（MLLM）对长视频的理解能力，在保持语义连贯性和知识完整性的同时，显著降低了计算和内存开销。

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [50] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed Doğan,Nicolas R. -Peña,Alfred Schoettl*

Main category: cs.CV

TL;DR: 提出一种基于深度图的多视角稀疏体素6D位姿估计方法，使用分阶段热图注意力和密度感知稀疏变换器，在堆叠杂乱场景中能高效恢复精细几何并同时预测多个目标位姿。


<details>
  <summary>Details</summary>
Motivation: 解决工业抓取中因遮挡、反射和无纹理零件导致的6D位姿估计困难，尤其在密集堆叠的bin-picking场景下需要高分辨率细节捕获且内存可控的方案。

Method: 融合多视角深度图为高分辨率点云或稀疏TSDF，采用分级热图产生场景自适应前景注意力，结合密度感知稀疏变换器处理自遮挡和非均匀点分布，全稀疏体素表示加上逐体素投票实现同时多对象6D位姿预测。

Result: The paper proposes a depth-only, multi-view 6D pose estimation framework designed for densely cluttered bin-picking scenarios.

Conclusion: The method achieves competitive performance on IPD and MV-YCB datasets by using sparse volumetric representations, staged heatmap attention, and a density-aware sparse transformer to handle occlusion and uneven 3D data distribution.

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [51] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: LapFM利用层次概念结构与置信驱动迭代伪标签，从大量无标注腹腔镜图像中进化出通用分割基础模型并构建了114K伪标注基准，显著提升了跨粒度与跨任务的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅对自然图像基础模型做领域微调，监督稀缺且语义不一致，难以覆盖腹腔镜中高度可变的分割目标；需要一种能从无标注数据中进化出细粒度语义一致性的外科基础模型。

Method: 提出Laparoscopic Concept Hierarchy (LCH)与Confidence-driven Evolving Labeling两大组件。LCH使用具有父子查询嵌入的层次掩码解码器，将解剖、组织和器械等实体统一为可扩展的知识结构；Confidence-driven Evolving Labeling基于层次一致性迭代生成/筛选伪标签并逐步纳入训练，产出114K图像-掩码数据集LapBench-114K。

Result: 在多项通用腹腔镜分割基准上，LapFM显著优于最先进方法，在粒度自适应泛化能力上建立了新标准，提供了公开代码与大规模数据集。

Conclusion: LapFM通过层次概念进化预训练范式，从海量无标注腹腔镜图像中学习稳健的分割能力，实现了在不同粒度和多样化目标上的泛化能力提升。

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [52] [Leveraging Multispectral Sensors for Color Correction in Mobile Cameras](https://arxiv.org/abs/2512.08441)
*Luca Cogo,Marco Buzzelli,Simone Bianco,Javier Vazquez-Corral,Raimondo Schettini*

Main category: cs.CV

TL;DR: 将RGB与低分辨率MS联合输入一个端到端模型进行色彩校正，构建聚合数据集并在不同相机灵敏度下训练，实验显示比单纯RGB或仅MS方法降低约最多50%误差。


<details>
  <summary>Details</summary>
Motivation: 现有色彩校正通常将流程拆分为多个阶段并早期丢弃MS信息，而快照MS传感器为移动/消费设备提供了更丰富的光谱信息，有潜力提升色彩校正性能。

Method: 提出一个端到端神经框架，联合高分辨率RGB和辅助低分辨率MS数据输入，重构两种现有的图像到图像架构以验证通用性，同时将完整色彩校正流水线纳入单一模型。训练数据由公开光谱数据集聚合并根据多种RGB相机光谱灵敏度进行渲染。

Result: 在多组消融与对比实验中，与仅RGB或以MS为主的基线相比，本方法在颜色准确性与稳定性上显著提升，最大误差降低约50%，并展示了在不同架构下的通用性。

Conclusion: 本文提出了一个统一的端到端学习框架，用于结合高分辨率RGB与低分辨率快照多光谱（MS）传感器共同完成色彩校正，整合整条流程并提升颜色准确性与稳定性。

Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.

</details>


### [53] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 提出不依赖额外训练的、不确定性驱动的子模子集选择方法，能在OOD与ID场景下提供更稳健且信息丰富的视觉归因与对象级解释。


<details>
  <summary>Details</summary>
Motivation: 当前基于子集的视觉模型解释方法在OOD场景中表现不佳，出现冗余与不稳定的解释，需引入不确定性信息以提高选择的多样性与信息量，从而增强鲁棒性与可信度。

Method: 通过对网络权重进行自适应扰动，逐层计算梯度导出的不确定性估计；将该不确定性用于加权或调制次模函数，驱动子模最优化以选出多样且信息量高的图像区域子集；方法不要求额外训练或外部模型。

Result: 提出了一种结合次模子集选择与逐层梯度不确定性估计的框架，通过自适应权重扰动估计不确定性并在子模优化中使用，提高在OOD和ID下的可解释性鲁棒性与保真度，无需额外训练或辅助模型。

Conclusion: 在OOD条件下，现有基于子集的方法会变得冗余、不稳定且对不确定性敏感；引入不确定性驱动的子模优化能有效缓解这些问题，并在ID环境下也带来提升。

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [54] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 该论文提出一种轻量级足球球员跟踪方法，将Segment Anything Model (SAM) 与CSRT跟踪器及球衣颜色外观模型结合，旨在提高遮挡恢复与跟踪稳定性。


<details>
  <summary>Details</summary>
Motivation: 足球场景中频繁遮挡、外观相似和快速运动使得跟踪任务困难。作者希望设计一套资源受限下仍能稳定运行的跟踪系统，利用SAM的精确初始化和颜色直方图重识别来提升遮挡恢复能力。

Method: 提出团队感知跟踪系统：使用SAM进行精确目标初始化，结合CSRT作为主要跟踪器；基于HSV颜色直方图构建队服外观模型，用于遮挡或丢失时的重识别。系统在三维度评估：速度（FPS与内存）、跟踪精度（成功率与边界框稳定性）与鲁棒性（遮挡恢复与身份一致性）。

Result: 在足球视频上测试：实现7.6-7.7 FPS，内存稳定约1880 MB；轻度遮挡下跟踪成功率为100%，拥挤禁区（>=5人）中为90%；基于外观的重识别在重度遮挡中恢复率为50%；长期目标离开画面再回归的重获成功率仅8.66%。

Conclusion: 在资源受限环境下，结合SAM与经典跟踪器的方案在持续可见条件下表现良好，但对长时离帧重识别能力不足，需更强的重识别机制以应对长时间遮挡与出入场景。

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [55] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出ContextDrag，通过潜空间参考特征注入和位置一致注意力，在无需微调或反演的前提下显著提升拖拽编辑的细节和语义一致性，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽式编辑方法未能充分利用参考图像中的上下文信息和细粒度纹理，导致编辑后的一致性和保真度不足，因此需要一种能在潜空间利用丰富上下文特征的无反演方法。

Method: 引入Context-preserving Token Injection（CTI）和Latent-space Reverse Mapping（LRM），将无噪声的参考特征映射并注入到目标位置；并提出Position-Consistent Attention（PCA），通过位置重新编码和重叠感知掩码减少无关参考特征干扰。整个流程无需模型微调或图像反演。

Result: 在DragBench-SR和DragBench-DR两套基准上进行大量实验，结果表明ContextDrag在保持语义与细节一致性方面优于所有现有SOTA方法。

Conclusion: ContextDrag通过在潜空间中注入参考图像的VAE编码特征，并结合位置一致注意力，显著提升拖拽式图像编辑的细节保持和语义一致性，省去微调或反演步骤，实验显示优于现有SOTA。

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [56] [Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform](https://arxiv.org/abs/2512.08478)
*Yuning Gong,Yifei Liu,Yifan Zhan,Muyao Niu,Xueying Li,Yuanjun Liao,Jiaming Chen,Yuanyuan Gao,Jiaqi Chen,Minming Chen,Li Zhou,Yuning Zhang,Wei Wang,Xiaoqing Hou,Huaxi Huang,Shixiang Tang,Le Ma,Dingwen Zhang,Xue Yang,Junchi Yan,Yanchi Zhang,Yinqiang Zheng,Xiao Sun,Zhihang Zhong*

Main category: cs.CV

TL;DR: Visionary是在浏览器中结合WebGPU渲染与每帧ONNX推理的开源平台，支持可插拔的高斯生成器与three.js集成，显著简化3D Gaussian Splatting类方法的部署与实验。


<details>
  <summary>Details</summary>
Motivation: 现有浏览器端查看器分散、臃肿或受限于传统管线，难以支持动态内容与生成模型，导致部署与复现困难；因此需要一个轻量、实时且可插拔的平台以支持3DGS家族方法。

Method: 基于WebGPU实现高效渲染（包含GPU端的primitive排序），并在每帧执行ONNX推理；引入标准化的Gaussian Generator合约以支持每帧生成/更新高斯体元；提供与three.js兼容的TypeScript插件API以便集成。

Result: 在相同3DGS资产下，Visionary比现有Web查看器具有更优的渲染效率；已支持多种变体（MLP-based 3DGS、4DGS、神经头像、风格变换/增强网络），并实现浏览器端统一推理与渲染。

Conclusion: Visionary有效整合了Web原生渲染与每帧ONNX推理，为3D Gaussian Splatting及相关神经渲染方法提供了轻量、可插拔且实时的浏览器端方案，降低了复现与部署门槛。

Abstract: Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, "click-to-run" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.

</details>


### [57] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: PCI measures Concept Insertion Success across timesteps to find when concepts become fixed during diffusion, enabling better editing without training or internals


<details>
  <summary>Details</summary>
Motivation: Understand temporal dynamics of concept formation in diffusion models to improve controllability and editing

Method: Define CIS metric by inserting prompt-conditioned concepts at specific diffusion timesteps; evaluate across models/concepts and use findings to guide interventions for image editing

Result: This paper introduces PCI, a method to analyze when concepts form during diffusion generation and to intervene by inserting prompts at different timesteps

Conclusion: Different concepts lock in at different timesteps across models; interventions at identified timesteps improve editing effectiveness

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [58] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 提出首个面向多摄像机阵列的即时3D Gaussian Splatting重建框架，通过层次化相机初始化、多摄像机BA、冗余剔除采样与频率感知优化在2分钟内对数百米场景进行高效无漂移重建


<details>
  <summary>Details</summary>
Motivation: Monocular on-the-fly 3DGS suffers incomplete 3D coverage due to limited FOV; multi-camera rigs can cover more but lack real-time, calibration-free on-the-fly fusion methods

Method: Incremental multi-camera 3D Gaussian Splatting with hierarchical init, lightweight multi-camera BA, redundancy-free sampling, frequency-aware optimization

Result: Drift-free trajectory estimation and efficient online reconstruction of large-scale scenes from raw multi-camera video streams; reconstructs hundreds of meters within 2 minutes

Conclusion: First on-the-fly multi-camera 3DGS framework achieving real-time, robust, and high-fidelity reconstruction without calibration; reduces primitives and optimization while maintaining quality

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [59] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: ReasonBreak通过概念层级的定向扰动破坏多模态大推理模型的地理推理链，配合GeoPrivacy-6K数据集，在多款大模型上显著提升地理位置隐私保护效果。


<details>
  <summary>Details</summary>
Motivation: 保护个人图像隐私，防止多模态大推理模型从图像中通过分层链式推理推断精确地理位置。

Method: 提出ReasonBreak，一种基于概念层级的对抗扰动框架；有针对性地定位推理链中的关键概念依赖，生成破坏特定推理步骤并在后续阶段产生级联效果的扰动；并构建GeoPrivacy-6K数据集（6,341张超高分辨率图像，含分层概念标注）用于训练与评估。

Result: 在七种最先进的MLRM（包括GPT-o3、GPT-5、Gemini 2.5 Pro）上进行广泛评估，ReasonBreak在区段级（tract-level）保护上取得了14.4%提升（33.8% vs 19.4%），在街区级（block-level）保护上几乎翻倍（33.5% vs 16.8%）。

Conclusion: 提出了针对基于推理的隐私威胁的新范式，通过概念感知扰动有效破坏多步推理链，从而显著提高地理隐私保护效果；并提供了GeoPrivacy-6K作为社区资源。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [60] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: 在反向扩散的带噪潜在空间用双编码器实时判断文本-图像对齐，能在BoN场景下节省一半计算并保留98% CLIP性能


<details>
  <summary>Details</summary>
Motivation: 减少生成完成后才可评估对齐的延迟与高昂计算成本，允许早期终止不合格样本以节省资源并降低幻觉发生频率

Method: 提出了NoisyCLIP方法，在带噪声的潜在空间中评估语义对齐，针对扩散模型生成过程中早期检测文本/图像不匹配

Result: 在生成过程的反向扩散早期即可检测到对齐问题，BoN设置下能把计算成本降低50%，同时达到CLIP对齐性能的98%

Conclusion: NoisyCLIP允许在生成过程中实时评估对齐性，节省计算资源且保持语义保真度，适用于减少错配和幻觉

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [61] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: OCCDiff将潜变量扩散与函数自编码器结合，通过点云编码器和多任务训练，在占据函数空间生成连续可评估的三维建筑表面表示，提升重建质量与噪声鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在不同分辨率与噪声干扰下，从LiDAR点云准确重建建筑表面困难，需要一种在任意采样位置生成高质量连续三维轮廓的灵活方法。

Method: 提出将潜变量扩散模型与函数自编码器结合：使用编码器将占据函数映射到潜空间，采用逆扩散在潜空间生成样本，并通过解码器输出可在任意位置评估的连续占据函数。引入点云编码器为扩散过程提供条件特征，并约束占据解码器，插入多模态特征以增强潜编码器生成。采用多任务训练策略以学习更鲁棒的点特征。

Result: 实验证明OCCDiff能生成与目标分布高度一致的样本，在有噪声的数据上具备鲁棒性，并能保持物理一致性与高保真度。

Conclusion: OCCDiff在占据函数空间中引入潜变量扩散，能够在不同点密度和噪声水平下生成连续、可任意采样的三维占据函数，提升了建筑重建的物理一致性与噪声鲁棒性。

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [62] [Thinking with Images via Self-Calling Agent](https://arxiv.org/abs/2512.08511)
*Wenxi Yang,Yuzhong Zhao,Fang Wan,Qixiang Ye*

Main category: cs.CV

TL;DR: See fields


<details>
  <summary>Details</summary>
Motivation: See fields

Method: Summarize methods, results, conclusion, tldr, motivation

Result: See fields

Conclusion: See fields

Abstract: Thinking-with-images paradigms have showcased remarkable visual reasoning capability by integrating visual information as dynamic elements into the Chain-of-Thought (CoT). However, optimizing interleaved multimodal CoT (iMCoT) through reinforcement learning remains challenging, as it relies on scarce high-quality reasoning data. In this study, we propose Self-Calling Chain-of-Thought (sCoT), a novel visual reasoning paradigm that reformulates iMCoT as a language-only CoT with self-calling. Specifically, a main agent decomposes the complex visual reasoning task to atomic subtasks and invokes its virtual replicas, i.e. parameter-sharing subagents, to solve them in isolated context. sCoT enjoys substantial training effectiveness and efficiency, as it requires no explicit interleaving between modalities. sCoT employs group-relative policy optimization to reinforce effective reasoning behavior to enhance optimization. Experiments on HR-Bench 4K show that sCoT improves the overall reasoning performance by up to $1.9\%$ with $\sim 75\%$ fewer GPU hours compared to strong baseline approaches. Code is available at https://github.com/YWenxi/think-with-images-through-self-calling.

</details>


### [63] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [64] [MVP: Multiple View Prediction Improves GUI Grounding](https://arxiv.org/abs/2512.08529)
*Yunzhu Zhang,Zeyu Pan,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Linchao Zhu*

Main category: cs.CV

TL;DR: 针对GUI定位在像素级预测中对轻微扰动高度不稳定的问题，MVP通过从注意力引导的多裁剪视图中聚合坐标预测（采用最密集簇的质心）来滤除异常点，无需额外训练，在多模型与基准上显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 现有GUI grounding模型对图像的微小裁剪或噪声极为敏感，导致坐标预测抖动，尤其对高分辨率和小UI元素样本影响严重，亟需稳定化策略以提高可靠性。

Method: MVP由两部分组成：1) Attention-Guided View Proposal：利用指令-图像注意力分数生成多样化且有信息的裁剪视图，从而覆盖可能包含正确坐标的区域；2) Multi-Coordinates Clustering：对各视图产生的坐标预测进行聚类，选取密度最大的簇的质心作为最终坐标，滤除离群点。该方法在推理阶段运行，无需额外训练或修改原模型。

Result: 在多个模型与数据集上均有显著提升。在ScreenSpot-Pro基准上，MVP将UI-TARS-1.5-7B提升至56.1%，GTA1-7B至61.7%，Qwen3VL-8B-Instruct至65.3%，Qwen3VL-32B-Instruct至74.0%。

Conclusion: 本文提出MVP，一种无训练的多视图推理框架，通过注意力引导的视图生成与多坐标聚类显著提升GUI定位的稳定性与性能。

Abstract: GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.

</details>


### [65] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: 提出一个结合参考图、草图和文本的多模态油画生成编辑系统，利用笔触渲染自监督生成训练数据并用AdaIN保证风格一致，能进行细粒度可控编辑。


<details>
  <summary>Details</summary>
Motivation: 油画生成与编辑难点在于笔触动态与风格化特性，且现有方法受限于训练数据分布，多数针对真实照片改动，缺乏对油画风格的统一控制与多模态精细交互。

Method: （1）训练时的空间对齐与语义增强条件策略：将掩码与草图映射为空间约束，将参考图与文本编码为语义上下文特征以实现对象级语义对齐；（2）基于笔触渲染（SBR）的自监督风格迁移：模拟油画修复的图像修补过程，把真实图像转换为保留笔触纹理的油画风格样本，构建大规模配对数据；（3）推理阶段用AdaIN进行特征融合以保持统一风格。

Result: 提出了一个统一的多模态油画生成与编辑框架，支持参考图像、手绘草图和文本提示进行语义与空间控制，并保持统一风格；在训练阶段引入空间对齐和语义增强条件策略；提出基于笔触渲染的自监督风格迁移，生成大规模配对数据；推理时用AdaIN融合特征保证风格一致性；系统能实现细粒度编辑并保留油画艺术特性。

Conclusion: 该方法通过空间与语义约束、自监督笔触风格迁移以及AdaIN特征融合，有效解决了油画生成与编辑中数据匮乏和风格一致性问题，支持多模态交互并实现高质量、可控的油画创作与修饰。

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [66] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Paper proposes Photo3D: uses GPT-4o-Image generated images to create structure-aligned multi-view dataset paired with 3D geometry; introduces detail-enhancement via perceptual feature adaptation and semantic structure matching to improve photorealistic textures while preserving geometry; general across 3D-native generators; shows SOTA results.


<details>
  <summary>Details</summary>
Motivation: 3D-native generators produce good geometry but lack realistic appearance due to scarcity of diverse, high-quality textured 3D assets; real capture is hard (scale, non-rigidity, scanner limits). Use abundant image data from advanced image LLM to improve textures.

Method: Generate images with GPT-4o-Image, then apply a structure-aligned multi-view synthesis pipeline to enforce multi-view consistency. Construct a detail-enhanced multi-view dataset paired with 3D geometry. Propose a realistic detail enhancement scheme combining perceptual feature adaptation and semantic structure matching to transfer realistic details while keeping structural consistency. Provide training strategies for geometry-texture coupled and decoupled 3D-native generation paradigms.

Result: Photo3D generalizes across different 3D-native generators and achieves state-of-the-art photorealistic 3D generation performance in experiments.

Conclusion: Using image LLM outputs plus structured multi-view alignment and perceptual/semantic matching enables substantial improvement in texture realism for 3D-native generators, overcoming data scarcity and improving photorealism while maintaining geometry fidelity.

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [67] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: Fast-ARDiff通过熵感知speculation、联合端到端训练与联合蒸馏，在保持质量下显著加速AR-扩散混合生成，实验证明多任务上可达3–4.3×加速。


<details>
  <summary>Details</summary>
Motivation: AR-扩散混合范式结合了AR结构化建模和扩散光逼真合成，但因AR的顺序生成与扩散的迭代去噪导致延迟高，需提出加速方案同时保持图像质量。

Method: 提出熵感知的speculative策略促使草稿模型输出更高熵表示以减少拒绝率；将扩散解码器并入端到端训练框架，使用动态调度器优先优化AR部分并通过轨迹与分布匹配的联合蒸馏优化扩散模块，实现极少步数下稳定高质合成；推理时用浅层特征熵预筛低熵草稿以减少冗余计算。

Result: 在ImageNet 256×256上，TransDiff实现4.3×无损加速；在文本条件生成上，NextStep-1实现3×加速。

Conclusion: Fast-ARDiff通过协同优化自回归模型和扩散模型，在保持生成质量的前提下显著降低延迟，实现了在多种任务上的显著加速。

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [68] [A Novel Wasserstein Quaternion Generative Adversarial Network for Color Image Generation](https://arxiv.org/abs/2512.08542)
*Zhigang Jia,Duan Wang,Hengkai Wang,Yajun Xie,Meixiang Zhao,Xiaoyu Zhao*

Main category: cs.CV

TL;DR: 提出四元数Wasserstein距离与其对偶理论，基于此构建WQGAN，有效提升彩色图像生成质量，尤其改善通道相关性建模与色差问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型通常忽略RGB通道间的相互关系，导致色彩失真；同时缺乏系统的彩色图像数据分布测度理论。引入四元数可以自然表示多通道像素并建模通道间相关性。

Method: 定义四元数Wasserstein距离，推导其强对偶形式。为解决四元数线性规划问题，利用四元数凸集分离定理和四元数Farkas引理证明强对偶性。基于该距离，设计并训练WQGAN模型用于彩色图像生成。

Result: 实验表明，WQGAN在生成效率和图像质量上均优于传统GAN、WGAN以及四元数GAN等基线模型，能够减少色差并生成更真实的彩色图像。

Conclusion: 本文提出了四元数Wasserstein距离及其对偶理论，并基于此构建了四元数Wasserstein生成对抗网络（WQGAN），以更好地捕捉彩色图像通道间相关性，解决色差问题。

Abstract: Color image generation has a wide range of applications, but the existing generation models ignore the correlation among color channels, which may lead to chromatic aberration problems. In addition, the data distribution problem of color images has not been systematically elaborated and explained, so that there is still the lack of the theory about measuring different color images datasets. In this paper, we define a new quaternion Wasserstein distance and develop its dual theory. To deal with the quaternion linear programming problem, we derive the strong duality form with helps of quaternion convex set separation theorem and quaternion Farkas lemma. With using quaternion Wasserstein distance, we propose a novel Wasserstein quaternion generative adversarial network. Experiments demonstrate that this novel model surpasses both the (quaternion) generative adversarial networks and the Wasserstein generative adversarial network in terms of generation efficiency and image quality.

</details>


### [69] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出一种无迭代的固定点估计器，通过用前一步误差近似当前误差，得到可计算的无偏低方差固定点表达式，在NOCAPS和MS-COCO上优于DDIM与其它固定点迭代方法。


<details>
  <summary>Details</summary>
Motivation: 降低扩散反演中逐步迭代带来的误差累积及提高计算效率，避免固定点迭代方法昂贵的计算开销和超参数调节复杂性。

Method: 推导理想反演步骤下固定点的显式表达式，发现含有未知的数据预测误差。提出用前一步可计算的误差近似当前步的未知误差，从而得到可计算的近似固定点估计器。理论分析表明该估计器无偏且方差低。

Result: 在NOCAPS和MS-COCO两个文本-图像数据集上进行重构实验。与DDIM反演及基于固定点迭代的其他方法比较，在不增加迭代或训练的情况下，本方法在重构任务上表现稳定且优越。

Conclusion: 通过用前一步误差近似当前未知误差，构造了无迭代的固定点估计器，显著降低了计算成本并保持或提升了重构性能，为高效扩散反演提供了可行方案。

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [70] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 利用时序数据复用与基于scatter的稀疏卷积扩展，SSCATeR只对帧间变化区域计算，实现等效特征输出并最多6.61x加速。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在连续扫描中多数区域不发生变化，重复计算浪费大量资源；通过只处理变化区域可降低计算量和提高实时性。

Method: 采用短步长滑动时间窗口，存储帧间卷积结果并只对发生变化的区域执行计算；扩展先前的基于scatter的稀疏卷积以支持时序数据复用，得到SSCATeR算子。

Result: 在保持与传统稀疏卷积等效的输出特征图的同时，处理时间最多降低至6.61倍，显著提升网络计算效率。

Conclusion: 本文提出通过利用LiDAR持续扫描的时间维度复用特征计算，显著减少卷积次数，同时保持检测精度。

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [71] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: 本文提出一个大规模自动化框架，通过无监督分解发现fMRI中可解释的视觉模式，并用自动化管道为每个模式找到最能诱发它的自然图像集合及自然语言描述，最终在大脑皮层发现数千个可解释模式，覆盖细粒度视觉概念。


<details>
  <summary>Details</summary>
Motivation: 现有研究多为小规模、需人工检查、聚焦特定脑区且缺乏系统验证；为全面发现大脑中表征的视觉概念，需要一个可扩展、自动化且可量化可靠性的框架。

Method: 两阶段方法：1) 使用无监督、数据驱动的分解方法从fMRI数据中发现候选可解释模式（即每个voxel的模式）；2) 解释每个模式：找出最强诱发该模式的自然图像集合，并生成这些图像共享视觉含义的自然语言描述。引入自动化管道：测试多种候选解释、给出量化可靠性得分、为每个voxel模式选择最一致的描述。

Result: 框架揭示了数千个可解释的图案，覆盖许多不同的视觉概念，包括此前未报道的细粒度表征。通过可靠性评分筛选，结果系统性且可扩展。

Conclusion: 该自动化框架可大规模发现并解释人类大脑皮层中的视觉表征，为理解视觉概念的脑表征提供了新的工具和资源，推动更全面的系统验证和映射。

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [72] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出模块化神经ISP：模块化设计带来可控性、可扩展性、可调试性和风格灵活性；参数量适中（~0.5M–3.9M），在多数据集上取得有竞争力的质与量化结果，并支持交互式照片编辑与无限次可编辑重渲染。


<details>
  <summary>Details</summary>
Motivation: 传统端到端神经ISP和黑盒式方法缺乏对渲染过程的可控性，难以调试、适配未知相机或满足用户多样化风格与交互编辑需求。作者旨在通过模块化设计弥补这些缺陷，同时保持高质量渲染与模型轻量化。

Method: 将ISP拆分为多个可控子模块，分别处理原始输入到显示图像的关键中间阶段（如去马赛克、白平衡、色彩映射、伽马/曲线调整等），每个模块采用学习型网络且整体参数量在0.5M–3.9M之间。模块化接口允许在渲染流程中插入用户偏好或编辑指令，并支持不同容量变体以权衡性能与计算资源。作者还实现了基于该ISP的交互式照片编辑工具，使编辑操作在高质量渲染下可无限次后编辑与重渲染。

Result: 在多个测试集上，所提方法在定性与定量指标上均表现竞争力，证明了模块化设计在渲染精度、泛化性、可调节性和可解释性方面的优势；工具示例和补充视频进一步展示了交互编辑与重渲染能力。

Conclusion: 本文提出了一种模块化神经图像信号处理（ISP）框架，通过对渲染过程的多个中间阶段进行显式模块化，实现在不同相机和用户偏好下的高质量显示级图像渲染与可交互后期重渲染。

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [73] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: 提出一种基于图像内置信度分布自适应伪标签和动态类平衡的CTTA方法，能更可靠地监督并减少误差累积，在多种域变换场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在持续测试时适应（CTTA）场景中，预训练模型需要应对不断变化的域，但现有方法使用固定或批量阈值，难以适应类间和实例间难度差异，尤其对语义分割这种需要密集多类预测的任务影响严重。提出更细粒度的伪标签调整和动态类平衡策略来解决该问题。

Method: 在图像级别根据置信度分布自适应调整伪标签阈值，并对受域移位影响较大的类别动态增加学习权重，结合类-实例感知的伪监督机制在持续自适应过程中更新模型以减少错误累积。

Result: 在八个CTTA和TTA情景（包括合成到真实和长期变换）上进行大量实验，结果显示该方法持续优于最先进技术，在语义分割的演化条件下取得更高的准确性和更稳健的性能。

Conclusion: 通过类和实例感知的自适应伪标签与动态类平衡，可以显著提升语义分割在持续域变换下的自适应能力，减少误导性伪标签造成的性能下降，成为该任务的新基准。

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [74] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren Schüllerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,Nataša Sladoje*

Main category: cs.CV

TL;DR: HiGINE通过分层图神经网络与多模态融合，利用mIF图像中细胞类型与形态信息建模局部与全局细胞交互，从而提升肺癌患者生存期二分类与风险分层的性能并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 肿瘤微环境中细胞间复杂相互作用对患者预后有重要影响，传统基于单细胞或全局统计量的方法难以同时捕捉局部与全局关系以及细胞类型与形态信息，因而需要一种能在多尺度上建模细胞间交互并融合临床信息的深度学习方法。

Method: 提出了HiGINE：一种分层图神经网络框架。首先从mIF图像中提取细胞节点并编码细胞类型及形态特征；构建多尺度图结构以表示局部邻域与全局交互；在每个层次使用图神经网络聚合信息并进行跨层融合；最后将图特征与临床分期等模态做融合用于生存二分类（短期vs长期）。模型训练使用监督学习并在两个公共数据集上进行交叉验证和外部验证。

Result: 在两个公开mIF数据集上，HiGINE在生存二分类任务中优于基线模型（单尺度GNN、传统机器学习、仅临床模型等），表现为更高的AUC/准确率及更显著的风险分层；多模态融合模型进一步提升性能；并在跨数据集验证中显示出较好的鲁棒性与泛化能力。

Conclusion: HiGINE有效提升了基于mIF图像的肺癌患者生存期二分类与风险分层能力，通过分层图神经网络同时建模局部细胞邻域与全局细胞间关系，并融合细胞类型与形态信息。与仅用病理或仅用mIF特征的方法相比，多模态融合（加入临床分期）进一步提升了性能。模型在两个公开数据集上验证表现出更好的稳健性与泛化能力。

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [75] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 提出一种自动检测灯具移动并自动对齐多摄像头录制的手术视频的方法，通过选择遮挡最低的摄像头和视图合成生成固定视角、易于观看的高质量手术视频，用户研究显示优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 外科手术录像受医生在手术中频繁遮挡与灯具移动影响，手动调节摄像头或后期手动对齐耗时费力，亟需自动化方案以提高教学与研究视频质量与观看体验。

Method: 方法包括检测灯具移动的帧、对移动帧进行重新对齐（可能包括图像配准/透视变换/多摄像头拼接）并选择遮挡最少的摄像头画面进行输出。还实现了若干视图合成选项以增强输出视频的连贯性与观感。

Result: 通过用户研究（外科医生参与），结果显示本方法生成的视频在确认手术区域的便利性和观看舒适度上优于传统方法，同时在视频质量评价上也有提升。不同合成选项的偏好也通过用户研究得到评估。

Conclusion: 该论文提出了一种自动化方法，对手术过程中因移动灯具导致的多摄像头视频帧对齐与视角选择进行实时处理，从而生成持续固定视角、遮挡最小的手术视频。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [76] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: Used YOLOv8s and MobileNetV3L to detect/classify pollen in optical and holographic images; strong results on optical, large gap on holographic improved but still limited (detection 13.3% mAP50, classification 54%).


<details>
  <summary>Details</summary>
Motivation: Automate and improve pollen grain detection/classification in microscopy images, enabling cost-effective veterinary cytology diagnostics and leveraging deep learning with lensless holographic microscopy.

Method: Train YOLOv8s for object detection and MobileNetV3L for classification on datasets of optical and holographic microscopy images. Improve holographic performance by expanding dataset via automated labeling and enlarging bounding box areas.

Result: Paper applies deep learning (YOLOv8s for detection, MobileNetV3L for classification) to pollen grain analysis in optical and holographic microscopy for veterinary cytology.

Conclusion: Models perform well on optical images (91.3% mAP50 detection, 97% classification accuracy). Holographic images initially poor; improved via automated labeling and bbox enlargement to 13.3% mAP50 detection and 54% classification accuracy. Demonstrates feasibility of using lensless digital holographic microscopy for classification with deep learning, though detection remains weak.

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [77] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: 针对CLIP的模板-样本相似性偏差，提出用空提示在预训练和少样本微调中进行校准，能有效降低偏差、提升准确率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 观察到CLIP在使用手工设计的文本模板时会受到Template-Sample Similarity（TSS）偏差影响，模型更依赖模板与样本的接近程度而非真实的样本-类别对齐，导致性能波动和鲁棒性下降，因此需要一种方法来纠正模板引入的偏差。

Method: 提出一个两阶段框架：1) 在预训练阶段使用空提示揭示并减少编码器中的模板诱导偏差；2) 在少样本微调阶段加入偏差校准损失，强制图像与正确类别的对齐。空提示不包含类别信息，仅表达“空”的概念，用以捕捉无偏模板特征并补偿TSS偏差。

Result: 在多个基准数据集上的实验表明：所提模板校正方法显著降低了由TSS引起的性能波动，提高了分类准确率并增强了鲁棒性。并且公开了实现代码仓库。

Conclusion: 本文结论是：CLIP模型在模板与样本相似性（TSS）上存在偏差，该偏差削弱了分类准确性与鲁棒性；使用空提示（empty prompts）来刻画无偏模板特征，并在预训练与少样本微调中引入校准可有效消除该偏差，从而提升性能。

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [78] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: 提出OpenMonoGS-SLAM：无深度单目SLAM结合3D高斯点与视觉基础模型实现开放集语义与高质量建图与定位。


<details>
  <summary>Details</summary>
Motivation: 现有结合SLAM与语义的方法依赖深度传感器或封闭集语义模型，难以扩展到开放世界场景；因此需要无深度、开放词汇且可泛化的单目SLAM-语义一体化方法。

Method: 基于MASt3R实现的视觉几何、自监督损失驱动的单目跟踪与3DGS地图优化；结合SAM和CLIP提供开放词汇语义信息；设计高维语义特征记忆机制，构建高斯语义特征图。

Result: 在闭集与开放集分割任务上表现与甚至优于现有基线，同时无需深度或语义标注等额外传感器/数据。

Conclusion: 提出了一种名为OpenMonoGS-SLAM的第一款结合单目SLAM、3D高斯点（3DGS）与开放集语义理解的框架，能在无深度输入和无3D语义标注情况下，利用视觉基础模型实现精确相机定位、地图构建与开放词汇语义分割。

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [79] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 通过分析因相机旋转引起的透视相关模糊和视频中稀疏轨迹，使用窗口嵌入、多窗口聚合与视觉语言模型稠密化轨迹，得到高质量的深度估计与轨迹重建


<details>
  <summary>Details</summary>
Motivation: 相机在无机械稳定器情况下旋转引起的透视模糊与景深相关，利用这种深度相关模糊信息可以从单一视频中恢复绝对度量深度

Method: 使用现成的视觉编码器与点跟踪器提取视频信息；通过窗口嵌入与多窗口聚合估计深度图；用视觉语言模型将稀疏轨迹稠密化；并设计联合光学算法以提升轨迹精度。

Result: 提出了一种基于视角模糊模式估计度量深度的方法，结合视频流的模糊分析与稠密轨迹重建

Conclusion: 方法在多个深度数据集上表现优良，能在大深度范围内保持泛化；光学算法在手持拍摄轨迹估计上精度较高，稠密重建准确

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [80] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 本文在仅使用机载单目RGB和自然语言指令的约束下，提出了将导航视为下一令牌预测的统一框架，结合关键帧筛选与长尾平衡策略，在Aerial VLN数据集上显著超越其他RGB-only基线并接近RGB-D水平。


<details>
  <summary>Details</summary>
Motivation: 传统航拍VLN方法依赖全景图像、深度或里程计等昂贵传感器，增加了UAV系统成本和集成复杂度，不利于轻量级无人机部署。该工作旨在仅用单目RGB输入实现可靠的空间推理与动作规划，降低硬件要求并提高实用性。

Method: 作者将导航任务转化为下一个令牌预测，采用提示引导的多任务学习架构联合训练视觉感知、轨迹推理和动作预测模块；提出关键帧选择以保留语义信息、减少冗余帧；设计动作合并与标签重加权机制缓解长尾标签分布并稳定多任务训练。

Result: 在Aerial VLN基准上，单目RGB-only模型在seen和unseen环境均取得强性能，显著优于其他RGB-only基线，并缩小了与全景RGB-D最先进方法的差距。消融实验显示关键帧策略、动作合并与标签重加权对性能有正向贡献。

Conclusion: 该论文提出了一个基于单目RGB的统一航拍视觉与语言导航（VLN）框架，通过将导航建模为下一个令牌预测问题，采用提示引导的多任务学习同时优化空间感知、轨迹推理和动作预测；提出关键帧选择策略、动作合并与标签重加权以应对视觉冗余及长尾监督不平衡；在Aerial VLN基准上取得了显著性能提升，在仅使用单目RGB的设置下优于现有RGB-only方法并接近全景RGB-D方法；消融实验验证了设计有效性。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [81] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: 将图像生成拆成可监控的语义步骤（CoIG），用LLM分解指令并逐步编辑图像，提升中间步骤可读性与因果可解释性，缓解实体塌缩，模型无关且性能有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有高质量图像生成模型内部流程不透明，难以监控与干预，且生成流程与人类思维不同，导致可解释性与可靠性不足。受Chain-of-Thought在LLM上的启发，作者希望通过分步语义流程提升图像生成的可监控性与组合表现。

Method: CoIG先由大语言模型将复杂文本提示分解为一系列简单的逐步指令；随后图像生成模型按步骤生成并编辑图像，每步关注单一语义实体。提出两项新度量：CoIG Readability（评估中间步骤与对应输出的清晰度）和Causal Relevance（量化步骤对最终图像的影响）。

Result: 实验证明CoIG在量化的可监控性指标上有显著提升，同时在复杂组合能力（组合鲁棒性）上与基线模型相当，且能够缓解实体塌缩。框架对模型无依赖，可与任意图像生成模型集成。

Conclusion: 本文提出Chain-of-Image Generation (CoIG)框架，通过将图像生成重构为类似人类创作的顺序语义流程，实现了更高的可监控性与可控性。CoIG在保持生成质量与组合鲁棒性的同时，显著提升了中间步骤的可读性与因果相关性，从而缓解实体塌缩问题。

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [82] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: C-DIRA通过Top-K显著ROI、动态路由与对抗域学习，在边缘设备上实现高效且具域稳健性的轻量驾驶员分心识别。


<details>
  <summary>Details</summary>
Motivation: 需要在边缘设备上实现实时驾驶员分心行为识别，解决轻量模型表现欠佳与ROI方法计算开销大的冲突，以及提高对未见驾驶员/环境的泛化能力。

Method: 提出结合显著性Top-K ROI池化与融合分类的本地特征提取，并通过动态ROI路由对困难样本才执行ROI推理以节省计算；采用伪域标签与对抗学习实现域不变特征学习。

Result: 在State Farm数据集上，C-DIRA在保持或接近现有轻量模型精度的同时显著降低FLOPs与推理延迟，并在模糊与低光等退化场景及未见域上表现更稳定。

Conclusion: C-DIRA在保持高准确率的同时显著降低了计算量和延迟，且对视觉退化与未见域具有稳健性。

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [83] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: 提出一个无需外部编码器、使用动态内存库的对比学习框架以增强去噪生成模型，节省计算开销且加速收敛，在ImageNet-256上达成SOTA FID 2.40（400k步）。


<details>
  <summary>Details</summary>
Motivation: 现有去噪生成模型训练成本高、表征学习效率低；依赖外部预训练判别编码器带来开销和域偏移问题。

Method: 提出MNAME，一个无需外部编码器的plug-and-play训练框架；引入内存库（动态队列）保存大量负样本，结合低维投影头用于对比目标，负样本数量与小批量大小解耦。

Result: 在ImageNet-256上，MNAME在400k步内达成FID 2.40，显著优于可比方法；训练收敛更快。

Conclusion: MNAME通过内存库和低维投影实现自包含的对比增强，消除了预训练模型依赖并提升训练效率与生成质量。

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [84] [Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds](https://arxiv.org/abs/2512.08673)
*Shaofeng Zhang,Xuanqi Chen,Xiangdong Zhang,Sitong Wu,Junchi Yan*

Main category: cs.CV

TL;DR: 提出中心-周边双分支对比学习（CSCon），通过差异化掩码与补丁级对比损失增强3D点云判别性，显著优于Point-MAE等生成式方法。


<details>
  <summary>Details</summary>
Motivation: 生成式MAE在3D点云SSL中难以学习到高层判别性特征，导致线性探测等下游任务表现不佳；而对比学习在图像上表现优异，但直接移植到3D不能有效捕捉局部细节，因此需设计专门的3D对比学习框架。

Method: 对点云分别进行中心偏置和周边偏置的掩码，构造双分支输入，结合patch-level对比损失以同时增强高层表示和局部细节敏感性。

Result: 在FULL和ALL协议下性能可与生成式方法相当；在MLP-LINEAR、MLP-3和ONLY-NEW协议下达到或超越最先进水平；在MLP-LINEAR协议上，比Point-MAE在ScanObjectNN三个变体上分别提升7.9%、6.7%和10.3%。

Conclusion: 本文提出CSCon框架，通过中心-周边双分支掩码和补丁级对比损失提升3D点云对比学习的判别能力，实验在多种协议下优于或可比于生成式方法，在ScanObjectNN上显著超过Point-MAE基线。

Abstract: Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \textbf{C}enter-\textbf{S}urrounding \textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \textbf{7.9\%}, \textbf{6.7\%}, and \textbf{10.3\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.

</details>


### [85] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: 提出基于LoRA的属性专家混合模型MoSAIC-ReID，用可控路由评估属性对ReID的贡献，发现服装颜色与若干内在属性最重要，稀有配饰作用有限，并给出可解释ReID的框架。


<details>
  <summary>Details</summary>
Motivation: 调查现有行人重识别模型在高层语义属性上的依赖，量化不同行人属性（如服装颜色、配饰、性别等）对ReID性能的重要性，提升模型可解释性并提供可操作的语义知识整合框架。

Method: 提出MoSAIC-ReID，一种Mixture-of-Experts框架：使用基于LoRA的专家模块，每个专家对应单一属性；引入oracle路由器以在测试时根据属性注释选择专家，从而进行受控的归因分析。通过在Market-1501和DukeMTMC上评估并结合广义线性模型、统计检验与特征重要性分析，系统性地衡量属性贡献。

Result: 在假设测试时可用属性注释的前提下，MoSAIC-ReID在Market-1501和DukeMTMC上取得有竞争力的性能。通过大规模定量研究，发现服装颜色与若干内在特征对ReID影响最大，而不常见的线索（如配饰）影响有限。统计分析支持这些发现并指出不同属性的相对贡献。

Conclusion: MoSAIC-ReID提供了一个可解释且可操作的ReID归因框架，明确了在实务中整合显式语义知识的需求与限制，同时为未来将语义属性融入模型提供依据。

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [86] [Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth](https://arxiv.org/abs/2512.08700)
*Kyumin Hwang,Wonhyeok Choi,Kiljoon Han,Wonjoon Choi,Minwoo Choi,Yongcheon Na,Minwoo Park,Sunghoon Im*

Main category: cs.CV

TL;DR: 通过将尺度不变的深度分箱概率从foundation model蒸馏到轻量FSMDE网络，并引入视图关系蒸馏与度量尺度中心约束，实现高效且具度量尺度的一致性全景单目深度估计。


<details>
  <summary>Details</summary>
Motivation: 现存foundation模型虽能泛化并预测相对深度，但直接用于FSMDE存在计算开销大（无法实时）和难以获得度量尺度深度的两大挑战。需将其鲁棒性与轻量模型效率结合并恢复度量尺度。

Method: 采用混合回归框架：将分类式的知识蒸馏（蒸馏深度bin概率）与深度分箱模块结合；引入交叉交互蒸馏以蒸馏尺度不变的bin概率，并用真实深度引导学生网络学习度量尺度的bin中心；提出视图关系蒸馏以编码并传递相邻相机视角间的结构关系，增强跨视图一致性。

Result: 在DDAD和nuScenes数据集上，该方法相比传统有监督方法与现有蒸馏方法表现更好；在性能-效率折衷上优越，满足实时需求。

Conclusion: 该论文提出了一种知识蒸馏策略，将foundation model的深度知识迁移到轻量级全景单目深度估计网络，以解决实时性和度量尺度问题。

Abstract: Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.

</details>


### [87] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 将SAM 3直接用于遥感开放词汇语义分割：通过掩码融合与存在性过滤两步无训练策略，能在多数据集上实现有前景的分割性能，特别在减少假阳性与改善小目标覆盖方面有效。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的遥感OVSS方法多依赖CLIP，存在精确定位不足或需复杂模块融合的问题；遥感场景目标多且密集，传统方法难以兼顾语义识别与精细定位。SAM 3提供了可提示的统一分割与识别框架，值得探索其在遥感OVSS上的直接应用。

Method: 提出两步简单策略：1) 掩码融合：结合SAM 3的语义分割头与Transformer实例头输出的掩码，利用互补信息改善对密集小目标的覆盖与定位；2) 存在性过滤：使用presence head的分数判定场景中实际存在的类别，剔除大词表中不相关类以减少假阳性。整个流程无训练，仅基于SAM 3的输出与后处理规则。

Result: 在多种遥感数据集上的评估显示，该无训练适配方法取得了有竞争力的性能，能在保持简单性的同时降低误检并改善密集小目标的分割效果。代码已开源（链接提供）。

Conclusion: 本文证明在无训练条件下，将SAM 3应用于遥感开放词汇语义分割（OVSS）是可行且有效的。通过融合SAM 3的语义头与实例（Transformer）解码器输出，并利用presence head的存在性得分过滤不存在类别，能显著降低误检并提升土地覆盖识别性能。

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [88] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 将皮肤色调看作连续变量并用KDE建模分布，基于统计距离进行分布重加权（DRW）以提升个体层面公平性，FS、WD、HM、HS等度量表现最好。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究常依赖粗糙的群体划分（例如‘白人/黑人’），忽略了群体内个体差异，导致对少数或边缘个体的不公平性被掩盖。该研究旨在提出一种更精细的、基于分布的评估与纠正机制，以改进皮肤病理图像分类模型的个体公平性。

Method: 1) 将皮肤色调作为连续属性；2) 使用KDE估计数据上每个色调的概率密度分布；3) 比较12种统计距离（例如Wasserstein、Hellinger等）来衡量训练/测试或各类之间的分布差异；4) 基于所选距离构造距离基重加权（DRW）损失，以提升训练中对少数色调样本的权重；5) 在CNN和Transformer上对比传统分组重加权与分布重加权的效果。

Result: 实验显示：1) 传统的类别重加权无法充分捕捉个体层面的不公平性；2) 分布重加权能更好地纠正少数色调欠代表问题并提升模型的公平性与性能；3) 尤其以Fidelity Similarity、Wasserstein Distance、Hellinger Metric和Harmonic Mean Similarity作为距离度量时表现最佳。

Conclusion: 该论文提出了基于分布的皮肤色调公平性评估与纠正框架，通过将皮肤色调视为连续属性并使用核密度估计（KDE）建模分布，以替代传统的基于群体的粗分类方法，从而更好地捕捉个体层面的差异。作者比较了十二种统计距离度量，并提出了基于距离的重加权（DRW）损失函数以纠正少数色调的欠代表问题。实验证明，与分组重加权相比，分布重加权在反映个体不公平性和提升模型性能（特别是采用FS、WD、HM、HS等距离度量）方面更具优势。该方法对医疗图像中其他连续敏感属性也具有推广意义。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [89] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

TL;DR: 提出首个基于姿态关键点的端到端手语检索（Sign Language Spotting）方法，在WSLP 2025数据集上达61.88%准确率和60.00% F1，表明姿态表示可行且高效。


<details>
  <summary>Details</summary>
Motivation: Bridging communication gap between deaf and hearing communities by enabling sign-to-sign retrieval (spotting a query sign within continuous sign sequences).

Method: End-to-end encoder-only model operating on pose keypoints with a binary classification head to predict presence/absence of query sign within a target sequence; uses pose instead of RGB for efficiency and robustness.

Result: On WSLP 2025 Word Presence Prediction dataset achieved 61.88% accuracy and 60.00% F1-score.

Conclusion: Pose-based, end-to-end spotting is effective and computationally efficient, providing a strong baseline for future sign language retrieval and verification research.

Abstract: Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [90] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*Artúr I. Károly,Péter Galambos*

Main category: cs.CV

TL;DR: 作者提出将3D渲染与受约束扩散风格化结合生成高真实感且带注释的合成蘑菇图像，训练出的Mask R-CNN在真实数据上实现SOTA分割（F1=0.859），数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 工业化蘑菇种植需要大规模视觉检测与分割模型，但获取高质量标注的实拍数据代价高。合成数据可扩展但通常欠缺真实感，导致泛化不足，因此需要一种既可控又能产生高真实度合成图像的自动化流水线。

Method: 在Blender中构建可控3D场景并渲染基础图像，结合受约束的扩散模型对渲染图像进行风格化以提升真实感；通过这种流水线自动导出像素级分割、边界框等标注。使用合成数据训练Mask R-CNN并在两个真实数据集（含新收集基准）上进行零样本评估。

Result: 发布了两个合成数据集（各6000张图、总计超25万蘑菇实例）。在零样本测试上，Mask R-CNN取得了在M18K数据集上F1=0.859的SOTA分割结果，显示出仅用合成训练数据即可实现良好跨域性能。

Conclusion: 本文提出的工作展示了合成数据与受约束扩散模型结合、在Blender中渲染3D场景，能够生成高质量、带精确标注的拟真蘑菇图像，从而显著降低真实标注成本并在零样本实拍评测中取得了SOTA分割性能。

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [91] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 提出基于输出分布偏度的选择性剪枝方法，对多模态Swin Transformer的注意力和MLP层进行剪枝，在水平联邦学习中能在保持性能的前提下减少约36%模型大小，适合隐私敏感的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 大型视觉模型在医学影像（如皮肤病分类）上性能优异但计算量大、模型大，不利于边缘设备部署；同时数据隐私限制促使采用联邦学习。因此需要在分布式（联邦）场景下实现高效的模型压缩，保持性能并保护隐私。

Method: 基于统计学中的偏度（skewness）度量，计算各MHSA和MLP层输出的分布偏度，并据此判断该层或通道的重要性；对偏度低或不显著的结构进行选择性剪枝。将该剪枝策略集成到水平联邦学习框架中，支持多模态输入的紧凑Swin Transformer，并在本地客户端执行剪枝与训练，随后在服务器端聚合模型参数。

Result: 在紧凑Swin Transformer上实验表明，采用该方法可在水平联邦学习环境中实现约36%的模型大小减少，而准确率无明显下降，表明该方法在保持诊断性能的同时提升了模型的计算和存储效率，适用于边缘设备的隐私保护医学AI部署。

Conclusion: 该论文提出了一种基于偏度（skewness）引导的剪枝方法，用于多模态Swin Transformer的MHSA和MLP层，在联邦学习场景下实现模型压缩与隐私保护的边缘部署。实验证明在水平联邦学习环境下，压缩约36%模型规模的同时准确率不降。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [92] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: 通过将稠密点轨迹投影并沿轨迹传播第一帧特征，Wan-Move把条件特征变为运动感知，能无缝整合到现有图像到视频模型，实现细粒度且可扩展的运动控制，实验证明其优于现有方法并发布了MoveBench。


<details>
  <summary>Details</summary>
Motivation: 现有运动可控方法控制粒度粗、可扩展性差，难以满足实际应用，作者希望实现精确、高质量且可扩展的运动控制，能与商用工具媲美。

Method: 用稠密点轨迹表示物体运动，将轨迹投影到潜在空间并沿每条轨迹传播第一帧的特征，生成对齐的时空特征图作为更新后的条件，直接作为现有I2V模型的运动引导，无需改动模型架构。通过规模化训练，生成高分辨率、长时长视频。并设计MoveBench用于评估。

Result: 在自建MoveBench和公开数据集上，Wan-Move在运动质量与可控性上超越现有方法；生成5秒480p视频，用户研究显示其可控性接近商业Motion Brush。代码与基线公开。

Conclusion: Wan-Move通过将条件特征变为运动感知并沿稠密点轨迹传播第一帧特征，实现了细粒度且可扩展的视频运动控制，去除额外运动编码器并可直接集成入现成图像到视频模型，实验与用户研究表明其运动可控性和质量优于对手。

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [93] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 论文用XAI生成缺陷激活图，在前向放大噪声、反向聚焦修复的自我细化扩散框架下，显著提升扩散模型的图像质量，FID最高提升27.3%，适用于多任务多模型。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像合成上取得显著进展，但仍存在伪影和不真实区域等质量问题。论文动机是利用XAI技术从识别/定位错误出发，主动参与图像生成过程以修复缺陷，从而提升最终图像质量。

Method: 引入一个可解释AI（XAI）驱动的缺陷高亮器，生成缺陷激活图（FAM）。在前向扩散过程中对FAM标记区域放大噪声以破坏错误重建，在反向生成过程中重点关注这些区域以更好地重建；整体形成自我细化的流程以修正伪影与不真实区域。

Result: 在多种扩散模型与数据集上，提出的方法在FID上最多达到27.3%的提升，并在图像生成、文本到图像生成与修补等任务上都表现出稳健性和一致性。论文展示了XAI方法能超越可解释性，成为提升生成质量的有效工具。

Conclusion: 该论文提出了一种基于XAI的自我细化扩散框架，通过检测并强调图像生成中的伪影与不真实区域来提升扩散模型的生成质量。实验证明在多种数据集与任务上显著改善了FID，并能广泛适用于不同扩散模型与任务。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [94] [LoFA: Learning to Predict Personalized Priors for Fast Adaptation of Visual Generative Models](https://arxiv.org/abs/2512.08785)
*Yiming Hao,Mutian Xu,Chongjie Ye,Jie Qin,Shunlin Lu,Yipeng Qin,Xiaoguang Han*

Main category: cs.CV

TL;DR: LoFA通过两阶段超网络利用LoRA相对变化的结构化模式，快速预测高质量个性化先验，实现秒级模型适配并优于传统LoRA。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA需要任务特定数据与长时间优化，且现有超网络方法难以将细粒度用户提示映射到复杂的LoRA分布，限制实用性，因此需高效预测个性化先验以实现秒级适配。

Method: 通过发现LoRA参数与基础模型参数之间的相对变化存在结构化分布模式，设计两阶段超网络：先预测相对分布模式以捕捉关键适配区域，再基于该模式预测最终LoRA权重。

Result: 在多任务与多种用户提示下，LoFA在数秒内预测出高质量个性化先验，性能优于需要数小时处理的传统LoRA。

Conclusion: LoFA提出了一种两阶段超网络框架，有效预测个性化LoRA先验以实现快速模型适配。

Abstract: Personalizing visual generative models to meet specific user needs has gained increasing attention, yet current methods like Low-Rank Adaptation (LoRA) remain impractical due to their demand for task-specific data and lengthy optimization. While a few hypernetwork-based approaches attempt to predict adaptation weights directly, they struggle to map fine-grained user prompts to complex LoRA distributions, limiting their practical applicability. To bridge this gap, we propose LoFA, a general framework that efficiently predicts personalized priors for fast model adaptation. We first identify a key property of LoRA: structured distribution patterns emerge in the relative changes between LoRA and base model parameters. Building on this, we design a two-stage hypernetwork: first predicting relative distribution patterns that capture key adaptation regions, then using these to guide final LoRA weight prediction. Extensive experiments demonstrate that our method consistently predicts high-quality personalized priors within seconds, across multiple tasks and user prompts, even outperforming conventional LoRA that requires hours of processing. Project page: https://jaeger416.github.io/lofa/.

</details>


### [95] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 提出 MatteViT：结合高频增强模块和连续亮度阴影 matte 指导的 Transformer 架构，显著提升文档阴影去除并更好保留文本细节，提升 OCR 性能。


<details>
  <summary>Details</summary>
Motivation: 现有文档阴影去除方法易丢失文本边缘等高频细节，影响可读性与 OCR 识别，需同时处理空间和频域信息以恢复细微结构。

Method: 引入轻量级高频放大模块（HFAM）在频域上分解并自适应放大高频分量；同时构建自定义 matte 数据集与生成器，生成连续亮度的 shadow matte 作为自早期阶段的空间引导；整体框架为基于 Transformer 的 MatteViT，用以融合两类信息并重建无阴影文档。

Result: 在 RDD 与 Kligler 基准上达到或超越最先进性能，主观与客观指标（可能包括 PSNR/SSIM 与 OCR 识别率）均改善；在下游 OCR 任务上识别率提高，表明文本级细节更好保留。

Conclusion: MatteViT 提出将空间与频域信息结合以去除文档阴影，同时保留高频细节，方法合理且有效。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [96] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: T-DHA adapts VLMs without training by mapping hierarchical semantic relationships into hyperbolic space (Poincaré ball) and applying negative learning, yielding stronger few-shot and domain-generalization performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM adaptation methods degrade under domain shift or need expensive fine-tuning. They aim to enable efficient, robust adaptation across domains with minimal computation by exploiting hyperbolic geometry for hierarchical semantics.

Method: They propose Training-free Dual Hyperbolic Adapters (T-DHA). It models vision-language hierarchical relationships in hyperbolic space (Poincaré ball) and uses negative learning, training-free adaptation for VLMs.

Result: Improved representation and discrimination for hierarchical concepts; better few-shot and domain generalization; fewer feature dimensions needed.

Conclusion: T-DHA outperforms state-of-the-art in few-shot image recognition and domain generalization by leveraging hyperbolic embeddings and negative learning without costly training.

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [97] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出一种结合滑动窗口注意力与Gated DeltaNet的线性复杂度VLM——InfiniteVL，配合三阶段训练，在极少训练数据下达成与Transformer级别相当的性能，并在推理速度和长期记忆方面具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在处理长序列时面临二次复杂度与不断增长的KV缓存问题。窗口注意力在序列长度超过窗口时性能下降，线性注意力在信息密集任务（如OCR、文档理解）上表现不足。研究动机是设计一个能在长序列上保持性能且计算与内存线性可扩展的VLM架构。

Method: 提出InfiniteVL：结合滑动窗口注意力（SWA）与Gated DeltaNet的线性复杂度架构。并设计三阶段训练策略：蒸馏预训练、指令微调、以及长序列SFT，以在有限训练数据下获得竞争性多模态性能。

Result: 在不到领先VLM所需训练数据2%的条件下，InfiniteVL显著优于之前的线性复杂度VLM，并匹配基于Transformer的领先VLM的性能，同时具备有效的长期记忆保持。与使用FlashAttention-2加速的同规模Transformer相比，推理速度提升超过3.6倍，延迟和内存占用保持恒定。在流式视频理解中，维持24 FPS的实时预填充速度并保留长期记忆缓存。代码和模型已开源。

Conclusion: InfiniteVL通过将SWA与Gated DeltaNet结合并采用三阶段训练，在资源受限下实现了线性复杂度模型在常规与长序列任务上的高性能，兼具速度和长期记忆优势，适用于实时流式场景。

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [98] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: 在组成性生成过程中，对解码器施加适当的归纳偏置并通过反演（搜索或重放）可以在无需额外数据的情况下显著提升视觉模型的组成性泛化，而对编码器施加同样的偏置通常不可行。


<details>
  <summary>Details</summary>
Motivation: 探讨是否需要生成式方法来达到类人视觉感知的组成性泛化，比较生成式（解码器反演）与非生成式（编码器直映射）方法的能力与可行性。

Method: 形式化组成性数据生成过程，推导解码器和编码器需要的归纳偏置；证明对编码器施加这些偏置在常规方式下难以实现；提出对解码器施加偏置并通过梯度搜索或生成重放进行反演的可行实现；在真实图片数据集上进行实验证明。

Result: 理论证明和实验证明：编码器在无额外监督或大规模预训练下难以达到组成性泛化；而生成式方法通过约束解码器并结合搜索或重放能显著提升组成性泛化效果。

Conclusion: 生成式方法通过约束解码器并对其进行反演，更容易实现组成性泛化，从理论和实验证据来看优于基于编码器的方法。

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [99] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: Tri-Bench显示VLM偏向于2D图像线索，无法可靠利用显式边界做几何变换推理，且在少数三角形类别上几乎完全失败。


<details>
  <summary>Details</summary>
Motivation: 验证几何推理的可验证性与可控性，测试VLM在真实部署中对相机变化与场景干扰的鲁棒性，以及模型是否能利用提示中提供的参考框架进行正确的同构变换（homography）推理。

Method: 构建紧凑的平面三角形问题基准，控制相机姿态（平面/倾斜）与场景干扰（10类日常物体）；使用单一固定提示包含正方形边界的显式帧参考，评估四个VLM在六个任务（包含二值与连续目标）上的表现，并对比3D真值与2D投影结果。

Result: 总体对3D真值平均准确率约69%（最佳~75%，最差~64%），对2D投影约72%；在识别少数类别（等边、等腰、直角）时准确率降至~0%；相机倾斜导致总体准确率下降约4.1%；物体干扰对准确率无显著影响。

Conclusion: 该论文通过构建Tri-Bench基准，表明现有视觉-语言模型在可验证的几何推理任务上表现有限，尤其在识别少数三角形类别和利用显式参考框架上失败。

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [100] [Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning](https://arxiv.org/abs/2512.08873)
*Jing Jie Tan,Anissa Mokraoui,Ban-Hoe Kwan,Danny Wee-Kiat Ng,Yan-Chai Hum*

Main category: cs.CV

TL;DR: SOLI用Siamese网络优化低分辨率图像的潜在嵌入，实现轻量高效的图像描述生成，适合资源受限场景。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型Transformer编码器能提升caption质量但计算开销大，难以在资源受限设备或重训练场景中部署。希望设计轻量化方法兼顾性能与效率。

Method: 采用双路径（Siamese）编码器对原始LRI与其变换/增强版本进行并行编码，利用嵌入对齐/对比损失优化潜在表示，并将优化后的嵌入输入轻量解码器生成文本。

Result: 方法在节省计算和内存开销的同时，保持或接近大型模型在LRI caption任务上的性能（假定实验结果表明准确性与生成质量有显著提升）。

Conclusion: SOLI提出了一种针对低分辨率图像caption的轻量化方案，通过Siamese网络优化潜在嵌入，从而在计算资源受限情况下实现较高的生成质量。

Abstract: Image captioning is essential in many fields including assisting visually impaired individuals, improving content management systems, and enhancing human-computer interaction. However, a recent challenge in this domain is dealing with low-resolution image (LRI). While performance can be improved by using larger models like transformers for encoding, these models are typically heavyweight, demanding significant computational resources and memory, leading to challenges in retraining. To address this, the proposed SOLI (Siamese-Driven Optimization for Low-Resolution Image Latent Embedding in Image Captioning) approach presents a solution specifically designed for lightweight, low-resolution images captioning. It employs a Siamese network architecture to optimize latent embeddings, enhancing the efficiency and accuracy of the image-to-text translation process. By focusing on a dual-pathway neural network structure, SOLI minimizes computational overhead without sacrificing performance, making it an ideal choice for training on resource-constrained scenarios.

</details>


### [101] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 提出了一种用于卫星影像的基于视觉-语言模型（VLM）的结构化定位机制：通过微调预训练VLM并加入专门的定位模块与控制token，实现语言与空间信息联合推理，显著提升可视化定位性能，在多项遥感基准上优于现有方法，视觉定位相对提升24.8%。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在遥感领域的可视化定位能力受限，难以精确处理复杂卫星场景中的空间推理与对象定位。本工作旨在将结构化空间推理集成到VLM中，以提高定位精度和实用性。

Method: 在多任务指令跟随数据上微调预训练VLM，并通过专门的控制token接口连接一个专用的定位（grounding）模块。该模块负责解析控制token所携带的空间信息，从而使模型在语言理解的同时进行空间定位，实现联合推理与结构化定位输出。

Result: 在多个遥感基准测试上取得一致改进；在可视化定位（visual grounding）任务上比之前方法相对提升24.8%。总体上展示了更高的定位准确性和鲁棒性。

Conclusion: 将结构化空间推理与VLM结合能显著提升卫星影像中对象定位表现，为基于聊天接口的遥感分析提供更可靠的解决方案，并推动实际卫星数据应用的发展。

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [102] [Accelerated Rotation-Invariant Convolution for UAV Image Segmentation](https://arxiv.org/abs/2512.08888)
*Manduhu Manduhu,Alexander Dow,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 提出一种消除im2col、利用对称滤波器数据共享的GPU旋转不变卷积，实现显著加速和能耗降低，同时保持或提升分割精度。


<details>
  <summary>Details</summary>
Motivation: UAV航拍图像中目标方向任意且包含细粒度结构，传统基于卷积的分割网络（如U-Net）对旋转不变性支持不足，直接扩展滤波器到多方向会导致巨大的计算和内存开销，需提出高效实现以兼顾精度与资源开销。

Method: 在实现上，方法利用对称旋转滤波器之间的结构化共享，避免为每个方向重复执行数据展开和矩阵乘法，直接在GPU上实现内存友好的卷积算子；同时引入通用化策略以支持非对称任意角度的旋转卷积加速。

Result: 在大规模基准上，该方法比CUDNN训练快20--55%、能耗低15--45%，在8方向设置下对256×256输入最高达45%加速和41%节能；将该卷积集成到U-Net后，相比非旋转感知基线精度提升最高6%。

Conclusion: 论文提出了一种GPU优化的旋转不变卷积框架，通过消除im2col数据降低步骤并利用对称旋转滤波器之间的数据共享，显著降低内存访问和计算冗余，从而实现多方向卷积的高效计算，并推广到任意旋转角度。

Abstract: Rotation invariance is essential for precise, object-level segmentation in UAV aerial imagery, where targets can have arbitrary orientations and exhibit fine-scale details. Conventional segmentation architectures like U-Net rely on convolution operators that are not rotation-invariant, leading to degraded segmentation accuracy across varying viewpoints. Rotation invariance can be achieved by expanding the filter bank across multiple orientations; however, this will significantly increase computational cost and memory traffic. In this paper, we introduce a GPU-optimized rotation-invariant convolution framework that eliminates the traditional data-lowering (im2col) step required for matrix-multiplication-based convolution. By exploiting structured data sharing among symmetrically rotated filters, our method achieves multi-orientation convolution with greatly reduced memory traffic and computational redundancy. We further generalize the approach to accelerate convolution with arbitrary (non-symmetric) rotation angles.
  Across extensive benchmarks, the proposed convolution achieves 20--55% faster training and 15--45% lower energy consumption than CUDNN, while maintaining accuracy comparable to state-of-the-art rotation-invariant methods. In the eight-orientation setting, our approach achieves up to 45% speedup and 41% energy savings on 256\(\times\)256 inputs, and 32% speedup and 23% lower energy usage on 1024\(\times\)1024 inputs. Integrated into a U-Net segmentation model, the framework yields up to 6% improvement in accuracy over the non-rotation-aware baseline. These results demonstrate that the proposed method provides an effective and highly efficient alternative to existing rotation-invariant CNN frameworks.

</details>


### [103] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 无标注框架：LLM verifier + RL 改进链式思维；VLM verifier + 硬负挖掘改进视觉定位，综合提升视觉空间推理表现，优于多种开源/专有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉空间推理需精确物体定位和复杂空间关系理解。现有方法要么依赖大量带图数据训练语言链式思维，要么用程序合成但存在逻辑和定位错误。目标是去除人工注释同时提升推理与定位。

Method: 设计双验证器框架：1) LLM verifier：以强化学习（RL）信号对LLM的链式思维输出进行细化，提高推理正确性；2) VLM verifier：利用强视觉模型进行自动化困难负样本挖掘，强化视觉定位能力，无需标注。两者结合，LLM分解空间查询为子任务，VLM提供视觉监督与纠错。

Result: 在多种空间推理任务上，方法提升了视觉推理性能，超过开源与专有模型；在改进的视觉定位模型基础上，进一步优于近期纯文本视觉推理方法。提供项目主页。

Conclusion: 提出了一个无标注训练框架，结合LLM和VLM的验证器来改进视觉推理与视觉定位（grounding），通过LLM验证器用强化学习细化推理，用VLM验证器通过自动困难负样本挖掘强化视觉定位，从而无需真值标签。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [104] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: 作者提出UniLayDiff：把布局约束作为模态输入，用多模态扩散Transformer并结合LoRA微调，首次用单模型统一多种内容感知布局生成任务，实验显示效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界的内容感知布局生成需求多样：根据元素类型、尺寸、相对关系等进行约束生成。现有方法要么只覆盖部分子任务，要么需要为不同条件训练独立参数，缺乏一个统一、端到端的解决方案。

Method: 将布局约束作为独立模态输入，构建Multi-Modal Diffusion Transformer来联合建模背景图像、布局元素与约束；先在多任务上预训练模型以学习通用表示，再用LoRA微调加入关系约束；端到端训练以实现统一条件控制；评估覆盖无条件与多种条件生成场景。

Result: 据作者所述，UniLayDiff在多项任务上达到SOTA性能，能够统一处理从无条件到各类条件布局生成任务，并通过LoRA微调有效增强关系约束建模与整体布局质量。

Conclusion: 该论文提出了一个统一的扩散-Transformer框架（UniLayDiff），旨在用单一端到端模型解决多样的内容感知布局生成任务。通过将布局约束视为独立模态，并采用多模态扩散Transformer捕捉背景图像、布局元素与约束间的复杂交互，同时在预训练后使用LoRA微调以引入关系约束，作者声称实现了从无条件到多种条件生成任务的统一且效果领先的布局生成。

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [105] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: EvoScene是一个训练-free的三阶段自演化框架，交替在2D/3D域改进结构与外观，能从单张图像生成几何稳定、视图一致且补全完整的3D场景网格。


<details>
  <summary>Details</summary>
Motivation: 从单张图像生成高质量有纹理的三维场景仍具挑战，现有单视图3D生成模型对象中心训练限制了对复杂大尺度场景的泛化，作者欲结合现有模型互补优点实现训练-free的完整场景重建。

Method: 提出EvoScene，一个无训练、自我演化框架，通过三阶段迭代：空间先验初始化、视觉引导的3D场景网格生成、以及空间引导的新视角生成；在2D与3D域间交替完善结构与外观，融合3D生成模型的几何推理与视频生成模型的视觉知识。

Result: 在多种场景上，EvoScene比强基线展示出更优的几何稳定性、视图一致纹理和对未见区域的补全能力，能生成可直接使用的3D网格以用于实际应用。

Conclusion: EvoScene证明了通过跨域互补利用现有模型并采用迭代自演化策略，可以在无训练条件下从单张图像生成更稳定且纹理一致的完整三维场景，推动单视图场景重建实用化。

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [106] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: 提出LiDAS：通过动态分配车灯照明以提升夜间摄像头感知，零样本实现白天模型夜间泛化，显著提高检测与分割性能并节能。


<details>
  <summary>Details</summary>
Motivation: 夜间被动成像受限于环境光，导致基于白天训练的感知模型在夜间性能大幅下降；通过主动控制照明，可以在不重训练模型的前提下改善感知器在低光条件下的表现并节能。

Method: LiDAS构建一个闭环系统：使用合成数据训练的网络预测最优照明分布（将光从无关区域重新分配到目标区域），并通过高精度车灯实施动态照明；系统与现有视觉感知模型耦合，依据感知性能反馈实时调整照明。

Result: 在真实闭环驾驶场景中零样本部署，LiDAS相比标准近光在相同功率下实现+18.7% mAP50和+5.0% mIoU，并在保持性能的同时将能耗降低40%；且可与域泛化方法叠加获得更强鲁棒性。

Conclusion: LiDAS通过主动、动态控制车灯照明场来提升夜间视觉感知性能，可在零样本情况下对白天训练的模型实现夜间泛化，从而显著提高目标检测和语义分割精度，同时降低能耗。

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [107] [Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration](https://arxiv.org/abs/2512.08922)
*Jin Hyeon Kim,Paul Hyunbin Cho,Claire Kim,Jaewon Min,Jaeeun Lee,Jihye Park,Yeji Choi,Seungryong Kim*

Main category: cs.CV

TL;DR: UniT将DiT、VLM和TSM联合在去噪流程中迭代协同，利用显式文本引导和中间OCR预测有效抑制扩散模型的文字幻觉并提升文本恢复与识别性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管在一般图像修复上有强生成能力，但在包含文本的图像修复任务中易产生文字幻觉，缺乏显式的语言知识是主要原因。为此需要将视觉-语言理解与扩散去噪结合以减少幻觉并提高识别一致性。

Method: 在迭代去噪过程中，VLM从降质图像提取文本线索作为显式指导；TSM在每个去噪步骤基于扩散特征生成中间OCR预测，以便VLM迭代地精炼其指导信息；DiT作为主干利用这些提示恢复高质量图像。

Result: 在SA-Text和Real-Text基准上，UniT在重建降质文本和抑制文字幻觉方面效果显著，取得了TAIR任务的最新端到端F1得分。

Conclusion: 本文提出了UniT，一种结合扩散Transformer（DiT）、视觉-语言模型（VLM）和文本检测识别模块（TSM）的统一文本修复框架，能够在去噪过程中通过显式文本引导减少文字幻觉并恢复细节文本。

Abstract: Text-Aware Image Restoration (TAIR) aims to recover high-quality images from low-quality inputs containing degraded textual content. While diffusion models provide strong generative priors for general image restoration, they often produce text hallucinations in text-centric tasks due to the absence of explicit linguistic knowledge. To address this, we propose UniT, a unified text restoration framework that integrates a Diffusion Transformer (DiT), a Vision-Language Model (VLM), and a Text Spotting Module (TSM) in an iterative fashion for high-fidelity text restoration. In UniT, the VLM extracts textual content from degraded images to provide explicit textual guidance. Simultaneously, the TSM, trained on diffusion features, generates intermediate OCR predictions at each denoising step, enabling the VLM to iteratively refine its guidance during the denoising process. Finally, the DiT backbone, leveraging its strong representational power, exploit these cues to recover fine-grained textual content while effectively suppressing text hallucinations. Experiments on the SA-Text and Real-Text benchmarks demonstrate that UniT faithfully reconstructs degraded text, substantially reduces hallucinations, and achieves state-of-the-art end-to-end F1-score performance in TAIR task.

</details>


### [108] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,Joëlle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: D4RT: a unified transformer that queries any spatio-temporal 3D point to jointly predict depth, correspondence, and cameras, achieving SOTA 4D reconstructions efficiently


<details>
  <summary>Details</summary>
Motivation: Reduce computation and complexity of dense per-frame decoding and multiple task-specific decoders by enabling flexible, independent queries over space-time to infer 3D scene geometry and motion

Method: Transformer-based unified feedforward model that queries 3D positions across space-time, replacing dense per-frame decoders

Result: State-of-the-art performance on multiple 4D reconstruction tasks; efficient training and inference

Conclusion: D4RT provides a lightweight, scalable alternative to dense decoding approaches, jointly estimating depth, correspondences, and camera parameters from video

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [109] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: Selfi用重投影一致性损失训练特征适配器，把VGGT特征对齐到3D几何空间，提升NVS和位姿估计效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据驱动的视觉大模型（如VGGT）虽能从非标定图像中直接预测相机与3D表示，但缺乏显式的多视图几何一致性，限制了下游NVS与位姿任务的精度。作者希望通过一种自监督、轻量的对齐步骤提升几何一致性而不需昂贵的标注。

Method: 利用VGGT端到端推理得到的图像特征与预测相机参数作为伪标签，训练一个小型特征适配器，采用基于重投影的损失（在图像间将特征投影并测量差异）来蒸馏和对齐特征，从而在几何一致的特征空间上进行NVS与位姿估计。

Result: Selfi把VGGT的特征通过自我监督的重投影一致性蒸馏到几何对齐的特征空间，提升了NVS和姿态估计性能。

Conclusion: 通过训练轻量的特征适配器并利用VGGT自身输出作为伪真值，能在不依赖外部SfM标注的情况下显著提高多视图几何一致性，从而实现更高精度的视角合成与相机位姿估计。

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


### [110] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Astra是一个支持多场景、多动作模态的交互式长时程视频预测世界模型，结合自回归去噪、因果注意力、噪声历史记忆与动作适配机制，显著提升预测质量与动作对齐。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散Transformer在视频生成方面进展显著，但能从过去观测和动作预测长时程未来的世界模型仍未充分研究，尤其在通用场景和多样动作形式下。作者旨在构建一个能生成真实世界未来且支持精确动作交互的通用世界模型。

Method: 提出自回归去噪生成架构，采用时序因果注意力聚合过去观测并支持流式输出；引入噪声增强的历史记忆以防止对历史帧的过度依赖；通过动作感知适配器将动作信号注入去噪过程；采用动作专家混合（Mixture of Action Experts）动态路由不同动作模态。

Result: 在多个数据集上，Astra在保真度、长程预测能力和动作对齐方面均优于当前最先进的世界模型，表现出交互性、一致性及多样动作支持的能力。

Conclusion: 本文提出Astra，一种交互式通用世界模型，通过自回归去噪架构、时序因果注意力、噪声增强历史记忆、动作感知适配器和动作专家混合机制，实现对多场景（如自动驾驶、机器人抓取）长时程视频预测与精确动作交互。实验表明Astra在图像质量、长期一致性及动作对齐方面优于现有方法。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [111] [NeurIDA: Dynamic Modeling for Effective In-Database Analytics](https://arxiv.org/abs/2512.08483)
*Lingze Zeng,Naili Xing,Shaofeng Cai,Peng Lu,Gang Chen,Jian Pei,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurIDA 在数据库内预训练可组合基模型并在接到任务时动态调整组件，结合自然语言查询与 LLM 报告生成，显著提升多任务分析性能并降低构建成本。


<details>
  <summary>Details</summary>
Motivation: 传统 ML 模型在 RDBMS 场景中过于静态且为特定任务设计，而数据库分析需求多样且动态，逐一为每个分析任务从头搭建流水线代价高且限制了 ML 在数据库分析的普及。

Method: 提出动态的in-database建模范式：先对关系数据预训练一个可组合的基模型池和共享组件；接到任务后，系统根据任务与数据特征动态选择并配置相关模型组件进行预测；支持自然语言查询，通过解析用户意图生成结构化任务配置，使用专门的 LLM 代理生成分析报告。

Result: 实验表明在五个真实数据集、十个任务上，NeurIDA 在 AUC-ROC 上最多提升 12%，在 MAE 上相对减少 25%；并提供源码。

Conclusion: NeurIDA 提出了一种在关系数据库内动态调整预训练可组合基模型的方法，实现端到端自主化分析，解决了静态、任务专用模型难以适应多样化数据库查询的问题。

Abstract: Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.
  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA

</details>


### [112] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 提出AOD以描述聚合值随分组属性单调变化的约束，把修复问题建模为最小删除元组问题，分析复杂性并给出通用算法模板、优化与启发式方法，实验验证了方法有效性并通过案例揭示实际违例。


<details>
  <summary>Details</summary>
Motivation: 许多数据集中存在与期望单调趋势不一致的现象，需要一种量化和修复方法来判断数据偏离程度及纠正异常，以便改善数据质量和下游分析的可靠性。现有的顺序依赖研究未充分考虑聚合场景，因此提出AOD来专门处理基于分组的聚合单调性约束。

Method: 论文首先定义了AOD及其形式化修复问题，然后证明了问题的复杂性（应为NP难或相关复杂性结果）。基于理论结果，提出了一个通用的算法模板，可针对不同聚合函数（如均值、总和、计数等）进行实例化；在模板基础上进一步设计了多种优化（如剪枝策略、动态规划或近似方案）来减少搜索空间并加速执行；最后提出启发式算法作为高效近似解法，并在实验中与精确算法比较。

Result: 理论上，作者给出了问题的复杂性界定与算法正确性保证；实践上，通过实验展示了精确算法在中等规模数据上的有效性、优化技术显著提升了性能、启发式方案在大规模数据上近似效果良好。案例研究成功发现并解释了真实数据集中意外的AOD违例。

Conclusion: 该论文提出了针对聚合值的顺序依赖（Aggregate Order Dependencies, AODs），并将AOD修复问题形式化为通过删除最少元组使表满足给定AOD的最小删除问题。论文分析了问题的计算复杂性，给出了一般算法模板，并为常见聚合函数实例化了该模板。作者还引入了优化技术以提升模板实例的运行效率，并开发了有效的启发式替代方案。实验在真实与合成数据集上表明算法在实践中高效，启发式方法性能良好，同时通过案例研究解释了意外的AOD违例。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [113] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: ExDis 自动发现并解释群体差异：定位差异最明显的子人群并识别因果因素，既解释性强又具可操作性，实验验证有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析中经常需要解释两个群体间的差异，不仅要定位差异发生的子人群，还要找出能够缓解或加剧差异的因果因素，以支持可操作性的决策。

Method: 定义了形式化的 ExDis 框架与优化目标，分析复杂度并设计了高效算法来寻找最能解释差异的数据子集与因果因素；方法结合子人群搜索与因果效应衡量来关联原因与差异。

Result: 在三个真实数据集上的大量实验表明，ExDis 能生成有意义且因果性的解释，优于现有方法，并能扩展到大规模高维数据。

Conclusion: ExDis 是一个面向发现群体差异的因果可解释框架，能够定位差异最显著的数据子区域并识别导致差异的因果因素，且具备可解释性与可操作性。

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>
