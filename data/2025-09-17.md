<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 108]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Artificial Intelligence in Breast Cancer Care: Transforming Preoperative Planning and Patient Education with 3D Reconstruction](https://arxiv.org/abs/2509.12242)
*Mustafa Khanbhai,Giulia Di Nardo,Jun Ma,Vivienne Freitas,Caterina Masino,Ali Dolatabadi,Zhaoxun "Lorenz" Liu,Wey Leong,Wagner H. Souza,Amin Madani*

Main category: cs.CV

TL;DR: U-Mamba结合人-机交互实现了乳腺MRI的高精度分割与3D重建，展示出良好泛化性，对临床规划和患者教育具有积极影响。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型在不同数据集上的泛化能力有限，影响术前三维重建与规划。旨在通过人-机协同与新模型架构提升分割精度和跨场景泛化性，从而改善临床决策与患者沟通。

Method: 回顾性采集120例乳腺MRI（T1加权及动态对比增强），进行匿名化及人工分割；配准并分割全乳腺、纤维腺体组织和肿瘤；使用ITK-SNAP实现三维可视化。引入人-机交互流程，基于U-Mamba模型进行自动分割并用人工微调，使用Dice系数评估性能；通过临床医师与患者访谈评估临床相关性。

Result: 在T1加权图像上，U-Mamba对全乳腺、纤维腺体组织和肿瘤的平均DSC分别为0.97±0.013、0.96±0.024和0.82±0.12；模型能生成准确的三维重建并可视化复杂解剖结构。临床访谈显示该方法有助于术前规划、术中导航、决策支持，并提升患者理解与参与。

Conclusion: 本研究提出的人-机交互机器学习方法（U-Mamba）在多样化乳腺MRI数据上的分割与三维重建表现优异，具有良好的泛化能力，可提升术前规划、术中导航和患者教育。

Abstract: Effective preoperative planning requires accurate algorithms for segmenting
anatomical structures across diverse datasets, but traditional models struggle
with generalization. This study presents a novel machine learning methodology
to improve algorithm generalization for 3D anatomical reconstruction beyond
breast cancer applications. We processed 120 retrospective breast MRIs (January
2018-June 2023) through three phases: anonymization and manual segmentation of
T1-weighted and dynamic contrast-enhanced sequences; co-registration and
segmentation of whole breast, fibroglandular tissue, and tumors; and 3D
visualization using ITK-SNAP. A human-in-the-loop approach refined
segmentations using U-Mamba, designed to generalize across imaging scenarios.
Dice similarity coefficient assessed overlap between automated segmentation and
ground truth. Clinical relevance was evaluated through clinician and patient
interviews. U-Mamba showed strong performance with DSC values of 0.97
($\pm$0.013) for whole organs, 0.96 ($\pm$0.024) for fibroglandular tissue, and
0.82 ($\pm$0.12) for tumors on T1-weighted images. The model generated accurate
3D reconstructions enabling visualization of complex anatomical features.
Clinician interviews indicated improved planning, intraoperative navigation,
and decision support. Integration of 3D visualization enhanced patient
education, communication, and understanding. This human-in-the-loop machine
learning approach successfully generalizes algorithms for 3D reconstruction and
anatomical segmentation across patient datasets, offering enhanced
visualization for clinicians, improved preoperative planning, and more
effective patient education, facilitating shared decision-making and empowering
informed patient choices across medical applications.

</details>


### [2] [RU-Net for Automatic Characterization of TRISO Fuel Cross Sections](https://arxiv.org/abs/2509.12244)
*Lu Cai,Fei Xu,Min Xian,Yalei Tang,Shoukun Sun,John Stempien*

Main category: cs.CV

TL;DR: 作者用2000+张注释TRISO截面图训练并比较多种CNN分割模型，提出的RU-Net在IoU上表现最好，能加速和客观化TRISO显微分析。


<details>
  <summary>Details</summary>
Motivation: TRISO燃粒在服役中会发生内核膨胀和缓冲层致密化等形貌变化，传统人工逐粒显微观察耗时且主观，因而需要自动且客观的图像分割工具来加速后照射分析并获得统计性结论。

Method: 构建了包含2000余张经注释的受照TRISO截面显微图像数据集，比较了RU-Net、U-Net、ResNet和Attention U-Net等网络在分割不同TRISO层时的表现；以IoU为主要评估指标来量化分割精度。

Result: 在构建的数据集上，RU-Net在IoU指标上优于U-Net、ResNet和Attention U-Net，表明其在分割TRISO各层方面具有更高的准确性和可靠性。使用这些CNN可以显著减少人工劳动并提高分割结果的一致性。

Conclusion: 本研究证明基于卷积神经网络（尤其是作者提出的RU-Net）可以高效且客观地对显微截面图像中的TRISO燃粒层进行自动分割，从而加速统计分析并减少人工主观性。

Abstract: During irradiation, phenomena such as kernel swelling and buffer
densification may impact the performance of tristructural isotropic (TRISO)
particle fuel. Post-irradiation microscopy is often used to identify these
irradiation-induced morphologic changes. However, each fuel compact generally
contains thousands of TRISO particles. Manually performing the work to get
statistical information on these phenomena is cumbersome and subjective. To
reduce the subjectivity inherent in that process and to accelerate data
analysis, we used convolutional neural networks (CNNs) to automatically segment
cross-sectional images of microscopic TRISO layers. CNNs are a class of
machine-learning algorithms specifically designed for processing structured
grid data. They have gained popularity in recent years due to their remarkable
performance in various computer vision tasks, including image classification,
object detection, and image segmentation. In this research, we generated a
large irradiated TRISO layer dataset with more than 2,000 microscopic images of
cross-sectional TRISO particles and the corresponding annotated images. Based
on these annotated images, we used different CNNs to automatically segment
different TRISO layers. These CNNs include RU-Net (developed in this study), as
well as three existing architectures: U-Net, Residual Network (ResNet), and
Attention U-Net. The preliminary results show that the model based on RU-Net
performs best in terms of Intersection over Union (IoU). Using CNN models, we
can expedite the analysis of TRISO particle cross sections, significantly
reducing the manual labor involved and improving the objectivity of the
segmentation results.

</details>


### [3] [Modular, On-Site Solutions with Lightweight Anomaly Detection for Sustainable Nutrient Management in Agriculture](https://arxiv.org/abs/2509.12247)
*Abigail R. Cohen,Yuming Sun,Zhihao Qin,Harsh S. Muriki,Zihao Xiao,Yeonju Lee,Matthew Housley,Andrew F. Sharkey,Rhuanito S. Ferrarezi,Jing Li,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 提出一套在能耗-精度之间可调的多光谱影像分层管道，AE用于早期异常检测，VI+RF与整图ViT用于状态估计，并对能量效率进行了量化比较，展示了在边缘农业诊断中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前营养管理与成像方法在实时性和能耗上存在冲突，难以在资源受限的现场部署；因此需要一个在精度与能效间可调的体系，支持早期报警与更详尽的状态估计。

Method: 基于多光谱成像，设计了三级管道：使用自编码器(AE)进行异常早期预警；针对状态估计比较了两种模块：基于植被指数(VI)的特征+随机森林(RF)与基于原始整图的深度学习模型(ViT)。并对不同方法进行了能量消耗的综合分析。实验采用氮营养亏缺试验（T1 100%、T2 50%、T3 25%肥力），评估检测性能与能耗权衡。

Result: AE实现了高能效的异常检测（移栽后9天对T3样本净检测率73%），总体能耗远低于因氮浪费产生的隐含能量。状态估计模块呈现精度-能耗权衡：ViT在磷和钙估计上优于RF（R2分别为0.61 vs 0.58, 0.48 vs 0.35），但能耗更高。

Conclusion: 该论文提出了一个分层、可扩展的管道，实现了在资源受限环境下的早期异常检测与植株状态估计，兼顾能效与精度，为边缘诊断和农业可持续性提供了实际可行性。

Abstract: Efficient nutrient management is critical for crop growth and sustainable
resource consumption (e.g., nitrogen, energy). Current approaches require
lengthy analyses, preventing real-time optimization; similarly, imaging
facilitates rapid phenotyping but can be computationally intensive, preventing
deployment under resource constraints. This study proposes a flexible, tiered
pipeline for anomaly detection and status estimation (fresh weight, dry mass,
and tissue nutrients), including a comprehensive energy analysis of approaches
that span the efficiency-accuracy spectrum. Using a nutrient depletion
experiment with three treatments (T1-100%, T2-50%, and T3-25% fertilizer
strength) and multispectral imaging (MSI), we developed a hierarchical pipeline
using an autoencoder (AE) for early warning. Further, we compared two status
estimation modules of different complexity for more detailed analysis:
vegetation index (VI) features with machine learning (Random Forest, RF) and
raw whole-image deep learning (Vision Transformer, ViT). Results demonstrated
high-efficiency anomaly detection (73% net detection of T3 samples 9 days after
transplanting) at substantially lower energy than embodied energy in wasted
nitrogen. The state estimation modules show trade-offs, with ViT outperforming
RF on phosphorus and calcium estimation (R2 0.61 vs. 0.58, 0.48 vs. 0.35) at
higher energy cost. With our modular pipeline, this work opens opportunities
for edge diagnostics and practical opportunities for agricultural
sustainability.

</details>


### [4] [Humor in Pixels: Benchmarking Large Multimodal Models Understanding of Online Comics](https://arxiv.org/abs/2509.12248)
*Yuriel Ryan,Rui Yang Tan,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: PixelHumor为多面板漫画的多模态幽默与叙事理解提供基准；实验发现现有LMM表现远低于人类，强调需改进多模态融合与序列推理。


<details>
  <summary>Details</summary>
Motivation: 幽默理解是社会智能的重要组成部分，但现有LMM在结合视觉与文本线索以把握连贯叙事和幽默方面能力有限，需建立基准促进改进。

Method: 构建2,800条多面板漫画注释数据集，设计评测任务（如面板排序、幽默理解），并用多种先进LMM评估性能。

Result: 顶级模型在面板排序任务仅达61%准确率，远低于人类水平，显示模型在整合视觉与文本以理解叙事与幽默方面存在明显差距。

Conclusion: PixelHumor表明当前LMM在多模态幽默与叙事理解上存在显著不足，需要改进多模态融合与序列推理能力。

Abstract: Understanding humor is a core aspect of social intelligence, yet it remains a
significant challenge for Large Multimodal Models (LMMs). We introduce
PixelHumor, a benchmark dataset of 2,800 annotated multi-panel comics designed
to evaluate LMMs' ability to interpret multimodal humor and recognize narrative
sequences. Experiments with state-of-the-art LMMs reveal substantial gaps: for
instance, top models achieve only 61% accuracy in panel sequencing, far below
human performance. This underscores critical limitations in current models'
integration of visual and textual cues for coherent narrative and humor
understanding. By providing a rigorous framework for evaluating multimodal
contextual and narrative reasoning, PixelHumor aims to drive the development of
LMMs that better engage in natural, socially aware interactions.

</details>


### [5] [OnlineHOI: Towards Online Human-Object Interaction Generation and Perception](https://arxiv.org/abs/2509.12250)
*Yihong Ji,Yunze Liu,Yiyao Zhuo,Weijiang Yu,Fei Ma,Joshua Huang,Fei Yu*

Main category: cs.CV

TL;DR: 文章提出在线HOI生成与感知新任务并提出OnlineHOI框架（Mamba+Memory），在多个在线HOI基准上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 现实场景中HOI信息是在线到达的，离线方法在此类场景下表现不佳，因此需要研究在线HOI生成与感知任务。

Method: 基于Mamba框架构建网络，加入记忆模块以在每个时间步高效整合历史信息，实现流式数据建模与持续生成/感知。

Result: 在Core4D和OAKIN-K2的在线生成任务以及HOI4D的在线感知任务上达到了最新的实验绩效（SOTA）。

Conclusion: 本文提出在线HOI（Human-Object Interaction）生成与感知任务，并设计了基于Mamba且带记忆机制的OnlineHOI框架，在在线情境下显著优于传统离线方法。

Abstract: The perception and generation of Human-Object Interaction (HOI) are crucial
for fields such as robotics, AR/VR, and human behavior understanding. However,
current approaches model this task in an offline setting, where information at
each time step can be drawn from the entire interaction sequence. In contrast,
in real-world scenarios, the information available at each time step comes only
from the current moment and historical data, i.e., an online setting. We find
that offline methods perform poorly in an online context. Based on this
observation, we propose two new tasks: Online HOI Generation and Perception. To
address this task, we introduce the OnlineHOI framework, a network architecture
based on the Mamba framework that employs a memory mechanism. By leveraging
Mamba's powerful modeling capabilities for streaming data and the Memory
mechanism's efficient integration of historical information, we achieve
state-of-the-art results on the Core4D and OAKINK2 online generation tasks, as
well as the online HOI4D perception task.

</details>


### [6] [EfficientNet-Based Multi-Class Detection of Real, Deepfake, and Plastic Surgery Faces](https://arxiv.org/abs/2509.12258)
*Li Kun,Milena Radenkovic*

Main category: cs.CV

TL;DR: 深度学习催生的深度伪造技术能生成以假乱真的图像/视频，威胁隐私、名誉和国家安全，尤其可能被用于选举干预，论文呼吁加强检测、法规与公众防护。


<details>
  <summary>Details</summary>
Motivation: 动机是警示并研究深度伪造技术的负面社会影响，探讨其对隐私、公众信任、政治稳定与国家安全的威胁，促使研究者和政策制定者关注检测、监管与伦理对策。

Method: 论文主要通过综述分析的方法，回顾深度学习和深度伪造技术的发展、面部替换及图像/视频生成技术的工作原理，并讨论其在现实世界中的滥用案例与影响，可能辅以案例研究与风险评估。

Result: 文章得出结论：深度伪造技术带来显著社会风险，现有技术可能难以完全检测与防范，亟需完善检测工具、法律监管和公众教育以减少滥用，并提出未来研究方向。

Conclusion: 该论文总结深度学习在产生深度伪造（deepfake）方面带来的社会风险，指出这种技术虽有广泛应用，但在隐私、名誉和国家安全等方面可能产生严重负面影响，并强调深度伪造对选举和公众舆论的潜在破坏性。

Abstract: Currently, deep learning has been utilised to tackle several difficulties in
our everyday lives. It not only exhibits progress in computer vision but also
constitutes the foundation for several revolutionary technologies. Nonetheless,
similar to all phenomena, the use of deep learning in diverse domains has
produced a multifaceted interaction of advantages and disadvantages for human
society. Deepfake technology has advanced, significantly impacting social life.
However, developments in this technology can affect privacy, the reputations of
prominent personalities, and national security via software development. It can
produce indistinguishable counterfeit photographs and films, potentially
impairing the functionality of facial recognition systems, so presenting a
significant risk.
  The improper application of deepfake technology produces several detrimental
effects on society. Face-swapping programs mislead users by altering persons'
appearances or expressions to fulfil particular aims or to appropriate personal
information. Deepfake technology permeates daily life through such techniques.
Certain individuals endeavour to sabotage election campaigns or subvert
prominent political figures by creating deceptive pictures to influence public
perception, causing significant harm to a nation's political and economic
structure.

</details>


### [7] [A Modern Look at Simplicity Bias in Image Classification Tasks](https://arxiv.org/abs/2509.12265)
*Xiaoguang Chang,Teng Wang,Changyin Sun*

Main category: cs.CV

TL;DR: 提出频率感知的简单性偏好度量并在CLIP模型上验证，发现SB对不同图像分类任务（如OOD泛化与对抗鲁棒性）影响不同，提示需根据任务特性调整模型归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 现有关于简单性偏好的研究多集中在小模型或合成任务，大型模型（如CLIP）上的SB测量困难且不清楚其对不同图像分类任务性能的影响，需提出更合适的度量并探索SB与任务表现的关系。

Method: 理论分析现有复杂度度量的局限性，提出频率感知的SB度量；在CLIP模型上通过两种SB调制方法验证该度量的有效性；在多种图像分类任务（零样本、微调等）上研究SB与性能的关系。

Result: 新提出的频率感知度量较传统度量更能区分CLIP模型在SB上的差异；在不同任务上观察到多样化关系——例如，较强的SB有助于OOD泛化但与对抗鲁棒性相关性较弱或负相关。

Conclusion: 本文提出并验证了一个频率感知的简单性偏好（SB）度量，用以更细粒度地刻画大型CLIP模型的简单性偏置；该度量在经过两种SB调制方法的模型上比以往度量更具信息性与一致性。实验还表明，不同任务对SB的需求不同：强烈的SB有利于OOD泛化但不利于对抗鲁棒性，强调将模型归纳偏置与目标任务特性对齐的重要性。

Abstract: The simplicity Bias (SB) of neural networks, i.e.\ their tendency to
represent simple functions, is a key factor in their generalization
capabilities. Recent studies show that an excessive SB may harm performance on
complex tasks, and the need for this bias varies across tasks. Many of these
studies focus on simple models or synthetic tasks. It remains challenging to
measure the SB in large models and little is known about the relevance of the
SB to various image classification tasks.
  In this paper, we investigate the relationship between the SB in CLIP models
and their performance across image classification tasks. First, we
theoretically analyze the potential limitation of existing measures of
complexity that have been used to characterize small models. To address this,
we propose a frequency-aware measure capturing finer-grained SB differences. We
validate this measure on CLIP models subjected to two recent SB-modulation
methods, demonstrating that it is more informative and consistent than previous
measures. Second, we examine the relation between the SB of those models and
their performance across a range of image classification tasks, including
zero-shot and fine-tuning settings. These experiments reveal a range of
behaviors. For example, a stronger SB correlates with a better performance on
OOD generalization than on adversarial robustness. These results highlight the
benefits of aligning a model's inductive biases with the characteristics of the
target task.

</details>


### [8] [GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions](https://arxiv.org/abs/2509.12277)
*Mehdi Yousefzadeh,Parsa Esfahanian,Sara Rashidifar,Hossein Salahshoor Gavalan,Negar Sadat Rafiee Tabatabaee,Saeid Gorgin,Dara Rahmati,Maryam Daneshpazhooh*

Main category: cs.CV

TL;DR: GraphDerm通过融合像素级标定、病灶几何与患者元数据构建人口图并用谱GNN分类，在ISIC基准上显著优于仅图像方法，且稀疏边策略便于高效部署。


<details>
  <summary>Details</summary>
Motivation: 当前许多皮肤镜AI仅用图像忽略患者元数据和物理尺度，后者对几何特征分析和分型有重要作用；将这些信息以图结构融合可能提高分类性能。

Method: 在ISIC 2018/2019数据上，合成带尺子的图像并标注精确掩码；用U-Net（SE-ResNet-18）分割病灶和标尺；用轻量1D-CNN基于标尺掩码的两点相关回归像素/毫米；从病灶掩码计算真实尺度描述符；节点特征由EfficientNet-B3提取，边根据元数据和几何相似性构建（全权重或阈值化）；用谱GNN做半监督节点分类，图外基线为纯图像ANN。

Result: 分割Dice分别为0.904和0.908；尺度回归MAE 1.5像素，RMSE 6.6；图模型AUC 0.9812，阈值化稀疏图（约25%边）AUC 0.9788，图像仅模型AUC 0.9440；多数类别AUC在0.97-0.99区间。

Conclusion: 将标定尺度、病灶几何和患者元数据统一到群体图上，能显著提升多类皮肤镜图像分类性能，稀疏图仍能保持接近最优的准确率。

Abstract: Introduction. Dermoscopy aids melanoma triage, yet image-only AI often
ignores patient metadata (age, sex, site) and the physical scale needed for
geometric analysis. We present GraphDerm, a population-graph framework that
fuses imaging, millimeter-scale calibration, and metadata for multiclass
dermoscopic classification, to the best of our knowledge the first ISIC-scale
application of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,
synthesize ruler-embedded images with exact masks, and train U-Nets
(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter are
regressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.
From lesion masks we compute real-scale descriptors (area, perimeter, radius of
gyration). Node features use EfficientNet-B3; edges encode metadata/geometry
similarity (fully weighted or thresholded). A spectral GNN performs
semi-supervised node classification; an image-only ANN is the baseline.
Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scale
regression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with a
thresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440
for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99
range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata in
a population graph yields substantial gains over image-only pipelines on
ISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficient
deployment. Scale-aware, graph-based AI is a promising direction for
dermoscopic decision support; future work will refine learned edge semantics
and evaluate on broader curated benchmarks.

</details>


### [9] [PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models](https://arxiv.org/abs/2509.12278)
*Wanru Zhuang,Wenbo Li,Zhibin Lan,Xu Han,Peng Li,Jinsong Su*

Main category: cs.CV

TL;DR: 提出位置感知TIMT（PATIMT）与基准PATIMTBench，含自适应OCR精炼流水线与1,200条人工测试样本，微调后紧凑型LVLMs在细粒度与布局保留翻译任务上表现优秀。


<details>
  <summary>Details</summary>
Motivation: 现有TIMT只关注对图像中所有文本的翻译，缺乏位置信息输出且场景覆盖有限；实际应用需要细粒度、保留布局的翻译以支持编辑与定位等下游任务。

Method: 构建PATIMTBench覆盖10个真实场景；提出自适应图像OCR精炼流水线，根据场景选择合适OCR并精炼富文本图像结果；人工构建并审核1,200条高质量测试实例；对LVLMs进行微调并评估两大子任务（区域级翻译与带定位的整图翻译）。

Result: 在PATIMTBench上，经过微调的紧凑型LVLMs在区域翻译与带定位的整图翻译两项任务上取得了SOTA表现；实验还显示训练数据具有良好可扩展性与泛化能力。

Conclusion: 该文将传统的文本图像翻译扩展为位置感知文本图像机器翻译（PATIMT），实现细粒度与布局保留的翻译，提出新的基准与数据处理流程，并证明了紧凑型大视觉语言模型在此任务上的有效性。

Abstract: Text Image Machine Translation (TIMT) aims to translate texts embedded within
an image into another language. Current TIMT studies primarily focus on
providing translations for all the text within an image, while neglecting to
provide bounding boxes and covering limited scenarios. In this work, we extend
traditional TIMT into position-aware TIMT (PATIMT), aiming to support
fine-grained and layoutpreserving translation, which holds great practical
value but remains largely unexplored. This task comprises two key sub-tasks:
regionspecific translation and full-image translation with grounding. To
support existing models on PATIMT and conduct fair evaluation, we construct the
PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world
scenarios. Specifically, we introduce an Adaptive Image OCR Refinement
Pipeline, which adaptively selects appropriate OCR tools based on scenario and
refines the results of text-rich images. To ensure evaluation reliability, we
further construct a test set, which contains 1,200 high-quality instances
manually annotated and reviewed by human experts. After fine-tuning on our
data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art
performance on both sub-tasks. Experimental results also highlight the
scalability and generalizability of our training data

</details>


### [10] [Domain Adaptive SAR Wake Detection: Leveraging Similarity Filtering and Memory Guidance](https://arxiv.org/abs/2509.12279)
*He Gao,Baoxiang Huang,Milena Radenkovic,Borui Li,Ge Chen*

Main category: cs.CV

TL;DR: 提出SimMemDA：结合风格迁移、实例级相似性筛选和置信度记忆库的伪标签校准，解决光学→SAR的跨模态尾流检测域适应问题，显著提升检测效果。


<details>
  <summary>Details</summary>
Motivation: SAR图像的特征抽象且噪声大，导致标注困难；而光学图像特征更明显但直接迁移到SAR会因模态差异性能下降，因此需要一种无监督的跨模态域自适应方法来提高SAR上的尾流检测效果。

Method: 方法包括：1) 使用WakeGAN将光学图像风格迁移到SAR域，生成伪SAR风格图像；2) 设计实例级特征相似性筛选机制，优先选择与目标域分布相似的源样本以减少负迁移；3) 引入带置信度的特征记忆库，并结合K近邻置信度加权融合策略动态校准目标域伪标签；4) 通过区域混合训练将源注释与校准后的目标伪标签结合以增强泛化。

Result: 实验表明SimMemDA在跨模态船舶尾流检测任务上提升了准确性和鲁棒性，验证了实例级相似性筛选和记忆引导伪标签校准的有效性与可行性。

Conclusion: 该文提出的SimMemDA方法通过风格迁移、实例级相似性筛选和记忆引导的伪标签校准，有效缓解了光学与SAR之间的域差异，提升了跨模态船舶尾流检测的准确性与鲁棒性。

Abstract: Synthetic Aperture Radar (SAR), with its all-weather and wide-area
observation capabilities, serves as a crucial tool for wake detection. However,
due to its complex imaging mechanism, wake features in SAR images often appear
abstract and noisy, posing challenges for accurate annotation. In contrast,
optical images provide more distinct visual cues, but models trained on optical
data suffer from performance degradation when applied to SAR images due to
domain shift. To address this cross-modal domain adaptation challenge, we
propose a Similarity-Guided and Memory-Guided Domain Adaptation (termed
SimMemDA) framework for unsupervised domain adaptive ship wake detection via
instance-level feature similarity filtering and feature memory guidance.
Specifically, to alleviate the visual discrepancy between optical and SAR
images, we first utilize WakeGAN to perform style transfer on optical images,
generating pseudo-images close to the SAR style. Then, instance-level feature
similarity filtering mechanism is designed to identify and prioritize source
samples with target-like distributions, minimizing negative transfer.
Meanwhile, a Feature-Confidence Memory Bank combined with a K-nearest neighbor
confidence-weighted fusion strategy is introduced to dynamically calibrate
pseudo-labels in the target domain, improving the reliability and stability of
pseudo-labels. Finally, the framework further enhances generalization through
region-mixed training, strategically combining source annotations with
calibrated target pseudo-labels. Experimental results demonstrate that the
proposed SimMemDA method can improve the accuracy and robustness of cross-modal
ship wake detection tasks, validating the effectiveness and feasibility of the
proposed method.

</details>


### [11] [Uncertainty-Aware Hourly Air Temperature Mapping at 2 km Resolution via Physics-Guided Deep Learning](https://arxiv.org/abs/2509.12329)
*Shengjie Kris Liu,Siqin Wang,Lu Zhang*

Main category: cs.CV

TL;DR: 提出Amplifier Air-Transformer，通过云修复的卫星地表温度并结合ERA5与地表属性，以深度学习生成不确定性可估的2 km小时级近地面气温，站点验证误差约1.93°C。


<details>
  <summary>Details</summary>
Motivation: 动机在于单一数据源（气象站或卫星）各有局限，缺乏在时空上无缝覆盖的高分辨率小时级近地面气温数据，故需融合多源数据并引入物理先验以生成连续高质量产品。

Method: 方法包括两步神经网络：第一步为重建GOES-16被云遮挡的地表温度，网络嵌入年周期编码、线性放大ERA5温度以获取细尺度信息，并用卷积层捕捉时空变异；第二步将重建的地表温度转换为空气温度，利用地表属性的潜在关系；并通过深度集成学习估计预测不确定性。

Result: 在2018-2024年美国本土的大规模数据集（777亿地表温度像素与1.55亿站点记录）上测试，站点验证的小时气温映射精度为1.93°C，并提供不确定性估计；研究结果和数据集已公开。

Conclusion: 该研究提出了一种名为Amplifier Air-Transformer的数据驱动且受物理约束的深度学习方法，能够在云遮挡情况下重建地表温度并将其转换为空气温度，生成美国本土2 km分辨率小时级近地面气温数据。

Abstract: Near-surface air temperature is a key physical property of the Earth's
surface. Although weather stations offer continuous monitoring and satellites
provide broad spatial coverage, no single data source offers seamless data in a
spatiotemporal fashion. Here, we propose a data-driven, physics-guided deep
learning approach to generate hourly air temperature data at 2 km resolution
over the contiguous United States. The approach, called Amplifier
Air-Transformer, first reconstructs GOES-16 surface temperature data obscured
by clouds. It does so through a neural network encoded with the annual
temperature cycle, incorporating a linear term to amplify ERA5 temperature
values at finer scales and convolutional layers to capture spatiotemporal
variations. Then, another neural network transforms the reconstructed surface
temperature into air temperature by leveraging its latent relationship with key
Earth surface properties. The approach is further enhanced with predictive
uncertainty estimation through deep ensemble learning to improve reliability.
The proposed approach is built and tested on 77.7 billion surface temperature
pixels and 155 million air temperature records from weather stations across the
contiguous United States (2018-2024), achieving hourly air temperature mapping
accuracy of 1.93 C in station-based validation. The proposed approach
streamlines surface temperature reconstruction and air temperature prediction,
and it can be extended to other satellite sources for seamless air temperature
monitoring at high spatiotemporal resolution. The generated data of this study
can be downloaded at https://doi.org/10.5281/zenodo.15252812, and the project
webpage can be found at https://skrisliu.com/HourlyAirTemp2kmUSA/.

</details>


### [12] [DS@GT AnimalCLEF: Triplet Learning over ViT Manifolds with Nearest Neighbor Classification for Animal Re-identification](https://arxiv.org/abs/2509.12353)
*Anthony Miyaguchi,Chandrasekaran Maruthaiyannan,Charles R. Clark*

Main category: cs.CV

TL;DR: Domain-specific backbones (MegaDescriptor) significantly outperform general-purpose ones (DINOv2) after post-hoc metric learning; general-purpose manifolds resist fine-grained reshaping, so domain-specific pretraining is crucial for animal re-ID.


<details>
  <summary>Details</summary>
Motivation: To investigate whether post-hoc metric learning can adapt general-purpose embeddings for animal re-identification and to assess importance of domain-specific pretraining for limited-data fine-grained re-ID tasks.

Method: Compared DINOv2 and MegaDescriptor backbones, applied a triplet-learning projection head, used K-NN classifier with thresholding for known/new ID detection, evaluated using BAKS and BAUS metrics, analyzed validation loss and qualitative embedding visualizations.

Result: Triplet head improved MegaDescriptor by +0.13 points on averaged BAKS/BAUS, but only +0.03 for DINOv2; general-purpose embeddings showed stagnant validation loss and were qualitatively harder to reshape; K-NN with thresholding effectively separated known vs new individuals when backbone was strong.

Conclusion: The paper concludes that post-hoc metric learning's effectiveness depends strongly on backbone embedding quality and domain specificity; domain-specific MegaDescriptor benefits notably from a triplet projection head while general-purpose DINOv2 shows limited improvability, indicating general-purpose manifolds are harder to reshape for fine-grained re-ID.

Abstract: This paper details the DS@GT team's entry for the AnimalCLEF 2025
re-identification challenge. Our key finding is that the effectiveness of
post-hoc metric learning is highly contingent on the initial quality and
domain-specificity of the backbone embeddings. We compare a general-purpose
model (DINOv2) with a domain-specific model (MegaDescriptor) as a backbone. A
K-Nearest Neighbor classifier with robust thresholding then identifies known
individuals or flags new ones. While a triplet-learning projection head
improved the performance of the specialized MegaDescriptor model by 0.13
points, it yielded minimal gains (0.03) for the general-purpose DINOv2 on
averaged BAKS and BAUS. We demonstrate that the general-purpose manifold is
more difficult to reshape for fine-grained tasks, as evidenced by stagnant
validation loss and qualitative visualizations. This work highlights the
critical limitations of refining general-purpose features for specialized,
limited-data re-ID tasks and underscores the importance of domain-specific
pre-training. The implementation for this work is publicly available at
github.com/dsgt-arc/animalclef-2025.

</details>


### [13] [GhostNetV3-Small: A Tailored Architecture and Comparative Study of Distillation Strategies for Tiny Images](https://arxiv.org/abs/2509.12380)
*Florian Zager,Hamza A. A. Gardi*

Main category: cs.CV

TL;DR: 为低分辨率任务改造架构比应用通用蒸馏方法更有效：GhostNetV3-Small 在 CIFAR-10 上表现更好，而蒸馏策略在此场景无效且降低了精度。


<details>
  <summary>Details</summary>
Motivation: 深度网络在边缘设备上的部署受限于计算资源，研究者希望通过模型压缩与适配（架构调整与知识蒸馏）在低资源环境和低分辨率输入下仍能保持较高精度。

Method: 基于 GhostNetV3 设计了更适合低分辨率输入的 GhostNetV3-Small（通过调整网络层配置和通道/卷积结构以适配 CIFAR-10 的输入尺寸）；同时比较了多种知识蒸馏策略（经典蒸馏、教师助理、教师集合），在相同训练/评估条件下对比其对小模型性能的影响。

Result: GhostNetV3-Small 在 CIFAR-10 上达到了 93.94% 的准确率，显著优于原始 GhostNetV3；而所有测试的蒸馏方法均未能超过直接训练的基线模型，表现下降。

Conclusion: 在小分辨率图像分类（如CIFAR-10）场景下，对架构进行针对性调整比简单套用蒸馏技术更能提升轻量网络性能；GhostNetV3-Small 在该任务上明显优于原始 GhostNetV3，但常见的知识蒸馏方法反而未能带来收益。

Abstract: Deep neural networks have achieved remarkable success across a range of
tasks, however their computational demands often make them unsuitable for
deployment on resource-constrained edge devices. This paper explores strategies
for compressing and adapting models to enable efficient inference in such
environments. We focus on GhostNetV3, a state-of-the-art architecture for
mobile applications, and propose GhostNetV3-Small, a modified variant designed
to perform better on low-resolution inputs such as those in the CIFAR-10
dataset. In addition to architectural adaptation, we provide a comparative
evaluation of knowledge distillation techniques, including traditional
knowledge distillation, teacher assistants, and teacher ensembles. Experimental
results show that GhostNetV3-Small significantly outperforms the original
GhostNetV3 on CIFAR-10, achieving an accuracy of 93.94%. Contrary to
expectations, all examined distillation strategies led to reduced accuracy
compared to baseline training. These findings indicate that architectural
adaptation can be more impactful than distillation in small-scale image
classification tasks, highlighting the need for further research on effective
model design and advanced distillation techniques for low-resolution domains.

</details>


### [14] [From Orthomosaics to Raw UAV Imagery: Enhancing Palm Detection and Crown-Center Localization](https://arxiv.org/abs/2509.12400)
*Rongkun Zhu,Kangning Cui,Wei Tang,Rui-Feng Wang,Sarra Alqahtani,David Lutz,Fan Yang,Paul Fine,Jordan Karubian,Robert Plemmons,Jean-Michel Morel,Victor Pauca,Miles Silman*

Main category: cs.CV

TL;DR: Study shows raw UAV images improve in-situ palm detection/localization, orthomosaics help cross-domain transfer, and crown-center labels boost localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Reduce preprocessing limitations of orthomosaics and assess whether raw UAV imagery can improve palm detection and precise crown-center localization for field deployment in tropical forests.

Method: Compared state-of-the-art object detectors and keypoint models across raw vs orthomosaic images; evaluated within-domain and cross-domain transfer; trained models with and without crown-center annotations.

Result: Raw imagery gave superior performance in deployment-relevant settings; orthomosaics provided more robust cross-domain generalization; adding crown-center annotations improved localization beyond bounding-box centroids.

Conclusion: Raw UAV imagery is better for deployment scenarios; orthomosaics better for cross-domain generalization; crown-center annotations improve localization.

Abstract: Accurate mapping of individual trees is essential for ecological monitoring
and forest management. Orthomosaic imagery from unmanned aerial vehicles (UAVs)
is widely used, but stitching artifacts and heavy preprocessing limit its
suitability for field deployment. This study explores the use of raw UAV
imagery for palm detection and crown-center localization in tropical forests.
Two research questions are addressed: (1) how detection performance varies
across orthomosaic and raw imagery, including within-domain and cross-domain
transfer, and (2) to what extent crown-center annotations improve localization
accuracy beyond bounding-box centroids. Using state-of-the-art detectors and
keypoint models, we show that raw imagery yields superior performance in
deployment-relevant scenarios, while orthomosaics retain value for robust
cross-domain generalization. Incorporating crown-center annotations in training
further improves localization and provides precise tree positions for
downstream ecological analyses. These findings offer practical guidance for
UAV-based biodiversity and conservation monitoring.

</details>


### [15] [DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction](https://arxiv.org/abs/2509.12430)
*Mayank Patel,Rahul Jain,Asim Unmesh,Karthik Ramani*

Main category: cs.CV

TL;DR: 本文提出MechBench数据集和DYNAMO模型，首次系统化地从分割CAD点云学习齿轮等机械装配体的耦合SE(3)运动，结果优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有工作通常假设简化运动学或依赖关节注释，难以处理通过齿合或轴对齐等几何耦合实现的运动。需要一个系统化数据和方法来学习从几何推断耦合机械运动。

Method: 构建了693个合成齿轮装配体的数据集MechBench，提供分部件的真值运动轨迹；提出DYNAMO模型，从分割的CAD点云输入，利用依赖关系感知的神经网络直接预测每个零件的SE(3)运动轨迹。

Result: 实验表明DYNAMO在多种齿轮配置上优于强基线，能实现准确且时间一致的运动预测，验证了MechBench与DYNAMO在学习耦合机械运动方面的有效性。

Conclusion: 该论文提出了针对齿轮等机械装配体中通过几何耦合产生的联动运动的系统性研究框架，并通过合成数据集MechBench和依赖感知模型DYNAMO，有效恢复了基于接触和传动关系而非预定义关节的零件运动。

Abstract: Understanding the motion of articulated mechanical assemblies from static
geometry remains a core challenge in 3D perception and design automation. Prior
work on everyday articulated objects such as doors and laptops typically
assumes simplified kinematic structures or relies on joint annotations.
However, in mechanical assemblies like gears, motion arises from geometric
coupling, through meshing teeth or aligned axes, making it difficult for
existing methods to reason about relational motion from geometry alone. To
address this gap, we introduce MechBench, a benchmark dataset of 693 diverse
synthetic gear assemblies with part-wise ground-truth motion trajectories.
MechBench provides a structured setting to study coupled motion, where part
dynamics are induced by contact and transmission rather than predefined joints.
Building on this, we propose DYNAMO, a dependency-aware neural model that
predicts per-part SE(3) motion trajectories directly from segmented CAD point
clouds. Experiments show that DYNAMO outperforms strong baselines, achieving
accurate and temporally consistent predictions across varied gear
configurations. Together, MechBench and DYNAMO establish a novel systematic
framework for data-driven learning of coupled mechanical motion in CAD
assemblies.

</details>


### [16] [Cott-ADNet: Lightweight Real-Time Cotton Boll and Flower Detection Under Field Conditions](https://arxiv.org/abs/2509.12442)
*Rui-Feng Wang,Mingrui Xu,Matthew C Bauer,Iago Beffart Schardong,Xiaowen Ma,Kangning Cui*

Main category: cs.CV

TL;DR: Cott-ADNet是一个基于YOLOv11n的轻量实时检测器，通过两项新模块提升弱特征检测与多尺度感受野，在实地棉铃识别上兼具精度与效率，公开了数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 手工采摘效率低且易错过最佳收获期，准确识别棉铃及其成熟度对于自动化收割、产量估算和育种研究至关重要。

Method: 基于YOLOv11n改进，优化卷积结构并引入NeLU增强的全局注意力机制和膨胀感受野SPPF模块，以增强低对比弱特征捕捉与多尺度上下文建模，同时保持低算力开销。

Result: 在4,966张训练集及1,216张外部验证集上，模型实现了Precision 91.5%、Recall 89.8%、mAP50 93.3%、mAP 71.3%、F1 90.6%，计算量仅7.5 GFLOPs，且对尺度与旋转变化表现稳定。

Conclusion: 本文提出的Cott-ADNet在复杂田间环境下实现了轻量化、实时的棉铃与花器识别，为自动化收割和表型分析提供了可行方案。

Abstract: Cotton is one of the most important natural fiber crops worldwide, yet
harvesting remains limited by labor-intensive manual picking, low efficiency,
and yield losses from missing the optimal harvest window. Accurate recognition
of cotton bolls and their maturity is therefore essential for automation, yield
estimation, and breeding research. We propose Cott-ADNet, a lightweight
real-time detector tailored to cotton boll and flower recognition under complex
field conditions. Building on YOLOv11n, Cott-ADNet enhances spatial
representation and robustness through improved convolutional designs, while
introducing two new modules: a NeLU-enhanced Global Attention Mechanism to
better capture weak and low-contrast features, and a Dilated Receptive Field
SPPF to expand receptive fields for more effective multi-scale context modeling
at low computational cost. We curate a labeled dataset of 4,966 images, and
release an external validation set of 1,216 field images to support future
research. Experiments show that Cott-ADNet achieves 91.5% Precision, 89.8%
Recall, 93.3% mAP50, 71.3% mAP, and 90.6% F1-Score with only 7.5 GFLOPs,
maintaining stable performance under multi-scale and rotational variations.
These results demonstrate Cott-ADNet as an accurate and efficient solution for
in-field deployment, and thus provide a reliable basis for automated cotton
harvesting and high-throughput phenotypic analysis. Code and dataset is
available at https://github.com/SweefongWong/Cott-ADNet.

</details>


### [17] [Deep learning for 3D point cloud processing -- from approaches, tasks to its implications on urban and environmental applications](https://arxiv.org/abs/2509.12452)
*Zhenxin Zhang,Zhihua Xu,Yuwei Cao,Ningli Xu,Shuye Wang,Shen'ao Cui,Zhen Li,Rongjun Qin*

Main category: cs.CV

TL;DR: 本文对点云深度学习方法在场景补全、配准、语义分割与建模等任务上的适用性与实践差距进行了元综述，指出数据规模、密度变化与多模态等问题是向实际应用转化的关键瓶颈，并提出相应改进方向。


<details>
  <summary>Details</summary>
Motivation: 尽管网络架构层出不穷，但现有综述较少关注点云在真实应用中遇到的数据规模、点密度变化、多模态融合与场景多样性等实际问题；因此需要一篇侧重实务价值与任务适配性的元综述来桥接算法与应用。

Method: 通过梳理并比较面向场景补全、配准、语义分割与建模等关键任务的深度学习方法与数据集，结合城乡与环境应用场景的实际需求，论文采用综述与批判性分析相结合的方式识别技术瓶颈与应用鸿沟。

Result: 总结了当前任务—场景对应关系、主流数据集的覆盖与不足、常用方法在处理超大规模与异构数据时的性能瓶颈，并提出未来研究方向，如效率与可扩展性优化、鲁棒性与跨域适应、多模态融合与标准化评测等。

Conclusion: 本文对点云处理领域中以深度学习为主导的方法进行了元综述，强调了现有研究在算法设计上取得的进展，但指出在大规模、异质场景和跨模态应用向实务转化时仍存在明显差距。作者总结了任务层面的挑战与机会，并在算法与应用实践两个层面给出改进建议。

Abstract: Point cloud processing as a fundamental task in the field of geomatics and
computer vision, has been supporting tasks and applications at different scales
from air to ground, including mapping, environmental monitoring, urban/tree
structure modeling, automated driving, robotics, disaster responses etc. Due to
the rapid development of deep learning, point cloud processing algorithms have
nowadays been almost explicitly dominated by learning-based approaches, most of
which are yet transitioned into real-world practices. Existing surveys
primarily focus on the ever-updating network architecture to accommodate
unordered point clouds, largely ignoring their practical values in typical
point cloud processing applications, in which extra-large volume of data,
diverse scene contents, varying point density, data modality need to be
considered. In this paper, we provide a meta review on deep learning approaches
and datasets that cover a selection of critical tasks of point cloud processing
in use such as scene completion, registration, semantic segmentation, and
modeling. By reviewing a broad range of urban and environmental applications
these tasks can support, we identify gaps to be closed as these methods
transformed into applications and draw concluding remarks in both the
algorithmic and practical aspects of the surveyed methods.

</details>


### [18] [Two-Stage Decoupling Framework for Variable-Length Glaucoma Prognosis](https://arxiv.org/abs/2509.12453)
*Yiran Song,Yikai Zhang,Silvia Orengo-Nania,Nian Wang,Fenglong Ma,Rui Zhang,Yifan Peng,Mingquan Lin*

Main category: cs.CV

TL;DR: 提出TSDF：1) 用自监督汇聚多源数据学习通用特征；2) 用注意力聚合可变长度时序输入，改善青光眼预后预测，实验证明在OHTS与GRAPE上有效且紧凑。


<details>
  <summary>Details</summary>
Motivation: 背景是现有方法受限于固定长度输入和端到端模型在小样本医学数据上的性能瓶颈，且多数据源下监督信息不一致导致难以直接融合训练。作者希望提高模型在可变随访长度、多来源数据条件下的泛化与鲁棒性，同时控制模型复杂度。

Method: 方法包括两部分：1）特征表示模块：采用自监督学习将多个标注方式/规模不同的青光眼数据集合并训练，以学习更鲁棒的图像/临床特征表示；2）时序聚合模块：引入基于注意力的可变长度序列处理器，对不同长度的随访序列进行加权聚合，输出预后预测。整体采用解耦训练以缓解小样本端到端训练的不稳性。

Result: 在两个基准数据集OHTS与GRAPE上进行广泛实验，结果显示TSDF在性能和鲁棒性上优于基线方法，且参数规模小、对数据规模差异具有适应性。

Conclusion: 该文提出的两阶段解耦框架（TSDF）通过先学习通用的自监督特征表示，再用注意力机制聚合可变长度时序输入，能够有效利用多源、多尺度数据并保持模型参数紧凑，从而提升青光眼预后预测的性能与鲁棒性。

Abstract: Glaucoma is one of the leading causes of irreversible blindness worldwide.
Glaucoma prognosis is essential for identifying at-risk patients and enabling
timely intervention to prevent blindness. Many existing approaches rely on
historical sequential data but are constrained by fixed-length inputs, limiting
their flexibility. Additionally, traditional glaucoma prognosis methods often
employ end-to-end models, which struggle with the limited size of glaucoma
datasets. To address these challenges, we propose a Two-Stage Decoupling
Framework (TSDF) for variable-length glaucoma prognosis. In the first stage, we
employ a feature representation module that leverages self-supervised learning
to aggregate multiple glaucoma datasets for training, disregarding differences
in their supervisory information. This approach enables datasets of varying
sizes to learn better feature representations. In the second stage, we
introduce a temporal aggregation module that incorporates an attention-based
mechanism to process sequential inputs of varying lengths, ensuring flexible
and efficient utilization of all available data. This design significantly
enhances model performance while maintaining a compact parameter size.
Extensive experiments on two benchmark glaucoma datasets:the Ocular
Hypertension Treatment Study (OHTS) and the Glaucoma Real-world Appraisal
Progression Ensemble (GRAPE),which differ significantly in scale and clinical
settings,demonstrate the effectiveness and robustness of our approach.

</details>


### [19] [Image Tokenizer Needs Post-Training](https://arxiv.org/abs/2509.12474)
*Kai Qiu,Xiang Li,Hao Chen,Jason Kuen,Xiaohao Xu,Jiuxiang Gu,Yinyi Luo,Bhiksha Raj,Zhe Lin,Marios Savvides*

Main category: cs.CV

TL;DR: 作者通过在tokenizer训练中加入采样噪声模拟（主训练）并在生成器上微调decoder（后训练），减少重建与生成分布差异，显著提升图像生成质量，提出pFID评价指标，并在多种配置上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成流程依赖冻结的预训练tokenizer，但该tokenizer仅关注重建任务，未考虑生成采样时的错误，导致重建分布与生成分布显著不一致，进而影响最终生成质量。

Method: 提出两阶段tokenizer训练：1) 主训练(main-training)：引入潜在扰动策略模拟采样过程中产生的异常token，采用可插拔的训练方案提升tokenizer鲁棒性；设计pFID作为评估指标。2) 后训练(post-training)：在固定且已训练好的生成器上优化tokenizer的decoder以减少生成token与重建token分布差异。实验在~400M生成器上验证，结合自回归与扩散式生成器及离散/连续tokenizer进行广泛测试。

Result: 采用主训练后，离散tokenizer与~400M生成器取得约1.60 gFID；加入后训练后进一步降至1.36 gFID。实验还证明后训练可有效应用到其他离散/连续tokenizer及不同类型生成器上，提升稳健性和生成质量。

Conclusion: 本文提出通过改进图像离散tokenizer的训练策略，缓解重建与生成分布差异，从而提升生成质量。主要结论是：在主训练中加入潜在扰动模拟采样噪声并进行后训练优化decoder，能显著提高生成质量和收敛速度，并用新的评估指标pFID关联tokenizer性能与生成质量。

Abstract: Recent image generative models typically capture the image distribution in a
pre-constructed latent space, relying on a frozen image tokenizer. However,
there exists a significant discrepancy between the reconstruction and
generation distribution, where current tokenizers only prioritize the
reconstruction task that happens before generative training without considering
the generation errors during sampling. In this paper, we comprehensively
analyze the reason for this discrepancy in a discrete latent space, and, from
which, we propose a novel tokenizer training scheme including both
main-training and post-training, focusing on improving latent space
construction and decoding respectively. During the main training, a latent
perturbation strategy is proposed to simulate sampling noises, \ie, the
unexpected tokens generated in generative inference. Specifically, we propose a
plug-and-play tokenizer training scheme, which significantly enhances the
robustness of tokenizer, thus boosting the generation quality and convergence
speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully
correlates the tokenizer performance to generation quality. During
post-training, we further optimize the tokenizer decoder regarding a
well-trained generative model to mitigate the distribution difference between
generated and reconstructed tokens. With a $\sim$400M generator, a discrete
tokenizer trained with our proposed main training achieves a notable 1.60 gFID
and further obtains 1.36 gFID with the additional post-training. Further
experiments are conducted to broadly validate the effectiveness of our
post-training strategy on off-the-shelf discrete and continuous tokenizers,
coupled with autoregressive and diffusion-based generators.

</details>


### [20] [Towards Foundational Models for Single-Chip Radar](https://arxiv.org/abs/2509.12482)
*Tianshu Huang,Akarsh Prabhakara,Chuhan Chen,Jay Karhade,Deva Ramanan,Matthew O'Toole,Anthony Rowe*

Main category: cs.CV

TL;DR: 作者构建了1M样本的原始毫米波雷达数据集并提出GRT基础模型，证明用原始数据训练的大尺度Transformer可显著提升低成本单芯片雷达的3D占据和语义分割性能，且性能随数据量以对数规律提升，完全发挥潜力需约3000小时数据。


<details>
  <summary>Details</summary>
Motivation: 廉价单芯片毫米波雷达抗遮挡且适应各种环境，但角分辨率差；缺乏标准化基础模型和大规模数据集，导致现有方法多为从头训练且数据集偏小，限制性能提升。

Method: 收集大型原始雷达数据集并训练基于Transformer的基础模型（GRT）；在多场景下评估模型的泛化能力、微调能力和数据规模效应；进行广泛消融研究，比较原始数据与常用有损表示的效果。

Result: GRT能在多样化场景下泛化，微调到不同任务效果良好，表现出每增加10倍数据约20%的对数尺度增益；使用原始雷达数据相比有损表示效果相当于增加10倍训练数据；估计完全释放GRT潜力需约100M样本（3000小时）。

Conclusion: 该论文构建了最大规模的原始单芯片毫米波雷达数据集（1M样本，29小时），并提出了通用雷达Transformer（GRT），用于从4D原始雷达数据预测3D占据和语义分割，达到相当于高分辨率传感器的效果。

Abstract: mmWave radars are compact, inexpensive, and durable sensors that are robust
to occlusions and work regardless of environmental conditions, such as weather
and darkness. However, this comes at the cost of poor angular resolution,
especially for inexpensive single-chip radars, which are typically used in
automotive and indoor sensing applications. Although many have proposed
learning-based methods to mitigate this weakness, no standardized foundational
models or large datasets for the mmWave radar have emerged, and practitioners
have largely trained task-specific models from scratch using relatively small
datasets.
  In this paper, we collect (to our knowledge) the largest available raw radar
dataset with 1M samples (29 hours) and train a foundational model for 4D
single-chip radar, which can predict 3D occupancy and semantic segmentation
with quality that is typically only possible with much higher resolution
sensors. We demonstrate that our Generalizable Radar Transformer (GRT)
generalizes across diverse settings, can be fine-tuned for different tasks, and
shows logarithmic data scaling of 20\% per $10\times$ data. We also run
extensive ablations on common design decisions, and find that using raw radar
data significantly outperforms widely-used lossy representations, equivalent to
a $10\times$ increase in training data. Finally, we roughly estimate that
$\approx$100M samples (3000 hours) of data are required to fully exploit the
potential of GRT.

</details>


### [21] [Evaluating Robustness of Vision-Language Models Under Noisy Conditions](https://arxiv.org/abs/2509.12492)
*Purushoth,Alireza*

Main category: cs.CV

TL;DR: 评估VLMs在光照、模糊、压缩等受控噪声下的鲁棒性，结合词汇级和句向量相似度度量，发现参考描述质量、模型规模与噪声类型共同决定性能，JPEG和运动模糊尤其破坏性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务中表现优异，但其在现实世界中常遇到的图像噪声条件下的鲁棒性仍未被充分理解，因此需要系统化评估以指导模型设计与应用。

Method: 在多个数据集上对若干先进的VLMs进行受控图像扰动（光照变化、运动模糊、压缩伪影等），使用词汇级评估指标（BLEU、METEOR、ROUGE、CIDEr）与基于句向量的神经相似度度量来量化语义对齐度，比较模型在不同噪声与数据集条件下的表现差异。

Result: 发现（1）参考（GT）描述的详尽程度对评估结果影响显著；（2）大模型（如LLaVA）在语义理解上优势明显，但并不总是优于小模型；（3）JPEG压缩与运动模糊等噪声对模型性能有显著破坏作用。为未来鲁棒多模态学习提供了标准化基准。

Conclusion: 本文提出了一个针对视觉-语言模型（VLMs）在受控噪声扰动下的全面评估框架，揭示了不同噪声类型对多模态模型的显著影响，并指出模型规模与鲁棒性之间并非简单正相关。

Abstract: Vision-Language Models (VLMs) have attained exceptional success across
multimodal tasks such as image captioning and visual question answering.
However, their robustness under noisy conditions remains unfamiliar. In this
study, we present a comprehensive evaluation framework to evaluate the
performance of several state-of-the-art VLMs under controlled perturbations,
including lighting variation, motion blur, and compression artifacts. We used
both lexical-based metrics (BLEU, METEOR, ROUGE, CIDEr) and neural-based
similarity measures using sentence embeddings to quantify semantic alignment.
Our experiments span diverse datasets, revealing key insights: (1)
descriptiveness of ground-truth captions significantly influences model
performance; (2) larger models like LLaVA excel in semantic understanding but
do not universally outperform smaller models; and (3) certain noise types, such
as JPEG compression and motion blur, dramatically degrade performance across
models. Our findings highlight the nuanced trade-offs between model size,
dataset characteristics, and noise resilience, offering a standardized
benchmark for future robust multimodal learning.

</details>


### [22] [Instance-Guided Class Activation Mapping for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2509.12496)
*Ali Torabi,Sanjog Gaihre,MD Mahbubur Rahman,Yaqoob Majeed*

Main category: cs.CV

TL;DR: IG-CAM 利用实例级引导、影响函数和多尺度边界增强，在弱监督语义分割中实现了更完整的对象覆盖和精确边界，达到新的 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有 WSSS 方法仅关注最具判别性的区域、边界定位不精确的问题，通过引入实例级信息和样本影响关系来获得更完整和精细的对象定位图，从而提升弱监督分割性能。

Method: 方法包括三部分：1) 实例引导细化：利用（假定存在的）实例/分割掩码引导 CAM 生成，扩展到完整物体区域；2) 影响函数集成：计算训练样本对预测的影响，增强特征鲁棒性；3) 多尺度边界增强：采用逐步细化的多尺度策略提高边界精确度，并在生成的 CAM 上应用边界强化和 CRF 精修。

Result: 在 PASCAL VOC 2012 数据集上，IG-CAM 在未后处理情况下达到 82.3% mIoU，CRF 后提升到 86.6%，显著优于现有方法；定性展示在 600 张图像上均表现稳健，且消融实验显示三大模块均带来性能增益。

Conclusion: IG-CAM 提出了一种结合实例级引导、影响函数和多尺度边界增强的弱监督语义分割方法，能够在仅用图像级标签的情况下生成完整覆盖且边界精确的定位图，实验证明其在 PASCAL VOC 2012 上取得了优异的 mIoU（82.3% 未后处理，CRF 后 86.6%），并通过消融验证了各组件贡献。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) addresses the challenge of
training segmentation models using only image-level annotations, eliminating
the need for expensive pixel-level labeling. While existing methods struggle
with precise object boundary localization and often focus only on the most
discriminative regions, we propose IG-CAM (Instance-Guided Class Activation
Mapping), a novel approach that leverages instance-level cues and influence
functions to generate high-quality, boundary-aware localization maps. Our
method introduces three key innovations: (1) Instance-Guided Refinement that
uses ground truth segmentation masks to guide CAM generation, ensuring complete
object coverage rather than just discriminative parts; (2) Influence Function
Integration that captures the relationship between training samples and model
predictions, leading to more robust feature representations; and (3)
Multi-Scale Boundary Enhancement that employs progressive refinement strategies
to achieve sharp, precise object boundaries. IG-CAM achieves state-of-the-art
performance on the PASCAL VOC 2012 dataset with an mIoU of 82.3% before
post-processing, which further improves to 86.6% after applying Conditional
Random Field (CRF) refinement, significantly outperforming previous WSSS
methods. Our approach demonstrates superior localization accuracy, with
complete object coverage and precise boundary delineation, while maintaining
computational efficiency. Extensive ablation studies validate the contribution
of each component, and qualitative comparisons across 600 diverse images
showcase the method's robustness and generalization capability. The results
establish IG-CAM as a new benchmark for weakly supervised semantic
segmentation, offering a practical solution for scenarios where pixel-level
annotations are unavailable or prohibitively expensive.

</details>


### [23] [Artist-Created Mesh Generation from Raw Observation](https://arxiv.org/abs/2509.12501)
*Yao He,Youngjoong Kwon,Wenxiao Cai,Ehsan Adeli*

Main category: cs.CV

TL;DR: 将点云修复转为2D inpainting，利用生成模型实现端到端从噪声点云到艺术家风格网格的重建，初步在ShapeNet上取得有希望的结果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常要求输入点云干净完整或依赖复杂的多阶段流程，难以处理真实传感器（如LiDAR或移动RGB-D）采集的嘈杂、不完整数据。本工作希望提供适用于真实场景的简洁、高效端到端方案。

Method: 核心是将3D点云修复重构问题重新表述为2D图像修复（inpainting）任务，从而利用强大的生成模型进行点云精修并直接输出高质量网格。

Result: 在ShapeNet数据集上的初步实验表明，该框架能生成更干净、完整的网格，显示出方法的潜力。

Conclusion: 该文提出了一种端到端方法，将噪声或不完整的点云直接生成艺术家风格（artist-style）的网格模型。

Abstract: We present an end-to-end framework for generating artist-style meshes from
noisy or incomplete point clouds, such as those captured by real-world sensors
like LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial for
commercial graphics pipelines due to their compatibility with animation and
texturing tools and their efficiency in rendering. However, existing approaches
often assume clean, complete inputs or rely on complex multi-stage pipelines,
limiting their applicability in real-world scenarios. To address this, we
propose an end-to-end method that refines the input point cloud and directly
produces high-quality, artist-style meshes. At the core of our approach is a
novel reformulation of 3D point cloud refinement as a 2D inpainting task,
enabling the use of powerful generative models. Preliminary results on the
ShapeNet dataset demonstrate the promise of our framework in producing clean,
complete meshes.

</details>


### [24] [Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery](https://arxiv.org/abs/2509.12511)
*Benjamin Vail,Rahul Harsha Cheppally,Ajay Sharda,Sidharth Rai*

Main category: cs.CV

TL;DR: 提出基于实例分割+点云重建+PCA轴对齐切片的几何感知管线，从RGB-D图像自动估计茎秆直径，具备鲁棒性与可扩展性，适用于高通量表型分析。


<details>
  <summary>Details</summary>
Motivation: 传统茎秆直径测量方法耗时、易出错、不利于大规模表型分析，需要一种自动化、可扩展且鲁棒的测量方法。

Method: 结合深度学习实例分割、RGB-D到3D点云重建，以及基于主成分分析(PCA)的轴对齐切片来估计直径；通过切片在局部平面上拟合圆或椭圆以计算直径，从而提高鲁棒性。

Result: 方法在模拟或实验数据上表现出对弯曲、遮挡和噪声的鲁棒性，实现了可扩展的直径估计，支持育种和农学研究的高通量表型测量需求（假设论文包含定量评估）。

Conclusion: 该论文提出了一种几何感知的计算机视觉管线，用于从RGB-D图像估计茎秆直径，能够缓解弯曲、遮挡和噪声影响，为高通量表型分析提供可扩展且可靠的方法。

Abstract: Accurate, high-throughput phenotyping is a critical component of modern crop
breeding programs, especially for improving traits such as mechanical
stability, biomass production, and disease resistance. Stalk diameter is a key
structural trait, but traditional measurement methods are labor-intensive,
error-prone, and unsuitable for scalable phenotyping. In this paper, we present
a geometry-aware computer vision pipeline for estimating stalk diameter from
RGB-D imagery. Our method integrates deep learning-based instance segmentation,
3D point cloud reconstruction, and axis-aligned slicing via Principal Component
Analysis (PCA) to perform robust diameter estimation. By mitigating the effects
of curvature, occlusion, and image noise, this approach offers a scalable and
reliable solution to support high-throughput phenotyping in breeding and
agronomic research.

</details>


### [25] [Neural Collapse-Inspired Multi-Label Federated Learning under Label-Distribution Skew](https://arxiv.org/abs/2509.12544)
*Can Peng,Yuyuan Liu,Yingyu Yang,Pramit Saha,Qianye Yang,J. Alison Noble*

Main category: cs.CV

TL;DR: 针对联邦学习中常被忽视的多标签异构问题，论文提出基于神经塌陷结构的特征解耦与对齐方法，通过共享NC引导和正则化损失提升特征聚类性，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习大多关注单标签任务，而现实场景（如医学影像）常为多标签且各客户端标签分布严重偏斜，传统方法难以处理标签共现、标签间依赖及局部全局关系差异。借助NC的几何结构能促使特征更易聚类和对齐，改善异构多标签FL性能。

Method: 设计特征解耦模块从图像级特征中分离出类相关特征，并以预定义的共享NC结构为引导，通过正则化损失（鼓励类内紧密、类间分离）对齐各客户端的解耦特征分布；在联邦聚合过程中保持此结构以降低本地分布差异的负面影响。

Result: 在四个基准数据集和八种不同实验设置下，该方法显著优于现有方法，证明在多标签且标签偏斜的联邦学习中具有有效性。

Conclusion: 提出在联邦学习多标签场景下，通过引入神经塌陷（NC）结构并结合特征解耦来对齐客户端特征分布，从而获得更紧凑、可聚类的语义特征，缓解由于数据异质性导致的模型冲突。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. However, the performance of
deep learning often deteriorates in FL due to decentralized and heterogeneous
data. This challenge is further amplified in multi-label scenarios, where data
exhibit complex characteristics such as label co-occurrence, inter-label
dependency, and discrepancies between local and global label relationships.
While most existing FL research primarily focuses on single-label
classification, many real-world applications, particularly in domains such as
medical imaging, often involve multi-label settings. In this paper, we address
this important yet underexplored scenario in FL, where clients hold multi-label
data with skewed label distributions. Neural Collapse (NC) describes a
geometric structure in the latent feature space where features of each class
collapse to their class mean with vanishing intra-class variance, and the class
means form a maximally separated configuration. Motivated by this theory, we
propose a method to align feature distributions across clients and to learn
high-quality, well-clustered representations. To make the NC-structure
applicable to multi-label settings, where image-level features may contain
multiple semantic concepts, we introduce a feature disentanglement module that
extracts semantically specific features. The clustering of these disentangled
class-wise features is guided by a predefined shared NC structure, which
mitigates potential conflicts between client models due to diverse local data
distributions. In addition, we design regularisation losses to encourage
compact clustering in the latent feature space. Experiments conducted on four
benchmark datasets across eight diverse settings demonstrate that our approach
outperforms existing methods, validating its effectiveness in this challenging
FL scenario.

</details>


### [26] [Agent4FaceForgery: Multi-Agent LLM Framework for Realistic Face Forgery Detection](https://arxiv.org/abs/2509.12546)
*Yingxin Lai,Zitong Yu,Jun Wang,Linlin Shen,Yong Xu,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出基于LLM的多代理模拟与ARS筛选的数据生成框架，用以生成带细粒度文本-图像一致性标签的多样化伪造样本，显著提升人脸伪造检测在真实场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前人脸伪造检测在离线基准与现实世界间存在显著性能差距，归因于训练数据的生态无效性；需要模拟人类伪造意图与社交媒体上复杂的文本-图像对抗交互来提升数据真实性与覆盖面。

Method: 构建LLM驱动的多代理系统，代理具备个人档案与记忆模块，在模拟的社交环境中以迭代方式生成伪造样本并进行文本-图像交互；引入自适应拒采样（ARS）机制以保证数据质量与多样性；生成带有细粒度文本-图像一致性标签的数据，供多种检测器训练与评估。

Result: 通过大量实验表明，使用Agent4FaceForgery生成的数据能为多种架构的检测器带来显著性能提升，证明了该框架在改善检测器泛化性与实际效用方面的价值。

Conclusion: Agent4FaceForgery通过模拟多代理的伪造生成与社交互动过程，显著缩小了训练数据与真实场景间的差距，从而提升了人脸伪造检测器在真实世界的效果。

Abstract: Face forgery detection faces a critical challenge: a persistent gap between
offline benchmarks and real-world efficacy,which we attribute to the ecological
invalidity of training data.This work introduces Agent4FaceForgery to address
two fundamental problems: (1) how to capture the diverse intents and iterative
processes of human forgery creation, and (2) how to model the complex, often
adversarial, text-image interactions that accompany forgeries in social media.
To solve this,we propose a multi-agent framework where LLM-poweredagents,
equipped with profile and memory modules, simulate the forgery creation
process. Crucially, these agents interact in a simulated social environment to
generate samples labeled for nuanced text-image consistency, moving beyond
simple binary classification. An Adaptive Rejection Sampling (ARS) mechanism
ensures data quality and diversity. Extensive experiments validate that the
data generated by our simulationdriven approach brings significant performance
gains to detectors of multiple architectures, fully demonstrating the
effectiveness and value of our framework.

</details>


### [27] [Explicit Multimodal Graph Modeling for Human-Object Interaction Detection](https://arxiv.org/abs/2509.12554)
*Wenxuan Ji,Haichao Shi,Xiao-Yu zhang*

Main category: cs.CV

TL;DR: 提出基于GNN的多模态图网络MGNM，通过四阶段图结构和多层次视觉-语言特征交互，显著提升HOI检测，在HICO-DET和V-COCO上达到SOTA并改善类别平衡。


<details>
  <summary>Details</summary>
Motivation: Transformer虽强，但未显式建模HOI固有的关系结构，限制了交互识别能力；GNN天然适合表示实体间关系，因而可用于增强HOI检测。

Method: 设计了一个多模态图网络框架，包括四阶段图结构（可能涉及构建节点、边、消息传递与预测四个步骤）以及一个多层次特征交互机制，用于在图中融合不同层次的视觉和语言特征以增强信息传递。

Result: 在HICO-DET和V-COCO两大基准上达到SOTA性能；与更先进的目标检测器结合时，性能有明显提升，并在稀有/非稀有类别间保持有效平衡。

Conclusion: 本文提出的MGNM通过在四阶段图结构中显式建模人-物对关系，并融合多层次视觉与语言特征，实现了对交互信息的更好传播，从而推动HOI检测性能提升。

Abstract: Transformer-based methods have recently become the prevailing approach for
Human-Object Interaction (HOI) detection. However, the Transformer architecture
does not explicitly model the relational structures inherent in HOI detection,
which impedes the recognition of interactions. In contrast, Graph Neural
Networks (GNNs) are inherently better suited for this task, as they explicitly
model the relationships between human-object pairs. Therefore, in this paper,
we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork
\textbf{M}odeling (MGNM) that leverages GNN-based relational structures to
enhance HOI detection. Specifically, we design a multimodal graph network
framework that explicitly models the HOI task in a four-stage graph structure.
Furthermore, we introduce a multi-level feature interaction mechanism within
our graph network. This mechanism leverages multi-level vision and language
features to enhance information propagation across human-object pairs.
Consequently, our proposed MGNM achieves state-of-the-art performance on two
widely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with a
more advanced object detector, our method demonstrates a significant
performance gain and maintains an effective balance between rare and non-rare
classes.

</details>


### [28] [VQT-Light:Lightweight HDR Illumination Map Prediction with Richer Texture.pdf](https://arxiv.org/abs/2509.12556)
*Kunliang Xie*

Main category: cs.CV

TL;DR: VQT-Light结合VQVAE离散编码与ViT全局建模，把光照估计转为多类分类，达到更丰富纹理、更高保真且40FPS的高效估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复光照图细节纹理或在速度与纹理保真度之间难以兼顾，需一种既保留细节又高效的方案。VQVAE和ViT的结合可分别解决连续表征的塌陷与局部卷积的全局建模不足。

Method: 框架包含两个模块：利用VQVAE对光照图提取离散表征以避免后验塌陷；采用ViT对输入图像建模以捕捉全局上下文并预测视野外光照。将光照估计表述为多类分类问题，从而生成带有细节纹理的光照图。

Result: 模型在多个评估指标上优于现有方法，推理速度达40 FPS，生成的光照图在质感细节与保真度上有所提升。

Conclusion: 提出的VQT-Light通过将照明估计作为多类分类任务，并结合VQVAE离散特征与ViT全局建模，实现了更丰富纹理和更高保真度的光照图估计，同时保持轻量与高速推理。

Abstract: Accurate lighting estimation is a significant yet challenging task in
computer vision and graphics. However, existing methods either struggle to
restore detailed textures of illumination map, or face challenges in running
speed and texture fidelity. To tackle this problem, we propose a novel
framework (VQT-Light) based on VQVAE and ViT architecture. VQT-Light includes
two modules: feature extraction and lighting estimation. First, we take
advantages of VQVAE to extract discrete features of illumination map rather
than continuous features to avoid "posterior collapse". Second, we capture
global context and dependencies of input image through ViT rather than CNNs to
improve the prediction of illumination outside the field of view. Combining the
above two modules, we formulate the lighting estimation as a multiclass
classification task, which plays a key role in our pipeline. As a result, our
model predicts light map with richer texture and better fidelity while keeping
lightweight and fast. VQT-Light achieves an inference speed of 40FPS and
improves multiple evaluation metrics. Qualitative and quantitative experiments
demonstrate that the proposed method realizes superior results compared to
existing state-of-the-art methods.

</details>


### [29] [Adaptive Sampling Scheduler](https://arxiv.org/abs/2509.12569)
*Qi Wang,Shuliang Zhu,Jinjia Zhou*

Main category: cs.CV

TL;DR: 提出一种适用于多种一致性蒸馏框架的自适应采样调度器，通过时间步重要性动态选取、交替采样优化及稳定化处理，显著提升采样效率与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有一致性蒸馏在目标时间步选择上依赖确定性或随机策略，需为不同蒸馏过程设计特定采样调度器，限制了灵活性与实际应用中的采样潜力。

Method: 基于时间步重要性计算（dynamic target timestep selection），沿解轨迹进行前向去噪与反向加噪的交替采样优化（optimized alternating sampling），并结合平滑裁剪与色彩平衡技术以稳定高指导尺度下的生成结果。

Result: 在多种一致性蒸馏方法上进行的综合实验表明，该自适应调度器显著提升了生成性能并具有良好泛化性与适配性，尤其在高指导尺度下表现稳定且质量更高。

Conclusion: 本文提出了一种通用的自适应采样调度器，能在多种一致性蒸馏框架下动态选择目标时间步并优化采样过程，从而提升生成质量与采样效率。

Abstract: Consistent distillation methods have evolved into effective techniques that
significantly accelerate the sampling process of diffusion models. Although
existing methods have achieved remarkable results, the selection of target
timesteps during distillation mainly relies on deterministic or stochastic
strategies, which often require sampling schedulers to be designed specifically
for different distillation processes. Moreover, this pattern severely limits
flexibility, thereby restricting the full sampling potential of diffusion
models in practical applications. To overcome these limitations, this paper
proposes an adaptive sampling scheduler that is applicable to various
consistency distillation frameworks. The scheduler introduces three innovative
strategies: (i) dynamic target timestep selection, which adapts to different
consistency distillation frameworks by selecting timesteps based on their
computed importance; (ii) Optimized alternating sampling along the solution
trajectory by guiding forward denoising and backward noise addition based on
the proposed time step importance, enabling more effective exploration of the
solution space to enhance generation performance; and (iii) Utilization of
smoothing clipping and color balancing techniques to achieve stable and
high-quality generation results at high guidance scales, thereby expanding the
applicability of consistency distillation models in complex generation
scenarios. We validated the effectiveness and flexibility of the adaptive
sampling scheduler across various consistency distillation methods through
comprehensive experimental evaluations. Experimental results consistently
demonstrated significant improvements in generative performance, highlighting
the strong adaptability achieved by our method.

</details>


### [30] [DisorientLiDAR: Physical Attacks on LiDAR-based Localization](https://arxiv.org/abs/2509.12595)
*Yizhen Lao,Yu Zhang,Ziting Wang,Chengbo Wang,Yifei Xue,Wanpeng Shao*

Main category: cs.CV

TL;DR: 通过识别并去除/遮挡点云中的关键区域，DisorientLiDAR能够在数字和物理场景中有效破坏LiDAR定位，展现出对自动驾驶定位系统的真实威胁。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击多集中于3D感知（如目标检测、语义分割），对LiDAR定位攻击探索不足，而定位对自动驾驶安全至关重要。作者旨在证明通过针对关键点的选择性删除/遮挡可造成严重定位漂移，进而暴露安全风险。

Method: 逆向定位模型（特征提取网络）以识别Top-K关键点/关键区域，随后在点云数据中删除或遮挡这些区域；在物理实现中使用近红外吸收材料遮蔽关键区域，评估包括在KITTI数据集上的HRegNet、D3Feat、GeoTransformer以及Autoware平台。

Result: 在KITTI上对三种配准模型实验表明删除包含Top-K关键点的区域显著降低配准精度；在Autoware平台上隐藏少量关键区域导致明显定位漂移；物理实验使用近红外吸收材料遮挡关键区域复制了数字实验效果，表明攻击具可行性与普适性。

Conclusion: 本文提出DisorientLiDAR，通过识别并删除点云中的关键区域，能显著破坏基于LiDAR的定位。攻击在模拟与实际平台上均有效，且可通过近红外吸收材料在物理世界实现。

Abstract: Deep learning models have been shown to be susceptible to adversarial attacks
with visually imperceptible perturbations. Even this poses a serious security
challenge for the localization of self-driving cars, there has been very little
exploration of attack on it, as most of adversarial attacks have been applied
to 3D perception. In this work, we propose a novel adversarial attack framework
called DisorientLiDAR targeting LiDAR-based localization. By
reverse-engineering localization models (e.g., feature extraction networks),
adversaries can identify critical keypoints and strategically remove them,
thereby disrupting LiDAR-based localization. Our proposal is first evaluated on
three state-of-the-art point-cloud registration models (HRegNet, D3Feat, and
GeoTransformer) using the KITTI dataset. Experimental results demonstrate that
removing regions containing Top-K keypoints significantly degrades their
registration accuracy. We further validate the attack's impact on the Autoware
autonomous driving platform, where hiding merely a few critical regions induces
noticeable localization drift. Finally, we extended our attacks to the physical
world by hiding critical regions with near-infrared absorptive materials,
thereby successfully replicate the attack effects observed in KITTI data. This
step has been closer toward the realistic physical-world attack that
demonstrate the veracity and generality of our proposal.

</details>


### [31] [Exploring Spectral Characteristics for Single Image Reflection Removal](https://arxiv.org/abs/2509.12627)
*Pengbo Guo,Chengxu Liu,Guoshuai Zhao,Xingsong Hou,Jialie Shen,Xueming Qian*

Main category: cs.CV

TL;DR: 通过重建反射光谱并结合空间-光谱先验与变换器联合恢复，本文提出的方法能更好地区分并去除反射，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅在图像域处理忽视了反射光的光谱差异，导致难以区分重叠的反射与透射成分；利用光谱信息可通过波长差异更好分辨不同光源引起的反射。

Method: 提出了Spectral Codebook用于重建反射光谱，设计了两个光谱先验细化模块（空间维像素再分配和波长维自适应增强），并引入Spectrum-Aware Transformer在光谱与像素域联合恢复透射内容。

Result: 在三个不同的反射去除基准数据集上，所提方法在去反射效果和泛化能力方面均优于最新的基线模型。

Conclusion: 本文提出了基于光谱学习的反射去除方法，通过重建反射光的光谱并结合空间-光谱先验与变换器架构，有效区分并去除反射，实验表明优于现有方法。

Abstract: Eliminating reflections caused by incident light interacting with reflective
medium remains an ill-posed problem in the image restoration area. The primary
challenge arises from the overlapping of reflection and transmission components
in the captured images, which complicates the task of accurately distinguishing
and recovering the clean background. Existing approaches typically address
reflection removal solely in the image domain, ignoring the spectral property
variations of reflected light, which hinders their ability to effectively
discern reflections. In this paper, we start with a new perspective on spectral
learning, and propose the Spectral Codebook to reconstruct the optical spectrum
of the reflection image. The reflections can be effectively distinguished by
perceiving the wavelength differences between different light sources in the
spectrum. To leverage the reconstructed spectrum, we design two spectral prior
refinement modules to re-distribute pixels in the spatial dimension and
adaptively enhance the spectral differences along the wavelength dimension.
Furthermore, we present the Spectrum-Aware Transformer to jointly recover the
transmitted content in spectral and pixel domains. Experimental results on
three different reflection benchmarks demonstrate the superiority and
generalization ability of our method compared to state-of-the-art models.

</details>


### [32] [Maps for Autonomous Driving: Full-process Survey and Frontiers](https://arxiv.org/abs/2509.12632)
*Pengxin Chen,Zhipeng Luo,Xiaoqi Jiang,Zhangcai Yin,Jonathan Li*

Main category: cs.CV

TL;DR: 论文回顾了HD、Lite与Implicit地图的演进、生产流程与挑战，并探讨了将新型地图表示整合进端到端自动驾驶系统的方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶对地图的需求随感知与决策能力提升而变化，现有HD地图成本高、维护难，促使轻量化和隐式表示的研究以提高可扩展性与实时性。

Method: 通过对现有文献和技术路线进行分类回顾，总结每一阶段的地图表示、制作流程、关键技术挑战及社区提出的解决方案，并讨论将新表示融入端到端框架的方法。

Result: 总结出各阶段地图在表示复杂性、生产成本、实时更新和与感知融合等方面的权衡；列举了相关的制作流程改进、语义压缩、学习式生成与隐式场景表示等研究进展。

Conclusion: 本文将地图演进为HD、Lite和Implicit三阶段，强调隐式地图为未来趋势并能与端到端自动驾驶整合。

Abstract: Maps have always been an essential component of autonomous driving. With the
advancement of autonomous driving technology, both the representation and
production process of maps have evolved substantially. The article categorizes
the evolution of maps into three stages: High-Definition (HD) maps, Lightweight
(Lite) maps, and Implicit maps. For each stage, we provide a comprehensive
review of the map production workflow, with highlighting technical challenges
involved and summarizing relevant solutions proposed by the academic community.
Furthermore, we discuss cutting-edge research advances in map representations
and explore how these innovations can be integrated into end-to-end autonomous
driving frameworks.

</details>


### [33] [CIARD: Cyclic Iterative Adversarial Robustness Distillation](https://arxiv.org/abs/2509.12633)
*Liming Lu,Shuchao Pang,Xu Zheng,Xiang Gu,Anan Du,Yunhuai Liu,Yongbin Zhou*

Main category: cs.CV

TL;DR: CIARD通过多教师对比推挤对齐与连续对抗再训练，解决双教师目标冲突与鲁棒教师退化，显著提升学生模型对抗鲁棒性和干净样本准确率。


<details>
  <summary>Details</summary>
Motivation: 现有对抗鲁棒蒸馏（ARD）在提升鲁棒性的同时常导致干净样本性能下降。作者归因于双教师框架中两个问题：双教师目标冲突（清洁教师与鲁棒教师优化目标不一致）和训练中迭代生成的对抗样本导致鲁棒教师性能退化。

Method: 提出Cyclic Iterative ARD(CIARD)：1) 多教师框架，引入对比推挤（contrastive push-loss）对齐以协调清洁教师与鲁棒教师间的冲突目标；2) 持续对抗再训练（continuous adversarial retraining），使鲁棒教师在训练中不断适应由迭代生成的对抗样本，防止性能下降。

Result: 在CIFAR-10、CIFAR-100与Tiny-ImageNet上，CIARD在多种攻击场景下平均提升对抗防御率3.53个百分点，并在干净样本准确率上平均提高5.87个百分点，成为在鲁棒性与泛化间的新基准。代码已开源。

Conclusion: CIARD通过多教师对比推挤损失与连续对抗再训练，有效缓解双教师目标冲突与鲁棒教师性能退化问题，显著提升学生模型在对抗与干净样本上的表现，达成鲁棒性与泛化性的平衡。

Abstract: Adversarial robustness distillation (ARD) aims to transfer both performance
and robustness from teacher model to lightweight student model, enabling
resilient performance on resource-constrained scenarios. Though existing ARD
approaches enhance student model's robustness, the inevitable by-product leads
to the degraded performance on clean examples. We summarize the causes of this
problem inherent in existing methods with dual-teacher framework as: 1. The
divergent optimization objectives of dual-teacher models, i.e., the clean and
robust teachers, impede effective knowledge transfer to the student model, and
2. The iteratively generated adversarial examples during training lead to
performance deterioration of the robust teacher model. To address these
challenges, we propose a novel Cyclic Iterative ARD (CIARD) method with two key
innovations: a. A multi-teacher framework with contrastive push-loss alignment
to resolve conflicts in dual-teacher optimization objectives, and b. Continuous
adversarial retraining to maintain dynamic teacher robustness against
performance degradation from the varying adversarial examples. Extensive
experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that CIARD
achieves remarkable performance with an average 3.53 improvement in adversarial
defense rates across various attack scenarios and a 5.87 increase in clean
sample accuracy, establishing a new benchmark for balancing model robustness
and generalization. Our code is available at https://github.com/eminentgu/CIARD

</details>


### [34] [Beyond Artificial Misalignment: Detecting and Grounding Semantic-Coordinated Multimodal Manipulations](https://arxiv.org/abs/2509.12653)
*Jinjie Shen,Yaxiong Wang,Lechao Cheng,Nan Pu,Zhun Zhong*

Main category: cs.CV

TL;DR: 提出SAMM数据集用于评估语义对齐的多模态篡改，并设计RamDG通过检索外部证据进行增强检测与定位，在该更具现实性的基准上取得明显性能提升（+2.06%）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态篡改检测基准存在跨模态错位伪影，容易被检测器利用而不反映现实攻击场景。真实攻击往往保持图文语义一致，故需要针对语义协同的篡改进行研究与数据集构建。

Method: 方法包括两部分：数据集构建与模型设计。数据集SAMM通过两阶段流水线生成——先用先进的图像编辑方法进行视觉篡改，再自动生成与视觉内容语义一致的文本描述；模型RamDG采用检索增强策略，从外部知识库检索上下文证据，将其作为辅助文本，与输入图像与文本一并编码，结合图像伪造定位模块与深度篡改检测模块进行联合训练与推理。

Result: 在构建的SAMM数据集上，RamDG相较于现有最先进方法在检测准确率上提升约2.06%，并在篡改定位任务上表现更稳健。作者已开源数据与代码。

Conclusion: 本文提出并验证了用于检测语义协同编辑的多模态篡改的新任务与方法，结论是：在更贴近现实的SAMM数据集上，所提RamDG框架通过检索外部证据并融合图文编码，在篡改检测与定位上明显优于现有方法，检测准确率提升约2.06%。

Abstract: The detection and grounding of manipulated content in multimodal data has
emerged as a critical challenge in media forensics. While existing benchmarks
demonstrate technical progress, they suffer from misalignment artifacts that
poorly reflect real-world manipulation patterns: practical attacks typically
maintain semantic consistency across modalities, whereas current datasets
artificially disrupt cross-modal alignment, creating easily detectable
anomalies. To bridge this gap, we pioneer the detection of
semantically-coordinated manipulations where visual edits are systematically
paired with semantically consistent textual descriptions. Our approach begins
with constructing the first Semantic-Aligned Multimodal Manipulation (SAMM)
dataset, generated through a two-stage pipeline: 1) applying state-of-the-art
image manipulations, followed by 2) generation of contextually-plausible
textual narratives that reinforce the visual deception. Building on this
foundation, we propose a Retrieval-Augmented Manipulation Detection and
Grounding (RamDG) framework. RamDG commences by harnessing external knowledge
repositories to retrieve contextual evidence, which serves as the auxiliary
texts and encoded together with the inputs through our image forgery grounding
and deep manipulation detection modules to trace all manipulations. Extensive
experiments demonstrate our framework significantly outperforms existing
methods, achieving 2.06\% higher detection accuracy on SAMM compared to
state-of-the-art approaches. The dataset and code are publicly available at
https://github.com/shen8424/SAMM-RamDG-CAP.

</details>


### [35] [MFAF: An EVA02-Based Multi-scale Frequency Attention Fusion Method for Cross-View Geo-Localization](https://arxiv.org/abs/2509.12673)
*YiTong Liu,TianZhu Liu,YanFeng GU*

Main category: cs.CV

TL;DR: 提出EVA02+MFAF框架，通过MFB捕获多尺度频谱特征并用FSA进行频域空间注意力融合，显著提高跨视角地理定位的鲁棒性和抗干扰能力，在三个基准上表现良好。


<details>
  <summary>Details</summary>
Motivation: 现有方法在通过分割特征图提取信息时忽略了空间与语义信息，且难以同时捕获低频结构与高频细节，导致跨视角匹配困难。

Method: 构建MFB模块以在多尺度上提取低频结构信息和高频边缘细节，结合FSA模块对频域特征进行空间注意力加权以抑制背景噪声和视角变化影响，整体基于EVA02骨干进行特征提取与融合。

Result: 在多个基准数据集（University-1652、SUES-200、Dense-UAV）上的实验表明，MFAF在无人机定位与导航任务中取得了竞争性表现。

Conclusion: 该文提出基于EVA02的多尺度频谱注意力融合（MFAF）方法，通过多频段分支块(MFB)和频谱感知空间注意(FSA)模块提高跨视角地理定位的特征鲁棒性与干扰抑制能力，实验在University-1652、SUES-200和Dense-UAV上达到了有竞争力的性能。

Abstract: Cross-view geo-localization aims to determine the geographical location of a
query image by matching it against a gallery of images. This task is
challenging due to the significant appearance variations of objects observed
from variable views, along with the difficulty in extracting discriminative
features. Existing approaches often rely on extracting features through feature
map segmentation while neglecting spatial and semantic information. To address
these issues, we propose the EVA02-based Multi-scale Frequency Attention Fusion
(MFAF) method. The MFAF method consists of Multi-Frequency Branch-wise Block
(MFB) and the Frequency-aware Spatial Attention (FSA) module. The MFB block
effectively captures both low-frequency structural features and high-frequency
edge details across multiple scales, improving the consistency and robustness
of feature representations across various viewpoints. Meanwhile, the FSA module
adaptively focuses on the key regions of frequency features, significantly
mitigating the interference caused by background noise and viewpoint
variability. Extensive experiments on widely recognized benchmarks, including
University-1652, SUES-200, and Dense-UAV, demonstrate that the MFAF method
achieves competitive performance in both drone localization and drone
navigation tasks.

</details>


### [36] [A Comparative Study of YOLOv8 to YOLOv11 Performance in Underwater Vision Tasks](https://arxiv.org/abs/2509.12682)
*Gordon Hung,Ivan Felipe Rodriguez*

Main category: cs.CV

TL;DR: 作者对YOLOv8–v11在两个水下数据集上进行受控对比，发现精度在v9后饱和但速度提升明显，推荐YOLOv10作为AUV嵌入式部署的首选。


<details>
  <summary>Details</summary>
Motivation: 评估近年YOLO变体在水下视觉任务中的表现，考虑水下成像挑战（光衰减、浑浊、类别不平衡）和AUV计算资源受限，以找出适合嵌入式部署的轻量检测器。

Method: 作者构建了两个水下数据集（珊瑚病害和鱼类），在四种训练数据量划分下训练并评估YOLOv8-s到YOLOv11-s，使用相同超参数（100轮，640px，batch=16，T4 GPU），评估指标包括精度、召回、mAP50、mAP50-95、单张推理时间和FPS，并用Grad-CAM做后验可视化。

Result: 在两个数据集上，精度指标在YOLOv9后无显著提升，但推理速度随着版本进步显著提高；YOLOv10-s在速度与精度间提供最佳折中，并发布了可复现的基准代码与数据集划分。

Conclusion: YOLO系列在水下图像上表现出精度在YOLOv9后趋于饱和，主要改进集中在效率上，因此YOLOv10在嵌入式AUV部署中在速度-精度折衷上表现最佳。

Abstract: Autonomous underwater vehicles (AUVs) increasingly rely on on-board
computer-vision systems for tasks such as habitat mapping, ecological
monitoring, and infrastructure inspection. However, underwater imagery is
hindered by light attenuation, turbidity, and severe class imbalance, while the
computational resources available on AUVs are limited. One-stage detectors from
the YOLO family are attractive because they fuse localization and
classification in a single, low-latency network; however, their terrestrial
benchmarks (COCO, PASCAL-VOC, Open Images) leave open the question of how
successive YOLO releases perform in the marine domain. We curate two openly
available datasets that span contrasting operating conditions: a Coral Disease
set (4,480 images, 18 classes) and a Fish Species set (7,500 images, 20
classes). For each dataset, we create four training regimes (25 %, 50 %, 75 %,
100 % of the images) while keeping balanced validation and test partitions
fixed. We train YOLOv8-s, YOLOv9-s, YOLOv10-s, and YOLOv11-s with identical
hyperparameters (100 epochs, 640 px input, batch = 16, T4 GPU) and evaluate
precision, recall, mAP50, mAP50-95, per-image inference time, and
frames-per-second (FPS). Post-hoc Grad-CAM visualizations probe feature
utilization and localization faithfulness. Across both datasets, accuracy
saturates after YOLOv9, suggesting architectural innovations primarily target
efficiency rather than accuracy. Inference speed, however, improves markedly.
Our results (i) provide the first controlled comparison of recent YOLO variants
on underwater imagery, (ii) show that lightweight YOLOv10 offers the best
speed-accuracy trade-off for embedded AUV deployment, and (iii) deliver an
open, reproducible benchmark and codebase to accelerate future marine-vision
research.

</details>


### [37] [StereoCarla: A High-Fidelity Driving Dataset for Generalizable Stereo](https://arxiv.org/abs/2509.12683)
*Xianda Guo,Chenming Zhang,Ruilin Wang,Youmin Zhang,Wenzhao Zheng,Matteo Poggi,Hao Zhao,Qin Zou,Long Chen*

Main category: cs.CV

TL;DR: StereoCarla是基于CARLA的高保真、多样化合成立体数据集，能显著提升立体匹配模型的跨域泛化能力，并兼容多数据集合并训练。


<details>
  <summary>Details</summary>
Motivation: 现有学习型立体匹配方法受限于训练数据多样性，导致泛化能力不足。通过构建多样化、可控的高保真合成数据集，旨在改善模型在真实场景下的泛化性。

Method: 基于CARLA模拟器构建大规模合成立体数据集，涵盖多种相机配置（基线、视角、传感器位置）和环境条件（光照、天气、道路几何），并在四个真实数据集上进行交叉域评估；比较了与11个现有数据集的训练效果，并测试了多数据集合并训练的效果。

Result: 在KITTI2012、KITTI2015、Middlebury、ETH3D等基准上，使用StereoCarla训练的模型在跨域泛化精度上优于基于11个现有数据集训练的模型；将StereoCarla加入多数据集训练能显著提升泛化精度。

Conclusion: StereoCarla通过高保真合成数据提升了在多基准上的跨域泛化性能，能作为训练或混合训练的数据源以增强立体匹配模型的鲁棒性。

Abstract: Stereo matching plays a crucial role in enabling depth perception for
autonomous driving and robotics. While recent years have witnessed remarkable
progress in stereo matching algorithms, largely driven by learning-based
methods and synthetic datasets, the generalization performance of these models
remains constrained by the limited diversity of existing training data. To
address these challenges, we present StereoCarla, a high-fidelity synthetic
stereo dataset specifically designed for autonomous driving scenarios. Built on
the CARLA simulator, StereoCarla incorporates a wide range of camera
configurations, including diverse baselines, viewpoints, and sensor placements
as well as varied environmental conditions such as lighting changes, weather
effects, and road geometries. We conduct comprehensive cross-domain experiments
across four standard evaluation datasets (KITTI2012, KITTI2015, Middlebury,
ETH3D) and demonstrate that models trained on StereoCarla outperform those
trained on 11 existing stereo datasets in terms of generalization accuracy
across multiple benchmarks. Furthermore, when integrated into multi-dataset
training, StereoCarla contributes substantial improvements to generalization
accuracy, highlighting its compatibility and scalability. This dataset provides
a valuable benchmark for developing and evaluating stereo algorithms under
realistic, diverse, and controllable settings, facilitating more robust depth
perception systems for autonomous vehicles. Code can be available at
https://github.com/XiandaGuo/OpenStereo, and data can be available at
https://xiandaguo.net/StereoCarla.

</details>


### [38] [SmokeBench: A Real-World Dataset for Surveillance Image Desmoking in Early-Stage Fire Scenes](https://arxiv.org/abs/2509.12701)
*Wenzhuo Jin,Qianfeng Yang,Xianhao Wu,Hongming Chen,Pengpeng Li,Xiang Chen*

Main category: cs.CV

TL;DR: 发布了包含精确对齐真实监控烟雾/无烟图像对的SmokeBench数据集，并对多种去烟方法进行了基准评估，推动真实火场图像去烟研究。


<details>
  <summary>Details</summary>
Motivation: 早期火灾（点火后0-15分钟）烟雾严重降低监控可见度，影响应急响应；现有去烟研究受限于缺乏大规模真实配对数据集，阻碍了监督学习和严谨评估。

Method: 作者采集并整理了大规模真实监控场景的成对图像（有烟/无烟），保证配准精确，涵盖多种场景设置和烟雾浓度；基于该数据集对多种现有去烟方法进行了全面基准测试。

Result: 构建并公开了SmokeBench数据集，提供精确对齐的有烟-无烟图像对；通过基准测试展示了现有方法在该真实场景下的性能表现，为后续算法改进提供参考。

Conclusion: 本文提出了一个面向真实监控火场的去烟基准数据集SmokeBench，为推动实用的图像去烟研究提供了基础资源。

Abstract: Early-stage fire scenes (0-15 minutes after ignition) represent a crucial
temporal window for emergency interventions. During this stage, the smoke
produced by combustion significantly reduces the visibility of surveillance
systems, severely impairing situational awareness and hindering effective
emergency response and rescue operations. Consequently, there is an urgent need
to remove smoke from images to obtain clear scene information. However, the
development of smoke removal algorithms remains limited due to the lack of
large-scale, real-world datasets comprising paired smoke-free and
smoke-degraded images. To address these limitations, we present a real-world
surveillance image desmoking benchmark dataset named SmokeBench, which contains
image pairs captured under diverse scenes setup and smoke concentration. The
curated dataset provides precisely aligned degraded and clean images, enabling
supervised learning and rigorous evaluation. We conduct comprehensive
experiments by benchmarking a variety of desmoking methods on our dataset. Our
dataset provides a valuable foundation for advancing robust and practical image
desmoking in real-world fire scenes. This dataset has been released to the
public and can be downloaded from https://github.com/ncfjd/SmokeBench.

</details>


### [39] [RIS-FUSION: Rethinking Text-Driven Infrared and Visible Image Fusion from the Perspective of Referring Image Segmentation](https://arxiv.org/abs/2509.12710)
*Siju Ma,Changsiyu Gong,Xiaofeng Fan,Yong Ma,Chengjie Jiang*

Main category: cs.CV

TL;DR: 作者提出将指称图像分割与文本驱动红外-可见光图像融合联合优化的RIS-FUSION，核心为LangGatedFusion模块，并发布MM-RIS数据集，实验显示在mIoU上显著优于现有方法（>11%）。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动图像融合缺乏目标对齐的监督/评估任务；RIS与文本驱动融合的共同目标是突出文本所指目标，因此将两者联合可提供明确的监督信号以提升融合质量。

Method: 提出RIS-FUSION框架，核心是LangGatedFusion模块，该模块将文本特征注入融合主干以增强语义对齐；同时采用级联结构联合训练融合与RIS；并构建MM-RIS数据集用于多模态指称分割训练与评估。

Result: 在新构建的MM-RIS基准上，RIS-FUSION在mIoU上比现有方法高出超过11%，并在多项定性/定量指标上表现领先。

Conclusion: 该论文通过将文本驱动的红外-可见光图像融合与指称图像分割（RIS）任务联合优化，提出了一种级联框架RIS-FUSION，使多模态文本更有效地引导融合过程，从而提升语义对齐与目标突出能力。

Abstract: Text-driven infrared and visible image fusion has gained attention for
enabling natural language to guide the fusion process. However, existing
methods lack a goal-aligned task to supervise and evaluate how effectively the
input text contributes to the fusion outcome. We observe that referring image
segmentation (RIS) and text-driven fusion share a common objective:
highlighting the object referred to by the text. Motivated by this, we propose
RIS-FUSION, a cascaded framework that unifies fusion and RIS through joint
optimization. At its core is the LangGatedFusion module, which injects textual
features into the fusion backbone to enhance semantic alignment. To support
multimodal referring image segmentation task, we introduce MM-RIS, a
large-scale benchmark with 12.5k training and 3.5k testing triplets, each
consisting of an infrared-visible image pair, a segmentation mask, and a
referring expression. Extensive experiments show that RIS-FUSION achieves
state-of-the-art performance, outperforming existing methods by over 11% in
mIoU. Code and dataset will be released at
https://github.com/SijuMa2003/RIS-FUSION.

</details>


### [40] [Learning by Imagining: Debiased Feature Augmentation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2509.12711)
*Haozhe Zhang,Chenchen Jing,Mingyu Liu,Qingsheng Wang,Hao Chen*

Main category: cs.CV

TL;DR: DeFA通过解耦-重构的特征扩增加去偏策略，利用seen原语合成高保真组合特征，有效提升CZSL中未见组合的识别性能，并在多个数据集上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 受到神经科学中想象与知觉共享神经过程的启发，利用已有Seen原语（属性、对象）合成新组合特征，解决属性与对象纠缠以及长尾分布导致的泛化困难。

Method: 提出Debiased Feature Augmentation（DeFA），采用先解耦属性与对象特征再重构合成高保真组合特征的框架，同时引入去偏机制缓解长尾分布和偏置问题，以增强合成特征的多样性与可辨识性。

Result: 在三个常用数据集的闭世界与开世界评测中，DeFA达到或超过现有最先进方法的性能，表现出更好的泛化与鲁棒性。

Conclusion: DeFA通过可分离重构与去偏策略相结合，有效提升了在CZSL任务中对未见属性-对象组合的泛化能力。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen
attribute-object compositions by learning prior knowledge of seen primitives,
\textit{i.e.}, attributes and objects. Learning generalizable compositional
representations in CZSL remains challenging due to the entangled nature of
attributes and objects as well as the prevalence of long-tailed distributions
in real-world data. Inspired by neuroscientific findings that imagination and
perception share similar neural processes, we propose a novel approach called
Debiased Feature Augmentation (DeFA) to address these challenges. The proposed
DeFA integrates a disentangle-and-reconstruct framework for feature
augmentation with a debiasing strategy. DeFA explicitly leverages the prior
knowledge of seen attributes and objects by synthesizing high-fidelity
composition features to support compositional generalization. Extensive
experiments on three widely used datasets demonstrate that DeFA achieves
state-of-the-art performance in both \textit{closed-world} and
\textit{open-world} settings.

</details>


### [41] [AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models](https://arxiv.org/abs/2509.12715)
*Heng Zhang,Haichuan Hu,Yaomin Shen,Weihao Yu,Yilei Yuan,Haochen You,Guo Cheng,Zijian Zhang,Lubin Gan,Huihui Wei,Hao Zhang,Jin Huang*

Main category: cs.CV

TL;DR: 提出AsyMoE，通过三类专门化专家应对视觉-语言模态不对称性，显著提高了多模态任务性能并降低了激活参数量。


<details>
  <summary>Details</summary>
Motivation: 现有MoE在视觉-语言任务中受模态不对称性影响：视觉为空间完整信息、语言为序列上下文信息，导致难以兼顾模态特定特征与跨模互融，且语言专家在深层会丧失对输入证据的依赖。为此需要一种能显式建模不对称性并抑制语言专家参数化偏置的新型MoE架构。

Method: 通过系统分析语言专家在深层逐步丧失上下文记忆并依赖参数化知识的现象，作者设计AsyMoE：1）模态内专家处理视觉或语言的模态特定特征；2）超曲率（hyperbolic）跨模态专家用于分层次的跨模信息融合；3）证据优先语言专家用于抑制参数偏置、保持语言上下文对视觉证据的依赖。实现上在模型架构中为不同层/部分分配对应专家组并采用选择性激活策略以减少激活参数量。

Result: 实验显示AsyMoE相比于普通（vanilla）MoE和仅模态专用的MoE，分别提升了26.58%和15.45%准确率，并在参数激活方面比密集模型减少了25.45%。

Conclusion: AsyMoE提出了一种针对视觉-语言模型中模态不对称性的混合专家架构，通过三类专门化专家（模态内专家、超曲率跨模态专家、证据优先语言专家）平衡模态专属处理与跨模互动，从而抑制语言专家在深层依赖参数知识的问题，并提升多模态理解性能。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on multimodal tasks through scaled architectures and extensive training.
However, existing Mixture of Experts (MoE) approaches face challenges due to
the asymmetry between visual and linguistic processing. Visual information is
spatially complete, while language requires maintaining sequential context. As
a result, MoE models struggle to balance modality-specific features and
cross-modal interactions. Through systematic analysis, we observe that language
experts in deeper layers progressively lose contextual grounding and rely more
on parametric knowledge rather than utilizing the provided visual and
linguistic information. To address this, we propose AsyMoE, a novel
architecture that models this asymmetry using three specialized expert groups.
We design intra-modality experts for modality-specific processing, hyperbolic
inter-modality experts for hierarchical cross-modal interactions, and
evidence-priority language experts to suppress parametric biases and maintain
contextual grounding. Extensive experiments demonstrate that AsyMoE achieves
26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific
MoE respectively, with 25.45% fewer activated parameters than dense models.

</details>


### [42] [EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer](https://arxiv.org/abs/2509.12718)
*Pukun Zhao,Longxiang Wang,Miaowei Wang,Chen Chen,Fanqing Zhou,Haojian Huang*

Main category: cs.CV

TL;DR: 提出两个动态、局部可观测的空间推理基准与主观体验记忆机制，揭示模型在长期推理与记忆上的不足并提供评估平台。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理基准多为静态或全局可观测，不能反映部分可观测、动态变化下的长期规划与记忆利用难题；需要新的基准来系统考察模型在局部感知与环境反馈紧耦合情形下的能力。

Method: 构建两个动态基准环境，每次动作引发环境结构变化，要求模型持续更新认知与策略；设计主观体验驱动的记忆机制用于经验提取与跨任务迁移；通过实验评估主流模型在这两个任务上的表现，分析其限点。

Result: 基准揭示了主流模型在动态空间推理和长期记忆方面的局限，主观体验记忆机制在跨任务迁移方面提供一定改进，但仍有改进空间；公开了代码与数据以支持后续研究。

Conclusion: 论文提出了两个动态空间推理基准（局部可观测的迷宫导航和Match-2消除），强调在局部感知、环境反馈和全局目标耦合下的长期推理与记忆挑战，并提出基于主观体验的记忆机制用于跨任务经验迁移与验证。实验显示主流模型在动态空间推理与长期记忆上存在关键局限，为未来方法学提供了测试平台。

Abstract: Most existing spatial reasoning benchmarks focus on static or globally
observable environments, failing to capture the challenges of long-horizon
reasoning and memory utilization under partial observability and dynamic
changes. We introduce two dynamic spatial benchmarks, locally observable maze
navigation and match-2 elimination that systematically evaluate models'
abilities in spatial understanding and adaptive planning when local perception,
environment feedback, and global objectives are tightly coupled. Each action
triggers structural changes in the environment, requiring continuous update of
cognition and strategy. We further propose a subjective experience-based memory
mechanism for cross-task experience transfer and validation. Experiments show
that our benchmarks reveal key limitations of mainstream models in dynamic
spatial reasoning and long-term memory, providing a comprehensive platform for
future methodological advances. Our code and data are available at
https://anonymous.4open.science/r/EvoEmpirBench-143C/.

</details>


### [43] [SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation](https://arxiv.org/abs/2509.12721)
*Jingdong Zhang,Weikai Chen,Yuan Liu,Jionghao Wang,Zhengming Yu,Zhuowen Shen,Bo Yang,Wenping Wang,Xin Li*

Main category: cs.CV

TL;DR: 将几何展开为多层球面投影图像，利用2D扩散先验得到一致、灵活且高效的单视图3D生成方法，显著提升几何质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有单视图3D生成依赖多视图扩散先验，存在视角不一致、无法表示复杂内部结构和非平凡拓扑的缺陷；需要一种能在图像域内高效且一致地编码完整几何信息的方法。

Method: 提出一种将三维几何编码为注入性（injective）球面映射的多层2D图像表示，利用图像域的2D扩散先验进行训练与微调，支持从单视图生成闭合或开放表面并表现嵌套内部结构。

Result: 实验表明SPGen在几何质量和计算效率上显著优于现有基线方法，能更好地保持视图一致性、表示内部结构并支持高效微调。

Conclusion: SPGen通过将几何信息投影到包围球并展开为多层球面投影(SP)图像表示，解决了单视图3D生成中多视图不一致和复杂拓扑重建问题，最终可生成更一致、更灵活且更高效的三维模型。

Abstract: Existing single-view 3D generative models typically adopt multiview diffusion
priors to reconstruct object surfaces, yet they remain prone to inter-view
inconsistencies and are unable to faithfully represent complex internal
structure or nontrivial topologies. In particular, we encode geometry
information by projecting it onto a bounding sphere and unwrapping it into a
compact and structural multi-layer 2D Spherical Projection (SP) representation.
Operating solely in the image domain, SPGen offers three key advantages
simultaneously: (1) Consistency. The injective SP mapping encodes surface
geometry with a single viewpoint which naturally eliminates view inconsistency
and ambiguity; (2) Flexibility. Multi-layer SP maps represent nested internal
structures and support direct lifting to watertight or open 3D surfaces; (3)
Efficiency. The image-domain formulation allows the direct inheritance of
powerful 2D diffusion priors and enables efficient finetuning with limited
computational resources. Extensive experiments demonstrate that SPGen
significantly outperforms existing baselines in geometric quality and
computational efficiency.

</details>


### [44] [Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models](https://arxiv.org/abs/2509.12724)
*Yunhan Zhao,Xiang Zheng,Xingjun Ma*

Main category: cs.CV

TL;DR: 引入弱防御作为引导，结合图像对抗扰动、带防御风格的文本优化和红队后缀生成，Defense2Attack显著提升了对VLM的绕过效果和效率，在多模型多基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM绕过方法虽有进展，但在有效性和效率上仍有改进空间。作者发现将弱防御策略反向利用可以提升绕过性能，从而提出Defense2Attack。

Method: Defense2Attack包括三部分：1) 视觉优化器，在图像中嵌入带有肯定和鼓励语义的通用对抗扰动；2) 文本优化器，使用防御风格的提示词来精炼输入；3) 红队后缀生成器，通过强化微调生成增强绕过效果的后缀。

Result: 在四个VLM模型和四个安全基准上的实验证明，Defense2Attack在单次尝试中就能实现优越的绕过效果，超过了通常需要多次尝试的最先进攻击方法。

Conclusion: 本文提出了一种新颖的绕过视觉-语言模型(VLMs)安全防护的攻击方法Defense2Attack，展示了将弱防御融入攻击流程能显著提升绕过效果和效率的现象。

Abstract: Despite their superb capabilities, Vision-Language Models (VLMs) have been
shown to be vulnerable to jailbreak attacks. While recent jailbreaks have
achieved notable progress, their effectiveness and efficiency can still be
improved. In this work, we reveal an interesting phenomenon: incorporating weak
defense into the attack pipeline can significantly enhance both the
effectiveness and the efficiency of jailbreaks on VLMs. Building on this
insight, we propose Defense2Attack, a novel jailbreak method that bypasses the
safety guardrails of VLMs by leveraging defensive patterns to guide jailbreak
prompt design. Specifically, Defense2Attack consists of three key components:
(1) a visual optimizer that embeds universal adversarial perturbations with
affirmative and encouraging semantics; (2) a textual optimizer that refines the
input using a defense-styled prompt; and (3) a red-team suffix generator that
enhances the jailbreak through reinforcement fine-tuning. We empirically
evaluate our method on four VLMs and four safety benchmarks. The results
demonstrate that Defense2Attack achieves superior jailbreak performance in a
single attempt, outperforming state-of-the-art attack methods that often
require multiple tries. Our work offers a new perspective on jailbreaking VLMs.

</details>


### [45] [Effective Gaussian Management for High-fidelity Object Reconstruction](https://arxiv.org/abs/2509.12742)
*Jiateng Liu,Hao Gao,Jiu-Cheng Xie,Chi-Man Pun,Jian Xiong,Haolun Li,Feng Xu*

Main category: cs.CV

TL;DR: 提出一种模型无关的高斯管理方法，通过密度化策略与轻量化表示显著提高重建精度并减少参数。


<details>
  <summary>Details</summary>
Motivation: 当前高斯Splatting方法在属性分配上不加区分，导致双重监督下梯度冲突，影响重建质量与效率。

Method: 引入表面重建模块监督下的密度化策略，动态为高斯激活球面谐波或法线；基于梯度幅值自适应调整每个高斯的SH阶数，并进行任务解耦裁剪以删除对重建影响小的高斯。

Result: 在多项实验中，所提方法在重建质量和参数效率上均优于现有最先进方法，且可与其他框架无缝结合以减少模型体积。

Conclusion: 该论文通过动态密度化策略和轻量化高斯表示，有效提升了基于高斯的高保真重建质量与效率。

Abstract: This paper proposes an effective Gaussian management approach for
high-fidelity object reconstruction. Departing from recent Gaussian Splatting
(GS) methods that employ indiscriminate attribute assignment, our approach
introduces a novel densification strategy that dynamically activates spherical
harmonics (SHs) or normals under the supervision of a surface reconstruction
module, which effectively mitigates the gradient conflicts caused by dual
supervision and achieves superior reconstruction results. To further improve
representation efficiency, we develop a lightweight Gaussian representation
that adaptively adjusts the SH orders of each Gaussian based on gradient
magnitudes and performs task-decoupled pruning to remove Gaussian with minimal
impact on a reconstruction task without sacrificing others, which balances the
representational capacity with parameter quantity. Notably, our management
approach is model-agnostic and can be seamlessly integrated into other
frameworks, enhancing performance while reducing model size. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art approaches in both reconstruction quality and efficiency,
achieving superior performance with significantly fewer parameters.

</details>


### [46] [Modelling and analysis of the 8 filters from the "master key filters hypothesis" for depthwise-separable deep networks in relation to idealized receptive fields based on scale-space theory](https://arxiv.org/abs/2509.12746)
*Tony Lindeberg,Zahra Babaiee,Peyman M. Kiasari*

Main category: cs.CV

TL;DR: 通过方差度量、簇分析与离散高斯+差分模型，证明深度可分离网络中的卷积核可用离散尺度空间滤波器良好近似，替换后性能无显著下降。


<details>
  <summary>Details</summary>
Motivation: 验证深度可分离卷积网络中学习到的感受野是否具有可解释的尺度空间结构，并探索是否可用简洁的离散尺度空间模型来逼近并替换学习到的滤波器，从而提高可解释性与模型简化。

Method: 计算滤波器的加权均值和加权方差以量化空间扩展；基于簇聚类得到8个“主键滤波器”；用离散高斯核的平滑加差分算子建立两种参数化模型（一种允许各向异性尺度，一种为各向同性）；通过匹配空间方差或最小化离散l1/l2范数来拟合模型参数；在网络中用理想化滤波器替换并评估预测性能。

Result: 实验显示：所提理想化模型在定性上与学习到的滤波器高度相似；参数化模型能以匹配空间方差或最小化l1/l2误差的方式很好拟合；将学习滤波器替换为理想化滤波器后，网络性能保持良好，表明这些滤波器可被离散尺度空间滤波器近似。

Conclusion: 作者得出结论：深度可分离卷积网络中学习到的滤波器可由离散尺度空间（Gaussian离散近似）上的差分算子与平滑操作组合的理想化模型良好近似，且替换为这些理想化滤波器不会明显损害网络性能。

Abstract: This paper presents the results of analysing and modelling a set of 8
``master key filters'', which have been extracted by applying a clustering
approach to the receptive fields learned in depthwise-separable deep networks
based on the ConvNeXt architecture.
  For this purpose, we first compute spatial spread measures in terms of
weighted mean values and weighted variances of the absolute values of the
learned filters, which support the working hypotheses that: (i) the learned
filters can be modelled by separable filtering operations over the spatial
domain, and that (ii) the spatial offsets of the those learned filters that are
non-centered are rather close to half a grid unit. Then, we model the clustered
``master key filters'' in terms of difference operators applied to a spatial
smoothing operation in terms of the discrete analogue of the Gaussian kernel,
and demonstrate that the resulting idealized models of the receptive fields
show good qualitative similarity to the learned filters.
  This modelling is performed in two different ways: (i) using possibly
different values of the scale parameters in the coordinate directions for each
filter, and (ii) using the same value of the scale parameter in both coordinate
directions. Then, we perform the actual model fitting by either (i) requiring
spatial spread measures in terms of spatial variances of the absolute values of
the receptive fields to be equal, or (ii) minimizing the discrete $l_1$- or
$l_2$-norms between the idealized receptive field models and the learned
filters.
  Complementary experimental results then demonstrate the idealized models of
receptive fields have good predictive properties for replacing the learned
filters by idealized filters in depthwise-separable deep networks, thus showing
that the learned filters in depthwise-separable deep networks can be well
approximated by discrete scale-space filters.

</details>


### [47] [What Makes a Good Generated Image? Investigating Human and Multimodal LLM Image Preference Alignment](https://arxiv.org/abs/2509.12750)
*Rishab Parthasarathy,Jasmine Collins,Cory Stephenson*

Main category: cs.CV

TL;DR: 作者用可控合成图像和人类偏好数据比较了人类与多模态LLM在图像质量属性判断上的差异，发现LLM难以像人类一样捕捉属性间关系与判断复杂属性。


<details>
  <summary>Details</summary>
Motivation: 自动化评估生成式图像模型仍具挑战，研究旨在探明多模态LLM如何利用诸如风格、构图等对人类重要的概念来生成图像质量判断，并与人类判断做对比。

Method: 作者通过合成生成的图像对收集人类偏好数据，采用任务间相关性分析来理解人类在不同图像质量属性上的关系，并对LLM重复相同分析；还构建了对每个属性高度可控的合成数据集以逐一评估各属性的判别能力。

Result: 构建的数据集显示人类在审美、无伪影、解剖学准确性、构图正确性、对象一致性和风格等属性上判断一致性高且属性间相关性明显；但LLM对这些属性的关系建模较弱，且在某些属性（如解剖学准确性）上判别能力显著不足。

Conclusion: 本文比较了多模态大语言模型（LLM）与人类在图像质量评估上的差异，发现LLM在捕捉不同图像属性之间的关系及判断某些属性（如解剖学准确性）方面表现较弱。

Abstract: Automated evaluation of generative text-to-image models remains a challenging
problem. Recent works have proposed using multimodal LLMs to judge the quality
of images, but these works offer little insight into how multimodal LLMs make
use of concepts relevant to humans, such as image style or composition, to
generate their overall assessment. In this work, we study what attributes of an
image--specifically aesthetics, lack of artifacts, anatomical accuracy,
compositional correctness, object adherence, and style--are important for both
LLMs and humans to make judgments on image quality. We first curate a dataset
of human preferences using synthetically generated image pairs. We use
inter-task correlation between each pair of image quality attributes to
understand which attributes are related in making human judgments. Repeating
the same analysis with LLMs, we find that the relationships between image
quality attributes are much weaker. Finally, we study individual image quality
attributes by generating synthetic datasets with a high degree of control for
each axis. Humans are able to easily judge the quality of an image with respect
to all of the specific image quality attributes (e.g. high vs. low aesthetic
image), however we find that some attributes, such as anatomical accuracy, are
much more difficult for multimodal LLMs to learn to judge. Taken together,
these findings reveal interesting differences between how humans and multimodal
LLMs perceive images.

</details>


### [48] [Recurrent Cross-View Object Geo-Localization](https://arxiv.org/abs/2509.12757)
*Xiaohan Zhang,Si-Yuan Cao,Xiaokai Bai,Yiming Li,Zhangkai Shen,Zhe Wu,Xiaoxi Hu,Hui-liang Shen*

Main category: cs.CV

TL;DR: ReCOT通过可学习tokens的循环注意力、SAM蒸馏与RFEM，把跨视图目标地理定位改为可迭代精化的过程，提升精度且大幅减参。


<details>
  <summary>Details</summary>
Motivation: 现有一-shot方法对特征噪声敏感且缺乏纠错机制，难以精确定位目标。作者通过循环定位和知识蒸馏解决稳定性与语义引导问题。

Method: 引入可学习任务tokens进行迭代注意力定位；采用SAM蒸馏引入分割先验；设计参考特征增强模块(RFEM)通过层次注意力突出目标相关区域。

Result: 在标准CVOGL基准上达到SOTA性能，同时参数量比上一SOTA减少约60%。

Conclusion: 本文提出ReCOT，将CVOGL从一次性检测转为循环定位，通过可学习tokens与参考特征反复交互，显著提高定位精度并减少参数量。

Abstract: Cross-view object geo-localization (CVOGL) aims to determine the location of
a specific object in high-resolution satellite imagery given a query image with
a point prompt. Existing approaches treat CVOGL as a one-shot detection task,
directly regressing object locations from cross-view information aggregation,
but they are vulnerable to feature noise and lack mechanisms for error
correction. In this paper, we propose ReCOT, a Recurrent Cross-view Object
geo-localization Transformer, which reformulates CVOGL as a recurrent
localization task. ReCOT introduces a set of learnable tokens that encode
task-specific intent from the query image and prompt embeddings, and
iteratively attend to the reference features to refine the predicted location.
To enhance this recurrent process, we incorporate two complementary modules:
(1) a SAM-based knowledge distillation strategy that transfers segmentation
priors from the Segment Anything Model (SAM) to provide clearer semantic
guidance without additional inference cost, and (2) a Reference Feature
Enhancement Module (RFEM) that introduces a hierarchical attention to emphasize
object-relevant regions in the reference features. Extensive experiments on
standard CVOGL benchmarks demonstrate that ReCOT achieves state-of-the-art
(SOTA) performance while reducing parameters by 60% compared to previous SOTA
approaches.

</details>


### [49] [A-TDOM: Active TDOM via On-the-Fly 3DGS](https://arxiv.org/abs/2509.12759)
*Yiwei Xu,Xiang Wang,Yifei Yu,Wentian Gan,Luca Morelli,Giulio Perda,Xiongwu Xiao,Zongqian Zhan,Xin Wang,Fabio Remondino*

Main category: cs.CV

TL;DR: A-TDOM通过在线SfM、增量3D高斯场优化与正交splatting，实现了秒级更新的近实时True Digital Orthophoto Map生成，兼顾渲染质量与几何准确性。


<details>
  <summary>Details</summary>
Motivation: 传统TDOM生成依赖复杂的离线摄影测量流程，导致延迟且易受相机位姿、DSM误差与遮挡影响；因此需要一种能即时更新并保持几何与影像质量的近实时方案。

Method: 方法基于On-the-Fly SfM获取每张图像的相机位姿和稀疏点云，向3D高斯场中逐步加入新的高斯并在此前未重建或粗糙重建区域进行优化；结合正交splatting渲染策略，在每次3DGS更新后即可渲染TDOM。

Result: 在多个基准数据集上的初步实验显示，A-TDOM能在秒级内对每张新增图像完成3DGS优化并主动渲染TDOM，同时保持可接受的渲染质量与几何精度。

Conclusion: 本文提出的A-TDOM实现了近实时的TDOM生成，能够在每张新图像到达后通过在线3D高斯场（3DGS）优化和正交投影即时渲染，减小传统离线流程的延迟。

Abstract: True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product in
various fields such as urban management, city planning, land surveying, etc.
However, traditional TDOM generation methods generally rely on a complex
offline photogrammetric pipeline, resulting in delays that hinder real-time
applications. Moreover, the quality of TDOM may degrade due to various
challenges, such as inaccurate camera poses or Digital Surface Model (DSM) and
scene occlusions. To address these challenges, this work introduces A-TDOM, a
near real-time TDOM generation method based on On-the-Fly 3DGS optimization. As
each image is acquired, its pose and sparse point cloud are computed via
On-the-Fly SfM. Then new Gaussians are integrated and optimized into previously
unseen or coarsely reconstructed regions. By integrating with orthogonal
splatting, A-TDOM can render just after each update of a new 3DGS field.
Initial experiments on multiple benchmarks show that the proposed A-TDOM is
capable of actively rendering TDOM in near real-time, with 3DGS optimization
for each new image in seconds while maintaining acceptable rendering quality
and TDOM geometric accuracy.

</details>


### [50] [DyGLNet: Hybrid Global-Local Feature Fusion with Dynamic Upsampling for Medical Image Segmentation](https://arxiv.org/abs/2509.12763)
*Yican Zhao,Ce Wang,You Hao,Lei Li,Tianli Liao*

Main category: cs.CV

TL;DR: DyGLNet结合单头自注意力与多尺度膨胀卷积并使用可学习偏移的动态上采样，实现了高效且精确的医学影像分割，尤其在边界和小目标上表现优秀，同时计算量较低。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像分割中病灶尺度多变、组织边界模糊和计算量大的问题，提升边界和小目标的分割精度，同时保证推理效率以便临床应用。

Method: 提出SHDCBlock：将单头自注意力与多尺度膨胀卷积结合，用于协同建模局部细节与全局上下文；提出DyFusionUp：基于可学习偏移的动态自适应上采样，用于高保真特征图重建；整体采用轻量化设计以降低计算开销。

Result: 在七个公开数据集上的实验表明，DyGLNet在边界准确性和小目标分割上优于已有方法，并具有更低的计算复杂度。

Conclusion: DyGLNet通过融合全局与局部特征并引入动态上采样，在多项医学影像分割任务上实现了更高的边界精度和小目标分割性能，同时保持较低计算复杂度，适合临床应用场景。

Abstract: Medical image segmentation grapples with challenges including multi-scale
lesion variability, ill-defined tissue boundaries, and computationally
intensive processing demands. This paper proposes the DyGLNet, which achieves
efficient and accurate segmentation by fusing global and local features with a
dynamic upsampling mechanism. The model innovatively designs a hybrid feature
extraction module (SHDCBlock), combining single-head self-attention and
multi-scale dilated convolutions to model local details and global context
collaboratively. We further introduce a dynamic adaptive upsampling module
(DyFusionUp) to realize high-fidelity reconstruction of feature maps based on
learnable offsets. Then, a lightweight design is adopted to reduce
computational overhead. Experiments on seven public datasets demonstrate that
DyGLNet outperforms existing methods, particularly excelling in boundary
accuracy and small-object segmentation. Meanwhile, it exhibits lower
computation complexity, enabling an efficient and reliable solution for
clinical medical image analysis. The code will be made available soon.

</details>


### [51] [BATR-FST: Bi-Level Adaptive Token Refinement for Few-Shot Transformers](https://arxiv.org/abs/2509.12768)
*Mohammed Al-Habib,Zuping Zhang,Abdulrahman Noman*

Main category: cs.CV

TL;DR: 提出BATR-FST：结合MIM预训练与双层自适应令牌精炼（聚类、加权、双层注意力、图传播、类分离惩罚），显著改善ViT在少样本分类中的表现。


<details>
  <summary>Details</summary>
Motivation: 动机是克服ViT在少样本学习中令牌级互动难以细化、训练样本有限和归纳偏置不足的问题；现有方法受限于刚性令牌匹配或简单相似度度量，难以同时融合全局语境与局部特征精炼。

Method: 方法包括两阶段：1) 预训练阶段使用MIM重建被遮挡的图像区域以学习补丁级特征；2) 元微调阶段的BATR模块包含令牌聚类（捕捉局部交互）、不确定性感知加权（优先可信特征）、双层注意力（平衡簇内与簇间关系）以及图令牌传播（保证支撑与查询间语义一致性）和类分离惩罚（强化判别边界）。

Result: 在三个基准少样本数据集上的大量实验表明，BATR-FST在1-shot和5-shot情形下均优于对比方法，显著提升了基于Transformer的少样本分类性能。

Conclusion: 该论文通过在预训练阶段引入掩码图像建模(MIM)获得可迁移的补丁级表示，在元微调阶段提出双层自适应令牌精炼模块(Bi-Level Adaptive Token Refinement，BATR)，结合令牌聚类、不确定性感知令牌加权、双层注意力、图令牌传播与类间分离惩罚，逐步改善ViT在少样本场景下的令牌表示，从而提升少样本分类性能。

Abstract: Vision Transformers (ViTs) have shown significant promise in computer vision
applications. However, their performance in few-shot learning is limited by
challenges in refining token-level interactions, struggling with limited
training data, and developing a strong inductive bias. Existing methods often
depend on inflexible token matching or basic similarity measures, which limit
the effective incorporation of global context and localized feature refinement.
To address these challenges, we propose Bi-Level Adaptive Token Refinement for
Few-Shot Transformers (BATR-FST), a two-stage approach that progressively
improves token representations and maintains a robust inductive bias for
few-shot classification. During the pre-training phase, Masked Image Modeling
(MIM) provides Vision Transformers (ViTs) with transferable patch-level
representations by recreating masked image regions, providing a robust basis
for subsequent adaptation. In the meta-fine-tuning phase, BATR-FST incorporates
a Bi-Level Adaptive Token Refinement module that utilizes Token Clustering to
capture localized interactions, Uncertainty-Aware Token Weighting to prioritize
dependable features, and a Bi-Level Attention mechanism to balance
intra-cluster and inter-cluster relationships, thereby facilitating thorough
token refinement. Furthermore, Graph Token Propagation ensures semantic
consistency between support and query instances, while a Class Separation
Penalty preserves different class borders, enhancing discriminative capability.
Extensive experiments on three benchmark few-shot datasets demonstrate that
BATR-FST achieves superior results in both 1-shot and 5-shot scenarios and
improves the few-shot classification via transformers.

</details>


### [52] [CECT-Mamba: a Hierarchical Contrast-enhanced-aware Model for Pancreatic Tumor Subtyping from Multi-phase CECT](https://arxiv.org/abs/2509.12777)
*Zhifang Gong,Shuo Gao,Ben Zhao,Yingjing Xu,Yijun Yang,Shenghong Ju,Guangquan Zhou*

Main category: cs.CV

TL;DR: 提出基于Mamba模块的多期CECT时空建模与多尺度融合方法，有效利用相位内/间对比信息，在胰腺肿瘤PDAC/PNETs区分任务上表现优异（Acc 97.4%, AUC 98.6%）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用多期CECT的时空上下文信息方面不足，而放射科医师通常通过多期影像对比判断肿瘤亚型，因此提出一种能够自动结合多期CECT数据、同时建模时空对比信息的方法以提高胰腺肿瘤亚型诊断的准确性。

Method: 核心方法为引入双重层次的对比增强感知Mamba模块，包含两种新颖的空间和时间采样序列以捕获相位内与相位间的对比变化；在时间建模中加入相似性引导的精炼模块以强化对局部肿瘤区域的时序变化学习；并设计空间互补整合器与多粒度融合模块对不同尺度语义进行编码和聚合。

Result: 在270例临床病例的内部数据集上进行验证，模型在PDAC与PNETs二分类任务上达到97.4%的准确率和98.6%的AUC，表明方法在区分这两类胰腺肿瘤方面具有较高的性能。

Conclusion: 该论文提出了一种针对多期增强CT（CECT）图像进行胰腺肿瘤亚型诊断的自动化方法，利用设计的Mamba模块及多层次融合策略，有效挖掘跨期时空上下文信息，从而在区分PDAC与PNETs上取得高性能，显示出较高的实用潜力。

Abstract: Contrast-enhanced computed tomography (CECT) is the primary imaging technique
that provides valuable spatial-temporal information about lesions, enabling the
accurate diagnosis and subclassification of pancreatic tumors. However, the
high heterogeneity and variability of pancreatic tumors still pose substantial
challenges for precise subtyping diagnosis. Previous methods fail to
effectively explore the contextual information across multiple CECT phases
commonly used in radiologists' diagnostic workflows, thereby limiting their
performance. In this paper, we introduce, for the first time, an automatic way
to combine the multi-phase CECT data to discriminate between pancreatic tumor
subtypes, among which the key is using Mamba with promising learnability and
simplicity to encourage both temporal and spatial modeling from multi-phase
CECT. Specifically, we propose a dual hierarchical contrast-enhanced-aware
Mamba module incorporating two novel spatial and temporal sampling sequences to
explore intra and inter-phase contrast variations of lesions. A
similarity-guided refinement module is also imposed into the temporal scanning
modeling to emphasize the learning on local tumor regions with more obvious
temporal variations. Moreover, we design the space complementary integrator and
multi-granularity fusion module to encode and aggregate the semantics across
different scales, achieving more efficient learning for subtyping pancreatic
tumors. The experimental results on an in-house dataset of 270 clinical cases
achieve an accuracy of 97.4% and an AUC of 98.6% in distinguishing between
pancreatic ductal adenocarcinoma (PDAC) and pancreatic neuroendocrine tumors
(PNETs), demonstrating its potential as a more accurate and efficient tool.

</details>


### [53] [Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection](https://arxiv.org/abs/2509.12784)
*Zhehao Li,Yucheng Qian,Chong Wang,Yinghao Lu,Zhihao Yang,Jiafei Wu*

Main category: cs.CV

TL;DR: 提出结合可供性三元组与可学习文本提示的上下文化表示学习网络，通过注意力融合语言与视觉信息，提升对工具依赖等复杂HOI的检测效果，在HICO-Det和V-COCO上取得优异结果。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法对上下文建模不足，难以处理依赖工具或复杂场景的交互，需引入更丰富的语义和功能信息来提高推理能力。

Method: 在传统两阶段HOI框架上扩展，引入三元组<human, tool, object>以显式建模工具或辅助实体的功能角色（affordance）；同时设计可学习的文本提示（包含实例类别），并通过注意力机制与全局和区域视觉特征融合，从而对齐语言与图像。

Result: 在HICO-Det和V-COCO数据集上，在大多数场景下优于现有方法，证明方法在捕捉上下文依赖交互方面有效。

Conclusion: 作者提出了一种将可供性引导的推理与基于提示的上下文表示相结合的HOI检测网络，以捕捉复杂交互关系。

Abstract: Human-Object Interaction (HOI) detection aims to simultaneously localize
human-object pairs and recognize their interactions. While recent two-stage
approaches have made significant progress, they still face challenges due to
incomplete context modeling. In this work, we introduce a Contextualized
Representation Learning Network that integrates both affordance-guided
reasoning and contextual prompts with visual cues to better capture complex
interactions. We enhance the conventional HOI detection framework by expanding
it beyond simple human-object pairs to include multivariate relationships
involving auxiliary entities like tools. Specifically, we explicitly model the
functional role (affordance) of these auxiliary objects through triplet
structures <human, tool, object>. This enables our model to identify
tool-dependent interactions such as 'filling'. Furthermore, the learnable
prompt is enriched with instance categories and subsequently integrated with
contextual visual features using an attention mechanism. This process aligns
language with image content at both global and regional levels. These
contextualized representations equip the model with enriched relational cues
for more reliable reasoning over complex, context-dependent interactions. Our
proposed method demonstrates superior performance on both the HICO-Det and
V-COCO datasets in most scenarios. Codes will be released upon acceptance.

</details>


### [54] [Double Helix Diffusion for Cross-Domain Anomaly Image Generation](https://arxiv.org/abs/2509.12787)
*Linchun Wu,Qin Zou,Xianbiao Qi,Bo Du,Zhongyuan Wang,Qingquan Li*

Main category: cs.CV

TL;DR: DH-Diff通过双螺旋生成架构与域解耦注意力实现高保真、结构一致的异常图像与掩码联合生成，显著提升合成数据质量与检测效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成异常样本方法中异常与背景结构不一致及图像与掩码特征纠缠问题，提升合成数据的多样性与真实性以增强下游检测器性能。

Method: 提出双螺旋结构的生成框架，包含特征分离、连接与融合模块；引入域解耦注意力以独立增强图像与掩码特征；以及语义分数图对齐模块用于保证异常前景与背景结构的一致性；支持文本与图形引导。

Result: 实验显示在多项评估中DH-Diff在样本多样性、感知真实性以及下游异常检测性能上显著优于现有方法。

Conclusion: 该论文提出的DH-Diff通过解耦图像与掩码特征、以及语义分数图对齐，有效提升了合成异常图像的结构一致性与感知真实性，从而带来更好的检测性能。

Abstract: Visual anomaly inspection is critical in manufacturing, yet hampered by the
scarcity of real anomaly samples for training robust detectors. Synthetic data
generation presents a viable strategy for data augmentation; however, current
methods remain constrained by two principal limitations: 1) the generation of
anomalies that are structurally inconsistent with the normal background, and 2)
the presence of undesirable feature entanglement between synthesized images and
their corresponding annotation masks, which undermines the perceptual realism
of the output. This paper introduces Double Helix Diffusion (DH-Diff), a novel
cross-domain generative framework designed to simultaneously synthesize
high-fidelity anomaly images and their pixel-level annotation masks, explicitly
addressing these challenges. DH-Diff employs a unique architecture inspired by
a double helix, cycling through distinct modules for feature separation,
connection, and merging. Specifically, a domain-decoupled attention mechanism
mitigates feature entanglement by enhancing image and annotation features
independently, and meanwhile a semantic score map alignment module ensures
structural authenticity by coherently integrating anomaly foregrounds. DH-Diff
offers flexible control via text prompts and optional graphical guidance.
Extensive experiments demonstrate that DH-Diff significantly outperforms
state-of-the-art methods in diversity and authenticity, leading to significant
improvements in downstream anomaly detection performance.

</details>


### [55] [Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation](https://arxiv.org/abs/2509.12791)
*Julien Walther,Rémi Giraud,Michaël Clément*

Main category: cs.CV

TL;DR: SPAM将学得的特征与大规模预训练语义无关分割相结合，生成既准确又规则的超像素，支持任意先验和交互式应用，并在实验中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统超像素方法依赖低级特征，深度学习方法虽用高层特征但牺牲规则性；因此需要兼顾准确性与规则性的超像素方法。

Method: 训练一个特征提取模型用于超像素生成，并在推理阶段结合大规模预训练的语义无关分割模型（如SAM）来对齐目标掩码；能处理任意先验高层分割，解决不确定区域，并支持交互式关注特定对象。

Result: 在多项定性和定量实验中，SPAM优于现有最先进方法，在分割任务上表现更好；代码与预训练模型已公开。

Conclusion: SPAM提出了一种在保持超像素规则性的同时利用高层语义信息提高分割准确性的框架。

Abstract: Superpixels are widely used in computer vision to simplify image
representation and reduce computational complexity. While traditional methods
rely on low-level features, deep learning-based approaches leverage high-level
features but also tend to sacrifice regularity of superpixels to capture
complex objects, leading to accurate but less interpretable segmentations. In
this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework
for segmenting images into accurate yet regular superpixels. We train a model
to extract image features for superpixel generation, and at inference, we
leverage a large-scale pretrained model for semantic-agnostic segmentation to
ensure that superpixels align with object masks. SPAM can handle any prior
high-level segmentation, resolving uncertainty regions, and is able to
interactively focus on specific objects. Comprehensive experiments demonstrate
that SPAM qualitatively and quantitatively outperforms state-of-the-art methods
on segmentation tasks, making it a valuable and robust tool for various
applications. Code and pre-trained models are available here:
https://github.com/waldo-j/spam.

</details>


### [56] [Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation](https://arxiv.org/abs/2509.12815)
*Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D Studio 是一个整合多模块的 AI 平台，可从单张概念图或文本快速生成游戏级别的 3D 模型和 PBR 纹理，兼顾视觉质量与引擎兼容性，显著加速游戏资产制作。


<details>
  <summary>Details</summary>
Motivation: 降低制作高质量 3D 资产的门槛和人力成本，缩短迭代周期，将创意快速转化为可用于实时引擎的技术性资产。

Method: 将若干先进神经模块（零部件级 3D 生成、拓扑/多边形生成、语义 UV 展开等）整合到统一管线，支持从概念输入到优化几何与 PBR 纹理输出的全流程自动化。

Result: 生成的资产在视觉质量和技术规范上均达到了现代游戏引擎的要求，有效减少了迭代时间并降低了 3D 内容创作的入门难度。

Conclusion: Hunyuan3D Studio 提出了一套端到端的 AI 驱动 3D 资产生成平台，能够把概念图或文本描述自动转化为符合游戏引擎要求的高质量 3D 模型，简化并加速游戏开发流程。

Abstract: The creation of high-quality 3D assets, a cornerstone of modern game
development, has long been characterized by labor-intensive and specialized
workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered
content creation platform designed to revolutionize the game production
pipeline by automating and streamlining the generation of game-ready 3D assets.
At its core, Hunyuan3D Studio integrates a suite of advanced neural modules
(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into
a cohesive and user-friendly system. This unified framework allows for the
rapid transformation of a single concept image or textual description into a
fully-realized, production-quality 3D model complete with optimized geometry
and high-fidelity PBR textures. We demonstrate that assets generated by
Hunyuan3D Studio are not only visually compelling but also adhere to the
stringent technical requirements of contemporary game engines, significantly
reducing iteration time and lowering the barrier to entry for 3D content
creation. By providing a seamless bridge from creative intent to technical
asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted
workflows in game development and interactive media.

</details>


### [57] [SAGA: Selective Adaptive Gating for Efficient and Expressive Linear Attention](https://arxiv.org/abs/2509.12817)
*Yuan Cao,Dong Wang*

Main category: cs.CV

TL;DR: SAGA通过输入自适应门控增强线性注意力的表达能力并配以无额外内存开销的Hadamard分解，兼顾效率与性能，在高分辨率任务中显著优于PVT-T并接近或超过softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力通过对历史KV进行均匀压缩导致特征冗余和与查询方向的不对齐，产生低秩KV特征图并导致性能落后于softmax注意力；因此需要一种既保留线性复杂度又能增强KV表达多样性的方法。

Method: 提出了可学习的选择性门控机制用于聚合历史KV信息，并设计了高效的Hadamard乘积分解方法以避免额外内存开销；在架构上将(Q(KV))的线性注意力与门控模块结合，实现在高分辨率下更高吞吐量和更低显存占用。

Result: 在1280×1280分辨率下，与PVT-T相比，吞吐量提升1.76×，峰值GPU显存降低2.69×；在ImageNet上Top-1精度最高提升4.4%。

Conclusion: SAGA通过输入自适应可学习门对KV信息进行选择性调制，提升了线性注意力的表达能力并缓解低秩问题，从而在保持线性复杂度的同时显著提高性能。

Abstract: While Transformer architecture excel at modeling long-range dependencies
contributing to its widespread adoption in vision tasks the quadratic
complexity of softmax-based attention mechanisms imposes a major bottleneck,
particularly when processing high-resolution images. Linear attention presents
a promising alternative by reformulating the attention computation from $(QK)V$
to $Q(KV)$, thereby reducing the complexity from $\mathcal{O}(N^2)$ to
$\mathcal{O}(N)$ while preserving the global receptive field. However, most
existing methods compress historical key-value (KV) information uniformly,
which can lead to feature redundancy and the loss of directional alignment with
the query (Q). This uniform compression results in low-rank $KV$ feature maps,
contributing to a performance gap compared to softmax attention. To mitigate
this limitation, we propose \textbf{S}elective \textbf{A}daptive
\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which
introduces input-adaptive learnable gates to selectively modulate information
aggregation into the $KV$ feature map. These gates enhance semantic diversity
and alleviate the low-rank constraint inherent in conventional linear
attention. Additionally, we propose an efficient Hadamard-product decomposition
method for gate computation, which introduces no additional memory overhead.
Experiments demonstrate that SAGA achieves a 1.76$\times$ improvement in
throughput and a 2.69$\times$ reduction in peak GPU memory compared to PVT-T at
a resolution of $1280 \times 1280$. Moreover, it improves top-1 accuracy by up
to 4.4\% on the ImageNet dataset, demonstrating both computational efficiency
and model effectiveness.

</details>


### [58] [Data Scaling Laws for Radiology Foundation Models](https://arxiv.org/abs/2509.12818)
*Maximilian Ilse,Harshita Sharma,Anton Schwaighofer,Sam Bond-Taylor,Fernando Pérez-García,Olesya Melnichenko,Anne-Marie G. Sykes,Kelly K. Horst,Ashish Khandelwal,Maxwell Reynolds,Maria T. Wetscherek,Noel C. F. Codella,Javier Alvarez-Valle,Korfiatis Panagiotis,Valentina Salvatelli*

Main category: cs.CV

TL;DR: 在同一计算与评估下，对两类视觉编码器在最多350万张胸片上持续预训练表明：不同编码器在任务上能力有明显分化，结构化监督能进一步提升性能，且少量域内数据即可超越公开模型，支持医疗机构通过中心化持续预训练获得显著收益。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉基础模型虽在自然图像任务迁移良好，但医疗影像领域数据量受限，对不同预训练范式与数据规模如何影响下游医疗任务的理解不足。作者旨在系统研究在巨量中心化胸片上对两类编码器持续预训练的效果，并评估是否能用相对较小的域内数据超越公开模型。

Method: 在单一机构的至多350万张胸片上，对两类视觉编码器（MI2与RAD-DINO）进行持续预训练，固定计算预算和评估协议。评估包含分类（放射学发现、导管/管线）、分割（导管/管线）和放射报告生成。比较使用不同监督信号（报告文本与结构化标签/UniCL）对MI2的影响，并分析样本规模对性能的作用。

Result: 主要发现包括：1) MI2在发现类任务上随数据规模更好扩展；2) RAD-DINO在管线/导管相关任务更强；3) 将报告与结构化标签结合用于MI2的持续预训练（UniCL）能进一步提升性能，表明结构化监督的价值；4) 对部分任务，仅需约30k域内样本即可超越公开预训练权重；5) 中心机构的持续预训练是一条可行且高效的提升路径。

Conclusion: 作者结论是：在胸部X光领域，中心机构的持续预训练（continual pretraining）可以显著提升不同下游任务的性能，尤其当利用结构化监督（如UniCL结合报告和标签）时效果更好。不同预训练范式表现差异明显：MI2（CLIP范式）对疾病发现任务更有利，RAD-DINO（DINOv2范式）则在导管/管线相关任务上更强。少量（如3万）高质量域内样本已足以超越公开权重基础模型。

Abstract: Foundation vision encoders such as CLIP and DINOv2, trained on web-scale
data, exhibit strong transfer performance across tasks and datasets. However,
medical imaging foundation models remain constrained by smaller datasets,
limiting our understanding of how data scale and pretraining paradigms affect
performance in this setting. In this work, we systematically study continual
pretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINO
representing the two major encoder paradigms CLIP and DINOv2, on up to 3.5M
chest x-rays from a single institution, holding compute and evaluation
protocols constant. We evaluate on classification (radiology findings, lines
and tubes), segmentation (lines and tubes), and radiology report generation.
While prior work has primarily focused on tasks related to radiology findings,
we include lines and tubes tasks to counterbalance this bias and evaluate a
model's ability to extract features that preserve continuity along elongated
structures. Our experiments show that MI2 scales more effectively for
finding-related tasks, while RAD-DINO is stronger on tube-related tasks.
Surprisingly, continually pretraining MI2 with both reports and structured
labels using UniCL improves performance, underscoring the value of structured
supervision at scale. We further show that for some tasks, as few as 30k
in-domain samples are sufficient to surpass open-weights foundation models.
These results highlight the utility of center-specific continual pretraining,
enabling medical institutions to derive significant performance gains by
utilizing in-domain data.

</details>


### [59] [Exploring Metric Fusion for Evaluation of NeRFs](https://arxiv.org/abs/2509.12836)
*Shreyas Shivakumara,Gabriel Eilertsen,Karljohan Lundin Palmerius*

Main category: cs.CV

TL;DR: 将DISTS与VMAF通过合适归一化与融合策略结合，可更可靠地评估NeRF输出，与主观评分的相关性优于单一指标。


<details>
  <summary>Details</summary>
Motivation: 单一图像质量评价指标难以全面评估NeRF生成的独特伪影；DISTS与VMAF从结构/纹理与视频感知两方面互补，融合可能弥补各自不足。

Method: 对DISTS和VMAF分别进行两种归一化（可能为线性和秩次/量化）并采用两种融合策略（线性加权融合与学习融合或非线性融合），在三种配置下对比相关系数（如Spearman、Pearson）与主观分数。

Result: 融合方法在多数配置和两个数据集上比单独指标显示更高的一致性和相关系数，证明融合策略更稳健且具更好泛化性，但具体性能依赖于归一化与融合方式的选择。

Conclusion: 融合基于DISTS和VMAF的指标可提高与主观评分的相关性，且在合成与户外数据集上表现稳健。

Abstract: Neural Radiance Fields (NeRFs) have demonstrated significant potential in
synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however,
remains a challenge due to the unique artifacts they exhibit, and no individual
metric performs well across all datasets. We hypothesize that combining two
successful metrics, Deep Image Structure and Texture Similarity (DISTS) and
Video Multi-Method Assessment Fusion (VMAF), based on different perceptual
methods, can overcome the limitations of individual metrics and achieve
improved correlation with subjective quality scores. We experiment with two
normalization strategies for the individual metrics and two fusion strategies
to evaluate their impact on the resulting correlation with the subjective
scores. The proposed pipeline is tested on two distinct datasets, Synthetic and
Outdoor, and its performance is evaluated across three different
configurations. We present a detailed analysis comparing the correlation
coefficients of fusion methods and individual scores with subjective scores to
demonstrate the robustness and generalizability of the fusion metrics.

</details>


### [60] [Leveraging Large Language Models to Effectively Generate Visual Data for Canine Musculoskeletal Diagnoses](https://arxiv.org/abs/2509.12866)
*Martin Thißen,Thi Ngoc Diep Tran,Barbara Esteve Ratsch,Ben Joel Schönbein,Ute Trapp,Beate Egner,Romana Piat,Elke Hergenröther*

Main category: cs.CV

TL;DR: 本文提出用LLM生成映射到200+解剖区域的合成视觉标注，成功在犬类髌骨脱位诊断上用合成数据训练模型并在真实数据达成88% F1，显示LLM合成数据在稀缺数据场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 真实世界中某些医学/兽医条件样本稀缺或采集成本高，需探索LLM生成合成视觉标注以扩充训练数据。

Method: 将视觉标注映射为200+肌肉/关节区域的文本表示，使用few-shot提示、chain-of-thought和guided decoding生成1000份关节脱位（髌骨脱位）合成视觉文档，并为多诊断生成额外1000份二分类数据集，训练仅用合成数据的模型进行评估。

Result: 合成数据对位置和严重度敏感，与性别无关。用合成数据训练的模型在70份真实文档上获得F1=88%。

Conclusion: LLMs可用于生成兽医肌肉骨骼视觉文档的合成训练数据，能在数据稀缺场景下提高下游分类模型性能。

Abstract: It is well-established that more data generally improves AI model
performance. However, data collection can be challenging for certain tasks due
to the rarity of occurrences or high costs. These challenges are evident in our
use case, where we apply AI models to a novel approach for visually documenting
the musculoskeletal condition of dogs. Here, abnormalities are marked as
colored strokes on a body map of a dog. Since these strokes correspond to
distinct muscles or joints, they can be mapped to the textual domain in which
large language models (LLMs) operate. LLMs have demonstrated impressive
capabilities across a wide range of tasks, including medical applications,
offering promising potential for generating synthetic training data. In this
work, we investigate whether LLMs can effectively generate synthetic visual
training data for canine musculoskeletal diagnoses. For this, we developed a
mapping that segments visual documentations into over 200 labeled regions
representing muscles or joints. Using techniques like guided decoding,
chain-of-thought reasoning, and few-shot prompting, we generated 1,000
synthetic visual documentations for patellar luxation (kneecap dislocation)
diagnosis, the diagnosis for which we have the most real-world data. Our
analysis shows that the generated documentations are sensitive to location and
severity of the diagnosis while remaining independent of the dog's sex. We
further generated 1,000 visual documentations for various other diagnoses to
create a binary classification dataset. A model trained solely on this
synthetic data achieved an F1 score of 88% on 70 real-world documentations.
These results demonstrate the potential of LLM-generated synthetic data, which
is particularly valuable for addressing data scarcity in rare diseases. While
our methodology is tailored to the medical domain, the insights and techniques
can be adapted to other fields.

</details>


### [61] [Cumulative Consensus Score: Label-Free and Model-Agnostic Evaluation of Object Detectors in Deployment](https://arxiv.org/abs/2509.12871)
*Avinaash Manoharan,Xiangyu Yin,Domenik Helm,Chih-Hong Cheng*

Main category: cs.CV

TL;DR: CCS通过测试时增强和框一致性，提供了一个标签无关、模型无关的部署检测器监控指标，与常规指标高度一致。


<details>
  <summary>Details</summary>
Motivation: 部署环境中缺少标注，无法用传统指标评估检测器表现，需要一个可在无标注场景下持续监控检测器可靠性的度量。

Method: 对每张图像进行多种测试时数据增强，收集各视图预测框，计算预测框间IoU，取最大重叠并归一化，最后在增强对间平均得到空间一致性分数。

Result: 在Open Images和KITTI上与F1、PDQ和OCC达成90%以上一致性；对单阶段和两阶段检测器均适用，能在样本级指出表现差的情况。

Conclusion: CCS是一个无需标注的检测器监控指标，通过测试时数据增强和预测框的一致性度量，能在实际部署中持续评估检测器性能。

Abstract: Evaluating object detection models in deployment is challenging because
ground-truth annotations are rarely available. We introduce the Cumulative
Consensus Score (CCS), a label-free metric that enables continuous monitoring
and comparison of detectors in real-world settings. CCS applies test-time data
augmentation to each image, collects predicted bounding boxes across augmented
views, and computes overlaps using Intersection over Union. Maximum overlaps
are normalized and averaged across augmentation pairs, yielding a measure of
spatial consistency that serves as a proxy for reliability without annotations.
In controlled experiments on Open Images and KITTI, CCS achieved over 90%
congruence with F1-score, Probabilistic Detection Quality, and Optimal
Correction Cost. The method is model-agnostic, working across single-stage and
two-stage detectors, and operates at the case level to highlight
under-performing scenarios. Altogether, CCS provides a robust foundation for
DevOps-style monitoring of object detectors.

</details>


### [62] [Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation](https://arxiv.org/abs/2509.12878)
*Qianguang Zhao,Dongli Wang,Yan Zhou,Jianxun Li,Richard Irampa*

Main category: cs.CV

TL;DR: 通过把预训练扩散编码器引入原型构建，PENet用双流原型与对齐校准模块扩展并对齐原型表示，显著提升少样本3D点云语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 原型方法在少样本3D分割中受限于有限表示能力与支持集与查询集特征空间不一致，借助扩散模型泛化特性扩展原型表示范围以提高鲁棒性与对齐性。

Method: 利用预训练扩散模型的条件编码器提取可泛化特征，构建双流学习器（Intrinsic Learner与Diffusion Learner），再通过Prototype Assimilation Module用推拉交叉引导注意力对齐原型，最后用Prototype Calibration Mechanism防止语义漂移。

Result: 在多个few-shot设置下，PENet在S3DIS与ScanNet数据集上显著优于现有最先进方法，证明了双源大容量原型与对齐校准机制的有效性。

Conclusion: 本文提出PENet，通过扩展原型容量解决少样本三维点云语义分割中的类内多样性和集合不一致问题，有效提升了在S3DIS和ScanNet上的表现。

Abstract: Few-shot 3D point cloud semantic segmentation aims to segment novel
categories using a minimal number of annotated support samples. While existing
prototype-based methods have shown promise, they are constrained by two
critical challenges: (1) Intra-class Diversity, where a prototype's limited
representational capacity fails to cover a class's full variations, and (2)
Inter-set Inconsistency, where prototypes derived from the support set are
misaligned with the query feature space. Motivated by the powerful generative
capability of diffusion model, we re-purpose its pre-trained conditional
encoder to provide a novel source of generalizable features for expanding the
prototype's representational range. Under this setup, we introduce the
Prototype Expansion Network (PENet), a framework that constructs big-capacity
prototypes from two complementary feature sources. PENet employs a dual-stream
learner architecture: it retains a conventional fully supervised Intrinsic
Learner (IL) to distill representative features, while introducing a novel
Diffusion Learner (DL) to provide rich generalizable features. The resulting
dual prototypes are then processed by a Prototype Assimilation Module (PAM),
which adopts a novel push-pull cross-guidance attention block to iteratively
align the prototypes with the query space. Furthermore, a Prototype Calibration
Mechanism (PCM) regularizes the final big capacity prototype to prevent
semantic drift. Extensive experiments on the S3DIS and ScanNet datasets
demonstrate that PENet significantly outperforms state-of-the-art methods
across various few-shot settings.

</details>


### [63] [Lego-Edit: A General Image Editing Framework with Model-Level Bricks and MLLM Builder](https://arxiv.org/abs/2509.12883)
*Qifei Jia,Yu Liu,Yajie Chai,Xintong Yao,Qiming Lu,Yasen Zhang,Runyu Shi,Ying Huang,Guoquan Zhang*

Main category: cs.CV

TL;DR: Lego-Edit用MLLM组织可组合的模型级编辑工具并以三阶段强化学习用开放指令训练，从而提升指令驱动图像编辑在开放域的泛化与可扩展性，实验达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有指令驱动图像编辑方法在面对真实、开放域用户指令时泛化性差，限制实际应用；利用MLLM的推理与通用能力结合可组合工具箱可提升开放域适应能力。

Method: 构建模型级工具箱（若干在小规模数据上高效训练的编辑子模型与图像操作函数），由MLLM负责调用与组合；采用三阶段渐进式强化学习，在无标注开放域指令上通过反馈提升MLLM的推理与决策能力。

Result: 在GEdit-Bench和ImgBench上取得SOTA表现；对开放域指令展现鲁棒推理能力；能在不额外微调的情况下使用新引入的编辑工具。

Conclusion: Lego-Edit通过将多模态大模型作为指挥者，组织一套轻量级可组合的模型级编辑工具，并用三阶段强化学习在开放指令上训练，从而显著提升了指令驱动图像编辑在开放域指令下的泛化能力。

Abstract: Instruction-based image editing has garnered significant attention due to its
direct interaction with users. However, real-world user instructions are
immensely diverse, and existing methods often fail to generalize effectively to
instructions outside their training domain, limiting their practical
application. To address this, we propose Lego-Edit, which leverages the
generalization capability of Multi-modal Large Language Model (MLLM) to
organize a suite of model-level editing tools to tackle this challenge.
Lego-Edit incorporates two key designs: (1) a model-level toolkit comprising
diverse models efficiently trained on limited data and several image
manipulation functions, enabling fine-grained composition of editing actions by
the MLLM; and (2) a three-stage progressive reinforcement learning approach
that uses feedback on unannotated, open-domain instructions to train the MLLM,
equipping it with generalized reasoning capabilities for handling real-world
instructions. Experiments demonstrate that Lego-Edit achieves state-of-the-art
performance on GEdit-Bench and ImgBench. It exhibits robust reasoning
capabilities for open-domain instructions and can utilize newly introduced
editing tools without additional fine-tuning.
  Code is available: https://github.com/xiaomi-research/lego-edit.

</details>


### [64] [Runge-Kutta Approximation and Decoupled Attention for Rectified Flow Inversion and Semantic Editing](https://arxiv.org/abs/2509.12888)
*Weiming Chen,Zhihan Zhu,Yijia Wang,Zhihai He*

Main category: cs.CV

TL;DR: 该工作为Rectified Flow引入了高阶Runge-Kutta反演和解耦的多模态注意力（DDTA），显著提升了反演准确性与文本引导编辑的精确度，在重建与编辑任务上取得了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 动机来自两大现实挑战：1) Rectified Flow模型在实际应用中反演精度较低，导致与源图像一致性不足；2) 多模态扩散Transformer中的注意力存在纠缠，妨碍精确控制文本对图像编辑的影响。论文旨在分别从数值解法和模型结构两方面解决这些问题。

Method: 方法包括：1) 将Runge-Kutta高阶数值解法应用于Rectified Flow模型的反演过程，从而提高逆过程的精度和稳定性；2) 在扩散Transformer内部将文本注意力与图像注意力解耦（DDTA），以消除注意力的多模态纠缠并实现更精确的语义控制。作者通过实验比较展示了在重建和编辑任务上的性能提升。

Result: 实验结果表明，提出的Runge-Kutta反演方法与DDTA机制在图像重建任务中提升了保真度，在基于文本的图像编辑任务中实现了更高的可编辑性，达到或超过现有方法的性能。作者提供了代码实现供复现。

Conclusion: 该论文提出了两个关键改进：基于Runge-Kutta的高阶反演方法用于Rectified Flow模型以提升反演精度；以及一种解耦的多模态注意力机制（DDTA）用于扩散Transformer以改善文本与图像注意力的可控性。总体结论是这两项方法显著提高了图像重建与文本引导编辑的保真性和可编辑性。

Abstract: Rectified flow (RF) models have recently demonstrated superior generative
performance compared to DDIM-based diffusion models. However, in real-world
applications, they suffer from two major challenges: (1) low inversion accuracy
that hinders the consistency with the source image, and (2) entangled
multimodal attention in diffusion transformers, which hinders precise attention
control. To address the first challenge, we propose an efficient high-order
inversion method for rectified flow models based on the Runge-Kutta solver of
differential equations. To tackle the second challenge, we introduce Decoupled
Diffusion Transformer Attention (DDTA), a novel mechanism that disentangles
text and image attention inside the multimodal diffusion transformers, enabling
more precise semantic control. Extensive experiments on image reconstruction
and text-guided editing tasks demonstrate that our method achieves
state-of-the-art performance in terms of fidelity and editability. Code is
available at https://github.com/wmchen/RKSovler_DDTA.

</details>


### [65] [MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization](https://arxiv.org/abs/2509.12893)
*Yiyi Zhang,Yuchen Yuan,Ying Zheng,Jialun Pei,Jinpeng Li,Zheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出MEJO：用S^2D解耦共享/特定表征并引入MLLM概率提示增强语义，用CGL重平衡头/尾类梯度，显著改善外科三元组识别的长尾与优化冲突问题。


<details>
  <summary>Details</summary>
Motivation: 外科三元组识别存在长尾数据分布，传统多任务学习在跨任务协同上虽有收益，但受任务共享与特定表征纠缠以及类别不平衡导致的优化冲突影响，需在表征解耦与梯度协调上做改进。

Method: 方法包括两个核心模块：1) S^2D（Shared-Specific-Disentangled）学习，将表征分解为任务共享与任务特定分量，并采用MLLM驱动的概率提示池对视觉特征进行语义增强，同时为各任务设计时空维度的专属提示；2) CGL（Coordinated Gradient Learning）策略，解析并重平衡来自头部与尾部类别的正负梯度，以缓解类不平衡带来的训练冲突。

Result: 在CholecT45与CholecT50上进行的大量实验表明，MEJO在三元组识别上优于现有方法，表现出更好的鲁棒性与对长尾类别的改善。

Conclusion: 本文提出的MEJO框架通过同时解决跨任务与任务内的优化冲突，有效提升了外科三元组识别的性能，实验验证了方法在CholecT45/CholecT50数据集上的优越性。

Abstract: Surgical triplet recognition, which involves identifying instrument, verb,
target, and their combinations, is a complex surgical scene understanding
challenge plagued by long-tailed data distribution. The mainstream multi-task
learning paradigm benefiting from cross-task collaborative promotion has shown
promising performance in identifying triples, but two key challenges remain: 1)
inter-task optimization conflicts caused by entangling task-generic and
task-specific representations; 2) intra-task optimization conflicts due to
class-imbalanced training data. To overcome these difficulties, we propose the
MLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- and
intra-task optimization for surgical triplet recognition. For inter-task
optimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learning
scheme that decomposes representations into task-shared and task-specific
components. To enhance task-shared representations, we construct a Multimodal
Large Language Model (MLLM) powered probabilistic prompt pool to dynamically
augment visual features with expert-level semantic cues. Additionally,
comprehensive task-specific cues are modeled via distinct task prompts covering
the temporal-spatial dimensions, effectively mitigating inter-task ambiguities.
To tackle intra-task optimization conflicts, we develop a Coordinated Gradient
Learning (CGL) strategy, which dissects and rebalances the positive-negative
gradients originating from head and tail classes for more coordinated learning
behaviors. Extensive experiments on the CholecT45 and CholecT50 datasets
demonstrate the superiority of our proposed framework, validating its
effectiveness in handling optimization conflicts.

</details>


### [66] [DialNav: Multi-turn Dialog Navigation with a Remote Guide](https://arxiv.org/abs/2509.12894)
*Leekyeung Han,Hyunji Min,Gyeom Hwangbo,Jonghyun Choi,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出DialNav任务与RAIN数据集，结合导航与多轮对话的综合评测，公开数据与代码，分析模型表现并指出关键挑战。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航或对话研究未能综合考虑多轮协作式远程引导场景和对话在导航中的关键作用，且缺乏配套的数据集与全面评估标准，故提出DialNav并构建RAIN数据集以填补这一空白。

Method: 构建DialNav任务设定、收集人类-人类对话配对导航轨迹的RAIN数据集、设计同时评估导航与对话的综合基准，搭建评估框架，并用多种Navigator和Guide模型进行实验对比分析。

Result: 发布了RAIN数据集、代码与评测框架；实验证明不同模型在导航与对话任务上有显著差异，强调通信、位置推断等为关键挑战，为未来研究提供基准与资源。

Conclusion: 该工作提出了一个新的协作式具身对话任务DialNav，强调导航代理与远程引导者通过多轮对话协同到达目标位置，要求引导者推断代理位置，并重视整体评估。作者收集发布了RAIN数据集，并搭建了全面的评测基准与评价框架，进行了大量实验分析，指出关键挑战并公开资源以促进后续研究。

Abstract: We introduce DialNav, a novel collaborative embodied dialog task, where a
navigation agent (Navigator) and a remote guide (Guide) engage in multi-turn
dialog to reach a goal location. Unlike prior work, DialNav aims for holistic
evaluation and requires the Guide to infer the Navigator's location, making
communication essential for task success. To support this task, we collect and
release the Remote Assistance in Navigation (RAIN) dataset, human-human dialog
paired with navigation trajectories in photorealistic environments. We design a
comprehensive benchmark to evaluate both navigation and dialog, and conduct
extensive experiments analyzing the impact of different Navigator and Guide
models. We highlight key challenges and publicly release the dataset, code, and
evaluation framework to foster future research in embodied dialog.

</details>


### [67] [Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models](https://arxiv.org/abs/2509.12897)
*Jianfei Zhao,Feng Zhang,Xin Sun,Lingxing Kong,Zhixing Tan,Chong Feng*

Main category: cs.CV

TL;DR: 通过跨层记忆平滑注意力并基于不确定性提前终止，CLVS让LVLM在视觉理解（尤其关系/属性）上更准确且更稳定。


<details>
  <summary>Details</summary>
Motivation: 观察到LVLM虽能定位关键对象但关注时间短，假设延长关键对象的关注有助于提升视觉理解能力。

Method: 在第一层用位置无偏的视觉注意力初始化一个视觉记忆；后续层的注意力结合前层记忆并迭代更新，从而在层间维持对关键对象的平滑关注；利用不确定性指标判断视觉理解何时完成并终止平滑过程。

Result: 在三个不同的LVLM和四个基准上的实验显示，CLVS提高了多种视觉理解任务的性能，尤其在关系与属性理解任务上提升显著，并达到或超过现有SOTA。

Conclusion: CLVS能通过跨层平滑注意力显著提升LVLM在视觉理解任务上的表现，尤其在关系和属性理解上效果显著；方法普适，能在多个LVLM和基准上取得SOTA。

Abstract: Large Vision-Language Models (LVLMs) can accurately locate key objects in
images, yet their attention to these objects tends to be very brief. Motivated
by the hypothesis that sustained focus on key objects can improve LVLMs' visual
capabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of
CLVS is to incorporate a vision memory that smooths the attention distribution
across layers. Specifically, we initialize this vision memory with
position-unbiased visual attention in the first layer. In subsequent layers,
the model's visual attention jointly considers the vision memory from previous
layers, while the memory is updated iteratively, thereby maintaining smooth
attention on key objects. Given that visual understanding primarily occurs in
the early and middle layers of the model, we use uncertainty as an indicator of
completed visual understanding and terminate the smoothing process accordingly.
Experiments on four benchmarks across three LVLMs confirm the effectiveness and
generalizability of our method. CLVS achieves state-of-the-art performance on a
variety of visual understanding tasks, with particularly significant
improvements in relation and attribute understanding.

</details>


### [68] [MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image Fusion](https://arxiv.org/abs/2509.12901)
*Guihui Li,Bowei Dong,Kaizhi Dong,Jiayi Li,Haiyong Zheng*

Main category: cs.CV

TL;DR: MSGFusion用结构化的场景图将文本与视觉深度结合，细粒度建模实体/属性/关系并驱动融合，提升了细节、结构和语义一致性，且在多项基准和下游任务上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习融合方法主要依赖低层视觉线索，难以捕获高层语义；使用无结构文本指导也无法提供实体/属性/关系以及空间定位，从而限制了细粒度融合效果。

Method: 提出基于多模态场景图的融合框架，包括场景图表示、层次聚合、图驱动融合三个连续模块；通过同步优化高层语义和低层细节，实现细粒度的实体、属性和空间关系建模并用于引导融合过程。

Result: 在多个公开基准上，MSGFusion在细节保留、结构清晰度、语义一致性和下游任务（低光目标检测、语义分割、医学图像融合）泛化性方面显著优于最先进方法。

Conclusion: MSGFusion通过将结构化场景图与红外/可见图像深度耦合，能够在保持细节和结构清晰性的同时实现语义一致的融合，显著优于现有方法。

Abstract: Infrared and visible image fusion has garnered considerable attention owing
to the strong complementarity of these two modalities in complex, harsh
environments. While deep learning-based fusion methods have made remarkable
advances in feature extraction, alignment, fusion, and reconstruction, they
still depend largely on low-level visual cues, such as texture and contrast,
and struggle to capture the high-level semantic information embedded in images.
Recent attempts to incorporate text as a source of semantic guidance have
relied on unstructured descriptions that neither explicitly model entities,
attributes, and relationships nor provide spatial localization, thereby
limiting fine-grained fusion performance. To overcome these challenges, we
introduce MSGFusion, a multimodal scene graph-guided fusion framework for
infrared and visible imagery. By deeply coupling structured scene graphs
derived from text and vision, MSGFusion explicitly represents entities,
attributes, and spatial relations, and then synchronously refines high-level
semantics and low-level details through successive modules for scene graph
representation, hierarchical aggregation, and graph-driven fusion. Extensive
experiments on multiple public benchmarks show that MSGFusion significantly
outperforms state-of-the-art approaches, particularly in detail preservation
and structural clarity, and delivers superior semantic consistency and
generalizability in downstream tasks such as low-light object detection,
semantic segmentation, and medical image fusion.

</details>


### [69] [AREPAS: Anomaly Detection in Fine-Grained Anatomy with Reconstruction-Based Semantic Patch-Scoring](https://arxiv.org/abs/2509.12905)
*Branko Mitic,Philipp Seeböck,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 提出基于重构+补丁相似性评分的生成式异常检测方法，能更好适应正常的细粒度组织变异，在胸CT与脑MRI上显著提升像素级分割表现。


<details>
  <summary>Details</summary>
Motivation: 医学影像中正常组织存在细粒度的解剖差异（如肺部结构），使得现有生成式异常检测方法容易将正常变异误判为异常，需提出能适应细粒度变异的检测方法。

Method: 方法由两部分组成：1) 图像到图像的翻译网络用于生成无异常的重构图像；2) 对原图与重构图进行分块（patch）级相似性评分，基于补丁差异来定位异常区域。

Result: 在胸部CT用于感染性病灶的检测与分割，以及在T1脑MRI用于缺血性脑卒中病灶分割的评估中，新方法在像素级异常分割上均优于其他重建类基线，DICE相对提升+1.9%（胸CT）和+4.4%（脑MRI）。

Conclusion: 提出了一种针对细粒度组织变异的生成式异常检测方法，通过图像到图像的异常去除重构加上基于补丁相似性的评分实现精确定位，在胸部CT和脑MRI上表现优于现有重建类方法，像素级DICE分别提升约1.9%和4.4%。

Abstract: Early detection of newly emerging diseases, lesion severity assessment,
differentiation of medical conditions and automated screening are examples for
the wide applicability and importance of anomaly detection (AD) and
unsupervised segmentation in medicine. Normal fine-grained tissue variability
such as present in pulmonary anatomy is a major challenge for existing
generative AD methods. Here, we propose a novel generative AD approach
addressing this issue. It consists of an image-to-image translation for
anomaly-free reconstruction and a subsequent patch similarity scoring between
observed and generated image-pairs for precise anomaly localization. We
validate the new method on chest computed tomography (CT) scans for the
detection and segmentation of infectious disease lesions. To assess
generalizability, we evaluate the method on an ischemic stroke lesion
segmentation task in T1-weighted brain MRI. Results show improved pixel-level
anomaly segmentation in both chest CTs and brain MRIs, with relative DICE score
improvements of +1.9% and +4.4%, respectively, compared to other
state-of-the-art reconstruction-based methods.

</details>


### [70] [T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking](https://arxiv.org/abs/2509.12913)
*Hojat Ardi,Amir Jahanshahi,Ali Diba*

Main category: cs.CV

TL;DR: T-SiamTPN在SiamTPN上加入时序融合与注意力交互，有效提升航拍目标跟踪的鲁棒性与精度，同时在嵌入式设备上保持实时性（7.1FPS），成功率/精度分别提升约13.7%和14.7%。


<details>
  <summary>Details</summary>
Motivation: 现有Siamese相关跟踪器过分依赖空间线索、忽视时间依赖，且相关操作的线性特性难以应对复杂非线性外观变化，导致在尺度变化、动态背景、遮挡等航空场景中性能受限。

Method: 在SiamTPN框架中加入时间特征融合和基于注意力的时序交互模块，以强化时间一致性并获得更丰富的表征；仍保留轻量化设计以维持计算效率，并在Jetson Nano上实现和测试。

Result: 与基线相比，T-SiamTPN在成功率上提高13.7%，在精度上提高14.7%，在Jetson Nano上以7.1 FPS运行，证明在效果和效率间取得了平衡，并在多项评测中达到与SOTA竞争的表现。

Conclusion: 本文提出的T-SiamTPN通过在SiamTPN基础上引入显式时序建模，提升了航拍目标跟踪在长期跟踪和遮挡场景下的鲁棒性，结论是时间信息对Siamese跟踪框架显著有益；同时在边缘设备上保持实时性。

Abstract: Aerial object tracking remains a challenging task due to scale variations,
dynamic backgrounds, clutter, and frequent occlusions. While most existing
trackers emphasize spatial cues, they often overlook temporal dependencies,
resulting in limited robustness in long-term tracking and under occlusion.
Furthermore, correlation-based Siamese trackers are inherently constrained by
the linear nature of correlation operations, making them ineffective against
complex, non-linear appearance changes. To address these limitations, we
introduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends
the SiamTPN architecture with explicit temporal modeling. Our approach
incorporates temporal feature fusion and attention-based interactions,
strengthening temporal consistency and enabling richer feature representations.
These enhancements yield significant improvements over the baseline and achieve
performance competitive with state-of-the-art trackers. Crucially, despite the
added temporal modules, T-SiamTPN preserves computational efficiency. Deployed
on the resource-constrained Jetson Nano, the tracker runs in real time at 7.1
FPS, demonstrating its suitability for real-world embedded applications without
notable runtime overhead. Experimental results highlight substantial gains:
compared to the baseline, T-SiamTPN improves success rate by 13.7% and
precision by 14.7%. These findings underscore the importance of temporal
modeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and
efficient solution for aerial object tracking. Code is available at:
https://github.com/to/be/released

</details>


### [71] [A Novel Compression Framework for YOLOv8: Achiev-ing Real-Time Aerial Object Detection on Edge Devices via Structured Pruning and Channel-Wise Distillation](https://arxiv.org/abs/2509.12918)
*Melika Sabaghian,Mohammad Ali Keyvanrad,Seyyedeh Mahila Moghadami*

Main category: cs.CV

TL;DR: 论文通过稀疏训练+结构化剪枝+通道级蒸馏的三阶段管线，高效压缩YOLOv8，实现显著加速且仅带来极小精度损失，适合空中小目标的边缘部署。


<details>
  <summary>Details</summary>
Motivation: 动机是将YOLOv8等深度检测模型有效压缩以便在边缘设备或资源受限环境中实时部署，同时尽量不牺牲检测性能，尤其是在无人机/空中影像中常见的小目标检测场景。

Method: 方法包括三阶段：1) 稀疏感知训练（引入动态稀疏性以在训练中剪除不重要参数）；2) 结构化通道剪枝（利用BatchNorm缩放因子筛选并删除冗余通道）；3) 通道级知识蒸馏（Channel-Wise KD），通过可调温度与损失加权策略从原始模型传递知识以恢复精度，特别针对小/中尺度目标。最后辅以TensorRT轻量化优化。

Result: 在VisDrone数据集上多版本YOLOv8验证效果显著。以YOLOv8m为例：参数从25.85M降到6.85M（降幅73.51%），FLOPs从49.6G降到13.3G，MACs从101G降到34.5G，AP50仅下降2.7%至47.9，推理速度从26FPS提升到45FPS；进一步使用TensorRT后AP50微降至47.6但速度提升至68FPS。

Conclusion: 该论文提出的三阶段压缩流水线在保持精度的同时显著降低YOLOv8模型规模与计算量，适合资源受限的空中目标检测部署。

Abstract: Efficient deployment of deep learning models for aerial object detection on
resource-constrained devices requires significant compression without
com-promising performance. In this study, we propose a novel three-stage
compression pipeline for the YOLOv8 object detection model, integrating
sparsity-aware training, structured channel pruning, and Channel-Wise Knowledge
Distillation (CWD). First, sparsity-aware training introduces dynamic sparsity
during model optimization, effectively balancing parameter reduction and
detection accuracy. Second, we apply structured channel pruning by leveraging
batch normalization scaling factors to eliminate redundant channels,
significantly reducing model size and computational complexity. Finally, to
mitigate the accuracy drop caused by pruning, we employ CWD to transfer
knowledge from the original model, using an adjustable temperature and loss
weighting scheme tailored for small and medium object detection. Extensive
experiments on the VisDrone dataset demonstrate the effectiveness of our
approach across multiple YOLOv8 variants. For YOLOv8m, our method reduces model
parameters from 25.85M to 6.85M (a 73.51% reduction), FLOPs from 49.6G to
13.3G, and MACs from 101G to 34.5G, while reducing AP50 by only 2.7%. The
resulting compressed model achieves 47.9 AP50 and boosts inference speed from
26 FPS (YOLOv8m baseline) to 45 FPS, enabling real-time deployment on edge
devices. We further apply TensorRT as a lightweight optimization step. While
this introduces a minor drop in AP50 (from 47.9 to 47.6), it significantly
improves inference speed from 45 to 68 FPS, demonstrating the practicality of
our approach for high-throughput, re-source-constrained scenarios.

</details>


### [72] [MATTER: Multiscale Attention for Registration Error Regression](https://arxiv.org/abs/2509.12924)
*Shipeng Liu,Ziliang Xiong,Khac-Hoang Ngo,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 将PCR质量验证由分类改为回归，结合多尺度特征与注意力聚合，能更精细、稳健地估计配准误差，提升异构点云场景下和下游建图任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将配准质量视为分类任务，输出粗糙的类别标签，无法满足细粒度误差评估与下游任务的需求。作者希望通过回归提供更精确的误差量化并提高实际建图性能。

Method: 提出了多尺度特征提取与基于注意力的聚合机制，扩展了以往与错配相关的特征表示；使用回归网络直接预测配准误差而非离散类别。

Result: 在多数据集上，方法实现了更准确和鲁棒的配准误差估计，尤其在空间密度异构的点云上表现突出；在以重配准帧数受限的建图场景中，相比最先进的分类方法显著提升了建图质量。

Conclusion: 本文将点云配准质量验证问题从分类改为回归，从而提供更细粒度的误差估计，实验证明对异构密度点云和下游建图任务效果显著提升。

Abstract: Point cloud registration (PCR) is crucial for many downstream tasks, such as
simultaneous localization and mapping (SLAM) and object tracking. This makes
detecting and quantifying registration misalignment, i.e.,~{\it PCR quality
validation}, an important task. All existing methods treat validation as a
classification task, aiming to assign the PCR quality to a few classes. In this
work, we instead use regression for PCR validation, allowing for a more
fine-grained quantification of the registration quality. We also extend
previously used misalignment-related features by using multiscale extraction
and attention-based aggregation. This leads to accurate and robust registration
error estimation on diverse datasets, especially for point clouds with
heterogeneous spatial densities. Furthermore, when used to guide a mapping
downstream task, our method significantly improves the mapping quality for a
given amount of re-registered frames, compared to the state-of-the-art
classification-based method.

</details>


### [73] [4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D Radar](https://arxiv.org/abs/2509.12931)
*Xiao Tang,Guirong Zhuo,Cong Wang,Boyuan Zheng,Minqing Huang,Lianqing Zheng,Long Chen,Shouyi Lu*

Main category: cs.CV

TL;DR: 4DRadar-GS利用4D Radar信息和速度指导追踪，提升自监督动态驾驶场景的3D重建与时间一致性，在OmniHD-Scenes上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是现有自监督单目或基于频域、光流的方法在动态物体重建中受限于运动估计不准和时间一致性弱，导致重建不完整或失真，因此希望通过引入4D Radar改善动态目标分割和深度尺度恢复并提升追踪一致性。

Method: 方法上，作者结合4D Radar的速度与空间信息，设计了Gaussian初始化用于提供准确的点云高斯表示，并提出Velocity-guided PointTrack (VGPT)与重建模块联合训练，在场景流监督下实现细粒度轨迹追踪与时间一致性。

Result: 在OmniHD-Scenes上，4DRadar-GS在动态驾驶场景3D重建任务中优于现有方法，展示出更准确的动态物体形状恢复和更好的时间一致性（实验细节和指标未在摘要中给出）。

Conclusion: 该论文提出的4DRadar-GS有效改进了动态场景下的3D重建，特别是在动态物体的分割与时间一致性追踪方面具有显著提升，且在OmniHD-Scenes数据集上达到了最先进的结果。

Abstract: 3D reconstruction and novel view synthesis are critical for validating
autonomous driving systems and training advanced perception models. Recent
self-supervised methods have gained significant attention due to their
cost-effectiveness and enhanced generalization in scenarios where annotated
bounding boxes are unavailable. However, existing approaches, which often rely
on frequency-domain decoupling or optical flow, struggle to accurately
reconstruct dynamic objects due to imprecise motion estimation and weak
temporal consistency, resulting in incomplete or distorted representations of
dynamic scene elements. To address these challenges, we propose 4DRadar-GS, a
4D Radar-augmented self-supervised 3D reconstruction framework tailored for
dynamic driving scenes. Specifically, we first present a 4D Radar-assisted
Gaussian initialization scheme that leverages 4D Radar's velocity and spatial
information to segment dynamic objects and recover monocular depth scale,
generating accurate Gaussian point representations. In addition, we propose a
Velocity-guided PointTrack (VGPT) model, which is jointly trained with the
reconstruction pipeline under scene flow supervision, to track fine-grained
dynamic trajectories and construct temporally consistent representations.
Evaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art
performance in dynamic driving scene 3D reconstruction.

</details>


### [74] [Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings](https://arxiv.org/abs/2509.12938)
*Abdalla Arafa,Didier Stricker*

Main category: cs.CV

TL;DR: 通过对象级高斯分解与多视角CLIP嵌入聚合，绕开可微渲染语义蒸馏，实现精确的3D开域对象检索与提取，并兼顾2D分割性能。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting尽管能实时光线渲染，但高斯模糊导致3D语义理解困难。通过避免在可微渲染管线中学习语义，减少语义平均化与跨对象混合的问题。

Method: 先对场景进行对象级高斯预分解，对每个对象聚合多视角的CLIP图像-文本特征，构建“嵌入包”。检索时将文本查询与对象级嵌入直接比对；任务适配时将对象ID投影到像素用于2D分割或投影回高斯用于3D提取。

Result: 方法在3D开域对象提取上显著优于基于高斯级别或像素级蒸馏的方法，并在2D开域分割任务上保持与最先进方法可比的性能，同时实现了更清晰的对象边界和3D一致性。

Conclusion: 本文提出了一种基于对象级别高斯簇和多视角CLIP特征聚合的全新范式，绕开了对可微渲染的语义学习依赖，实现了更精确的3D开域对象检索与提取。

Abstract: Novel view synthesis has seen significant advancements with 3D Gaussian
Splatting (3DGS), enabling real-time photorealistic rendering. However, the
inherent fuzziness of Gaussian Splatting presents challenges for 3D scene
understanding, restricting its broader applications in AR/VR and robotics.
While recent works attempt to learn semantics via 2D foundation model
distillation, they inherit fundamental limitations: alpha blending averages
semantics across objects, making 3D-level understanding impossible. We propose
a paradigm-shifting alternative that bypasses differentiable rendering for
semantics entirely. Our key insight is to leverage predecomposed object-level
Gaussians and represent each object through multiview CLIP feature aggregation,
creating comprehensive "bags of embeddings" that holistically describe objects.
This allows: (1) accurate open-vocabulary object retrieval by comparing text
queries to object-level (not Gaussian-level) embeddings, and (2) seamless task
adaptation: propagating object IDs to pixels for 2D segmentation or to
Gaussians for 3D extraction. Experiments demonstrate that our method
effectively overcomes the challenges of 3D open-vocabulary object extraction
while remaining comparable to state-of-the-art performance in 2D
open-vocabulary segmentation, ensuring minimal compromise.

</details>


### [75] [Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance to Event Domain](https://arxiv.org/abs/2509.12959)
*Yuqi Xie,Shuhan Ye,Chong Wang,Jiazhen Xu,Le Shen,Yuanbin Qian,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出TMKT：在时间步维度对RGB与DVS输入进行混合，并引入模态感知辅助目标，实现更平滑的跨模态知识迁移，提升SNN在事件相机分类任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机（DVS）与SNN结合具备能源高效的视觉处理潜力，但受限于事件数据稀疏性和数据量不足，导致训练困难。现有从RGB到DVS的知识迁移方法忽视了两者之间的显著分布差异，效果受限。

Method: TMKT利用SNN的异步时序特性，在不同时间步对RGB和DVS信号进行混合，并配合模态感知的辅助学习目标以支持跨模态的标签混合。整体训练过程中对混合输入和辅助损失进行联合优化，从而实现平滑的知识传递。

Result: 在多组数据集上的大量实验表明，TMKT能有效缓解模态偏移并在脉冲图像分类任务上取得优越表现，证明了该方法的有效性。

Conclusion: 本文提出了一种名为Time-step Mixup knowledge transfer (TMKT)的方法，通过在时间步维度对RGB与DVS输入进行细粒度插值，实现跨模态知识迁移，从而缓解模态差异并提升脉冲神经网络（SNN）在事件相机数据上的分类性能。

Abstract: The integration of event cameras and spiking neural networks holds great
promise for energy-efficient visual processing. However, the limited
availability of event data and the sparse nature of DVS outputs pose challenges
for effective training. Although some prior work has attempted to transfer
semantic knowledge from RGB datasets to DVS, they often overlook the
significant distribution gap between the two modalities. In this paper, we
propose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing
strategy that exploits the asynchronous nature of SNNs by interpolating RGB and
DVS inputs at various time-steps. To enable label mixing in cross-modal
scenarios, we further introduce modality-aware auxiliary learning objectives.
These objectives support the time-step mixup process and enhance the model's
ability to discriminate effectively across different modalities. Our approach
enables smoother knowledge transfer, alleviates modality shift during training,
and achieves superior performance in spiking image classification tasks.
Extensive experiments demonstrate the effectiveness of our method across
multiple datasets. The code will be released after the double-blind review
process.

</details>


### [76] [MMMS: Multi-Modal Multi-Surface Interactive Segmentation](https://arxiv.org/abs/2509.12963)
*Robin Schön,Julian Lorenz,Katja Ludwig,Daniel Kienzle,Rainer Lienhart*

Main category: cs.CV

TL;DR: 提出MMMS框架和新的评估指标，通过在融合后加入交互信息的网络设计和多模态输入，显著减少用户交互次数并兼顾响应时间。


<details>
  <summary>Details</summary>
Motivation: 解决同一图像中存在多重且相互缠绕的表面分割问题，提升交互式分割在复杂场景下的准确性和效率，同时考虑用于减少延迟的系统设计约束（RGB骨干为黑盒且交互信息在后期融合）。

Method: 构建了一个网络架构，输入包含RGB图像（仅作为黑盒骨干）、若干非RGB模态、错误掩码和编码的点击信息；在图像特征提取和模态融合后再集成交互信息以降低响应时间；并提出多模态融合策略用于提升分割性能。

Result: 多模态输入能带来性能提升：在DeLiVER数据集上平均每个表面NoC@90降低最多1.28次点击，在MFNet上降低最多1.19次；此外，他们的RGB-only基线在单掩码交互分割场景中表现具竞争力甚至优于部分方法。

Conclusion: 该论文提出了一种用于交互式分割的多模态多表面（MMMS）方法，能够在用户点击引导下生成并修正多表面分割掩码，并设计了新的评估指标以处理表面重叠与相邻带来的挑战。

Abstract: In this paper, we present a method to interactively create segmentation masks
on the basis of user clicks. We pay particular attention to the segmentation of
multiple surfaces that are simultaneously present in the same image. Since
these surfaces may be heavily entangled and adjacent, we also present a novel
extended evaluation metric that accounts for the challenges of this scenario.
Additionally, the presented method is able to use multi-modal inputs to
facilitate the segmentation task. At the center of this method is a network
architecture which takes as input an RGB image, a number of non-RGB modalities,
an erroneous mask, and encoded clicks. Based on this input, the network
predicts an improved segmentation mask. We design our architecture such that it
adheres to two conditions: (1) The RGB backbone is only available as a
black-box. (2) To reduce the response time, we want our model to integrate the
interaction-specific information after the image feature extraction and the
multi-modal fusion. We refer to the overall task as Multi-Modal Multi-Surface
interactive segmentation (MMMS). We are able to show the effectiveness of our
multi-modal fusion strategy. Using additional modalities, our system reduces
the NoC@90 by up to 1.28 clicks per surface on average on DeLiVER and up to
1.19 on MFNet. On top of this, we are able to show that our RGB-only baseline
achieves competitive, and in some cases even superior performance when tested
in a classical, single-mask interactive segmentation scenario.

</details>


### [77] [ICDAR 2025 Competition on FEw-Shot Text line segmentation of ancient handwritten documents (FEST)](https://arxiv.org/abs/2509.12965)
*Silvia Zottin,Axel De Nardin,Giuseppe Branca,Claudio Piciarelli,Gian Luca Foresti*

Main category: cs.CV

TL;DR: FEST竞赛在U-DIADS-TL数据集上以每手稿仅三张标注图像的少样本设置，推动古手稿文本行分割领域发展，目标是培养鲁棒、低标注需求的自动化工具以服务人文研究。


<details>
  <summary>Details</summary>
Motivation: 历史手稿具有不规则书写、墨迹褪色、重叠行和非线性文本流等挑战，且缺乏大规模标注数据，使得传统全监督方法难以推广，因而需要研究少样本或弱监督的分割方法。

Method: 通过举办FEST竞赛，限制训练样本为每份手稿三张标注图像，要求参赛者在U-DIADS-TL数据集上开发鲁棒的少样本学习或迁移学习方法，可能融合数据增强、元学习、半监督学习或基于检测/分割的深度模型。

Result: 竞赛将促生多种适应性强的文本行分割方法，提升古文献数字化与检索效率，降低人力标注成本，并为后续研究提供基准数据和评测协议。

Conclusion: 本竞赛提出了在极少标注（每手稿仅三幅图像）条件下进行古手稿文本行分割的挑战，强调方法需适应复杂、退化和非线性布局，推动文史领域自动化分析工具的可用性。

Abstract: Text line segmentation is a critical step in handwritten document image
analysis. Segmenting text lines in historical handwritten documents, however,
presents unique challenges due to irregular handwriting, faded ink, and complex
layouts with overlapping lines and non-linear text flow. Furthermore, the
scarcity of large annotated datasets renders fully supervised learning
approaches impractical for such materials. To address these challenges, we
introduce the Few-Shot Text Line Segmentation of Ancient Handwritten Documents
(FEST) Competition. Participants are tasked with developing systems capable of
segmenting text lines in U-DIADS-TL dataset, using only three annotated images
per manuscript for training. The competition dataset features a diverse
collection of ancient manuscripts exhibiting a wide range of layouts,
degradation levels, and non-standard formatting, closely reflecting real-world
conditions. By emphasizing few-shot learning, FEST competition aims to promote
the development of robust and adaptable methods that can be employed by
humanities scholars with minimal manual annotation effort, thus fostering
broader adoption of automated document analysis tools in historical research.

</details>


### [78] [SHREC 2025: Protein surface shape retrieval including electrostatic potential](https://arxiv.org/abs/2509.12976)
*Taher Yacoub,Camille Depenveiller,Atsushi Tatsuma,Tin Barisin,Eugen Rusakov,Udo Gobel,Yuxu Peng,Shiqiang Deng,Yuki Kagaya,Joon Hong Park,Daisuke Kihara,Marco Guerra,Giorgio Palmieri,Andrea Ranieri,Ulderico Fugacci,Silvia Biasotti,Ruiwen He,Halim Benhabiles,Adnane Cabani,Karim Hammoudi,Haotian Li,Hao Huang,Chunyan Li,Alireza Tehrani,Fanwang Meng,Farnaz Heidar-Zadeh,Tuan-Anh Yang,Matthieu Montes*

Main category: cs.CV

TL;DR: 在11555个蛋白表面数据集上比较15种方法，结果显示结合静电势的检索方法效果最好，尤其对少样本类别有帮助。


<details>
  <summary>Details</summary>
Motivation: 评估在蛋白分子表面检索任务中，是否将静电势作为分子表面额外描述符能提升检索效果，尤其在有限样本类别中检验其有效性。

Method: 组织了SHREC 2025蛋白表面形状检索评测赛道，收集并标注了11555个计算了静电势的蛋白表面数据集；9支团队提交了15种方法；采用多种评价指标（Accuracy, Balanced accuracy, F1, Precision, Recall）对检索性能进行评估并比较。

Result: 在所有评价指标上，融合静电势信息的方法表现最佳；该优势在样本量有限的类别中仍然明显，表明额外的表面描述符能提高检索稳健性。

Conclusion: 结合静电势信息能显著提升基于蛋白表面形状的检索性能，尤其在数据稀缺的类别上也能带来提升。

Abstract: This SHREC 2025 track dedicated to protein surface shape retrieval involved 9
participating teams. We evaluated the performance in retrieval of 15 proposed
methods on a large dataset of 11,555 protein surfaces with calculated
electrostatic potential (a key molecular surface descriptor). The performance
in retrieval of the proposed methods was evaluated through different metrics
(Accuracy, Balanced accuracy, F1 score, Precision and Recall). The best
retrieval performance was achieved by the proposed methods that used the
electrostatic potential complementary to molecular surface shape. This
observation was also valid for classes with limited data which highlights the
importance of taking into account additional molecular surface descriptors.

</details>


### [79] [Improving Accuracy and Efficiency of Implicit Neural Representations: Making SIREN a WINNER](https://arxiv.org/abs/2509.12980)
*Hemanth Chandravamsi,Dhanush V. Shenoy,Steven H. Frankel*

Main category: cs.CV

TL;DR: 提出WINNER：对SIREN权重加入基于谱心自适应尺度的高斯噪声，简单有效地缓解谱偏置与谱瓶颈，在多类信号拟合任务中显著优于基础SIREN。


<details>
  <summary>Details</summary>
Motivation: 当SIREN的频率支持与目标信号频谱不匹配时，模型难以拟合信号，严重时出现输出近零的“谱瓶颈”；需提出一种简单且无额外参数的方法，使初始化对目标频谱有感知，从而避免训练失败。

Method: 在SIREN的基础权重初始化（均匀）上添加高斯噪声，噪声方差由目标信号的谱心（spectral centroid）自适应确定；该方法不引入额外可训练参数，类似随机傅里叶嵌入以缓解谱偏置，同时保持SIREN结构。

Result: 在音频拟合上达到或超过现有最优，在图像和3D形状拟合任务上相较于基础SIREN有显著性能提升；展示了自适应目标感知初始化在深度网络训练中的潜力。

Conclusion: 论文提出了针对SIREN（正弦表示网络）在频谱对齐不当时导致的“谱瓶颈”问题的解决方案WINNER，通过在权重均匀初始化上加入根据目标信号谱心自适应尺度的高斯噪声，缓解了谱偏置问题，从而提升了音频、图像和3D形状拟合性能。

Abstract: We identify and address a fundamental limitation of sinusoidal representation
networks (SIRENs), a class of implicit neural representations. SIRENs Sitzmann
et al. (2020), when not initialized appropriately, can struggle at fitting
signals that fall outside their frequency support. In extreme cases, when the
network's frequency support misaligns with the target spectrum, a 'spectral
bottleneck' phenomenon is observed, where the model yields to a near-zero
output and fails to recover even the frequency components that are within its
representational capacity. To overcome this, we propose WINNER - Weight
Initialization with Noise for Neural Representations. WINNER perturbs uniformly
initialized weights of base SIREN with Gaussian noise - whose noise scales are
adaptively determined by the spectral centroid of the target signal. Similar to
random Fourier embeddings, this mitigates 'spectral bias' but without
introducing additional trainable parameters. Our method achieves
state-of-the-art audio fitting and significant gains in image and 3D shape
fitting tasks over base SIREN. Beyond signal fitting, WINNER suggests new
avenues in adaptive, target-aware initialization strategies for optimizing deep
neural network training. For code and data visit
cfdlabtechnion.github.io/siren_square/.

</details>


### [80] [PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era](https://arxiv.org/abs/2509.12989)
*Xu Zheng,Chenfei Liao,Ziqiao Weng,Kaiyu Lei,Zihao Dongfang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Lu Qi,Li Chen,Danda Pani Paudel,Kailun Yang,Linfeng Zhang,Luc Van Gool,Xuming Hu*

Main category: cs.CV

TL;DR: 本文综述了全景（360度）视觉的最新进展与应用需求，提出PANORAMA全景系统架构，讨论技术突破、数据集、趋势与开放挑战，旨在推动面向实体AI时代的通用全景视觉系统研究。


<details>
  <summary>Details</summary>
Motivation: 传统针孔相机视觉在视野覆盖方面存在局限，随着机器人等领域对环境理解的需求增长，全景视觉能提供更全面的场景信息，推动更可靠的决策与更鲁棒的系统，故推动该领域的研究与系统化设计。

Method: 论文通过综述方式，整合学术与工业界的研究成果，梳理全景视觉在生成、感知、理解方面的突破，分析现有数据集，并基于实践需求提出PANORAMA系统架构及其组成与功能。

Result: 归纳了全景视觉在生成、感知、理解上的若干关键进展，提出PANORAMA架构并列出未来研究方向、开放挑战与跨社区影响，强调构建通用且鲁棒的全景AI系统的必要性与可行路径。

Conclusion: 该论文总结了全景视觉（360度视觉）在机器人、工业检测和环境监测等领域的重要性，强调其在场景感知完整性和决策可靠性上的优势，并指出该领域正处于快速发展期。作者提出了名为PANORAMA的理想全景系统架构，包含四个关键子系统，讨论了全景生成、感知、理解及相关数据集的最新进展，提出未来路线图与挑战。

Abstract: Omnidirectional vision, using 360-degree vision to understand the
environment, has become increasingly critical across domains like robotics,
industrial inspection, and environmental monitoring. Compared to traditional
pinhole vision, omnidirectional vision provides holistic environmental
awareness, significantly enhancing the completeness of scene perception and the
reliability of decision-making. However, foundational research in this area has
historically lagged behind traditional pinhole vision. This talk presents an
emerging trend in the embodied AI era: the rapid development of omnidirectional
vision, driven by growing industrial demand and academic interest. We highlight
recent breakthroughs in omnidirectional generation, omnidirectional perception,
omnidirectional understanding, and related datasets. Drawing on insights from
both academia and industry, we propose an ideal panoramic system architecture
in the embodied AI era, PANORAMA, which consists of four key subsystems.
Moreover, we offer in-depth opinions related to emerging trends and
cross-community impacts at the intersection of panoramic vision and embodied
AI, along with the future roadmap and open challenges. This overview
synthesizes state-of-the-art advancements and outlines challenges and
opportunities for future research in building robust, general-purpose
omnidirectional AI systems in the embodied AI era.

</details>


### [81] [Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection](https://arxiv.org/abs/2509.12990)
*Boyu Han,Qianqian Xu,Shilong Bao,Zhiyong Yang,Sicong Li,Qingming Huang*

Main category: cs.CV

TL;DR: 提出DR-MoE：两阶段（特征级和分类级）专家融合，结合Frozen/LoRA ViViT与三类不同目标的分类器，针对细微与稀少错误检测表现优异。


<details>
  <summary>Details</summary>
Motivation: 动机是第一人称视频中错误行为通常细微且稀少，类别不平衡与难以区分的样本使得单一模型难以稳健识别，因此设计多专家、多损失以提升对罕见/模糊错误的识别能力。

Method: 方法包含两阶段：第一阶段使用Frozen ViViT和LoRA微调的ViViT提取特征，并通过特征级专家模块融合；第二阶段训练三个目标不同的分类器（重加权交叉熵、AUC 损失、标签感知 + SAM），再通过分类级专家模块融合预测。

Result: 结果显示在检测稀有和模糊错误方面取得较强性能（论文宣称），并公开了代码库。

Conclusion: 该论文提出了一个针对第一人称视频中错误操作检测的双阶段加权专家混合框架（DR-MoE），主要用于识别细微且少见的错误样本。作者声称方法在识别稀有和模糊错误实例上效果显著。

Abstract: In this report, we address the problem of determining whether a user performs
an action incorrectly from egocentric video data. To handle the challenges
posed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted
Mixture-of-Experts (DR-MoE) framework. In the first stage, features are
extracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are
combined through a feature-level expert module. In the second stage, three
classifiers are trained with different objectives: reweighted cross-entropy to
mitigate class imbalance, AUC loss to improve ranking under skewed
distributions, and label-aware loss with sharpness-aware minimization to
enhance calibration and generalization. Their predictions are fused using a
classification-level expert module. The proposed method achieves strong
performance, particularly in identifying rare and ambiguous mistake instances.
The code is available at https://github.com/boyuh/DR-MoE.

</details>


### [82] [Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection](https://arxiv.org/abs/2509.12995)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Jinhua Zeng,Bin Li*

Main category: cs.CV

TL;DR: 用现代表征的VFM加上线性分类器比专门检测器更能稳健检测真实世界的AI生成图像；评估泛化性必须使用与模型预训练无关的新数据。


<details>
  <summary>Details</summary>
Motivation: 专门的AI生成图像检测器在真实世界场景中出现高假阴性率，导致性能崩溃；寻找更稳健且简单的方法以提升实际检测准确性。

Method: 在与专门检测器相同的数据上，对现代VFM（如Perception Encoder、Meta CLIP2）训练简单线性分类器；通过文本-图像相似性探测VLM是否学会将合成图像与伪造相关概念对齐；并构建一个在VFM预训练截断日期之后抓取的新数据集检验数据暴露效应。

Result: 在相同训练数据下，基于现代VFM的线性分类器在‘in-the-wild’基准上提高了超过20%的准确率；研究还发现新VLM能把合成图像对齐到‘AI-generated’等概念，这种能力在模型未见过的新数据上大幅下降，支持数据暴露假设。

Conclusion: 更新的视觉基础模型（VFM）配合线性分类器在检测真实世界中的AI生成图像上，表现显著优于专门设计的检测器；评估泛化性时需确保测试集与模型的全部训练历史（包括预训练）无关。

Abstract: While specialized detectors for AI-generated images excel on curated
benchmarks, they fail catastrophically in real-world scenarios, as evidenced by
their critically high false-negative rates on `in-the-wild' benchmarks. Instead
of crafting another specialized `knife' for this problem, we bring a `gun' to
the fight: a simple linear classifier on a modern Vision Foundation Model
(VFM). Trained on identical data, this baseline decisively `outguns' bespoke
detectors, boosting in-the-wild accuracy by a striking margin of over 20\%.
  Our analysis pinpoints the source of the VFM's `firepower': First, by probing
text-image similarities, we find that recent VLMs (e.g., Perception Encoder,
Meta CLIP2) have learned to align synthetic images with forgery-related
concepts (e.g., `AI-generated'), unlike previous versions. Second, we speculate
that this is due to data exposure, as both this alignment and overall accuracy
plummet on a novel dataset scraped after the VFM's pre-training cut-off date,
ensuring it was unseen during pre-training. Our findings yield two critical
conclusions: 1) For the real-world `gunfight' of AI-generated image detection,
the raw `firepower' of an updated VFM is far more effective than the
`craftsmanship' of a static detector. 2) True generalization evaluation
requires test data to be independent of the model's entire training history,
including pre-training.

</details>


### [83] [Drone Detection Using a Low-Power Neuromorphic Virtual Tripwire](https://arxiv.org/abs/2509.12997)
*Anton Eldeborg Lundin,Rasmus Winzell,Hanna Hamrell,David Gustafsson,Hannes Ovrén*

Main category: cs.CV

TL;DR: 用事件相机+脉冲神经网络在神经形态芯片上实现低功耗无人机检测，能耗显著优于边缘GPU，支持长期电池运行与虚拟警戒线部署，模型主要依赖无人机形状特征。


<details>
  <summary>Details</summary>
Motivation: 小型无人机对军事和民用目标构成日益严重的威胁，亟需早期自动化检测，且在无电力或受限场景中要求超低功耗与小体积设备。

Method: 使用事件相机采集数据，训练脉冲神经网络（SNN），并将模型部署到神经形态硬件上；评估与边缘GPU参考方案的能耗对比；通过合成数据辅助训练并分析模型依赖的特征（倾向于机身形状而非螺旋桨时序特征）。

Result: 神经形态方案在能耗上比边缘GPU低若干数量级，可实现电池供电下运行一年以上；多台单元可构成虚拟警戒线，检测到入侵时间和位置；合成数据可用于训练，模型更依赖目标形状信息。

Conclusion: 本工作展示了一个完全神经形态的无人机检测系统，结合事件相机、脉冲神经网络和神经形态芯片，能在低功耗下实现长期部署并构建虚拟警戒线。

Abstract: Small drones are an increasing threat to both military personnel and civilian
infrastructure, making early and automated detection crucial. In this work we
develop a system that uses spiking neural networks and neuromorphic cameras
(event cameras) to detect drones. The detection model is deployed on a
neuromorphic chip making this a fully neuromorphic system. Multiple detection
units can be deployed to create a virtual tripwire which detects when and where
drones enter a restricted zone. We show that our neuromorphic solution is
several orders of magnitude more energy efficient than a reference solution
deployed on an edge GPU, allowing the system to run for over a year on battery
power. We investigate how synthetically generated data can be used for
training, and show that our model most likely relies on the shape of the drone
rather than the temporal characteristics of its propellers. The small size and
low power consumption allows easy deployment in contested areas or locations
that lack power infrastructure.

</details>


### [84] [Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single Image](https://arxiv.org/abs/2509.13013)
*Gaofeng Liu,Hengsen Li,Ruoyu Gao,Xuetong Li,Zhiyuan Ma,Tao Fang*

Main category: cs.CV

TL;DR: 提出Dream3DAvatar：通过Pose-Adapter、ID-Adapter-G/ID-Adapter-R、BLIP2与多视图融合Transformer，两阶段从单张图生成高保真、文本可控且动画就绪的3D Gaussian Splat头像，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 单张图像重建三维全身头像本质上信息不足，尤其是遮挡区域的几何和纹理难以控制；需要一种既能保证多视图几何一致性又能在遮挡处提供语义和身份可控性的方案。

Method: 第一阶段：基于SDXL的轻量级多视图生成模型，加入Pose-Adapter注入SMPL-X渲染与骨架信息以增强几何与姿态一致性；加入ID-Adapter-G注入高分辨率人脸特征保持面部身份；使用BLIP2生成多视图的高质量文本描述以增强对遮挡区域的文本可控性。第二阶段：设计前馈Transformer及多视图特征融合模块，将生成图像重建为高保真3D Gaussian Splat表示；引入ID-Adapter-R通过门控机制将面部特征有效融合到重建过程以提高高频细节恢复。

Result: 大量实验表明，方法能生成真实、可动画的3D头像，无需后处理；在多个评估指标上优于现有基线方法。

Conclusion: 该论文提出了Dream3DAvatar，一种高效的、文本可控的两阶段单张图像到三维头像重建框架；通过多种adapter和多视图融合重建3D Gaussian Splat（3DGS），能在无后处理下生成可动画的高保真三维头像，且在若干指标上优于基线。

Abstract: With the rapid advancement of 3D representation techniques and generative
models, substantial progress has been made in reconstructing full-body 3D
avatars from a single image. However, this task remains fundamentally
ill-posedness due to the limited information available from monocular input,
making it difficult to control the geometry and texture of occluded regions
during generation. To address these challenges, we redesign the reconstruction
pipeline and propose Dream3DAvatar, an efficient and text-controllable
two-stage framework for 3D avatar generation. In the first stage, we develop a
lightweight, adapter-enhanced multi-view generation model. Specifically, we
introduce the Pose-Adapter to inject SMPL-X renderings and skeletal information
into SDXL, enforcing geometric and pose consistency across views. To preserve
facial identity, we incorporate ID-Adapter-G, which injects high-resolution
facial features into the generation process. Additionally, we leverage BLIP2 to
generate high-quality textual descriptions of the multi-view images, enhancing
text-driven controllability in occluded regions. In the second stage, we design
a feedforward Transformer model equipped with a multi-view feature fusion
module to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)
from the generated images. Furthermore, we introduce ID-Adapter-R, which
utilizes a gating mechanism to effectively fuse facial features into the
reconstruction process, improving high-frequency detail recovery. Extensive
experiments demonstrate that our method can generate realistic, animation-ready
3D avatars without any post-processing and consistently outperforms existing
baselines across multiple evaluation metrics.

</details>


### [85] [Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models](https://arxiv.org/abs/2509.13031)
*Yan Chen,Long Li,Teng Xi,Long Zeng,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出一种两阶段强化学习方法：先强化视觉感知（粗/细粒度），再强化推理，生成PeBR-R1，在七个数据集上显著提升视觉推理性能。


<details>
  <summary>Details</summary>
Motivation: 直接迁移用于LLM的强化学习方法到VLM上效果欠佳，因为VLM需要先实现准确的视觉感知，才能进行有效推理，因此需要联合提升感知与推理能力的训练框架。

Method: 提出两阶段强化学习框架：第一阶段通过数据层次采样与粗细粒度视觉理解增强模型的感知能力；第二阶段专注于推理能力提升。训练中采用分层数据采样来缓解RL的消失优势问题。

Result: 在七个基准数据集上的实验表明，所训练的PeBR-R1在多种视觉推理任务上均优于对比方法，验证了方法有效性。

Conclusion: 基于两阶段强化学习的设计，可以同时提升VLM的感知与推理能力，从而在多种视觉推理基准上取得显著性能提升。

Abstract: Reinforcement learning (RL) has proven highly effective in eliciting the
reasoning capabilities of large language models (LLMs). Inspired by this
success, recent studies have explored applying similar techniques to
vision-language models (VLMs), aiming to enhance their reasoning performance.
However, directly transplanting RL methods from LLMs to VLMs is suboptimal, as
the tasks faced by VLMs are inherently more complex. Specifically, VLMs must
first accurately perceive and understand visual inputs before reasoning can be
effectively performed. To address this challenge, we propose a two-stage
reinforcement learning framework designed to jointly enhance both the
perceptual and reasoning capabilities of VLMs. To mitigate the vanishing
advantage issue commonly observed in RL training, we first perform
dataset-level sampling to selectively strengthen specific capabilities using
distinct data sources. During training, the first stage focuses on improving
the model's visual perception through coarse- and fine-grained visual
understanding, while the second stage targets the enhancement of reasoning
abilities. After the proposed two-stage reinforcement learning process, we
obtain PeBR-R1, a vision-language model with significantly enhanced perceptual
and reasoning capabilities. Experimental results on seven benchmark datasets
demonstrate the effectiveness of our approach and validate the superior
performance of PeBR-R1 across diverse visual reasoning tasks.

</details>


### [86] [HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models](https://arxiv.org/abs/2509.13067)
*Xu Li,Yuxuan Liang,Xiaolei Chen,Yi Zheng,Haotian Chen,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 研究发现HR-LVLMs的视觉token重要性存在差异，CLS token有两阶段注意力行为，不同token角色互补。提出训练免费HERO，通过tile级预算与功能感知选择早期丢弃token，显著提升推理效率-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像被切分为局部tiles以增强细粒度理解，但导致视觉token数激增，带来计算和内存瓶颈，需找到在不显著损失性能下减少token的策略。

Method: 对HR-LVLM的视觉token利用做系统性实证分析，发现局部tile重要性由显著性与任务相关性共同决定；CLIP编码器的CLS token在不同层呈现两阶段注意力模式；不同阶段强调的token编码不同粒度信息并互补。基于此提出HERO框架：先估计tile级重要性分配token预算，再在编码器功能上选择互补token并在早期丢弃，整个流程训练免费。

Result: HERO在不同基准和模型规模上实现了更好的效率-精度平衡，证明通过内容自适应和功能感知的早期丢弃策略可以在无需额外训练的情况下显著减少推理开销，同时保持或仅小幅下降性能。

Conclusion: HR-LVLMs在高分辨率细粒度理解上有效，但因切分成大量token导致计算与内存开销大。HERO通过早期丢弃不重要token并结合内容自适应预算分配与功能感知选择，在不需训练的条件下改善了效率-精度权衡。

Abstract: By cropping high-resolution images into local tiles and encoding them
independently, High-Resolution Large Vision-Language Models (HR-LVLMs) have
demonstrated remarkable fine-grained visual understanding capabilities.
However, this divide-and-conquer paradigm significantly increases the number of
visual tokens, resulting in substantial computational and memory overhead. To
better understand and address this challenge, we empirically investigate visual
token utilization in HR-LVLMs and uncover three key findings: (1) the local
tiles have varying importance, jointly determined by visual saliency and task
relevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage
attention pattern across layers, with each stage attending to different types
of visual tokens; (3) the visual tokens emphasized at different stages encode
information at varying levels of granularity, playing complementary roles
within LVLMs. Building on these insights, we propose HERO, a High-resolution
visual token early dropping framework that integrates content-adaptive token
budget allocation with function-aware token selection. By accurately estimating
tile-level importance and selectively retaining visual tokens with
complementary roles, HERO achieves superior efficiency-accuracy trade-offs
across diverse benchmarks and model scales, all in a training-free manner. This
study provides both empirical insights and practical solutions toward efficient
inference in HR-LVLMs.

</details>


### [87] [TFANet: Three-Stage Image-Text Feature Alignment Network for Robust Referring Image Segmentation](https://arxiv.org/abs/2509.13070)
*Qianqi Lu,Yuxiang Xie,Jing Zhang,Shiwei Zou,Yan Chen,Xidao Luan*

Main category: cs.CV

TL;DR: TFANet提出分三阶段逐步强化图文对齐：跨尺度双向注意-跨模态扫描融合-词级语义深化，针对复杂多目标场景与语义丢失问题进行改进，提高RIS任务的对齐与分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有RIS方法在复杂场景下易出现模态错配与语言语义损失，导致同类目标定位不准确或分割不完整，需提升跨模态对齐与保留细粒度语义。

Method: 引入三阶段框架：KPS（Multiscale Linear Cross-Attention Module，跨尺度双向注意实现多粒度语义交换）、KFS（Cross-modal Feature Scanning Module，通过选择性扫描建立长程跨模态依赖并融合特征）、KIS（Word-level Linguistic Feature-guided Semantic Deepening Module，用词级引导弥补语义退化）。

Result: 通过三阶段对齐和专门模块设计，TFANet应提升了对复杂场景下目标的定位与分割完整性，增强了长程跨模态依赖建模与语义保真性。

Conclusion: 该论文提出了TFANet，通过三阶段的层次化对齐策略，系统性提升了图像与文本在细粒度任务中的对齐能力。方法在复杂场景、多目标及语义丢失问题上有针对性改进，合理且具有实际意义。

Abstract: Referring Image Segmentation (RIS) is a task that segments image regions
based on language expressions, requiring fine-grained alignment between two
modalities. However, existing methods often struggle with multimodal
misalignment and language semantic loss, especially in complex scenes
containing multiple visually similar objects, where uniquely described targets
are frequently mislocalized or incompletely segmented. To tackle these
challenges, this paper proposes TFANet, a Three-stage Image-Text Feature
Alignment Network that systematically enhances multimodal alignment through a
hierarchical framework comprising three stages: Knowledge Plus Stage (KPS),
Knowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the
first stage, we design the Multiscale Linear Cross-Attention Module (MLAM),
which facilitates bidirectional semantic exchange between visual features and
textual representations across multiple scales. This establishes rich and
efficient alignment between image regions and different granularities of
linguistic descriptions. Subsequently, the KFS further strengthens feature
alignment through the Cross-modal Feature Scanning Module (CFSM), which applies
multimodal selective scanning to capture long-range dependencies and construct
a unified multimodal representation. This is essential for modeling long-range
cross-modal dependencies and enhancing alignment accuracy in complex scenes.
Finally, in the KIS, we propose the Word-level Linguistic Feature-guided
Semantic Deepening Module (WFDM) to compensate for semantic degradation
introduced in earlier stages.

</details>


### [88] [Using KL-Divergence to Focus Frequency Information in Low-Light Image Enhancement](https://arxiv.org/abs/2509.13083)
*Yan Xingyang,Huang Xiaohong,Zhang Zhao,You Tian,Xu Ziheng*

Main category: cs.CV

TL;DR: 提出LLFDisc：U形频率感知网络+基于闭式KL的频域分布损失与VGG-KL感知损失，能更好对齐傅里叶信息，改善结构与亮度恢复，实验证明优于传统MSE方法并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统傅里叶频域建模常用像素级损失（如MSE），过分关注局部像素导致全局频域信息丢失，作者希望直接在频域层面以分布对齐方式更稳健地恢复亮度与结构信息。

Method: 提出了U形深度增强网络（LLFDisc），结合跨注意力和门控模块以增强频率感知能力；在损失设计上引入频域分布感知损失，利用闭式KL散度最小化频域信息分布差异；并在VGG感知损失上嵌入KL散度以提升结构保真度。

Result: 在多项基准测试上，LLFDisc在主观视觉效果和客观指标上均优于现有方法，文中宣称达到SOTA，且代码将公开。

Conclusion: LLFDisc通过在频域直接拟合傅里叶信息并引入基于KL散度的分布感知损失，配合U形网络、跨注意力与门控机制，实现了比传统逐像素MSE更鲁棒的频域对齐，从而在定性和定量上均取得了SOTA性能。

Abstract: In the Fourier domain, luminance information is primarily encoded in the
amplitude spectrum, while spatial structures are captured in the phase
components. The traditional Fourier Frequency information fitting employs
pixel-wise loss functions, which tend to focus excessively on local information
and may lead to global information loss. In this paper, we present LLFDisc, a
U-shaped deep enhancement network that integrates cross-attention and gating
mechanisms tailored for frequency-aware enhancement. We propose a novel
distribution-aware loss that directly fits the Fourier-domain information and
minimizes their divergence using a closed-form KL-Divergence objective. This
enables the model to align Fourier-domain information more robustly than with
conventional MSE-based losses. Furthermore, we enhance the perceptual loss
based on VGG by embedding KL-Divergence on extracted deep features, enabling
better structural fidelity. Extensive experiments across multiple benchmarks
demonstrate that LLFDisc achieves state-of-the-art performance in both
qualitative and quantitative evaluations. Our code will be released at:
https://github.com/YanXY000/LLFDisc

</details>


### [89] [Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling](https://arxiv.org/abs/2509.13084)
*Yunyao Lu,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

TL;DR: 本文提出双网络+不确定性感知伪标签和对比学习的半监督3D医学图像分割方法，有效抑制伪标签噪声并增强特征监督，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 有监督分割模型依赖大量标注，但医学影像标注昂贵，半监督方法通过伪标签利用未标注数据，但现有方法受伪标签噪声和特征空间监督不足的影响。

Method: 双网络架构+交叉一致性增强（使用交叉伪标签与熵过滤监督）+基于KL散度的不确定性动态权重调整+自监督对比学习将不确定体素特征对齐到可靠类别原型。

Result: 在Left Atrial、NIH Pancreas和BraTS-2019三组3D数据集上表现稳定优越，例如在Left Atrial上以10%标注数据达到89.95% Dice，同时消融实验验证各模块有效性。

Conclusion: 提出了一个基于双网络的半监督3D医学图像分割框架，通过交叉一致性增强模块、动态加权的不确定性感知伪标签策略及自监督对比学习对不确定体素进行特征对齐，从而减少伪标签噪声并提升特征空间监督，实验证明在三个数据集上均优于现有方法。

Abstract: Despite the remarkable performance of supervised medical image segmentation
models, relying on a large amount of labeled data is impractical in real-world
situations. Semi-supervised learning approaches aim to alleviate this challenge
using unlabeled data through pseudo-label generation. Yet, existing
semi-supervised segmentation methods still suffer from noisy pseudo-labels and
insufficient supervision within the feature space. To solve these challenges,
this paper proposes a novel semi-supervised 3D medical image segmentation
framework based on a dual-network architecture. Specifically, we investigate a
Cross Consistency Enhancement module using both cross pseudo and
entropy-filtered supervision to reduce the noisy pseudo-labels, while we design
a dynamic weighting strategy to adjust the contributions of pseudo-labels using
an uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In
addition, we use a self-supervised contrastive learning mechanism to align
uncertain voxel features with reliable class prototypes by effectively
differentiating between trustworthy and uncertain predictions, thus reducing
prediction uncertainty. Extensive experiments are conducted on three 3D
segmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed
approach consistently exhibits superior performance across various settings
(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared to
the state-of-the-art methods. Furthermore, the usefulness of the proposed
modules is further validated via ablation experiments.

</details>


### [90] [A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual Assembly Control](https://arxiv.org/abs/2509.13089)
*Jonas Werheid,Shengjie He,Aymen Gannouni,Anas Abdelrazeq,Robert H. Schmitt*

Main category: cs.CV

TL;DR: 使用基于CAD的合成图像流水线训练目标检测器，在模拟与真实数据上均表现优异，为SME提供一种低成本、高效的视觉装配检测方案。


<details>
  <summary>Details</summary>
Motivation: 将计算机视觉用于装配质量控制对SME有成本和数据需求障碍，合成数据有潜力减少人工采集与标注，但在装配质量场景的实际应用仍有限，因此提出一个易集成、数据高效的解决方案。

Method: 基于CAD模型生成模拟装配场景并渲染图像，构建带标注的合成数据集；使用目标检测算法（可能为YOLO或Faster R-CNN类模型）在合成数据上训练，并在真实拍摄数据上测试迁移性能。

Result: 在合成训练数据上mAP@0.5:0.95高达99.5%，在真实相机拍摄的测试数据上迁移mAP达约93%，证明生成流水线和方法在准确识别行星齿轮系统组件方面有效。

Conclusion: 合成数据结合基于CAD的场景生成与目标检测，可显著降低人工采集标注成本；在模拟数据上达到极高的检测精度，并在真实摄像头数据上取得良好迁移效果，表明对中小企业具有实际应用价值。

Abstract: Quality control of assembly processes is essential in manufacturing to ensure
not only the quality of individual components but also their proper integration
into the final product. To assist in this matter, automated assembly control
using computer vision methods has been widely implemented. However, the costs
associated with image acquisition, annotation, and training of computer vision
algorithms pose challenges for integration, especially for small- and
medium-sized enterprises (SMEs), which often lack the resources for extensive
training, data collection, and manual image annotation. Synthetic data offers
the potential to reduce manual data collection and labeling. Nevertheless, its
practical application in the context of assembly quality remains limited. In
this work, we present a novel approach for easily integrable and data-efficient
visual assembly control. Our approach leverages simulated scene generation
based on computer-aided design (CAD) data and object detection algorithms. The
results demonstrate a time-saving pipeline for generating image data in
manufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)
up to 99,5% for correctly identifying instances of synthetic planetary gear
system components within our simulated training data, and up to 93% when
transferred to real-world camera-captured testing data. This research
highlights the effectiveness of synthetic data generation within an adaptable
pipeline and underscores its potential to support SMEs in implementing
resource-efficient visual assembly control solutions.

</details>


### [91] [Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery Detection -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2509.13107)
*Kohou Wang,Huan Hu,Xiang Liu,Zezhou Chen,Ping Chen,Zhaoxiang Liu,Shiguo Lian*

Main category: cs.CV

TL;DR: 提出HDFF：将Swin-MLP、CoAtNet、EfficientNetV2、DaViT的多阶段微调特征拼接并训分类器，在MultiFFDI上得分0.96852，排名20/184，验证了层次化模型融合的效果。


<details>
  <summary>Details</summary>
Motivation: 针对深度伪造手段多样、检测模型需具备高泛化性的问题，通过多模型融合利用不同架构的互补性提升稳健性和泛化能力。

Method: 采用四个不同架构的预训练子模型（Swin-MLP、CoAtNet、EfficientNetV2、DaViT），对MultiFFDI数据集进行多阶段微调；将各子模型的特征向量拼接后训练最终分类器，实现层次化深度特征融合。

Result: 在MultiFFDI竞赛中私有榜单得分0.96852，排名第20，证明所提融合策略在复杂图像分类/伪造检测任务上的有效性。

Conclusion: 纸提出的HDFF通过融合多种预训练模型特征，有效提升了面部伪造检测的泛化能力，最终在竞赛中获得了较好名次（私有榜0.96852，20/184）。

Abstract: The proliferation of sophisticated deepfake technology poses significant
challenges to digital security and authenticity. Detecting these forgeries,
especially across a wide spectrum of manipulation techniques, requires robust
and generalized models. This paper introduces the Hierarchical Deep Fusion
Framework (HDFF), an ensemble-based deep learning architecture designed for
high-performance facial forgery detection. Our framework integrates four
diverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,
which are meticulously fine-tuned through a multi-stage process on the
MultiFFDI dataset. By concatenating the feature representations from these
specialized models and training a final classifier layer, HDFF effectively
leverages their collective strengths. This approach achieved a final score of
0.96852 on the competition's private leaderboard, securing the 20th position
out of 184 teams, demonstrating the efficacy of hierarchical fusion for complex
image classification tasks.

</details>


### [92] [Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving](https://arxiv.org/abs/2509.13116)
*Ruibo Li,Hanyu Shi,Zhe Wang,Guosheng Lin*

Main category: cs.CV

TL;DR: 用前景/背景或非地面/地面掩码替代运动标注，结合鲁棒一致性Chamfer损失，实现弱/自监督点云类无关运动预测，在保持性能的同时大幅减少注释需求。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中理解动态环境重要，但获取类别无关的运动标注成本高。观察到户外场景前景（移动）与背景（静止）分离，利用前景/背景或非地面/地面掩码可替代昂贵运动标注以指导模型学习。

Method: 基于前景/背景或非地面/地面掩码，引入弱监督范式，结合自监督损失并设计了鲁棒一致性感知Chamfer距离损失，利用多帧信息和鲁棒惩罚以抑制异常值。提出三种方法：弱监督（部分/完全掩码）、更少注释弱监督（利用非地面提示）和完全自监督（无注释）。

Result: 在实验中，弱监督和自监督模型优于现有自监督方法；弱监督方法在某些指标上可与部分监督方法相当，表明在注释量与性能之间取得良好平衡。

Conclusion: 该论文提出弱监督和自监督的点云类无关运动预测方法，通过利用前景/背景或非地面/地面掩码替代运动标注，实现显著减少注释量的同时保持性能。

Abstract: Understanding motion in dynamic environments is critical for autonomous
driving, thereby motivating research on class-agnostic motion prediction. In
this work, we investigate weakly and self-supervised class-agnostic motion
prediction from LiDAR point clouds. Outdoor scenes typically consist of mobile
foregrounds and static backgrounds, allowing motion understanding to be
associated with scene parsing. Based on this observation, we propose a novel
weakly supervised paradigm that replaces motion annotations with fully or
partially annotated (1%, 0.1%) foreground/background masks for supervision. To
this end, we develop a weakly supervised approach utilizing
foreground/background cues to guide the self-supervised learning of motion
prediction models. Since foreground motion generally occurs in non-ground
regions, non-ground/ground masks can serve as an alternative to
foreground/background masks, further reducing annotation effort. Leveraging
non-ground/ground cues, we propose two additional approaches: a weakly
supervised method requiring fewer (0.01%) foreground/background annotations,
and a self-supervised method without annotations. Furthermore, we design a
Robust Consistency-aware Chamfer Distance loss that incorporates multi-frame
information and robust penalty functions to suppress outliers in
self-supervised learning. Experiments show that our weakly and self-supervised
models outperform existing self-supervised counterparts, and our weakly
supervised models even rival some supervised ones. This demonstrates that our
approaches effectively balance annotation effort and performance.

</details>


### [93] [Advancing Real-World Parking Slot Detection with Large-Scale Dataset and Semi-Supervised Baseline](https://arxiv.org/abs/2509.13133)
*Zhihao Zhang,Chunyu Lin,Lang Nie,Jiyuan Wang,Yao Zhao*

Main category: cs.CV

TL;DR: 提供了最大规模的环视停车位数据集CRPS-D和首个半监督停车位检测方法SS-PSD，通过教师-学生与置信度掩码一致性显著提高检测效果并公开资源。


<details>
  <summary>Details</summary>
Motivation: 现有数据集规模小且场景缺乏真实噪声，人工标注昂贵且易出错，亟需更大、更复杂的数据集与利用未标注数据的算法以提升停车位检测鲁棒性。

Method: 构建大规模环视摄像头数据集（多光照、天气、倾斜车位等），并设计半监督教师-学生框架（置信度引导的掩码一致性和自适应特征扰动）用于利用未标注数据进行训练。

Result: 在CRPS-D和现有数据集上，SS-PSD优于当前SoTA方法，随着未标注数据增多，性能提升更明显；同时公开了代码与数据集。

Conclusion: 本文提出了大型停车位检测数据集CRPS-D并提出半监督检测基线SS-PSD，显著提升了泊车位检测性能，尤其在大量未标注数据条件下效果更好。

Abstract: As automatic parking systems evolve, the accurate detection of parking slots
has become increasingly critical. This study focuses on parking slot detection
using surround-view cameras, which offer a comprehensive bird's-eye view of the
parking environment. However, the current datasets are limited in scale, and
the scenes they contain are seldom disrupted by real-world noise (e.g., light,
occlusion, etc.). Moreover, manual data annotation is prone to errors and
omissions due to the complexity of real-world conditions, significantly
increasing the cost of annotating large-scale datasets. To address these
issues, we first construct a large-scale parking slot detection dataset (named
CRPS-D), which includes various lighting distributions, diverse weather
conditions, and challenging parking slot variants. Compared with existing
datasets, the proposed dataset boasts the largest data scale and consists of a
higher density of parking slots, particularly featuring more slanted parking
slots. Additionally, we develop a semi-supervised baseline for parking slot
detection, termed SS-PSD, to further improve performance by exploiting
unlabeled data. To our knowledge, this is the first semi-supervised approach in
parking slot detection, which is built on the teacher-student model with
confidence-guided mask consistency and adaptive feature perturbation.
Experimental results demonstrate the superiority of SS-PSD over the existing
state-of-the-art (SoTA) solutions on both the proposed dataset and the existing
dataset. Particularly, the more unlabeled data there is, the more significant
the gains brought by our semi-supervised scheme. The relevant source codes and
the dataset have been made publicly available at
https://github.com/zzh362/CRPS-D.

</details>


### [94] [MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation](https://arxiv.org/abs/2509.13149)
*Minqing Huang,Shouyi Lu,Boyuan Zheng,Ziyao Li,Xiao Tang,Guirong Zhuo*

Main category: cs.CV

TL;DR: MSDNet用多阶段（重建引导 + 扩散引导）特征蒸馏，把LiDAR先验高效迁移到4D雷达，实现高保真低延迟的点云超分和改进下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有4D雷达超分方法训练成本高或依赖复杂扩散采样，导致推理延迟高且泛化差，难以兼顾精度与效率。

Method: 两阶段蒸馏：第一阶段为基于重建的特征蒸馏，通过特征重建对齐并稠密化学生特征；第二阶段为基于扩散的特征蒸馏，将阶段一输出视作带噪教师特征，使用轻量级扩散网络精炼，并引入噪声适配器匹配特征噪声与扩散时间步。

Result: 在VoD与内部数据集上，MSDNet在重建质量和推理速度上均优于对比方法，并在下游任务上持续提升性能。

Conclusion: MSDNet通过多阶段蒸馏将稠密LiDAR先验高效迁移到4D雷达特征，实现高质量重建与低延迟推理的平衡。

Abstract: 4D radar super-resolution, which aims to reconstruct sparse and noisy point
clouds into dense and geometrically consistent representations, is a
foundational problem in autonomous perception. However, existing methods often
suffer from high training cost or rely on complex diffusion-based sampling,
resulting in high inference latency and poor generalization, making it
difficult to balance accuracy and efficiency. To address these limitations, we
propose MSDNet, a multi-stage distillation framework that efficiently transfers
dense LiDAR priors to 4D radar features to achieve both high reconstruction
quality and computational efficiency. The first stage performs
reconstruction-guided feature distillation, aligning and densifying the
student's features through feature reconstruction. In the second stage, we
propose diffusion-guided feature distillation, which treats the stage-one
distilled features as a noisy version of the teacher's representations and
refines them via a lightweight diffusion network. Furthermore, we introduce a
noise adapter that adaptively aligns the noise level of the feature with a
predefined diffusion timestep, enabling a more precise denoising. Extensive
experiments on the VoD and in-house datasets demonstrate that MSDNet achieves
both high-fidelity reconstruction and low-latency inference in the task of 4D
radar point cloud super-resolution, and consistently improves performance on
downstream tasks. The code will be publicly available upon publication.

</details>


### [95] [TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual Document Images](https://arxiv.org/abs/2509.13151)
*Rohan Kumar,Jyothi Swaroopa Jinka,Ravi Kiran Sarvadevabhatla*

Main category: cs.CV

TL;DR: TexTAR：基于2D RoPE的上下文感知Transformer与新的数据选择管线，在作者构建的多语言多域数据集MMTAD上，实现了比现有方法更好的文本属性识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算效率或在噪声、多语言场景下的适应性方面存在不足，难以准确识别诸如加粗、斜体、下划线和删除线等文本属性，因此需要一种高效且具上下文感知能力的解决方案。

Method: 使用多任务学习框架，结合上下文增强的数据选择管线，并在Transformer中引入2D RoPE风格的位置编码以将文本上下文信息整合到输入特征中，从而更精确地预测字体属性。

Result: 在作者构建并发布的多语言、多域数据集MMTAD上，TexTAR在各种评估指标上优于现有方法，证明了上下文信息对提升TAR性能的有效性。

Conclusion: TexTAR提出了一种面向文本属性识别的上下文感知Transformer模型，通过改进的数据选择和2D RoPE风格的位置编码机制显著提升了多任务、多语言、多场景下的属性识别性能。

Abstract: Recognizing textual attributes such as bold, italic, underline and strikeout
is essential for understanding text semantics, structure, and visual
presentation. These attributes highlight key information, making them crucial
for document analysis. Existing methods struggle with computational efficiency
or adaptability in noisy, multilingual settings. To address this, we introduce
TexTAR, a multi-task, context-aware Transformer for Textual Attribute
Recognition (TAR). Our novel data selection pipeline enhances context
awareness, and our architecture employs a 2D RoPE (Rotary Positional
Embedding)-style mechanism to incorporate input context for more accurate
attribute predictions. We also introduce MMTAD, a diverse, multilingual,
multi-domain dataset annotated with text attributes across real-world documents
such as legal records, notices, and textbooks. Extensive evaluations show
TexTAR outperforms existing methods, demonstrating that contextual awareness
contributes to state-of-the-art TAR performance.

</details>


### [96] [Enhancing Video Large Language Models with Structured Multi-Video Collaborative Reasoning (early version)](https://arxiv.org/abs/2509.13161)
*Zhihao He,Tianyao He,Tieyuan Chen,Yun Xu,Huabin Liu,Chaofan Gan,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 提出通过时空图结构化视频并对多视频图进行融合，再构建结构化提示以输入大语言模型，从而用相关视频协同提升视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 单个视频往往存在时空信息缺失，导致视频-语言模型在复杂推理时产生幻觉或错误；引入多相关视频可以补全缺失信息，但直接输入大量冗余视频令牌到模型会无效或有害，故需高效结构化与融合方法。

Method: 构建视频结构化模块（将视频表示为时空图）、图融合模块（将相关视频的结构化知识与有价值信息融合到增强的图节点）、以及多视频结构化提示（将图、视觉、文本令牌整合输入大语言模型）。

Result: 通过大规模实验验证框架有效性，展示该方法在提升视频推理准确性和减少幻觉方面的潜力（文中宣称广泛实验证实）。

Conclusion: 该论文提出了一个用于多视频协同的视频-语言模型框架，通过结构化视频表示和图融合来缓解单视频推理中的时空信息不完整问题，从而降低幻觉与不准确性。

Abstract: Despite the prosperity of the video language model, the current pursuit of
comprehensive video reasoning is thwarted by the inherent spatio-temporal
incompleteness within individual videos, resulting in hallucinations and
inaccuracies. A promising solution is to augment the reasoning performance with
multiple related videos. However, video tokens are numerous and contain
redundant information, so directly feeding the relevant video data into a large
language model to enhance responses could be counterproductive. To address this
challenge, we propose a multi-video collaborative framework for video language
models. For efficient and flexible video representation, we establish a Video
Structuring Module to represent the video's knowledge as a spatio-temporal
graph. Based on the structured video representation, we design the Graph Fusion
Module to fuse the structured knowledge and valuable information from related
videos into the augmented graph node tokens. Finally, we construct an elaborate
multi-video structured prompt to integrate the graph, visual, and textual
tokens as the input to the large language model. Extensive experiments
substantiate the effectiveness of our framework, showcasing its potential as a
promising avenue for advancing video language models.

</details>


### [97] [WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory](https://arxiv.org/abs/2509.13172)
*Ruifei Ding,Zhe Chen,Wen Fan,Chen Long,Huijuan Xiao,Yelu Zeng,Zhen Dong,Bisheng Yang*

Main category: cs.CV

TL;DR: WHU-STree：一个跨城、多模态、注释丰富的街道树数据集（21k树、50种），支持10+任务，基准实验证实多模态融合与跨域泛化的重要性，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 城市街道树对生态和社会福祉重要，但传统实地调查耗时费力；现有自动化MMS数据集存在规模小、标注欠缺或单模态等限制，阻碍综合分析和现实部署，因而需要一个大规模、跨城、多模态且注释全面的数据集来推动算法研究与工程应用。

Method: 通过在两座不同城市使用移动测绘系统同步采集点云与高分辨率图像，构建包含21,007个树实例、50个树种和2个形态参数的多模态数据集，并为超过10项街道树相关任务设计注释与评估基线；在树种分类和个体树分割两项关键任务上基准测试代表性方法，分析多模态融合和跨域适应性效果。

Result: 发布WHU-STree数据集（21,007实例、50树种、同步点云与高分辨率图像），基准实验显示多模态数据融合能显著提升性能，且跨域通用性（即不同城市间迁移）是实际应用的关键挑战；论文还指出了多模态融合、多任务协同、跨域泛化、空间模式学习和面向街树管理的大型多模态语言模型等未来方向。

Conclusion: 该论文提出并发布了一个跨城市、多模态、注释丰富的街道树数据集WHU-STree，填补了现有MMS采集树数据集在场景规模、标注丰富度和模态多样性方面的不足，为街道树资产管理和相关视觉任务提供了重要数据基础。

Abstract: Street trees are vital to urban livability, providing ecological and social
benefits. Establishing a detailed, accurate, and dynamically updated street
tree inventory has become essential for optimizing these multifunctional assets
within space-constrained urban environments. Given that traditional field
surveys are time-consuming and labor-intensive, automated surveys utilizing
Mobile Mapping Systems (MMS) offer a more efficient solution. However, existing
MMS-acquired tree datasets are limited by small-scale scene, limited
annotation, or single modality, restricting their utility for comprehensive
analysis. To address these limitations, we introduce WHU-STree, a cross-city,
richly annotated, and multi-modal urban street tree dataset. Collected across
two distinct cities, WHU-STree integrates synchronized point clouds and
high-resolution images, encompassing 21,007 annotated tree instances across 50
species and 2 morphological parameters. Leveraging the unique characteristics,
WHU-STree concurrently supports over 10 tasks related to street tree inventory.
We benchmark representative baselines for two key tasks--tree species
classification and individual tree segmentation. Extensive experiments and
in-depth analysis demonstrate the significant potential of multi-modal data
fusion and underscore cross-domain applicability as a critical prerequisite for
practical algorithm deployment. In particular, we identify key challenges and
outline potential future works for fully exploiting WHU-STree, encompassing
multi-modal fusion, multi-task collaboration, cross-domain generalization,
spatial pattern learning, and Multi-modal Large Language Model for street tree
asset management. The WHU-STree dataset is accessible at:
https://github.com/WHU-USI3DV/WHU-STree.

</details>


### [98] [More performant and scalable: Rethinking contrastive vision-language pre-training of radiology in the LLM era](https://arxiv.org/abs/2509.13175)
*Yingtai Li,Haoran Lai,Xiaoqian Zhou,Shuai Ming,Wenxin Ma,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: 用LLMs自动生成大规模银标准标签，低成本使能有监督预训练，从而在医学视觉-语言任务上带来显著性能提升并实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像-报告对齐的数据稀缺与标注成本问题，探索LLMs在自动标注与放大有监督预训练数据方面的潜力，从而提升视觉-语言模型的性能与可扩展性。

Method: 使用现代LLMs对放射学报告进行直接分类/标签提取（无需复杂提示工程），构建50k CT图像-报告对的银标准数据集；在此基础上训练3D ResNet-18视觉编码器并采用CLIP式对比视觉-语言预训练；评估零样本诊断和跨模态检索任务。

Result: LLM提取标签在实验中达到>96% AUC，成本约$3获得50k对数据；基于银标准训练的视觉编码器性能与基于专门BERT模型提取的标签相当；在多个数据集上获得SOTA结果：CT-RATE零样本诊断AUC 83.8%、RAD-ChestCT AUC 77.3%、图像-图像MAP@50=53.7%、报告-图像Recall@100=52.2%。

Conclusion: 本文展示了利用大语言模型（LLMs）自动从放射科报告中提取诊断标签，低成本构建大规模“银标准”数据集，并通过有监督预训练显著提升医学对比视觉-语言对齐性能。

Abstract: The emergence of Large Language Models (LLMs) presents unprecedented
opportunities to revolutionize medical contrastive vision-language
pre-training. In this paper, we show how LLMs can facilitate large-scale
supervised pre-training, thereby advancing vision-language alignment. We begin
by demonstrate that modern LLMs can automatically extract diagnostic labels
from radiology reports with remarkable precision (>96\% AUC in our experiments)
without complex prompt engineering, enabling the creation of large-scale
"silver-standard" datasets at a minimal cost (~\$3 for 50k CT image-report
pairs). Further, we find that vision encoder trained on this "silver-standard"
dataset achieves performance comparable to those trained on labels extracted by
specialized BERT-based models, thereby democratizing the access to large-scale
supervised pre-training. Building on this foundation, we proceed to reveal that
supervised pre-training fundamentally improves contrastive vision-language
alignment. Our approach achieves state-of-the-art performance using only a 3D
ResNet-18 with vanilla CLIP training, including 83.8\% AUC for zero-shot
diagnosis on CT-RATE, 77.3\% AUC on RAD-ChestCT, and substantial improvements
in cross-modal retrieval (MAP@50=53.7\% for image-image, Recall@100=52.2\% for
report-image). These results demonstrate the potential of utilizing LLMs to
facilitate {\bf more performant and scalable} medical AI systems. Our code is
avaiable at https://github.com/SadVoxel/More-performant-and-scalable.

</details>


### [99] [Road Obstacle Video Segmentation](https://arxiv.org/abs/2509.13181)
*Shyam Nandan Rai,Shyamgopal Karthik,Mariana-Iuliana Georgescu,Barbara Caputo,Carlo Masone,Zeynep Akata*

Main category: cs.CV

TL;DR: 将道路障碍物分割从逐帧问题提升为视频任务，构建基准、评估多种方法并提出两种基于视觉基础模型的强基线，实现在长序列视频上的SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多在单帧上做道路障碍物分割，忽略时序相关性，导致连续帧预测不一致；因此将问题设为视频级别以利用帧间相关性提升稳定性和精度。

Method: 通过收集/改造四个道路障碍物视频分割基准，设计评估协议，对11种现有图像和视频分割方法进行比较实验；并基于视觉基础模型（可能为大模型或预训练视觉模型）构建两种时序感知的基线方法以提高一致性与鲁棒性。

Result: 在四个适配的评测集上，对比11种方法，提出的两种基线在长序列视频上表现最佳，达到新的最先进水平，并提供关于方法性能和设计取舍的分析与见解。

Conclusion: 本文提出将道路障碍物分割视为时序任务，构建并适配了四个视频分割评测基准，评估了11种图像/视频分割方法，并基于视觉基础模型提出两种强基线，取得了长序列视频分割的新SOTA。

Abstract: With the growing deployment of autonomous driving agents, the detection and
segmentation of road obstacles have become critical to ensure safe autonomous
navigation. However, existing road-obstacle segmentation methods are applied on
individual frames, overlooking the temporal nature of the problem, leading to
inconsistent prediction maps between consecutive frames. In this work, we
demonstrate that the road-obstacle segmentation task is inherently temporal,
since the segmentation maps for consecutive frames are strongly correlated. To
address this, we curate and adapt four evaluation benchmarks for road-obstacle
video segmentation and evaluate 11 state-of-the-art image- and video-based
segmentation methods on these benchmarks. Moreover, we introduce two strong
baseline methods based on vision foundation models. Our approach establishes a
new state-of-the-art in road-obstacle video segmentation for long-range video
sequences, providing valuable insights and direction for future research.

</details>


### [100] [Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection in Public Surveillance](https://arxiv.org/abs/2509.13210)
*Ligang Chang,Shengkai Xu,Liangchang Shen,Binhan Xu,Junqiao Wang,Tianyu Shi,Yanhui Du*

Main category: cs.CV

TL;DR: 提出了一种轻量化的时空暴力检测框架Vi-SAFE：用GhostNetV3+EMA优化的YOLOv8做人体检测，TSN做时序分类，在RWF-2000上准确率0.88且效率较高。


<details>
  <summary>Details</summary>
Motivation: 应对公共监控中小目标（行人）、复杂环境和实时时序分析的挑战，既要保证准确率又需降低计算资源以便在线部署。

Method: 使用轻量化的GhostNetV3作为YOLOv8骨干，加入EMA注意力机制并进行剪枝以降低计算成本；YOLOv8负责检测并裁剪出行人区域，TSN在提取到的人体区域上进行二分类（暴力/非暴力）；两部分分别训练。

Result: 在RWF-2000数据集上，Vi-SAFE准确率为0.88，高于仅用TSN（0.77），并在效率上也优于若干现有方法，证明了该方法在公共安全监控中的实用性。

Conclusion: 该论文提出了Vi-SAFE，一个结合改进YOLOv8与TSN的时空框架，用于视频监控下的暴力检测，实验在RWF-2000上达到了0.88的准确率，优于单独TSN及部分现有方法。

Abstract: Violence detection in public surveillance is critical for public safety. This
study addresses challenges such as small-scale targets, complex environments,
and real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal
framework that integrates an enhanced YOLOv8 with a Temporal Segment Network
(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as
a lightweight backbone, an exponential moving average (EMA) attention
mechanism, and pruning to reduce computational cost while maintaining accuracy.
YOLOv8 and TSN are trained separately on pedestrian and violence datasets,
where YOLOv8 extracts human regions and TSN performs binary classification of
violent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE
achieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming
existing methods in both accuracy and efficiency, demonstrating its
effectiveness for public safety surveillance. Code is available at
https://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.

</details>


### [101] [End4: End-to-end Denoising Diffusion for Diffusion-Based Inpainting Detection](https://arxiv.org/abs/2509.13214)
*Fei Wang,Xuecheng Wu,Zheng Zhang,Danlei Huang,Yuheng Huang,BoWang*

Main category: cs.CV

TL;DR: 提出End4：通过去噪重建与SPFM多尺度融合提升扩散式修复图像检测效果，泛化好且鲁棒，伴随新基准评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以识别扩散模型产生的修复（inpainting）图像，即使训练集中包含类似修补样本，故提出针对扩散类修复图像检测的专门方法以应对潜在滥用风险。

Method: 提出端到端去噪重建检测框架（End4），构建去噪重建模型以对齐重建与检测的潜在空间，并设计Scale-aware Pyramid-like Fusion Module(SPFM)在不同尺度的注意力引导下融合特征以提升判别性；并建立包含五类掩模区域的基准数据集进行评估。

Result: 在构建的五类掩模基准上，End4对未见掩模模式有较好泛化能力，且在多种扰动下保持稳健，实验结果显示优于现有对比方法（文中给出详细定量指标）。

Conclusion: End4通过在重建和检测流程之间对齐潜在空间、引入SPFM增强多尺度局部特征，能够有效检测基于扩散模型的修复图像并对未见掩模模式和扰动保持鲁棒性。

Abstract: The powerful generative capabilities of diffusion models have significantly
advanced the field of image synthesis, enhancing both full image generation and
inpainting-based image editing. Despite their remarkable advancements,
diffusion models also raise concerns about potential misuse for malicious
purposes. However, existing approaches struggle to identify images generated by
diffusion-based inpainting models, even when similar inpainted images are
included in their training data. To address this challenge, we propose a novel
detection method based on End-to-end denoising diffusion (End4). Specifically,
End4 designs a denoising reconstruction model to improve the alignment degree
between the latent spaces of the reconstruction and detection processes, thus
reconstructing features that are more conducive to detection. Meanwhile, it
leverages a Scale-aware Pyramid-like Fusion Module (SPFM) that refines local
image features under the guidance of attention pyramid layers at different
scales, enhancing feature discriminability. Additionally, to evaluate detection
performance on inpainted images, we establish a comprehensive benchmark
comprising images generated from five distinct masked regions. Extensive
experiments demonstrate that our End4 effectively generalizes to unseen masking
patterns and remains robust under various perturbations. Our code and dataset
will be released soon.

</details>


### [102] [Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation](https://arxiv.org/abs/2509.13229)
*Hugo Carlesso,Josiane Mothe,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 提出面向轻量级网络的课程式多任务自监督框架CMTSSL，通过掩码建模+解耦空间/光谱拼图并结合课程学习，提升高光谱影像下游分割性能，适合卫星机载部署。


<details>
  <summary>Details</summary>
Motivation: 高光谱影像具有高维光谱通道，但卫星带宽和机载计算受限，需要紧凑高效的模型；现有自监督方法要么只关注单一任务（空间或光谱），要么模型过大，不适合机载部署，因此需要一个针对轻量级网络、能同时学习空间与光谱表示的自监督框架。

Method: 提出CMTSSL框架：1) 主任务为掩码图像建模以恢复被遮挡的光谱-空间信息；2) 两个解耦的拼图子任务分别处理空间和光谱重排序；3) 采用课程学习策略（从简单到复杂逐步增加遮挡和扰动）来训练轻量级编码器；整体设计同时进行空间与光谱自监督，计算高效，适合资源受限环境。

Result: 在四个公开基准数据集上的下游语义分割任务中，使用的轻量级架构相较于一些先进模型在参数量上减少超过1.6万倍，同时在性能上取得一致性提升，证明该方法在泛化表示学习与实际机载应用中有效。

Conclusion: 该论文提出了一种结合掩码图像建模和空间/光谱拼图任务的多任务自监督学习框架，采用课程学习以逐步增加训练难度，从而提升轻量级编码器在高光谱影像上的表征能力；实验在四个公开数据集上验证，显著提升下游分割任务性能，且模型参数量极小，适合卫星机载部署。

Abstract: Hyperspectral imaging (HSI) captures detailed spectral signatures across
hundreds of contiguous bands per pixel, being indispensable for remote sensing
applications such as land-cover classification, change detection, and
environmental monitoring. Due to the high dimensionality of HSI data and the
slow rate of data transfer in satellite-based systems, compact and efficient
models are required to support onboard processing and minimize the transmission
of redundant or low-value data, e.g. cloud-covered areas. To this end, we
introduce a novel curriculum multi-task self-supervised learning (CMTSSL)
framework designed for lightweight architectures for HSI analysis. CMTSSL
integrates masked image modeling with decoupled spatial and spectral jigsaw
puzzle solving, guided by a curriculum learning strategy that progressively
increases data complexity during self-supervision. This enables the encoder to
jointly capture fine-grained spectral continuity, spatial structure, and global
semantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously
addresses spatial and spectral reasoning within a unified and computationally
efficient design, being particularly suitable for training lightweight models
for onboard satellite deployment. We validate our approach on four public
benchmark datasets, demonstrating consistent gains in downstream segmentation
tasks, using architectures that are over 16,000x lighter than some
state-of-the-art models. These results highlight the potential of CMTSSL in
generalizable representation learning with lightweight architectures for
real-world HSI applications. Our code is publicly available at
https://github.com/hugocarlesso/CMTSSL.

</details>


### [103] [Intelligent Vacuum Thermoforming Process](https://arxiv.org/abs/2509.13250)
*Andi Kuswoyo,Christos Margadji,Sebastian W. Pattinson*

Main category: cs.CV

TL;DR: 使用视觉数据和图像增强、k-NN算法，本文在少量数据下实现了真空热成形工艺参数的预测与优化，能够减少缺陷并提升生产效率。


<details>
  <summary>Details</summary>
Motivation: 真空热成形工艺受材料属性和模具配置变化影响较大，造成零件质量不稳定；传统基于规则或大量实验数据的方法成本高且效率低，故提出一种廉价且数据需求低的视觉预测与优化方案。

Method: 构建包含不同工艺参数下真空成形样件的视觉数据库，采用图像增强扩充训练数据，基于图像特征（可能是预处理或手工/自动特征提取）使用k近邻算法将低质量样件映射到高质量样件对应的工艺参数调整建议。

Result: 模型在调整加热功率、加热时间和抽真空时间方面表现良好，能有效降低缺陷率并提高产能效率；不过对细节公示有限，可能依赖图像质量和数据多样性。

Conclusion: 本文提出的基于视觉的质量控制系统能够在数据量有限的情况下，通过图像采集与增强结合k-NN映射实现工艺参数的预测与优化，从而减少真空成形件缺陷并提升生产效率。

Abstract: Ensuring consistent quality in vacuum thermoforming presents challenges due
to variations in material properties and tooling configurations. This research
introduces a vision-based quality control system to predict and optimise
process parameters, thereby enhancing part quality with minimal data
requirements. A comprehensive dataset was developed using visual data from
vacuum-formed samples subjected to various process parameters, supplemented by
image augmentation techniques to improve model training. A k-Nearest Neighbour
algorithm was subsequently employed to identify adjustments needed in process
parameters by mapping low-quality parts to their high-quality counterparts. The
model exhibited strong performance in adjusting heating power, heating time,
and vacuum time to reduce defects and improve production efficiency.

</details>


### [104] [ResidualViT for Efficient Temporally Dense Video Encoding](https://arxiv.org/abs/2509.13255)
*Mattia Soldan,Fabian Caba Heilbron,Bernard Ghanem,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: ResidualViT通过可学习残差连接、token reduction和轻量蒸馏，在保持性能的同时显著降低帧级特征开销与提高速度。


<details>
  <summary>Details</summary>
Motivation: 高时间分辨率的视频任务需要逐帧特征，计算开销大；利用视频帧间冗余降低特征计算成本。

Method: 设计ResidualViT架构，包含可学习残差连接和token reduction模块；并提出轻量蒸馏策略去拟合原始foundation model的帧特征。

Result: 在四项任务和五个数据集上测试（零样本与完全监督），计算成本最多减少60%，推理速度提升到2.5倍，精度接近原始模型。

Conclusion: 提出ResidualViT，在保持接近基线精度的同时显著减少计算成本和加速推理。

Abstract: Several video understanding tasks, such as natural language temporal video
grounding, temporal activity localization, and audio description generation,
require "temporally dense" reasoning over frames sampled at high temporal
resolution. However, computing frame-level features for these tasks is
computationally expensive given the temporal resolution requirements. In this
paper, we make three contributions to reduce the cost of computing features for
temporally dense tasks. First, we introduce a vision transformer (ViT)
architecture, dubbed ResidualViT, that leverages the large temporal redundancy
in videos to efficiently compute temporally dense frame-level features. Our
architecture incorporates (i) learnable residual connections that ensure
temporal consistency across consecutive frames and (ii) a token reduction
module that enhances processing speed by selectively discarding temporally
redundant information while reusing weights of a pretrained foundation model.
Second, we propose a lightweight distillation strategy to approximate the
frame-level features of the original foundation model. Finally, we evaluate our
approach across four tasks and five datasets, in both zero-shot and fully
supervised settings, demonstrating significant reductions in computational cost
(up to 60%) and improvements in inference speed (up to 2.5x faster), all while
closely approximating the accuracy of the original foundation model.

</details>


### [105] [RadGame: An AI-Powered Platform for Radiology Education](https://arxiv.org/abs/2509.13270)
*Mohammed Baharoon,Siavash Raissi,John S. Jun,Thibault Heintz,Mahmoud Alabbad,Ali Alburkani,Sung Eun Kim,Kent Kleinschmidt,Abdulrahman O. Alhumaydhi,Mohannad Mohammed G. Alghamdi,Jeremy Francis Palacio,Mohammed Bukhaytan,Noah Michael Prudlo,Rithvik Akula,Brady Chrisler,Benjamin Galligos,Mohammed O. Almutairi,Mazeen Mohammed Alanazi,Nasser M. Alrashdi,Joel Jihwan Hwang,Sri Sai Dinesh Jaliparthi,Luke David Nelson,Nathaniel Nguyen,Sathvik Suryadevara,Steven Kim,Mohammed F. Mohammed,Yevgeniy R. Semenov,Kun-Hsing Yu,Abdulrhman Aljouie,Hassan AlOmaish,Adam Rodman,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: RadGame通过AI与游戏化实现可扩展即时反馈，在放射定位与报告写作训练中显著优于传统被动学习。


<details>
  <summary>Details</summary>
Motivation: 传统放射学训练依赖被动病例暴露或有监督的实时练习，难以实现即时、可扩展的反馈。目标是用AI驱动的游戏化平台填补这一反馈缺口，提高学习效率与可获取性。

Method: 提出两个模块：RadGame Localize（玩家绘制异常的边界框，自动与放射科医师注释比较，并用视觉-语言模型生成对漏诊的可视化解释）与RadGame Report（玩家在给定X线、年龄与指征下撰写发现，系统基于放射报告生成指标给予结构化反馈，指出错误与遗漏，产出性能与风格评分）。使用大规模公开数据集作为地面真相并在前瞻性评价中比较与传统学习方法的效果。

Result: 前瞻性评估显示：使用RadGame的参与者在定位准确率上提升68%，相比之下传统被动方法仅提升17%；在报告写作准确率上提升31%，而传统方法仅提升4%。

Conclusion: RadGame能显著提升放射学学习者在定位异常与报告书写方面的能力，通过将游戏化、公开数据集与AI自动反馈结合，提供可扩展且即时的训练与评价。研究表明其在定位与报告两项任务上均优于传统被动学习方式。

Abstract: We introduce RadGame, an AI-powered gamified platform for radiology education
that targets two core skills: localizing findings and generating reports.
Traditional radiology training is based on passive exposure to cases or active
practice with real-time input from supervising radiologists, limiting
opportunities for immediate and scalable feedback. RadGame addresses this gap
by combining gamification with large-scale public datasets and automated,
AI-driven feedback that provides clear, structured guidance to human learners.
In RadGame Localize, players draw bounding boxes around abnormalities, which
are automatically compared to radiologist-drawn annotations from public
datasets, and visual explanations are generated by vision-language models for
user missed findings. In RadGame Report, players compose findings given a chest
X-ray, patient age and indication, and receive structured AI feedback based on
radiology report generation metrics, highlighting errors and omissions compared
to a radiologist's written ground truth report from public datasets, producing
a final performance and style score. In a prospective evaluation, participants
using RadGame achieved a 68% improvement in localization accuracy compared to
17% with traditional passive methods and a 31% improvement in report-writing
accuracy compared to 4% with traditional methods after seeing the same cases.
RadGame highlights the potential of AI-driven gamification to deliver scalable,
feedback-rich radiology training and reimagines the application of medical AI
resources in education.

</details>


### [106] [Image Realness Assessment and Localization with Multimodal Features](https://arxiv.org/abs/2509.13289)
*Lovish Kaushik,Agnij Biswas,Somdyuti Paul*

Main category: cs.CV

TL;DR: 利用视觉-语言模型生成的文本提示与图像特征融合，构建多模态系统，能更准确地评估AI生成图像的整体真实度并定位局部不一致区域，降低对人工标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 人工标注真实感或局部不一致的成本高且主观性强，迫切需要一种自动、可扩展且与人类主观感知一致的方法来量化AI生成图像的“真实度”并定位不真实区域，以便在应用和生成模型训练中提供反馈。

Method: 构建一个利用视觉-语言模型（VLM）生成视觉不一致文本描述的管道，将这些文本提示与图像特征融合，训练一个多模态网络以预测整体真实度评分，并产生稠密的真实度地图用于像素/区域级别的不一致检测。

Result: 实验显示，多模态方法在客观真实度预测上优于仅基于图像的方法，同时生成的稠密真实度地图能有效区分现实与非现实空间区域，证明用VLM生成的文本作为替代标注是可行且有效的。

Conclusion: 该论文提出了一个多模态框架，能同时进行AI生成图像的整体客观真实度评估与局部不一致区域识别，利用大规模视觉语言模型生成的文本描述替代人工标注，从而实现可扩展且可靠的真实度评估与不一致检测。

Abstract: A reliable method of quantifying the perceptual realness of AI-generated
images and identifying visually inconsistent regions is crucial for practical
use of AI-generated images and for improving photorealism of generative AI via
realness feedback during training. This paper introduces a framework that
accomplishes both overall objective realness assessment and local inconsistency
identification of AI-generated images using textual descriptions of visual
inconsistencies generated by vision-language models trained on large datasets
that serve as reliable substitutes for human annotations. Our results
demonstrate that the proposed multimodal approach improves objective realness
prediction performance and produces dense realness maps that effectively
distinguish between realistic and unrealistic spatial regions.

</details>


### [107] [StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with Texture-Geometry Dual Guidance](https://arxiv.org/abs/2509.13301)
*Zefan Qu,Zhenwei Wang,Haoyuan Wang,Ke Xu,Gerhard Hancke,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: 提出零训练的StyleSculptor，通过SD-Attn实现内容与风格的跨3D注意力融合并用方差驱动的通道选择避免内容泄露，辅以SGC实现可控的纹理/几何风格化，效果优于基线。


<details>
  <summary>Details</summary>
Motivation: 在游戏与虚拟现实等应用中常需生成与现有资产风格一致的新3D模型，但现有从文本或图像生成3D的方法难以提供可控的风格迁移，尤其在几何与纹理的精细控制方面存在挑战，因而需要一个能在零样本/零训练条件下实现风格指导的通用方法。

Method: 提出StyleSculptor框架，核心为Style Disentangled Attention (SD-Attn)模块，通过跨3D注意力机制在内容与风格间动态交互，实现稳定的特征融合；引入基于3D特征patch方差的风格-内容通道解耦策略以选择性注入风格特征；并设计Style Guided Control (SGC)实现仅几何或仅纹理的风格控制和强度调节。该方法为零训练（training-free）并支持多张风格图。

Result: 实验表明StyleSculptor在生成高保真3D资产方面优于现有基线方法，能有效捕捉用户提供的纹理与几何风格，支持风格强度调节与仅纹理/仅几何的专属风格化。

Conclusion: 此文提出了一种无需训练、可进行风格引导的3D资产生成方法，能够在输入内容图与风格图的条件下实现细粒度的纹理与几何风格控制，并通过实验展示优于基线方法的效果。

Abstract: Creating 3D assets that follow the texture and geometry style of existing
ones is often desirable or even inevitable in practical applications like video
gaming and virtual reality. While impressive progress has been made in
generating 3D objects from text or images, creating style-controllable 3D
assets remains a complex and challenging problem. In this work, we propose
StyleSculptor, a novel training-free approach for generating style-guided 3D
assets from a content image and one or more style images. Unlike previous
works, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,
enabling fine-grained 3D style control that captures the texture, geometry, or
both styles of user-provided style images. At the core of StyleSculptor is a
novel Style Disentangled Attention (SD-Attn) module, which establishes a
dynamic interaction between the input content image and style image for
style-guided 3D asset generation via a cross-3D attention mechanism, enabling
stable feature fusion and effective style-guided generation. To alleviate
semantic content leakage, we also introduce a style-disentangled feature
selection strategy within the SD-Attn module, which leverages the variance of
3D feature patches to disentangle style- and content-significant channels,
allowing selective feature injection within the attention framework. With
SD-Attn, the network can dynamically compute texture-, geometry-, or
both-guided features to steer the 3D generation process. Built upon this, we
further propose the Style Guided Control (SGC) mechanism, which enables
exclusive geometry- or texture-only stylization, as well as adjustable style
intensity control. Extensive experiments demonstrate that StyleSculptor
outperforms existing baseline methods in producing high-fidelity 3D assets.

</details>


### [108] [3D Aware Region Prompted Vision Language Model](https://arxiv.org/abs/2509.13317)
*An-Chieh Cheng,Yang Fu,Yukang Chen,Zhijian Liu,Xiaolong Li,Subhashree Radhakrishnan,Song Han,Yao Lu,Jan Kautz,Pavlo Molchanov,Hongxu Yin,Xiaolong Wang,Sifei Liu*

Main category: cs.CV

TL;DR: SR-3D 通过在 2D 特征中加入 3D 位置信息并共享视觉 token 空间，实现了灵活区域提示与强跨视图空间理解，在多项基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前 2D-3D 融合方法需要大量多帧或逐帧标注，不能灵活支持不同提示方式；目标是统一 2D 与 3D 表示，降低标注成本并提升跨视图空间理解能力。

Method: 在 2D 视觉特征中加入 3D 位置信息嵌入，构建共享的视觉 token 空间，使 3D 模型能够利用强大的 2D 先验进行跨帧空间推理；支持以边框、分割掩码或直接在 3D 中标注区域作为提示；在训练与推理时融合单视图特征与多视角 3D 表示。

Result: 在通用 2D 视觉语言和专门的 3D 空间基准上均达到或超过现有最佳性能；在无传感器 3D 输入的真实视频中亦能准确推断空间关系与度量。

Conclusion: SR-3D 提出了一种将单视图 2D 图像与多视图 3D 数据通过共享视觉 token 空间连接的方法，支持灵活的区域提示并可在无须全面多帧标注下进行 3D 推断，实验表明在 2D-3D 场景理解任务上达到 SOTA。

Abstract: We present Spatial Region 3D (SR-3D) aware vision-language model that
connects single-view 2D images and multi-view 3D data through a shared visual
token space. SR-3D supports flexible region prompting, allowing users to
annotate regions with bounding boxes, segmentation masks on any frame, or
directly in 3D, without the need for exhaustive multi-frame labeling. We
achieve this by enriching 2D visual features with 3D positional embeddings,
which allows the 3D model to draw upon strong 2D priors for more accurate
spatial reasoning across frames, even when objects of interest do not co-occur
within the same view. Extensive experiments on both general 2D vision language
and specialized 3D spatial benchmarks demonstrate that SR-3D achieves
state-of-the-art performance, underscoring its effectiveness for unifying 2D
and 3D representation space on scene understanding. Moreover, we observe
applicability to in-the-wild videos without sensory 3D inputs or ground-truth
3D annotations, where SR-3D accurately infers spatial relationships and metric
measurements.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [109] [ScaleDoc: Scaling LLM-based Predicates over Large Document Collections](https://arxiv.org/abs/2509.12610)
*Hengrui Zhang,Yulong Hui,Yihao Liu,Huanchen Zhang*

Main category: cs.DB

TL;DR: ScaleDoc通过离线语义表示+在线轻量代理+自适应级联过滤，显著降低LLM调用并加速大规模文档的语义谓词执行。


<details>
  <summary>Details</summary>
Motivation: 传统谓词适用于基于值的数据，但对大规模非结构化文档与语义查询需求日益增加。直接用LLM逐条推理昂贵且不可行，需要在性能与语义能力间取得平衡。

Method: 离线阶段：使用LLM为每个文档生成语义表示；在线阶段：针对每个查询在表示上训练轻量代理模型进行大规模过滤，只有不确定样本才交由LLM决策。引入对比学习框架训练代理模型以生成可靠的判决分数，并设计自适应级联机制，根据准确性目标确定过滤策略。

Result: 在三套数据集上评估，ScaleDoc实现了>2×的端到端加速，并将昂贵的LLM调用减少了最多85%。

Conclusion: 本文提出ScaleDoc，通过将谓词执行分离为离线表示和在线过滤阶段，从而在保证准确率目标下显著减少LLM调用，提升语义谓词执行效率。

Abstract: Predicates are foundational components in data analysis systems. However,
modern workloads increasingly involve unstructured documents, which demands
semantic understanding, beyond traditional value-based predicates. Given
enormous documents and ad-hoc queries, while Large Language Models (LLMs)
demonstrate powerful zero-shot capabilities, their high inference cost leads to
unacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novel
system that addresses this by decoupling predicate execution into an offline
representation phase and an optimized online filtering phase. In the offline
phase, \textsc{ScaleDoc} leverages a LLM to generate semantic representations
for each document. Online, for each query, it trains a lightweight proxy model
on these representations to filter the majority of documents, forwarding only
the ambiguous cases to the LLM for final decision. Furthermore,
\textsc{ScaleDoc} proposes two core innovations to achieve significant
efficiency: (1) a contrastive-learning-based framework that trains the proxy
model to generate reliable predicating decision scores; (2) an adaptive cascade
mechanism that determines the effective filtering policy while meeting specific
accuracy targets. Our evaluations across three datasets demonstrate that
\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reduces
expensive LLM invocations by up to 85\%, making large-scale semantic analysis
practical and efficient.

</details>
