<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.DB](#cs.DB) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model](https://arxiv.org/abs/2509.02659)
*Zilong Guo,Yi Luo,Long Sha,Dongxu Wang,Panqu Wang,Chenyang Xu,Yi Yang*

Main category: cs.CV

TL;DR: 将端到端驾驶与多模态VLM结合，在单摄像头条件下实现了排行榜上最佳的相机-only性能，展示了VLM对视觉驾驶任务的显著提升。


<details>
  <summary>Details</summary>
Motivation: 探索强大的大语言模型/多模态VLM是否能改善端到端自主驾驶性能，以及验证在仅使用单摄像头的限制下，基于视觉的端到端方法能否达到竞争性表现。

Method: 在端到端架构中引入强大的大语言模型，尤其是多模态VLM，作为感知与推理组件，结合单摄像头输入进行训练和推断，实现从视觉输入直接到驾驶控制的映射。

Result: 在驾驶任务上取得了显著性能提升，并在只使用单摄像头的设置下成为排行榜上最好的相机-only方案，证明了方法的有效性。

Conclusion: 该论文表明将端到端驾驶架构与多模态视觉语言模型（VLM）结合，可以显著提升驾驶任务性能，且在只使用单摄像头的条件下达到了排行榜上最佳的相机-only结果。

Abstract: End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.

</details>


### [2] [PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?](https://arxiv.org/abs/2509.02807)
*Mennatullah Siam*

Main category: cs.CV

TL;DR: 作者发现现有视频视觉定位任务被静态特征主导，构建MoCentric-Bench和四种运动探测策略，提出单帧基线与运动适配方法，挑战模型在时序运动理解与像素级定位方面的能力。


<details>
  <summary>Details</summary>
Motivation: 检查视频MLLM是否利用运动信息进行像素级定位，及其能否根据描述运动模式的语言分割对象。

Method: 提出四种运动中心探测方法和构建MoCentric-Bench基准，设计单帧基线与运动适配策略进行对比实验。

Result: 发现单帧信息常能解决任务，提出的基准和适配方法提升了运动敏感性并实现最先进性能；并发布代码数据集。

Conclusion: 视频多模态大语言模型在像素级视觉定位上仍有限，现有基准被静态线索主导。

Abstract: Multi-modal large language models (MLLMs) have shown impressive
generalization across tasks using images and text modalities. While their
extension to video has enabled tasks such as video question answering and video
captioning, their pixel-level visual grounding abilities are less studied. In
this work, we raise the pertinent question of whether motion is used in
pixel-level visual grounding and whether video MLLMs can segment objects based
on natural language expressions describing their motion patterns. We identify
the shortcomings in the current benchmarks, where we show that a single frame
can often suffice for capturing the motion referring expression without any
temporal reasoning. To address this, we introduce four motion-centric probing
techniques, particularly designed for the visual grounding task, to study video
MLLMs' ability to identify true motion from a fake one and their ability to
grasp the motion order. Consequently, we provide a motion-centric benchmark,
MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveraging
the interaction between motion and language rather than being dominated by
static appearance cues emphasized in existing visual grounding datasets. We
further establish strong single-image baselines that are on par with or
outperform prior methods. Finally, we explore simple motion-centric adaptation
techniques that provide state-of-the-art performance on our MoCentric-Bench.
Our motion-centric benchmark, evaluation and findings challenge future models
to improve dense spatiotemporal grounding and pixel-level understanding within
videos. Code and datasets will be made publicly available at
https://github.com/MSiam/PixFoundation-2.0.git.

</details>


### [3] [Multi-Scale Deep Learning for Colon Histopathology: A Hybrid Graph-Transformer Approach](https://arxiv.org/abs/2509.02851)
*Sadra Saremi,Amirhossein Ahmadkhan Kordbacheh*

Main category: cs.CV

TL;DR: 该文提出HG-TNet混合多尺度深度学习架构，将变压器分支与CNN分支结合，并引入胶囊网络、图注意力和残差学习及自监督旋转预测以提高LC25000结肠癌组织图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 结肠癌早期检测关键，单一网络难同时兼顾全局语境与局部细节，且要保留空间结构。因此提出混合架构整合变压器与CNN优点并加入胶囊与图注意力以改善分类性能。

Method: 模型由两条分支组成：1) transformer分支通过卷积嵌入将图像划分为patch并经transformer编码器提取全局上下文；2) CNN分支通过卷积层捕捉局部细节。两分支特征融合，并结合胶囊网络保留空间关系、图注意力与残差模块增强表达，配合自监督旋转预测任务提升鲁棒性。

Result: 在LC25000数据集上，HG-TNet在准确率和损失等指标上均优于基线模型，且通过胶囊网络更好保持空间顺序，实验证明多模块协同提高了分类效果。

Conclusion: HG-TNet在LC25000数据集上表现优于常规模型，通过混合全局-局部特征、保持空间关系与自监督训练，提高诊断表示的鲁棒性与分类准确率。

Abstract: Colon cancer also known as Colorectal cancer, is one of the most malignant
types of cancer worldwide. Early-stage detection of colon cancer is highly
crucial to prevent its deterioration. This research presents a hybrid
multi-scale deep learning architecture that synergizes capsule networks, graph
attention mechanisms, transformer modules, and residual learning to advance
colon cancer classification on the Lung and Colon Cancer Histopathological
Image Dataset (LC25000) dataset. The proposed model in this paper utilizes the
HG-TNet model that introduces a hybrid architecture that joins strength points
in transformers and convolutional neural networks to capture multi-scale
features in histopathological images. Mainly, a transformer branch extracts
global contextual bonds by partitioning the image into patches by
convolution-based patch embedding and then processing these patches through a
transformer encoder. Analogously, a dedicated CNN branch captures fine-grained,
local details through successive Incorporation these diverse features, combined
with a self-supervised rotation prediction objective, produce a robust
diagnostic representation that surpasses standard architectures in performance.
Results show better performance not only in accuracy or loss function but also
in these algorithms by utilizing capsule networks to preserve spatial orders
and realize how each element individually combines and forms whole structures.

</details>


### [4] [PRECISE-AS: Personalized Reinforcement Learning for Efficient Point-of-Care Echocardiography in Aortic Stenosis Diagnosis](https://arxiv.org/abs/2509.02898)
*Armin Saadat,Nima Hashemi,Hooman Vaseli,Michael Y. Tsang,Christina Luong,Michiel Van de Panne,Teresa S. M. Tsang,Purang Abolmaesumi*

Main category: cs.CV

TL;DR: 提出基于强化学习的主动视频采集框架，动态选择最有信息量的超声视频，对2,572例患者测试，准确率80.6%，仅使用47%视频，提升检测效率与可及性


<details>
  <summary>Details</summary>
Motivation: Echo is gold-standard but limited access; POCUS more accessible but operator/view selection and expertise limit utility; need to reduce number of views while maintaining accuracy via dynamic selection

Method: Reinforcement-learning driven active video acquisition for aortic stenosis classification

Result: On 2,572 patients, achieves 80.6% classification accuracy while using only 47% of echo videos compared to full acquisition

Conclusion: 主动特征采集可在保持较高准确率的同时显著减少影像采集量，有助于提高AS诊断的效率、可扩展性与个性化

Abstract: Aortic stenosis (AS) is a life-threatening condition caused by a narrowing of
the aortic valve, leading to impaired blood flow. Despite its high prevalence,
access to echocardiography (echo), the gold-standard diagnostic tool, is often
limited due to resource constraints, particularly in rural and underserved
areas. Point-of-care ultrasound (POCUS) offers a more accessible alternative
but is restricted by operator expertise and the challenge of selecting the most
relevant imaging views. To address this, we propose a reinforcement learning
(RL)-driven active video acquisition framework that dynamically selects each
patient's most informative echo videos. Unlike traditional methods that rely on
a fixed set of videos, our approach continuously evaluates whether additional
imaging is needed, optimizing both accuracy and efficiency. Tested on data from
2,572 patients, our method achieves 80.6% classification accuracy while using
only 47% of the echo videos compared to a full acquisition. These results
demonstrate the potential of active feature acquisition to enhance AS
diagnosis, making echocardiographic assessments more efficient, scalable, and
personalized. Our source code is available at:
https://github.com/Armin-Saadat/PRECISE-AS.

</details>


### [5] [LiGuard: A Streamlined Open-Source Framework for Rapid & Interactive Lidar Research](https://arxiv.org/abs/2509.02902)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: LiGuard是一个面向激光雷达研究的开源框架，集成I/O、处理流程、算法组件与可视化，支持交互式配置与项目化代码生成，减少重复开发并提升复用性与协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前激光雷达研究中大量重复开发特定场景代码，导致工作量浪费且对数据或方法的微小变动需大幅重写代码；需要一个通用框架来提升开发效率与可复用性。

Method: 提出并实现一个开源框架LiGuard，提供内置的数据I/O、预/后处理、常用算法组件；支持交互式增删重排算法和参数调整；生成结构化目录和代码文件以便共享复用；包含可视化模块用于分类、检测、分割和跟踪结果展示。

Result: 通过若干案例研究验证了LiGuard的有效性，表明其可加速项目开发、简化算法组合与参数调试，并促进代码与组件的共享重用。

Conclusion: LiGuard有效地解决了激光雷达研究中代码重复与难以复用的问题，提供模块化、可视化和项目结构化管理，有助于加速开发与协作。

Abstract: There is a growing interest in the development of lidar-based autonomous
mobility and Intelligent Transportation Systems (ITS). To operate and research
on lidar data, researchers often develop code specific to application niche.
This approach leads to duplication of efforts across studies that, in many
cases, share multiple methodological steps such as data input/output (I/O),
pre/post processing, and common algorithms in multi-stage solutions. Moreover,
slight changes in data, algorithms, and/or research focus may force major
revisions in the code. To address these challenges, we present LiGuard, an
open-source software framework that allows researchers to: 1) rapidly develop
code for their lidar-based projects by providing built-in support for data I/O,
pre/post processing, and commonly used algorithms, 2) interactively
add/remove/reorder custom algorithms and adjust their parameters, and 3)
visualize results for classification, detection, segmentation, and tracking
tasks. Moreover, because it creates all the code files in structured
directories, it allows easy sharing of entire projects or even the individual
components to be reused by other researchers. The effectiveness of LiGuard is
demonstrated via case studies.

</details>


### [6] [PercepTwin: Modeling High-Fidelity Digital Twins for Sim2Real LiDAR-based Perception for Intelligent Transportation Systems](https://arxiv.org/abs/2509.02903)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 用高保真数字孪生和公开数据构建可复现的合成LiDAR数据流水线，以降低标注成本、提升Sim2Real效果并支持大规模数据生成。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的LiDAR感知依赖大规模标注数据，但人工标注成本高、耗时长，限制系统可扩展性；Sim2Real是替代方案，但受限于仿真与现实世界的保真度，因此需要高保真的数字孪生来提升仿真数据有效性。

Method: 基于卫星影像和OpenStreetMap等公开资源，按步骤重建静态几何（建筑、道路、路缘）、道路基础设施和动态交通场景；结合传感器配置与高保真物理渲染器生成LiDAR点云；提供脚本化流水线与最佳实践来确保可重复性与规模化数据生成。

Result: 提出的工作生成了高质量、可扩展的合成数据流水线并展示了其在多样场景下的适用性；在方法描述中给出具体工具与资源，展示可重复性与成本—效率优势（文中宣称可用于训练或预训练并改善下游感知性能）。

Conclusion: 本文提出并验证了使用高保真数字孪生构建大规模合成LiDAR数据集的可重复方法，从而为Sim2Real学习提供可靠数据基础，能显著降低人工标注成本并提升场景多样性与可控性。

Abstract: LiDAR-based perception in intelligent transportation systems (ITS), for tasks
such as object detection, tracking, and semantic and instance segmentation, is
predominantly solved by deep neural network models which often require
large-scale labeled datasets during training to achieve generalization.
However, creating these datasets is costly. time consuming and require human
labor before the datasets are ready for training models. This hinders
scalability of the LiDAR-based perception systems in ITS. Sim2Real learning
offers scalable alternative, however, its effectiveness is dependent on the
fidelity of the source simulation(s) to real-world, in terms of environment
structure, actor dynamics, and sensor emulations. In response, this paper
introduces a rigorous and reproducible methodology for creating large-scale,
high-quality synthetic datasets using High-Fidelity Digital Twins (HiFi DTs).
The proposed workflow outlines the steps, tools, and best practices for
digitally replicating real-world environments, encompassing static geometry
modeling, road infrastructure replication, and dynamic traffic scenario
generation. Leveraging open-source and readily available resources such as
satellite imagery and OpenStreetMap data, alongside specific sensor
configurations, this paper provides practical, detailed guidance for
constructing robust synthetic environments. These environments subsequently
facilitate scalable, cost-effective, and diverse dataset generation, forming a
reliable foundation for robust Sim2Real learning.

</details>


### [7] [High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception](https://arxiv.org/abs/2509.02904)
*Muhammad Shahbaz,Shaurya Agarwal*

Main category: cs.CV

TL;DR: 本文提出高保真数字孪生（HiFi DT）用于缩小仿真到现实（Sim2Real）的LiDAR感知域差异，通过引入真实背景几何、车道级拓扑与传感器参数，生成域内合成数据并训练3D目标检测器，实验表明在真实数据上性能提升4.8%，并通过CD、MMD、EMD、FD等指标在输入与特征层面定量评估域对齐。


<details>
  <summary>Details</summary>
Motivation: 仿真训练的LiDAR感知模型在现实中性能下降，主要由于分布差异；通过高保真数字孪生生成更接近真实分布的训练数据，可降低域差距并提升仿真训练模型在现实中的表现。

Method: 构造包含真实世界背景几何、车道级路网拓扑及传感器规格与安装位置的高保真数字孪生环境，生成拟域内合成点云，使用现成的3D检测器在合成数据上训练并在真实数据上评估，同时计算Chamfer Distance、MMD、EMD、Fréchet Distance在输入与潜在特征层面的分布差距。

Result: 在实验中，HiFi DT生成的数据训练得到的3D检测器在真实数据上的性能比用真实数据训练的模型高出4.8%；多种分布距离度量在输入与特征层面均显示HiFi DT显著减少域差异。

Conclusion: 高保真数字孪生能显著减少Sim2Real域差异，提升LiDAR感知模型在真实场景的泛化能力，证明了基于精确环境与传感器建模的仿真数据在ITS中的实用性。

Abstract: Sim2Real domain transfer offers a cost-effective and scalable approach for
developing LiDAR-based perception (e.g., object detection, tracking,
segmentation) in Intelligent Transportation Systems (ITS). However, perception
models trained in simulation often under perform on real-world data due to
distributional shifts. To address this Sim2Real gap, this paper proposes a
high-fidelity digital twin (HiFi DT) framework that incorporates real-world
background geometry, lane-level road topology, and sensor-specific
specifications and placement. We formalize the domain adaptation challenge
underlying Sim2Real learning and present a systematic method for constructing
simulation environments that yield in-domain synthetic data. An off-the-shelf
3D object detector is trained on HiFi DT-generated synthetic data and evaluated
on real data. Our experiments show that the DT-trained model outperforms the
equivalent model trained on real data by 4.8%. To understand this gain, we
quantify distributional alignment between synthetic and real data using
multiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy
(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at both
raw-input and latent-feature levels. Results demonstrate that HiFi DTs
substantially reduce domain shift and improve generalization across diverse
evaluation scenarios. These findings underscore the significant role of digital
twins in enabling reliable, simulation-based LiDAR perception for real-world
ITS applications.

</details>


### [8] [Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach](https://arxiv.org/abs/2509.02918)
*Midhat Urooj,Ayan Banerjee,Farhat Shaikh,Kuntal Thakur,Sandeep Gupta*

Main category: cs.CV

TL;DR: 提出KG-DG：将Vision Transformer与专家知识的符号推理结合，用病变本体和血管分割等结构化规则特征与深度特征通过置信度加权融合，通过最小化域嵌入的KL散度实现单域和多域泛化，在四个数据集上跨域准确率提升最高5.2%，整体比ViT提高6%，符号单元也能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决医疗影像中单源训练模型在真实分布漂移下泛化能力差的问题，通过引入专家知识的符号推理来补充深度模型的泛化与可解释性。

Method: 使用ViT提取深度视觉特征，构建基于临床病变本体的符号特征和视网膜血管分割信息，采用置信度加权策略融合符号与视觉表征；通过最小化不同域的域嵌入之间的KL散度来对齐高层临床语义，从而改善SDG和MDG。

Result: 在APTOS、EyePACS、Messidor-1、Messidor-2上进行实验，跨域设置最高提升精度5.2%，相较ViT提升6%；符号模块单独在MDG上达63.67%平均精度，病变特征单独达84.65%，神经符号整合在SDG上获得最佳结果。

Conclusion: KG-DG通过将符号知识（病变本体、规则特征、血管分割）与视觉Transformer融合，实现了在DR分类任务上更好的域间泛化和鲁棒性，符号模块既提高了可解释性也作为有效正则化器，整体优于纯神经基线。

Abstract: Domain generalization remains a critical challenge in medical imaging, where
models trained on single sources often fail under real-world distribution
shifts. We propose KG-DG, a neuro-symbolic framework for diabetic retinopathy
(DR) classification that integrates vision transformers with expert-guided
symbolic reasoning to enable robust generalization across unseen domains. Our
approach leverages clinical lesion ontologies through structured, rule-based
features and retinal vessel segmentation, fusing them with deep visual
representations via a confidence-weighted integration strategy. The framework
addresses both single-domain generalization (SDG) and multi-domain
generalization (MDG) by minimizing the KL divergence between domain embeddings,
thereby enforcing alignment of high-level clinical semantics. Extensive
experiments across four public datasets (APTOS, EyePACS, Messidor-1,
Messidor-2) demonstrate significant improvements: up to a 5.2% accuracy gain in
cross-domain settings and a 6% improvement over baseline ViT models. Notably,
our symbolic-only model achieves a 63.67% average accuracy in MDG, while the
complete neuro-symbolic integration achieves the highest accuracy compared to
existing published baselines and benchmarks in challenging SDG scenarios.
Ablation studies reveal that lesion-based features (84.65% accuracy)
substantially outperform purely neural approaches, confirming that symbolic
components act as effective regularizers beyond merely enhancing
interpretability. Our findings establish neuro-symbolic integration as a
promising paradigm for building clinically robust, and domain-invariant medical
AI systems.

</details>


### [9] [A Data-Driven RetinaNet Model for Small Object Detection in Aerial Images](https://arxiv.org/abs/2509.02928)
*Zhicheng Tang,Jinwen Tang,Yi Shang*

Main category: cs.CV

TL;DR: DDR-Net在RetinaNet基础上引入数据驱动的特征与锚框自适应模块及新采样策略，提升了航空影像小目标检测性能，尤其在有限数据条件下效果明显优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在航空影像中小目标检测难度大且标注成本高，传统模型在小目标和有限数据场景表现欠佳，故提出一种能在少量数据上仍保持高精度的自适应方法。

Method: 基于RetinaNet框架，DDR-Net加入数据驱动的特征图选择与锚框估计模块，以及一种针对小样本训练的创新采样策略；在训练过程中这些模块自适应学习以优化检测性能。

Result: 在多个航空鸟类影像数据集上的实证评估显示，DDR-Net在小目标检测上显著优于RetinaNet和其他当代模型，且在数据量有限时表现尤为突出，降低了数据采集与训练成本。

Conclusion: DDR-Net通过数据驱动方法自动选择最优特征图和锚框估计，并引入新型采样技术，在有限数据下也能提高小目标检测精度，从而显著优于RetinaNet和其他模型。

Abstract: In the realm of aerial imaging, the ability to detect small objects is
pivotal for a myriad of applications, encompassing environmental surveillance,
urban design, and crisis management. Leveraging RetinaNet, this work unveils
DDR-Net: a data-driven, deep-learning model devised to enhance the detection of
diminutive objects. DDR-Net introduces novel, data-driven techniques to
autonomously ascertain optimal feature maps and anchor estimations, cultivating
a tailored and proficient training process while maintaining precision.
Additionally, this paper presents an innovative sampling technique to bolster
model efficacy under limited data training constraints. The model's enhanced
detection capabilities support critical applications including wildlife and
habitat monitoring, traffic flow optimization, and public safety improvements
through accurate identification of small objects like vehicles and pedestrians.
DDR-Net significantly reduces the cost and time required for data collection
and training, offering efficient performance even with limited data. Empirical
assessments over assorted aerial avian imagery datasets demonstrate that
DDR-Net markedly surpasses RetinaNet and alternative contemporary models. These
innovations advance current aerial image analysis technologies and promise
wide-ranging impacts across multiple sectors including agriculture, security,
and archaeology.

</details>


### [10] [STAR: A Fast and Robust Rigid Registration Framework for Serial Histopathological Images](https://arxiv.org/abs/2509.02952)
*Zeyu Liu,Shengwei Ding*

Main category: cs.CV

TL;DR: STAR是一个开源、轻量的序列WSI刚性配准框架，结合染色感知预处理和层次化相关策略，在多种染色和组织上实现快速稳健对齐，适合作为可复现的基线工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖复杂的可变形或深度学习算法，计算量大且难以复现；而对于连续切片情形，刚性配准通常已足够，但相关轻量级框架尚不完善。STAR旨在填补这一空白，提供快速可复现的工具。

Method: 提出了一个轻量级刚性配准管线：结合染色条件的预处理、分层粗到细相关搜索、自适应核尺度调整和内置质量控制；实现多分辨率相关性计算以应对部分组织重叠和跨染色差异。

Result: 在ANHIR 2019和ACROBAT 2022等多器官、多扫描条件数据集上，STAR在分钟级别内稳定完成配准，对跨染色变化和部分组织重叠表现鲁棒；并通过H&E-IHC对齐、多IHC面板构建和失败模式分析展示其实用性和局限性。

Conclusion: STAR提供了一个快速、稳健且开源的刚性配准框架，适用于序列全片组织切片（WSI）在多种染色和组织类型间的对齐，能够在分钟级别完成配准并内置质量控制，适合作为可复现的基线工具以促进临床应用和大规模配对数据准备。

Abstract: Registration of serial whole-slide histopathological images (WSIs) is
critical for enabling direct comparison across diverse stains and for preparing
paired datasets in artificial intelligence (AI) workflows such as virtual
staining and biomarker prediction. While existing methods often rely on complex
deformable or deep learning approaches that are computationally intensive and
difficult to reproduce, lightweight rigid frameworks-sufficient for many
consecutive-section scenarios-remain underdeveloped. We introduce STAR (Serial
Tissue Alignment for Rigid registration), a fast and robust open-source
framework for multi-WSI alignment. STAR integrates stain-conditioned
preprocessing with a hierarchical coarse-to-fine correlation strategy, adaptive
kernel scaling, and built-in quality control, achieving reliable rigid
registration across heterogeneous tissue types and staining protocols,
including hematoxylin-eosin (H&E), special histochemical stains (e.g., PAS,
PASM, Masson's), and immunohistochemical (IHC) markers (e.g., CD31, KI67).
Evaluated on the ANHIR 2019 and ACROBAT 2022 datasets spanning multiple organs
and scanning conditions, STAR consistently produced stable alignments within
minutes per slide, demonstrating robustness to cross-stain variability and
partial tissue overlap. Beyond benchmarks, we present case studies on H&E-IHC
alignment, construction of multi-IHC panels, and typical failure modes,
underscoring both utility and limitations. Released as an open and lightweight
tool, STAR provides a reproducible baseline that lowers the barrier for
clinical adoption and enables large-scale paired data preparation for
next-generation computational pathology.

</details>


### [11] [Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability](https://arxiv.org/abs/2509.02962)
*Shuai Jiang,Yunfeng Ma,Jingyu Zhou,Yuan Bian,Yaonan Wang,Min Liu*

Main category: cs.CV

TL;DR: 文章通过设计缺失感知的跨模态提示和以文本为桥的对称对比学习，提升了RGB与3D模态缺失场景下的工业缺陷检测性能。


<details>
  <summary>Details</summary>
Motivation: 应对实际工业场景中传感器不确定性导致的模态缺失问题，使多模态融合在缺失情况下仍能保持鲁棒性。

Method: 提出跨模态提示学习（包括一致性提示、模态特定提示和缺失感知提示）和对称对比学习（通过二元文本提示作为桥梁，进行三模态对比预训练）。

Result: 在总缺失率0.7的设置下，方法达到73.83% I-AUROC和93.05% P-AUROC，分别较最先进方法提升3.84%和5.58%，并在不同缺失类型和缺失率下表现优越。

Conclusion: 提出的方法在处理模态缺失的多模态工业表面缺陷检测任务中有效，能够通过提示学习和对比预训练提升跨模态一致性与缺陷检测性能。

Abstract: Multimodal industrial surface defect detection (MISDD) aims to identify and
locate defect in industrial products by fusing RGB and 3D modalities. This
article focuses on modality-missing problems caused by uncertain sensors
availability in MISDD. In this context, the fusion of multiple modalities
encounters several troubles, including learning mode transformation and
information vacancy. To this end, we first propose cross-modal prompt learning,
which includes: i) the cross-modal consistency prompt serves the establishment
of information consistency of dual visual modalities; ii) the modality-specific
prompt is inserted to adapt different input patterns; iii) the missing-aware
prompt is attached to compensate for the information vacancy caused by dynamic
modalities-missing. In addition, we propose symmetric contrastive learning,
which utilizes text modality as a bridge for fusion of dual vision modalities.
Specifically, a paired antithetical text prompt is designed to generate binary
text semantics, and triple-modal contrastive pre-training is offered to
accomplish multimodal learning. Experiment results show that our proposed
method achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7
for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%
respectively), and outperforms existing approaches to varying degrees under
different missing types and rates. The source code will be available at
https://github.com/SvyJ/MISDD-MM.

</details>


### [12] [EdgeAttNet: Towards Barb-Aware Filament Segmentation](https://arxiv.org/abs/2509.02964)
*Victor Solomon,Piet Martens,Jingyu Liu,Rafal Angryk*

Main category: cs.CV

TL;DR: 提出将可学习边缘图注入注意力的EdgeAttNet，在太阳丝状体分割（含barb）上提升精度、减少参数并加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模长程依赖和细微空间结构，导致对太阳丝状体（filament）尤其是分支(barb)的识别不佳；通过引导注意力机制利用边缘先验可增强空间敏感性。

Method: 在U-Net骨干上，在网络瓶颈处加入基于输入图像可学习的边缘图，并通过线性变换将该边缘信息注入自注意力的K和Q矩阵，使注意力聚焦边界和细节；端到端训练。

Result: 在MAGFILO数据集上，EdgeAttNet在分割精度和barb识别上显著优于U-Net和U-Net+Transformer基线，并具备更少参数和更快推理速度，适合实际部署。

Conclusion: EdgeAttNet通过将输入图像学习到的边缘图融入自注意力的Key和Query，提升了分割对细节（尤其barb）的捕捉能力，从而在MAGFILO数据集上优于U-Net及其Transformer变体，且参数更少、推理更快。

Abstract: Accurate segmentation of solar filaments in H-alpha observations is critical
for determining filament chirality, a key factor in the behavior of Coronal
Mass Ejections (CMEs). However, existing methods often fail to capture
fine-scale filament structures, particularly barbs, due to a limited ability to
model long-range dependencies and spatial detail.
  We propose EdgeAttNet, a segmentation architecture built on a U-Net backbone
by introducing a novel, learnable edge map derived directly from the input
image. This edge map is incorporated into the model by linearly transforming
the attention Key and Query matrices with the edge information, thereby guiding
the self-attention mechanism at the network's bottleneck to more effectively
capture filament boundaries and barbs. By explicitly integrating this
structural prior into the attention computations, EdgeAttNet enhances spatial
sensitivity and segmentation accuracy while reducing the number of trainable
parameters.
  Trained end-to-end, EdgeAttNet outperforms U-Net and other U-Net-based
transformer baselines on the MAGFILO dataset. It achieves higher segmentation
accuracy and significantly better recognition of filament barbs, with faster
inference performance suitable for practical deployment.

</details>


### [13] [KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966)
*Yujin Wang,Tianyi Wang,Quanfeng Liu,Wenxian Fan,Junfeng Jiao,Christian Claudel,Yunbing Yan,Bingzhao Gao,Jianqiang Wang,Hong Chen*

Main category: cs.CV

TL;DR: KEPT结合检索、CoT提示与分阶段微调，使VLM能够利用场景先验与规划约束进行可解释且高精度的短期轨迹预测，实验表明对精度与安全性均有明显提升并适合部署。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在将推理与场景动力学及领域知识对齐方面不足，导致短期轨迹预测精度和安全性不佳，故引入检索与显式规划约束以增强可解释性和可靠性。

Method: 提出了TFSF视频编码器（用自监督及hard-negative mining训练）、k-means+HNSW检索栈用于场景对齐示例检索、将检索到的先验嵌入CoT提示并施加规划约束，以及三阶段微调策略逐步对齐语言头与空间/动力学/时序规划。

Result: 在nuScenes上取得SOTA：NoAvg下平均L2为0.70m、碰撞率0.21%；TemAvg（带轻量ego信息）下平均L2为0.31m、碰撞率0.07%；Top-2检索示例在准确性-安全性上最优，检索延迟亚毫秒级。

Conclusion: KEPT通过检索增强与链式思维提示，有效提升了前视视频驱动的短期自车轨迹预测性能，并在nuScenes上实现了最先进的开环评估结果，兼顾精度与安全性。

Abstract: Accurate short-horizon trajectory prediction is pivotal for safe and reliable
autonomous driving, yet existing vision-language models (VLMs) often fail to
effectively ground their reasoning in scene dynamics and domain knowledge. To
address this challenge, this paper introduces KEPT, a knowledge-enhanced VLM
framework that predicts ego trajectories directly from consecutive front-view
driving frames. KEPT couples a temporal frequency-spatial fusion (TFSF) video
encoder, trained via self-supervised learning with hard-negative mining, with a
scalable k-means + HNSW retrieval stack that supplies scene-aligned exemplars.
Retrieved priors are embedded into chain-of-thought (CoT) prompts with explicit
planning constraints, while a triple-stage fine-tuning schedule incrementally
aligns the language head to metric spatial cues, physically feasible motion,
and temporally conditioned front-view planning. Evaluated on nuScenes dataset,
KEPT achieves state-of-the-art performance across open-loop protocols: under
NoAvg, it achieves 0.70m average L2 with a 0.21\% collision rate; under TemAvg
with lightweight ego status, it attains 0.31m average L2 and a 0.07\% collision
rate. Ablation studies show that all three fine-tuning stages contribute
complementary benefits, and that using Top-2 retrieved exemplars yields the
best accuracy-safety trade-off. The k-means-clustered HNSW index delivers
sub-millisecond retrieval latency, supporting practical deployment. These
results indicate that retrieval-augmented, CoT-guided VLMs offer a promising,
data-efficient pathway toward interpretable and trustworthy autonomous driving.

</details>


### [14] [VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results](https://arxiv.org/abs/2509.02969)
*Dasong Li,Sizhuo Ma,Hang Hua,Wenjie Li,Jian Wang,Chris Wei Zhou,Fengbin Guan,Xin Li,Zihao Yu,Yiting Lu,Ru-Ling Liao,Yan Ye,Zhibo Chen,Wei Sun,Linhan Cao,Yuqin Cao,Weixia Zhang,Wen Wen,Kaiwei Zhang,Zijian Chen,Fangfang Lu,Xiongkuo Min,Guangtao Zhai,Erjia Xiao,Lingfeng Zhang,Zhenjie Su,Hao Cheng,Yu Liu,Renjing Xu,Long Chen,Xiaoshuai Hao,Zhenpeng Zeng,Jianqin Wu,Xuxu Wang,Qian Yu,Bo Hu,Weiwei Wang,Pinxin Liu,Yunlong Tang,Luchuan Song,Jinxi He,Jiaru Wu,Hanjia Lyu*

Main category: cs.CV

TL;DR: VQualA 2025通过发布真实UGC短视频数据集并举办竞赛，促进了多模态短视频参与度预测方法的发展，吸引了大量研究者提交模型并取得实质性进展。


<details>
  <summary>Details</summary>
Motivation: 短视频在社交媒体上的广泛传播使得理解和预测UGC的用户参与度成为重要问题，现有方法在多模态信息融合与应对真实交互噪声方面仍不足，亟需公开数据与竞赛推动社区进步。

Method: 构建并发布了一个基于真实用户交互的短视频UGC数据集，提供视觉、音频及创作者元数据等多模态输入；组织竞赛平台，接收并评审参赛者提交的模型；鼓励使用多模态融合与鲁棒建模策略以应对真实世界噪声与偏差。

Result: 挑战吸引97名参与者，产生15份有效测试提交，参赛方法广泛采用视觉、音频、元数据等多模态特征，展示了对短视频参与度预测的性能提升并推动了该研究方向的进一步探索。

Conclusion: 本论文总结了VQualA 2025挑战赛在短视频参与度预测方向的组织、数据集与参赛成果，展示了多模态特征在真实UGC短视频流行度建模中的重要性，并指出挑战推动了该领域方法学的发展。

Abstract: This paper presents an overview of the VQualA 2025 Challenge on Engagement
Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge
focuses on understanding and modeling the popularity of user-generated content
(UGC) short videos on social media platforms. To support this goal, the
challenge uses a new short-form UGC dataset featuring engagement metrics
derived from real-world user interactions. This objective of the Challenge is
to promote robust modeling strategies that capture the complex factors
influencing user engagement. Participants explored a variety of multi-modal
features, including visual content, audio, and metadata provided by creators.
The challenge attracted 97 participants and received 15 valid test submissions,
contributing significantly to progress in short-form UGC video engagement
prediction.

</details>


### [15] [InstaDA: Augmenting Instance Segmentation Data with Dual-Agent System](https://arxiv.org/abs/2509.02973)
*Xianbao Hou,Yonghao He,Zeyd Boukhers,John See,Hu Su,Wei Sui,Cong Yang*

Main category: cs.CV

TL;DR: 提出 InstaDA：一个训练-free 的双智能体（文本与图像）数据增强框架，通过提示迭代（Prompt Rethink）与基于训练图像生成实例，提高 LVIS 上实例分割性能，明显优于基线与竞争模型。


<details>
  <summary>Details</summary>
Motivation: 目标是解决实例分割数据标注昂贵与类别不平衡问题，利用生成模型和现有训练数据更充分地扩展数据集，同时加强 LLM 与扩散模型的深度协作。

Method: 方法包含两个独立自动化工作流：T-Agent（文本智能体）借助 LLM 与扩散模型协同生成图像，并通过 Prompt Rethink 迭代优化提示与增强图像利用；I-Agent（图像智能体）基于训练图像条件生成新的实例以丰富数据分布。两者均为训练-free。

Result: InstaDA 提出了一种无需训练的双智能体系统（Text-Agent 和 Image-Agent）用于增强实例分割数据，通过 LLM 与扩散模型协同生成和基于已有图像条件生成实例，提高数据多样性与分布覆盖。

Conclusion: InstaDA 在 LVIS 1.0 验证集上带来显著提升（+4.0 box AP，+3.3 mask AP），并优于 DiverGen，尤其在常见与频繁类别上有更大收益，表明该方法能有效缓解类别不平衡并提升数据利用率。

Abstract: Acquiring high-quality instance segmentation data is challenging due to the
labor-intensive nature of the annotation process and significant class
imbalances within datasets. Recent studies have utilized the integration of
Copy-Paste and diffusion models to create more diverse datasets. However, these
studies often lack deep collaboration between large language models (LLMs) and
diffusion models, and underutilize the rich information within the existing
training data. To address these limitations, we propose InstaDA, a novel,
training-free Dual-Agent system designed to augment instance segmentation
datasets. First, we introduce a Text-Agent (T-Agent) that enhances data
diversity through collaboration between LLMs and diffusion models. This agent
features a novel Prompt Rethink mechanism, which iteratively refines prompts
based on the generated images. This process not only fosters collaboration but
also increases image utilization and optimizes the prompts themselves.
Additionally, we present an Image-Agent (I-Agent) aimed at enriching the
overall data distribution. This agent augments the training set by generating
new instances conditioned on the training images. To ensure practicality and
efficiency, both agents operate as independent and automated workflows,
enhancing usability. Experiments conducted on the LVIS 1.0 validation set
indicate that InstaDA achieves significant improvements, with an increase of
+4.0 in box average precision (AP) and +3.3 in mask AP compared to the
baseline. Furthermore, it outperforms the leading model, DiverGen, by +0.3 in
box AP and +0.1 in mask AP, with a notable +0.7 gain in box AP on common
categories and mask AP gains of +0.2 on common categories and +0.5 on frequent
categories.

</details>


### [16] [SPENet: Self-guided Prototype Enhancement Network for Few-shot Medical Image Segmentation](https://arxiv.org/abs/2509.02993)
*Chao Fan,Xibin Jia,Anqi Xiao,Hongyuan Yu,Zhenghan Yang,Dawei Yang,Hui Xu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 提出了SPENet，用于少样本医学图像分割。通过多层原型生成（全局+可变数量的局部原型）和查询引导的局部原型增强来应对类内差异与负面局部原型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有原型方法仅用单一全局原型忽略类内变异，且部分支持图像的局部原型与查询图像差异大反而有害，需多粒度原型与查询引导的增强以提高匹配鲁棒性。

Method: 提出MPG模块同时生成全局原型与自适应数量的局部原型以实现多粒度度量；提出QLPE模块利用查询图像信息自适应地增强/修正支持局部原型，抑制负面局部原型影响；网络整体为原型增强的少样本分割框架。

Result: 在三个公共医学图像分割数据集上进行大量实验，SPENet在指标上优于现有最先进方法，验证了其有效性。

Conclusion: SPENet通过多粒度原型和查询引导的局部原型增强，能有效缓解支持与查询图像间差异带来的匹配误差，在三个公共医学数据集上取得了优于现有方法的最先进性能。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) aims to segment novel classes of
medical objects using only a few labeled images. Prototype-based methods have
made significant progress in addressing FSMIS. However, they typically generate
a single global prototype for the support image to match with the query image,
overlooking intra-class variations. To address this issue, we propose a
Self-guided Prototype Enhancement Network (SPENet). Specifically, we introduce
a Multi-level Prototype Generation (MPG) module, which enables
multi-granularity measurement between the support and query images by
simultaneously generating a global prototype and an adaptive number of local
prototypes. Additionally, we observe that not all local prototypes in the
support image are beneficial for matching, especially when there are
substantial discrepancies between the support and query images. To alleviate
this issue, we propose a Query-guided Local Prototype Enhancement (QLPE)
module, which adaptively refines support prototypes by incorporating guidance
from the query image, thus mitigating the negative effects of such
discrepancies. Extensive experiments on three public medical datasets
demonstrate that SPENet outperforms existing state-of-the-art methods,
achieving superior performance.

</details>


### [17] [SOPSeg: Prompt-based Small Object Instance Segmentation in Remote Sensing Imagery](https://arxiv.org/abs/2509.03002)
*Chenhao Wang,Yingrui Ji,Yu Meng,Yunjian Zhang,Yao Zhu*

Main category: cs.CV

TL;DR: 提出SOPSeg，通过自适应放大、边缘辅助解码和旋转框提示改善遥感小目标分割，构建并将发布小目标实例分割数据集。


<details>
  <summary>Details</summary>
Motivation: 当前研究多集中于小目标检测，而像素级标注成本高、技术难度大导致小目标实例分割数据集缺失；此外现有SAM在小目标分割上表现不佳，原因是其1/16的粗糙特征分辨率丢失细节。

Method: 提出了基于提示（prompt）的框架SOPSeg，包括区域自适应放大策略以保留细粒度信息，自定义解码器融合边缘预测与逐步细化，以及针对遥感中常用的旋转边界框设计的新型提示机制。

Result: SOPSeg在小目标分割任务上优于现有方法，并能促进遥感任务的数据集高效构建。作者基于SODA-A构建了一个全面的小目标实例分割数据集，计划公开模型与数据集以支持后续研究。

Conclusion: 本文提出的SOPSeg可有效提升遥感影像中小目标的实例分割性能，尤其在细节保留与边界精细化方面有明显改进，并能用于构建小目标实例分割数据集。

Abstract: Extracting small objects from remote sensing imagery plays a vital role in
various applications, including urban planning, environmental monitoring, and
disaster management. While current research primarily focuses on small object
detection, instance segmentation for small objects remains underexplored, with
no dedicated datasets available. This gap stems from the technical challenges
and high costs of pixel-level annotation for small objects. While the Segment
Anything Model (SAM) demonstrates impressive zero-shot generalization, its
performance on small-object segmentation deteriorates significantly, largely
due to the coarse 1/16 feature resolution that causes severe loss of fine
spatial details. To this end, we propose SOPSeg, a prompt-based framework
specifically designed for small object segmentation in remote sensing imagery.
It incorporates a region-adaptive magnification strategy to preserve
fine-grained details, and employs a customized decoder that integrates edge
prediction and progressive refinement for accurate boundary delineation.
Moreover, we introduce a novel prompting mechanism tailored to the oriented
bounding boxes widely adopted in remote sensing applications. SOPSeg
outperforms existing methods in small object segmentation and facilitates
efficient dataset construction for remote sensing tasks. We further construct a
comprehensive small object instance segmentation dataset based on SODA-A, and
will release both the model and dataset to support future research.

</details>


### [18] [Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers](https://arxiv.org/abs/2509.03006)
*Tzuhsuan Huang,Cheng Yu Yeo,Tsai-Ling Huang,Hong-Han Shuai,Wen-Huang Cheng,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文通过在训练中加入空间域CNN与频率域Transformer组成的集成攻击网络，显著提升了后处理深度水印在WAVES基准上的鲁棒性，特别是在再生攻击下取得约18.7%的性能增益。


<details>
  <summary>Details</summary>
Motivation: 与嵌入到生成过程的水印方法相比，后处理水印更灵活，可用于任意生成模型输出并允许对单张图像嵌入独特水印，因此需提高其在实际图像处理（压缩、再生成等）下的鲁棒性。

Method: 在训练阶段构建多种攻击网络（CNN与Transformer，分别在空间域和频域），作为对抗干扰来训练后处理水印模型，从而增强其对各种图像处理攻击的鲁棒性；使用WAVES基准与平均比特准确率评价。

Result: 引入的集成攻击网络使基线方法在WAVES基准下鲁棒性显著提升；具体地，对Regeneration Attack，方法使StegaStamp的性能提升约18.743%。

Conclusion: 该论文提出在后处理水印中引入集成对抗网络以提升鲁棒性，并发现空间域CNN与频率域Transformer的组合效果最佳，显著提高了在WAVES基准上对多种攻击（尤其是再生攻击）的比特准确率。

Abstract: Recent studies on deep watermarking have predominantly focused on
in-processing watermarking, which integrates the watermarking process into
image generation. However, post-processing watermarking, which embeds
watermarks after image generation, offers more flexibility. It can be applied
to outputs from any generative model (e.g. GANs, diffusion models) without
needing access to the model's internal structure. It also allows users to embed
unique watermarks into individual images. Therefore, this study focuses on
post-processing watermarking and enhances its robustness by incorporating an
ensemble attack network during training. We construct various versions of
attack networks using CNN and Transformer in both spatial and frequency domains
to investigate how each combination influences the robustness of the
watermarking model. Our results demonstrate that combining a CNN-based attack
network in the spatial domain with a Transformer-based attack network in the
frequency domain yields the highest robustness in watermarking models.
Extensive evaluation on the WAVES benchmark, using average bit accuracy as the
metric, demonstrates that our ensemble attack network significantly enhances
the robustness of baseline watermarking methods under various stress tests. In
particular, for the Regeneration Attack defined in WAVES, our method improves
StegaStamp by 18.743%. The code is released
at:https://github.com/aiiu-lab/DeepRobustWatermark.

</details>


### [19] [Lesion-Aware Visual-Language Fusion for Automated Image Captioning of Ulcerative Colitis Endoscopic Examinations](https://arxiv.org/abs/2509.03011)
*Alexis Ivan Lopez Escamilla,Gilberto Ochoa,Sharib Al*

Main category: cs.CV

TL;DR: 提出将视觉可解释性（Grad-CAM）与注意力模块（CBAM）结合到ResNet→T5图像描述流水线，并以自然语言注入临床元数据，旨在生成可解释、结构化的UC内镜报告，同时提升MES分级与病变检测性能。


<details>
  <summary>Details</summary>
Motivation: 动机是提升内镜图像报告的可解释性和临床实用性：传统自动描述可能忽视病变部位或临床分级信息，难以用于可靠的内镜记录与决策支持，因此通过病变感知与临床提示来提高描述质量及分级准确性。

Method: 方法上采用ResNet提取图像嵌入，利用Grad-CAM生成病变热图，并在特征融合或注意力模块中引入CBAM以增强关键区域关注；解码端使用T5生成文本描述。临床元数据（MES等级、血管模式、出血、红斑、脆性、溃疡）作为自然语言提示注入到T5，以引导生成并同时输出MES分类与病变标签。

Result: 结果上相较于基线方法，论文报告生成的描述质量（可能用BLEU、ROUGE或临床相关评估指标衡量）和MES分类准确率均有所提升；系统还能生成与临床实践对齐的结构化描述和病变标签，增强报告可信度与可用性。

Conclusion: 该论文构建了一个针对溃疡性结肠炎(UC)的病变感知图像描述框架，结合视觉特征、可解释性热图和注意力机制，并将临床元数据以自然语言提示融入生成过程，从而实现结构化、临床对齐的内镜报告与MES分级及病变标注。

Abstract: We present a lesion-aware image captioning framework for ulcerative colitis
(UC). The model integrates ResNet embeddings, Grad-CAM heatmaps, and
CBAM-enhanced attention with a T5 decoder. Clinical metadata (MES score 0-3,
vascular pattern, bleeding, erythema, friability, ulceration) is injected as
natural-language prompts to guide caption generation. The system produces
structured, interpretable descriptions aligned with clinical practice and
provides MES classification and lesion tags. Compared with baselines, our
approach improves caption quality and MES classification accuracy, supporting
reliable endoscopic reporting.

</details>


### [20] [Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens](https://arxiv.org/abs/2509.03025)
*Sohee Kim,Soohyun Ryu,Joonhyung Park,Eunho Yang*

Main category: cs.CV

TL;DR: 作者发现 LVLMs 内部存在一组对“文本缺乏视觉证据”敏感的 FFN 神经元（VA），基于此构建检测器并在生成时据此纠正模型输出，显著减少模型将不可见文本误当作图像内容的错误。


<details>
  <summary>Details</summary>
Motivation: 观察到 LVLMs 在面对文本信息（如问题中包含的实体）时容易假定这些信息在图像中存在，导致错误回答；因此希望探究模型自身是否具备判断文本概念是否在图像中出现的内部能力，并利用该能力减少错误。

Method: 通过神经元级别分析识别出一类在视觉缺失时有特征性激活模式的 FFN 神经元（VA 神经元），构建检测模块基于这些激活模式判断输入 token 是否与图像有视觉对应，并在生成阶段通过重新解释问题或替换被检测为缺失的 token 来校正输出。

Result: 实验表明所提出的检测与修正方法能有效降低模型错误假定视觉存在的倾向，并在多种 LVLM 上展现出良好的通用性与效果提升。

Conclusion: 该论文发现并解决了大视觉-语言模型在处理文本输入时常错误地将缺乏视觉证据的文字视为图像内容的问题，提出并验证了利用内部 FFN 神经元信号检测“视觉缺失”并据此修正模型输出的方法。

Abstract: Large Vision-Language Models (LVLMs) generate contextually relevant responses
by jointly interpreting visual and textual inputs. However, our finding reveals
they often mistakenly perceive text inputs lacking visual evidence as being
part of the image, leading to erroneous responses. In light of this finding, we
probe whether LVLMs possess an internal capability to determine if textual
concepts are grounded in the image, and discover a specific subset of
Feed-Forward Network (FFN) neurons, termed Visual Absence-aware (VA) neurons,
that consistently signal the visual absence through a distinctive activation
pattern. Leveraging these patterns, we develop a detection module that
systematically classifies whether an input token is visually grounded. Guided
by its prediction, we propose a method to refine the outputs by reinterpreting
question prompts or replacing the detected absent tokens during generation.
Extensive experiments show that our method effectively mitigates the models'
tendency to falsely presume the visual presence of text input and its
generality across various LVLMs.

</details>


### [21] [Background Matters Too: A Language-Enhanced Adversarial Framework for Person Re-Identification](https://arxiv.org/abs/2509.03032)
*Kaicong Huang,Talha Azfar,Jack M. Reilly,Thomas Guggisberg,Ruimin Ke*

Main category: cs.CV

TL;DR: 引入前景与背景双分支跨模态学习，通过语义对齐与对抗学习同时强化前景和抑制背景，在多项ReID基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前方法多集中在前景信息或依赖人工标注分割，而忽视背景语义可能提供的抑制和辅助作用。受人类感知启发，背景语义与前景同样重要，可帮助消除干扰并增强识别。

Method: 构建视觉-文本双分支网络，分别提取前景与背景特征；采用内部语义对齐（对齐具有相同语义的视觉与文本特征）和跨语义对抗学习（惩罚前景与背景特征的相似性）训练策略，使模型学会抑制背景干扰并增强前景判别力。

Result: 在两个完整行人ReID基准和两个遮挡ReID基准上进行了全面实验，方法在多个指标上达到或超越当前最先进水平，验证了方法的有效性与泛化性。

Conclusion: 本文提出了一个端到端的双分支跨模态特征提取框架，同时建模前景与背景信息，通过内部语义对齐和跨语义对抗学习来区分并强化前景/背景表征，从而在ReID任务中抑制背景噪声并提升身份相关特征的聚焦能力。

Abstract: Person re-identification faces two core challenges: precisely locating the
foreground target while suppressing background noise and extracting
fine-grained features from the target region. Numerous visual-only approaches
address these issues by partitioning an image and applying attention modules,
yet they rely on costly manual annotations and struggle with complex
occlusions. Recent multimodal methods, motivated by CLIP, introduce semantic
cues to guide visual understanding. However, they focus solely on foreground
information, but overlook the potential value of background cues. Inspired by
human perception, we argue that background semantics are as important as the
foreground semantics in ReID, as humans tend to eliminate background
distractions while focusing on target appearance. Therefore, this paper
proposes an end-to-end framework that jointly models foreground and background
information within a dual-branch cross-modal feature extraction pipeline. To
help the network distinguish between the two domains, we propose an
intra-semantic alignment and inter-semantic adversarial learning strategy.
Specifically, we align visual and textual features that share the same
semantics across domains, while simultaneously penalizing similarity between
foreground and background features to enhance the network's discriminative
power. This strategy drives the model to actively suppress noisy background
regions and enhance attention toward identity-relevant foreground cues.
Comprehensive experiments on two holistic and two occluded ReID benchmarks
demonstrate the effectiveness and generality of the proposed method, with
results that match or surpass those of current state-of-the-art approaches.

</details>


### [22] [MedLiteNet: Lightweight Hybrid Medical Image Segmentation Model](https://arxiv.org/abs/2509.03041)
*Pengyang Yu,Haoquan Wang,Gerard Marks,Tahar Kechadi,Laurence T. Yang,Sahraoui Dhelim,Nyothiri Aung*

Main category: cs.CV

TL;DR: MedLiteNet是一种面向皮肤镜分割的轻量级CNN-Transformer混合网络，结合Mobile Inverted Bottleneck、跨尺度token混合和边界感知自注意力，在小样本医疗数据上兼顾效率与边界精细度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN受限于感受野难以建模长距离依赖，Transformer虽能获取全局上下文但参数量大、计算复杂度高，不适合小样本的皮肤科数据；因此需要一种轻量且能兼顾局部细节与全局上下文的网络架构。

Method: 编码器采用深度可分离的Mobile Inverted Bottleneck块降低计算量；在瓶颈层加入跨尺度token混合单元以实现不同分辨率间的信息交换；嵌入边界感知自注意力模块以增强病变边界的表征；通过层次化特征提取和多尺度上下文聚合实现高精度分割。

Result: 在摘要中未给出具体定量结果，但声称通过层次化特征提取、多尺度上下文聚合和边界感知自注意力实现高精度分割，同时计算和参数开销受控。

Conclusion: 提出了适用于皮肤镜图像分割的轻量级CNN-Transformer混合模型MedLiteNet，旨在在小样本医疗数据上提高分割精度并减少计算开销。

Abstract: Accurate skin-lesion segmentation remains a key technical challenge for
computer-aided diagnosis of skin cancer. Convolutional neural networks, while
effective, are constrained by limited receptive fields and thus struggle to
model long-range dependencies. Vision Transformers capture global context, yet
their quadratic complexity and large parameter budgets hinder use on the
small-sample medical datasets common in dermatology. We introduce the
MedLiteNet, a lightweight CNN Transformer hybrid tailored for dermoscopic
segmentation that achieves high precision through hierarchical feature
extraction and multi-scale context aggregation. The encoder stacks depth-wise
Mobile Inverted Bottleneck blocks to curb computation, inserts a
bottleneck-level cross-scale token-mixing unit to exchange information between
resolutions, and embeds a boundary-aware self-attention module to sharpen
lesion contours.

</details>


### [23] [DCDB: Dynamic Conditional Dual Diffusion Bridge for Ill-posed Multi-Tasks](https://arxiv.org/abs/2509.03044)
*Chengjie Huang,Jiafeng Yan,Jing Li,Lu Bai*

Main category: cs.CV

TL;DR: 提出一种解耦扩散与条件生成、采用与噪声计划一致的动态条件的双扩散桥训练范式，增强多任务尤其病态任务的表现，实验证明优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决多任务场景下扩散模型难以利用任务间内在相关性、特别是在训练数据匮乏的病态问题中表现差，以及传统静态条件控制难以适应动态演化特性。

Method: 构建双扩散桥：一个用于生成条件（条件扩散），一个用于主任务重建（主扩散）；两者共享噪声时间表，动态条件随时间逐步调整统计分布并携带时间信息；理论上分析不同条件形式下的学习目标与注意力权重变化；在去雾与可见光-红外融合上做大量实验并与基线比较。

Result: 提出动态条件双扩散桥（dynamic conditional double diffusion bridge）训练范式：解耦扩散与条件生成、使用与噪声计划相同生成的动态条件以逐步调整统计特性并嵌入时间信息，从而降低学习难度；在去雾和可见光-红外融合任务上在公开数据集上达到多指标最好性能，并公开代码。

Conclusion: 动态条件减少了网络在单步去噪过程中的学习难度并改善注意力分布，解耦策略避免对监督数据的依赖，整体方法在多个病态多任务上取得了最优结果。

Abstract: Conditional diffusion models have made impressive progress in the field of
image processing, but the characteristics of constructing data distribution
pathways make it difficult to exploit the intrinsic correlation between tasks
in multi-task scenarios, which is even worse in ill-posed tasks with a lack of
training data. In addition, traditional static condition control makes it
difficult for networks to learn in multi-task scenarios with its dynamically
evolving characteristics. To address these challenges, we propose a dynamic
conditional double diffusion bridge training paradigm to build a general
framework for ill-posed multi-tasks. Firstly, this paradigm decouples the
diffusion and condition generation processes, avoiding the dependence of the
diffusion model on supervised data in ill-posed tasks. Secondly, generated by
the same noise schedule, dynamic conditions are used to gradually adjust their
statistical characteristics, naturally embed time-related information, and
reduce the difficulty of network learning. We analyze the learning objectives
of the network under different conditional forms in the single-step denoising
process and compare the changes in its attention weights in the network,
demonstrating the superiority of our dynamic conditions. Taking dehazing and
visible-infrared fusion as typical ill-posed multi-task scenarios, we achieve
the best performance in multiple indicators on public datasets. The code has
been publicly released at: https://anonymous.4open.science/r/DCDB-D3C2.

</details>


### [24] [Isolated Bangla Handwritten Character Classification using Transfer Learning](https://arxiv.org/abs/2509.03061)
*Abdul Karim,S M Rafiuddin,Jahidul Islam Razin,Tahira Alam*

Main category: cs.CV

TL;DR: 论文用迁移学习结合3DCNN、ResNet和MobileNet，在Bangla Lekha Isolated数据集上实现了99.46%测试准确率的孟加拉手写字符分类器。


<details>
  <summary>Details</summary>
Motivation: 孟加拉文字字符众多且包含复合字符，现有工作在识别准确率或泛化能力上仍有提升空间；通过迁移学习和深度网络结合，希望提高识别准确率并解决训练深层网络的梯度问题。

Method: 使用迁移学习结合多种深度网络架构（3D卷积神经网络、残差网络ResNet和轻量化MobileNet），构建分类器以避免梯度消失问题，端到端训练，输入为字符图像，输出为84类概率分布。

Result: 在Bangla Lekha Isolated数据集（166,105张图像，84类）上，训练集准确率为99.82%，测试集准确率为99.46%，优于已有基线方法。

Conclusion: 该论文提出了基于迁移学习的端到端模型，高效识别孟加拉手写基本字符、特殊字符与复合字符，实验在Bangla Lekha Isolated数据集上表现优异。

Abstract: Bangla language consists of fifty distinct characters and many compound
characters. Several notable studies have been performed to recognize Bangla
characters, both handwritten and optical. Our approach uses transfer learning
to classify the basic, distinct, as well as compound Bangla handwritten
characters while avoiding the vanishing gradient problem. Deep Neural Network
techniques such as 3D Convolutional Neural Network (3DCNN), Residual Neural
Network (ResNet), and MobileNet are applied to generate an end-to-end
classification of all possible standard formations of handwritten characters in
the Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105
Bangla character image samples categorized into 84 distinct classes, is used
for this classification model. The model achieved 99.82% accuracy on training
data and 99.46% accuracy on test data. Comparisons with various
state-of-the-art benchmarks of Bangla handwritten character classification show
that the proposed model achieves better accuracy in classifying the data.

</details>


### [25] [High Cursive Complex Character Recognition using GAN External Classifier](https://arxiv.org/abs/2509.03062)
*S M Rafiuddin*

Main category: cs.CV

TL;DR: 作者提出ADA-GAN，通过生成高质量且经对抗性扰动的合成手写字符并筛选后扩增训练数据，提升模型在连笔和复杂字符分类任务上的鲁棒性和效果。


<details>
  <summary>Details</summary>
Motivation: 提高手写字符分类器对连笔和复杂字符的鲁棒性，通过生成对抗网络生成合成数据并结合对抗扰动与外部分类器增强训练集，从而改善在复杂字符上的分类性能。

Method: 使用GAN生成手写字符图像；判别器计算置信度并筛选高置信样本；对筛选样本添加对抗扰动；将这些样本加入训练集用于训练外部分类器（卷积神经网络）；比较在不同复杂度字符集上的分类准确率，验证方法鲁棒性。

Result: 提出了ADA-GAN模型：生成器生成手写字符图像，判别器对生成图像给出置信度，符合阈值的图像在加入对抗扰动后用于扩充训练集；外部分类器在增强数据上训练，表现出比常规卷积神经网络更稳健，在连笔与复杂字符上准确率下降更小。

Conclusion: ADA-GAN在面对字符复杂度提升时，比传统CNN保持更高的准确率，说明通过生成有选择的数据增强并结合对抗噪声可以有效提高复杂手写字符分类性能。

Abstract: Handwritten characters can be trickier to classify due to their complex and
cursive nature compared to simple and non-cursive characters. We present an
external classifier along with a Generative Adversarial Network that can
classify highly cursive and complex characters. The generator network produces
fake handwritten character images, which are then used to augment the training
data after adding adversarially perturbed noise and achieving a confidence
score above a threshold with the discriminator network. The results show that
the accuracy of convolutional neural networks decreases as character complexity
increases, but our proposed model, ADA-GAN, remains more robust and effective
for both cursive and complex characters.

</details>


### [26] [TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis](https://arxiv.org/abs/2509.03095)
*Clément Hervé,Paul Garnier,Jonathan Viquerat,Elie Hachem*

Main category: cs.CV

TL;DR: 使用TRELLIS学到的通用3D表面特征，能在标注稀缺的医疗动脉瘤任务上显著提升分类、分割与流场预测性能，证明了通用3D生成模型向专业医疗领域迁移表示的有效性。


<details>
  <summary>Details</summary>
Motivation: 动机在于：医疗领域可用的标注3D数据稀缺，直接训练高性能模型困难；而通用3D生成模型在大规模数据上学到的几何表示可能包含有益的结构信息，可迁移到专门的医疗任务以弥补数据不足。

Method: 方法包括：使用在大规模非医疗3D数据上训练的生成模型TRELLIS提取表面特征，替换传统的点法向量或网格描述符；将这些TRELLIS特征输入到现有的分类、分割和图神经网络（用于流场预测）模型中，进行端到端微调或特征融合以增强下游任务表现。

Result: 实验结果表明：在Intra3D的动脉瘤识别任务、3D网格分割任务和AnXplore的时间演化血流预测任务中，加入TRELLIS表面嵌入后，分类准确率、F1、分割质量均显著优于最先进基线，血流模拟误差降低约15%。

Conclusion: 本论文结论是：将来自通用3D生成模型TRELLIS的表面几何嵌入特征迁移到医疗血管/动脉瘤任务中，可显著提升多项下游任务的性能，并将血流场预测误差降低约15%。

Abstract: Intracranial aneurysms pose a significant clinical risk yet are difficult to
detect, delineate and model due to limited annotated 3D data. We propose a
cross-domain feature-transfer approach that leverages the latent geometric
embeddings learned by TRELLIS, a generative model trained on large-scale
non-medical 3D datasets, to augment neural networks for aneurysm analysis. By
replacing conventional point normals or mesh descriptors with TRELLIS surface
features, we systematically enhance three downstream tasks: (i) classifying
aneurysms versus healthy vessels in the Intra3D dataset, (ii) segmenting
aneurysm and vessel regions on 3D meshes, and (iii) predicting time-evolving
blood-flow fields using a graph neural network on the AnXplore dataset. Our
experiments show that the inclusion of these features yields strong gains in
accuracy, F1-score and segmentation quality over state-of-the-art baselines,
and reduces simulation error by 15\%. These results illustrate the broader
potential of transferring 3D representations from general-purpose generative
models to specialized medical tasks.

</details>


### [27] [Backdoor Poisoning Attack Against Face Spoofing Attack Detection Methods](https://arxiv.org/abs/2509.03108)
*Shota Iwamatsu,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 通过将欺骗样本特征无视感知地嵌入真实图像，该论文提出的后门中毒攻击能使特定欺骗样本绕过人脸反欺诈系统，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 深度学习基的人脸反欺诈系统依赖大量训练数据，若训练集中被注入恶意样本，可能导致模型在特定攻击上失效，进而造成安全风险。

Method: 提出一种后门中毒攻击方法：在不引人注意的情况下，将从欺骗样本中提取的特征嵌入到真人人脸图像中，作为触发器，从而训练阶段植入后门使得相应的欺骗在测试时被误判为真人。

Result: 在公开数据集上的实验表明，该方法能有效躲避现有反欺诈检测器，构成现实威胁。

Conclusion: 该论文展示了在人脸反欺诈检测中存在后门中毒的现实威胁，攻击者可使特定攻击样本逃避检测。

Abstract: Face recognition systems are robust against environmental changes and noise,
and thus may be vulnerable to illegal authentication attempts using user face
photos, such as spoofing attacks. To prevent such spoofing attacks, it is
crucial to discriminate whether the input image is a live user image or a
spoofed image prior to the face recognition process. Most existing spoofing
attack detection methods utilize deep learning, which necessitates a
substantial amount of training data. Consequently, if malicious data is
injected into a portion of the training dataset, a specific spoofing attack may
be erroneously classified as live, leading to false positives.In this paper, we
propose a novel backdoor poisoning attack method to demonstrate the latent
threat of backdoor poisoning within face anti-spoofing detection. The proposed
method enables certain spoofing attacks to bypass detection by embedding
features extracted from the spoofing attack's face image into a live face image
without inducing any perceptible visual alterations.Through experiments
conducted on public datasets, we demonstrate that the proposed method
constitutes a realistic threat to existing spoofing attack detection systems.

</details>


### [28] [Information transmission: Inferring change area from change moment in time series remote sensing images](https://arxiv.org/abs/2509.03112)
*Jialu Li,Chen Wu,Meiqi Hu*

Main category: cs.CV

TL;DR: CAIM-Net通过从粗到细提取变化时刻并用多尺度时间CAM加权，再由加权时刻推断变化区域，实现变化时刻与变化区域的一体化检测，提升了时空一致性和检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常将变化区域检测与变化时刻识别视为独立任务，但变化区域能由变化时刻推断，因而应保证两者结果一致性，充分利用时间序列信息与空间变化特征的内在联系。

Method: 设计三步流水线：1) 差异提取与增强：轻量级编码器配合批次维堆叠快速提取差异特征，并用边界增强卷积强化差异边界；2) 粗略变化时刻提取：利用增强特征进行时空相关性分析，并通过两种方法确定粗略变化时刻；3) 精细变化时刻提取与变化区域推断：引入多尺度时间CAM增强来自粗略时刻的权重，然后根据加权变化时刻推断像素级变化区域。

Result: 通过在时间序列遥感图像上的验证，CAIM-Net在同时定位变化区域和识别变化时刻方面表现优异（文中宣称提升准确性与一致性），证明了基于时刻推断区域的可行性与有效性。

Conclusion: CAIM-Net提出从时间序列变化时刻推断变化区域的新范式，使变化时刻与变化区域结果保持一致，提高了时空关联性利用和检测准确性。

Abstract: Time series change detection is a critical task for exploring ecosystem
dynamics using time series remote sensing images, because it can simultaneously
indicate where and when change occur. While deep learning has shown excellent
performance in this domain, it continues to approach change area detection and
change moment identification as distinct tasks. Given that change area can be
inferred from change moment, we propose a time series change detection network,
named CAIM-Net (Change Area Inference from Moment Network), to ensure
consistency between change area and change moment results. CAIM-Net infers
change area from change moment based on the intrinsic relationship between time
series analysis and spatial change detection. The CAIM-Net comprises three key
steps: Difference Extraction and Enhancement, Coarse Change Moment Extraction,
and Fine Change Moment Extraction and Change Area Inference. In the Difference
Extraction and Enhancement, a lightweight encoder with batch dimension stacking
is designed to rapidly extract difference features. Subsequently, boundary
enhancement convolution is applied to amplify these difference features. In the
Coarse Change Moment Extraction, the enhanced difference features from the
first step are used to spatiotemporal correlation analysis, and then two
distinct methods are employed to determine coarse change moments. In the Fine
Change Moment Extraction and Change Area Inference, a multiscale temporal Class
Activation Mapping (CAM) module first increases the weight of the
change-occurring moment from coarse change moments. Then the weighted change
moment is used to infer change area based on the fact that pixels with the
change moment must have undergone a change.

</details>


### [29] [Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection](https://arxiv.org/abs/2509.03113)
*Shan Wang,Maying Shen,Nadine Chang,Chuong Nguyen,Hongdong Li,Jose M. Alvarez*

Main category: cs.CV

TL;DR: 用梯度自反估计token影响并在对比解码中引入物体相关视觉token，以无额外成本地同时缓解文本-视觉偏置和共现偏置，显著降低多模态模型幻觉（LLaVA-QA90上最高+92%准确率）。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在视觉问答等任务中存在幻觉，主要来源于两类偏置：文本-视觉偏置（过度依赖文本信息）和共现偏置（训练数据中对象配对的统计模式）。现有方法多为启发式，未考虑偏置在不同实例间的波动，因而效果有限。

Method: 1) 使用梯度反射估计每类token对最终决策的影响；2) 基于影响估计检测物体相关的视觉token；3) 将检测到的视觉token整合到影响感知的对比解码过程中，以调整生成概率，从而减少对文本线索或统计共现的过度依赖。整个流程不依赖额外训练或外部数据。

Result: 在多个实验中，本方法显著降低了幻觉现象，尤其在LLaVA-QA90数据集上最高实现92%的准确率提升。方法在不需额外资源的前提下取得了强效果。

Conclusion: 该论文提出了一种基于梯度的自我反思方法估计不同类型token（视觉、提示、历史输出）的影响，进而识别与物体相关的视觉token，并将其融入影响感知对比解码框架，旨在同时减轻文本-视觉偏置和共现偏置导致的幻觉问题。方法无需额外资源（微调、额外模型或数据统计），在LLaVA-QA90上可显著提高准确率，报告最高92%的提升。

Abstract: Hallucinations in multimodal large language model are caused by the
text-visual bias and the co-occurrence bias. The former reflects an
over-reliance on text information in the decision-making process, while the
latter arises from the statistical object-pairing patterns abstracted from the
training data. Existing mitigation methods heuristically address these biases
without understanding the fluctuating bias level across the instances. We first
propose estimating the influence of respective token types (visual, prompt, and
previous outputs) using a gradient-based self-reflection method. The estimated
token influence further enables the detection of object-related visual tokens
and their integration into an influence-aware contrastive decoding framework to
mitigate both types of biases simultaneously. Our method operates without the
need for additional resources, such as costly fine-tuning, extra models, or
data statistics. Extensive experiments show it effectively reduces
hallucinations, achieving up to a 92% accuracy increase on LLaVA-QA90.

</details>


### [30] [Towards Realistic Hand-Object Interaction with Gravity-Field Based Diffusion Bridge](https://arxiv.org/abs/2509.03114)
*Miao Xu,Xiangyu Zhu,Xusheng Liang,Zidu Wang,Jinlin Wu,Zhen Lei*

Main category: cs.CV

TL;DR: 提出GravityDB：一种基于引力场的扩散桥方法，用于生成无穿透、稳定且具真实手变形的手-物体交互，并通过文本语义引导交互区域，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂手和物体几何时常出现穿透或接触处留有明显缝隙，且难以捕捉手部在交互中的形变；需要一种能同时保证物理合理性和语义一致性的交互生成方法。

Method: 基于引力场的扩散桥模拟可变形手表面与刚性物体之间的相互作用；同时将文本描述的语义信息融入引力场构建来引导交互区域；利用扩散过程模拟吸引和碰撞的演化以避免穿透并产生接触变形。

Result: 在多个数据集上的大量定性和定量实验表明，该方法能有效减少穿透、提高抓持稳定性并生成更真实的手部变形，同时语义引导使交互区域更符合描述。

Conclusion: 该论文提出了一种基于引力场的扩散桥（GravityDB），将手-物体交互建模为一个吸引驱动过程，从而生成物理上合理、无穿透、稳定抓持并包含真实手部变形的交互结果。

Abstract: Existing reconstruction or hand-object pose estimation methods are capable of
producing coarse interaction states. However, due to the complex and diverse
geometry of both human hands and objects, these approaches often suffer from
interpenetration or leave noticeable gaps in regions that are supposed to be in
contact. Moreover, the surface of a real human hand undergoes non-negligible
deformations during interaction, which are difficult to capture and represent
with previous methods. To tackle these challenges, we formulate hand-object
interaction as an attraction-driven process and propose a Gravity-Field Based
Diffusion Bridge (GravityDB) to simulate interactions between a deformable hand
surface and rigid objects. Our approach effectively resolves the aforementioned
issues by generating physically plausible interactions that are free of
interpenetration, ensure stable grasping, and capture realistic hand
deformations. Furthermore, we incorporate semantic information from textual
descriptions to guide the construction of the gravitational field, enabling
more semantically meaningful interaction regions. Extensive qualitative and
quantitative experiments on multiple datasets demonstrate the effectiveness of
our method.

</details>


### [31] [Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation](https://arxiv.org/abs/2509.03141)
*Mattia Litrico,Francesco Guarnera,Mario Valerio Giuffrida,Daniele Ravì,Sebastiano Battiato*

Main category: cs.CV

TL;DR: 提出TADM-3D：一个结合脑龄引导与双向时间正则化的3D扩散模型，用以生成更符合临床时间进展的未来MRI，且在OASIS-3与NACC上展示了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间-结构关系捕捉、单纯插值生成未来病理进展以及2D切片架构忽略3D解剖语境方面存在不足，尤其在年龄不平衡数据上难以学习时间效应。

Method: 构建3D时序感知扩散模型（TADM-3D），将预训练的脑龄估计器（BAE）作为条件引导生成过程，并引入Back-In-Time Regularisation（BITR）通过前向（基线->随访）与反向（随访->基线）训练增强时间一致性；使用全体积3D架构避免切片级方法的语境损失。

Result: 在OASIS-3数据集上训练并在NACC外部数据集验证，实验显示TADM-3D生成的随访MRI更好反映预期的年龄差异和结构变化（文中声称提高时间感知和生成质量）；代码将在接受后公开。

Conclusion: TADM-3D提出了一种基于3D扩散模型的时序感知方法，结合预训练脑龄估计器与双向时间正则化（BITR），在OASIS-3上训练并在NACC上外部验证，能够生成更符合年龄变化和时间间隔的未来MRI，从而改善长期结构变化预测。

Abstract: Generating realistic MRIs to accurately predict future changes in the
structure of brain is an invaluable tool for clinicians in assessing clinical
outcomes and analysing the disease progression at the patient level. However,
current existing methods present some limitations: (i) some approaches fail to
explicitly capture the relationship between structural changes and time
intervals, especially when trained on age-imbalanced datasets; (ii) others rely
only on scan interpolation, which lack clinical utility, as they generate
intermediate images between timepoints rather than future pathological
progression; and (iii) most approaches rely on 2D slice-based architectures,
thereby disregarding full 3D anatomical context, which is essential for
accurate longitudinal predictions. We propose a 3D Temporally-Aware Diffusion
Model (TADM-3D), which accurately predicts brain progression on MRI volumes. To
better model the relationship between time interval and brain changes, TADM-3D
uses a pre-trained Brain-Age Estimator (BAE) that guides the diffusion model in
the generation of MRIs that accurately reflect the expected age difference
between baseline and generated follow-up scans. Additionally, to further
improve the temporal awareness of TADM-3D, we propose the Back-In-Time
Regularisation (BITR), by training TADM-3D to predict bidirectionally from the
baseline to follow-up (forward), as well as from the follow-up to baseline
(backward). Although predicting past scans has limited clinical applications,
this regularisation helps the model generate temporally more accurate scans. We
train and evaluate TADM-3D on the OASIS-3 dataset, and we validate the
generalisation performance on an external test set from the NACC dataset. The
code will be available upon acceptance.

</details>


### [32] [Preserving instance continuity and length in segmentation through connectivity-aware loss computation](https://arxiv.org/abs/2509.03154)
*Karol Szustakowski,Luk Frank,Julia Esser,Jan Gründemann,Marie Piraud*

Main category: cs.CV

TL;DR: 本文提出两种新的损失函数（Negative Centerline Loss 与 Simplified Topology Loss），用于改善CNN在生物医学长条状结构分割上的连通性，尤其在信号缺失处能减少断裂，提升实例长度测量。作者还讨论了下采样与间距校正等实验设计细节，并在3D光片荧光显微镜的轴突起始区数据集上验证，结果优于标准CNN和已有拓扑损失。


<details>
  <summary>Details</summary>
Motivation: 许多生物医学分割任务更看重细长结构的连续性与长度而非逐像素精度，而现有损失函数难以保证在信号缺失处保持连通性，因此需要专门的拓扑/中心线相关损失。

Method: 作者设计了两种损失：Negative Centerline Loss（针对中心线位置罚项，鼓励预测填补断裂）和Simplified Topology Loss（简化的拓扑一致性项），并在训练时与常规损失结合；同时通过调整下采样和体素间距校正等实验设置来增强连续性。

Result: 在3D AIS数据集上，所提方法在减少每个实例的断裂数目、尤其是信号丢失区域，及提升实例长度估计方面均超越标准CNN与现有拓扑感知损失。

Conclusion: 将结构先验嵌入损失设计能显著提升生物学分割任务中结构连续性的可靠性，本文方法在减少分割断裂和改善实例长度计算方面表现较好。

Abstract: In many biomedical segmentation tasks, the preservation of elongated
structure continuity and length is more important than voxel-wise accuracy. We
propose two novel loss functions, Negative Centerline Loss and Simplified
Topology Loss, that, applied to Convolutional Neural Networks (CNNs), help
preserve connectivity of output instances. Moreover, we discuss characteristics
of experiment design, such as downscaling and spacing correction, that help
obtain continuous segmentation masks. We evaluate our approach on a 3D
light-sheet fluorescence microscopy dataset of axon initial segments (AIS), a
task prone to discontinuity due to signal dropout. Compared to standard CNNs
and existing topology-aware losses, our methods reduce the number of
segmentation discontinuities per instance, particularly in regions with missing
input signal, resulting in improved instance length calculation in downstream
applications. Our findings demonstrate that structural priors embedded in the
loss design can significantly enhance the reliability of segmentation for
biological applications.

</details>


### [33] [Count2Density: Crowd Density Estimation without Location-level Annotations](https://arxiv.org/abs/2509.03170)
*Mattia Litrico,Feng Chen,Michael Pound,Sotirios A Tsaftaris,Sebastiano Battiato,Mario Valerio Giuffrida*

Main category: cs.CV

TL;DR: 仅用图像总体人数标签，通过历史地图库+无监督先验+超几何采样生成伪密度图，并加对比空间正则，显著提升密度图预测和分区计数表现。


<details>
  <summary>Details</summary>
Motivation: 减少点级标注依赖，降低标注成本并提高方法在真实场景的可扩展性，同时恢复密度图的空间定量信息以支持子区域计数。

Method: 初始化用无监督显著性估计构建历史地图银行；训练期间用EMA更新银行并从中采样以生成伪密度图，采样位置按超几何分布、采样次数由图像计数决定；加入自监督对比空间正则，促使拥挤区域特征相似、与背景差异化。

Result: Count2Density提出利用仅有计数标签训练生成伪密度图并加入对比空间正则以恢复空间信息，能实现子区域计数并在多数据集上优于相关方法。

Conclusion: Count2Density能从粗糙计数标签有效恢复定量空间密度信息，在半监督和跨域场景均表现良好，各组件（历史地图库、无监督先验、超几何采样、对比正则）均有贡献。

Abstract: Crowd density estimation is a well-known computer vision task aimed at
estimating the density distribution of people in an image. The main challenge
in this domain is the reliance on fine-grained location-level annotations,
(i.e. points placed on top of each individual) to train deep networks.
Collecting such detailed annotations is both tedious, time-consuming, and poses
a significant barrier to scalability for real-world applications. To alleviate
this burden, we present Count2Density: a novel pipeline designed to predict
meaningful density maps containing quantitative spatial information using only
count-level annotations (i.e., total number of people) during training. To
achieve this, Count2Density generates pseudo-density maps leveraging past
predictions stored in a Historical Map Bank, thereby reducing confirmation
bias. This bank is initialised using an unsupervised saliency estimator to
provide an initial spatial prior and is iteratively updated with an EMA of
predicted density maps. These pseudo-density maps are obtained by sampling
locations from estimated crowd areas using a hypergeometric distribution, with
the number of samplings determined by the count-level annotations. To further
enhance the spatial awareness of the model, we add a self-supervised
contrastive spatial regulariser to encourage similar feature representations
within crowded regions while maximising dissimilarity with background regions.
Experimental results demonstrate that our approach significantly outperforms
cross-domain adaptation methods and achieves better results than recent
state-of-the-art approaches in semi-supervised settings across several
datasets. Additional analyses validate the effectiveness of each individual
component of our pipeline, confirming the ability of Count2Density to
effectively retrieve spatial information from count-level annotations and
enabling accurate subregion counting.

</details>


### [34] [AutoDetect: Designing an Autoencoder-based Detection Method for Poisoning Attacks on Object Detection Applications in the Military Domain](https://arxiv.org/abs/2509.03179)
*Alma M. Liezenga,Stefan Wijnja,Puck de Haan,Niels W. T. Brink,Jip J. van Stijn,Yori Kamphuis,Klamer Schutte*

Main category: cs.CV

TL;DR: 研究表明：军事目标检测器可被补丁式数据中毒攻击影响，但现实中攻击需要大量污染数据；现有检测方法不足，提出的轻量级AutoDetect自编码器切片重建误差方法能更好地检测被污染样本；强调需更大、更具代表性军事数据集以进一步研究。


<details>
  <summary>Details</summary>
Motivation: 动机是：随着开源数据集与预训练模型广泛使用，军事AI系统面临越来越高的数据中毒风险；但针对目标检测器（尤其军事场景）的中毒攻击与检测研究匮乏，而此类攻击在军事领域后果严重，因而需要实证研究攻击影响并提出有效检测手段。

Method: 方法包括：构建MilCivVeh小型军事车辆数据集；实现并修改BadDet为补丁式中毒攻击以评估目标检测器脆弱性；比较多种检测方法（专用中毒检测和工业视觉异常检测）；提出AutoDetect——对图像分块，用轻量自编码器重建并以重建误差作为异常分数进行样本分类。

Result: 结果显示：修改后的BadDet能实现正的攻击成功率，但需要污染大量数据，令其实用性受限；现有检测方法普遍表现不佳；AutoDetect在分割图像并基于重建误差进行检测时，准确率优于比较方法，且运行更快、占用内存更少。

Conclusion: 本论文结论为：军事领域的目标检测器确实易受补丁式数据中毒攻击影响，但要在实践中取得高攻击成功率需污染大量训练数据；现有专用中毒检测与工业视觉异常检测方法效果不足，作者提出的AutoDetect（基于自编码器的切片重建误差检测）在准确性、速度和内存开销上表现更优，能较好区分被污染与干净样本；同时强调需要大型、代表性的军事数据集以进一步评估风险与检测方法。

Abstract: Poisoning attacks pose an increasing threat to the security and robustness of
Artificial Intelligence systems in the military domain. The widespread use of
open-source datasets and pretrained models exacerbates this risk. Despite the
severity of this threat, there is limited research on the application and
detection of poisoning attacks on object detection systems. This is especially
problematic in the military domain, where attacks can have grave consequences.
In this work, we both investigate the effect of poisoning attacks on military
object detectors in practice, and the best approach to detect these attacks. To
support this research, we create a small, custom dataset featuring military
vehicles: MilCivVeh. We explore the vulnerability of military object detectors
for poisoning attacks by implementing a modified version of the BadDet attack:
a patch-based poisoning attack. We then assess its impact, finding that while a
positive attack success rate is achievable, it requires a substantial portion
of the data to be poisoned -- raising questions about its practical
applicability. To address the detection challenge, we test both specialized
poisoning detection methods and anomaly detection methods from the visual
industrial inspection domain. Since our research shows that both classes of
methods are lacking, we introduce our own patch detection method: AutoDetect, a
simple, fast, and lightweight autoencoder-based method. Our method shows
promising results in separating clean from poisoned samples using the
reconstruction error of image slices, outperforming existing methods, while
being less time- and memory-intensive. We urge that the availability of large,
representative datasets in the military domain is a prerequisite to further
evaluate risks of poisoning attacks and opportunities patch detection.

</details>


### [35] [PPORLD-EDNetLDCT: A Proximal Policy Optimization-Based Reinforcement Learning Framework for Adaptive Low-Dose CT Denoising](https://arxiv.org/abs/2509.03185)
*Debopom Sutradhar,Ripon Kumar Debnath,Mohaimenul Azam Khan Raiaan,Yan Zhang,Reem E. Mohamed,Sami Azam*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习（PPO）和编码器-解码器的低剂量CT去噪方法PPORLD-EDNetLDCT，通过自定义gym环境实时优化去噪策略，在多个数据集上优于传统和深度学习方法，PSNR、SSIM和RMSE指标显著提升，并在COVID-19分类任务中提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT为了减少辐射但会带来噪声和图像质量下降，传统迭代和监督学习去噪方法常损失结构细节或泛化能力不足，故引入强化学习以动态、自适应地优化去噪策略以保留结构信息并提升图像质量。

Method: 方法基于PPO强化学习框架，构建自定义gym环境以实时反馈图像质量指标；使用编码器-解码器网络作为去噪策略的动作执行器；训练过程中以PSNR/SSIM/RMSE等指标作为奖励信号，动态调整策略以优化去噪效果。

Result: 在低剂量CT图像与投影数据集上取得PSNR=41.87、SSIM=0.9814、RMSE=0.00236；在NIH-AAPM-Mayo Clinic数据集上PSNR=41.52、SSIM=0.9723、RMSE=0.0051；COVID-19 LDCT分类任务中经处理图像分类准确率提升至94%，较未使用RL去噪提升4%。

Conclusion: PPORLD-EDNetLDCT通过引入强化学习动态优化去噪策略，能够在保持结构细节的同时降低噪声，实验证明在多个标准数据集上性能优于现有方法，并能提高下游分类任务的准确率，具有实际应用潜力。

Abstract: Low-dose computed tomography (LDCT) is critical for minimizing radiation
exposure, but it often leads to increased noise and reduced image quality.
Traditional denoising methods, such as iterative optimization or supervised
learning, often fail to preserve image quality. To address these challenges, we
introduce PPORLD-EDNetLDCT, a reinforcement learning-based (RL) approach with
Encoder-Decoder for LDCT. Our method utilizes a dynamic RL-based approach in
which an advanced posterior policy optimization (PPO) algorithm is used to
optimize denoising policies in real time, based on image quality feedback,
trained via a custom gym environment. The experimental results on the low dose
CT image and projection dataset demonstrate that the proposed PPORLD-EDNetLDCT
model outperforms traditional denoising techniques and other DL-based methods,
achieving a peak signal-to-noise ratio of 41.87, a structural similarity index
measure of 0.9814 and a root mean squared error of 0.00236. Moreover, in
NIH-AAPM-Mayo Clinic Low Dose CT Challenge dataset our method achived a PSNR of
41.52, SSIM of 0.9723 and RMSE of 0.0051. Furthermore, we validated the quality
of denoising using a classification task in the COVID-19 LDCT dataset, where
the images processed by our method improved the classification accuracy to
94\%, achieving 4\% higher accuracy compared to denoising without RL-based
denoising. This method offers a promising solution for safer and more accurate
LDCT imaging.

</details>


### [36] [AIVA: An AI-based Virtual Companion for Emotion-aware Interaction](https://arxiv.org/abs/2509.03212)
*Chenxi Li*

Main category: cs.CV

TL;DR: Introduce ours: integrates multimodal sentiment perception into LLMs via MSPN and prompt engineering, plus TTS and avatar for empathetic virtual companion.


<details>
  <summary>Details</summary>
Motivation: LLMs lack multimodal emotional perception; need emotion-aware HCI agents.

Method: Develop MSPN using cross-modal fusion transformer and supervised contrastive learning; design emotion-aware prompt engineering; integrate TTS and animated avatar modules for expressive output.

Result: Proposed ours with MSPN (cross-modal transformer + supervised contrastive learning), emotion-aware prompts, TTS and animated avatar; enables empathetic, expressive interactions; applications in companion robotics, social care, mental health.

Conclusion: ours shows a framework for creating emotion-aware agents by fusing multimodal sentiment cues into LLM-driven interactions, improving empathy and expressiveness in HCI across several domains.

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved
natural language understanding and generation, enhancing Human-Computer
Interaction (HCI). However, LLMs are limited to unimodal text processing and
lack the ability to interpret emotional cues from non-verbal signals, hindering
more immersive and empathetic interactions. This work explores integrating
multimodal sentiment perception into LLMs to create emotion-aware agents. We
propose \ours, an AI-based virtual companion that captures multimodal sentiment
cues, enabling emotionally aligned and animated HCI. \ours introduces a
Multimodal Sentiment Perception Network (MSPN) using a cross-modal fusion
transformer and supervised contrastive learning to provide emotional cues.
Additionally, we develop an emotion-aware prompt engineering strategy for
generating empathetic responses and integrate a Text-to-Speech (TTS) system and
animated avatar module for expressive interactions. \ours provides a framework
for emotion-aware agents with applications in companion robotics, social care,
mental health, and human-centered AI.

</details>


### [37] [RTGMFF: Enhanced fMRI-based Brain Disorder Diagnosis via ROI-driven Text Generation and Multimodal Feature Fusion](https://arxiv.org/abs/2509.03214)
*Junhao Jia,Yifei Sun,Yunyou Liu,Cheng Yang,Changmiao Wang,Feiwei Qin,Yong Peng,Wenwen Min*

Main category: cs.CV

TL;DR: 提出RTGMFF框架，通过自动生成ROI级文本并与多模态特征融合，改进fMRI脑病诊断。核心包括ROI驱动文本生成、频率-空间混合编码器和自适应语义对齐模块，在ADHD-200和ABIDE数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI诊断受低信噪比、个体差异及模型对频率信息感知不足影响，且缺乏可使激活/连接模式被文本化的标注。作者旨在通过生成可重复的ROI文本并融合频率与空间信息来缓解这些问题。

Method: 方法由三部分组成：1) ROI驱动fMRI文本生成，将激活、连接、年龄、性别等信息确定性地编码为ROI级文本token；2) 混合频率-空间编码器，包含层次小波分支（捕捉频率结构）与跨尺度Transformer（捕捉长程空间依赖）；3) 自适应语义对齐模块，将ROI token序列与视觉特征映射至共享嵌入空间，并用正则化余弦相似度损失缩小模态差距。

Result: 在ADHD-200与ABIDE基准上，RTGMFF在诊断准确率、敏感性、特异性和ROC AUC上均超越现有方法，表明该框架能有效利用自动生成的语义信息与多尺度频率-空间特征。

Conclusion: RTGMFF通过引入可重复的ROI文本描述、结合小波频域分支与跨尺度Transformer的混合编码，并用正则化余弦相似度对齐文本与视觉特征，显著提升了fMRI疾病诊断的准确性、敏感性、特异性与AUC。

Abstract: Functional magnetic resonance imaging (fMRI) is a powerful tool for probing
brain function, yet reliable clinical diagnosis is hampered by low
signal-to-noise ratios, inter-subject variability, and the limited frequency
awareness of prevailing CNN- and Transformer-based models. Moreover, most fMRI
datasets lack textual annotations that could contextualize regional activation
and connectivity patterns. We introduce RTGMFF, a framework that unifies
automatic ROI-level text generation with multimodal feature fusion for
brain-disorder diagnosis. RTGMFF consists of three components: (i) ROI-driven
fMRI text generation deterministically condenses each subject's activation,
connectivity, age, and sex into reproducible text tokens; (ii) Hybrid
frequency-spatial encoder fuses a hierarchical wavelet-mamba branch with a
cross-scale Transformer encoder to capture frequency-domain structure alongside
long-range spatial dependencies; and (iii) Adaptive semantic alignment module
embeds the ROI token sequence and visual features in a shared space, using a
regularized cosine-similarity loss to narrow the modality gap. Extensive
experiments on the ADHD-200 and ABIDE benchmarks show that RTGMFF surpasses
current methods in diagnostic accuracy, achieving notable gains in sensitivity,
specificity, and area under the ROC curve. Code is available at
https://github.com/BeistMedAI/RTGMFF.

</details>


### [38] [LGBP-OrgaNet: Learnable Gaussian Band Pass Fusion of CNN and Transformer Features for Robust Organoid Segmentation and Tracking](https://arxiv.org/abs/2509.03221)
*Jing Zhang,Siying Tao,Jiao Li,Tianhe Wang,Junchen Wu,Ruqian Hao,Xiaohui Du,Ruirong Tan,Rui Li*

Main category: cs.CV

TL;DR: 提出LGBP-OrgaNet：CNN+Transformer双分支并用可学习高斯带通融合与双向交叉融合块，实现无损类器官分割与追踪，结果显示精度和鲁棒性良好。


<details>
  <summary>Details</summary>
Motivation: 传统荧光标记会破坏类器官结构，且手动或破坏性方法不适合长期追踪，因此需要一种自动化、非破坏性的类器官分割与追踪方法来评估生长/发育状态。

Method: 提出双分支网络（CNN分支+Transformer分支），使用Learnable Gaussian Band Pass Fusion模块在特征级别融合两分支信息；在解码器中采用Bidirectional Cross Fusion Block进行多尺度特征交互，并通过逐步拼接+上采样完成解码与输出。

Result: 在类器官分割数据集上SROrga（可能为LGBP-OrgaNet的实现）展示了令人满意的分割精度与鲁棒性，能够用于量化类器官体积/形状等指标，辅助肿瘤治疗与药物筛选研究。

Conclusion: 该论文提出了一种无损的类器官分割与追踪方法LGBP-OrgaNet，通过融合CNN与Transformer特征并引入可学习高斯带通融合模块以及双向交叉融合块，实现了多尺度特征有效融合，最终在数据集上表现出较好的分割准确性与鲁棒性。

Abstract: Organoids replicate organ structure and function, playing a crucial role in
fields such as tumor treatment and drug screening. Their shape and size can
indicate their developmental status, but traditional fluorescence labeling
methods risk compromising their structure. Therefore, this paper proposes an
automated, non-destructive approach to organoid segmentation and tracking. We
introduced the LGBP-OrgaNet, a deep learning-based system proficient in
accurately segmenting, tracking, and quantifying organoids. The model leverages
complementary information extracted from CNN and Transformer modules and
introduces the innovative feature fusion module, Learnable Gaussian Band Pass
Fusion, to merge data from two branches. Additionally, in the decoder, the
model proposes a Bidirectional Cross Fusion Block to fuse multi-scale features,
and finally completes the decoding through progressive concatenation and
upsampling. SROrga demonstrates satisfactory segmentation accuracy and
robustness on organoids segmentation datasets, providing a potent tool for
organoid research.

</details>


### [39] [PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR](https://arxiv.org/abs/2509.03262)
*Fabio F. Oberweger,Michael Schwingshackl,Vanessa Staderini*

Main category: cs.CV

TL;DR: 提出PI3DETR端到端从点云直接预测多种3D参数化曲线，方法简单、鲁棒并在ABC数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 避免以往工作中常见的中间表示和多阶段处理，提升对噪声和采样密度变化的鲁棒性，适应实际LiDAR和3D传感场景。

Method: 在3DETR基础上引入几何感知匹配策略和专门的损失函数，实现统一检测多种参数化曲线类型（立方贝塞尔、线段、圆和弧）。可选的后处理步骤用于精细化预测。

Result: 在ABC数据集上达到了新的最先进水平，并能有效推广到真实传感器数据。

Conclusion: PI3DETR提出了一个端到端框架，直接从点云预测3D参数化曲线实例，简化流程并提高鲁棒性。

Abstract: We present PI3DETR, an end-to-end framework that directly predicts 3D
parametric curve instances from raw point clouds, avoiding the intermediate
representations and multi-stage processing common in prior work. Extending
3DETR, our model introduces a geometry-aware matching strategy and specialized
loss functions that enable unified detection of differently parameterized curve
types, including cubic B\'ezier curves, line segments, circles, and arcs, in a
single forward pass. Optional post-processing steps further refine predictions
without adding complexity. This streamlined design improves robustness to noise
and varying sampling densities, addressing critical challenges in real world
LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC
dataset and generalizes effectively to real sensor data, offering a simple yet
powerful solution for 3D edge and curve estimation.

</details>


### [40] [SynBT: High-quality Tumor Synthesis for Breast Tumor Segmentation by 3D Diffusion Model](https://arxiv.org/abs/2509.03267)
*Hongxu Yang,Edina Timko,Levente Lippenszky,Vanda Czipczer,Lehel Ferenczi*

Main category: cs.CV

TL;DR: 提出一种用于大FOV三维乳腺MRI肿瘤合成的patch-to-volume自编码器+掩码条件扩散模型（SynBT），能生成逼真肿瘤并将分割模型Dice提升2-3%。


<details>
  <summary>Details</summary>
Motivation: 现有肿瘤合成方法多基于小patch，难以在大体积/大FOV的MRI中生成真实且高质量的肿瘤，限制了合成数据用于提升分割性能的效果。

Method: 构建patch-to-volume自编码器将高分辨率体积压缩到潜在空间，同时保持大FOV的信息；在潜在特征上训练一个基于掩码条件的扩散模型以在选定乳腺组织区域生成三维肿瘤。

Result: 在大型公开数据集上的分割任务中，使用SynBT合成数据进行增强，常见分割模型的Dice得到了约2-3%的提升，证明合成肿瘤在提升分割性能方面的有效性。

Conclusion: 提出的SynBT能在大视野高分辨率乳腺MRI中生成高质量合成肿瘤，且通过用于数据增强可提升分割模型的Dice 2-3%。

Abstract: Synthetic tumors in medical images offer controllable characteristics that
facilitate the training of machine learning models, leading to an improved
segmentation performance. However, the existing methods of tumor synthesis
yield suboptimal performances when tumor occupies a large spatial volume, such
as breast tumor segmentation in MRI with a large field-of-view (FOV), while
commonly used tumor generation methods are based on small patches. In this
paper, we propose a 3D medical diffusion model, called SynBT, to generate
high-quality breast tumor (BT) in contrast-enhanced MRI images. The proposed
model consists of a patch-to-volume autoencoder, which is able to compress the
high-resolution MRIs into compact latent space, while preserving the resolution
of volumes with large FOV. Using the obtained latent space feature vector, a
mask-conditioned diffusion model is used to synthesize breast tumors within
selected regions of breast tissue, resulting in realistic tumor appearances. We
evaluated the proposed method for a tumor segmentation task, which demonstrated
the proposed high-quality tumor synthesis method can facilitate the common
segmentation models with performance improvement of 2-3% Dice Score on a large
public dataset, and therefore provides benefits for tumor segmentation in MRI
images.

</details>


### [41] [PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection](https://arxiv.org/abs/2509.03277)
*Qihang Zhou,Shibo He,Jiangtao Yan,Wenchao Meng,Jiming Chen*

Main category: cs.CV

TL;DR: 本文提出PointAD+，通过隐式渲染与显式几何层级联合、G-aggregation与跨层对比对齐，将CLIP的2D泛化能力成功迁移到3D无监督异常检测，支持插拔RGB并在ZS场景下展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 将CLIP在2D上的强泛化能力扩展到3D异常检测，以应对未知类别和高语义多样性的对象，综合利用点云与像素信息提升检测性能。

Method: 提出PointAD（基于点-像素对应的隐式3D表示）与PointAD+（引入显式空间几何表示、G-aggregation使点表示具备空间感知、层级表示学习及跨层对比对齐结合渲染与几何提示），测试时可插拔使用RGB信息。

Result: 在零样本（ZS）3D异常检测任务上，PointAD+在多类未知对象上实现了优越性能，能够综合捕捉渲染与空间异常语义，实现更全面的异常理解。

Conclusion: PointAD+通过结合隐式（渲染像素）与显式（空间几何）表示，并采用层级文本提示与跨层对比对齐，有效将CLIP的2D泛化能力迁移至3D无监督异常检测，实现对未知类别的3D异常检测与分割。

Abstract: In this paper, we aim to transfer CLIP's robust 2D generalization
capabilities to identify 3D anomalies across unseen objects of highly diverse
class semantics. To this end, we propose a unified framework to comprehensively
detect and segment 3D anomalies by leveraging both point- and pixel-level
information. We first design PointAD, which leverages point-pixel
correspondence to represent 3D anomalies through their associated rendering
pixel representations. This approach is referred to as implicit 3D
representation, as it focuses solely on rendering pixel anomalies but neglects
the inherent spatial relationships within point clouds. Then, we propose
PointAD+ to further broaden the interpretation of 3D anomalies by introducing
explicit 3D representation, emphasizing spatial abnormality to uncover abnormal
spatial relationships. Hence, we propose G-aggregation to involve geometry
information to enable the aggregated point representations spatially aware. To
simultaneously capture rendering and spatial abnormality, PointAD+ proposes
hierarchical representation learning, incorporating implicit and explicit
anomaly semantics into hierarchical text prompts: rendering prompts for the
rendering layer and geometry prompts for the geometry layer. A cross-hierarchy
contrastive alignment is further introduced to promote the interaction between
the rendering and geometry layers, facilitating mutual anomaly learning.
Finally, PointAD+ integrates anomaly semantics from both layers to capture the
generalized anomaly semantics. During the test, PointAD+ can integrate RGB
information in a plug-and-play manner and further improve its detection
performance. Extensive experiments demonstrate the superiority of PointAD+ in
ZS 3D anomaly detection across unseen objects with highly diverse class
semantics, achieving a holistic understanding of abnormality.

</details>


### [42] [Empowering Lightweight MLLMs with Reasoning via Long CoT SFT](https://arxiv.org/abs/2509.03321)
*Linyu Ou*

Main category: cs.CV

TL;DR: 对于<7B参数的轻量级MLLMs，先用长CoT进行SFT是提升推理能力的关键，随后可通过RL获得进一步改进。


<details>
  <summary>Details</summary>
Motivation: 探索长Chain-of-Thought数据对参数低于7B的轻量级多模态语言模型(MLLMs)推理能力的效果，弥补现有关于大型LLMs的RL与可验证奖励研究在轻量级模型上的空白。

Method: 采用两阶段训练：先用包含长CoT的监督微调(SFT)训练模型，然后在该基础上进行强化学习(RL)以进一步提升性能。

Result: 实验证明：1) SFT使用长CoT显著提升MLLMs的推理能力；2) 在SFT后追加RL阶段可以带来额外性能提升；3) SFT是RL有效性的关键前提。

Conclusion: SFT阶段使用long CoT数据对轻量级多模态语言模型的推理能力提升至关重要；在SFT之后可通过RL进一步提升性能。

Abstract: While Reinforcement Learning with Verifiable Rewards has enhanced the
reasoning of large-scale language models (LLMs), its efficacy for lightweight
multimodal language models (MLLMs) with fewer than seven billion parameters
remains underexplored. This paper investigates the role of long
Chain-of-Thought (long CoT) data in enhancing the reasoning abilities of such
MLLMs. Our findings demonstrate that Supervised Fine-Tuning (SFT) with long CoT
data significantly improves MLLM reasoning. Furthermore, we observe that after
this initial SFT phase, MLLMs can achieve additional performance gains through
a subsequent RL stage. We conclude that a SFT stage with long CoT data is a
critical prerequisite for developing the reasoning capabilities of lightweight
MLLMs.

</details>


### [43] [Heatmap Guided Query Transformers for Robust Astrocyte Detection across Immunostains and Resolutions](https://arxiv.org/abs/2509.03323)
*Xizhe Zhang,Jiayang Zhu*

Main category: cs.CV

TL;DR: 提出混合CNN-Transformer检测器，热图引导查询生成空间锚点，轻量Transformer提升密集区域判别，优于主流检测器在星形胶质细胞检测任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 星形胶质细胞形态复杂且染色差异大，自动检测困难。为提高小/弱信号和密集簇中细胞的检测性能，提出结合局部特征和全局上下文的混合架构与热图引导机制。

Method: 模型由局部特征提取的CNN骨干、热图引导的查询/锚生成模块、以及轻量级Transformer上下文推理模块组成；通过热图生成小/暗细胞的空间锚点，Transformer用于在密集簇中改善判别，最终通过检测头输出边界框与置信度并用FROC评估性能。

Result: 提出了一种结合CNN与Transformer的混合检测器，用于自动检测组织学图像中的星形胶质细胞。该方法通过热图引导查询机制生成针对小而微弱细胞的空间锚点，并使用轻量级Transformer模块增强密集簇中细胞的判别能力。在ALDH1L1和GFAP染色数据集上，该模型在灵敏度和假阳性数的权衡方面优于Faster R-CNN、YOLOv11和DETR，FROC分析支持了结果。

Conclusion: 混合CNN-Transformer架构能更鲁棒地检测染色依赖和形态复杂的星形胶质细胞，为计算病理学提供可行的检测工具基础。

Abstract: Astrocytes are critical glial cells whose altered morphology and density are
hallmarks of many neurological disorders. However, their intricate branching
and stain dependent variability make automated detection of histological images
a highly challenging task. To address these challenges, we propose a hybrid CNN
Transformer detector that combines local feature extraction with global
contextual reasoning. A heatmap guided query mechanism generates spatially
grounded anchors for small and faint astrocytes, while a lightweight
Transformer module improves discrimination in dense clusters. Evaluated on
ALDH1L1 and GFAP stained astrocyte datasets, the model consistently
outperformed Faster R-CNN, YOLOv11 and DETR, achieving higher sensitivity with
fewer false positives, as confirmed by FROC analysis. These results highlight
the potential of hybrid CNN Transformer architectures for robust astrocyte
detection and provide a foundation for advanced computational pathology tools.

</details>


### [44] [InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds](https://arxiv.org/abs/2509.03324)
*Yixiong Jing,Cheng Zhang,Haibing Wu,Guangming Wang,Olaf Wysocki,Brian Sheil*

Main category: cs.CV

TL;DR: InfraDiffusion通过将点云投影为深度图并用DDNM零样本恢复，显著提高了在低光环境下的砖块级分割效果，适用于砌体结构的自动化检测。


<details>
  <summary>Details</summary>
Motivation: 传统方法在低光条件下难以获取高分辨率RGB图像，而点云虽不受光照影响但稀疏且噪声多，限制了精细分割（如砖块和缺陷检测）。因此需要一种无需额外标注、能提升点云到像素级分割能力的通用方法。

Method: 方法包括：1) 使用虚拟相机对稀疏、无结构的点云进行多视角投影生成深度图；2) 将深度图视为图像恢复问题，采用DDNM在无监督、零样本条件下恢复和增强深度图细节；3) 恢复后的深度图输入SAM进行砖块级分割；4) 在桥梁和隧道砌体点云数据集上进行定量与定性评估。

Result: 在砌体桥梁与隧道点云数据集上的实验表明，InfraDiffusion提升了深度图的视觉与几何质量，从而使SAM在砖块级分割任务上的性能显著提高（论文中给出定量指标提升，具体数值请参考原文）。

Conclusion: 本文提出了InfraDiffusion，一种无需任务特定训练的零样本框架，通过虚拟相机将砌体点云投影为深度图，并基于去噪扩散零空间模型（DDNM）进行恢复，以增强深度图的视觉清晰度和几何一致性，从而显著提升基于SAM的砖块级分割效果。

Abstract: Point clouds are widely used for infrastructure monitoring by providing
geometric information, where segmentation is required for downstream tasks such
as defect detection. Existing research has automated semantic segmentation of
structural components, while brick-level segmentation (identifying defects such
as spalling and mortar loss) has been primarily conducted from RGB images.
However, acquiring high-resolution images is impractical in low-light
environments like masonry tunnels. Point clouds, though robust to dim lighting,
are typically unstructured, sparse, and noisy, limiting fine-grained
segmentation. We present InfraDiffusion, a zero-shot framework that projects
masonry point clouds into depth maps using virtual cameras and restores them by
adapting the Denoising Diffusion Null-space Model (DDNM). Without task-specific
training, InfraDiffusion enhances visual clarity and geometric consistency of
depth maps. Experiments on masonry bridge and tunnel point cloud datasets show
significant improvements in brick-level segmentation using the Segment Anything
Model (SAM), underscoring its potential for automated inspection of masonry
assets. Our code and data is available at
https://github.com/Jingyixiong/InfraDiffusion-official-implement.

</details>


### [45] [Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing](https://arxiv.org/abs/2509.03376)
*Hui Chen,Liangyu Liu,Xianchao Xiu,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出T-CAGU：用Transformer捕获全局依赖、用内容自适应多阶图神经网络强化局部一致性并通过图残差保持全局信息，从而在高光谱解混任务中提升精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以同时兼顾全局依赖（长程交互）与局部一致性（边界细节），导致在保留细节和全局语义上存在折衷。为此提出联合Transformer与内容自适应图网络的方案。

Method: 设计了一个由Transformer编码器提取全局特征、内容自适应图神经网络（包含多阶传播和图残差机制）增强局部一致性与结构自适应性，以及相应的解混网络来估计端元与丰度的框架。多阶传播用于动态学习图结构以增强鲁棒性，图残差用于保持全局信息并稳定训练。

Result: 在若干标准高光谱数据集上，T-CAGU在重建误差、丰度估计精度等指标上优于最新方法，实验表明其在保留边界细节和抵抗噪声方面效果显著。

Conclusion: T-CAGU通过将Transformer的全局建模能力与内容自适应图神经网络的局部传播相结合，有效保持了图像的长程依赖与边界细节，从而在高光谱无混合任务上优于现有方法。

Abstract: Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remote
sensing images into a set of endmembers and their corresponding abundances.
Despite significant progress in this field using deep learning, most methods
fail to simultaneously characterize global dependencies and local consistency,
making it difficult to preserve both long-range interactions and boundary
details. This letter proposes a novel transformer-guided content-adaptive graph
unmixing framework (T-CAGU), which overcomes these challenges by employing a
transformer to capture global dependencies and introducing a content-adaptive
graph neural network to enhance local relationships. Unlike previous work,
T-CAGU integrates multiple propagation orders to dynamically learn the graph
structure, ensuring robustness against noise. Furthermore, T-CAGU leverages a
graph residual mechanism to preserve global information and stabilize training.
Experimental results demonstrate its superiority over the state-of-the-art
methods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.

</details>


### [46] [TinyDrop: Tiny Model Guided Token Dropping for Vision Transformers](https://arxiv.org/abs/2509.03379)
*Guoxin Wang,Qingyuan Wang,Binhua Huang,Shaowu Chen,Deepu John*

Main category: cs.CV

TL;DR: TinyDrop是一个无需训练、可插拔的轻量级引导token-drop方法：推理时用小模型评估token重要性，丢弃低重要性token，从而在多种ViT上实现高达80% FLOPs降低且精度损失很小。


<details>
  <summary>Details</summary>
Motivation: 大型ViT在图像分类上性能出色但计算昂贵，处理所有image token导致推理开销高。希望在不牺牲精度且无需重新训练主模型的条件下显著降低推理FLOPs。

Method: 提出一个训练-free的token-dropping框架：在推理阶段使用额外的轻量级视觉模型对每个token的重要性进行估计，然后仅将高重要性token送入大型ViT进行注意力计算；该框架无需改动主模型结构，插拔式使用，并兼容多种ViT变体。

Result: 在标准图像分类基准上，TinyDrop可将ViT的FLOPs最多降低约80%，并仅带来极小的精度下降，显示出良好的泛化能力与实际应用价值。

Conclusion: TinyDrop能够在不改变大型ViT架构和训练流程的前提下，通过训练无关的轻量级引导模型在推理时估计并丢弃低重要性token，从而大幅减少计算量，同时保持分类准确率的几乎不变。

Abstract: Vision Transformers (ViTs) achieve strong performance in image classification
but incur high computational costs from processing all image tokens. To reduce
inference costs in large ViTs without compromising accuracy, we propose
TinyDrop, a training-free token dropping framework guided by a lightweight
vision model. The guidance model estimates the importance of tokens while
performing inference, thereby selectively discarding low-importance tokens if
large vit models need to perform attention calculations. The framework operates
plug-and-play, requires no architectural modifications, and is compatible with
diverse ViT architectures. Evaluations on standard image classification
benchmarks demonstrate that our framework reduces FLOPs by up to 80% for ViTs
with minimal accuracy degradation, highlighting its generalization capability
and practical utility for efficient ViT-based classification.

</details>


### [47] [Human Preference-Aligned Concept Customization Benchmark via Decomposed Evaluation](https://arxiv.org/abs/2509.03385)
*Reina Ishikawa,Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出D-GPTScore，通过分解评估维度并使用MLLM进行逐项评分，配合CC-AlignBench基准，在单/多概念定制评估上更贴近人类偏好，显著优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 动机是现有指标对概念定制生成图像的评估往往过于宽泛或过窄，无法与人类主观偏好对齐，尤其在多概念场景下评估更为困难，需要对单个概念及概念间交互进行细粒度评估。

Method: 方法上，作者提出将整体评估标准分解为若干子方面（如与提示一致性、与概念图像相似性、概念间交互合理性等），并利用多模态大语言模型对每个方面进行打分，最终融合为综合评分。作者还构建了包含单概念和多概念任务的CC-AlignBench基准，用于阶段性评估。

Result: 结果表明D-GPTScore在作者构建的CC-AlignBench上明显优于现有方法，与人类偏好的相关性更高，能够更准确地反映单/多概念生成质量，并指出了未来研究中需要解决的关键挑战。

Conclusion: 该论文提出了一种新的评估方法D-GPTScore，旨在更好地与人类偏好对齐，通过将评估拆解为更细的方面并使用多模态大模型进行评分，从而提高了对单概念与多概念定制生成图像的评估一致性。

Abstract: Evaluating concept customization is challenging, as it requires a
comprehensive assessment of fidelity to generative prompts and concept images.
Moreover, evaluating multiple concepts is considerably more difficult than
evaluating a single concept, as it demands detailed assessment not only for
each individual concept but also for the interactions among concepts. While
humans can intuitively assess generated images, existing metrics often provide
either overly narrow or overly generalized evaluations, resulting in
misalignment with human preference. To address this, we propose Decomposed GPT
Score (D-GPTScore), a novel human-aligned evaluation method that decomposes
evaluation criteria into finer aspects and incorporates aspect-wise assessments
using Multimodal Large Language Model (MLLM). Additionally, we release Human
Preference-Aligned Concept Customization Benchmark (CC-AlignBench), a benchmark
dataset containing both single- and multi-concept tasks, enabling stage-wise
evaluation across a wide difficulty range -- from individual actions to
multi-person interactions. Our method significantly outperforms existing
approaches on this benchmark, exhibiting higher correlation with human
preferences. This work establishes a new standard for evaluating concept
customization and highlights key challenges for future research. The benchmark
and associated materials are available at
https://github.com/ReinaIshikawa/D-GPTScore.

</details>


### [48] [Scalable and Loosely-Coupled Multimodal Deep Learning for Breast Cancer Subtyping](https://arxiv.org/abs/2509.03408)
*Mohammed Amer,Mohamed A. Suliman,Tu Bui,Nuria Garcia,Serban Georgescu*

Main category: cs.CV

TL;DR: 提出一个松耦合、可扩展的多模态框架，使用图像+图谱的双重WSI表示与CNV和临床数据融合，通过新的融合策略在乳腺癌分型上超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床环境中可用模态因地点和患者而异，乳腺癌分型是可从多模态信息中获益的重要任务，因此需要一个可扩展且鲁棒的多模态融合方法。

Method: 提出一个可扩展的松耦合多模态框架，融合CNV、临床记录和双基表示的病理整片图像（图像+图谱），并设计新的多模态融合策略，实现无需重新训练即可添加或移除模态。

Result: 将双重WSI表示与CNV和临床记录结合，并使用提出的融合策略，在乳腺癌分型任务上超越现有最先进方法，实验显示性能显著提升。

Conclusion: 该框架通过松耦合、多模态融合和双重WSI表示显著提高了乳腺癌分子分型性能，具有良好的可扩展性和跨模态适应性。

Abstract: Healthcare applications are inherently multimodal, benefiting greatly from
the integration of diverse data sources. However, the modalities available in
clinical settings can vary across different locations and patients. A key area
that stands to gain from multimodal integration is breast cancer molecular
subtyping, an important clinical task that can facilitate personalized
treatment and improve patient prognosis. In this work, we propose a scalable
and loosely-coupled multimodal framework that seamlessly integrates data from
various modalities, including copy number variation (CNV), clinical records,
and histopathology images, to enhance breast cancer subtyping. While our
primary focus is on breast cancer, our framework is designed to easily
accommodate additional modalities, offering the flexibility to scale up or down
with minimal overhead without requiring re-training of existing modalities,
making it applicable to other types of cancers as well. We introduce a
dual-based representation for whole slide images (WSIs), combining traditional
image-based and graph-based WSI representations. This novel dual approach
results in significant performance improvements. Moreover, we present a new
multimodal fusion strategy, demonstrating its ability to enhance performance
across a range of multimodal conditions. Our comprehensive results show that
integrating our dual-based WSI representation with CNV and clinical health
records, along with our pipeline and fusion strategy, outperforms
state-of-the-art methods in breast cancer subtyping.

</details>


### [49] [Time-Scaling State-Space Models for Dense Video Captioning](https://arxiv.org/abs/2509.03426)
*AJ Piergiovanni,Ganesh Satish Mallya,Dahun Kim,Anelia Angelova*

Main category: cs.CV

TL;DR: 将SSM时间尺度扩展并引入Transfer State以保持长期状态，实现在长视频上的高效、在线密集视频描述，计算开销显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有密集视频描述方法在处理长视频时受限于计算复杂度和内存，且需全局视频作为输入，无法在线处理；因此需要一种能扩展到更长序列、支持流式处理且计算高效的模型。

Method: 在标准SSM基础上设计Transfer State机制，使模型既保留SSM的长序列建模能力又具备递归（持续状态传递）特性，从而维持长期上下文信息。通过时间尺度扩展和状态传递，减少计算量和内存占用，支持逐帧或分段在线生成字幕。

Result: 在密集视频描述任务上，提出的方法在视频长度扩展性上表现良好，并实现了约7倍的FLOPs减少，同时支持即刻生成字幕的在线处理。

Conclusion: 该论文提出了一种基于状态空间模型(SSM)的时间尺度扩展方法，称为Transfer State SSM，用于密集视频描述任务，能够在更长的视频序列上保持状态并实现在线流式生成。

Abstract: Dense video captioning is a challenging video understanding task which aims
to simultaneously segment the video into a sequence of meaningful consecutive
events and to generate detailed captions to accurately describe each event.
Existing methods often encounter difficulties when working with the long videos
associated with dense video captioning, due to the computational complexity and
memory limitations. Furthermore, traditional approaches require the entire
video as input, in order to produce an answer, which precludes online
processing of the video. We address these challenges by time-scaling
State-Space Models (SSMs) to even longer sequences than before. Our approach,
State-Space Models with Transfer State, combines both the long-sequence and
recurrent properties of SSMs and addresses the main limitation of SSMs which
are otherwise not able to sustain their state for very long contexts,
effectively scaling SSMs further in time. The proposed model is particularly
suitable for generating captions on-the-fly, in an online or streaming manner,
without having to wait for the full video to be processed, which is more
beneficial in practice. When applied to dense video captioning, our approach
scales well with video lengths and uses 7x fewer FLOPs.

</details>


### [50] [Decoding Visual Neural Representations by Multimodal with Dynamic Balancing](https://arxiv.org/abs/2509.03433)
*Kaili sun,Xingyu Miao,Bing Zhai,Haoran Duan,Yang Long*

Main category: cs.CV

TL;DR: 提出将EEG、图像和文本整合的多模态框架，通过文本作为语义锚点来增强EEG与视觉内容的对应，使用adapter模块对齐高维预训练表示，采用MCDB动态平衡模态贡献，并引入SPR随机扰动正则化以提升泛化；在ThingsEEG上Top-1/Top-5分别提升约2.0%和4.7%。


<details>
  <summary>Details</summary>
Motivation: EEG信号信噪比低、与视觉语义对齐困难，直接用图像-EEG对齐受限，加入文本显式语义有助于桥接低层EEG与高层语义之间的语义鸿沟。

Method: 构建三模态共享语义空间，利用文本文本表示作为对齐锚点；设计adapter模块稳定地对齐并融合预训练的视觉/文本特征；提出MCDB动态调节模态权重以缓解文本占主导问题；引入SPR在模态优化中加高斯噪声以增强泛化。

Result: 在ThingsEEG基准上，方法在Top-1准确率提高约2.0%，Top-5提高约4.7%，超越之前的方法。

Conclusion: 引入文本作为显式语义标签并结合adapter、MCDB与SPR，可以显著提升低信噪比EEG解码视觉语义的性能，优于现有SOTA。

Abstract: In this work, we propose an innovative framework that integrates EEG, image,
and text data, aiming to decode visual neural representations from low
signal-to-noise ratio EEG signals. Specifically, we introduce text modality to
enhance the semantic correspondence between EEG signals and visual content.
With the explicit semantic labels provided by text, image and EEG features of
the same category can be more closely aligned with the corresponding text
representations in a shared multimodal space. To fully utilize pre-trained
visual and textual representations, we propose an adapter module that
alleviates the instability of high-dimensional representation while
facilitating the alignment and fusion of cross-modal features. Additionally, to
alleviate the imbalance in multimodal feature contributions introduced by the
textual representations, we propose a Modal Consistency Dynamic Balance (MCDB)
strategy that dynamically adjusts the contribution weights of each modality. We
further propose a stochastic perturbation regularization (SPR) term to enhance
the generalization ability of semantic perturbation-based models by introducing
dynamic Gaussian noise in the modality optimization process. The evaluation
results on the ThingsEEG dataset show that our method surpasses previous
state-of-the-art methods in both Top-1 and Top-5 accuracy metrics, improving by
2.0\% and 4.7\% respectively.

</details>


### [51] [Joint Training of Image Generator and Detector for Road Defect Detection](https://arxiv.org/abs/2509.03465)
*Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: 提出JTGD：联合训练图像生成器和检测器，采用双判别器与CLIP-FID损失，生成更难、更逼真的样本做数据增强，在无集成/无TTA条件下在RDD2022上表现最佳且参数量显著更小，适合边缘部署。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上进行实时、轻量且高效的道路缺陷检测，避免使用集成方法和测试时增强以降低计算和存储开销，同时提升检测鲁棒性和准确性。

Method: 设计一个联合训练框架JTGD：包含一个生成器、两个判别器（用于补丁级与图像级判别）和一个检测器；引入基于CLIP特征的Fréchet Inception Distance作为生成质量度量；在训练过程中生成器生成更难样本以增强检测器，整体端到端联合优化，无需集成或测试时增强。

Result: 提出JTGD框架，联合训练生成器与检测器，使用双判别器保证生成补丁和整体图像逼真；引入基于CLIP的Fréchet Inception Distance损失提升生成质量；生成器与检测器的联合训练使生成更难的样本用于数据增强，从而在RDD2022基准上超越现有方法，同时模型参数少于竞争基线的20%。

Conclusion: JTGD通过联合训练生成器与检测器并结合双判别器和CLIP-FID损失，能生成高质量且更具挑战性的合成样本作为增强，从而在保持模型轻量的同时提升道路缺陷检测性能，适合在资源受限的边缘设备上部署。

Abstract: Road defect detection is important for road authorities to reduce the vehicle
damage caused by road defects. Considering the practical scenarios where the
defect detectors are typically deployed on edge devices with limited memory and
computational resource, we aim at performing road defect detection without
using ensemble-based methods or test-time augmentation (TTA). To this end, we
propose to Jointly Train the image Generator and Detector for road defect
detection (dubbed as JTGD). We design the dual discriminators for the
generative model to enforce both the synthesized defect patches and overall
images to look plausible. The synthesized image quality is improved by our
proposed CLIP-based Fr\'echet Inception Distance loss. The generative model in
JTGD is trained jointly with the detector to encourage the generative model to
synthesize harder examples for the detector. Since harder synthesized images of
better quality caused by the aforesaid design are used in the data
augmentation, JTGD outperforms the state-of-the-art method in the RDD2022 road
defect detection benchmark across various countries under the condition of no
ensemble and TTA. JTGD only uses less than 20% of the number of parameters
compared with the competing baseline, which makes it more suitable for
deployment on edge devices in practice.

</details>


### [52] [Parameter-Efficient Adaptation of mPLUG-Owl2 via Pixel-Level Visual Prompts for NR-IQA](https://arxiv.org/abs/2509.03494)
*Yahya Benmahane,Mohammed El Hassouni*

Main category: cs.CV

TL;DR: 提出用像素空间可视提示在冻结的mPLUG-Owl2上做NR-IQA，仅训练≤600K参数，推理时将提示加到图像并使用文本查询，实验证明在多数据集上性能与全微调/专用模型相近，首次将像素级视觉提示用于NR-IQA。


<details>
  <summary>Details</summary>
Motivation: 当前对MLLM的适配通常依赖于大范围微调，成本高且参数多。为实现对低层次视觉任务（如NR-IQA）的高效适配，作者尝试用参数极少的像素级视觉提示来替代全量微调，从而降低计算与存储开销，同时利用强大的预训练多模态模型能力。

Method: 在像素空间生成可训练的视觉提示向量（parameter-efficient visual prompts），将其直接加到输入图像上并输入到冻结的mPLUG-Owl2模型，同时使用文本提示"Rate the technical quality of the image."，仅微调提示参数（≤600K），通过在不同失真类型和数据集上训练/评估来验证性能。

Result: 在KADID-10k、KonIQ-10k、AGIQA-3k上对合成失真、现实失真和AI生成失真评测，方法在KADID-10k上达到0.93 SRCC，与全量微调方法和专门的NR-IQA模型相比表现具有竞争力，证明了极少参数的视觉提示在NR-IQA任务上的有效性。

Conclusion: 论文提出了一种在像素空间优化可视提示（visual prompts）的参数高效自适应方法，用于无参考图像质量评估（NR-IQA），在保持基础多模态大模型（mPLUG-Owl2）冻结的情况下，仅训练最多60万个参数（<0.01%），推理时将提示与图像相加并结合文本查询进行评估。该方法在多个数据集（KADID-10k、KonIQ-10k、AGIQA-3k）上对合成、真实、AI生成失真均取得有竞争力的结果，KADID-10k上达0.93 SRCC，首次将像素空间可视提示应用于NR-IQA，证明了MLLM在低层视觉任务上的高效适配潜力。

Abstract: In this paper, we propose a novel parameter-efficient adaptation method for
No- Reference Image Quality Assessment (NR-IQA) using visual prompts optimized
in pixel-space. Unlike full fine-tuning of Multimodal Large Language Models
(MLLMs), our approach trains only 600K parameters at most (< 0.01% of the base
model), while keeping the underlying model fully frozen. During inference,
these visual prompts are combined with images via addition and processed by
mPLUG-Owl2 with the textual query "Rate the technical quality of the image."
Evaluations across distortion types (synthetic, realistic, AI-generated) on
KADID- 10k, KonIQ-10k, and AGIQA-3k demonstrate competitive performance against
full finetuned methods and specialized NR-IQA models, achieving 0.93 SRCC on
KADID-10k. To our knowledge, this is the first work to leverage pixel-space
visual prompts for NR-IQA, enabling efficient MLLM adaptation for low-level
vision tasks. The source code is publicly available at https: // github. com/
yahya-ben/ mplug2-vp-for-nriqa .

</details>


### [53] [OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and Generation](https://arxiv.org/abs/2509.03498)
*Han Li,Xinyu Peng,Yaoming Wang,Zelin Peng,Xin Chen,Rongxiang Weng,Jingang Wang,Xunliang Cai,Wenrui Dai,Hongkai Xiong*

Main category: cs.CV

TL;DR: OneCAT用纯解码器自回归Transformer结合模态专用MoE和多尺度视觉自回归，去掉了外部视觉编码器，提升高分辨率效率并在生成/编辑/理解任务上超越开源对手。


<details>
  <summary>Details</summary>
Motivation: 减少对外部视觉组件依赖、提高高分辨率输入推理效率，并探索纯自回归方法能否成为统一多模态理解、生成与编辑的足够而优雅的基础。

Method: 采用纯解码器（decoder-only）Transformer，使用模态专用的Mixture-of-Experts（MoE）结构并以单一自回归（AR）目标训练；引入多尺度视觉自回归机制以减少解码步数；在推理时不依赖ViT或视觉tokenizer，支持动态分辨率。

Result: 在多模态生成、编辑与理解的基准测试中，OneCAT超过了现有开源统一多模态模型，达到了更高效的推理和更好的任务性能，同时显著减少了解码步骤和推理资源开销。

Conclusion: OneCAT证明了纯解码器自回归架构在统一多模态智能上的可行性与高效性。通过在模型内部整合模态专用的MoE和多尺度视觉自回归机制，OneCAT在无需外部视觉编码器或视觉标记器的情况下，实现了高分辨率输入的高效推理，并在生成、编辑与理解任务上优于现有开源统一多模态模型。

Abstract: We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.

</details>


### [54] [DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea video](https://arxiv.org/abs/2509.03499)
*Kevin Barnard,Elaine Liu,Kristine Walz,Brian Schlining,Nancy Jacobsen Stout,Lonny Lundsten*

Main category: cs.CV

TL;DR: 作者发布了首个公开深海视频多目标跟踪基准，使用HOTA评估多检测模型与跟踪器表现，并提供数据与可复现工具。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对深海视频的公开多目标跟踪基准，限制了对检测器与跟踪器在深海环境中性能评估与比较；因此需要构建专用基准并提供可复现流程。

Method: 收集并标注四段代表中层与底栖深海生境的视频序列；采用多种MBARI与FathomNet单类目标检测模型进行检测；使用若干跟踪器与Higher Order Tracking Accuracy(HOTA)指标评估检测、定位与关联性能；提供数据集、生成流程与Python示例Notebook。

Result: 构建并发布了包含四段视频的深海多目标跟踪基准，展示了不同检测器与跟踪器在HOTA指标下的性能差异；同时发布了数据、文档与示例代码以便社区使用和扩展。

Conclusion: 该论文构建了首个公开的深海视频多目标跟踪基准数据集，并提供完整流程与代码，便于评估与比较检测器与跟踪器在深海场景的表现。

Abstract: Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.

</details>


### [55] [Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data](https://arxiv.org/abs/2509.03501)
*Honglu Zhou,Xiangyu Peng,Shrikant Kendre,Michael S. Ryoo,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CV

TL;DR: Strefer auto-generates temporally dense, fine-grained synthetic instructions and pseudo-annotations (subjects, objects, masklets, actions, timelines) to teach Video LLMs spatiotemporal referring and reasoning, improving performance on related tasks.


<details>
  <summary>Details</summary>
Motivation: Video LLMs lack fine-grained spatiotemporal referring and reasoning needed for real-world AI companions.

Method: We propose Strefer, a synthetic instruction generation framework for Video LLMs

Result: Training with Strefer data improves spatial and temporal disambiguation and space-time-aware reasoning without proprietary models or heavy human annotation.

Conclusion: Strefer establishes a foundation for perceptually grounded, instruction-tuned Video LLMs that handle spatial and temporal references effectively.

Abstract: Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.

</details>


### [56] [A comprehensive Persian offline handwritten database for investigating the effects of heritability and family relationships on handwriting](https://arxiv.org/abs/2509.03510)
*Abbas Zohrevand,Javad Sadri,Zahra Imani*

Main category: cs.CV

TL;DR: 该论文构建了一个包含210个家庭、涵盖多代亲属（祖父母、父母、叔姑、兄弟姐妹、表亲等）手写样本的数据库，用于研究遗传与家族关系对书写的影响。收集了数字、字母、形状和段落等多种手写成分，记录了完整家庭关系。作者通过比较手写特征与风格发现了家庭成员间的相似性，并将数据库公开供模式识别社区使用。


<details>
  <summary>Details</summary>
Motivation: 探究手写是否具有遗传成分及家族关系是否影响书写风格，但此前缺乏包含多代家庭完整关系与多种手写样本的公开数据库。

Method: 设计专用采集表格，收集210个家庭中不同成员的多种手写样本（数字、字母、形状、自由段落），记录详细的家庭关系；提取手写特征并进行比较分析以检测相似性。

Result: 构建并公开了一个包含210个家庭手写样本的数据库，初步比较分析发现家庭内部在手写特征和风格上存在若干相似性，数据库为进一步研究提供基础数据。

Conclusion: 该数据库填补了研究手写遗传性与家族影响的资源空白，初步分析显示家庭成员在手写特征和风格上存在相似性，数据库可支持进一步的遗传学、法医和模式识别研究。

Abstract: This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.

</details>


### [57] [Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage, but Not Direct the Play?](https://arxiv.org/abs/2509.03516)
*Ouxiang Li,Yuan Wang,Xinting Hu,Huijuan Huang,Rui Chen,Jiarong Ou,Xin Tao,Pengfei Wan,Fuli Feng*

Main category: cs.CV

TL;DR: 提出T2I-CoReBench：一个12维、1080条提示、含13500个检查问题的复杂基准，揭示当前27个T2I模型在构图和尤其推理能力上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在推理能力上有进展，但现有基准不能全面、精细地评估构图与推理能力，且提示复杂度不足，难以反映模型在高密度场景和多步推理上的能力。

Method: 作者将构图按场景图元素（实例、属性、关系）划分，将推理按推理哲学框架（演绎、归纳、溯因）划分，构成12维评估分类；同时为提高复杂度，每条提示语设计高构图密度和多步推理，并为每个提示配备是/否检查表用于细粒度评估；数据集包含1080条提示和约13500个检查表问题；在27个模型上进行了广泛实验。

Result: 实验结果表明：在高密度复杂场景下，模型的构图能力仍有限；推理能力更差，所有模型都难以从提示中推断隐含元素，推理是关键瓶颈。

Conclusion: 该论文提出了一个面向文本到图像生成模型的综合基准T2I-CoReBench，用以评估模型在构图和推理两大能力上的表现。

Abstract: Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving](https://arxiv.org/abs/2509.02718)
*Fangzhou Wu,Sandeep Silwal*

Main category: cs.DB

TL;DR: 提出一种训练-free的在线LLM路由算法，基于近似最近邻和一次性初始优化，理论与实证均显示显著的性能、成本和吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法多为离线训练，难以适应在线高吞吐和严格预算限制的场景，需要一个低开销且能快速部署的路由方案。

Method: 使用近似最近邻搜索估计查询特征，对一小批初始查询进行一次性优化以学习路由策略，然后将该策略应用于后续在线查询；算法无需训练并兼顾效率与精度。

Result: 在3个基准数据集和8个对比基线的实验中，平均整体性能提升3.55×，成本效率提升1.85×，吞吐量接近4.25×；理论上算法在自然假设下达到1-o(1)竞争比。

Conclusion: 本文提出了首个无训练的在线路由算法，能在高查询量和有限token预算场景下高效路由并具有理论竞争比保证。

Abstract: Increasing demand for Large Language Models (LLMs) services imposes
substantial deployment and computation costs on providers. LLM routing offers a
cost-efficient solution by directing queries to the optimal LLM based on model
and query features. However, existing works primarily focus on offline
scenarios and struggle to adapt to online settings with high query volume and
constrained token budgets. In this work, we introduce the first training-free
algorithm for online routing scenarios. Our algorithm leverages approximate
nearest neighbor search to efficiently estimate query features and performs a
one-time optimization over a small set of initial queries to learn a routing
strategy that guides future routing. We provide theoretical guarantees
demonstrating that our algorithm achieves a competitive ratio of $1 - o(1)$
under natural assumptions, which is further validated by extensive experiments
across 3 benchmark datasets and 8 baselines, showing an average improvement of
3.55$\times$ in overall performance, 1.85$\times$ in cost efficiency, and
nearly 4.25$\times$ in throughput.

</details>


### [59] [Cut Costs, Not Accuracy: LLM-Powered Data Processing with Guarantees](https://arxiv.org/abs/2509.02896)
*Sepanta Zeighami,Shreya Shankar,Aditya Parameswaran*

Main category: cs.DB

TL;DR: BARGAIN通过基于数据和任务的自适应抽样与精确统计估计，智能调配便宜与昂贵LLM，显著降低成本并保证输出质量（准确率/精确率/召回率）。平均节省成本可达86%。


<details>
  <summary>Details</summary>
Motivation: 在大规模文本处理场景下，顶级LLM昂贵但准确，轻量LLM便宜但质量较低。需要在成本和质量之间取得平衡，同时提供理论保证。

Method: 设计自适应抽样策略与统计估计程序，利用LLM置信度（如对数概率）并结合任务与数据特性，使用现代统计工具对低成本LLM输出质量进行紧致估计，从而决定对哪些记录调用高质量LLM；支持对准确率、精确率或召回率的保证。

Result: 提出BARGAIN方法，通过自适应抽样和统计估计来判断何时使用便宜LLM，从而在保持精度/召回/准确率保证的同时大幅降低成本。实验显示在8个真实数据集上相比最先进方法平均最多节省86%的成本，并提供更强理论保证。

Conclusion: BARGAIN能在保证所需质量指标下显著降低处理成本，比现有方法更有效且具更强理论保证，可推广到不同质量指标（准确率/精确/召回）。

Abstract: Large Language Models (LLMs) are being increasingly used as a building block
in data systems to process large text datasets. To do so, LLM model providers
offer multiple LLMs with different sizes, spanning various cost-quality
trade-offs when processing text at scale. Top-of-the-line LLMs (e.g., GPT-4o,
Claude Sonnet) operate with high accuracy but are prohibitively expensive when
processing many records. To avoid high costs, more affordable but lower quality
LLMs (e.g., GPT-4o-mini, Claude Haiku) can be used to process records, but we
need to ensure that the overall accuracy does not deviate substantially from
that of the top-of-the-line LLMs. The model cascade framework provides a
blueprint to manage this trade-off, by using the confidence of LLMs in their
output (e.g., log-probabilities) to decide on which records to use the
affordable LLM. However, existing solutions following this framework provide
only marginal cost savings and weak theoretical guarantees because of poor
estimation of the quality of the affordable LLM's outputs. We present BARGAIN,
a method that judiciously uses affordable LLMs in data processing to
significantly reduce cost while providing strong theoretical guarantees on the
solution quality. BARGAIN employs a novel adaptive sampling strategy and
statistical estimation procedure that uses data and task characteristics and
builds on recent statistical tools to make accurate estimations with tight
theoretical guarantees. Variants of BARGAIN can support guarantees on accuracy,
precision, or recall of the output. Experimental results across 8 real-world
datasets show that BARGAIN reduces cost, on average, by up to 86% more than
state-of-the-art, while providing stronger theoretical guarantees on accuracy
of output, with similar gains when guaranteeing a desired level of precision or
recall.

</details>


### [60] [CARPO: Leveraging Listwise Learning-to-Rank for Context-Aware Query Plan Optimization](https://arxiv.org/abs/2509.03102)
*Wenrui Zhou,Qiyu Liu,Jingshu Peng,Aoqian Zhang,Lei Chen*

Main category: cs.DB

TL;DR: CARPO用Transformer和列表式排序对候选计划集做整体评估，并结合OOD检测与top-k回退，显著提升查询优化器效果，TPC-H上Top-1率74.54%、执行时间大幅降低。


<details>
  <summary>Details</summary>
Motivation: 传统代价模型与启发式规则常导致生成次优执行计划，现有学习型优化器多基于成对排序存在不一致性与局部次优问题，需一种能全局评估候选集并提高稳健性的方案。

Method: 采用Transformer对候选执行计划集合进行整体编码和评分，结合列表式损失进行训练；在推理阶段加入OOD检测，异常时触发top-k回退策略以保证决策可靠性；框架兼容现有计划嵌入方法。

Result: 在TPC-H与STATS基准上，CARPO较原生PostgreSQL与Lero显著提升：TPC-H Top-1 Rate达74.54%（Lero为3.63%），总执行时间从PostgreSQL的22577.87 ms降至3719.16 ms。

Conclusion: CARPO提出了一种基于列表学习排序（listwise learning-to-rank）和Transformer模型的查询优化框架，通过对候选计划集合的整体评估与混合决策（包含OOD检测与top-k回退策略）提高了查询计划选择的鲁棒性与性能。

Abstract: Efficient data processing is increasingly vital, with query optimizers
playing a fundamental role in translating SQL queries into optimal execution
plans. Traditional cost-based optimizers, however, often generate suboptimal
plans due to flawed heuristics and inaccurate cost models, leading to the
emergence of Learned Query Optimizers (LQOs). To address challenges in existing
LQOs, such as the inconsistency and suboptimality inherent in pairwise ranking
methods, we introduce CARPO, a generic framework leveraging listwise
learning-to-rank for context-aware query plan optimization. CARPO distinctively
employs a Transformer-based model for holistic evaluation of candidate plan
sets and integrates a robust hybrid decision mechanism, featuring
Out-Of-Distribution (OOD) detection with a top-$k$ fallback strategy to ensure
reliability. Furthermore, CARPO can be seamlessly integrated with existing plan
embedding techniques, demonstrating strong adaptability. Comprehensive
experiments on TPC-H and STATS benchmarks demonstrate that CARPO significantly
outperforms both native PostgreSQL and Lero, achieving a Top-1 Rate of
\textbf{74.54\%} on the TPC-H benchmark compared to Lero's 3.63\%, and reducing
the total execution time to 3719.16 ms compared to PostgreSQL's 22577.87 ms.

</details>


### [61] [Adaptive KV-Cache Compression without Manually Setting Budget](https://arxiv.org/abs/2509.03136)
*Chenxia Tang,Jianchun Liu,Hongli Xu,Liusheng Huang*

Main category: cs.DB

TL;DR: GVote通过对未来查询采样并聚合重要keys，自适应确定KV-cache压缩预算，减少约2x内存占用并维持或提高推理精度。


<details>
  <summary>Details</summary>
Motivation: 传统KV-cache压缩固定压缩比无法适应多样化工作负载，造成资源分配和推理性能次优；因此需要一种自适应、按需分配缓存预算的方法。

Method: 基于蒙特卡罗风格的潜在查询采样，预测未来查询的注意力需求；对被选中的keys进行聚合，以确定最佳缓存预算并进行压缩，系统无需手动预算设定。

Result: 在GSM8K、RULER和Longbench等基准上，GVote相比基线在保持相同或更好准确率的同时实现约2倍的内存减少。

Conclusion: GVote提出了一种自适应KV-cache压缩方案，通过模拟未来查询并聚合关键键来动态决定缓存预算，从而避免手动设定压缩比，提升内存效率且保持或提升精度。

Abstract: Large language models (LLMs) inference relies heavily on KV-caches to
accelerate autoregressive decoding, but the resulting memory footprint grows
rapidly with sequence length, posing significant efficiency challenges. Current
KV-cache compression methods suffer from a Procrustes' bed problem: they force
diverse workloads into fixed compression ratios, leading to suboptimal resource
allocation and inference performance. To this end, we present GVote, an
adaptive KV-cache compression scheme that eliminates manual budget
specification while achieving superior accuracy-efficiency trade-offs. GVote
operates on the principle that the important keys are the aggregation of keys
required by future queries. The method predicts future query attention demands
by Monte-Carlo style sampling potential queries and aggregating selected keys
to determine the optimal cache budget without manual specification.
Experimental evaluation demonstrates GVote's effectiveness across multiple
benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote
exhibits 2$\times$ memory reduction while the accuracy maintains higher or
comparable.

</details>


### [62] [BAMG: A Block-Aware Monotonic Graph Index for Disk-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2509.03226)
*Huiling Li,Jianliang Xu*

Main category: cs.DB

TL;DR: 引入块感知的单调近邻图（BMRNG）及其实用变体BAMG，通过联合考虑几何邻接与存储布局并采用块优先搜索，显著降低磁盘I/O并提高ANNS吞吐，同时保持高召回。


<details>
  <summary>Details</summary>
Motivation: 在大规模磁盘存储的ANNS中，磁盘I/O是性能瓶颈，但现有图索引方法要么只优化存储布局，要么只构建图而忽略两者交互，导致磁盘读取效率低。

Method: 提出BMRNG理论结构并证明I/O单调路径存在；构造线性时间可扩展变体BAMG，从单调图出发进行块感知边剪枝；解耦存储（向量与图分离）；设计多层导航图和优先块内遍历的block-first搜索算法。

Result: 在真实数据集上，BAMG比最先进方法吞吐提升至多2.1x，I/O读取减少最多52%，召回率相当。

Conclusion: 本文提出了BMRNG/BAMG，将几何距离与存储布局联合考虑，从而保证存在I/O单调的搜索路径，并通过块感知的边剪枝与解耦存储提高块利用率，显著减少磁盘I/O并提升吞吐。

Abstract: Approximate Nearest Neighbor Search (ANNS) over high-dimensional vectors is a
foundational problem in databases, where disk I/O often emerges as the dominant
performance bottleneck at scale. Existing graph indexing solutions for
disk-based ANNS typically either optimize the storage layout for a given graph
or construct the graph independently of the storage layout, thus overlooking
their interaction. In this paper, we propose the Block-aware Monotonic Relative
Neighborhood Graph (BMRNG), a novel graph structure that jointly considers both
geometric distance and storage layout for edge selection, theoretically
guaranteeing the existence of I/O monotonic search paths. To address the
scalability challenge of BMRNG construction, we further develop a practical and
efficient variant, the Block-Aware Monotonic Graph (BAMG), which can be
constructed in linear time from a monotonic graph considering the storage
layout. BAMG integrates block-aware edge pruning with a decoupled storage
design that separates raw vectors from the graph index, thereby maximizing
block utilization and minimizing redundant disk reads. Additionally, we design
a multi-layer navigation graph for adaptive and efficient query entry, along
with a block-first search algorithm that prioritizes intra-block traversal to
fully exploit each disk I/O operation. Extensive experiments on real-world
datasets demonstrate that BAMG achieves up to 2.1x higher throughput and
reduces I/O reads by up to 52% compared to state-of-the-art methods, while
maintaining comparable recall.

</details>


### [63] [NeurStore: Efficient In-database Deep Learning Model Management System](https://arxiv.org/abs/2509.03228)
*Siqi Xiang,Sheng Wang,Xiaokui Xiao,Cong Yue,Zhanhao Zhao,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurStore通过张量级去重与增量量化并配合压缩感知加载，在数据库内实现高效的深度学习模型存储与利用，显著降低存储开销同时保持可接受的加载性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据库通常将深度学习模型作为整体文件存储或采用忽视模型结构特性的压缩方法，导致模型存储开销大且低效。随着模型数量和规模增长，需要面向数据库的更细粒度和结构感知的模型管理方案。

Method: 提出张量化存储引擎：使用增强的HNSW索引张量并对相似张量存储增量（delta）以实现张量级去重；设计增量量化算法对delta张量做有效压缩，以在可控精度损失下提高压缩比；实现压缩感知的模型加载，可以直接在压缩张量上进行计算以提升利用性能。

Result: 实验表明NeurStore在压缩比方面优于现有最先进方法，并在模型加载吞吐上具有竞争力，证明其在存储效率和利用性能上的有效性。

Conclusion: NeurStore通过张量级存储引擎、基于相似性阈值的张量去重、增量量化压缩以及压缩感知加载机制，实现了在数据库内高效管理深度学习模型，达到更高的压缩比并在加载性能上与现有方法竞争。

Abstract: With the prevalence of in-database AI-powered analytics, there is an
increasing demand for database systems to efficiently manage the ever-expanding
number and size of deep learning models. However, existing database systems
typically store entire models as monolithic files or apply compression
techniques that overlook the structural characteristics of deep learning
models, resulting in suboptimal model storage overhead. This paper presents
NeurStore, a novel in-database model management system that enables efficient
storage and utilization of deep learning models. First, NeurStore employs a
tensor-based model storage engine to enable fine-grained model storage within
databases. In particular, we enhance the hierarchical navigable small world
(HNSW) graph to index tensors, and only store additional deltas for tensors
within a predefined similarity threshold to ensure tensor-level deduplication.
Second, we propose a delta quantization algorithm that effectively compresses
delta tensors, thus achieving a superior compression ratio with controllable
model accuracy loss. Finally, we devise a compression-aware model loading
mechanism, which improves model utilization performance by enabling direct
computation on compressed tensors. Experimental evaluations demonstrate that
NeurStore achieves superior compression ratios and competitive model loading
throughput compared to state-of-the-art approaches.

</details>
