<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.DB](#cs.DB) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: 通过神经符号自监督训练的SGClip生成可提示的场景图，并嵌入ESCA框架，弥合像素到语义的结构化差距，从而显著提升多模态大模型在embodied任务中的感知与控制表现。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM训练依赖高层次视觉-声音-文本配对，缺乏像素级与文本语义之间的结构化对齐，导致智能体在复杂环境下感知与执行能力不足。

Method: 提出SGClip——一种基于CLIP、可提示、开域的场景图生成模型；通过神经符号学习在87K+开域视频和视频-字幕自监督下训练，无需人工场景图标注；将其集成到ESCA框架以提升多模态LLM的时空理解与动作定位能力。

Result: SGClip在场景图生成和动作定位基准上表现优异；ESCA在两种embodied环境中使开源与商用MLLMs性能均有稳定提升，显著降低感知错误，并使开源模型超越专有基线。

Conclusion: ESCA通过引入SGClip为MLLMs提供了像素级到语义级的结构化时空对齐，有效改善了智能体的感知与决策性能。

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 通过射线感知与类别平衡监督改进token质量，并用Ray PE对齐模态位置编码，提出CrossRay3D，在nuScenes上兼顾精度、速度和鲁棒性，优于现有稀疏多模态检测器。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏跨模态检测器忽视token表示质量，导致前景表示较差和性能受限；改进几何结构保留与类别分布建模可提升小目标表示与整体检测性能。

Method: 提出核心模块Ray-Aware Supervision (RAS)保留训练阶段的几何信息，Class-Balanced Supervision按类别重加权以保留小目标相关token；引入Ray Positional Encoding用于模态间位置编码对齐；将这些模块整合进端到端稀疏多模态检测器CrossRay3D。

Result: 在nuScenes上，CrossRay3D实现72.4 mAP和74.7 NDS，速度比其他领先方法快1.84倍，并在单一或缺失传感器情况下表现出强鲁棒性。

Conclusion: 本文提出的Sparse Selector（SS）通过射线感知监督和类别平衡监督提升稀疏跨模态检测器的token表示质量，并设计了Ray Positional Encoding解决LiDAR与图像分布差异，集成入CrossRay3D，在nuScenes上取得领先性能与速度，同时对部分或全部缺失传感器具备鲁棒性。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 用YOLO检测多类路面缺陷并结合VLM生成JSON格式的结构化维修计划，实现从检测到可执行维护指令的闭环原型。


<details>
  <summary>Details</summary>
Motivation: 城市基础设施缺陷（裂缝、坑洞、泄漏）威胁公共安全，人工巡检成本高且危险；现有自动化系统通常只针对单一缺陷或输出非结构化信息，难以直接用于维修行动。

Method: 使用YOLO系列模型对路面裂缝、坑洞和泄漏等多种缺陷进行检测与分割，随后将检测结果输入视觉语言模型（VLM，如QwenVL或LLaVA）生成场景感知的结构化摘要与维修建议。系统在公开数据集和实际CCTV片段上进行了评估。

Result: 在公开数据集和采集的CCTV片段上，系统能够准确识别多种缺陷并生成连贯的维修摘要。作者还讨论了将系统扩展到城市级部署的挑战。

Conclusion: 该论文提出了一个利用街道CCTV视频流进行多缺陷检测与结构化报告的端到端系统，结合YOLO家族目标检测器与视觉语言模型，能够生成可直接指导维护的JSON格式行动计划。

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: 提出IAD-GPT：用LLM生成详细异常提示激活视觉-语言模型，并通过文本引导增强与多掩码融合提升像素级感知，在MVTec-AD和VisA上达成最优或接近最优的异常检测与分割效果。


<details>
  <summary>Details</summary>
Motivation: 传统IAD方法缺乏多轮人机对话和细粒度描述能力，且基于大模型的方法未充分发挥大模型在异常检测中的潜力。作者希望结合丰富文本语义与图像的像素/图像级信息，使MLLMs能更好地解释和定位工业图像中的异常。

Method: 使用异常提示生成器（APG）由LLM生成对象级详细异常提示，驱动预训练视觉语言模型（如CLIP）进行检测与分割；提出Text-Guided Enhancer，通过图像特征与正常/异常文本提示交互动态选择增强路径；设计Multi-Mask Fusion将多重掩码作为专家知识融合以加强像素级感知。整体形成IAD-GPT多模态管线，支持自监督与少样本场景。

Result: 在MVTec-AD和VisA数据集上，IAD-GPT在自监督与少样本的异常检测与分割任务上实现了领先性能（论文宣称SOTA），代码已开源。

Conclusion: 本文提出将多模态大模型用于工业异常检测，提出IAD-GPT框架，结合APG、Text-Guided Enhancer和Multi-Mask Fusion，实现多轮对话式、细粒度的异常检测与分割，实验在MVTec-AD和VisA上取得了SOTA性能。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 结构化报告显著提高效率，AI预填充可进一步提升准确性与满意度。


<details>
  <summary>Details</summary>
Motivation: 探索结构化报告与AI辅助结构化报告对放射科医师读片行为、诊断准确性、效率与用户体验的影响。

Method: 前瞻性研究，8名读者（4名新手、4名非新手）在自定义查看器与眼动追踪系统下分别以FT、SR、AI-SR三种模式各分析35张床旁胸片；采用Cohen’s κ评估诊断准确性，记录报告时间与眼动指标，并用混合广义线性模型和Bonferroni校正进行统计。

Result: AI-SR的Cohen’s κ最高（0.71），显著优于FT（0.58）与SR（0.60）；报告时间显著缩短（FT 88s → SR 37s → AI-SR 25s）；眼动指标（视跳次数与注视时长）在SR与AI-SR显著降低；新手在SR下将注视更多转向影像，非新手仍专注影像；用户偏好AI-SR。

Conclusion: SR提高了效率并引导视觉注意力，AI预填充SR进一步提升诊断准确性与用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出IFEF用于系统识别交叉属性偏差，并用BWA按子群分布自适应调节数据增强，有效提升弱势子群性能并减少公平性差距（最高提升24个百分点，公平差距下降35%，p<0.05）。


<details>
  <summary>Details</summary>
Motivation: 针对在带偏数据上训练的模型常出现的交叉属性（如对象类别与环境条件共同作用）偏差，缺乏系统性分析工具与可操作的缓解方法。

Method: 提出Intersectional Fairness Evaluation Framework (IFEF)结合定量公平性指标和可解释性工具识别偏差模式；提出Bias-Weighted Augmentation (BWA)，根据子群分布统计自适应调整增强强度以重平衡训练数据；在Open Images V7上对五个目标类进行实验评估。

Result: 在Open Images V7五类实验中，BWA对弱势类-环境交叉子群的准确率提升最多达24个百分点，公平性指标差异平均下降35%，且多次独立运行的统计检验表明改进具有显著性（p<0.05）。

Conclusion: 该论文提出了一个面向图像分类器的交叉属性偏差分析与缓解框架，并通过基于子群分布的自适应数据增强显著改善了弱势交叉子群的准确率与公平性指标。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 可微分并有收敛证明的2^n对数/移位量化方法，支持权重与激活多比特量化，15个epoch在ImageNet上能接近全精度，推理开销小。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法通常不可微，反向传播中人为设定梯度，且对激活的对数/移位量化要么被忽略要么导致较大精度损失；此外，需要能扩展到多比特的对数量化以提高表达能力和精度。

Method: 提出一种可微分的量化函数，支持log/shift（2^n）形式以及n比特扩展，并给出该方法收敛到最优网络的数学证明；训练中直接优化量化参数（或通过可导近似），避免了手工指定梯度。

Result: 在ImageNet + ResNet-18上，权重量化（shift bit）训练15个epoch后，精度下降<1%；在权重与激活同时量化下也达到与SOTA可比的精度，且推理成本仅比1-bit非对数量化略增，且无需高精度乘法。

Conclusion: 该论文提出了可微分且有收敛证明的对称/移位（logarithmic）量化方法，旨在同时量化权重与激活，扩展到多比特的2^n形式量化，并在ImageNet上的ResNet-18实验中，在15个epoch内实现了接近全精度的精度损失（权重量化下<1%）；与SOTA方法在权重+激活量化下精度可比，推理仅增加较少的CPU指令，且不需要高精度乘法。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet，通过SPM、SRFM和SSEM三模块分别增强形状判别、细长裂缝感受野和小尺度检测，实验证明在RDD2022上达到SOTA且兼顾实时性。


<details>
  <summary>Details</summary>
Motivation: 道路维护对实现SDG11至关重要，而当前损伤检测存在形状多样、细长裂缝难以识别和小尺度目标误检率高等挑战，因此需要一种既能捕捉细长结构又能提升小目标识别的高效检测网络。

Method: 提出了由三部分组成的网络：1) 形状感知模块(SPM)：通过大可分离核注意力(LSKA)进行多尺度特征聚合以增强形状判别；2) 条带感受野模块(SRFM)：使用大条带卷积与池化以捕捉细长裂缝特征；3) 小尺度增强模块(SSEM)：利用高分辨率P2特征图、专用检测头与动态上采样改善小目标检测。

Result: 在RDD2022基准测试中表现优异：在中国子集上比基线分别提高F1-score 4.4个百分点、mAP50 2.9个百分点、mAP50:95 3.4个百分点；在全数据集上取得80.33%的最高F1-score，同时保持有竞争力的推理速度。

Conclusion: 该论文提出了针对道路表面裂缝等损伤的检测难点（形状多样、细长裂缝难以捕捉、小尺度损伤识别误差高）的一种新型神经网络StripRFNet，结论是该方法在RDD2022基准上优于现有方法，在保证实时推理的同时取得了更高的F1、mAP50和mAP50:95，证明了其实用价值。

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms在训练时用颜色扰动和扩散模型数据增强增强鲁棒性，在推理时用目标级扰动并通过评分方差量化不确定性，从而提高检测性能并能区分真假阳性。


<details>
  <summary>Details</summary>
Motivation: 面对数据偏差与分布漂移造成的预测不确定性，提出一种轻量级且针对目标的变换策略以增强模型对光照、颜色变化及样本多样性的鲁棒性，并在推理时实时量化不确定性以提升决策可靠性。

Method: 训练阶段对目标进行颜色空间扰动并使用扩散模型合成多样行人样本；推理阶段对检测到的目标进行扰动并计算检测分数的方差作为不确定性度量，基于不确定性过滤假阳性并恢复假阴性。

Result: 在NuImages 10K数据集与YOLOv8上实验显示，训练阶段提高了所有类别的精度并降低不确定性；推理阶段对假阳性预测出更高不确定性，从而改善精-召曲线。

Conclusion: ObjectTransforms通过对单个目标在训练和推理阶段施加特定变换，有效降低并量化视觉目标检测的不确定性，提高检测精度与鲁棒性。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是使用Aria Gen 2眼镜采集的开放式自我视角多模态数据集，首发包含五类日常活动场景的原始传感器和算法输出，数据公开并提供开源工具。


<details>
  <summary>Details</summary>
Motivation: 为研究者提供高质量、真实的自我视角多模态数据以推动机器感知、AR/VR和人机交互等领域的发展，并通过增量发布确保及时获取。

Method: 使用Aria Gen 2眼镜捕获日常五种场景（清洁、烹饪、用餐、玩耍、户外步行）的原始传感器数据，并提供多种机器感知算法（输出数据）处理结果，伴随开源工具和示例。

Result: 初始发布包含单一主要受试者Dia'ane及其朋友在五类场景中的记录，数据包括原始传感器和算法输出，证明设备在不同用户和条件下的鲁棒感知能力，数据已公开可用并配套工具。

Conclusion: 该论文发布了一个基于Aria Gen 2眼镜的自我视角多模态数据集，并展示了设备在感知佩戴者、环境和交互方面的能力。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 训练免费、基于rectified flow的采样期引导方法，通过部件感知与自相似损失实现鲁棒外观与几何细节转移，并用GPT驱动评估替代传统指标。


<details>
  <summary>Details</summary>
Motivation: 当前外观转移方法在输入与外观对象几何差异大时表现欠佳，直接应用三维生成模型效果不理想。作者受universal guidance启发，寻找一种无需训练、能在采样过程中引入外观与几何提示的通用方案。

Method: 使用预训练的条件rectified flow模型（以图像或文本为条件），在无训练的情况下于采样阶段插入周期性引导。这些引导被建模为可微损失，论文提出两种指导：基于部件感知的外观损失与自相似性损失。方法可与不同扩散模型和指导函数扩展。

Result: 在定性和定量评估中均优于基线；论文指出传统指标在无真实标签且输入差异大的情况下无法衡量局部细节，故提出使用基于GPT的系统对输出进行客观排序，并以用户研究验证评估一致性。

Conclusion: 该论文提出了一种基于预训练rectified flow并受universal guidance启发的训练免费方法，通过在采样过程中周期性加入可微引导（如局部感知的外观损失和自相似损失），实现将图像或文本的外观转移到任意3D资产上，能同时传递纹理与几何细节，优于基线方法。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出回归型自监督方法定位X射线图像骨骼地标，预训练显著提升下游分类，旨在用于C臂自动化以辅助取栓术。


<details>
  <summary>Details</summary>
Motivation: 取栓术过程资源和人员密集，自动化C臂控制可提高效率与安全性。为此提出用深度学习自动识别并定位骨骼地标，以辅助或自动化介入影像设备（C臂）的轨迹规划与控制。

Method: 构建回归型自监督预任务，网络学习预测图像中多个人体骨骼关键点的位置（坐标回归）；在预训练后，用下游分类任务对不同骨骼地标类别进行微调与评估；与现有方法在回归误差和分类准确率上比较。

Result: 在多个数据集/任务上，所提模型在关键点回归误差和骨骼地标分类准确率上均优于基线方法。位置回归作为预训练任务显著提升了下游分类表现。未来工作将扩展至从骨盆到头部的完整C臂自动化轨迹优化。

Conclusion: 本文提出了一种基于自监督学习的框架，通过回归型预任务对血管造影图像中的骨骼标志点进行定位，从而提升下游分类任务（骨骼地标分类）性能。实验证明该方法在回归与分类上均优于现有方法，且位置回归预任务对分类性能有显著提升。

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch用异步双分支（分别优化编码器或解码器）配合去耦dropout、Pair-wise CutMix和一致性匹配，显著改善了半监督脑MRI分割的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像标注稀缺，使得半监督学习重要，但传统教师-学生全网联合优化在困难场景下易造成收敛和稳定性问题，需要更鲁棒的训练机制。

Method: 提出双分支架构，每分支交替只优化编码器或解码器并冻结另一部分；引入Decoupled Dropout Perturbation作为跨分支正则化；设计Pair-wise CutMix Cross-Guidance通过增强输入对交换伪标签提升模型多样性；提出Consistency Matching，用冻结的稳定教师预测来细化伪标签以减轻确认偏差。

Result: 在ISLES2022和BraTS等脑MRI数据集上，DuetMatch在多种半监督设置下均领先于最新方法，显示出更好的鲁棒性和泛化能力。

Conclusion: DuetMatch通过双分支异步优化成功提高了半监督医学图像分割的稳定性和性能，特别在脑MRI基准数据集上优于现有方法。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出基于X光图像的C臂自主导航方法，预测3D位移并校准不确定性，结合骨骼正则化，在DeepDRR合成数据上实现高精度且校准良好的定位，可用于安全的自主C臂系统。


<details>
  <summary>Details</summary>
Motivation: 手术中C臂定位依赖人工操作，导致放射暴露和手术延误；希望通过自主导航减少这些问题并提高射线成像引导介入的效率与安全性。

Method: 从任意初始位姿的X光图像输入，模型输出指向每个解剖目标的3D位移向量；采用联合建模的不可约性（aleatoric）与认知性（epistemic）不确定性估计，并用保序（conformal）预测对不确定性进行校准，训练时引入概率损失和骨骼姿态正则化以保证解剖学合理性；在DeepDRR合成X光数据上做定量评估。

Result: 在合成数据集上，不同网络架构均取得了较强的定位准确性，同时预测区域（3D置信区间）经校准后表现良好，证明方法在定位与不确定性量化上的有效性。

Conclusion: 本文提出了一个可自主导航C臂至预定义解剖标志的管线，通过X光图像预测指向每个目标的3D位移并给出不确定性估计，结合可概率化损失与骨骼姿态正则化，在合成DeepDRR数据集上验证，展示了高定位精度与良好校准的置信区间，表明该管线可作为安全可靠自主C臂系统的组件。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 通过IQA自动预筛选并配合成本节省公式，在背景修补任务上用AutoML实现约51.61%成本下降。


<details>
  <summary>Details</summary>
Motivation: 生成式模型虽然能按文本或参考图生成高质量图像，但总体产出中高质量图像的比例较低，导致人工质量评审昂贵且低效。引入自动预筛选可以提高送审图像的平均质量、减少人工工时与成本。

Method: 提出并推导一个量化公式，估算基于IQA引擎的精确率（precision）和通过率（pass yield）对整体生产成本节约的影响；在背景修补任务上使用AutoML训练IQA分类器作为预筛选器，并将其性能带入公式进行成本节省计算与实证验证。

Result: 在背景修补用例中，使用简单AutoML建立的IQA预筛选器在实际数据与模型参数下带来51.61%的成本节省，证明了所提出公式与方法的实用性与可观降本效果。

Conclusion: 该论文通过在生成图像生产流程前引入自动化图像质量评估（IQA）预筛选阶段，证明可以显著降低人工审查成本。给出一个基于IQA精确率和通过率的成本节省公式，并在背景修补（inpainting）场景中用AutoML构建的简单IQA模型验证，取得约51.61%的成本节省。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: fMRI signals align more with language-model text space; using structured text as intermediate plus object-centric diffusion and attribute-relationship search (PRISM) yields better image reconstruction from fMRI, reducing perceptual loss up to 8%.


<details>
  <summary>Details</summary>
Motivation: Determine which latent space best matches neural activity and how to organize it to effectively reconstruct visual stimuli from fMRI.

Method: Map fMRI to structured text space (language model text embeddings), adapt text reps and generative model; object-centric diffusion composes objects; attribute-relationship search selects attributes/relations aligned to neural activity.

Result: PRISM outperforms prior methods on real-world datasets, up to 8% reduction in perceptual loss.

Conclusion: Structured text latent space aligns better with fMRI for image reconstruction; PRISM improves reconstruction via object-centric diffusion and attribute-relationship search.

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 针对热带农业遥感制图挑战，本文提出并演示了以数据为中心的25项策略，最终给出9步实操流水线以提升模型鲁棒性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 热带地区的高云量、多样化作物历时和数据稀缺性使传统以模型为中心的方法难以推广，亟需关注数据质量和采集策略以提高模型泛化能力。

Method: 综述并优先排序了25种数据相关技术（如可信学习、核心集选择、数据增强、主动学习等），并提出了包含9种成熟可行方法的实用流水线。

Result: 确认了25种可用于大规模农业制图的策略，筛选出9种最成熟、最简单可实施的方法，生成易于部署的实操流水线。

Conclusion: 本文主张以数据为中心的AI（DCAI）方法，通过数据质量提升与策划增强热带农业遥感制图的鲁棒性与可扩展性。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出StretchySnake：通过多尺度采样与权重插值的灵活训练，使视频SSM在各种时空尺度上既高效又鲁棒，显著提升动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型多为Transformer导向，固定的分辨率/长度训练导致在未见过的时空尺度上性能下降；SSM拥有线性复杂度和隐藏状态递归性，天然适合长序列处理，但缺乏专门的训练方法来发挥其可伸缩性。

Method: 在训练阶段对视频进行不同的时间和空间分辨率采样，并通过动态插值模型权重来适配任意时空尺度；提出并比较了五种可选的灵活训练变体，选出对视频SSM最有效的一种。

Result: 在短时动作（UCF-101、HMDB-51）和长时动作（COIN、Breakfast）基准上相比Transformer和SSM基线最高提升达28%，并在细粒度动作数据集（SSV2、Diving-48）上保持强适应性。

Conclusion: 本文提出了一种针对视频任务的灵活训练方法，使状态空间模型（SSM）具备跨时空尺度的适应性，从而在短时和长时视频动作识别均表现优异。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 本文提出将ViT与Mamba（SSM）异构集成的VM-BeautyNet用于面部美感预测，在SCUT-FBP5500上取得SOTA性能并通过可视化验证两分支互补性。


<details>
  <summary>Details</summary>
Motivation: 现有CNN难以捕获全局整体面部特征，而ViT虽能建模长程空间关系但计算复杂度高；Mamba（SSM）以线性复杂度高效处理长程依赖。通过异构融合，期望结合二者优点提升FBP性能与可解释性。

Method: 提出异构集成架构VM-BeautyNet，包含ViT骨干用于捕捉全局结构与对称性，Mamba（基于SSM）骨干用于高效建模长程依赖与序列化纹理特征；两者特征融合并进行回归预测，训练与评估在SCUT-FBP5500数据集上。

Result: 在SCUT-FBP5500上取得PC=0.9212、MAE=0.2085、RMSE=0.2698，超过现有方法；Grad-CAM可视化表明ViT侧重全局结构、Mamba侧重局部纹理，两者互补。

Conclusion: VM-BeautyNet通过融合ViT与Mamba（SSM）骨干，利用两者互补特性，显著提升面部美感评分性能，实验证明在SCUT-FBP5500上达到SOTA水平；Grad-CAM可视化显示两分支关注不同面部区域，增强了可解释性。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 本文构建了一个结合CNN与图像采集硬件的OCSCC检测系统，证明分辨率提升能改善检测但存在边际递减，并提供了用于测试与开放访问的应用。


<details>
  <summary>Details</summary>
Motivation: OCSCC早期症状不明显且常被漏诊，早期检测可挽救生命；CNN在图像分割与模式识别上表现优异，配合合适硬件能提高检测效率与准确性。

Method: 构建并训练了一个CNN，使用4293张包含良性、恶性和阴性样本的训练图像；测试集中图像被调整为5种常见分辨率以评估分辨率对性能的影响；设计并使用了增强图像采集硬件以捕捉更高细节；开发了一个应用用于测试与开放访问模型。评价指标包括精确率、召回率和mAP。

Result: 模型在不同分辨率下的预测准确性随分辨率增加而提高，但提高趋势为对数关系，表明在更高像素下收益递减。硬件增强提升了图像细节，进而改善检测效果。具体数值（如精确率、召回率、mAP）在摘要未给出。

Conclusion: 该论文提出并实现了一个用于口腔鳞状细胞癌（OCSCC）检测的系统，结合卷积神经网络（CNN）和专用图像采集硬件。实验表明，随着图像分辨率增加，模型预测准确率按对数关系提升，且高像素数存在边际收益递减。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D 是一个包含439名参与者、500小时、5400万帧的多模态三维运动数据集，包含单人动作与多人互动，附带手部追踪、体型、文本注释和独立音频。


<details>
  <summary>Details</summary>
Motivation: 弥补现有数据集中缺乏规模、多人互动和手部细节的三维运动数据，支持社交、协作和日常场景下更真实的人物建模与合成。

Method: 在多相机采集舞台对439名参与者进行录制，使用人体与手部追踪算法生成每帧的三维关节点和体型参数；同时为每位参与者提供文本注释和单独的音频轨道。

Result: 收集500小时、超5400万帧的高质量三维追踪数据，覆盖提示动作、手势、步态以及多人对话和协作场景，并提供手部追踪与体型信息、文本注释和独立音轨。

Conclusion: Embody 3D 提供了大规模、多模态、高质量的单人和多人三维运动数据集，适用于表情/肢体建模、动作合成、社交行为分析等研究。

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 利用人-物交互的主动观测，提出在线增量式场景分解与重建，结合Gaussian splatting实现高效真实感动态场景建模，在真实场景中表现良好。


<details>
  <summary>Details</summary>
Motivation: 动因在于传统基于静态观察的物体级重建存在本质模糊性，而人类的主动交互能够揭示物体结构与边界，从而提供额外的动态线索用于更精确的场景理解与建模。

Method: 方法利用第一视角（egocentric）流中的人-物交互线索，联合执行摄像机与物体位姿估计、实例分解以及在线地图更新，结合Gaussian splatting技术用于高效、真实感的动态场景建模与渲染。系统为在线增量式流程，能够在观察交互时动态细化分解与重建结果。

Result: 在多种真实场景下进行验证，结果表明该方法在姿态估计、实例分解及在线地图一致性方面具有明显优势，并能以光真实感和高效渲染实现一致的动态场景建模。

Conclusion: 本文提出了一种基于人-物交互的主动场景分解与重建方法，通过观察人类的有意动作来逐步解构并重建动态环境，从而解决静态物体重建中的模糊性问题。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus通过运动掩码提示与基于规则的偏离检测，在保证接近SOTA准确率的同时显著提升实时性能，适合实际视频分析部署。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在零样本VAD中性能好但计算开销大且视觉定位不稳定，难以实时部署；因此需要一种兼顾效率与准确性的方案。

Method: 离线学习正常行为规则；在线先用轻量级过滤器（motion mask prompting 引导VLM关注运动相关区域）筛选，再用VLM进行精细推理；用规则偏离检测识别异常。

Result: 在四个数据集上评估，平均在NVIDIA L40S上达到57.68 fps，速度提升约151.79倍，准确率97.2%，与最先进VLM方法相当。

Conclusion: 提出了Cerberus，一种双阶段级联系统，结合离线学习行为规则与在线轻量过滤+细粒度视觉语言模型推理，实现实时视频异常检测。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: 提出OpenLVLM-MIA：一个去偏的6,000图像基准，揭示现有MIA在LVLM上依赖数据分布偏差，真实攻击能力有限，强调需更严格的隐私评估与防护方法。


<details>
  <summary>Details</summary>
Motivation: 指出先前MIA研究中报告的高成功率可能源自数据构建过程中引入的分布偏差，而非模型记忆或真实成员识别；因此需要一个透明且无偏的基准来准确评估MIA在LVLM上的实际威胁。

Method: 作者构建了一个受控基准OpenLVLM-MIA：包含6,000张图像，精心平衡成员与非成员样本的分布，并在三个不同训练阶段提供真实成员标签；在此基准上，对现有最先进的MIA方法进行了评估，比较有无分布偏差条件下的攻击成功率。

Result: 在无偏的OpenLVLM-MIA基准上，当前最先进的MIA方法表现接近随机猜测，表明这些方法依赖数据偏差而非有效识别成员；该基准为未来开发更强的隐私保护技术提供了可靠评价平台。

Conclusion: OpenLVLM-MIA表明在LVLM上的现有MIA评估结果在很大程度上受数据集构建引入的分布偏差影响，而非真正识别成员身份；在去偏基准下，先进MIA方法的效果退化到随机水平，说明当前MIA方法在实际隐私威胁评价上仍有限。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch 是一个无需训练的跨图笔触注意力方法，能有效将参考素描的笔触属性迁移到内容图像，同时保持语义与结构完整，结果接近手绘。


<details>
  <summary>Details</summary>
Motivation: 需要在保持语义和内容的前提下，将参考草图的笔触属性（线宽、形变、纹理稀疏性）精确迁移到目标图像。

Method: 提出了无训练框架，关键为在自注意力层内引入跨图笔触注意力（cross-image stroke attention），并结合自适应对比增强与语义聚焦注意力以强化内容保持与前景强调。

Result: 在表达性笔触控制和语义一致性上优于既有方法，生成风格忠实且接近人工结果，代码开源。

Conclusion: Stroke2Sketch 在保持语义结构的同时，实现了从参考风格到目标内容的精确笔触属性迁移，生成的素描在笔触表达与语义连贯性上优于现有方法。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 作者构建ScaleDF大数据集，证明深伪检测性能随域/方法数量按幂律改进，可预测资源需求，并分析预训练、增强与缩放界限。


<details>
  <summary>Details</summary>
Motivation: 深伪技术快速演进，现有检测方法泛化能力不足；作者希望理解数据规模（域和方法多样性）对检测性能的刻画规律，从而可以预测并以数据中心策略应对未来深伪攻击。

Method: 构建了大规模数据集ScaleDF（5.8M真实图像、8.8M伪造图像，涵盖51个域与102种生成方法），通过一系列受控实验测量检测模型在不同真实域数、生成方法数和训练样本数下的性能，拟合并验证误差与数据规模之间的幂律关系，同时评估预训练和数据增强的影响以及缩放极限。

Result: 发现平均检测误差随真实域数或生成方法数按幂律衰减，能用该规律预测额外数据需求以达成目标性能；预训练和数据增强对性能有帮助但在缩放下表现差异，另外讨论了单纯放大数据的限制。

Conclusion: 该论文结论是：深伪检测性能遵循幂律缩放规律，随着真实图像域数量或生成方法数量增加，检测误差按可预测的幂律衰减；构建大规模数据集ScaleDF以支持该观察，并讨论预训练、数据增强和缩放的局限性。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: 通过局部窗口注意力＋低分辨率全局引导的混合架构，Scale-DiT实现高效可扩展的4K图像生成，兼顾全局语义和局部细节，同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型受注意力的二次复杂度限制和缺乏原生4K训练数据，难以在超高分辨率上同时保证细节与全局结构的一致性。

Method: 将高分辨率潜变量划分为固定大小的局部窗口以近线性复杂度计算局部注意力；引入低分辨率潜变量并使用缩放位置锚点注入全局语义；通过轻量级LoRA在去噪过程中桥接全局与局部路径；采用Hilbert曲线重排token并实现融合内核跳过masked操作以提高GPU效率。

Result: 在FID、IS、CLIP Score等定量指标和主观评估上，Scale-DiT在全局连贯性和局部细节方面优于或匹配依赖原生4K训练的数据驱动方法；推理速度提升超过2倍，内存占用更低，并能无附加高分辨率训练数据下扩展到4K×4K。

Conclusion: Scale-DiT通过层级化局部注意力结合低分辨率全局引导，有效降低注意力复杂度并维持语义一致性，实现在无需高分辨率训练数据下生成4K×4K图像，推理速度和内存使用显著优于稠密注意力基线。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX 用边缘轻量模型做快速预览、云端大模型做最终精修，并通过噪声预测器动态分配计算，减少延迟和云负载，同时保留或提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成计算成本高，用户需多次调整提示，导致高延迟与云资源负担，故提出云-边协作以缓解这些问题。

Method: 在设备端运行轻量级扩散模型用于快速预览，云端运行高容量模型做最终细化；引入噪声水平预测器动态分配计算负载，在边缘和云端之间优化延迟与资源消耗的权衡。

Result: 实验表明在保持相似图像质量的情况下，DiffusionX 平均生成时间比 Stable Diffusion v1.5 快 15.8%，且仅比 Tiny-SD 慢 0.9% 但图像质量显著更好，证明了其高效性与可扩展性。

Conclusion: DiffusionX 提出了一种云端与边缘协同的多轮提示生成框架，能在用户交互生成场景中减少延迟与云端开销，同时保持或提升图像质量。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: 提出一种令牌级增强（TokenAR）用于多参考自回归图像生成：包括Token Index Embedding、Instruct Token Injection与ITD策略；并构建了首个大规模InstructAR数据集，实验证明优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统自回归（AR）条件图像生成在多参考图像场景下难以将不同参考身份解耦，容易导致身份混淆，影响生成中多个目标的一致性与多样性。

Method: 提出Token Index Embedding对令牌索引进行聚类表示；Instruct Token Injection注入额外视觉先验作为补充；Identity-Token Disentanglement（ITD）策略显式分离身份特征，从而在AR模型中增强引用图像的独立表示能力。

Result: 在新建的InstructAR数据集（28K训练对，每例含两参考主体、相对提示与背景掩码）上，TokenAR在多参考图像生成任务中优于现有SOTA模型，兼顾高质量背景重建与身份一致性，并实现高多样性。

Conclusion: TokenAR通过令牌级增强机制有效解决了多参考图像生成中的身份混淆问题，显著提升了身份一致性与背景重建质量，且在新构建的InstructAR数据集上优于当前SOTA方法。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 本工作揭示了后训练策略（SFT vs RL）会根本性改变MLLM的视觉表征：RL能产出更精确的视觉表示；基于此提出PIVOT方法，低成本提升视觉编码器性能并显著加强MLLM在视觉任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前研究过度强调LLM主干对MLLM性能的影响，忽视了视觉编码器在模型感知图像方面的核心作用；另外，从SFT到RL的训练范式转变使得对视觉编码器受训练方式影响的理解尤为重要。

Method: 通过比较SFT与RL在多种视觉相关基准（视觉VQA、ImageNet分类、分割任务）上的表现，并结合梯度可视化等分析手段，系统评估后训练策略对视觉编码器表征的影响；提出PIVOT并在MLLM中验证其有效性。

Result: 实验显示RL优于SFT，在强视觉相关任务上提升明显；RL导致的视觉表征更为精确且定位明确；PIVOT能以不到标准视觉预训练1%的计算成本，训练出在下游任务上超越更大、更重训练模型的视觉编码器。

Conclusion: RL训练相较于SFT能显著改善多模态语言模型(MLLM)的视觉表征，使其更具定位精度和任务相关性；基于此可设计低成本但高效的视觉编码器优化策略（PIVOT）。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 提出GradNorm：用梯度幅值评估名词与图像的语义相关性，具有理论保证并显著提升LaIC聚类效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CLIP特征空间进行名词筛选，但缺乏严格理论依据；论文旨在提供有理论保证且更有效的筛选策略，以提升视觉表示的判别性。

Method: 通过构造一个基于交叉熵与softmax输出之间差异的梯度反向传播量化每个名词的'正性'，并基于该梯度幅值筛选出与图像语义相关的名词。

Result: 理论上给出误差界，证明GradNorm能区分正名词并包含现有策略为特殊情形；实证在多个基准数据集上达到了最先进的聚类性能。

Conclusion: 该论文提出了GradNorm，一种基于梯度的名词筛选方法，用于语言辅助图像聚类（LaIC），理论上有保证并在实验中优于现有方法。

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: MIRAD——首个面向社交制造的异常检测基准，包含高度个性化产品、六个分散节点和多样成像条件；SOTA方法在该数据集上性能显著下降，凸显现实工业场景下的挑战并推动行业5.0质量控制研究。


<details>
  <summary>Details</summary>
Motivation: 社交制造背景下产品高度定制、批量小且生产分散，导致缺乏适用的数据集与算法，现有基准不反映真实工业场景，亟需现实数据促进鲁棒缺陷检测研究。

Method: 提出并公开了MIRAD数据集，包含多类高度个性化产品、来自六个地理分散制造节点的图像以及显著的成像异质性；并对现有一类、多类和零样本异常检测方法进行了广泛评估。

Result: 在MIRAD上，所有评估的SOTA方法相比传统基准均出现显著性能下降，表明现有模型难以应对个性化生产中的大类内变化、跨节点分布差异与多样化成像条件。

Conclusion: 本文构建了首个面向社交制造的异常检测基准数据集MIRAD，揭示了在高度个性化、小批量和分散成像条件下现有SOTA方法性能显著下降，强调需要面向实际工业场景的新方法。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 发布一个含3000段、多层注释（阶段、实例分割、交互追踪、技能评分）的多中心白内障手术视频数据集，并提供基准与跨中心泛化实验。


<details>
  <summary>Details</summary>
Motivation: 现有白内障手术数据集缺乏足够的多样性和细粒度注释，难以训练具有良好泛化能力的深度学习模型，因而需要一个规模更大、注释更全面且包含多中心数据的数据集来推动手术AI研究。

Method: 收集3000段来自两个手术中心、不同经验水平外科医生的手术视频，并对视频进行四层注释：手术阶段的时序标注、器械与组织的实例分割、器械-组织交互追踪和基于ICO-OSCAR等量表的定量技能评分。随后在关键手术AI任务（流程识别、场景分割、技能评估）上进行了基准实验，并设计了跨中心域自适应的基线实验（在部分中心训练、在留出中心测试）。

Result: 提供了包含四层注释的3000个手术视频数据集，并通过一系列基准实验展示了该数据集在流程识别、场景分割和自动化技术评分等任务上的可用性，同时给出了跨中心泛化的初步结果。数据和注释已对外发布。

Conclusion: 该论文构建了一个规模较大且注释丰富的白内障超声乳化手术视频数据集，填补了现有数据集在多样性与注释深度方面的空白，对外开放并提供基准实验。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个面向印度场景的端到端自动化坑洞监测平台，结合YOLO检测、时间戳-GPS同步、路段-承包商治理与OSM可视化，实现问题发现到责任追踪及修复验证的闭环，但需关注模型泛化、定位误差与系统规模化能力。


<details>
  <summary>Details</summary>
Motivation: 印度道路多样且维护薄弱，人工巡检成本高且难以覆盖，需低成本、可扩展且支持责任追踪的自动化坑洞监测与治理平台。

Method: 基于自标注7000+行车记录帧微调Ultralytics YOLO模型进行坑洞检测；通过OCR提取视频时间戳并与外部GPS日志同步实现精确地理定位；采用优化后后端数据库管理路段与承包商元数据；构建安全登录接口、告警机制及基于OSM的动态可视化网页前端。

Result: 系统实现了从检测到修复验证的端到端自动化流程：准确坑洞检测、GPS精确标注、承包商关联与自动告警、基于OSM的动态路况可视化以及可供公众和管理者访问的分析仪表盘；演示与平台已公开。

Conclusion: iWatchRoadv2将实时检测、地理标注与可视化结合，能够显著提升路况监测与养护管理的透明度与效率，但需要评估泛化性、定位精度与运维可扩展性。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是第一个处理可变拓扑并同时建模关节、子组件形状与非刚性形变的植物3D参数化模型，配套大豆农场数据集，能用于合成、重建与生物物理模拟。


<details>
  <summary>Details</summary>
Motivation: 现有强大的参数化3D模型主要针对人类与动物，缺乏对植物（尤其农作物）同等表达能力的模型；需要能处理物种间拓扑变化和多源形变的植物模型以推动重建与模拟应用。

Method: 基于数据驱动学习的紧凑表征，Demeter将植物形态的拓扑、形状、关节与非刚性形变编码进参数化模型；设计用于处理可变拓扑，并分别建模三种形变来源。实验包括形状合成、结构重建和生物物理过程模拟。

Result: Demeter在合成植物形状、重建结构和模拟生物物理过程上表现良好；同时发布了用于作物植物建模的大规模带真实标签的大豆数据集。

Conclusion: Demeter提出了一个用于植物的参数化3D形状模型，能够处理不同物种的拓扑变化，并同时建模关节、子组件形状和非刚性形变三类形状变化，实验验证了在合成、重建和生物物理模拟任务上的有效性；文章还提供了来自大豆农场的大规模标注数据集。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 为边缘AR/VR设备提出的轻量级手部姿态估计框架，通过稀疏卷积、SPLite解码器和量化感知训练，显著提升推理速度和内存效率，精度仅有极小下降。


<details>
  <summary>Details</summary>
Motivation: AR/VR设备对实时性、低功耗和低延迟的需求迫使模型在边缘设备上运行，研究旨在在效率与准确性之间取得平衡，满足资源受限设备的部署需求。

Method: 使用ResNet-18主干并在其上应用稀疏卷积以利用手部图像的稀疏性；设计SPLite解码器以加速解码过程；对模型进行量化感知训练以减少内存并保持精度。评估在Raspberry Pi 5上进行性能测试，并在FreiHAND等数据集上测试精度。

Result: 在Raspberry Pi 5 CPU上实现约2.98x加速；通过稀疏卷积实现约42%端到端效率提升；SPLite解码器使解码帧率提升3.1x；量化感知训练仅使PA-MPJPE从9.0mm增加到9.1mm；总体在保持与SOTA可比精度的同时显著提高计算效率。

Conclusion: 该论文提出了面向AR/VR边缘设备的轻量级手部姿态估计框架，通过编码器-解码器结构、稀疏卷积、轻量解码器SPLite和量化感知训练，在提高推理速度和降低内存占用的同时，仅带来极小的精度下降。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM利用3D Gaussian Splatting渲染结合全局到局部的多视图MLLM代理策略，实现开放世界、推理驱动的精准3D分割和实用交互功能，无需大量3D后训练。


<details>
  <summary>Details</summary>
Motivation: 弥合擅长推理的2D视觉语言模型与需要空间理解的3D分割之间的能力鸿沟，减少对3D专属后训练的依赖，提升对复杂、含推理或模糊指令的理解与执行能力。

Method: 在3D Gaussian Splatting上渲染多张全局视角图像并并行输入多模态大模型进行粗定位，聚合响应以确定目标，再合成若干目标近景图进行细粒度分割，最终回投得到一致的3D掩码。

Result: 在LERF、3D-OVS及新设的REALM3D基准上表现优异，并支持移除、替换、风格迁移等多种3D交互任务，展示出实用性与通用性。

Conclusion: REALM通过在3D Gaussian Splatting上结合多视图渲染与MLLM代理，实现了从复杂人类指令到精确3D分割的桥接，避免了大量3D特定后训练，可在开放世界场景下对显性与隐性指令进行推理式分割。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: 提出SSL4RL：用自监督任务生成可验证奖励，结合RL微调VLMs，提升视觉利用与推理能力，并可推广到图学习。


<details>
  <summary>Details</summary>
Motivation: VLMs常依赖语言先验或文本捷径而忽视视觉证据；现有的RL方法受限于缺乏可扩展且可靠的奖励机制。引入可验证的SSL奖励以对齐模型行为。

Method: 将传统的SSL目标（如图像旋转预测、掩码块重建）重新表述为密集自动的奖励信号，并以此作为RL的回报进行VLM微调。通过系统消融实验分析任务难度、模型规模和语义对齐等因素对效果的影响。

Result: 在视觉中心与视觉-语言推理任务上取得显著提升；消融分析给出任务设计要点；在图学习上也取得重要增益，证明方法通用性。

Conclusion: SSL4RL提出了用自监督学习（SSL）任务生成可验证奖励信号来进行强化学习微调，从而解决了VLMs对视觉证据利用不足的问题。该方法避免了人工偏好数据或不可靠的AI评估器，能提升视觉中心和视觉-语言推理基准的表现，并能推广到图学习领域。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一款针对点与线段的轻量级联合匹配器，核心为ALMP模块，能高效利用线段连通性，提升匹配性能并大幅降低计算资源需求，适合实时与边缘应用。


<details>
  <summary>Details</summary>
Motivation: 传统点、线匹配多为独立任务，且现有联合匹配（如GlueStick）虽然减少了整体开销，但架构复杂、计算量大，难以适配实时和边缘场景；因此需要一个更高效的联合匹配器。

Method: 在单一网络框架中同时处理点和线特征，采用GNN结构并引入ALMP以利用线段的拓扑信息进行注意力消息传递；整体架构更轻量以适配实时和边缘设备部署。

Result: 在多项基准测试中，LightGlueStick达到了新的最先进水平（state-of-the-art），在准确性与效率之间取得更好权衡，代码已开源。

Conclusion: LightGlueStick提出了一种轻量级的点与线特征匹配器，通过引入Attentional Line Message Passing (ALMP)模块显式建模线的连通性，实现了高效的节点间信息交流，从而在保持或提升匹配性能的同时显著降低计算复杂度。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出EDVD任务并设计EDVD-LLaMA：通过ST-SIT提取时空细节并用Fg-MCoT在大模型中引入人脸特征硬约束，实现可追溯、像素级的可解释深伪视频检测；构建ER-FF++set用于双重监督，实验证明方法在准确性和可解释性上均优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 传统深伪视频检测方法可解释性差、泛化能力不足，难以应对快速演化的伪造技术，因而需要能给出可验证推理过程的检测器。

Method: 方法包含两大模块：1）Spatio-Temporal Subtle Information Tokenization（ST-SIT），用于提取并融合跨帧的全局与局部细微深伪特征，生成富含时空语义的信息令牌供大模型推理；2）Fine-grained Multimodal Chain-of-Thought（Fg-MCoT），在链式思维中引入人脸特征作为硬约束，以实现像素级时空定位、抑制幻觉并提高推理可靠性。还构建了ER-FF++set数据集用于双重监督。

Result: 实验表明EDVD-LLaMA在检测准确率、可解释性及跨伪造方式和跨数据集泛化能力方面表现优越，能提供更可信的检测与解释。作者将公开代码与数据集。

Conclusion: 该论文提出了可解释的深度伪造视频检测（EDVD）任务，并设计了基于大模型推理的多模态框架EDVD-LLaMA，能同时提供检测决策与可验证的推理过程。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 提出RefAtomNet++：通过多层次语义对齐跨注意力与多轨迹Mamba建模，改进跨模态对齐与召回，在大规模RefAVA++数据集上实现细粒度、语言引导的人物动作识别SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如RefAtomNet）虽利用主体注意力突出显著特征，但跨模态对齐与检索能力不足，导致在复杂多人场景中定位目标人与细粒度动作识别效果不佳，因此需要更强的语义对齐与多轨迹建模来提升性能。

Method: 提出多层次语义对齐跨注意力机制，结合部分关键词、场景属性、整体句子级别的多轨迹（Mamba）建模；在部分关键词和场景属性层通过动态选择每个时间步最近的视觉空间tokens构建扫描轨迹；聚合时在不同语义层融合时空tokens以增强跨模态对齐与检索。

Result: 在扩展的RefAVA++数据集（>2.9M帧，>75.1k标注人）上，RefAtomNet++超越之前基线与RefAtomNet，建立新的SOTA，证明多层次语义对齐跨注意力和多轨迹Mamba建模能有效提升语言引导的原子级视频动作识别性能。

Conclusion: RefAtomNet++通过多层次语义对齐的跨注意力和多轨迹Mamba建模显著提升了跨模态对齐与召回能力，从而在RefAVA++数据集上实现了新的SOTA，改进了目标人物定位和细粒度动作识别性能。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出基于各向异性高斯和Bhattacharyya距离的旋转不变损失，集成到现有检测器后显著提升旋转目标检测的mAP与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器对轴对齐物体表现良好，但在旋转目标上受限于方向变化，现有损失对旋转鲁棒性不足，需一种能捕捉几何和方向信息的旋转不变损失函数。

Method: 将旋转目标表示为高斯分布，使用Bhattacharyya距离构造旋转不变的损失；在高斯协方差矩阵中采用各向异性约束以区分长/宽不一致目标；将该损失集成到现有深度学习旋转目标检测器并通过大规模实验验证。

Result: 在多个旋转目标检测基准上，所提方法在mAP上显著优于现有方法，证明了各向异性高斯+Bhatta距离损失在定位精度和鲁棒性上的有效性，可作为新的基准方法。

Conclusion: 该论文提出了一种基于高斯边界框表示与Bhattacharyya距离的改进损失函数，并引入各向异性高斯以解决正方形样本方差处理问题，显著提升了旋转目标检测精度。

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种简单高效的visual prompt初始化方法，通过语义对齐和注入新表示方向，显著提升自监督模型的prompt调优性能，尤其在小样本和困难任务中效果明显。


<details>
  <summary>Details</summary>
Motivation: 现有视觉prompt调优在自监督模型上常表现为prompt未能专门化或丰富表示空间，导致在困难和数据稀缺场景下表现受限。VIPAMIN旨在通过更好的初始化解决这些问题。

Method: 提出一种初始化策略：用单次前向传播定位语义上有信息的嵌入区域并将prompt与之对齐，同时注入预训练子空间之外的新表示方向（具体通过轻量操作构造额外向量并加入prompt空间）。该方法无需微调骨干，仅调整prompt，计算开销低。

Result: 在多种任务和不同数据规模上，VIPAMIN稳定提升性能，在视觉prompt调优任务上达到新的最先进水平。代码开源。

Conclusion: VIPAMIN通过语义对齐与引入新表示方向，有效提升了在自监督骨干上的视觉prompt调优效果，特别在数据稀缺和困难任务上显著改善性能，成为该领域新的SOTA方法。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 提出结合分割与检测的WDA框架与实例感知伪标签选择，利用稀疏点标注显著提升跨域线粒体实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统UDA在实际应用中性能有限，而在目标域上获得完整标注代价高昂。稀疏点标注标注成本低且不需专家知识，本文探讨如何最大化利用这些不完整与粗糙的标注来提升域适应分割。

Method: 构建多任务学习框架，同步进行分割与中心检测，设计交互教学（cross-teaching）机制和面向类别的跨域对比学习，在无标记区域引入分割自训练，并提出基于实例感知的伪标签（IPL）选择策略，利用检测任务辅助从语义层面选择可靠且多样的伪标签。

Result: 在多组具有挑战性的EM数据集上做了全面验证与对比，方法优于现有UDA与WDA方法，并在若干设置下显著接近监督上界；在纯UDA设置下也比其他UDA方法有明显提升。

Conclusion: 本文提出的弱监督域适应（WDA）方法能在目标域仅用稀疏点标注下显著提升线粒体实例分割性能，缩小与有监督上界的差距，并在无监督域适应（UDA）设置下亦有明显改进。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出通过在无标签轨迹上训练Q模型生成动作级“未来”特征，并与指令融合用于A*-风格搜索，从而使VLN智能体具备前瞻性、提升长期决策效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于历史信息决策，忽视动作的未来影响和长期结果，导致策略短视；因此作者希望构建能考虑长期后果的前瞻性智能体。

Method: 在大规模无标签轨迹数据上采用Q-learning训练一个任务无关的Q模型，为每个候选动作生成Q特征；设计跨模态未来编码器，将Q特征与自然语言导航指令融合，输出反映未来前景的动作评分；将未来评分与基于历史的原始评分结合，采用A*-样式搜索策略进行路径探索。

Result: 在常用的目标导向VLN数据集上进行了大量实验，结果表明该方法在导航成功率和路径效率等指标上优于基线，验证了引入Q特征和未来编码器的有效性。

Conclusion: 该论文提出了一种基于Q-learning的前瞻性导航模型，通过在无标签轨迹上训练Q模型以捕捉室内布局和物体关系，用于生成每个候选动作的Q特征，并与指令通过跨模态未来编码器融合，结合历史分数采用A*风格搜索，从而提升目标导向视觉语言导航的长期决策能力。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: 提出层次化高斯压缩（结构层+运动层）并结合人脸注意力和SMPL-X语义运动编码，显著提升动态数字人3DGS的压缩效率与渲染质量，适合流式3D头像。


<details>
  <summary>Details</summary>
Motivation: 通用的3DGS压缩缺乏人类先验，导致码率效率和解码端重建质量不足，限制了流式3D数字人应用。提出利用人体先验与层次化设计提升压缩与渲染效果。

Method: 将高斯表示拆为结构层（StyleUNet生成，从姿态映射到高斯）与运动层（基于SMPL-X表示时序姿态变化），支持分层压缩、渐进解码和多模态可控渲染；训练时在StyleUNet中加入人脸注意力以保留身份与表情细节。

Result: 实验显示HGC-Avatar在视觉质量和压缩效率上明显优于先前方法，可实现快速流式渲染并支持从视频或文本等输入的可控姿态渲染。

Conclusion: HGC-Avatar通过层次化地分解高斯表示（结构层+运动层），并结合人脸注意力和基于SMPL-X的语义运动编码，实现了在低比特率下对动态数字人进行高质量重建和可控渲染，适用于流式3D头像系统。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench基于真实审稿意见构建科学论文多模态不一致基准（262个样本），设计识别/修复/匹配三任务并采用JSON答案格式，21款主流LMM表现低下（26.1-54.2%），提示该问题仍具挑战性。


<details>
  <summary>Details</summary>
Motivation: 当前对LMM评估多模态科学理解的基准多关注单一模态或使用合成错误，难以反映真实论文中跨文本、图表、方程等模态间的复杂不一致问题。为提高科学助手的可信度，需要一个基于真实审稿人报告的不一致性基准来测评模型的实用推理能力。

Method: 通过多阶段管道：（1）审稿评论挖掘获取潜在不一致候选；（2）采用大语言模型辅助过滤以初步筛选关联性强的评论；（3）人工验证与整理，最终得到262个真实不一致样本；基于此数据集构建三项评估任务，并设计JSON结构化答案以缓解多项选择捷径问题；最后在21款LMM上进行基准测试评估。

Result: 构建了PRISMM-Bench，包含262条真实审稿人标注的不一致问题；设计三类任务（识别、修复、配对）与JSON答案格式；在21个领先LMM上评测，模型准确率位于26.1%到54.2%之间，表现普遍较差，显示模型尚不能可靠处理科学文献中的多模态不一致性。

Conclusion: 本文提出了PRISMM-Bench，这是首个基于真实审稿人标注的不一致性基准，用于评估大规模多模态模型在科学论文中的一致性检测与修正能力。基准包含262个从242篇论文中挖掘的真实不一致样本，并设计了识别、修复与配对三项任务，同时引入结构化JSON答案以减少选择题的语言偏差。对21款主流LMM评测显示其在多模态科学推理上表现较差（26.1-54.2%），提示该领域仍有显著改进空间。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: 在YOLOv8上加入商品分割与伪深度估计的辅助学习（并做深度归一化），提出OOS-DSD方法，显著提升了货架缺货检测的mAP。


<details>
  <summary>Details</summary>
Motivation: 利用多任务学习的辅助信息（商品分割和深度估计）改善货架上商品缺货检测的鲁棒性和精度，特别是补充视觉深度信息以帮助区分遮挡或摆放空位等导致的缺货判断难题。

Method: 基于YOLOv8，在主干上增加卷积分支同时进行缺货检测、商品分割和场景深度估计；分割和缺货使用真实标签训练，深度分支使用Depth Anything V2生成的伪标签训练，并提出深度归一化处理以稳定训练。

Result: 在基准上，OOS-DSD使mAP相比SOTA提升1.8%；消融实验显示加入辅助任务带来3.7%提升，深度归一化进一步带来4.2%提升。

Conclusion: 提出的OOS-DSD通过引入深度估计辅助任务，能够提升缺货检测性能，实验显示优于现有SOTA方法，并且消融实验验证了辅助学习和深度归一化的有效性。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出基于GAT自编码器的代表性中心图像分类与检索方法，利用图结构和邻域上下文生成类别代表，先分类后类内检索，实验显示优于或可比传统特征方法。


<details>
  <summary>Details</summary>
Motivation: 增强图像分类与检索中对邻域关系与上下文特征的利用，克服纯特征方法忽略局部结构或全局语义一致性的缺点，通过代表性模型简化分类与检索过程。

Method: 构建以图节点表示图像（或其代表），边表示相似性；使用GAT强调重要特征与邻域关系，作为自编码器的编码器部分生成上下文感知的潜在向量；从嵌入中提取类别代表（如聚类中心或平均向量）；查询图像编码后与类别代表比较确定类别，然后在该类别中检索最相似图像。

Result: 作者通过在GAT自编码器与基于传统特征的对比实验，展示了代表性中心方法在分类与检索任务上的有效性（具体性能提升取决于实验数据与基线，摘要未给出具体数值）。

Conclusion: 方法提出了一种基于图和图注意力网络自编码器的代表性中心图像分类与检索框架，通过构建图结构并利用GAT提取上下文相关的潜在表示，再以类别代表进行分类与检索，思路合理并能增强邻域信息利用。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: 通过重构和释义对齐两个互补辅助任务对CLIP微调，可增强文本编码器对词间关系的建模，从而改善视觉-语言模型的组合推理能力。


<details>
  <summary>Details</summary>
Motivation: 标准对比学习使文本编码器偏向单词级别对齐而忽略词间关系，导致组合推理能力不足。

Method: 在预训练CLIP基础上微调，引入两项辅助任务：1) token级重构（用冻结的预训练解码器基于原始caption的嵌入去重建替代caption）；2) 句子级对齐（在嵌入空间显式对齐释义句子）。

Result: READ-CLIP在五个主要组合推理基准上取得SOTA，较最强基线最高提升4.1%，并且对已有CLIP变体（NegCLIP, FSC-CLIP）同样有效。

Conclusion: READ通过在contrastive训练上加入重构与句子对齐辅助目标，能显著提升视觉-语言模型的组合推理能力，领先于常规模型微调方法。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [51] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: GaitRDAE通过对每个视觉区域自适应地搜索时序尺度并进行区域级注意力激励，提升了步态识别在动态运动与协变量条件下的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用预定义或固定的区域与等同的时序尺度，难以处理运动区域随时间动态变化以及不同区域应有不同时序模式的问题，尤其在存在服装、视角等协变量时更难提取稳定的判别性运动特征。

Method: 提出两个核心模块：区域感知动态聚合（RDA）模块用于为每个区域动态搜索最优的时序感受野（自适应时序尺度），以及区域感知动态激励（RDE）模块用于对含稳定行为模式的运动区域给予更高权重、抑制受协变量影响较大的静态区域。整体框架将区域划分、时序自适应建模与注意力融合以学习更具判别力的时序特征。

Result: 在若干步态识别基准数据集上，GaitRDAE在准确率/识别率上达到了最先进水平（state-of-the-art），证明了自适应区域与时序机制在步态识别任务中的有效性。

Conclusion: 该论文提出了一种面向步态识别的区域感知动态聚合与激励框架（GaitRDAE），通过自动搜索运动区域并为不同区域分配自适应时序尺度以及对应注意力机制，提高了对动态变化与视外扰动的鲁棒性，实验在若干基准数据集上达到或超过现有最优。

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [52] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 在真实政治deepfake数据集上的系统评估显示：现有检测器泛化差、易被简单操控击败，需开发政治情境驱动的检测方法以提升现实可用性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型快速普及，政治性deepfake的传播威胁公众信息生态与民主机构，现有在实验室合成数据上训练的检测器可能无法应对真实社交平台上的政治deepfake。

Method: 构建基于Political Deepfakes Incident Database的真实世界政治deepfake基准，系统评估学术、政府和工业界的先进检测器，并测试对简单操控（如压缩、裁剪等）的鲁棒性。

Result: 在真实政治deepfake基准上，多数检测器性能显著下降；付费商业工具优于免费模型，但仍不足以可靠检测真实政治deepfake；检测器对常见视频操作敏感，提示需要情境化的检测框架。

Conclusion: 当前主流深度伪造检测器在真实政治deepfake上泛化能力弱，学术和政府模型表现尤差，付费工具略优，但整体对抗简单操作（尤其视频）仍脆弱。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [53] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 论文指出LVLM的对象幻觉主要源自视觉编码器，并提出无需训练的SHIELD框架，通过重加权视觉token、噪声token和对抗对抗+对比解码三策略，有效降低幻觉且保持通用性。


<details>
  <summary>Details</summary>
Motivation: 尽管先前工作聚焦于LLM组件，作者观察到幻觉根源于视觉编码器的偏差和脆弱性，因此提出从视觉编码器角度干预以缓解对象幻觉。

Method: 提出SHIELD框架，包括三种策略：对视觉token重加权（降低统计偏差）、引入基于噪声的token（抵消内在偏差）、以及对抗攻击结合对比解码（处理脆弱性）。该方法为训练无需修改模型参数，通过对输入token和解码过程的操作实现。

Result: 在多种基准和不同LVLM家族上实验证明SHIELD在减轻对象幻觉方面有效，同时在通用LVLM基准上也取得强性能，表明方法具有广泛适用性。

Conclusion: 该论文首次将大视觉-语言模型（LVLM）的对象幻觉问题归因于视觉编码器，并提出了一种训练免费（training-free）的防护框架SHIELD，有效减轻幻觉问题。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [54] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出VisionSelector：可微Top-K与课程退火的轻量评分器，实现对视觉tokens的端到端可学习压缩，在多种保留率下保持优异性能并显著加速预填充。


<details>
  <summary>Details</summary>
Motivation: 高分辨率或多图输入产生大量视觉tokens，导致MLLM在计算和内存上瓶颈。现有启发式压缩方法易丢信息并产生注意力汇聚等偏差，且在高压缩比下性能严重下降，因此需要一种自适应、可学习且高效的token压缩方案。

Method: 在不修改MLLM主干的前提下，设计一个12.85M参数的评分器模块（VisionSelector），输出token重要性评分并通过可微Top-K机制选择保留token；引入课程退火缩小训练与推理之间的差距，实现对任意压缩率的泛化；训练为轻量微调，仅更新评分器参数。

Result: VisionSelector以仅12.85M可训练参数的轻量模型，在不同压缩预算下均显著优于先前方法：在MME任务上以30%保留率保持100%准确率；在10%保留率下相较于先前方法提升12.14%；并实现两倍prefill加速。

Conclusion: 该论文提出了一种可插拔、轻量级的视觉token选择模块VisionSelector，通过可微Top-K与课程退火策略，将token压缩转化为端到端可学习的决策过程，从而在不同保留率下自适应选择关键视觉token，避免了启发式规则的偏差和性能骤降。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [55] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 本文提出一种融合U-Net、EfficientNet和Transformer并采用剪枝量化加速的实时医学影像分析框架，在多模态数据上实现高精度低延迟且具可解释性，适配边缘/云端部署，能显著加速临床工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统图像处理在精度、鲁棒性和实时性方面不足，临床影像解释耗时且受临床医生间差异影响，需一种兼顾速度与准确性的解决方案。

Method: 集成U-Net用于分割、EfficientNet用于分类与Transformer用于特征融合，结合模型剪枝、量化和GPU加速等实时优化策略，并支持PACS/EHR互操作。

Result: 在公开基准数据集上分类准确率>92%、分割Dice>91%、推理时间<80ms，并通过Grad-CAM和分割叠加提供可视化解释。

Conclusion: 提出的深度学习框架在多模态医学影像的实时分析上具有显著潜力，可提升诊断准确性并减少推理延迟，适用于边缘与云端部署，增强临床可解释性。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [56] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出一种结合语义分割与单目深度估计的室内无人机视觉导航系统，采用自适应尺度因子将非度量深度转换为度量距离，并通过知识蒸馏训练轻量级分割网络，实验表现出高精度距离估计与良好导航成功率，适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 动机是实现低成本、仅依赖相机的室内无人机自主导航，解决单目深度非度量问题与计算资源受限平台上的实时语义分割需求，从而避免依赖GPS或LiDAR等昂贵/不可用传感器。

Method: 方法包括语义分割与单目深度估计的融合，提出自适应尺度因子算法利用语义地面检测与相机内参将相对深度转为绝对距离；知识蒸馏框架中用基于颜色的SVM教师生成训练数据，训练轻量级U-Net学生模型（1.6M参数）用于实时分割；还可用更强的分割模型替换教师。系统通过室内5x4米实验和数字孪生及真实飞行测试评估，并在最后用来自最佳方法的示范数据训练端到端学生网络以学习完整飞行策略。

Result: 结果显示自适应尺度因子使平均距离误差为14.4 cm；结合分割与深度估计可以增加巡检航程并缩短任务时间，受控与数字孪生测试中成功率达100%；端到端压缩网络在实测中达87.5%成功率，证明方法在结构化室内环境中的可行性和计算效率。

Conclusion: 该文提出了一种基于单目视觉的室内小型无人机自主飞行系统，结合语义分割与单目深度估计，通过自适应尺度因子将非度量深度转换为度量距离，并采用知识蒸馏训练轻量级分割网络，实现实时避障、探索和安全降落。系统在受控实验与数字孪生环境中表现良好：实现14.4 cm的平均距离误差、增加巡查距离、缩短任务时间并保持100%成功率；通过端到端学习得到的紧凑网络在实测中达87.5%任务成功率。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [57] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个包含647条对话、覆盖484任务的多轮VLM评估集，使用GPT-4o基于37项检查表指标自动评估，结果显示即便最强模型也只达约50%成功率，凸显多轮场景挑战。


<details>
  <summary>Details</summary>
Motivation: 现有多轮数据集覆盖不足，无法全面反映现实中复杂多轮视觉-语言交互需求，需新基准评估模型在多轮对话中的表现。

Method: 从12个已有VLM评估基准中抽取与重写对话，构建647条对话（平均4轮），并设计基于检查表的评估方案，使用GPT-4o自动评估37个维度指标。

Result: 在MultiVerse上评估18个VLM，最强模型（如GPT-4o）在复杂多轮对话中仅达约50%成功率；完整对话上下文对弱模型有显著提升。

Conclusion: MultiVerse提出了一个覆盖广泛任务的多轮视觉-语言对话基准，显示当前VLM在多轮场景中仍有明显不足。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [58] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 将3D场景图存入图数据库，使用Cypher查询作为LLM检索工具的RAG方法，使语言接地在大规模场景图上更高效且性能更好。


<details>
  <summary>Details</summary>
Motivation: 直接把3DSG序列化为LLM上下文无法扩展到大规模或丰富的场景图，需更高效的检索机制以减少token负担并保持或提升接地性能。

Method: 将3DSG编码到图数据库，用检索增强生成(RAG)策略选择子图，向LLM提供Cypher查询接口供其检索相关场景信息；在指令执行和场景问答任务上与基线比较。

Result: 使用Cypher接口的RAG方法在大规模3DSG上较上下文窗口和代码生成基线显著更可扩展，提升了地面化任务性能并大幅减少了场景图token数量；适用于本地和云模型。

Conclusion: 本文提出将3D场景图存入图数据库，并以Cypher查询作为LLM的检索工具，以实现可扩展的语言接地。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [59] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: 本文提出UTAP——一种固定的可迁移对抗扰动，能以微小且视觉不可见的噪声广泛破坏病理学基础模型的表现，凸显亟需更强的鲁棒性与防御方法。


<details>
  <summary>Details</summary>
Motivation: 评估并揭示病理学基础模型在实际应用中可能遭遇的普遍且可迁移的对抗攻击脆弱性，以推动更高标准的鲁棒性评估和防御机制研究。

Method: 通过优化固定的弱扰动噪声模式（UTAP），在不依赖特定数据集或模型结构下加入到病理图像中，系统性破坏基础模型的特征表示，从而导致下游任务性能下降。

Result: 在多种最先进的病理学基础模型和多个数据集上，UTAP以视觉上不可察觉的固定噪声显著降低模型性能，展现了良好的跨视野、跨数据集与跨模型迁移性。

Conclusion: UTAP显著暴露了病理学基础模型在特征表示上的普遍脆弱性，能在视觉上难以察觉的情况下破坏多种模型的下游任务表现，因此对模型安全与可靠部署构成实质性威胁。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [60] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: HYDRA通过教师-学生混合知识蒸馏策略，显著提高了从RGB到高光谱的泛化重建精度（+18%）并加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度注意力方法对稀疏光谱表现良好，但在包含数百个通道的现代HSI传感器上效果受限；需提出能在未见场景下鲁棒恢复高维光谱的模型。

Method: 提出HYDRA架构：使用一个Teacher模型学习高光谱图像的潜在表征，Student模型学习将自然三通道图像映射到Teacher编码域；并结合混合知识蒸馏和特定损失函数进行训练，以增强泛化能力和重建精度。

Result: 在多个评估指标上达到了SOTA，性能提升约18%，并在不同通道深度下实现比现有SOTA更快的推理速度。

Conclusion: HYDRA通过教师-学生蒸馏框架和新的训练策略，有效提升了从三通道图像到高维HSI的泛化重建性能，解决了多尺度注意力方法在高光谱通道数下的局限性。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [61] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 通过Perception Agent提取语言对齐的3D信息（含SOG模块），并由Reasoning Agent闭环剪枝形成最小充分信息集，从而显著改善VLM的3D空间推理性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM受限于以2D为主的预训练，在3D空间理解和定位表述性方向（如面向、朝向）上存在不足；此外，多余或噪声的3D信息会干扰推理。因此需要一种能够筛选出回答问题所需的最少充分3D信息的方法。

Method: 提出MSSR双智能体框架：Perception Agent使用多功能感知工具箱（包括新颖的SOG模块）从专家模型中提取语言关联的方向和其他3D感知结果，Reasoning Agent对提取的信息进行迭代精简以达到最小充分性，通过闭环请求补充或剪除冗余信息直至完成MSS。

Result: 在两个挑战性基准上，MSSR显著提升了准确率并达成最先进性能，同时提供可解释的推理路径，能够作为高质量训练数据源。

Conclusion: 本文提出通过构建最小充分信息集（MSS）来提升视觉-语言模型在三维空间推理任务的表现，解决了2D预训练导致的3D理解不足和冗余3D信息导致的推理失败两个瓶颈。实验表明该方法在两个基准上实现了最先进成绩，并能生成可解释的推理路径。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [62] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: SDPA++：使用自融合生成伪真值并结合patch级别模型集成的自监督OCT去噪框架，免标注数据，在VIP Cup真实噪声数据上显著提升图像质量指标。


<details>
  <summary>Details</summary>
Motivation: 获取干净-真实噪声配对数据困难（斑点噪声、临床采集限制），因此需要仅依赖噪声图像的自监督去噪方法来提高OCT图像质量以支持临床诊断。

Method: 首先利用自融合(self-fusion)和自监督去噪生成伪真值图像；随后用这些伪真值作为目标，训练多个去噪模型的ensemble，并采用基于patch的策略对输出进行聚合以增强细节和降低噪声。

Result: 在IEEE SPS VIP Cup真实噪声OCT数据集上，方法在CNR、MSR、TP、EP等指标上均有提升，证明了仅用真实噪声图像也能改善图像清晰度和边缘/纹理保留，利于临床应用。

Conclusion: 该论文提出了一种基于自监督和patch聚合的OCT图像去噪通用框架SDPA++，只需纯噪声图像即可生成伪真值，训练去噪模型并通过patch策略集成提高图像质量。

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [63] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 提出DCCL，通过增强跨域类内连通性（激进数据增强、跨域正样本、模型锚定及生成式变换损失）解决将对比学习直接用于域泛化时的性能下降问题，在多项基准上取得更好泛化效果并开源代码。


<details>
  <summary>Details</summary>
Motivation: 对比学习本应通过学习类分离表示来提升域泛化，但直接将CL应用于DG反而会退化性能。论文通过CL理论分析指出这是由于DG情形下类内样本跨域连通性不足，导致表示在未见域上不连贯。于是提出增强跨域连接性的方案以改善泛化。

Method: 在数据层面，引入更激进的数据增强与跨域正样本对以改善类内连通性；在模型层面，提出模型锚定（model anchoring）利用预训练表示中的类内信息来引导学习，并结合生成式变换损失以补强表示对未见域的嵌入能力。整体以对比学习框架为基础，强调连接不同域内同类样本。

Result: 在五个标准DG基准上进行大量实验，结果显示DCCL优于最先进基线方法，即使在无域监督下也能取得更好性能。论文还开源了实现代码。

Conclusion: 该论文提出了Domain-Connecting Contrastive Learning (DCCL)，通过增强跨域类内连通性来提升对未见目标域的泛化性能，实验证明在五个DG基准上优于现有方法，且可在无域标签的情况下工作。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [64] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM 用一致性模型和 Transformer 实现一步人类动作预测，保持或超越扩散模型精度，但推理速度快很多。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型虽然效果好，但需要多步去噪推理，计算开销大，推理慢；因此希望设计一个能在保持精度的同时大幅减少推理步数的单步生成方法。

Method: 采用基于 Transformer 的时空架构，使用时间嵌入来建模长程依赖并保持动作连贯性；基于一致性模型训练单步映射，替代扩散模型的多步去噪流程。

Result: 在 Human3.6M 和 HumanEva-I 数据集上的实验表明，HumanCM 在准确性上与最先进的扩散模型相当或更优，同时将推理步数减少了最多两个数量级。

Conclusion: HumanCM 提出了一种基于一致性模型的一步人类动作预测方法，通过学习噪声与干净动作状态之间的自一致映射，实现单步高效生成，从而显著减少推理步骤。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [65] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 首次将Chain-of-Thought引入3D场景理解，提供了SCENECOT方法和185K有根CoT数据集，显著提升了有根问答性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有3D大模型在有根问答方面表现不足，主要因为缺乏对人类式场景-物体有根推理机制的探索。作者希望通过引入CoT和构建大规模有根数据集来弥补这一空白，提升3D场景问答的可解释性与准确性。

Method: 提出SCENECOT方法：在3D场景中引入有根的Chain-of-Thought推理，先将复杂问题分解为若干子任务，再通过多模态专家模块（如目标检测、语义分割、关系推断等）生成相应视觉线索，最后基于这些线索逐步推理得到答案。同时构建了大规模数据集SCENECOT-185K以支持训练和评估。

Result: 在多个复杂3D场景推理基准上进行大量实验，结果表明该框架在性能和grounding-QA一致性上表现突出，证明这是CoT首次成功应用于3D场景理解，能实现类似人类的逐步推理并具备扩展到更广场景的潜力。

Conclusion: 该论文提出了将Chain-of-Thought（CoT）推理方法应用于3D场景理解的首创框架，通过对复杂推理任务进行分解并结合多模态专家模块构建视觉线索，实现了场景-物体级的有根问题回答，从而提高了模型的推理透明性和可解释性。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [66] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM通过以残差形式在BEV空间预测未来变化并用对齐模块减少误差累积，提升了占据预测与规划表现，避免了重建静态背景的低效。


<details>
  <summary>Details</summary>
Motivation: 传统端到端驾驶世界模型在预测未来场景时浪费大量容量重建静态背景，降低对动态变化的建模能力与规划相关性。

Method: 从视觉观测构建当前时刻的BEV表示，以上一步BEV作为时间先验，只预测受自车动作与场景影响的变化残差；加入对齐模块校正语义和动态失配；评估不同预测—规划耦合方案。

Result: 在nuScenes基准上，IR-WM在4D占据预测和轨迹规划任务上取得了领先性能，显示出隐式未来状态对规划精度的显著提升。

Conclusion: 提出了一种隐式残差世界模型IR-WM，通过只预测BEV特征的残差并利用上一步BEV为先验，避免重建静态背景，提高了时序预测和规划效果。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [67] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: 提出UKANFormer：在解码器引入GL-Trans以融合全局与局部信息，在噪声监督下实现比训练标签更精确的珊瑚礁分割，IoU=67.0%、像素精度=83.98%。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁映射产品（如Allen Coral Atlas）覆盖广但空间精度与语义一致性不足，尤其在细粒度边界划分上有局限，需能在噪声标注下实现高精度分割的模型。

Method: 在UKAN基础上于解码器中加入Global-Local Transformer (GL-Trans)模块，融合全局语义与局部边界信息以抵抗噪声监督。

Result: 在噪声标签设置下，UKANFormer实现珊瑚类别IoU 67.00%和像素精度83.98%，优于常规模型，并在视觉/结构上超越训练用噪声标签。

Conclusion: UKANFormer能在噪声标注（来自Allen Coral Atlas）的条件下，通过架构改进实现比训练标签更准确的珊瑚礁语义分割，从而支持大尺度生态监测。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [68] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 这篇综述构建了化身AI世界模型的统一框架与三轴分类，系统整合数据与评测，评估现有方法并强调物理一致性、实时性与长时序一致性等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 化身AI需要能够预测和推演动作如何改变未来世界状态，世界模型作为内部模拟器对感知、预测与决策至关重要，但该领域方法多样需统一框架与评估。

Method: 通过形式化问题与学习目标，提出三轴分类（功能性、时间建模、空间表示），并对比现有方法，整理跨领域数据资源与指标，给出定量比较与挑战总结。

Result: 提出三轴分类细分方法维度，系统汇总数据与评测指标，定量比较最先进模型，并指出数据稀缺、评测偏向像素而非物理一致性、性能与效率权衡等未解决问题。

Conclusion: 本文综述了化身AI中世界模型的统一框架，提出三轴分类并系统化数据集与评测，指出若干关键挑战，例如评价指标、计算效率与长时序一致性问题。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [69] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 离散自回归图像模型结合束搜索能在推理时显著提升生成效果，表明架构而非单纯放大参数对搜索优化至关重要。


<details>
  <summary>Details</summary>
Motivation: 动机是：虽然推理时的搜索显著提升了大型语言模型，但将同样的策略移植到图像扩散模型效果有限，作者探寻是否其它图像生成架构（如离散自回归）能更好利用搜索改进生成质量。

Method: 作者在离散自回归视觉模型上应用束搜索，并与扩散模型和随机采样方法做系统比较与消融实验；通过早期剪枝和计算重用说明离散令牌空间带来的效率；还用验证器分析速度与推理能力间的权衡。

Result: 结果显示：束搜索显著提升文本到图像生成质量；在基准上，一个2B参数的自回归模型通过束搜索能超越12B参数的扩散模型；消融展示收益来源于离散令牌空间的早期剪枝与计算重用；验证器分析揭示速度与推理能力的权衡。

Conclusion: 本文结论是：在图像生成中，离散自回归模型通过束搜索能够在推理时有效利用搜索策略，从而在性能上超越更大参数的扩散模型，表明模型架构对推理时优化至关重要。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [70] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 提出首个基于人类标注的SR伪影显著性数据集（1302例），并训练轻量回归器生成显著性热图，能更精确地检测和量化对人类观察者有影响的伪影。


<details>
  <summary>Details</summary>
Motivation: 随着SR模型能力提升，生成的伪影增多且影响不一，需以人类感知为准来评估伪影的显著性，从而更合理地评价和缓解伪影。

Method: 构建一个包含1302个伪影实例的数据集，涵盖11种现代图像SR方法；通过众包为每个伪影获得显著性评分；基于此训练轻量级回归器，生成空间显著性热图用于检测显著伪影。

Result: 所训练的回归器在检测显著伪影任务上优于现有方法；同时公开数据集和代码以推动显著性感知的评估与缓解研究。

Conclusion: 本文提出通过人类显著性来度量SR（超分辨）模型生成伪影的重要性，认为伪影不应被视作二元缺陷，而应按对观察者的显著性分级。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [71] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR通过GMWTConvs扩大感受野、MCAM建模通道依赖、以及MTELoss约束纹理恢复，目标是提升图像恢复中细节与纹理的重建效果，并在效率上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的方法受限于较小感受野和缺乏通道特征建模，导致恢复结果在细纹理和细节上表现不足。作者旨在通过扩展感受野与通道依赖建模来提升重建精细纹理的能力。

Method: 提出了三项关键技术：1) 全局多尺度小波变换卷积（GMWTConvs）用于扩展模型感受野并保留多尺度纹理特征；2) Mamba-Based通道感知模块（MCAM）用于捕捉通道间长距离依赖，增强对颜色、边缘和纹理的敏感性；3) 多尺度纹理增强损失（MTELoss）用于在训练中显式指导模型保留细节纹理结构。

Result: 作者宣称WaMaIR在大量实验中优于最先进方法，在图像恢复质量（细节和纹理保留）和计算效率上取得改进，但摘要未给出具体数值或基准数据。

Conclusion: 本文提出的WaMaIR框架通过扩大感受野并进行通道感知建模，有助于改善图像恢复中的纹理细节重建，实验表明在性能和效率上优于现有方法。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [72] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 引入‘区域—全局’双层语义对齐，用场景与区域级文本描述指导局部编辑，从而提升图像编辑的一致性与语义正确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的局部图像编辑往往仅依赖局部线索，忽略各部分相互关系和整体语义，导致编辑不一致或过渡不自然。为此提出一种能让区域理解其在全图中角色的方法。

Method: 提出双层引导机制：1) 区域表示结合全图上下文并与详细的区域级描述对齐；2) 全图与由大视觉语言模型生成的场景级描述对齐。使用这些文本描述作为显式参考，驱动局部修改与全局结构保持一致。

Result: 在实验中，与基线方法相比，Region in Context在编辑一致性、语义对齐性和自然过渡上表现更好，产生更和谐的修改。作者公开了代码。

Conclusion: 本文提出Region in Context框架，通过多层次语义对齐实现基于文本的图像局部编辑，强调区域在整体场景中的角色以提升编辑一致性与自然过渡。实验表明该方法能产生更连贯、符合指令的编辑结果。

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [73] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: EMRRG利用对预训练Mamba网络的参数高效微调（Partial LoRA）、SSM视觉骨干与混合解码器LLM，实现了端到端的X光医学报告生成，并在三大数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖LLM或对视觉编码器微调不足，忽视了非Transformer视觉架构（如Mamba）和跨注意力机制改进的潜力，本文旨在探索这些方向以提升X光影像报告生成性能。

Method: 将X光图像分块并tokenize，利用SSM（状态空间模型）视觉骨干提取特征，采用Partial LoRA对Mamba网络进行参数高效微调；随后用带混合解码器的LLM生成报告，支持端到端训练。

Result: 在三个常用基准数据集上进行了大规模实验，证明Partial LoRA与SSM-based Mamba骨干结合混合解码器LLM的策略能显著提升MRG性能（具体度量结果文摘中未给出）。

Conclusion: 本文提出了EMRRG框架，通过对预训练的Mamba网络进行参数高效微调（例如Partial LoRA），结合SSM-based视觉主干与混合解码器LLM，实现了端到端X光影像报告生成，且在三大基准数据集上验证了有效性。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [74] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE用可微渲染与Lie代数驱动的BA式迭代优化并联合更新颜色参数，提升了纹理缺失与光照变化场景下的6D位姿估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于2D-3D特征对应的方法在无纹理物体和光照变化场景下表现欠佳，作者希望通过直接利用渲染误差进行可微优化，减少对明确特征对应的依赖，提高鲁棒性。

Method: 作者将位姿回归问题类比成Bundle Adjustment，构建了一个基于3D形状模型（3DGS）的可微渲染流水线。利用Lie代数参数化位姿变换，使得渲染过程对位姿可微分，并通过输入图像与渲染图像之间的差异迭代优化位姿。同时对3DGS模型的颜色参数进行联合更新以适应光照变化。

Result: 在三个公开基准上取得了较之前模型分别为1.4%、2.8%和2.5%的绝对精度提升，表明方法在复杂条件下具有更好的估计能力。

Conclusion: 本文提出的GS2POSE通过引入基于Lie代数的姿态可微渲染和颜色参数更新，有效提升了在纹理缺失与光照变化情况下的6D位姿估计性能，实验证明在T-LESS、LineMod-Occlusion和LineMod数据集上均有显著提升。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [75] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出一个训练免费的视频理解管线：用预训练VLM提取语义轨迹，KTS分割，密度聚类找主题，选关键帧并用VLM生成描述，达成零样本视频结构化分析与摘要。


<details>
  <summary>Details</summary>
Motivation: 将在静态图像上表现出色的零样本推理能力扩展到视频领域，避免昂贵且不可扩展的端到端有监督训练，通过训练自由方法实现可解释、通用的视频结构化理解。

Method: 利用冻结的预训练VLM视觉编码器将视频帧映射为高维语义特征轨迹，应用Kernel Temporal Segmentation进行时序分割，再对分割片段做密度基聚类以发现宏观场景，从每簇选择代表性关键帧并用VLM生成文本描述来构建多模态摘要。

Result: 提出的训练自由管线能够在零-shot场景下对视频进行事件分割、场景聚类和多模态摘要生成，且方法可解释、模型无关并具备实用性。

Conclusion: 该论文提出了一个无需训练的基于预训练视觉语言模型（VLM）与经典机器学习算法结合的视频理解框架，通过语义特征轨迹、时序分割（KTS）和密度聚类实现视频结构化分析与摘要生成。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [76] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS通过在冻结的MLLM上添加可训练头，利用注意力图提取关键点生成与掩码解码器兼容的点特征，实现高效且不损害泛化性的像素级分割扩展。


<details>
  <summary>Details</summary>
Motivation: 当前为MLLMs加入分割能力常需微调模型输出，使之兼容掩码解码器，破坏输出空间并损害模型的通用性；因此需要一种不会影响原模型泛化性的插件式方案。

Method: 在冻结的MLLM上附加一个轻量可训练的头部，通过精炼注意力图的空间线索，提取关键点并将其描述为与掩码解码器兼容的点状特征，从而实现分割输出。

Result: 实验显示LENS在分割任务上与基于微调的方法相当或更优，并在保持MLLM原有泛化能力方面显著优于微调方法。

Conclusion: LENS能在不微调主模型的情况下，为MLLMs提供像素级分割能力，保持模型原有的泛化性并取得与微调方法相当甚至更好的分割性能。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [77] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出一种结合几何先验与时间一致性的完全无监督二值道路分割方法，利用弱标签与互信息约束精炼，Cityscapes上IoU=0.82。


<details>
  <summary>Details</summary>
Motivation: 降低对昂贵人工标注数据的依赖，实现可扩展的无监督道路分割模型，适用于自动驾驶场景。

Method: 1) 使用几何先验生成弱标签：地平线以上设为非道路，车辆前方预定义四边形设为道路。2) 通过跟踪局部特征点跨帧 enforcing temporal consistency，并利用互信息最大化惩罚不一致的标签来精炼标签，提升精度与稳定性。

Result: 在Cityscapes数据集上取得IoU=0.82，显示了简单设计下的高精度分割结果。

Conclusion: 该方法证明了在不依赖人工标注的情况下，仅通过几何先验和时间一致性即可实现高精度二值道路分割，具备良好的准确性和时序稳定性。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [78] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出PIF：利用扩散模型先验与文本反演，优化文本提示来提取并迁移摄影风格，兼顾风格可控性与内容保真。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么无法从参考图像中学习到有意义的摄影概念，要么在迁移过程中破坏内容图像的主体结构，因此需要一种能提取摄影概念并在保留内容的前提下进行可控迁移的方法。

Method: 基于预训练的text-to-image扩散模型作为生成先验，PIF先学习摄影概念的平均外观和与文本提示的调节方式，再通过文本反演（优化提示词）从参考图像中学习具体摄影风格，最终应用优化后的提示完成风格迁移。

Result: PIF在各种摄影风格提取与迁移任务中表现优异，能够提取并转移多种摄影风格，同时保持内容一致性。

Conclusion: PIF通过结合预训练文本到图像扩散模型与文本反演技术，实现了可控且个性化的摄影风格迁移，能有效提取参考图像的摄影概念并在保留内容的同时进行风格转换。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [79] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 公开了一个多品种、多场景的荔枝检测与成熟度分类数据集（11,414张、9,658标签对），具备严格标注流程和基线模型评测，对采摘机器人研究有重要价值。


<details>
  <summary>Details</summary>
Motivation: 缺乏一致且全面标注的公开荔枝数据集，阻碍了基于视觉的采摘机器人和果实成熟度识别研究与应用。

Method: 在多品种、多天气、多时间段采集RGB与深度图像，进行了数据增强，采用三人独立标注加一人复核的聚合验证流程，并用三种代表性深度学习模型进行了基线实验评估。

Result: 构建了11,414张图像（878原始RGB、8,780增强RGB、1,756深度），包含9,658对检测与成熟度标签，覆盖Nuomici、Feizixiao、Heiye、Huaizhi等品种，并提供统计分析与基线实验结果。

Conclusion: 本文构建并公开了首个系统性、包含成熟度标注的荔枝自然生长环境图像数据集，为视觉采摘机器人研究提供关键数据资源。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [80] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个映射到WoRMS的约92.5万属级珊瑚点标注的全球性数据集，提供源内与跨源基准。监督学习源内效果好，但跨域和零-shot表现差，数据集旨在推动域泛化与细粒度珊瑚分类研究。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁由于气候变化等人为压力快速衰退，亟需可扩展、自动化的监测工具；现有数据集规模、地理范围或标签细粒度不足，且不易用于机器学习。

Method: 汇集来自76个Curated CoralNet来源和红海Al Wajh站点的图像，构建约92.5万条属级硬珊瑚点标注，并将标签映射到WoRMS；提出两种评测设置：源内（within-source）和跨源（cross-source）基准；对监督学习和零样本分类进行分析和评估。

Result: 构建了大规模、全球性的细粒度珊瑚图像数据集ReefNet并公开；实验表明监督模型在源内表现良好但跨域性能显著下降，零样本模型整体性能较低，尤其在稀有属和视觉相似属上。

Conclusion: 本文提出了ReefNet数据集以支持大规模、自动化的珊瑚礁监测，结论是尽管在源内监督学习表现良好，但跨域泛化和零样本识别仍然表现差，特别是对于稀有和视觉相似的属。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [81] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提取与融合五类纹理特征，并结合AdaptMoist域自适应与AMI保存准则，可在多源木屑数据上稳定、准确地预测含水率，适用于木屑相关工业。


<details>
  <summary>Details</summary>
Motivation: 现有直接法耗时且破坏样品，现有间接快速方法在来源多样时准确性下降，需解决源域差异对数据驱动模型性能的影响。

Method: 从木屑图像中提取五类纹理特征（单独评估及组合）；提出AdaptMoist域自适应方法基于纹理特征进行特征对齐与知识迁移；引入基于调整互信息(AMI)的模型保存准则。

Result: 组合特征在同域达到95%准确率；AdaptMoist将跨域平均准确率从57%提升到80%，提升约23%。

Conclusion: 基于五类纹理特征的组合能高效预测木屑含水率，并通过域自适应显著提高跨源鲁棒性。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [82] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo，通过姿态感知头编码器、DDIM像素镜像损失和分布感知适配器解决人台到真人视频生成中的头-体不对齐与身份漂移，实现更真实一致的换装视频生成。


<details>
  <summary>Details</summary>
Motivation: 在线时尚展示中，人台展示成本低但缺乏真实感和细节，目标是将人台视频转换为可控身份的高保真真人视频以提升展示效果。

Method: 方法包括：1）动态姿态感知头部编码器，将面部语义与身体姿态融合以生成跨帧一致的身份嵌入；2）在像素空间引入镜像损失，利用DDIM一阶去噪恢复细致人脸细节；3）设计分布感知适配器，对齐身份与服装特征的统计分布以增强时间一致性。

Result: 在UBC、ASOS和新采集的MannequinVideos三套数据集上，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有最先进方法。

Conclusion: 该文提出了M2HVideo，一个针对人台到真人视频生成的框架，通过融合面部语义与身体姿态、镜像损失和分布自适应适配器来缓解头部-身体不对齐和身份漂移问题，实现了更好的服装一致性与身份保持。

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [83] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 提出2DGS-R：通过法线一致性正则化+原地克隆+不透明度冻结的分层训练，低开销提升2D Gaussian Splatting的渲染质量并保持几何精度。


<details>
  <summary>Details</summary>
Motivation: 原始3D/2D Gaussian Splatting在渲染质量与几何精度间存在权衡，单阶段训练难以兼顾两者；需要分层策略分别优化渲染和几何。

Method: 先用法线一致性正则化训练原始2D高斯盘，挑选渲染欠佳的高斯盘并通过原地克隆操作增强，最后在固定不透明度下微调模型。

Result: 相比原始2DGS，仅增加约1%存储与极少训练时间，即可获得更高渲染质量并保持细致几何结构，提升视觉保真度和几何重建准确性。

Conclusion: 2DGS-R通过分层训练显著提升渲染质量的同时保持几何精度，证明了在小额存储和训练开销下能平衡渲染与几何恢复的矛盾。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [84] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer通过CBAM+MixVisionTransformer实现高效轻量的多类武器语义分割，兼顾精度与实时性，适合边缘安全设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测仅给出边界框，不足以支持精细的威胁评估；现有分割模型要么精度高但资源消耗大，要么轻量但精度不足，需在准确性与效率间取得平衡以用于实时边缘部署。

Method: 在MixVisionTransformer编码器中集成CBAM以增强通道与空间注意力，并采用一个集成注意力的hamburger解码器进行多尺度特征融合，针对五类（手枪、步枪、刀、左轮、人体）执行语义分割。

Result: 在实验中，ArmFormer达到了80.64% mIoU和89.13% mFscore，实时推理速度82.26 FPS，计算量4.886G FLOPs，参数3.66M，显著优于需高达48倍计算量的重型模型。

Conclusion: ArmFormer是一种针对武器实例的轻量级Transformer分割框架，通过将CBAM与MixVisionTransformer结合，在保证低计算量的同时提升分割精度，适合边缘设备部署。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [85] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL通过同时在表示和标签空间进行双向对齐（DPR+PCBC与区域/实例匹配），改善半监督医学图像分割并取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 当前SSMIS方法只关注标签空间一致性，忽视表示空间对齐，导致潜在特征不具判别性或空间不连贯。

Method: 构建双分支协作框架：标签空间通过Dual-Path Regularization(DPR)和Progressively Cognitive Bias Correction(PCBC)实现多尺度跨分支一致性并缓解误差累积；表示空间通过区域级和病灶实例级匹配对齐复杂病变特征。

Result: 在四个公开基准和一个专有CBCT数据集上，BARL持续超越最先进方法；消融实验验证各组件有效性。

Conclusion: 提出的BARL能在表示空间和标签空间同时对齐，提高半监督医学图像分割性能，实验证明优于现有方法。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [86] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 将点云配准与内存库异常检测联合学习，得到旋转不变且局部判别性强的特征，从而提高3D点云异常检测的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于内存库的方法在特征变换一致性和判别力方面不足，尤其难以捕获局部几何细节与实现旋转不变；配准失败时性能严重下降，因此将配准作为表征学习的引导可提升鲁棒性与检测可靠性。

Method: 设计一个‘配准诱导的旋转不变特征提取’框架，将配准目标和内存库式异常检测目标一并嵌入特征学习过程，通过学习局部几何结构与样本间特征相似性来同时优化对齐与表示；可能包含基于局部特征聚合、旋转不变描述子和内存库比对的模块，并在训练中引入配准损失与重构/对比损失。

Result: 在Anomaly-ShapeNet和Real3D-AD两个数据集上进行大量实验，结果显示该方法在效果与泛化能力上均优于现有方法，尤其在旋转场景和注册困难情况下表现更稳健。

Conclusion: 该文提出将点云配准与特征提取联合优化，以获得旋转不变且局部判别性强的特征，从而提升基于内存库的3D点云异常检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [87] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 本文提出神经元级别的fMRI-AN对应分析，揭示VLMs在多模态处理上与人脑存在细粒度类脑机制，且不同架构表现出不同的模态专长与跨模态整合特性。


<details>
  <summary>Details</summary>
Motivation: 弥补现有研究的两项不足：单模态ANN无法反映大脑的多模态处理能力；多模态ANN研究多关注高层输出，忽视个别神经元的作用。

Method: 结合细粒度人工神经元分析与fMRI体素编码，比较两种不同架构的VLMs（CLIP与METER），分析AN对BN（生物神经元/体素）活动的预测能力、功能冗余、极性模式及架构影响。

Result: 发现ANs能跨语言、视觉、注意、默认模式等功能网络预测BN活动；AN与BN存在表征重叠与功能冗余；AN呈现与BN相似的极性（相反激活）模式；不同VLM架构导致不同的BN驱动：CLIP呈模态专一性，METER呈跨模态统一激活。

Conclusion: 作者提出了一种基于神经元级别的分析框架，将人工神经元（AN）与fMRI体素编码关联，用于研究视觉-语言模型（VLMs）中多模态信息处理与人脑活动的对应关系，得出VLMs在神经元层面具有类脑分层处理特性。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [88] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff：在扩散模型中集成分类器以实现更可控的皮肤镜图像生成与更强的分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的类条件生成模型在生成特定医疗类别（如皮肤癌类型）的图像时控制力不足，限制了其在诊断等应用中的实用性，因而需要一种能同时生成和分类、并提升类别可控性的模型。

Method: 在扩散模型中嵌入一个分类器，利用分类器的预测指导生成过程（class-conditioned guidance），使模型在采样时更好地遵循目标类别，从而产生更真实、多样性的皮肤镜图像；同时训练使分类器获得改进的下游诊断能力。

Result: Class-N-Diff在合成图像的真实性与多样性上优于传统类条件扩散模型，并且内置分类器在诊断任务上表现更好（作者在代码库中提供实现）。

Conclusion: Class-N-Diff通过在扩散模型中集成分类器，实现了同步生成与分类皮肤镜图像，从而提高了类条件图像合成的可控性与真实性，并提升了分类性能。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [89] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: 提出Edit-R1后训练框架：用DiffusionNFT进行无似然策略优化，借助MLLM作为通用奖励并配合组过滤机制，提升指令图像编辑泛化与性能，取得SOTA结果并具有模型无关性。


<details>
  <summary>Details</summary>
Motivation: 监督微调导致模型易过拟合标注模式，限制泛化与探索能力；缺乏通用奖励模型使基于策略的优化困难。

Method: 提出Diffusion Negative-aware Finetuning（DiffusionNFT）作为无似然性策略优化方法，与流匹配正向过程一致，可使用高阶采样器并提高训练效率；引入MLLM作为训练无关的统一奖励模型，利用输出logits提供细粒度反馈；设计低方差组过滤机制以降低MLLM评分噪声并稳定优化。

Result: 在ImgEdit和GEdit-Bench上取得SOTA，得分4.49和7.83；框架对多种基模型（如Qwen-Image-Edit、FLUX-Kontext）均有显著性能提升，且代码与模型公开。

Conclusion: 本文提出基于策略优化的Edit-R1后训练框架，通过DiffusionNFT和MLLM奖励模型解决过拟合与奖励缺失问题，有效提升指令驱动图像编辑表现。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [90] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 利用地面可见光相机序列与航班/气象数据，提出模块化概率归属框架，为凝结尾迹到航班的归属提供强基线与可扩展平台。


<details>
  <summary>Details</summary>
Motivation: 非CO2效应中凝结尾迹对航空气候影响显著，但现有卫星归属受限于分辨率和尾迹漂移。地面相机能在尾迹早期高时空分辨率下捕捉薄而线性的尾迹，便于归属验证模型并校准物理模拟。

Method: 使用GVCCS数据集的地面相机序列，构建理论尾迹（基于飞机监视与气象资料），设计多种几何表示与距离度量，并加入时间平滑与概率分配策略，实现从观测尾迹到理论尾迹的匹配与归属。

Result: 提出的框架为凝结尾迹-航班归属提供了强基线，支持不同几何/距离度量和概率分配策略，能在GVCCS上进行评估并为后续优化（例如更复杂相似性度量、机器学习匹配器）奠定基础。

Conclusion: 本文提出并评估了基于地面可见光相机的视频序列将观测到的飞机凝结尾迹(contrails)追溯到产生航班的模块化框架，证明该方法在凝结尾迹形成后短时间内具备较高可辨识性，能够作为卫星方法的有益补充。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [91] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: ViT-based models, especially SegFormer, achieve high accuracy and favorable speed-accuracy trade-offs for thermal weapon segmentation on a 9.7k SAM2-annotated dataset.


<details>
  <summary>Details</summary>
Motivation: CNNs struggle with long-range dependencies and fine structural details in thermal imagery; ViTs may offer better global context modeling for improved segmentation under low-light/occlusion.

Method: Adapted four transformer-based models (SegFormer, DeepLabV3+, SegNeXt, Swin) for binary thermal weapon segmentation, trained on a custom 9,711-image thermal dataset annotated via SAM2, using MMSegmentation augmentations and standard training protocols for fair comparison.

Result: SegFormer-b5 achieved best mIoU 94.15% and Pixel Accuracy 97.04%; SegFormer-b0 fastest (98.32 FPS) with mIoU 90.84%; SegNeXt-mscans balanced 85.12 FPS and 92.24% mIoU; DeepLabV3+ R101-D8 reached 92.76% mIoU at 29.86 FPS. Overall robust generalization shown.

Conclusion: Transformer-based architectures, especially SegFormer variants, significantly improve thermal weapon segmentation over traditional methods, offering strong accuracy and speed trade-offs suitable for surveillance.

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [92] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 提出Res-Bench基准与新的鲁棒性指标（Spearman, ACE, RCE），系统评估MLLM对图像分辨率的稳定性，比较预处理与微调策略，发现不同模型在分辨率变化下表现差异大且微调可提升稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注语义性能，忽视模型在输入分辨率变化下性能是否稳定——即分辨率鲁棒性，这在实际多模态应用中非常重要。

Method: 构建包含14400样本、12个分辨率级别和6个能力维度的基准集；设计新的评价框架，引入Spearman相关、ACE和RCE等鲁棒性指标；对主流MLLMs进行大规模评测，并比较预处理（填充、超分辨率）与微调方法的影响。

Result: 通过指标分析发现：不同模型和任务在分辨率敏感性上差异大；简单的预处理（如超分辨率）能在一定程度上改善稳定性，但可能引入偏差；针对性微调可显著提升鲁棒性。提出的Res-Bench和指标为后续研究提供了统一评估工具。

Conclusion: 本文提出Res-Bench，系统评估多模态大模型在不同图像分辨率下的鲁棒性，发现现有MLLMs在分辨率变动下表现不稳定且差异显著。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [93] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 本文综述了医用图像领域的大模型，分类讨论视觉与视语模型的训练与应用，做了定量元分析，指出主要挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前医用图像大模型研究碎片化，缺乏系统性综述来梳理模型架构演进、训练范式与临床应用之间的关系，亟需一篇结构化的综述为研究者和临床从业者提供路线图。

Method: 按架构将研究分为纯视觉FMs和视觉-语言FMs，分析各自的训练范式（自监督、监督、跨模态对齐）、数据集规模与类型，并对下游任务（分割、检测、诊断、报告生成等）进行分类汇总。并基于文献做了定量元分析，统计数据集使用频次、发表时间趋势及应用分布。

Result: 总结出视觉与视觉-语言FMs各自优劣与适用场景；量化分析显示近年数据集规模和跨模态研究显著增长；提出若干关键挑战与解决方向，并列出未来研究重点如高效微调、可解释性、跨机构泛化与法规合规。

Conclusion: 本综述系统性梳理了医用图像领域的大模型（foundation models, FMs）研究进展，指出其在零样本和少样本任务上表现优异，但在领域适配、计算资源、可解释性和法规合规等方面仍存在挑战。作者呼吁结合联邦学习、知识蒸馏和高效微调等技术，推进可解释性与临床集成的研究。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [94] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出将扩散蒸馏视为Bregman散度密度比匹配的Di-Bregman框架，理论统一多种目标并提升了一步采样质量。


<details>
  <summary>Details</summary>
Motivation: 缓解扩散/流模型多步采样的高计算代价，找到统一且有理论依据的蒸馏目标以训练高质量的一步生成器。

Method: 通过将蒸馏目标构造成对密度比的Bregman散度最小化，构建紧凑的凸分析理论，推导出与现有目标（如reverse-KL）的一致性或差异，并基于该目标训练单步学生生成器。

Result: 在CIFAR-10和text-to-image任务上，Di-Bregman在一阶采样下比reverse-KL蒸馏获得更好的FID，并在视觉保真度上接近教师模型。

Conclusion: Di-Bregman提供了将扩散蒸馏问题统一为基于Bregman散度的密度比匹配的理论框架，证明了多种现有目标可以在该框架下解释，并在图像生成任务上实现了在一步采样下的指标提升。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [95] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出CARE，通过序列-图像对比对齐与联合分类，融合时间与空间信息，显著提升事件触发传感器的ADL识别性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列的方法保留时间顺序但对噪声敏感且缺乏空间感知；基于图像的方法捕捉全局与隐式空间相关性但损失细粒度时间动态并扭曲传感器布局；简单融合无法强制两种视图对齐，导致互补信息未被充分利用。

Method: 提出端到端框架CARE，包含时间感知且抗噪的序列编码、空间敏感且频率敏感的图像表示，以及序列-图像对比对齐（SICA）与交叉熵分类的联合目标，统一训练以获得对齐且具判别力的嵌入。

Result: 在三个CASAS数据集上达到SOTA：Milan 89.8%、Cairo 88.9%、Kyoto7 73.3%，并在传感器故障和布局变化下表现出鲁棒性。

Conclusion: CARE通过对序列和图像两种表征进行对比对齐与分类联合优化，成功整合了时间和空间信息，提高了ADL识别的鲁棒性与准确性。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [96] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 利用大多模态模型实现无需训练的在线视频步骤定位，并通过贝叶斯滤波（依赖矩阵+进度估计）提升准确性，优于现有训练型离线方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频步骤定位依赖昂贵的标注及离线整段视频处理，不适合需要在线决策或训练数据匮乏的场景；利用LMM的零样本能力有望实现无需训练的在线VSG。

Method: 首先对输入视频的窗内若干帧使用LMM进行零样本步骤识别；随后提出BaGLM框架，通过（1）用大语言模型提取步骤转移的依赖矩阵；（2）估计步骤进度；结合贝叶斯滤波对当前预测与历史状态进行概率融合，从而实现在线逐帧/窗口的步骤判定。

Result: 在三个数据集上的实验表明，基于LMM的在线零样本策略已经超越了许多需训练的离线模型；BaGLM进一步通过注入历史依赖和进度估计，取得了比最先进训练型离线方法更好的性能。

Conclusion: 本文提出一种无需训练且能在线运行的视频步骤定位方法，利用大多模态模型（LMM）对有限帧进行零样本推断，并通过贝叶斯滤波融入历史信息，显著优于现有需训练的离线方法。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [97] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 研究发现视频特征对时序视频定位影响大，不同编码器造成性能和错误模式差异，支持探索特征互补与融合策略以避免架构过拟合。


<details>
  <summary>Details</summary>
Motivation: 当前研究对少量视频表示集中，可能导致架构过拟合；需要系统评估不同视频特征对同一定位架构的影响。

Method: 在经典时序视频定位模型上，使用多种视频编码器（基于CNN、时序推理模块和Transformer）提取特征，并在三个基准数据集（Charades-STA、ActivityNet-Captions、YouCookII）上比较性能与错误分布。

Result: 通过实验发现，不同视频编码器生成的特征显著影响定位性能，存在可观察的错误模式和性能差异，表明某些特征在时间建模或场景表征上有短板，同时提示特征融合可能提高鲁棒性。

Conclusion: 本文表明视频特征选择对时序视频定位性能影响显著，同一模型在不同特征上性能波动大，并揭示某些特征会导致特定错误模式，提示特征间可能互补。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [98] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 针对低分辨率泛化的基准和在MillionAID上自监督训练的iBOT表明：小尺度下遥感专用基础模型并不一定比通用视觉模型更好。


<details>
  <summary>Details</summary>
Motivation: 探讨是否有必要训练专用的遥感基础模型，因遥感图像具备不同特性和对鲁棒性的特殊需求。

Method: 设计了一个评估遥感模型对低分辨率图像泛化能力的基准测试（两个下游任务），并在MillionAID数据集上对iBOT（自监督视觉编码器）进行训练，且加入若干遥感特定的修改。

Result: 在所设计的基准和实验设置下，专用预训练模型在ViT-B规模上未能持续优于通用预训练基线。

Conclusion: 作者论证：在小规模（ViT-B）下，专门为遥感设计的基础模型并不显著优于通用视觉基础模型。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [99] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG利用多模态LLM生成富化查询并结合轻量回归解码器与多实例学习，提升时序视频定位效果，兼具SOTA性能与强零样本能力。


<details>
  <summary>Details</summary>
Motivation: 直接用LLM进行定位会受限于信息缺失与幻觉，故引入查询富化以补充细节，再用专门解码器精确回归时间边界，同时用多实例学习减小噪声影响。

Method: 两阶段流程：1）用多模态LLM将原始文本查询扩展成包含缺失细节的富化句子；2）用轻量解码器基于富化句子的上下文化表示预测精确时间边界。训练时采用多实例学习目标动态选择最优查询版本以抑制噪声和幻觉。

Result: 在多个时序视频定位和段落定位基准上取得SOTA，优于所有既有基于LLM的方法，并在零样本评估中明显领先于专用模型或至少相当。

Conclusion: ED-VTG通过将多模态大模型用于生成富化查询并用轻量解码器回归时间边界，实现细粒度视频时序定位，取得SOTA和优异零样本能力。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [100] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: W2R2通过训练中分离“What/Where”表示并加入对齐与伪标签损失，抑制2D快捷方式，提升3D目标定位的准确性与稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态3D定位模型过度依赖易获得的2D图像特征进行粗略定位，忽视3D几何信息，导致融合效果受限和定位精度下降，需要通过训练层面抑制快捷路径（shortcut）以增强3D理解。

Method: 提出W2R2训练框架，核心思想是将2D特征作为“What”语义指示器、3D特征作为"Where"空间锚点；设计双目标损失，包括对齐损失（使用适配的交叉熵监督融合预测）和伪标签损失（基于边际机制惩罚过度依赖2D的伪输出）。

Result: 在ScanRefer与ScanQA数据集上，W2R2显著提升了定位准确率与鲁棒性，尤其在场景复杂、物体遮挡或室外混乱环境中表现更佳。

Conclusion: W2R2通过明确分离2D语义与3D空间表示并在训练阶段施加专门约束，有效缓解了VLMs中的“2D语义偏差”，在不改变推理结构下提升了3D定位性能。

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [101] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 提出基于条件StyleGAN2-ADA/StyleGAN3与CycleGAN的指纹合成流程，生成高质量按指身份标注的活体与多材料伪造指纹，构建两个合成数据集，并通过FID、识别率、质量指标与匹配测试证明了质量与隐私保护性。


<details>
  <summary>Details</summary>
Motivation: 大型真实指纹数据集获取成本高、耗时且受隐私法律限制，合成数据可降低成本、便于共享并保护个人隐私，同时为活体检测/伪装检测模型提供多样化训练样本。

Method: 使用条件StyleGAN2-ADA和StyleGAN3分别生成按十指身份标注的高清活体指纹图像，然后用CycleGAN将活体指纹域转换为不同材料（如EcoFlex、Play-Doh等）的伪造指纹，从而得到成对的活体与伪造样本。构建两个合成数据集（DB2、DB3），每个含1500张指纹覆盖十指和多次印象，并包含八种伪造材料。评估指标包括FID、TAR@FAR、NFIQ2、MINDTCT以及匹配实验用于隐私泄露检测。

Result: StyleGAN3在生成质量上表现最佳，FID低至5；生成指纹在识别系统上取得TAR=99.47%（0.01% FAR），StyleGAN2-ADA也达到98.67%。质量评估（NFIQ2、MINDTCT）显示合成指纹具备可用质量，匹配实验未发现显著身份泄露，证明合成数据具有较强的隐私保护特性。

Conclusion: 该论文提出了一种基于条件StyleGAN2-ADA/StyleGAN3与CycleGAN的指纹图像生成框架，能生成高分辨率、按手指身份条件控制的真人指纹及其对应的仿真假指纹，旨在解决数据收集成本与隐私问题。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [102] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 基于999例多中心CT数据的比较研究显示：VNet+半监督学习能提供高精度、放射组学稳定且被临床医师信任的肺肿瘤分割与预后预测，支持将AI作为‘初稿-医师修订’的工作流以推动临床落地。


<details>
  <summary>Details</summary>
Motivation: 手工分割时间长且可变性大，阻碍影像组学和基于影像的预后模型在临床的稳健应用；目标是构建一个有临床参与（clinician-in-the-loop）的DL流程，提高分割与下游预后模型的可重复性、准确性和临床信任度，促进AI在肺癌CT影像学中的可行落地。

Method: 使用来自12个公开数据集共999例多中心CT影像，比较5种3D分割模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），在整张图像与点击点裁剪图像上与专家轮廓做基准；通过497个PySERA提取的放射组学特征评估分割可重复性（Spearman相关、ICC、Wilcoxon、MANOVA）；在降维（38种）与分类器（24种）组合下比较监督学习与半监督学习在预后建模上的表现；并邀请6位临床医师从7个维度对自动掩模进行定性评估。

Result: VNet在各项指标中最优：Dice=0.83、IoU=0.71；放射组学稳定性（平均Spearman相关=0.76、ICC=0.65）；在SSL框架下预测性能最高（准确率=0.88、F1=0.83）；SSL普遍优于监督学习；放射科医师更青睐VNet生成的肿瘤及肿周表征与更平滑的边界，并倾向以AI掩模为初稿进行人工修订而非完全替代。

Conclusion: 本研究表明：在临床参与的深度学习流水线中，VNet结合半监督学习(SSL)在肺癌胸部CT肿瘤分割及预后建模上表现最佳，兼具高分割精度、放射组学稳定性与预测性能，并获得放射科医师对边界与肿周表现的偏好，支持以AI生成初始掩模供医师修订的工作流，有助推动以临床为中心的AI落地。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [103] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 通过不局限于类质心、可调数量的类代表选择策略，在检索阶段显著提升行人重识别的准确性与mAP，且可用于多种嵌入方法作为后处理提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注特征提取和目标函数改进，但在检索阶段如何选择更合适的类代表这一方向研究不足，且过去使用类质心的方法在检索效果上并非最佳。

Method: 在训练和检索阶段采用可调节数量的类表征（不仅限于质心），通过平衡精确率与mAP的选择策略生成每类的若干代表向量，应用于多个已有重识别嵌入方法上进行后处理检索。

Result: 在多种重识别基线嵌入上引入该代表选择方法后，检索性能（尤其在精确率与mAP权衡上）超越当代最优结果，并能根据应用需求调整每类表示数量。

Conclusion: 本文提出一种广义的类表示选择方法，通过不局限于类质心来选择表征，从而在检索阶段提升行人重识别性能，并在多种嵌入上均显著优于现有方法。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [104] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 利用输出熵信号在推理阶段无监督微调value cache的V-Reason方法，能有效改善LMM的视频推理行为，接近RL性能并大幅降低计算/生成开销。


<details>
  <summary>Details</summary>
Motivation: 当前基于LMM的视频推理依赖代价高昂的RL和冗长的链式思考，且缺乏对思考过程的有效控制；通过研究模型输出熵模式，发现高质量模型有可利用的微探索/微利用轨迹，因而可在推理阶段进行无监督调节。

Method: 提出V-Reason方法：在推理时基于熵的目标对小型可训练控制器进行少量优化步骤以调整LMM的value cache，无需RL或监督微调；该方法促使模型在思考阶段进行有控的微探索并在结束时显著降低熵以完成最终利用。

Result: 在多个视频推理数据集上，V-Reason在不进行任何训练的条件下，比基础指令调优模型显著提升，平均精度与RL训练模型差距缩小至0.6个百分点，同时输出token数比RL模型减少58.6%。

Conclusion: 本文提出利用模型输出熵作为信号，在推理阶段无监督地通过少量可训练控制器参数、对价值缓存进行优化来调节LMM的微探测/微利用行为，从而提升视频推理性能并显著减少生成长度。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [105] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 通过固定编码器并插入可训练的轻量neck，比对通用Hiera与专用SAM2的下游适应性，发现SAM2虽在空间任务更强，但牺牲了对语义/概念任务的通用性；每一级适配增加表征瓶颈，量化了专门化的代价。


<details>
  <summary>Details</summary>
Motivation: 理解通用基础视觉模型与其专用变体在特征编码设计上的权衡，为高效特征编码与下游适配策略提供定量依据。

Method: 保持编码器权重冻结，使用轻量且可训练的neck模块作为探针，比较Hiera（通用）与SAM2（专用）特征在不同下游任务上的适应性；并通过信息论指标量化表示的专门化成本；提出交叉-neck分析以追踪不同适配层级对表征的影响。

Result: 实验证明SAM2在深度估计等空间任务上性能优越，但在姿态估计与图像字幕等语义距离较远任务上不及Hiera；信息论度量显示专门化导致有意义的语义信息丢失；交叉-neck分析指出每次适配都会进一步压缩表征信息。

Conclusion: SAM2在空间相关任务上的专门化提高了性能，但削弱了广泛语义信息，导致在与空间不相关的任务上（如姿态估计和图像描述）表现不佳。每一级适配都引入额外的表示瓶颈，显示出专门化的可量化代价。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [106] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: ProDAT通过密度感知的tail-drop机制实现了单模型下的渐进式点云编码，显著提升了编码效率（SemanticKITTI上PSNR-D2 BD-rate降低28.6%+，ShapeNet降低18.15%+）。


<details>
  <summary>Details</summary>
Motivation: 点云数据体积大、实时性和带宽受限场景下难以部署高质量服务。现有学习型点云编码方法通常使用固定潜在表示，无法支持在传输受限时的渐进式解码（先粗略再细化）。因此需要一种在单模型下支持多级细节恢复的编码机制。

Method: 方法包括利用点云密度信息作为重要性指导，对潜在特征和坐标进行自适应的tail-drop（丢弃）和解码安排，使得模型能够按重要性顺序解码，从而实现渐进式多码率输出。具体细节可能包括密度估计模块、按密度排序或分层的latent剪枝策略，以及在训练过程中对不同压缩率下的重建损失进行联合优化。

Result: 在SemanticKITTI上针对PSNR-D2指标，ProDAT相比最先进学习型方法实现了超过28.6%的BD-rate提升；在ShapeNet上也取得了超过18.15%的BD-rate改进。并且模型支持多码率渐进式解码，兼顾效率与灵活性。

Conclusion: 该论文提出了ProDAT，一种基于密度感知的tail-drop机制，实现了点云几何编码的渐进式解码，并在保持单模型的前提下支持多码率输出。实验表明在SemanticKITTI和ShapeNet上分别获得了显著的BD-rate改善，证明了方法在编码效率和渐进解码能力上的优势。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [107] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: 提出FMCAF：频域滤波+跨模态交叉注意力的通用预处理融合模块，显著提升RGB-IR多模态检测的泛化与性能（VEDAI +13.9% mAP@50，LLVIP +1.1%）。


<details>
  <summary>Details</summary>
Motivation: 在光照恶劣或分辨率受限等挑战条件下，单一模态性能下降，利用RGB与红外互补信息可以提高检测鲁棒性，但现有方法多依赖数据集专门调优，缺乏通用性。

Method: 提出了两个关键模块：Freq-Filter在频域抑制冗余光谱特征，和MCAF利用跨注意力实现更有效的模态间特征共享。两者作为预处理架构插入到检测流水线前。

Result: 相比简单拼接融合，FMCAF在VEDAI上mAP@50提升+13.9%，在LLVIP上提升+1.1%，显示出在航空车辆检测与低光行人检测任务上的有效性。

Conclusion: FMCAF通过频域滤波和跨模态注意力融合显著提升了RGB-IR多模态目标检测的鲁棒性和泛化能力。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [108] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane通过平面先验与动态重分类器改进Gaussian Splatting的平面重建，生成更精确和结构化的平面网格，且保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有Gaussian Splatting在新视图合成表现优越，但在重建平面区域时常出现不够光滑与精确的问题，影响后续场景编辑与物理仿真应用。

Method: 利用现成的分割与法线预测模型生成鲁棒的平面先验，约束平面高斯的参数化表示并在训练中强制几何一致性；引入动态高斯重分类器，将持续高梯度的平面高斯重新标注为非平面以增强训练鲁棒性；用优化后的平面先验优化网格布局、减少顶点与面数。

Result: 在不损失渲染质量的前提下，加入平面先验显著提升了不同基线方法提取网格的几何精度，得到更干净、连贯的平面网格并减少拓扑复杂度，同时支持将物体在平面上解耦与灵活操控的应用。

Conclusion: GSPlane通过引入平面先验和动态高斯重分类器，有效提升了Gaussian Splatting在平面区域的重建精度与网格结构质量。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [109] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 提出一种针对预训练扩散模型的条件优化策略：通过潜在精炼和条件-噪声潜在的双向交互，提高低光图像重建的保真度与可控性，且可插拔集成于现有网络。


<details>
  <summary>Details</summary>
Motivation: 动机是解决PTDB方法在追求感知真实感时往往牺牲内容保真度的问题，尤其在低光图像重建中，暗区信息严重退化，使得传统单向条件传递和粗糙的VAE编码限制了恢复质量。

Method: 方法包括两个核心模块：1）潜在精炼管线：引入生成式先验以恢复VAE编码过程中丢失的空间细节，从而获得更准确的条件潜在；2）双向交互机制：在扩散去噪过程中让精炼后的条件潜在与噪声潜在动态交互，增强对暗光信息的控制。该策略为插拔式，可无缝集成到现有基于ControlNet的预训练扩散网络中。

Result: 实验结果显示：在多种低光恢复基准上，提出的方法能显著提升保真度，同时保持或提升视觉质量，且可作为无缝插件应用于已有预训练扩散修复框架，带来一致性的性能提升。

Conclusion: 该论文提出了一种用于预训练扩散模型的条件优化策略，通过潜在空间精炼和条件-噪声潜在的双向交互来提升在低光等恶劣场景下的重建保真度，同时保持视觉真实感。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [110] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 通过对LED光谱进行精细优化并使用光谱调制，该工作在D65感知白光下实现对人眼不可察觉但对消费级相机可检测的环境光水印，能在低帧率视频中嵌入约128比特用于元数据传输。


<details>
  <summary>Details</summary>
Motivation: 在不改变视觉感受的前提下，通过环境光隐蔽地嵌入可被普通相机读取的元数据，用于隐私保护和内容验证等场景。

Method: 基于联合优化：考虑人眼的光谱灵敏度、消费级相机传感器的光谱响应以及窄带LED组合产生接近D65“白光”的能力；采用光谱调制（spectral modulation）而非强度调制以保证不可见性；以窄带LED的加权组合改变光谱轮廓来编码信息，支持在30–60fps低帧率下提取。

Result: 实现了在10秒视频内嵌入128比特信息的能力，信息速率虽低但足以承载关键元数据；水印对人眼基本不可见且可被典型消费相机可靠检测。

Conclusion: 该论文展示了通过优化LED光源的光谱，使其对人眼不可察觉但对消费级相机可检测，从而在环境照明中嵌入水印的可行性。

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [111] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: GOOD通过像素级和特征级双重引导扩散采样，生成语义稳定且多样的OOD样本，并提出自适应融合评分，显著提高OOD检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本容易导致语义不稳定和位移多样性不足，难以泛化到现实的OOD；因此需要直接在像素与特征两个层级引导扩散采样以获得更可控且多样的OOD样本。

Method: 在扩散模型采样中引入双层引导：一是基于像素空间中对数配分函数梯度的图像级引导，使生成样本远离高密度ID区域（降低输入似然）；二是基于ID分类器潜在空间中k-NN距离的特征级引导，鼓励采样到特征稀疏区域。此外，设计了自适应融合图像/特征不一致性的统一OOD评分。

Result: 实验显示，使用GOOD生成的样本进行训练能显著提升OOD检测性能；论文提供了定量和定性分析证明GOOD在生成可控、多样OOD样本以及提升检测鲁棒性方面的有效性。

Conclusion: 该文提出GOOD框架，通过直接在扩散采样过程中引导轨迹向OOD区域移动，从而生成更具语义稳定性和多样性的OOD样本，显著提升OOD检测性能。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [112] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D通过动力学感知VAE+双条件扩散模型+迭代Chamfer优化，从单视图精确重建和估计关节化物体的形状与运动学参数。


<details>
  <summary>Details</summary>
Motivation: 关节化物体具有多部件几何和可变关节配置，导致不同状态下结构差异大，给三维重建与位姿估计带来挑战。需要一个同时建模几何与运动学的统一方法以提高从单视图恢复的准确性。

Method: 构建Kinematic-Aware VAE将SDF、关节角度、零件分割编码到结构化潜在空间；使用两个条件扩散模型（一个回归全局位姿和关节参数，另一个从部分观测生成动力学感知潜码）；最后采用双向迭代优化通过Chamfer距离最小化在保持关节约束下精化重建和运动学参数。

Result: 在多个数据集（合成、半合成、真实）上，所提方法在重建精度和运动学估计上优于基线（文中宣称），展示了对复杂关节化物体的有效性。

Conclusion: 方法提出了一个统一框架KineDiff3D，通过引入动力学感知VAE和双条件扩散模型，同时结合迭代优化，提高了从单视图输入重建和估计关节参数的能力。该方法在合成、半合成和真实数据集上表现良好，能够准确重建关节化物体并估计运动学属性。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [113] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: 通过融合深度/法线先验进行有监督微调并结合基于组长度的奖励的强化学习，GACO-CAD提升了单视图到参数化CAD代码的几何准确性与简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在从2D推断3D几何时空间推理能力不足，导致生成的CAD模型几何不准或建模过程冗长。

Method: 第一阶段在有监督微调中，将深度图与法线图作为密集几何先验与RGB组合为多通道输入；第二阶段采用强化学习引入group length reward并用动态加权策略稳定训练，以同时提升几何保真度与建模紧凑性。

Result: 在DeepCAD和Fusion360数据集上，GACO-CAD在相同MLLM骨干下取得了领先表现，代码有效性、几何精度和建模简洁性均优于现有方法。

Conclusion: GACO-CAD通过两个阶段的后训练框架显著提升了从单张图像生成可编辑参数化CAD模型的几何精度与建模简洁性。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [114] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 预处理（尤其是人脸检测）显著影响人脸对抗样本的迁移性；通过输入变换可部分缓解，提升攻击在不同预处理下的泛化。


<details>
  <summary>Details</summary>
Motivation: 虽然人脸预处理是端到端人脸识别系统的关键环节，但在黑盒攻击研究中常被忽视；研究旨在量化预处理差异对攻击成功率的影响并提高攻击的泛化能力。

Method: 评估多种现成对抗攻击在黑盒设置下对不同人脸预处理（人脸检测模型、下采样插值方法）的可迁移性，并提出一种基于输入变换的预处理不变方法以提升迁移性。

Result: 发现人脸检测器选择可使攻击成功率下降最多78%，下采样插值影响较小；白盒场景下预处理需求反而削弱攻击；提出的输入变换方法使迁移性提升最高27%。

Conclusion: 预处理对人脸识别对抗攻击的迁移性影响显著，应在攻击与防御研究中被重视。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [115] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出不需训练的两阶段采样（GtR）并配合FTS，显著加速MAR模型生成且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 单步并行生成的Masked Autoregressive模型受限于处理空间相关视觉token的复杂性，直接并行生成难以兼顾速度和质量；作者假设从空白生成比在已有结构上补全更难，故提出分两阶段解耦难度以加速。

Method: 设计了两阶段采样：慢速的结构生成（生成全局语义骨架）与快速的细节重建（补全剩余像素）；并引入Frequency-Weighted Token Selection (FTS)基于高频能量分配更多计算预算给细节token。实现对MAR模型的加速且无需额外训练。

Result: 在ImageNet分类条件与文本到图像生成上实验证明，在保持相似质量下对MAR-H实现约3.72倍加速，FID和IS指标与原始模型相当或更优，且优于现有加速方法。

Conclusion: 该论文提出的Generation then Reconstruction (GtR)是一种无训练、分层采样策略，通过先生成图像的全局结构再重建细节来提升Masked Autoregressive模型的生成速度，同时保持生成质量。

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [116] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 本文首次对浮游生物OOD检测进行大规模系统评测，构建OOD基准并发现ViM表现最佳，代码开源。


<details>
  <summary>Details</summary>
Motivation: 浮游生物形态复杂、物种多样且不断发现新种，导致训练/测试分布不一致，实用部署时容易出现不可预测错误，现有研究缺乏与现代计算机视觉进展的系统整合和大规模评估基准。

Method: 基于DYB-PlanktonNet数据集设计多种模拟分布偏移的OOD基准（包括近似OOD与远离OOD场景），并系统评估了22种OOD检测方法，通过统一实验设置比较性能指标。

Result: 实验显示ViM方法在构建的基准上总体领先，尤其在Far-OoD场景中有显著关键指标提升，提供了可信的算法选型依据。

Conclusion: 该论文构建了大规模、系统化的浮游生物OOD检测基准，证明ViM方法在多种分布偏移场景下表现最佳，为算法选择与后续研究提供参考。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [117] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 首次联合建模头部外观与手-脸交互引起的形变：用深度顺序+接触正则化保证姿态关系，学习手致形变PCA基并加接触损失，提升几何准确性与物理合理性。


<details>
  <summary>Details</summary>
Motivation: 现有逼真3D头部重建方法通常只关注面部，忽略自然的手-脸交互（如托腮、抚触），这些交互包含重要的认知与姿态信息，且会引起面部局部的非刚性形变，需被建模。

Method: 在姿态追踪阶段结合深度顺序损失与接触正则化以保证手与脸的正确空间关系；为手致形变学习专用PCA基，从而将复杂的形变估计简化为紧凑的PCA参数回归；并加入受物理启发的接触损失以减少穿透并增强物理合理性。

Result: 在iPhone拍摄的RGB(D)视频及自行构造的合成数据上，方法在外观和面部变形几何准确性上均优于现有表面重建SOTA，并显著减少手脸穿透与不合理变形。

Conclusion: 该论文提出一个联合学习头部头模与手-脸交互所致的非刚性形变的新框架，能生成更物理合理的高质量3D头部化身并减少穿透伪影。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [118] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC通过GPT引导的域增强、切线空间的曲率感知插值和统一的超曲率损失，在无需episodic训练的情况下，实现了对未知域与未知类别的有效泛化，并在多个域泛化数据集上达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法通常要求训练时同时访问标注和未标注数据且来自同一域，限制了其在开放世界、域移位场景下的应用。DG-GCD放宽了这一假设，但现有方法（如DG2CD-Net）依赖高成本的episodic训练和任务向量聚合，存在效率与误差累积问题。作者希望设计高效且稳健的框架以在未知域和未知类别上泛化。

Method: 提出HIDISC框架：1) 使用GPT-guided diffusion对源域进行轻量但多样的域增强；2) 引入Tangent CutMix，在流形切线空间进行曲率感知的样本插值，合成伪新类样本；3) 设计统一的损失，包括惩罚Busemann对齐、混合超曲率对比正则和自适应离群体排斥；4) 学习曲率参数以自适应数据集复杂度。无需情景式（episodic）训练。

Result: 在PACS、Office-Home和DomainNet三个数据集上，HIDISC优于现有的欧氏和超曲率基线（包括DG-GCD方法），在分类与未知类别发现任务上取得一致的性能提升，证明方法在域与类泛化上的有效性与效率优势。

Conclusion: HIDISC通过超曲率（双曲）表征学习，结合源域扩增和切线空间插值等技术，在不进行情景式训练的情况下实现了对未知域和未知类别的泛化，从而在DG-GCD任务上取得了SOTA表现。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [119] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出零样本、提示感知的分层视觉token剪枝方法，在保留任务相关性的同时保持多样性，能在高剪枝率下实现近无损性能并减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 随着VLM处理更大输入，视觉token冗余显著，导致推理成本高；现有基于注意力或多样性的方法忽视文本提示，不能有效保证任务相关性。

Method: 分层策略：先基于提示选择核心任务相关token，再补充多样性token以保留上下文信息；在无监督/零样本情形下计算token与文本提示的相关性并结合多样性度量进行筛选。

Result: 在多个模型和基准测试上，方法在剪枝高达90% token的条件下，精度几乎无损甚至优于现有最先进方法，同时显著降低GPU显存和推理延迟。

Conclusion: 该论文提出了一种面向提示（prompt-aware）的视觉token剪枝方法，通过在任务相关性和信息多样性之间平衡选择token，实现零样本（zero-shot）下的大幅剪枝同时保持性能。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [120] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 通过构建2003–2025年人工标注的数据集并微调SAM的mask decoder，作者实现了高精度的河岸侵蚀与消失聚落识别（IoU 86.3%，Dice 92.6%），为政策与灾害管理提供量化工具。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国大河频繁侵蚀导致村庄与农田消失，传统人工监测耗时费力，需自动化、精确的遥感方法来长期追踪和量化土地损失以支持救灾与政策决策。

Method: 首先用简单的颜色通道分析对水体与陆地进行粗分割，然后基于该粗分割对SAM的mask decoder进行微调，使其识别河岸侵蚀的细微特征；使用2003–2025年谷歌地球影像并人工标注消失聚落作为训练/验证数据。

Result: 微调后的SAM在测试集上达到平均IoU 86.30%和Dice 92.60%，优于传统方法和通用深度模型；并提供可视化掩码与变化量化以证明其在不同区域与年份的适用性。

Conclusion: 本研究成功展示了将通用视觉模型SAM微调以识别河岸侵蚀和消失聚落的可行性，并提供了首个带标注的孟加拉国河蚀导致消失聚落数据集。结果表明微调后的模型在IoU和Dice指标上显著优于传统与通用深度学习方法，能为政策制定与灾害管理提供可视化与量化依据。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [121] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 通过在TimeSformer基础上融合小地图战术事件标注，对VALORANT回合胜负进行预测，增强后的模型在回合中后期达约81%准确率，优于仅用小地图的信息。


<details>
  <summary>Details</summary>
Motivation: 现有电子竞技胜负预测多依赖比赛日志与统计数据，缺乏对比赛录像中战术性信息（特别是FPS小地图）挖掘的研究。VALORANT作为需复杂策略的FPS，利用小地图与战术事件信息有望提升回合结果预测性能。

Method: 基于TimeSformer的视频识别模型，输入为包含小地图图像序列和附加的战术事件标签（如角色位置、关键事件）进行训练；比较仅使用小地图信息和使用战术标签增强的数据集的表现，重点分析回合不同阶段的预测准确率。

Result: 在包含战术事件标注的数据集上训练的模型在回合中后期（中期以后）达到约81%的预测准确率，显著优于仅用小地图信息训练的模型，表明战术特征能有效提升预测能力。

Conclusion: 本文提出通过分析比赛录像中的小地图信息并结合战术事件标注来预测VALORANT回合胜负。实验表明，加入详细战术标签的数据集训练的模型在回合中后期能显著提高准确率，达约81%，优于仅使用小地图原始信息的模型。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [122] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: 针对内镜图像CIL问题，EndoCIL通过分布对齐回放、先验正则化类平衡损失和分类器梯度校准三项技术综合减轻遗忘并提升新旧类性能，实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 内镜图像存在领域差异大、类不平衡严重的特点，导致现有基于回放的增量学习方法在保持旧类性能上表现不佳，因此需要专门针对内镜诊断场景设计的CIL方法。

Method: 提出统一框架EndoCIL，包含三部分：1) 基于最大均值差异的回放MDBR，用分布对齐的贪心策略选择多样且具代表性的样本；2) 先验正则化的类平衡损失PRCBL，将先验类分布与平衡权重融入损失以应对相位间和相位内的不均衡；3) 全连接梯度校准CFG，通过调整分类器梯度减轻对新类的偏倚。

Result: 在四个公开内镜数据集上进行广泛实验，结果表明EndoCIL在不同缓冲区大小与评估指标下普遍优于最先进方法，能更好地在稳定性与可塑性之间取得平衡。

Conclusion: EndoCIL能有效缓解内镜影像CIL中的灾难性遗忘问题，在多个数据集和不同缓冲区大小下整体优于现有方法，显示良好的临床推广潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [123] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 基于DINOv2并加入registers抑制注意力扰动，用于提取微小判别特征以检测人脸伪造，实验证明在ICCV2025挑战和SiW上有效且泛化性好。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别系统易被持有注册用户照片的攻击绕过，因此需要在识别前准确检测伪造图像。由于伪造与真实图像在宏观上差异小，作者希望利用自监督表示和注意力抑制来捕捉微小但判别性的细节特征，提高检测鲁棒性与泛化能力。

Method: 采用DINOv2自监督视觉表征模型作为特征提取骨干，结合“registers”模块用于抑制注意力中的噪声与扰动，增强对微小差异的敏感性；随后在提取的特征上训练分类器以区分活体与伪造图像。

Result: 在“The 6th Face Anti-Spoofing Workshop: Unified Physical-Digital Attacks Detection@ICCV2025”提供的数据集和SiW数据集上进行了实验，结果表明该方法在检测物理和数字伪造攻击方面具有有效性能并展示了较强的泛化性（论文摘要未给出具体数值指标）。

Conclusion: 该论文提出了一种基于DINOv2的防欺骗检测方法，通过引入registers抑制注意力机制中的扰动，从而聚焦于区分真实与伪造面部图像的细微特征。实验在ICCV2025挑战赛数据集和SiW上验证了方法的有效性，表现出良好的泛化能力。

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [124] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 通过发现MLLM的三阶段跨模态交互并据此无需训练地剪除绝大多数视觉token注意力计算，VisiPruner显著提升了计算效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 当前的MLLM在多模态token数量增加时注意力计算呈二次增长，造成计算开销巨大，且现有剪枝方法缺乏对模型如何处理和融合多模态信息的根本理解。

Method: 对MLLM进行逐层分析以识别跨模态交互阶段：浅层识别任务意图并将视觉token作为被动注意力汇聚点；中层由少量关键视觉token驱动的跨模态融合；深层舍弃视觉token只进行语言精炼。基于该观察，提出VisiPruner，在推理阶段训练免疫地剪除大部分视觉相关注意力计算。

Result: VisiPruner在LLaVA-v1.5 7B上最多可减少99%的视觉注意力计算和53.9%的FLOPs，优于现有token剪枝方法，并能泛化到不同的MLLM。

Conclusion: 本文通过系统性分析揭示了多模态大模型中跨模态交互的三阶段过程，并基于此提出了VisiPruner，一个训练免疫的视觉token剪枝框架，显著降低了计算开销并在多模型上表现出良好的泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [125] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该工作提出QV-M2多时刻数据集与MMR评测指标，并设计FlashMMR（含Multi-moment Post-verification模块）以提升多时刻检索性能，在多项指标上超过SOTA，推动视频时序定位研究更贴近实际场景。


<details>
  <summary>Details</summary>
Motivation: 现有MR方法与数据集主要面向单一目标时刻，但现实查询常对应多个相关时段，导致模型与评估方式无法满足实际视频时序定位需求，因此需要新的数据集、评测指标与方法来支持多时刻检索。

Method: 构建QV-M2数据集（2,212条标注，6,384个视频片段），提出Multi-moment Post-verification模块：先基于候选段进行受限的时序调整，再用验证模块重新评估候选段并剔除低置信度提案；将该模块整合到FlashMMR框架中，并在QV-M2和QVHighlights上对6种现有MR方法进行重训练与评估。

Result: 在QV-M2上，FlashMMR相比先前SOTA在G-mAP上提升3.00%，在mAP@3+tgt提升2.70%，在mR@3提升2.56%；验证了QV-M2作为MMR训练与评估基准的有效性，同时FlashMMR作为强基线表现优越。

Conclusion: 该论文针对单一时刻检索（SMR）与实际应用中存在的多时刻检索（MMR）不匹配的问题，提出了高质量数据集QV-M2和新的评估指标，并设计了FlashMMR框架，通过后验验证模块提升多时刻边界精度，从而在多个指标上超越现有SOTA，奠定了MMR研究基准。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [126] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出结合时序聚类与人口统计学感知频域数据增强的公平可解释深伪检测方法，在多数据集与网络上实现了更好的公平性与准确性折中。


<details>
  <summary>Details</summary>
Motivation: 现有深伪检测方法容易偏见、缺乏透明性且忽略时序信息，导致在不同人口群体上效果不一致和不可解释的决策，需要一个同时提高公平性、鲁棒性和可解释性的检测框架。

Method: 使用序列（时间）聚类对视频帧进行时序建模，提取概念（concepts）以辅助可解释性；提出基于人口统计学的重采样和频域变换的数据增强方法以平衡不同群体并保留伪造痕迹；在Xception和ResNet等SoTA网络上训练并在FaceForensics++、DFD、Celeb-DF、DFDC上评估。

Result: 在四个主流数据集和两种SoTA架构上，提出的方法比现有方法在公平性-准确性权衡上表现更优；人口统计学感知的数据增强显著减少了对弱势群体的性能下降；时序建模与概念提取提高了检测的可靠性与可解释性。

Conclusion: 该论文提出了一个兼顾公平性、可解释性和时序信息建模的深度伪造检测框架，通过序列聚类与概念提取实现时序特征学习，并结合人口统计学感知的数据增强在频域保持伪造痕迹，从而在准确性与公平性之间取得折中。

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [127] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision：一个 2400 万样本、半自动化清洗和去重的人机协作视觉-语言语料库，提升了 VLM 性能并开源。


<details>
  <summary>Details</summary>
Motivation: 当前公开视觉-语言数据集存在碎片化、不一致和污染问题，阻碍 VLM 进步，因此需要一个统一、清洗、规模化的数据资源。

Method: 构建了一个半自动化、人类在环的数据摄取与归一化流程：自动化批量摄取与 schema 映射；人工审核映射和抽查样本，修复问题并重跑；进行严格的跨源去重和对 66 个公开基准的去污染；对 agentic/GUI 任务统一动作空间并验证轨迹可执行性。

Result: 构建了包含 2400 万样本的 FineVision 语料库（185 个子集，来自 200+ 来源），训练的模型在广泛评估中优于基于现有开放混合数据的模型。作者同时发布语料和工具。

Conclusion: FineVision 提出并验证了一个高质量、大规模的视觉-语言数据集，通过半自动化的人机协作流程实现统一、去重与清洗，显著提升了训练模型的性能。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [128] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出Plug-and-Forecast (PnF)：将多模态大语言模型通过提示生成结构化场景理解并蒸馏为嵌入，作为插件增强现有运动预测模型，利用零样本能力在Waymo和nuScenes上实现稳定性能提升，无需微调。


<details>
  <summary>Details</summary>
Motivation: 标准运动预测模型在典型条件下表现良好，但在多样化真实场景的成本可控泛化仍是挑战。自然语言能更灵活地描述复杂情境，便于快速适配目标行为，因此用MLLM补强感知与预测能力。

Method: 设计针对性的提示（prompts）让MLLM从场景输入中推理并输出结构化描述，将这些描述蒸馏为可学习的嵌入向量，作为附加特征并入现有的运动预测模型（plug-and-play），通过零样本推理能力提升预测性能。

Result: 在Waymo Open Motion Dataset和nuScenes上，将PnF整合到两种SOTA运动预测模型后，均获得了稳定的性能提升，且无需对MLLM做微调。

Conclusion: PnF通过将多模态大语言模型（MLLMs）作为插件，利用自然语言来提取并表征复杂场景信息，从而增强现有运动预测模型的泛化能力，且无需对MLLM进行微调。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [129] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: SG-CLDFF结合显著性预处理与跨层深度特征融合，通过多任务训练和可解释性检查，显著提高了WBC分割与分类性能并增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: WBC显微图像存在染色差异、复杂背景与类别不均衡，导致自动分割与分类困难，需提升鲁棒性并提供可解释性以便临床应用。

Method: 计算显著性先验突出候选WBC区域，采用轻量混合主干（类似EfficientSwin）提取多分辨率特征，使用受ResNeXt-CC启发的跨层融合模块聚合浅深层互补信息，采用多任务训练（分割与分类）并引入类别加权损失与显著性一致性正则化，同时通过Grad-CAM进行可解释性约束。

Result: 在BCCD、LISC、ALL-IDB等公开基准上，方法在IoU、F1和分类准确率上均优于强CNN和Transformer基线，消融实验表明显著性预处理与跨层融合各自带来性能提升。

Conclusion: 提出的SG-CLDFF通过显著性引导的预处理与跨层深度特征融合，有效提升了WBC分割与分类的鲁棒性与可解释性。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [130] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 基于YOLOv11识别蓝色标记并驱动伺服电机自动对准LED手术灯，验证集mAP@50=96.7%，旨在减少手动调整、降低医生疲劳并提供一致照明。


<details>
  <summary>Details</summary>
Motivation: 传统手术灯需手动调整，导致手术过程中外科医生疲劳、颈部劳损以及照明漂移和阴影问题，影响手术精度与安全。提出自动化照明以减轻负担并提供稳定、精准的照明。

Method: 使用YOLOv11检测蓝色球形标记，训练集为带注释的模拟手术场景图像。检测到标记后将其中心位置传递给控制模块，由两个伺服电机（倾仰和方位）驱动带有高功率LED的灯具，通过倒推角度或PID控制使灯光指向检测到的位置。系统集成了摄像头、YOLO推理模块和伺服控制器。

Result: 在包含模拟手术场景的验证集上，YOLOv11模型取得96.7% mAP@50的性能。系统通过实时检测与伺服控制实现对目标位置的快速对准，论文声称可减轻医生物理应激并提高照明一致性。

Conclusion: 本文提出将YOLOv11用于识别放置于手术目标位置上方的蓝色标记球，并通过两个伺服电机控制的倾转云台将高功率LED光源对准目标，从而实现自动手术照明。实验在模拟手术场景图像上训练和验证，达到96.7% mAP@50，表明检测可靠，系统可减少外科医生的体力负担并提高照明一致性。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [131] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 研究发现自监督训练越久可能损害语义分割等密集预测任务性能（SDD）。提出DSE指标在无标签条件下估计表示质量，并基于此做模型选择和正则化，平均可提升mIoU 3%并缓解退化。


<details>
  <summary>Details</summary>
Motivation: 发现自监督学习中一个反直觉现象：更长的训练并不总提高下游密集任务性能，反而可能下降，因此需要无标签的训练期间性能评估指标来选择合适模型并缓解性能退化。

Method: 在16种最先进的SSL方法上进行广泛实验证明SDD的存在；提出DSE，包括类相关性度量和有效维度度量；基于DSE设计模型选择策略和DSE正则化项；在四个基准和多种设置上评估并对比下游语义分割性能。

Result: 基于DSE的模型选择在16种SSL方法上平均提升mIoU约3.0%，计算成本可忽略；DSE正则化能稳定减少密集预测性能退化；DSE与下游性能高度相关。

Conclusion: 长时间自监督训练会导致面向密集预测任务（如语义分割）性能下降（Self-supervised Dense Degradation, SDD）。在多种SSL方法、模型和数据集上该现象普遍存在。提出的Dense representation Structure Estimator (DSE)可在无标签情况下估计表示对密集任务的质量，并用于模型选择和正则化，从而缓解SDD并提高mIoU。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [132] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个针对长时信息密集视频的多模态评测基准，包含约1000个视频、六类任务与严格质量控制，揭示现有全模态模型在时间定位和长程因果推理上的不足以及多模态融合中的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 当前全模态模型在长时视频理解、精确时间定位和长程因果推理等方面表现不足，且缺乏专门评测长时信息密集视频的基准；因此构建LongInsightBench以推动该方向研究。

Method: 从开源FineVideo中筛选约1000个时长较长且视觉与音频信息密集的视频（如讲座、采访、vlog），设计六类任务场景（包括事件内与事件间任务），并构建半自动三步QA质量控制流程，合成问题与选项用于评测。

Result: 实验证明现有全模态模型在时间定位（T-Loc）和长程因果推理（CE-Caus）任务上仍有明显缺陷；扩展实验显示多模态融合过程中存在信息丢失与处理偏差。数据集与代码已公开。

Conclusion: 本文提出了LongInsightBench，一个针对理解长视频的多模态基准，侧重语言、视角、动作和上下文元素，通过视觉、音频和文本三模态评估模型在长时信息密集视频上的理解能力。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [133] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: 提出CausalMamba：先去卷积再用Conditional Mamba做因果推断，解决BOLD扭曲与可扩展性问题，在模拟与实测数据上显著优于DCM并揭示动态网络重配置。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI因果推断受血氧动力学扭曲（BOLD）导致的反问题病态及传统方法（如DCM）计算不可行性的限制，需要一种既能纠正信号失真又能扩展到大规模网络的方法。

Method: 两阶段方法：1) BOLD去卷积以恢复潜在神经活动；2) 使用Conditional Mamba架构对去卷积后的神经时间序列进行因果图推断，具体包括模块化的可扩展优化与条件化判别机制。

Result: 在模拟数据上较DCM提高37%准确率；在真实任务fMRI数据上以88%保真度恢复已知神经通路，常规方法在99%以上被试中无法识别这些回路；还发现工作记忆任务中执行网络与显著性网络根据刺激策略性切换主因果枢纽。

Conclusion: CausalMamba通过先进行BOLD信号去卷积再用Conditional Mamba进行因果图推断，解决了fMRI因果推断中的模糊与计算难题，能更准确、可扩展地恢复神经因果关系。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [134] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 作者展示对抗性衣物可有效规避现有多种对抗性补丁防御，暴露了这些方法在大覆盖面积和自然外观场景下的弱点。


<details>
  <summary>Details</summary>
Motivation: 观察到增大补丁尺寸可使现有防御失效，进而研究更具现实性的攻击形式——对抗性衣物，评估防御方法在更大覆盖面积和更自然外观下的鲁棒性。

Method: 作者系统评估了多种针对对抗性补丁的防御方法，对数字世界和物理世界中的大面积对抗性衣物进行了攻击测试，并制作了一套可在多种被防护模型（以Faster R-CNN为主）上通用的对抗衣物。

Result: 实验发现所有评估的防御方法在数字与物理场景中均表现不佳；作者制作的一套对抗衣物在未防御检测器上ASR为96.06%，在九个有防御的模型上物理世界ASR仍超过64.84%。

Conclusion: 现有针对对抗性补丁的防御方法在面对覆盖面积更大的对抗性衣物时显著失效，表明这些防御并未解决尺度和自然外观带来的实用攻击问题。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [135] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff用字符级先验和区域掩码注意力改进扩散修复，使低质车牌图像的恢复与识别更鲁棒，实验证明效果显著。


<details>
  <summary>Details</summary>
Motivation: 在现实条件下拍摄的低质量车牌图像严重影响LPR系统性能，需一种既能修复图像又能提升识别率的有监督引导方法，尤其是利用字符级先验增强恢复细节。

Method: 提出基于扩散模型的CharDiff框架，利用外部分割与OCR模块提取字符级先验，并设计CHARM模块通过区域掩码将每字符引导限定到其对应区域，从而避免区域间干扰。

Result: 在Roboflow-LP数据集上，CharDiff在恢复质量与识别准确率上显著优于基线模型，CER相对降低约28%。

Conclusion: CharDiff通过字符级引导与区域掩码注意力有效提升了车牌图像修复与识别性能，证明了结构化字符先验在扩散模型中的实用性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [136] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: 提出iDETEX——一个能同时做质量定位、感知与描述的多模态LLM，通过任务专用离线/在线增强和数据混合训练，在ViDA-UGC上达SOTA并夺得ICCV竞赛冠军。


<details>
  <summary>Details</summary>
Motivation: 现有IQA从标量质量预测转向更可解释、对齐人类感知的评估方式，需求更细粒度和可解释的图像质量评估方法。

Method: 设计了任务专用的离线增强模块和数据混合策略，并辅以在线增强策略，以在异构子任务间高效泛化训练；构建统一的MLLM用于三大子任务的并行学习。

Result: 在ViDA-UGC上各子任务达到SOTA表现，模型在ICCV MIPI 2025 Detailed IQA Challenge中排名第一。

Conclusion: iDETEX是一个统一的多模态大语言模型，可同时实现质量定位（grounding）、感知（perception）和描述（description），在大规模ViDA-UGC基准上取得SOTA并获得ICCV MIPI 2025挑战赛第一名，证明了其准确性与可解释性。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [137] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出一种基于NCM距离分布与softmax概率一致性比较的后处理OSR方法，无需重训练，在两个动物数据集上实现高且稳定的未知检测性能（AUROC 93.41/95.35）。


<details>
  <summary>Details</summary>
Motivation: 现有野生动物分类模型在开集情形下对未知类过度自信，且大多数OSR方法需要对预训练模型进行再训练，限制了实用性。本工作旨在设计一个不需重训练、稳健的后处理OSR方法。

Method: 对每个输入计算其到各类最近类均值的距离并构造基于距离的概率分布；将该分布与分类头输出的softmax概率进行比对（衡量一致性/差异）以判定样本是否为未知；方法作为后处理模块，可作用于预训练分类模型，无需额外训练。

Result: 在非洲和瑞典动物数据集上分别达到AUROC 93.41和95.35，且在两个数据集上都位列前三，显示出比一些当前SOTA在单一数据集上优秀方法更稳定的跨数据集表现。

Conclusion: 本文提出了一种无需重训练的后处理开集识别方法，通过比较特征空间（基于最近类均值NCM距离分布）与logit空间（softmax概率）的匹配程度来检测未知类，从而在两个野生动物数据集上取得稳定且靠前的性能。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [138] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 该工作通过从SAM迁移语义并在事件特征中融合语义信息，提出语义感知监督，从而显著改善了事件到视频的重建效果。


<details>
  <summary>Details</summary>
Motivation: 事件相机只记录强度变化，缺乏静态物体与背景的语义信息，导致现有E2V方法在语义重建上表现欠佳，需引入外部视觉语义知识。

Method: 提出CFA模块将来自SAM的视觉语义迁移到事件编码器，并设计SFF块将帧模态的语义与事件特征融合；引入基于SAM生成类别标签的语义感知感知监督以提升语义重建。

Result: 在多个基准数据集上，Semantic-E2VID在帧质量上显著优于现有最先进的E2V方法，实验结果和消融分析支持其有效性。

Conclusion: Semantic-E2VID通过引入跨模态特征对齐和语义感知特征融合，有效弥补事件相机模态中缺失的静态语义信息，从而提升事件到视频重建的质量。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [139] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H通过窗口级跨任务注意力与轻量级ViT骨干，实现面向边缘设备的高效单目多任务空间感知，兼顾精度与实时性，且在多个基准与真实数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现实时空间感知需要高效的多任务模型，既要利用任务间互补信息提高精度，又要最小化计算与延迟，传统独立模型或共享编码器-解码器在效率或任务一致性上存在不足。

Method: 提出了Window-Based Cross-Task Attention Module，在窗口内跨任务交换结构化特征并保留任务特有信息；使用轻量级ViT（DINOv2）骨干和多头Hydra形态解码器为每个任务提供专门分支，优化计算开销以满足实时部署需求。

Result: 在NYUDv2上优于现有多任务模型，在Hypersim上超越单任务深度与语义基线，在Cityscapes上也取得更好成绩，并在笔记本硬件上保持计算效率，同时在真实世界数据上验证了其实用性。

Conclusion: M2H是一个在轻量级ViT+DINOv2骨干上构建的实时多任务框架，能在保证计算效率的同时，通过跨任务注意力模块改善语义分割、深度、边缘和法线估计的一致性与细节保留，适用于边缘设备和实际场景的单目空间感知与3D场景图构建。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [140] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 无训练开销，通过LLM注意力选token+循环历史重用+caption问答，实现高效且准确的流式Video-LLM。


<details>
  <summary>Details</summary>
Motivation: 目标解决Video-LLM在流式长视频场景下的计算与时延问题，使模型在处理小时级视频并需及时响应问题时仍保持高效与准确。

Method: 方法包括三部分：1) 利用LLM的注意力信息选取对当前短片有贡献的视觉token，丢弃约95%不重要token；2) 将已选token以循环方式处理以维持时间一致性的历史上下文；3) 将问答任务转化为基于caption的轻量级回答机制。该方法无需额外训练，可直接应用于标准Video-LLM。

Result: 在流式视频基准测试上达到或超过现有最优性能，兼顾计算效率与理解效果，显著减少处理token数量同时保持问答质量。

Conclusion: 提出了一种训练免费、兼容常规Video-LLM的流式视频理解方法，通过基于LLM注意力的视觉token选择、对已选token的循环处理以及基于字幕的问答，显著提升流式效率并保持性能。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [141] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 大规模实证评估表明，合成面部数据在隐私保护下可替代真实数据，部分合成数据集性能超越主流真实数据集，并可通过生成参数主动缓解偏见。


<details>
  <summary>Details</summary>
Motivation: Address ethical and legal issues from using real-face datasets collected without consent by evaluating whether synthetic facial data is a viable privacy-preserving alternative for recognition tasks.

Method: Systematic literature review to identify 25 synthetic facial datasets from 2018-2025; large-scale experimental validation using over 10 million synthetic samples; evaluation against seven requirements (identity leakage, intra-class variability, separability, scale, sourcing ethics, bias, benchmark reliability); comparison on five standard benchmarks; demographic bias analysis and ablation studies on generation parameters.

Result: Top synthetic datasets (VariFace, VIGFace) achieved 95.67% and 94.91% recognition accuracies, surpassing CASIA-WebFace (94.70%); public datasets Vec2Face and CemiFace reached 93.52% and 93.22%; synthetic data maintained intra-class variability and identity separability; limited biases inherited but generation parameters enable mitigation; overall synthetic data is viable and ethical alternative.

Conclusion: Synthetic facial datasets can effectively replace many real datasets for facial recognition, achieving comparable or superior accuracy while offering privacy and controllable bias mitigation; however, challenges remain in benchmark standardization and ensuring fully unbiased generation.

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [142] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出一种基于多表情特征融合与自适应类平衡的注意力网络，用于帕金森病严重程度分级诊断，能缓解单一表情依赖和类别不平衡问题，实验显示有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于面部表情的PD诊断方法存在依赖单一表情类型易导致误诊、忽视不同PD阶段间类别不平衡从而影响性能，以及多为二分类而非分级诊断的不足。为提高诊断可靠性与分级能力，提出融合多表情并处理类不平衡的新方法。

Method: 方法包括：1）采集或从现有数据集中提取多种类型的面部表情特征（例如静态表情、动态表情、面部关键点/动作单元等）；2）使用注意力机制对不同表情特征进行加权融合，以自动学习各特征在严重度判定中的重要性；3）设计自适应类平衡策略，在训练过程中根据类别分布及样本分类难度动态调整样本或损失的权重；4）在多类别（多个PD阶段）设置下训练并评估模型。

Result: 论文在实验中展示了该方法在PD严重程度分级任务上取得了有竞争力的结果。消融实验或对比实验表明：注意力机制的特征融合优于简单拼接或平均融合；自适应类平衡策略能明显改善少数类（如重度或轻度）样本的识别率，从而提升整体指标。具体数值未在摘要中给出。

Conclusion: 该论文提出了一种基于多种面部表情特征并通过注意力机制融合的帕金森病（PD）严重程度诊断方法，同时引入自适应类平衡策略以缓解类别不平衡问题。实验表明该方法在PD严重程度诊断上具有良好性能，注意力融合和自适应类平衡均能提升效果。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [143] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans通过闭环的双向知识迁移与跨模态定位与去噪蒸馏模块，提升弱监督可交互性定位在复杂场景下的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督可交互性定位方法仅单向从外部交互图像迁移到第一视角，难以在复杂或遮挡情形下有效提取与转移交互知识，需通过双向闭环提升鲁棒性。

Method: 提出闭环框架LoopTrans，包含统一的跨模态定位模块和去噪知识蒸馏模块，实现从exocentric到egocentric的知识迁移并反向增强exocentric特征提取；利用图像与视频基准进行评估。

Result: 在图像与视频基准上各项指标均有稳定提升，能处理人体完全遮挡物体交互区域等挑战性场景。

Conclusion: LoopTrans通过构建闭环的知识双向迁移，克服了以往单向迁移的局限，使外部交互视角与第一视角之间实现互补提升，从而更鲁棒地定位可交互区域。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [144] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 本文构建了一个结合YOLOv11与BoT-SORT的马厩视觉监测原型，利用Foundation Models辅助标注自定义数据集，可检测五类事件并处理盲区；对马匹事件表现良好，但人员检测因数据不足仍有限。


<details>
  <summary>Details</summary>
Motivation: 人工监测马厩行为费时费力，早期发现健康与福利问题对马匹照顾至关重要，因而需要自动化、实时的监测系统以降低人力成本并提高响应速度。

Method: 使用YOLOv11进行目标检测，BoT-SORT进行多目标跟踪；基于轨迹与空间关系推断事件状态；构建带注释的自定义数据集，利用CLIP与GroundingDINO辅助标注；考虑摄像头盲区进行事件判断。

Result: 定性评估表明系统在马匹相关事件检测上表现可靠，但在人（人员）检测上受限于数据稀缺性；整体为实时行为监测提供了可行基础，并指出需要更多样本与改进以提升人员识别。

Conclusion: 该论文提出了一个基于视觉的马厩行为监测原型系统，能够自动检测和跟踪马匹与人，推断五类事件状态，支持实时监控并有望改善动物福利与马厩管理。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [145] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect用多检测器融合生成伪标签训练ESPNet，得到语义感知的密集关键点，显著提升密度与重复性，Oxford数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统和现有学习型关键点检测器在光度变化、关键点密度、重复性以及对复杂场景的适应性方面存在局限，且缺乏语义理解，无法优先考虑视觉重要区域，因此需一种能兼顾密度、鲁棒性和语义感知的新型检测器。

Method: 首先将7种关键点检测器和2种边缘检测器的输出融合生成“真值”掩码，捕捉角点、斑点以及显著边缘和纹理等多样视觉线索；然后使用这些掩码作为标签训练轻量且高效的ESPNet，从而实现密集且语义感知的关键点检测。

Result: 在Oxford Affine Covariant Regions数据集上，DeepDetect在关键点密度、重复性和正确匹配数方面均优于其他检测器，分别取得了最大平均关键点密度0.5143、平均重复性0.9582以及正确匹配数59003。

Conclusion: DeepDetect提出了一种基于深度学习的密集关键点检测器，通过融合多种传统关键点和边缘检测器的输出作为伪标签训练轻量级ESPNet模型，实现了语义感知的高密度、高重复性关键点检测。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [146] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 用AV1运动向量生成稠密亚像素对应和短轨迹，计算效率高、匹配密集且可用于小规模SfM，但更密集匹配会增加BA开销。


<details>
  <summary>Details</summary>
Motivation: 在资源受限或需要高效处理的视频场景中，探索能否直接利用压缩域信息（视频编码的运动向量）替代传统昂贵的像素域特征匹配以节省计算资源并保持匹配质量。

Method: 利用AV1编码产生的运动向量作为前端，进行亚像素插值生成稠密对应，使用余弦一致性筛除伪匹配，构建短轨迹并用于配对几何评估与小规模SfM重建。

Result: 在短视频测试中，该压缩域前端与顺序SIFT在性能上相当但CPU消耗远低，匹配更密集并在配对几何上具有竞争力。对117帧片段的SfM示例，MV匹配能注册所有图像并重建0.46–0.62百万点，重投影误差约0.51–0.53像素；BA时间随匹配密度增长。

Conclusion: 压缩域运动向量（MV）可被有效重用以生成稠密亚像素对应与短轨迹，过滤后能与SIFT顺序方法在短视频上实现相当匹配质量并显著降低CPU开销。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [147] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 提出HQ-NightRain数据集和CST-Net，通过可学的色彩空间转换和隐式光照引导，提升夜间图像去雨效果并公开数据与代码。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨比白天更具挑战性，主要由于夜间场景复杂且缺乏高质量、能反映雨与光照耦合效应的数据集；夜间雨斑在亮度（Y）通道更明显，利用合适色彩空间可利于去雨。

Method: 提出了可学习的色彩空间转换器（CSC），将图像转换到Y通道进行去雨；引入隐式光照引导以捕捉并利用光照信息提高模型鲁棒性；构建并训练CST-Net在新数据集上进行评估。

Result: 在HQ-NightRain数据集上，CST-Net在视觉质量和定量指标上均优于现有方法，表明数据集和方法的有效性；源码与数据集已公开。

Conclusion: 本文提出了高质量夜雨数据集HQ-NightRain和一种色彩空间转换网络CST-Net，改进了夜间去雨任务的真实性与去雨性能。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [148] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: Focus on stronger initialization for sparse-view 3D Gaussian Splatting: enhance SfM coverage with frequency-aware augmentation, supplement with self-initialized Gaussian points, and apply simple point-cloud regularization — leading to consistent gains over prior methods.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS overfits training views and produces artifacts; controlled ablations show initialization is decisive, so focus on improving initialization rather than only training-time constraints.

Method: Propose three complementary initialization techniques: (i) frequency-aware SfM with low-frequency view augmentation and relaxed multi-view correspondences to improve low-texture coverage; (ii) 3DGS self-initialization that introduces additional learned Gaussian points guided by photometric supervision; (iii) point-cloud regularization enforcing multi-view consistency and uniform spatial coverage via geometric/visibility priors.

Result: Demonstrated consistent improvements on LLFF and Mip-NeRF360 datasets in sparse-view settings; approach yields cleaner and more reliable point clouds and better novel view rendering.

Conclusion: Initialization quality critically determines performance in sparse-view 3D Gaussian Splatting; improving initialization yields larger gains than adding training-time regularizers.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [149] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld使用稀疏动态查询、可随车调制的感知模块与基于回归的状态条件预测，显著提升了4D语义占据世界模型的灵活性与性能，并通过自调度训练实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有占据世界模型受限于静态固定的嵌入或栅格，感知灵活性不足且分类式“原位分类”与连续动态场景存在不匹配，需要更灵活和动态的查询机制与连续预测方法。

Method: 提出Range-Adaptive Perception模块（通过随车状态调制的可学习查询并结合时空关联拓展感知范围）、State-Conditioned Forecasting模块（用回归引导替代分类预测以对齐动态查询与连续4D环境），以及Temporal-Aware Self-Scheduling训练策略。

Result: 在感知、预测与规划任务上均实现了SOTA水平；视觉化与消融实验验证了方法在灵活性、适应性与效率方面的优势。

Conclusion: SparseWorld通过稀疏与动态查询成功构建了更灵活、高效且适应性强的4D语义占据世界模型，解决了传统基于固定格子嵌入的感知局限和基于分类的预测不连续问题。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [150] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: 通过熵引导的双聚类+最优传输生成更精细的伪掩码，并用于MaskFormer训练，AutoSOD实现了在无监督显著性目标检测上显著提升。


<details>
  <summary>Details</summary>
Motivation: 观察到边界像素与内部像素在几何特性上差异显著，且在原型质量较弱时最优传输的全局一致性约束被弱化；因此提出按像素不确定性(熵)分流以提升原型质量和OT对齐效果，从而得到高质量伪掩码，推动无监督SOD性能接近有监督水平。

Method: 在原有Prototypical Optimal Transport框架中，用熵引导的双聚类头替换单一k-means：高熵像素用谱聚类捕捉边界/零散结构，低熵像素用k-means聚类捕捉显著物体内部；随后通过最优传输对两组原型进行对齐，生成更具全局一致性且分部感知的伪掩码。最终用这些伪掩码监督MaskFormer式网络训练，省去离线投票。

Result: 在五个基准上，AutoSOD在F-measure上比现有无监督方法最高优越26%，比弱监督方法最高优越36%，并且在训练效率上优于SelfMask，缩小了与全监督模型的性能差距。

Conclusion: 作者提出了一个名为POTNet的原型-最优传输方法改进，将像素按熵分为边界(高熵)和内部(低熵)两类，分别用谱聚类和k-means聚类，再用最优传输对齐原型，从而生成更精确的伪掩码，并用于训练MaskFormer风格的编码器-解码器，形成端到端的无监督SOD流水线AutoSOD。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [151] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 用少量真标签生成伪标签并构建评分量表以引导LLM进行上下文感知的零样本视频摘要，实现了稳定且可解释的打分准则，显著优于现有无标注和零样本方法。


<details>
  <summary>Details</summary>
Motivation: 动机是克服现有监督方法标注成本高且泛化差、无监督方法难以捕捉高层语义与叙事线索、以及现有零样本提示对模板和归一化敏感的问题，利用LLM的理解能力并用伪标签与量表稳定其评分行为。

Method: 方法首先用小比例的真实注释生成高置信度伪标签，并把这些伪标签聚合成结构化、适配数据集的评分量表；推理时对首尾段仅根据描述评分，对中间段提供邻近场景的简短上下文摘要以评估叙事连贯性与冗余，LLM据此给出可解释分数用于选取关键片段。

Result: 在SumMe和TVSum数据集上分别取得F1 57.58和63.05，优于无监督和先前零样本基线，接近监督方法表现，表明该方法能稳定LLM评分并提高摘要质量。

Conclusion: 该论文提出了一种结合少量人工标注生成高置信度伪标签，并基于这些伪标签构建可解释评分量表（rubric），用于引导大语言模型（LLM）在零样本视频摘要任务中进行稳定打分和选段的框架，从而在无需微调的情况下提升摘要质量。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [152] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出一个优化数据、模型、训练与基础设施的训练框架，训练出开源的MUG-V 10B，兼顾效率与效果，并开放完整代码与权重以促进社区研究与应用。


<details>
  <summary>Details</summary>
Motivation: 动机是克服视频生成中跨模态对齐、长序列和复杂时空依赖带来的巨量计算与训练难题，从而实现在算力可接受范围内的高质量大规模视频生成模型。

Method: 方法包括：改进数据预处理与视频压缩流程以降低计算与存储开销；设计可扩展的模型架构以支持参数扩展；采用分阶段（课程式）预训练与对齐为重点的后训练策略；利用Megatron-Core实现高效的多节点近线性扩展训练。

Result: 结果显示MUG-V 10B在总体性能上可与最新SOTA视频生成器匹配，并在电商场景的视频生成任务中通过人工评测超过领先开源基线。同时团队开源了模型权重、训练代码和推理流水线，推动社区复现与应用。

Conclusion: 该论文提出了一个面向大规模视频生成的系统化训练框架，通过在数据处理、模型结构、训练策略和基础设施四个方面的优化，显著提升了训练效率和生成质量，最终发布了10B参数的MUG-V模型并开源了完整训练与推理栈。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [153] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出半监督双时点3D分割网络MambaX-Net：利用前一时点掩码与Mamba增强交叉注意力及形状提取模块，通过nnU-Net伪标签自训练，在纵向前列腺监测场景下实现更鲁棒的区域分割。


<details>
  <summary>Details</summary>
Motivation: 主动监测（AS）需通过多时点MRI追踪前列腺肿瘤进展，但现有深度学习模型多基于单时间点且依赖专家标注，难以迁移到纵向数据且受限于标签匮乏与噪声伪标签。

Method: 提出半监督的双时点3D分割架构：以时间t的MRI和上一时点的分割掩码为输入，包含Mamba增强交叉注意力模块（将Mamba块融入交叉注意力以捕捉时序演变与远程空间依赖）和形状提取模块（将前一时点掩码编码为潜在解剖表示）；结合基于预训练nnU-Net生成伪标签的自训练策略进行半监督学习。

Result: 在纵向AS数据集上，MambaX-Net在前列腺分区分割任务上显著优于对比的U-Net和Transformer基线，在有限与噪声数据下仍保持更好的分割精度。

Conclusion: 本文提出的MambaX-Net在纵向前列腺监测场景下能有效提升分割性能，尤其在标签稀缺和噪声伪标签条件下仍优于现有U-Net和Transformer方法。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [154] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet用仅图像级标签，通过分类器-重构器对抗学习、PAAM和CECCM模块生成高质量伪标签，实现接近有监督的像素级道路裂缝检测。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现可扩展且成本低的道路裂缝像素级检测。

Method: 提出了一个由分类器（生成CAMs）、重构器（测量特征可推断性）和检测器（输出像素级裂缝检测结果）组成的端到端框架。训练时，分类器与重构器通过交替对抗学习促使CAM覆盖完整裂缝区域，检测器从后处理的CAM生成伪标签学习。引入PAAM融合高层语义与低层结构，CECCM通过中心高斯权重和一致性约束精化CAM用于伪标签生成。

Result: 在三个图像级数据集上进行大量实验，结果显示WP-CrackNet与有监督方法性能可比，且优于现有弱监督方法。

Conclusion: WP-CrackNet在弱监督道路裂缝检测任务上能在仅使用图像级标签的情况下，达到接近有监督方法的检测效果，并优于现有弱监督方法，推动了大规模道路巡检的可扩展性。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [155] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D extends VGGT to dynamic 4D scenes using a dynamics-aware mask to disentangle motion for better pose and geometry; yields superior pose, depth, and point cloud results without post-processing.


<details>
  <summary>Details</summary>
Motivation: Existing 3D feed-forward models like VGGT are trained on static datasets and fail in real-world dynamic scenarios with moving/deformable objects; need a model that can handle dynamics without costly post-processing.

Method: Introduce a dynamics-aware aggregator that predicts a dynamics-aware mask to separate static and dynamic cues; use this mask to suppress motion cues for pose estimation and amplify them for geometry reconstruction in a feedforward architecture.

Result: PAGE-4D outperforms VGGT in dynamic scenarios across tasks: camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.

Conclusion: PAGE-4D successfully extends VGGT to handle dynamic scenes by disentangling static and dynamic information via a dynamics-aware aggregator, enabling accurate camera pose estimation and improved geometry reconstruction without post-processing.

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [156] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 提出UCIS4K数据集和UCIS-SAM（CBOM+FDTIM+MFFAM），通过通道与频域增强显著提升水下伪装实例分割性能，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有伪装实例分割方法多以陆地数据为主，对水下场景中的色偏、低对比和强伪装表现不足，因此需要专门的数据集与模型模块来提升水下伪装目标的检测与分割性能。

Method: 基于Segment Anything Model构建UCIS-SAM，包含：1) 通道平衡优化模块（CBOM）用于增强通道特征以适应水下色彩特性；2) 频域真实融合模块（FDTIM）在频域突出目标内在特征、抑制伪装干扰；3) 多尺度特征频率聚合模块（MFFAM）在多频段上强化低对比度目标边界。并使用新建UCIS4K数据集及公开基准进行大量实验。

Result: 在UCIS4K及公开基准上的实验表明，UCIS-SAM在分割精度和边界定位上均优于现有最先进方法，证明所提出的通道增强与频域处理策略对水下伪装实例分割有效。

Conclusion: 本文提出了首个水下伪装实例分割数据集UCIS4K并设计了基于SAM的UCIS-SAM网络，针对水下环境的色彩失真、低对比度和模糊等问题，通过三种模块（CBOM、FDTIM、MFFAM）提升了模型的水下特征提取和伪装目标分割能力，实验显示优于现有方法。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [157] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft用GPS和多智能体策略将文本解析为可程序化的图形化子任务，生成结构化、有纹理且可交互的3D资产，提升了可编辑性与语义准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D方法生成无结构网格、交互性差，不适合艺术工作流，需一种结构化、可编辑且语义丰富的生成方法。

Method: 引入Graph-based Procedural Shape (GPS) 表示，将复杂自然语言分解为子任务图；使用层次化LLM智能体初始化GPS，并通过迭代的程序化建模与绘制步骤精化模型和纹理；框架支持动画与用户编辑交互。

Result: 在定性和定量实验中，ShapeCraft在几何准确性与语义丰富性上优于现有基于LLM的方法，并展示了动画与用户定制编辑的多样性与可行性。

Conclusion: 本文提出ShapeCraft，用程序化形状程序（GPS）和多智能体框架实现从文本到结构化、可交互的3D模型生成，显著改善了几何准确性与语义表达。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [158] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 结合UAV实测点云与BIM合成数据的机器学习框架，实现了对基础设施3D点云中关键构件的高效自动分割，降低人工标注成本并缩短训练时间。


<details>
  <summary>Details</summary>
Motivation: UAV结合摄影测量能高效重建基础设施的细致3D模型，但从模型中分割出特定结构部件仍依赖耗时且易出错的人工标注，需自动化解决方案以提高监测效率与准确性。

Method: 构建基于机器学习的3D点云自动分割框架，利用UAV航拍生成的高分辨率点云与从BIM模型生成的合成点云共同训练分割模型，以弥补标注数据不足并提升模型泛化。

Result: 在轨道结构数据集上验证，模型能够高精度识别并分割轨道与枕木等主要部件；在使用小规模实测数据加BIM合成数据的设置下，训练时间大幅减少且分割性能保持在合理水平。

Conclusion: 提出的框架能有效结合UAV实测点云与BIM合成数据，减少人工标注需求，提高结构部件分割的精度与效率，能在轨道数据集上准确分割轨枕与轨道等主要构件，并在小规模训练集配合BIM数据时显著缩短训练时间且保持合理准确率。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [159] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2通过五个简单元素构建的简洁重建式框架，实现了跨模态、跨任务的统一无监督异常检测，并在多项基准上领先，特别是在多类和少样本场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有UAD方法在多类场景下表现落后于一对一模型，且研究被细分为多个专用子领域，造成部署困难，因而需要一个统一且高效的解决方案。

Method: 在标准重建框架中通过协调五个简单要素（论文未在摘要中列明具体验要素名称），贯彻“少即是多”的设计理念，从而实现性能提升并保持方法通用性和可扩展性。

Result: 在12个基准上广泛测试，Dinomaly2在2D、多视图、RGB-3D、RGB-IR等多种模态及单类、多类、少样本等设置均表现优越。多类模型在MVTec-AD和VisA上分别达到99.9%和99.3% I-AUROC；使用每类仅8个正常样本时在MVTec-AD和VisA上达到98.7%和97.4% I-AUROC。

Conclusion: Dinomaly2提出了一个统一且简洁的重建式无监督异常检测框架，能在多类、多模态、多任务场景下取得领先性能，特别在多类设置和少样本情况下显示出显著提升。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [160] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: 构建大规模跨年份细粒度车辆数据集CaMiT，提出并评估时间递增学习与时间感知生成方法，以提升模型对随时间变化外观的适应性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中物体外观随时间变化，现有视觉模型在跨时间测试时性能下降，且缺乏用于研究时间演化的细粒度数据集和相应的学习策略与生成方法。

Method: 构建了包含787K标注样本（190个车型，2007-2023）和5.1M无标签样本（2005-2023）的CaMiT数据集；进行了静态（传统）预训练与时间递增预训练两种策略比较；提出时间递增分类设定并评估两种方案：更新骨干网络的时间递增预训练和仅更新最终分类器的时间递增分类学习；在生成端加入时间元数据进行时间感知图像生成。

Result: 静态在域内预训练在资源使用上优于大规模通用模型且表现竞争性但面对跨年测试准确率下降；时间递增学习（两种策略）均能提升时间鲁棒性；时间感知生成能产生更符合时代特征的图像。

Conclusion: 该论文提出了一个面向时间演化的细粒度车辆数据集CaMiT，并在此基础上研究了跨年识别性能下降问题，提出并验证了两类时间递增学习策略以及时间感知的图像生成方法，展示了在时间鲁棒性与资源效率间的权衡。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [161] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: 利用高分辨率LiDAR DEM与自监督跨视角蒸馏预训练，DINO-CV能在植被遮挡与少量标注条件下成功分割低矮干石墙，性能显著且泛化良好。


<details>
  <summary>Details</summary>
Motivation: 传统光学影像受植被遮挡影响严重且人工标注成本高，导致大量干石墙未被识别。使用DEM可以绕开光学遮挡，捕捉地形结构信息；自监督跨视角学习可缓解标注数据稀缺问题。

Method: 提出DINO-CV框架：利用高分辨率航空LiDAR生成的DEM及其衍生影像作为输入，通过自监督的跨视角预训练策略（基于知识蒸馏）学习不同DEM衍生图之间的不变视觉和几何表示，支持ResNet、Wide ResNet与Vision Transformer等骨干网络，并在下游分割任务中进行微调。

Result: 在澳大利亚Budj Bim世界遗产区数据上，DINO-CV在测试区取得68.6% mIoU；仅用10%标注微调仍达63.8% mIoU，表明方法在植被茂密、标注匮乏的场景中具有实用性。

Conclusion: DINO-CV在高分辨率LiDAR DEM衍生产品上通过自监督跨视角蒸馏学习，有效提高了在植被遮挡和标注稀缺下的低矮干石墙分割性能，实验证明在Budj Bim地区能达到68.6% mIoU，少量标注（10%）下仍保持63.8% mIoU。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [162] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 比较LoRA微调的VLM与个性化轻量3D CNN在联邦暴力检测上的表现与能效，发现CNN更节能且在若干指标略优，VLM适合复杂/多模态场景，推荐混合部署。


<details>
  <summary>Details</summary>
Motivation: 在视频监控场景中，需在保持高准确性的同时降低通信、计算成本和环境影响，探究在非IID条件下哪些轻量或多模态模型更适合联邦暴力检测任务。

Method: 选取LLaVA-7B作为VLM代表和一个65.8M参数的CNN3D作为紧凑模型，采用联邦学习框架在真实非IID的数据分布下进行比较。评估指标包括准确率、校准（log loss）、ROC AUC及训练/推理能耗和CO2排放。对LoRA微调的VLM进行零样本和联邦微调对比，同时对个性化CNN3D进行本地微调，分析性能与能效权衡。

Result: 两种方法均达到>90%准确率；CNN3D在ROC AUC和log loss上略优且能耗更低；VLM在场景理解与多模态推断上更有优势。作者还量化了训练/推理的能耗与CO2排放，并提出混合部署策略作为可重复的基线。

Conclusion: 作者比较了两种节能的联邦学习方法用于暴力检测：基于视觉-语言模型（VLM）的零样本与LoRA微调，以及个性化轻量级3D CNN训练。结论是两者均能达到90%以上的准确率，且在ROC AUC和log loss上，CNN3D略优于LoRA微调的VLM，同时能耗更低；VLM在上下文推理和多模态推断上仍具优势。作者建议混合部署策略：平时用轻量CNN分类，复杂场景启用VLM。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [163] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 提出基于双线程的流式4D全景分割框架，预测与推理分离以实现实时、鲁棒的动态对象分割，且可扩展至现有分割方法，实验证明在多数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 在高度动态环境（如密集人群撤离与复杂场景下的自动驾驶）中，需要在有限时延内对实时流帧进行细粒度感知；现有方法在高FPS下鲁棒性和实时性不足。

Method: 设计了Dual-Thread系统，包括预测线程和推理线程：预测线程利用历史运动与几何信息提取特征并预测未来动态；推理线程对齐最新记忆并补偿自车运动及动态物体运动，确保对到达帧的及时推理。框架可泛化地集成到现有3D/4D分割方法以实现实时能力。

Result: 在HOI4D、SemanticKITTI和nuScenes数据集上的综合实验表明，该框架在复杂场景中对动态物体的预测与分割表现优越，尤其在高帧率下展示出更强鲁棒性。

Conclusion: 本论文提出了一种名为4DSegStreamer的双线程流式4D全景分割框架，能在高帧率实时场景下高效且鲁棒地处理动态对象。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [164] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 提出针对物理真实感的PICABench与PICAEval，并构建PICA-100K用于学习物理。评测显示当前模型在物理一致性上仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有编辑模型虽能完成复杂指令，但忽略伴随的物理效应，导致生成不真实；需要系统评估并推动模型学习物理一致性。

Method: 提出PICABench基于八个子维度（光学、力学、状态转换）评估常见编辑操作，并用VLM作为评判器结合每例区域级人工标注与问题的PICAEval协议，同时通过从视频中学习构建PICA-100K训练集。

Result: 使用PICABench和PICAEval评估主流模型后发现大多数模型在物理一致性方面表现欠佳，提出的视频学习数据集为改进提供初步方向。

Conclusion: 物理真实感在图像编辑领域仍然是主要短板，尽管指令完成度高，但阴影、反射、相互作用等物理效应未被充分处理。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [165] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: IC-MoE 通过专家混合（含投票融合）和语义引导对比学习，在保留预训练结构的同时增强高层语义表示，显著提升医学图像分割效果与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有对自然图像分割基础模型微调到医学图像时存在两大问题：高层语义特征表示不足以及微调过程破坏预训练权重结构，使得性能和泛化受限。

Method: 构建三类专家（基础专家、语义专家、自适应专家），采用像素概率自适应投票策略进行专家选择与融合以保证标签一致性与负载均衡；引入语义引导的对比学习以改善对比学习中弱监督问题，强化高层特征表示。

Result: 在三个公开医学图像分割数据集上的大量实验表明，IC-MoE 超越了现有 SOTA 方法，表现出更好的分割精度和跨场景泛化能力。

Conclusion: IC-MoE 提出了一种基于专家混合与语义引导对比学习的微调策略，能在保留预训练权重结构完整性的同时增强高层语义特征表示，从而提升医学图像分割性能与泛化能力。

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [166] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 提出多语种TIPR基准和Bi-IRRA框架，通过双向隐式关系推理与多维全局对齐，有效改善跨模态与跨语言对齐，达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有TIPR方法要么是全局对齐忽视细粒度差异，要么是局部对齐依赖先验显式部位信息，且大多集中于英语，限制了多语言场景下的应用。作者旨在同时处理细粒度跨模态关系与多语种扩展。

Method: Bi-IRRA包含：1) 双向隐式关系推理模块：通过对图像与文本的掩码预测（双向预测被掩码的图像/文本），隐式建模局部跨模态/跨语言关系，避免显式部位对齐对先验的依赖；2) 多维全局对齐模块：在全局层面建立跨模态桥接，减轻模态异质性；3) 多语种数据构建：先用大语言模型自动翻译，再结合领域知识进行修正，形成多语种TIPR基准。

Result: 在所构建的多语种TIPR数据集上，Bi-IRRA在所有评测集上均取得新的最先进（SOTA）性能，且代码与数据已开源。

Conclusion: 该论文提出了面向多语种文本到图像行人检索（multilingual TIPR）的新基准和方法Bi-IRRA，通过双向隐式关系推理与对齐以及多维全局对齐，解决跨模态和跨语言的对齐困难，取得了新的SOTA成绩。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [167] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 提出OP3Det：一种无提示的类无关开世界3D检测器，融合2D语义与3D几何先验并用跨模态专家混合动态融合图像与点云，实现对未知类别的高效发现，实验显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有闭集3D检测器难以推广到开放世界，直接使用3D开放词汇模型受词汇扩展与语义重叠限制，亟需一种不依赖手工文本提示、可发现未知类别的通用3D对象检测方法。

Method: 通过结合2D语义先验与3D几何先验生成类无关候选框，并使用跨模态专家混合（Mixture of Experts）动态路由点云与图像特征进行学习，从而获取泛化的3D对象性表征。

Result: 在多项实验中，OP3Det在召回/AR指标上最多领先现有开放世界3D检测器16.0%，并较闭集3D检测器提升13.5%。

Conclusion: OP3Det有望显著提升3D开放世界目标检测能力，实现类无关且无需文本提示的普适目标发现。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [168] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出无需复杂技巧的ODE采样器参数化并结合对抗训练（GAS），实现少步扩散采样下更高的细节保真与更少伪影。


<details>
  <summary>Details</summary>
Motivation: 现有将扩散模型蒸馏为少步ODE求解器的方法虽减少了函数评估次数，但通常依赖复杂训练技巧且未专注于细粒度细节保持，导致生成图像细节欠佳或出现伪影。

Method: 提出一种简洁的ODE采样器参数化（Generalized Solver），并在原有蒸馏损失上结合对抗训练以形成GAS；训练目标同时保留蒸馏对齐和对抗判别，以减少伪影并增强细节。

Result: 在相似资源约束下，GAS在视觉质量与细节保真上优于现有求解器训练方法，并降低采样步骤数，同时代码已公开。

Conclusion: 本文提出了Generalized Solver（GS）及其对抗扩展Generalized Adversarial Solver（GAS），通过简单的ODE采样器参数化和加入对抗训练，在无需复杂训练技巧的前提下提升了少步扩散采样的细节保真与视觉质量。

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [169] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT通过进化近似Hessian非对角结构并结合自监督重要性评分，提供一种快速、无标签、无重训的结构化剪枝方法，使预训练视觉Transformer能在任意计算预算下弹性推理，且效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型只有若干预先确定的尺寸，难以满足现实部署中多变的计算资源和延迟需求，因而需要一种能在不重训且无需标签的情形下快速生成不同计算预算下模型的方案。

Method: 结合梯度信息与网络间结构相关性的进化式逼近（近似Hessian的非对角元素），并设计自监督重要性评分用于层内过滤与剪枝，整个过程在单次执行中完成且对没有分类头的模型也适用。

Result: 在DINO、SigLIPv2、DeiT和AugReg等预训练模型上，SnapViT在各种稀疏率下均优于现有最先进方法；在单个A100 GPU上少于5分钟即可生成可调节的弹性模型。

Conclusion: 该论文提出了SnapViT，一种可在单次运行中为预训练视觉Transformer生成可弹性调整计算预算的结构化剪枝方法，实现无需重训练和标签的数据驱动模型压缩。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [170] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 通过图像2x2分块并对每块独立识别再集成决策，本文在手绘图像上实现了对帕金森病检测的高准确率和对未见患者的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于数据集规模不足和对未见患者表现不稳定，作者通过分块与集成策略增强数据多样性和局部特征鲁棒性以提高泛化能力。

Method: 两阶段流程：第一阶段对手绘图类型（圆、蜿蜒、螺旋）进行分类；第二阶段将图像划分为2x2块，分别提取每块特征并进行PD识别，最后用集成方法融合各块决策得到最终结果。

Result: 在NewHandPD数据集上取得了对见过患者97.08%和未见患者94.91%的准确率，未见/见过准确率下降仅2.17个百分点，显著优于以往工作的4.76个百分点下降。

Conclusion: 本文提出的基于图像分块与集成决策的两阶段方法，有效提升了帕金森病（PD）检测的鲁棒性与对未见患者的泛化能力。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [171] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 本文提出基于YOLOv11与荧光通道叠加的两步自动化流程，高效准确地对流式细胞图像中的血细胞簇进行分类与表型识别（>95%准确率）。


<details>
  <summary>Details</summary>
Motivation: 现有基于机器学习的流式细胞图像自动分析多集中在单细胞上，但缺乏针对具有不规则形状和异质细胞组成的细胞簇的自动分析工具，且簇内细胞鉴定需要多通道染色信息。

Method: 两步法：1) 使用微调的YOLOv11对亮场图像进行图像分类/检测，将图像分为细胞簇与非簇，并与CNN和ViT等模型比较性能；2) 将聚类轮廓与多通道荧光染色区域叠加，通过空间重叠来判定簇内细胞的表型，以避免细胞碎片和染色伪影带来的误判。

Result: 框架在簇分类和表型识别两项任务上均达到了95%以上的准确率，优于传统CNN和ViT模型；方法在血细胞数据集上验证，并具有扩展到免疫细胞和肿瘤细胞簇分析的潜力。

Conclusion: 该论文提出了一个自动化框架，用于分析流式细胞术获得的循环血细胞簇（CCC）图像，能够将图像分为细胞簇与非簇并识别簇内细胞类型，从而为疾病相关生物标志物分析提供工具。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [172] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS提出了从未约束雨滴图像到清晰3DGS重建的完整评测管线和真实数据集，揭示了对焦、位姿与点云初始化等因素对3D重建的关键影响，为提升雨天下3DGS鲁棒性提供基准与方向。


<details>
  <summary>Details</summary>
Motivation: 现有评测多基于合成雨滴且假设已知相机位姿，无法反映真实场景中雨滴对位姿估计与点云初始化的干扰以及合成与真实雨滴的域差，导致3DGS在实际应用中性能下降。

Method: 构建完整管线：数据准备（采集包含雨滴对焦、背景对焦和无雨真值的真实数据集）、数据处理（处理不同雨滴干扰、相机位姿估计与点云初始化评估）、雨滴感知3DGS评估（单图去雨方法对比与3DGS训练对比）。通过实验分析不同组件对最终重建的影响。

Result: 收集了真实雨滴重建数据集并通过实验揭示：相机对焦位置显著影响3DGS重建质量；不准确的位姿估计与点云初始化会严重干扰重建；现有方法在真实雨滴下泛化能力较差。基准为开发更鲁棒的3DGS方法指明方向。

Conclusion: 本文提出了RaindropGS基准，针对真实雨滴条件下3D Gaussian Splatting（3DGS）重建受阻问题，强调现有合成评测不足并给出改进方向。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [173] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench：面向多轮视频对话评估的基准，涵盖六项以感知与交互为核心的能力，共987个多轮对话，评估显示现有MLLM在多轮视频对话上存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型评估多局限于单轮视觉问答，无法衡量模型在连续、多轮交互场景下的理解、跟踪和对话管理能力。为推动更符合实际应用的研究，需要一个专注于多轮视频对话的全面基准。

Method: 构建MT-Video-Bench：定义六类能力（集中于感知与交互）、收集并精心编制987个多轮视频对话样本，覆盖多个领域和真实应用场景（如体育分析、基于视频的智能教学）。随后使用该基准对主流开源与闭源MLLMs进行大规模评测并分析其表现差异与缺陷。

Result: 构建了包含987个多轮对话的MT-Video-Bench并公开计划发布。基准评测结果显示多种先进模型在多轮视频对话上存在显著性能差距和短板，尤其在跨轮上下文理解、长时记忆、交互式推理等方面表现不足。

Conclusion: 该论文提出了MT-Video-Bench，一个用于评估多模态大语言模型（MLLM）在多轮视频对话场景下能力的基准测试。作者发现现有评测多为单轮问答，无法反映实际多轮交互需求；通过构建涵盖987个多轮对话、六项核心能力的基准，强调感知与交互能力。基准评测了多种开放与闭源模型，揭示了在多轮视频对话处理上的明显差距和局限。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [174] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 针对离线签名伪造检测的跨数据集泛化问题，本文比较了原始图像与shell预处理两种特征学习策略。原始图像模型总体更优，shell方法显示改进空间，但尚需进一步优化以实现稳定的跨域鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前离线签名识别方法在跨数据集场景下泛化性差，手写风格和采集协议差异导致性能下降，故本研究探索特征学习与预处理策略以提高跨域鲁棒性。

Method: 使用两条实验流水线：1) 基于原始签名图像训练的深度学习模型；2) 采用shell预处理后的图像训练的模型。在三种公开基准（CEDAR、ICDAR、GPDS Synthetic）上进行训练和跨数据集测试，比较性能差异并分析行为模式。

Result: 实验显示：原始图像模型总体上在三个基准上取得更高性能；shell预处理模型在某些情形下表现出独特行为模式，具有改进潜力，但尚未超越原始图像方法。研究未得出二者的决定性优劣结论。

Conclusion: 本文旨在提升离线签名伪造检测的跨数据集泛化能力，但未能确定预处理(shell)方法比原始图像更优。研究表明：原始图像模型在多数基准上表现更好，但shell预处理展现出进一步优化的潜力。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [175] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 用DiT变体的I2V模型按关键帧条件生成行人轨迹，短期效果可行，长期和复杂交互仍需改进。


<details>
  <summary>Details</summary>
Motivation: 评估大规模预训练的I2V模型能否在拥挤公共场景中捕捉并生成现实的行人运动动态，用以探索其作为软世界模型在智能交通与行人行为建模中的应用潜力。

Method: 使用基于DiT的图像到视频模型，在行人轨迹基准数据集上，将部分观测关键帧作为条件输入，生成后续视频帧；然后从生成视频中提取行人位置信息，计算速度、加速度、转向角等动力学指标，并与真实轨迹进行定量比较。

Result: 模型能在短期内（若干步）生成大体合理的行人移动模式，能复现部分流动性与行人间交互，但在长期一致性、碰撞避免、细粒度目标导向行走（如避障、路线保持）和多主体交互复杂性方面表现不足。

Conclusion: 这些模型在生成行人运动模式方面表现出一定潜力，但仍有明显局限性。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [176] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种基于矩阵分解与投影残差的无训练多参考VPR方法，在多外观和多视角场景下显著提升Recall@1，兼容多种描述子且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么通过大规模训练提升鲁棒性但代价高昂，要么通过描述子层面的投票或聚合避免训练但常依赖启发式规则且在外观和视角变化下收益有限，因此需要一种无训练、描述子无关且能更好整合多参考信息的方法。

Method: 将多个参考描述子按列构成矩阵，利用矩阵分解得到基表示（basis），然后将查询描述子投影到基空间并计算投影残差作为匹配度量；该方法不依赖训练，兼容多种描述子，轻量且高效。

Result: 在文中构建的SotonMV基准上以及其他多外观数据集上实验表明，该方法在多外观数据上相比单参考Recall@1提高约18%，在非结构化（多视角）数据上相比多参考基线提升约5%，显示出优越的泛化能力与轻量性。

Conclusion: 本文提出了一种无训练、描述子无关的多参考视觉定位方法，通过将多个参考描述子矩阵分解为基表示并采用投影残差匹配来联合建模地点，从而提升在多外观和多视角变化下的识别性能。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [177] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 通过Deep-UNet分割+双DenseNet编码器的跨注意力融合，并结合Transformer整合临床元数据，本文实现了更准确且可解释的皮肤病变分类。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型存在“黑盒”问题且易受背景伪线索影响；同时，利用分割后的病变区域和临床信息可引导模型关注病理要点，提升性能与可靠性。

Method: 方法包含两个阶段：1) 提出带双注意力门（DAG）与ASPP的Deep-UNet用于病变分割；2) 分类采用两个DenseNet201编码器（原图与分割图）提取特征，通过多头交叉注意力进行融合，并用基于Transformer的模块融合患者元数据（年龄、性别、部位）；最后用Grad-CAM可视化模型注意力。

Result: 在HAM10000及ISIC2018/2019数据集上，提出方法取得了最先进的分割性能，并在分类准确率和平均AUC上相较基线有显著提升；Grad-CAM证明模型关注实际病变区域而非背景。

Conclusion: 本文提出结合精确分割与临床元数据的双编码器注意力框架，可同时提升皮肤病变分类的准确性与可解释性，结果显示在分割和分类任务上均明显优于基线并能避免背景相关伪线索。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [178] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA在不需训练且与模型架构无关的条件下，通过将视觉稀疏拆分为prefill剪枝与decoding检索，显著加速VLM推理并提升部分任务精度。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在高分辨率图像、长视频、多轮对话场景中受制于大量视觉token导致的推理延迟，需要一种同时兼顾推理效率与多轮对话保真度的稀疏化方法。

Method: 提出将视觉稀疏性解耦为两部分：1) 在prefill阶段进行查询无关的冗余视觉token剪枝以减少缓存大小；2) 在decoding阶段基于查询检索相关token以保证多轮对话的上下文一致性。该方法在无需训练且与架构无关的AWQ优化推理管线中实现。

Result: 在长上下文视频任务上，SparseVILA在prefill阶段最多加速4.0倍，decoding阶段2.5倍，端到端总体加速2.6倍；并在文档理解与推理任务上提高了准确率。

Conclusion: SparseVILA通过在预填充(prefill)和解码(decoding)阶段分离视觉稀疏策略，实现了在长上下文视频与文档理解任务上显著的推理加速，同时保持或提升任务精度。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [179] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: 提出UltraCUA，通过混合低层GUI动作与高层程序化工具调用，配合大规模合成任务和两阶段训练，显著提升计算机使用代理的效果与效率。


<details>
  <summary>Details</summary>
Motivation: 现有CUA仅依赖低级GUI操作，导致对视觉锚定要求高、执行链长、易出错；而程序化接口（API、工具）能提供更可靠、高效的能力，但CUA未能利用。

Method: 构建可扩展的程序化工具管线、生成1.7万+可验证任务的合成数据、收集混合动作轨迹（低层GUI与高层工具调用）、采用监督微调+在线强化学习的两阶段训练，使模型学会在低层与高层动作之间策略性切换。

Result: 7B与32B模型在OSWorld上相比基线平均提升22%（成功率），步骤数减少11%；在WindowsAgentArena的域外评估中达21.7%成功率，优于在Windows数据上训练的基线。混合动作显著降低错误传播并保持执行效率。

Conclusion: UltraCUA有效提升了计算机使用任务的执行效率与成功率，通过混合动作融合GUI原语与高层程序化工具调用，减少级联失败并提高速度。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [180] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: 通过把长文本渲成图像并用VLM处理，Glyph实现了显著的token压缩与速度提升，同时保持与主流LLM相当的长上下文理解能力，适用于百万token级扩展与多模态任务。


<details>
  <summary>Details</summary>
Motivation: 直接扩大token级别的上下文窗口在计算和内存上代价高昂；通过视觉化文本可以高效压缩长期文本，使现有VLM/LLM能处理更长的语境而无需线性放大序列长度。

Method: 将文本按可控渲染参数（如字号、行间距、布局等）转为图像输入到VLM，并引入由LLM驱动的遗传搜索来自动寻找在压缩率和准确率之间的最佳渲染配置；同时评估预填充、解码和SFT训练效率提升。

Result: 在多项长上下文基准上实现3-4x的token压缩，性能与Qwen3-8B等主流LLM相当；预填充与解码速度约提升4x，SFT训练速度提升约2x；极端压缩情形下，128K上下文的VLM可扩展处理到百万token级任务；渲染文本也有利于文档理解等多模态下游任务。

Conclusion: Glyph通过将长文本渲染为图像并用视觉-语言模型处理，提供了一种可行的长上下文扩展替代方案，显著压缩文本并保持模型性能，从而缓解了百万级token上下文的计算与内存瓶颈。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: 针对MM-DiT提出ConsistEdit，通过vision-only attention、mask-guided pre-attention fusion和query/key/value差异化操作，实现了在所有步骤与层上的一致且强力的文本指导编辑，适用于图像/视频、多轮和多区域编辑。


<details>
  <summary>Details</summary>
Motivation: 现有训练零成本的注意力控制方法难以在强编辑能力与保持源一致性之间取得平衡，尤其在多轮和视频编辑中易累积视觉误差；且多数方法只做全局一致性，无法对局部属性做细粒度修改。MM-DiT的新架构和跨模态融合机制为解决这些问题提供了机会。

Method: 分析MM-DiT的注意力机制，提出三项关键技术：视觉仅注意力控制（vision-only attention control）、掩码引导的预注意力融合（mask-guided pre-attention fusion）和区分性操作query/key/value token，并将这些机制整合进ConsistEdit，在所有推理步骤及注意力层统一应用以实现稳定的一致性与强编辑能力。

Result: 在大量图像和视频编辑任务上（包括结构一致与非一致场景），ConsistEdit达到了最先进性能，是首个可在所有推理步和注意力层上无需手工调整且支持多轮多区域编辑的方法，同时实现渐进结构一致性控制。

Conclusion: ConsistEdit在MM-DiT架构上通过对注意力机制的深入分析，提出了视觉单独控制、基于掩码的预注意融合以及查询/键/值的差异化操作，从而在保持源一致性与增强编辑强度之间取得更好平衡，实现了在所有推理步和注意力层上的无手工调整编辑，支持多轮和多区域编辑并可渐进控制结构一致性。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [182] [Unified Peripartum Database with Natural-Language-to-SQL Capabilities at Udine University Hospital: Design and Prototype](https://arxiv.org/abs/2510.16388)
*Doriana Armenise,Ginevra Battello,Andrea Brunello,Lorenza Driul,Angelo Montanari,Elisa Rizzante,Nicola Saccomanno,Andrea Salvador,Serena Xodo,Silvia Zermano*

Main category: cs.DB

TL;DR: 构建了一个基于ER图的围产期关系数据库并加装NL2SQL接口，整合异构产科数据，便于临床查询与研究。


<details>
  <summary>Details</summary>
Motivation: 医院中产科信息往往分散在不同电子病历模块、设备及实验室系统，阻碍围产期护理、审核与可复现研究。需要可查询、可计算的数据资产来支持临床决策与研究。

Method: 与临床医师共同定义需求，绘制实体-关系图（ER图），由此推导逻辑模式并实现SQL数据库，将产科历史、当前妊娠记录、产程、分娩及新生儿结局等异构来源整合入统一关系模型；在其上构建NL2SQL模块，使临床人员能以自然语言发起查询。

Result: 设计并原型实现了一个围产期统一关系型数据库，并部署了自然语言到SQL的查询层，能将异构数据连接为可查询的临床记录，降低临床人员进行审计与探索性分析的门槛。

Conclusion: 该论文提出并验证了一种实用方案，将分散的围产期临床数据整合到统一的关系型数据库，并提供自然语言到SQL的查询层，以便临床审计与探索性研究。

Abstract: The fragmentation of obstetric information across electronic health record
modules, device repositories, and laboratory systems, as it is common in
hospitals, hinders both intrapartum care and reproducible research. In this
work, we present a practical blueprint for transforming heterogeneous
peripartum records into computable, queryable assets by designing and
prototyping a unified peripartum relational database with
natural-language-to-SQL (NL2SQL) capabilities at the Obstetrics Clinic of Udine
University Hospital. Requirements were co-defined with clinicians and
formalized as an Entity-Relationship diagram, from which the logical schema and
SQL implementation of the database were then derived. The latter integrates
heterogeneous sources to connect maternal anamnestic and longitudinal history,
current-pregnancy findings, intrapartum course, and delivery and neonatal
outcomes. The NL2SQL layer enables clinicians to pose natural-language queries
to the system, lowering barriers to audit and exploratory analysis.

</details>


### [183] [Declarative Techniques for NL Queries over Heterogeneous Data](https://arxiv.org/abs/2510.16470)
*Elham Khabiri,Jeffrey O. Kephart,Fenno F. Heath III,Srideepika Jayaraman,Fateh A. Tipu,Yingjie Li,Dhruv Shah,Achille Fokoue,Anu Bhamidipaty*

Main category: cs.DB

TL;DR: 为解决工业环境下数据库与API混合调用的异构性，作者扩展Spider数据集并提出声明式方法，实验显示该方法优于现有LLM基线，且公开了基准。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中数据源高度异构（多数据库、外部API等），现有将自然语言转为API/数据库调用的LLM方法难以应对这种异构性，导致实用性不足。

Method: 作者通过扩展Spider数据集，加入需要同时调用数据库和API的任务，构建两种异构场景；提出一种声明式处理异构数据源的方法（可能通过中间抽象层或统一查询语言）来翻译自然语言查询为组合调用，并与现有基于LLM的agent和代码生成系统进行对比实验。

Result: 在新构建的两个扩展Spider基准上，声明式方法在准确性或鲁棒性上显著优于最先进的LLM agentic或命令式代码生成系统；并且作者将增强后的基准公开以供社区使用。

Conclusion: 本文提出在异构工业数据源下，用声明式方法比基于LLM的agent或代码生成更有效地处理组合数据库与API调用的问题，显著提高了问答系统在扩展后的Spider基准上的表现，并发布了两个新基准。

Abstract: In many industrial settings, users wish to ask questions in natural language,
the answers to which require assembling information from diverse structured
data sources. With the advent of Large Language Models (LLMs), applications can
now translate natural language questions into a set of API calls or database
calls, execute them, and combine the results into an appropriate natural
language response. However, these applications remain impractical in realistic
industrial settings because they do not cope with the data source heterogeneity
that typifies such environments. In this work, we simulate the heterogeneity of
real industry settings by introducing two extensions of the popular Spider
benchmark dataset that require a combination of database and API calls. Then,
we introduce a declarative approach to handling such data heterogeneity and
demonstrate that it copes with data source heterogeneity significantly better
than state-of-the-art LLM-based agentic or imperative code generation systems.
Our augmented benchmarks are available to the research community.

</details>


### [184] [AVOCADO: The Streaming Process Mining Challenge](https://arxiv.org/abs/2510.17089)
*Christian Imenkamp,Andrea Maldonado,Hendrik Reiter,Martin Werner,Wilhelm Hasselbring,Agnes Koschmider,Andrea Burattin*

Main category: cs.DB

TL;DR: AVOCADO是一个面向流式过程挖掘的标准化挑战框架，分离概念与实例层，定义了多项流式评估指标，旨在促进社区协作与方法比较；但缺乏实现细节与实验验证。


<details>
  <summary>Details</summary>
Motivation: 应对流式事件数据的实时性与增量处理需求，解决现有过程挖掘方法在流数据场景下缺乏统一评测基准的问题，推动社区在流式过程挖掘方向的研究与比较。

Method: 提出框架性设计（分层挑战结构、评估指标集合），并用指标（准确率、MAE、RMSE、处理延迟、鲁棒性）对算法进行评估。主要是规范化和建议导向的方法论，未见具体实现或开源平台细节。

Result: 提出AVOCADO框架作为基础规范，并邀请社区扩展（如加入吞吐量、内存消耗指标及处理乱序事件等现实问题）。当前未展示大规模实验结果或基准对比。

Conclusion: AVOCADO提出了一个标准化的流式过程挖掘挑战框架，通过将概念层和实例化层分离，为算法评估提供结构化流程，并采用了多项流式特有的评估指标。该框架有助于促进社区协作与创新，但当前描述在实现细节和实验验证上较为薄弱。

Abstract: Streaming process mining deals with the real-time analysis of streaming data.
Event streams require algorithms capable of processing data incrementally. To
systematically address the complexities of this domain, we propose AVOCADO, a
standardized challenge framework that provides clear structural divisions:
separating the concept and instantiation layers of challenges in streaming
process mining for algorithm evaluation. The AVOCADO evaluates algorithms on
streaming-specific metrics like accuracy, Mean Absolute Error (MAE), Root Mean
Square Error (RMSE), Processing Latency, and robustness. This initiative seeks
to foster innovation and community-driven discussions to advance the field of
streaming process mining. We present this framework as a foundation and invite
the community to contribute to its evolution by suggesting new challenges, such
as integrating metrics for system throughput and memory consumption, and
expanding the scope to address real-world stream complexities like out-of-order
event arrival.

</details>


### [185] [Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models](https://arxiv.org/abs/2510.17301)
*Panos Kalnis. Shuo Shang,Christian S. Jensen*

Main category: cs.DB

TL;DR: MapMuse uses LLMs + RAG and cinematic storytelling to turn spatio-temporal data into engaging narratives (heatmap-wide and single-trip perspectives); promising approach with open research questions.


<details>
  <summary>Details</summary>
Motivation: Traditional spatio-temporal visualizations are complex and often inaccessible to general audiences; aim to translate complex movement data into emotionally engaging, narrative-driven stories to improve understanding and actionability.

Method: Use of large language models with retrieval-augmented generation and agent-based techniques, applying cinematic storytelling principles to generate narratives from spatio-temporal datasets; case study on taxi trajectories with heatmap and single-journey narratives.

Result: Produced two narrative perspectives: a heatmap-based urban mobility story revealing patterns, and a single-journey narrative enriched with landmarks and temporal context; argued storytelling improves comprehension and engagement.

Conclusion: MapMuse demonstrates that cinematic storytelling combined with LLMs and RAG can make spatio-temporal data more accessible and engaging, but remains exploratory with open challenges.

Abstract: Spatio-temporal data captures complex dynamics across both space and time,
yet traditional visualizations are complex, require domain expertise and often
fail to resonate with broader audiences. Here, we propose MapMuse, a
storytelling-based framework for interpreting spatio-temporal datasets,
transforming them into compelling, narrative-driven experiences. We utilize
large language models and employ retrieval augmented generation (RAG) and
agent-based techniques to generate comprehensive stories. Drawing on principles
common in cinematic storytelling, we emphasize clarity, emotional connection,
and audience-centric design. As a case study, we analyze a dataset of taxi
trajectories. Two perspectives are presented: a captivating story based on a
heat map that visualizes millions of taxi trip endpoints to uncover urban
mobility patterns; and a detailed narrative following a single long taxi
journey, enriched with city landmarks and temporal shifts. By portraying
locations as characters and movement as plot, we argue that data storytelling
drives insight, engagement, and action from spatio-temporal information. The
case study illustrates how MapMuse can bridge the gap between data complexity
and human understanding. The aim of this short paper is to provide a glimpse to
the potential of the cinematic storytelling technique as an effective
communication tool for spatio-temporal data, as well as to describe open
problems and opportunities for future research.

</details>


### [186] [Approximate Nearest Neighbor Search of Large Scale Vectors on Distributed Storage](https://arxiv.org/abs/2510.17326)
*Kun Yu,Jiabao Jin,Xiaoyao Zhong,Peng Cheng,Lei Chen,Zhitao Shen,Jingkuan Song,Hengtao Shen,Xuemin Lin*

Main category: cs.DB

TL;DR: DSANN是一种支持分布式存储的图-聚类混合ANNS系统，通过并发构建、点聚合图与异步I/O，在分布式环境下实现对十亿级向量的高效索引与搜索，解决单机存储带来的成本、规模和可用性问题。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的ANNS算法的索引需保存在单机内存或磁盘，造成高存储成本、规模受限和单点故障，分布式存储虽能缓解这些问题，但缺乏高效有效的分布式向量索引算法。

Method: 采用并发索引构建方法降低构建复杂度；引入Point Aggregation Graph（点聚合图）利用图结构信息聚合相似向量；通过异步I/O与分布式存储结合，提高查询吞吐量与存储效率。

Result: 实验表明，DSANN能在分布式存储场景下高效且有效地对大规模向量数据集进行索引、存储与搜索，提升存储效率并保证高召回率与吞吐量。

Conclusion: 提出了一个图-聚类混合索引与搜索系统DSANN，实现了在分布式存储环境下对十亿级向量的高效索引、存储与近似最近邻搜索，同时保证索引服务的高可用性。

Abstract: Approximate Nearest Neighbor Search (ANNS) in high-dimensional space is an
essential operator in many online services, such as information retrieval and
recommendation. Indices constructed by the state-of-the-art ANNS algorithms
must be stored in single machine's memory or disk for high recall rate and
throughput, suffering from substantial storage cost, constraint of limited
scale and single point of failure. While distributed storage can provide a
cost-effective and robust solution, there is no efficient and effective
algorithms for indexing vectors in distributed storage scenarios. In this
paper, we present a new graph-cluster hybrid indexing and search system which
supports Distributed Storage Approximate Nearest Neighbor Search, called DSANN.
DSANN can efficiently index, store, search billion-scale vector database in
distributed storage and guarantee the high availability of index service. DSANN
employs the concurrent index construction method to significantly reduces the
complexity of index building. Then, DSANN applies Point Aggregation Graph to
leverage the structural information of graph to aggregate similar vectors,
optimizing storage efficiency and improving query throughput via asynchronous
I/O in distributed storage. Through extensive experiments, we demonstrate DSANN
can efficiently and effectively index, store and search large-scale vector
datasets in distributed storage scenarios.

</details>


### [187] [DeepEye-SQL: A Software-Engineering-Inspired Text-to-SQL Framework](https://arxiv.org/abs/2510.17586)
*Boyan Li,Chong Chen,Zhujun Xue,Yinan Mei,Yuyu Luo*

Main category: cs.DB

TL;DR: 将Text-to-SQL视作小型软件开发并用SDLC驱动的可验证流水线（语义检索、N版本生成、单元测试与LLM修正、置信度聚类选择）能显著提高系统级可靠性，且在不开微调下用~30B LLM达到了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在模块层面改进但缺乏贯穿全流程的结构化编排，导致系统级可靠性不足。作者主张把问题视为软件工程问题，通过SDLC流程强制正确性。

Method: 提出基于SDLC的四阶段流水线：语义值检索与健壮的schema linking；多样化推理的N版本SQL生成；通过单元测试和LLM引导的定向修正进行确定性验证；基于执行结果聚类的置信度感知选择与不平衡的两两裁决策略。使用约30B的开源LLM无微调实验。

Result: 在BIRD-Dev上取得73.5%执行准确率，在Spider-Test上取得89.8%，超越了当前最先进方法，表明有原则的编排比单纯放大模型更关键。

Conclusion: 本论文认为Text-to-SQL需要从自由生成转向软件工程式的结构化编排，以提升系统级可靠性，并提出了DeepEye-SQL框架。

Abstract: Large language models (LLMs) have advanced Text-to-SQL, yet existing
solutions still fall short of system-level reliability. The limitation is not
merely in individual modules - e.g., schema linking, reasoning, and
verification - but more critically in the lack of structured orchestration that
enforces correctness across the entire workflow. This gap motivates a paradigm
shift: treating Text-to-SQL not as free-form language generation but as a
software-engineering problem that demands structured, verifiable orchestration.
We present DeepEye-SQL, a software-engineering-inspired framework that reframes
Text-to-SQL as the development of a small software program, executed through a
verifiable process guided by the Software Development Life Cycle (SDLC).
DeepEye-SQL integrates four synergistic stages: it grounds ambiguous user
intent through semantic value retrieval and robust schema linking; enhances
fault tolerance with N-version SQL generation using diverse reasoning
paradigms; ensures deterministic verification via a tool-chain of unit tests
and targeted LLM-guided revision; and introduces confidence-aware selection
that clusters execution results to estimate confidence and then takes a
high-confidence shortcut or runs unbalanced pairwise adjudication in
low-confidence cases, yielding a calibrated, quality-gated output. This
SDLC-aligned workflow transforms ad hoc query generation into a disciplined
engineering process. Using ~30B open-source LLMs without any fine-tuning,
DeepEye-SQL achieves 73.5% execution accuracy on BIRD-Dev and 89.8% on
Spider-Test, outperforming state-of-the-art solutions. This highlights that
principled orchestration, rather than LLM scaling alone, is key to achieving
system-level reliability in Text-to-SQL.

</details>


### [188] [This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!](https://arxiv.org/abs/2510.17748)
*William Zhang,Wan Shen Lim,Andrew Pavlo*

Main category: cs.DB

TL;DR: Booster用查询级历史上下文和LLM建议加束搜索来辅助现有DBMS调优器，应对工作负载和模式变化，显著提升效果与速度。


<details>
  <summary>Details</summary>
Motivation: 现有自动调优器无法有效利用查询级历史信息，难以在工作负载漂移或跨模式迁移等环境变化时快速重新优化DBMS。

Method: 将历史产物构建为查询-配置上下文；使用LLM基于相关上下文为每个查询生成配置建议；采用束搜索将查询级建议组合成整体DBMS配置；将Booster作为一个辅助层，插入到现有的不同类型调优器（代价模型、机器学习、LLM）之上进行评估。

Result: 在多个OLAP工作负载上，Booster帮助不同的先进调优器在环境变化场景下发现相较于从历史配置继续调优的替代方法可高达74%更优的配置，并且能将调优时间缩短至最多4.7倍。

Conclusion: Booster通过将历史调整记录组织为基于查询的上下文并利用大语言模型生成每个查询的配置建议，再通过束搜索合成查询级建议为整体配置，显著改善了调优器在环境变化下的适应性与效率。

Abstract: Tuning database management systems (DBMSs) is challenging due to trillions of
possible configurations and evolving workloads. Recent advances in tuning have
led to breakthroughs in optimizing over the possible configurations. However,
due to their design and inability to leverage query-level historical insights,
existing automated tuners struggle to adapt and re-optimize the DBMS when the
environment changes (e.g., workload drift, schema transfer).
  This paper presents the Booster framework that assists existing tuners in
adapting to environment changes (e.g., drift, cross-schema transfer). Booster
structures historical artifacts into query-configuration contexts, prompts
large language models (LLMs) to suggest configurations for each query based on
relevant contexts, and then composes the query-level suggestions into a
holistic configuration with beam search. With multiple OLAP workloads, we
evaluate Booster's ability to assist different state-of-the-art tuners (e.g.,
cost-/machine learning-/LLM-based) in adapting to environment changes. By
composing recommendations derived from query-level insights, Booster assists
tuners in discovering configurations that are up to 74% better and in up to
4.7x less time than the alternative approach of continuing to tune from
historical configurations.

</details>
