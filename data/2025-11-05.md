<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.DB](#cs.DB) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [iFlyBot-VLA Technical Report](https://arxiv.org/abs/2511.01914)
*Yuan Zhang,Chenyu Xue,Wenjie Xu,Chao Ji,Jiajia wu,Jia Pan*

Main category: cs.CV

TL;DR: iFlyBot-VLA通过在大规模跨体现操作数据上训练潜在动作模型，并用潜在动作与频域离散动作token对VLM进行双重监督，结合混合训练策略，实现了视觉-语言与动作的对齐，提升了机器人操作的3D感知、推理与动作生成能力，并在基准与真实场景中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 目标是让视觉-语言模型不仅能理解和推理三维任务场景，还能直接参与动作生成，通过对语言、视觉和动作表示空间的对齐，实现从感知到控制的无缝连接，提高操作任务的成功率与泛化能力。

Method: 方法包括：1) 在大规模人类与机器人操作视频上预训练潜在动作模型；2) 提出双层动作表示（潜在动作+结构化离散动作token），对VLM和动作专家进行联合监督；3) 采用混合训练策略，将机器人轨迹数据与通用问答及空间问答数据混合训练VLM，利用频域变换将连续控制信号转为离散token以编码低层动力学。

Result: 在LIBERO Franka基准上，iFlyBot-VLA在多种操作任务上展现出优越性能；真实机器人评估也显示了在多样且具有挑战性的操作任务上达到有竞争力的成功率。此外，作者计划开源部分自建数据集以促进社区研究。

Conclusion: 该论文提出了一个名为iFlyBot-VLA的大规模视觉-语言-动作模型，通过双层动作表示和混合训练策略实现了VLM与动作专家的联合训练，提升了模型的3D感知与推理能力，在基准和真实世界实验中表现优越。

Abstract: We introduce iFlyBot-VLA, a large-scale Vision-Language-Action (VLA) model
trained under a novel framework. The main contributions are listed as follows:
(1) a latent action model thoroughly trained on large-scale human and robotic
manipulation videos; (2) a dual-level action representation framework that
jointly supervises both the Vision-Language Model (VLM) and the action expert
during training; (3) a mixed training strategy that combines robot trajectory
data with general QA and spatial QA datasets, effectively enhancing the 3D
perceptual and reasoning capabilities of the VLM backbone. Specifically, the
VLM is trained to predict two complementary forms of actions: latent actions,
derived from our latent action model pretrained on cross-embodiment
manipulation data, which capture implicit high-level intentions; and structured
discrete action tokens, obtained through frequency-domain transformations of
continuous control signals, which encode explicit low-level dynamics. This dual
supervision aligns the representation spaces of language, vision, and action,
enabling the VLM to directly contribute to action generation. Experimental
results on the LIBERO Franka benchmark demonstrate the superiority of our
frame-work, while real-world evaluations further show that iFlyBot-VLA achieves
competitive success rates across diverse and challenging manipulation tasks.
Furthermore, we plan to open-source a portion of our self-constructed dataset
to support future research in the community

</details>


### [2] [Challenging DINOv3 Foundation Model under Low Inter-Class Variability: A Case Study on Fetal Brain Ultrasound](https://arxiv.org/abs/2511.01915)
*Edoardo Conti,Riccardo Rosati,Lorenzo Federici,Adriano Mancini,Maria Chiara Fiorentin*

Main category: cs.CV

TL;DR: 构建FetalUS-188K数据集并在其上对DINOv3进行自监督领域预训练，结果表明同域预训练显著优于自然图像初始化，在低类间差异的胎儿脑超声标准位面分类任务中提升最多20%加权F1。


<details>
  <summary>Details</summary>
Motivation: 评估并验证当前视觉基础模型在低类间差异（anatomically similar structures）情况下对胎儿脑标准位面的判别能力，填补此类模型在医学超声细粒度识别任务上的系统性评估空白。

Method: 作者整理并聚合了所有公开胎儿超声数据集，构建了多中心基准FetalUS-188K（>188k张标注图像）。在该数据集上用自监督方式对DINOv3进行领域自适应预训练，随后采用冻结骨干的线性探测和全量微调两种适配协议，在两种初始化（基于FetalUS-188K预训练 vs. 自然图像DINOv3权重）下比较表现。

Result: 在FetalUS-188K上领域自适应预训练的模型在加权F1分数上比自然图像初始化提高了最多约20%。同域预训练帮助模型保留微小的回声和结构线索，尤其提高了对中间平面（如TV）的辨识能力。

Conclusion: 本研究表明，在胎儿脑超声这种类内差异小的任务中，通用视觉基础模型（如自然图像预训练的DINOv3）泛化能力不足，而在同域超声数据上进行自监督领域自适应预训练能显著提升区分相近标准位面的能力，进而提高临床可信度。

Abstract: Purpose: This study provides the first comprehensive evaluation of foundation
models in fetal ultrasound (US) imaging under low inter-class variability
conditions. While recent vision foundation models such as DINOv3 have shown
remarkable transferability across medical domains, their ability to
discriminate anatomically similar structures has not been systematically
investigated. We address this gap by focusing on fetal brain standard
planes--transthalamic (TT), transventricular (TV), and transcerebellar
(TC)--which exhibit highly overlapping anatomical features and pose a critical
challenge for reliable biometric assessment.
  Methods: To ensure a fair and reproducible evaluation, all publicly available
fetal ultrasound datasets were curated and aggregated into a unified
multicenter benchmark, FetalUS-188K, comprising more than 188,000 annotated
images from heterogeneous acquisition settings. DINOv3 was pretrained in a
self-supervised manner to learn ultrasound-aware representations. The learned
features were then evaluated through standardized adaptation protocols,
including linear probing with frozen backbone and full fine-tuning, under two
initialization schemes: (i) pretraining on FetalUS-188K and (ii) initialization
from natural-image DINOv3 weights.
  Results: Models pretrained on fetal ultrasound data consistently outperformed
those initialized on natural images, with weighted F1-score improvements of up
to 20 percent. Domain-adaptive pretraining enabled the network to preserve
subtle echogenic and structural cues crucial for distinguishing intermediate
planes such as TV.
  Conclusion: Results demonstrate that generic foundation models fail to
generalize under low inter-class variability, whereas domain-specific
pretraining is essential to achieve robust and clinically reliable
representations in fetal brain ultrasound imaging.

</details>


### [3] [Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users](https://arxiv.org/abs/2511.01990)
*Saurabh Kaushik,Lalit Maurya,Elizabeth Tellman,ZhiJie Zhang*

Main category: cs.CV

TL;DR: 系统比较表明，GFMs在洪水映射上能提供小到中等的性能提升，同时降低标注与计算成本；Clay V1.5综合性能与效率最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管GFMs在遥感时空信息提取上被认为潜力巨大，但是否能超越传统分割模型（如U-Net）尚不明确，且缺乏跨传感器与数据可用性情景的系统比较，无法为最终用户提供明确模型选择建议。

Method: 研究将三种GFM（Prithvi 2.0、Clay V1.5、DOFA）及UViT，与传统模型（TransNorm、U-Net、Attention U-Net）在PlanetScope、Sentinel-1、Sentinel-2多传感器数据上进行系统性比较；采用留一区域交叉验证、少样本实验与模型复杂度/推理时间对比，并以mIoU为主要评估指标及视觉对比分析细节保持性。

Result: GFMs之间性能差异小（2–5%），Clay在PlanetScope和Sentinel-2上最优（mIoU分别为0.79和0.70），Prithvi在Sentinel-1上最优（0.57）；在5个区域的留一法交叉验证中，Clay表现略优；在19个站点的更广泛验证中，Clay较U-Net平均提高约4%；少样本（5张图）下Clay在PlanetScope达0.64 mIoU，大幅领先其他GFMs；Clay参数最少（26M），推理速度显著快于Prithvi和DOFA。

Conclusion: GFMs在洪水淹没映射任务上总体上优于或至少与传统U-Net持平，尤其在低标注样本和计算资源受限情境下更具优势；Clay在多数情形下表现最佳，Prithvi在合成孔径雷达（Sentinel-1）上表现领先。

Abstract: Geo-Foundational Models (GFMs) enable fast and reliable extraction of
spatiotemporal information from satellite imagery, improving flood inundation
mapping by leveraging location and time embeddings. Despite their potential, it
remains unclear whether GFMs outperform traditional models like U-Net. A
systematic comparison across sensors and data availability scenarios is still
lacking, which is an essential step to guide end-users in model selection. To
address this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (a
Prithvi variant), against TransNorm, U-Net, and Attention U-Net using
PlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performance
among all GFMs, with only 2-5% variation between the best and worst models
across sensors. Clay outperforms others on PlanetScope (0.79 mIoU) and
Sentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). In
leave-one-region-out cross-validation across five regions, Clay shows slightly
better performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07),
0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA
(0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, and
Sentinel-1, respectively. Across all 19 sites, leave-one-region-out
cross-validation reveals a 4% improvement by Clay compared to U-Net. Visual
inspection highlights Clay's superior ability to retain fine details. Few-shot
experiments show Clay achieves 0.64 mIoU on PlanetScope with just five training
images, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computational
time, Clay is a better choice due to its smaller model size (26M parameters),
making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M).
Contrary to previous findings, our results suggest GFMs offer small to moderate
improvements in flood mapping accuracy at lower computational cost and labeling
effort compared to traditional U-Net.

</details>


### [4] [Locally-Supervised Global Image Restoration](https://arxiv.org/abs/2511.01998)
*Benjamin Walder,Daniel Toader,Robert Nuster,Günther Paltauf,Peter Burgholzer,Gregor Langer,Lukas Krainer,Markus Haltmeier*

Main category: cs.CV

TL;DR: 提出一种利用图像多重不变性进行确定性欠采样下自监督重建的方法，在PAM上证明可在更少标签下达到或超过有监督性能。


<details>
  <summary>Details</summary>
Motivation: 传统有监督方法需求大量完全采样的地面真值，而自监督方法虽然允许不完整地面真值但通常依赖随机采样以在期望上覆盖整幅图像。很多实际应用（如特定仪器或采集策略）只能获得固定确定性的欠采样模式，不能通过随机化获得覆盖。为此，需要一种在固定欠采样下仍能进行高质量重建的学习方法。

Method: 在学习框架中，作者不依赖随机采样覆盖，而是针对固定确定性采样模式设计自监督重建方法。关键是利用图像分布的多种不变性（如几何变换、不变噪声模型或统计对称性），构造训练目标使模型可以从部分覆盖的观测中恢复完整图像。理论分析表明这些不变性足以弥补采样不足，并证明与完全监督等价的收敛性或误差界。实验在PAM上进行，比较了上采样和修复任务，展示了在较少全采样数据下取得的良好结果。

Result: 理论证明利用多重不变性可以在确定性欠采样下达到与全监督方法相当的性能。实证结果在PAM上的图像上采样任务中显示，提出的方法在重建质量上优于或等同于基线方法，同时显著减少所需的完全采样地面真值样本数。

Conclusion: 本文提出了一种在固定、确定性欠采样模式下进行图像重建的方法，通过利用图像分布的多重不变性，理论上可以达到与有监督方法相当的重建性能，并在光声显微成像（PAM）上验证了方法有效性，能在更少的完全采样标签下实现竞争或更优的结果。

Abstract: We address the problem of image reconstruction from incomplete measurements,
encompassing both upsampling and inpainting, within a learning-based framework.
Conventional supervised approaches require fully sampled ground truth data,
while self-supervised methods allow incomplete ground truth but typically rely
on random sampling that, in expectation, covers the entire image. In contrast,
we consider fixed, deterministic sampling patterns with inherently incomplete
coverage, even in expectation. To overcome this limitation, we exploit multiple
invariances of the underlying image distribution, which theoretically allows us
to achieve the same reconstruction performance as fully supervised approaches.
We validate our method on optical-resolution image upsampling in photoacoustic
microscopy (PAM), demonstrating competitive or superior results while requiring
substantially less ground truth data.

</details>


### [5] [Towards Selection of Large Multimodal Models as Engines for Burned-in Protected Health Information Detection in Medical Images](https://arxiv.org/abs/2511.02014)
*Tuan Truong,Guillermo Jimenez Perez,Pedro Osorio,Matthias Lenga*

Main category: cs.CV

TL;DR: LMM提升OCR效果但并非万能，复杂场景收益大；根据可读性与部署约束选择合适模型与模块化部署。


<details>
  <summary>Details</summary>
Motivation: 提高医学影像中保护性健康信息(PHI)检测的准确性与可部署性，探索LMM在OCR与语义理解方面的潜力，找出在不同场景下的最佳流水线与模型选择。

Method: 系统性基准测试三种闭源与开源LMM（GPT-4o、Gemini 2.5 Flash、Qwen 2.5 7B），比较两类流水线：仅文本分析与OCR+语义分析，使用WER/CER等OCR指标与PHI检测准确率评估。

Result: LMM在OCR指标上（WER 0.03-0.05, CER 0.02-0.03）优于EasyOCR，但整体PHI检测性能提升不稳定；复杂印记（低对比、特殊字体或重叠）场景中，LMM带来明显增益；对高可读性文本，后处理文本分析模型能力更关键。

Conclusion: LMM能在医学影像文字识别上显著优于传统OCR，但OCR改进不总是直接提升PHI检测准确率；复杂印记场景中收益最大；在高可读性文本条件下，不同流水线差异不大。

Abstract: The detection of Protected Health Information (PHI) in medical imaging is
critical for safeguarding patient privacy and ensuring compliance with
regulatory frameworks. Traditional detection methodologies predominantly
utilize Optical Character Recognition (OCR) models in conjunction with named
entity recognition. However, recent advancements in Large Multimodal Model
(LMM) present new opportunities for enhanced text extraction and semantic
analysis. In this study, we systematically benchmark three prominent closed and
open-sourced LMMs, namely GPT-4o, Gemini 2.5 Flash, and Qwen 2.5 7B, utilizing
two distinct pipeline configurations: one dedicated to text analysis alone and
another integrating both OCR and semantic analysis. Our results indicate that
LMM exhibits superior OCR efficacy (WER: 0.03-0.05, CER: 0.02-0.03) compared to
conventional models like EasyOCR. However, this improvement in OCR performance
does not consistently correlate with enhanced overall PHI detection accuracy.
The strongest performance gains are observed on test cases with complex imprint
patterns. In scenarios where text regions are well readable with sufficient
contrast, and strong LMMs are employed for text analysis after OCR, different
pipeline configurations yield similar results. Furthermore, we provide
empirically grounded recommendations for LMM selection tailored to specific
operational constraints and propose a deployment strategy that leverages
scalable and modular infrastructure.

</details>


### [6] [StrengthSense: A Dataset of IMU Signals Capturing Everyday Strength-Demanding Activities](https://arxiv.org/abs/2511.02027)
*Zeyu Yang,Clayton Souza Leite,Yu Xiao*

Main category: cs.CV

TL;DR: StrengthSense：29人、10个IMU、11种力量活动的公开数据集，附视频标注与关节角度验证，适用于人体活动识别与健康监测研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集中缺少针对高强度力量活动的IMU记录，阻碍了力度/耐力/爆发力监测算法与应用开发。

Method: 在29名健康受试者身上使用10个IMU（放置于四肢和躯干），并通过视频录像进行标注；对数据进行了预处理与技术验证，包括IMU估算关节角度与视频提取关节角度的比较分析。

Result: 提供包含11种力量需求活动及2种非力量活动的公开IMU数据集，完成数据收集、预处理、注释和基于视频的技术验证，证明了传感器数据的准确性与可靠性。

Conclusion: StrengthSense弥补了力量相关活动传感数据集的缺口，为研究与应用提供了高质量IMU数据。

Abstract: Tracking strength-demanding activities with wearable sensors like IMUs is
crucial for monitoring muscular strength, endurance, and power. However, there
is a lack of comprehensive datasets capturing these activities. To fill this
gap, we introduce \textit{StrengthSense}, an open dataset that encompasses IMU
signals capturing 11 strength-demanding activities, such as sit-to-stand,
climbing stairs, and mopping. For comparative purposes, the dataset also
includes 2 non-strength demanding activities. The dataset was collected from 29
healthy subjects utilizing 10 IMUs placed on limbs and the torso, and was
annotated using video recordings as references. This paper provides a
comprehensive overview of the data collection, pre-processing, and technical
validation. We conducted a comparative analysis between the joint angles
estimated by IMUs and those directly extracted from video to verify the
accuracy and reliability of the sensor data. Researchers and developers can
utilize \textit{StrengthSense} to advance the development of human activity
recognition algorithms, create fitness and health monitoring applications, and
more.

</details>


### [7] [Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis](https://arxiv.org/abs/2511.02046)
*Soham Joshi,Shwet Kamal Mishra,Viswanath Gopalakrishnan*

Main category: cs.CV

TL;DR: 提出首个端到端管道，结合OCR、ROI检测、caption与问题生成自动合成并验证大规模text-VQA数据集（~72K QA对，44K图像）。


<details>
  <summary>Details</summary>
Motivation: 手工标注text-VQA数据集耗时耗力；现有大模型与OCR技术成熟，利用这些资源自动合成高质量QA对以便扩充数据集有现实需求。

Method: 管道先通过OCR（text spotting）检测识别场景文本，再用ROI检测定位感兴趣区域，生成图像/区域的caption，接着基于caption与OCR结果使用问题生成模型合成问题与答案，最后进行验证过滤以确保答案与图像文本一致。

Result: 构建了一个自动化合成与验证pipeline，生成了约72K QA对（基于44K图像），据称是首个此类大规模自动合成并验证的text-VQA数据集管道。

Conclusion: 该工作提出了一个端到端自动化管道，综合利用OCR、ROI检测、图像描述与问题生成模型，自动合成并验证大规模text-VQA QA对，规模约72K QA对，44K图像。

Abstract: Creation of large-scale databases for Visual Question Answering tasks
pertaining to the text data in a scene (text-VQA) involves skilful human
annotation, which is tedious and challenging. With the advent of foundation
models that handle vision and language modalities, and with the maturity of OCR
systems, it is the need of the hour to establish an end-to-end pipeline that
can synthesize Question-Answer (QA) pairs based on scene-text from a given
image. We propose a pipeline for automated synthesis for text-VQA dataset that
can produce faithful QA pairs, and which scales up with the availability of
scene text data. Our proposed method harnesses the capabilities of multiple
models and algorithms involving OCR detection and recognition (text spotting),
region of interest (ROI) detection, caption generation, and question
generation. These components are streamlined into a cohesive pipeline to
automate the synthesis and validation of QA pairs. To the best of our
knowledge, this is the first pipeline proposed to automatically synthesize and
validate a large-scale text-VQA dataset comprising around 72K QA pairs based on
around 44K images.

</details>


### [8] [Markerless Augmented Reality Registration for Surgical Guidance: A Multi-Anatomy Clinical Accuracy Study](https://arxiv.org/abs/2511.02086)
*Yue Yang,Fabian Necker,Christoph Leuze,Michelle Chen,Andrey Finegersh,Jake Lee,Vasu Divi,Bruce Daniel,Brian Hargreaves,Jie Ying Wu,Fred M Baik*

Main category: cs.CV

TL;DR: 在HoloLens 2上实现的深度-only无标记AR配准，经人工初始化与全局到局部配准，可在术中对小/低曲率解剖体达到约3–4 mm中位误差，接近中等风险临床任务的误差阈值。


<details>
  <summary>Details</summary>
Motivation: 提升无需标记物的头戴式显示器在手术中对小型或低曲率解剖区域的配准可用性与精度，从而减少对配准标记的依赖并推进AR在外科导航中的临床应用。

Method: 采用AHAT深度点云与CT生成的皮肤网格对齐，流程包含深度偏差校正、人工引导的初始化、全局配准与局部配准；通过AR追踪工具在模型上验证‘表面追踪误差’指标，并在七次术中目标试验（足、耳、下肢）收集大量点位数据进行临床评估。

Result: 在模型验证中，AR追踪与CT真值距离高度一致（中位绝对误差~0.78–0.80 mm，RMSE~0.97–1.20 mm）。术中每点中位误差为3.9 mm；按解剖分：足3.2 mm、耳4.3 mm、下肢5.3 mm；5 mm覆盖率分别为92–95%、84–90%、72–86%；足与下肢差异显著（p<0.001）。

Conclusion: 本文表明在HoloLens 2上实现的仅基于深度的无标记增强现实配准管线，在实际外科环境中对小尺寸或低曲率解剖结构能达到接近临床可接受范围的配准精度。

Abstract: Purpose: In this paper, we develop and clinically evaluate a depth-only,
markerless augmented reality (AR) registration pipeline on a head-mounted
display, and assess accuracy across small or low-curvature anatomies in
real-life operative settings. Methods: On HoloLens 2, we align Articulated HAnd
Tracking (AHAT) depth to Computed Tomography (CT)-derived skin meshes via (i)
depth-bias correction, (ii) brief human-in-the-loop initialization, (iii)
global and local registration. We validated the surface-tracing error metric by
comparing "skin-to-bone" relative distances to CT ground truth on leg and foot
models, using an AR-tracked tool. We then performed seven intraoperative target
trials (feet x2, ear x3, leg x2) during the initial stage of fibula free-flap
harvest and mandibular reconstruction surgery, and collected 500+ data per
trial. Results: Preclinical validation showed tight agreement between AR-traced
and CT distances (leg: median |Delta d| 0.78 mm, RMSE 0.97 mm; feet: 0.80 mm,
1.20 mm). Clinically, per-point error had a median of 3.9 mm. Median errors by
anatomy were 3.2 mm (feet), 4.3 mm (ear), and 5.3 mm (lower leg), with 5 mm
coverage 92-95%, 84-90%, and 72-86%, respectively. Feet vs. lower leg differed
significantly (Delta median ~1.1 mm; p < 0.001). Conclusion: A depth-only,
markerless AR pipeline on HMDs achieved ~3-4 mm median error across feet, ear,
and lower leg in live surgical settings without fiducials, approaching typical
clinical error thresholds for moderate-risk tasks. Human-guided initialization
plus global-to-local registration enabled accurate alignment on small or
low-curvature targets, improving the clinical readiness of markerless AR
guidance.

</details>


### [9] [From Instance Segmentation to 3D Growth Trajectory Reconstruction in Planktonic Foraminifera](https://arxiv.org/abs/2511.02142)
*Huahua Lin,Xiaohao Cai,Mark Nixon,James M. Mulqueeney,Thomas H. G. Ezard*

Main category: cs.CV

TL;DR: 本文首次提出了一个从CT数据自动分割并排序放射虫腔室以重建三维生长轨迹的可重复端到端流程，为大规模生态和发育研究奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖逐腔室的人工分割，耗时且主观，缺乏自动化手段；理解放射虫腔室生长轨迹对研究个体发育和生态适应性至关重要，促进大规模数据驱动的生态研究。

Method: 基于多种实例分割模型（针对不同腔室空间特征优化）对断层扫描体数据进行腔室分割，并设计专门的腔室排序算法将分割结果按生长顺序重建为三维轨迹；同时对模型的下游影响进行定量与定性评估。

Result: 在专家注释的数据集上，提出的管道在减少人工工作量的同时，保持了生物学上有意义的准确度。尽管对小腔室存在欠分割问题，但腔室排序算法具有鲁棒性，能在部分分割的情况下稳定重建发育轨迹。

Conclusion: 本文提出了一个端到端的自动化管道，结合实例分割与腔室排序算法，实现了从高分辨率CT重构放射虫（foraminifera）三维生长轨迹，显著减少人工分割工作且保持生物学意义的准确性。

Abstract: Planktonic foraminifera, marine protists characterized by their intricate
chambered shells, serve as valuable indicators of past and present
environmental conditions. Understanding their chamber growth trajectory
provides crucial insights into organismal development and ecological adaptation
under changing environments. However, automated tracing of chamber growth from
imaging data remains largely unexplored, with existing approaches relying
heavily on manual segmentation of each chamber, which is time-consuming and
subjective. In this study, we propose an end-to-end pipeline that integrates
instance segmentation, a computer vision technique not extensively explored in
foraminifera, with a dedicated chamber ordering algorithm to automatically
reconstruct three-dimensional growth trajectories from high-resolution computed
tomography scans. We quantitatively and qualitatively evaluate multiple
instance segmentation methods, each optimized for distinct spatial features of
the chambers, and examine their downstream influence on growth-order
reconstruction accuracy. Experimental results on expert-annotated datasets
demonstrate that the proposed pipeline substantially reduces manual effort
while maintaining biologically meaningful accuracy. Although segmentation
models exhibit under-segmentation in smaller chambers due to reduced voxel
fidelity and subtle inter-chamber connectivity, the chamber-ordering algorithm
remains robust, achieving consistent reconstruction of developmental
trajectories even under partial segmentation. This work provides the first
fully automated and reproducible pipeline for digital foraminiferal growth
analysis, establishing a foundation for large-scale, data-driven ecological
studies.

</details>


### [10] [Fast Measuring Pavement Crack Width by Cascading Principal Component Analysis](https://arxiv.org/abs/2511.02144)
*Zhicheng Wang,Junbiao Pang*

Main category: cs.CV

TL;DR: 提出一种PCA与RPCA结合的级联方法，通过先分割再按裂缝形态选择PCA或RPCA确定主轴，实现从任意像素快速准确测宽，在三数据集上优于现有方法，但依赖分割质量和对噪声的稳健性需进一步验证。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以应对非均匀裂缝边界且缺乏从任意像素快速测量宽度的能力，需一个既准确又高效的自动化测量框架。

Method: 三阶段流程：1) 使用现有裂缝检测算法得到二值分割；2) 对近似平行裂缝用PCA确定主方向轴；3) 对不规则裂缝用RPCA提取主传播轴（MPA），并据此从任意像素位置快速测量裂缝宽度。

Result: 在三个公开数据集上的综合评估表明，该方法在计算速度和宽度测量精度上均超过现有最先进技术。

Conclusion: 该文提出的PCA+RPCA级联框架能够在计算效率和测量精度上优于现有方法，适用于规则与不规则裂缝宽度提取，但对分割质量与噪声稳健性依赖较大。

Abstract: Accurate quantification of pavement crack width plays a pivotal role in
assessing structural integrity and guiding maintenance interventions. However,
achieving precise crack width measurements presents significant challenges due
to: (1) the complex, non-uniform morphology of crack boundaries, which limits
the efficacy of conventional approaches, and (2) the demand for rapid
measurement capabilities from arbitrary pixel locations to facilitate
comprehensive pavement condition evaluation. To overcome these limitations,
this study introduces a cascaded framework integrating Principal Component
Analysis (PCA) and Robust PCA (RPCA) for efficient crack width extraction from
digital images. The proposed methodology comprises three sequential stages: (1)
initial crack segmentation using established detection algorithms to generate a
binary representation, (2) determination of the primary orientation axis for
quasi-parallel cracks through PCA, and (3) extraction of the Main Propagation
Axis (MPA) for irregular crack geometries using RPCA. Comprehensive evaluations
were conducted across three publicly available datasets, demonstrating that the
proposed approach achieves superior performance in both computational
efficiency and measurement accuracy compared to existing state-of-the-art
techniques.

</details>


### [11] [Autobiasing Event Cameras for Flickering Mitigation](https://arxiv.org/abs/2511.02180)
*Mehdi Sefidgar Dilmaghani,Waseem Shariff,Cian Ryan,Joe Lemley,Peter Corcoran*

Main category: cs.CV

TL;DR: 提出基于CNN的自动偏置调节方法，无需额外硬件，在25–500 Hz范围内显著降低闪烁，提升事件相机在不同光照下的人脸检测性能与边缘稳定性。


<details>
  <summary>Details</summary>
Motivation: 避免依赖额外硬件或复杂软件滤波，通过利用事件相机自身偏置设置自动抑制由快速光强变化引起的闪烁，从而提升事件相机在多种光照和频率条件下的可靠性和下游任务性能。

Method: 使用简单的卷积神经网络在空间域检测闪烁事件，并动态调整相机的特定偏置参数（autobiasing），无需额外硬件或外部滤波软件；在不同光照条件和频率下通过人脸检测（YOLO）框架评估效果。

Result: 在人脸检测任务中，YOLO置信度和检测帧比例显著提升；平均梯度（作为边缘/闪烁指标）在明亮条件下降低38.2%，在弱光条件下降低53.6%，表明闪烁显著减少。

Conclusion: 本文提出了一种基于事件相机自带偏置自适应调节的去闪烁方法，能在约25 Hz到500 Hz的频率范围内有效减少闪烁对事件数据的影响。

Abstract: Understanding and mitigating flicker effects caused by rapid variations in
light intensity is critical for enhancing the performance of event cameras in
diverse environments. This paper introduces an innovative autonomous mechanism
for tuning the biases of event cameras, effectively addressing flicker across a
wide frequency range -25 Hz to 500 Hz. Unlike traditional methods that rely on
additional hardware or software for flicker filtering, our approach leverages
the event cameras inherent bias settings. Utilizing a simple Convolutional
Neural Networks -CNNs, the system identifies instances of flicker in a spatial
space and dynamically adjusts specific biases to minimize its impact. The
efficacy of this autobiasing system was robustly tested using a face detector
framework under both well-lit and low-light conditions, as well as across
various frequencies. The results demonstrated significant improvements:
enhanced YOLO confidence metrics for face detection, and an increased
percentage of frames capturing detected faces. Moreover, the average gradient,
which serves as an indicator of flicker presence through edge detection,
decreased by 38.2 percent in well-lit conditions and by 53.6 percent in
low-light conditions. These findings underscore the potential of our approach
to significantly improve the functionality of event cameras in a range of
adverse lighting scenarios.

</details>


### [12] [Pinpointing Trigger Moment for Grounded Video QA: Enhancing Spatio-temporal Grounding in Multimodal Large Language Models](https://arxiv.org/abs/2511.02182)
*Jinhwan Seo,Yoonki Cho,Junhyug Noh,Sung-eui Yoon*

Main category: cs.CV

TL;DR: 提出基于CORTEX提示的三阶段流水线（视频QA、时空定位、跟踪），通过触发时刻锚定最可见帧，显著提高GVQA的时空归因与跟踪性能，HOTA从0.2704提升到0.4968。


<details>
  <summary>Details</summary>
Motivation: GVQA需要多模态复杂推理、清晰的视觉归因和跨帧目标追踪。现有方法难以同时兼顾准确回答、可解释的视觉 grounding 与稳定的时序跟踪，因此提出分解式流水线与触发时刻机制以降低任务复杂度并提高鲁棒性。

Method: 将GVQA拆解为三个模块：1) 使用视频-语言模型进行现场视频推理与问答，输出答案并预测目标的触发时刻；2) 基于触发时刻在关键帧上进行细粒度的空间定位（bounding box/segmentation）以实现视觉归因；3) 对定位到的目标进行时序跟踪以生成完整时空轨迹。关键方法是CORTEX prompt，用于确定“单帧最可见”触发时刻，作为后续模块的锚点。

Result: 在ICCV 2025 Perception Test Challenge上，该方法取得HOTA=0.4968，相比去年的冠军分数0.2704有显著提升，表明触发时刻与三阶段流水线对时空归因与跟踪效果的改善。

Conclusion: 该工作通过三阶段流水线（视频推理与QA、时空定位、目标跟踪）解决GVQA任务，并提出CORTEX提示以定位触发时刻，作为时空定位与跟踪的锚点，显著提升了性能。

Abstract: In this technical report, we introduce a framework to address Grounded Video
Question Answering (GVQA) task for the ICCV 2025 Perception Test Challenge. The
GVQA task demands robust multimodal models capable of complex reasoning over
video content, grounding the resulting answers visually, and tracking the
referenced objects temporally. To achieve this capability, our proposed
approach decomposes the GVQA task into a three-stage pipeline: (1) Video
Reasoning \& QA, (2) Spatio-temporal Grounding and (3) Tracking. Our key
contribution is the introduction of a trigger moment, derived from our proposed
CORTEX prompt, which pinpoints the single most visible frame of a target object
to serve as a robust anchor for grounding and tracking. To this end, we achieve
the HOTA score of 0.4968, which marks a significant improvement over the
previous year's winning score of 0.2704 on GVQA task.

</details>


### [13] [MM-UNet: Morph Mamba U-shaped Convolutional Networks for Retinal Vessel Segmentation](https://arxiv.org/abs/2511.02193)
*Jiawen Liu,Yuanbo Zeng,Jiaming Liang,Yizhen Yang,Yiheng Zhang,Enhui Cai,Xiaoqi Sheng,Hongmin Cai*

Main category: cs.CV

TL;DR: 该论文提出MM-UNet，通过形态学感知卷积和反向选择性状态引导模块，专门改善视网膜血管的细长分支分割，在DRIVE和STARE上实现了小幅但稳健的F1提升，并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管为极细且分支复杂的结构，且图像间全局形态差异大，传统卷积网络在细小结构和拓扑保持方面表现欠佳，需设计针对性模块以提升精度与鲁棒性。

Method: 方法包括两大创新模块：1) Morph Mamba Convolution层替代逐点卷积，利用形态学与状态感知特征采样增强对分支拓扑的感知；2) Reverse Selective State Guidance模块将反向引导理论与状态空间建模结合，提升几何边界感知与解码效率。整体架构为改进的UNet（MM-UNet）。

Result: 在两个公开数据集上进行了广泛实验：DRIVE数据集上F1提升1.64%，STARE数据集上F1提升1.25%，展示了在分割精度上的优越性和进步，代码已开源。

Conclusion: 本文提出的MM-UNet通过引入Morph Mamba Convolution和Reverse Selective State Guidance模块，专门针对视网膜血管的细长分支结构和全局形态变化，提高了分割精度与鲁棒性，实验证明在DRIVE和STARE数据集上分别获得了1.64%和1.25%的F1提升，结论可信且具有实用价值。

Abstract: Accurate detection of retinal vessels plays a critical role in reflecting a
wide range of health status indicators in the clinical diagnosis of ocular
diseases. Recently, advances in deep learning have led to a surge in retinal
vessel segmentation methods, which have significantly contributed to the
quantitative analysis of vascular morphology. However, retinal vasculature
differs significantly from conventional segmentation targets in that it
consists of extremely thin and branching structures, whose global morphology
varies greatly across images. These characteristics continue to pose challenges
to segmentation precision and robustness. To address these issues, we propose
MM-UNet, a novel architecture tailored for efficient retinal vessel
segmentation. The model incorporates Morph Mamba Convolution layers, which
replace pointwise convolutions to enhance branching topological perception
through morph, state-aware feature sampling. Additionally, Reverse Selective
State Guidance modules integrate reverse guidance theory with state-space
modeling to improve geometric boundary awareness and decoding efficiency.
Extensive experiments conducted on two public retinal vessel segmentation
datasets demonstrate the superior performance of the proposed method in
segmentation accuracy. Compared to the existing approaches, MM-UNet achieves
F1-score gains of 1.64 $\%$ on DRIVE and 1.25 $\%$ on STARE, demonstrating its
effectiveness and advancement. The project code is public via
https://github.com/liujiawen-jpg/MM-UNet.

</details>


### [14] [Language-Enhanced Generative Modeling for PET Synthesis from MRI and Blood Biomarkers](https://arxiv.org/abs/2511.02206)
*Zhengjie Zhang,Xiaoxie Mao,Qihao Guo,Shaoting Zhang,Qi Huang,Mu Zhou,Fang Xie,Mianxin Liu*

Main category: cs.CV

TL;DR: 用LLM增强的多模态生成模型可从MRI和血液标志物合成高保真Abeta-PET图像，提升无创AD诊断性能，可能减少对昂贵PET的依赖。


<details>
  <summary>Details</summary>
Motivation: Abeta-PET成本高且可及性差，研究旨在探索是否可用成本更低、普及性更高的MRI与血基生物标志物通过生成模型预测PET空间分布，从而替代或补充PET用于AD诊断。

Method: 收集566例受试者的阿尔法-β PET、T1加权MRI和血液生物标志物数据，构建一个将LLM与影像和生物标志物融合的语言增强生成模型以合成PET图像；评估合成图像的结构相似性（SSIM）、区域相关性（Pearson's r）、诊断一致性（准确率），并将合成PET用于全自动AD诊断管线（分类模型及AUC评估）；同时进行了消融实验以验证LLM与prompt工程的作用。

Result: 合成PET在结构细节和区域模式上与真实PET高度一致（SSIM≈0.920，Pearson's r≈0.955）；基于合成PET的诊断与真实PET诊断一致性高（准确率0.80）；合成PET驱动的分类模型AUC=0.78，优于仅用T1（AUC=0.68）或仅用BBM（AUC=0.73），合并合成PET与BBM进一步提高至AUC=0.79；消融分析显示LLM与prompt工程能提升性能。

Conclusion: 该研究提出了一种基于大语言模型增强的多模态生成模型，能够以高保真度从MRI和血液生物标志物合成阿尔茨海默病相关的阿尔法-β PET空间分布，从而在降低直接PET检查需求的同时提升自动化诊断管线的性能。

Abstract: Background: Alzheimer's disease (AD) diagnosis heavily relies on amyloid-beta
positron emission tomography (Abeta-PET), which is limited by high cost and
limited accessibility. This study explores whether Abeta-PET spatial patterns
can be predicted from blood-based biomarkers (BBMs) and MRI scans. Methods: We
collected Abeta-PET images, T1-weighted MRI scans, and BBMs from 566
participants. A language-enhanced generative model, driven by a large language
model (LLM) and multimodal information fusion, was developed to synthesize PET
images. Synthesized images were evaluated for image quality, diagnostic
consistency, and clinical applicability within a fully automated diagnostic
pipeline. Findings: The synthetic PET images closely resemble real PET scans in
both structural details (SSIM = 0.920 +/- 0.003) and regional patterns
(Pearson's r = 0.955 +/- 0.007). Diagnostic outcomes using synthetic PET show
high agreement with real PET-based diagnoses (accuracy = 0.80). Using synthetic
PET, we developed a fully automatic AD diagnostic pipeline integrating PET
synthesis and classification. The synthetic PET-based model (AUC = 0.78)
outperforms T1-based (AUC = 0.68) and BBM-based (AUC = 0.73) models, while
combining synthetic PET and BBMs further improved performance (AUC = 0.79).
Ablation analysis supports the advantages of LLM integration and prompt
engineering. Interpretation: Our language-enhanced generative model synthesizes
realistic PET images, enhancing the utility of MRI and BBMs for Abeta spatial
pattern assessment and improving the diagnostic workflow for Alzheimer's
disease.

</details>


### [15] [Object-Centric 3D Gaussian Splatting for Strawberry Plant Reconstruction and Phenotyping](https://arxiv.org/abs/2511.02207)
*Jiajia Li,Keyi Zhu,Qianwen Zhang,Dong Chen,Qi Sun,Zhaojian Li*

Main category: cs.CV

TL;DR: 提出了结合SAM-2前景分割与3D Gaussian Splatting的目标式草莓无背景三维重建方法，显著提高重建精度并降低计算成本，能自动估算高度与冠幅等表型特征，优于传统流程。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析依赖耗时、破坏性的手工测量；而现有基于神经渲染的三维重建方法通常重建整个场景，包含背景噪声，导致计算资源浪费并影响下游表型测量的准确性。因此需要一种去背景、面向目标的高效无损重建方法，以便准确自动地提取表型特征。

Method: 构建了一个预处理管线：首先对多视图图像或视频使用Segment Anything Model v2进行草莓植株分割，生成alpha通道背景掩码并去除背景像素；然后在仅含前景的图像上使用3D Gaussian Splatting进行重建以得到干净的三维表示；最后对得到的3D点云或高斯体使用DBSCAN聚类去除残余噪声，并用PCA估算植株高度和冠幅等表型参数。

Result: 实验结果表明，经过前景分割并仅对植株进行3DGS重建的流程，比传统整景重建在几何精度上有明显提升，同时减少了重建时间（论文中给出显著的时间和精度对比）；使用DBSCAN+PCA估算的植株高度和冠幅与地面实测值具有更高的一致性，表型提取更可靠且可扩展。

Conclusion: 该论文提出了一个以目标为中心的草莓三维重建框架，通过在重建前对输入图像进行前景分割（使用SAM-2和alpha通道遮罩），实现去背景的3D Gaussian Splatting，从而提高几何重建精度并降低计算成本。去背景的点云便于后续的聚类与主成分分析，用于自动估算植株高度和冠幅等表型特征。实验表明该方法在准确性和效率上优于传统整景重建流程。

Abstract: Strawberries are among the most economically significant fruits in the United
States, generating over $2 billion in annual farm-gate sales and accounting for
approximately 13% of the total fruit production value. Plant phenotyping plays
a vital role in selecting superior cultivars by characterizing plant traits
such as morphology, canopy structure, and growth dynamics. However, traditional
plant phenotyping methods are time-consuming, labor-intensive, and often
destructive. Recently, neural rendering techniques, notably Neural Radiance
Fields (NeRF) and 3D Gaussian Splatting (3DGS), have emerged as powerful
frameworks for high-fidelity 3D reconstruction. By capturing a sequence of
multi-view images or videos around a target plant, these methods enable
non-destructive reconstruction of complex plant architectures. Despite their
promise, most current applications of 3DGS in agricultural domains reconstruct
the entire scene, including background elements, which introduces noise,
increases computational costs, and complicates downstream trait analysis. To
address this limitation, we propose a novel object-centric 3D reconstruction
framework incorporating a preprocessing pipeline that leverages the Segment
Anything Model v2 (SAM-2) and alpha channel background masking to achieve clean
strawberry plant reconstructions. This approach produces more accurate
geometric representations while substantially reducing computational time. With
a background-free reconstruction, our algorithm can automatically estimate
important plant traits, such as plant height and canopy width, using DBSCAN
clustering and Principal Component Analysis (PCA). Experimental results show
that our method outperforms conventional pipelines in both accuracy and
efficiency, offering a scalable and non-destructive solution for strawberry
plant phenotyping.

</details>


### [16] [Estimation of Segmental Longitudinal Strain in Transesophageal Echocardiography by Deep Learning](https://arxiv.org/abs/2511.02210)
*Anders Austlid Taskén,Thierry Judge,Erik Andreas Rye Berg,Jinyang Yu,Bjørnar Grenne,Frank Lindseth,Svend Aakhus,Pierre-Marc Jodoin,Nicolas Duchateau,Olivier Bernard,Gabriel Kiss*

Main category: cs.CV

TL;DR: 提出autoStrain管线，比较TeeFlow与TeeTracker两种DL运动估计方法，基于SIMUS生成的synTEE数据训练；TeeTracker更精确(0.65 mm误差)，在16例临床验证上SLS估计与临床参考接近，加入缺血模拟能改进异常检测精度。


<details>
  <summary>Details</summary>
Motivation: 当前应变估计方法依赖大量人工操作与专家经验，效率低、资源消耗大，难以用于连续监测与大规模临床应用，需要自动化、准确的运动估计方法来实现高效的SLS评估。

Method: 构建autoStrain管线并比较两种深度学习运动估计方法：TeeFlow(基于RAFT的密集逐帧光流)和TeeTracker(基于CoTracker的稀疏点轨迹长序列预测)。使用SIMUS仿真流水线生成包含80例带真实心肌运动标注的合成TEE数据(synTEE)进行训练与评估，并在16例真实患者上进行临床验证。

Result: 在synTEE测试集上，TeeTracker在运动估计上优于TeeFlow，平均位移误差为0.65 mm；临床16例验证显示autoStrain估计的SLS与临床参考一致，平均差值为1.09%，95%一致性区间为-8.90%~11.09%。在合成数据中加入缺血模拟提高了异常形变量化的准确性。

Conclusion: 该研究提出了首个用于经食管超声心动图(TEE)的自动化室壁节段纵向应变(SLS)估计管线autoStrain，结合深度学习运动估计方法，在合成数据和临床数据上均显示出良好性能，能够提升区域左室功能评估的效率与精度。

Abstract: Segmental longitudinal strain (SLS) of the left ventricle (LV) is an
important prognostic indicator for evaluating regional LV dysfunction, in
particular for diagnosing and managing myocardial ischemia. Current techniques
for strain estimation require significant manual intervention and expertise,
limiting their efficiency and making them too resource-intensive for monitoring
purposes. This study introduces the first automated pipeline, autoStrain, for
SLS estimation in transesophageal echocardiography (TEE) using deep learning
(DL) methods for motion estimation. We present a comparative analysis of two DL
approaches: TeeFlow, based on the RAFT optical flow model for dense
frame-to-frame predictions, and TeeTracker, based on the CoTracker point
trajectory model for sparse long-sequence predictions.
  As ground truth motion data from real echocardiographic sequences are hardly
accessible, we took advantage of a unique simulation pipeline (SIMUS) to
generate a highly realistic synthetic TEE (synTEE) dataset of 80 patients with
ground truth myocardial motion to train and evaluate both models. Our
evaluation shows that TeeTracker outperforms TeeFlow in accuracy, achieving a
mean distance error in motion estimation of 0.65 mm on a synTEE test dataset.
  Clinical validation on 16 patients further demonstrated that SLS estimation
with our autoStrain pipeline aligned with clinical references, achieving a mean
difference (95\% limits of agreement) of 1.09% (-8.90% to 11.09%).
Incorporation of simulated ischemia in the synTEE data improved the accuracy of
the models in quantifying abnormal deformation. Our findings indicate that
integrating AI-driven motion estimation with TEE can significantly enhance the
precision and efficiency of cardiac function assessment in clinical settings.

</details>


### [17] [Can Foundation Models Revolutionize Mobile AR Sparse Sensing?](https://arxiv.org/abs/2511.02215)
*Yiqin Zhao,Tian Guo*

Main category: cs.CV

TL;DR: 将视觉基础模型用于移动稀疏感知能显著提高跨帧几何扭曲与3D重建精度，但在资源与实时应用上仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 移动感知受限于计算与能耗，需要通过稀疏感知采样子集数据以提高效率，但传统方法因时空信息缺失导致精度下降，探讨基础模型能否改变这一权衡。

Method: 基于真实移动AR数据，使用基础视觉模型（foundation models）来增强几何感知的图像扭曲技术，并将其整合进稀疏感知流程，评估其在信息重用和3D场景重建任务上的表现与可扩展性。

Result: 实验表明引入基础模型在几何感知的图像扭曲任务上有显著提升，能更准确地跨帧重用信息，并在3D重建任务上达到领先性能，同时展示了方法在规模扩展上的潜力。

Conclusion: 本文证明基础模型可显著提升移动稀疏感知中的跨帧几何感知图像扭曲和3D重建性能，但仍存在可扩展性、资源消耗与实时性等挑战需要解决。

Abstract: Mobile sensing systems have long faced a fundamental trade-off between
sensing quality and efficiency due to constraints in computation, power, and
other limitations. Sparse sensing, which aims to acquire and process only a
subset of sensor data, has been a key strategy for maintaining performance
under such constraints. However, existing sparse sensing methods often suffer
from reduced accuracy, as missing information across space and time introduces
uncertainty into many sensing systems. In this work, we investigate whether
foundation models can change the landscape of mobile sparse sensing. Using
real-world mobile AR data, our evaluations demonstrate that foundation models
offer significant improvements in geometry-aware image warping, a central
technique for enabling accurate reuse of cross-frame information. Furthermore,
our study demonstrates the scalability of foundation model-based sparse sensing
and shows its leading performance in 3D scene reconstruction. Collectively, our
study reveals critical aspects of the promises and the open challenges of
integrating foundation models into mobile sparse sensing systems.

</details>


### [18] [Collaborative Attention and Consistent-Guided Fusion of MRI and PET for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2511.02228)
*Delin Ma,Menghui Zhou,Jun Qi,Yun Yang,Po Yang*

Main category: cs.CV

TL;DR: 引入LPR补偿缺失模态、共享+独立编码器保留共享与特异信息、以及一致性引导对齐分布，通过协同注意力融合提高MRI+PET的AD诊断效果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法多侧重跨模态互补性，忽视模态特有信息且受模态间分布差异影响导致表示偏差和噪声，进而限制分类性能。

Method: 设计了可学习参数表示（LPR）模块以补偿缺失模态信息；采用共享编码器和模态独立编码器并行提取共享与特异表示；引入一致性引导机制显式对齐模态潜在分布；最后在融合阶段进行协同注意力融合以整合互补信息并抑制噪声。

Result: 在ADNI数据集上的实验表明，所提方法在阿尔茨海默病诊断任务中优于现有融合策略，取得更高的分类性能（论文未给出具体数值）。

Conclusion: 该论文提出了一个面向MRI和PET多模态融合的阿尔茨海默病诊断框架，重点在于同时保留跨模态共享特征与模态特异特征，并通过一致性引导机制校准模态间分布差异，从而提升诊断性能。

Abstract: Alzheimer's disease (AD) is the most prevalent form of dementia, and its
early diagnosis is essential for slowing disease progression. Recent studies on
multimodal neuroimaging fusion using MRI and PET have achieved promising
results by integrating multi-scale complementary features. However, most
existing approaches primarily emphasize cross-modal complementarity while
overlooking the diagnostic importance of modality-specific features. In
addition, the inherent distributional differences between modalities often lead
to biased and noisy representations, degrading classification performance. To
address these challenges, we propose a Collaborative Attention and
Consistent-Guided Fusion framework for MRI and PET based AD diagnosis. The
proposed model introduces a learnable parameter representation (LPR) block to
compensate for missing modality information, followed by a shared encoder and
modality-independent encoders to preserve both shared and specific
representations. Furthermore, a consistency-guided mechanism is employed to
explicitly align the latent distributions across modalities. Experimental
results on the ADNI dataset demonstrate that our method achieves superior
diagnostic performance compared with existing fusion strategies.

</details>


### [19] [Monocular absolute depth estimation from endoscopy via domain-invariant feature learning and latent consistency](https://arxiv.org/abs/2511.02247)
*Hao Li,Daiwei Lu,Jesse d'Almeida,Dilara Isik,Ehsan Khodapanah Aghdam,Nick DiSanto,Ayberk Acar,Susheela Sharma,Jie Ying Wu,Robert J. Webster III,Ipek Oguz*

Main category: cs.CV

TL;DR: 提出在潜在特征空间通过对抗学习与方向性一致性进行域对齐，以增强翻译合成图像对真实内镜帧的单目绝对深度估计，实验证明在气道假体数据上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 真实内镜场景中难以获得绝对（公制）深度标注，限制了在真实影像上监督学习深度估计。虽然通过图像翻译可将有标注的合成图像风格化以近真实影像，但图像级翻译常保留域差距，导致训练出的深度网络在真实图像上性能不足。因此需要在更高层次（潜在特征）缩小域差距以提升绝对深度预测。

Method: 将翻译后的合成图像与真实内镜帧输入同一深度估计网络；在中间潜在特征空间引入对抗判别器以促使特征域混合，并通过方向性特征一致性损失保持深度相关信息的可转移性，从而学习域不变特征；使用合成图像的已知深度作为监督，并在真实图像上使用无监督损失或不使用像素级深度标签。

Result: 在中心气道假体的内镜视频数据集上进行评估（具有人工对齐的绝对深度图），相比当前最先进的单目深度估计方法，本方法在绝对深度误差和相对深度排序指标上均有显著提升，并且在不同骨干网络与预训练权重下均能稳定提高结果。代码已开源。

Conclusion: 作者提出了一种基于潜在特征对齐的无监督域适配方法，以改善内镜单目深度估计的绝对深度预测能力。通过在特征空间使用对抗学习和方向性特征一致性，网络学习域不变表示，从而缩小真实与合成（已翻译）图像之间的域差距。该方法独立于图像翻译过程，可与多种骨干网络和预训练权重配合使用，并在气道假体内镜视频的绝对深度评估中优于现有方法。

Abstract: Monocular depth estimation (MDE) is a critical task to guide autonomous
medical robots. However, obtaining absolute (metric) depth from an endoscopy
camera in surgical scenes is difficult, which limits supervised learning of
depth on real endoscopic images. Current image-level unsupervised domain
adaptation methods translate synthetic images with known depth maps into the
style of real endoscopic frames and train depth networks using these translated
images with their corresponding depth maps. However a domain gap often remains
between real and translated synthetic images. In this paper, we present a
latent feature alignment method to improve absolute depth estimation by
reducing this domain gap in the context of endoscopic videos of the central
airway. Our methods are agnostic to the image translation process and focus on
the depth estimation itself. Specifically, the depth network takes translated
synthetic and real endoscopic frames as input and learns latent
domain-invariant features via adversarial learning and directional feature
consistency. The evaluation is conducted on endoscopic videos of central airway
phantoms with manually aligned absolute depth maps. Compared to
state-of-the-art MDE methods, our approach achieves superior performance on
both absolute and relative depth metrics, and consistently improves results
across various backbones and pretrained weights. Our code is available at
https://github.com/MedICL-VU/MDE.

</details>


### [20] [Medical Report Generation: A Hierarchical Task Structure-Based Cross-Modal Causal Intervention Framework](https://arxiv.org/abs/2511.02271)
*Yucheng Song,Yifan Ge,Junhao Li,Zhining Liao,Zhifang Liao*

Main category: cs.CV

TL;DR: HTSC-CIF decomposes MRG into low/mid/high-level tasks to align entity locations, enhance cross-modal embeddings, and remove confounders via causal intervention, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing MRG methods tackle isolated problems and fail to jointly address domain knowledge gaps, poor text-visual embeddings, and cross-modal biases; a unified hierarchical approach can resolve all three.

Method: Hierarchical task decomposition into low-level (entity-spatial alignment), mid-level (Prefix Language Modeling and Masked Image Modeling for mutual cross-modal guidance), and high-level (cross-modal causal intervention via front-door intervention) components.

Result: Extensive experiments show HTSC-CIF significantly outperforms SOTA MRG methods; code release planned upon acceptance.

Conclusion: HTSC-CIF effectively addresses insufficiencies in domain knowledge, cross-modal alignment, and spurious correlations, yielding state-of-the-art performance in MRG tasks.

Abstract: Medical Report Generation (MRG) is a key part of modern medical diagnostics,
as it automatically generates reports from radiological images to reduce
radiologists' burden. However, reliable MRG models for lesion description face
three main challenges: insufficient domain knowledge understanding, poor
text-visual entity embedding alignment, and spurious correlations from
cross-modal biases. Previous work only addresses single challenges, while this
paper tackles all three via a novel hierarchical task decomposition approach,
proposing the HTSC-CIF framework. HTSC-CIF classifies the three challenges into
low-, mid-, and high-level tasks: 1) Low-level: align medical entity features
with spatial locations to enhance domain knowledge for visual encoders; 2)
Mid-level: use Prefix Language Modeling (text) and Masked Image Modeling
(images) to boost cross-modal alignment via mutual guidance; 3) High-level: a
cross-modal causal intervention module (via front-door intervention) to reduce
confounders and improve interpretability. Extensive experiments confirm
HTSC-CIF's effectiveness, significantly outperforming state-of-the-art (SOTA)
MRG methods. Code will be made public upon paper acceptance.

</details>


### [21] [Are Euler angles a useful rotation parameterisation for pose estimation with Normalizing Flows?](https://arxiv.org/abs/2511.02277)
*Giorgos Sfikas,Konstantina Nikolaidou,Foteini Papadopoulou,George Retsinas,Anastasios L. Kesidis*

Main category: cs.CV

TL;DR: 研究证明在法线流模型中用欧拉角表示3D姿态是一个实用且有时优于更复杂表示的方法，能生成有意义的概率性姿态估计，尤其适用于不确定或多模态的姿态问题。


<details>
  <summary>Details</summary>
Motivation: 在传感器噪声、投影模糊和物体对称性导致姿态模糊或多解的场景下，提供概率性姿态估计比单点估计更有价值。研究是否采用简单直观的欧拉角参数化在概率模型（Normalizing Flows）中能带来实际好处。

Method: 基于欧拉角的参数化，将三维旋转表示转换为可由Normalizing Flows学习的连续变量分布；训练过程中以目标图像和规范姿态为条件输入，学习后验分布；可能对比了其他参数化（如四元数、旋转矩阵、轴角等）并评估概率输出质量。

Result: 论文表明欧拉角参数化能在Normalizing Flows框架下成功建模3D姿态的概率分布，并在若干任务或数据集上表现出与更复杂参数化方法相当或具有优势的性能，尤其在实现复杂度、训练稳定性或解释性方面表现良好。

Conclusion: 作者认为在法线流（Normalizing Flows）框架下使用欧拉角参数化来估计物体三维姿态是可行且有益的。尽管欧拉角存在缺陷，但在模型复杂性、直观性和实现便利性方面具有优势，并能在多模态或不确定姿态场景下提供有用的概率输出。

Abstract: Object pose estimation is a task that is of central importance in 3D Computer
Vision. Given a target image and a canonical pose, a single point estimate may
very often be sufficient; however, a probabilistic pose output is related to a
number of benefits when pose is not unambiguous due to sensor and projection
constraints or inherent object symmetries. With this paper, we explore the
usefulness of using the well-known Euler angles parameterisation as a basis for
a Normalizing Flows model for pose estimation. Isomorphic to spatial rotation,
3D pose has been parameterized in a number of ways, either in or out of the
context of parameter estimation. We explore the idea that Euler angles, despite
their shortcomings, may lead to useful models in a number of aspects, compared
to a model built on a more complex parameterisation.

</details>


### [22] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: 提出SAIL-RL：用双重奖励教模型何时以及如何思考，提升多模态模型的推理质量与可靠性，减少幻觉，并在多模型尺度上取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅奖励最终结果且思考策略单一，导致模型在简单问题上过度思考、在复杂问题上不足以思考，且推理过程不可靠。为此提出一个能评估推理质量并自适应思考深度的框架。

Method: 在现有模型（如SAIL-VL2）上进行强化学习后训练，使用Thinking Reward评估推理过程的事实依据、逻辑连贯性和答案一致性；使用Judging Reward判定是否应进行复杂推理或直接回答，并据此调整模型的思考策略。

Result: 在4B与8B参数规模的SAIL-VL2上进行实验，SAIL-RL在推理与多模态理解基准上均有提升，显著降低幻觉率，并在若干任务上与GPT-4o等闭源商用模型表现竞争。

Conclusion: SAIL-RL通过引入双重奖励（Thinking Reward与Judging Reward）在后训练阶段提升多模态大语言模型的推理能力，避免了只以结果为导向的不足，并能自适应决定是否需要深度思考，减少幻觉并在多模态推理基准上显著提升性能。

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [23] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 将HME视为图：用BLSTM获得原始符号与关系候选，用2D-CFG扩展关系，再用GNN去除错误连边，生成精确的符号标签图，实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统HME识别方法难以同时处理符号识别与复杂的二维结构关系，图结构可以自然表示符号间的空间依赖，GNN能在图上进行有效的关系建模与推断，从而提升结构识别性能。

Method: 先用深度双向LSTM进行符号分割、识别与空间关系分类，构建初始原语图；随后用2D上下文无关文法(2D-CFG)解析生成所有可能的空间关系候选；再用基于图神经网络的连边预测模型对候选图进行精修，去除冗余连边，得到最终的符号标签图。

Result: 在实验中，所提方法在HME结构识别任务上取得了有竞争力的性能，表明通过图建模与GNN精修能有效改善结构恢复的准确性。

Conclusion: 本文提出将手写数学表达式(HME)建模为图，通过节点为符号、边表示空间依赖，利用GNN进行连接预测与结构精修，最终生成符号标签图。实验证明该方法在结构识别上具有较好效果。

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [24] [Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization](https://arxiv.org/abs/2511.02329)
*Shaohan Li,Yunpeng Shi,Gilad Lerman*

Main category: cs.CV

TL;DR: 提出基于循环一致性和改进 MPLS 的稳健全局相机位姿估计方法，理论与实验证明在鲁棒性和样本复杂度上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前全局位姿估计在存在噪声和外点时仍然脆弱，且通常依赖束调整来精化结果。作者希望利用循环一致性的结构化信息，构建一个在噪声和外点下更稳健且样本复杂度低的全局求解器。

Method: 将消息传递最小二乘 (MPLS) 适配于相机位置估计，重定义循环一致性（用迭代中估计的距离替代真实距离），引入 Welsch 稳健损失并在旋转同步中融合循环一致性；同时设计一个可插拔的外点剔除模块，整体避免束调整。

Result: 理论上给出了迄今最强的确定性精确恢复保证，证明仅凭循环一致性即可达到目前已知的最低样本复杂度；在合成与真实数据上，Cycle-Sync 在多种基准上超越了领先的位姿估计器和包含束调整的完整 SfM 管线。

Conclusion: Cycle-Sync 提出一种基于循环一致性的全局相机位姿估计框架，重点是用于相机位置的改进 MPLS 求解器，并通过引入 Welsch 型稳健损失和基于鲁棒子空间恢复的外点剔除模块提升鲁棒性；实验表明优于现有方法。

Abstract: We introduce Cycle-Sync, a robust and global framework for estimating camera
poses (both rotations and locations). Our core innovation is a location solver
that adapts message-passing least squares (MPLS) -- originally developed for
group synchronization -- to camera location estimation. We modify MPLS to
emphasize cycle-consistent information, redefine cycle consistencies using
estimated distances from previous iterations, and incorporate a Welsch-type
robust loss. We establish the strongest known deterministic exact-recovery
guarantee for camera location estimation, showing that cycle consistency alone
-- without access to inter-camera distances -- suffices to achieve the lowest
sample complexity currently known. To further enhance robustness, we introduce
a plug-and-play outlier rejection module inspired by robust subspace recovery,
and we fully integrate cycle consistency into MPLS for rotation
synchronization. Our global approach avoids the need for bundle adjustment.
Experiments on synthetic and real datasets show that Cycle-Sync consistently
outperforms leading pose estimators, including full structure-from-motion
pipelines with bundle adjustment.

</details>


### [25] [GAFD-CC: Global-Aware Feature Decoupling with Confidence Calibration for OOD Detection](https://arxiv.org/abs/2511.02335)
*Kun Zou,Yongheng Xu,Jianxing Yu,Yan Pan,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 提出GAFD-CC：一种基于分类权重的全局感知特征解耦与基于logit置信度自适应融合的后验OOD检测方法，能细化ID/OOD边界并抑制误报，在大规模基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练（post-hoc）OOD检测方法虽利用特征和logit信息，但常忽视两者之间的固有相关性，导致边界判别不够精细。论文动机在于通过显式挖掘并利用特征与分类权重（进而与logit）的相关性，来提升检测的鲁棒性与判别力。

Method: 方法包括两步：1) 基于分类权重的全局感知特征解耦，将特征按与全局分类权重方向的正负相关分离，得到促进边界细化的正相关特征和抑制误报的负相关特征；2) 将这些解耦后的多尺度特征与基于logit的置信度进行自适应融合，从而综合利用特征与logit之间的内在相关性进行后验OOD检测。

Result: 在多个大规模基准数据集上，GAFD-CC在OOD检测指标（如FPR95、AUROC等）上优于或匹配当前最先进方法，且展示了良好的泛化能力和稳定性。

Conclusion: GAFD-CC通过全局感知的特征解耦和基于logit的置信度自适应融合，提高了OOD检测的判别性，能有效细化ID/OOD边界并抑制误报，实验表明其在大规模基准上表现优越且具较好泛化能力。

Abstract: Out-of-distribution (OOD) detection is paramount to ensuring the reliability
and robustness of learning models in real-world applications. Existing post-hoc
OOD detection methods detect OOD samples by leveraging their features and
logits information without retraining. However, they often overlook the
inherent correlation between features and logits, which is crucial for
effective OOD detection. To address this limitation, we propose Global-Aware
Feature Decoupling with Confidence Calibration (GAFD-CC). GAFD-CC aims to
refine decision boundaries and increase discriminative performance. Firstly, it
performs global-aware feature decoupling guided by classification weights. This
involves aligning features with the direction of global classification weights
to decouple them. From this, GAFD-CC extracts two types of critical
information: positively correlated features that promote in-distribution
(ID)/OOD boundary refinement and negatively correlated features that suppress
false positives and tighten these boundaries. Secondly, it adaptively fuses
these decoupled features with multi-scale logit-based confidence for
comprehensive and robust OOD detection. Extensive experiments on large-scale
benchmarks demonstrate GAFD-CC's competitive performance and strong
generalization ability compared to those of state-of-the-art methods.

</details>


### [26] [M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings](https://arxiv.org/abs/2511.02349)
*Jiankai Tang,Tao Zhang,Jia Li,Yiru Zhang,Mingyu Zhang,Kegang Wang,Yuming Hao,Bolin Wang,Haiyang Li,Xingyao Wang,Yuanchun Shi,Yuntao Wang,Sichong Qian*

Main category: cs.CV

TL;DR: 首次公开双视角手机PPG数据集M3PD并提出融合面部与指尖的F3Mamba，显著降低心率估计误差并提升真实场景鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有移动PPG受运动、光照与单视角限制，且缺乏针对心血管患者的公开跨设备数据集，限制实际临床应用与方法比较。

Method: 收集60名参与者（47名心血管病患者）同时用手机前后摄像头同步拍摄面部与指尖视频，建立M3PD双视角数据集。提出F3Mamba模型，基于Mamba的时间建模对两路视频信号进行特征提取与融合以估计心率。与单视角基线比较，评估心率估计误差与鲁棒性。

Result: 在包含心血管患者的真实场景中，F3Mamba将心率误差相比单视角基线降低21.9%至30.2%，在挑战性条件下表现出更高的稳定性。公开了数据与代码以促进后续研究。

Conclusion: 该论文通过公开发布双视角智能手机PPG数据集（M3PD）并提出F3Mamba模型，展示了融合面部与指尖视频可显著提高心率估计精度与鲁棒性，为移动无创心率监测在心血管患者中的应用提供可复现的数据与方法支持。

Abstract: Portable physiological monitoring is essential for early detection and
management of cardiovascular disease, but current methods often require
specialized equipment that limits accessibility or impose impractical postures
that patients cannot maintain. Video-based photoplethysmography on smartphones
offers a convenient noninvasive alternative, yet it still faces reliability
challenges caused by motion artifacts, lighting variations, and single-view
constraints. Few studies have demonstrated reliable application to
cardiovascular patients, and no widely used open datasets exist for
cross-device accuracy. To address these limitations, we introduce the M3PD
dataset, the first publicly available dual-view mobile photoplethysmography
dataset, comprising synchronized facial and fingertip videos captured
simultaneously via front and rear smartphone cameras from 60 participants
(including 47 cardiovascular patients). Building on this dual-view setting, we
further propose F3Mamba, which fuses the facial and fingertip views through
Mamba-based temporal modeling. The model reduces heart-rate error by 21.9 to
30.2 percent over existing single-view baselines while improving robustness in
challenging real-world scenarios. Data and code:
https://github.com/Health-HCI-Group/F3Mamba.

</details>


### [27] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: CoCoVa通过迭代的连续潜在思维和动态注意机制，实现了更接近视觉连续性的跨模态推理，从而提高了小/中型VLM在多项任务上的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有VLM受限于离散语言token空间，无法充分表达视觉感知中高维、连续及默会的思维过程，需引入连续潜在表示以桥接视觉与语言的表征差异。

Method: 提出CoCoVa框架：1) 使用Latent Q-Former作为动态推理引擎，迭代精炼潜在思维向量链；2) 设计动态token选择机制以聚焦显著视觉区域；3) 采用多任务训练目标（对比学习+基于扩散的重构）以保持潜在向量与视觉/文本模态对齐。

Result: 实验表明CoCoVa在准确率和token效率上均优于强基线：1.5B骨干模型可在几乎所有基准上与7B-9B模型竞争或超越；扩展到7B LLM骨干仍能接近SOTA；定性分析显示潜在空间具有可解释的结构化推理模式。

Conclusion: CoCoVa通过引入连续的潜在思维向量链和迭代的Latent Q-Former实现了跨模态的连续推理，缓解了语言标记空间对视觉理解的限制，在多项视觉-语言任务上提升了准确性和token效率，且小模型在若干基准上可与更大模型媲美。

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [28] [RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning](https://arxiv.org/abs/2511.02384)
*Jiahe Song,Chuang Wang,Bowen Jiang,Yinfan Wang,Hao Zheng,Xingjian Wei,Chengjin Liu,Junyuan Gao,Yubin Wang,Lijun Wu,Jiang Wu,Qian Yu,Conghui He*

Main category: cs.CV

TL;DR: 将反应图解析重构为图像字幕任务，利用MolYOLO预绘BBox与索引作为视觉提示（BIVP），结合LVLMs实现SOTA解析，并发布了规模更大的RxnCaption-11k数据集。


<details>
  <summary>Details</summary>
Motivation: 现有化学反应数据主要以论文图像形式存在，难以被机器直接读取和用于训练；传统基于坐标预测的解析复杂且不易与LVLMs整合，故提出更自然的语言化解析方法。

Method: 设计RxnCaption框架：先用MolYOLO检测分子并在图像上预绘BBox与索引（BIVP），然后将解析任务作为自然语言描述输入给LVLMs进行生成；构建并训练RxnCaption-VL模型；构建RxnCaption-11k数据集并按四种布局均衡测试集，进行大规模实验比较。

Result: BIVP策略显著提高了结构提取质量并简化模型设计；RxnCaption-VL在多项评测指标上达到了最先进水平；RxnCaption-11k为社区提供了更大、更均衡的真实世界反应图数据集。

Conclusion: 该论文提出了将化学反应图解析问题（RxnDP）转化为图像字幕生成问题的新范式，通过在图像上预绘分子边界框和索引（BIVP）来提示大型视觉-语言模型，简化了解析流程并提升了结构提取质量。作者同时构建了大规模数据集RxnCaption-11k并在多项指标上取得了SOTA结果。

Abstract: Large-scale chemical reaction datasets are crucial for AI research in
chemistry. However, existing chemical reaction data often exist as images
within papers, making them not machine-readable and unusable for training
machine learning models. In response to this challenge, we propose the
RxnCaption framework for the task of chemical Reaction Diagram Parsing (RxnDP).
Our framework reformulates the traditional coordinate prediction driven parsing
process into an image captioning problem, which Large Vision-Language Models
(LVLMs) handle naturally. We introduce a strategy termed "BBox and Index as
Visual Prompt" (BIVP), which uses our state-of-the-art molecular detector,
MolYOLO, to pre-draw molecular bounding boxes and indices directly onto the
input image. This turns the downstream parsing into a natural-language
description problem. Extensive experiments show that the BIVP strategy
significantly improves structural extraction quality while simplifying model
design. We further construct the RxnCaption-11k dataset, an order of magnitude
larger than prior real-world literature benchmarks, with a balanced test subset
across four layout archetypes. Experiments demonstrate that RxnCaption-VL
achieves state-of-the-art performance on multiple metrics. We believe our
method, dataset, and models will advance structured information extraction from
chemical literature and catalyze broader AI applications in chemistry. We will
release data, models, and code on GitHub.

</details>


### [29] [Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2511.02395)
*Leon Schwarzer,Matthias Zeller,Daniel Casado Herraez,Simon Dierl,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出一种聚类引导的对比自监督预训练策略，结合动态点移除和有限标注微调，实现稀疏噪声雷达点云的高效移动目标分割，提升了标签效率与SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 雷达能在单帧提供多普勒速度以支持快速移动目标分割，且不需时间序列积累，但雷达点云稀疏且噪声多，标注昂贵，故引入自监督学习以减少标注需求。

Method: 两步法：1）基于聚类的对比自监督表示学习，提出新的对比损失并通过动态点移除进行聚类精炼以获得运动感知表示；2）在有限标注数据上进行监督微调。

Result: 经自监督预训练后，方法在有限标注条件下显著提升了现有最先进方法的性能，提高了标签效率并增强了对雷达数据的运动表征能力。

Conclusion: 该论文提出了一种针对稀疏且噪声较大的雷达点云的自监督移动目标分割方法，通过对比式预训练与有限标注微调相结合，提升了标签效率和分割性能。

Abstract: Moving object segmentation is a crucial task for safe and reliable autonomous
mobile systems like self-driving cars, improving the reliability and robustness
of subsequent tasks like SLAM or path planning. While the segmentation of
camera or LiDAR data is widely researched and achieves great results, it often
introduces an increased latency by requiring the accumulation of temporal
sequences to gain the necessary temporal context. Radar sensors overcome this
problem with their ability to provide a direct measurement of a point's Doppler
velocity, which can be exploited for single-scan moving object segmentation.
However, radar point clouds are often sparse and noisy, making data annotation
for use in supervised learning very tedious, time-consuming, and
cost-intensive. To overcome this problem, we address the task of
self-supervised moving object segmentation of sparse and noisy radar point
clouds. We follow a two-step approach of contrastive self-supervised
representation learning with subsequent supervised fine-tuning using limited
amounts of annotated data. We propose a novel clustering-based contrastive loss
function with cluster refinement based on dynamic points removal to pretrain
the network to produce motion-aware representations of the radar data. Our
method improves label efficiency after fine-tuning, effectively boosting
state-of-the-art performance by self-supervised pretraining.

</details>


### [30] [A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds](https://arxiv.org/abs/2511.02397)
*Kuo-Liang Chung,Ting-Chung Tang*

Main category: cs.CV

TL;DR: 提出基于重叠率自适应分组的点云颜色校正（KBI、JKHE、HE三级策略），在1086对点云上验证并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有颜色校正方法多集中于二维图像，而三维点云的颜色一致性在渲染与压缩等应用中是基础且重要的问题，尤其在不同采集或对齐条件下需消除颜色差异。作者希望提出一种适用于点云的自适应分组校正策略以提升一致性。

Method: 先估计对齐后源/目标点云的重叠率，依据重叠率自适应分组：低重叠率分为Gcl与Gmod，高重叠率再增加Gdist。Gcl使用基于K近邻的双边插值(KBI)进行颜色传播；Gmod使用联合KBI与直方图均衡化(JKHE)；Gdist直接采用直方图均衡化(HE)。此外进行了分组影响分析与消融研究。

Result: 在1086对测试点云上与现有方法比较，所提算法在颜色一致性方面取得了期望的改进（论文声称优越），并公开了C++代码以供复现。论文还讨论了分组无影响属性及消融实验结果。

Conclusion: 本文提出了一种基于分组的混合颜色校正算法，通过估计对齐后的源/目标点云重叠率，自适应将目标点分为近邻组Gcl、中等组Gmod（以及在高重叠率下的远距离组Gdist），并对不同组采用不同的校正策略以实现颜色一致性。实验在1086对点云上验证了优越性，并公开了C++实现。

Abstract: Color consistency correction for color point clouds is a fundamental yet
important task in 3D rendering and compression applications. In the past, most
previous color correction methods aimed at correcting color for color images.
The purpose of this paper is to propose a grouping-based hybrid color
correction algorithm for color point clouds. Our algorithm begins by estimating
the overlapping rate between the aligned source and target point clouds, and
then adaptively partitions the target points into two groups, namely the close
proximity group Gcl and the moderate proximity group Gmod, or three groups,
namely Gcl, Gmod, and the distant proximity group Gdist, when the estimated
overlapping rate is low or high, respectively. To correct color for target
points in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) method
is proposed. To correct color for target points in Gmod, a joint KBI and the
histogram equalization (JKHE) method is proposed. For target points in Gdist, a
histogram equalization (HE) method is proposed for color correction. Finally,
we discuss the grouping-effect free property and the ablation study in our
algorithm. The desired color consistency correction benefit of our algorithm
has been justified through 1086 testing color point cloud pairs against the
state-of-the-art methods. The C++ source code of our algorithm can be accessed
from the website: https://github.com/ivpml84079/Point-cloud-color-correction.

</details>


### [31] [Purrturbed but Stable: Human-Cat Invariant Representations Across CNNs, ViTs and Self-Supervised ViTs](https://arxiv.org/abs/2511.02404)
*Arya Shah,Vaibhav Tripathi*

Main category: cs.CV

TL;DR: 作者建立了一个冻结编码器基准，发现自监督DINO ViT比有监督ViT、CNN和windowed Transformer更能将猫与人类的视觉表征对齐，尤其在浅层模块，表明自监督+ViT归纳偏置有助于跨物种表征一致性。


<details>
  <summary>Details</summary>
Motivation: 动机是理解猫（Feliscatus）与人类在眼部解剖差异（如猫的竖直瞳孔）如何在下游视觉表征中体现，及不同现代视觉模型在跨物种表征对齐上的差异，为神经科学提供可检验假设。

Method: 作者构建了一个"冻结编码器基准"，使用层级CKA（线性与RBF）和RSA对比猫与人视觉表征，覆盖卷积网络、有监督与自监督ViT、以及windowed Transformer，另外报告了分布性与稳定性测试。分析是层级的，考察不同深度块的相似性与表征几何。

Result: 主要结果包含：DINO ViT-B/16在平均CKA-RBF≈0.814、CKA-linear≈0.745、RSA≈0.698等指标上领先，且对齐在早期块达到峰值；有监督ViT在CKA上较好但RSA较低，表明几何差异；CNN和windowed ViT表现逊色。论文还发布了代码与数据以便重现。

Conclusion: 这篇论文结论是：在比较多种视觉模型（卷积网络、有监督ViT、windowed ViT、自监督DINO ViT）时，DINO ViT-B/16在猫-人视觉表征对齐上表现最好，尤其在浅层模块，表明自监督和ViT的token级学习有助于跨物种统计特征桥接；有监督ViT在某些指标上接近但几何对应性较弱；CNN和windowed ViT相对较差，提示架构归纳偏置影响跨物种对齐。论文提出了可检验的神经科学假设并公开了代码与数据。

Abstract: Cats and humans differ in ocular anatomy. Most notably, Felis Catus (domestic
cats) have vertically elongated pupils linked to ambush predation; yet, how
such specializations manifest in downstream visual representations remains
incompletely understood. We present a unified, frozen-encoder benchmark that
quantifies feline-human cross-species representational alignment in the wild,
across convolutional networks, supervised Vision Transformers, windowed
transformers, and self-supervised ViTs (DINO), using layer-wise Centered Kernel
Alignment (linear and RBF) and Representational Similarity Analysis, with
additional distributional and stability tests reported in the paper. Across
models, DINO ViT-B/16 attains the most substantial alignment (mean CKA-RBF
$\approx0.814$, mean CKA-linear $\approx0.745$, mean RSA $\approx0.698$),
peaking at early blocks, indicating that token-level self-supervision induces
early-stage features that bridge species-specific statistics. Supervised ViTs
are competitive on CKA yet show weaker geometric correspondence than DINO
(e.g., ViT-B/16 RSA $\approx0.53$ at block8; ViT-L/16 $\approx0.47$ at
block14), revealing depth-dependent divergences between similarity and
representational geometry. CNNs remain strong baselines but below plain ViTs on
alignment, and windowed transformers underperform plain ViTs, implicating
architectural inductive biases in cross-species alignment. Results indicate
that self-supervision coupled with ViT inductive biases yields representational
geometries that more closely align feline and human visual systems than widely
used CNNs and windowed Transformers, providing testable neuroscientific
hypotheses about where and how cross-species visual computations converge. We
release our code and dataset for reference and reproducibility.

</details>


### [32] [IllumFlow: Illumination-Adaptive Low-Light Enhancement via Conditional Rectified Flow and Retinex Decomposition](https://arxiv.org/abs/2511.02411)
*Wenyang Wei,Yang yang,Xixi Jia,Xiangchu Feng,Weiwei Wang,Renzhen Wang*

Main category: cs.CV

TL;DR: IllumFlow将条件Rectified Flow与Retinex分解结合：用流场建模照明变化、专门去噪反射率，达成更好亮度调整与去噪的低光增强效果。


<details>
  <summary>Details</summary>
Motivation: 低光图像既存在复杂的照明变化又常伴有高频噪声，直接端到端增强难以同时兼顾照明适配与噪声抑制。将问题分解到Retinex的照明与反射率两个子问题，有助于分别优化照明调节和去噪。

Method: 先按Retinex将图像分解为反射率和照明分量；对照明分量使用条件Rectified Flow建模为连续的光流场以表示照明变化；对反射率分量使用专门的去噪网络，并利用流场生成的数据增强提升去噪效果和去色差能力；最后合成恢复图像，支持可定制的亮度增强。

Result: 在多个低光增强和曝光校正数据集上，IllumFlow在定量指标和主观视觉效果上都优于现有方法，能精确适配不同照明并保持色彩一致性与细节。

Conclusion: IllumFlow通过将条件Rectified Flow与Retinex理论结合，实现了对光照和反射率的分离优化，从而在提升亮度的同时有效抑制噪声和保持色彩忠实。实验结果显示其在低光照增强和曝光校正任务上优于现有方法。

Abstract: We present IllumFlow, a novel framework that synergizes conditional Rectified
Flow (CRF) with Retinex theory for low-light image enhancement (LLIE). Our
model addresses low-light enhancement through separate optimization of
illumination and reflectance components, effectively handling both lighting
variations and noise. Specifically, we first decompose an input image into
reflectance and illumination components following Retinex theory. To model the
wide dynamic range of illumination variations in low-light images, we propose a
conditional rectified flow framework that represents illumination changes as a
continuous flow field. While complex noise primarily resides in the reflectance
component, we introduce a denoising network, enhanced by flow-derived data
augmentation, to remove reflectance noise and chromatic aberration while
preserving color fidelity. IllumFlow enables precise illumination adaptation
across lighting conditions while naturally supporting customizable brightness
enhancement. Extensive experiments on low-light enhancement and exposure
correction demonstrate superior quantitative and qualitative performance over
existing methods.

</details>


### [33] [ChartM$^3$: A Multi-Stage Code-Driven Pipeline for Constructing Multi-Dimensional and Multi-Step Visual Reasoning Data in Chart Comprehension](https://arxiv.org/abs/2511.02415)
*Duo Xu,Hao Cheng,Xin Lin,Zhen Xie,Hao Wang*

Main category: cs.CV

TL;DR: 提出自动化代码驱动的多阶段生成管道（RAG+CoT）用于构建大规模、多维、多步骤的图表推理数据集ChartM^3，显著提升MLLM对复杂图表的理解与推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在处理复杂图表和计算密集型推理任务时覆盖不足，缺少系统性、多样化且可模拟真实场景的训练数据。为此需要自动化生成高质量、多步骤、多维度的图表推理数据集以提升模型能力。

Method: 管道包含：1）使用RAG检索专业图表模板；2）利用链式思维（CoT）生成用于模拟真实数据分布的推理代码；3）基于这些代码进行图表渲染和问题相关的统计计算；4）通过模型驱动的评估迭代提升图表多样性与数据质量。

Result: 构建了ChartM^3数据集：38K张图表、142K个训练问答对，以及2,871个高质量评测样本。通过SFT和RL训练，数据集显著提升了模型的推理能力和跨域泛化，使小模型在复杂图表理解上能达到与大模型可比的性能。

Conclusion: 本文提出了一个自动化、多阶段、代码驱动的管道，用于系统地生成用于视觉推理的图表数据集，通过RAG检索专业图表模板并结合CoT生成推理代码，模拟真实数据分布，从而驱动图表渲染和统计计算，最终构建了ChartM^3数据集并证明能显著提升模型的图表理解与推理能力。

Abstract: Complex chart understanding tasks demand advanced visual recognition and
reasoning capabilities from multimodal large language models (MLLMs). However,
current research provides limited coverage of complex chart scenarios and
computation-intensive reasoning tasks prevalent in real-world applications.
This study proposes an automated multi-stage code-driven pipeline for
systematically generating visual reasoning datasets to address these
limitations. The pipeline integrates retrieval-augmented generation (RAG) to
retrieve professional chart templates and employs chain-of-thought (CoT)
strategies to generate reasoning codes that simulate real data distributions,
thereby driving chart rendering and question-related statistical computations.
Through model-based evaluation, the pipeline enhances chart diversity and data
quality. Using this framework, we construct ChartM$^3$, a multi-dimensional and
multi-step dataset containing 38K charts and 142K Q&A pairs for training, along
with 2,871 high-quality evaluation samples for enabling practical performance
assessment. Supervised fine-tuning (SFT) and reinforcement learning (RL)
experiments demonstrate that our dataset significantly improves reasoning
capabilities and cross-domain generalization performance, enabling smaller
models to achieve performance comparable to larger-scale models in complex
chart comprehension.

</details>


### [34] [Synthetic Crop-Weed Image Generation and its Impact on Model Generalization](https://arxiv.org/abs/2511.02417)
*Garen Boyadjian,Cyrille Pierre,Johann Laconte,Riccardo Bertoglio*

Main category: cs.CV

TL;DR: 作者用Blender程序化生成多样化合成数据训练分割模型，结果显示合成数据能将sim-to-real差距控制在约10%，并在跨域泛化上优于真实数据，建议采用合成与真实混合训练策略。


<details>
  <summary>Details</summary>
Motivation: 真实田间标注成本高昂，合成数据可降低标注开销，但需研究合成与真实之间的域差距及其对分割模型泛化的影响。

Method: 在Blender中构建程序化生成管线，随机化植物生长、杂草密度、光照、相机角度等因素，自动生成带注释的合成作物-杂草图像，随后用这些数据训练并评估多种先进的语义分割模型，与真实数据进行对比和跨域测试。

Result: 在基准测试中，使用合成数据训练导致的sim-to-real差距约为10%，优于先前方法；合成数据在跨域测试中表现良好，甚至超过真实数据，表明合成数据具有很好的泛化能力。

Conclusion: 合成数据可显著缩小仿真到真实的语义分割差距，且在跨域泛化上优于真实数据，支持混合训练策略以提高农业分割模型性能。

Abstract: Precise semantic segmentation of crops and weeds is necessary for
agricultural weeding robots. However, training deep learning models requires
large annotated datasets, which are costly to obtain in real fields. Synthetic
data can reduce this burden, but the gap between simulated and real images
remains a challenge. In this paper, we present a pipeline for procedural
generation of synthetic crop-weed images using Blender, producing annotated
datasets under diverse conditions of plant growth, weed density, lighting, and
camera angle. We benchmark several state-of-the-art segmentation models on
synthetic and real datasets and analyze their cross-domain generalization. Our
results show that training on synthetic images leads to a sim-to-real gap of
10%, surpassing previous state-of-the-art methods. Moreover, synthetic data
demonstrates good generalization properties, outperforming real datasets in
cross-domain scenarios. These findings highlight the potential of synthetic
agricultural datasets and support hybrid strategies for more efficient model
training.

</details>


### [35] [From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics](https://arxiv.org/abs/2511.02427)
*Nicolas Schuler,Lea Dewald,Nick Baldig,Jürgen Graf*

Main category: cs.CV

TL;DR: 论文评估了面向移动机器人边缘设备的小型VLM在场景理解和动作识别上的可行性，结果显示有潜力但仍受限于准确性、偏差与复杂场景处理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨在移动机器人与边缘计算场景中，能否用资源受限的小型VLM实现可靠的场景理解与常识推理，从而平衡精度与推理速度，降低部署门槛。

Method: 构建针对移动机器人场景的管线，选取多种小型VLM进行评估；在多样化现实世界城市景观、校园与室内场景数据集上测量场景解释和动作识别性能；分析模型在边缘设备上的推理时间、资源占用及偏差表现。

Result: 实验表明小型VLM在部分场景与动作类型上能取得可接受结果并实现实时推理，但在复杂动作、长时上下文与易混淆类别上准确率显著下降；存在类偏见与场景偏差；需要后处理、模型蒸馏或多模态融合以提升鲁棒性。

Conclusion: 小型视觉-语言模型在场景理解与动作识别任务上具备一定潜力，但在边缘设备部署时受限于精度、实时性与偏差问题，尚无法完全替代大型模型。

Abstract: Video Understanding, Scene Interpretation and Commonsense Reasoning are
highly challenging tasks enabling the interpretation of visual information,
allowing agents to perceive, interact with and make rational decisions in its
environment. Large Language Models (LLMs) and Visual Language Models (VLMs)
have shown remarkable advancements in these areas in recent years, enabling
domain-specific applications as well as zero-shot open vocabulary tasks,
combining multiple domains. However, the required computational complexity
poses challenges for their application on edge devices and in the context of
Mobile Robotics, especially considering the trade-off between accuracy and
inference time. In this paper, we investigate the capabilities of
state-of-the-art VLMs for the task of Scene Interpretation and Action
Recognition, with special regard to small VLMs capable of being deployed to
edge devices in the context of Mobile Robotics. The proposed pipeline is
evaluated on a diverse dataset consisting of various real-world cityscape,
on-campus and indoor scenarios. The experimental evaluation discusses the
potential of these small models on edge devices, with particular emphasis on
challenges, weaknesses, inherent model biases and the application of the gained
information. Supplementary material is provided via the following repository:
https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/

</details>


### [36] [KAO: Kernel-Adaptive Optimization in Diffusion for Satellite Image](https://arxiv.org/abs/2511.02462)
*Teerapong Panboonyuen*

Main category: cs.CV

TL;DR: KAO在扩散模型中通过对潜在空间进行核自适应优化并加入显式传播，实现了对超高分辨率卫星影像的高效精确修复，兼具预调与后调方法的优点。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么需大量重训练的预调模型，要么计算开销大的后调模型，难以在VHR卫星图像（如DeepGlobe、Massachusetts Roads）上兼顾效率与精度，因而需要一种高效且灵活的修复框架。

Method: 提出Latent Space Conditioning（潜在空间条件化）在潜在空间中优化参数，结合Kernel-Adaptive Optimization（核自适应优化）和Explicit Propagation（显式传播）用于扩散过程的前向-逆向融合，以达到高效稳定的图像修复。

Result: 在DeepGlobe和Massachusetts Roads等VHR数据集上，KAO在恢复精度与计算效率之间取得了良好平衡，并在基准测试中显著优于现有方法，成为新的性能基线。

Conclusion: 论文提出的KAO方法通过在扩散模型中对紧凑潜在空间进行核自适应优化，并引入显式传播实现前向-逆向融合，从而在超高分辨率卫星图像修复任务上提高了准确性和效率。

Abstract: Satellite image inpainting is a crucial task in remote sensing, where
accurately restoring missing or occluded regions is essential for robust image
analysis. In this paper, we propose KAO, a novel framework that utilizes
Kernel-Adaptive Optimization within diffusion models for satellite image
inpainting. KAO is specifically designed to address the challenges posed by
very high-resolution (VHR) satellite datasets, such as DeepGlobe and the
Massachusetts Roads Dataset. Unlike existing methods that rely on
preconditioned models requiring extensive retraining or postconditioned models
with significant computational overhead, KAO introduces a Latent Space
Conditioning approach, optimizing a compact latent space to achieve efficient
and accurate inpainting. Furthermore, we incorporate Explicit Propagation into
the diffusion process, facilitating forward-backward fusion, which improves the
stability and precision of the method. Experimental results demonstrate that
KAO sets a new benchmark for VHR satellite image restoration, providing a
scalable, high-performance solution that balances the efficiency of
preconditioned models with the flexibility of postconditioned models.

</details>


### [37] [MVAFormer: RGB-based Multi-View Spatio-Temporal Action Recognition with Transformer](https://arxiv.org/abs/2511.02473)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shotaro Tora*

Main category: cs.CV

TL;DR: 针对STAR任务提出MVAFormer，通过在特征图上分离同/跨视角自注意力进行视角间合作，保留空间信息并提升识别性能（F-measure +4.4%）。


<details>
  <summary>Details</summary>
Motivation: 现有多视角合作方法多针对对整段视频识别单一动作的设置，使用压缩后的嵌入向量导致空间信息丢失，不适用于需要保留空间信息并对每个人进行时序识别的STAR任务，因此需要一种能在保留空间信息的情况下进行视角间合作的新方法。

Method: 提出了一种Transformer-based合作模块，直接在特征图上进行视角间融合而非使用丢失空间信息的嵌入向量；在模块中将自注意力分为同视角注意力和跨视角注意力，以分别建模视角内与视角间关系；整体框架用于STAR设置，即对视频中每个人的动作进行时序识别。

Result: 在新收集的数据集上实验表明，MVAFormer在F-measure上比对比基线高约4.4个百分点，表明在STAR设置下该方法能有效提升多视角动作识别性能。

Conclusion: 该文提出了适用于时空序列动作识别（STAR）设置的多视角动作识别方法MVAFormer，通过在视角间引入基于Transformer的合作模块，保持空间特征图信息并区分相同视角与不同视角间的自注意力，从而更有效地建模多视角关系。实验在新收集的数据集上显示，MVAFormer在F-measure上相比基线提升约4.4个百分点。

Abstract: Multi-view action recognition aims to recognize human actions using multiple
camera views and deals with occlusion caused by obstacles or crowds. In this
task, cooperation among views, which generates a joint representation by
combining multiple views, is vital. Previous studies have explored promising
cooperation methods for improving performance. However, since their methods
focus only on the task setting of recognizing a single action from an entire
video, they are not applicable to the recently popular spatio-temporal action
recognition~(STAR) setting, in which each person's action is recognized
sequentially. To address this problem, this paper proposes a multi-view action
recognition method for the STAR setting, called MVAFormer. In MVAFormer, we
introduce a novel transformer-based cooperation module among views. In contrast
to previous studies, which utilize embedding vectors with lost spatial
information, our module utilizes the feature map for effective cooperation in
the STAR setting, which preserves the spatial information. Furthermore, in our
module, we divide the self-attention for the same and different views to model
the relationship between multiple views effectively. The results of experiments
using a newly collected dataset demonstrate that MVAFormer outperforms the
comparison baselines by approximately $4.4$ points on the F-measure.

</details>


### [38] [OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control](https://arxiv.org/abs/2511.02483)
*Xilong Zhou,Jianchun Chen,Pramod Rao,Timo Teufel,Linjie Lyu,Tigran Minasian,Oleksandr Sotnychenko,Xiaoxiao Long,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: OLATverse：9M真实物体图像、765个物体、35相机×331光源，可精确控制照明并含法线与反照率标注，为逆渲染与重光照研究提供大规模真实评价平台。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖合成数据训练和小规模真实数据测试，限制了现实感与泛化能力；因此需要大规模真实且光照可控的数据集以促进方法在真实场景中的发展。

Method: 使用35台DSLR相机从多视角拍摄765个真实物体，并用331个可控光源按多种照明配置逐一点亮；提供相机标定参数、物体掩码、光度法表面法线与漫反射反照率等辅助数据；构建全面的评测集用于逆渲染与法线估计基准。

Result: 数据集包含约9M张图像、765个物体、每物体35个相机视角与331个光源配置；同时提供校准与辅助标注，构建首个全面的真实物体逆渲染与法线估计基准。

Conclusion: OLATverse是一个大规模、高保真、受控光照的真实物体数据集，填补了合成训练与小规模真实评价之间的空白，能推动逆渲染、视图合成与重光照方法的现实世界泛化。

Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions. While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization. To address this gap, OLATverse
offers two key advantages over existing datasets: large-scale coverage of real
objects and high-fidelity appearance under precisely controlled illuminations.
Specifically, OLATverse contains 765 common and uncommon real-world objects,
spanning a wide range of material categories. Each object is captured using 35
DSLR cameras and 331 individually controlled light sources, enabling the
simulation of diverse illumination conditions. In addition, for each object, we
provide well-calibrated camera parameters, accurate object masks, photometric
surface normals, and diffuse albedo as auxiliary resources. We also construct
an extensive evaluation set, establishing the first comprehensive real-world
object-centric benchmark for inverse rendering and normal estimation. We
believe that OLATverse represents a pivotal step toward integrating the next
generation of inverse rendering and relighting methods with real-world data.
The full dataset, along with all post-processing workflows, will be publicly
released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.

</details>


### [39] [Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization](https://arxiv.org/abs/2511.02489)
*Tao Liu,Kan Ren,Qian Chen*

Main category: cs.CV

TL;DR: 利用目标检测+图神经网络做图匹配，针对UAV与卫星图像的跨视角与异构匹配问题，提升了检索与定位鲁棒性并具有良好泛化。


<details>
  <summary>Details</summary>
Motivation: 在GNSS受限环境下，传统基于卫星的定位失败且公开UAV定位数据集有限，现有通过极坐标变换、透视变换或GAN的跨域方法存在错位、内容丢失或逼真性限制。利用目标检测聚焦于稳定的语义实例并用图结构建模关系，可以减轻视角、模态与时序变化带来的匹配困难。

Method: 方法先用现代目标检测器在UAV与卫星图像中提取语义实例，构建图结构：节点表示检测到的实例，边表示图像内与图像间的关系；随后用图神经网络对图进行表征学习，计算细粒度节点相似度并用于图匹配与图像检索以估计查询图像位姿。

Result: 在公开与真实世界数据集上的大量实验表明，该方法在处理异构外观差异方面表现出色，检索与定位性能强，且具有良好的泛化能力，能扩展到可见光-红外等更大模态差异场景。作者并将数据集公开发布。

Conclusion: 本文提出了一种基于目标检测和图神经网络的无人机(UAV)跨视角定位框架，通过检测提取显著实例并构建图进行节点关系推理，从而在跨时间、跨视角和异构航片匹配任务中实现鲁棒的地图匹配和检索式定位。

Abstract: With the rapid growth of the low-altitude economy, UAVs have become crucial
for measurement and tracking in patrol systems. However, in GNSS-denied areas,
satellite-based localization methods are prone to failure. This paper presents
a cross-view UAV localization framework that performs map matching via object
detection, aimed at effectively addressing cross-temporal, cross-view,
heterogeneous aerial image matching. In typical pipelines, UAV visual
localization is formulated as an image-retrieval problem: features are
extracted to build a localization map, and the pose of a query image is
estimated by matching it to a reference database with known poses. Because
publicly available UAV localization datasets are limited, many approaches
recast localization as a classification task and rely on scene labels in these
datasets to ensure accuracy. Other methods seek to reduce cross-domain
differences using polar-coordinate reprojection, perspective transformations,
or generative adversarial networks; however, they can suffer from misalignment,
content loss, and limited realism. In contrast, we leverage modern object
detection to accurately extract salient instances from UAV and satellite
images, and integrate a graph neural network to reason about inter-image and
intra-image node relationships. Using a fine-grained, graph-based
node-similarity metric, our method achieves strong retrieval and localization
performance. Extensive experiments on public and real-world datasets show that
our approach handles heterogeneous appearance differences effectively and
generalizes well, making it applicable to scenarios with larger modality gaps,
such as infrared-visible image matching. Our dataset will be publicly available
at the following URL: https://github.com/liutao23/ODGNNLoc.git.

</details>


### [40] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: DetectiumFire是一个规模大、注释详尽的火灾多模态数据集（22.5k图像+2.5k视频），支持检测、生成与推理任务，可推动火灾智能安全系统研发。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在火灾场景应用受限，主要因缺乏公开、高质量、规模化的火灾数据集；需要覆盖多种火灾类型与现实场景以支持模型训练与评估。

Method: 构建并清洗22.5k高分辨率图像与2.5k真实视频，提供边界框等传统视觉标签与详细文本提示；通过任务基准（目标检测、扩散图像生成、视觉-语言推理）进行验证。

Result: 数据集在规模、样本多样性和质量上优于现有基准，能提升检测性能、支持扩散模型生成逼真火灾图像并增强视觉-语言推理能力；数据集已公开发布。

Conclusion: DetectiumFire填补了火灾领域高质量多模态数据集的空白，为检测、生成和推理任务提供了有力的数据支持。

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [41] [Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes](https://arxiv.org/abs/2511.02503)
*Robinson Umeike,Neil Getty,Yin Xiangyu,Yi Jiang*

Main category: cs.CV

TL;DR: 提出PtychoBench并系统比较SFT与ICL，发现最佳适配策略与任务模态相关：视觉任务上二者结合最佳，文本任务上ICL优于SFT。


<details>
  <summary>Details</summary>
Motivation: 推动高级显微镜工作流自动化，考察通用基础模型（LLMs、VLMs）在科学专用任务上的最佳领域适配策略，为科学领域智能代理系统提供方法指南。

Method: 构建PtychoBench基准（多模态多任务），在数据稀缺设置下对视觉伪影检测任务使用VLMs、对参数推荐任务使用LLMs，系统评估SFT与ICL两种特化策略，并与强基线（GPT-4o、DINOv3分类器等）对比；同时分析上下文提示与上下文干扰效应。

Result: 视觉任务：SFT与ICL互补，微调模型结合上下文示例达到Micro-F1=0.728；文本任务：在大型基础模型上ICL表现最好，峰值Micro-F1=0.847，超过强力SFT“超级专家”模型的0-shot Micro-F1=0.839；确认上下文提示优越性与微调中一致的上下文干扰现象。

Conclusion: 该论文提出了PtychoBench基准，并比较了SFT与ICL两种领域特化策略在多模态、高级显微成像任务中的表现，结论是最佳策略依任务而异：视觉任务上SFT与ICL互补，结合两者最佳；文本任务上ICL在大模型上优于SFT。论文还确认了上下文感知提示的有效性与微调模型中存在的一致性上下文干扰现象。

Abstract: The automation of workflows in advanced microscopy is a key goal where
foundation models like Language Models (LLMs) and Vision-Language Models (VLMs)
show great potential. However, adapting these general-purpose models for
specialized scientific tasks is critical, and the optimal domain adaptation
strategy is often unclear. To address this, we introduce PtychoBench, a new
multi-modal, multi-task benchmark for ptychographic analysis. Using this
benchmark, we systematically compare two specialization strategies: Supervised
Fine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies
on a visual artifact detection task with VLMs and a textual parameter
recommendation task with LLMs in a data-scarce regime. Our findings reveal that
the optimal specialization pathway is task-dependent. For the visual task, SFT
and ICL are highly complementary, with a fine-tuned model guided by
context-aware examples achieving the highest mean performance (Micro-F1 of
0.728). Conversely, for the textual task, ICL on a large base model is the
superior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a
powerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirm
the superiority of context-aware prompting and identify a consistent contextual
interference phenomenon in fine-tuned models. These results, benchmarked
against strong baselines including GPT-4o and a DINOv3-based classifier, offer
key observations for AI in science: the optimal specialization path in our
benchmark is dependent on the task modality, offering a clear framework for
developing more effective science-based agentic systems.

</details>


### [42] [ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing](https://arxiv.org/abs/2511.02505)
*Yaosen Chen,Wei Wang,Xuming Wen,Han Yang,Yanru Zhang*

Main category: cs.CV

TL;DR: 提出一种结合视觉语义匹配与能量模型的镜头组装方法，通过学习参考视频的属性与语法规则，自动生成符合风格与叙事需求的镜头序列。


<details>
  <summary>Details</summary>
Motivation: 现有智能视频编辑技术难以捕捉创作者独特的艺术风格，自动化镜头组装需要兼顾叙事逻辑与艺术表达，因此提出能量基优化以学习参考风格并生成符合语义与风格的镜头序列。

Method: 方法包括：1) 使用大语言模型生成剧本并与视频库进行视觉-语义匹配，得到候选镜头子集；2) 对参考视频进行镜头分割与标注，提取镜头尺寸、相机运动和语义等属性；3) 基于能量模型对这些属性建模，为候选镜头序列打分；4) 结合多条语法规则进行能量最小化以实现镜头组装优化。

Result: 方法能自动按照特定逻辑、叙事需求或艺术风格排列与组合镜头，使无编辑经验的用户也能创作出视觉上引人注目的视频。

Conclusion: 本论文提出了一种基于能量模型的镜头组装优化方法，能够在保留创作者艺术表达的同时自动化镜头的排序与组合，学习参考视频的组装风格并生成连贯的视频序列。

Abstract: Shot assembly is a crucial step in film production and video editing,
involving the sequencing and arrangement of shots to construct a narrative,
convey information, or evoke emotions. Traditionally, this process has been
manually executed by experienced editors. While current intelligent video
editing technologies can handle some automated video editing tasks, they often
fail to capture the creator's unique artistic expression in shot assembly.To
address this challenge, we propose an energy-based optimization method for
video shot assembly. Specifically, we first perform visual-semantic matching
between the script generated by a large language model and a video library to
obtain subsets of candidate shots aligned with the script semantics. Next, we
segment and label the shots from reference videos, extracting attributes such
as shot size, camera motion, and semantics. We then employ energy-based models
to learn from these attributes, scoring candidate shot sequences based on their
alignment with reference styles. Finally, we achieve shot assembly optimization
by combining multiple syntax rules, producing videos that align with the
assembly style of the reference videos. Our method not only automates the
arrangement and combination of independent shots according to specific logic,
narrative requirements, or artistic styles but also learns the assembly style
of reference videos, creating a coherent visual sequence or holistic visual
expression. With our system, even users with no prior video editing experience
can create visually compelling videos. Project page:
https://sobeymil.github.io/esa.com

</details>


### [43] [Keeping it Local, Tiny and Real: Automated Report Generation on Edge Computing Devices for Mechatronic-Based Cognitive Systems](https://arxiv.org/abs/2511.02507)
*Nicolas Schuler,Lea Dewald,Jürgen Graf*

Main category: cs.CV

TL;DR: 提出一个仅基于本地模型、支持边缘部署的多模态自动报告流水线，适用于移动机器人多场景评估，兼顾隐私与实用性，并在多域数据集上验证。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习推动认知机器人在动态非结构化环境中的应用，关键任务（如自动驾驶、服务与护理机器人）需要对大量异构感知数据进行高效评估；自动化报告能帮助评估与接受，但又需保护参与者隐私并支持边缘部署，所以提出本地化的多模态报告生成方法。

Method: 设计并实现一个利用多模态传感器（视觉、可能的深度/IMU等）的端到端流水线：感知模块提取特征、本地多模态融合模型生成自然语言描述，并在边缘设备上推理以避免外部服务；在多场景数据集（室内、室外、城市）上进行定量和定性评估。

Result: 在覆盖室内、室外与城市场景的多域数据集上，系统实现了可用的自动化自然语言报告，给出定量指标（未在摘要列出具体数值）与定性示例；并将示例报告与补充材料在公共仓库发布。

Conclusion: 该论文提出了一个基于本地模型的多模态自动化报告生成流水线，旨在移动机器人领域实现隐私保护和边缘部署，提升对异构大量数据的评估效率与可接受性。

Abstract: Recent advancements in Deep Learning enable hardware-based cognitive systems,
that is, mechatronic systems in general and robotics in particular with
integrated Artificial Intelligence, to interact with dynamic and unstructured
environments. While the results are impressive, the application of such systems
to critical tasks like autonomous driving as well as service and care robotics
necessitate the evaluation of large amount of heterogeneous data. Automated
report generation for Mobile Robotics can play a crucial role in facilitating
the evaluation and acceptance of such systems in various domains. In this
paper, we propose a pipeline for generating automated reports in natural
language utilizing various multi-modal sensors that solely relies on local
models capable of being deployed on edge computing devices, thus preserving the
privacy of all actors involved and eliminating the need for external services.
In particular, we evaluate our implementation on a diverse dataset spanning
multiple domains including indoor, outdoor and urban environments, providing
quantitative as well as qualitative evaluation results. Various generated
example reports and other supplementary materials are available via a public
repository.

</details>


### [44] [LiteVoxel: Low-memory Intelligent Thresholding for Efficient Voxel Rasterization](https://arxiv.org/abs/2511.02510)
*Jee Won Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: LiteVoxel 是一个自适应的稀疏体素光栅化训练管线，通过低频感知损失、分位数剪枝与受控优先细分，提升稳定性并降低峰值显存40%~60%，同时保留视觉质量。


<details>
  <summary>Details</summary>
Motivation: 动机是解决稀疏体素光栅化在重建中常见的问题：低频信息欠拟合、依赖脆弱的剪枝启发式规则以及体素过度增长导致的 GPU 内存占用升高，进而影响训练可预测性和效率。

Method: 方法包括三大改进：1) 通过逆 Sobel 重加权结合中期 gamma 斜坡，使损失对低频（平坦区域）更敏感，且仅在几何稳定后将梯度预算转移到这些区域；2) 用基于深度分位数的最大混合权重剪枝取代固定阈值，辅以 EMA-滞后守卫稳定；3) 在显式增长预算下，采用基于射线投影足迹和优先级驱动的细分以精炼结构并限制体素增长。

Result: 在 Mip-NeRF 360（6个场景）和 Tanks & Temples（3个场景）数据集上的消融与整体系统实验表明，LiteVoxel 在保持 PSNR/SSIM、训练时间和帧率与强基线相当的同时，显著减少低频区域与边界不稳定错误，将峰值显存降低约40%~60%，并保留先前方法遗漏的低频细节。

Conclusion: LiteVoxel 提供了一种自动调节的稀疏体素光栅化训练流程，有效缓解低频内容欠拟合、脆弱剪枝阈值与内存暴增问题，从而实现更稳定且更省内存的场景重建。

Abstract: Sparse-voxel rasterization is a fast, differentiable alternative for
optimization-based scene reconstruction, but it tends to underfit low-frequency
content, depends on brittle pruning heuristics, and can overgrow in ways that
inflate VRAM. We introduce LiteVoxel, a self-tuning training pipeline that
makes SV rasterization both steadier and lighter. Our loss is made
low-frequency aware via an inverse-Sobel reweighting with a mid-training
gamma-ramp, shifting gradient budget to flat regions only after geometry
stabilize. Adaptation replaces fixed thresholds with a depth-quantile pruning
logic on maximum blending weight, stabilized by EMA-hysteresis guards and
refines structure through ray-footprint-based, priority-driven subdivision
under an explicit growth budget. Ablations and full-system results across
Mip-NeRF 360 (6scenes) and Tanks & Temples (3scenes) datasets show mitigation
of errors in low-frequency regions and boundary instability while keeping
PSNR/SSIM, training time, and FPS comparable to a strong SVRaster pipeline.
Crucially, LiteVoxel reduces peak VRAM by ~40%-60% and preserves low-frequency
detail that prior setups miss, enabling more predictable, memory-efficient
training without sacrificing perceptual quality.

</details>


### [45] [Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data](https://arxiv.org/abs/2511.02541)
*Jessica Plassmann,Nicolas Schuler,Georg von Freymann,Michael Schuth*

Main category: cs.CV

TL;DR: 在仅用完好样本训练的条件下，学生-教师特征匹配优于自编码器，实现鲁棒的无监督剪切干涉缺陷检测与精确定位，具有工业应用潜力。


<details>
  <summary>Details</summary>
Motivation: 减轻对标注数据和专家人工判读的依赖，使剪切干涉检测可在工业场景下可扩展和高效。

Method: 构建三个无监督模型：全连接自编码器、卷积自编码器、学生-教师特征匹配，均仅用完好样本训练；设计可控试验件生成可复现缺陷并在理想与带全局变形条件下采集剪切干涉图；比较两类训练集（仅未变形与包含全局变形）。评价包括二分类性能与学生-教师模型的空间定位，并用基于标签的YOLOv8作为定位参考。

Result: 学生-教师模型在分类和定位上均优于自编码器，特征表示在t-SNE中更易分离；包含全局变形的训练集提高了模型在实际变形场景下的稳健性；YOLOv8作为上限参考验证定位质量。

Conclusion: 学生-教师特征匹配模型在无监督缺陷检测任务中表现最佳，既能实现稳健的二分类，又能精确定位缺陷。

Abstract: Shearography is a non-destructive testing method for detecting subsurface
defects, offering high sensitivity and full-field inspection capabilities.
However, its industrial adoption remains limited due to the need for expert
interpretation. To reduce reliance on labeled data and manual evaluation, this
study explores unsupervised learning methods for automated anomaly detection in
shearographic images. Three architectures are evaluated: a fully connected
autoencoder, a convolutional autoencoder, and a student-teacher feature
matching model. All models are trained solely on defect-free data. A controlled
dataset was developed using a custom specimen with reproducible defect
patterns, enabling systematic acquisition of shearographic measurements under
both ideal and realistic deformation conditions. Two training subsets were
defined: one containing only undistorted, defect-free samples, and one
additionally including globally deformed, yet defect-free, data. The latter
simulates practical inspection conditions by incorporating deformation-induced
fringe patterns that may obscure localized anomalies. The models are evaluated
in terms of binary classification and, for the student-teacher model, spatial
defect localization. Results show that the student-teacher approach achieves
superior classification robustness and enables precise localization. Compared
to the autoencoder-based models, it demonstrates improved separability of
feature representations, as visualized through t-SNE embeddings. Additionally,
a YOLOv8 model trained on labeled defect data serves as a reference to
benchmark localization quality. This study underscores the potential of
unsupervised deep learning for scalable, label-efficient shearographic
inspection in industrial environments.

</details>


### [46] [Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction](https://arxiv.org/abs/2511.02558)
*Ali Farki,Elaheh Moradi,Deepika Koundal,Jussi Tohka*

Main category: cs.CV

TL;DR: 作者通过比较五种深度网络，在ADNI和AIBL上实现基线MRI到多年后随访MRI的高保真体素级预测，模型泛化至独立数据集，展示了个体化脑影像预后的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统研究多预测认知评分或临床结局，而不是直接预测随访影像。作者旨在通过图像到图像的纵向预测来建模复杂的空间分布型神经退行性变化，从而实现更细粒度的个体化预后。

Method: 在ADNI和AIBL两个纵向队列上实现并比较了五种深度学习架构（UNet、U2-Net、UNETR、Time-Embedding UNet、ODE-UNet），直接以体素级别预测随访MRI并用全局与局部相似性指标评估预测与真实随访的差异。

Result: 最佳模型实现高保真预测，所有模型在独立外部数据集上均表现良好，说明跨队列稳健性，并能在体素级提供个体化预测。

Conclusion: 论文表明深度学习模型能够基于基线MRI预测数年后的个体脑部MRI，且在不同队列间具有良好泛化性。

Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI)
is a central challenge in neuroimaging and has important implications for
studying neurodegenerative diseases such as Alzheimer's disease (AD). Most
existing approaches predict future cognitive scores or clinical outcomes, such
as conversion from mild cognitive impairment to dementia. Instead, here we
investigate longitudinal MRI image-to-image prediction that forecasts a
participant's entire brain MRI several years into the future, intrinsically
modeling complex, spatially distributed neurodegenerative patterns. We
implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR,
Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL).
Predicted follow-up MRIs are directly compared with the actual follow-up scans
using metrics that capture global similarity and local differences. The best
performing models achieve high-fidelity predictions, and all models generalize
well to an independent external dataset, demonstrating robust cross-cohort
performance. Our results indicate that deep learning can reliably predict
participant-specific brain MRI at the voxel level, offering new opportunities
for individualized prognosis.

</details>


### [47] [The Urban Vision Hackathon Dataset and Models: Towards Image Annotations and Accurate Vision Models for Indian Traffic](https://arxiv.org/abs/2511.02563)
*Akash Sharma,Chinmay Mhatre,Sankalp Gawali,Ruthvik Bokkasam,Brij Kishore,Vishwajeet Pattanaik,Tarun Rambha,Abdul R. Pinjari,Vijay Kovvali,Anirban Chakraborty,Punit Rathore,Raghu Krishnapuram,Yogesh Simmhan*

Main category: cs.CV

TL;DR: UVH-26：26k 1080p 印度城市路侧摄像头图像，1.8M框，14类，本地化共识注释与微调的YOLO/DETR模型，显著优于COCO预训练模型，填补印度交通场景数据空白。


<details>
  <summary>Details</summary>
Motivation: 现有公开基准（如COCO）缺乏代表印度复杂交通场景（如大量三轮车、二轮车、重叠与拥堵场景等）的数据，导致检测器在该域表现受限；因此需要大规模、真实道路摄像头采集并标注的数据集以提升模型在新兴国家交通环境中的适用性。

Method: 从班加罗尔2800台监控摄像头的4周视频中采样出26,646张1080p图像，通过一次众包黑客松（565名大学生）进行多类别边界框标注（共1.8M框）。使用多数投票和STAPLE两种方法融合得到约28.3万-31.6万个共识目标框，随后在这些注释上微调并训练多种主流检测器（YOLO系列、RT-DETR、DAMO-YOLO），并以mAP50、mAP75、mAP50:95评估。

Result: 发布了UVH-26数据集（含UVH-26-MV与UVH-26-ST两套共识注释）及6个微调模型权重。与在COCO上训练的等效模型相比，在常见类别（Car/Bus/Truck）上mAP50:95提升8.4–31.5%，RT-DETR-X在UVH-26上达到0.67的mAP50:95，而COCO权重为0.40，表明领域数据能显著提高检测性能。

Conclusion: 本论文首次公开发布了面向印度交通场景的大规模标注摄像头图像数据集UVH-26，并展示了该数据集在提升基于检测器模型（如YOLO、DETR等）性能上的显著效果，证明了领域特定数据对复杂道路环境的重要性。

Abstract: This report describes the UVH-26 dataset, the first public release by
AIM@IISc of a large-scale dataset of annotated traffic-camera images from
India. The dataset comprises 26,646 high-resolution (1080p) images sampled from
2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently
annotated through a crowdsourced hackathon involving 565 college students from
across India. In total, 1.8 million bounding boxes were labeled across 14
vehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler
(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,
Hatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k
consensus ground truth bounding boxes and labels were derived for distinct
objects in the 26k images using Majority Voting and STAPLE algorithms. Further,
we train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,
and DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,
mAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in
mAP50:95 over equivalent baseline models trained on COCO dataset, with
RT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40
for COCO-trained weights for common classes (Car, Bus, and Truck). This
demonstrates the benefits of domain-specific training data for Indian traffic
scenarios. The release package provides the 26k images with consensus
annotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the
6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the
heterogeneity of Indian urban mobility directly from operational traffic-camera
streams, UVH-26 addresses a critical gap in existing global benchmarks, and
offers a foundation for advancing detection, classification, and deployment of
intelligent transportation systems in emerging nations with complex traffic
conditions.

</details>


### [48] [Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification](https://arxiv.org/abs/2511.02564)
*Md Rashidunnabi,Kailash A. Hambarde,Vasco Lopes,Joao C. Neves,Hugo Proenca*

Main category: cs.CV

TL;DR: 作者在ViT基础上以七个轻量adapter模块构建MTF-CVReID，实现低计算开销下显著提升跨视角视频ReID性能和时间一致性，适用于空地等极端视角场景。


<details>
  <summary>Details</summary>
Motivation: 针对空地（aerial-ground）等极端视角转换导致的视角偏移、尺度差异和视频帧间时序不一致，现有方法难以同时兼顾性能与实时性，因此设计轻量模块以增强跨视角和时序鲁棒性。

Method: 在ViT-B/16骨干上引入七个补充模块：Cross-Stream Feature Normalization（纠正相机/视角偏差）、Multi-Resolution Feature Harmonization（尺度稳定）、Identity-Aware Memory Module（强化身份特征）、Temporal Dynamics Modeling（短期时序编码）、Inter-View Feature Alignment（视角不变对齐）、Hierarchical Temporal Pattern Learning（多尺度时序特征）、Multi-View Identity Consistency Learning（对比学习的跨视角身份一致性约束）。整体以adapter式模块为主，仅增加约2M参数和0.7 GFLOPs。

Result: 在AG-VPReID基准上所有高度水平均实现最先进性能，并在G2A-VReID和MARS上展现出良好的跨数据集泛化能力；同时保持约189 FPS的实时效率。

Conclusion: 该论文提出了一种轻量级的、多模块适配器框架（MTF-CVReID），针对视频跨视角行人重识别中的视角差异、尺度变化和时序不一致问题，显著提升了跨视角鲁棒性和时间一致性，且计算代价小，实时性好。

Abstract: Video-based person re-identification (ReID) in cross-view domains (for
example, aerial-ground surveillance) remains an open problem because of extreme
viewpoint shifts, scale disparities, and temporal inconsistencies. To address
these challenges, we propose MTF-CVReID, a parameter-efficient framework that
introduces seven complementary modules over a ViT-B/16 backbone. Specifically,
we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera and
view biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scale
stabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) to
reinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) for
motion-aware short-term temporal encoding; (5) Inter-View Feature Alignment
(IVFA) for perspective-invariant representation alignment; (6) Hierarchical
Temporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;
and (7) Multi-View Identity Consistency Learning (MVICL) that enforces
cross-view identity coherence using a contrastive learning paradigm. Despite
adding only about 2 million parameters and 0.7 GFLOPs over the baseline,
MTF-CVReID maintains real-time efficiency (189 FPS) and achieves
state-of-the-art performance on the AG-VPReID benchmark across all altitude
levels, with strong cross-dataset generalization to G2A-VReID and MARS
datasets. These results show that carefully designed adapter-based modules can
substantially enhance cross-view robustness and temporal consistency without
compromising computational efficiency. The source code is available at
https://github.com/MdRashidunnabi/MTF-CVReID

</details>


### [49] [A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding](https://arxiv.org/abs/2511.02565)
*Jingyu Lu,Haonan Wang,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: VCFlow通过模拟人类视觉的腹背流分工并用特征级对比学习，做到快速且无需个体重训练的跨被试视觉重建，兼顾速度与准确性，适合临床推广。


<details>
  <summary>Details</summary>
Motivation: 解决现有脑解码依赖大量被试特定训练数据且计算耗时的局限，推进可临床应用的通用（subject-agnostic）视觉重建方法。

Method: 提出Visual Cortex Flow Architecture（VCFlow），构建对应早期视觉皮层、腹侧流和背侧流的分支来解耦并融合多维表征；在特征层面引入对比学习以增强主体不变的语义表征；优化推理流程以在不重训练的情况下实现每段重建仅用10秒。

Result: 在牺牲平均仅7%精度的前提下，VCFlow无需对新被试重训练就能以10秒/段的速度生成重建视频；展示了跨被试泛化能力提升并保持竞争性重建质量。

Conclusion: VCFlow通过仿生性的层级解码结构和特征级对比学习，有效提升了跨被试视觉重建的泛化性，实现了快速、无需个体重训练的视频重建。

Abstract: Subject-agnostic brain decoding, which aims to reconstruct continuous visual
experiences from fMRI without subject-specific training, holds great potential
for clinical applications. However, this direction remains underexplored due to
challenges in cross-subject generalization and the complex nature of brain
signals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), a
novel hierarchical decoding framework that explicitly models the ventral-dorsal
architecture of the human visual system to learn multi-dimensional
representations. By disentangling and leveraging features from early visual
cortex, ventral, and dorsal streams, VCFlow captures diverse and complementary
cognitive information essential for visual reconstruction. Furthermore, we
introduce a feature-level contrastive learning strategy to enhance the
extraction of subject-invariant semantic representations, thereby enhancing
subject-agnostic applicability to previously unseen subjects. Unlike
conventional pipelines that need more than 12 hours of per-subject data and
heavy computation, VCFlow sacrifices only 7\% accuracy on average yet generates
each reconstructed video in 10 seconds without any retraining, offering a fast
and clinically scalable solution. The source code will be released upon
acceptance of the paper.

</details>


### [50] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai,Ryugo Morita,Shunsuke Kitada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出TAUE和NTC方法，实现零-shot、无训练的分层图像生成，通过中间噪声移植保证跨层一致性，性能可比微调方法，适用于可控合成编辑。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型输出单一扁平图像，缺乏分层可控性；现有解决方案要么需要大规模微调数据，要么只生成孤立前景，无法产出完整一致的多层场景。

Method: 核心方法为Noise Transplantation and Cultivation (NTC)：从前景和复合生成流程中提取中间潜在表示，并将其移植到后续层的初始噪声中，以保持跨层一致性，免去微调或额外数据集。

Result: 实验表明，在无需训练的情况下，TAUE在层级一致性方面与微调方法性能相当，同时保持高图像质量与保真度，并扩展到复杂合成编辑等下游应用。

Conclusion: 该论文提出了一种无需训练的分层图像生成框架TAUE，通过在不同生成过程中提取和移植中间噪声表示，实现前景、背景与合成层之间的语义与结构一致性。

Abstract: Despite the remarkable success of text-to-image diffusion models, their
output of a single, flattened image remains a critical bottleneck for
professional applications requiring layer-wise control. Existing solutions
either rely on fine-tuning with large, inaccessible datasets or are
training-free yet limited to generating isolated foreground elements, failing
to produce a complete and coherent scene. To address this, we introduce the
Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a
novel framework for zero-shot, layer-wise image generation. Our core technique,
Noise Transplantation and Cultivation (NTC), extracts intermediate latent
representations from both foreground and composite generation processes,
transplanting them into the initial noise for subsequent layers. This ensures
semantic and structural coherence across foreground, background, and composite
layers, enabling consistent, multi-layered outputs without requiring
fine-tuning or auxiliary datasets. Extensive experiments show that our
training-free method achieves performance comparable to fine-tuned methods,
enhancing layer-wise consistency while maintaining high image quality and
fidelity. TAUE not only eliminates costly training and dataset requirements but
also unlocks novel downstream applications, such as complex compositional
editing, paving the way for more accessible and controllable generative
workflows.

</details>


### [51] [Zero-Shot Multi-Animal Tracking in the Wild](https://arxiv.org/abs/2511.02591)
*Jan Frederik Meier,Timo Lüddecke*

Main category: cs.CV

TL;DR: 利用Grounding DINO与SAM2并辅以启发式策略，作者实现了一个零样本多动物追踪系统，在多数据集上表现良好，无需重训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法在不同栖息地、运动模式和物种外观上表现不稳，需要大量微调和场景特定设计；希望利用视觉基础模型的通用性实现无需重训的通用多动物追踪。

Method: 结合Grounding DINO检测器和SAM2分割/跟踪器，辅以精心设计的启发式规则来完成目标生成、关联与轨迹维护，实现零样本跨域追踪。

Result: 在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上获得了强劲且一致的性能，证明方法在多物种与多环境下的泛化能力。

Conclusion: 本文提出基于视觉基础模型的零样本多动物追踪框架，展示了在多个数据集上的稳健性，表明无需针对性重训练或超参调整即可实现有效追踪。

Abstract: Multi-animal tracking is crucial for understanding animal ecology and
behavior. However, it remains a challenging task due to variations in habitat,
motion patterns, and species appearance. Traditional approaches typically
require extensive model fine-tuning and heuristic design for each application
scenario. In this work, we explore the potential of recent vision foundation
models for zero-shot multi-animal tracking. By combining a Grounding Dino
object detector with the Segment Anything Model 2 (SAM 2) tracker and carefully
designed heuristics, we develop a tracking framework that can be applied to new
datasets without any retraining or hyperparameter adaptation. Evaluations on
ChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstrate
strong and consistent performance across diverse species and environments. The
code is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.

</details>


### [52] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: 提出UniChange，一种基于MLLM的统一变化检测方法，利用特殊token与文本提示实现BCD与SCD统一，能从多源数据学习并在四个基准上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前变化检测模型受限于单一类型标注数据，难以同时利用BCD与SCD数据，导致泛化性差。MLLM的语言先验和统一能力为解决该问题提供了可能。

Method: 将图像编码与生成式语言模型融合；引入[T1]、[T2]、[CHANGE]三个特殊token以区分时相与变化实体；通过文本prompt引导类别识别，无需固定分类头，从而实现跨数据集多任务学习。

Result: 在WHU-CD、S2Looking、LEVIR-CD+、SECOND四个基准上分别取得IoU 90.41、53.04、78.87、57.62，超越此前所有方法，验证了模型的通用性与性能优势。

Conclusion: 本文提出了基于多模态大模型（MLLM）的统一变化检测框架UniChange，通过引入特殊标记和文本提示实现了二元变化检测（BCD）与语义变化检测（SCD）的统一，能从多源、类别定义冲突的数据中学习，提升泛化能力，实验在四个基准上取得SOTA性能。

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [53] [Robust Face Liveness Detection for Biometric Authentication using Single Image](https://arxiv.org/abs/2511.02645)
*Poulami Raha,Yeongnam Chae*

Main category: cs.CV

TL;DR: 提出一个轻量级CNN用于快速活体检测并发布了500+视频的2D欺骗数据集，能检测打印/显示、视频和包裹攻击，CPU上1-2秒完成识别。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别系统易受展示攻击（spoofing）影响，需要实时、准确且轻量的活体检测方法以保障安全系统的可靠性。

Method: 设计并训练轻量级CNN架构，收集并使用新建的2D欺骗攻击数据集（500+视频，60名被试）进行验证；在论文中通过演示视频展示了检测方法的效果。

Result: 提出的模型能在CPU上快速运行并有效识别打印/显示、视频和包裹式攻击；并提供了一个新的二维欺骗攻击视频数据集和演示视频作为辅助验证。

Conclusion: 该论文提出了一个轻量级的卷积神经网络框架，用于检测针对人脸识别的展示攻击（打印/显示、视频和包裹攻击），并实现了快速的活体检测（在CPU上1-2秒）。

Abstract: Biometric technologies are widely adopted in security, legal, and financial
systems. Face recognition can authenticate a person based on the unique facial
features such as shape and texture. However, recent works have demonstrated the
vulnerability of Face Recognition Systems (FRS) towards presentation attacks.
Using spoofing (aka.,presentation attacks), a malicious actor can get
illegitimate access to secure systems. This paper proposes a novel light-weight
CNN framework to identify print/display, video and wrap attacks. The proposed
robust architecture provides seamless liveness detection ensuring faster
biometric authentication (1-2 seconds on CPU). Further, this also presents a
newly created 2D spoof attack dataset consisting of more than 500 videos
collected from 60 subjects. To validate the effectiveness of this architecture,
we provide a demonstration video depicting print/display, video and wrap attack
detection approaches. The demo can be viewed in the following link:
https://rak.box.com/s/m1uf31fn5amtjp4mkgf1huh4ykfeibaa

</details>


### [54] [Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models](https://arxiv.org/abs/2511.02650)
*Tianfan Peng,Yuntao Du,Pengzhou Ji,Shijie Dong,Kailin Jiang,Mingchuan Ma,Yijun Tian,Jinhe Bi,Qian Li,Wei Du,Feng Xiao,Lizhen Cui*

Main category: cs.CV

TL;DR: 提出UniPruneBench统一评估视觉token剪枝，覆盖多数据集、多模型和系统级指标，发现随机剪枝强、无方法全能、OCR最脆弱、剪枝率主导性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前视觉token压缩方法评估分散且不一致，需要一个统一、可扩展的基准来全面比较不同剪枝方法在多模态LLM上的表现。

Method: 构建统一评测框架，覆盖六个能力维度、十个数据集、十种压缩算法和三大家族的LMMs；采用标准化协议，并纳入运行时和预填充延迟等系统级指标。

Result: 实验表明：随机剪枝表现出人意料的强劲基线；没有单一方法在所有场景中都占优；不同任务对剪枝敏感度差异显著，OCR最脆弱；剪枝比例是主导性能下降的关键因素。

Conclusion: 本文提出了UniPruneBench，用于统一评估视觉token剪枝在多模态大模型中的效率与性能折衷，认为该基准能成为未来研究的可靠基础。

Abstract: Large multimodal models (LMMs) often suffer from severe inference
inefficiency due to the large number of visual tokens introduced by image
encoders. While recent token compression methods, such as pruning and merging,
have shown promise in reducing redundancy, their evaluation remains fragmented
and inconsistent. In this work, we present UniPruneBench, a unified and
extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench
provides standardized protocols across six ability dimensions and ten datasets,
covering ten representative compression algorithms and three families of LMMs
(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates
system-level metrics such as runtime and prefilling latency to provide a
holistic view. Our experiments uncover several key findings: (1) random pruning
is a surprisingly strong baseline, (2) no single method consistently
outperforms others across scenarios, (3) pruning sensitivity varies
significantly across tasks, with OCR being most vulnerable, and (4) pruning
ratio is the dominant factor governing performance degradation. We believe
UniPruneBench will serve as a reliable foundation for future research on
efficient multimodal modeling.

</details>


### [55] [Differentiable Hierarchical Visual Tokenization](https://arxiv.org/abs/2511.02652)
*Marius Aasan,Martine Hjelkrem-Tan,Nico Catalano,Changkyu Choi,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出一种与现有Transformer兼容的端到端可微分、自适应的像素级tokenizer，通过层次化信息准则选择实现对分类和密集预测的性能提升，并支持栅格到矢量的转换。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformer使用固定的patch划分，忽视图像的空间和语义结构，导致信息浪费和下游任务性能受限，故提出自适应分词器来提高表达能力和任务适应性。

Method: 提出层次化模型选择机制，使用信息准则（如AIC/BIC类）在不同分辨率/分割方案间选择最优表示，并以端到端可微方式学习分词参数。

Result: 在图像分类和密集预测任务上均取得竞争性性能，支持从栅格到矢量的直接转换（raster-to-vector），并可用于微调预训练模型以提高性能。

Conclusion: 该论文提出了一个可微分的动态分词器（tokenizer），能够根据图像内容进行像素级自适应划分，同时与现有Transformer架构和预训练模型兼容。

Abstract: Vision Transformers rely on fixed patch tokens that ignore the spatial and
semantic structure of images. In this work, we introduce an end-to-end
differentiable tokenizer that adapts to image content with pixel-level
granularity while remaining backward-compatible with existing architectures for
retrofitting pretrained models. Our method uses hierarchical model selection
with information criteria to provide competitive performance in both
image-level classification and dense-prediction tasks, and even supports
out-of-the-box raster-to-vector conversion.

</details>


### [56] [Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2511.02685)
*Chao Yuan,Zanwu Liu,Guiwei Zhang,Haoxuan Xu,Yujian Zhao,Guanglin Niu,Bo Li*

Main category: cs.CV

TL;DR: 提出无需额外推理参数的模态过渡表示学习（MTRL），通过生成中间图像并配合对比和正则化损失，有效桥接可见光与红外差距，提升VI-ReID性能并超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 可见光与红外模态间存在本质差距，现有方法依赖中间表示（生成图像或特征融合）但对中间表示利用不足或引入复杂性与额外参数，需一种高效且无额外推理负担的对齐方法。

Method: 使用中间生成图像作为模态过渡表示，结合模态过渡对比损失和模态查询正则化损失训练；不增加推理参数，保持与主干网络相同的推理速度。

Result: 在三个典型VI-ReID数据集上，方法显著且稳定地优于现有SOTA，且不增加推理时间。

Conclusion: 本文提出的MTRL通过生成中间图像作为可见光到红外的过渡，能更有效地对齐跨模态特征，从而提升VI-ReID性能。

Abstract: Visible-infrared person re-identification (VI-ReID) technique could associate
the pedestrian images across visible and infrared modalities in the practical
scenarios of background illumination changes. However, a substantial gap
inherently exists between these two modalities. Besides, existing methods
primarily rely on intermediate representations to align cross-modal features of
the same person. The intermediate feature representations are usually create by
generating intermediate images (kind of data enhancement), or fusing
intermediate features (more parameters, lack of interpretability), and they do
not make good use of the intermediate features. Thus, we propose a novel
VI-ReID framework via Modality-Transition Representation Learning (MTRL) with a
middle generated image as a transmitter from visible to infrared modals, which
are fully aligned with the original visible images and similar to the infrared
modality. After that, using a modality-transition contrastive loss and a
modality-query regularization loss for training, which could align the
cross-modal features more effectively. Notably, our proposed framework does not
need any additional parameters, which achieves the same inference speed to the
backbone while improving its performance on VI-ReID task. Extensive
experimental results illustrate that our model significantly and consistently
outperforms existing SOTAs on three typical VI-ReID datasets.

</details>


### [57] [VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models](https://arxiv.org/abs/2511.02712)
*Zhicheng Zhang,Weicheng Wang,Yongjie Zhu,Wenyu Qin,Pengfei Wan,Di Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 提出VidEmo与分阶段情感推理框架并构建2.1M样本Emo-CFG数据集，使用课程化学习与情感树强化学习提升视频情绪理解，在15个任务上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 情感具有动态性和线索依赖性，现有VideoLLMs难以对复杂演化的情绪状态进行合理解释与推理，因此需要统一的分阶段框架和专门的模型与数据来提升视频情绪理解与推理能力。

Method: 引入VidEmo视频情感基础模型，采用两阶段微调：1) 课程化情感学习注入情感知识；2) 基于情感树的强化学习用于情感推理；同时构建2.1M样本的Emo-CFG数据集用于训练和评估。

Result: 在包含15个人脸感知任务的评估中，所提方法在各项任务上表现具有竞争力并达到新的里程碑，展示了其在情感理解与推理方面的有效性。

Conclusion: 该论文提出了一种分阶段的情感推理框架，结合基础属性感知、表达分析和高层情绪理解，通过情感知识注入和基于情感树的强化学习提升情绪推理能力，并建立大规模情感指令数据集Emo-CFG，实验证明在人脸感知相关任务上取得了优异表现。

Abstract: Understanding and predicting emotion from videos has gathered significant
attention in recent studies, driven by advancements in video large language
models (VideoLLMs). While advanced methods have made progress in video emotion
analysis, the intrinsic nature of emotions poses significant challenges.
Emotions are characterized by dynamic and cues-dependent properties, making it
difficult to understand complex and evolving emotional states with reasonable
rationale. To tackle these challenges, we propose a novel affective cues-guided
reasoning framework that unifies fundamental attribute perception, expression
analysis, and high-level emotional understanding in a stage-wise manner. At the
core of our approach is a family of video emotion foundation models (VidEmo),
specifically designed for emotion reasoning and instruction-following. These
models undergo a two-stage tuning process: first, curriculum emotion learning
for injecting emotion knowledge, followed by affective-tree reinforcement
learning for emotion reasoning. Moreover, we establish a foundational data
infrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)
consisting of 2.1M diverse instruction-based samples. Emo-CFG includes
explainable emotional question-answering, fine-grained captions, and associated
rationales, providing essential resources for advancing emotion understanding
tasks. Experimental results demonstrate that our approach achieves competitive
performance, setting a new milestone across 15 face perception tasks.

</details>


### [58] [LLEXICORP: End-user Explainability of Convolutional Neural Networks](https://arxiv.org/abs/2511.02720)
*Vojtěch Kůr,Adam Bajger,Adam Kukučka,Marek Hradil,Vít Musil,Tomáš Brázdil*

Main category: cs.CV

TL;DR: LLEXICORP 用大语言模型自动化概念命名与解释，将 CRP 的定量相关性转为面向不同受众的直观叙述，提升了 CNN 可解释性的可访问性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前基于概念的归因工作流程高度依赖人工——专家需观察激活图为概念命名并从相关性图合成冗长解释，限制了可访问性与扩展性。

Method: 构建模块化流水线：先用 CRP 在顶层通道中识别概念原型并回溯相关性，生成激活图与概率分布；然后用精心设计的提示（包括示例教学和任务分离）将这些多模态输入提交给大语言模型，分别进行命名与解释；最后根据目标受众调整输出风格。

Result: 在 ImageNet 上使用 VGG16 的定性评估表明，该方法能生成可信且可读的概念命名与解释，降低了理解深度神经网络的门槛。

Conclusion: LLEXICORP 将概念相关性传播与多模态大语言模型结合，自动为概念原型命名并生成可定制的自然语言解释，提高了 CNN 可解释性的可访问性和可扩展性。

Abstract: Convolutional neural networks (CNNs) underpin many modern computer vision
systems. With applications ranging from common to critical areas, a need to
explain and understand the model and its decisions (XAI) emerged. Prior works
suggest that in the top layers of CNNs, the individual channels can be
attributed to classifying human-understandable concepts. Concept relevance
propagation (CRP) methods can backtrack predictions to these channels and find
images that most activate these channels. However, current CRP workflows are
largely manual: experts must inspect activation images to name the discovered
concepts and must synthesize verbose explanations from relevance maps, limiting
the accessibility of the explanations and their scalability.
  To address these issues, we introduce Large Language model EXplaIns COncept
Relevance Propagation (LLEXICORP), a modular pipeline that couples CRP with a
multimodal large language model. Our approach automatically assigns descriptive
names to concept prototypes and generates natural-language explanations that
translate quantitative relevance distributions into intuitive narratives. To
ensure faithfulness, we craft prompts that teach the language model the
semantics of CRP through examples and enforce a separation between naming and
explanation tasks. The resulting text can be tailored to different audiences,
offering low-level technical descriptions for experts and high-level summaries
for non-technical stakeholders.
  We qualitatively evaluate our method on various images from ImageNet on a
VGG16 model. Our findings suggest that integrating concept-based attribution
methods with large language models can significantly lower the barrier to
interpreting deep neural networks, paving the way for more transparent AI
systems.

</details>


### [59] [Dynamic Reflections: Probing Video Representations with Text Alignment](https://arxiv.org/abs/2511.02767)
*Tyler Zhu,Tengda Han,Leonidas Guibas,Viorica Pătrăucean,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: 本文提出并系统研究视频-文本表示对齐，发现对齐高度依赖测试时视觉与文本信息丰富度，提出测试时标度律并证明其预测性；语义对齐与下游性能、时间推理能力存在关联，说明对齐可作为零样本探针评估视频表示。


<details>
  <summary>Details</summary>
Motivation: 尽管图像-文本对齐已有大量研究，但视频的时间性使得视频-文本对齐尚未得到充分探讨。本研究旨在用零样本对齐分析作为探针，评估视频与文本编码器在时空数据上的表示能力与泛化性。

Method: 通过系统性实验评估现代视频与语言编码器的对齐程度，比较静态图像与多帧视频输入、单条与多条文本描述对对齐的影响；提出参数化测试时标度律并拟合实测数据；在多种下游任务（语义/非语义与时间推理任务）上检验对齐与任务性能的相关性。

Result: 1) 对齐受输入模态信息量影响大，尤其是先进视频编码器下；2) 提出的参数化标度律能预测对齐随信息量变化的趋势；3) 语义对齐与多种下游任务表现呈相关，表明强对齐或可指示通用视频理解能力；4) 时间推理任务与对齐结果相关，提供了更具挑战性的评价基准。

Conclusion: 本文首次系统研究了视频-文本表示对齐，发现对齐性能受测试时视觉（静态图像与多帧视频）和文本（单一字幕与多条描述）信息丰富度影响显著，并提出了参数化的测试时标度律以拟合观察到的行为，显示出良好的预测能力。同时，语义对齐与下游任务表现（语义与非语义任务）之间存在相关性，提示强对齐可能反映通用视频表示能力；此外，将时间推理作为挑战性基准与对齐相关联。

Abstract: The alignment of representations from different modalities has recently been
shown to provide insights on the structural similarities and downstream
capabilities of different encoders across diverse data types. While significant
progress has been made in aligning images with text, the temporal nature of
video data remains largely unexplored in this context. In this work, we conduct
the first comprehensive study of video-text representation alignment, probing
the capabilities of modern video and language encoders. Our findings reveal
several key insights. First, we demonstrate that cross-modal alignment highly
depends on the richness of both visual (static images vs. multi-frame videos)
and text (single caption vs. a collection) data provided at test time,
especially when using state-of-the-art video encoders. We propose parametric
test-time scaling laws that capture this behavior and show remarkable
predictive power against empirical observations. Secondly, we investigate the
correlation between semantic alignment and performance on both semantic and
non-semantic downstream tasks, providing initial evidence that strong alignment
against text encoders may be linked to general-purpose video representation and
understanding. Finally, we correlate temporal reasoning with cross-modal
alignment providing a challenging test-bed for vision and language models.
Overall, our work introduces video-text alignment as an informative zero-shot
way to probe the representation power of different encoders for spatio-temporal
data. Project page can be found at https://video-prh.github.io/

</details>


### [60] [PercHead: Perceptual Head Model for Single-Image 3D Head Reconstruction & Editing](https://arxiv.org/abs/2511.02777)
*Antonio Oroz,Matthias Nießner,Tobias Kirschstein*

Main category: cs.CV

TL;DR: PercHead用DINOv2+SAM2.1感知监督结合双分支编码器与ViT解码器以及Gaussian Splatting，实现了单视图可视角一致的3D人头重建与基于分割图+文本/图像的语义3D编辑，效果达SOTA且对极端视角稳健。


<details>
  <summary>Details</summary>
Motivation: 单张图像重建3D人头与在3D中语义编辑面临遮挡、感知监督稀缺和编辑歧义等挑战，需一种对几何与外观均具有强泛化感知的方案并兼顾编辑可控性与视角一致性。

Method: 双分支编码器+ViT解码器，通过迭代交叉注意力将2D特征提升到3D空间，渲染采用Gaussian Splatting；核心监督信号来自DINOv2与SAM2.1的感知监督；编辑时替换编码器并微调，通过分割图控制几何、文本或参考图像控制外观。

Result: 在新视角合成任务中达到SOTA表现，对极端视角表现尤为稳健；可扩展到语义3D编辑，支持通过交互GUI以分割图绘制与文本/图像提示完成直观的几何雕刻与风格化。

Conclusion: PercHead提出了一个统一且可扩展的单视图3D人头重建与语义3D编辑框架，在新视角合成和极端视角稳健性上达到了SOTA水平，并通过可替换编码器支持基于分割图+文本/图像的可控编辑。

Abstract: We present PercHead, a method for single-image 3D head reconstruction and
semantic 3D editing - two tasks that are inherently challenging due to severe
view occlusions, weak perceptual supervision, and the ambiguity of editing in
3D space. We develop a unified base model for reconstructing view-consistent 3D
heads from a single input image. The model employs a dual-branch encoder
followed by a ViT-based decoder that lifts 2D features into 3D space through
iterative cross-attention. Rendering is performed using Gaussian Splatting. At
the heart of our approach is a novel perceptual supervision strategy based on
DINOv2 and SAM2.1, which provides rich, generalized signals for both geometric
and appearance fidelity. Our model achieves state-of-the-art performance in
novel-view synthesis and, furthermore, exhibits exceptional robustness to
extreme viewing angles compared to established baselines. Furthermore, this
base model can be seamlessly extended for semantic 3D editing by swapping the
encoder and finetuning the network. In this variant, we disentangle geometry
and style through two distinct input modalities: a segmentation map to control
geometry and either a text prompt or a reference image to specify appearance.
We highlight the intuitive and powerful 3D editing capabilities of our model
through a lightweight, interactive GUI, where users can effortlessly sculpt
geometry by drawing segmentation maps and stylize appearance via natural
language or image prompts.
  Project Page: https://antoniooroz.github.io/PercHead Video:
https://www.youtube.com/watch?v=4hFybgTk4kE

</details>


### [61] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 将多模态理解转为图像到SVG代码生成并用CodeVQA评估符号保真性；VCoder通过迭代修订和外部视觉工具显著提升SVG生成效果，缩小视觉中心编码与语言中心编码之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前研究偏重语言中心的代码任务，视觉中心的可执行代码（如SVG）尚未充分探索；作者认为SVG作为紧凑、可解释且可执行的视觉表示，能促进符号化视觉推理与下游任务。

Method: 构建三个子基准（MM-Vet、MMMU、CV-Bench）覆盖常识、专业和视觉感知领域；设计CodeVQA评价协议：用策略模型在渲染SVG上回答问题以检测符号保真性；提出VCoder代理框架，包括“思考与修订”（迭代分析差异并修正SVG代码）和“借助视觉工具行动”（引入检测器、解析器提供结构化线索）。

Result: 前沿视觉语言模型（VLMs）在生成保真SVG方面表现不足；VCoder在总体上比最高Claude-4-Opus提升12.3分；人类与VLM在渲染后的SVG上表现更差，但二者一致性的存在表明符号化视觉表示有潜力。

Conclusion: 本文提出VCode基准，将多模态理解任务表述为从图像生成可执行的SVG代码，从而保留符号化信息以供下游推理使用。通过CodeVQA评估SVG的符号保真性，并提出VCoder框架以改进模型生成性能，实验显示VCoder显著提升了SVG生成质量。

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>


### [62] [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://arxiv.org/abs/2511.02779)
*Yiyang Zhou,Haoqin Tu,Zijun Wang,Zeyu Wang,Niklas Muennighoff,Fan Nie,Yejin Choi,James Zou,Chaorui Deng,Shen Yan,Haoqi Fan,Cihang Xie,Huaxiu Yao,Qinghao Ye*

Main category: cs.CV

TL;DR: MIRA展示了中间视觉图像对复杂推理的关键作用；当前模型依赖文本难以达标，但给图像提示后性能显著上升，表明需要增强模型生成/利用中间视觉表征的能力。


<details>
  <summary>Details</summary>
Motivation: 模仿人类“绘图思考”过程，评估并推动模型在需要生成并利用中间视觉图像（草图、结构图、路径图等）的复杂推理任务上的能力，因为仅文字难以表达或引导此类结构化空间推理。

Method: 提出MIRA基准，包含546道多模态题目，给出三类评估输入（直接图像+问题、文本CoT提示、Visual-CoT带注释图像+文本思考提示），并用pass@k与多数投票评估上界表现；比较私有与开源强模型在不同设置下的表现。

Result: 在仅文本提示下模型表现较差；提供中间视觉线索（Visual-CoT）后，各模型平均相对提升33.7%；扩展搜索空间或文本化Visual-CoT提示仅带来有限提升，表明显式视觉信息更关键。

Conclusion: MIRA基准表明：在需要中间视觉图像的复杂推理任务中，当前多模态大模型在仅靠文本提示下表现显著不足，但提供中间图像提示能显著提升性能，证明“绘图思考”对推理至关重要。

Abstract: We propose MIRA, a new benchmark designed to evaluate models in scenarios
where generating intermediate visual images is essential for successful
reasoning. Unlike traditional CoT methods that rely solely on text, tasks in
MIRA require models to generate and utilize intermediate images - such as
sketches, structural diagrams, or path drawings - to guide their reasoning
process. This setup closely mirrors how humans solve complex problems through
"drawing to think". To solve this, MIRA focuses on tasks that are intrinsically
challenging and involve complex structures, spatial relationships, or reasoning
steps that are difficult to express through language alone. To ensure that our
evaluation data is of high-quality, we include 546 multimodal problems,
annotated with intermediate visual images and final answers. We also propose a
unified evaluation protocol for MIRA that spans three levels of evaluation
input: direct input with image and question only, text-only CoT input with
image and thinking prompts, and Visual-CoT input with both annotated image
clues and textual thinking prompts. To probe the upper bound of model capacity
on our benchmark, we also report pass@k and majority voting accuracies under
different k settings. Experimental results show that existing multimodal large
language models, including strongest private models as well as strong
open-weight models, perform poorly when relying solely on textual prompts.
However, when intermediate visual cues are provided, model performance improves
consistently, yielding an average relative gain of 33.7% across all models and
tasks. We also probe the upper bound by expanding the search space and
designing textual prompts aligned with Visual-CoT, but both yield only limited
improvements compared to our Visual-CoT setting. These results underscore the
critical role of imagined visual information in enabling successful reasoning
on MIRA.

</details>


### [63] [AI-Generated Image Detection: An Empirical Study and Future Research Directions](https://arxiv.org/abs/2511.02791)
*Nusrat Tasnim,Kutub Uddin,Khalid Mahmood Malik*

Main category: cs.CV

TL;DR: 提出统一评测框架，系统比较10种取证方法与7个数据集，使用多指标和可解释性分析，发现当前方法泛化性差且可解释性有限，呼吁更鲁棒可解释算法发展。


<details>
  <summary>Details</summary>
Motivation: 当前取证研究存在基准不统一、训练协议不一致和评价指标有限三大缺陷，导致方法间难以公平比较、真实鲁棒性被掩盖，限制了在安全关键场景中的部署。

Method: 构建统一基准，比较10种最先进的取证方法（包括从头训练、冻结特征、微调三种训练协议），使用7个公开数据集（GAN与扩散模型生成的图像），并采用多种评估指标（准确率、平均精度、ROC-AUC、错误率、类别敏感度）和可解释性分析（置信度曲线、Grad-CAM热图）。

Result: 大量实验显示方法在分布内表现可能很好，但跨模型迁移能力显著下降；不同训练协议与数据类型导致性能差异大；可解释性分析揭示模型决策不稳定性。

Conclusion: 本文提出了一个统一的基准评测框架，用于在可控且可复现的条件下系统评估深度伪造取证方法，揭示现有方法在泛化性和可解释性上的局限，并为研发更健壮方法提供指导。

Abstract: The threats posed by AI-generated media, particularly deepfakes, are now
raising significant challenges for multimedia forensics, misinformation
detection, and biometric system resulting in erosion of public trust in the
legal system, significant increase in frauds, and social engineering attacks.
Although several forensic methods have been proposed, they suffer from three
critical gaps: (i) use of non-standardized benchmarks with GAN- or
diffusion-generated images, (ii) inconsistent training protocols (e.g.,
scratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail
to capture generalization and explainability. These limitations hinder fair
comparison, obscure true robustness, and restrict deployment in
security-critical applications. This paper introduces a unified benchmarking
framework for systematic evaluation of forensic methods under controlled and
reproducible conditions. We benchmark ten SoTA forensic methods (scratch,
frozen, and fine-tuned) and seven publicly available datasets (GAN and
diffusion) to perform extensive and systematic evaluations. We evaluate
performance using multiple metrics, including accuracy, average precision,
ROC-AUC, error rate, and class-wise sensitivity. We also further analyze model
interpretability using confidence curves and Grad-CAM heatmaps. Our evaluations
demonstrate substantial variability in generalization, with certain methods
exhibiting strong in-distribution performance but degraded cross-model
transferability. This study aims to guide the research community toward a
deeper understanding of the strengths and limitations of current forensic
approaches, and to inspire the development of more robust, generalizable, and
explainable solutions.

</details>


### [64] [PLUTO-4: Frontier Pathology Foundation Models](https://arxiv.org/abs/2511.02826)
*Harshith Padigela,Shima Nofallah,Atchuth Naveen Chilaparasetti,Ryun Han,Andrew Walker,Judy Shen,Chintan Shah,Blake Martin,Aashish Sood,Elliot Miller,Ben Glass,Andy Beck,Harsha Pokkalla,Syed Ashar Javed*

Main category: cs.CV

TL;DR: PLUTO-4通过两种ViT架构与大规模DINOv2自监督预训练，在病理图像任务上实现SOTA，兼顾部署效率（4S）与性能极限（4G）。


<details>
  <summary>Details</summary>
Motivation: 扩展先前的PLUTO模型到更大规模与更强的通用性，满足在不同空间尺度与生物学语境下的病理图像分析需求并提升实际部署的效率与性能。

Method: 提出两种Vision Transformer架构：小型高效的PLUTO-4S（支持多尺度FlexiViT与2D-RoPE嵌入）和前沿规模的PLUTO-4G（单补丁尺寸训练以最大化表现力与稳定性）；在551,164张WSI的多机构大规模语料上用基于DINOv2的自监督目标进行预训练。

Result: 在公开和内部基准上取得SOTA表现，PLUTO-4S在部署效率与稳健性上表现优良，PLUTO-4G在多个基准上建立新性能记录，具体表现包括皮肤病理诊断提升约11%。

Conclusion: PLUTO-4显著提升了病理学基础模型在多种下游任务的表现，特别是在皮肤病理诊断等领域取得了显著的性能增益，表明其在研究和临床部署中具有较高的潜力。

Abstract: Foundation models trained on large-scale pathology image corpora have
demonstrated strong transfer capabilities across diverse histopathology tasks.
Building on this progress, we introduce PLUTO-4, our next generation of
pathology foundation models that extend the Pathology-Universal Transformer
(PLUTO) to frontier scale. We share two complementary Vision Transformer
architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model
optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE
embeddings, and a frontier-scale PLUTO-4G model trained with a single patch
size to maximize representation capacity and stability. Both models are
pretrained using a self-supervised objective derived from DINOv2 on a large
multi-institutional corpus containing 551,164 WSIs from 137,144 patients across
over 50 institutions, spanning over 60 disease types and over 100 stains.
Comprehensive evaluation across public and internal benchmarks demonstrates
that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying
spatial and biological context, including patch-level classification,
segmentation, and slide-level diagnosis. The compact PLUTO-4S provides
high-throughput and robust performance for practical deployment, while PLUTO-4G
establishes new performance frontiers across multiple pathology benchmarks,
including an 11% improvement in dermatopathology diagnosis. These diverse
improvements underscore PLUTO-4's potential to transform real-world
applications as a backbone for translational research and diagnostic use cases.

</details>


### [65] [Densemarks: Learning Canonical Embeddings for Human Heads Images via Point Tracks](https://arxiv.org/abs/2511.02830)
*Dmitrii Pozdeev,Alexey Artemov,Ananta R. Bhattarai,Artem Sevastopolsky*

Main category: cs.CV

TL;DR: DenseMarks用ViT把头部像素投射到3D规范立方体嵌入，借助成对跟踪匹配与多任务与空间连续性约束训练，得到覆盖全头且对姿势鲁棒的密集对应表示，提升点匹配与单目头部跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂姿态、遮挡与头发区域的稠密对应上表现不足，需一种鲁棒且覆盖整头（含头发）的统一表示以支持跟踪与几何任务。

Method: 使用Vision Transformer为每个像素预测3D嵌入；构建成对点匹配数据集（来自跟踪器）并以对比损失训练；结合多任务学习（人脸关键点、分割）和通过潜在立方体特征施加空间连续性约束，形成可解释可查询的规范空间。

Result: 在几何感知点匹配与基于单目图像的3D Morphable Model头部跟踪上取得最先进成绩；方法对姿态变化鲁棒并能覆盖全头区域；将公开代码与模型。

Conclusion: DenseMarks提出了一种用于人头的密集对应学习表示，通过将2D像素映射到3D规范立方体内的嵌入，实现跨人物/跨姿势的一致表示，适合语义部位匹配、跟踪与重建。

Abstract: We propose DenseMarks - a new learned representation for human heads,
enabling high-quality dense correspondences of human head images. For a 2D
image of a human head, a Vision Transformer network predicts a 3D embedding for
each pixel, which corresponds to a location in a 3D canonical unit cube. In
order to train our network, we collect a dataset of pairwise point matches,
estimated by a state-of-the-art point tracker over a collection of diverse
in-the-wild talking heads videos, and guide the mapping via a contrastive loss,
encouraging matched points to have close embeddings. We further employ
multi-task learning with face landmarks and segmentation constraints, as well
as imposing spatial continuity of embeddings through latent cube features,
which results in an interpretable and queryable canonical space. The
representation can be used for finding common semantic parts, face/head
tracking, and stereo reconstruction. Due to the strong supervision, our method
is robust to pose variations and covers the entire head, including hair.
Additionally, the canonical space bottleneck makes sure the obtained
representations are consistent across diverse poses and individuals. We
demonstrate state-of-the-art results in geometry-aware point matching and
monocular head tracking with 3D Morphable Models. The code and the model
checkpoint will be made available to the public.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [66] [An Experimental Comparison of Alternative Techniques for Event-Log Augmentation](https://arxiv.org/abs/2511.01896)
*Alessandro Padella,Francesco Vinci,Massimiliano de Leoni*

Main category: cs.DB

TL;DR: 对七种事件日志增强方法的系统比较表明：结合资源队列的随机转移系统方法在生成高质量合成日志方面最优，且事件日志增强优于传统数据增强。


<details>
  <summary>Details</summary>
Motivation: 当前过程挖掘与机器学习结合需要大量事件日志，真实日志不足时需通过增强生成更多合成轨迹；但缺乏对现有增强技术的全面比较，难以指导实践选择合适方法。

Method: 针对四个维度（相似性、预测信息保持、信息损失/增强、计算时间）对七种方法与基线STS方法进行实验评估；使用八个真实世界事件日志作为测试集，并比较传统数据增强方法。

Result: 实验表明：事件日志增强技术普遍优于传统数据增强（后者未考虑过程约束）；在多项指标综合考量下，基于STS并结合资源队列建模的方法表现最好；不同方法在相似性、预测信息保持与计算效率上存在权衡。

Conclusion: 本文对七种先进的事件日志增强方法在八个事件日志上的效果进行了系统比较，结论指出：结合资源队列建模的随机转移系统（stochastic transition system，STS）方法在多项评估指标上表现最佳，能够生成更高质量的合成事件日志。

Abstract: Process mining analyzes and improves processes by examining transactional
data stored in event logs, which record sequences of events with timestamps.
However, the effectiveness of process mining, especially when combined with
machine or deep learning, depends on having large event logs. Event log
augmentation addresses this limitation by generating additional traces that
simulate realistic process executions while considering various perspectives
like time, control-flow, workflow, resources, and domain-specific attributes.
Although prior research has explored event-log augmentation techniques, there
has been no comprehensive comparison of their effectiveness. This paper reports
on an evaluation of seven state-of-the-art augmentation techniques across eight
event logs. The results are also compared with those obtained by a baseline
technique based on a stochastic transition system. The comparison has been
carried on analyzing four different aspects: similarity, preservation of
predictive information, information loss/enhancement, and computational times
required. Results show that, considering the different criteria, a technique
based on a stochastic transition system combined with resource queue modeling
would provide higher quality synthetic event logs. Event-log augmentation
techniques are also compared with traditional data-augmentation techniques,
showing that the former provide significant benefits, whereas the latter fail
to consider process constraints.

</details>


### [67] [Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows](https://arxiv.org/abs/2511.01942)
*Khalil Rejiba,Sang-Hyeok Lee,Christina Gasper,Martina Freund,Sandra Korte-Kerzel,Ulrich Kerzel*

Main category: cs.DB

TL;DR: 作者提出并实现了一个基于openBIS的综合研究数据管理基础设施，通过自动化元数据提取、大对象存储与交互式溯源支持跨机构的缺陷相图研究，实现可追溯与可复用的数据流。


<details>
  <summary>Details</summary>
Motivation: CRC 1394需要统一描述晶体缺陷态的缺陷相图，这要求系统性整合跨团队和地点的异构实验与仿真数据，提升数据可复现性与可重用性，从而推动材料设计。

Method: 将openBIS电子实验记录/实验室信息管理系统与新开发的伴随应用结合，提供大对象存储、异构与专有格式的自动元数据提取、交互式溯源图以及自动化报告与分析工作流，实现实验、数据与分析的无缝工作流。

Result: 建立了一个综合的RDM系统，关键技术包括openBIS和扩展应用，成功实现了易用的大数据存储、自动元数据捕获与分布式数据的联合访问，降低了数据管理成本并支持跨机构缺陷相图构建。

Conclusion: 构建统一的RDM基础设施能显著降低数据采集与整理的摩擦，促进跨机构可追溯与可复用的数据集，从而加速缺陷相图的构建与材料设计研究。

Abstract: Defect phase diagrams provide a unified description of crystal defect states
for materials design and are central to the scientific objectives of the
Collaborative Research Centre (CRC) 1394. Their construction requires the
systematic integration of heterogeneous experimental and simulation data across
research groups and locations. In this setting, research data management (RDM)
is a key enabler of new scientific insight by linking distributed research
activities and making complex data reproducible and reusable.
  To address the challenge of heterogeneous data sources and formats, a
comprehensive RDM infrastructure has been established that links experiment,
data, and analysis in a seamless workflow. The system combines: (1) a joint
electronic laboratory notebook and laboratory information management system,
(2) easy-to-use large-object data storage, (3) automatic metadata extraction
from heterogeneous and proprietary file formats, (4) interactive provenance
graphs for data exploration and reuse, and (5) automated reporting and analysis
workflows. The two key technological elements are the openBIS electronic
laboratory notebook and laboratory information management system, and a newly
developed companion application that extends openBIS with large-scale data
handling, automated metadata capture, and federated access to distributed
research data.
  This integrated approach reduces friction in data capture and curation,
enabling traceable and reusable datasets that accelerate the construction of
defect phase diagrams across institutions.

</details>


### [68] [InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations](https://arxiv.org/abs/2511.02002)
*Xiangru Jian,Zhengyuan Dong,M. Tamer Özsu*

Main category: cs.DB

TL;DR: 提出了一种结合规则和LLM的交互式SPARQL生成与细化系统，通过从AST生成结构化解释并由LLM润色，支持用户交互和自我细化，显著提升了查询准确性与可用性。


<details>
  <summary>Details</summary>
Motivation: SPARQL语法复杂且需要对底层数据结构有深入理解，导致非专业用户难以编写正确查询。利用自然语言解释和交互细化可以降低门槛，提高查询正确率和用户满意度。

Method: 先基于规则从SPARQL的AST提取结构化解释，再使用LLM对这些解释进行语言润色；支持用户通过直接反馈或让LLM自我细化来迭代修改查询；系统架构将规则引擎与LLM模块结合以保证准确性与可读性。

Result: 在标准基准上，InteracSPARQL在查询准确率、解释清晰度和用户满意度方面显著优于基线方法；实验还表明规则方法与LLM润色的结合在可解释性与鲁棒性上具有协同效应。

Conclusion: 该论文提出了InteracSPARQL，一个结合规则方法与大型语言模型（LLM）用于生成和细化SPARQL查询的交互系统，通过从SPARQL抽象语法树(AST)直接生成结构化自然语言解释并再由LLM润色，从而提高非专家用户的理解与迭代改进能力。

Abstract: In recent years, querying semantic web data using SPARQL has remained
challenging, especially for non-expert users, due to the language's complex
syntax and the prerequisite of understanding intricate data structures. To
address these challenges, we propose InteracSPARQL, an interactive SPARQL query
generation and refinement system that leverages natural language explanations
(NLEs) to enhance user comprehension and facilitate iterative query refinement.
InteracSPARQL integrates LLMs with a rule-based approach to first produce
structured explanations directly from SPARQL abstract syntax trees (ASTs),
followed by LLM-based linguistic refinements. Users can interactively refine
queries through direct feedback or LLM-driven self-refinement, enabling the
correction of ambiguous or incorrect query components in real time. We evaluate
InteracSPARQL on standard benchmarks, demonstrating significant improvements in
query accuracy, explanation clarity, and overall user satisfaction compared to
baseline approaches. Our experiments further highlight the effectiveness of
combining rule-based methods with LLM-driven refinements to create more
accessible and robust SPARQL interfaces.

</details>


### [69] [Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements](https://arxiv.org/abs/2511.02062)
*Yuting Yang,Tiancheng Yuan,Jamal Hashim,Thiago Garrett,Jeffrey Qian,Ann Zhang,Yifan Wang,Weijia Song,Ken Birman*

Main category: cs.DB

TL;DR: 为满足AI应用的SLO需求，Vortex提出SLO优先的推理流水线，显著降低并稳定延迟，通常在相同SLO下支持超过两倍请求率，RDMA环境下优势更大。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理和集成到终端应用的AI增加，对实时性和服务级别延迟目标（SLO）的需求增长，现有服务平台（如TorchServe、Ray Serve）为追求高吞吐采用批处理，导致尾延迟不稳定，难以满足SLO。Vortex旨在解决这一矛盾。

Method: Vortex通过重构推理流水线（可能包含调度、请求拆分、优先级控制、网络优化如RDMA利用等）来避免传统批处理带来的延迟波动，从而在相同任务下实现更低且更稳定的延迟表现。

Result: 在多个负载场景下，Vortex的流水线相比TorchServe和Ray Serve能在同一SLO目标下支持超过两倍的请求率，并且在启用RDMA时优势更明显，表明其在延迟和吞吐权衡上更适合SLO驱动的场景。

Conclusion: Vortex提出了以SLO为优先的ML推理与知识检索服务方案，通过设计专注于低延迟和稳定性的流水线，克服了现有基于批处理以追求高吞吐而导致的尾延迟不可预测问题。

Abstract: There is growing interest in deploying ML inference and knowledge retrieval
as services that could support both interactive queries by end users and more
demanding request flows that arise from AIs integrated into a end-user
applications and deployed as agents. Our central premise is that these latter
cases will bring service level latency objectives (SLOs). Existing ML serving
platforms use batching to optimize for high throughput, exposing them to
unpredictable tail latencies. Vortex enables an SLO-first approach. For
identical tasks, Vortex's pipelines achieve significantly lower and more stable
latencies than TorchServe and Ray Serve over a wide range of workloads, often
enabling a given SLO target at more than twice the request rate. When RDMA is
available, the Vortex advantage is even more significant.

</details>


### [70] [Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets](https://arxiv.org/abs/2511.02096)
*Savo Tomovic*

Main category: cs.DB

TL;DR: 用组合数系统对多对多关系进行唯一编码，新增列存储编码并通过Rank-Join重建关系，旨在替代桥表处理多值维度问题，论文提出概念框架但缺少大规模实证。


<details>
  <summary>Details</summary>
Motivation: 解决数据仓库建模中多值维度和桥表（bridge tables）设计与实现的复杂性，减少物理表数量并将组合信息封装到主表中以简化管理。

Method: 基于组合数系统（组合编号），对I中元素的k组合进行编号，G中的每个实体通过(h,k)对或单一编码表示其对应的I的组合；通过新增单列存储编码并定义新的Rank-Join关系代数运算来查询和重建组合关系。

Result: 提出了编码方案和Rank-Join操作的概念性框架，说明可将组合关系信息封装为单列候选键并支持基于编码的查询。但文中未详述编码在大规模数据、变化频繁的组合或实际性能/可扩展性方面的实证评估。

Conclusion: 提出了一种将多对多关系以组合编码形式嵌入实体表G中，避免独立关系表存储的方法；引入Rank-Join运算并将新列作为候选键以实现关联查询。

Abstract: In this paper we propose an approach to implement specific relation-ship set
between two entities called combinatorial relationship set. For the
combinatorial relationship set B between entity sets G and I the mapping
cardinality is many-to-many. Additionally, entities from G can be uniquely
encoded with a pair of values (h, k) generated with the procedure for numbering
combinations of entities from I. The encoding procedure is based on
combinatorial number system that provides a representation of all possible k
-combinations of a set of n elements by a single number. In general
many-to-many relationship sets are represented by a relation or table, while
the combinatorial relationship is not physically stored as separate table.
However, all information is encapsulated into a single column added to G. The
new column is a candidate key in G. Additional operation named Rank-Join to
fundamental relational-algebra is presented to combine information from g and i
associated with a combinatorial relationship set. Motivation for combinatorial
relationship originates from challenges in designing and implementing
multivalued dimensions and bridge tables in data-warehouse models.

</details>


### [71] [Accelerating Graph Similarity Search through Integer Linear Programming](https://arxiv.org/abs/2511.02611)
*Andrea D'Ascenzo,Julian Meffert,Petra Mutzel,Fabrizio Rossi*

Main category: cs.DB

TL;DR: 用ILP构造更强的下界并设计层次化过滤与阈值感知的ILP验证，有效加速编辑距离约束下的图相似性检索，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GED精确计算复杂度高（NP-hard），在大规模图库中直接验证不可行。通过在filter-and-verification框架中改进过滤下界，可以显著减少需要执行昂贵验证的候选，从而提升整体相似性检索效率。

Method: 首先构造一个基于ILP的下界公式，并证明该下界主导已有的分支匹配（branch match）下界；然后设计一个层次化的下界算法序列，按从低成本到高成本逐步筛选；最后提出一个利用给定阈值改进的ILP模型，对每个候选实例求解以确定是否通过阈值过滤。

Result: 在一个经过良好评估的测试集上进行大量实验，结果表明所提方法在多数阈值设置下显著优于现有最先进算法，减少了运行时间和候选数量。

Conclusion: 本文提出了基于整数线性规划（ILP）的下界估计方法，并结合下界层次结构与利用阈值的ILP新模型，用于图相似性检索在编辑距离约束下的过滤阶段，从而提高过滤效率并减少需要精确验证的候选图数量。

Abstract: The Graph Edit Distance (GED) is an important metric for measuring the
similarity between two (labeled) graphs. It is defined as the minimum cost
required to convert one graph into another through a series of (elementary)
edit operations. Its effectiveness in assessing the similarity of large graphs
is limited by the complexity of its exact calculation, which is NP-hard
theoretically and computationally challenging in practice. The latter can be
mitigated by switching to the Graph Similarity Search under GED constraints,
which determines whether the edit distance between two graphs is below a given
threshold. A popular framework for solving Graph Similarity Search under GED
constraints in a graph database for a query graph is the
filter-and-verification framework. Filtering discards unpromising graphs, while
the verification step certifies the similarity between the filtered graphs and
the query graph. To improve the filtering step, we define a lower bound based
on an integer linear programming formulation. We prove that this lower bound
dominates the effective branch match-based lower bound and can also be computed
efficiently. Consequently, we propose a graph similarity search algorithm that
uses a hierarchy of lower bound algorithms and solves a novel integer
programming formulation that exploits the threshold parameter. An extensive
computational experience on a well-assessed test bed shows that our approach
significantly outperforms the state-of-the-art algorithm on most of the
examined thresholds.

</details>


### [72] [EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes](https://arxiv.org/abs/2511.02674)
*Tim Otto*

Main category: cs.DB

TL;DR: EasyTUS用LLM生成表嵌入并结合向量检索，构建了高效可扩展的表并集搜索系统，并提供TUSBench用于标准化评测，实验证明其在准确率与速度上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数据湖中异构数据原始保存虽然便于摄取，但把数据准备与查询复杂度转移到数据发现任务上，尤其是表并集检索（TUS），需要有效方法在大量表中找到可并集的表。

Method: 方法包含三模块：1) Table Serialization：对表进行统一格式化与采样；2) Table Representation：利用大模型对序列化后的表生成语义嵌入；3) Vector Search：使用近似最近邻索引加速向量检索。并设计TUSBench作为统一评测基准。

Result: 通过TUSBench评测，EasyTUS在MAP上平均最多提升34.3%，数据准备阶段最多加速79.2倍，查询处理最快7.7倍；在缺失元数据场景下仍保持强性能，显示其稳健性与适应性。

Conclusion: EasyTUS提出了一种基于LLM的模块化TUS框架，能够在数据湖中高效、可扩展地完成表并集检索；通过Table Serialization、Table Representation（LLM生成嵌入）和Vector Search三步管道实现语义匹配与快速检索。

Abstract: Data lakes enable easy maintenance of heterogeneous data in its native form.
While this flexibility can accelerate data ingestion, it shifts the complexity
of data preparation and query processing to data discovery tasks. One such task
is Table Union Search (TUS), which identifies tables that can be unioned with a
given input table. In this work, we present EasyTUS, a comprehensive framework
that leverages Large Language Models (LLMs) to perform efficient and scalable
Table Union Search across data lakes. EasyTUS implements the search pipeline as
three modular steps: Table Serialization for consistent formatting and
sampling, Table Representation that utilizes LLMs to generate embeddings, and
Vector Search that leverages approximate nearest neighbor indexing for semantic
matching. To enable reproducible and systematic evaluation, in this paper, we
also introduce TUSBench, a novel standardized benchmarking environment within
the EasyTUS framework. TUSBench supports unified comparisons across approaches
and data lakes, promoting transparency and progress in the field. Our
experiments using TUSBench show that EasyTUS consistently outperforms most of
the state-of the-art approaches, achieving improvements in average of up to
34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,
and up to 7.7x faster query processing performance. Furthermore, EasyTUS
maintains strong performance even in metadata-absent settings, highlighting its
robustness and adaptability across data lakes.

</details>


### [73] [Relational Deep Dive: Error-Aware Queries Over Unstructured Data](https://arxiv.org/abs/2511.02711)
*Daren Chao,Kaiwen Chen,Naiqing Guan,Nick Koudas*

Main category: cs.DB

TL;DR: ReDD 动态为每个查询发现可连接最小模式，基于 LLM 隐状态的轻量分类器进行表格填充，并通过统计校准的 SCAPE 与混合策略实现低于1%的错误率与100%模式召回，适合高要求的分析任务。


<details>
  <summary>Details</summary>
Motivation: 非结构化文本普遍存在，但分析查询需要结构化表示，现有方法（如 RAG）缺乏模式意识且跨文档对齐困难，导致高错误率，特别是在高风险分析场景中需要可控的低错误率与可解释的错误检测机制。

Method: 两阶段流水线：第一阶段为迭代式模式发现（ISD），针对每个查询识别最小且可连接的关系模式；第二阶段为表格数据填充（TDP），利用 LLM 隐状态训练轻量分类器完成实体/关系抽取并进行纠错。核心还包括 SCAPE（统计校准的错误检测，提供覆盖性保证）与 SCAPE-HYB（在准确性与人工校正成本之间做混合优化）。

Result: 在多种数据集上的实验表明：ReDD 将抽取错误率从高达约30%降低到低于1%；模式召回达到100%；整体保持高精度。模块化设计允许在精度与人工成本之间细粒度控制。

Conclusion: ReDD 提出了一种面向查询的关系化信息抽取框架，通过动态发现最小可连接模式并结合基于 LLM 隐层的轻量分类器进行表格填充，辅以统计校准的错误检测与混合人机校正策略，能在多数据集上将抽取错误率显著降低（最高从约30%降至低于1%），同时保持模式完整性与高精度，适用于高敏感度分析场景。

Abstract: Unstructured data is pervasive, but analytical queries demand structured
representations, creating a significant extraction challenge. Existing methods
like RAG lack schema awareness and struggle with cross-document alignment,
leading to high error rates. We propose ReDD (Relational Deep Dive), a
framework that dynamically discovers query-specific schemas, populates
relational tables, and ensures error-aware extraction with provable guarantees.
ReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)
identifies minimal, joinable schemas tailored to each query, and (2) Tabular
Data Population (TDP) extracts and corrects data using lightweight classifiers
trained on LLM hidden states. A main contribution of ReDD is SCAPE, a
statistically calibrated method for error detection with coverage guarantees,
and SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy
and human correction costs. Experiments across diverse datasets demonstrate
ReDD's effectiveness, reducing data extraction errors from up to 30% to below
1% while maintaining high schema completeness (100% recall) and precision.
ReDD's modular design enables fine-grained control over accuracy-cost
trade-offs, making it a robust solution for high-stakes analytical queries over
unstructured corpora.

</details>
